{"title": "Kolmogorov-Arnold Fourier Networks", "authors": ["Jusheng Zhang", "Yijia Fan", "Kaitong Cai", "Keze Wang"], "abstract": "Although Kolmogorov-Arnold (Kolmogorov, 1933) based interpretable networks (KAN) have strong theoretical expressiveness, they face significant parameter explosion and high-frequency feature capture challenges in high-dimensional tasks. To address this issue, we propose the Kolmogorov-Arnold-Fourier Network (KAF), which effectively integrates trainable Random Fourier Features (RFF (Tancik et al., 2020; Bracewell & Bracewell, 1986)) and a novel hybrid GELU-Fourier activation mechanism to balance parameter efficiency and spectral representation capabilities. Our key technical contributions include: (1) merging KAN's dual-matrix structure through matrix association properties to substantially reduce parameters; (2) introducing learnable RFF initialization strategies to eliminate spectral distortion in high-dimensional approximation tasks; (3) implementing an adaptive hybrid activation function that progressively enhances frequency representation during the training process. Comprehensive experiments demonstrate the superiority of our KAF across various domains including vision, NLP, audio processing, and differential equation-solving tasks, effectively combining theoretical interpretability with practical utility and computational efficiency. We will release the source code of our method in accordance with the review policy.", "sections": [{"title": "1. Introduction", "content": "The interpretability of deep neural networks (Howard et al., 2017; Han et al., 2016) has long been one of the core challenges in the field of machine learning. The Kolmogorov-Arnold(Liu et al., 2024; Kolmogorov, 1933) theorem states that any continuous multivariate function can be represented through a combination of univariate functions (Mhaskar & Micchelli, 1996; Barron, 1993). This theory provides significant inspiration for the design of interpretable neural network architectures. Based on this theory, Kolmogorov-Arnold Networks (KAN) (Liu et al., 2024; Schmidt-Hieber, 2021) have been proposed, which replace the fixed activation functions in traditional multilayer perceptrons (MLPs (Rumelhart et al., 1986)) with learnable B-spline (De Boor, 1972) basis functions, theoretically demonstrating strong expressive potential and flexibility. By introducing trainable nonlinear activation functions, KAN enables the network to dynamically adjust the shape of the activation functions according to the characteristics of the data, thereby enhancing the adaptability and performance of the model.\nHowever, despite the significant theoretical advantages of KAN, its practical application faces two fundamental issues that severely limit its generalization and adoption in high-dimensional tasks: Inefficient Parameter Utilization: The dual-matrix architecture of KAN (i.e., the activation function matrix and the B-spline coefficient matrix) leads to a rapid increase in the number of parameters. Compared to traditional MLPs, where the parameter count scales with input x output + bias, KAN's parameter count grows several times larger. This makes it challenging to apply KAN to high-dimensional tasks such as computer vision. The explosion in parameters (Xu et al., 2019; Mhaskar & Micchelli, 1996; Barron, 1993) not only increases the storage and computational costs of the model but also significantly reduces the efficiency of both training and inference (Tan & Le, 2020; 2021). The B-spline (De Boor, 1972; Eldar & Unser, 2021; Aziznejad et al., 2023; Li et al., 2023; Wu et al., 2022)basis functions employed by KAN exhibit inherent spectral limitations when performing function approximation in high-dimensional spaces. The smoothness of B-spline basis functions makes it difficult to accurately capture high-frequency components of signals, leading to suboptimal performance when processing data with rich spectral features, such as natural images or audio waveforms. This limitation in spectral representation capability adversely affects the model's performance and stability in practical applications(Xu et al., 2019). This dilemma creates a paradox between theory and practice: although KAN"}, {"title": "2. Related Work", "content": "Multi-Layer Perceptrons and Current Challenges. The design and optimization of deep learning (Touvron et al., 2021)models remain central to machine learning research. Traditional MLPs(Rumelhart et al., 1986), among the earliest neural networks (Han et al., 2016), offer simplicity and scalability, with rich theoretical foundations. While ResNet (He et al., 2015b) and Transformer (Vaswani et al., 2023) models have shown remarkable performance across various tasks, MLPs face challenges in theoretical interpretability and practical bottlenecks. Traditional activation functions like ReLU (Nair & Hinton, 2010; Glorot et al., 2011) and Sigmoid (Elfwing et al., 2017) often fail to adapt to complex data, and despite their efficiency, MLPs struggle with high-frequency features and complex distributions. Improving activation mechanisms and parameter efficiency has become crucial for enhancing MLPs' adaptability to high-dimensional data.\nKolmogorov-Arnold Networks and Scalability Issues. The Kolmogorov-Arnold (Fan et al., 2019) Theorem underpins networks for approximating continuous multivariable functions. The pioneering KAN replaced fixed activations with B-spline (De Boor, 1972) functions but faces challenges in high-dimensional applications due to parameter explosion and GPU inefficiency. Recent improvements include KAN-Transformer, MLP-KAN with sparse parameters, and FAN(Dong et al., 2024) with Fourier activations, all seeking to balance interpretability with scalability.\nEnhancing Spectral Representation with KAF. To address high-frequency modeling challenges, Random Fourier Features (RFF (Rahimi & Recht, 2007b; Yu et al., 2016)) enable spectral domain mapping, with variants like Learnable RFF and SIREN enhancing expressiveness. Our proposed KAF incorporates GELU and learnable Fourier features, with scale factor control and variance initialization. This reduces parameters while improving spectral representation. KAF maintains KAN's interpretability while enhancing scalability and efficiency, showing superior performance in capturing high-frequency(Sitzmann et al., 2020) details across NLP, vision, audio, and traditional machine learning tasks."}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Kolmogorov-Arnold Theorem", "content": "The Kolmogorov-Arnold (Bracewell & Bracewell, 1986; Kolmogorov, 1933) theorem, proposed by Soviet mathematicians Vladimir Arnold and Andrey Kolmogorov in the 1950s, states that any continuous (Kidger & Lyons, 2020; Maiorov & Pinkus, 2021) multivariate function f : [0, 1]d \u2192 R can be represented as a superposition of univariate functions:\n$$f(x_1, x_2,...,x_d) = \\sum_{q=1}^{2d+1} \\Phi_q(\\sum_{p=1}^{d} \\phi_{q,p}(x_p)),$$\nwhere Iq : R \u2192 R and $q,p: [0,1] \u2192 R are univariate continuous functions. This theorem provides a theoretical foundation for dimensionality reduction in high-dimensional function approximation.\nIn (Berner et al., 2022b; Poggio et al., 2017), the theorem suggests that high-dimensional functions can be captured"}, {"title": "3.2. Kolmogorov-Arnold Network (KAN)", "content": "Although the Kolmogorov-Arnold (Bracewell & Bracewell, 1986)theorem was proposed quite early, (KAN (Liu et al., 2024)) is proposed according to this theorem, demonstrating that this structure can, in a sense, serve as an alternative to traditional MLP models. In the KAN network, each layer can be represented by the following formula:\n$$f(x) = \\Phi \\circ x = \\sum_{i=1}^{d_{in}} \\Phi_{1,i}(x_i) \\sum_{i=1}^{d_{in}},$$\nwhere I is a matrix of basis functions. This formula aligns with the form of the Kolmogorov-Arnold theorem. However, in practical applications, they chose B-spline (Bach, 2016) basis functions as the basis functions q,p, and added an external activation function SILU to guide the update of the KAN layer(Ramachandran et al., 2017; Elfwing et al., 2017). The formula can be expressed as\n$\\varphi(x) = w_h \\text{silu}(x) + w_s \\text{spline}(x)$,\n$\\text{spline}(x) = \\sum_i c_i B_i(x)$.\nAmong them, I represents the basis function matrix, where B-spline basis functions and the SiLU activation function"}, {"title": "3.3. KAF: Kolmogorov-Arnold Fourier Network", "content": "In the previous discussion, we pointed out that traditional networks based on the Kolmogorov-Arnold theorem (KAN) often face multiple challenges in practical applications. To address these issues, we propose an alternative approach\u2014KAF (Kolmogorov-Arnold Fourier Network). By replacing B-spline basis functions with Random Fourier Features (RFF(Fathony et al., 2021; Tancik et al., 2020; Bracewell & Bracewell, 1986)), which are more efficient for GPU acceleration, and introducing hybrid spectral correction for the activation functions, the network retains the advantages of the Kolmogorov-Arnold theory while achieving training efficiency and inference speed closer to that of MLPs (Rumelhart et al., 1986). This section provides a detailed explanation of the overall architecture of the KAF network, the utilization of Random Fourier Features within the network, the design and scaling principles of the GELU-Fourier hybrid activation function, as well as the RFF weight initialization strategy and the theoretical justification for \u03c3 = 1.64.\nOverall Architecture. In the overall framework of KAF, we follow the core idea of the Kolmogorov-Arnold theorem, which approximates high-dimensional target functions through the composition of several low-dimensional learnable functions. Unlike the KAN network, which directly utilizes B-spline basis functions, KAF employs Random"}, {"title": "GELU-Fourier Hybrid Activation.", "content": "Design Motivation: To balance low-frequency smoothness and high-frequency representation capability, we propose a hybrid activation function:\n$$H(x) = \\alpha \\odot GELU(x) + \\beta \\cdot V\\psi(x)$$\nwhere \u03b1, \u03b2 \u2208 Rd are learnable channel-wise scaling factors, V \u2208 Rd\u00d72k is the frequency-domain projection matrix, and \\odot represents element-wise multiplication. Initialization Strategy of KAF:\n$\\alpha^{(0)} \\leftarrow 1, \\beta^{(0)} \\leftarrow \\epsilon 1, (\\epsilon = 10^{-2}), V \\sim N(0, 0.01)$.\nThe dynamic property of this initialization manifests in the following way: At the early stage of training, the small initialization of the high-frequency component \u03b2 ensures that its norm is much smaller than that of the low-frequency component a, prioritizing the learning of low-frequency features. As training progresses, the natural growth of weights allows the norm of \u03b2 to increase approximately proportionally to the training time t, thereby gradually enhancing the representation of high-frequency features."}, {"title": "Implementation of the Kolmogorov-Arnold Architecture and RFF Initialization.", "content": "Theorem Definition: The Kolmogorov-Arnold representation theorem states that any continuous function f \u2208 C([0, 1]d) can be expressed as a finite composition of univariate functions:\n$$f(x) = \\sum_{q=0}^{2d} \\Phi_q(\\sum_{p=1}^{d} \\phi_{q,p} (x_p));$$\nwhere q,p: IR \u2192 R are univariate nonlinear functions, and Iq : R \u2192 R are composition functions.\nArchitecture Implementation: We modularize the neural network to efficiently approximate this mapping, establishing the following correspondences:\n$$\\Phi_{q,p}(x_p) \\rightarrow GELU(w_1^{(q)} x_p + b_1^{(q)}) + \\text{RFF}(x_p),$$\n$$\\Phi_q(.) \\rightarrow \\text{Linear}(.)$$\nHere, VRFF(xp) = [cos(w1xp + 01), sin(w\u2081xp + 01),...] represents the Random Fourier Features (RFF), and aq, Bq \u2208 Rk are learnable modulation parameters.\nSpectral Complementarity Mechanism: - GELU Properties: The activation function \u03c3(wx + b) provides a smooth gating effect in the low-frequency domain, satisfying E[\u03c3(wx)] \u03b1"}, {"title": "4. Experiments", "content": "The objective of this experiment is to evaluate the performance of mainstream models when their MLP(Rumelhart et al., 1986) or KAN components are replaced with KAF.\nBy maintaining consistent parameters, we conducted experiments across a variety of tasks including simple visual tasks, NLP tasks, audio tasks, and machine learning tasks, utilizing models such as ResNet-18 (He et al., 2015b), DeiT (Touvron et al., 2021) (from the MLP-KAN architecture), MLPmixer (Berner et al., 2022a), and GPT-2 (Brown et al., 2020). Additionally, we tested the performance of KAF in function fitting and solving differential equations. All experiments employed either the Adam (Kingma & Ba, 2014) optimizer, with learning rates appropriately selected according to the specific task. The experimental environment was set up with RTX 4090D GPU."}, {"title": "4.1. Comprehensive Evaluation Based on Kanbefair", "content": "Based on Kanbefair (Yu et al., 2024), we conducted a comprehensive evaluation of KAF on vision (Dosovitskiy et al., 2021), NLP (Brown et al., 2020), audio, and machine learning tasks to compare its performance with existing models. We selected MLP (with GELU activation), KAN, FAN (Dong et al., 2024), and GPKAN (Yang & Wang, 2024) for experimentation. Note that we use the original input by default in all experiments, leaving layernorm disabled."}, {"title": "4.1.1. EXPERIMENTAL SETUP", "content": "To account for differences in model convergence speeds, KAN was trained for 100 epochs, while other models were trained for 40 epochs. During training, the maximum test accuracy was recorded as the primary evaluation metric, focusing on classification tasks across 8 vision datasets, 2 NLP datasets, 2 audio datasets, and 4 additional machine learning datasets. For all models, hidden layer sizes were explored in the range of 2, 4, 8, 16, 32, 64, 128, 256, 512, and 1024. For KAF, the key parameters included 9 grids, an activation expectation of 1.64, and GELU as the activation function. For KAN, the number of B-spline grids was set to 3, 5, 10, or 20; B-spline degrees were configured as 2, 3, or 5; and the B-spline range was [-1, 1], [-2, 2], or [-4, 4]. For MLP, we experimented with both GELU (Hendrycks & Gimpel, 2016) and ReLU (Nair & Hinton, 2010; Glorot et al., 2011) activations. FAN's p_ratio was set to 0.25 to regulate the proportion of preserved information during transformation, and GPKAN used GELU-based initialization for enhanced convergence."}, {"title": "4.1.2. EXPERIMENTAL RESULTS", "content": "As shown in Figure 2, we conducted a systematic comparison of KAF and several baseline models (e.g., ResNet, ViT, MLP-Mixer) on vision datasets, including MNIST (LeCun et al., 1998), EMNIST (Cohen et al., 2017), KMNIST (Clanuwat et al., 2018), CIFAR 10/100 (Krizhevsky et al., 2009), and SVHN (Netzer et al., 2011). The results in Figure 2 demonstrate that KAF consistently achieves the"}, {"title": "4.2. Experiments on Using KAF Components in Complex Vision Models", "content": "To comprehensively evaluate the performance of KAF in large-scale vision models after replacing corresponding layers, we assess its impact on accuracy, computation time, and generalization across various models. In this section, we conduct comparative experiments on commonly used large-scale vision models, including ResNet-18, ViT-Tiny, and MLP-Mixer-S/16, as well as the latest model incorporating KAN components, MLP_KAN (based on DeiT). In each case, we replace the original KAN or MLP modules with KAF, KAN, MLP, GPKAN, and FAN, respectively, to analyze their performance differences."}, {"title": "4.2.1. EXPERIMENTAL SETUP", "content": "The experiments utilize CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), and ImageNet-1K for training and testing. ResNet-18 and MLP-Mixer-S/16 are trained for 100 epochs, while ViT-Tiny (He et al., 2024; Dosovitskiy et al., 2021) and MLP_KAN are trained for 300 epochs. All other training hyperparameters follow the official recommended settings for each model. Both KAF and MLP use GELU as the activation function; for KAN, we set the grid size to 5 and the B-spline degree to 3; FAN's p_ratio is fixed at 0.25; GPKAN adopts GELU-based initialization; LayerNorm is disabled by default; and Dropout follows each model's standard setting."}, {"title": "4.2.2. EXPERIMENTAL RESULTS", "content": "Table 1 summarizes performance across multiple datasets when various feature Mixers replace the original modules. KAF generally meets or exceeds baseline accuracy while maintaining a reasonable parameter count and computational cost. For instance, in ResNet-18 on CIFAR-10, KAF achieves 91.72% Top-1 accuracy (compared to MLP's 91.19%), and on ImageNet-1K, it improves MLP-Mixer from 63.5% to 64.7%. Although compact approaches like FAN can reduce parameters, they often yield lower accuracy (e.g., 58.2% on ImageNet-1K). Similarly, with ViT-T/16, KAF's moderate parameter increment still outper-"}, {"title": "4.3. Experiments on LLMs with KAF Components", "content": "To evaluate the potential of KAF in language models, we integrated it into the GPT-2 architecture by replacing the Feed-Forward Network (FFN)'s MLP with KAF or KAN. We then trained and evaluated the models on large-scale text datasets, assessing their impact on language modeling quality and model complexity."}, {"title": "4.3.1. EXPERIMENTAL SETUP", "content": "We conducted experiments using OpenWebText and WikiText, two widely used text datasets. The base model was GPT-2 (Small version), where the two-layer MLP in the Feed-Forward Network (FFN) was replaced with KAF or KAN while maintaining the same parameter scale. All other Transformer configurations(Vaswani et al., 2023), including multi-head attention, token embeddings, and positional encoding, remained consistent with the official GPT-2 implementation."}, {"title": "4.3.2. EXPERIMENTAL RESULTS", "content": "Table 2 presents a comparison of GPT-2 using MLP, KAF, and KAN as the FFN components on the WikiText and OpenWebText datasets. The results show that KAF boosts language modeling performance and training efficiency. On WikiText, it reduces PLL from 184.53 to 180.85 while cutting training time from 20h37m to 19h20m, indicating improved performance without significant overhead. In contrast, KAN converges poorly, with PLL escalating to 39,782 and training time rising sharply due to large parameter"}, {"title": "4.4. Performance of KAF in Function Approximation and Differential Equation Solving Tasks", "content": "To comprehensively validate the capability of KAF in complex function approximation and PDE solving(Raissi et al., 2017), we have carefully designed experiments covering a wide range of complexities, dimensions, and degrees of nonlinearity. First, we constructed eight function approximation tasks to evaluate KAF's performance in capturing complex nonlinear relationships. Second, we selected four PDE-solving problems involving multiple physical parameters to assess their applicability in scientific computing. Various hyperparameter configurations were employed in the experiments to ensure the reliability and generalization ability of the results."}, {"title": "4.4.1. EXPERIMENTAL SETUP", "content": "We conducted 8 function approximation and 4 PDE solving(Raissi et al., 2017; Han et al., 2018) tasks, addressing varying complexities, dimensions, and nonlinearities. For function approximation and PDE tasks, we trained models with hidden layer sizes ranging from 8 to 512 for up to 1000 epochs. More detailed experimental settings are detailed in"}, {"title": "4.4.2. FUNCTION APPROXIMATION TASKS", "content": "We presents the eight target functions used in the experiments (see Table 4 in Appendix E), covering a variety of mathematical properties, including periodicity, non-linearity, high dimensionality, discontinuity, and chaos. The results of the experiment are shown in 4(see Appendix E). According to the experimental data, KAF's minimum test RMSE in most function approximation tasks is significantly lower than that of MLP, GPKAN, and FAN, demonstrating superior fitting capability and generalization performance. In the Bessel task, KAF achieves a test RMSE of 2.55 \u00d7 10-6, which is considerably lower than MLP's 1.43 \u00d7 10-5. For the Highly-Nonlinear and Multi-Scale tasks, KAF attains RMSE values of 3.18 \u00d7 10-5 and 4.98 \u00d7 10-5, while MLP exhibits significantly higher errors of 1.41 \u00d7 10-4 and 1.85 \u00d7 10-2, respectively."}, {"title": "4.4.3. PDE SOLVING TASKS", "content": "As shown in Figure 5 in Appendix E, for numerical solving tasks across four types of PDEs (Poisson, 1D Wave, Heat, and Burgers), traditional MLP exhibits higher overall errors or lower stability. In contrast, KAF, which leverages learnable approximation, generally achieves better or comparable accuracy. For Poisson and Heat equations, both KAF and KAN significantly outperform MLP in terms of error reduction, while FAN also maintains a comparable level of accuracy. However, GPKAN, due to its sensitivity to parameter scale and initialization, demonstrates noticeable instability or larger errors, highlighting its challenges in achieving robust performance under these conditions. Overall, KAF, which incorporates learnable grid functions or compact functionals, provides greater flexibility in function approximation for PDE-solving tasks."}, {"title": "4.5. Ablation Experiment", "content": "In this section, we conducted two ablation experiments: 1) On the CIFAR-10 dataset, we used a single-layer network to study the effectiveness and completeness of each component and strategy. Additionally, we plotted the scaling factor curves to observe the variations of different factors during the experiment. The detailed results are presented in Appendix F; 2) We performed function fitting experiments on sin(x) and cos(x) in comparison with KAN and MLP (GELU/RELU) to demonstrate that our model outperforms traditional methods in fitting periodic and high-frequency signals. The full experimental results are shown in Appendix F."}, {"title": "5. Discussions", "content": "The Kolmogorov-Arnold-Fourier network (KAF) addresses the scalability and spectral limitations of Kolmogorov-Arnold Networks (KAN) by integrating trainable random Fourier features (RFF) and a hybrid GELU-Fourier activation mechanism. This innovative design improves parameter efficiency and high-frequency function approximation while preserving a degree of interpretability. Experimental results demonstrate KAF's effectiveness across a diverse range of tasks, including vision, natural language processing (NLP), and solving differential equations, highlighting its practicality in high-dimensional learning scenarios.However, KAF's reliance on Fourier-based representations introduces certain trade-offs, such as potential challenges in interpretability and increased computational overhead. Furthermore, the learnability of RFF-based mappings is sensitive to initialization and hyperparameter tuning, which can impact convergence stability and model performance.Future research could focus on several promising directions. One avenue is the hybridization of KAF with larger-scale models to further enhance its capacity and scalability. Another is the exploration of more robust initialization schemes and adaptive optimization techniques to improve the stability of RFF-based mappings. Additionally, extending KAF to tackle more complex, real-world problems that demand efficient multi-frequency representation could unlock new applications and broaden its impact."}, {"title": "A. Detailed derivation of parameter quantities and FLOPs calculations", "content": null}, {"title": "A.1. KAN with B-splines: Parameter Counting", "content": "Number of B-spline Basis Parameters.\nLet the B-spline(De Boor, 1972) order be K, and divide the domain into G segments. Then: - Each segment needs K + 1 control points, total (G+ K + 1). - Boundary smoothness of order K \u2212 1 adds 2(K \u2013 1) virtual points. - Total per univariate spline: G + 3K \u2212 1. (Sometimes simplified to G + K + 3.)\nSingle-Layer Parameter Decomposition in KAN.\nFor dimension din \u2192 dout: - Internal function (B-spline projection): din \u00d7 dout splines, each with (G + K + 3) parameters.\nHence,\n$$\\text{ParamsKAN} = d_{in} d_{out} (G + K + 3) + d_{out}.$$"}, {"title": "A.2. KAF with RFF: FLOPs Decomposition", "content": "Single-layer KAF. - Random Fourier Feature (RFF(Rahimi & Recht, 2007b)) mapping: cos(WTx + b) and sin(W\u00afx + b) each require one matrix multiplication. Total FLOPs(Tang et al., 2018):\n$$2 \u00d7 (d_{in} \u00d7 M) \u00d7 2 = 4d_{in} M.$$\n- Linear combination (GELU + RFF): Element-wise scaling of a \u00d8 GELU(x) and b\u00d8RFF(x). Total FLOPs:\n$$2 \u00d7 (d_{in} \u00d7 M) \u00d7 2 = 4d_{in} M.$$\nFinal linear projection: Matrix multiplication W(1) . (.) and bias addition. Total FLOPs:\n$$2d_{in} d_{out}$$\n- Activation function: GELU activation requires 5din FLOPs.\nTotal FLOPS:\n$$\\text{FLOPSKAF} = 4d_{in} M + 2d_{in} + 2d_{in} d_{out} + 5d_{in}.$$"}, {"title": "A.3. MLP FLOPs Computation", "content": "Standard MLP. - Linear layer W\u2208 Rdout\u00d7din: 2 din dout FLOPs (multiply+add). - Activation: - ReLU: 1 FLOP per output (comparison), total dout. - GELU: about 5 dout FLOPS.\nHence, for a GELU-MLP:\n$$\\text{FLOPSMLP} = 2 d_{in} d_{out} +5 d_{out}.$$"}, {"title": "B. Kernel Approximation and Gradient Derivation of Random Fourier Features (RFF)", "content": null}, {"title": "B.1. Convergence Proof of RFF Kernel Approximation", "content": null}, {"title": "B.1.1. BOCHNER'S THEOREM AND THE FOURIER DUALITY OF KERNEL FUNCTIONS", "content": "According to Bochner's(Rahimi & Recht, 2007a; Gradshteyn & Ryzhik, 2014) theorem, any translation-invariant positive definite kernel function k(x, y) = k(x - y) can be expressed as the Fourier transform of a Gaussian measure:\n$$k(x - y) = \\int_{\\mathbb{R}^d} e^{i \\omega^T (x-y)} p(\\omega) d\\omega$$\nwhere p(w) is the spectral distribution corresponding to the kernel function. For the Gaussian kernel k(x,y) = e-l|x-y||2/(202), its spectral distribution is:\n$$p(\\omega) = N(\\omega; 0, \\sigma^{-2}I_d).$$"}, {"title": "B.1.2. EXPECTATION OF INNER PRODUCT OF RANDOM FOURIER FEATURES", "content": "Define the RFF mapping:\n$$z(x) = \\sqrt{\\frac{2}{m}} [\\text{cos}(\\omega_1^T x + b_1), \\text{sin}(\\omega_1^T x + b_1), ..., \\text{cos}(\\omega_m^T x + b_m), \\text{sin}(\\omega_m^T x + b_m)],$$\nwhere wi ~ p(w), b\u2081 ~ U[0, 2\u03c0]. The expectation of the inner product is:\n$$\\mathbb{E} [z(x)^T z(y)] = \\frac{2}{m} \\sum_{i=1}^{m} \\mathbb{E} [\\text{cos}(\\omega_i^T x + b_i) \\text{cos}(\\omega_i^T y + b_i) + \\text{sin}(\\omega_i^T x + b_i) \\text{sin}(\\omega_i^T y + b_i)]$$\n$$= \\frac{2}{m} \\sum_{i=1}^{m} \\mathbb{E} [\\text{cos}(\\omega_i^T (x - y))] \\text{(using trigonometric identity)}$$\n$$= \\mathbb{E}_{\\omega \\sim p(\\omega)} [2\\text{cos}(\\omega^T (x - y))] \\text{(m } \\rightarrow \\infty \\text{ converges by law of large numbers)}$$\n$$= \\mathbb{E}_{\\omega \\sim p(\\omega)} [e^{i\\omega^T (x-y)} + e^{-i\\omega^T (x-y)}]$$\n$$= 2 \\cdot \\mathbb{R}e (\\mathbb{E}_{\\omega \\sim p(\\omega)} [e^{i\\omega^T (x-y)}])$$\n$$= 2 \\cdot \\mathbb{R}e (k(x - y)) = 2k(x - y) \\text{(since k(x - y) is a real-valued symmetric function)}.$$\nHowever, since the original scaling factor is \u221a2/m, the actual expectation of the inner product is:\n$$\\mathbb{E} [z(x)^T z(y)] = k(x - y).$$"}, {"title": "B.1.3. ERROR BOUND AND CONVERGENCE RATE", "content": "According to Rahimi & Recht (Rahimi & Recht, 2007a), when using m random frequencies, for any x, y \u2208 X, we have:\n$$\\mathbb{P} \\bigg( \\underset{x,y}{\\text{sup}} |z(x)^T z(y) - k(x, y)| > \\epsilon \\bigg) \\geq \\frac{2\\sqrt{\\text{diam}(X)}}{e} \\text{exp} \\bigg( - \\frac{m \\epsilon^2}{4(\\sigma_p \\sqrt{d+2})^2} \\bigg),$$\nwhere op is the variance of p(w), and diam(X) is the diameter of the input space. Thus, the convergence rate is:\n$$O(1/\\sqrt{m})$$"}, {"title": "B.2. Differentiability and Gradient Computation of RFF", "content": null}, {"title": "\u0392.2.1. \u0391\u039dALYTICAL GRADIENT EXPRESSIONS", "content": "Let w \u2208 Rd be a row of the frequency matrix W, and b be the corresponding phase shift. For an input x \u2208 Rd:\nGradient of the cosine term:\n$$\\frac{\\partial}{\\partial w} \\text{cos}(\\omega^T x + b) = -x \\text{sin}(\\omega^T x + b), \\frac{\\partial}{\\partial b} \\text{cos}(\\omega^T x + b) = - \\text{sin}(\\omega^T x + b)$$\n- Gradient of the sine term:\n$$\\frac{\\partial}{\\partial w} \\text{sin}(\\omega^T x + b) = x \\text{cos}(\\omega^T x + b), \\frac{\\partial}{\\partial b} \\text{sin}(\\omega^T x + b) = \\text{cos}(\\omega^T x + b)$$\nFor a matrix W \u2208 Rd\u00d7m, gradients accumulate row-wise. For Wij (the i-th row, j-th column):\n$$\\frac{\\partial}{\\partial W_{ij}} \\text{cos}(W_j^T x + b_j) = -x_i \\text{sin}(W_j^T x + b_j)$$\nwhere W; is the j-th column of W."}, {"title": "B.2.2. IMPLEMENTATION IN BACKPROPAGATION", "content": "In automatic differentiation frameworks(Baydin et al., 2018) (e.g., PyTorch), the gradient computation for RFF follows these steps: 1. Forward pass: Compute cos(WTx + b) and sin(WTx + b). 2. Backward pass: Using the chain rule, the gradient tensor for W is -x \u2297 sin(WTx + b) (outer product) and x \u2297 cos(WTx + b). The gradient for b is directly \u2013 sin(W+x+b) and cos(WTx + b). 3. Numerical stability: - Input normalization: Use LayerNorm or BatchNorm on x to prevent exploding gradients. - Gradient clipping: Restrict || w ||2 \u2264 7 to avoid instability from high-frequency noise."}, {"title": ""}]}