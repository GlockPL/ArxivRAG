{"title": "Computational Approaches to Arabic-English Code-Switching", "authors": ["Caroline Nabil Samy Sabty"], "abstract": "Natural Language Processing (NLP) is a vital computational method for addressing language processing, analysis, and generation. NLP tasks form the core of many daily applications, from automatic text correction to speech recognition. While significant research has focused on NLP tasks for the English language, less attention has been given to Modern Standard Arabic and Dialectal Arabic. Globalization has also contributed to the rise of Code-Switching (CS), where speakers mix languages within conversations and even within individual words (intra-word CS). This is especially common in Arab countries, where people often switch between dialects or between dialects and a foreign language they master. CS between Arabic and English is frequent in Egypt, especially on social media. Consequently, a significant amount of code-switched content can be found online. Such code-switched data needs to be investigated and analyzed for several NLP tasks to tackle the challenges of this multilingual phenomenon and Arabic language challenges. No work has been done before for several integral NLP tasks on Arabic-English CS data. In this work, we focus on the Named Entity Recognition (NER) task and other tasks that help propose a solution for the NER task on CS data, e.g., Language Identification. This work addresses this gap by proposing and applying state-of-the-art techniques for Modern Standard Arabic and Arabic-English NER. We have created the first annotated CS Arabic-English corpus for the NER task. Also, we apply two enhancement techniques to improve the NER tagger on CS data using CS contextual embeddings and data augmentation techniques. All methods showed improvements in the performance of the NER taggers on CS data. Finally, we propose several intra-word language identification approaches to determine the language type of a mixed text and identify whether it is a named entity or not.", "sections": [{"title": "Introduction", "content": "Humans invented natural language to communicate with each other. Natural Language Processing (NLP) is a research field dealing with how computers manipulate natural language inputs and outputs [218]. Nowadays, many trending applications are being used daily, such as Siri\u00b9 and Alexa\u00b2 that rely on NLP tasks; ranging from simple ones like automatic text correction to more complex ones like information extraction and speech recognition. Much work has been conducted on various NLP tasks for some significant languages (e.g. English). However, relatively less work is available for other languages used in computing (e.g. Arabic).\nArabic is one of the languages most spoken globally, ranking the sixth, as it is spoken by around 274 million people [74]. It exists in different forms, such as Modern Standard Arabic (MSA) and Dialectal Arabic (DA). MSA is mostly used in a formal context. DA is used more in everyday life communications (written or spoken) between Arabic speakers. There are several dialectal types of Arabic in different countries, such as, for example, Egyptian, Levantine, Gulf, and Iraqi dialects of Arabic. In addition to the main two forms of Arabic, native speakers switch between their dialect and other dialects or foreign languages in the same conversation, which is known as code-switching (CS) [78]. CS is defined as the embedding of linguistic units such as phrases, words, and morphemes of one language into an utterance of another language [176]. As a result of globalization and better quality of education, a significant percentage of the population in Arab countries have become bilingual/multilingual. This has raised the frequency of\nCS among Arabs in their daily communications. For instance, in Algeria, people tend to code-switch Arabic with French; meanwhile, in Egypt, people tend to code-switch it with English, as shown in the following example of code-switching within the same sentence:\n\u0643\u0623\u0633 \u0627\u0644\u0633\u0648\u0628\u0631 \u0627\u0644\u0633\u0639\u0648\u062f\u064a \u0627\u0644\u0647\u0644\u0627\u0644 vs \u0627\u0644\u0627\u062a\u062d\u0627\u062f\nLondon \u0641\u064a Saturday \u064a\u0648\u0645\n(The Saudi Super Cup Al Hilal vs Al Aitihad on Saturday in London.)\nAs user-generated content increases, it is easy to observe that users mix between different languages in the same word, a practice known as intra-word CS. For example, speakers can say quizat which is composed from the English word quiz and the suffix at referring to plural in Arabic. Multilingual social media users are generating a huge amount of data containing a high frequency of CS. However, such mixed words/sentences are usually overlooked in NLP tasks; as NLP tasks are designed to process texts written in a single language. The rise of social media networks and other informal communication platforms has led to an increased need to apply NLP tasks on CS, and not only DA and MSA.\nThough posing several orthographic and morphological challenges, along with the challenges posed by the different dialectal variations, Arabic NLP has achieved many successes and developments [64]. There are still considerable gaps for several essential NLP tasks, such as Language Identification (LID) and Named Entity Recognition (NER) tasks. Less work has been done for these tasks on MSA and DA than on other languages, and no work has been done for Arabic-English CS data. NER refers to recognizing spans of text that refer to real-world entities and classifying them into different types or categories (e.g., person, location, organization). NER has proved to be highly significant to various tasks in NLP, such as information retrieval and question answering tasks. Also, there are several use-cases for the NER task such, as text classification, customer support, content recommendation and semantic annotaion. Applying NER task on Arabic-English CS data faces more challenges than the already existing ones of automatically processing MSA and DA. Some of these added challenges are having two different languages in the same sentence as well as the limited or lack of CS data for the NER tasks.\nIn this work, we tackled the challenge to apply the NER task on CS data by presenting several NER taggers and complementing the taggers with other\nNLP tasks that help in enhancing the performance on such data. Also, we implement CS Intra-word Language Identification (LID) approaches. Language Identification (LID) refers to the task of determining the language type of a text. Intra-word LID involves segmenting mixed words and tagging each part with its corresponding language identification and states whether a word is a named entity or not. It could be used as a pre-processing task for other NLP tasks.\nIn this work, state-of-the-art techniques were used to implement the Arabic Name Entity Recognition task and Language Identification of intra-word. The main contributions included:\na) Collecting and annotating the required Arabic-English CS corpus for NER task\nb) Developing different taggers for MSA and CS NER\nc) Applying enhancement techniques to improve the performance of the CS NER taggers\nd) Creating Arabic-English CS corpus for intra-word LID\ne) Developing different approaches for LID of intra-word CS"}, {"title": "Linguistic Background", "content": "Languages reflect the human mind. It is a way of communication where humans can express themselves. One of the oldest languages is Arabic. It is the official language of 24 countries dominantly lying in the middle east, and north Africa [234]. Arabic is also one of the top nine languages used on the web [75]. It is a rich morphological language."}, {"title": "Varieties of Arabic Language", "content": "The Arabic language has three main varieties: Classical Arabic (CA), Modern Standard Arabic (MSA), and Dialectal Arabic (DA). CA was used in the Quran and early Islamic literature. MSA is the formal language in almost all Arab countries. It is used in schools and universities, in the media, and in formal writing such as Arabic newspapers and letters. It is one of the six official languages of the United Nations used in their meetings and documents. DA (Colloquial) is the language used in informal daily communication [64,220]. Within each Arab country and its regions, there are different dialects such as Egyptian, Lebanese and Tunisian. The Arabic dialects themselves differ, sometimes significantly, depending on many factors (e.g., geographical location, social and economic status). There is a huge gap between the written form of Arabic MSA and the different Arabic dialects as spoken in different areas due to their significant number. Recently, DA is also being used as the main written language on social media. Nevertheless, the main focus of most NLP research was on MSA.\nIn addition to the three main varieties of Arabic, native Arabic speakers typically mix MSA and dialectal Arabic. Due to the presence of many multilingual speakers in Arabic countries, people often mix multiple languages in the same context, known as Code-Switching (CS) [47]. CS is defined as the embedding of linguistic units such as phrases, words, and morphemes of one language into an utterance of another language [176]. This linguistic behavior (or practice) occurs in both forms of the language, whether spoken or written. The primary language appears the most inside an utterance, while the secondary language used inside the utterance is the language of embedded words or phrases. There are three main types of Code-Switching: Inter-sentential, Intra-sentential and Intra-word.\n\u2022 Inter-sentential CS refers to switching between different languages from one sentence to another [43]. For example:\n\u0645\u0645\u0643\u0646 \u0646\u062e\u0631\u062c \u0628\u0643\u0631\u0627. !That's a great idea\n(We can go out tomorrow. That's a great idea!)\n\u2022 Intra-sentential CS refers to using multiple languages within the same sentence [43]. For example:\n\u0628\u0639\u062f \u0627\u0644 weekend \u0639\u0646\u062f\u064a project \u0648 lab \u0648 \u0627\u062e\u0631\u062c.\n(After the weekend, I have a project and a lab and I will go out.)\n\u2022 Intra-word CS refers to mixing between different languages in the same word [158]. For example:\nThe word quizat which is composed from the English word quiz and the suffix at indicating a plural word in Arabic.\nResearchers use several definitions for CS, such as [175] who decided to use the term Code-Mixing (CM) instead of intra-sentential CS. However, other researchers do not distinguish between the different types of CS such as inter or intra-sentential and refer to both types as Code-Switching [49]. The phenomenon of code-switching has been increasingly reported in linguistic studies in the past"}, {"title": "Challenges of Arabic Language", "content": "Although Arabic is one of the languages that are used the most, it presents several challenges for NLP tasks. One of the significant problems that face NER and LID, in general, is the lack of labeled data, a task for which large annotated corpora are needed. A huge scarcity in available resources exists, especially for dialectal Arabic and Code-Switching. Moreover, Arabic poses several orthographic and morphological challenges, in addition to having dialectal variations [64].\nThe Arabic script is one of the main linguistic properties that are the most challenging to the automatic processing of Arabic [83]. The alphabet of the Arabic language is composed of 28 letters that are all consonants, three long vowels, and three short vowels. The long vowels are (\u0627) pronounced (Alef), (\u0648) pronounced as (Waw), and (\u064a) pronounced as (Ya'a). The short vowels, present in the pronunciation of words, distinguish words from each other, but no unique letters represent them in writing. They are sometimes represented by special marks called diacritics above or below a letter. The majority of Arabic text, especially on social media, does not carry the diacritic marks and this leads to orthographic ambiguity [64]. The diacritics give various meanings to the same lexical form. It is easy for a native Arabic speaker to read such words and distinguish between them based on the context surrounding the word. Nevertheless, it is very challenging for a computational system. For instance, the word \u064a\u062d\u0649 without diacritics, it could be considered a named entity of a type person name Yahya, or not a named entity and could be a verb (gives life back) or a verb (greets) [260].\nOne of the characteristics of the Arabic letters is that they have various shapes according to their position in the word. For example, the letter \u0645 has three forms, it is written at the beginning as \u0645\u0640\u0640 , at the middle as \u0645 or at the end as \u0640\u0645 [223]. The lack of consistency in Arabic orthography poses particular challenges for different NLP tasks such as NER. One of the reasons English NER is easier than the Arabic NER is that most of the names begin with capital letters. These show that a word or its succession is a named entity but that is not an Arabic option. For example, in English, the name Sara starts with a capital letter, but in Arabic the same name \u0633\u0627\u0631\u0647 does not contain any special marks or indications that the word refers to a person name. Moreover, it is common in Arabic, similar to other languages, to face ambiguity between named entities. For example, \u0623\u062d\u0645\u062f \u0623\u0628\u062f Ahmed Abad) refers to both a person name and a location name. This is a conflicting situation as the same NE could be tagged as two different NE types [220]. It is also expected that nouns and adjectives that are not named entities could be mixed up with Arabic nouns that are named entities. For example, the word \u0623\u0645\u0644 could mean hope or refers to a name of a person, which might cause ambiguity while recognizing the named entities [223].\nThere is a high degree of spelling inconsistency while writing MSA and DA, especially on social media. Furthermore, a non-Arabic word could be transcribed into Arabic, and it will be called Arabizi. There are no specific transcription schemes for such words as Arabic script has a high level of ambiguity. For instance, the NE \"Washington\" referring to a city could be transcribed to any of the following Arabic words: \u0648\u0634\u0646\u0637\u0646 \u060c \u0648\u0627\u0634\u0646\u063a\u0637\u0646 \u060c \u0648\u0627\u0634\u0646\u0637\u0646 \u060c \u0648\u0627\u0634\u0646\u0637\u0646 [223]. Western European languages have fewer speech sounds than Arabic, making Arabic complicated by having many NE variants that might be incorrect.\nThe performance of information retrieval and other tasks, including NER, is affected by the representations of the words and their extracted morphosyntactic features. In morphologically complex languages, such as Arabic, it is challenging to extract such features due to its extremely inflectional properties [77]. In other languages like English, clitics are treated as separate words. However, in Arabic, they are agglutinated to the words as features for several indications (e.g., number, gender, and mood) [64]. The syntactic relationship between words in the sentence is represented by the inflectional endings. Thus, the relationships"}, {"title": "Traditional Machine Learning Approaches", "content": "Machine learning (ML) builds systems that can apply statistical learning techniques to learn from data, identify patterns and make predictions automatically. ML methods focus on extracting meaningful data from narrative text, which is a distinctive sub-field of NLP [162]. Concerning the NER task, the generation of statistical models for named entity predictions is achieved using ML algorithms that determine named entity types from annotated texts [220]. Machine learning uses learning algorithms that need large training and testing data along with a set of features from these data. There are three categories of machine learning methods, supervised, semi-supervised, and unsupervised. The difference between the three methods is mainly the usage of prior knowledge. Supervised methods learn to predict by training on annotated examples. They are usually used for classification or regression tasks.\nUsually a corpus is divided into two or three parts. The first one is the train, used for training the parameters of the model. The second one is the test, used in the final evaluation of the model. The third one is the validation used to reach the optimal values of the model hyper-parameters. Generally, the ML process in the NER task starts by converting the text into structured data, text featurization. One of the best known and currently used text featurization methods is word embeddings discussed in this Chapter as well.\nOne of the simplest supervised approaches is the Na\u00efve Bayes; it is usually used as a benchmark or baseline in several NLP tasks. Among the traditional supervised machine learning approaches that have proven to be very successful in different NLP tasks, especially in NER, is Conditional Random Fields (CRF) [37]."}, {"title": "Na\u00efve Bayes Classifier", "content": "Simple Bayesian Classifier is one of the most effective classifications used in machine learning. These probabilistic approaches put relevant assumptions about the problem data, and build a model based on these assumptions. While training the machine learning model the parameters of the probabilistic model are learned. The new input classifications are applied following the Bayes rules. Na\u00efve Bayes is one of the classifiers based on Bayes theorem of probability used to predict the class of unknown data-sets. The model assumes that there is no relation between the different features of the input and ignores the correlations. The label or class Y could be related to only one feature node. This assumption is not accurate in most of the tasks; however, it makes the task simpler especially when dealing with a large number of attributes [165]."}, {"title": "Conditional Random Fields", "content": "CRFs are one of the main statistical machine learning techniques. They are used as sequence classifiers to segment and label sequence data based on probabilistic models [140]. The probabilistic sequence classifiers compute a probability distribution over the possible labels to choose the best label sequence of units (e.g., words, sentences, letters). CRF could be considered as an enhancement or generalization of Hidden Markov Models (HMM) and Maximum Entropy (ME) [140]. The HMM considers the observed events such as the words available in the input and the causal factors in the probabilistic model that are hidden, such as part-of-speech tags [170]. The labeling decision is only dependent on the current input with its corresponding observed object. In general, for the NER task, the goal is to predict label Y for input X. The HMM is a generative probabilistic model which specifies the joint distribution: $p(X, Y)$. Nevertheless, the CRF is a discriminative undirected graph model, and specifies the conditional distribution of the y labels given x inputs: $p(Y|X)$ [87].\nThe ME method calculates estimated probabilities relying mainly on the imposed constraints while making a few other assumptions. The constraints are deduced from the training data, and they express some relationships between features and output [40]. This method selects the probability distribution with the highest entropy that satisfies the previous property [53]. While the ME model considers the dependencies between neighboring states, CRF calculates in a single model all the state transitions globally to avoid the Label Bias problem of \u039c\u0395 [246].\nOne of the main advantages of CRF is the ability to consider contextual information before assigning a label to a word. The input feature states of the"}, {"title": "Deep Learning Approaches", "content": "Recently, in several NLP tasks, the state-of-the-art performance was achieved using deep learning techniques, a subset of machine learning. The main advantage of deep learning models is automatically extracting complex features from input data instead of the manual handcrafted feature engineering used in other ML methods. It also refers to Artificial Neural Network (ANN) with multiple complex neural network layers.\nANN is inspired by how the human brain processes information. The central computational units of a Neural Network (NN) are artificial neurons where directed edges interconnect them. The connections between the neurons are represented by the weights, which determine the impact of one neuron on another [135]. ANN is designed to recognize patterns and detect trends represented as numerical values in vectors, into which all input data should be translated. The neural unit, as shown in Figure 2.2a takes as input vector x having weight w expressing the importance of this input to the output and z is the weighted sum of inputs. The output of the unit is y which is based on the activation function; in this example, it is the Sigmoid function [5]. The activation function determines whether and to what extent the input should progress and affect the output of the network. The weights w are adjusted based on an error signal/feedback during learning to\nfind the desired output. The selection of the type of activation function is made after experimenting with several functions and selecting the one that gets the best results on the validation data."}, {"title": "Convolution Neural Network", "content": "The Convolutional Neural Networks (CNN) is a deep learning algorithm commonly used in the Computer Vision field [143]. It could be considered as a specialized Feed Forward Neural Network. It takes the input image and specifies learn-able weights and biases to different objects in the image to assess their importance. CNN checks four key ideas which are shared weights, local connections, use of many layers, and pooling layers. The CNN automatically extracts features from the image by applying a collection of filters to create a final hierarchical structure of features. Each filter has a weight that is learned from the training data [35]. CNN is also used in different NLP tasks with text input [125]. The\nprocess starts by sliding filters of different window sizes over the input that could be word embeddings. Each filter has a weight and generates a new feature for the different windows. A feature map is generated by sliding the filter over each window. As a result of some calculations that consider a small segment of the input sequence and share the parameters with the calculations to its left and right"}, {"title": "Recurrent Neural Network", "content": "Recurrent Neural Network (RNN) [81] is a specific kind of ANN that has internal memory designed to remember its past input every time a new input is given. It is composed of neurons connected by weighted arcs w and models sequential data better than Feed Forward Network as it models the relationships between the inputs over time using a feedback mechanism/loops [5]. For every element of a sequence, it performs the same task with the output while depending on previous computations. RNN is commonly used in the NER and LID tasks and has\nshowed success due to its capabilities of handling sequential tasks [100,144]. As illustrated in Figure 2.4, the input vector is $x_t$ that represents the current input and $h_t$ represents the current hidden timestamp layer. $W_{hx}$ is a weighted matrix of current timestamp layer that is used in multiplication of the input vector and it is given to the activation function. The function computes the activation value for the hidden timestamp layer. This hidden layer is used to calculate the corresponding output of using weight at output state $W_{oh}$ and hidden state. This RNN"}, {"title": "Transformers", "content": "Transformers are a recent type of neural network architecture [241]. They have become a mainstream architecture used in several NLP tasks as they are very powerful. The transformer models solve the disadvantages of the LSTM models as they allow parallelism in training and capture long term dependencies of tokens in a sequence. In comparison with BiLSTM models that cover the full context of a sequence by concatenating two models in one vector, the transformers monitor in one model the sequence bidirectionally. This method is better than the concatenation as it gives better representation of sequence. This is achieved by modeling direct dependencies between each two words in a sequence. Transformers model dependencies using an attention mechanism. As shown in Figure 2.8 its architecture depends on stacked layers of self-attention and point-wise in forming encoder and decoder components; the encoder is the component on the left and the decoder is the one on the right. Few concepts must be defined in order to better understand the architecture of the transforms such as, for example, attention, self-attention, encoder and decoder."}, {"title": "Configuration of Deep Neural Network", "content": "Model design variables determine the network structure, such as the number and size of the hidden layers, and the hyper-parameters determine how the network is trained, such as learning and dropout rate [69]. The following are some model variables and hyper-parameters that affect the training of deep learning models.\nEpoch: It is a random cutoff number, usually defined as \"one pass over the whole dataset\". It is used to separate training into different phases, which is useful for logging and for periodic assessment [56].\nBatch Size: It is the number of sentences that will be propagated through the network. The training dataset will be divided by the number of batch sizes. Small batch sizes are attractive since they can make convergence in fewer epochs. However, large batch sizes provide more data-parallelism, which successively enhances computational efficiency and scalability [67].\nActivation Function: It is used to establish non linearity to models, which allows deep learning models to learn non-linear prediction. It calculates the output from the summation of the weighted input signals of the neural network and also maps the result between 0 to 1 or -1 to 1, depending on the function. The main reason for using it is to transform the input signal in a network into an output signal. It is a mathematical equation that is responsible for determining the output of a neural network. Each neuron has a function attached that determines whether it should be activated or not. This function is applied to the summation of the product of input nodes and their weights. A Neural Network unaccompanied by the Activation function would directly be a Linear regression model, which"}, {"title": "Word Embeddings", "content": "One of the powerful developments in the NLP field is word embeddings. This is used to convert input text to a machine-readable format. One of the traditional techniques to do so is one-hot encoding. This represents each sequence text input in d dimensional space, where d is the vocabulary size in the dataset. If the term is presented in the document, it will get 1 and 0 otherwise. In the case of a large corpus, this method will generate huge vectors that are very sparse and inefficient. Thus, later word embeddings were introduced where each word is projected to a dense vector; of short length and most elements are non-zero. These word embeddings preserve the semantic distance between words, and semantically similar words are grouped near each other. Word embedding or representation is done by mapping every word $w_i$ in a sentence to vector $x_i$ in multi-dimensional space. The word embeddings are stored in a matrix X. Thus, an input represented as a sequence of words $w_1,...,w_t$ is represented as a corresponding sequence of word embeddings $x_1,..,x_t$, that is given to the neural network.\nAs it is hard to identify the similarity between the same Arabic words having different prefixes and suffixes, the usage of word embedding makes it easier to find these similarities as similar words are mapped to nearby vectors. For instance, the words \u0627\u0644\u062f\u0648\u0644\u0629 the country ,(\u0627\u0644\u062f\u0648\u0644\u062a\u0627\u0646 the two countries) \u0627\u0644\u062f\u0648\u0644 (the countries) differ in their morphological forms, however, their embedding should be placed near each other in the space.\nLarge datasets are used to train and save the embeddings. Then these could be used as pre-trained word embeddings models in various downstream NLP tasks to boost their performance. Moreover, they avoid training an embedding model from scratch as learning independent representations for words from the training data alone is difficult [194]. Thus, adding word embedding to deep learning models enhances the performance by specifying syntactic and semantic word relationships. There are two main categories of word embeddings: classical (non-contextual) and contextual word embeddings. The main difference between these two categories is whether the context of a word affects its embedding and changes it or not. It is challenging to learn high-quality representations because of the different characteristics of the words and changes they undergo based on the linguistic contexts [189].\nClassical word embeddings do not consider the context of the words, and the generated vectors are static; they do not change based on the context of the word. For instance, the word \u201capple\u201d has a different meaning in the following two sentences \u201cI want to eat an apple\u201d and \u201cApple store is very crowded today\u201d. Thus, using classical word embeddings will generate the same vector for the word \"apple\" in both sentences. Therefore, to overcome this problem of ambiguity, contextual word embeddings are being introduced, and several types are being proposed in the NLP field. Contextual word embeddings generate different embeddings for the same words based on their context, which is essential to capture the semantics of the ambiguous words based on their context [189]. In this example, \"apple\" will be given different vectors in both sentences. This will help"}, {"title": "Classical Word Embeddings", "content": "There are two main classes of embeddings, word-level and character-level. The Word2vec and GloVe embeddings will be discussed next from a word-level class and FastText embeddings from the character-level class."}, {"title": "Word2vec (W2V)", "content": "Word2vec (W2V) learns word representations using neural networks. The architecture of W2V is a Feed Forward Neural Network with one hidden layer. The representations are created by training a classifier to distinguish nearby and far-away words. The two main prediction/learning approaches commonly used in W2V are continuous Bag-of-Words (CBOW) and Skip-gram. CBOW predicts the current word based on the given context. In comparison, Skip-gram predicts surrounding words given the current word. W2V provides multiple degrees of similarity between different words by mapping to nearby vectors [169]. The most popular pre-trained Word2Vec embedding models were developed by Google. It was trained on 100 billion words of Google News data-set."}, {"title": "GloVe", "content": "It is another widely known model for generating pre-trained embeddings. Deriving the relationship between words from global statistics is the main idea of Glove. The training of this algorithm is computed on aggregated global word-word co-occurrence statistics in a large amount of textual data. One of the English pre-trained models of GloVe [187], was trained on Wikipedia 2014 and Gigaword 5 data. Both Word2vec and Glove word embeddings perform similarly in most applications. However, the training of Glove is based on matrix factorization, thus, it could be easier parallelized. Besides, both of them share the same main limitations, one of which being the problem of out-of-vocabulary words, which is solved by introducing the character-level embeddings such FastText."}, {"title": "Contextual Word Embeddings", "content": "Contextual word embeddings are considered the new approach for representing the vector of a word in a given text, as compared to Word2Vec and GloVe models. These embeddings provide different vector representations of a single word and are derived from pre-trained bidirectional language models on a large text corpus [190]. The contextual embeddings perform better when being used in a downstream like NER on ambiguous, complex, and unseen languages [22]. In the following part several types of contextual embeddings will be discussed, ELMo, BERT, Contextual string embeddings, Pooled FLAIR embeddings, ELECTRA and MUSE."}, {"title": "ELMO", "content": "Embeddings from Language Models (ELMo) was one of the first contextualized word embeddings based on language modeling trained using BiLSTM. The task of language modeling is unsupervised learning used in ELMo in the pre-training phase. ELMo combines all layers with a weighted average pooling operation. A language model generates the following word based on the previous words in the sentence. The purpose is to distribute over sentences in the original training data as close as possible to the resulting distribution. The internal representations from the BiLSTM language model is transferred after pre-training on a large dataset to form ELMo embeddings [190]. Several pre-trained ELMo models for several languages include the Arabic language implemented by [189]."}, {"title": "BERT", "content": "Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based language model. It replaces language modeling with Masked Language Model (MLM) and Next Sentence Prediction (NSP) tasks. MLMs are trained on masking out random tokens by replacing them with a special token [MASK] inside a sentence. The model then tries to predict the masked tokens using the whole context of a sequence. Besides, NSP models are trained on distinguishing whether two input sentences are continuous segments or are separate [68]. BERT models sentences as a sequence of tokens. Each input token in a sequence flows through stacked encoders and outputs a hidden state representing the word embedding. This enables BERT to train in parallel. The data-path of input"}, {"title": "Evaluation Metrics", "content": "It is essential to discuss the evaluation metrics commonly used to evaluate and compare the different NER techniques and models. Evaluating a system requires a comparison between the outputs with the gold-standard annotations. The gold-standard corpus contains annotated instances with the same types of named entities, and this corpus is usually tagged manually. The process starts by randomly dividing the corpus into training and testing sets with portions of 70%-30% or 80%-20% respectively and is followed by the learning process of the model that uses the training set. Then entities are extracted from the test set using the trained model. For each named entity type several evaluation metrics could be calculated.\nThe different NER forums have suggested various evaluation schemes; in this work, the exact-match evaluation suggested at CoNLL-2003 [235] is followed. A simple method used to analyze the success rate and denote the right and wrong predictions is the confusion matrix. The rows of the matrix represents an actual class and the columns represent a predicted class. As shown in Table 2.1, the possible classification cases are denoted as TP and TN representing the correctly classified number of positive and negative named entities as well as FN and FP representing the misclassified negative and positive named entities, respectively.\nThe information given by the confusion matrix is not enough to evaluate the performance and more concise metrics are used.\nThere are four common metrics that use numerical values to represent several aspects of the quality of a system, Accuracy, Precision, Recall, and F1-score (micro-averaged). The Accuracy is a common evaluation metric in machine learning classification tasks. However, it gives high values to systems that do not return any results. Accuracy is the percentage of correct predictions from the total predictions made by the system.\n$Accuracy = \\frac{TP+TN}{TP+TN+FP + FN}$ (2.1)\nThe Precision is the percentage of named entities recognized by the correct system. It is calculated by computing the ratio of several correct answers to the total number of answers as shown in Equation 2.2 [117].\n$Precision = \\frac{\u03a4\u03a1}{TP+FP}$ (2.2)\nThe Recall is the percentage of named entities in the corpus/golden annotations found by the system. It is calculated by computing the ratio of the number of correct system answers to the expected total number of answers as shown in Equation 2.3. A named entity is considered correct if it matches the corresponding entity in the corpus.\n$Recall = \\frac{TP}{TP+FN}$ (2.3)\nThe F1-score combines those two values, as shown in Equation 2.4. It is the harmonic mean of Precision and Recall used to balance their results.\n$F1 - score = \\frac{2* Precision * Recall}{Precision + Recall}$ (2.4)"}, {"title": "Related Work", "content": "Many efforts have been made to improve the performance of several NLP tasks for the English language and develop similar Arabic language techniques. This chapter discusses some related work for the NER task on monolingual and code-switched data. Then, we discuss previous work of other related NLP tasks on CS data."}, {"title": "Named Entity Recognition", "content": "The NER task was officially coined for the first time in the Message Understanding Conferences (MUC-6) in 1995 [231]. Detecting and classifying entities in the text was one of the first steps needed for most information extraction applications. Information extraction refers to the task of converting unstructured information in text into structured data. One of the earliest Arabic NER research was done in 1998 [160], and more work was done for the Arabic language starting 2007 [221]. First, in this section, an overview of some of the previous work related to NER on monolingual English and Arabic data is presented. Second, we give an overview of NER approaches applied to code-switched data."}, {"title": "Named Entity Recognition on Monolingual Data", "content": "Much research has been conducted on NER for monolingual text. Due to the morphological complexity of the Arabic language, fewer attempts have been made to tackle the problem of NER in Arabic as compared to other languages such as English. The two main approaches mainly used in NER systems are the rule-based and machine learning-based approaches.\nThe rule-based approach is one of the earliest approaches used and depends on grammar/hand-crafted rules, usually represented as regular expressions. The advantage of such an approach is that it relies on lexical resources and does not require annotated training data. For instance, this approach has been used for English NER in [110, 168, 195", "160": "they developed TAGARAB, an Arabic name recognizer that uses a pattern-recognition engine integrated with morphological analysis. The role of the morphological analyzer is to decide where a name ends, and the non-name context begins. They randomly selected fourteen documents and tagged them manually. The system achieved an F1-score of 85% for recognizing four different entity types.\nIn addition, in [167"}]}