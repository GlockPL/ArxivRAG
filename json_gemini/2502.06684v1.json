{"title": "EquiTabPFN: A Target-Permutation Equivariant Prior Fitted Networks", "authors": ["Michael Arbel", "David Salinas", "Frank Hutter"], "abstract": "Recent foundational models for tabular data, such as TabPFN, have demonstrated remarkable effectiveness in adapting to new tasks through in-context learning. However, these models overlook a crucial equivariance property: the arbitrary ordering of target dimensions should not influence model predictions. In this study, we identify this oversight as a source of incompressible error, termed the equivariance gap, which introduces instability in predictions. To mitigate these issues, we propose a novel model designed to preserve equivariance across output dimensions. Our experimental results indicate that our proposed model not only addresses these pitfalls effectively but also achieves competitive benchmark performance.", "sections": [{"title": "1. Introduction", "content": "Tabular data, a prevalent format in many real-world applications, has historically presented unique challenges to foundational models due to its lack of inherent structure compared to image or text data. Foundation models like TabPFN (Hollmann et al., 2023) have recently emerged, leveraging transformers to address classification problems in tabular settings. These models stand out by eliminating the need for hyperparameter tuning by performing both training and prediction in a single forward pass through a transformer.\nThe core of those foundational tabular models is a pre-training procedure that trains a transformer to predict on randomly sampled test data given the corresponding (random) training data. Such approaches leverage transformers to perform attention over rows of data, considering all training points simultaneously. Applying attention over rows is critical as it allows us to respect one symmetry of tabular data, e.g. that the order of rows does not matter, which is the case when data is drawn from an i.i.d. distribution (a frequent assumption in supervised learning).\nHowever, row-order symmetry is not the only symmetry relevant to tabular data. Another key symmetry pertains to feature order, where the arrangement of columns should not influence model predictions. Recent work (M\u00fcller et al., 2024; Hollmann et al., 2025) has addressed this challenge by employing bi-attention mechanisms similar to those studied in earlier work (Kossen et al., 2022). This approach alternates attention over rows and columns, making the models equivariant to feature permutations and better suited for handling another inherent symmetry of tabular data.\nA third important symmetry is the target equivariance of the model: the order in which we encode the target component should not affect the predictions. To the best of our knowledge, target equivariance has not been considered in the context of foundational tabular models. In this paper, we study this aspect and make the following contributions:\n\u2022 We demonstrate the critical role of target equivariance in tabular foundational models.\n\u2022 We provide a theoretical analysis, showing that the error of TabPFN models can be decomposed into two components: one that measures the model's equivariance gap and another that evaluates its fit to the data.\n\u2022 We propose a novel model that incorporates target equivariance, ensuring more robust and consistent predictions.\n\u2022 We show how the architecture we propose improves current foundational tabular architecture in both artificial and real-world datasets."}, {"title": "2. Related work", "content": "Initial work applying transformers to tabular data considered fitting a single transformer per dataset, with attention applied to every column of a single datapoint (Gorishniy et al., 2023). Hollmann et al. (2023) proposed instead to apply attention to the entire dataset at once (e.g., applying attention over the rows of the dataset) and demonstrated how such a model could be trained on synthetic data. This approach has the advantage of requiring only a single forward pass through a transformer to perform both training and predictions. Several works have leveraged this architecture for applications such as Bayesian Optimization (M\u00fcller et al., 2023), forecasting (Dooley et al., 2024), learning curve extrapolation (Adriaensen et al., 2023), and fairness analysis (Robertson et al., 2024).\nTo improve the original architecture, follow-up work proposed new approaches, such as learning the (fast) weights of an MLP capable of performing well on the test portion of a particular dataset (M\u00fcller et al., 2023) while achieving much lower latency. Other research focused on modifying the original architecture to align with the natural symmetries of tabular data. For instance, Hollmann et al. (2025); M\u00fcller et al. (2024) applied a bi-attention mechanism (Kossen et al., 2022), enabling predictions invariant to the order of features. In particular, the recent work TabPFNv2 (Hollmann et al., 2025) introduced several improvements to the original TabPFN architecture and prior. In addition to being concurrent work, the focus is orthogonal to ours, as we specifically analyze and enhance the output representation of TabPFN.\nTo the best of our knowledge, no previous approach has proposed a target-equivariant architecture for foundational tabular models, though several works have proposed modifications to the output architecture of TabPFN. M\u00fcller et al. (2024) introduced a modification of the output, replacing the linear projection from token embeddings to the target with Generalized Additive Models. This approach improves the interpretability of the model by constructing shape functions for each feature, which can be analyzed independently. Margeloiu et al. (2024) explored training classifiers on features generated by TabPFN. While their method includes non-parametric classifiers such as KNN, the proposed model is not target-equivariant. It also requires training a model at inference time unlike our approach that learns this adaptation during the pre-training.\nWe are not aware of many theoretical studies analyzing TabPFN, with the exception of (Nagler, 2023). Our work demonstrates the existence of an incompressible term in the pre-training loss, which quantifies the lack of equivariance. Additionally, our architecture enables tabular foundational models to be viewed as (deep) kernel machines, as the output is non-parametric. We believe this perspective opens exciting research directions, given that kernel approaches naturally lend themselves to theoretical analysis (Jacot et al., 2018)."}, {"title": "3. Background on Prior-Fitted Networks", "content": "Hollmann et al. (2023) introduced a pre-trained model, TabPFN, that leverages the transformer architecture (Vaswani et al., 2017) to perform in-context learning on unseen tabular datasets without the need for any further training. Below, we briefly review the three main ingredients for pre-training of this model as it will help to describe the architectural changes we propose to make the model equivariant to target permutations."}, {"title": "3.1. Dataset prior", "content": "A key ingredient to the pre-training procedure is the use of artificial datasets that are sampled using a sophisticated generative model meant to capture the real-world distribution of datasets. More precisely, any dataset of multiple covariate/target pairs (x, y) is generated by first sampling a latent parameter \\( \\psi \\) characterizing the dataset according to a pre-defined prior \\( p(\\psi) \\), then sampling each covariate/target pair (x, y) in the dataset according to a conditional distribution \\( p(x, y) \\) characterized by \\( \\psi \\). The pre-training procedure requires repeatedly generating two collections of training and test samples \\( (x_n, Y_n)_{n=1}^N \\) and \\( (x_m, y_m)_{m=1}^M \\) all of which are sampled from the conditional distribution \\( p(x, y|\\psi) \\) using a shared value of \\( \\psi \\) sampled from the prior \\( p(\\psi) \\). Here, we will use the same generative mechanism as Hollmann et al. (2023) for pre-training the proposed equivariant model."}, {"title": "3.2. Pre-training procedure", "content": "TabPFN is trained by predicting test target values \\( (y_m^*)_{m=1}^M \\) given corresponding test covariates \\( (x_m^*)_{m=1}^M \\) as well as training covariates/target pairs \\( (x_n, Y_n)_{n=1}^N \\). Denoting for simplicity (X, Y) and \\( (X^*, Y^*) \\), the collection of training and test samples, TabPFN's pre-training objective is given by:\n\\begin{equation}\n    L(f) := \\mathbb{E} [l (f_{X,Y} (X^*),Y^*)],\\tag{1}\n\\end{equation}\nwhere \\( (y, y') \\mapsto l(y, y') \\in \\mathbb{R} \\) is a point-wise loss, while \\( f_{X,Y} (X^*) \\) is the output of the network when provided with the training collection (X, Y) and test queries \\( X^* \\). Here, the expectation is over the collections (X, Y) and \\( (X^*, Y^*) \\) sampled according to the dataset prior. The loss \\( l \\) is, typically, chosen to be the mean-squared-error loss in the case of regression or the cross-entropy loss in the case of classification. Note that each test query \\( x_m^* \\) is processed independently by the network f so that \\( f_{X,Y} (X^*) = (f_{X,Y}(x_m^*))_{m=1}^M \\). The above procedure trains the model to directly infer new covariate/target dependences from the provided training pairs and will be used in our setting as well."}, {"title": "3.3. TabPFN architecture", "content": "The architecture used in TabPFN can be decomposed as into three modules, an encoder, a backbone and a decoder.\nLinear Encoder. The encoder module constructs training and test tokens \\( (e_n)_{n=1}^N \\) and \\( (e_m)_{m=1}^M \\) that are provided to the transformer backbone assuming the inputs x and y are vectors of fixed dimensions p and qmax. Each training token \\( e_n \\) is obtained by linearly embedding both covariate \\( x_n \\) and target \\( y_n \\) into a feature space of fixed dimension d and then summing both embeddings, i.e. \\( e_n = Ux_n + Vy_n \\), where U and V are trainable matrices of sizes \\( d \\times p \\) and \\( d \\times q \\). On the other hand, the test token consists only in embedding the test covariate \\( x_m^* \\), i.e. \\( e_m^* = Ux_m^* \\) since the test target \\( y_m^* \\) is not provided to the network. While targets with smaller dimensions can be handled by a simple zero-padding procedure (see Hollmann et al. (2023)), the encoder cannot easily accommodate target data with dimensions greater than q.\nTransformer backbone. The backbone consists of a succession of residual multi-head self-attention layers between all tokens followed by a residual feed-forward network applied to each token. In order to avoid information leakage from test to train data, an attention mask ensures that all tokens can only attend to the training tokens. This also ensures that test tokens are processed independently from each other. The residual connections in the backbone preserve the initial feature dimension d, so that each token is still associated to a particular sample while gradually incorporating information from all training tokens.\nMLP decoder. The decoder consists of a one-hidden layer MLP that takes each test output token \\( e_m^* \\) produced by the transformer backbone and produces a prediction vector \\( \\hat{y}_m \\) of fixed dimension q. This choice of decoder requires a predefined target dimension q. Thus, it cannot be used post-hoc on new data with higher target dimensions.\nA notable property of the TabPFN architecture is its invariance of the test predictions to the order by which training and test points are provided. This property is desirable since the order of training points is arbitrary and should not influence predictions on test samples. However, the network lacks equivariance w.r.t. the targets' dimensions, meaning that predictions are highly dependent on the order by which the training target dimensions are provided, an undesirable property as we show next."}, {"title": "4. Target permutation equivariance in PFN", "content": "When presented with a new unseen dataset of covariate/target pairs \\( (x_n, Y_n)_{n=1}^N \\), the order of the component's target is arbitrary. In other words, given target vectors of the form \\( y_n = ((y_n)_1,\u2026\u2026\u2026, (y_n)_q) \\), these could as well be presented in a different order by applying a permutation \\( \\sigma \\) to the components of \\( y_n \\) to obtain a permuted target \\( \\sigma(y_n) = ((y_n)_{\\sigma(1)},..., (y_n)_{\\sigma(q)}) \\). The transformed dataset \\( (x_n, \\sigma(y_n))_{n=1}^N \\) is still essentially the same as the original dataset up to the permutation as we only changed the order of the target components. For instance, when the target represents a one hot encoding vector of 2 classes: \"red\" or \"blue\", it should not matter whether we encode \"red\" as the first or the second class in the one-hot vector. Consequently, a pre-trained model should be able to provide consistent predictions regardless of the component's order. More formally, the model should satisfy the following equivariance property:"}, {"title": "Definition 4.1 (Target permutation equivariance)", "content": "A function f is permutation equivariant in the targets' components iff for any training data (X, Y) and test covariates \\( X^* \\):\n\n\\sigma^{-1} (f_{X,\\sigma(Y)} (X^*)) = f_{X,Y} (X^*),\\tag{2}\n\nwhere \\( S_q \\) denotes the set of all possible permutations of the components of a vector of q elements."}, {"title": "4.2. Non-equivariance of TabPFN model", "content": "As introduced in Hollmann et al. (2023), TabPFN is not permutation equivariant in the target's components. Consequently, it is not guaranteed to provide consistent predictions when the target components are permuted, thus affecting its robustness. To illustrate the implications of non-equivariance on the robustness of TabPFN, we consider a toy classification problem in 2 dimensions, where 9 training covariates are positioned on a 2-dimensional regular grid, each corresponding to a different class inspired by McCarter (2024). The pre-trained TabPFN model is then used to predict the classes on a regular grid of 900 points.\nThe non-equivariance of TabPFN is mainly due to the choice of both encoders and decoders that force an arbitrary order on the dimensions of the targets. In Section 5, we overcome this limitation by introducing a new architecture that is target permutation equivariant. As illustrated in Figure 1 (bottom), the predictions made by the proposed network remain the same regardless of the class ordering. Next, we provide a theoretical justification for enforcing equivariance in the model by analyzing the optimal solution to the pre-training objective considered in Equation (1)."}, {"title": "4.3. Bounds on non-equivariant models", "content": "From a theoretical standpoint, permutation equivariance of a pre-trained model is also a natural and important property to satisfy. To illustrate this fact, we start with a decomposition of the objective into an equivariance gap and an optimality error and then establish that the optimal solution of the pre-training problem in Equation (1) must be permutation equivariant in the target's components, as soon as the model is expressive enough.\nEquivariance gap. We define the equivariance gap \\( \\mathcal{E}^{equi}(f) \\) of a function f relative to L as the difference between the objective value at f and at its symmetrized version \\( f^{equi} \\):\n\\begin{equation}\n    \\mathcal{E}^{equi}(f) := L(f) \u2013 L(f^{equi}).\\tag{3}\n\\end{equation}\nHere, the symmetrized function \\( f^{equi} \\) is an equivariant function obtained from f by the averaging operation:\n\n\\begin{equation}\n    f_{X,Y}^{equi} (X^*) = \\mathbb{E}_{\\sigma} [\\sigma^{-1} (f_{X,\\sigma(Y)} (X^*))],\\tag{4}\n\\end{equation}"}, {"title": "Proposition 4.2.", "content": "Under Assumptions (A) and (B), the objective L(f) in Equation (1) admits the decomposition:\n\n\\begin{equation}\n    L(f) = L(f^{equi}) + \\mathcal{E}^{equi}(f),\\tag{5}\n\\end{equation}\nwhere the equivariance gap \\( \\mathcal{E}^{equi}(f) \\) is always non-negative and only equal to 0 when f is equivariant to permutations. Furthermore, when l is the squared loss (i.e. \\( l(y,y') = ||y - y' ||^2 \\)), then\n\\begin{equation}\n    \\mathcal{E}^{equi} (f) = \\mathbb{E}_{p} [|| f_{X,Y} (X^*) - f_{X,Y}^{equi} (X^*) ||^2] .\n\\end{equation}\n\nA consequence of Proposition 4.3 is that a non-equivariant model must learn to be equivariant in addition to reducing the prediction error, as we further verify empirically in Section 6. Next, we introduce an equivariant architecture that bypasses these limitations while resulting in competitive performance."}, {"title": "5. Target Equivariant Prior Fitted Network", "content": "We introduce EquiTabPFN, a new model architecture for in-context learning on tabular data, that is permutation equivariant w.r.t. the target's components. Our architecture integrates self-attention mechanisms across data points and data components to leverage relationships between datapoints while preserving equivariance by processing individual attributes (such as the targets components). Following the notation in Section 3, EquiTabPFN takes training covariate/target pairs (X, Y) and test covariate \\( X^* \\) as inputs and produces a prediction \\( Y^* = f_{X,Y} (X^*) \\) of the test target \\( Y^* \\). Unlike TabPFN which requires fixed target dimensions for all datasets, EquiTabPFN allows the dimensions of the target to change depending on the dataset as a consequence of its equivariant architecture. EquiTabPFN consists of four major modules: a target equivariant encoder, an attention module both across data components and across data points, and a non-parametric equivariant decoder, each designed to facilitate the end-to-end learning of components interactions and datapoint relationships"}, {"title": "5.1. Target equivariant encoder", "content": "The encoder constructs training and test tokens by applying a linear projection to both covariates and targets so that target equivariance is preserved. Specifically, following (Hollmann et al., 2023), each training and test covariate vector \\( x_n \\) and \\( x_m \\) is encoded into a single token of dimension d by applying a linear projection matrix U of size \\( d \\times p \\). However, instead of adding a linear projection of the training targets to each corresponding training token, as done in the case of TabPFN (see Section 3), we compute a token for each component \\( (y_n)_j \\) of a training target by multiplying them with an embedding vector V of dimension d for all \\( 1 \\le j \\le q \\). This operation amounts to applying a 1 \u00d7 1 convolution along the components of each target which preserves target equivariance. Since, the validation target \\( Y^* \\) is not provided to the model as input, it is replaced by a trainable prediction token \\( W^{pred} \\) of dimension d that is repeated M \u00d7 q times to form an initial guess \\( \\tilde{Y}^0 \\) of the target. All these embeddings along with prediction tokens are collected to form a single tensor E of shape (N + M, q + 1, d), where the blocks \\( E_{:N,1,:} \\), \\( E_{N:M,1,:} \\) correspond to embeddings of X and \\( X^* \\), \\( E_{:N,1:q,:} \\) represents the embedding of Y while \\( E_{N:M,1:q,:} \\) denotes the initial guess \\( \\tilde{Y}^0 \\) obtained using the prediction token. This tensor is then processed by the attention modules as described next."}, {"title": "5.2. Self-Attention Mechanisms", "content": "The core of the architecture involves two alternating self-attention modules: self-attention across components SelfAtt_c and self-attention across datapoints SelfAtt_b used for transforming the tokens. These alternating self-attention layers allow the model to learn both intra-samples components interactions and inter-samples relationships. Following standard design choices for transformers (Vaswani et al., 2017), we apply residual connections and layer normalization to ensure stability and robust gradient flow, i.e.:\n\n\\begin{equation}\n  E \\leftarrow LN (E + SelfAtt_{c/b}(E)),\n\\end{equation}\n\n\\begin{equation}\n    E \\leftarrow LN (E + MLP(E)),\n\\end{equation}\n\nwhere LN denotes the layer normalization layer (Ba et al., 2016), \\( SelfAtt_{c/b} \\) denotes one of the considered self-attention mechanisms and MLP is a one hidden-layer network acting on each embedding independently. Below, we describe both self-attention mechanisms in more detail.\nSelf-attention across target components allows interactions among components within each datapoint. It is applied independently per samples to preserve equivariance w.r.t. to the samples. We further employ a masking strategy that we found useful empirically: forcing target tokens to attend only to the covariate token, while allowing the covariate token to attend to all tokens."}, {"title": "5.3. Non-parametric equivariant decoder", "content": "The decoder aggregates the processed embeddings to produce prediction \\( \\hat{Y} \\). This is achieved in two steps: an attention module first computes an intermediate prediction \\( \\tilde{Y} = (\\tilde{y}_m)_{m=1}^M \\) in the form of a weighted average of training targets Y, then a residual correction is added to produce the final prediction. More precisely, the attention module uses the embeddings of the training and validation samples as keys and queries, while the attention values are simply the training targets Y:\n\n\\begin{equation}\n    \\tilde{y}_m \\triangleq \\sum_{n=1}^N Y_n SoftMax \\left( \\frac{E_{i,:} E_{n,i,:}^T}{\\sqrt{(1+q)d}} \\right)\\tag{6}\n\\end{equation}\nThe residual correction, in the form of a point-wise MLP, operates independently on each dimension j of the attention output \\( \\tilde{y}_m \\) so that equivariance is preserved while enabling nonlinear interactions between training values.\nWithout the residual correction and removing the dependence of the keys and queries embeddings on the training targets Y (for instance by setting the weights of the target encoder and pointwise MLP to 0), the decoder becomes a linear non-parametric regression estimator (Tsybakov, 2009, Definition 1.7), which is a generalization of Nadaraya-Watson's estimator \u00b9 (Nadaraya, 1964; Watson, 1964). However, linear estimators are known to be suboptimal compared to non-linear ones (Donoho & Johnstone, 1998). This motivates introducing a nonlinear dependence of the estimator to Y, in our setting, to increase the expressiveness of the decoder allowing it to adapt to the prediction task at hand. Experimentally, we also found a clear improvement when adding such a residual correction and making the embeddings dependent on the targets."}, {"title": "6. Experiments", "content": "We perform numerical experiments to evaluate our proposed model and compare it with two state-of-the-art pre-trained models for tabular dataset (TabPFN (Hollmann et al., 2023) and MotherNet (M\u00fcller et al., 2023)), as well as classical machine learning methods."}, {"title": "6.1. Experimental setup", "content": "Training procedure. We use a similar training protocol as in Hollmann et al. (2023), in which the model is trained on classification datasets generated according to their proposed artificial dataset prior. In this protocol, each dataset has a fixed size of 1024 and is split into training and test uniformly at random. The maximum number of classes is fixed to 10, while the maximum dimension of the covariate vector is fixed to 100. Following M\u00fcller et al. (2023), we represent the target y as a one-hot encoding vector whose dimension is the number of classes in the dataset. Moreover, we employ the exact same strategy for handling missing values in the covariates. Training is performed using 153600 batches of 72 synthetically generated datasets each, which means the model was exposed to ~11M artificial datasets during pre-training. This is a similar order of magnitude of datasets used for pre-training TabPFN by Hollmann et al. (2023). We used the Adam optimizer (Kingma & Ba, 2015) with initial learning rate of 0.0001 and linear-warmup scheduler for the first 10 epochs followed by cosine annealing (Loshchilov & Hutter, 2017) as in Hollmann et al. (2023). The total training time of the network lasts approximately 4 days on a single A100 GPU with 80GB of GPU memory. The resulting network is then used for all our evaluations without altering its parameters.\nArchitecture details. We use an EquiTabPFN network with 12 self-attention layers alternating between both type of attention introduced in Section 5: 6 blocks SelfAtt_c and 6 blocks SelfAtt_b. Each self-attention layer consists of a multi-head attention blocks with 4 heads, embeddings of dimension 512, and hidden layers of dimension 1024. This choice ensures a fair comparison with the models used in Hollmann et al. (2023); M\u00fcller et al. (2023), since the number of parameters (25.17M) are of the same order when counting them as proposed in Kaplan et al. (2020)."}, {"title": "6.2. Evaluation of the equivariance gap", "content": "We re-trained TabPFN using the same training protocol as Hollmann et al. (2023) to then evaluate its equivariance gap at intermediate training steps.  We use over 512 random datasets drawn from the prior and report standard deviation. We do not make those plots for EquiTabPFN as the equivariance error is exactly 0 by construction.\nIn Figure 3 (left), the equivariance error is clear and slowly decreases during training. This non-equivariance 1) induces additional error for the model as demonstrated in Proposition 4.2 and 2) causes the model to provide surprising predictions given that permuting the output order can change the results as seen in Figure 1.\nTo avoid this issue, Hollmann et al. (2023) proposed to run multiple predictions while randomly permuting the output. If done with all possible permutations, this gives an equivariant function as discussed in Equation (4) however, this requires making O(q!) forward passes where q is the number of features. This becomes quickly prohibitive, even for q = 10 as considered in the original study. This clearly shows that while ensembling more permutation helps to make the model more equivariant, many ensembles are required, in particular when considering more classes as one needs to approximate the expectation of Eq. 4 which has O(q!) terms."}, {"title": "6.3. Evaluation on real-world datasets", "content": "Following Hollmann et al. (2023), we evaluate EquiTabPFN on 30 real-world classification datasets from the OpenML-CC18 benchmark suite (Bischl et al., 2021) that contain less than 2000 samples, 100 covariates and 10 classes"}, {"title": "6.4. Evaluation on datasets with unseen class counts", "content": "Table 2 presents the average AUC for 9 datasets with more than 10 classes from the OpenML-CC18 benchmark suite (Bischl et al., 2021) (see Table 4 of the appendix for a list of the 9 datasets). This represents an out-of-distribution scenario, as the pretraining phase only included datasets with up to 10 classes. However, since our architecture is equivariant, it can handle any number of classes during inference, within memory constraints. Notably, EquiTabPFN outperforms the baselines, even with the number of classes exceeding those encountered during training.\nIt is worth emphasizing that performing inference on datasets with more classes than those seen during pre-training would be challenging for TabPFN and MotherNet. This limitation arises because the method would need to project vectors with dimensions exceeding those encountered during training."}, {"title": "6.5. Additional experiment details", "content": "The results of Tables 1 and 2 are obtained using the same evaluation protocol as in Hollmann et al. (2023) with code for training the baseline methods and selecting their hyperparameters provided by the authors of (M\u00fcller et al., 2023)\u00b2. Specifically, each dataset is split randomly into training and test subsets of the same size (N = M = 1000). Predictions for each method are averaged over 5 random splits of each dataset. The three pre-trained models we consider (EquiTabPFN, TabPFN, MotherNet) do not require any dataset specific tuning, while all other baselines are trained by allocating 60 minutes budget for hyper-parameter selection for"}, {"title": "7. Conclusion", "content": "In this paper, we discussed the property of target equivariance and its importance for foundational tabular models. We proved that having a non-equivariant model provably worsens the pre-training objective with an incompressible term and proposed an architecture that is equivariant to target permutation. We showed that our new architecture performs better than TabPFN, Mothernet and traditional tabular baselines on artificial and real-world datasets. We hope this work can be useful to interpret tabular foundational models as deep kernel machines and offer new insights to understand the performance of tabular foundational models.\nWe will release our code for training and evaluating our model, enabling reproducibility of our results."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Proofs", "content": "Proof of Proposition 4.2. By Assumption (B) we can express L as an expectation of the form:\n\\begin{equation}\n    L(f) = \\mathbb{E}_{p} \\mathbb{E}_{\\sigma} [l (f_{X,\\sigma(Y)} (X^*), \\sigma(Y^*))],\\tag{7}\n\\end{equation}\nBy Assumption (A), l is invariant to permutations, which allows to further write:\n\\begin{equation}\n    L(f) = \\mathbb{E}_{p} \\mathbb{E}_{\\sigma} [l (\\sigma^{-1} (f_{X,\\sigma(Y)} (X^*)),Y^*)].\\tag{8}\n\\end{equation}\nWe will show that \\( \\mathcal{E}^{equi}(f) = L(f) \u2013 L(f^{equi}) \\) is non-negative and vanishes only when f is equivariant. This is a direct consequence of Jensen's inequality applied to the strictly convex function \\( y \\rightarrow l(y, y^*) \\) (Assumption (A)). Indeed, for any samples \\( (X, Y, X^*, Y^*) \\), the following holds:\n\nrequi\n  l (fy (X*), Y*) : = l (E, [\u03c3\u00af\u00b9 (fx,\u03c3(Y) (X*))], Y*)\n\u2264 Eol (\u03c3\u00af\u00b9 (fx,\u03c3(Y) (X*)),Y*),\nrequi\n  (fa (X*), Y*) = \nl (f(X*), Y*) almost surely. However, since l is strictly convex in its first argument (Assumption (A)), the previous\nwhere the first line follows by definition of fequi while the second line uses Jensen's inequality. Further taking the expectation\nw.r.t. p shows that Eequi(f) \u2265 0. If Eequi (f) = 0, then by the above inequality\nequality is only possible when fequi = f almost surely, meaning that f is equivariant.\nFinally, to show the final result, we note that:\n\nquif) =L(f) \u2013 L(fquu)\n=EpEo [|\u03c3\u00af\u00b9 (fx,o(Y) (X*)) \u2013 foquy (X*) + foy feque (X*) \u2013 Y*||2] \u2013 L(fequi)\nE, [|\u03c3\u00af\u00b9 (fx.(Y) (X*)) fouy (X*) 2\nT\nT\n2EE [(-1 (fx.qY) (X*)) (1 (X*)||2\n+2 E, (\u03c3 (fx.5(Y) (X*)) foy (X", "that": "nEequi(f) = EpEo [||\u03c3\u00af\u00b9 (fx,o(Y) (X*)) \u2013 foquy (X*) ||2]\nFinally, we use the invariance of the squared error to permutations, the equivariance of fequi to permutations, and the\ninvariance of p to permutations to get:\n(\u03c3\u00af\u00b9 (fx,a(Y) (X*)) (1 (X*) = EpEo [fx,(Y) (X*) 2 Ep [fx,(Y) - fouy (X*)]2\n= Ep [fx,y (X*) - foquy (X*)] 2 ."}, {"title": "B. Binary classification decision boundary", "content": "In Figure 5, we show the decision boundary on 3 binary classification datasets for multiple baselines. To illustrate the stability of the method, we do not do ensembling for TabPFN and EquiTabPFN."}, {"title": "C. Additional experiment results", "content": "In Figure 6, we show the boxplot of normalized AUC in all datasets. The AUC is min-max normalized with the minimum and maximum scores observed on each dataset.\nWhen computing critical diagram in Figure 4, we use Autorank implementation (Herbold, 2020) with default hyperparameters which corresponds to a significance level of 0.05."}, {"title": "D. Experimental details", "content": ""}]}