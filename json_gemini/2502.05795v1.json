{"title": "The Curse of Depth in Large Language Models", "authors": ["Wenfang Sun", "Xinyuan Song", "Pengxiang Li", "Lu Yin", "Yefeng Zheng", "Shiwei Liu"], "abstract": "In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models (LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling, which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Our experimental results, spanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling significantly enhances LLM pre-training performance compared to Pre-LN. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training. Our code is available at LayerNorm-Scaling.", "sections": [{"title": "1. Introduction", "content": "Recent studies reveal that the deeper layers (Transformer blocks) in modern LLMs tend to be less effective than the earlier ones (Yin et al., 2024; Gromov et al., 2024; Men et al., 2024). On the one hand, this interesting observation provides an effective indicator for LLM compression. For instance, we can compress deeper layers significantly more (Yin et al., 2024; Lu et al., 2024; Dumitru et al., 2024) to achieve high compression ratios. Even more aggressively, entire deep layers can be pruned completely without compromising performance for the sake of more affordable LLMs (Muralidharan et al., 2024; Siddiqui et al., 2024).\nOn the other hand, having many layers ineffective is undesirable as modern LLMs are extremely resource-intensive to train, often requiring thousands of GPUs trained for multiple months, let alone the labor used for data curation and administration (Achiam et al., 2023; Touvron et al., 2023). Ideally, we want all layers in a model to be well-trained, with sufficient diversity in features from layer to layer, to maximize the utility of resources (Li et al., 2024b). The existence of ill-trained layers suggests that there must be something off with current LLM paradigms. Addressing such limitations is a pressing need for the community to avoid the waste of valuable resources, as new versions of LLMs are usually trained with their previous computing paradigm which results in ineffective layers.\nTo seek the immediate attention of the community, we introduce the concept of the Curse of Depth (CoD) to systematically present the phenomenon of ineffective deep layers in various LLM families, to identify the underlying reason behind it, and to rectify it by proposing LayerNorm Scaling. We first state the Curse of Depth below.\nThe Curse of Depth. The Curse of Depth refers to the observed phenomenon where deeper layers in modern large language models (LLMs) contribute significantly less to learning and representation compared to earlier layers. These deeper layers often exhibit remarkable robustness to pruning and perturbations, implying they fail to perform meaningful transformations. This behavior prevents these layers from effectively contributing to training and representation learning, resulting in resource inefficiency.\nEmpirical Evidence of CoD. The ineffectiveness of deep layers in LLMs has been previously reported. Yin et al. (2024) found that deeper layers of LLMs can tolerate significantly higher levels of pruning compared to shallower layers, achieving high sparsity. Similarly, Gromov et al."}, {"title": "Mitigating CoD through LayerNorm Scaling", "content": "We propose LayerNorm Scaling, which scales the output of Layer Normalization by the square root of the depth. LayerNorm Scaling effectively scales down the output variance across layers of Pre-LN, leading to considerably lower training loss and achieving the same loss as Pre-LN using only half tokens. Figure 1 compares the layerwise output variance across different setups: (1) Pre-LN, (2) Pre-LN with Scaled Initialization (Takase et al., 2023b), and (3) LayerNorm Scaling. As shown, Pre-LN exhibits significant variance explosion in deeper layers. In contrast, LayerNorm Scaling effectively reduces output variance across layers, enhancing the contribution of deeper layers during training. This adjustment leads to significantly lower training loss compared to Pre-LN. Unlike previous LayerNorm variants (Li et al., 2024b; Liu et al., 2020), LayerNorm Scaling is simple to implement, requires no hyperparameter tuning, and introduces no additional parameters during training. Furthermore, we further show that the model pre-trained with LayerNorm Scaling achieves better performance on downstream tasks in self-supervised fine-tuning, all thanks to the more effective deep layers learned.\nContributions.\n\u2022 We introduce the Curse of Depth to highlight, understand, and rectify the phenomenon in LLMs that is commonly overlooked deep layers fail to contribute as effectively as they should.\n\u2022 We identify the root cause as Pre-LN, which causes output variance to grow exponentially with model depth. This leads to deep Transformer blocks having derivatives close to the identity matrix, rendering them ineffective during training. While scaled initialization (Shoeybi et al., 2020) helps mitigate variance at initialization, it does not prevent explosion during training.\n\u2022 To mitigate this issue, we propose LayerNorm Scaling, which inversely scales the output of Pre-LN by the square root of the depth. This adjustment ensures that all layers contribute effectively to learning, thereby improving LLM performance."}, {"title": "2. Empirical Evidence of the Curse of Depth", "content": "To empirically analyze the impact of layer normalization on the Curse of Depth in LLMs, we conduct a series of evaluations inspired by Li et al. (2024b), extending their methodology to compare Pre-LN and Post-LN models."}, {"title": "2.1. Experimental Setup", "content": "Methods: We evaluate Pre-LN and Post-LN models by assessing the impact of layer pruning at different depths. Our hypothesis is that Pre-LN models exhibit diminishing effectiveness in deeper layers, whereas Post-LN has less effective early layers. To verify this, we empirically quantify the contribution of individual layers to overall model performance across a diverse set of LLMs.\nLLMs: We conduct experiments on multiple widely adopted LLMs: BERT-Large (Devlin, 2019), Mistral-7B (Jiang et al., 2023), LLaMA2-7B/13B (Touvron et al., 2023), DeepSeek-7B (Bi et al., 2024), and Qwen-7B (Bai et al., 2023). These models were chosen to ensure architectural and application diversity. BERT-Large represents a Post-LN model, whereas the rest are Pre-LN-based. This selection enables a comprehensive evaluation of the effects of layer normalization across varying architectures and model scales."}, {"title": "2.2. Layer Pruning Analysis", "content": "Figure 2 presents the performance drop ($\\Delta P^{(l)}$) across different layers for six LLMs, including one Post-LN model (BERT-Large) and five Pre-LN models (Mistral-7B, LLaMA2-13B, Qwen-7B, DeepSeek-7B and LLaMA2-7B).\nAs shown in Figure 2 (a), pruning deeper layers in BERT-Large leads to a significant decline in accuracy on SQUAD v1.1, while pruning earlier layers has minimal impact. The"}, {"title": "3. Analysis of the Curse of Depth", "content": ""}, {"title": "3.1. Preliminaries", "content": "This paper primarily focuses on Pre-LN Transformer (Baevski and Auli, 2019; Dai et al., 2019). Let $x_l \\in \\mathbb{R}^d$ be the input vector at the $l$-th layer of Transformer, where $d$ denotes the feature dimension of each layer. For simplicity, we assume all layers to have the same dimension $d$. The layer output $y_l$ is calculated as follows:\n$y = x_{l+1} = x_l + \\text{FFN}(\\text{LN}(x_l))$,\n$x_l = x_l + \\text{Attn}(\\text{LN}(x_l))$,"}, {"title": "3.2. Pre-LN Transformers", "content": "Assumption 1. Let $x_l$ and $x'_l$ denote the input and intermediate vectors of the $l$-th layer. Moreover, let $W_l$ denote the model parameter matrix at the $l$-th layer. We assume that, for all layers, $x_l$, $x'_l$, and $W_l$ follow normal and independent distributions with mean $\\mu = 0$.\nLemma 1. Let $\\sigma^2_{x'_l}$, and $\\sigma^2_{x_l}$, denote the variances of $x'_l$ and $x_l$, respectively. These two variances exhibit the same overall growth trend, which is:\n$\\sigma^2_{x_l} = \\sigma^2_{x_1} \\Theta(\\prod\\limits_{k=1}^{l-1} (1 + \\frac{1}{\\sigma^2_{x_k}}))$,\nwhere the growth of $\\sigma^2_{x_l}$, is sub-exponential, as shown by the following bounds:\n$\\Theta(L) \\leq \\sigma^2_{x_l} \\leq \\Theta(\\text{exp}(L))$.\nHere, the notation $\\Theta$ means: if $f(x) \\in \\Theta(g(x))$, then there exist constants $C_1, C_2$ such that $C_1 |g(x)| \\leq |f(x)| \\leq C_2|g(x)|$ as $x \\rightarrow \\infty$. The lower bound $\\Theta(L) \\leq \\sigma^2_{x_l}$ indicates that $\\sigma^2_{x_l}$ grows at least linearly, while the upper bound $\\sigma^2_{x_l} < \\Theta(\\text{exp}(L))$ implies that its growth does not exceed an exponential function of $L$.\nBased on Assumption 1 and the work of (Takase et al., 2023b), we obtain the following:\nTheorem 1. For a Pre-LN Transformer with $L$ layers, using Equations (2) and (3), the partial derivative $\\frac{\\partial y_L}{\\partial x_1}$ can be written as:\n$\\frac{\\partial y_L}{\\partial x_1} = \\prod\\limits_{l=1}^{L-1} \\frac{\\partial x'_l}{\\partial x_l}$.\nThe Euclidean norm of $\\frac{\\partial y_L}{\\partial x_1}$ is given by:\n$||\\frac{\\partial y_L}{\\partial x_1}||_2 = \\prod\\limits_{l=1}^{L-1}(1 + A + \\frac{B}{\\sigma^2_{x_l}})$,", "number": "3.2."}, {"title": "3.3. Post-LN Transformers", "content": "For Post-LN Transformers, we continue to adopt Assumption 1. In this setting, each layer is followed by a layer normalization (LN) step, ensuring that the variances $\\sigma^2_{x'_l}$ and $\\sigma^2_{x_l}$ remain fixed at 1 across all layers. Consequently, the norm $||\\frac{\\partial y_l}{\\partial x}||_2$ exhibits minimal variation from one layer to the next, indicating stable gradient propagation.\nSince the variance is effectively controlled by LN in Post-LN Transformers, an explicit variance-based analysis becomes less critical. Nonetheless, there remain other important aspects to investigate in deeper Post-LN architectures, such as the evolution of feature mappings and the behavior of covariance kernels over deep layers. These directions will be pursued in future work."}, {"title": "4. LayerNorm Scaling", "content": "Our theoretical and empirical analyses indicate that Pre-LN amplifies output variance, leading to the Curse of Depth and"}, {"title": "4.1. Theoretical Analysis of LayerNorm Scaling", "content": "Lemma 2. After applying our scaling method, the variances of $x'_l$ and $x_l$, denoted as $\\hat{\\sigma}^2_{x'_l}$ and $\\hat{\\sigma}^2_{x_l}$, respectively, exhibit the same growth trend, which is:\n$\\hat{\\sigma}^2_{x_{l+1}} = \\hat{\\sigma}^2_{x'_l} \\Theta(1 + \\frac{1}{l\\sigma^2_{x_l}})$,\nwith the following growth rate bounds:\n$\\Theta(L) \\leq \\hat{\\sigma}^2_{x_l} \\leq \\Theta(L^{(2-\\epsilon)})$.\nwhere $\\epsilon$ is a small number with $0 < \\epsilon < 1/4$.\nFrom Lemma 2, we can conclude that our scaling method effectively slows the growth of the variance upper bound, reducing it from exponential to polynomial growth. Specifically, it limits the upper bound to a quadratic rate instead of an exponential one. Based on Theorem 1, after scaling, we obtain the following:\nTheorem 2. For the scaled Pre-LN Transformers, the Euclidean norm of $||\\frac{\\partial y_L}{\\partial x_1}||_2$ is given by:\n$||\\frac{\\partial y_L}{\\partial x_1}||_2 \\leq \\prod\\limits_{l=1}^{L-1} (1 + A + \\frac{B}{l\\sigma^2_{x'_l}})$,\nwhere $A$ and $B$ are dependent on the scaled neural network parameters. Then the upper bound for the norm is given as follows: when $\\hat{\\sigma}^2_{x'_l}$ grows at $O(L^{(2-\\epsilon)})$, (i.e., at its upper bound), we obtain:\n$\\hat{\\sigma}^2_{x'_l} \\sim O(L^{(2-\\epsilon)}), \\quad ||\\frac{\\partial y_L}{\\partial x_1}||_2 < \\omega(1)$,\nwhere $\\omega$ denotes that if $f(x) = \\omega(g(x))$, then $\\lim\\limits_{x \\rightarrow \\infty} \\frac{f(x)}{g(x)} = \\infty$. Meanwhile, when $\\hat{\\sigma}^2_{x'_l}$ grows linearly (i.e., at its lower bound), we obtain:\n$\\hat{\\sigma}^2_{x'_l} \\sim l, \\quad ||\\frac{\\partial y_L}{\\partial x_1}||_2 < \\Theta(L)$.\nThe detailed descriptions of $A$ and $B$, and $\\epsilon$, along with the full proof, are provided in Appendices A.3 and A.4.\nBy comparing Theorem 1 (before scaling) with Theorem 2 (after scaling), we observe a substantial reduction in the upper bound of variance. Specifically, it decreases from exponential growth $O(\\text{exp}(L))$ to at most quadratic growth $\\Theta(L^2)$. In fact, this growth is even slower than quadratic expansion, as it follows $\\Theta(L^{(2-\\epsilon)})$ for some small $\\epsilon > 0$.\nWhen we select a reasonable upper bound for this expansion, we find that $||\\frac{\\partial y_L}{\\partial x_1}||_2$ no longer possesses a strict upper bound. That is, as the depth increases, $||\\frac{\\partial y_L}{\\partial x_1}||_2$ continues to grow gradually. Consequently, fewer layers act as identity mappings compared to the original Pre-LN where nearly all deep layers collapsed into identity transformations. Instead, the after-scaled network effectively utilizes more layers, even as the depth approaches infinity, leading to improved expressivity and trainability."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. LLM Pre-training", "content": "To evaluate the effectiveness of LayerNorm Scaling, we follow the experimental setup of Li et al. (2024b), using the same model configurations and training conditions to compare it with widely used normalization techniques, including Post-LN (Nguyen and Salazar, 2019), DeepNorm (Wang et al., 2024), and Pre-LN (Dai et al., 2019). In line with Lialin et al. (2023) and Zhao et al. (2024), we conduct experiments using LLaMA-based architectures with model sizes of 130M, 250M, 350M, and 1B parameters, ensuring consistency in architecture and training settings.\nThe architecture incorporates RMSNorm (Shazeer, 2020) and SwiGLU activations (Zhang and Sennrich, 2019), which are applied consistently across all model sizes and normalization methods. For optimization, we use the Adam optimizer (Kingma, 2015) and adopt size-specific learning rates: 1 \u00d7 10-\u00b3 for models up to 350M parameters, and 5 \u00d7 10-4 for the 1B parameter model. All models share the same architecture, hyperparameters, and training schedule, with the only difference being the choice of normalization method. Unlike Mix-LN (Li et al., 2024b), which introduces an additional hyperparameter \u03b1 manually set to 0.25, LayerNorm Scaling requires no extra hyperparameters, making it simpler to implement.  Table 1 shows that LayerNorm Scaling consistently outperforms other normalization methods across different model sizes. While DeepNorm performs comparably to Pre-LN on smaller models, it struggles with larger architectures like LLaMA-1B, showing signs of instability and divergence in loss values. Similarly, Mix-LN outperforms Pre-LN in smaller models but faces convergence issues with LLaMA-350M, indicating its sensitivity to architecture design and hyperparameter tuning due to the introduction of Post-LN. Notably, Mix-LN was originally evaluated on LLaMA-1B with 50,000 steps (Li et al., 2024b), while our setting extends training to 100,000 steps, where Mix-LN fails to converge, highlighting its instability in large-scale settings caused by the usage of Post-LN.\nIn contrast, LayerNorm Scaling solves the Curse of Depth without compromising the training stability thanks to its simplicity. LayerNorm Scaling achieves the lowest perplexity across all tested model sizes, showing stable performance improvements over existing methods. For instance, on LLaMA-130M and LLaMA-1B, LayerNorm"}, {"title": "5.3. LayerNorm Scaling Reduces Output Variance", "content": "As LayerNorm Scaling aims to reduce output variance, we validate this by comparing it with two scaling approaches: LayerScale (Touvron et al., 2021) and Scaled Initialization (Shoeybi et al., 2020). LayerScale applies per-channel weighting using a diagonal matrix, $\\text{diag}(\\lambda_1, ..., \\lambda_d)$, where each weight $\\lambda_i$ is initialized to a small value (e.g., $\\lambda_i = \\epsilon$). Unlike LayerNorm Scaling, LayerScale learns the scaling factors automatically, which does not necessarily induce a down-scaling effect. Scaled Initialization scales the initialization of $W_0$ and $W_2$ to small values by $\\frac{1}{\\sqrt{2L}}$, where $L$ is the total number of transformer layers. Since scaling is applied only at initialization, we argue that Scaled Initialization may not effectively reduce variance throughout training. We further verify this in Figure 1, where we can see the output variance of Scaled Initialization is as large as Pre-LN.  Table 4 presents the results of LLaMA-130M and LLaMA-250M. First, we can see that LayerScale degrades performance. While Scaled Initialization slightly improves over Pre-LN, it falls short of LayerNorm Scaling and the gap becomes larger for the larger model."}, {"title": "5.4. Enhancing Deep Layers with LayerNorm Scaling", "content": "To evaluate how LayerNorm Scaling improves deep layer effectiveness, we conduct a layer pruning experiment on LLaMA-130M, systematically removing individual layers and measuring the performance drop ($\\Delta P^{(l)}$) on the ARC-e benchmark (Clark et al., 2018). Figure 4 compares the pruning effects between standard Pre-LN and LayerNorm Scaling. In the Pre-LN, removing deep layers results in minimal performance degradation, indicating their limited"}, {"title": "6. Related Work", "content": "Layer Normalization in Language Models. LN (Ba, 2016) was initially applied after the residual connection in the original Transformer (Vaswani, 2017), which is known as Post-LN. Later on, Pre-LN (Baevski and Auli, 2019; Dai et al., 2019; Nguyen and Salazar, 2019) dominated LLMs, due to its compelling performance and stability (Brown et al., 2020; Touvron et al., 2023; Jiang et al., 2023; Bi et al., 2024). Prior works have studied the effect of Pre-LN and Post-LN. Xiong et al. (2020) proves that Post-LN tends to have larger gradients near the output layer, which necessitates smaller learning rates to stabilize training, whereas Pre-LN scales down gradients with the depth of the model, working better for deep Transformers. Wang et al. (2019) empirically confirmed that Pre-LN facilitates stacking more layers and Post-LN suffers from gradient vanishing. The idea of connecting multiple layers was proposed in previous works (Bapna et al., 2018; Dou et al., 2018; Wang et al., 2019). Adaptive Model Initialization (Admin) was introduced to use additional parameters to control residual dependencies, stabilizing Post-LN. DeepNorm (Wang et al., 2024) enables stacking 1000-layer Transformer by upscaling the residual connection before applying LN. Additionally, Ding et al. (2021) proposed Sandwich LayerNorm, normalizing both the input and output of each transformer sub-layer. Takase et al. (2023a) introduced B2T to bypass all LN except the final one in each layer. Li et al. (2024b) recently combines Post-LN and Pre-LN to enhance the middle layers."}, {"title": "7. Conclusion", "content": "In this paper, we introduce the concept of the Curse of Depth in LLMs, highlighting an urgent yet often overlooked phenomenon: nearly half of the deep layers in modern LLMs are less effective than expected. We discover the root cause of this phenomenon is Pre-LN which is widely used in almost all modern LLMs. To tackle this issue, we introduce LayerNorm Scaling. By scaling the output variance inversely with the layer depth, LayerNorm Scaling ensures that all layers, including deeper ones, contribute meaningfully to training. Our experiments show that this simple modification improves performance, reduces resource usage, and stabilizes training across various model sizes. LayerNorm Scaling is easy to implement, hyperparameter-free, and provides a robust solution to enhance the efficiency and effectiveness of LLMs."}, {"title": "8. Impact Statement", "content": "This paper introduces the Curse of Depth in LLMs to call attention to the AI community an urgent but often overlooked phenomenon that nearly half of layers in modern LLMs are not as effective as we expect. The impact of this phenomenon is large that a significant amount of resources used to train LLMs are somehow wasted. We further introduce LayerNorm Scaling to ensure that all layers contribute meaningfully to model training. The result is a significant improvement in model efficiency, enabling better performance with fewer computational resources and training tokens. This innovation not only enhances LLM effectiveness across a variety of tasks but also reduces the environmental and financial costs of training large-scale models, making LLM development more sustainable and accessible. LayerNorm Scaling presents a simple, hyperparameter-free solution that can be easily adopted, offering immediate practical benefits to the AI research community."}]}