{"title": "A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions", "authors": ["Youliang Zhang", "Ronghui Li", "Yachao Zhang", "Liang Pan", "Jingbo Wang", "Yebin Liu", "Xiu Li"], "abstract": "Extracting physically plausible 3D human motion from videos is a critical task. Although existing simulation-based motion imitation methods can enhance the physical quality of daily motions estimated from monocular video capture, extending this capability to high-difficulty motions remains an open challenge. This can be attributed to some flawed motion clips in video-based motion capture results and the inherent complexity in modeling high-difficulty motions. Therefore, sensing the advantage of segmentation in localizing human body, we introduce a mask-based motion correction module (MCM) that leverages motion context and video mask to repair flawed motions, producing imitation-friendly motions; and propose a physics-based motion transfer module (PTM), which employs a pretrain and adapt approach for motion imitation, improving physical plausibility with the ability to handle in-the-wild and challenging motions. Our approach is designed as a plug-and-play module to physically refine the video motion capture results, including high-difficulty in-the-wild motions. Finally, to validate our approach, we collected a challenging in-the-wild test set to establish a benchmark, and our method has demonstrated effectiveness on both the new benchmark and existing public datasets.", "sections": [{"title": "1. Introduction", "content": "Physical plausible 3D human motion is in high demand across various fields, including virtual reality, game, animation industries, and academic research on virtual humans and robotics [4, 7, 13, 27, 32, 38, 46, 62, 64, 77]. With technological advancements, monocular video-based motion capture algorithms provide a fast and convenient pipeline for obtaining 3D motion closely aligned with video. However, these methods inherently lack dynamic modeling, resulting in significant physical unrealisms such as floating, foot sliding, self-intersections, and ground penetration, things get worse when facing high-difficulty motions.\nTo enhance physical realism, some methods use dynamic equations and train a network to predict physical parameters [10, 24, 59, 70]. However, these methods often struggle to improve motion plausibility due to oversimplified dynamic equations. Other methods [9, 12, 51, 73] use physical-simulation-based motion imitation as a post-processing module, learning motion control policy to imitate the reference motion (video motion capture results) in a simulated physical environment. With high-quality reference motions, these methods improve the physical realism of daily motions such as walking, running, and jumping. However, they can not handle high-difficulty movements like gymnastics and martial arts. Regarding this, we aim to extend the physical restoration ability of motion imitation in high-difficulty and in-the-wild motions, meeting broader requirements for motion asset acquisition.\nReviewing the characteristics of high-difficulty motions, they often involve rapid movement, extreme poses, skilled force control, and follow a long-tail distribution in existing datasets. This presents two major challenges for existing motion imitation methods to enhance the physical plausibility of complex motions within a physical simulation environment: (1) Flawed Reference Motions: As shown in Fig 1(b), even the state-of-the-art video motion capture algorithms estimate flawed motions when facing challenging movements. Such brief disruptions can easily cause failures in the motion imitation process and are obvious in the human senses. (2) Inherent Imitation Complexity: The long-tail distribution of high-difficulty motions and their complex force control skills make it challenging for current motion imitation methods to track high-difficulty motions, shown in Fig 1(c). Moreover, a single controller struggles to generalize across a diverse range of high-difficulty movements, facing catastrophic forgetting issues, where rapid loss of old knowledge occurs when learning new skills [33].\nTo solve the issue of flawed reference motion, we propose a mask-conditioned correction module (MCM). Due to the blurred frames caused by rapid and extreme poses, video motion capture algorithms struggle to localize body parts accurately when facing high-difficulty motions. Notably, the nature of segmentation methods to distinguish the foreground and background mitigates the effects of blurred frames, allowing them to define an approximate range of body. Also, the flaw motion occurs over a short time and is surrounded by rich motion contexts, making segmentation-guided interpolation and replacement of flaw motion possible. We first leverage a large visual semantic segmentation model [20] to obtain human segmentation masks of the input video. Then, guided by segmentation masks and reference motion context, we regenerate context-consistent and imitation-friendly motions to replace the flawed motion, allowing accurate and complete motion imitation.\nTo tackle the inherent imitation complexity of diverse challenging movements, we propose a Physics-based Motion Transfer Module (PTM) consisting of a pre-trained imitation controller and a test time adaptation strategy (TTA). Utilizing the trial-and-error nature of reinforcement learning and pre-trained motion prior, we targeted adapting our network to current motion in test time, naturally addressing the long-tail distribution and domain gap issues by parameter updating. To better achieve complex force control, our TTA contains unique imitation settings for adaptation to facilitate tracking the noisy yet challenging motions captured from videos. The pretrain and adapt pattern of PTM greatly improves the imitation ability, allowing the successful physical restoration of in-the-wild and high-difficulty motions.\nThrough our proposed novel MCM and PTM, we successfully address the failure issues of flaw motion and complex motion simulation, achieving physical authenticity restoration in high-difficulty motions while faithfully retaining the original movements. It is worth mentioning that without additional training, our method can be conveniently integrated into any video motion capture method, directly repairing in-the-wild and high-difficulty motions.\nTo validate the effectiveness of our proposed motion restoration method, we collected 206 high-difficulty motion videos entirely in the wild, including activities such as rhythmic gymnastics, taekwondo, and yoga. Our method demonstrated strong performance on this in-the-wild test set, which is significantly more challenging than the training set, further proving the effectiveness of our approach. Our contributions can be summarized as follows:\n\u2022 We propose a plug-and-play motion restoration method that enhances the physical realism of motions captured from monocular video. It can be integrated with existing motion capture algorithms to improve the efficiency of obtaining high-quality 3D motion.\n\u2022 We introduced an MCM for correcting short-term flawed motions, producing consistent and imitation-friendly motions for physical restoration.\n\u2022 We introduced a PTM with a pretrain and adapt motion imitation pattern, allowing the physical restoration of high-difficulty and in-the-wild motions.\n\u2022 We collected a challenging in-the-wild test set to establish a benchmark, and our method demonstrates effectiveness on both the new benchmark and existing public datasets."}, {"title": "2. Related Work", "content": "2.1. Video Motion Capture\nMost works for video motion capture are to recover the parameters of a parametric human model [2, 6, 11, 17, 21, 28, 30, 36, 49, 54, 58]. Recently, many methods have started to consider moving cameras. TRACE [55] and WHAM [53] propose regressing per-frame poses and translations. SLAHMR [71] and PACE [22] integrate SLAM [56, 57] with motion priors [45] into the optimization framework. TRAM [67] leverages the scene background to derive motion scale. GVHMR [50] estimates human poses in a novel Gravity-View coordinate system. While these methods achieve significant success in reconstructing high-difficulty motions from videos, they suffer from serious physical issues and occasionally experience flawed motions when facing complex movements. Our proposed physical motion"}, {"title": "3. Physics-based Motion Restoration", "content": "Our method takes video-captured motion as reference motions and focuses on restoring their physical realism while preserving original motion patterns. The motion representation $\u00e6_t$ consists of joint position $p_t \\in R^{J \\times 3}$ and rotation $\\theta_t \\in R^{J \\times 6}$ [79], compatible with SMPL format [28]. J means the joint number of the humanoid. The reference velocities $q_t$ is also considered, which consists of the linear $v_t \\in R^{J \\times 3}$ and angular $w_t \\in R^{J \\times 6}$ velocity. An overview of our method is provided in Fig 2. Given the reference motion (video motion capture results) and corresponding video, our MCM detects and corrects the flawed motion. Our PTM inputs the corrected motion and performs physical restoration by motion imitation. In PTM, a pre-trained controller and a carefully designed adaptation strategy are well-cooperated to solve the dynamics of a single motion. This pre-train and adapt pattern makes our PTM perform well in tracking high-difficulty and in-the-wild motions."}, {"title": "3.1. Preliminaries", "content": "Motion Imitation. The problem of controlling a humanoid to follow a reference motion sequence can be formulated as a Markov Decision Process, defined by the tuple $M = \\langle S, A, P_{physics}, R, \u03b3 \\rangle$, which consists of states, actions, transition dynamics, reward, and a discount factor. At step t, agent samples an action $a_t$ from the policy $\u03c0_{PTM}(a_t|s_t)$ based on the current state $s_t$, and the environment responds with the next state $s_{t+1}$ and a reward $r_t$. Proximal Policy Optimization [48] is used to optimize the policy $\u03c0_{PTM}$ by maximize the expected discounted return $E_{\\[\u03c0_{PTM}\\]} \\[\\sum_{t=1}^{T} \u03b3^{t-1}r_t\\]$. The state $s_t$ consists of positions, rotations, and linear and angular velocities of humanoid, as well as the next frame information $g_t$. We define $g_t$ as the difference between the current frame and next frame reference motion [33, 74]. The action $a_t$ specifies the target humanoid joint angles for the controller at each degree of freedom (DoF). Given an action and current motion $x_t$ and velocity $q_t$, the torque to be applied is computed as:\n$T_t^i = k_p^i (a_t^i - x_t^i) - k_d^i q_t^i$,  (1)\nwhere $i$ means joint DoF index, $k_p$ and $k_d$ are manually-specified gains. Our policy $\u03c0_{PTM}$ is constructed with multilayer perceptions and ReLU functions. A discriminator from AMP [43] is used to predict whether a given state $s_t$ and action $a_t$ is sampled from the demonstrations M or generated by policy PTM. The reward consists of a reconstruction reward $r_c$ to follow the reference motion, a style reward $r_{amp}$ produced by the amp discriminator, and an energy penalty reward $r_{energy}$ [41] to prevents motion jitter.\n$r_t = r_c + r_{amp} + r_{energy}$  (2)\nMotion Diffusion Model. The diffusion model consists of a forward diffusion process that progressively adds noise to the clean data and a reverse diffusion process trained to reverse this process. The forward diffusion process introduces noise for N steps formulated using a Markov chain:\n$q(x_{1:N} | x_0) := \\prod_{n=1}^{N} q(x_n | x_{n-1})$,  (3)\nReverse process employs a learnable network $f_\u03b8$ to denoise."}, {"title": "3.2. Mask-conditioned Motion Correction Module", "content": "Flaw motions in video capture results manifest as incoherent frames within the motion sequence inconsistent with the surrounding motion context. Although such brief disruptions have a minimal impact on metrics, they can easily cause failures in the physics simulation process and are obvious in the human senses. To address this issue, our MCM first detects flawed motion and then regenerates the flawed motion segment guided by the motion context and human mask signals, ultimately replacing the flawed motion.\nMismatch Detection. Given the reference motion and its corresponding video, we project the 3D positions of the reference motion into 2D camera coordinates. Also, an object detection algorithm is used to extract the corresponding 2D keypoints from the video. Using the Object Keypoint Similarity (OKS) algorithm, we compute the matching degree between the two sets of keypoints and obtain a similarity score sequence. Frames with a similarity score below a certain threshold will be flagged as flaw motion.\n$OKS = \\frac{\\sum_i exp(-d_i^2 / 2s^2 e^2) \u03b4(v_i > 0)}{\\sum_i \u03b4(v_i > 0)}$  (4)\nwhere $v_i$ represents the visibility flag, $e$ denotes the scale factor, and $d_i$ is the distance difference between the projection and the detection results. Additionally, segmentation algorithms can also be used to detect flaw motion. We project the SMPL-generated mesh onto the 2D plane and treat it as a set of pixel points. The matching similarity can be calculated by determining the proportion of projected mesh points that are contained within the human mask.\nMotion Correction Goal Design. Unlike traditional motion in-between methods, the goal of MCM is to replace flawed motions in the reference sequence based on contextual information, ensuring that the corrected motion is smooth and reasonable. Thus, we focus on the model's ability to fill temporal gaps. Since the input is motion captured from video, ensuring the generated motion remains consistent with the original video content is crucial. Therefore, we use the human mask obtained from segmentation as a conditional signal to guide the in-between process.\nMask-conditioned Diffusion In-betweening. Given a reference motion sequence $x \\in R^{N \\times D}$, the segmented human mask (obtained from SAM [20]) $m \\in R^{N \\times w \\times h}$, and a keyframe signal $c \\in R^{N}$ (mismatch detection results to identifies flaw motion), this module correct the reference motion by replacing mismatched motion frames. We employ a pre-trained Vision Transformer (ViT) as the mask feature extractor to capture rich human pose information from the segmentation mask. The mask combined with the motion context is used as the condition of the motion diffusion model. Following [3, 18], we concatenate the resulting sample, keyframe signal, and mask features as model input to inform the generation model with condition signal.\nTraining process. A random motion segment, selected at a random sequence position, is chosen as the generation target. Our model is trained to reconstruct this segment. Both keyframe conditioning signals c and mask conditioning signal m are set to 0 for 10% of training data to make our model suited for unconditioned motion generation."}, {"title": "3.3. Physics-based Motion Transfer Module", "content": "Our PTM consists of a pre-trained imitation controller and a test time adaptation strategy. The controller, as defined in the Motion Imitation section, obtains basic motion imitation ability with substantial motion prior. The TTA strategy (with a set of adaptation settings) is carefully designed to adapt the controller to explore a single specific motion, solving the dynamic modeling of noised high-difficulty motion. Compared to previous methods, the pretrain and adapt pattern avoids limiting the model to a specific data domain. It models the dynamics for a single motion based on the normal prior, significantly enhancing the ability to handle out-of-domain and high-difficulty motions.\nImitation Controller Pre-training. Acquiring as much motion prior knowledge as possible during the pre-training phase is essential to accelerate the adaptation process. Therefore, we perform pre-training on 4 datasets: AMASS [37], Human3.6M [16], AIST++ [25, 60], and Motion-X [26] kungfu subset. The pre-training phase employs strict reconstruction rewards and early termination conditions.\nRL-based Test Time Adaptation. With a pre-trained imitation controller, rich motion priors open up the possibility for rapid adaptation during test time. Utilizing the trial-and-error nature of reinforcement learning, we propose RL-based test time adaptation, which involves performing a limited number of experiment steps on the current test data under specific adaptation settings (updating network parameters). Each motion sequence is treated as an individual instance, and the adaptation is performed for each instance independently. This method is particularly useful when dealing with high-difficulty and low-quality motions. The following designs are used in the adaptation process.\nRelative Reward. The captured reference motion contains jitter or fault roots with error accumulation, making constructing a full reconstruction reward detrimental. Therefore, in the adaptation process, we designed a relative reward $r_e$ that neglects the absolute root position, maintaining global orientation and translation through explicit guidance from rotation and implicit guidance from velocity. The relative reward is formulated as:\n$r_e = e_{wp} || rela(p_t) - rela(P_t) || + e_{wr} ||\\Theta_t - \\Theta_t^r|| + e_{wv} ||v_t - v_t^r || + e_{ww} ||w_t - w_t^r||$,  (5)\nwhere $p_t$ means the joint position of reference motion, $rela()$ means to ignore the gravity axis part of root joints. $|| \\Theta || $ means rotation difference and $w$ is weights factor.\nEarly Termination. In high-difficulty motion tracking, there is a large displacement between humanoid and reference motion, as reference motion often involves frequent floating and penetration. This phenomenon makes defining when an adaptation step should be terminated challenging, which is crucial for adaptation efficiency and preventing undesirable behaviors. Therefore, we design a relative termination condition by calculating each joint's mean relative distance between humanoid and reference motion. One adaptation step will be terminated when the distance exceeds threshold $d_{term}$. We also introduce termination condition $F_V$ and $F_E$ based on joint height and ground contact to consider falls and erroneous contacts occur. The full termination $F_t$ is defined below, a smaller threshold $d_{term}$ indicating a stricter adherence to reference motion.\n$F_t = (\\frac{1}{J} \\sum_{i=1}^{J} ||rela(p_t^i) - rela(P_t^i)|| > d_{term} ) \\lor F_V \\lor F_E$  (6)\nResidual Force. During the test-time adaptation phase, we introduce residual forces [72] to compensate for the dynamics mismatch. This is important because complex motions often involve airborne flips and jumps (commonly seen in gymnastics and martial arts), which frequently rely on elastic trampolines and mats for execution. The use of external forces is necessary to account for the absence of these environmental conditions in our simulations."}, {"title": "4. Experimental Results and Analysis", "content": "4.1. Datasets\nWe use four datasets to train our model: AMASS [37], Human3.6M [16], AIST++ [25, 60], and Motion-X [26] kungfu subset. AIST++ contains 5 hours of diverse dance motions, Motion-X is a huge motion generation dataset and its kungfu subset contains complex kungfu motions over 1k clips. We perform our evaluations on the test set of AIST++, EMDB [19], and kungfu. Sequences involving human-object interactions are removed for all datasets.\nWe collected 206 high-difficulty motion videos from YouTube, including rhythmic gymnastics (floor, ball, ribbon), dance (breakdancing, ballet, yoga), and martial arts (kungfu, taekwondo). We use these videos as in-the-wild data for evaluation. Compared to the previously mentioned datasets, these videos contain more complex motions, posing greater challenges for physics-based motion repair tasks. These data can also be used to evaluate the generalization capabilities of 3D human motion recovery methods, and we will make them publicly available.\n4.2. Metrics\nFollowing the latest method [50, 53, 67], we evaluate camera-coordinate metrics using the widely used MPJPE, Procrustes-aligned MPJPE (PA-MPJPE), Per Vertex Error (PVE), and Acceleration error (Accel). For world-coordinate metrics, we divide the global sequences into shorter segments of 100 frames aligning each segment with GT like GVHMR [50]. We then report the World-aligned Mean Per Joint Position Error (WA-MPJPE100), the World"}, {"title": "4.3. Implementation details", "content": "It takes around 2-3 days to get our pre-trained PTM with a single NVIDIA A100 GPU. During inference, restoring normal motions (such as running and jumping) requires fewer adaptation steps (less than 500) or may not require any adaptation at all. In contrast, restoring high-difficulty motions (such as continuous rolls and aerial maneuvers) necessitates between 2,000 and 4,000 steps, depending on the complexity of the motion and the quality of the reference action. We adopt the motion diffusion model GMD with UNet architecture as our in-between baseline [5, 14]. For more implementation details, please refer to the Appendix."}, {"title": "4.4. Comparison with the State-of-the-Art", "content": "We selected two state-of-the-art (SOTA) video motion capture methods, TRAM [67] and GVHMR [50], and SOTA physical informed method PhysPT [76] for comparison. The comparison results are presented in Table 1.\nFor world coordinate metrics, our method outperforms the original motions in most cases. This improvement stems from the direct relationship between the world coordinate system and physical space. Particularly in the EMDB dataset, where prolonged displacement leads to the accumulation of errors in local perspectives over time and space, our method effectively mitigates these issues, resulting in improvements in world coordinate metrics. Previous simulation methods typically emphasized perfect tracking of reference motions, focusing more on imitation rather than restoration. Consequently, when there is a significant deviation in the root position along the gravity axis, these methods often fail due to the influence of gravity, especially when attempting to imitate complex or flawed motions. In contrast, our approach prioritizes repair, utilizing a unique PTM that enables stable tracking of high-difficulty motions.\nFor 3D motion restoration in the camera coordinate, our method shows minor discrepancies from the original motions in both the Kungfu and AIST++ datasets. This can be attributed to two main reasons: 1) Our method uses noisy motions as input without knowledge of the GT motions, which inherently disadvantages us. 2) Our repair approach models directly in physical space without considering camera parameters, making it challenging to achieve perfect restoration from specific camera viewpoints. Despite these challenges, our results on the EMDB dataset exhibit slight improvements compared to the original motions. This is because our method accounts for the physical space and offers a better understanding of long-term global trajectory movements, whereas noisy motions accumulate errors due to changes in time and spatial position.\nRegarding 2D similarity, the repaired motions in the Kungfu dataset show slight improvements over the original motions. This is due to current video motion capture methods being prone to brief flaw motions when dealing with complex motions, which become more pronounced when projected onto 2D images. Our MCM replaces flaw motion based on the human masks and motion context, enhancing the 2D restoration of the repaired motions.\nIn terms of physical authenticity metrics, our method successfully restored the physical realism of motions, resulting in significant improvements in ground penetration, foot sliding, and floating. Our method keeps the ground penetration for all datasets below 0.5; notably, for the EMDB dataset, we reduced the ground penetration from as high as 82 to 0.24. This is attributed to the long-term global trajectory changes, where the errors in the gravity axis accumulate along movements, leading to severe ground penetration and floating. For all three datasets, our method reduces the self-penetration by over 50%. In the Kungfu dataset, we successfully lowered the self-penetration of GVHMR from 0.079 to 0.018. Furthermore, foot sliding also shows consistent improvement for all datasets, largely due to the presence of friction in the physical environment. While motion simulation offers inherent advantages, our differentiation from other physics-based simulation methods lies in the ability to track in-the-wild and complex motions."}, {"title": "4.5. Qualitative Evaluation", "content": "In Figure 3, we select high-difficulty in-the-wild motions for visualization and illustrate a comparison against SOTA techniques. GVHMR captures human motions from video and acts as a noise motion generator for PhysPT, PHC+ [34], and our methods. GVHMR successfully captures human motion from a monocular camera, yet the resulting motion exhibits significant physical issues such as floating and penetration. Additionally, it manifests short-term flawed motion when faced with more complex movements. Due to simplified physical rules and the unawareness of the high-difficulty motion distribution, PhysPT faces challenges in both physical repair and preservation of the original motion when dealing with complex motions. Moreover, it is inef-"}, {"title": "4.6. Ablation Studies", "content": "Physical Transfer Ability. We evaluate the simulation capabilities of our proposed PTM using the Kungfu dataset, comparing them against SOTA motion imitation methods, the results are presented in Table 3. Compared to outstanding motion imitation methods UHC and PHC+, our PTM demonstrates commendable performance in tracking martial arts, achieving a 98% success rate and considerable enhancements across other metrics. These results affirm the superior capability of our methods in motion imitation, particularly in tracking high-difficulty motions. The improvement in metrics is attributed to the relative design tailored for complex motions and the ability of TTA to mitigate the forgetting phenomenon. Previous studies have indicated that motion imitation can suffer from rapid loss of earlier knowledge when attempting to imitate a wide array of motions [33]. In our PTM, the controller's memory serves as a cornerstone for learning new actions, allowing the optimization of specific data samples without retaining this memory. We aim for the model to proactively explore solutions rather than merely reproducing answers it has memorized.\nEffect of RL-TTA Strategy. In Table 4, we conduct an ablation study on various components of the RL-TTA strategy, with experiments carried out on a high-difficulty in-the-wild dataset to demonstrate its effectiveness in handling challenging movements. While the pre-trained controller can track daily motions well, it struggles when facing difficult movements. The reason why success rates increase with early termination is that traditional early termination strategies impose overly strict requirements on humanoids, making it easy to fail when facing poor-quality motion, greatly hindering the learning process. Since high-difficulty motion often involves airborne motions that rely on external apparatuses such as trampolines, the absence of residual forces prevents humanoids from executing these movements. The TTA contributes the most enhancement by simplifying the humanoid's objectives to a specific action.\nEffect of Mask-conditioned Motion Correction. As shown in Table 5, we conduct an ablation study on our MCM, which corrects flawed motions in reference motion. The experiments are performed on a high-difficulty in-the-wild dataset. Experimental results show that our MCM can enhance the simulation success rate and improve 2D similarity by replacing flaw motions that result in simulation failure. Regarding modality selection, we find that human masks outperform 2D keypoints, as masks contain more comprehensive shape and motion information. When facing complex movements, keypoint detection algorithms may misidentify or overlook some joints, while segmentation algorithms exhibit greater stability and only require distinguishing between human foreground and background."}, {"title": "5. Conclusion", "content": "This paper introduces a plug-and-play motion restoration method to enhance the physical quality of in-the-wild high-difficulty motions. Our method integrates easily with any video motion capture method, greatly improving the efficiency of obtaining high-quality 3D motions. The MCM accurately corrected the flawed motion in video motion"}]}