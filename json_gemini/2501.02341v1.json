{"title": "UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude Mobility", "authors": ["Yonglin Tian", "Fei Lin", "Yiduo Li", "Tengchao Zhang", "Qiyao Zhang", "Xuan Fu", "Jun Huang", "Xingyuan Dai", "Yutong Wang", "Chunwei Tian", "Bai Li", "Yisheng Lv", "Levente Kov\u00e1cs", "Fei-Yue Wang"], "abstract": "Low-altitude mobility, exemplified by unmanned aerial vehicles (UAVs), has introduced transformative advancements across various domains, like transportation, logistics, and agriculture. Leveraging flexible perspectives and rapid maneuverability, UAVs extend traditional systems' perception and action capabilities, garnering widespread attention from academia and industry. However, current UAV operations primarily depend on human control, with only limited autonomy in simple scenarios, and lack the intelligence and adaptability needed for more complex environments and tasks. The emergence of large language models (LLMs) demonstrates remarkable problem-solving and generalization capabilities, offering a promising pathway for advancing UAV intelligence. This paper explores the integration of LLMs and UAVs, beginning with an overview of UAV systems' fundamental components and functionalities, followed by an overview of the state-of-the-art in LLM technology. Subsequently, it systematically highlights the multimodal data resources available for UAVs, which provide critical support for training and evaluation. Furthermore, it categorizes and analyzes key tasks and application scenarios where UAVs and LLMs converge. Finally, a reference roadmap towards agentic UAVs is proposed, aiming to enable UAVs to achieve agentic intelligence through autonomous perception, memory, reasoning, and tool utilization. Related resources are available at https://github.com/Hub-Tian/UAVs_Meet_LLMs.", "sections": [{"title": "1. Introduction", "content": "The rapid development of UAVs has introduced transformative solutions for monitoring and transportation across various sectors, including intelligent transportation, logistics, agriculture, and industrial inspection. With their flexible spatial mobility, UAVs significantly enhance the perception and decision-making capabilities of intelligent systems, offering a robust approach for upgrading traditional systems and improving operational efficiency. Given these advantages, UAV technology has attracted substantial attention from both academic researchers and industry practitioners.\nDespite their promise, the majority of UAVs currently depend on human operators for flight control. This dependency not only incurs high labor costs but also introduces safety risks, as operators are limited by the range and sensitivity of onboard sensors when assessing environmental conditions. Such limitations impede the scalability and broader application of UAVs in complex environments. Furthermore, UAV flight control is inherently challenging due to the high degrees of freedom in movement and the need for precise navigation, obstacle avoidance, and real-time environmental perception, all of which complicate the path toward fully autonomous flight.\nRecent advancements in artificial intelligence, particularly in foundation models (FMs) such as ChatGPT, SORA, and various AI-generated content (AIGC) frameworks, have catalyzed significant transformations across industries. Large language models (LLMs) are endowed with near-human levels of commonsense reasoning and generalization capabilities, enabling advanced understanding, flexible adaptation, and real-time responsiveness in diverse applications. The integration of LLMs with UAV systems offers a promising avenue to enhance autonomy, providing UAVs with advanced reasoning capabilities and enabling more effective responses to dynamic environments.\nInitial studies have explored integrating LLMs with UAVs in areas such as navigation [1, 2], perception[3, 4], planning [5, 6]. These early efforts highlight the potential of combining LLMs with UAV systems to foster more sophisticated autonomous behaviors. However, there remains a lack of systematic reviews on the integration of LLMs and UAVs, particularly regarding the frameworks and methodologies that support this interdisciplinary convergence. To advance the understanding of UAV and LLM integration, this paper provides a systematic review of the existing frameworks and methodologies, offering insights into the potential pathways for further advancing this interdisciplinary convergence. The main contributions of this paper are as follows."}, {"title": "2. Systematic Overview of UAV Systems", "content": "This section provides a brief overview of intelligent UAVS from the perspectives of functional modules and embodied configurations. The functional modules encompass the core components of UAV systems, including the perception module, planning module, communication module, control module, navigation module, human-drone interaction module, and payload module, highlighting their roles and contributions to UAV"}, {"title": "2.1. Functional Modules of UAVs", "content": ""}, {"title": "2.1.1. Perception Module", "content": "The Perception Module serves as the UAV's \"eyes and ears,\" collecting and interpreting data from a variety of onboard sensors to build a comprehensive understanding of the surrounding environment. These sensors include RGB cameras, event-based cameras, thermal cameras, 3D cameras, LiDAR, radar, and ultrasonic sensors [12]. By converting raw sensor data into actionable insights, such as detecting obstacles, identifying landmarks, and assessing terrain features. The Perception Module provides the situational awareness essential for safe and autonomous flight [13, 14].\nBeyond basic environmental monitoring, the Perception Module also supports collaborative tasks in multi-UAV operations, including the detection and tracking of other drones to facilitate coordinated swarm behavior. Advanced computer vision and machine learning techniques play a pivotal role in this process, enhancing the accuracy and robustness of object detection [15, 16], semantic segmentation [17, 18], and motion estimation [19, 20]. Sensor fusion methods are often employed to combine complementary data sources, such as fusing LiDAR depth maps with high-resolution camera imagery, thereby mitigating the limitations of individual sensors while capitalizing on their unique strengths [21, 22]. This robust, multimodal perception framework enables UAVs to adapt to changing conditions (e.g., varying lighting, dynamic environments) and carry out complex missions with minimal human intervention."}, {"title": "2.1.2. Navigation Module", "content": "he Navigation Module is responsible for translating the planned trajectories from the Planning Module into precise flight paths by continuously estimating and adjusting the UAV's position, orientation, and velocity [23]. To achieve this, it relies on a variety of onboard sensors [24], such as GPS [25, 26], inertial measurement units [27, 28], visual odometry, and barometric sensors or magnetometers to gather real-time information about the UAV's state [29, 30]. Sensor-fusion algorithms, including Kalman filters (e.g., Extended or Unscented Kalman Filters) and particle filters, are employed to integrate data from disparate sources, enhancing the reliability and accuracy of state estimation.\nIn GPS-denied or cluttered environments, the Navigation Module may employ simultaneous localization and mapping techniques or visual SLAM to provide robust localization and environment mapping [31, 29, 32, 33, 34]. Such advanced solutions enable the UAV to maintain a high level of situational"}, {"title": "2.1.3. Planning Module", "content": "The Planning Module is pivotal in translating high-level mission objectives into concrete flight trajectories and actions, relying on input from the Perception Module to ensure safe navigation [35, 36]. Path-planning algorithms span a broad range of techniques aimed at computing feasible and often optimized routes around obstacles. These methods include heuristic algorithms such as the A* algorithm [37], Evolutionary Algorithms [38], Simulated Annealing [39, 40], Particle Swarm Optimization [41, 42, 43], Pigeon-Inspired Optimization [44], Artificial Bee Colony [45, 46], etc. Machine learning approaches, including Neural Networks [47, 48, 49], and Deep Reinforcement Learning [50, 51]are also employed for more adaptive and data-driven planning. Additionally, sampling-based strategies like Rapidly-exploring Random Trees offer flexible frameworks for dealing with high-dimensional or dynamically changing environments [52]. By leveraging one or a combination of these methods, UAVs are able to devise safe, collision-free trajectories that optimize key performance metrics, such as travel time, energy consumption, or overall mission efficiency. [53, 54, 44, 55]. These techniques enable UAVs to operate autonomously within complex or uncertain environments by continuously adapting their planned path in real-time, particularly important when unforeseen changes occur in terrain, obstacle locations, or mission parameters.\nIn multi-UAV or swarm operations, the Planning Module also plays a key role in coordinating flight routes among individual drones, ensuring collision avoidance and maintaining cohesive group behaviors [56, 57, 58]. This collaborative planning capability not only enhances mission efficiency but also reduces the risk of inter-UAV interference. By dynamically updating trajectories and sharing relevant information, the Planning Module underpins robust, reliable operations that align with overall mission goals."}, {"title": "2.1.4. Control Module", "content": "The Control Module is responsible for generating low-level commands that regulate the UAV's actuators\u2014including motors, servos, and other control surfaces\u2014to maintain stable and responsive flight. Acting as the \"muscle\" of the system, it continuously adjusts key parameters such as altitude, velocity, orientation, and attitude in response to real-time feedback from onboard sensors. By closing the control loop with reference inputs provided by the Navigation and Planning Modules, the Control Module ensures that the UAV adheres to desired flight trajectories and mission objectives [59, 60].\nTo manage potential disturbances (e.g., wind gusts, payload variations) and compensate for modeling uncertainties, a variety of classical and modern control strategies"}, {"title": "2.1.5. Communication Module", "content": "The Communication Module underpins all data exchanges between the UAV, ground control stations, and other external entities, such as satellites, edge devices, or cloud-based services, ensuring that critical telemetry, control, and payload information flows seamlessly. Typical communication methods range from short-range radio frequency systems and Wi-Fi links to more sophisticated, longer-range networks like 4G, 5G, or even satellite-based links, each selected to meet the specific mission requirements regarding bandwidth, latency, and range [70, 71, 72, 73].\nIn UAV swarm operations, the Communication Module becomes particularly vital, it relays commands to and from ground control and facilitates inter-UAV coordination by sharing situational data (e.g., positions, sensor readings) in real time [73, 74]. Robust communication protocols often augmented with encryption and authentication mechanisms guard against unauthorized access and malicious interference, while techniques like adaptive channel selection and multi-hop ad-hoc routing can mitigate signal degradation and ensure reliable connectivity in dynamic environments [75]. By managing and prioritizing different data streams (telemetry, payload, command and control), the Communication Module serves as the backbone that keeps all subsystems in sync and supports the UAV's overall operational objectives [76]."}, {"title": "2.1.6. Interaction Module", "content": "The Interaction Module is designed to facilitate seamless communication and collaboration between the UAV and human operators or other agents in the operating environment [77, 78]. It encompasses user interfaces and interaction paradigms that may include voice commands, gesture recognition, augmented or virtual reality displays, or touchscreen-based data visualization systems [79, 80, 81, 82, 83]. Additional methods such as adaptive user interface design that tailors the displayed information to the operator's skill level and workload, or haptic feedback mechanisms that provide tactile alerts for critical events can further enhance situational awareness and user experience [84]. These interfaces enable ground personnel to issue high-level commands, review mission progress, and intervene when necessary, ensuring that operators maintain oversight and decision-making authority [85].\nIn swarm or multi-UAV contexts, the Interaction Module becomes even more integral. It not only allows central decision makers to coordinate multiple drones but also enables human operators to receive aggregated situational data from across the swarm, potentially flagging anomalies or emergent behaviors in real time. These human-UAV interaction channels are particularly critical in collaborative missions (for example, search and rescue, environmental monitoring, or infrastructure inspection), where on-the-spot guidance or feedback may be required to adapt the UAVs' behavior to evolving conditions [86, 87, 88]. By providing robust mechanisms for manual overrides and real-time communication, the Interaction Module strikes a balance between autonomous operation and human-in-the-loop supervision, enhancing both mission effectiveness and operational safety [87, 89, 90]."}, {"title": "2.1.7. Payload Module", "content": "The Payload Module oversees the equipment or cargo the UAV carries to accomplish its mission objectives. Depending on the task, these payloads may range from cameras for surveillance, to delivery packages, to advanced sensors for environmental monitoring, to specialized hardware for tasks such as search and rescue [91]. Consequently, the Payload Module must address a variety of operational needs, including power supply, secure data transmission, mechanical support, and proper stabilization to ensure reliable performance under diverse conditions [59, 92].\nIn practice, this module often integrates features such as vibration damping, thermal management, and secure mounting solutions to protect delicate components and maintain optimal functionality during flight. Moreover, in some UAV designs, the Payload Module is designed to be interchangeable. This modular approach, which typically employs standardized mounting and connectivity interfaces, enables rapid swapping of payloads and streamlines the process of configuring a UAV for different mission profiles. As a result, operators can expand the drone's capabilities without requiring an entirely new platform, thereby enhancing flexibility and reducing both deployment time and cost [93, 94, 95, 96] .\nOverall, the Payload Module plays a crucial role in bridging the UAV's core flight systems with the mission-specific tools essential for achieving operational objectives. By accommodating a wide range of payloads and ensuring they are powered, protected, and efficiently connected, the Payload Module significantly extends the UAV's applicability across various industries and mission types."}, {"title": "2.2. Embodied Configurations of UAVs", "content": "UAVs can be categorized into several types based on their geometric configurations. These include fixed-wing UAVs, multi-"}, {"title": "2.2.1. Fixed-Wing UAVs", "content": "Fixed-wing UAVs feature a predetermined wing shape that generates lift as air flows over the wings, enabling forward motion [92]. These UAVs are known for their high speed, long endurance, and stable flight, making them ideal for long-duration missions. However, they require advanced piloting skills and cannot perform hovering [97]. Fixed-wing UAVs are commonly used for monitoring fields, forests, highways, and railways [92]."}, {"title": "2.2.2. Multirotor UAVs", "content": "Multirotor UAVs are one of the most prevalent types in daily life, typically equipped with multiple rotors (commonly four, six, or more) to generate lift through rotor rotation. Their advantages include low cost, ease of operation, and the ability for vertical take-off and landing (VTOL) and hovering, making them suitable for precision tasks. However, they have limited endurance and relatively low speed. Multirotor UAVs are often used for tasks such as photography, agricultural monitoring, and spraying."}, {"title": "2.2.3. Unmanned Helicopters", "content": "Unmanned helicopters are equipped with one or two powered rotors to provide lift and enable attitude control. This design allows for vertical take-off, hovering, and high maneuverability. Compared to multirotor UAVs, unmanned helicopters have superior payload capacity, enabling them to carry heavier equipment or sensors. Their strengths include long endurance and excellent wind resistance, making them stable even in strong winds. The main limitation is their relatively low speed. Unmanned helicopters find widespread applications in areas such as traffic surveillance, resource exploration, forest fire prevention, and military reconnaissance."}, {"title": "2.2.4. Hybrid UAV", "content": "Hybrid UAVs combine the strengths of both fixed-wing and multirotor UAVs, offering a versatile design that allows for VTOL while also achieving the long endurance and high speed typical of fixed-wing UAVs. These UAVs typically feature a combination of rotors for lift during vertical flight and wings for sustained forward motion. The main advantage of hybrid UAVs is their flexibility, enabling them to perform a wide range of missions, including those requiring both hovering and long-duration flight. However, the complexity of their design and mechanisms results in higher costs and more demanding maintenance."}, {"title": "2.2.5. Flapping-Wing UAV", "content": "Flapping-wing UAVs are bio-inspired unmanned aerial vehicles that mimic the flight mechanisms of birds or insects. These UAVs rely on unsteady aerodynamic effects generated by wing flapping to achieve flight. They offer quieter operation, higher efficiency, and increased maneuverability compared to conventional UAVs. Their compact size is a notable advantage, but they generally have a lower payload capacity. Additionally, the design and control systems of flapping-wing UAVs are more complex due to the dynamic nature of their flight mechanics."}, {"title": "2.2.6. Unmanned Airship", "content": "Unmanned airships are a type of aerial vehicle that utilizes lightweight gases for buoyancy and employs propulsion and external structural elements for movement and directional control. These airships are highly cost-effective and produce low flight noise. However, their agility is limited, and they operate at relatively low speeds. Due to their large size, unmanned airships are highly susceptible to wind influences, which can affect their stability and operational reliability."}, {"title": "2.3. UAV Swarm", "content": "UAV swarms involve multiple UAVs working collaboratively to achieve a shared objective, offering advantages in redundancy, scalability, and efficiency compared to individual UAV operations[98]. The swarm approach relies on decentralized decision-making, allowing UAVs to adjust their behaviors in response to the actions of their peers and environmental changes. Swarm algorithms often draw inspiration from biological systems[99], such as flocks of birds or colonies of ants, utilizing techniques like consensus algorithms[100], particle swarm optimization[101], or behavior-based coordination[102]. Effective swarm operation requires seamless communication, robust control mechanisms, and cooperative planning to manage the complexities of distributed systems[103]. This concept is particularly useful in applications like large-area surveillance, precision agriculture, and search and rescue, where multiple UAVS can cover a greater area more efficiently than a single vehicle.\nIn this section, we will discuss key components essential for effective UAV swarm operation, including task allocation, communication architecture, path planning, and formation control."}, {"title": "2.3.1. Task allocation in UAV swarm", "content": "UAV swarms can be regarded as a team tasked with executing a given mission, consisting of a set of tasks, and they must be responsible for task allocation among their members internally [104]. Task allocation is one of the most important problems in UAV swarm operations, as it directly affects the efficiency of the mission. It has been proven that finding the optimal solution to the task allocation problem is NP-hard, and the difficulty increases exponentially with the scale of the UAV swarm and the number of tasks[105]. Typically, the UAV swarm task allocation problem is modeled as a Traveling Salesman Problem (TSP)[106] or Vehicle Routing Problem (VRP)[107], mixed-integer linear programming (MILP) model[108], or a cooperative multi-task allocation problem (CMTAP) model[109]. Commonly used algorithms include heuristic algorithms, AI-based methods, mathematical programming methods, and market mechanism-based algorithm.\nHeuristic algorithms commonly applied to the task allocation problem include Genetic Algorithms (GA), Particle Swarm Optimization (PSO), and Simulated Annealing (SA), among others. These algorithms use random search methods to find feasible solutions, preventing the problem from getting trapped in local optima. Genetic algorithms generate random solutions through crossover and mutation strategies, and then iteratively improve these solutions by selecting the best candidates. Eventually, they converge to a solution close to the optimal. For example, Han et al. [110] proposed a Fuzzy Elite Strategy Genetic Algorithm (FESGA) to efficiently solve complex task allocation problems. Yan et al. [111] proposed an improved genetic algorithm to address the integrated task allocation and path planning problem for multi-UAVs attacking multiple targets.\nPSO balances exploration and exploitation processes and iteratively produces solutions that approach the optimal. The algorithm is simple to implement, fast, and widely applied in task allocation problems. Jiang et al.[112] proposed an improved PSO algorithm to solve multi-constraint task allocation problems, which is suitable for solving complex combinatorial optimization problems. Since most research treats multi-UAV task allocation as a single-objective optimization problem, Gao et al. [113] applied an improved Multi-Objective PSO (MOPSO) algorithm to solve the task allocation problem for multiple UAVs."}, {"title": "2.3.2. Communication architecture in UAV swarm", "content": "For UAV swarms, communication is essential for coordination, enabling collaborative work and maintaining stability during operations. Communication can be achieved through two main approaches: infrastructure-based architectures [123] and Flying Ad-hoc Network (FANET) architectures [124]. Each method offers unique advantages and challenges, which will be discussed below.\nInfrastructure-based Architectures: This architecture depends on ground control stations (GCS) [70] to manage the swarm. The GCS collects telemetry data from UAVs and transmits commands, either in real-time or through pre-programmed instructions. Its key advantages include centralized computation and real-time optimization, eliminating the need for inter-drone communication networks [123]. However, this approach has notable limitations: the entire system is vulnerable to single-point failures in the GCS, UAVs must remain within the GCS communication range, and the architecture lacks the flexibility of distributed decision-making [123].\nFlying Ad-hoc Network (FANET) Architecture: FANETS consist of UAVs communicating directly with one another without needing a central access point. This decentralized network enables UAVs to coordinate tasks autonomously, with at least one UAV maintaining a link to a ground base or satellite. FANETs benefit from flexibility, scalability, and reduced dependency on infrastructure. However, they require robust communication protocols and may face challenges in managing dynamic topologies and ensuring reliability [124]."}, {"title": "2.3.3. Path planning in UAV swarm", "content": "UAV swarm path planning refers to selecting an optimal path for the UAV swarm from the starting position to all target positions, while ensuring the predefined distance between UAVs to avoid collisions[125]. The optimal path generally refers to the shortest path length, shortest travel time, least energy consumption, and other event-specific constraints [125]. The criteria for the optimal path need to be determined based on the actual problem. UAV path planning algorithms can generally be divided into three major categories: intelligent optimization algorithms, mathematical programming methods, and AI-based approaches. Below, we briefly introduce these three methods.\nIn nature, various group behaviors, such as flocks of birds, schools of fish, and ant colonies, follow specific rules that enable efficient food searching or migration. These behaviors can be abstracted into mathematical models for information transfer, path planning, and coordinated control, which are applicable to UAV swarm scheduling. Common intelligent optimization algorithms for UAV swarms include Ant Colony Optimization (ACO), Genetic Algorithms (GA), Simulated Annealing (SA), and Particle Swarm Optimization (PSO). For instance, ACO mimics the foraging behavior of ants, where ants probabilistically select paths based on pheromone concentration, ultimately finding optimal or near-optimal solutions. Researchers such as Turker et al.[126] have applied SA to UAV swarm path planning, while Wei et al. [127] used ACO for the same purpose. Beyond heuristic algorithms inspired by nature, mathematical models like Mixed-Integer Linear Programming (MILP)"}, {"title": "2.3.4. UAV Swarm Formation Control Algorithm", "content": "The UAV swarm relies on effective formation control algorithms that enable it to autonomously form and maintain a formation to perform tasks, and switch or rebuild the formation based on specific tasks[132]. The primary approaches to formation control are centralized, decentralized, and distributed control algorithms [133].\nCentralized Control: Centralized control involves a central unit that oversees task allocation and resource management, with individual UAVs primarily responsible for data input, output, and storage [132]. This approach simplifies decision-making, ensures coordinated actions, and is relatively straightforward to implement. However, it is susceptible to high communication overhead and single points of failure; if the central unit fails, the entire system may collapse. Common methods in centralized control include virtual structure [134] and leader-follower approaches [135, 136].\nDecentralized Control: In decentralized control, each UAV makes decisions based on local sensors and controllers, without requiring explicit communication with other UAVs [137]. UAVs adjust their movements to maintain formation based on local conditions and predefined rules. The primary advantages of this approach include flexibility and ease of adapting formations. However, the lack of access to global information results in poor control performance, requiring continuous iteration[138].\nDistributed Control: Distributed control involves extensive communication between UAVs, enabling them to coordinate and maintain formation through shared information. UAVs work collaboratively to make optimal decisions based on both local data and pre-established rules. Compared to decentralized control, distributed control benefits from more robust collaboration and improved flexibility. However, it requires higher communication demands and more complex algorithms to manage coordination, which increases both the computational burden and the risk of communication failures. Typical methods include behavior method[139] and consistency method [140]."}, {"title": "3. Preliminaries on Foundation models", "content": "This section provides an overview of FMs, including LLMs, Vision Foundation Models (VFMs), and Vision Language Models (VLMs). It highlights their core characteristics and technical advantages, with the aim of offering foundational insights and guidance for the deep integration of these models with UAV systems."}, {"title": "3.1. LLMs", "content": "In recent years, LLMs have seen rapid advancements, with increasingly larger models being trained on diverse, large-scale corpora [217]. These models have consistently set new performance benchmarks in various NLP tasks and have been widely adopted in both academic research and industrial applications [218, 219, 220, 221]. This section provides an overview of the core capabilities of LLMs, including their generalization and reasoning abilities, followed by an introduction to typical LLMs from leading research organizations."}, {"title": "3.1.1. Core Capabilities of LLMs", "content": "Generalization Capability: Benefiting from training on large-scale corpora and the substantial size of the models, LLMs exhibit strong transfer capabilities, including zero-shot and few-shot learning. These capabilities enable LLMs to generalize effectively to new tasks, either without task-specific examples or with limited guidance, making them versatile tools for a wide range of applications. In zero-shot learning, without additional task-specific training, LLMs can solve relevant problems solely through natural language instructions. In few-shot learning, the model can quickly adapt to new tasks by leveraging several examples from the support set along with the corresponding task instructions [222].\nThe design of natural language instructions or prompts is crucial in enhancing generalization capability. Prompts not only provide natural language descriptions of tasks but also guide the model to perform tasks accurately based on input examples [223, 141, 143]. Furthermore, LLMs exhibit in-context learning, which allows them to learn and adapt to new tasks directly from the context provided within the prompt, such as task instructions and examples, without requiring any explicit retraining or model updates [141, 224, 225].\nComplex Problem-Solving Capability: LLMs demonstrate a remarkable ability to solve complex problems by generating intermediate reasoning steps or structured logical pathways, facilitating a systematic and step-by-step approach to addressing challenges. This capability is exemplified by the Chain of Thought (CoT) framework, where intricate problems are decomposed into a series of manageable sub-tasks, each solved sequentially using examples of step-by-step reasoning [226, 227, 228, 229]. Besides, LLMs also demonstrate advanced capabilities in task planning and the orchestration of tools, enabling them to invoke appropriate resources to address specific sub-task requirements and efficiently integrate workflows to achieve comprehensive solutions [230, 231, 232]."}, {"title": "3.1.2. Typical LLMs", "content": "Several notable milestones have marked the development of LLMs. OpenAI's GPT series, spanning GPT-3, GPT-3.5, and GPT-4, has set benchmarks in language understanding, generation, and reasoning tasks by leveraging extensive parameters and optimized architectures [141, 142, 143]. Anthropic's Claude models, including Claude 2 and Claude 3, prioritize safety and controllability through reinforcement learning, excelling in multi-task generalization and robustness [144, 145, 146]. The Mistral series employs sparse activation techniques to balance efficiency with performance, emphasizing low-latency inference [147, 148].\nGoogle's PaLM series [149, 150] stands out for its multimodal capabilities and large-scale parameterization, while the subsequent Gemini series extends these features to improve generalizability and multilingual support [151, 152]. In the open-source ecosystem, Meta's Llama models, including Llama 2 and Llama 3, excel in multilingual tasks and complex problem-solving. Derivative models like Vicuna enhance conversational abilities and task adaptability through fine-tuning on conversational datasets and techniques like Low-Rank Adaptation (LoRA) [153, 154, 155, 156]. Similarly, the Qwen series, pre-trained on multilingual datasets and instruction-tuned, demonstrates adaptability across diverse tasks [157].\nSeveral other models have achieved significant progress in specialized domains. InternLM [159], BuboGPT [160], ChatGLM [161, 162, 163], and DeepSeek [164, 165, 166] focus on domain-specific tasks such as knowledge-based Q&A, conversational generation, and information retrieval, enabled by task-specific fine-tuning and targeted extensions. Notably, LiveBench [233] has emerged as a comprehensive benchmarking platform, addressing limitations of previous evaluation standards. It systematically assesses LLMs' real-world capabilities across multi-task scenarios, offering valuable insights for model development and application."}, {"title": "3.2. VLMs", "content": "VLMs are multimodal models that extend the capabilities of LLMs by integrating visual and textual information[234]. These models are designed to tackle a range of tasks that require both vision and language understanding, such as visual question answering (VQA) and image captioning [235, 236, 237, 238, 239]. This section introduces several typical VLM models highlighting their technical features and application scenarios. OpenAI's GPT-4V [167] is a prominent representative in VLMs, demonstrating powerful visual perception capabilities [240]. The upgraded GPT-40 introduces more advanced optimization algorithms, allowing it to accept arbitrary combinations of text, audio, and image inputs while delivering rapid responses. The lightweight version, GPT-40 mini, is designed for mobile devices and edge computing scenarios, balancing efficient performance with deployability by reducing computational resource consumption [241]. GPT ol-preview excels in reasoning, particularly in programming and solving complex problems [242]. Anthropic's Claude 3 Opus exhibits robust multi-task generalization and controllability, while Claude 3.5"}, {"title": "3.3. VFMs", "content": "In recent years, the concept of VFMs has emerged as a core technology in computer vision. The primary goal of VFMs is to extract diverse and highly expressive image features, making them directly applicable to various downstream tasks. These models are typically characterized by large-scale parameters, remarkable generalization capabilities, and outstanding cross-task transfer performance, albeit with relatively high training costs [194]. CLIP is a pioneering representative in the field of VFMs. By employing weakly supervised training on large-scale image-text pairs, it efficiently aligns visual and textual embeddings, laying a solid foundation for multimodal learning"}, {"title": "3.3.1. VFM for Object Detection", "content": "The core advantage of VFMs in object detection lies in their powerful zero-shot detection capabilities. GLIP [190] unifies object detection and phrase grounding tasks, demonstrating exceptional zero-shot and few-shot transfer capabilities across various object-level recognition tasks. Zhang et al. [191] proposed DINO, which optimized the architecture of the DETR model [245], significantly enhancing detection performance and efficiency. Subsequent work, Grounding-DINO [192], introduced text supervision to improve accuracy. Additionally, DINOv2 [193] adopted a discriminative self-supervised learning approach, enabling the extraction of robust image features and achieving excellent performance in downstream tasks without fine-tuning. AM-RADIO [194] integrated the capabilities of VFMs such as CLIP [186], DINOv2 [193], and SAM [198] through a multi-teacher distillation method, resulting in strong representational power to support complex visual tasks. DINO-WM [195] incorporated DINOv2 into world models, enabling zero-shot planning capabilities. Additionally, YOLO-World [196] enhances the generalization capability of YOLO detectors through an efficient pretraining scheme, achieving outstanding performance in open vocabulary and zero-shot detection tasks."}, {"title": "3.3.2. VFM for Image Segmentation", "content": "VFMs have demonstrated significant improvements over traditional methods in image segmentation tasks. L\u00fcdecke et al. [197] proposed CLIPSeg, based on the CLIP model, which supports semantic segmentation, instance segmentation, and zero-shot segmentation. Kirillov et al. [198] developed the Segment Anything Model (SAM), achieving zero-shot segmentation capabilities across diverse scenarios through pretraining on large-scale and diverse datasets. Subsequent research further extended SAM's applications, such as Embodied-SAM [199] and Point-SAM [200], which expanded SAM's functionality to 3D scenes. Open-Vocabulary SAM [201] combined SAM with CLIP's knowledge transfer strategies, effectively optimizing segmentation and recognition tasks simultaneously. Pan et al. [202] proposed TAP (Tokenize Anything), a foundational model centered on visual perception, which improves the SAM architecture by introducing visual prompts to enable simultaneous completion of segmentation, recognition, and description tasks for arbitrary regions. EfficientSAM [203] and MobileSAM [204] optimize SAM's representation, significantly reducing model complexity and achieving lightweight designs while maintaining excellent task performance. Recently, SAM 2 [205] introduced memory modules to the original model, enabling real-time segmentation for videos of arbitrary length while addressing complex challenges like occlusion and multi-object tracking. SAMURAI [206] builds upon SAM 2 by integrating a Kalman filter, addressing the limitations of memory management in SAM 2, and achieving superior video segmentation performance without requiring retraining or fine-tuning.\nBeyond the SAM series, other VFM architectures have also significantly advanced image segmentation. Models such as SegGPT [207], Osprey [208], and SEEM [209] have demonstrated notable adaptability in arbitrary segmentation tasks and multimodal scenarios. Additionally, VFMs have shown important applications in other segmentation tasks. For example, Liu et al. [210] proposed the Seal framework for segmenting point cloud sequences, while the LISA [211] adopted the Embedding-as-Mask approach to endow multimodal large models with reasoning-based segmentation capabilities. LISA can process complex natural language instructions and generate fine-grained segmentation results, expanding the scope and complexity of segmentation model applications."}, {"title": "3.3.3. VFM for Monocular Depth Estimation", "content": "In the field of monocular depth estimation", "212": "achieves zero-shot depth estimation by combining relative and absolute depth estimation methods. ScaleDepth [213", "modules": "scene scale prediction and relative depth estimation, achieving advanced performance in indoor, outdoor, unconstrained, and unseen scenarios. Additionally, Depth Anything [214"}]}