{"title": "MobileExperts: A Dynamic Tool-Enabled Agent Team in Mobile Devices", "authors": ["Jiayi Zhang", "Chuang Zhao", "Yihan Zhao", "Zhaoyang Yu", "Ming He", "Jianping Fan"], "abstract": "The attainment of autonomous operations in mobile computing devices has con-\nsistently been a goal of human pursuit. With the development of Large Language\nModels (LLMs) and Visual Language Models (VLMs), this aspiration is progres-\nsively turning into reality. While contemporary research has explored automation\nof simple tasks on mobile devices via VLMs, there remains significant room for\nimprovement in handling complex tasks and reducing high reasoning costs. In this\npaper, we introduce MobileExperts, which for the first time introduces tool formula-\ntion and multi-agent collaboration to address the aforementioned challenges. More\nspecifically, MobileExperts dynamically assembles teams based on the alignment\nof agent portraits with the human requirements. Following this, each agent embarks\non an independent exploration phase, formulating its tools to evolve into an expert.\nLastly, we develop a dual-layer planning mechanism to establish coordinate collab-\noration among experts. To validate our effectiveness, we design a new benchmark\nof hierarchical intelligence levels, offering insights into algorithm's capability to\naddress tasks across a spectrum of complexity. Experimental results demonstrate\nthat MobileExperts performs better on all intelligence levels and achieves ~22%\nreduction in reasoning costs, thus verifying the superiority of our design.", "sections": [{"title": "1 Introduction", "content": "The human pursuit of creating autonomous device operation agents to perform tasks independently\ncontinues, intensified by the widespread adoption of personal electronic devices[33, 22, 28, 32, 34, 20].\nPeople leverage these intelligent assistants to help themselves, not only completing mundane tasks on\na daily basis, but also surpassing their own capabilities to perform higher-level operations. In previous\nresearch, achieving autonomous planning and execution abilities in agents has been challenging due\nto limitations in intelligence levels. However, the advent and evolution of large language models\n(LLMs) bring forward the potential for its realization. A significant number of studies concentrate on\nLLM-based agents [23, 4]and demonstrate remarkable competency in varied domains.\nRecent researchers attempt to implement this concept using LLMs or VLMs. With the integration of\na LLM/VLM core, it's possible to configure the agent's action space to interact with a device enabling\nthe agent to operate and control the specific device autonomously. We refer to this type of work as\n\"Device Operation Agent (DOA)\". Among all possible device types, mobile devices, due to their\nclose ties with human daily life, are the platform most urgently needing DOAs. Currently, research\non mobile device DOA has become a popular field of focus. For example, research work such as\n[26, 33, 22] demonstrate the application of LLM and VLM to accomplish automated operational\ntasks."}, {"title": "2 Related Work", "content": "Due to the extraordinary performance demonstrated by LLMs under the Zero Shot paradigm, research\non LLM-based agents become a hot topic. Recent research on LLM-based agents, covering various\nresoning paradigms[25, 30, 29, 3] and structures [14, 17, 18], has significantly improved single-agent\ncapabilities. This has led to important advances across multiple fields [6, 19, 21]. Inspired by the\nconcept of swarm intelligence, the LLM-based MultiAgent System has become a hot subtopic in this\nresearch field. By integrating agents with different skills and characteristics into one system, this\nsystem can achieve more complex and advanced objectives than single agents. For example, in the\nfields of software engineering [7, 16], social simulation [15, 24], and building general problem-solving\nframeworks [10, 27, 9], multi-agent systems have demonstrated their ability to solve higher-level and\nbroader range of problems.\nWith the development of VLM, the Agent has gradually shifted from a single text module to multi-\nmodal interaction, and has derived fields that can interact with the real world, such as the Device\nOperation Agent(DOA)[22, 5, 33], robot control [11]. However, in the field where VLM is used as\nthe core of the Agent, research on using swarm intelligences to accomplish more complex tasks is\nstill relatively less. We hope to supplement research in this field through MobileExperts, promoting\nthe development of Agents in multi-modal multi-agent systems."}, {"title": "2.1 LLM Based MultiAgent System", "content": "However, based on the intelligence levels defined by [12], even the best-performing DOA is currently\nonly at the level of \"Simple Step Following\" and \"Deterministic Task Automation\". Additionally,\ncurrent UI-based DOAs are facing considerable challenges regarding reasoning efficiency and time\ncost. A typical UI operation may require 5 to 10 steps, while each step of an existing UI-based\nagent requires the invocation of the VLM model to choose the action, undoubtedly adding significant\nreasoning and time costs.\nTo solve these problems, we introduce MobileExperts, a multimodal multi-agent framework for\nmobile devices on android, designed to enhance DOA intelligence and reduce reasoning costs and\ntime. It features two key features: (1) Code-Combined Procedure Memory (Tool) Formulation: To\nminimize reliance on the VLM model, we propose a new form of tool pattern: with the code writing\nability of LLM, MobileExperts combines the basic actions of experts through code combination to\nform reusable code block tools. These tools are stored in the procedure memory shared by all experts\nin the system, thereby reducing the cost of tool formation and effectively improving operational\nefficiency. (2) Expert Collaboration via Double-layer Planning: MobileExperts adopts double layer\nplanning method to solve long-term planning problems: the first layer is the team task distributed layer,\nat this level, tasks are decomposed into dependent subtasks for experts to execute, these dependencies\nform a collaborative network between experts; the second layer is expert task decomposition layer,\nthe subtasks received by the experts are further decomposed into smaller action units to gradually\nachieve the goal. By these methods, MobileExperts not only improves the intelligence level but also\nreduces the cost of reasoning and time, providing a more efficient task execution mode for the DOA\nfield.\nIn addition, to effectively measure the capability of the MobileExperts framework, we proposed\nExpert-Eval to measure the performance of DOA at different intelligence levels. We compared\nMobileExperts with multiple baselines on this benchmark and conducted ablation experiments on the\nmodule to verify the effectiveness of the framework. We summarize the contributions of this paper as\nbelow:\n\u2022 MobileExperts is an autonomous multimodal agent framework that excels in handling complex\ndevice operation tasks, which enhanced by dynamic agent collaboration mechanism.\n\u2022 We propose an automatic code based tool formation method based on VLM, utilizing action\ntrajectories accumulated within the interaction with environment, which fills the gap in tool\nformation for the mobile device operation field.\n\u2022 We design a tiered intelligence evaluation benchmark, Expert Eval, to assess the performance of\nmobile device operation agents across tasks of varying complexities."}, {"title": "2.2 Device Operation Agent", "content": "Device Operation Agent (DOA) is an agent system that enables automation of human computing\ndevices by configuring their action spaces, using either LLM or VLM. Currently, DOA mainly applies\nto three types of devices: web browsers, computers, and mobile devices. The present research trend\nfocuses on developing User Interface (UI)-based agents that can automate user tasks on these devices.\nThe progression of this type of research can be summarized via two primary directions.\nFirst, UI-based Agents [12] simulate basic user operations thereby transforming user tasks into a\nseries of interactions with the device's UI. In this process, the core model of DOA is undergoing\ntransformation.Earlier studies utilized the LLM to describe the sequence of actions for tasks by\nstructuring device UI elements into textual data, such as HTML and XML formats. This LLM-\ncentric approach was reflected across web, computer and mobile devices[26, 35]. However, with the\ndevelopment of VLM, end-to-end methods show greater potential as the core model of DOA and\nachieved superior performance in multiple platforms[28, 32, 5, 20, 22, 33]. But an increase in agents\nthat apply VLM has introduced new challenges for DOA's development: accurately interpreting\nthe meaning behind each UI element's operation becomes significantly difficult for the universal\nmultimodal models. This leads to the second direction in the development of DOA enhancing\nthe model's understanding of the UI. For instance, research like [1, 8, 13, 31, 2] carried out in-depth\ntraining on the UI elements of Android, Apple as well as PC operating systems, achieving comparable\nperformance with large-scale parameter models with a smaller scale parameter volume.\nAs previously mentioned, UI-based Agents need to frequently call the model when executing opera-\ntions, leading to costs in reasoning and time. For devices based on web browsers or computers, this\nproblem can be solved by generating executable scripts with LLM to form tools[32, 28]. However,\ndue to the limitations of mobile device operating systems, this tool formation strategy is not applicable\non mobile devices, impacting the operation efficiency of DOA on mobile devices.\nIn response to this limitation, we propose the MobileExperts, aiming to develop an innovative tool\nformation strategy for mobile devices, effectively solving the above problems, reducing reasoning\nand time costs in a mobile environment."}, {"title": "3 Methodology", "content": "In this section, we initially present an overview of MobileExperts, our proposed multi-agent collabo-\nrative framework for solving complex tasks in the field of mobile device operation agent, as shown in\nFigure 1."}, {"title": "3.1 Expert Development", "content": "Ensuring that mobile device operating agents have high efficiency, strong generalization, and effective\nerror correction across various devices is crucial for their successful integration into real-world\napplications. However, due to inherent differences among mobile devices, an agent not specifically\ntrained for each device may struggle to meet these criteria. To address this, we've introduced an\nexploration stage designed to equip the agent with unique tools and knowledge, enabling it to become\nproficient on a specific mobile device. The exploration process of MobileExperts is illustrated in the\nfigure 2."}, {"title": "3.1.1 Exploration Process", "content": "Expert exploration is a pivotal phase in the mobileexperts. During this phase, each Expert \\( \\mathcal{E} \\) leverages\ntheir individual portraits to formulate multiple explore subtasks \\( \\mathcal{S} \\) aligned with the current requirement\n\\( \\mathcal{R} \\) as Equation 1\n\\[\\mathcal{S} \\triangleq \\{r_{1}, r_{2}, \\dots, r_{n}\\} = \\text{Decompose}(\\mathcal{E}, \\mathcal{R}),\\tag{1}\\]\nAs the Expert engages in individual exploration phases, they tailor the generation of a tool set \\( \\mathcal{T} \\) and\nmemories unique to user's device, including device-specific interface memories \\( \\mathcal{M}_{\\text{interface}} \\) and insight\nmemories \\( \\mathcal{M}_{\\text{insight}} \\) as shown in Equation 2. This process enables the Expert to become a domain\nspecialist on the user's device, effectively addressing device-specific challenges.\n\\[\\mathcal{T}_{i}, \\mathcal{M}_{\\text{interface}}, \\mathcal{M}_{\\text{insight}} = \\mathcal{E}(r_{i})\\tag{2}\\]"}, {"title": "3.1.2 Code Combination Based Tool Formulation", "content": "In the mobile device environment, the mobile device operation agent faces complex challenges of\nlearning different application usage methods. Current studies primarily concentrate on enhancing the\nagent's recognition of icons to tackle the challenge, neglecting the method of forming tools through\nexploration. The absence of such tools forces these agents to rely on a Visual Language Model\n(VLM) for each action, which has led to significant room for improvement in the reasoning costs of\nthe mobile device operation agent. To surmount this challenge, we introduce an innovative approach,\nCode Combination Based Action Extension. Specifically, we have defined a basic operational space\nfor experts, drawing upon human behaviors. We task experts with the exploration of the user device to"}, {"title": "3.1.3 Memory Formulation", "content": "Accumulating memory from the Action Trajectory of exploration is a critical step to ensure the Expert\nhas the ability to maintain its generalization capability and self-correction ability through device\nexploration. During the exploration stage, we pay special attention to the accumulation of memory of\nthe functions of the user's interface icons, as well as the accumulation of insights closely related to\nthe expert's own responsibilities.\nInterFace Memory InterFace memory outlines a memory mechanism exclusively designed for\ndevice-specific icons. Due to the diversity of mobile device icon layouts and the widespread existence\nof system themes, generic VLMs struggle to directly discern the functions of different icons on the\nmobile interface.\nTo address this issue, we have proposed a Confidence-based Icon Memory Mechanism. Differing\nfrom previous research[33], our method encourages the expert to guess and verify the functions of\nthe icons related to the task goal, instead of being limited to the icons that have been manipulated,\nachieving three types (Verified, Hypothesized, Uncharted) of confidence labelling for different\nicons. By forming such memories, we managed to enhance the scope of interface memory while"}, {"title": "3.2 Expert Collaboration", "content": "Device Operation Agents face another crucial challenge in real-world applications: maintaining\naction consistency during high-frequency environmental interactions while effectively decomposing\ncomplex long-term tasks. To address this, we propose an innovative Expert Act Manner, ensuring\nconsistency and efficiency in the Agent's execution of tasks. Furthermore, inspired by [6], we\nintroduce a dual-layer planning mechanism at both Team and Expert levels, transforming complex\ntasks into atomic tasks with shorter execution paths."}, {"title": "3.2.1 Expert Act Manner", "content": "The fundamental principle behind maintaining action consistency in Device Operation Agents lies in\nits role as the foundation for agent decision-making. In previous studies, experts typically employed\nthe React paradigm to reason about and interact with the current environment, and enhancing their\nperception of the current state by summarizing action trajectories. While this approach maintains\nconsistency in atomic tasks, it often results in significant information loss when confronting long-term\ntasks, consequently compromising the decision accuracy of Operation Agents. To mitigate this\nlimitation, we propose an enhanced agent behavior paradigm coupled with a proactive working\nmemory processing function.\nSelf Verifying Manner: We propose self verifying manner, which enhances expert's ability on visual-\nlinguistic tasks by incorporating dual-state visual information input and an 'Observe-Verify-Act' cycle.\nThis mechanism continuously comparing previous and current visual states while considering action\nobjectives. This approach enables the expert to maintain better consistency between consecutive\nactions, resulting in more precise and context-aware decision-making in device operation tasks.\nProactive Working Memory: During Expert Level's task execution, critical information generated\nby the Expert is stored as its Working Memory. For long-term tasks, simply placing accumulated\nextensive Working Memory within the prompt leads to increased token and time costs, while excess\ninformation may also affect the Expert's decision-making ability. To address this issue, we've\nincorporated a Proactive Working Memory Processing function as a BasicOperation, allowing the\nExpert to efficiently manage its working memory during task execution. This ensures the Expert\nmaintains action consistency during long-term tasks while reducing token costs."}, {"title": "3.2.2 Expert Collaboration Via Double-Layer Planning", "content": "Accomplishing complex and long-term tasks, such as managing a Twitter account, presents two\nmajor challenges for Experts. The first challenge lies in decomposing the task into atomic task that\nan Expert can complete. The second challenge involves executing these tasks with high quality. In\nreal-world scenarios, complex tasks typically require team collaboration. Therefore, developing an\nappropriate collaborative mechanism for Experts becomes crucial in addressing this issue. We tackle\nthese challenges by implementing a double-layer planning mechanism and a team-shared memory\npool, enabling effective collaboration among Experts.\nThrough Expert Development, Experts with diverse capabilities and roles form a team. Double layer\nplanning operates at both team and expert levels to plan user instructions. Specifically, the team task\ndistributed Layer decomposes instructions into a graph organized as a DAG (Directed Acyclic Graph)\nbased on user directives and team member role descriptions. Each node in this graph represents a\ntask assigned to an Expert with corresponding abilities.\nThis graph structure also establishes a communication mechanism among Experts. By maintaining\na team-shared Memory Pool, each Expert uses the Commit method to store structured descriptions\nof task states and accumulated working memories upon completion of Team Level tasks. Experts\ndependent on these tasks then use the Fetch method to selectively extract relevant information, storing\nit in their individual working memories to support current task execution.\nAt the expert task decomposition layer, the planner breaks down Team Level task descriptions into\nsequences of atomic tasks. Based on the status of these atomic tasks, the Planner can dynamically\nadjust the sequence order or repeat execution, implementing a flexible regulation mechanism to\nensure successful completion of atomic tasks."}, {"title": "4 Experiments", "content": "In order to comprehensively evaluate the ability of MobileExperts in handling diversified and complex\ntasks, we have constructed a new benchmark - Expert-eval. Based on this Benchmark, we compared\nour method with two baselines, and performed ablation experiments on various key modules of\nMobileExperts. Meanwhile, in order to intuitively display the performance of MobileExperts in tasks\nof different intelligence levels, we have carried out a case study to deeply analyze its behavior strategy\nand effect. Considering the requirement of overall action space for api accessibility, we chose to"}, {"title": "4.1 Settings", "content": "Expert-Eval. In order to comprehensively evaluate MobileExperts' performance in handling diverse\napplications and tasks of different complexities, we have designed a new benchmark named Expert-\nEval. Expert-Eval aims to measure the intelligence level of mobile device operation agent at varying\nlevels, which is closely related to the planning capabilities required for the task. We have defined\nthree intelligence levels, ranging from simple to complex as follows:\n\u2022 C1 (Executor): Task execution is directly in accordance with the user's instructions, with no\nadditional planning required.\n\u2022 C2 (Planner): The task requires basic planning, the system needs to autonomously plan and\nexecute tasks with clear objectives.\n\u2022 C3 (Strategist): The task requires complex, long-term planning, the system needs to formulate\ndetailed plans for tasks of high complexity with clear objectives, and make adjustments during\nexecution based on feedback.\nIn addition, in order to verify the generalization ability of MobileExperts in different application\nscenarios, we divide applications into three main categories: Social Media, Online Service, and\nProductivity Tool. Based on these classifications, we have designed a series of tasks that require the\nuse of one or more applications to test the adaptability and flexibility of MobileExperts in actual\napplications.\nMetrics. We design five metrics to measure the performance of the mobile device operation agent in\nvarious dimensions.\n\u2022 Success Rate: This metric measures whether the agent successfully completes tasks. If the task\nis completed, the score is marked as 1."}, {"title": "4.2 Experiment Results", "content": "In this experiment, we used a unified mobile device, pre-installed with all the necessary applications,\nand tested expert-eval on three methods. In order to ensure the consistency of the experimental results,\nwe set the maximum number of automatic explorations for individual tasks of MobileExperts and\nAppAgent to 10 times, and required each method to try a maximum of three times, finally choosing\nthe best results as the outcome. In addition, considering the importance of the process score, we set\nthe maximum number of execution steps for all methods's C1 and C2 Task to 15 steps to prevent\ncycle errors, ensuring the efficiency of the experiment and the reliability of the results. The results of\nthe experiment are presented in table 3."}, {"title": "5 Conclusion", "content": "This paper introduces MobileExperts, an innovative multimodal agent framework designed to address\ncomplex mobile device operation tasks. The framework achieves breakthrough performance through\ntwo core features: code combination based tool formulation on mobile devices and expert collab-\noration via double-layer planning. This approach not only provides a novel method for automatic\ntool formulation in the field of mobile device operation agents but also introduces innovative action\nstrategies for agent teams. Experimental results demonstrate that MobileExperts excels in handling\nhigh-complexity tasks while significantly reducing reasoning costs. These findings not only confirm\nthe framework's potential to enhance agent capabilities and reduce resources consumption but also\nadvance research in mobile device operation automation."}]}