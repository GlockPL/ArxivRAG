{"title": "Label-Augmented Dataset Distillation", "authors": ["Seoungyoon Kang", "Youngsun Lim", "Hyunjung Shim"], "abstract": "Traditional dataset distillation primarily focuses on im- age representation while often overlooking the important role of labels. In this study, we introduce Label-Augmented Dataset Distillation (LADD), a new dataset distillation framework enhancing dataset distillation with label aug- mentations. LADD sub-samples each synthetic image, gen- erating additional dense labels to capture rich semantics. These dense labels require only a 2.5% increase in storage (ImageNet subsets) with significant performance benefits, providing strong learning signals. Our label-generation strategy can complement existing dataset distillation meth- ods and significantly enhance their training efficiency and performance. Experimental results demonstrate that LADD outperforms existing methods in terms of computational overhead and accuracy. With three high-performance dataset distillation algorithms, LADD achieves remarkable gains by an average of 14.9% in accuracy. Furthermore, the effectiveness of our method is proven across various datasets, distillation hyperparameters, and algorithms. Fi- nally, our method improves the cross-architecture robust- ness of the distilled dataset, which is important in the appli- cation scenario.", "sections": [{"title": "1. Introduction", "content": "Dataset distillation, also called dataset condensation, cre- ates a small synthetic training set to reduce training costs. The synthesized dataset enables faster training while main- taining a performance comparable to that achieved with the source dataset. For example, FrePo [45] attained 93% of full dataset training performance using merely one image per class in MNIST [6]. Dataset distillation can be applied in various fields. These include privacy-free training data generation (e.g., federated learning [12,31,46], medical im- age computing [20, 31]), fast training (e.g., network archi- tecture search [41-43]), or compact training data generation (e.g., continual learning [41-43]).\nThe efficacy of distilled datasets is typically evaluated based on the test accuracy achieved by models trained by these datasets. The distilled dataset must maximally encapsulate essential information of the source dataset within a limited number of synthetic samples. Prior re- search [2, 21, 22, 33, 36, 42, 43] has refined the optimization objective within the bi-loop nested meta-learning frame- work for dataset synthesis. Some methods have further ex- plored optimization spaces beyond image [3,9] and efficient ways to utilize pixel-space [17]. Additionally, several ap- proaches [4, 34, 45] develop algorithms to reduce the com- putational cost induced by the bi-loop optimization. How- ever, these efforts mostly focus on data representation in images, overlooking the important roles of labels.\nLabels, pivotal in supervised learning, pair with images to provide strong learning signals. In contrast to images, labels provide highly compressed representations because they are defined in a semantic space. For instance, in the ImageNette-128 [16], representing a \u201ccassette player\" re- quires 49,000 scalars (128 \u00d7 128 \u00d7 3) for the image, but only ten scalars for its one-hot vector label. This substantial difference between image and label suggests a new perspec- tive to dataset distillation, emphasizing the potential of har- nessing more information from labels rather than images.\nAddressing the overlooked potential of labels in dataset distillation, we introduce Label-Augmented Dataset Distil- lation (LADD). LADD effectively exploits labels in a dis- tilled dataset. Our approach comprises two main stages: distillation and deployment, as depicted in Fig. 1. In the distillation stage, we first generate synthetic images us- ing existing distillation algorithms. Subsequently, we ap- ply an image sub-sampling algorithm to each synthetic im- age. For each sub-image (termed a local view), we generate a dense label, sub-image's soft label, which encapsulates high-quality information. During the deployment stage, LADD uniquely merges global view images with their orig- inal labels and local view images with the corresponding dense labels, delivering diverse learning signals.\nLADD presents three key benefits over prior methods: (1) enhanced storage efficiency with smaller increments in dataset sizes, (2) reduced computational demands, and (3)"}, {"title": "2. Related work", "content": "Preliminary: dataset distillation. Dataset distillation is the process of synthesizing a dataset, denoted as D, which comprises a small, representative subset of samples ex- tracted from a larger source dataset Ds. With the number of total classes C and the number of images per class (IPC), the distilled dataset D contains C \u00d7 IPC image-label pairs (i.e., D = {(xi, Yi)CIPC}).\nTo achieve dataset distillation, algorithms employ a bi- loop optimization strategy consisting of two phases: the inner-loop and the outer-loop. The inner loop simulates"}, {"title": "3. Method", "content": "We propose Label-Augmented Dataset Distillation (LADD), a specialized label augmentation method for dataset distillation. During the dataset distillation stage, LADD conducts a label augmentation process to images distilled by conventional image-level dataset distillation algorithms. For each image $x_i$, we produce additional groups of soft labels, denoted dense labels, and create a label-augmented dataset $D_{LA}$. Specifically, to obtain $D_{LA}$, the label augmentation step goes through two processes: (1) an image sub-division and (2) a dense label generation. In the deployment stage, LADD uses both global (i.e., full images with hard labels) and local data (i.e., sub-sampled images with dense labels) to train the network effectively. Fig. 1 depicts the overview of our method.\nIn the following section, we describe details of the label augmentation process (Sec. 3.1) and the labeler acquisition (Sec. 3.2). Finally, we demonstrate the training procedure of the deployment stage (Sec 3.3)."}, {"title": "3.1. Label Augmentation", "content": "We denote the image-level distilled dataset $D = \\{(x_i, Y_i) \\}_{i \\in [1,C \\times IPC]}$, where C is the number of classes in the source dataset $D_s$ and IPC is the number of images per class. In our framework, D is generated using an existing image-level distillation algorithm. By preserv- ing the effectiveness of the image-level distilled dataset, our method synergizes with state-of-the-art dataset distillation algorithms, leveraging their strengths.\nImage Sub-Sampling. We define a function S that samples synthetic image $x_i \\in D$ into several sub-images. Consid- ering the memory-constrained environment, dynamic sub- image sampling is not an optimal choice because it requires saving additional sampling parameters. Therefore, we re- strict S to be a static strategy sampler. We sample $N^2$ sub- images from $x_i$. Each sub-image covers R% of each axis. To achieve a uniform sampling across $x_i$, we maintain a consistent stride $(100\\% \u2013 R\\%)/(N \u2212 1)$ for cropping. For example, for $x_i$ of 128 \u00d7 128 pixels, using R = 62.5% and N = 5, we obtain 25 sub-images of 80 \u00d7 80 pixels each, ap- plying a 12-stride. After the sub-sampling, we resize each sub-image to match the dimension of $x_i$. For clarity, we"}, {"title": "Algorithm 1 Label Augmentation", "content": "1: Input: Distilled dataset D = {(xi, Yi)}, Labeler g,\n Sub-sampling function S\n2: Output: Label augmented dataset DLA\n3: for each image xi in D do\n4: for j = 1 to N2 do\n5:  $x_{i,j} \\leftarrow S_j(x_i)$ d Generate j-th sub-image\n6:  $Y_{i,j}^d \\leftarrow g(x_{i,j})$ d Generate sub-image soft label\n7: end for\n8: Add $(x_i, Y_i, y_i^d)$ to $D_{LA}$\n9: end for\n10: return DLA\ndenote the sub-sampling function S as below:\n$x_{i,j} = S_j(x_i),$\\nwhere $j \\in [1, N^2]$ is the index of sub-sampled image.\nDense Label Generation. Sub-images, derived from the same original image, vary in visual content. In detail, each sub-image exhibits distinct patterns, conveying differ- ent levels of class information. We generate labels for each sub-image $x_{i,j}$, resulting in $N^2$ labels for each synthetic im- age $x_i$. To capture rich information in these labels, we opt for soft labeling. We develop the labeler $y^s_i = g(x)$, where x denotes the image and $y^s$ is the corresponding soft label. We train the labeler on the source dataset Ds from scratch. Then, we obtain a dense label $y_i^d$ from each sub-image:\n$Y_{i,j}^d = g(S_j(x_i)).$\\nWe will discuss how to train g in Sec 3.2.\nAfter the dense label generation, we obtain the original hard label $y_i$ and a dense label $y_i^d$ containing $N^2$ soft labels for a synthetic image $x_i$. We denote the label augmented dataset as $D_{LA} = \\{(x_i, Y_i, Y_i^d)|i \\in [1,C \\times IPC]\\}$. The synthesis process of $D_{LA}$ is illustrated in Algorithm 1.\nOne straightforward approach might involve optimizing labels as part of the distillation process. However, it adds complexity to an already complicated optimization process, potentially leading to instability. Furthermore, it reduces computational efficiency due to slower convergence and in- creased operations per iteration. Instead, our LADD first applies existing distillation methods for image-level distil- lation. Subsequently, we perform a label-augmentation step on the distilled data, producing final datasets with our gen- erated labels. In this way, LADD enjoys significant perfor- mance gains with minimal computational overhead.\nBoth LADD and knowledge distillation [15] use a teacher model but differ in the medium of knowledge trans- fer. Knowledge distillation transfers knowledge through an online teacher during the evaluation stage. However, LADD produces a dataset of images and augmented labels which"}, {"title": "3.2. Acquiring Labeler g.", "content": "LADD employs a labeler g to generate dense labels, em- ploying the same labeler across all evaluations for fairness. To minimize overhead, we design g as a small network mir- roring the distillation architecture (ConvNetD5). We train it for 50 epochs with a learning rate of 0.015, saving param- eters at epochs 10, 20, 30, 40, and 50. We use the model trained up to 10 epochs as our early-stage labeler g, as it provides general and essential information for sub-images. This is well-aligned with existing dataset distillation meth- ods [2, 13]. Although g is trained on a source dataset, it ap- propriately predicts labels for distilled images because the distilled dataset retains local structures of the source data.\nApart from our chosen method, classifiers trained on dif- ferent data, including zero-shot models like CLIP [23], can be used as g. However, they do not produce more effective dense labels than our method. This is because these pre- trained models are not trained on the distilled dataset and have different architectures from those used in distillation."}, {"title": "3.3. Training in Deployment Stage", "content": "We closely follow the deployment stage from existing ap- proaches. Given the dataset $D_{LA}$ and an optimized learn- ing rate \u03b7, we conduct standard classification training on the target network $h(x, \\phi)$. Additionally, we modify the data in- put and training loss to effectively utilize informative dense labels in $D_{LA}$:\n$L_{cls} = CE(h(x_i, \\phi), y_i) + \\sum_{i,j}^{N^2} CE(h(S_j(x_i), \\phi), Y_{i,j}^d),$\\nwhere CE(,) is a cross-entropy loss. The dimensions of $Y_i$ (one-hot) and $y_{i,j}^d$ (soft) are the same as $R^C$, and the di- mension of $y_i^d$ is $R^{N^2\\times C}$. Through this process, we provide diverse training feedback through augmented dense labels beyond the signal provided by D."}, {"title": "4. Experiment", "content": "4.1. Implementation details\nImage Sub-Sampling. The sub-sampling function is se- lected as a uniform sampler S with R = 62.5% and N = 5; R and N are determined experimentally (experiments are in Sup.A). Throughout the experiments, 25 sub-images are generated per synthetic image, and each sub-image is 80 \u00d7 80 in size when using 128 \u00d7 128 source dataset."}, {"title": "4.2. Quantitative evaluation", "content": "We quantitatively evaluate LADD by benchmarking it against representative distillation methods (MTT [2], AST [29], and GLaD [3]) in various IPC settings. LADD in- curs additional memory usage compared to the baseline because of labeler training and label augmentation. For fair comparison, we evaluate the baselines with incremented IPC (i.e., IPC+1), labeled as baseline++. We focus on 4-CAE results in Tab. 1 since MTT and AST are not fully compatible with heterogeneous architectures (e.g., several experiments failed to converge on ViT architecture). The additional memory overhead for both images (uint8) and labels (float32) is calculated utilizing the Python zipfile library [10], the standard compression method.\nTab. 1 presents the results for varying IPC on the Im- ageNette. The quantitative analysis reveals that LADD surpasses the baseline, showing an average improve- ment of 15% at 5 IPC. Notably, our method outperforms baseline++ in all cases except at 1 IPC. At 1 IPC, baseline++ entails a 100.1% increase in memory us- age. In contrast, LADD achieves comparable performance with only a 2.5% storage overhead, resulting in 40 times greater memory efficiency. For 5 IPC, baseline++ re- quires 20.7% more memory to accommodate an extra im- age per class. Conversely, LADD requires only an additional 2.5% memory while achieving, on average, a 13.2% better performance than baseline++ across three models. Con- sequently, we conclude that our approach shows impressive performances in terms of accuracy and efficiency, creating synergies with existing dataset distillation algorithms.\nWe evaluate the cross-architecture robustness of our method. Tab. 2 shows results for five architectures during the deployment stage. Notably, the baseline's ViT exhibits the weakest performance due to the architectural divergence"}, {"title": "4.3. Impact of Dense Labels in LADD", "content": "In this section, we investigate the most efficient ways to uti- lize a distilled dataset. We designate GLaD (MTT) as our"}, {"title": "4.4. Dataset Quality Analysis", "content": "We employ GradCAM [28] to visually investigate the rea- sons behind performance improvements from label aug- mentation. Fig. 4 displays the GradCAM results for GLAD (MTT) and LADD, both trained on ImageNette at 5"}, {"title": "4.5. Ablation Study", "content": "The ablation study on LADD-GLaD(MTT) using the Ima- geNette at 5 IPC concentrates on identifying the ideal train- ing steps for the labeler. The labeler creates soft labels that encapsulate meaningful information for specific sub-"}, {"title": "5. Conclusion and Limitation", "content": "In this work, we highlight the overlooked role of labels in distilled datasets. Addressing this limitation, we introduce Label-Augmented Dataset Distillation (LADD), a method that effectively utilizes labels. Our approach enriches labels with useful information, orthogonal to the images. This yields three major advantages: (1) enhanced efficiency in distillation computation, (2) improved memory capacity ef- ficiency, and (3) increased dataset robustness.\nExtensive experiments demonstrate that LADD en- hances various distillation methods with minimal extra computational and memory resources. On five ImageNet subsets and three baseline methods, LADD achieves an av- erage performance improvement of 14.9% with only a 2.5% memory increase. Remarkably, LADD surpasses baselines with more images per class while using fewer computa- tional resources and memory capacity. LADD with 5 IPC delivers 12.9% more accuracy than a 6 IPC baseline while using eight times less memory. We confirmed that datasets distilled using LADD enable more robust training across diverse architectures. Additionally, results from Grad- CAM [28] visualizations show that models trained with our dataset accurately and robustly capture object locations.\nLimitation. Our approach requires training a labeler to gen- erate dense labels, which may need extra resources. How- ever, this is more efficient than re-distilling the dataset with more images per class. Once trained, the labeler continu- ously produces dense labels for the same dataset."}, {"title": "Supplementary Material for Label-Augmented Dataset Distillation", "content": "A. Sub-Sampling Hyperparameter N and R\nWe perform the study on LADD-GLaD(MTT) using the Im- ageNette dataset at 5 IPC. It aims to determine the opti- mal size (R) and the number (N) of image sub-samplings. We test four different sizes R and quantities N, validating LADD-GLAD (MTT) with 5-CAE. Tab. S1 shows that an in- crease in N correlates with improved overall accuracy. This is expected, as a higher number of soft labels in a dense la- bel encompasses more information. However, increasing N also results in greater memory inefficiency. For instance, comparing N = 5 with N = 7, the performance gain is a mere 1.1%, but the overhead rises by 94%. Therefore, balancing the performance-efficiency trade-off is crucial. Hence, we select N = 5 for our model, considering both performance and efficiency.\nR represents the size of the sub-image. If R is too small, vital objects representing the target class may be absent in most sub-images. This results in performance degradation due to information loss. Conversely, if R is too large, label augmentation efficiency drops because of redundant infor- mation in each sub-image. Our observations indicate that R = 62.5% yields the most accurate results. Therefore, we choose R = 62.5% for our model.\nB. Fair Comparison Settings for RDED\nRDED [32] introduces an efficient approach for distilling large-scale datasets. It achieves a remarkable 42% top-1 validation accuracy with ResNet-18 [14] on the ImageNet- 1K dataset [5]. RDED first generates diverse and realistic data through an optimization-free algorithm backed by v-information theory [37], which is equivalent to the distilla- tion step. In the deployment stage, the method augments the distilled images and computes the corresponding soft labels from the teacher model. Then, it trains the test model using the augmented images and soft labels.\nDespite the remarkable performance of RDED, we iden- tified that the method does not align with the purpose of dataset distillation. Dataset distillation aims to distill the knowledge from a given dataset into a terse data sum- mary [25]. However, RDED uses a teacher model for soft label prediction of augmented images in the deployment stage. Specifically, RDED generates an unlimited number of images and labels via image augmentation that fully ex- ploits the teacher model's knowledge. Thus, RDED aligns more with knowledge distillation rather than dataset distil- lation in the deployment stage.\nTherefore, we assess the performance of RDED while ensuring it complies with the purpose of dataset distillation"}, {"title": "C. Performance Degradation in TESLA", "content": "TESLA consistently depicts low accuracy in both Tab. 3 and Tab. 5. Although we used the official code and tuned the hyperparameters, we could not successfully train TESLA. Thus, we investigated the reason for this result.\nTESLA introduces a method to reduce the high GPU memory issue arising from the bi-loop nested optimization problem in MTT [2]. Through a formulaic improvement, it reduces unnecessary computation graphs while achiev- ing the same objective. Specifically, TESLA claims that the gradient for each batch image only depends on the iter- ation involving the images. Thereby, the model can remove the computation graph after computing the gradient for each image.\nWe found an oversight in TESLA's formulation: it does not consider the inner-loop model parameters as dependent variables of the image from different iterations. This means TESLA simplifies the objective of MTT by ignoring the feedback from different training iterations to reduce compu- tations. This explains why TESLA is incapable of achieving a similar high accuracy to MTT in Tab. 3. Detailed proof can be found in Sec. E."}, {"title": "D. Experiments on Small Dataset: CIFAR-10", "content": "We evaluate LADD on the small-sized image dataset, CIFAR-10 [18]. We adopt the same hyperparameters (i.e., R and N) defined in Sec. 4.1, with an image size of 32 \u00d7 32. We apply LADD to the distilled dataset from DATM [13], which is the current state-of-the-art method for small-sized datasets. To account for the small-sized image, we use a 3-layer convolutional network (ConvNetD3) for both the distillation and deployment stages. Tab. S2 reports the de- ployment stage performance at 1 and 10 IPC. The results demonstrate that our method improves DATM and achieves the highest performance compared to other methods. There- fore, we conclude that LADD also boosts performance in small-sized datasets."}, {"title": "E. Mathematical Analysis on TESLA", "content": "In this section, we derive the mathematical differences be- tween TESLA and MTT to explain the performance differ- ence in Tab. 3 and Tab. 5."}, {"title": "E.1. Objective Function of MTT", "content": "We briefly review the mathematical expression of MTT to understand the oversight in TESLA. MTT defines the $L_{sim}$ through the parameter distance:\n$L_{sim} = \\frac{||\\theta_{t+T} - \\theta^{*}_{t+M}||^2}{||\\theta_t - \\theta^{*}_{t+M}||^2},$ \nwhere $\\theta_t$ and $\\theta^{*}_{t+M}$ are the model parameters trained on source dataset $D_s$ for t and t+M steps, respectively. Start- ing from the $\\theta_t$, MTT trains the model for $i \\in [0,T)$ steps on the distilled dataset $D$ following the SGD rule and cross- entropy loss. The trained parameter is denoted as:\n$\\hat{\\theta}_{t+i+1} = \\hat{\\theta}_{t+i} - \\beta \\nabla_\\theta l(\\hat{\\theta}_{t+i}; X_i),$ \nwhere $X_i$ is sub-batch of $D$ and $l(\\theta_{t+i}; X_i)$ is the cross- entropy loss. $\\beta$ indicates the learning rate for the inner-loop. We can expand $\\hat{\\theta}_{t+T}$ as:\n$\\hat{\\theta}_{t+T} = \\theta_t - \\beta \\nabla_\\theta l(\\theta_t; X_0) - \\beta \\nabla_\\theta l(\\hat{\\theta}_{t+1}; X_1) - ... - \\beta \\nabla_\\theta l(\\hat{\\theta}_{t+T-1}; X_{T-1}).$"}, {"title": "E.2. Cause of Performance Degradation", "content": "TESLA claims two points. First, the elements of the first term G only involve the gradients in a single batch and thus can be pre-computed. Second, the computation graph of $\\nabla_\\theta l(\\theta_{t+i}; X_i)$ is not required in the derivative of any other batch $X_{j \\neq i}$. Based on these points, TESLA computes the gradient for each batch $X_i$ as:\n$\\frac{\\partial ||\\theta_{t+T} - \\theta^{*}_{t+M}||^2}{\\partial X_i} = 2 \\beta (\\theta^{*}_{t+M} - \\theta_t)^T \\frac{\\partial}{\\partial X_i} \\nabla_\\theta l(\\theta_{t+i}; X_i) + 2 \\beta^2 G^T \\frac{\\partial}{\\partial X_i} \\nabla_\\theta l(\\theta_{t+i}; X_i).$\nSince Eqn. S6 can be computed for each batch, TESLA asserts that the memory requirement can be significantly reduced by not retaining the computation graph for all batches."}, {"title": "F. Visualization of Sub-Samples", "content": "Fig. S1 demonstrates examples of the results after apply- ing sub-sampling to the distilled dataset. After distilling the Imagenette dataset using the MTT and GLaD methods, the images from the Tench and Church classes were extracted, and these are the original images shown on the left of each sample. Sub-sampling is then performed with hyperparam- eters set to N = 5 and R = 62.5%, starting from the top-left corner of the original image. As a result, 25 sub-images are generated for each original image, which are displayed on the right of each sample."}, {"title": "G. Future Works", "content": "We aim to quantize the LADD to reduce storage require- ments and improve training efficiency. Furthermore, we plan to explore the application of LADD in tasks that re- quire higher computational costs, such as vision-language models. We will optimize the balance between dense and hard labels through ablation studies or by learning a weight parameter. Additionally, we intend to experiment with alter- native static sub-sampling methods to enhance overall per- formance and scalability across diverse tasks."}]}