{"title": "Gradient-Guided Conditional Diffusion Models for\nPrivate Image Reconstruction: Analyzing Adversarial\nImpacts of Differential Privacy and Denoising", "authors": ["Tao Huang", "Jiayang Meng", "Hong Chen", "Guolong Zheng", "Xu Yang", "Xun Yi", "Hua Wang"], "abstract": "We investigate the construction of gradient-guided conditional diffusion models\nfor reconstructing private images, focusing on the adversarial interplay between\ndifferential privacy noise and the denoising capabilities of diffusion models. While\ncurrent gradient-based reconstruction methods struggle with high-resolution images\ndue to computational complexity and prior knowledge requirements, we propose\ntwo novel methods that require minimal modifications to the diffusion model's\ngeneration process and eliminate the need for prior knowledge. Our approach\nleverages the strong image generation capabilities of diffusion models to reconstruct\nprivate images starting from randomly generated noise, even when a small amount\nof differentially private noise has been added to the gradients. We also conduct\na comprehensive theoretical analysis of the impact of differential privacy noise\non the quality of reconstructed images, revealing the relationship among noise\nmagnitude, the architecture of attacked models, and the attacker's reconstruction\ncapability. Additionally, extensive experiments validate the effectiveness of our\nproposed methods and the accuracy of our theoretical findings, suggesting new\ndirections for privacy risk auditing using conditional diffusion models.", "sections": [{"title": "Introduction", "content": "In recent years, the fields of machine learning and federated learning have garnered significant\nattention due to their ability to harness large-scale data for various applications. However, large-scale\ndata often carries private information, such as faces, genders, and so on. One critical concern that has\nemerged is the privacy of the training data. Gradients, a fundamental component in the optimization\nprocess of machine learning models, typically shared among different parties in federated learning,\ninherently contain private information about the training data. Data reconstruction attacks Zhu et\nal. [2019]; Jeon et al. [2021a]; Fang et al. [2023a], in particular, aim to reconstruct the original\ntraining data from the gradients, posing a severe threat to data privacy. When it comes to private\nimage reconstruction, attackers are highly motivated to illicitly acquire private images, especially\nhigh-resolution images, due to commercial interests or other reasons, as there are many situations\nin which high-resolution images are needed. For instance, in fields like radiology, high-resolution\nimages can make the difference between missing and identifying critical patient conditions. Detailed\nimages help in diagnosing diseases, planning treatments, and monitoring patient progress. These\nimages are costly to access and always contain private information.\nExisting gradient-based image reconstruction attacks are only effective for low-resolution images,\nwith poor performance or very slow reconstruction speeds for high-resolution images. This is because\ncurrent methods require repeated differentiation of randomly initialized images Zhu et al. [2019];\nZhao et al. [2020], and as the dimensionality of these images increases, the differentiation becomes\nmore difficult, leading to greater reconstruction errors. Reconstruction methods based on Generative\nAdversarial Networks (GANs) Jeon et al. [2021a]; Fang et al. [2023a] or fine-tuned diffusion models\nMeng et al. [2024]; Xue et al. [2024] introduce additional computational complexity and require\ncertain prior conditions. For example, the distribution of the initial images must match that of the\nprivate images.\nDiffusion models Sohl-Dickstein et al. [2015]; Ho et al. [2020]; Song and Ermon [2019]; Song\net al. [2020b], inspired by the physical process of diffusion, iteratively add noise to data, break\nup the original data into pure Gaussian noise, and learn to reverse this process by predicting the\nadded Gaussian noise, thereby generating new data from a Gaussian random vector by denoising\nthe added Gaussian noise step by step. Conditional diffusion models Dhariwal and Nichol [2021];\nHo and Salimans [2022]; Chung et al. [2022c,b]; Zhu et al. [2023]; Bansal et al. [2023]; Yu et al.\n[2023]; He et al. [2023]; Yang et al. [2024] extend this approach by conditioning the generation\nprocess on additional information, allowing for more controlled and targeted data generation. Due\nto the excellent image-generating ability of diffusion models, recent work Meng et al. [2024] has\ndemonstrated that private gradients can serve as the guidance for fine-tuning pre-trained diffusion\nmodels to generate private images and the adversary can obtain a gradient-based fine-tuned diffusion\nmodel to conduct an effective image reconstruction attack.\nTo mitigate the privacy risks associated with gradient leakage, one effective method is to add\ndifferentially private noise Dwork et al. [2014] to the original gradients such that the adversary\ncould not infer private information from these noisy gradients Abadi et al. [2016]; Zhu et al. [2019];\nMeng et al. [2024]; Wang et al. [2022]; El Mestari et al. [2024]. Differential privacy Dwork et al.\n[2014] ensures that the added noise masks the private information, thereby reducing the risk of data\nreconstruction attacks.\nBased on the noising ability of differential privacy and the denoising ability of diffusion models,\nwe are interested in the following issues: Given the capability of conditional diffusion models to\ngenerate high-resolution images, is it possible for an attacker to incorporate stolen or inadvertently\nleaked gradients into the conditional diffusion model as a guiding condition, thereby reconstructing\nthe original image? Current reconstruction methods require certain priors. Could this prerequisite be\neliminated in a conditional diffusion model without compromising the reconstruction quality? In other\nwords, can a conditional diffusion model, starting from a randomly generated noise, utilize stolen\nor leaked gradients to reconstruct the original private image? Could diffusion models' denoising\nability contribute to the data reconstruction attack even if the adversary captures the noisy gradients\nwith injected differentially private noise? By analyzing the interplay between the noise introduced\nby differential privacy and the denoising ability of diffusion models, we can better understand their\nadversarial impacts.\nIn this paper, we delve into how to establish a gradient-guided conditional diffusion model to recon-\nstruct private images and the adversarial impacts between differential privacy's noising mechanism\nand the denoising ability of diffusion models. We aim to provide a comprehensive analysis of how\nthese two factors interact and influence the overall privacy of private data. Through this investigation,\nwe seek to advance the understanding of differential privacy-preserving techniques in the context of\nprotecting private gradients. In detail, we first discuss how the attacker obtains conditional diffusion\nmodels based on gradients to reconstruct private images if they capture the original or differentially\nprivate gradients. Then, we study the adversarial impacts of differential privacy's noising mechanism\nand the denoising ability of diffusion models by exploring how the differentially private noise scale"}, {"title": "Related Work and Preliminary", "content": "In deep learning, model training is executed in parallel across different nodes sometimes, and\nsynchronization occurs through gradient exchange, for example, federated learning Kone\u010dn\u1ef3 et\nal. [2016]. Recent studies on the risks associated with reconstructing private training images in\ndistributed or federated scenarios reveal that adversaries can steal gradients by eavesdropping on\ncommunication channels Wu et al. [2024], impersonating federated learning participants Hitaj et al.\n[2017a], and so on. It is indeed possible to extract private training data from the shared or stolen\ngradients Fredrikson et al. [2015]; Hitaj et al. [2017b]; Melis et al. [2019]; Zhu et al. [2019]; Zhao\net al. [2020]. Namely, the adversary can recover the training inputs from these shared or stolen\ngradients, questioning the safety of gradient sharing in preserving data privacy. Usually, the attacker\noptimizes a dummy input to minimize the difference between its gradient in the attacked model and\nthe leaked gradient Zhu et al. [2019]; Zhao et al. [2020]. The optimization can be formulated as:\n$\\min L(V_F(x', W), V_F(x, W)) = \\min ||V_F(x', W) - V_F(x, W) ||^2$ (1)\nwhere x' is the dummy input initialized randomly, $V_F(x', W)$ is the gradient of the loss with\nrespect to the image x' for the current model weights W, and $V_F(x, W)$ is the leaked gradient that\nthe attacker obtains. F is the loss function of the attacked model and $L(V_F(x', W), V_F(x, W))$ is the reconstruction loss. The reconstruction loss adopts the mean square error (MSE) loss in Eq.(1).\nIn the realm of reconstructing training inputs with shared gradients based on the reconstruction\nloss, various advanced methodologies Geiping et al. [2020]; Yin et al. [2021]; Jeon et al. [2021b];\nLi et al. [2022]; Fang et al. [2023b] have emerged. Geiping et al. [2020] achieves efficient image\nrecovery through a meticulously designed loss function. Yin et al. [2021]; Jeon et al. [2021b]; Li\net al. [2022] propose the utilization of Generative Adversarial Networks (GANs) acting as prior\ninformation, which offers an effective approximation of natural image spaces, significantly enhancing\nattack methodologies. Fang et al. [2023b] exhibits outstanding generalization capabilities in practical\nsettings, underscoring its adaptability and wider applicability across various scenarios. Meng et\nal. [2024] reconstructs high-resolution images accurately by fine-tuning diffusion model, but adds\ncomputational complexity and requires specific prior conditions."}, {"title": "Diffusion Models and Conditional Diffusion Models", "content": "Diffusion models Ho et al. [2020]; Nichol and Dhariwal [2021]; Song et al. [2020a]; Rombach et al.\n[2022], inspired by the physical process of diffusion, which describes the movement of particles from\nregions of higher concentration to lower concentration, are a class of generative models in machine\nlearning that have gained significant attention due to their ability to produce high-quality, diverse\nsamples. Diffusion models work by gradually adding noise to data over a series of steps, transforming\nthe data into a pure noise distribution. This process is then reversed to generate data samples from the\nnoise. The two key processes involved are the forward diffusion process and the reverse generation\nprocess.\nSpecifically, the stochastic differential equation (SDE) for the data noising process $x_t, t \\in [0, T]$ in\nthe following form Song et al. [2020a]:\n$dx = - \\frac{\\beta(t)}{2} x dt + \\sqrt{\\beta(t)}dw$ (2)\n, where $\u00df(t) : R \\rightarrow R > 0$ is the continuous noise schedule of the process, typically taken to be a\nmonotonically increasing linear function of t Ho et al. [2020], and w is the standard Wiener process.\nThe data distribution is defined as $x_0 \\sim P_{data}$. A simple, tractable distribution (e.g. isotropic Gaussian)\nis achieved when t = T, i.e. $x_T \\sim N(0, I)$. To recover the data-generating distribution starting from\nthe tractable distribution, it can be achieved via the corresponding reverse SDE of Eq.(3):\n$dx = -(\\frac{\\beta(t)}{2}x - \\beta(t) \\nabla_{x_t} \\log p_t(x_t)) dt + \\sqrt{\\beta(t)}dw$ (3)\n, where dt corresponds to time running backward and dw is the standard Wiener process running\nbackward. The time-dependent drift function $\\nabla_{x_t} \\log p_t(x_t)$ is closely related to the conditional\ndiffusion models which will be discussed later.\nIn the discrete settings with N bins, we define $x_t = x(\\frac{t}{N}), \\beta_t = \\beta(\\frac{t}{N})$ and we can get the\ndiscrete version of the forward and corresponding reverse SDE, which hints the Denoising Diffusion\nProbabilistic Models (DDPM) Ho et al. [2020]. If $x_0 \\sim P_{data}$ represents the original data, the\nprocess generates a sequence {$X_1, X_2, ..., X_T$} where each $x_t$ is a noisier version of $x_{t\u22121}$, and $x_T$\nis indistinguishable from pure noise. This can be mathematically represented as Ho et al. [2020];\nNichol and Dhariwal [2021]:\n$x_t = \\sqrt{\\overline{\\alpha}_t}x_0 + \\sqrt{1 - \\overline{\\alpha}_t}\\epsilon_t, \\epsilon_t \\sim N (0, I)$ (4)\nwhere $\\alpha_t := \\Pi_{i=1}^{t} (1 - \\beta_s)$, {$\\beta_t$}$_{t=1}^{t}$ are fixed variance schedules and $ \\epsilon_t$ is the Gaussian noise\nfrom a standard normal distribution.\nThe reverse process is a denoising process. It involves learning a parameterized model $p_\\theta (x_{t-1}|X_t)$\nto reverse the forward steps. It is modeled by a neural network trained to predict the noise $ \\epsilon_t$ that is\nadded at each step in the forward process. The sampling methods of different diffusion models differ.\nIn DDPM, the reverse process is a stochastic Markovian process, which is formulated as Ho et al.\n[2020]; Nichol and Dhariwal [2021]:\n$x_{t-1} = \\frac{1}{\\sqrt{1 - \\beta_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\overline{\\alpha}_t}}\\epsilon_\\theta(x_t, t)) + \\sigma_tz$ (5)\n, where $ \\epsilon_\\theta (x_t, t)$ is a prediction of $ \\epsilon_t$ parameterized by $ \\theta$, and $z \\sim N (0, I)$. Meanwhile, Denoising\nDiffusion Implicit Model (DDIM) Song et al. [2020a] is proposed as an alternative non-Markovian\ndenoising process that has a distinct sampling process as follows Song et al. [2020a]:\n$x_{t-1} = \\sqrt{\\overline{\\alpha}_{t-1}}f_\\theta(x_t, t) + \\sqrt{1-\\overline{\\alpha}_{t-1}} \\cdot \\sigma_t\\epsilon_\\theta(x_t, t) + \\sigma_tz$ (6)\n, where $z \\sim N (0, I)$, and $f_\\theta (x_t, t)$ is the prediction of $x_0$ at t given $x_t$ and $\\epsilon_\\theta (x_t, t)$:\n$f_\\theta (x_t, t) := \\frac{x_t - \\sqrt{1 - \\overline{\\alpha}_t}\\epsilon_\\theta (x_t, t)}{\\sqrt{\\overline{\\alpha}_t}}$ (7)\nEq.(7) is derived from Eq.(4). In DDIM, the reverse diffusion process is deterministic when $ \\sigma_t = 0$,\nresulting in its ability to generate images stably.\nTraining diffusion models involves optimizing the parameters $ \\theta$ of the reverse process, which tries to\nminimize the difference between the noise added in the forward step t and the predicted noise in the\nreverse step t, in simple terms, makes the generated image similar to the input one. The optimization\nobjective can be expressed as Ho et al. [2020]; Nichol and Dhariwal [2021]:\n$\\min_\\theta \\mathbb{E}_{t,x_0,\\epsilon_t} [||\\epsilon_t - \\epsilon_\\theta (x_t, t) ||^2]$ . (8)"}, {"title": "Differential Privacy", "content": "Conditional diffusion models Dhariwal and Nichol [2021]; Ho and Salimans [2022]; Chung et al.\n[2022c]; Yu et al. [2023]; He et al. [2023]; Yang et al. [2024] introduce the given condition or\nmeasurement y with an additional likelihood term p(xt|y). Considering the Bayes rule,\n$\\nabla_{x_t} \\log p(x_t|y) = \\nabla_{x_t} \\log p(x_t) + \\nabla_{x_t} \\log p(y|x_t)$. (9)\nLeveraging the diffusion model as the prior whose reverse SDE is expressed as Eq.(3), it is straight-\nforward to obtain the reverse SDE of the conditional diffusion models, namely the reverse diffusion\nsampler for sampling from the posterior distribution:\n$dx = -(\\frac{\\beta(t)}{2}x - \\beta(t) (\\nabla_{x_t} \\log p(x_t) + \\nabla_{x_t} \\log p(y|x_t))) dt + \\sqrt{\\beta(t)}dw$ (10)\n, where p(xt) is the prior that is determined by $ \\epsilon_\\theta (x_t, t)$. For various scientific problems, we have a\npartial measurement y that is derived from xt, and the distribution is denoted as p(y|xt).\nDifferential privacy Dwork et al. [2014] is a mathematical framework designed to provide strong\nprivacy guarantees for individual data entries in a dataset. It achieves this by ensuring that the output\nof a computation does not significantly change when any single data entry is modified. This ensures\nthat sensitive information about individuals remains protected, even when an adversary has access to\nthe output of the computation.\nDefinition 1 (Differential privacy Dwork et al. [2014]). A randomized mechanism M satisfies\n(\u03b5, \u03b4)-differential privacy (DP) if for any two adjacent datasets D and D' that differ by a single\nindividual's data, and for all S \u2286 Range(M)(the set of all possible outputs of M), the following\ninequality holds: Pr[M(D) \u2208 S] < e \u2022 Pr[M(D') \u2208 S] + \u03b4.\nIn the context of machine learning, gradients are computed during the training process to update the\nmodel's parameters. To protect these gradients, differentially private noise (such as Gaussian noise)\nis added to the gradients before they are used for the parameter updates Abadi et al. [2016]; Zhu et al.\n[2019]; Meng et al. [2024]; Wang et al. [2022]; El Mestari et al. [2024]. The method is well known\nas Differentially Private Stochastic Gradient Descent (DP-SGD). The typical four steps undergo: (1)\nCompute the gradient of the loss function with respect to model parameters; (2) Clip the gradient to\nlimit its sensitivity; (3) Add Gaussian noise to the clipped gradient; (4) Update the model parameters\nusing the noisy gradient. Formally, for a gradient g with sensitivity \u2206g, the DP-SGD update model's\nparameters with noisy gradients g' = g + N(0, \u03c3\u00b2I), where o is chosen via Lemma 1 according to\nthe Gaussian mechanism formula to ensure (\u20ac, \u03b4)-DP."}, {"title": "Gaussian Mechanism for Differential Privacy", "content": "Lemma 1 (Gaussian Mechanism for Differential Privacy Dwork et al. [2014]). Let $M : D \\rightarrow \\mathbb{R}^k$\nbe a function with l2-sensitivity $\u25b3_2M = ||M(D)\u2212M(D')||$ which measures the maximum change in\nthe Euclidean norm of M for any two adjacent datasets D and D' that differ by a single individual's\ndata. The Gaussian Mechanism adds noise drawn from N(0, \u03c3\u00b2I) to the output of M and provides\n(\u03b5, \u03b4)-differential privacy if $\\sigma > \\frac{\u25b3_2M \\cdot \\sqrt{2 \\log (1.25/\u03b4)}}{\u03b5}$"}, {"title": "Reconstruction of Private Images", "content": "Machine learning models, particularly deep learning models, are often susceptible to various types of\nattacks that can compromise the confidentiality of the training data. One such attack is the image\nreconstruction attack, which leverages leaked gradients during the training process to steal private\nimages. When a model is trained using gradient-based optimization methods, such as stochastic\ngradient descent (SGD), the gradients of the loss function with respect to the model parameters are\ncomputed and used to update the parameters. These gradients inherently carry information about the\ntraining data. If an attacker gains access to these gradients, they can potentially reverse-engineer the\nimages that were used to compute them.\nMathematically, consider a machine learning model F(x; W) parameterized by W. Given a private\ndata x \u2208 X, the original gradient g0(x) is given by:\n$g_0(x) = \\nabla_w F(x; W)$. (11)\nAssuming the attacker obtains g0(x) and has access to F(x; W) (i.e., the attacker can calculate the\ngradient of an arbitrary data by feeding F(x; W) with the data x), the attacker can iteratively update\na dummy image x' which is randomly initialized to minimize the difference between the gradient of\nx' and the leaked gradient g0(x), effectively reconstructing the original image x.\nTo mitigate the privacy risks posed by gradient leakage, differential privacy (DP) can be employed.\nDP ensures that the inclusion or exclusion of a single training sample does not significantly affect the\nmodel's output, thereby providing privacy guarantees for individual data record. In the context of\nprotecting private gradients, DP can be implemented by adding noise to the gradients. In this paper,\nwe consider the Gaussian mechanism, where differentially private noise is sampled from the Gaussian\ndistribution. Specifically, the noisy gradient, denoted as g = g0(x) + N(0, \u03c3\u00b2\u0399), where N (0, \u03c3\u00b2\u0399)\nrepresents Gaussian noise, with o\u00b2 being the noise scale determined by the privacy parameters in\nDP. The Gaussian mechanism in DP has been demonstrated to be effective in mitigating the risk of\nimage reconstruction attacks Zhu et al. [2019]; Meng et al. [2024]; Wang et al. [2022]; El Mestari et\nal. [2024].\nExisting work Meng et al. [2024] has shown that a pre-trained diffusion model can be fine-tuned\nvia gradient guidance, allowing the fine-tuned diffusion model to recover high-resolution private\nimages. In our view, $ \\nabla_w F(x; W)$ can be considered a mapping x \u2192 g, and the leaked gradients\ng can serve as the measurement or condition. In this section, we broaden the discussion to explore\nhow an attacker might guide a pre-trained diffusion model $ \\epsilon_\\theta (x)$ to generate images through a series\nof denoising reverse steps if they capture the gradients g without fine-tuning diffusion model itself,\nwhich adds computational complexity. Specifically, we investigate how an attacker could establish\na conditional diffusion model $ \\epsilon_\\theta (x|g)$. Moreover, we examine the adversarial relationship between\nthe data reconstruction capability of conditional diffusion models and the noise-adding ability of\ndifferential privacy. In other words, we analyze how the noise scale o\u00b2 affects the data reconstruction\nability of diffusion models.\nWhen establishing the conditional diffusion model, the attacker designs an attack loss function\n$L_{attack} (g(x), g; \\theta)$, where x \u223c $ \\epsilon_\\theta (x|g)$, to measure the difference between the gradients g(x) and g.\nTypically, the attack loss is symmetric, meaning that $L_{attack} (g(x), g; \\theta) = L_{attack} (g, g(x); \\theta)$. The\nattacker's objective is to establish the conditional diffusion model $ \\epsilon_\\theta (x|g)$ and generate x \u223c $ \\epsilon_\\theta (x|g)$\nthat minimizes $L_{attack} (g(x), g; \\theta)$. For simplicity, $L_{attack} (g(x), g; \\theta)$, L (g(x)), and L (x) are used\ninterchangeably to represent the attack loss, which is a composite function. The properties of L (g(x))\nimpact the quality of image reconstruction. The adversarial relationship between the denoising ability\nof $ \\epsilon_\\theta (x|g)$ and the noise-adding ability of differential privacy is linked to the properties of L (g(x)),\nwhich we discuss further in this paper."}, {"title": "Adversarial Impacts of Differential Privacy and Conditional Denoising Diffusion Models", "content": "In this section, we discuss how to establish the conditional diffusion attack model $ \\epsilon_\\theta (x|g)$ and the\nimpacts of the scale of differentially private noise on the reconstruction quality if g is injected with\ndifferentially private Gaussian noise. When establishing the conditional diffusion attack model\nvia Eq.(10), it is difficult to derive $p(y = g|x_t)$ since $x_t$ is time-dependent. However, a tractable\napproximation for $p(y = g|x_t)$ is not difficult to design. Given the posterior mean $x_0 = f_\\theta (x_t,t)$\nthat can be effectively computed at the intermedia steps in Eq.(7), we use the following approximation:\n$P_\\theta(g|x_t) \u2248 P_\\theta(g|x_0) = P_\\theta(g|x_0(x_t))$, which is related to Jensen's inequality where Jensen gap\nquantifies the approximation error.\nDefinition 2 (Jensen Gap Chung et al. [2022b]). Let x be a random variable with distribution p(x).\nFor some function f that may or may not be convex, the Jensen gap is defined as\n$J(f,x \u223c p(x)) = E[f(x)] \u2212 f(E[x])$ (12)\n, where the expectation is taken over p(x).\nThe Jensen gap measures the approximation error introduced by the approximation $P_\\theta(g|x_t) \u2248 P_\\theta(g|x_0) = P_\\theta(g|x_0(x_t))$. A smaller Jensen gap indicates a lower approximation error, which in\nturn suggests improved reconstruction quality of the conditional diffusion attack model $ \\epsilon_\\theta (x|g)$.\nDPS-Based Conditional Diffusion Attack Model (DPS-based Method). Under differential privacy,\nthe noisy gradient g follows a Gaussian distribution. Specifically, the gradient g(x) with respect\nto the reconstructed image x should also adhere to a Gaussian distribution. For ease of analysis,\nwe assume $g(x) \u223c N(\\hat{g}_0(x), \u03c3\u00b2I)$. The attacker's objective is to identify an image x such that\n$g_0(x)$ approximates g0. Given the assumption that $g(x) \u223c N(\\hat{g}_0(x), \u03c3\u00b2I)$, we can derive the\nconditional diffusion model $ \\epsilon_\\theta (x|g)$ based on Diffusion Posterior Sampling (DPS) Chung et al.\n[2022b]. Algorithm 1 outlines the reconstruction process, and Theorem 1 provides the upper bound\nof the Jensen Gap. The key guidance is step 8 in Algorithm 1.\n$x_{t-1} \u2190 x_{t-1} - \\frac{$\\beta_t}{\\sqrt{1 - \\overline{\\alpha}_t}}$ \\frac{x_t}{\\sigma_t} ||g - g_0(x_0)||$ (13)"}, {"title": "DPS-Based Reconstruction Method", "content": "Algorithm 1: DPS-Based Reconstruction Method\nInput: N, g, {$ \\beta_t$}$_{t=1}^{N}$, {$ \\overline{\\alpha}_t$}$_{t=1}^{N}$, F(x; W)\n$x_N \u223c N(0, I)$\nfor t = N; t - -; t \u2265 1 do\n$x_0 \u2190 f_\\theta(x_t,t)$\n$x_0 \u2190 \\frac{1}{\\sqrt{\\overline{\\alpha}_t}}(x_t - (1 - \\overline{\\alpha}_t)\\hat{x})$\n$z \u223c N(0, I)$\n$x_{t-1}\u2190 \\sqrt{\\overline{\\alpha}_{t-1}} \\frac{\\sqrt{\\overline{\\alpha}_{t}}}{\\sqrt{1 - \\overline{\\alpha}_t}} x_t + \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}\u03b2_t}{\\sqrt{1 - \\overline{\\alpha}_t}} x_0 + \u03c3_tz$\n$g_0(x_0) = \\nabla_w F(x_0; W)$\n$x_{t-1} \u2190 x_{t-1} - \\frac{$\\beta_t}{\\sqrt{1 - \\overline{\\alpha}_t}}$ \\frac{x_t}{\\sigma_t} ||g - g_0(x_0)||$ (8)\nreturn $x_0$\nTheorem 1 (Upper Bound of Jensen Gap of Reconstruction Error). Considering a machine learning\nmodel F(x; W) that is equipped with differentially private Gaussian noise N (0, \u03c3\u00b2\u0399). Assuming the\nattacker captures the noisy gradient g = g0(x0) + N(0, \u03c3\u00b2I), where g0(x0) = $ \\nabla_wF(x_0; W)$ and\nx0 is some private image. The attacker establishes a conditional diffusion attack model $ \\epsilon_\\theta (x|g)$ by\nAlgorithm 1 under the assumption that $P_\\theta(g|x_t) \u2248 P_\\theta(g|x_0) = P_\\theta(g|x_0(x_t))$ where $x_0(x_t)$ is the\nreconstructed version of x0. Under these conditions, the reconstruction error can be quantified by\nJensen Gap, which is upper bounded by:\n$I (\\nabla_w F(x; W), P_\\theta(x_0|x_t)) \\le \\frac{d}{\\sqrt{2\u03c0\u03c3\u00b2}} ||\\nabla_x\\nabla_wF(x; W) || \\int ||x_0 - x0||dP_\\theta(x_0|x_t)$ (14)"}, {"title": "Proof of theorm 1", "content": "Proof. Analogous to the proof of Theorem 1 in Chung et al. [2022b", "Gap\nis": "n$J (\\nabla_wF(x; W)", "by": "n$RV = \\mathbb{E}_{v \\sim p, \\mathbb{E}_{x \\sim pdata (x)} ||\\nabla_x (v^T\\nabla_wF(x; W)) || \\\\\n\u2248 \\frac{1}{NM} \\sum_{i=1}^{N}\\sum_{j=1}^{M} ||\\nabla_x (v^T\\nabla_wF(x_i; W)) ||$ (17)\n, where pdata(x) is the distribution of the private dataset X. In the experimental section, we will\nthoroughly discuss the relationship between the vulnerability of different attacked models and the\nvalue of RV. This analysis provides insight into why different models exhibit varying levels of\nresistance when subjected to the same reconstruction attack. And"}]}