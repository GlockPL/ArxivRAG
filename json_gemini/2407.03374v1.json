{"title": "An Outline of Prognostics and Health Management Large Model: Concepts, Paradigms, and Challenges", "authors": ["Laifa Tao", "Shangyu Li", "Haifei Liu", "Qixuan Huang", "Liang Ma", "Guoao Ning", "Yiling Chen", "Yunlong Wu", "Bin Li", "Weiwei Zhang", "Zhengduo Zhao", "Wenchao Zhan", "Wenyan Cao", "Chao Wang", "Hongmei Liu", "Jian Ma", "Mingliang Suo", "Yujie Cheng", "Yu Ding", "Dengwei Song", "Chen Lu"], "abstract": "Prognosis and Health Management (PHM), critical for ensuring task completion by complex systems and preventing unexpected failures, is widely adopted in aerospace, manufacturing, maritime, rail, energy, etc. However, PHM's development is constrained by bottlenecks like generalization, interpretation and verification abilities. Presently, generative artificial intelligence (AI), represented by Large Model, heralds a technological revolution with the potential to fundamentally reshape traditional technological fields and human production methods. Its capabilities, including strong generalization, reasoning, and generative attributes, present opportunities to address PHM's bottlenecks. To this end, based on a systematic analysis of the current challenges and bottlenecks in PHM, as well as the research status and advantages of Large Model, we propose a novel concept and three progressive paradigms of Prognosis and Health Management Large Model (PHM-LM) through the integration of the Large Model with PHM. Subsequently, we provide feasible technical approaches for PHM-LM to bolster PHM's core capabilities within the framework of the three paradigms. Moreover, to address core issues confronting PHM, we discuss a series of technical challenges of PHM-LM throughout the entire process of construction and application. This comprehensive effort offers a holistic PHM-LM technical framework, and provides avenues for new PHM technologies, methodologies, tools, platforms and applications, which also potentially innovates design, research & development, verification and application mode of PHM. And furthermore, a new generation of PHM with Al will also capably be realized, i.e., from custom to generalized, from discriminative to generative, and from theoretical conditions to practical applications.", "sections": [{"title": "1. Introduction", "content": "Prognostics and Health Management (PHM) serves as an essential mechanism for a complex system to mitigate catastrophic incidents, ensure mission reliability, enhance operational efficacy, and curtail maintenance expenditures. By seamlessly integrating into the entire life cycle of a complex system, PHM has transpired as an essential fulcrum, wielding a pivotal influence reminiscent of a trump card. The application of PHM technology has increased the sortie rate of advanced fighters such as F-35 by 25%, reduced the maintenance manpower by 20%-40%, and reduced the use and support costs by 50% (Chen L et al., 2016). Furthermore, helicopters like the UH-60L Black Hawk experienced an elevation in operational readiness by 27%, a decrement in unplanned maintenance by 52%, and an overall decline in repair tasks by 17% (Baozhen Z & Ping W, n.d.). Large manufacturing conglomerates, such as Boeing, have also reaped the benefits of PHM by curtailing losses due to malfunctions by an impressive 15% (M D et al., 2016) Hence, the intrinsic value of PHM in fortifying mission success, elevating safety measures, paring down maintenance overheads, and amplifying efficacy within the industry cannot be understated.\nDue to the influence of complex operating conditions and harsh environments, existing PHM methods face a series of impediments to achieving satisfactory results. Non-ideal characteristics of PHM data and knowledge(J. Peng et al., 2022; S. Wang et al., 2023; Zio, 2022), inadequate generalization capabilities of PHM models(Chu & Zhu, 2021; Tian et al., 2022), and the dependency of PHM capabilities on specific scenarios(Y. Sun, Lu, et al., 2023; Ye et al., 2020) have resulted in a plethora of pain points for current PHM technological approaches. Although potential interim solutions to a minority of these issues exist in contemporary research, these solutions often exhibit strong customization, weak generalization, and are accompanied by exorbitant costs. The improvements they bring are frequently marginal, failing to meet the capability demands of PHM technologies in application scenarios. Furthermore, the rapid emergence of various new systems and products, along with the application in new and intricate scenarios, exacerbates the conditions for PHM technological deployment. Yet, there are demands for multi-scale PHM capabilities that span from individual components to system clusters(W. Yang et al., 2021), as well as the rapid formation of PHM system capabilities. This juxtaposition leads to an inherent contradiction and conflict between traditional PHM methods and these new requirements. There is an urgent need to explore a new PHM methodological framework that holistically addresses the series of bottlenecks confronting PHM and fulfills the rapid capability formation demands of PHM in the current era.\nArtificial General Intelligence (AGI)(Alattas et al., 2021; Iman et al., 2021; Nti et al., 2022) denotes an artificial intelligence that possesses intelligence equivalent to or surpassing that of humans, capable of manifesting all intelligent behaviors characteristic of average humans. It is quintessential to the realization of AI. In recent years, Large Model technologies, exemplified by Large Language Model (LLM) (Devlin et al., 2019; Openai et al., n.d.; Vaswani et al., 2017), have achieved remarkable strides, unveiling colossal potential towards the actualization of AGI. LLM employ generative modeling on vast textual corpora. When both dataset scale and model size reach a certain inflection point, they can exhibit astounding generative prowess(Ganguli et al., 2022). LLM, with ChatGPT(Brown et al., 2020; Lewis et al., 2020; Radford et al., n.d.) being a paragon, employ techniques like instruction alignment, reinforcement learning(Uc-Cetina et al., 2023), fine-tuning(P. Liu, Yuan, et al., 2023), and thought chain for training and adjustment(Wei, Wang, et al., 2022). This equips them with robust generalization, inference, decision-making, and generative capabilities. In applications like human-machine dialogue and Q&A, their performance rivals or even surpasses human-level competence. Preliminary applications of these models have been explored in specialized fields such as healthcare(J. K. Kim et al., 2023; Lokesh et al., 2022), finance(Pawan Kumar Rajpoot & Ankur Parikh, 2022; H. Yang et al., 2023), manufacturing(Lykov & Tsetserukou, 2023; L. Wu, Qiu, et al., 2023), and education(Jeon & Lee, 2023; Tsai et al., 2023).. Currently, the philosophy and technology behind Large Model are being researched and employed across myriad fields. They are upending conventional specialized field research and application paradigms, ushering in a novel technological revolution. This paradigm shift is profoundly transforming traditional technological field development patterns and the modus operandi of human production.\nGiven this context and leveraging the capabilities of generative Large Model - notably their inference(Zheng et al., 2023), robust generalization(Igl et al., 2019), and multi-modal information analysis(Lyu et al., 2023) along with the current efficacies of Large Model across various fields, there is a pressing need to address the bottlenecks and practical requirements pertaining to PHM technology. Aligning with global technological advancements, there's an imperative for in-depth integrative research between Large Model and PHM technologies. The overarching objective is to augment the core PHM capabilities and the proficiency of tasks throughout the entire life cycle of PHM systems. This would involve surmounting challenges associated with constructing, training, optimizing, and deploying PHM-LM (Prognosis and Health Management Large Model). By synergizing applications of Large Model, collaborating between ordinary models and Large Model, and emphasizing specialized field Large Model development, there's an opportunity to refine current PHM operational frameworks, enhance PHM algorithm competencies, and bolster downstream PHM tasks. This would, in turn, disrupt conventional PHM design, research and development, validation, and application paradigms. It paves the way for the inception of novel technologies, methodologies, platforms, and applications under the Large Model system framework for PHM. Ultimately, the ambition is to catalyze a paradigm shift in PHM technology: transitioning from bespoke to generic solutions, from discriminative to generative approaches, and from idealized settings to more pragmatic, real-world implementations.\nIn light of the practical demands associated with research on the convergence of Large Model and PHM, this paper integrates the technological advantages of Large Model, the systems engineering process throughout the entire life cycle of PHM, and the challenges faced by PHM. Building on a systematic review of the extant bottlenecks and challenges in PHM, coupled with a profound analysis of the merits of Large Model technology, we innovatively introduce a new concept of the PHM-LM. Additionally, we spotlight three prototypical innovative paradigms for future technical research and application advancements in this realm:"}, {"title": "2. Systematic Analysis of the Current Challenges and Bottlenecks in the PHM Field", "content": "In this paper, the PHM system is defined as one of the subsystems delivered with the system, possessing system-level functional tasks. Consequently, based on the real-world requirements of the PHM system and centered on the system engineering process (as shown in Fig. 2-1), this paper comprehensively analyzes the entire life cycle process of the PHM system(Ma et al., 2022; Tidriri et al., 2016). It systematically examines the series of significant issues faced by the PHM system and the PHM algorithm & model at different stages. Subsequently, the challenges confronting PHM are summarized and organized."}, {"title": "2.1 Analysis and Description of PHM Issues Throughout Life Cycle", "content": "Surveying the development process structures of various advanced PHM systems, the entire life cycle of a PHM system can be broadly divided into five major phases. This section analyzes the essential tasks and work items to be completed in each phase and systematically examines the unresolved issues and capabilities that need enhancement at each stage.\nPhase One: Conceptual Design Phase\nDuring the conceptual design phase, preliminary PHM system design concepts are formed based on PHM design requirements. These concepts undergo validation and evaluation tasks, resulting in reports related to the PHM system development validation and evaluation analysis of the PHM system validation. This phase will produce overarching validation reports for the PHM system, including architecture validation, component validation, and software design validation. Key technological maturity, risk analysis and countermeasures, safety, and economic indicators of the PHM system are evaluated to complete the conceptual design. The design and validation work in the conceptual design phase requires a wealth of PHM field knowledge as support. The efficiency of knowledge utilization is a significant factor affecting the efficiency of this phase.\nPhase Two: Preliminary Design Phase\nIn the preliminary design phase, based on the initial overall design, subsystem design, support solutions, and data/software system design, the core PHM functions such as testability, monitoring, detection, localization, isolation, evaluation, and prediction are designed. Subsequently, interface dependencies are allocated and docked based on the design content, completing the overall functionality, safety, economic, and risk assessments, laying the foundation for the detailed design of the PHM system.\nPhase Three: Detailed Design Phase\nDuring the detailed design phase, work is carried out on the technical solution of the PHM system, functional allocation prediction, etc., considering experimental research on the principles and functions of the PHM system. The detailed design is evaluated based on the experimental validation of principles and functions, and the prototype design of the PHM system is completed, including the prototype, simulation, and validation platforms.\nPhase Four: Development Phase\nAfter integrating the detailed design of the PHM system and the development standards and specifications, the construction of the PHM system starts from the underlying algorithm and gradually extends to the component level, sub-system level, and system level. Technical documents are written, providing related technical materials, help documents, and training materials for the PHM system.\nPhase Five: In-Service Phase\nDuring the in-service support phase, the PHM system serves alongside the system, realizing its PHM functions. As a large amount of usage data is generated, many iterative updates and new requirements will emerge, and the PHM system will need to undergo gradual iterative updates during the in-service support process."}, {"title": "2.2 Bottleneck Analysis of PHM", "content": "Based on the analysis in Section 2.1, to address the critical issues in the PHM field, break through the technological bottlenecks of PHM, verify and enhance the capabilities of PHM systems, and promote comprehensive development in the PHM field, this paper elucidates the challenges still faced by PHM technology from two perspectives: the PHM algorithm & model layer and the PHM system layer. This approach provides references at different levels for readers with varying research interests and needs, as illustrated in Fig. 2-2."}, {"title": "2.2.1 PHM Algorithm & model Layer", "content": "Challenge 1: How to lower the development threshold for algorithms & models?\nAdvanced Al technologies and big data processing capabilities have brought significant improvements to the capabilities of the PHM field. However, due to factors such as object differences, data constraints, knowledge disparities, and variations in usage environments and conditions, the threshold for developing customized PHM models is gradually increasing. Coupled with the explosive growth of current PHM algorithms & models, tasks such as multimodal data processing design, algorithm & model selection, underlying code writing, and model hyperparameter configuration and iterative updates pose higher technical requirements for users. There lies a key challenge for the widespread application of the PHM field on methods to achieve optimal algorithm & model recommendations, automatic generation of underlying code, and adaptive optimization updates of model hyperparameters to lower the PHM model development threshold.\nChallenge 2: How to effectively carry out algorithm & model validation?\nIn the PHM field, the capability validation of algorithms & models is crucial for ensuring PHM capabilities. However, the lack of data and knowledge during the design and development phase severely restricts the progress of validation tasks. Against this backdrop, how to innovate validation approaches and methods, establish auxiliary conditions for validation, and form a standardized, efficient, and trustworthy PHM algorithm & model validation system is a significant challenge that the PHM field urgently needs to address.\nChallenge 3: How to enhance the logicality and credibility of algorithms & models?\nThe low credibility of the black-box algorithms & models widely used in the PHM field is an undeniable shortcoming in the field. With ample field-specific knowledge support, figuring out how to supplement black-box algorithms with explanatory methods in the PHM field, thereby enhancing the logicality and credibility of algorithms & models, is a key challenge for the development of high-trust PHM.\nChallenge 4: How to improve the generalization performance of algorithms & models?\nWith the technological advancement in the PHM field, the requirements of PHM are continuously increasing. The challenge lies in identifying the common characteristics of different object PHM algorithms & models and the inherent common knowledge among numerous objects and systems. This ensures that the PHM algorithms & models maintain a high precision level across different objects, conditions, data formats, and constraint scenarios. Researching and developing PHM algorithms & models with strong generalization and good generalization, or exploring technical methods to ensure and enhance the generalization capability of PHM algorithms & models, is a key challenge for the widespread adoption ofPHM technology in the future."}, {"title": "2.2.2 PHM System Layer", "content": "Challenge 5: How to efficiently utilize knowledge in the PHM field?\nWith the development of the PHM field to date, the reliance on expert experience has become increasingly pronounced. The role of PHM knowledge spans various stages, including the refinement of PHM system design requirements, preliminary design (including algorithm & model design), detailed design (including algorithm & model design), algorithmic model validation, functional validation, and system integration validation. In an environment where multi-modal and multi-product data and knowledge are rapidly expanding, the challenge lies in enhancing the utilization efficiency of diverse knowledge and multi-modal data, replacing expert individual decision-making with question-answering reasoning based on generative model technology, increasing the connections between vast document knowledge, and consolidating fragmented knowledge with multi-modal data to form a comprehensive PHM knowledge base. This is a significant challenge for the future inheritance and development of the PHM field.\nChallenge 6: How to lower the design threshold for PHM systems?\nPHM system design often proceeds in parallel with the main system design and is subject to various constraints such as resources and deployment. Similar to PHM algorithm & model design, the design work becomes particularly complex due to constraints like monitoring objects, data transmission conditions, and computational resources. However, given the rapidly growing demand for PHM system development, relying solely on expert experience for PHM system design can no longer meet the efficiency and quality requirements of PHM system development. The challenge is to assist in PHM system design, construct generative PHM system design, lower the PHM system design threshold, and enhance the efficiency and quality ofPHM system design.\nChallenge 7: How to establish a PHM validation system?\nIn the PHM field, the validation and determination of solutions are crucial for ensuring that the PHM system achieves its intended functions and is developed with low consumption and high efficiency. However, the lack of data and knowledge during the design and development phase severely restricts the progress of PHM system validation tasks. Against this backdrop, the challenge lies in innovating validation methods, establishing auxiliary conditions for validation, and forming a standardized, efficient, and trustworthy PHM validation system.\nChallenge 8: How to enhance the basic performance and generalization of PHM systems?\nWith the technological advancement in the PHM field, the basic performance requirements for PHM system are continuously increasing. The challenge is to identify the common issues of different object PHM systems, avoid the awkward situation of having to develop a PHM system from scratch for different objects, and design a PHM system with strong generalization capacity that is widely applicable to various systems.\nChallenge 9: How to enhance the interaction efficiency and performance of PHM systems?\nWith the emergence of intelligent application terminals and diversified interaction forms, there's vast potential for the development of application forms and human-machine interaction models in the PHM field software platform. Simultaneously, user feedback from human-machine interactions and the vast generation of real data provide an essential foundation for optimizing and updating the performance of PHM algorithms & models during their service phase. Breaking away from the current traditional human-machine interaction forms, innovating intelligent PHM applications, and exploring feasible solutions for the adaptive optimization and enhancement of PHM algorithm & model performance during their service phase based on human-machine interaction feedback and real data are significant challenges for technological innovation in the PHM field."}, {"title": "2.3 Summary", "content": "Starting from the PHM system engineering process, this section systematically analyzes the various issues that PHM still faces in each development phase and summarizes the bottlenecks and challenges encountered. These challenges and problems can be categorized into two scenarios:\n(1) Scenario One: Issues such as the lack of generalization in PHM solutions and algorithms & models, low utilization rate of field knowledge, and insufficient optimization of the PHM system. Although there are some solutions or alternative approaches to these issues at present, the methods to address them are highly customized. They have weak generalization capabilities and often come with very high costs. The improvements they bring are often limited, necessitating further research and optimization.\n(2) Scenario Two: Problems like the difficulty in conducting validations due to a lack of data and knowledge, and the poor interpretability resulting from the extensive use of black-box models, are currently insurmountable given the existing technological conditions. These issues are significant impediments to the development of the entire PHM field. Failing to address these problems will severely impact the future growth of the PHM field. It's imperative to innovate from a technological perspective and fundamentally revolutionize the theoretical framework.\nBuilding on this foundation, Section 2.2 approaches from two different perspectives: the PHM algorithm & model level and the PHM system level. It identifies 9 major challenges facing the future development of the PHM field, focusing on knowledge utilization, logical supplementation, generalization capability, validation systems, development simplification, and interaction innovation. This is done to address more practical problems in PHM engineering and pave the way for technological innovation in the PHM field."}, {"title": "3. Analysis of The Current Status and Advantageous Features of Large Model Research", "content": "Large Model (LM) is a type of artificial intelligence model that imports massive amounts of unlabeled data into models with billions of parameters for large-scale pre-training, thereby training the model to adapt to various downstream tasks. Large Model can be divided into general field Large Model and specialized field Large Model. General field Large Model refers to large-scale pre-trained models handling multiple fields and tasks, while specialized field Large Model targets specific sectors or industries offering higher expertise and better performance for specific tasks. Currently, the most widely used Large Model is the LLM in the general field. This chapter, based on introducing the main development trajectory of Large Model, focuses on analyzing advantages and features as well as new technologies and ideas in the processes of construction and application. In particular, it elaborates on the advantages of the LLM in Large Model, analyzing their feasibility and rationality when applied to the field of PHM. All in all, this part sets the foundation for a seamless integration with EHM and addresses a series of issues in PHM."}, {"title": "3.1 The Overview of Large Model", "content": "3.1.1 Large Model Principle\n3.1.1.1 Introduction to Large Model\nWith the rise of deep learning and the enhancement of computational capabilities, researchers have begun to focus on how to build models of larger scale to tackle more complex problems. Consequently, the concept of Large Model was introduced and has gradually become a focal point of development in today's technology landscape(Lourenco & Eugenio, 2019). In the era of IT, with data volumes growing explosively daily and the computational power of computers continually increasing, a key research direction in computational science today is how to handle unstructured and multi-modal data(Lipnicka & Nowakowski, 2023; Mayer-Sch\u00f6nberger & Cukier, 2013). These data revolutions also enable Large Model to have access to richer training resources and to process increasingly intricate data. In 2017, Google introduced the Transformer architecture based on the self-attention mechanism(Vaswani et al., 2017) scaling up to hundreds of millions of parameters. Building on the Transformer, in 2018, the introduction of the BERT model(Devlin et al., 2018) saw Large Model breaking the 300 million parameter barrier for the first time. With the emergence of the GPT mode(Brown et al., 2020), Large Model has garnered growing attention.\nLarge Model utilizes massive data to train an extensive number of parameters, thereby handling complex, abstract tasks that traditional ML and DL can't achieve. The objective of designing Large Model is to enhance their representational capabilities and performance, capturing data patterns and regularities more effectively when dealing with intricate tasks. Initially, Large Model was intended to solve issues in natural language processing (NLP). However, as it continued to evolve, its application has expanded far beyond just the NLP realm. Large Model can fit training data better, thereby enhancing its accuracy and generalization capabilities. Additionally, it can learn more intricate features, leading to better performance on complex tasks. As a result, Large Model has found applications in various fields, achieving significant breakthroughs in areas such as NLP(X. Wang et al., 2023), computer vision(Kirillov et al., 2023), voice recognition(Y. Zhang et al., 2023), and even in fields like law and medicine(Lu et al., 2021). In short, Large Model is playing an increasingly pivotal role in contemporary technology.\n3.1.1.2 Large Model Basic Architecture\nCurrently, the majority of Large Model adopt the Transformer architecture, which overcomes the shortcomings of RNNs, previously the most commonly used in the field of NLP. As a result, Transformers are widely utilized by numerous LLM and another Large Model. Apart from the extensively used Transformer architecture, new architectures, such as RWKV, have been proposed, offering fresh perspectives for the construction of Large Model.\n(1) Transformer\nThe Transformer is the most popular architecture used in Large Model today, proposed by Google researchers in 2017. The Transformer comprises Encoder and Decoder modules. The encoder consists of a self-attention layer and a feed-forward neural network, employing residual connections and layer normalization within the layer. The encoder serves to transform input sequence data into a set of representation vectors. Meanwhile, the decoder also uses a combination of the self-attention mechanism and feed-forward neural networks, incorporating two attention layers and one feed-forward network, also employing residual connections and layer normalization. The decoder is responsible for outputting the predicted sequence. The Transformer effectively addresses the long-term dependency issue, enhances the model's parallelism, is broadly applied across various fields, and laid the foundation for models such as BERT and GPT(Devlin et al., 2019; Y. Liu et al., 2019; Openai et al., n.d.).\n(2) RWKV\nRWKV is an improved recurrent neural network (RNN) architecture. It overcomes the traditional drawbacks of RNNs that make them challenging to train on long sequences due to the vanishing gradient problem, as well as the Transformer architecture's downside of consuming a significant amount of memory during computation(B. Peng et al., 2023). Embodying the advantages of both the Transformer and RNN, RWKV can be efficiently trained and also support fast inference, making it a valuable architecture in the current landscape.\n(3) RetNet\nJointly introduced by Microsoft Research and Tsinghua University(Y. Sun, Dong, et al., 2023), RetNet is a novel architecture composed of multiple identical blocks. Each RetNet block encompasses a multi-scale retention (MSR) module and a feed-forward neural network. Furthermore, RetNet introduces a multi-scale retention mechanism as a substitute for the self-attention mechanism, supporting both parallel and recurrent computation modes. RetNet demonstrates excellence in long-sequence modeling, and its implementation and deployment are relatively straightforward. It provides a new direction and breakthrough in the design of Large Model architectures.\n3.1.1.3 Large Model Key Technology\n(1) Pre-training Data Collection\nTo train Large Model, knowledge needs to be learned from vast amounts of data and stored in the model's parameters. In order to enhance the efficiency of model training, high-quality data must be used(Penedo et al., 2023). Given the significant presence of noisy samples in real data, it becomes essential to filter this data. Currently, filtering methods mainly include model-based methods and heuristic-based methods. After filtering, the data needs to be de-duplicated, which can be categorized into two methods: fuzzy deduplication (such as SimHash(Detecting Near-Duplicates for Web Crawling Proceedings of the 16th International Conference on World Wide Web, n.d.)) and exact substring matching deduplication.\n(2) Model Architecture Design\nTypically, when training Large Model, increases in both data and model scale will enhance training results. However, the larger the model, the more training resources it occupies. Therefore, it's essential to employ DL architectures that can compute efficiently when building Large Model. Currently, Transformers and their variants are the most widely used deep learning architectures for building Large Model. Additionally, the size of the model should match the scale of the training data to prevent the waste of computational resources(Hoffmann et al., 2022).\n(3) Fine-tuning for Downstream Tasks\nAfter pre-training a Large Model, in order to adapt it to downstream tasks, it requires fine-tuning. This involves introducing specific downstream task data, allowing the Large Model to continue training on top of the pre-trained weights until it meets the requirements of the downstream task. Efficient fine-tuning methods are crucial for maximizing the performance of Large Model. At present, common fine-tuning methods include LoRA(E. J. Hu et al., 2021), Adapter(R. Zhang et al., 2023), Prefix-tuning(X. L. Li & Liang, 2021), P-turning(X. Liu et al., 2021), Prompt-tuning(Lester et al., 2021), and RLHF(Christiano OpenAI et al., 2017), among others."}, {"title": "3.1.2 Large Model Advantage Analysis", "content": "(1) Generalization Ability\nGeneralization ability refers to the model's capability to make accurate predictions with new data. When traditional DL models have too many parameters or overly complex structures, they tend to overfit(Philipp & Carbonell, 2018). However, after going through pre-training and fine-tuning, Large Model can capture more details and generalize better to new datasets and tasks. Therefore, compared to traditional DL model, Large Model has a stronger generalization ability.\n(2) Emergent Abilities\nTraditional DL designs model to solve specific tasks(Ganaie et al., 2022), whereas Large Model uses vast amounts of data for pre-training and then fine-tune for specific downstream tasks, allowing them to adapt to most tasks. As the parameter scale of Large Model increases to a certain threshold, the model's ability to handle certain problems grows rapidly, showing emergent abilities(Wei, Tay, et al., 2022). Therefore, compared to traditional DL models, Large Model can develop unexpected new capabilities during training, demonstrating emergent abilities.\n(3) Reasoning Ability\nTraditional DL struggles to complete a new task like humans can with just a few examples or instructions, such as in NLP. In contrast, Large Model(Weng et al., 2022), like GPT-3, excel in this aspect, effectively handling reasoning tasks like word replacement in sentences or basic arithmetic. Therefore, Large Model has a stronger reasoning ability.\n(4) Decision-making Ability\nThe decision-making ability is primarily exhibited in decision-intelligence Large Model. Existing decision-intelligence Large Model, like GATO(Reed et al., 2022), transplant pre-trained Large Model into decision-making tasks. They have made progress in reinforcement learning decisions and operations optimization decisions. Compared to traditional decision-making algorithms, Large Model excels in cross-task decision-making and rapid transfer abilities.\n(5) Generative Ability\nThe generative ability of Large Model mainly refers to the quality of content the model can produce and is one of the core capabilities of Large Model. For instance, LLM like ChatGPT can predict and supplement subsequent text content based on previous text; One-2-3-45 can generate 3D models from a single image or even textual content(M. Liu, Xu, et al., 2023). It's evident that Large Model can capture and analyze information better, producing high-quality content. Their performance in generation ability is notably outstanding, offering a significant advantage."}, {"title": "3.1.3 Large Model Mature Cases", "content": "At present, Large Model can be divided into general field LM and specialized field Large Model. Some mature cases are shown in the following table."}, {"title": "3.2 The Overview of Large Language Model", "content": "In the various types of Large Model, the development of Large Language Model (LLM) is particularly notable. It is an NLP model based on DL. By training on large-scale corpora, it learns the probability distribution of words, phrases, and sentences, enabling it to generate fluent and semantically accurate text. Currently, there are application examples on the market including the GPT series(Brown et al., 2020; OpenAI, 2023; Openai et al., n.d.; Radford et al., n.d.) and others."}, {"title": "3.2.1 Introduction to LLM", "content": "As a technology designed for computers to understand human language, the fundamental problem of NLP is word representation. This means converting words, the most basic linguistic units in natural language, into formats that machines can understand (Fan et al., 2023). However, there are many challenges in the process of(Fan et al., 2023) word representation. To address this, DL introduced word embedding, which constructs a low-dimensional dense vector space from vast amounts of text. This reduces the demand for storage space and addresses the sparsity issue of words.\nBased on these techniques, statistical language models were developed. Their primary function is to predict the next word based on the context. The approach used to achieve this is called the N-Gram language model. The N-Gram language model follows the Markov assumption, estimating the likelihood of the next word by counting the frequency of words that appear after a certain number of preceding words.\nBuilding on the foundation of language models, the advent of DL has driven the continuous evolution from neural language models to pre-trained models and up to the current LLM (Casola et al., 2022; Fan et al., 2023; Tripathy et al., 2021)."}, {"title": "3.2.2 LLM Mainstream Architecture", "content": "The Transformer architecture, due to its exceptional parallelism and capacity, has become the base structure backbone for the development of various LLM. This makes it possible to scale language models to hundreds of billions or even trillions of parameters. Generally speaking, existing LLM can be mainly divided into three types: encoder-decoder, causal decoder, and prefix decoder. Below is an introduction to some of the more representative architectures among them:\n(1) Seq2Seq(Sequence-to-Sequence)\nSeq2Seq (Sequence-to-Sequence) is a deep learning architecture used for NLP tasks, consisting of an encoder and a decoder (Lewis et al., 2019).\nSeq2Seq performs well in tasks like machine translation, text summarization, and dialogue generation. Its strength lies in handling input and output sequences of varying lengths and being able to learn the complex mappings between them. However, the computational complexity of Seq2Seq is relatively high when dealing with long sequences. Its ability to handle rare vocabulary and complex grammatical structures is limited. To solve these problems, researchers have proposed improved Seq2Seq models, such as those which based on the Transformer model use Self-Attention mechanisms to better capture dependencies in sequences(Chen et al., 2021; Ouyang et al., n.d.)..\n(2) Transformer\nThe Transformer is a deep learning architecture used for NLP tasks. It evolved from Seq2Seq and has been widely applied to other NLP tasks, such as text classification, named entity recognition, and question-answering systems.\nThe design philosophy behind the Transformer is based on the Self-Attention, which can weight each position in the input sequence and incorporate these weighted results as part of the inputs, allowing the model to better understand contextual information(Bahdanau et al., 2014)..\nThe Transformer architecture consists of encoders and decoders(Vaswani et al., 2017). The encoder is responsible for encoding the input sequence into a series of high-dimensional vector representations. The decoder then generates the corresponding output sequence based on the encoder's outputs and the target sequence. Both the encoder and the decoder are composed of multiple stacked self-attention layers and feed-forward neural network layers.\nIn addition to the encoder and decoder, the Transformer also introduces position encodings to represent the order information of each position in the input sequence. By encoding the positional information into fixed-length vectors, it can be added to word embeddings to produce the final input representation.\nModels that adopt the Transformer architecture can be roughly divided into bidirectional models that only use the encoder, represented by BERT, and unidirectional models that only use stacked decoders for training, represented by GPT(Devlin et al., 2018; Y. Liu et al., 2019; Openai et al., n.d.)."}, {"title": "3.2.3 LLM Key Technologies", "content": "The technology of LLM is based on existing NLP techniques. The establishment of LLM is achieved through three key technologies: pre-training of the language model, fine-tuning of the LLM, and designing appropriate prompt strategies for the tasks that various LLM will address.\n3.2.3.1 Pre-training\nPre-training is one of the critical steps in LLM(W. Khan et al., 2023; Sohail et al., 2023) and serves as the foundation for the powerful capabilities of these models. By utilizing vast amounts of unlabeled text data for unsupervised learning, pre-training can capture the statistical properties and contextual relationships of the language(R"}]}