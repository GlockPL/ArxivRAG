{"title": "An Outline of Prognostics and Health Management Large Model: Concepts, Paradigms, and Challenges", "authors": ["Laifa Tao", "Shangyu Li", "Haifei Liu", "Qixuan Huang", "Liang Ma", "Guoao Ning", "Yiling Chen", "Yunlong Wu", "Bin Li", "Weiwei Zhang", "Zhengduo Zhao", "Wenchao Zhan", "Wenyan Cao", "Chao Wang", "Hongmei Liu", "Jian Ma", "Mingliang Suo", "Yujie Cheng", "Yu Ding", "Dengwei Song", "Chen Lu"], "abstract": "Prognosis and Health Management (PHM), critical for ensuring task completion by complex systems and preventing unexpected failures, is widely adopted in aerospace, manufacturing, maritime, rail, energy, etc. However, PHM's development is constrained by bottlenecks like generalization, interpretation and verification abilities. Presently, generative artificial intelligence (AI), represented by Large Model, heralds a technological revolution with the potential to fundamentally reshape traditional technological fields and human production methods. Its capabilities, including strong generalization, reasoning, and generative attributes, present opportunities to address PHM's bottlenecks. To this end, based on a systematic analysis of the current challenges and bottlenecks in PHM, as well as the research status and advantages of Large Model, we propose a novel concept and three progressive paradigms of Prognosis and Health Management Large Model (PHM-LM) through the integration of the Large Model with PHM. Subsequently, we provide feasible technical approaches for PHM-LM to bolster PHM's core capabilities within the framework of the three paradigms. Moreover, to address core issues confronting PHM, we discuss a series of technical challenges of PHM-LM throughout the entire process of construction and application. This comprehensive effort offers a holistic PHM-LM technical framework, and provides avenues for new PHM technologies, methodologies, tools, platforms and applications, which also potentially innovates design, research & development, verification and application mode of PHM. And furthermore, a new generation of PHM with Al will also capably be realized, i.e., from custom to generalized, from discriminative to generative, and from theoretical conditions to practical applications.", "sections": [{"title": "1. Introduction", "content": "Prognostics and Health Management (PHM) serves as an essential mechanism for a complex system to mitigate catastrophic incidents, ensure mission reliability, enhance operational efficacy, and curtail maintenance expenditures. By seamlessly integrating into the entire life cycle of a complex system, PHM has transpired as an essential fulcrum, wielding a pivotal influence reminiscent of a trump card. The application of PHM technology has increased the sortie rate of advanced fighters such as F-35 by 25%, reduced the maintenance manpower by 20%-40%, and reduced the use and support costs by 50% (Chen L et al., 2016). Furthermore, helicopters like the UH-60L Black Hawk experienced an elevation in operational readiness by 27%, a decrement in unplanned maintenance by 52%, and an overall decline in repair tasks by 17% (Baozhen Z & Ping W, n.d.). Large manufacturing conglomerates, such as Boeing, have also reaped the benefits of PHM by curtailing losses due to malfunctions by an impressive 15% (M D et al., 2016) Hence, the intrinsic value of PHM in fortifying mission success, elevating safety measures, paring down maintenance overheads, and amplifying efficacy within the industry cannot be understated.\nDue to the influence of complex operating conditions and harsh environments, existing PHM methods face a series of impediments to achieving satisfactory results. Non-ideal characteristics of PHM data and knowledge(J. Peng et al., 2022; S. Wang et al., 2023; Zio, 2022), inadequate generalization capabilities of PHM models(Chu & Zhu, 2021; Tian et al., 2022), and the dependency of PHM capabilities on specific scenarios(Y. Sun, Lu, et al., 2023; Ye et al., 2020) have resulted in a plethora of pain points for current PHM technological approaches. Although potential interim solutions to a minority of these issues exist in contemporary research, these solutions often exhibit strong customization, weak generalization, and are accompanied by exorbitant costs. The improvements they bring are frequently marginal, failing to meet the capability demands of PHM technologies in application scenarios. Furthermore, the rapid emergence of various new systems and products, along with the application in new and intricate scenarios, exacerbates the conditions for PHM technological deployment. Yet, there are demands for multi-scale PHM capabilities that span from individual components to system clusters(W. Yang et al., 2021), as well as the rapid formation of PHM system capabilities. This juxtaposition leads to an inherent contradiction and conflict between traditional PHM methods and these new requirements. There is an urgent need to explore a new PHM methodological framework that holistically addresses the series of bottlenecks confronting PHM and fulfills the rapid capability formation demands of PHM in the current era.\nArtificial General Intelligence (AGI)(Alattas et al., 2021; Iman et al., 2021; Nti et al., 2022) denotes an artificial intelligence that possesses intelligence equivalent to or surpassing that of humans, capable of manifesting all intelligent behaviors characteristic of average humans. It is quintessential to the realization of AI. In recent years, Large Model technologies, exemplified by Large Language Model (LLM) (Devlin et al., 2019; Openai et al., n.d.; Vaswani et al., 2017), have achieved remarkable strides, unveiling colossal potential towards the actualization of AGI. LLM employ generative modeling on vast textual corpora. When both dataset scale and model size reach a certain inflection point, they can exhibit astounding generative prowess(Ganguli et al., 2022). LLM, with ChatGPT(Brown et al., 2020; Lewis et al., 2020; Radford et al., n.d.) being a paragon, employ techniques like instruction alignment, reinforcement learning(Uc-Cetina et al., 2023), fine-tuning(P. Liu, Yuan, et al., 2023), and thought chain for training and adjustment(Wei, Wang, et al., 2022). This equips them with robust generalization, inference, decision-making, and generative capabilities. In applications like human-machine dialogue and Q&A, their performance rivals or even surpasses human-level competence. Preliminary applications of these models have been explored in specialized fields such as healthcare(J. K. Kim et al., 2023; Lokesh et al., 2022), finance(Pawan Kumar Rajpoot & Ankur Parikh, 2022; H. Yang et al., 2023), manufacturing(Lykov & Tsetserukou, 2023; L. Wu, Qiu, et al., 2023), and education(Jeon & Lee, 2023; Tsai et al., 2023).. Currently, the philosophy and technology behind Large Model are being researched and employed across myriad fields. They are upending conventional specialized field research and application paradigms, ushering in a novel technological revolution. This paradigm shift is profoundly transforming traditional technological field development patterns and the modus operandi of human production.\nGiven this context and leveraging the capabilities of generative Large Model \u2013 notably their inference(Zheng et al., 2023), robust generalization(Igl et al., 2019), and multi-modal information analysis(Lyu et al., 2023) along with the current efficacies of Large Model across various fields, there is a pressing need to address the bottlenecks and practical requirements pertaining to PHM technology. Aligning with global technological advancements, there's an imperative for in-depth integrative research between Large Model and PHM technologies. The overarching objective is to augment the core PHM capabilities and the proficiency of tasks throughout the entire life cycle of PHM systems. This would involve surmounting challenges associated with constructing, training, optimizing, and deploying PHM-LM (Prognosis and Health Management Large Model). By synergizing applications of Large Model, collaborating between ordinary models and Large Model, and emphasizing specialized field Large Model development, there's an opportunity to refine current PHM operational frameworks, enhance PHM algorithm competencies, and bolster downstream PHM tasks. This would, in turn, disrupt conventional PHM design, research and development, validation, and application paradigms. It paves the way for the inception of novel technologies, methodologies, platforms, and applications under the Large Model system framework for PHM. Ultimately, the ambition is to catalyze a paradigm shift in PHM technology: transitioning from bespoke to generic solutions, from discriminative to generative approaches, and from idealized settings to more pragmatic, real-world implementations.\nIn light of the practical demands associated with research on the convergence of Large Model and PHM, this paper integrates the technological advantages of Large Model, the systems engineering process throughout the entire life cycle of PHM, and the challenges faced by PHM. Building on a systematic review of the extant bottlenecks and challenges in PHM, coupled with a profound analysis of the merits of Large Model technology, we innovatively introduce a new concept of the PHM-LM. Additionally, we spotlight three prototypical innovative paradigms for future technical research and application advancements in this realm:"}, {"title": "2. Systematic Analysis of the Current Challenges and Bottlenecks in the PHM Field", "content": "In this paper, the PHM system is defined as one of the subsystems delivered with the system, possessing system-level functional tasks. Consequently, based on the real-world requirements of the PHM system and centered on the system engineering process (as shown in Fig. 2-1), this paper comprehensively analyzes the entire life cycle process of the PHM system(Ma et al., 2022; Tidriri et al., 2016). It systematically examines the series of significant issues faced by the PHM system and the PHM algorithm & model at different stages. Subsequently, the challenges confronting PHM are summarized and organized."}, {"title": "2.1 Analysis and Description of PHM Issues Throughout Life Cycle", "content": "Surveying the development process structures of various advanced PHM systems, the entire life cycle of a PHM system can be broadly divided into five major phases. This section analyzes the essential tasks and work items to be completed in each phase and systematically examines the unresolved issues and capabilities that need enhancement at each stage."}, {"title": "Phase One: Conceptual Design Phase", "content": "During the conceptual design phase, preliminary PHM system design concepts are formed based on PHM design requirements. These concepts undergo validation and evaluation tasks, resulting in reports related to the PHM system development validation and evaluation analysis of the PHM system validation. This phase will produce overarching validation reports for the PHM system, including architecture validation, component validation, and software design validation. Key technological maturity, risk analysis and countermeasures, safety, and economic indicators of the PHM system are evaluated to complete the conceptual design. The design and validation work in the conceptual design phase requires a wealth of PHM field knowledge as support. The efficiency of knowledge utilization is a significant factor affecting the efficiency of this phase."}, {"title": "Phase Two: Preliminary Design Phase", "content": "In the preliminary design phase, based on the initial overall design, subsystem design, support solutions, and data/software system design, the core PHM functions such as testability, monitoring, detection, localization, isolation, evaluation, and prediction are designed. Subsequently, interface dependencies are allocated and docked based on the design content, completing the overall functionality, safety, economic, and risk assessments, laying the foundation for the detailed design of the PHM system."}, {"title": "Phase Three: Detailed Design Phase", "content": "During the detailed design phase, work is carried out on the technical solution of the PHM system, functional allocation prediction, etc., considering experimental research on the principles and functions of the PHM system. The detailed design is evaluated based on the experimental validation of principles and functions, and the prototype design of the PHM system is completed, including the prototype, simulation, and validation platforms."}, {"title": "Phase Four: Development Phase", "content": "After integrating the detailed design of the PHM system and the development standards and specifications, the construction of the PHM system starts from the underlying algorithm and gradually extends to the component level, sub-system level, and system level. Technical documents are written, providing related technical materials, help documents, and training materials for the PHM system."}, {"title": "Phase Five: In-Service Phase", "content": "During the in-service support phase, the PHM system serves alongside the system, realizing its PHM functions. As a large amount of usage data is generated, many iterative updates and new requirements will emerge, and the PHM system will need to undergo gradual iterative updates during the in-service support process."}, {"title": "2.2 Bottleneck Analysis of PHM", "content": "Based on the analysis in Section 2.1, to address the critical issues in the PHM field, break through the technological bottlenecks of PHM, verify and enhance the capabilities of PHM systems, and promote comprehensive development in the PHM field, this paper elucidates the challenges still faced by PHM technology from two perspectives: the PHM algorithm & model layer and the PHM system layer. This approach provides references at different levels for readers with varying research interests and needs, as illustrated in Fig. 2-2."}, {"title": "2.2.1 PHM Algorithm & model Layer", "content": "Challenge 1: How to lower the development threshold for algorithms & models?\nAdvanced Al technologies and big data processing capabilities have brought significant improvements to the capabilities of the PHM field. However, due to factors such as object differences, data constraints, knowledge disparities, and variations in usage environments and conditions, the threshold for developing customized PHM models is gradually increasing. Coupled with the explosive growth of current PHM algorithms & models, tasks such as multimodal data processing design, algorithm & model selection, underlying code writing, and model hyperparameter configuration and iterative updates pose higher technical requirements for users. There lies a key challenge for the widespread application of the PHM field on methods to achieve optimal algorithm & model recommendations, automatic generation of underlying code, and adaptive optimization updates of model hyperparameters to lower the PHM model development threshold.\nChallenge 2: How to effectively carry out algorithm & model validation?\nIn the PHM field, the capability validation of algorithms & models is crucial for ensuring PHM capabilities. However, the lack of data and knowledge during the design and development phase severely restricts the progress of validation tasks. Against this backdrop, how to innovate validation approaches and methods, establish auxiliary conditions for validation, and form a standardized, efficient, and trustworthy PHM algorithm & model validation system is a significant challenge that the PHM field urgently needs to address.\nChallenge 3: How to enhance the logicality and credibility of algorithms & models?\nThe low credibility of the black-box algorithms & models widely used in the PHM field is an undeniable shortcoming in the field. With ample field-specific knowledge support, figuring out how to supplement black-box algorithms with explanatory methods in the PHM field, thereby enhancing the logicality and credibility of algorithms & models, is a key challenge for the development of high-trust PHM.\nChallenge 4: How to improve the generalization performance of algorithms & models?\nWith the technological advancement in the PHM field, the requirements of PHM are continuously increasing. The challenge lies in identifying the common characteristics of different object PHM algorithms & models and the inherent common knowledge among numerous objects and systems. This ensures that the PHM algorithms & models maintain a high precision level across different objects, conditions, data formats, and constraint scenarios. Researching and developing PHM algorithms & models with strong generalization and good generalization, or exploring technical methods to ensure and enhance the generalization capability of PHM algorithms & models, is a key challenge for the widespread adoption ofPHM technology in the future."}, {"title": "2.2.2 PHM System Layer", "content": "Challenge 5: How to efficiently utilize knowledge in the PHM field?\nWith the development of the PHM field to date, the reliance on expert experience has become increasingly pronounced. The role of PHM knowledge spans various stages, including the refinement of PHM system design requirements, preliminary design (including algorithm & model design), detailed design (including algorithm & model design), algorithmic model validation, functional validation, and system integration validation. In an environment where multi-modal and multi-product data and knowledge are rapidly expanding, the challenge lies in enhancing the utilization efficiency of diverse knowledge and multi-modal data, replacing expert individual decision-making with question-answering reasoning based on generative model technology, increasing the connections between vast document knowledge, and consolidating fragmented knowledge with multi-modal data to form a comprehensive PHM knowledge base. This is a significant challenge for the future inheritance and development of the PHM field.\nChallenge 6: How to lower the design threshold for PHM systems?\nPHM system design often proceeds in parallel with the main system design and is subject to various constraints such as resources and deployment. Similar to PHM algorithm & model design, the design work becomes particularly complex due to constraints like monitoring objects, data transmission conditions, and computational resources. However, given the rapidly growing demand for PHM system development, relying solely on expert experience for PHM system design can no longer meet the efficiency and quality requirements of PHM system development. The challenge is to assist in PHM system design, construct generative PHM system design, lower the PHM system design threshold, and enhance the efficiency and quality ofPHM system design.\nChallenge 7: How to establish a PHM validation system?\nIn the PHM field, the validation and determination of solutions are crucial for ensuring that the PHM system achieves its intended functions and is developed with low consumption and high efficiency. However, the lack of data and knowledge during the design and development phase severely restricts the progress of PHM system validation tasks. Against this backdrop, the challenge lies in innovating validation methods, establishing auxiliary conditions for validation, and forming a standardized, efficient, and trustworthy PHM validation system.\nChallenge 8: How to enhance the basic performance and generalization of PHM systems?\nWith the technological advancement in the PHM field, the basic performance requirements for PHM system are continuously increasing. The challenge is to identify the common issues of different object PHM systems, avoid the awkward situation of having to develop a PHM system from scratch for different objects, and design a PHM system with strong generalization capacity that is widely applicable to various systems.\nChallenge 9: How to enhance the interaction efficiency and performance of PHM systems?\nWith the emergence of intelligent application terminals and diversified interaction forms, there's vast potential for the development of application forms and human- machine interaction models in the PHM field software platform. Simultaneously, user feedback from human-machine interactions and the vast generation of real data provide an essential foundation for optimizing and updating the performance of PHM algorithms & models during their service phase. Breaking away from the current traditional human-machine interaction forms, innovating intelligent PHM applications, and exploring feasible solutions for the adaptive optimization and enhancement of PHM algorithm & model performance during their service phase based on human- machine interaction feedback and real data are significant challenges for technological innovation in the PHM field."}, {"title": "2.3 Summary", "content": "Starting from the PHM system engineering process, this section systematically analyzes the various issues that PHM still faces in each development phase and summarizes the bottlenecks and challenges encountered. These challenges and problems can be categorized into two scenarios:\n(1) Scenario One: Issues such as the lack of generalization in PHM solutions and algorithms & models, low utilization rate of field knowledge, and insufficient optimization of the PHM system. Although there are some solutions or alternative approaches to these issues at present, the methods to address them are highly customized. They have weak generalization capabilities and often come with very high costs. The improvements they bring are often limited, necessitating further research and optimization.\n(2) Scenario Two: Problems like the difficulty in conducting validations due to a lack of data and knowledge, and the poor interpretability resulting from the extensive use of black-box models, are currently insurmountable given the existing technological conditions. These issues are significant impediments to the development of the entire PHM field. Failing to address these problems will severely impact the future growth of the PHM field. It's imperative to innovate from a technological perspective and fundamentally revolutionize the theoretical framework.\nBuilding on this foundation, Section 2.2 approaches from two different perspectives: the PHM algorithm & model level and the PHM system level. It identifies 9 major challenges facing the future development of the PHM field, focusing on knowledge utilization, logical supplementation, generalization capability, validation systems, development simplification, and interaction innovation. This is done to address more practical problems in PHM engineering and pave the way for technological innovation in the PHM field."}, {"title": "3. Analysis of The Current Status and Advantageous Features of Large Model Research", "content": "Large Model (LM) is a type of artificial intelligence model that imports massive amounts of unlabeled data into models with billions of parameters for large-scale pre-training, thereby training the model to adapt to various downstream tasks. Large Model can be divided into general field Large Model and specialized field Large Model. General field Large Model refers to large-scale pre-trained models handling multiple fields and tasks, while specialized field Large Model targets specific sectors or industries offering higher expertise and better performance for specific tasks. Currently, the most widely used Large Model is the LLM in the general field. This chapter, based on introducing the main development trajectory of Large Model, focuses on analyzing advantages and features as well as new technologies and ideas in the processes of construction and application. In particular, it elaborates on the advantages of the LLM in Large Model, analyzing their feasibility and rationality when applied to the field of PHM. All in all, this part sets the foundation for a seamless integration with EHM and addresses a series of issues in PHM."}, {"title": "3.1 The Overview of Large Model", "content": "3.1.1 Large Model Principle\n3.1.1.1 Introduction to Large Model\nWith the rise of deep learning and the enhancement of computational capabilities, researchers have begun to focus on how to build models of larger scale to tackle more complex problems. Consequently, the concept of Large Model was introduced and has gradually become a focal point of development in today's technology landscape(Lourenco & Eugenio, 2019). In the era of IT, with data volumes growing explosively daily and the computational power of computers continually increasing, a key research direction in computational science today is how to handle unstructured and multi-modal data(Lipnicka & Nowakowski, 2023; Mayer-Sch\u00f6nberger & Cukier, 2013). These data revolutions also enable Large Model to have access to richer training resources and to process increasingly intricate data. In 2017, Google introduced the Transformer architecture based on the self-attention mechanism(Vaswani et al., 2017) scaling up to hundreds of millions of parameters. Building on the Transformer, in 2018, the introduction of the BERT model(Devlin et al., 2018) saw Large Model breaking the 300 million parameter barrier for the first time. With the emergence of the GPT mode(Brown et al., 2020), Large Model has garnered growing attention.\nLarge Model utilizes massive data to train an extensive number of parameters, thereby handling complex, abstract tasks that traditional ML and DL can't achieve. The objective of designing Large Model is to enhance their representational capabilities and performance, capturing data patterns and regularities more effectively when dealing with intricate tasks. Initially, Large Model was intended to solve issues in natural language processing (NLP). However, as it continued to evolve, its application has expanded far beyond just the NLP realm. Large Model can fit training data better, thereby enhancing its accuracy and generalization capabilities. Additionally, it can learn more intricate features, leading to better performance on complex tasks. As a result, Large Model has found applications in various fields, achieving significant breakthroughs in areas such as NLP(X. Wang et al., 2023), computer vision(Kirillov et al., 2023), voice recognition(Y. Zhang et al., 2023), and even in fields like law and medicine(Lu et al., 2021). In short, Large Model is playing an increasingly pivotal role in contemporary technology.\n3.1.1.2 Large Model Basic Architecture\nCurrently, the majority of Large Model adopt the Transformer architecture, which overcomes the shortcomings of RNNs, previously the most commonly used in the field of NLP. As a result, Transformers are widely utilized by numerous LLM and another Large Model. Apart from the extensively used Transformer architecture, new architectures, such as RWKV, have been proposed, offering fresh perspectives for the construction of Large Model.\n(1) Transformer\nThe Transformer is the most popular architecture used in Large Model today, proposed by Google researchers in 2017. The Transformer comprises Encoder and Decoder modules. The encoder consists of a self-attention layer and a feed-forward neural network, employing residual connections and layer normalization within the layer. The encoder serves to transform input sequence data into a set of representation vectors. Meanwhile, the decoder also uses a combination of the self-attention mechanism and feed-forward neural networks, incorporating two attention layers and one feed-forward network, also employing residual connections and layer normalization. The decoder is responsible for outputting the predicted sequence. The Transformer effectively addresses the long-term dependency issue, enhances the model's parallelism, is broadly applied across various fields, and laid the foundation for models such as BERT and GPT(Devlin et al., 2019; Y. Liu et al., 2019; Openai et al., n.d.).\n(2) RWKV\nRWKV is an improved recurrent neural network (RNN) architecture. It overcomes the traditional drawbacks of RNNs that make them challenging to train on long sequences due to the vanishing gradient problem, as well as the Transformer architecture's downside of consuming a significant amount of memory during computation(B. Peng et al., 2023). Embodying the advantages of both the Transformer and RNN, RWKV can be efficiently trained and also support fast inference, making it a valuable architecture in the current landscape.\n(3) RetNet\nJointly introduced by Microsoft Research and Tsinghua University(Y. Sun, Dong, et al., 2023), RetNet is a novel architecture composed of multiple identical blocks. Each RetNet block encompasses a multi-scale retention (MSR) module and a feed-forward neural network. Furthermore, RetNet introduces a multi-scale retention mechanism as a substitute for the self-attention mechanism, supporting both parallel and recurrent computation modes. RetNet demonstrates excellence in long-sequence modeling, and its implementation and deployment are relatively straightforward. It provides a new direction and breakthrough in the design of Large Model architectures.\n3.1.1.3 Large Model Key Technology\n(1) Pre-training Data Collection\nTo train Large Model, knowledge needs to be learned from vast amounts of data and stored in the model's parameters. In order to enhance the efficiency of model training, high-quality data must be used(Penedo et al., 2023). Given the significant presence of noisy samples in real data, it becomes essential to filter this data. Currently, filtering methods mainly include model-based methods and heuristic-based methods. After filtering, the data needs to be de-duplicated, which can be categorized into two methods: fuzzy deduplication (such as SimHash(Detecting Near-Duplicates for Web Crawling Proceedings of the 16th International Conference on World Wide Web, n.d.)) and exact substring matching deduplication.\n(2) Model Architecture Design\nTypically, when training Large Model, increases in both data and model scale will enhance training results. However, the larger the model, the more training resources it occupies. Therefore, it's essential to employ DL architectures that can compute efficiently when building Large Model. Currently, Transformers and their variants are the most widely used deep learning architectures for building Large Model. Additionally, the size of the model should match the scale of the training data to prevent the waste of computational resources(Hoffmann et al., 2022).\n(3) Fine-tuning for Downstream Tasks\nAfter pre-training a Large Model, in order to adapt it to downstream tasks, it requires fine-tuning. This involves introducing specific downstream task data, allowing the Large Model to continue training on top of the pre-trained weights until it meets the requirements of the downstream task. Efficient fine-tuning methods are crucial for maximizing the performance of Large Model. At present, common fine-tuning methods include LoRA(E. J. Hu et al., 2021), Adapter(R. Zhang et al., 2023), Prefix-tuning(X. L. Li & Liang, 2021), P-turning(X. Liu et al., 2021), Prompt-tuning(Lester et al., 2021), and RLHF(Christiano OpenAI et al., 2017), among others.\n3.1.2 Large Model Advantage Analysis\n(1) Generalization Ability\nGeneralization ability refers to the model's capability to make accurate predictions with new data. When traditional DL models have too many parameters or overly complex structures, they tend to overfit(Philipp & Carbonell, 2018). However, after going through pre-training and fine-tuning, Large Model can capture more details and generalize better to new datasets and tasks. Therefore, compared to traditional DL model, Large Model has a stronger generalization ability.\n(2) Emergent Abilities\nTraditional DL designs model to solve specific tasks(Ganaie et al., 2022), whereas Large Model uses vast amounts of data for pre-training and then fine-tune for specific downstream tasks, allowing them to adapt to most tasks. As the parameter scale of Large Model increases to a certain threshold, the model's ability to handle certain problems grows rapidly, showing emergent abilities(Wei, Tay, et al., 2022). Therefore, compared to traditional DL models, Large Model can develop unexpected new capabilities during training, demonstrating emergent abilities.\n(3) Reasoning Ability\nTraditional DL struggles to complete a new task like humans can with just a few examples or instructions, such as in NLP. In contrast, Large Model(Weng et al., 2022), like GPT-3, excel in this aspect, effectively handling reasoning tasks like word replacement in sentences or basic arithmetic. Therefore, Large Model has a stronger reasoning ability.\n(4) Decision-making Ability\nThe decision-making ability is primarily exhibited in decision-intelligence Large Model. Existing decision-intelligence Large Model, like GATO(Reed et al., 2022), transplant pre-trained Large Model into decision-making tasks. They have made progress in reinforcement learning decisions and operations optimization decisions. Compared to traditional decision-making algorithms, Large Model excels in cross-task decision-making and rapid transfer abilities.\n(5) Generative Ability\nThe generative ability of Large Model mainly refers to the quality of content the model can produce and is one of the core capabilities of Large Model. For instance, LLM like ChatGPT can predict and supplement subsequent text content based on previous text; One-2-3-45 can generate 3D models from a single image or even textual content(M. Liu, Xu, et al., 2023). It's evident that Large Model can capture and analyze information better, producing high-quality content. Their performance in generation ability is notably outstanding, offering a significant advantage.\n3.1.3 Large Model Mature Cases\nAt present, Large Model can be divided into general field LM and specialized field Large Model. Some mature cases are shown in the following table."}, {"title": "3.2 The Overview of Large Language Model", "content": "In the various types of Large Model, the development of Large Language Model (LLM) is particularly notable. It is an NLP model based on DL. By training on large-scale corpora, it learns the probability distribution of words, phrases, and sentences, enabling it to generate fluent and semantically accurate text. Currently, there are application examples on the market including the GPT series(Brown et al., 2020; OpenAI, 2023; Openai et al., n.d.; Radford et al., n.d.) and others.\n3.2.1 Introduction to LLM\nAs a technology designed for computers to understand human language, the fundamental problem of NLP is word representation. This means converting words, the most basic linguistic units in natural language, into formats that machines can understand (Fan et al., 2023). However, there are many challenges in the process of(Fan et al., 2023) word representation. To address this, DL introduced word embedding, which constructs a low-dimensional dense vector space from vast amounts of text. This reduces the demand for storage space and addresses the sparsity issue of words.\nBased on these techniques, statistical language models were developed. Their primary function is to predict the next word based on the context. The approach used to achieve this is called the N-Gram language model. The N-Gram language model follows the Markov assumption, estimating the likelihood of the next word by counting the frequency of words that appear after a certain number of preceding words.\nBuilding on the foundation of language models, the advent of DL has driven the continuous evolution from neural language models to pre-trained models and up to the current LLM (Casola et al., 2022; Fan et al., 2023; Tripathy et al., 2021).\n3.2.2 LLM Mainstream Architecture\nThe Transformer architecture, due to its exceptional parallelism and capacity, has become the base structure backbone for the development of various LLM. This makes it possible to scale language models to hundreds of billions or even trillions of parameters. Generally speaking, existing LLM can be mainly divided into three types: encoder-decoder, causal decoder, and prefix decoder. Below is an introduction to some of the more representative architectures among them:\n(1) Seq2Seq(Sequence-to-Sequence)\nSeq2Seq (Sequence-to-Sequence) is a deep learning architecture used for NLP tasks, consisting of an encoder and a decoder (Lewis et al., 2019).\nSeq2Seq performs well in tasks like machine translation, text summarization, and dialogue generation. Its strength lies in handling input and output sequences of varying lengths and being able to learn the complex mappings between them. However, the computational complexity of Seq2Seq is relatively high when dealing with long sequences. Its ability to handle rare vocabulary and complex grammatical structures is limited. To solve these problems, researchers have proposed improved Seq2Seq models, such as those which based on the Transformer model use Self-Attention mechanisms to better capture dependencies in sequences(Chen et al., 2021; Ouyang et al., n.d.).. \n(2) Transformer\nThe Transformer is a deep learning architecture used for NLP tasks. It evolved from Seq2Seq and has been widely applied to other NLP tasks, such as text classification, named entity recognition, and question-answering systems.\nThe design philosophy behind the Transformer is based on the Self-Attention, which can weight each position in the input sequence and incorporate these weighted results as part of the inputs, allowing the model to better understand contextual information(Bahdanau et al., 2014)..\nThe Transformer architecture consists of encoders and decoders(Vaswani et al., 2017). The encoder is responsible for encoding the input sequence into a series of high- dimensional vector representations. The decoder then generates the corresponding output sequence based on the encoder's outputs and the target sequence. Both the encoder and the decoder are composed of multiple stacked self-attention layers and feed-forward neural network layers.\nIn addition to the encoder and decoder, the Transformer also introduces position encodings to represent the order information of each position in the input sequence. By encoding the positional information into fixed-length vectors, it can be added to word embeddings to produce the final input representation.\nModels that adopt the Transformer architecture can be roughly divided into bidirectional models that only use the encoder, represented by BERT, and unidirectional models that only use stacked decoders for training, represented by GPT(Devlin et al., 2018; Y. Liu et al., 2019; Openai et al., n.d.)."}, {"title": "3.2.3 LLM Key Technologies", "content": "The technology of LLM is based on existing NLP techniques. The establishment of LLM is achieved through three key technologies: pre-training of the language model, fine-tuning of the LLM, and designing appropriate prompt strategies for the tasks that various LLM will address.\n3.2.3.1 Pre-training\nPre-training is one of the critical steps in LLM(W. Khan et al., 2023; Sohail et al., 2023) and serves as the foundation for the powerful capabilities of these models. By utilizing vast amounts of unlabeled text data for unsupervised learning,and contextual relationships of the language(Ruan & Jin, 2022).\nDuring the pre-training phase, the model is fed with a large amount of text data and trained using an autoregressive generation approach. This means the model predicts the next word or character based on the already generated portion of the text, gradually producing coherent text. After pre-training, LLM acquire extensive linguistic knowledge and probability distributions. This enables them to generate coherent and natural text, enhancing their understanding and generation capabilities regarding language (Cao et al., 2023).\n3.2.3.2 LLM Adaptation Fine-tuning\nFine-tuning of LLM can be described as the process of further training and optimization of the model on supervised annotated data, following its pre-training phase(Chen et al., 2021). This can be subdivided into two primary methods: instruction tuning and alignment tuning. While instruction tuning aims to augment the capabilities of LLM, alignment tuning seeks to harmonize the model's reactions to textual stimuli with human values, guarding against potential moral dilemmas(Ouyang et al., n.d.).\n(1) Instruction Tuning\nInstruction tuning is a technique of adjusting the model's inputs. By modulating the instructions or prompts given to the model, one can enhance the quality of the model's generated outputs. In instruction tuning, there is no retraining of the model parameters. Instead, it is achieved by devising more apt input directives, leading the model to produce more accurate and relevant responses. The objective of instruction tuning is to impact the model's outputs by framing more precise directives, ensuring it aligns better with user requirements. This type of fine-tuning can enhance the model's adaptability, accuracy, and generalization capabilities, rendering it effectively operational in instruction-related applications.\n(2) Alignment Tuning\nDespite the impressive capabilities demonstrated by LLM, there remains the potential to inadvertently harm human society, such as by fabricating misinformation or emitting biased responses. Therefore, it's imperative to align these models with human values, ensuring they uphold standards of utility, honesty, and harmlessness. To this end, some studies have introduced reinforcement learning based on human feedback (RLHF(Christiano OpenAI et al., 2017)). By gathering high-quality human feedback, the learning reward model encourages the LLM to gradually adapt to human standards. However, certain research indicates that alignment tuning might, to some extent, diminish the model's overall versatility.\n3.2.3.3 Design appropriate prompting strategies based on the task\nAfter pre-training and fine-tuning, to deploy an LLM effectively, it is essential to devise appropriate prompting strategies for various tasks(Radford et al., n.d.). This has led to the development of In-Context Learning (ICL) and Chain-of-Thought Prompting (COT).\n(1) In-Context Learning (ICL)\nICL was first introduced in GPT-3 and subsequently became a standard method for using LLM(W. Khan et al., 2023; Sohail et al., 2023; Zhao et al., 2023)..\nThe fundamental premise of ICL is that, during text generation, it relies not only on the current input context but also uses previously generated text as an extended context. In every iteration, ICL merges the previously produced text with the current input context, resulting in a richer contextual representation. This iterative approach aids in addressing long-distance dependency challenges, enhancing the quality and coherence of the generated text(Dong et al., 2022).\n(2) Chain-of-Thought Prompting (COT)\nCOT involves the model generating outputs through a series of intermediate steps related to the inputs, presenting possible reasoning pathways or associated concepts(Sohail et al., 2023; Zhao et al., 2023). These prompts elucidate how the model arrives at specific answers or perspectives, shedding light on some of the model's internal mechanisms in reasoning and linguistic expression.\nIn LLM, COT leverages the model's linguistic comprehension and generation abilities combined with internal representations and attention mechanisms. This method offers insights into understanding the model's reasoning process and provides a more in-depth interpretation of the model's outputs.\nBased on the COT, which transforms the mind from a chain to a graph, the ETH Zurich research team proposes the graph of thoughts (GOT) approach to improve the LLM's ability in reasoning and verbal expression.\n3.2.4 LLM Advantages Analysis\nLeveraging vast datasets and sophisticated learning algorithms, LLM exhibit enhanced power, precision, and flexibility in NLP tasks. Surveys of Large Model indicate that numerous industries, including finance, law, and healthcare, are increasingly integrating LLM to construct specialized field applications. Regarding the PHM field, the strengths of LLM can be also harnessed to pioneer new research paradigms:\n(1) Information Extraction and Decision-making Support\nLLM can automatically extract key information from vast textual data, recognize fault patterns, and offer decision-making support. They are adept at thoroughly analyzing and understanding an abundance of information, such as machinery operational data, repair records, technical manuals, etc., assisting engineers and maintenance personnel in rapidly pinpointing issues and devising solutions.\n(2) Constructing Expert Knowledge Bases\nLLM can establish a knowledge base saturated with extensive PHM expertise. By learning and comprehending prior fault cases, repair experiences, and other knowledge, these models can present relevant solutions and recommendations. Moreover, they can furnish coherent explanations(Devlin et al., 2018), elevating the accuracy and efficiency of fault diagnosis.\n(3) Multi-field Applicability\nExhibiting strong generalization capacities (Brown et al., 2020) and knowledge transfer abilities (Xie et al., 2023), LLM are viable for machinery PHM across varied fields, encompassing industrial machinery, aerospace, ships, vehicles, etc. Their generalization and adaptability enable customization and optimization based on specialized field needs, thereby catering to diverse industry requirements for PHM.\n(4) Multi-modal Data\nLLM are capable of concurrently processing various data formats, such as text, images, sounds, etc. This implies they can extract information from textual descriptions of system while also analyzing system-related images, sounds, and other data types, culminating in a comprehensive assessment.\n(5) Algorithm Intelligent Recommendations\nIn the context of Q&A, LLM can produce personalized recommendation outcomes based on users' preferences and historical actions(Hou et al., 2023). Hence, they can be deployed for intelligent PHM algorithm recommendations, suggesting algorithms suitable for specific PHM cases to engineers, fulfilling varied task demands.\n3.2.5 LLM Mature Cases\nMajor tech companies and academia have delved deep into NLP for years, continually unveiling innovative LLM. By focusing on some typical LLM and understanding their characteristics and application status in the industry, we can provide cases for the combination of PHM and LLM.\n(1) GPT-4(OpenAI, 2023)\nThe latest generative pre-trained transformer model in the GPT series, GPT-4 is released by OpenAI. Supporting multimodal format inputs, GPT-4 is adept at resolving complex issues across disciplines like mathematics, coding, and healthcare, even without explicit prompts. GPT-4 has not yet released technical details and cannot be deployed on this basis, but its model performance is still leading the industry, and some Large Model in specific fields will generate training datasets based on the results of conversations with GPT-4.\n(2) Llama(Touvron et al., 2023)\nLlama, a collection of foundational language models of four different parameter scales, is released by Meta AI. During the fine-tuning phase, Llama undergoes supervised training based on labels specific to a given task, further enhancing the model's performance. At present, many Large Model in specific fields are choosing Llama as the base model to meet the needs of different downstream industries by adding different training sets of specialized fields. The latest Llama 2 has also been open source, compared with the previous generation in performance, reasoning efficiency has been significantly improved."}, {"title": "3.3 Summary", "content": "In summary, Large Model demonstrates outstanding capabilities in content generation, generalization, reasoning, and decision-making. The success of these models further validates the synergy of vast data, high computational power, and efficient algorithm architecture. The development process of Large Model harnesses an array of artificial intelligence-supported technologies, such as machine learning, computer vision, knowledge graphs, and natural language understanding. This showcases the pivotal role Al technologies play in the construction of Large Model.\nThe reasoning, decision-making, generalization, emergent, and generative attributes inherent in Large Model offer significant opportunities to address the current challenges in PHM, such as weak algorithm generalization, underutilization of field knowledge, and poor algorithm & model versatility. This convergence presents an organic juncture for the integration of Large Model technologies with PHM techniques. Furthermore, contemporary PHM extensively employs Al technologies to tackle related issues, such as fault diagnosis. Consequently, there exists a technical foundation and feasibility for the union of PHM with Large Model technology at a technological level.\nBlending the techniques related to Large Model with PHM can address current PHM challenges regarding algorithm, field knowledge, generalization performance, and developmental thresholds. This fusion promise to steer PHM towards a more efficient research paradigm."}, {"title": "4. Concept of PHM-LM and Progressive Research of Paradigms", "content": "As research into Large Model burgeons at an accelerated pace, the technical merits of generalization capacity, emergence capacity, and generative capacity brought forth by these models offer significant opportunities for the progressive evolution of the PHM. Consequently, when tackling the challenges and predicaments encountered in PHM, the comprehensive utilization of Large Model's capacities and their underlying principles introduces novel perspectives and theories for the future development of PHM(Jia et al., 2023; Jiao et al., 2020; Lei et al., 2020). Building upon an analysis of the developmental challenges in the PHM and the distinctive advantages offered by Large Model, this chapter introduces, for the first time, the concept and essence of PHM-LM. Furthermore, based on the form and degree of the integration between Large Model and PHM, it progressively outlines 3 typical paradigms of innovative research for the advancement of PHM-LM exploration and application, ranging from fundamental to advanced. Subsequently, under 3 typical innovation paradigms, considering the characteristics and capabilities of each paradigm, we propose the key PHM capabilities that can be enhanced by Large Model in different paradigms, as well as feasible technical approaches to enhance these capabilities. This section offers researchers engaged in the integration of Large Model with PHM a comprehensive and progressively detailed technical study roadmap and specific solution strategies."}, {"title": "4.1 Concept and Connotation of PHM-LM", "content": "Generally, a PHM-LM refers to an artificial intelligence model characterized by its vast parameter scale and complexity, deeply integrated with the specificities of the PHM specialized field, and capable of serving the entire life cycle of product PHM. The design and application of these PHM-LM aim to address the myriad complex tasks in PHM in a more intelligent, precise, and general manner. Given the multifaceted nature of PHM, these Large Model can be further categorized into:\n(1) Object-oriented PHM-LM, such as models for bearing PHM, gear PHM, motor PHM, and electronic product PHM;\n(2) Task-oriented PHM-LM, like models for data generation, solution generation, and verification & validation;\n(3) Algorithm & model-oriented PHM-LM, including models for fault diagnosis, fault prediction, and maintenance decision-making.\nCompared to traditional PHM systems, PHM-LM exhibit significant advancements in aspects like data, knowledge, model architecture, and capabilities. Traditional PHM models tend to be narrow in scope, handling limited data and knowledge. In contrast, the PHM-LM offer robust capabilities in processing multi- modal data, effectively handling both complex and non-ideal data conditions. On another front, while traditional PHM models often serve as small, purpose-specific models, the PHM-LM is the synergistic outcome of integrating Large Model technology with PHM modeling techniques. Essentially, Large Model are the consolidation and fusion of these smaller models. In terms of capabilities, the PHM-LM facilitate the resolution of a myriad of complex tasks throughout the entire PHM life cycle in an intelligent, precise, and general manner. They prove invaluable to PHM designers, positioning themselves as top-tier intelligent experts in the PHM field.\nThe PHM-LM, as an advanced generative Al model, is equipped with multi-modal data-driven capabilities, superior generalization, and practical performance. These attributes are poised to spur innovative breakthroughs in technical research within the PHM field and catalyze a comprehensive overhaul of its service models. In the sections that follow, we will delve into a step-by-step exposition and analysis of three representative innovative research paradigms for advancing the study and application of PHM-LM. Specifically, these paradigms are:\n\u2022 Paradigm I: PHM Paradigm based on LLM\n\u2022 Paradigm II: Parallel Paradigm of LLM and PHM Model\n\u2022 Paradigm III: Construction and Application Paradigm of PHM-LM"}, {"title": "4.2 Paradigm I: PHM Paradigm based on LLM", "content": "In the context of customized requirements, extensive research on fault prognostics(Bo\u0161koski et al., 2015), fault diagnosis(J. Liu, 2022; J. Sun et al., 2022), and health management(S. Khan & Yairi, 2018) has been conducted in PHM(Meng & Li, 2019). However, due to challenges such as insufficient generalization, there remains significant room for optimizing the development of PHM practices. Current obstacles, such as a lack of efficient knowledge integration(Gay et al., 2021), repetitiveness in textual design, complexity in data modalities(Farsi & Zio, 2019), and the difficulty of algorithm & model development(Vogl et al., 2019), have resulted in inefficiencies, poor generalization, and limited interpretability in traditional PHM services such as knowledge retrieval, Q&A, explanatory supplementations, redundant textual tasks, algorithm & model-assisted development, knowledge-based diagnostics, and maintenance decision-making. Generative LLM, with their strong generalization, reasoning, and comprehension capabilities, have already revolutionized sectors like finance(S. Wu et al., 2023), healthcare(Thirunavukarasu et al., 2023), and law. By offering rapid information retrieval, text processing, and case analysis, these models have enhanced knowledge utilization, work efficiency, and generalization in their respective fields. Such successes are testimonies to the potential improvements that LLM technology can bring to specialized fields of research.\nGiven this, a PHM paradigm rooted in LLM will exploit the language reasoning and generalization capabilities of foundational LLM. Through rapid fine-tuning with field-specific knowledge from the PHM field, this paradigm aims to build a specialized field LLM tailored for typical scenarios within the PHM realm, thereby addressing a host of issues present in this field. Anchored in this concept, the Paradigm I designs applications such as PHM knowledge engineering, LLM diagnostic expert, assisted algorithm & model development for PHM, text generation for PHM, and LLM-driven maintenance decision-making. The ultimate goal is to holistically enhance traditional PHM services, including knowledge integration, expert diagnostics, algorithm & model development, textual solution design, and maintenance decision-making, leveraging next-generation artificial intelligence LLM technology to elevate efficiency, generalization, and decision-making capabilities in PHM."}, {"title": "4.2.1 Approach 1: Knowledge Engineering of PHM based on LLM and Knowledge Graph", "content": "Based on the fusion of LLM and knowledge graphs, the PHM knowledge engineering aims to address the challenges of low knowledge utilization, inefficient retrieval, and insufficient decision interpretability. Guided by the principle presented in Paradigm I, which focuses on the application of LLM in specialized fields to tackle a range of issues in PHM, this approach leverages the strengths of LLM in language analysis, knowledge reasoning, and generalization. This approach revolves around the task of taking retrievable content as input and producing knowledge retrieval, reasoning, and decision outcomes. It is applied to various scenarios within PHM, including knowledge retrieval, interactive question answering, and reasoning-based decision- making. By employing the capabilities of LLM and knowledge graphs, this approach seeks to achieve efficient, accurate, and reliable knowledge querying and datasets interaction analysis, thereby contributing to the advancement of the PHM field(Y.-F. Li et al., n.d.).\nLLM possess remarkable capabilities in language analysis, knowledge reasoning, and generalization(Zhao et al., 2023). Moreover, architectures such as Transformers and RetNet offer parallel processing and enhanced computational efficiency. Their ability for context learning and fine-tuning empowers Transformer-based models to excel in handling vast amounts of information and solving complex problems, rendering them superior in certain fields(Bagal et al., 2022; Y. Xu et al., 2022). However, current LLM still face challenges such as poor real-time data processing and adapting to emerging trends. Without a comprehensive and real-time knowledge base, these limitations hinder their application in PHM. Leveraging the advantages of knowledge graphs in data mining, knowledge integration, and information processing, knowledge engineering in PHM based on knowledge graphs has achieved intelligent fault diagnosis, fault reasoning, and fault localization for specific entities (X. Tang et al., 2023). Nevertheless, relying solely on knowledge graph technology can only accomplish limited-scale PHM knowledge engineering for small-scale entities, falling short in enhancing the generalization capabilities of PHM models. Therefore, combining the strengths of LLM and knowledge graphs, this approach proposes a PHM knowledge engineering framework based on both components. This integrated approach aims to address the drawbacks of poor real-time performance and knowledge scarcity in LLM, as well as the limitations of knowledge graph generalization and limited scale. The result is a solution that enables rapid, credible, wide-ranging, and multi-modal data-supported knowledge retrieval, question answering, and decision reasoning functions.\nThe PHM knowledge engineering based on the interaction between LLM and knowledge graphs comprises two main components. The construction of a PHM knowledge graph involves the consolidation of sources for PHM knowledge. Driven by specific requirements, PHM knowledge is gathered, followed by data cleaning, vectorization, and structured transformation. The next step involves storing this information in a document dataset and constructing a knowledge graph that encompasses relevant concepts and service aspects. This knowledge graph serves as the foundation for data extraction in the interaction with LLM(Y. Tang et al., 2023). In the context of LLM interaction, the system receives various multi-modal inputs related to PHM from users. It deconstructs the user's requirements and processes the questions through vectorization. An intent generalization module enhances the generalization capability of the LLM, and the generalized requirements are used for searching the knowledge graph data(Y. Liu et al., 2022). The system then returns relevant graph concepts and attributes. The output of user queries is generated using the NLP module of the LLM. This process incorporates the reasoning capability of the LLM to validate and evaluate search results. Through this approach, it addresses challenges such as poor generalization in PHM and low knowledge utilization efficiency, achieving efficient, accurate, and trustworthy knowledge retrieval and dataset interaction analysis. This mechanism provides robust technical support for the management of health knowledge related to various devices and their critical components, as well as for PHM decision-making."}, {"title": "4.2.2 Approach 2: Diagnostic Expert based on LLM", "content": "Diagnostic expert based on LLM has faced limitations that have led to the stagnation of expert systems' knowledge bases and reasoning engines in the field of diagnostic expert systems. In order to overcome these limitations, the concept of using LLM to address a range of issues in the PHM field, as discussed in Paradigm I, is extended to provide a solution. Leveraging the strengths of LLM in knowledge reasoning and decision recommendation, a new approach is introduced. This approach involves utilizing the knowledge reasoning and decision recommendation capabilities of LLM to address the limitations of knowledge-based diagnostic expert systems. It focuses on incorporating various inputs such as textual descriptions of faults, audio signals like vibration patterns, images of faulty system, and more. These inputs are processed by the LLM, which then generates outputs related to potential directions for fault diagnosis research and diagnostic results that can be referenced. The application of this approach, in forms such as diagnostic result reasoning, contributes to the advancement of PHM technology by enhancing the efficiency of fault diagnosis.\nThe development of expert systems based on knowledge-based diagnosis has reached a bottleneck due to its inherent limitations(Akram et al., 2014; Cowan, 2001). However, the knowledge reasoning and decision recommendation capabilities of LLM have opened up new avenues for the previously stagnant expert systems. Currently, in fields like healthcare(Mesk\u00f3 & Topol, 2023) and finance, LLM utilizing expert knowledge have demonstrated enhanced diagnostic capabilities through knowledge- based reasoning. This emphasizes the potential of LLM to improve diagnostic systems within specialized fields. Thus, leveraging the strengths of LLM, a novel approach is proposed: the creation of a generative LLM diagnostic expert system formed by the integration of knowledge repositories and reasoning engines.\nUsers are required to provide a substantial amount of expertise in PHM to construct the knowledge repository, thereby endowing the expert system with proficiency in fault diagnosis. The diagnostic expert system based on the LLM necessitates a signal processing module for handling multi-modal inputs such as images of faulty components, FMECA tables, videos, etc., and transforming them into a format compatible with the language-processing capabilities of the LLM. The reasoning engine needs to leverage the robust reasoning capabilities of the LLM. It accomplishes this through context learning, fine-tuning, and similar techniques. This enables the system to analyze fault patterns within the input data, cross-reference them with the knowledge and data stored in the corpus, and generate logically coherent and convincing diagnostic outcomes. Additionally, the reasoning abilities of the LLM are employed to evaluate whether the results meet the requirements of the diagnosis, thereby catering to user needs."}, {"title": "4.2.3 Approach 3: PHM Algorithm & model Assisted Development", "content": "The PHM algorithm & model assisted development aims to address challenges related to the high demand for specialized knowledge and coding proficiency in the development process. Guided by the principle of the Paradigm I, which leverages the application of LLM in specialized fields to address a series of challenges in the field of PHM, this approach capitalizes on the advantages of the LLM's code generation and analysis capabilities. This approach aims to facilitate PHM algorithm & model development tasks by using natural language descriptions of code requirements and inputting code segments that need explanation. The outputs encompass the desired code, functions, models, and descriptive analyses of model functionalities. This assists in tasks such as code functionality analysis and code completion, resulting in lowered barriers to algorithm & model development and increased efficiency in the development process. Through these typical applications, the development of PHM technology is enhanced, leading to reduced complexity in algorithm & model development and improved efficiency.\nIn order to develop high-performing PHM algorithms & models, it requires individuals with a strong background in specialized knowledge and coding proficiency. Current LLM have already demonstrated the capability to perform basic code writing(Nijkamp et al., 2022) and interpretation (Leinonen et al., 2023). They hold significant potential in assisting algorithm & model development. However, while LLM possess general code writing and interpretation capabilities, they may not meet the professional standards and requirements for algorithm development on a large scale using collected data. To mitigate the need for high algorithm development skills among PHM algorithm & model developers(Rask Nielsen & Holten M\u00f8ller, 2022), it is possible to entirely leverage the advantages of LLM in functions such as code writing and analysis. By utilizing LLM for tasks like code completion and code functionality analysis in the realm of PHM algorithms, the difficulty for users in developing PHM models can be reduced, leading to increased efficiency in the development process.\nTo construct a corpus for PHM algorithms & models and their associated code based on historical data, the aim is to facilitate the pre-training of LLM. This pre- training equips the models with the capacity to engage in high-quality PHM model knowledge reasoning and model generation. Subsequently, utilizing fine-tuning methods, the LLM can analyze existing models and automatically offer developers completion code containing functional analysis explanations for users to select. The models can also detect and highlight warnings or errors present in the code, significantly reducing the complexity of PHM model development(Yuan et al., 2023). Simultaneously, within this framework, a module for handling multi-modal inputs is required. This module receives user descriptions of their code requirements and returns the needed code, functions, models, etc. in the form of answers to users' queries. By following the outlined approach, the objective of reducing the complexity of PHM model development and enhancing the efficiency of the development process can be achieved."}, {"title": "4.2.4 Approach 4: PHM Text Generation", "content": "The PHM text generation approach aims to address the challenges of high repetition and low design efficiency in the PHM field. Guided by the principle established in Paradigm I, which leverages the application of LLM in specialized fields, this approach capitalizes on the strengths of LLM, including language analysis, knowledge reasoning, decision-making, and generalization capabilities. By taking elements such as the requirements for generating solutions as input, the approach outputs PHM textual solution documents that adhere to the specified criteria and exhibit clear logic. This task of generating PHM text is exemplified through applications such as designing text and generating test outlines. Through this approach, the development of PHM technology can bring about a reduction in repetitive tasks and an enhancement in the efficiency of textual design, while benefiting from the language analysis, knowledge reasoning, solution decision-making, and generalization capabilities of LLM.\nIn the practical application of PHM, the diversity of PHM scenarios results in a lack of generalization in PHM solutions. This leads to the necessity of designing custom PHM solutions for different systems and components, resulting in excessive repetition of designs and significant customization demands. This, in turn, diminishes the efficiency of PHM design. Some existing LLM, such as ChatGPT, Llama, GLM, have already demonstrated preliminary capabilities in generating textual solutions(Hirosawa et al., 2023). Additionally, in other fields like finance, healthcare, and law, LLM have shown potential in assisting with professional tasks, achieving accuracy rates as high as 93.3%(J. Liu, Wang, et al., 2023). This showcases the considerable potential of LLM in generating specialized solutions in specialized fields. Thus, it is possible to harness the advantages of LLM in language analysis, knowledge reasoning, decision-making, and generalization capabilities to design a generative PHM textual solution design model. This model can leverage multi-modal inputs of PHM design related requirements to generate the needed PHM design. By doing so, it seeks to address the issue of excessive repetition in current PHM design and enhances the efficiency of PHM design.\nUsers leverage historical PHM knowledge, solution-related documents, and other corpora to establish a foundation. They employ methods like document segmentation and data classifiers for cleansing, filtering, and structuring the data in preparatory steps, resulting in a pre-training corpus suitable for generating textual solutions. Further, the pre-training of a mature LLM is conducted, imbuing it with robust generalization, reasoning, and multi-modal data processing capabilities. To enhance the effectiveness of the LLM in generating textual solutions, techniques like prompt engineering are employed. By combining usage scenarios and generation objectives, appropriate prompts are designed to guide the LLM's output. For instance, different types of solution requirements, such as diagnostic plans, predictive plans, and design plans, are reflected in the input. Instruction tuning and alignment tuning are employed to adhere to field-specific conventions and to shape the output of the LLM, ensuring it generates PHM textual solutions that meet the requirements coherently. The LLM evaluates the generated results, using specific metrics to measure if the outcomes align with the input requirements. Based on this technical framework, the PHM solution generation assisted by LLM facilitates PHM professionals in their system design tasks. This approach reduces redundancy, enhances efficiency, and improves the effectiveness of PHM text design in the PHM field."}, {"title": "4.2.5 Approach 5: Maintenance Decision Support based on LLM", "content": "The route of maintenance decision support based on LLM aims to address issues like inadequate integration and utilization of maintenance knowledge, as well as low efficiency in maintenance retrieval and decision-making. Guided by the concept introduced in Paradigm I, which utilizes LLM in specialized fields to address a range of problems in PHM, this approach capitalizes on the knowledge integration and decision reasoning capabilities of LLM. Structured data such as FMECA tables and unstructured data like fault descriptions are used as inputs to generate outputs of retrieved knowledge or referenceable maintenance decisions. This process forms the basis of maintenance decision support. By employing the capabilities of LLM, tasks such as retrieval of maintenance knowledge and generation of maintenance decisions are accomplished. This approach seeks to address the challenges arising from insufficient competencies among maintenance personnel, leading to low utilization of maintenance knowledge and poor retrieval efficiency. Through typical applications such as maintenance knowledge retrieval and decision generation, the approach improves the utilization of maintenance knowledge and enhances decision-making efficiency in PHM technology development, thereby resolving challenges stemming from inadequate skills among maintenance personnel and boosting both the retrieval efficiency and utilization of maintenance knowledge.\nMaintenance personnel responsible for system repairs require sufficient knowledge and experience. However, due to the complexities arising from the nature of the systems, fault patterns, and maintenance strategies, these personnel often lack the relevant knowledge and experience(Scott et al., 2022). Simultaneously, the underutilization of resources like maintenance manuals and historical maintenance experiences contributes to the inefficiency in knowledge retrieval and generation of maintenance plans, requiring significant time investments and resulting in low maintenance efficiency(Gawde et al., 2023). The knowledge integration and decision reasoning capabilities inherent in LLM offer a potential solution to break through the current bottlenecks in maintenance operations. This notion is reinforced by the successful application of LLM in fields such as finance and healthcare, where they have effectively demonstrated their capacity for knowledge synthesis and reasoning. Consequently, leveraging LLM to achieve generative maintenance knowledge retrieval and decision recommendation can address the challenges faced by maintenance personnel in terms of inadequate knowledge and experience, as well as the inefficiency in knowledge retrieval. This approach aims to enhance the utilization of maintenance knowledge and improve decision-making efficiency in the context of maintenance operations.\nTo establish a foundation for maintenance decision support using LLM, a series of steps can be taken. Initially, common knowledge from the maintenance manuals and related resources is collected, followed by preprocessing the data to construct a pre- training corpus specific to the maintenance field. This corpus is then utilized to pre- train the LLM. To enhance the model's performance on specific tasks within the maintenance field, a secondary pre-training phase can be undertaken. During this phase, private user data pertinent to maintenance, such as historical repair records and machinery knowledge, is used for secondary pre-training of the LLM. Additionally, the model undergoes fine-tuning with user-specific maintenance data, such as historical maintenance records and machinery knowledge, to further align it with the user's field- specific requirements. The model should be equipped to handle both structured data like FMECA tables and unstructured inputs like fault descriptions and knowledge retrieval queries. Instructions for generating relevant maintenance decisions or retrieving knowledge can be selected through techniques like manual labeling or model- based approaches. The model can then be fine-tuned using supervised fine-tuning (SFT) or instruction tuning to ensure that it effectively aligns with user requirements. Once fine-tuned, the model can generate maintenance decisions or knowledge retrieval results that match user needs. These outputs can be validated and evaluated for compliance with user requirements. In summary, this approach harnesses the robust generative, knowledge utilization, and reasoning capabilities of LLM to provide efficient maintenance decision support. By doing so, the approach seeks to address challenges such as low utilization of maintenance knowledge and the high expertise required from maintenance personnel, leading to improved maintenance efficiency."}, {"title": "4.3 Paradigm II: Parallel Paradigm of LLM and PHM Model", "content": "Different from the corpus data typically utilized in existing NLP field, data in PHM field commonly comprises time-series data collected by multiple types of sensors, encompassing vibration(Tiwari & Upadhyay, 2021), sound(Ding et al., 2023), electrical signal(Q. Yu et al., 2023), temperature(Jung et al., 2023), pressure(S. Tang et al., 2022), etc. The processing and analysis of typical signal data represent the strengths of conventional PHM models, and extensive research efforts have been dedicated to aspect(Lv et al., 2022; L. Tang et al., 2023). However, conventional PHM models often struggle to meet expected PHM requirements in the case of insufficient data conditions(Pan et al., 2022). In such cases, field-specific knowledge supplementation(Z. Wang et al., 2023; J. Yu & Liu, 2020) or external interventions(J. Wang et al., 2023; T. Zhang et al., 2023) are required to enhance the accuracy and capability of conventional PHM models and their operational patterns. LLM, which is equipped with such capabilities, can harness NLP techniques to process and comprehend specialized field knowledge. Consequently, LLM can be combined with the PHM model to build new parallel models to improve the latter's capabilities, cater to more elevated and detailed PHM demands. This concept of multi-models to realize parallel development has been explored in other specialized fields (Palo et al., 2023). Hence, Paradigm II considers building a parallel development framework of LLM and PHM model upon the mature LLM by integrating its knowledge process, logical reasoning, decision-making, and generalization advantages, which is tailored to guide the training and reinforcement learning processes of PHM models, to solve problems and address challenges related to PHM field. By combining the capabilities of LLM and PHM model through parallel development, we aim to optimize aspects such as algorithm recommendation and updating, thereby enhancing the competency in areas like knowledge-data fusion, interpretability, and credibility. This pursuit culminates in building a novel parallel model designed to tackle the present shortcomings within PHM field, including deficiencies in knowledge utilization, targeted guidance, assistance, and algorithm generalization. The parallel paradigm framework of LLM and PHM model is depicted in Fig.4-7."}, {"title": "4.3.1 Approach 1: PHM Technology of Knowledge and Data Fusion", "content": "The knowledge and data fusion PHM technology route aims to address the issues of underutilized existing knowledge and inadequate fusion of data and knowledge. Guided by the idea of Paradigm II that LLM lead PHM model to achieve parallel development, this approach leverages the substantial capabilities of LLM in knowledge transferring and generalization. It seeks to accomplish the task of knowledge and data fusion by taking PHM data and knowledge as inputs and producing comprehensive reasoning results. This endeavor is realized through typical applications such as knowledge-assisted data analysis and knowledge extraction based on data information. The ultimate goal is to achieve a thorough integration and utilization of both knowledge and data in the field of PHM, utilizing the knowledge transferring and generalization capabilities of LLM to enhance this fusion process.\nThe limitations arising from incomplete data acquisition, inconsistent data quality, challenging feature extraction, and insufficient analysis make it difficult for data-driven PHM models to achieve desired outcomes outcomes(Zio, 2022). These challenges necessitate the involvement of relevant field knowledge to facilitate analysis. In contrast, LLM is trained on vast amounts of text, allowing it to accumulate extensive knowledge and information. The synergy between these two models is manifested as follows: the PHM model processes and analyzes the data to uncover hidden patterns, correlations, and regularities, which are then handed over to the LLM for further processing. Simultaneously, the accumulated knowledge and expertise assist in the data analysis process of PHM model, enhancing efficiency and accuracy. The ultimate outcomes are comprehensive results that integrate both data and knowledge. By harnessing the robust knowledge processing capabilities of LLM and the data processing capabilities of PHM model, it becomes feasible to effectively address the shortcomings of data-driven analysis in terms of comprehensiveness and knowledge integration.\nKnowledge and data fusion technology involves two main components: LLM and PHM model. The fusion aims to address the shortcomings of PHM model, such as excessive reliance on high-quality data and poor interpretability. Leveraging the data processing capabilities of PHM model, series of data processing algorithms are employed to extract valuable information from raw data, thereby enhancing the accuracy of various model outcomes. Simultaneously, the powerful NLP capabilities of LLM are utilized to comprehend and analyze text, enabling the transferring and generalization of accumulated knowledge. Such knowledge is then fused with the results obtained from data processing, aiding the data analysis performed by PHM model and end up with comprehensive outcomes that combine data support with field expertise. Furthermore, the data supported by PHM model can be exploited by LLM to unearth information from the data, thereby enriching the knowledge base. This approach of knowledge and data fusion not only enhances the generalizability of PHM model, remedies the limitations of solely data-driven approaches, but also validates and enriches the knowledge accumulated by LLM."}, {"title": "4.3.2 Approach 2: PHM Algorithm Intelligent Recommendation", "content": "The PHM algorithm intelligent recommendation approach is devised to address the intricate challenges in PHM algorithm design arising from the diverse and complex elements involved. Guided by the idea of Paradigm II that LLM lead PHM model to achieve parallel development, this approach capitalizes on the knowledge"}]}