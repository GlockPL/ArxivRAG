{"title": "ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills", "authors": ["Tairan He", "Jiawei Gao", "Wenli Xiao", "Yuanhang Zhang", "Zi Wang", "Jiashun Wang", "Zhengyi Luo", "Guanqi He", "Nikhil Sobanbab", "Chaoyi Pan", "Zeji Yi", "Guannan Qu", "Kris Kitani", "Jessica Hodgins", "Linxi \"Jim\" Fan", "Yuke Zhu", "Changliu Liu", "Guanya Shi"], "abstract": "Humanoid robots hold the potential for unparalleled versatility for performing human-like, whole-body skills. However, achieving agile and coordinated whole-body motions remains a significant challenge due to the dynamics mismatch between simulation and the real world. Existing approaches, such as system identification (SysID) and domain randomization (DR) methods, often rely on labor-intensive parameter tuning or result in overly conservative policies that sacrifice agility. In this paper, we present ASAP (Aligning Simulation and Real Physics), a two-stage framework designed to tackle the dynamics mismatch and enable agile humanoid whole-body skills. In the first stage, we pre-train motion tracking policies in simulation using retargeted human motion data. In the second stage, we deploy the policies in the real world and collect real-world data to train a delta (residual) action model that compensates for the dynamics mismatch. Then ASAP fine-tunes pre-trained policies with the delta action model integrated into the simulator to align effectively with real-world dynamics. We evaluate ASAP across three transfer scenarios-IsaacGym to IsaacSim, IsaacGym to Genesis, and IsaacGym to the real-world Unitree G1 humanoid robot. Our approach significantly improves agility and whole-body coordination across various dynamic motions, reducing tracking error compared to SysID, DR, and delta dynamics learning baselines. ASAP enables highly agile motions that were previously difficult to achieve, demonstrating the potential of delta action learning in bridging simulation and real-world dynamics. These results suggest a promising sim-to-real direction for developing more expressive and agile humanoids.", "sections": [{"title": "I. INTRODUCTION", "content": "For decades, we have envisioned humanoid robots achieving or even surpassing human-level agility. However, most prior work [46, 74, 47, 73, 107, 19, 95, 50] has primarily focused on locomotion, treating the legs as a means of mobility. Recent studies [10, 25, 24, 26, 32] have introduced whole-body expressiveness in humanoid robots, but these efforts have primarily focused on upper-body motions and have yet to achieve the agility seen in human movement. Achieving agile, whole-body skills in humanoid robots remains a fundamental challenge due to not only hardware limits but also the mismatch between simulated dynamics and real-world physics.\nThree main approaches have emerged to bridge the dynamics mismatch: System Identification (SysID) methods, domain randomization (DR), and learned dynamics methods. SysID methods directly estimate critical physical parameters, such as motor response characteristics, the mass of each robot link, and terrain properties [102, 19]. However, these methods require a pre-defined parameter space [49], which may not fully capture the sim-to-real gap, especially when real-world dynamics fall outside the modeled distribution. SysID also often relies on ground truth torque measurements [29], which are unavailable on many widely used hardware platforms, limiting its practical applicability. DR methods, in contrast, first train control policies in simulation before deploying them on real-world hardwares [85, 79, 59]. To mitigate the dynamics mismatch between simulation and real-world physics, DR methods rely on randomizing simulation parameters [87, 68]; but this can lead to overly conservative policies [25], ultimately hindering the development of highly agile skills. Another approach to bridge dynamics mismatch is learning a dynamics model of real-world physics using real-world data. While this approach has demonstrated success in low-dimensional systems such as drones [81] and ground vehicles [97], its effectiveness for humanoid robots remains unexplored.\nTo this end, we propose ASAP, a two-stage framework that aligns the dynamics mismatch between simulation and real-world physics, enabling agile humanoid whole-body skills. ASAP involves a pre-training stage where we train base policies in simulation and a post-training stage that finetunes the policy by aligning simulation and real-world dynamics. In the pre-training stage, we train a motion tracking policy in simulation using human motion videos as data sources. These motions are first retargeted to humanoid robots [25], and a phase-conditioned motion tracking policy [67] is trained to follow the retargeted movements. However, directly deploying this policy on real hardware results in degraded performance due to the dynamics mismatch. To address this, the post-training stage collects real-world rollout data, including proprioceptive states and positions recorded by the motion capture system. The collected data are then replayed in simulation, where the dynamics mismatch manifests as tracking errors. We then train a delta action model that learns to compensate for these discrepancies by minimizing the difference between real-world and simulated states. This model effectively serves as a residual correction term for the dynamics gap. Finally, we fine-tune the pre-trained policy using the delta action model, allowing it to adapt effectively to real-world physics.\nWe validate ASAP on diverse agile motions and successfully achieve whole-body agility on the Unitree G1 humanoid robot [77]. Our approach significantly reduces motion tracking error compared to prior SysID, DR, and delta dynamics learning baselines in both sim-to-sim (IsaacGym to IsaacSim, IsaacGym to Genesis) and sim-to-real (IsaacGym to Real) transfer scenarios. Our contributions are summarized below.\n1) We introduce ASAP, a framework that bridges the sim-to-real gap by leveraging a delta action model trained via reinforcement learning (RL) with real-world data.\n2) We successfully deploy RL-based whole-body control policies in the real world, achieving previously difficult-to-achieve humanoid motions.\n3) Extensive experiments in both simulation and real-world settings demonstrate that ASAP effectively reduces dynamics mismatch, enabling highly agile motions on robots and significantly reducing motion tracking errors.\n4) To facilitate smooth transfer between simulators, we develop and open-source a multi-simulator training and evaluation codebase for help accelerate further research."}, {"title": "II. PRE-TRAINING: LEARNING AGILE HUMANOID SKILLS", "content": "To track expressive and agile motions, we collect a video dataset of human movements and retarget it to robot motions, creating imitation goals for motion-tracking policies, as shown in Figure 3 and Figure 2 (a).\nWe formulate the motion-tracking problem as a goal-conditioned reinforcement learning (RL) task, where the policy \\(\\pi\\) is trained to track the retargeted robot movement trajectories in the dataset \\(D_{Robot}^{Cleaned}\\). Inspired by [67], the state \\(s_t\\) includes the robot's proprioceptions and a time phase variable \\(\\phi \\in [0,1]\\), where \\(\\phi = 0\\) represents the start of a motion and \\(\\phi = 1\\) represents the end. This time phase variable alone is proven to be sufficient to serve as the goal state \\(s_g\\) for single-motion tracking [67]. The proprioceptions is defined as \\([q_{t-4:t}, \\dot{q}_{t-4:t}, w_{root_{t-4:t}}, g_{t-4:t}, a_{t-5:t-1}]\\), with 5-step history of joint position \\(q_t \\in \\mathbb{R}^{23}\\), joint velocity \\(\\dot{q}_t \\in \\mathbb{R}^{23}\\), root angular velocity \\(w_{root} \\in \\mathbb{R}^{3}\\), root projected gravity \\(g_t \\in \\mathbb{R}^{3}\\), and last action \\(a_{t-1} \\in \\mathbb{R}^{23}\\). Using the agent's proprioception \\(s_t\\) and the goal state \\(s_g\\), we define the reward as \\(r_t = R(s_t, s_g^*)\\), which is used for policy optimization. The specific reward terms can be found in Table I. The action \\(a_t \\in \\mathbb{R}^{23}\\) corresponds to the target joint positions and is passed to a PD controller that actuates the robot's degrees of freedom. To optimize the policy, we use the proximal policy optimization (PPO) [80], aiming to maximize the cumulative discounted reward \\(E_{\\pi} [\\Sigma_{t=0}^{T-1} \\gamma^t r_t]\\). We identify several design choices that are crucial for achieving stable policy training:"}, {"title": "III. POST-TRAINING: TRAINING DELTA ACTION MODEL AND FINE-TUNING MOTION TRACKING POLICY", "content": "The policy trained in the first stage can track the reference motion in the real-world but does not achieve high motion quality. Thus, during the second stage, as shown in Figure 2 (b) and (c), we leverage real-world data rolled out by the pre-trained policy to train a delta action model, followed by policy refinement through dynamics compensation using this learned delta action model.\nWe deploy the pretrained policy in the real world to perform whole-body motion tracking tasks (as depicted in Figure 9) and record the resulting trajectories, denoted as \\(D = {s_0, a_0,..., s_T, a_T}\\), as illustrated in Figure 2 (a). At each timestep t, we use a motion capture device and onboard sensors to record the state: \\(s_t = [\\text{phase}, v_{\\text{base}}, a_{\\text{base}}, w_{\\text{base}}, q_t, \\dot{q}_t]\\), where \\(\\text{phase} \\in \\mathbb{R}^3\\) represents the robot base 3D position, \\(v_{\\text{base}} \\in \\mathbb{R}^3\\) is base linear velocity, \\(a_{\\text{base}} \\in \\mathbb{R}^4\\) is the robot base orientation represented as a quaternion, \\(w_{\\text{pase}} \\in \\mathbb{R}^3\\) is the base angular velocity, \\(q_t \\in \\mathbb{R}^{23}\\) is the vector of joint positions, and \\(\\dot{q}_t \\in \\mathbb{R}^{23}\\) represents joint velocities.\nDue to the sim-to-real gap, when we replay the real-world trajectories in simulation, the resulting simulated trajectory will likely deviate significantly from real-world recorded trajectories. This discrepancy is a valuable learning signal for learning the mismatch between simulation and real-world physics. We leverage an RL-based delta/residual action model to compensate for the sim-to-real physics gap.\nAs illustrated in Figure 2 (b), the delta action model is defined as \\(\\Delta a_t = \\pi_{\\Delta}(s_t, a_t)\\), where the policy \\(\\pi_{\\Delta}\\) learns to output corrective actions based on the current state \\(s_t\\) and the action \\(a_t\\). These corrective actions (\\(\\Delta a_t\\)) are added to the real-world recorded actions (\\(a_t^*\\)) to account for discrepancies between simulation and real-world dynamics.\nThe RL environment incorporates this delta action model by modifying the simulator dynamics as follows: \\(s_{t+1} = f_{sim}(s_t, a_t^* + \\Delta a_t)\\) where \\(f_{sim}\\) represents the simulator's dynamics, \\(a_t^*\\) is the reference action recorded from real-world rollouts, and \\(\\Delta a_t\\) introduces corrections learned by the delta action model.\nWith the learned delta action model \\(\\pi_{\\Delta}(s_t, a_t)\\), we can reconstruct the simulation environment with\n\\[s_{t+1} = f_{ASAP}(s_t, a_t) = f_{sim}(s_t, a_t + \\pi_{\\Delta}(s_t, a_t)),\\]\nAs shown in Figure 2 (c), we keep the \\(\\pi_{\\Delta}\\) model parameters frozen, and fine-tune the pretrained policy with the same reward summarized in Table I.\nFinally, we deploy the fine-tuned policy without delta action model in the real world as shown in Figure 2 (d). The fine-tuned policy shows enhanced real-world motion tracking performance compared to the pre-trained policy. Quantitative improvements will be discussed in Section IV."}, {"title": "IV. PERFORMANCE EVALUATION OF ASAP", "content": "In this section, we present extensive experimental results on three policy transfers: IsaacGym [58] to IsaacSim [63], IsaacGym to Genesis [6], and IsaacGym to real-world Unitree G1 humanoid robot. Our experiments aim to address the following key questions:\nQ1: Can ASAP outperform other baseline methods to compensate for the dynamics mismatch?\nQ2: Can ASAP finetune policy to outperform SysID and Delta Dynamics methods?\nQ3: Does ASAP work for sim-to-real transfer?\nExperiments Setup. To address these questions, we evaluate ASAP on motion tracking tasks in both simulation (Section IV-A and Section IV-B) and real-world settings (Section IV-C).\nIn the simulation, we use the retargeted motion dataset from the videos we shoot, denoted as \\(D_{Robot}^{Cleaned}\\), which contains diverse human motion sequences. We select 43 motions categorized into three difficulty levels: easy, medium, and hard (as partially visualized in Figure 6), based on motion complexity and the required agility. ASAP is evaluated through simulation-to-simulation transfer by training policies in IsaacGym and using two other simulators, IsaacSim and Genesis, as a proxy of \"real-world\" environments. This setup allows for a systematic evaluation of ASAP's generalization and transferability. The success of the transfer is assessed by metrics described in subsequent sections.\nFor real-world evaluation, we deploy ASAP on Unitree G1 robot with fixed wrists to track motion sequences that has obvious sim-to-real gap. These sequences are chosen to capture a broad range of motor capabilities and demonstrate the sim-to-real capability for agile whole-body control. Baselines. We have the following baselines:\nOracle: This baseline is trained and evaluated entirely within IsaacGym. It assumes perfect alignment between the training and testing environments, serving as an upper bound for performance in simulation.\nVanilla (Figure 4 a): The RL policy is trained in IsaacGym and evaluated in IsaacSim, Genesis, or the real world.\nSysID (Figure 4 b): We identify the following representative parameters in our simulated model that best align the ones in the real world: base center of mass (CoM) shift (Cx, Cy, Cz), base link mass offset ratio \\(k_m\\) and low-level PD gain ratios \\((k_p^i, k_d^i)\\) where \\(i = 1, 2, ..., 23\\). Specifically, we search the best parameters among certain discrete ranges by replaying the recorded trajectories in real with different simulation parameters summarized in Table VII. We then finetune the pre-trained policy in IsaacGym with the best SysID parameters.\nDeltaDynamics (Figure 4 c): We train a residual dynamics model \\(f_{\\Delta}(s_t, a_t)\\) to capture the discrepancy between simulated and real-world physics. The detailed implementation is introduced in Section VIII-C\nMetrics. We report success rate, deeming imitation unsuccessful when, at any point during imitation, the average difference in body distance is on average further than 0.5m. We evaluate policy's ability to imitate the reference motion by comparing the tracking error of the global body position \\(E_{g-mpjpe}\\) (mm), the root-relative mean per-joint (MPJPE) \\(E_{mpjpe}\\) (mm), acceleration error \\(E_{acc}\\) (mm/frame\u00b2), and root velocity \\(E_{vel}\\) (mm/frame). The mean values of the metrics are computed across all motion sequences used."}, {"title": "V. EXTENSIVE STUDIES AND ANALYSES", "content": "In this section, we aim to thoroughly analyze ASAP by addressing three central research questions:\nQ4: How to best train the delta action model of ASAP?\nQ5: How to best use the delta action model of ASAP?\nQ6: Why and how does ASAP work?\nTo Answer Q4 (How to best train the delta action model of ASAP). we conduct a systematic study on key factors influencing the performance of the delta action model. Specifically, we investigate the impact of dataset size, training horizon, and action norm weight, evaluating their effects on both open-loop and closed-loop performance. Our analysis uncovers the essential principles for effectively training a high-performing delta action model.\na) Dataset Size: We analyze the impact of dataset size on the training and generalization of \\(\\pi_{\\Delta}\\). Simulation data is collected in Isaac Sim, and \\(\\pi_{\\Delta}\\) is trained in Isaac Gym. Open-loop performance is assessed on both in-distribution (training) and out-of-distribution (unseen) trajectories, while closed-loop performance is evaluated using the fine-tuned policy in Isaac Sim. As shown in Figure 10 (a), increasing the dataset size improves \\(\\pi_{\\Delta}\\)'s generalization, evidenced by reduced errors in out-of-distribution evaluations. However, the improvement in closed-loop performance saturates, with a marginal decrease of only 0.65% when scaling from 4300 to 43000 samples, suggesting limited additional benefit from larger datasets.\nb) Training Horizon: The rollout horizon plays a crucial role in learning \\(\\pi_{\\Delta}\\). As shown in Figure 10 (b), longer training horizons generally improve open-loop performance, with a horizon of 1.5s achieving the lowest errors across evaluation points at 0.25s, 0.5s, and 1.0s. However, this trend does not consistently extend to closed-loop performance. The best closed-loop results are observed at a training horizon of 1.0s, indicating that excessively long horizons do not provide additional benefits for fine-tuned policy.\nc) Action Norm Weight: Training \\(\\pi_{\\Delta}\\) incorporates an action norm reward to balance dynamics alignment and minimal correction. As illustrated in Figure 10 (c), both open-loop and closed-loop errors decrease as the action norm weight increases, reaching the lowest error at a weight of 0.1. However, further increasing the action norm weight causes open-loop errors to rise, likely due to the minimal action norm reward dominates in the delta action RL training. This highlights the importance of carefully tuning the action norm weight to achieve optimal performance.\nTo answer Q5 (How to best use the delta action model of ASAP?), we compare multiple strategies: fixed-point iteration, gradient-based optimization, and reinforcement learning (RL). Given a learned delta policy \\(\\pi_{\\Delta}\\) such that:\n\\[f_{sim}(s, a + \\pi_{\\Delta}(s, a)) \\approx f_{real}(s, a),\\]\nand a nominal policy \\(\\hat{\\pi}(s)\\) that performs well in simulation, the goal is to fine-tune \\(\\tilde{\\pi}(s)\\) for real-world deployment. A simple approach is one-step dynamics matching, which leads to the relationship:\n\\[\\tilde{\\pi}(s) = \\hat{\\pi}(s) - \\pi_{\\Delta}(s, \\hat{\\pi}(s)).\\]\nWe consider two RL-free methods: fixed-point iteration and gradient-based optimization. Fixed-point iteration refines \\(\\tilde{\\pi}(s)\\) iteratively, while gradient-based optimization minimizes a loss function to achieve a better estimate. These methods are compared against RL fine-tuning, which adapts \\(\\tilde{\\pi}(s)\\) using reinforcement learning in simulation. The detailed derivation of these two baselines is summarized in Section VIII-D."}, {"title": "VI. RELATED WORKS", "content": "In recent years, learning-based methods have made significant progress in whole-body control for humanoid robots. Primarily leveraging reinforcement learning algorithms [80] within physics simulators [58, 63, 88], humanoid robots have learned a wide range of skills, including robust locomotion [44, 98, 45, 48, 47, 74, 73, 19, 106], jumping [46], and parkour [50, 107]. More advanced capabilities, such as dancing [105, 32, 10], loco-manipulation [25, 53, 15, 24], and even backflipping [78], have also been demonstrated. Meanwhile, the humanoid character animation community has achieved highly expressive and agile whole-body motions in physics-based simulations [71, 86, 55], including cartwheels [67], backflips [69], sports movements [104, 90, 56, 91, 92], and smooth object interactions [86, 17, 22]. However, transferring these highly dynamic and agile skills to real-world humanoid robots remains challenging due to the dynamics mismatch between simulation and real-world physics. To address this challenge, our work focuses on learning and compensating for this dynamics mismatch, enabling humanoid robots to perform expressive and agile whole-body skills in the real world."}, {"title": "VII. CONCLUSION", "content": "We present ASAP, a two-stage framework that bridges the sim-to-real gap for agile humanoid control. By learning a universal delta action model to capture dynamics mismatch, ASAP enables policies trained in simulation to adapt seamlessly to real-world physics. Extensive experiments demonstrate significant reductions in motion tracking errors (up to 52.7% in sim-to-real tasks) and successful deployment of diverse agile skills\u2014including agile jumps and kicks on the Unitree G1 humanoid. Our work advances the frontier of sim-to-real transfer for agile whole-body control, paving the way for versatile humanoid robots in real-world applications."}, {"title": "VIII. LIMITATIONS", "content": "While ASAP demonstrates promising results in bridging the sim-to-real gap for agile humanoid control, our framework has several real-world limitations that highlights critical challenges in scaling agile humanoid control to real-world:\nHardware Constraints: Agile whole-body motions exert significant stress on robots, leading to motor overheating and hardware failure during data collection. Two Unitree G1 robots were broken to some extent during our experiments. This bottleneck limits the scale and diversity of real-world motion sequences that can be safely collected.\nDependence on Motion Capture Systems: Our pipeline requires MoCap setup to record real-world trajectories. This introduces practical deployment barriers in unstructured environments where MoCap setups are unavailable.\nData-Hungry Delta Action Training: While reducing the delta action model to 4 DoF ankle joints improved sample efficiency, training the full 23 DoF model remains impractical for real-world deployment due to the large demand of required motion clips (e.g., > 400 episodes in simulation for the 23 DoF delta action training).\nFuture directions could focus on developing damage-aware policy to mitigate hardware risks, leveraging MoCap-free alignment to eliminate the reliance on MoCap, and exploring adaptation techniques for delta action models to achieve sample-efficient few-shot alignment."}, {"title": "APPENDIX", "content": "To improve the robustness and generalization of the pre-trained policy in Figure 2 (a), we utilized the domain randomization technics listed in Table VI.\nUsing the collected real-world trajectory, we replay the action sequence \\(a_{real}^0,...,a_{real}^N\\) in simulation and record the resulting trajectory \\(s_{sim}^0,...,s_{sim}^N\\). The neural dynamics model \\(f_d^\\theta\\) is trained to predict the difference:\n\\[s_{t+1}^{\\text{real}} - s_{t+1}^{\\text{sim}} = f_d^\\theta(s_t^{\\text{real}}, a_t^{\\text{real}}), \\forall t.\\]\nIn practice, we compute the mean squared error (MSE) loss in an autoregressive setting, where the model predicts forward for K steps and uses gradient descent to minimize the loss. To balance learning efficiency and stability over long horizons, we implement a schedule that gradually increases K during training. Formally, the optimization objective is:\n\\[\\mathcal{L} = \\frac{1}{N} \\sum_{t=0}^N \\sum_{k=0}^K \\|s_{t+K}^{\\text{real}} - f^{\\text{sim}} (\\dots f^{\\text{sim}} (s_t, a_t), a_{t+1}, \\dots, a_{t+K})\\|. \\]\nAfter training, we freeze the residual dynamics model \\(f_d^\\theta\\) and integrate it into the simulator. During each simulation step, the robot's state is updated by incorporating the delta predicted by the dynamics model. In this augmented simulation environment, we finetune the previously pretrained policy to adapt to the corrected dynamics, ensuring improved alignment with real-world behavior.\nTo formalize the problem, we start by assuming one-step consistency between real and simulated dynamics:\n\\[f_{\\text{real}}(s, \\pi(s)) = f_{\\text{sim}}(s, \\pi(s) + \\pi_{\\Delta}(s, \\pi(s))).\\]\nUnder this assumption, one-step matching leads to the condition:\n\\begin{aligned}\n\\pi(s) + \\pi_{\\Delta}(s, \\pi(s)) &= \\hat{\\pi}(s), & \\text{(1)} \\\\\n\\Rightarrow \\pi(s) &= \\hat{\\pi}(s) - \\pi_{\\Delta}(s, \\hat{\\pi}(s)). & \\text{(2)}\n\\end{aligned}\nTo solve Equation (2), we consider:\n1) Fixed-Point Iteration: We initialize \\(y_0 = \\hat{\\pi}(s)\\) and iteratively update:\n\\[y_{k+1} = \\hat{\\pi}(s) - \\pi_{\\Delta}(s, y_k),\\]\nwhere \\(y_k\\) converges to a solution after K iterations.\n2) Gradient-Based Optimization: Define the loss function:\n\\[l(y) = \\|y + \\pi_{\\Delta}(s, y) - \\hat{\\pi}(s)\\|^2.\\]\nA gradient descent method minimizes this loss to solve for y.\nThese methods approximate \\(\\tilde{\\pi}(s)\\), but suffer from OOD issues when trained on limited trajectories. RL fine-tuning, in contrast, directly optimizes \\(\\tilde{\\pi}(s)\\) for real-world deployment, resulting in superior performance.\nProblem of One-Step Matching. Note that Equation (2) is derived from the one-step matching assumption (i.e., \\(\\tilde{\\pi}(s) + \\pi_{\\Delta}(s, \\tilde{\\pi}(s)) = \\hat{\\pi}(s)\\)). For multi-step matching, one has to differentiate through \\(f_{\\text{sim}}\\), which is, in general, intractable. Therefore, both fixed-point iteration and gradient-based optimization assume one-step matching. This also explains the advantages of RL-based fine-tuning: it effectively performs a gradient-free multi-step matching procedure."}]}