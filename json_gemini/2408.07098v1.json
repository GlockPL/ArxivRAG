{"title": "QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value Decomposition", "authors": ["Songchen Fu", "Shaojing Zhao", "Ta Li", "Yonghong Yan"], "abstract": "In multi-agent cooperative tasks, the presence of heterogeneous agents is familiar. Compared to cooperation among homogeneous agents, collaboration requires considering the best-suited sub-tasks for each agent. However, the operation of multi-agent systems often involves a large amount of complex interaction information, making it more challenging to learn heterogeneous strategies. Related multi-agent reinforcement learning methods sometimes use grouping mechanisms to form smaller cooperative groups or leverage prior domain knowledge to learn strategies for different roles. In contrast, agents should learn deeper role features without relying on additional information. Therefore, we propose QTypeMix, which divides the value decomposition process into homogeneous and heterogeneous stages. QTypeMix learns to extract type features from local historical observations through the TE loss. In addition, we introduce advanced network structures containing attention mechanisms and hypernets to enhance the representation capability and achieve the value decomposition process. The results of testing the proposed method on 14 maps from SMAC and SMACv2 show that QTypeMix achieves state-of-the-art performance in tasks of varying difficulty.", "sections": [{"title": "1. Introduction", "content": "As one of the essential branches in multi-agent reinforcement learning (MARL), cooperative MARL has become a solution for many unmanned robot cluster tasks Wang et al. (2022); Xu et al. (2018); Wang et al. (2020a). Unlike competitive and hybrid tasks, collaborative tasks guide the actions of each agent through team rewards to ensure that the goals of each agent are consistent and that there is no competitive relationship. In the distributed training with decentralized execution (DTDE) paradigm, each agent learns independently without utilizing explicit information exchange or global observation Lauer and Riedmiller (2000); De Witt et al. (2020). However, this approach of treating other agents as part of the environment is non-stationary Zhang et al. (2021); Wong et al. (2023). The centralized training with centralized execution (CTCE) paradigm allows for real-time information exchange or global observation to learn the joint policy of all agents through centralized actuators Gupta et al. (2017). Although the CTCE paradigm alleviates the non-stationary problem, most multi-agent systems have partial observability and communication constraints, making it difficult to achieve good adaptation Gronauer and Diepold (2022). Therefore, the centralized training with distributed execution (CTDE) paradigm Kraemer and Banerjee (2016) has been widely applied, which combines the advantages of the previous two paradigms and is easy to apply to various tasks Oliehoek et al. (2008); Foerster et al. (2016); Sunehag et al. (2017); Foerster et al. (2018).\nSimilar to single-agent reinforcement learning (RL), there are two main types of MARL algorithms in the CTDE paradigm: value-based and policy-based methods. The value-based methods mainly study how to decompose the joint value function learned from the team reward function into local value functions, forming individual strategies for each agent. This type of method is generally referred to as Value Function Factorization (VFF) methods, such as VDN Sunehag et al. (2017) and QMIX Rashid et al. (2020), which have shown good performance in many intelligent agent tasks but are challenging to apply to tasks with continuous action spaces. On the other hand, under the CTDE paradigm, policy-based methods directly learn parameterized policies and mainly adopt a centralized-cirtic-distributed-actor structure Foerster et al. (2018); Lowe et al. (2017); Yu et al. (2022). Although policy-based methods can be applied to tasks in continuous action spaces, they can easily lead to local optima under the guidance of team rewards."}, {"title": "2. Related Work", "content": "This article mainly researches value-based methods under the CTDE paradigm. In recent years, many VFF methods have been proposed, among which the two mainstream types of ideas are: increasing the representation ability of VFF by continuously reducing constraints Sunehag et al. (2017); Rashid et al. (2020); Son et al. (2019); Yang et al. (2020); Hu et al. (2021); Shen et al. (2022) and improving the algorithm's exploration ability for policy space and environment interaction Mahajan et al. (2019); Lyu and Amato (2020); Sun et al. (2021); Gupta et al. (2021). The purpose of increasing VFF representation ability is to find more accurate credit assignment methods, and the purpose of growing exploration ability is to improve the sampling efficiency of agents. However, the primitive flat VFF scheme Phan et al. (2021) has two main issues. First, it needs to work on accurately learning the impact of each agent's actions on the team value function through a global state. Second, it faces challenges distinguishing agents' contributions with varying abilities in heterogeneous agent scenarios. These issues will significantly decrease performance as the number of agents in the environment increases.\nIn earlier years, research in natural systems has demonstrated the effectiveness of grouping and division of labor in collaborative labor Gordon (1996); Jeanson et al. (2005); Wittemyer and Getz (2007). Inspired by this, methods of grouping according to specific prior rules or adaptive grouping have also been introduced into MARL, which makes individual contributions to the team more accessible to learn. Specifically, some methods group agents using prior knowledge Lhaksmana et al. (2018); Jiang et al. (2021), some methods automatically group agents based on specific rules during interactions with the environment Shao et al. (2022); Zang et al. (2024), and other methods learn the individuality or roles of the agents Wang et al. (2020b,c); Jiang and Lu (2021). However, due to the ample joint state space of multi-agent systems and the high exploration randomness, learning how to group agents entirely through neural networks places high demands on the design of the loss function. Therefore, we adopt a novel approach that has yet to be explored in prior studies. Rather than grouping agents based on functionality, proximity, or alignment with short-term goals, we focus on defining the roles various agents should fulfill within the team.\nIn this paper, we investigate how to make the learning process from individual action-value functions to the joint action-value function more directional from a new perspective, thereby obtaining a more accurate representation of the individual's impact on the team. To address the abovementioned issues, we propose a novel type-related VFF method, QTypeMix, which performs hierarchical value decomposition based solely on agent types provided by the environment or human input. Specifically, QTypeMix uses the global state to decompose the joint action-value function into type-level value functions and then uses the global state, local observations, and other information to decompose the type-level value functions into local utilities. Additionally, we design a feature extractor that extracts type-related observation embedding from each agent's historical observations to generate the network weights for the second-layer value decomposition process. It is important to note that while our method may seem similar to group-based VFF methods, it differs fundamentally. Rather than dividing agents into teams by type, we guide each to learn strategies most valuable for their specific type in the given task. Our contributions are as follows:\n(1) We propose a novel dual-layer value decomposition method, QTypeMix. This method innovatively leverages local observations and type-related embeddings to provide more direct guidance for the value function decomposition process. Compared to existing methods that only use global states or no information, QTypeMix exploits the common features in the historical observations of agents of the same type. This approach guides the network to different functionalities, increasing learning efficiency.\n(2) We designed a new feature extractor to derive type-related embeddings based on each agent's historical observations, which are trained through an additional loss function. The type-related embeddings guide the neural network in decomposing type-level value functions into local utilities, providing additional information for policy optimization.\n(3) We employed various advanced network architectures to maximize the neural network's representational cap\u0430\u0441ity and integrated the algorithm into mainstream MARL frameworks to ensure its generalization and reproducibility.\nThe experiments in this paper are based on selected scenarios from the StarCraft Multi-Agent Challenge (SAMC) Samvelyan et al. (2019) and SAMCv2 Ellis et al. (2024). The final results show that QTypeMix matches or exceeds the performance of existing SOTA methods in most scenarios and demonstrates outstanding performance in scenarios with many agent types. Our code is available at https://github.com/linkjoker1006/pymarl3."}, {"title": "3. Background", "content": "This paper focuses on fully cooperative multi-agent tasks, where all agents in the environment attempt to maximize a joint reward function while having different individual goals Wong et al. (2023). Similar to modeling single-agent dynamic decision-making in stochastic environments using partially observable Markov decision process (POMDP), fully cooperative multi-agent tasks are typically modeled as decentralized partially observable Markov decision process (Dec-POMDP) Bernstein et al. (2002); Oliehoek et al. (2016). Dec-POMDP is defined as a tuple G = (S,U, P,r, Z, O, \u03b7, \u03b3). s \u2208 S represents the global state of the environment. Each agent $a_i \\in A = \\{a_1, ..., a_n \\}$ takes an action $u \\in U$ at timestep t, forming the joint action $u_t \\in U^n$. The state transition distribution $P(s_{t+1}|s_t, u_t) : S \\times U^n \\times S \\rightarrow [0, 1]$ governs the environment's state transitions caused by the joint actions. $r(s,u) : S\\times U^n \\rightarrow \\mathbb{R}$ defines the team reward function shared by all agents and $\\gamma \\in [0, 1)$ is the discount factor. In a partially observable setting, agent $a_i$ cannot access the global state and can only sample local observation $z^i \\in Z$ through the observation function $O(s, i) : S \\times A \\rightarrow Z$. Therefore, the action-observation history of agent $a_i$ is $\u03c4^i \\inT = (Z\\times U)^*$ on which it conditions a policy $\u03c0^i (u^i|\u03c4^i) : T\\times U \\rightarrow [0, 1]$. The joint policy $\u03c0$ is based on the joint action-value function $Q^\u03c0 (s_t|u_t) = [E_{s_{t+1:\u221e},U_{t+1:\u221e}}[R_t|s_t, u_t]$, where $R_t = \\sum_{k=0}^\u221e \\gamma^k r_{t+k}$ is the discounted reward.\nQMIX: In RL, Deep Q-Network (DQN) Mnih et al. (2015) is the most typical value-based method. This method leverages deep neural networks to approximate the optimal action-value function $Q^*(s,a) = \\max_\u03c0 E[r_t + \u03b3r_{t+1} + \u03b3^2r_{t+2} + \u2026 | s_t = s, a_t = a, \u03c0]$. During training, DQN updates the policy's Q-value function by sampling transitions (s, u, r, s') with batch size B from the replay buffer D and minimizing the squared temporal difference (TD) error:\n\n$\\mathcal{L}(\\theta) = \\sum_{b=1}^{B} \\left[ y_{b}^{DQN} - Q(s, u; \\theta) \\right]^{2}$\n(1)\n\nwhere $y^{DQN} = r + \u03b3 \\max_{u'} Q(s', u'; \\theta^-)$. $\\theta^-$ are the parameters of the target network that are periodically copied from \u03b8 and kept constant for several iterations.\nAfter applying DQN to multi-agent systems, each agent has its independent action-value function $Q_i$. Although allowing multiple agents to update their policies during training independently can be effective in terms of results Tampuu et al. (2017), the non-stationarity caused by the impact of other agent's actions in the environment makes convergence unguaranteed. QMIX Rashid et al. (2020) is one of the most classic value decomposition algorithms, and the algorithm proposed in our paper is also an improvement based on this framework. It employs a mixing network to estimate the joint action-value $Q_{tot}$ as a monotonic combination of individual Q-value $Q_i$ of each agent. By controlling the non-negativity of the weights, QMIX maintains the consistency between centralized and distributed policies. Therefore, its monotonicity constraint can be expressed as:\n\n$\\frac{\\partial Q_{tot}}{\\partial Q_{a}} \\geq 0, \\forall a.$\n(2)\nThis monotonicity constraint enables QMIX to meet the important Individual-Global-Max (IGM) principle proposed in subsequent work Son et al. (2019), which states that for a joint action-value function $Q^*(\u03c4, u) : T^n\\times U^n \\rightarrow \\mathbb{R}$, if there exist individual action-value functions $[Q_i(\u03c4, u) : T \\times U \\rightarrow \\mathbb{R}]_{i=1}^n$, the following holds:\n\n$\\arg \\max_{u} Q^*(\u03c4, u) = \\begin{pmatrix} \\arg \\max_{u^1} Q_1(\u03c4^1, u^1))\\\\ :\\\\ \\arg \\max_{u^n} Q_n(\u03c4^n, u^n) \\end{pmatrix} \\forall \u03c4, u.$\n(3)\nQMIX's mixing network is also implemented through a four-layer structure similar to the hypernetworks Ha et al. (2016). It uses the global state to generate the weights and biases for the value decomposition process, allowing QMIX to access global information during training.\nAttention mechanism: Since its introduction, the attention mechanism Vaswani et al. (2017) has been widely applied in various research fields, and many MARL works Iqbal and Sha (2019); Das et al. (2019); Yang et al. (2020); Zhang et al. (2022); Pu et al. (2022) have also utilized this concept. Qatten utilizes a multi-head attention mechanism to replace the mixing network in QMIX, approximating $Q_{tot}$. AERL Pu et al. (2022) employs the graph attention operator to handle complex agent interactions and capture temporal correlations. An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where a compatibility function of the query with the corresponding key computes the weight assigned to each value. In practice, we compute the attention function on a set of queries simultaneously, packed into a matrix Q. The keys and values are also loaded into matrices K and V. We compute the matrix of outputs as:\n\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V.$\n(4)\nwhere $\\sqrt{d_k}$ represents the dimension of the values. This paper employs the multi-head attention mechanism to jointly enable the model to attend to information from different representation subspaces.\nHyper Policy Network: To overcome the curse of dimensionality in MARL, where the state space grows exponentially with the number of agents, Jianye et al. (2022) proposed Hyper Policy Network (HPN). This method uses inductive biases of permutation invariance (PI) and permutation equivariance (PE) to reduce the multi-agent state space, significantly improving existing MARL methods' performance and learning efficiency. Regarding results, the HPN series algorithms can be considered to have achieved state-of-the-art (SOTA) performance on both SMAC and SMACv2. Therefore, we use this class of methods as the primary baseline. In recognition of the outstanding contribution of HPN, the algorithm proposed in this paper adopts the same network structure for the module that extracts each agent's observation features."}, {"title": "4. Methodology", "content": "In this section, we introduce the overall architecture of QTypeMix and provide a detailed description of its implementation. In QTypeMix, types are defined as labels that distinguish agents with different capabilities or attributes, meaning that agents of the same type can be considered identical. For instance, in the SMAC environment where agent type information is provided, we use this information to categorize different unit types. Agents of the same unit type (e.g., marines) are considered completely identical. In environments where agent type information is not provided (e.g., Multi-Agent Particle Environment (MPE) Lowe et al. (2017)), we categorize agents based on attribute values such as size, acceleration, and maximum speed. If a multi-agent system has n agents A = {a\u2081, ..., an }, we can divide them into m types A = {T\u2081,\u2026\u2026,Tm},0 < m < n, where Tk = {ak\u2081, ..., akn\u2096}, k \u2208 {1,2,..., m}, nk = len(Tk). After categorizing by type, each agent belongs exclusively to one type, which means for \u2200k,l \u2208 {1,2,...,m} and k \u2260 l, T\u2096 \u2229 T\u2081 = \u00d8. Although the experiments in this paper are conducted in SMAC and SMACv2 environments, the above definition of types allows QTypeMix to be easily applied to most multi-agent simulation environments."}, {"title": "4.1. Simple type-wise value decomposition", "content": "A straightforward way to allow the model to distinguish between agents of various types is to introduce an additional layer into the flat VFF scheme. Using QMIX as an example, the algorithm represents the joint action-value function as a weighted sum of individual utilities plus a bias. Based on this setup, we add an extra value decomposition step according to the types of agents. The entire value decomposition process becomes $Q_{tot}$, first decomposed into $Q_{type}^k$, then decomposed into individual utilities $Q^i$. As shown in Fig. 1, this foundational algorithm is referred to as QTypeMix-B(hereafter referred to as QTypeMix-B). By introducing the Type Mixer, we aim for the model to adopt similar value decomposition standards for agents of the same type and ultimately emphasize the differences in value contributed by different types of agents to the task. Like the Total Mixer, the Type Mixer also requires additional global state information from the environment during training, which is unnecessary during execution."}, {"title": "4.2. Additional type information extraction", "content": "In Section 4.1, we proposed an intuitive and straightforward type-wise value decomposition method, aiming to better evaluate the action-values of different types of agents through the Type Mixer. In the attention mechanism of the Type Mixer, we use the agent's current local observation as the key. However, in multi-agent cooperative environments, relying solely on the local observation at the current time step makes it difficult to find associations with the type. For example, observing their local states at a single time step in an environment with two agents with different running speeds does not allow for distinguishing between them. If we can access the historical observations of each agent, we can quickly identify which agent runs faster and then allocate tasks more appropriately based on the mission objectives.\nAs shown in Fig. 3, after obtaining the global state at time step t and the local observations of each agent from the environment, we extract the type embedding $E_i$ for each agent from their historical observations ${Z_t^i, Z_{t-1}^i, ..., Z_{t-k}^i}$ using the Type Embedding Extractor. It consists of GRU units and linear layers generated by hypernetworks. To guide the network in updating in the desired direction, we design the TE loss in addition to the standard RL training process, which means that the updates for TE loss and TD Loss are mutually independent. The specific definition of the TE loss is as follows:\n\n$\\mathcal{L}_{TE}(\\theta_{te}) = \\sum_{b=1}^{B} \\left[ \\sum_{i=1}^{n} \\sum_{j=1}^{n} I(i, j) \\cdot \\cos (E_i(Z^i, S; \\theta_{te}), E_j(Z^j, S; \\theta_{te})) \\right]$\n(5)\n\n$I(i, j) = \\begin{cases} -1, & \\text{if } a_i \\in T_k, a_j \\in T_k. \\\\ 1, & \\text{if } a_i \\in T_k, a_j \\notin T_k. \\end{cases}$\nWe calculate the cosine similarity between the type embeddings of every two agents and use an indicator function I to control the positive or negative contributions, aiming to maximize the embedding differences between different types and minimize the embedding differences within the same type. Under the guidance of the TE loss, the type embeddings will incorporate more information related to the agents' types. For example, type A agents may have higher mobility, type B agents may possess greater attack power, and type C agents may have higher health points."}, {"title": "4.3. Algorithm overview", "content": "After introducing QTypeMix-B and the Type Embedding Extractor, we will present the overall architecture of QTypeMix. As shown in Figure 4, the entire algorithm includes two mixing networks and an embedding extractor, all of which involve using hypernetworks to generate weights and biases. To prevent the weights generated by the hypernetworks from being zeroed out by the activation function when they are negative, QTypeMix, like QMIX, uses ELU as the non-linear function instead of ReLU. Regarding the workflow, the agents' local observations $Z_t^i$ first pass through the HPN and the Type Embedding Extractor to obtain local utilities $Q_t^i$ and type embeddings $E_t^i$. In the homogeneous value decomposition stage, each agent's local observation and type embedding are concatenated to form the key (K). The global state serves as the value (V), and the local utilities serve as the query (Q). These are processed through a multi-head attention mechanism to calculate the type utility. The weights and biases for the multi-head attention and the weights and biases for the weighted summation of the multi-head results are all generated by a hypernetwork that takes the global state $S_t$ as input. In the heterogeneous value decomposition stage, the multiple $Q_{type}^k$ are fed into the Total Mixer, where the weights and biases are entirely generated by the hypernetworks, to obtain the estimated joint action-value $Q_{tot}$.\nAs described earlier, the network update for QTypeMix consists of two backpropagation steps. Although QTypeMix has unique characteristics in the value decomposition process, the TD error used for updating the network is consistent with other value-based methods:\n\n$\\mathcal{L}_{TD}(\\theta) = \\sum_{b=1}^{B} \\left[ r + \\gamma \\max_{u'} Q_{tot}(s', u'; \\theta^-) - Q_{tot}(s, u; \\theta) \\right]^2$\n(6)\nSince the gradient of the TD error passes through four neural networks, \u03b8 in Eq. 6 includes all the network parameters in QTypeMix: (\u03b8te, \u03b8hpn, \u03b8m1, \u03b8m2). That is when updating the TD error, we do not freeze the parameters of the Type Embedding Extractor. Instead, we add a weight \u03b1 to allow the two loss functions to update together. We believe this approach can further ensure the direction of updating and learning efficiency of the Type Embedding Extractor. Therefore, the global optimization objective of QTypeMix is to minimize:\n\n$\\mathcal{L}(\\theta) = \\mathcal{L}_{TD}(\\theta) + \\alpha \\cdot \\mathcal{L}_{TE}(\\theta_{te})$\n(7)\nThe detailed process of the QTypeMix algorithm is outlined in Appendix A."}, {"title": "5. Experiments", "content": "Experimental Setup: We select the commonly used SMAC Samvelyan et al. (2019) and the more challenging SMACv2 Ellis et al. (2024) as benchmarks to evaluate the performance of QTypeMix. According to the official recommendation, we use game version 4.6.2.69232. SMAC is a benchmark suite specifically designed for MARL research. It is based on the real-time strategy game StarCraft II. It offers a range of tasks and scenarios that allow researchers to test their multi-agent algorithms in complex, dynamic environments. In this environment, each allied unit is controlled by an RL agent, which can observe the distances, relative positions, unit types, and health of all ally and enemy units within its field of view at each time step. Built-in rules of the environment control all enemy units. To address the shortcomings of SMAC, SMACv2 introduces three changes: random team compositions, random starting positions, and realistic field of view and attack range. These changes encourage agents to focus more on understanding the observation space and prevent the learning of successful open-loop strategies (strategies conditioned only on the time step). Besides these changes, SMACv2 and SMAC are nearly identical in other aspects. The goal of the ally agents is to eliminate all enemy agents within a certain timeframe, and rewards are only given when enemy units are eliminated and victory is achieved. Moreover, both environments feature discrete joint action and state spaces, making them suitable for VFF methods. For more detailed settings regarding the environment parameters, please refer to Appendix B.1.\nBaseline Selection: Hu et al. (2021) achieves higher win rates by performing code-level optimizations on QMIX Rashid et al. (2020) and VDN Sunehag et al. (2017), and releases these improvements in pymarl2\u00b9. Due to their excellent performance among contemporary algorithms, we include them as one of the baselines, referred to as FT-QMIX and FT-VDN. Furthermore, to address the curse of dimensionality resulting from the increased number of agents, Jianye et al. (2022) introduces the HPN series methods incorporating PI and PE, achieving a 100% win rate in nearly all hard and super-hard SMAC scenarios. Consequently, we chose HPN-QMIX and HPN-VDN as state-of-the-art (SOTA) algorithms. To ensure fairness, all algorithms in this study are developed and tested using their open-source project, pymarl3\u00b2. Therefore, the subsequent sections will present the experimental results of six algorithms: QTypeMix, QTypeMix-B, HPN-QMIX, HPN-VDN, FT-QMIX, and FT-VDN.\nEvaluation Metrics: Since the ultimate goal of both SMAC and SMACv2 is to achieve victory in battles, we use the test win rate across different scenarios as the evaluation metric. The following sections will show how the test win rate changes for different algorithms as the number of training steps increases. Each model is trained for 10,050,000 steps, with a test of 32 episodes conducted every 10,000 steps. To ensure fairness, we use the same training parameters for each algorithm in the same scenarios and do not perform detailed tuning for any specific algorithm. For detailed training parameter settings, please refer to the Appendix B.2. Furthermore, we conducted longer testing periods to obtain more objective and accurate results. Each trained model is tested for win rates over 1280 (32*40) episodes in the SAMC scenario and 5120 (32*160) in the SMACv2 scenario. The difference in the number of test episodes is due to the insufficient randomness in the SMAC scenario when tested in a multi-threaded parallel manner, which results in longer testing times."}, {"title": "5.1. Experiments on SMAC", "content": "Since the proposed QTypeMix is a type-based VFF method, it is foreseeable that there will be no performance improvement in scenarios involving a small number of types. In SMAC, most scenarios involve only 1 to 2 types of agents and have limited randomness. Therefore, we selected 1 hard map and 4 super hard maps for our experiments."}, {"title": "5.2. Experiments on SMACv2", "content": "As noted earlier, SMAC maps are not fully capable of showcasing the superior performance of QTypeMix. Therefore, we conduct experiments on 9 maps selected from SMACv2. We group these maps according to the number of ally and enemy agents: 5 vs. 5 (protoss 5 vs. 5, terran 5 vs. 5, zerg 5 vs. 5), 10 vs. 11 (protoss 10 vs. 11, terran 10 vs. 11, zerg 10 vs. 11), and 15 vs. 16 (protoss 15 vs. 16, terran 15 vs. 16, zerg 15 vs. 16). On these maps, regardless of the number of allied agents, three types of units are generated with probabilities of [0.45, 0.45, 0.1]. The significant amount of randomness introduced makes SMACv2 maps much more challenging than those in SMAC. Achieving victory is almost impossible when there is a significant disparity in the initial unit strengths or unfavorable positioning.\nAs shown in Table 1 and Fig. 7, the experimental results on 9 SMACv2 maps indicate that QTypeMix exhibits the highest win rate among the 6 algorithms on all maps. This is particularly evident in scenarios with a larger number of ally agents (10 vs. 11 and 15 vs. 16), where the win rate of QTypeMix significantly improves. This confirms our earlier hypothesis that QTypeMix demonstrates its superiority in scenarios with a higher number of ally agent types (although the 5 vs. 5 series maps also feature three types of agents randomly, due to the limited number of agents, often only two types are present simultaneously). On the 5 vs. 5 series maps, the performance of QTypeMix, QTypeMix-B, and HPN-QMIX is similar. The test win rates in 10 vs. 11 and 15 vs. 16 series maps follow the general trend of QTypeMix > QTypeMix-B > HPN-QMIX. Comparing the results of QTypeMix and QTypeMix-B reveals the effectiveness of extracting type embeddings. QTypeMix provides a more precise direction for the model's learning, resulting in faster convergence speeds on most maps, even though its neural network is larger.\nIt is important to note that our objective is not to achieve the highest possible win rate for any specific algorithm through meticulous hyperparameter tuning. Instead, we adopt the algorithm parameter settings from Hu et al. (2021), allowing for a fair comparison by keeping common parameters consistent across all algorithms. This approach ensures that the conclusions drawn are more objective. Although fine-tuning the training and model parameters might yield better results on SMACv2 maps, that is beyond the scope of our study. For more detailed information on model parameters, please refer to the Appendix B.3."}, {"title": "6. Conclusion", "content": "Our core concept is to enable each agent to recognize the roles they are best suited for in heterogeneous multi-agent cooperative tasks. Therefore, this paper proposes a dual-layer VFF method, QTypeMix, which introduces type information. This method divides the value decomposition process into homogeneous and heterogeneous stages based on the type-related information provided by the environment. Utilizing fine-grained value decomposition accelerates learning efficiency. Additionally, it extracts hidden features of different types from the agents' historical observations, providing a more precise direction for the training process. Experimental results on SMAC and SMACv2 indicate that QTypeMix performs well in scenarios with fewer agents and types and demonstrates impressive improvements in more complex scenarios with more agents and types.\nHowever, QTypeMix also has some issues. The larger size mixers slow down the training speed of the model. In future work, we will explore filtering more practical information to reduce the model's dimensionality. Additionally, QTypeMix is just an instance of our core idea applied to the VFF method. One of our future research focuses will be on how to apply this concept to policy-based methods."}, {"title": "A. Architecture details", "content": ""}, {"title": "B. Experiment details", "content": "The SMAC and SMACv2 officials provide environment parameters for users to configure maps. For the sake of fairness, we primarily use the default parameters. It is important to note that since our algorithm does not target the exploration problem of agents, we set the parameter conic_fov to False on SMACv2 maps. This means that in our experiments, agents have a circular field of view on SMACv2 maps, similar to SMAC maps."}, {"title": "B.2. Training parameters", "content": "Training parameters in Table B.1 are consistent across all maps and algorithms. \u03f5 represents the probability of an agent taking random actions during training. Typically, a larger \u03f5 is used at the beginning of the algorithm to increase the diversity of samples. As we have emphasized multiple times, we do not aim to achieve better performance through fine-tuning parameters for any specific algorithm. Therefore, we adopt the parameter configurations from Jianye et al. (2022), which results in some maps using unique configurations. For example, batch_size_run controls the number of parallel environments, set to 4 on 3s5z vs. 3s6z and 8 on other maps. td_lambda is set to 0.3 on 6h vs. 8z and 0.6 on other maps. hpn_head_num is set to 2 on 3s5z vs. 3s6z and 6h vs. 8z, and 1 on other maps. Notably, hpn_head_num does not affect FT-QMIX and FT-VDN."}, {"title": "B.3. Model parameters", "content": "Model parameters are used to configure the neural networks involved in the algorithm. Table B.1 lists some important model parameters applied to all mentioned algorithms (if needed). mixing_embed_dim, hypernet_embed_dim, hpn_hyper_dim, and rnn_hidden_dim represent the output dimension of the linear layers in the Mixers, the internal dimension of the hypernetworks in the Mixers, the internal dimension of the hypernetworks in HPN, and the hidden state dimension of the GRUs, respectively. n_heads sets the number of attention heads for QTypeMix and QTypeMix-B."}, {"title": "CRediT authorship contribution statement", "content": "Songchen Fu: Conceptualization, Data curation, Formal analysis, Methodology, Project administration, Investigation, Software, Visualization, Writing \u2013 original draft, Writing \u2013 review and editing. Shaojing Zhao: Methodology, Investigation, Writing \u2013 review and editing. Ta Li: Supervision, Methodology, Writing \u2013 review and editing. Yonghong Yan: Supervision, Conceptualization, Resources."}]}