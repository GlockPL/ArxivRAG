{"title": "InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models", "authors": ["Hao Li", "Xiaogeng Liu", "Chaowei Xiao"], "abstract": "Prompt injection attacks pose a critical threat to large language models (LLMs), enabling goal hijacking and data leakage. Prompt guard models, though effective in defense, suffer from over-defense-falsely flagging benign inputs as malicious due to trigger word bias. To address this issue, we introduce NotInject, an evaluation dataset that systematically measures over-defense across various prompt guard models. NotInject contains 339 benign samples enriched with trigger words common in prompt injection attacks, enabling fine-grained evaluation. Our results show that state-of-the-art models suffer from over-defense issues, with accuracy dropping close to random guessing levels (60%). To mitigate this, we propose InjecGuard, a novel prompt guard model that incorporates a new training strategy, Mitigating Over-defense for Free (MOF), which significantly reduces the bias on trigger words. InjecGuard demonstrates state-of-the-art performance on diverse benchmarks including NotInject, surpassing the existing best model by 30.8%, offering a robust and open-source solution for detecting prompt injection attacks. The code and datasets are released at https://github.com/SaFoLab-WISC/InjecGuard.", "sections": [{"title": "1 Introduction", "content": "Prompt injection attacks (Perez and Ribeiro, 2022; Greshake et al., 2023; Liu et al., 2024) represent a serious and emerging threat to the security and integrity of large language models (LLMs) (Brown et al., 2020). These attacks exploit the models' reliance on natural language inputs by inserting malicious or manipulative prompts, leading to undesirable behaviors such as goal hijacking or sensitive data leakage. For instance, a well-known prompt injection technique involves instructing the LLM to \"ignore previous instructions\u201d, (Branch et al., 2022; Harang, 2023a; Perez and Ribeiro, 2022; Willison, 2022) which can override built-in safeguards and enable the execution of unauthorized actions. As the deployment of LLMs continues to expand across industries, the risk posed by prompt injection attacks becomes more pronounced, demanding effective and efficient defense mechanisms.\nTo address it, prompt guard models (Meta, 2024; ProtectAI.com, 2024; Deepset, 2024b; fmops, 2024; LakeraAI, 2024a) have recently been proposed as a promising solution. These models work by analyzing the semantic meaning of the input data to detect malicious intent before it reaches the LLM. Unlike LLM models, prompt guard models are lightweight and computationally efficient, as they do not require the high inference cost associated with LLMs. Additionally, these models operate independently, without the need for victim LLM's responses, further reducing computation costs compared to approaches like LLM guardrails (Inan et al., 2023). These attributes make prompt guard models adaptable to various environments, and an attractive solution in scenarios where speed and resource optimization are critical. We Despite these advantages, we find that existing prompt guard models face a critical limitation: the issue of over-defense. Over-defense arises when models misclassify inputs due to reliance on shortcuts, resulting in false positives where benign inputs are incorrectly flagged as threats. For instance, as shown in Fig. 2, commonly used words such as \"ignore\" or \"cancel\" can be in part of harmless sentences but are misclassified as malicious by many existing (even commercial) prompt guard models. Such over-defense problem reduces LLM accessibility, as the prompt guard model may reject legitimate user requests and block access. It can also cause significant disruptions in real-world applications, particularly in interactive systems like virtual assistants (Dong et al., 2023) and medical diagnostic tools (Thirunavukarasu et al., 2023), where immediate and reliable access is crucial.\nTo address this issue, we introduce NotInject, an evaluation dataset specifically designed to assess the over-defense issue of existing models. The dataset contains 339 carefully crafted benign inputs, developed using statistical methods on existing benign and attack datasets. The test cases in NotInject contain trigger words commonly found in prompt injection attacks, while still preserving benign intent. We further divide the dataset into three levels of difficulty, based on the number of trigger words present, enabling more fine-grained evaluation. Through systematic evaluations based on NotInject, we demonstrate that current prompt guard models, including current state-of-the-art (SotA) open-source solutions like ProtectAIv2 (ProtectAI.com, 2024), suffer from significant over-defense issues, with over-defense accuracy falling below 60%, which is close to random guessing (50%).\nIn addition to the dataset, we also propose a powerful prompt guard model, InjecGuard, which achieves a superior score in both performance and efficiency compared to other guardrail models (see Fig. 1). Since the training approach of existing prompt guard models are all closed-source and training data is not released, our journey starts with curating a comprehensive collection of training datasets with carefully designed data-centric augmentation techniques for addressing the long-tail problem. To further address the over-defense problem, instead of directly finetuning on a specific dataset (e.g, NotInject), which introduces unfair evaluation results, here, we introduce Mitigating Over-defense for Free (MOF), without relying on any specific over-defense datasets. As a result, InjecGuard achieves SotA performance across multiple benchmarks, including NotInject. Evaluation results show that InjecGuard outperforms existing prompt guard models, achieving over 83% average accuracy in detecting benign, malicious, and over-defense inputs, surpassing the open-sourced runner-up prompt guard model by 30.8%. Remarkably, InjecGuard achieves similar performance to GPT-40 (OpenAI, 2024a), an advanced commercial LLM, while being an lightweight model trained on DeBERTa (He et al., 2023). Furthermore, our model reaches this performance using a fully open-source dataset, unlike some existing prompt guard models that rely on closed datasets (fmops, 2024; Meta, 2024; ProtectAI.com, 2024; LakeraAI, 2024a), further promoting a transparency and open-source academic research environment."}, {"title": "2 Related Works", "content": "Prompt injection attacks. Prompt injection attacks have become a critical threat to LLMs. Since LLMs process inputs in natural language, they often face challenges distinguishing between legitimate user commands and external manipulative inputs, leaving them vulnerable. The concept is first identified in research by Perez and Ribeiro (2022), revealing that LLMs could be misled by simple, crafted inputs, resulting in goal hijacking and prompt leakage. Several studies have been proposed (Greshake et al., 2023; Wang et al., 2023; Pedro et al., 2023; Yan et al., 2023; Yu et al., 2023; Salem et al., 2023; Yip et al., 2024; Zhan et al., 2024; Liu et al., 2024; Pasquini et al., 2024; Shi et al., 2024), addressing various aspects of prompt injection attacks, such as handcrafted methods (Toyer et al., 2023), automatic attack algorithms (Liu et al., 2024), and benchmarks (Liu et al., 2023; Debenedetti et al., 2024). Public discussions (Harang, 2023b; Willison, 2022, 2023) have also underscored the risks of prompt injection attacks on commercial LLMs. Prompt Injection datasets are also introduced, such as PINT (LakeraAI, 2024b), Safeguard-Injection (Erdogan et al., 2024), TaskTracker (Abdelnabi et al., 2024), and BIPIA (Yi et al., 2023), etc.\nPrompt guard models. Prompt guard models"}, {"title": "3 Over-defense Dataset: NotInject", "content": "3.1 The Over-defense Issue\nWhile prompt guards offer several advantages, such as low overhead as previously mentioned, we have identified a critical limitation: they exhibit severe over-defense issues, even in some advanced models (Meta, 2024; ProtectAI.com, 2024). Specifically, these models learn a shortcut from certain trigger words, like \"ignore\", directly to the final prediction. As shown in Fig. 3 with red lines, ProtectAIv2 (ProtectAI.com, 2024), one of the SotA prompt guard models on PINT benchmark, distributes excessive and unbalanced attention to the word \"ignore\", leading it to incorrectly categorize a benign input as malicious. To further illustrate, we input a benign sentence containing the word \"ignore\" into two advanced prompt guard models: PromptGuard (Meta, 2024) and ProtectAIv2 (ProtectAI.com, 2024), and present the results in Fig. 2. Both models incorrectly classify the benign instruction containing the word \"ignore\" as malicious.\nIn this paper, we define the over-defense issue in prompt guard models as the tendency to predict malicious labels when benign sentences contain certain trigger words commonly used in prompt injection attacks.\n3.2 Construction of NotInject\nTo enhance the community's ability to address the above issues, we introduce an over-defense evaluation dataset that supports systematically evaluating the over-defense issue inherent in prompt guard models. As shown in Fig. 4, to build NotInject, there are three main steps: 1) Trigger word identification, which aims to find candidate trigger words likely to cause over-defense; 2) Trigger word refinement, which filters out misidentified words from the first step; and 3) Corpus generation, which uses LLMs to generate over-defense test cases based on the identified trigger words.\nTrigger Words Identification. We begin by collecting two primary datasets: a dataset containing known prompt injection attack examples, denoted as Dm (malicious dataset), and a benign dataset comprising typical user inputs devoid of malicious intent, denoted as Db. We collect these data from several open-source datasets, such as Alpaca (Taori et al., 2023) and Safeguard-Injection (Erdogan et al., 2024) (detailed information is provided in the Appendix. A). The injection dataset includes various malicious inputs crafted to manipulate LLMs, whereas the benign dataset represents non-harmful user interactions. Then, as illustrated in Fig. 4, we perform a word frequency analysis on both datasets. For each dataset, we compute the word frequencies to obtain frequency lists Fb and Fm for the benign and malicious datasets, respectively. We rank these words based on their frequencies from highest to lowest, resulting in two separate lists sorted by occurrence rates. Next, to identify injection-specific words, we compare the word frequency rank between the two datasets. By calculating the rank difference through, \\( \\Delta r(w) = R_b(w) - R_m(w) \\), we recognize words that are more frequent in the injection dataset but less common in the benign dataset. Words that exhibit a significantly higher frequency in the injection dataset are flagged as potential trigger words associated with prompt injections. Our detailed algorithm is shown in Alg. 1. We also visualize the top 20 words identified as trigger words by our method in Fig. 5.\nTrigger Words Refinement. Recognizing that automated methods may include irrelevant or common words not indicative of prompt injection attempts, we proceed with a word rectification process. We first employ LLM to automatically filter the list of candidate words to remove any that may not be pertinent. Specifically, We assess the potential harmfulness of words by asking GPT-40-mini (OpenAI (2024b), we use the 2024-07-18 version"}, {"title": "Algorithm 1 Trigger Words Identification", "content": "Require: Benign dataset Db, Malicious dataset Dm, Integer k\nCompute word frequencies in Db to get frequency list Fb\nCompute word frequencies in Dm to get frequency list Fm\nSort Fb in descending order to get rank list Rb\nSort Fm in descending order to get rank list Rm\nfor all words w in Rb U Rm do\nCompute rank difference (\\( \\Delta r(w) = R_b(w) - R_m(w) \\))\nAdd (w, \\( \\Delta r(w) \\)) to list L\nend for\nSort list L in descending order based on Af(w)\nreturn Top k words from list L\nby default unless otherwise specified) questions like, \"Do you think the word of {word} is especially frequent in malicious or prompt attack scenarios?\u201d Next, we perform a manual verification step, where we review the remaining words after LLM filtering to ensure that any words unrelated to prompt injection attacks are removed. Through these two steps, we have compiled a word list with the highest potential for malicious use.\nCorpus Generation. With a refined list of trigger words, we then generate sample sentences. We instruct the GPT-40-mini to craft new sentences that must include a specified number of the selected trigger words. Specifically, we select the generated 113 trigger words and create three subsets with distinct difficulty levels, determined by the number of trigger words used in the generation of new sentences. Namely, three subsets separately containing 1, 2, and 3 trigger words are built with 113 benign sentences per subset. It is imperative that these sentences are contextually coherent, semantically meaningful, and do not contain any prompt injection instructions or malicious content. The goal is to represent benign usage of the trigger words in everyday language, ensuring that the sentences reflect natural and diverse linguistic patterns. To accomplish this, we design a carefully curated prompt (see Appendix. C.1) to guide the LLM in generating safe and natural sentences. We then implement a polish process to further ensure the safety of the generated samples. In this process, the generated sentences combined with the prompt in Appendix. C.2 are re-input into the LLM to identify any potential injection vulnerabilities. Following this, we conduct a manual review to confirm the safety of all sentences and report the error ratio of three subsets in Fig. 8. This multi-step refinement process guarantees that all generated sentences are harmless. The final output forms the proposed NotInject, which contains a total of 339 generated samples with 113 for one-word subset, 113 for two-word subset, and 113 for three-word subset. NotInject encompasses a diverse set of topics to enable a thorough evaluation, including common queries from daily life, technique queries (eg., programming, system), virtual creations, and multilingual queries (eg., Chinese, Russian, etc.). The detailed category distribution is presented in Table. 1."}, {"title": "3.3 Evaluations on NotInject", "content": "Here, we present our evaluations based on the proposed NotInject, assessing 5 existing prompt guard models and an advanced LLM-based guardrail. To provide a comprehensive evaluation, in addition to the over-defense evaluation, we employ a three-dimensional metric. First, we measure malicious accuracy, which reflects how effectively the models detect malicious inputs using attack data from the PINT (LakeraAI, 2024b) and BIPIA datasets (Yi et al., 2023). Second, we evaluate benign accuracy, i.e., how accurately the models classify benign data as non-malicious, using benign data from both the PINT and WildGuard benchmark (Han et al., 2024). Finally, we assess over-defense accuracy, which captures the models' performance when encountering benign inputs that contain trigger words like \u201cignore\u201d, using our proposed NotInject dataset.\nBased on above, we evaluate existing prompt guard models (i.e., Deepset (Deepset, 2024b), Fmops (fmops, 2024), PromptGuard (Meta, 2024), ProtectAIv2 (ProtectAI.com, 2024), LakeraGuard (LakeraAI, 2024a)) and an advanced LLM-based guardrail designed to detect malicious content (i.e., LlamaGuard3 (Dubey et al., 2024)).\nThe results are shown in Fig. 6, where the x, y, and z axes represent malicious accuracy, benign accuracy, and over-defense accuracy, respectively. As illustrated, none of the existing prompt guard models (i.e., Deepset (Deepset, 2024b), Fmops (fmops, 2024), PromptGuard (Meta, 2024),"}, {"title": "4 InjecGuard", "content": "In this section, we aim to develop a robust prompt guard model that excels in malicious accuracy, benign accuracy, and over-defense accuracy. We also want to emphasize that existing prompt guard models are typically trained in a closed-source manner and disclose their training data. Our goal is to train our model using only publicly available data, thereby promoting transparency and facilitating academic research. Here, we introduce InjecGuard, a prompt guard model that achieves state-of-the-art performance across various benchmarks. To build InjecGuard, we first curate a comprehensive collection of open-source datasets for training and apply data-centric augmentation techniques to address the long-tail problem within the dataset. More importantly, we propose a novel training strategy, MOF, which effectively mitigates over-defense issues without relying on specific over-defense datasets.\n4.1 Data Collection and Augmentation\nSimilar to existing models, we first collect a wide range of prompt injection and benign corpus as the training data. Specifically, we select 20 open-source datasets, including the benign datasets like Alpaca (Taori et al., 2023), the prompt injection datasets like Safeguard-Injection (Erdogan et al., 2024), and the datasets generated by existing injection attack, such as TaskTrack (Abdelnabi et al., 2024)). However, upon analyzing these datasets, we identify a long-tail issue in these datasets: certain input formats frequently exploited in prompt injection attacks\u2014such as CSV files are under-represented. To address this, we implement a data-centric augmentation procedure, generating additional data for these long-tail formats. Using GPT-40-mini, we create some prompt injection samples in 17 formats, including Email, Document, Chat Conversation, JSON, Code, Markdown, HTML, URL, Base64, Table, XML, CSV, Config File, Log File, Image Link, Translation, and Website. After augmentation, our final training dataset for InjecGuard comprises 61,089 benign samples and 15, 666 prompt injection samples, where 435 samples are generated by our data-centric augmentation procedure. Detailed statistics of the training data and prompt for data-centric augmentation are provided in the Appendix. A and C.3, respectively.\n4.2 Mitigating Over-defense for Free (MOF)\nTo address the over-defense issue, we propose Mitigating Over-defense for Free (MOF), a novel method to identify and mitigate such biases, without relying on any specific over-defense datasets.\nAfter training our model with the aforementioned data (Sec. 4.1) using standard supervised learning, we perform a \"token-wised recheck\u201d to identify biases in the trained model. This procedure takes every token in the tokenizer vocabulary and inputs each token individually into the trained model. Ideally, these tokens should all be considered \"benign\u201d since they are individual tokens without any intent of prompt injection attack. By doing this, we can identify any biased words that are incorrectly predicted as \"attack\u201d by the model. We consider the bias toward these tokens as the root cause of the over-defense issue in the model. After identifying the biased tokens, we prompt GPT-40-mini to generate benign data (see Appendix. C.1) using random combinations of these tokens (including one-token, two-token, and three-token settings, as mentioned in Sec. 3.2). We generate 1,000 benign samples using this method. Afterward, a LLM refinement process similar to the method mentioned in Sec. 3.2 is employed to ensure the harmlessness of generated data. Subsequently, we incorporate these generated data into the training data introduced in Sec. 4.1 to form the final training dataset. Using this dataset, we retrain our prompt guard model from scratch and get the final InjecGuard.\nAs illustrated in Fig. 3, the retrained model focuses and makes the final prediction on the overall meaning of the entire input, rather than certain trigger words. And results in Fig. 6 show that our model achieves high accuracy across all three dimensions simultaneously. We also present ablation studies of the aforementioned training design in Tab. 3. More detailed evaluation results are provided in the following section."}, {"title": "5 Evaluations", "content": "5.1 Experimental Setups\nEvaluation Datasets. As mentioned in Sec. 3.3, we conduct our evaluations based on three aspects: benign accuracy, malicious accuracy, and over-defense accuracy. To fully verify the practicality of our proposed InjecGuard, we ensure that all evaluation datasets are not included in the training data. Specifically, For evaluating the benign accuracy, we utilize benign data from both the PINT benchmark (LakeraAI, 2024b) and WildGuard benchmark (Han et al., 2024). For evaluating the malicious accuracy, We use attack data from both the PINT benchmark (LakeraAI, 2024b) and BIPIA datasets (Yi et al., 2023). For evaluating the over-defense accuracy, we use the proposed NotInject dataset.\nMetrics. Since prompt guard models function as text classification systems that predict whether an input text is benign or malicious, we evaluate their performance using Accuracy, calculated as the proportion of correct predictions over the total number of test cases in the evaluation dataset:\n\\[ Accuracy = \\frac{Number \\space of \\space Correct \\space Predictions}{Total \\space Number \\space of \\space Test \\space Cases} \\] (1)\nAdditionally, we report the computational overhead in terms of Giga Floating Point Operations (GFLOPs), which quantifies the total number of floating-point operations required during inference. GFLOPS provide an estimate of the computational resources needed by the model. We also measure the inference time to assess the computational overhead introduced by the models.\nTraining Details of InjecGuard. We employ DeBERTaV3-base (He et al., 2023) as the backbone of InjecGuard and train it with a batch size of 32 for 3 epochs, using Adam (Diederik, 2015) optimizer and linear scheduler. The initial learning rate is set to 2e-5, with a 100-step warm-up phase. Besides, the maximum token length is set to 512.\nBaselines. We employ five existing prompt guard models as baselines: Fmops (fmops, 2024), Deepset (Deepset, 2024b), PromptGuard (Meta, 2024), ProtectAIv2 (ProtectAI.com, 2024), and LakeraGuard (LakeraAI, 2024a), which have been introduced in Sec. 2. We also consider LLM-based methods, including LlamaGuard3 (Dubey et al., 2024), Llama-2-chat-7b (Touvron et al., 2023), and GPT-40 (OpenAI, 2024a), which are evaluated by prompting them to determine a given input constitutes a prompt injection attack.\n5.2 Main Results\nWe compare our method with other baselines in terms of performance and overhead. The results are shown in Tab. 2 (detailed results for each subsets deferred to Appendix. B.1).\nPerformance Comparison. Our InjecGuard demonstrates superior performance compared to existing prompt guard models and even rivals commercial LLMs like GPT-40. Specifically, InjecGuard achieves an average accuracy of 83.48%, the best-performing commercial prompt guard model, by 6.25%, and exceeding the top open-source model, ProtectAIv2, by 30.83%. Our model excels across all evaluation categories, attaining an over-defense accuracy of 87.32%, benign accuracy of 85.74%, and malicious accuracy of 77.39%. This balanced performance indicates that InjecGuard is highly effective at correctly identifying benign and malicious inputs while minimizing false positives and negatives. The fact that InjecGuard achieves results comparable to GPT-40, despite being based"}, {"title": "5.3 Ablation Studies", "content": "The ablation study presented in Tab. 3 offers key insights into the effects of our training components on InjecGuard performance. Starting with the Basic Dataset, the model establishes a baseline average accuracy of 74.64%, with over-defense, benign, and malicious accuracies of 75.22%, 78.53%, and 70.17%, respectively. Introducing Data-centric Augmentation alone leads to a noticeable improvement in benign accuracy (from 78.53% to 81.36%) and malicious accuracy (from 70.17% to 75.95%), enhancing the model's ability to correctly classify both benign and malicious inputs. However, this augmentation comes at a cost, as evidenced by a significant reduction in over-defense accuracy from 75.22% to 64.31%. This decline indicates that while data-centric augmentation enriches the training data and improves classification capabilities, it inadvertently exacerbates the over-defense issue, making the model more prone to incorrectly classifying benign inputs as malicious\u2014a challenge commonly observed in existing models.\nThe introduction of the MOF, particularly when combined with Retraining from Scratch, addresses the adverse effects of data-centric augmentation on over-defense accuracy. Applying MOF with Retraining from Scratch to the Basic Dataset lifts the average accuracy to 81.89%, with over-defense, benign, and malicious accuracies at 89.38%, 84.73%, and 71.57%, respectively. This substantial improvement highlights MOF's effectiveness in mitigating over-defense issues caused by data-centric augmentation. As previously mentioned, employing Data-centric Augmentation can potentially reduce the performance of overdefense. However, when combined with MOF and retraining from scratch, the model achieves its highest average accuracy of 83.48%, with over-defense, benign, and malicious accuracies of 87.32%, 85.74%, and 77.39%. This combination leverages the strengths of both data augmentation and mitigation strategies, resulting in a robust model that not only benefits from enhanced classification performance but also maintains high over-defense accuracy. The ablation study demonstrates that data-centric augmentation alone can improve certain aspects of model performance, and the integration of MOF techniques is essential to counterbalance the increased over-defense, thereby ensuring that InjecGuard achieves optimal and balanced performance across all metrics."}, {"title": "6 Conclusions", "content": "In this study, we delved into the over-defense phenomenon observed in existing injection-based prompt guard models. To address this issue, we proposed a novel over-defense data sampling framework and introduced the NotInject benchmark to systematically evaluate the extent of over-defense in target prompt guard models. Furthermore, we presented InjecAgent, an advanced prompt guard model trained using a novel robust training strategy, MOF, designed to effectively mitigate model bias toward specific words. Our experimental results on various benchmark demonstrate that InjecAgent significantly outperforms existing prompt guard models across multiple dimensions, with substantial improvements in performance and robustness. To the best of our knowledge, this is the first work to provide a fully open-source prompt guard model against injection, including the training dataset, strategies, code, and model. We believe this will further promote transparency and foster an open-source academic research environment for advancing future LLM safety exploration.\nLimitations\nWhile our work shows significant improvement in mitigating over-defense in prompt guard models, the NotInject dataset, while carefully designed, may not fully capture the diversity of real-world benign inputs, particularly in domain-specific applications. This could result in the underestimation of models' over-defense tendency in complex, sensitive fields such as healthcare or finance. However, as our comprehensive evaluations have shown, the current design of NotInject is sufficient to reveal the over-defense issue in existing prompt guard models, highlighting the urgent need for improved approaches in this community. To further enhance the diversity of NotInject, our future work will incorporate domain-specific data through collaboration with industry partners.\nEthics Statement\nWe are committed to advancing the security and integrity of LLMs responsibly. In this research, we introduce NotInject, a dataset designed to assess and mitigate the over-defense issue in prompt guard models. Additionally, we introduce InjecGuard, a powerful prompt guard model developed using our novel approach, MOF, aimed at enhancing LLM security. All data used are synthetically generated or sourced from publicly available datasets, ensuring that no personal or sensitive information is involved. This approach safeguards privacy and complies with ethical standards regarding data use. While our work focuses on enhancing defensive mechanisms against prompt injection attacks, we acknowledge the potential for dual use in security research. We encourage the ethical and responsible use of NotInject to improve LLM security and not for malicious purposes. By addressing over-defense, we aim to reduce false positives and enhance accessibility for all users when prompt guard models are deployed. Our commitment to transparency is reflected in making both the dataset and model fully open-source, fostering collaboration, and allowing others to verify, replicate, and build upon our work for the betterment of the field."}, {"title": "Appendix", "content": "A Datasets\nA.1 Benign dataset\nOur benign data are collected from 14 open-source datasets, and our augmented over-defense dataset in Sec. 4.1. The open-source datasets include Alpaca (Taori et al., 2023), chatbot_instruction_prompts (Palla, 2024), open-instruct (VMware, 2023), xstest-v2-copy (R\u00f6ttger et al., 2023), grok-conversation-harmless (HuggingfaceH4, 2023), prompt-injections (Deepset, 2024a), safe-guard-prompt-injection (Erdogan et al., 2024), awesome-chatgpt-prompts (Ak\u0131n, 2023), no_robots (Rajani et al., 2023), ultrachat_200k (Ding et al., 2023), TaskTracker (Abdelnabi et al., 2024), BIPIA_train (Yi et al., 2023), jailbreak-classification (Hao, 2023), and Question Set (Shen et al., 2023). The data distribution is shown in Tab. 4.\nA.2 Malicious dataset\nOur malicious data are built based on 12 open-source datasets, and our augmented dataset in Sec. 4.1. The open-source datasets include InjecAgent (Zhan et al., 2024), prompt-injections (Deepset, 2024a), hackaprompt-dataset (Schulhoff et al., 2023), safe-guard-prompt-injection (Erdogan et al., 2024), ChatGPT-Jailbreak-Prompts (Romero, 2023), vigil-jailbreak-ada-002 (Swanda, 2023), Prompt-Injection-Mixed-Techniques (Yugen.ai, 2023), TaskTracker (Abdelnabi et al., 2024), StruQ (Chen et al., 2024), BIPIA_train (Yi et al., 2023), jailbreak-classification (Hao, 2023), and Question Set (Shen et al., 2023). The data distribution is shown in Tab. 5. Our augmented dataset consists of a large number of long-tail data types, such as XML, HTML, Markdown, etc. The specific data type distribution is shown in Tab. 6.\nB Additional Experimental Results\nB.1 Full Results\nIn Tab. 2, we have illustrated the comprehensive results on different dimensions, such as over-defense, benign, and malicious. In this section, we present detailed results for each evaluation benchmark across all dimensions, including our NotInject, Wildguard (Han et al., 2024), PINT-benchmark (LakeraAI, 2024b), and BIPIA (Yi et al., 2023). The results are shown in Tab. 7.\nB.2 Visualization of NotInject dataset\nTo further investigate the advantages of our method on confronting over-defense, we select the benign sentence from our over-defense dataset and perform a visualization to conduct qualitative analysis for model predictions. The results presented in Fig. 7 reveal that although the input is entirely safe, both PromptGuard (Meta, 2024) and ProtectAIv2 (ProtectAI.com, 2024) predict it as an injection with high confidence.\nIn contrast, our InjecGuardaccurately classifies the input as safe, highlighting the efficacy of the over-defense dataset in evaluating over-defense tendencies and demonstrating the robustness of the proposed InjecGuard.\nC Prompts\nIn this section, we illustrate the prompts used in our method.\nC.1 Word-based Generation Prompt\nIn Sec. 3.2 and Sec. 4.2, we leverage LLMs to generate benign sentences based on trigger words or tokens for both the NotInject dataset and our MOF strategy. The prompts used in this process are detailed in Fig. 9."}]}