{"title": "Sensitive Image Classification by Vision Transformers", "authors": ["Hanxian He", "Campbell Wilson", "Thanh Thi Nguyen", "Janis Dalins"], "abstract": "When it comes to classifying child sexual abuse images, managing similar inter-class correlations and diverse intra-class correlations poses a significant challenge. Vision transformer models, unlike conventional deep convolutional network models, leverage a self-attention mechanism to capture global interactions among contextual local elements. This allows them to navigate through image patches effectively, avoiding incorrect correlations and reducing ambiguity in attention maps, thus proving their efficacy in computer vision tasks. Rather than directly analyzing child sexual abuse data, we constructed two datasets: one comprising clean and pornographic images and another with three classes, which additionally include images indicative of pornography, sourced from Reddit and Google Open Images data. In our experiments, we also employ an adult content image benchmark dataset. These datasets served as a basis for assessing the performance of vision transformer models in pornographic image classification. In our study, we conducted a comparative analysis between various popular vision transformer models and traditional pre-trained ResNet models. Furthermore, we compared them with established methods for sensitive image detection such as attention and metric learning based CNN and Bumble. The findings demonstrated that vision transformer networks surpassed the benchmark pre-trained models, showcasing their superior classification and detection capabilities in this task.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid development of digital technology has signifi-cantly changed people's lives, allowing more young people,including children, to engage in online activities easily. Withthe widespread popularity of platforms like TikTok, Insta-gram, and similar video and live streaming applications, cou-pled with advancements in deepfakes and AI technologies,there has been a growing proliferation of explicit content,especially during the pandemic. The need for effective andreasonable classification and management of explicit contenthas become increasingly urgent. Unfortunately, only 37%of these technology companies have implemented measuresto detect grooming online, according to a survey by theWeProtect Global Alliance [1]. Telegram received severecriticism for not taking sufficient measures to address thelarge volume (around 100,000 instances) of pornographiccontent involving women generated using the DeepNude AItool on its platform in 2020 [2].The use of AI technology to intervene in pornographydetection is a current trend [3]. First, addressing onlinegrooming and live streaming of sexual material requires moredirect online pornography detection techniques. Secondly,the generation of pornography material through deepfaketechnology necessitates advanced AI detection methods. Thethird point is that research on explicit content, especiallychild sexual abuse material (CSAM), requires more refinedand accurate methods for rapid analysis. Furthermore, it iscrucial to identify sensitive content from vast amount ofinformation and detect and prevent its dissemination to mini-mize secondary harm. This requires specialized pornographicautomatic detection and classification technologies that canprovide quantifiable metrics and classification standards forcriminal investigation and law enforcement while reducingdirect exposure to sexual content.Currently, the classification of pornographic image mate-rial is primarily focused on two directions: one is simplydistinguishing whether it is pornographic material, and theother involves classifying and grading different levels ofpornographic material based on its content. However, in prac-tical operations, establishing a clear and unified classificationand grading standard, whether for image and video contentrating, content moderation, or criminal-related pornographyclassification, proves to be challenging.Due to the sensitivity of pornographic image material andthe influence of cultural, religious, and political factors indifferent countries, the management of pornographic materialvaries significantly. Even for research purposes, the creationand sharing of relevant datasets face substantial limitations,resulting in a scarcity of benchmark data in this domain.Moreover, the classification research related to pornographicimages is more prevalent in various countries' police sys-tems, employing traditional machine learning or CNN-basedmethods. Only recently has there been growing attention tomethods based on vision transformers, which were originallydeveloped for natural language processing tasks, and nowalso for image recognition tasks.In this study, we conducted a performance comparisonbetween different vision transformer models and traditionalmethods in pornographic image detection tasks. The structureof this paper is organized as follows. First, a literature reviewis presented, followed by the methods applied in the paper.Then, comparison experiments on pornography and porn-indicative image detection are provided. Lastly, the resultsare analyzed and the limitations of this study and potentialresearch directions are mentioned in the conclusion section."}, {"title": "II. RELATED WORK", "content": "Table I lists the majority of pornographic datasets widelyused in research. Among them, adult content image (ACI)[4], P2datasetFull [5], and Pornography-2M [6] are datasetsin which images are labeled only based on whether theyare explicit or not. Meanwhile, Nudenet goes a step furtherby providing detailed annotations based on the exposure ofdifferent body parts on top of the two-class labeling. Simi-larly, data sets such as LSPD, sexACT, and AIIA-PID4 havefurther detailed annotations that cover additional information.The LSPD dataset provides 93,810 labeled instances among50,212 images. Among these, pornographic images are pri-marily sourced from adult websites, while non-pornographicimages are obtained by searching 250 categories of keywordson Google. For each keyword, approximately 1,000 imageswere downloaded. The pornographic images are further an-notated using polygon masks for four private sexual organs:breasts, male and female genitals, and anus. The sexACTdataset was created by [7] from images downloaded fromGoogle by searching through keywords. Although 5,000images of children sexual abuse (CSA) provided by SpanishPolice and 1,112,531 images of adult from the Pornography-2M dataset are mentioned by [6] from the Spanish Groupfor Vision and Intelligent Systems (GVIS), the dataset is notavailable for general usage.Due to cultural, religious, and other factors, definitionsof explicit and non-explicit content vary. Additionally, def-initions of pornography can evolve rapidly over time. Associetal norms, cultural perspectives, and legal frameworkschange, the understanding and categorization of explicitcontent may undergo significant transformations. Therefore,it is necessary to introduce models capable of detectingpornography content and details while incorporating a sep-arate category for porn-indicative content that falls betweensafety and explicitness. In this paper, we define three classes(categories) involved in the detection of pornography images,which is shown in Table II. Building upon this foundation,we can further classify pornography materials into morespecific categories based on the varying laws and culturalcustoms of different countries. These subcategories can servedifferent purposes, such as law enforcement efforts, andallow for a more nuanced classification and handling ofsensitive materials."}, {"title": "B. Pornography Detection Tools", "content": "Pornography detection has been widely used in contentmoderation, sensitive content detection, scene genre clas-sification, and so on. AI algorithms allow for more so-phisticated analysis and detection of pornography materi-als. The automatic pornography detection tool NuDetective[11] developed by the Brazilian Federal Police is one ofthe most prominent binary machine learning-based foren-sic pornography detection tools. Of all these pornographydetection models based on ResNet, the Yahoo Open NotSafe For Work (NSFW) model [12] developed in 2016 isthe most representative. The Yahoo Open NSFW model is abinary ResNet-50 model finetuned from the ImageNet datasetfor NSFW classification, but the probability scoring andthreshold setting design hinder the wider application of thismodel. The Majura labeling schema proposed by [13] alsoadopted pre-trained VGG and ResNet18 models for childexploitation material (CEM) labeling. Similar open-sourcedsensitive image and video scene detection tools include theLAION safety toolkit\u00b9, the PysceneDetect\u00b2, the ffsubsync\u00b3,and the MoviePy4. Among them, the LAION safety toolkitis an NSFW detector trained on the LAION image dataset.The other three, however, are detection software based onvideos or video clips."}, {"title": "C. Pornography Detection Models", "content": "Different from pornography detection and rule-based tra-ditional machine learning methods, utilizing deep learning toautomatically extract key features for sensitive (pornography)content recognition allows for more comprehensive learningof factors that influence sensitive material classification,including pornography detection through feature maps. ForAI model-based pornography detection models, ResNet andattention model methods are the two most popular types ofdeep learning networks deployed in NSFW or CSAM mate-rial detection. The work by [7] utilized the ResNet modelsfrom Fast.ai and PyTorch libraries for NSFW classification.In that paper, the ResNet models including the ResNet152and ResNet101 demonstrated better performance than the"}, {"title": "III. BACKGROUND", "content": "Most classical neural sequence transduction models in-cluding the transformer models follow the same encoder-decoder structure [29]. The transformer model primarilyconsists of the encoder and the decoder two parts. Inthe encoder, multiple stacked layers are utilized, and eachlayer comprises a self-attention sub-layer and a feed-forwardneural network sub-layer. On the other hand, the decoderhas a similar structure with stacked layers, but each layerincludes three sub-layers. These sub-layers consist of a self-attention sub-layer, a feed-forward neural network sub-layer,and an additional encoder output attention sub-layer forreinforcement. Furthermore, both the encoder and decoderhave a process of residual connections and normalizationafter each sub-layer."}, {"title": "A. Vision Transformer", "content": "The vision transformer, commonly known as ViT, is amodel designed for image classification tasks. It utilizesa transformer-based architecture that operates on imagepatches. The input image is divided into patches of a fixedsize. Each patch is linearly embedded, incorporating itsvisual features, and position embeddings are introduced tocapture spatial information. The resulting sequence of em-bedded vectors is then passed through a standard transformerencoder. To enable classification, a conventional technique isemployed, which involves appending an additional learnable\"classification token\" to the sequence. This token serves as arepresentative element that summarizes the information fromthe image patches and aids in making the final classificationdecision. Using the attention mechanism of the transformerand the positional embeddings, the ViT achieves effectiveimage classification capabilities. The structure of the ViTtransformer module is provided in Fig. 1."}, {"title": "B. Swin Transformer Model", "content": "The Swin transformer is a hierarchical vision transformerusing \"shifted windows\", introduced in [31]. Unlike tradi-tional transformers that fixedly process input sequences, theSwin transformer operates on non-overlapping patches of animage. It adopts a hierarchical design by stacking multiplestages, where each stage contains a set of transformer blocks.In each stage, a shifted window mechanism is employed tocapture both local and global contextual information effi-ciently. This mechanism allows for the capture of long-rangedependencies while maintaining computational efficiency.The structure of a Swin transformer is provided in Fig. 2.The linear embedding and swin transformer block process arerepeated in the second, third, and fourth stages. In each stage,the features from the patch partitions are merged before beinginput to another Swin transformer block, producing a newoutput with a different dimension."}, {"title": "C. HiLo Attention Model", "content": "LITv2, also named improved fast vision transformers withHiLo Attention, is a model proposed by [32]. The HiLoattention mechanism draws inspiration from the observationthat different frequencies can capture image features fromdiverse granularity, i.e., high frequencies for local fine detailsand low frequencies for global features. This facilitatescapturing global contextual information and structural depen-dencies across the entire image. By separating the attentionheads based on frequency characteristics, the HiLo attentionmechanism enables the model to capture both local finedetails and global structures effectively. This disentanglementapproach improves the ability of the attention mechanism torepresent different frequency patterns, leading to enhancedperformance in speed and memory consumption. The struc-ture of the LITv2 model is provided in Fig. 3."}, {"title": "IV. EXPERIMENTS", "content": "In most CSAM classification studies, images or videosare often simply categorized as either pornography or non-pornography. However, in real-world scenarios, the definitionof pornographic images can vary significantly due to factorssuch as country, culture, religion, and other aspects. Toprovide a more accurate definition of pornographic andnonpornographic images, in our article, we defined certainimages that are suspected to contain pornographic content assuspicious pornography, i.e., porn-indicative. The definitionof each category is provided in Table II. Our experimentsin this study employed three datasets, namely 2-class P2,3-class P2, and ACI, as described in detail below.The process of creating the P2 dataset consisted of threestages. Basically, the idea is to supplement data based onthe original P2datasetFull dataset introduced in [5]. TheP2datasetFull is a public dataset with pornography andclean images collected online in three subfolders of training,testing, and validation. However, this dataset consists of alarge number of invalid images. For the purpose of creating abenchmark dataset for pornography image classification, wegathered samples from various public resources, includingReddit, ImageNet, and the P2datasetFull dataset. The firststage involved extracting photos indexed on Reddit andImageNet. Pornographic and indicative images are mostlycollected from the Reddit NSFW dataset. Meanwhile, neu-tral/clean images of human beings are selected from theImageNet dataset. The second stage involved the automaticdeletion of photos that were invalid or could not be read.The third stage involved the manual selection of imagesto exclude blurred and duplicated images, as well as or-ganize samples into subfolders based on their labels andcontent, namely pornography, porn indicative, and clean.Following this process, we were able to create a benchmarkpornography dataset, allowing us to evaluate and comparethe performance of different classifiers in image classificationtasks. The images in the P2 dataset are finally standardizedat a resolution of 256 \u00d7 256 pixels. We name the three-classdataset created as 3-class P2 and the reorganized two-classdataset without the indicative category as 2-class P2.To compare with the state-of-the-art research work, another 2-class sensitive dataset named ACI [4] is also includedin our experiments. The ACI dataset is inspected as cleanand organized and will be utilized directly in this paper. Theimages in the ACI dataset are all standardized to a resolutionof 128 x 128 pixels.We acknowledge the ethical concerns around the use ofadult sexual data obtained through the reuse of scrapeddatasets and do not in any way condone this methodologymore broadly. However, we have made use of this datasetin the interests of facilitating comparison of our results withother work in this area."}, {"title": "B. Experiment Settings", "content": "In previous related work, [13] utilized the ResNet18architecture, which was pre-trained with the Google Imagedataset, to address the task of CSAM classification. Specifically, leveraging the features learned from a large and diverse dataset, the pre-trained ResNet18 model demonstratedits effectiveness in accurately identifying and classifyingCSAM material. In this paper, we compared the pre-trainedResNet18 methods with several popular image transformer-based models, i.e., ViT, DeiT, Swin and LITv2 models withvarious alpha settings."}, {"title": "C. Results and Analysis", "content": "For most vision transformer models, three sizes of mod-els are provided on their official website according to theflops, the memory required, and the number of parameters,i.e., base, medium, and small models. In general, the term\"base model\" typically implies a larger model with moreparameters and computational complexity compared to a\"medium model,\" and a \"medium model\" is larger than a\"small model.\" A base model tends to have greater learn-ing capacity during training due to its increased parame-ters and layers. However, a small model is more efficientand memory-saving. The medium model makes a trade-off between learning capacity, computational efficiency, andmemory requirements. Here, we take LITv2 as an exampleto compare the differences between various models sizes interms of the dimension of the embeddings, the number ofheads, the depths of attention, as well as the number ofparameters in Table V. Taking the LITv2-base as a reference,the dimension of embeddings is 128, the number of attentionheads is [4, 8, 16, 32], and the depth of attention modules is[2, 2, 18, 2]. The input size of all these three models is 224\u00d7 224 pixels. In Table VI, we also provide the comparisonof the most popular vision transformers, including ViT, DeiT,and Swinv2. Here, ViT-base is equivalent to the \"small\"models in other vision transformers, as its naming conventionfollows \"base\", \"large\", and \"huge\u201d, which corresponds to\"small\", \"medium\" and \"base\" in other models.Limited to the computation resources, we select thesmall models to compare their performance on our datasets.As mentioned in Section III-C, the composition of high-frequency and low-frequency attention is a vital factor inthe representation of images. To compare the model per-formances with different ratios of high-frequency and low-frequency attention, we also test the LITv2 model with theratio values of 0.1, 0.4, and 0.9, respectively, for smallLITv2 model configurations in our case. The maximumaccuracy reported in the validation data sets for the modelstrained from scratch (with 300 training epochs) is reportedin Table VII. The accuracy reported on the test data set forthese trained models is reported in Table VIII.From Table VII, it can be observed that, even duringthe training phase, the classification results for the P2 3-class datasets, including the porn indicator category, aresignificantly lower compared to the classification with onlytwo classes: pornography and clean data. This is in partdue to the limited amount of data in the porn-indicativecategory, resulting in a clear bias. Additionally, the in-troduction of porn-indicative data diminishes the interclassdifferences between porn and clean categories, affecting theclassification results. Simultaneously comparing the train-ing accuracy between the P2 dataset and ACI, all visiontransformer models demonstrate higher training accuracy onthe ACI dataset, which has more training data. Additionally,overall, the LITv2 model outperforms the Swin-S and Deit-Smodels. Meanwhile, the Swin-S and DeiT-S models exhibitcomparable performance across all three datasets.Examining Tables VII and VIII together reveals that theclassification results in independent test data for modelstrained on the two-class dataset noticeably decrease com-pared to their performance on the validation data. How-ever, on datasets that include the porn-indicative category,the models consistently demonstrate stable performance.Specifically, on the 2-class P2 dataset, LITv2 models withdifferent ratios exhibit a significant reduction in classificationaccuracy, approximately 17% to 20%, while the Swin andDeiT models also show a decrease in accuracy, about 2%to 4%. For the ACI dataset, the small difference betweenthe accuracy of the validation and the accuracy of the testcan be attributed to several factors. Firstly, the ACI datasethas a larger training data size compared to the P2 dataset.Secondly, the distribution of data among the train, validation,and test sets in the ACI dataset is more consistent. This"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this study, we conducted a comparison between thewidely used ViT models, namely LITv2, ViT, DeiT, andSwin, and the pre-trained ResNet18 models for pornographyimage classification. We customized and fine-tuned thesemodels with various configurations tailored to our specificdatasets. We have compared models fine-tuned using check-points pre-trained on the ImageNet-1K data set with modelstrained from scratch across three datasets. We have also ex-plored the performance differences among vision transformermodels based on different attention principles. Additionally,we have examined variations in the performance of visiontransformer models with different complexities and the im-pact of varying ratios of local attention to global attention. As\na focal point of this paper, we delved into the variations in theclassification performance of different models, particularlywith the introduction of the \"porn-indicative\" category inpornography images.In general, models trained on more diverse training data(pre-trained models) tend to outperform models trained fromscratch. The comparison results among vision transformersbased on different attention principles indicate that the LITv2model, compared to traditional ViT models, distillation-based DeiT models, and sliding window-based Swin mod-els, demonstrates a better balance between high-frequencyand low-frequency attention, leading to more effective im-age representation. Furthermore, in scenarios with sufficientcomputational resources, larger models with more parame-ters typically outperform smaller models. Compared to theclassification of two-class pornography data, the fine-tunedmodels for the three-class pornography data showed minimaloverfitting during training. Simultaneously, these modelsmaintained high classification accuracy for both traditionalpornography and clean categories.A shortcoming of the paper is the limited number ofimages and unbalanced composition of datasets. To addressthis, we plan to incorporate transfer learning modules intoour model to mitigate the dataset's limitations in the future.Another issue is the lack of ethically compliant sensitiveimage datasets. It is challenging to determine whether thereis an overlap between datasets like ImageNet-1K and ACI,which could affect classification accuracy and results. Wewill create datasets that adhere to ethical standards forour research. To further analyze the impact of the visiontransformer model on classification, we will incorporatemore visualization methods to analyze classification results,addressing the challenge of presenting sensitive data moreappropriately. Additionally, we plan to conduct a moreextensive and fine-grained analysis of pornography images,exploring key details that determine explicit content."}]}