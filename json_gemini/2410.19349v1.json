{"title": "PEBR: A PROBABILISTIC APPROACH TO EMBEDDING BASED RETRIEVAL", "authors": ["Han Zhang", "Yunjiang Jiang", "Mingming Li", "Haowei Yuan", "Wen-Yun Yang"], "abstract": "Embedding retrieval aims to learn a shared semantic representation space for both queries and items, thus enabling efficient and effective item retrieval using approximate nearest neighbor (ANN) algorithms. In current industrial practice, retrieval systems typically retrieve a fixed number of items for different queries, which actually leads to insufficient retrieval (low recall) for head queries and irrelevant retrieval (low precision) for tail queries. Mostly due to the trend of frequentist approach to loss function designs, till now there is no satisfactory solution to holistically address this challenge in the industry. In this paper, we move away from the frequentist approach, and take a novel probabilistic approach to embedding based retrieval (namely pEBR) by learning the item distribution for different queries, which enables a dynamic cosine similarity threshold calculated by the probabilistic cumulative distribution function (CDF) value. The experimental results show that our approach improves both the retrieval precision and recall significantly. Ablation studies also illustrate how the probabilistic approach is able to capture the differences between head and tail queries.", "sections": [{"title": "Introduction", "content": "Search index is the core technology to enable fast information retrieval based on certain keywords in various modern computational systems, such as web search, e-commerce search, recommendation, and advertising in the past few decades. As a traditional type of search index, inverted index [1], which maps terms to documents in order to retrieve documents by term matching, has been the mainstream type of search index for decades, thanks to its efficiency and straightforward interpretability. Recently, with the advent of the deep learning and pre-training [2, 3, 4] era, embedding based retrieval, coupled with approximate nearest neighbor (ANN) search algorithms [5], have been established as a promising alternative in search index [6, 7] and recommendation index [8, 9, 10, 11], in part due to its semantic matching advantage and efficient ANN algorithms.\n\nThe origin of using dense vectors for retrieval probably can be dated back to Latent Semantic Analysis (LSA) [12] in the 90s, which is an unsupervised method utilizing singular-value decomposition to get the document representation. Then in the following decades, especially the past decade with the advent of deep learning, the embedding based retrieval model has been increasingly used in information retrieval. Typically, DSSM [13], including its variants C-DSSM [14], DPSR [6], etc. are typical supervised two-tower models, where query and item are embedded into dense vectors in the same semantic space. Then relevant items are retrieved by calculating the cosine similarity between query vectors and item vectors. On top of the above foundational models, recently a few more trends are going on in the industry: 1) joint training of two-tower model and embedding index [15, 16] to avoid the quantization distortion in ANN algorithms. 2)"}, {"title": "Related Works", "content": null}, {"title": "Embedding Retrieval", "content": "For traditional candidate retrieval, some new methods have been proposed, including latent semantic indexing (LSI) [42] based on matrix factorization, probabilistic latent semantic indexing (PLSI) [43] and semantic hashing [44] based on auto-encoding model. But all of these models are unsupervised and learned from word co-occurrence in documents, without any supervised labels. Our embedding retrieval [6] differs from the previous methods in that we train a supervised model to directly optimize relevance metrics based on a large-scale data set with relevant signals, i.e., clicks.\n\nTwo-tower architecture for deep neural networks has been widely adopted in existing recommendation works [45, 46, 47] to further incorporate item features. This model architecture is also known as a dual encoder in natural language processing [48, 49]. However, our existing embedding retrieval [6, 50, 51, 52] uses a more advanced two-tower model which is composed of a multi-head tower for query and an attention loss based on a soft dot product instead of a simple inner product.\n\nAt present, embedding retrieval technologies have been widely adopted in modern recommendation and advertising systems [8, 53, 9], but have not been widely used in search engines. We have begun to explore the application of embedded retrieval in industrial search engine systems.\n\nVarious types of indexes play an indispensable role in modern computational systems to enable fast information retrieval. In terms of traditional offline indexing methods, inverted index [1] has been widely used in web search, e-commerce search, recommendation, and advertising in the past few decades. Recently, with the advent of the deep learning era, methods such as LSH [54], Annoy [55], ScaNN [56], and Faiss [5] accelerate the retrieval of embedded vectors by building indexes offline. In terms of Deep learning methods, methods as Deep Quantization Network [57] and Deep Neural Networks for YouTube Recommendations [8], utilize deep learning techniques to realize efficient image retrieval and recommendation systems.\n\nHowever, we have proposed a novel method called Poeem [58] to learn embedding indexes jointly with any deep retrieval models. We introduce an end-to-end trainable indexing layer composed of space rotation, coarse quantization, and product quantization operations. Experimental results show that the proposed method significantly improves retrieval metrics over traditional offline indexing methods, and reduces the index-building time from hours to seconds."}, {"title": "Probabilistic Neural Network", "content": "Probabilistic modeling has been introduced in machine learning for a long history [31] to enhance model performance in various areas. For example, in NLP areas, researchers such as [59] have aimed to model the uncertainty of word representation and relationships by mapping words to probabilistic densities instead of point vectors in low-dimensional space. Other studies, like [39] and [38], have introduced different probabilistic distributions to model the ambiguity of word representation. In visual areas, probabilistic modeling has been successfully applied to enhance the performance of many tasks like face understanding, image segmentation, etc. For example, [60] and [61] have utilized probabilistic techniques to improve the accuracy and robustness of face recognition. In the context of image segmentation, [40] and [41] demonstrate the success of probabilistic models to capture uncertainty and ambiguity in the segmentation process, resulting in improved segmentation quality and better handling of complex visual scenes.\n\nTo represent the uncertainty of the input data, we can use probabilistic embedding. Probabilistic representations of data have a long history in machine learning. They were introduced for word embedding in 2014 [59]. In recent years, probabilistic embeddings have been introduced to visual tasks. Hedged Instance Embedding (HIB) [62] can be used to handle one-to-many correspondences for metric learning, while other results apply probabilistic embeddings to face understanding [63, 61], 2D-to-3D pose estimation [64], speaker diarization [65], and prototype embeddings [63]. Recently, there have been studies on zero-shot recognition using variational autoencoders [66], which use the 2-Wasserstein distance as a distributional alignment loss and learn classifiers on top of it. An approach called Probabilistic Cross-Modal Embedding (PCME) [67], which uses a probabilistic framework to model a wide range of one-to-many associations between images and captions works with probabilistic embeddings in multimodal tasks. Bayesian neural networks are also an approach that uses probabilistic modeling to deal with uncertainty in neural networks by introducing"}, {"title": "Method", "content": "In this section, we first formulate the embedding retrieval problem based on a two-tower architecture and review the existing frequentist approaches. Next, we explore the probabilistic approach which contains the choice of item distribution function and our carefully designed loss function based on the item distribution."}, {"title": "Problem Definition and Notations", "content": "The embedding retrieval model is typically composed of a query (or a user) tower and an item tower. For a given query q and an item d, the scoring output of the model is\n\n$f(q, d) = s(v_q, v_d)$,\n\nwhere $v_q \\in \\mathbb{R}^n$ denotes the n-dimensional embedding output of the query tower. Similarly, $v_d \\in \\mathbb{R}^n$ denotes the embedding output of item tower, of the same dimension to facilitate efficient retrieval. The scoring function $s(...)$ computes the final score between the query and item. Researchers and practitioners often choose s to be the cosine similarity function, equivalently inner product between normalized vectors, i.e., $s(v_q, v_d) = v_q^T v_d$, where the superscript $^T$ denotes matrix transpose. This simple setup has proven to be successful in many applications, e.g. [8]. The objective is to select the top k items $d_1, . . . , d_k$ from a pool of candidate items for each given query $v_q$, in terms of $s(v_q, v_{di})$.\n\nThe key design principle for such two-tower architecture is to make the query embedding and the item embeddings independent of each other, thus we can compute them separately after the model is trained. All item embeddings can be computed offline in order to build an item embedding index for fast nearest neighbor search online [5], and the query embedding can be computed online to handle all possible user queries. Even though the embeddings are computed separately, due to the simple dot product interaction between query and item towers, the query and item embeddings are still theoretically in the same geometric space. Then a fixed threshold of item number is used to get the top k relevant items. Thus, typically, the goal is equivalent to minimizing the loss for k query item pairs where the query is given."}, {"title": "Existing Frequentist Approaches", "content": "Most existing methods follow the frequentist approach to the loss function designs for embedding retrieval, which we can categorize into two typical cases: point-wise loss, and pair-wise loss. In the following section, we discuss each of them."}, {"title": "Point-wise Loss", "content": "The point-wise based method converts the original retrieval task into a binary classification, where each pair of query and item is computed individually. The goal is to optimize the embedding space where the similarity between the query and its relevant item is maximized, while the similarity between the query and the irrelevant item is minimized. It usually adopts the sigmoid cross entropy loss to train the model. The loss function can be defined below\n\n$L_{pointwise}(\\mathcal{D}) = \\sum_{i} log \\text{sigmoid}(s(v_{q_i}, v_{d^+_i})) + \\sum_{i,j} log \\text{sigmoid}(-s(v_{q_i}, v_{d^-_{ij}})),$\n\nwhere $d^+_i$ denotes the relevant items for query $q_i$ and $d^-_{ij}$ denotes the irrelevant ones, $sigmoid(x) = 1/(1 + exp(-x))$ is the standard sigmoid function. Though rarely seen in literature, this loss function works well in practice."}, {"title": "Pair-wise Loss", "content": "This kind of method aims to learn the partial order relationship between positive items and negative items from the perspective of metric learning, closing the distance between the query and positive item and pushing away from the negative. The classical work contains triple loss, margin loss, A-Softmax loss, and several variants (margin angle cosine). Without loss of generality, we formulate the loss as softmax cross-entropy\n\n$L_{pairwise}(\\mathcal{D}) = \\mathbb{E}_i [\\log \\text{softmax}(s(v_{q_i}, v_{d^+}), \\{s(v_{q_i}, v_{d^-_j})\\}_j)]$\n\n$= \\mathbb{E}_i [\\log \\frac{\\exp(s(v_{q_i}, v_{d^+} )/ \\tau)}{\\sum_j \\exp(s(v_{q_i}, v_{d^-_j}) / \\tau)}]$,\n\nwhere $\\tau$ is the so-called temperature parameter: lower temperature is less forgiving of mis-prediction of positive items by the model. In the same direction, researchers later proposed more advanced loss functions, by introducing max margin in angle space [68], in cosine space [69], and so on."}, {"title": "Limitations", "content": "Both point-wise and pair-wise loss functions have their advantages and limitations. Point-wise loss functions are relatively simpler to optimize, while they may not capture the partial order relationships effectively. In contrast, Pair-wise loss functions explicitly consider the relative ranking between items. As a result, pair-wise loss functions usually achieve better performance in embedding retrieval tasks. While both of them are frequentist approaches, in the sense that no underlying probabilistic distribution are learned and consequently there is no cutoff threshold in principle when retrieving items. Thus, we propose the following probabilistic approach to a more theoretically well-founded retrieval."}, {"title": "Retrieval Embeddings as a Maximum Likelihood Estimator", "content": "Given a query q, we propose to model the probability of retrieving item d, i.e. $p(d|q)$, which is related to the relevance between query q and item d, thus\n\n$p(d|q) \\propto p(r_{d,q}|q)$,\n\nwhere $r_{d,q}$ represents the relevance between the query q and document d. For all relevant items $d^+$, we assume $r_{d^+,q}$, where $d^+ \\in \\mathcal{d}^+$, follows a distribution whose probability density function is $f_{\\theta}$. For all irrelevant items $d^-$, we assume $r_{d^-,q}$, where $d^- \\in \\mathcal{d}^-$, follows a distribution of whose probability density function is $h_{\\theta}$. The likelihood function can be defined as\n\n$\\mathcal{L}(\\theta) = \\prod_{q} (\\prod_{d^+} p(d^+|q) \\prod_{d^-} p(d^-|q)) \\propto$\n\n$= \\prod_q(\\prod_{d^+} p(r_{d^+, q}|q) \\prod_{d^-} p(r_{d^-, q}|q)) = \\prod_q(\\prod_{d^+} f_{\\theta, q}(r_{d^+, q}) \\prod_{d^-} h_{\\theta, q}(r_{d^-, q}))$\n\nImportantly, the density for relevant ($f_{\\theta, q}$) and irrelevant ($h_{\\theta, q}$) items are both query dependent. This is a useful generalization from fixed density since different queries have different semantic scopes. Finally, the objective function can be defined as the log-likelihood\n\n$l(\\theta) = \\log \\mathcal{L}(\\theta) = \\sum_q (\\sum_{d^+} \\log f_{\\theta, q}(r_{d^+, q}) + \\sum_{d^-} \\log h_{\\theta, q}(r_{d^-, q}))$ \n\nWhen we choose different distributions for relevant and irrelevant items, the loss function can resemble a point-wise loss, which may lead to suboptimal performance compared to pair-wise loss functions. To address the limitation, we propose new probabilistic loss functions based on the principles of contrastive loss, which is a well-known pair-wise loss function."}, {"title": "Retrieval Embeddings As a Noise Contrastive Estimator", "content": "Apart from the above maximum likelihood estimator, the most widely used technique for retrieval model optimization is based on the InfoNCE loss [70], which is a form of noise contrastive estimator (an alternative to the maximal likelihood estimator) of the model parameter. In that setting, we need to choose two distributions, the so-called positive sample distribution $p(d^+|q)$, and background (noise) proposal distribution $p(d)$. In theory, the two are related, if we know the joint distribution of d and q. But in practice, we can either treat them as separate or simply hypothesize their ratio as the scoring function $r(d, q) := p(d|q)/p(d)$, without knowing them individually. The loss we are minimizing is thus the following negative log-likelihood of correctly identifying the positive item within the noise pool\n\n$L = - \\log \\frac{r(d^+, q_i)}{\\sum_j r(d^-_j, q_i)}$\n                                                                                                      (1)\nBy minimizing the loss function, the model aims to maximize $p(d^+|q)$ for the query q and one of its relevant items $d^+$, while pushing away q and its irrelevant items $d^-$, thus assign higher similarities to relevant items compared to irrelevant items. Based on the definition, in this section, we propose two types of distributions as the basis of the estimator, truncated exponential distribution in Section 3.4.1 and Beta distribution in Section 3.4.2."}, {"title": "Parametric Exponential InfoNCE (ExpNCE)", "content": "Here we propose to use the following truncated exponential distribution density function as the scoring function $r(d, q) \\propto \\exp(\\cos(v_d, v_q)/\\tau_q)$ where the function cos stands for the cosine similarity between the two vectors and the temperature $\\tau_q$ is query dependent. This offers an interesting alternative to the standard InfoNCE loss with log bi-linear distribution as\n\n$L_{ExpNCE} = \\sum_i \\log(1 + \\frac{\\exp(\\cos(v_{q_i}, v_{d^-_j}) / \\tau_q)}{\\exp(\\cos(v_{q_i}, v_{d^+}) / \\tau_q)})$,\n\nA nice property of the above probabilistic modeling is that we can use a simple cumulative density function (CDF) to decide the cutoff threshold. The CDF for the above truncated exponential distribution can be derived as follows\n\n$P_{ExpNCE}(x < t) = \\frac{\\int_{-1}^t e^{x / \\tau_q} dx}{\\int_{-1}^1 e^{x / \\tau_q} dx} = \\frac{e^{t/ \\tau_q} - e^{-1/ \\tau_q}}{e^{1/ \\tau_q} - e^{-1/ \\tau_q}},$\n\nwhich can be easily computed."}, {"title": "Parametric Beta InfoNCE (BetaNCE)", "content": "We call a probability distribution compactly supported if its cumulative distribution function F satisfies $F((-\\infty, -x]) = 1 - F([x, \\infty)) = 0$ for some $x > 0$. In other words, all the mass is contained in the finite interval $[-x,x]$. The best-known family of compact distributions in statistics is probably the Beta distributions, whose pdf are defined on [0, 1]. Since the cosine similarity used in the two-tower model has a value range of [-1,1], we expand the definition range of Beta distributions to [-1,1] and define its density as\n\n$f_{\\alpha, \\beta}(x) = \\frac{\\Gamma(\\alpha + \\beta)}{2 \\Gamma(\\alpha) \\Gamma(\\beta)} (\\frac{1+x}{2})^{\\alpha-1} (\\frac{1-x}{2})^{\\beta-1}$\n\nAn interesting special case is when $\\alpha = \\beta = 1$, which gives the following uniform CDF on [-1,1]\n\n$F_{1,1}(x) = \\frac{x+1}{2}$\n\nOne difficulty working with Beta distributions is that its CDF has no closed form, but rather is given by the incomplete Beta function 2: $B(t; \\alpha, \\beta) = \\int_0^t x^{\\alpha-1}(1 - x)^{B-1}dx$ is the incomplete Beta integral. The CDF for the above Beta"}, {"title": "Experiments", "content": "In this section, we first explain the details of the dataset, experimental setup, baseline methods and evaluation metrics. Then we show the comparison of experimental results between the baseline methods and pEBR. Finally we show the ablation study to intuitively illustrate how pEBR could perform better."}, {"title": "Experimental Setup", "content": null}, {"title": "Dataset and Setup", "content": "Our model was trained on a randomly sampled dataset consisting of 87 million user click logs collected over a period of 60 days. We trained the model on an Nvidia GPU A100 and employed the AdaGrad optimizer with a learning rate of 0.05, batch size of 1024, and an embedding dimension of 128. The training process converged after approximately 200,000 steps, which took around 12 hours."}, {"title": "Methods in Comparative Experiments", "content": "Our method is compatible with most two-tower retrieval models since it primarily modifies the loss function rather than the model architecture. Thus, we choose a classic two-tower retrieval model DSSM [13], which has been proven successful in various retrieval scenarios, as the backbone model. Then we focus on cutting off items retrieved by DSSM model with three methods:\n\n\u2022 DSSM-topk refers to the method which cuts off retrieved items using a fixed threshold of numbers. In other words, we take the top k items with the highest relevance scores. Note that in this case, all queries have the same number k of retrieved items.\n\n\u2022 DSSM-score refers to the method that cuts off retrieved items with a fixed threshold of relevance score. Items with relevance scores below the threshold are discarded. Thus, the queries have varying numbers of retrieved items. In practice, we also have a maximum number to prevent some head queries to retrieve too many items that the downstream ranking system can not handle.\n\n\u2022 PEBR refers to our proposed probabilistic method which cuts off retrieved items with a threshold derived by the probabilistic model, specifically the CDF value of the learned item distribution. We are using a default CDF value 0.5 if not further specified. In this experiment, we focus on using BetaNCE with $\\beta = 1$ as shown in Equation (2) to demonstrate the effectiveness of the methods. However, our method is general enough to accommodate other distributions, such as the proposed ExpNCE.\n\nBoth DSSM-topk and DSSM-score serve as baseline methods in our work, against which we compare the performance of our proposed pEBR method."}, {"title": "Experimental Metrics", "content": "We use two widely reported metrics, Recall@k and Precision@k to evaluate the quality and effectiveness of retrieval models [15, 9]. Recall@k (R@k) is a standard retrieval quality metric that measures the ratio of retrieved relevant items to the total number of relevant items. Precision@k (P@k) is another commonly used retrieval metric that calculates the ratio of true relevant items to the total number of retrieved items within the top k results.\n\nThere are some nuances that need a little more explanation. In DSSM-topk, a fixed number k of items is retrieved for each query to calculate R@k and P@k. While in DSSM-score and pEBR, a threshold of relevance score or a CDF value is used to cut off retrieved items, which results in a different number of items for each query. Thus for fair comparison, we tune the threshold slightly to make an average number k of items to be retrieved for all queries to calculate the metrics.\n\nMoreover, Note that there is a tradeoff between precision and recalls when varying the k value. In the experiments, we choose $k = 1500$ to optimize for recall, since it is the main goal of a retrieval system. Thus, the precision value is relatively very low."}, {"title": "Experimental Results", "content": "In this section, we compare the performance of our proposed method with the baseline methods. Since the retrieval performance differs significantly from head, torso to tail queries, we separate our evaluation dataset into three according parts to assess how the proposed method performs.\n\nAs shown in Table 1 we can draw the following conclusions: 1) DSSM-score performs better than DSSM-topk on both R@1500 and P@1500, which is as expected since the retrieved items of DSSM-score have better relevance scores than DSSM-topk, because items with lower relevance scores are cut off. 2) Our proposed model pEBR outperforms both DSSM-topk and DSSM-score with improvements of 0.79% and 0.44% on R@1500 respectively, which proves that pEBR captures the difference between queries successfully. pEBR learns varying item distributions for different queries, allowing for the determination of dynamic and optimal relevance score thresholds, thus leading to enhanced overall retrieval performance. 3) pEBR achieves better performance on separated evaluation sets, i.e. head queries, torso queries and tail queries. Moreover, we observe that the amount of improvement achieved by pEBR varies systematically with respect to query popularities. Specifically, compared to DSSM-topk, pEBR achieves 1.04%, 0.64% and 0.37% recall improvements on head queries, torso queries and tail queries respectively. This is because that head queries normally have much more relevant items than tail queries, thus a dynamic cutoff threshold could benefit head queries to retrieve more items significantly. 4) Similarly, pEBR achieves 0.163%, 0.302% and 0.324% precision improvements on head, torso, and tail queries on top of DSSM-topk. This is because tail queries normally do not have many relevant items, typically just tens of items. Thus retrieving a fixed number k of items hurts the precision significantly. pEBR appears to solve this problem nicely."}, {"title": "Ablation Study", "content": null}, {"title": "Visual Illustration of Distributions", "content": "As shown in Figure 1, we visualize the relevant item distribution for three queries with different frequencies. Since it is difficult to visualize 128 embedding dimension, we first need to apply a dimension reduction technique, specifically t-SNE [71] here to reduce the dimension to 3-D. Then we normalize the vectors as unit vectors and plot them on a sphere. Note that we are using cosine similarity thus the norms of the vectors do not matter. The head query, \"phone\", is a quite general one that can retrieve phones of various brands and models. As a result, the retrieved items are widely distributed and dispersed across the surface of the sphere. The torso query \"Huawei phone 5g\" is a more specific one as it focuses on phones from the brand Huawei with 5G capability. Consequently, the item distribution is more concentrated and has a narrower scope compared to the query \u201cphone\u201d. The tail query, \u201cHuawei mate50pro\u201d, is highly specific query as it specifies the brand (Huawei) and model (mate50pro), thus the number of relevant items is very small and the distribution becomes even more concentrated. These differences in item distributions reaffirm the conclusion that cutting off retrieved items by a fixed threshold of item numbers or a fixed relevance score is suboptimal for embedding retrieval."}, {"title": "Dynamic Retrieval Effect", "content": "In Figure 2, we show the histogram of retrieved item numbers cut off by CDF value 0.985 for both head, torso, and tail queries. In detail, we filter out the items with the cosine similarity x bellowing a threshold t to ensure the equation $P(x >= t) = 0.985$. In general, head queries have a more uniform distribution and the numbers lie mostly in the range of [500, 1500], while torso and tail queries share similar skewed distribution and the numbers lie in the range of [0, 1000] and [0, 500], respectively. This indicates that head queries retrieve more items than torso and tail queries, which confirms the assumption that queries with higher popularity need more candidates and queries with lower popularity need fewer."}, {"title": "Conclusion", "content": "In this paper, we have proposed a novel probabilistic embedding based retrieval model, namely pEBR, to address the challenges of insufficient retrieval for head queries and irrelevant retrieval for tail queries. Based on the noise constrastive estimator, we have proposed two instance models: ExpNCE and BetaNCE, with the assumption that relative items follow truncated exponential distribution or beta distribution, which allows us to easily compute a probabilistic CDF threshold instead of relying on fixed thresholds, such as a fixed number of items or a fixed relevance score. Comprehensive experiments and ablation studies show that the proposed method not only achieves better performance in terms of recall and precision metrics but also presents desirable item distribution and number of retrieved items for head and tail queries."}, {"title": "Inference Truncation", "content": "The major goal of using query-dependent item distribution in neural retrieval is to give statistical meaning to the retrieved candidate set. Previously a somewhat arbitrary combination of cosine similarity threshold and top-K threshold are used.\n\n$S_{t,k}(q) = TopK_{cos(q)}({\\{ d_i : cos(q, d_i) \\geq t \\}})$ \n\nThe cosine threshold does not account for the variability of item distributions across different queries, while the top-K threshold is mainly to save inference costs.\n\nNow for a given q-isotropic spherical distribution H, whose marginal density with respect to cos(q,) is given by $h: [-1,1] \\rightarrow \\mathbb{R+}$, we can compute its CDF as follows\n\n$P(H < t) = \\frac{\\int_{-1}^t h(x) (1-x^2)^{(n-3)/2}dx}{\\int_{-1}^1 h(x) (1 - x^2)^{(n-3)/2}dx}$                                                                                                                   (3)\n\nFor a given h and t, (3) can be computed numerically. For the two special h we are concerned here, we can give semi-closed forms:\n\n\u2022 For the Beta density (BetaNCE) $h(x) \\propto (1+x)^{\\alpha-1}(1 - x)^{\\beta-1}$, the normalization constant, after the rescaling [-1,1] \u2192 [0, 1], is a complete Beta integral\n\n$\\int_{-1}^0 (1+x)^{\\alpha+ \\frac{n-3}{2} -1} (1 - x)^{\\beta+ \\frac{n-3}{2} -1} dx = B(\\alpha + \\frac{n-3}{2} , \\beta+ \\frac{n-3}{2}) = \\frac{\\Gamma(\\alpha + \\frac{n-3}{2})\\Gamma(\\beta + \\frac{n-3}{2})}{\\Gamma(\\alpha + \\beta + \\frac{n - 4}{2})}$\n\nwhile the numerator is proportional to an incomplete Beta integral\n\n$\\int_{-1}^{1+t} x^{\\alpha+ \\frac{n-3}{2} -1} (1 - x)^{\\beta+ \\frac{n-3}{2} -1} dx =: B_{\\frac{1+t}{2}}( \\alpha + \\frac{n-3}{2} , \\beta- \\frac{n-3}{2}).$\n\n\u2022 For the truncated exponential density (corresponding to InfoNCE), $h(x) \\propto e^{x/\\tau}$, the integral we need to compute is the following modified Bessel integral\n\n$\\int_{-1}^t e^{x/\\tau} (1-x^2)^{\\frac{n-3}{2}} dx = \\int_{-1}^t e^{x/\\tau} (1+x)^{\\frac{n-3}{2}-1}(1-x)^{\\frac{n-3}{2}-1}dx.$\n\nThis admits a closed-form solution for any t provided n > 2 is odd, however, the solution has alternating signs, which is numerically unstable, especially for large n."}]}