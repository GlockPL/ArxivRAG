{"title": "Towards Data-Efficient Pretraining for Atomic Property Prediction", "authors": ["Yasir Ghunaim", "Hasan Abed Al Kader Hammoud", "Bernard Ghanem"], "abstract": "This paper challenges the recent paradigm in atomic property prediction that links progress to growing dataset sizes and computational resources. We show that pretraining on a carefully selected, task-relevant dataset can match or even surpass large-scale pretraining, while using as little as 1/24th of the computational cost. We introduce the Chemical Similarity Index (CSI), a novel metric inspired by computer vision's Fr\u00e9chet Inception Distance, for molecular graphs which quantifies the alignment between upstream pretraining datasets and downstream tasks. By selecting the most relevant dataset with minimal CSI distance, we show that models pretrained on a smaller, focused dataset consistently outperform those pretrained on massive, mixed datasets such as JMP, even when those larger datasets include the relevant dataset. Counterintuitively, we also find that indiscriminately adding more data can degrade model performance when the additional data poorly aligns with the task at hand. Our findings highlight that quality often outperforms quantity in pretraining for atomic property prediction.", "sections": [{"title": "1. Introduction", "content": "Machine learning is transforming molecular modeling, driving advancements in accurate predictions and simulations of molecular behavior. These breakthroughs directly impact the acceleration of progress in crucial fields such as drug discovery and global climate change mitigation. The improvements in this field have been primarily attributed to innovations in model architectures and the growing availability of large-scale molecular datasets. In recent years, the sizes of molecular datasets have increased dramatically - from tens of thou-\nIs scaling data and resources the only path forward in atomic property prediction, or can intelligent data selection achieve similar performance more efficiently?\nWhile data selection strategies for pretraining have been explored in fields like natural language processing and computer vision, this area remains largely underexplored in atomic property prediction, where unique challenges arise. In his paper, we challenge the prevailing assumption that \"bigger is better\" by exploring whether a smaller, strategically selected dataset can lead to comparable or even superior performance while substantially reducing computational demands. We introduce a pretraining paradigm that shifts the focus from data and compute scaling to selecting the most relevant upstream dataset for improved downstream performance.\nThrough a simple baseline, our experiments reveal two key insights:"}, {"title": "2. Related Work", "content": "Pretraining for Atomic Property Prediction. Inspired by the success of pretraining in computer vision and natural language processing, pretraining for atomic property prediction has gained significant attention in recent years. Most approaches in molecular machine learning focus on self-supervised learning, as generating labels for molecular datasets is computationally expensive. In contrast, fewer studies explore the effectiveness of supervised transfer learning. However, in both self-supervised and supervised settings, the focus has primarily been on improving feature representation, often overlooking the impact of pretraining dataset relevance on downstream performance. Recently, JMP introduced multi-domain pretraining, enabling joint pretraining on various upstream sources concurrently. While effective, JMP pretraining requires enormous computational resources to reproduce and does not reveal how each upstream source impacts downstream performance. Our work addresses this gap by systematically studying the relationship between upstream pretraining datasets and downstream performance, enabling researchers to develop effective pretraining models even with limited computational resources.\nComputational Budgeting. Recent research highlights the importance of studying model performance under computationally budgeted setups. In continual learning (CL), works that simple baselines often outperform state-of-the-art methods in compute-constrained settings. TiC-CLIP further demonstrates efficient rehearsal-based training for time-continuous data. For Vision Transformers, Pan et al. propose a framework to dynamically control model complexity during training, achieving competitive performance under varying budgets. Li et al. formalize budgeted training, showing that budget-aware learning rate schedules, such as linear decay, are critical for robust performance across tasks like image classification and object detection. In multi-domain learning, Berriel et al. introduce Budget-Aware Adapters, which reduce computational complexity while maintaining accuracy by selecting relevant feature channels. These findings across domains emphasize the critical need for more efficient approaches that can achieve competitive performance while minimizing computational costs.\nData Selection. Efficient training through data selection has been explored via two primary approaches: subset selection and dataset distillation. Subset selection aims to identify a representative subset of the training data that matches or even outperforms training on the full dataset. Several methods have been proposed for vision and NLP tasks . Dataset distillation, introduced by focuses on generating a smaller, synthetic subset of the dataset that preserves performance while reducing training time and storage requirements. Subsequent work has explored techniques such as meta-learning, gradient matching , and distribution matching . While most research in distillation has focused on vision tasks, a few studies have extended it to graph data though primarily targeting"}, {"title": "3. Formulation and Setup", "content": "In this section, we present our problem setup, notion of a computational budget, and the a formulation of dataset similarity. We then detail how we adapt the Fr\u00e9chet Inception Distance (FID) to the molecular domain, yielding the Chemical Similarity Index (CSI). Our setup is illustrated in Figure 2. Throughout this work, we use the term 'molecular' broadly to encompass both molecular and materials domains, as well as their respective datasets."}, {"title": "3.1. Formal Setting", "content": "Upstream and Downstream Datasets. Let {D^(1), D^(2),..., D^(K)} denote a collection of K upstream (pretraining) datasets, each containing molecular structures paired with relevant atomic properties (e.g., energies and forces). In the typical paradigm, the user"}, {"title": "Multi-task Pretraining.", "content": "We consider a neural network \u03a6(\u00b7; \u03b8), where \u03b8 encompasses the shared backbone parameters \u03b8_b and task-specific head parameters \u03b8_e (for energy prediction) and \u03b8_f (for force prediction). During pretraining, the network is trained to simultaneously predict energies and forces. Formally, the multi-task pretraining objective over an upstream dataset D^(i) is given by:\n\u03b8^* = arg min_\u03b8 L_pretrain(\u03b8; D^(i)),"}, {"title": "Fine-Tuning.", "content": "After the multi-task pretraining phase, the task-specific heads \u03b8_e and \u03b8_f are discarded, and a new task-specific head \u03b8_h is attached to the pretrained backbone \u03b8_b. The downstream objective then becomes:\n\u03b8_h^* = arg min_{\u03b8_h} L_finetune(\u03b8_b^*, \u03b8_h; D_d),"}, {"title": "Computational Budget.", "content": "Following Hammoud et al., we define the computational budget C to be the product of the number of epochs E and the number of unique samples N in the pretraining dataset:\nC = E * N."}, {"title": "Dataset Similarity.", "content": "A key objective of this work is to estimate how well an upstream dataset D_u aligns with a downstream dataset D_d. We therefore seek a distance metric \u03b4(D_u, D_d)\nthat quantifies their relevance or \u201csimilarity.\u201d In principle, a smaller value \u03b4(D_u, D_d) indicates a greater overlap or relevance. Thus, among multiple candidate upstream datasets {D^(1),..., D^(K)}, the one that minimizes\nargmin_{1<=i<=K} d (D^(i), D_d)\nshould provide the most effective pretraining for D_d. In this paper, we empirically test this assumption, examining whether lower \u03b4-values indeed correlate with improved downstream performance. This motivates using \u03b4 as a principled metric to select or combine upstream datasets under a fixed computational budget."}, {"title": "3.2. The Chemical Similarity Index (CSI)", "content": "Recap of FID. Our proposed Chemical Similarity Index (CSI) draws its inspiration from the well-known Fr\u00e9chet Inception Distance (FID). Recall that FID is commonly used in computer vision to compare two sets of images via their feature distributions. Specifically, if one extracts features (e.g., from an Inception network) for datasets X and Y and denotes their empirical means and covariances by \u03bc_X, \u03a3_X and \u03bc_Y, \u03a3_Y, then\nFID(X, Y) = ||\u03bc_X - \u03bc_Y||^2 + Tr(\u03a3_X + \u03a3_Y - 2(\u03a3_X \u03a3_Y)^{1/2}).\nThe central idea is to represent each sample in a feature space where distances encode semantic similarity and then compare the distributions of these representations for the two datasets.\nDesign Considerations for CSI. Adapting FID to the molecular domain involves several key design choices regarding how to best modify the metric:\n4.  Experiments\nWe evaluate the impact of pretraining on different upstream datasets for downstream performance and investigate how well the CSI values in Figure 3 reflect the relevance of these datasets. We begin by defining the datasets, baselines, and evaluation setup.\nUpstream Datasets: Following JMP,"}, {"title": "4.1. Does CSI Correlate with Better Performance?", "content": "In Section 3, we introduced the Chemical Similarity Index (CSI) and presented CSI values. The results indicate that ANI-1x exhibits the highest similarity to all downstream datasets. This finding raises a critical question:\nCan CSI reliably guide the selection of pretraining datasets to achieve optimal performance on specific downstream tasks?\nTable 1 summarizes the downstream performance of models pretrained on different datasets in an in-distribution setting. Notably, the ANI-1x model consistently outperforms all other individual datasets and even mixed variants. For instance, on both rMD17, SPICE and QM9 datasets, ANI-1x achieves an MAE of 5.4, 5.13 and 2.9, compared to 6.7, 5.71 and 3.3 for JMP-S. Remarkably, this performance is achieved with 24\u00d7 less computational budget than JMP-S. Additionally, pretraining on alternative upstream tasks results in weaker performance compared to ANI-1x, correlating with the trends observed in our CSI values.\nFurthermore, temperature-based mixing, which follows the JMP formulation and prioritizes high-CSI datasets (OC20"}, {"title": "4.2. What is the Effect of Computational Budget?", "content": "Building on our earlier findings, we now investigate how varying the computational budget impacts downstream performance. Specifically, we ask:\nDo our findings about dataset relevance in terms of CSI hold across different budget levels?\nTakeaway. Our findings are consistent across budget levels:"}, {"title": "4.3. What is the Effect of Changing the Architecture Size?", "content": "In the previous sections, we used the small variant, GemNet-OC-S, as our backbone. Here, we address the question:\nDoes the correlation between CSI and downstream performance hold across different architecture sizes?\nTakeaway. Our findings hold across backbone sizes, showing the potential of relevance-based upstream dataset selection over large-scale mixing for improved performance."}, {"title": "4.4. Is More Data Always Better?", "content": "The common pretraining paradigm assumes that larger and more diverse datasets lead to better generalization. Here, we challenge this assumption from another perspective:\nDoes adding pretraining data from less relevant sources improve or degrade downstream performance?\nTo evaluate this, we pretrain on a mixture of 2M samples from the best CSI dataset (ANI-1x) and 1M from OC22, a non-aligned upstream dataset. We then compare this pre-training checkpoint with the 2M-sample baseline in Figure 4. Surprisingly, adding OC22 degraded downstream performance despite the increased pretraining budget.\nTakeaway. Including additional pretraining data from less relevant sources can harm downstream performance. Our findings highlight the CSI metric as a practical tool for designing effective pretraining datasets."}, {"title": "5. Beyond In-Distribution", "content": "Recall that our pretraining process is conducted on upstream tasks involving molecules and catalysts, with energy and force as targets. For downstream tasks with different labels (e.g., band gap in QMOF) or from distinct chemical domains such as materials (e.g., MatBench and QMOF), we classify these as out-of-distribution (OOD). While our main results focused on ID evaluation, here we explore our metric's applicability to OOD tasks. Specifically, we examine three cases: the Band Gap property from QMOF , Phonons (the first non-energy target in JMP tables) from MatBench (Dunn et al., 2020), and \u0394\u025b from QM9, explicitly categorized as OOD in the JMP paper.\nTable 4 shows that \u0394\u025b in QM9 aligns with the CSI pattern, similar to ID evaluation, suggesting that CSI is effective for OOD in the label space. In QMOF, the different upstream sources achieve similar performance which lags behind the full pretraining by JMP. For MatBench (evaluated over 5 folds), OC22 achieves the best mean performance while OC20 lags behind, despite our metric predicting both to be equally suitable. Additionally, for both QMOF and Mat-Bench, mixed pretraining variants generalize better than individual sources. This suggests that when the downstream domain differs from all upstream sources, mixing diverse upstream domains provides the best performance.\nWhile our CSI metric holds well for ID evaluation, further research is needed to improve OOD generalization across different chemical domains. One direction is to explore additional upstream sources to better capture domain variations. Another potential extension is leveraging foundation models trained beyond energy and forces, which could enhance feature extraction and improve similarity assessments."}, {"title": "6. Conclusion", "content": "This paper challenges the prevailing trend of scaling data and computational resources in atomic property prediction by demonstrating that strategic data selection based on relevance can achieve comparable or superior performance with significantly fewer resources. We introduce the Chemical Similarity Index (CSI), a simple metric that quantifies upstream/downstream datasets alignment, enabling the selection of high-quality, task-relevant pretraining data. Our experiments reveal that smaller, focused datasets often outperform larger, mixed ones, and that indiscriminately adding data can degrade performance when relevance is low. These findings highlight that quality often outweighs quantity in pretraining, offering a more efficient and sustainable path forward for molecular machine learning."}, {"title": "A. More Epochs or More Data?", "content": "In this section, we explore the trade-off between increasing the number of training epochs and expanding the dataset size under a fixed computational budget. Specifically, we aim to answer the following question:\nGiven a fixed computational budget, is it more effective to train on a smaller dataset for more epochs or to train on a larger dataset for fewer epochs?\nSetup. To investigate this question, we compare two scenarios under the same computational budget of 10M samples: (1) training on 2M samples for 5 epochs, and (2) training on 1M samples for 10 epochs. We evaluate the performance of models pretrained on ANI-1x, Transition-1x, OC20, and OC22, and fine-tune them on the downstream datasets: rMD17, MD22, SPICE, and QM9. For comparison, we also include the results of JMP-L and JMP-S, which use 120M samples for 2 epochs.\nResults. Table 5 presents the downstream performance for the two scenarios. Across all datasets, ANI-1x consistently achieves the best performance, regardless of whether the model is trained on 2M samples for 5 epochs or 1M samples for 10 epochs. For example, on rMD17, ANI-1x achieves a test error of 5.4 in both scenarios, outperforming JMP-S (6.7) and JMP-L (5.1). Similarly, on SPICE, ANI-1x achieves a test error of 5.08 (2M samples, 5 epochs) and 5.04 (1M samples, 10 epochs), compared to 5.71 for JMP-S and 4.75 for JMP-L.\nInterestingly, increasing the number of epochs from 5 to 10 while reducing the dataset size from 2M to 1M does not significantly degrade performance for ANI-1x. This suggests that for highly relevant datasets like ANI-1x, training on fewer samples for more epochs can be as effective as training on more samples for fewer epochs. However, for less relevant datasets like OC20 and OC22, increasing the number of epochs does not compensate for the reduced dataset size, as their performance degrades significantly.\nConclusion. Our findings indicate that the choice between more epochs or more data depends on the relevance of the dataset to the downstream task. For highly relevant datasets like ANI-1x, training on fewer samples for more epochs can yield comparable or even superior performance, while for less relevant datasets, increasing the dataset size is more beneficial. This further underscores the importance of dataset quality and relevance, as quantified by CSI, in determining the optimal pretraining strategy."}, {"title": "B. Additional Analysis on the Metric Design", "content": null}, {"title": "C. Long-Tail Analysis", "content": "As discussed in Section 3, molecular understanding datasets are typically imbalanced and exhibit a skewed distribution. Figure 9 illustrates the effect of random sampling versus class-balanced sampling, which we employ in this study, in terms of class coverage. Notably, random sampling results in significant class underrepresentation, whereas class-balanced sampling ensures broader coverage across all classes."}, {"title": "D. Implementation Details", "content": "For both pretraining and fine-tuning experiments, we primarily follow the JMP hyperparameters. However, due to resource constraints requiring smaller batch sizes compared to JMP, we adjusted the learning rate to ensure training stability, as detailed below.\nFor pretraining, we use a batch size of 20 and a learning rate (LR) of 1e-4 for the small backbone (GemNet-OC-S). For the large backbone (GemNet-OC-L), the batch size is reduced to 12 to fit GPU memory. Additionally, when training with the OC22 dataset on the large backbone, a LR of 1e-4 caused gradient instability, thus we used a LR of 1e-5 for that particular run. Unless otherwise specified, each experiment is run for five epochs on the specified number of samples for each section of the paper. The best checkpoint is selected based on the performance in the validation set. To handle the large size of the upstream validation sets, validation is performed on a smaller subset of 2,000 samples.\nFor finetuning, we use the batch size specified in the JMP codebase and a default learning rate (LR) of 8e-5, except for cases where adjustments were needed to stabilize training. Specifically, we use 5e-5 for QMOF, 8e-4 for MatBench when pretrained on Transition1x."}]}