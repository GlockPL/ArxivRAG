{"title": "LLM-QE: Improving Query Expansion by Aligning Large Language Models with Ranking Preferences", "authors": ["Sijia Yao", "Pengcheng Huang", "Zhenghao Liu", "Yu Gu", "Yukun Yan", "Shi Yu", "Ge Yu"], "abstract": "Query expansion plays a crucial role in information retrieval, which aims to bridge the semantic gap between queries and documents to improve matching performance. This paper introduces LLM-QE, a novel approach that leverages Large Language Models (LLMs) to generate document-based query expansions, thereby enhancing dense retrieval models. Unlike traditional methods, LLM-QE designs both rank-based and answer-based rewards and uses these reward models to optimize LLMs to align with the ranking preferences of both retrievers and LLMs, thus mitigating the hallucination of LLMs during query expansion. Our experiments on the zero-shot dense retrieval model, Contriever, demonstrate the effectiveness of LLM-QE, achieving an improvement of over 8%. Furthermore, by incorporating answer-based reward modeling, LLM-QE generates more relevant and precise information related to the documents, rather than simply producing redundant tokens to maximize rank-based rewards. Notably, LLM-QE also improves the training process of dense retrievers, achieving a more than 5% improvement after fine-tuning. All codes are available at https://github.com/NEUIR/LLM-QE.", "sections": [{"title": "Introduction", "content": "Dense retrievers (Karpukhin et al., 2020; Xiong et al., 2021a) encode both queries and documents into a shared embedding space, enabling efficient retrieval of relevant documents through the KNN search. Despite their effectiveness, these models often encounter challenges presented by the semantic gap between query terms and document content. To overcome this challenge, existing work has utilized query expansion techniques (Abdul-Jaleel et al., 2004; Robertson and Jones, 1976) to enhance the performance of both unsupervised and supervised dense retrieval models (Gao et al., 2023; Yu et al., 2021b). By enriching the original query with additional terms, these techniques increase lexical overlap with relevant documents, effectively narrowing the semantic gap between queries and documents.\nRecent developments in query expansion models have focused primarily on the Generative Relevance Feedback (GRF) method (Mackie et al., 2023b; Claveau, 2021), which leverages Large Language Models (LLMs) to expand queries with contextually relevant content. This approach employs LLMs to produce documents (Wang et al., 2023a) or Chain-of-Thought (CoT) (Wei et al., 2022) that are relevant to the query, thereby significantly enriching the semantics of the original query. However, directly applying LLMs to generate query-related content can introduce irrelevant or distracting information due to the hallucinations inherent in LLMs (Ji et al., 2023; Xu et al., 2024).\nTo address this limitation, several works have explored the self-consistency (Wang et al., 2023c) of LLMs in query expansion. Specifically, they use the instruction to prompt LLMs to generate diverse query-related contents as expansion results and then average their representations to mitigate inconsistencies (Gao et al., 2023). Other works further incorporate feedback from retrievers to select higher-quality expansions. They cross-verify the LLM-generated documents and pseudo-relevant documents by measuring their similarity (Jia et al., 2024). Nevertheless, optimizing LLMs to generate more precise query expansion results by aligning the ranking preferences of both retrievers and LLMs remains under-explored.\nIn this paper, we propose Large Language Model-based Query Expansion (LLM-QE), a novel framework that trains LLMs to align with the ranking preferences of both retrievers and LLMs, thus reducing hallucinations during the generation of expansion results. As shown in Figure 1, LLM-QE starts with an unsupervised dense retriever, then prompts LLMs to generate document-based query expansions (Gao et al., 2023; Wang et al., 2023a), and finally utilizes the Direct Preference Optimization (DPO) method (Rafailov et al., 2023) to optimize the query expansion model. Additionally, LLM-QE considers the ranking preferences of both retrieval models and LLMs and designs a reward model that combines both rank-based and answer-based rewards. Specifically, the rank-based reward model treats the ground truth document as the query and re-ranks the LLM-generated documents to calculate the ranking score. Meanwhile, in the answer-based reward model, we prompt LLMs to generate an answer based on both the query and the ground truth document. Then the generated answer serves as a new query to calculate the ranking score among the expanded documents.\nOur experiments on BEIR (Thakur et al., 2021) demonstrate the effectiveness of LLM-QE, achieving more than 8% and 5% improvements in unsupervised and supervised settings, respectively. Further analysis reveals the crucial roles of both rank-based and answer-based rewards in training LLM-QE. The rank-based reward directly models the ranking preference of the dense retriever, encouraging LLMs to generate more semantically relevant content to match the ground truth documents. However, relying solely on rank-based reward usually results in longer expansions in order to win the rank-based reward during optimization. By incorporating the answer-based reward, the length of document-based query expansions is significantly reduced. This improvement stems from the fact that the answer-based reward helps LLMs better assess expansion results by evaluating their relevance to answer-related content."}, {"title": "Related Work", "content": "Dense retrievers (Karpukhin et al., 2020; Xiong et al., 2021a; Izacard et al., 2021; Yu et al., 2021c; Xiong et al., 2021b; Li et al., 2021) have demonstrated superior ranking performance by conducting semantic matching between queries and documents, which helps overcome the problem of vocabulary mismatch (Belkin et al., 1982). These models typically leverage Pre-trained Language Models (PLMs) to encode both queries and documents into a shared embedding space, which significantly enhances retrieval effectiveness. To further optimize this embedding space, dense retrievers are often trained contrastively using relevance signals between queries and documents (Karpukhin et al., 2020; Zhan et al., 2021). Some studies have also developed zero-shot dense retrievers by training on weak supervision data (Xie et al., 2023) or leveraging Large Language Models (LLMs) for query expansion and reformulation (Gao and Callan, 2022).\nQuery expansion is a long-standing research direction, originally proposed to rewrite queries and improve the retrieval accuracy of exact matching-based retrievers, such as BM25 (Robertson et al., 2009). Early query expansion methods primarily aim to bridge the lexical gap between queries and documents (Carpineto and Romano, 2012; Rocchio, 1971) by expanding queries with knowledge bases (Bhogal et al., 2007; Qiu and Frei, 1993; Voorhees, 1994) or Pseudo-Relevance Feedback (PRF) (Amati and Van Rijsbergen, 2002; Robertson, 1990; Rocchio, 1971). These PRF-based methods have proven their effectiveness in enhancing reranking techniques (Li et al., 2018; Ai et al., 2018; Yu et al., 2021a) and improving dense retrievers by learning better query representations (Yu et al., 2021b).\nRecent research of query expansion focuses on Generative Relevance Feedback (GRF) methods, which utilize generation models to directly produce query expansion results (Mackie et al., 2023b; Claveau, 2021; Wang et al., 2023b; Jagerman et al., 2023; Mackie et al., 2023a). These methods often employ LLMs to generate query-related documents (Wang et al., 2023a; Jagerman et al., 2023; Gao et al., 2023), leverage Chain-of-Thought (CoT) reasoning results (Wei et al., 2022; Jagerman et al., 2023; Trivedi et al., 2023), or utilize specific keywords (Li et al., 2024; Jagerman et al., 2023) to expand queries, thereby enhancing the ranking capabilities of lexical matching based retrieval models (Jagerman et al., 2023; Wang et al., 2023a), dense retrieval models (Wang et al., 2023a), and reranking models (Li et al., 2024).\nAlthough these studies have demonstrated their advantages in improving retrieval models, directly using LLMs for query expansion and reformulation poses potential risks due to LLM hallucinations (Shuster et al., 2021; Huang et al., 2023). To mitigate this issue, some works use different instructions to reduce inconsistency in query reformulation (Gao et al., 2023) or leverage rephrased questions as demonstrations to guide LLMs in generating more effective query expansion results (Koo et al., 2024). RaFe (Mao et al., 2024) further enhances the Retrieval-Augmented Generation (RAG) performance by taking reranking scores as training signals to optimize the query rewriting model. In contrast to these approaches, LLM-QE focuses on modeling the ranking preferences of both retrievers and LLMs, rewarding LLMs for generating more effective expansion results, and exploring its potential to build both unsupervised and supervised dense retrieval models."}, {"title": "Methodology", "content": "As illustrated in Figure 2, this section describes our LLM based query expansion method, LLM-QE. First, we introduce our query-expanded dense retrieval method (Sec. 3.1). Next, we use the DPO method to optimize LLMs for query expansion (Sec. 3.2). Finally, we build a reward model to train LLM-QE by modeling the ranking preferences of both dense retrievers and LLMs (Sec. 3.3)."}, {"title": "Query Expanded Dense Retrieval", "content": "Given a query q and a document collection \\(D = {d_1,..., d_k}\\), dense retrieval models (Karpukhin et al., 2020; Xiong et al., 2021a; Gao and Callan, 2021) first encode the query q and the i-th document \\(d_i\\) into vector representations q and \\(d_i\\) using PLMs, such as BERT (Devlin et al., 2019):\n\\[q = \\text{BERT}_q(q), \\ d_i = \\text{BERT}_d(d_i).\\]\nThen the score S(q, \\(d_i\\)) is calculated to estimate the relevance between q and \\(d_i\\), followed by a KNN search (Douze et al., 2024) to retrieve the top-ranked documents to satisfy the user needs.\nDifferent from existing dense retrieval models, we introduce our query expanded dense retrieval method. Firstly, we prompt the LLM (M) to produce a document-based query expansion:\n\\[d^{exp} = M(\\text{Instruct}_{q2d}, q),\\]\nwhere Instruct\\(_q2d\\) denotes the instruction that asks LLMs to generate document-like query expansion outcomes (Jagerman et al., 2023). The representation of expanded query \\(q^{exp}\\) is calculated by averaging the representations of the raw query q and the document-based query expansion \\(d^{exp}\\) to enhance both unsupervised and supervised dense retrievers:\n\\[q^{exp} = \\frac{q+ d^{exp}}{2}.\\]\nUnsupervised Dense Retrieval. The unsupervised dense retrieval models, such as Contriever (Izacard et al., 2021), are pre-trained to match text segments that share the same semantic information (Fang et al., 2020; Wu et al., 2020) and do not use the query-document relevance label during training (Bajaj et al., 2016).\nFollowing Gao et al. (2023), we can directly use Eq. 3 to represent the expanded query by incorporating the information of raw query q and the document-based query expansion \\(d^{exp}\\). Then the relevance score S(q, \\(d_i\\)) between the query q and the candidate document \\(d_i\\) can be calculated:\n\\[S(q, d_i) = \\text{sim}(q^{exp}, d_i),\\]\nwhere sim(\\(\u00b7\\)) is the similarity estimation function that conducts the dot product operation.\nSupervised Dense Retrieval. Then we describe the details of training and inference processes in the supervised dense retrieval scenario.\nTraining. During training the dense retriever augmented with query expansions, we first regard the expansion result \\(d^{exp}\\) as the query and then calculate the similarity score S(\\(d^{exp}\\), \\(d_i\\)) between \\(d^{exp}\\) and \\(d_i\\):\n\\[S(d^{exp}, d_i) = \\text{sim}(d^{exp}, d_i).\\]\nThen we can contrastively train the encoder to learn matching signals from both \\(d^{exp}\\) and the query-related document \\(d^*\\) using the training loss \\(L_{DR}\\):\n\\[L_{DR} = - \\text{log} \\frac{e^{S(d^{exp}, d^*)}}{e^{S(d^{exp}, d^*)} + \\mathbb{E}_{d_- \\in D^-} e^{S(d^{exp}, d_-)}}.\\]"}, {"title": "Expanding Queries by Training LLMS with Preference Optimization", "content": "Existing query expansion methods (Gao et al., 2023; Li et al., 2024) typically focus on directly prompting LLMs to generate various expansion results to mitigate hallucinations (Brown et al., 2020; Thoppilan et al., 2022). To obtain more tailored and query-specific expansion results, we fine-tune the LLM to align with ranking preferences using the Direct Preference Optimization (DPO) method (Amini et al., 2024).\nFirst, we prompt the LLM to generate multiple expansion documents by adjusting the temperature \\(\\tau\\) during sampling:\n\\[d^{exp} \\sim M(\\text{Instruct}_{q2d}, q).\\]\nThus, we can collect k document-based query expansions \\(D_q = {d^{exp}_1, d^{exp}_2, ..., d^{exp}_k}\\). Then we follow the DPO method to optimize the LLM (M) using the loss function L(M; \\(M^{Ref}\\)):\n\\[L(M; M^{Ref}) = - \\mathbb{E}_{(q, d^{exp}_+, d^{exp}_-) \\sim P} [\\text{log} \\sigma(\\beta (log \\frac{M(d^{exp}_+ | q)}{M^{Ref}(d^{exp}_+ | q)} - log \\frac{M(d^{exp}_- | q)}{M^{Ref}(d^{exp}_- | q)}))],\\]\nwhere \\(\\sigma\\) is the Sigmoid function and \\(\\beta\\) is a hyperparameter that controls the strength of the regulation from the reference model \\(M^{Ref}\\). \\(M^{Ref}\\) is frozen during DPO training. P is the dataset used for DPO training, which contains the triple (q, \\(d^{exp}_+\\), \\(d^{exp}_-\\)). \\(d^{exp}_+\\) and \\(d^{exp}_-\\) are positive and negative responses, which are sampled from the document-based query expansions \\(D_q\\):\n\\[R(d^{exp}_+) > R(d^{exp}_-),\\]\nwhere R(\\(d^{exp}\\)) is the reward model that is used to estimate the ranking preference of the document-based query expansion \\(d^{exp}\\). The details of the ranking preference modeling are introduced in Sec. 3.3."}, {"title": "Modeling Ranking Preferences for Rewarding Query Expansion Models", "content": "For the given query q, the quality of the i-th document-based query expansion \\(d^{exp}_i\\) is evaluated using the reward R(\\(d^{exp}_i\\)) defined as:\n\\[R(d^{exp}_i) = R_{rank}(d^{exp}_i) + R_{ans}(d^{exp}_i),\\]\nwhere \\(d^{exp}_i \\in D_q\\). \\(R_{rank}(d^{exp}_i)\\) and \\(R_{ans}(d^{exp}_i)\\) represent the rank-based reward score and the answer-based reward score, respectively. These scores are combined to model the ranking preference by capturing the relevance preference of the dense retriever and estimating the consistency with the question answering model.\nRank-based Reward. To assess the quality of the document-based query expansion \\(d^{exp}_i\\), the most straightforward approach is to use a ranking score. Specifically, we calculate the Mean Reciprocal Rank (MRR) score by treating the ground truth document \\(d^*\\) as the query and ranking the document-based query expansions \\(D^q\\):\n\\[R_{rank}(d^{exp}_i) = \\frac{1}{Rank(d^*, d^{exp}_i)},\\]\nwhere Rank(\\(d^*\\), \\(d^{exp}_i\\)) represents the rank of \\(d^{exp}_i\\) based on the relevant score sim(\\(d^*\\), \\(d^{exp}_i\\)). A higher reward score \\(R_{rank}(d^{exp}_i)\\) indicates that the document-based query expansion \\(d^{exp}_i\\) is more similar to the ground truth document \\(d^*\\).\nAnswer-based Reward. While the rank-based reward modeling accurately captures the preference of dense retrievers, it often leads to biases and fairness issues in reward modeling (Dai et al., 2024). Thus, relying solely on rank-based reward may cause the dense retrieval model to overfit to the inherent biases of dense retrievers. To address this challenge, we propose an answer-based reward modeling approach, which leverages the self-consistency of LLMs to calibrate the reward.\nSpecifically, we first instruct the LLM to generate an answer y based on the given query q and the ground truth document \\(d^*\\):\n\\[y = M(\\text{Instruct}_{q2a}, q, d^*),\\]\nwhere Instruct\\(_q2a\\) is the instruction that guides the LLM to generate a response to answer the query q. We then treat the LLM-generated answer y as a query and rank the document-based query expansions \\(D^q\\) to compute the answer-based reward:\n\\[R_{ans}(d^{exp}_i) = \\frac{1}{Rank(y, d^{exp}_i)},\\]\nwhere Rank(y, \\(d^{exp}_i\\)) denotes the rank of document \\(d^{exp}_i\\) based on its relevance score sim(y, \\(d^{exp}_i\\)).\nBoth y and \\(d^{exp}_i\\) are generated by the same LLM, using different instructions: Instruct\\(_q2a\\) for generating the answer and Instruct\\(_q2d\\) for generating the expansion documents. Higher-ranked documents indicate that y and \\(d^{exp}_i\\) share more semantic similarity, reflecting greater consistency and semantic agreement between y and \\(d^{exp}_i\\)."}, {"title": "Experimental Methodology", "content": "In this section, we introduce the datasets, evaluation metrics, baselines, and implementation details used in our experiments. More experimental details are shown in Appendix A.2.\nDataset. We utilize various datasets for training and evaluation. Data statistics are shown in Table 1.\nTraining. We use the publicly available E5 dataset (Wang et al., 2024; Springer et al., 2024) to train both the LLM-QE and dense retrievers. We concentrate on English-based question answering tasks and collect a total of 808,740 queries. From this set, we randomly sample 100,000 queries to construct the DPO training data, while the remaining queries are used for contrastive training. During the DPO preference pair construction, we first prompt LLMs to generate expansion documents, filtering out queries where the expanded documents share low similarity with the query. This results in a final set of 30,000 queries.\nEvaluation. We evaluate retrieval effectiveness using two retrieval benchmarks: MS MARCO (Bajaj et al., 2016) and BEIR (Thakur et al., 2021), in both unsupervised and supervised settings.\nEvaluation Metrics. We use nDCG@10 as the evaluation metric. Statistical significance is tested using a permutation test with p < 0.05.\nBaselines. We compare our LLM-QE model with three unsupervised retrieval models and five query expansion baseline models.\nThree unsupervised retrieval models\u2013BM25 (Robertson et al., 2009), Co-Condenser (Gao and Callan, 2022), and Contriever (Izacard et al., 2021)\u2013are evaluated in the experiments. Among these, Contriever serves as our primary baseline retrieval model, as it is used as the backbone model to assess the query expansion performance of LLM-QE. Additionally, we compare LLM-QE with Contriever in a supervised setting using the same training dataset.\nFor query expansion, we benchmark against five methods: Pseudo-Relevance Feedback (PRF), Q2Q, Q2E, Q2C, and Q2D. PRF is specifically implemented following the approach in Yu et al. (2021b), which enhances query understanding by extracting keywords from query-related documents. The Q2Q, Q2E, Q2C, and Q2D methods (Jagerman et al., 2023; Li et al., 2024) expand the original query by prompting LLMs to generate query-related queries, keywords, chains-of-thought (Wei et al., 2022), and documents.\nImplementation Details. For our query expansion model, we deploy the Meta-LLaMA-3-8B-Instruct (AI@Meta, 2024) as the backbone for the query expansion generator. The batch size is set to 16, and the learning rate is set to 2e \u2013 5. Optimization is performed using the AdamW optimizer. We employ LoRA (Hu et al., 2022) to efficiently fine-tune the model for 2 epochs. The temperature for the construction of the DPO data varies across \\(\\tau\\) \u2208 {0.8, 0.9, 1.0, 1.1}, with each setting sampled eight times. For the dense retriever, we utilize Contriever (Izacard et al., 2021) as the backbone. During training, we set the batch size to 1,024 and the learning rate to 3e \u2013 5, with the model trained for 3 epochs."}, {"title": "Evaluation Results", "content": "In this section, we present the overall performance of LLM-QE, conduct ablation studies, and analyze the effectiveness of different reward models. Detailed analysis of query expansion quality is provided in the Appendix A.3. The case study is shown in Appendix A.4."}, {"title": "Overall Performance", "content": "The retrieval performance of different query expansion models is shown in Table 2.\nWe begin by evaluating the performance of LLM-QE in an unsupervised setting. The evaluation results show that LLM-QE achieves an 8.6% improvement over the retriever Contriever, demonstrating the effectiveness of query expansion in enhancing unsupervised retrieval models. Among all query expansion baselines, Q2D performs the best, indicating that generating query-related documents as the expansion results is particularly suited for LLMs to generate effective expansion results. After DPO training, LLM-QE shows a significant improvement over the Q2D method, confirming the effectiveness of its training approach in enhancing the expansion capabilities of the Q2D model.\nAdditionally, LLM-QE demonstrates its robustness by extending its advantages to supervised retrieval training scenarios. After fine-tuning dense retrieval models using the E5 dataset, LLM-QE consistently outperforms the Contriever fine-tuned with raw queries across most datasets, achieving an average improvement of 5.5%. This indicates the effectiveness of our query expansion techniques in benefiting the training process of dense retrievers by narrowing the semantic gap between queries and documents."}, {"title": "Ablation Study", "content": "In this subsection, we present ablation studies to investigate the effectiveness of LLM-QE in various query expansion scenarios and analyze the impact of different query expansion models in both unsupervised and supervised settings.\nAs shown in Table 3, we first apply LLM-QE to several query expansion methods in the unsupervised setting, including Q2Q, Q2E, and Q2C. The results indicate that LLM-QE consistently improves the performance of different expansion formats by approximately 3%, highlighting its generalization ability. Among all the expansion formats, LLM-QE (Q2D) achieves the best performance, indicating that prompting the LLM to generate tailored pseudo-relevant documents for specific queries can bring more precise retrieval results (Gao et al., 2023).\nNext, we explore the role of the reward model by optimizing the LLM-QE (Q2D) under two settings: one with only the rank-based reward and the other with only the answer-based reward, resulting in the LLM-QE (Q2D) w/o Answer Reward model and the LLM-QE (Q2D) w/o Rank Reward model, respectively. The rank-based reward, which utilizes preference signals from the dense retriever, typically achieves better ranking performance than the answer-based reward. However, when both rewards are combined, LLM-QE (Q2D) shows further improvements, particularly in question-answering tasks. This demonstrates the effectiveness of answer-based reward modeling, which leverages the consistency between generated answers and query-expanded documents to estimate the quality of the expansion results.\nFinally, we assess the effectiveness of LLM-QE (Q2Q), LLM-QE (Q2E), LLM-QE (Q2C), and LLM-QE (Q2D) in the supervised setting. Overall, all query expansion models show a consistent improvement of more than 5% across various expansion formats, further confirming the efficacy of LLM-based query expansion models in better aligning the semantics of queries and their corresponding documents during contrastive training. Among all query expansion models, LLM-QE (Q2D) achieves the best retrieval performance, with an improvement of about 2% over other expansion models, demonstrating that LLM-QE (Q2D) can broaden its advantage in the supervised setting."}, {"title": "Effectiveness of Different Reward Models in Optimizing LLM-QE", "content": "Figure 3 illustrates that we evaluate the characteristics of reward models in LLM-QE based on the length, text similarity, and rank correlation of the query expansion results. We compare two ablation models, LLM-QE (w/o Rank Reward) and LLM-QE (w/o Answer Reward), with the LLM-QE model. The former two train LLM-QE models using only the answer-based reward and the rank-based reward, respectively.\nLength of Query Expansion. First, we analyze the average length of query expansions generated by different models, as shown in Figure 3(a).\nThe evaluation results show that LLM-QE (w/o Answer Reward) produces the longest query expansions, indicating that using only the rank-based reward encourages the model to generate redundant tokens related to the ground truth document. Such a behavior can help LLMs win a higher rank-based reward score. However, when the answer-based reward is incorporated, the length of the query expansions generated by LLM-QE significantly decreases. This proves the effectiveness of the answer-based reward in preventing excessively long query expansions, reducing the tendency of the LLM-QE model to overfit to the ranking preferences of dense retrievers, and mitigating the generation of noisy information.\nText Similarity Evaluation. In the following experiment, we evaluate the text similarity between document-based query expansions and LLM-generated answers/golden documents.\nAs illustrated in Figure 3(b), we first estimate the similarity between document-based query expansions and LLM-generated answers. LLM-QE (w/o Rank Reward) is designed to optimize LLMs to generate query expansions that are more closely related to the LLM-generated answers, thereby achieving the highest similarity score. In contrast, LLM-QE (w/o Answer Reward) yields the lowest BLEU score, indicating that rank-based rewards are less effective in guiding LLM-QE to align with the information in answers, which is particularly important for question answering tasks. When answer-based rewards are incorporated, the BLEU scores for LLM-QE increase, showing its effectiveness in generating more precise information for expansion. Next, we estimate the text similarity between query expansions generated by different models and ground-truth documents, as shown in Figure 3(c). The evaluation results reveal that LLM-QE (w/o Rank Reward) presents the lowest BLEU score, suggesting that optimizing solely with the answer-based reward is insufficient to improve the text similarity between query expansions and golden documents. In comparison, LLM-QE (w/o Answer Reward) achieves better performance, demonstrating the effectiveness of rank-based rewards in guiding LLMs to generate more relevant semantics for matching the golden documents. By combining both rewards, LLM-QE achieves the highest BLEU score, underscoring the importance of using both rewards to optimize LLMs to generate higher-quality query expansions.\nRank Correlation. Lastly, we assess the similarity between raw queries and query expansions by ranking these candidate documents and calculate the rank correlation.\nAs shown in Figure 3(d), we calculate the Pearson Correlation Coefficient (PCC) to measure the correlation between the query-based document rank and the expansion-based document rank. Specifically, we employ a fine-tuned retriever, BGE (Chen et al., 2024), to compute the similarity between raw queries and relevant documents. We then use expansions generated by different models as queries and calculate the similarity with candidate documents. Finally, we compute the PCC between these similarity score lists.\nThe results show that LLM-QE (w/o Rank Reward) achieves the highest PCC score, demonstrating the effectiveness of answer-based rewards in producing query expansions that better reflect user search intentions. In contrast, LLM-QE (w/o Answer Reward) achieves the lowest PCC score among all models, which shows that the information beyond the user intention is also incorporated in the expansion results. The additional information misleads the well-trained dense retriever, but benefits LLM-QE to produce more effective query expansions for Contriever."}, {"title": "Conclusion", "content": "This paper presents a novel query expansion framework, LLM-QE, which optimizes LLM-generated query expansions to align with the ranking preferences of dense retrievers. By incorporating both rank-based and answer-based reward models, LLM-QE effectively enhances the quality of query expansions by mitigating the generation of noisy information and avoiding excessively long expansions. Experimental results demonstrate that LLM-QE consistently improves performance in both unsupervised and supervised training scenarios, providing insights into integrating query expansion into dense retrieval training."}, {"title": "Limitations", "content": "Despite the effectiveness of LLM-QE in enhancing query expansion for dense retrieval, several limitations remain. First, LLM-QE depends on the quality of document-based query expansions generated by the LLM. If the LLM produces low-quality or biased expansions, retrieval performance may degrade. While the reward model helps mitigate this issue, further improvements in controlling LLM outputs are still an open challenge. Additionally, compared to vanilla dense retrieval models, LLM-QE introduces additional computational costs due to the need for generating document-based query expansions."}, {"title": "Appendix", "content": "A.1 License\nThe authors of 4 out of the 15 datasets in the BEIR benchmark (NFCorpus, FiQA-2018, Quora, Climate-Fever) and the authors of ELI5 in the E5 dataset do not report the dataset license in the paper or a repository. We summarize the licenses of the remaining datasets as follows.\nMS MARCO (MIT License); FEVER, NQ, and DBPedia (CC BY-SA 3.0 license); ArguAna and Touch\u00e9-2020 (CC BY 4.0 license); CQADupStack and TriviaQA (Apache License 2.0); SciFact (CC BY-NC 2.0 license); SCIDOCS (GNU General Public License v3.0); HotpotQA and SQUAD (CC BY-SA 4.0 license); TREC-COVID (Dataset License Agreement).\nAll these licenses and agreements permit the use of their data for academic purposes.\nA.2 Additional Experimental Details\nThis subsection outlines the components of the training data and presents the prompt templates used in the experiments.\nTraining Datasets. Following the setup of Wang et al. (2024), we use the following datasets: A.3 Query Expansion Quality of LLM-QE\nThis section evaluates the quality of query expansion of LLM-QE. As shown in Figure 4, we randomly select 100 samples from each dataset to assess the improvement in retrieval performance before and after applying LLM-QE.\nOverall, the evaluation results demonstrate that LLM-QE consistently improves retrieval performance in both unsupervised (Figure 4(a)) and supervised (Figure 4(b)) settings. However, for the MS MARCO dataset, LLM-QE demonstrates limited effectiveness in the supervised setting. This can be attributed to the fact that MS MARCO provides higher-quality training signals, allowing the dense retriever to learn sufficient matching signals from relevance labels. In contrast, LLM-QE leads to more substantial performance improvements on the NQ and HotpotQA datasets. This indicates that LLM-QE provides essential matching signals for dense retrievers, particularly in retrieval scenarios where high-quality training signals are scarce.\nA.4 Case Study\nTo further demonstrate the effectiveness of LLM-QE, we conduct a case study by randomly sampling a query from the evaluation dataset. We then compare retrieval performance using the raw queries, expanded queries by vanilla LLM, and expanded queries by LLM-QE.\nAs shown in Table 5, query expansion significantly improves retrieval performance compared to using the raw query. Both vanilla LLM and LLM-QE generate expansions that include key phrases, such as \"temperature\u201d, \u201chumidity\u201d, and \u201ccoronavirus\", which provide crucial signals for document matching. However, vanilla LLM produces inconsistent results, including conflicting claims about temperature ranges and virus survival conditions. In contrast, LLM-QE generates expansions that are more semantically aligned with the golden passage, such as \u201cthe virus may thrive in cooler and more humid environments, which can facilitate its transmission\u201d. This further demonstrates the effectiveness of LLM-QE in improving query expansion by aligning with the ranking preferences of both LLMs and retrievers."}]}