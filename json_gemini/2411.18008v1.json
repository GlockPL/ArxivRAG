{"title": "Causal and Local Correlations Based Network for Multivariate Time Series Classification", "authors": ["Mingsen Du", "Yanxuan Wei", "Xiangwei Zheng", "Cun Jia"], "abstract": "Recently, time series classification has attracted the attention of a large number of researchers, and hundreds of methods have been proposed. However, these methods often ignore the spatial correlations among dimensions and the local correlations among features. To address this issue, the causal and local correlations based network (CaLoNet) is proposed in this study for multivariate time series classification. First, pairwise spatial correlations between dimensions are modeled using causality modeling to obtain the graph structure. Then, a relationship extraction network is used to fuse local correlations to obtain long-term dependency features. Finally, the graph structure and long-term dependency features are integrated into the graph neural network. Experiments on the UEA datasets show that CaLoNet can obtain competitive performance compared with state-of-the-art methods.", "sections": [{"title": "1. Introduction", "content": "With the development of the Internet of Things, large numbers of sensors are now used to periodically collect data. These collected data naturally exist in the form of time series. Usually, time series contain multiple variables with correlations. In other words, the collected data are in the format of a multivariate time series (MTS). MTS classification aims to assign predefined labels to time series using supervised learning [1], and it is widely used in various real-life domains, such as human behavior recognition [2], healthcare [3, 4], macroeconomics [5], and misinformation detection [6].\nIn recent years, many different methods based on different patterns have been proposed. Among them, methods based on various correlations have been widely explored [7, 8]. The variables in MTS data usually exhibit complex correlations, and fully using these correlations can help build more accurate prediction models. Therefore, ignoring the correlations between variables leads to information loss and reduces prediction performance. By analyzing the local and dimensional correlations in MTS, we can discover hidden patterns, regularities, and potential structures within the data, thereby gaining a deeper understanding and insight. The correlations between variables may reflect certain causal mechanisms, providing clues for uncovering the underlying mechanisms that generate the data. This holds particular significance in fields such as healthcare, finance, and meteorology, as it helps unravel the intrinsic connections among phenomena.\nMTSs typically exhibit the following correlations: 1) Local correlations within a single dimension. Subsequent observations in a time series are influenced by the preceding observations. For example, future traffic flow is influenced by current traffic flow, and specific streets are more likely to be influenced by traffic information from neighboring areas.  2) Spatial correlations among dimensions. Each variable in an MTS is influenced by other variables. Some studies [10, 11] assume that the predicted values of individual variables are influenced by all other variables or exhibit other spatial correlations. Considering this information is highly beneficial for exploring the interactions among MTSs.\nRegarding local correlations, Transformer-based methods have shown great potential in recent years, and numerous methods have used this model for MTS tasks [12]. A large number of methods have been proposed to extract local correlations [13]. The authors of [7] proposed a variable-position Transformer to extract local correlations between variables. It takes time series subsequences, which can come from different variables and positions (time intervals), as input (at shape level). The local correlations capture long- and short-term dependencies between shapes to process them. In addition, other methods have improved the Transformer and made progress in extracting local correlation [14, 15, 16, 17, 18]. These methods have contributed to improving the capabilities of modeling time series data and processing local correlations.\nRegarding spatial correlations, numerous methods have been proposed, among which methods based on the self-attention mechanism (Transformer) and graph neural networks (GNNs) have received much attention. Methods based on the self-attention mechanism include a variable-position Transformer proposed by [7] to extract correlations between dimensions. The authors of [19] designed a channel-aware Transformer encoder to capture the complex relationships between different time channels in an MTS. However, the Transformer has the following major drawbacks when extracting spatial correlations from an MTS compared with a GNN: (1) Lack of explicit structural modeling capability. The Transformer relies on a fully connected self-attention mechanism to learn the correlations between sequences, but cannot explicitly model the topological relationships between variables. A GNN, by contrast, can directly operate on the defined graph structure, more easily capturing and using the explicit relationships among variables. (2) High computational complexity. The Transformer's self-attention mechanism must compute the correlation scores between all element pairs, resulting in a computational complexity of O(n\u00b2). On a high-dimensional MTS, the Transformer may encounter computational bottlenecks. A GNN, however, can effectively use sparse connections to reduce computational overhead. (3) Difficulty in using prior knowledge. The Transformer mainly learns the relationships between variables in a data-driven manner, and lacks mechanisms for using prior domain knowledge. In many application scenarios, however, we may already know some of the topological structures between variables, which could help simplify the learning task of a GNN. (4) Lack of reasoning ability on graphs. The Transformer learns implicit variable correlation representations and lacks the ability to reason on explicit graph structures like a GNN, which complicates interpretability analysis.\nMethods based on GNN and graph learning include the time dynamic GNN [8], which can extract hidden spatial-temporal dependencies without defining the graph structure and includes a time graph pooling layer to obtain a global graph-level representation of graph learning with learnable time parameters. Graph learning [20], [21], [22] provides greater flexibility, but also leads to higher computational complexity, uncertainty, and data demands. Compared with using a fixed predefined graph structure, graph learning methods have the following drawbacks: (1) Higher computational complexity. Graph learning methods need to simultaneously optimize the graph structure and model parameters, which usually significantly increases computational costs. Fixed graph structures, by contrast, avoid the overhead of learning the graph structure. (2) Poor convergence and stability. Because the graph structure and parameters are optimized simultaneously, graph learning problems are often non-convex, prone to suboptimal solutions, and have poor convergence and stability. In contrast, training models on a fixed graph structure is more stable. (3) Lack of theoretical guarantees. Current graph learning methods are mainly based on empirical or heuristic algorithms, which lack theoretical convergence guarantees and performance bounds, making it difficult to ensure that the learned graph structure is optimal. In some traditional tasks or scenarios with limited data, predefined fixed graph structures may be more suitable.\nOverall, the Transformer is more suitable for modeling global correlations, whereas the GNN excels at capturing and using structured prior knowledge, and the two models have some complementarity. Combining the advantages of these two types of models has the potential to better exploit the rich information in MTS data.\nHowever, current research still faces the following challenges: (1) Modeling the spatial correlations between MTS dimensions does not lead to explicit representations. Moreover, modeling in an unreasonable way may lead to indistinguishable representations, resulting in poor accuracy. The Transformer [7] relies on a fully connected self-attention mechanism to learn the correlations between sequences, but cannot explicitly model the topological relationships between variables. The GNN [8], by contrast, can directly operate on the defined graph structure or the learned graph structure, making it easier to capture and use the explicit relationships between variables. Compared with using a fixed predefined graph structure, graph learning methods [8, 20] have drawbacks such as higher computational complexity and poor convergence and stability. (2) Various MTS classification methods based on local correlations are often unable to more effectively and efficiently model local correlations and incorporate them into representation learning. Compared with other local correlation modeling methods, the Transformer has some unique advantages in extracting local correlations. For example, a Transformer based on the self-attention mechanism can directly capture the dependencies between any two time steps in the sequence, regardless of the time step distance. This allows it to effectively learn long-range local correlation patterns. However, the Transformer's self-attention mechanism must compute the correlation scores between all element pairs in the sequence, resulting in a high (quadratic) computational complexity. This is a challenge for long sequences and high-dimensional sequences. Extensive research [14, 16, 17] has been conducted on reducing complexity issues.\nTo address these challenges, this work proposes a novel end-to-end deep-learning model called the causal and local correlations based network (CaLoNet). In CaLoNet, the first step is to leverage causal correlations to model the pairwise spatial correlations between the dimensions of an MTS using graph structures. Next, a relationship extraction network is employed to fuse local correlations, enabling the extraction of long-term dependency features. Finally, the graph structures and long-term dependency features are incorporated into a GNN.\nThe main contributions of this paper are summarized as follows.\n1. A novel MTS classification network is designed to exploit spatial and local correlations. In this network, spatial correlations and local correlations are jointly used to model the time series for MTS classification.\n2. A novel strategy for constructing the graph structure of an MTS is proposed. This strategy characterizes the causal information through transfer entropy. Additionally, we further propose a graph construction strategy that serves as a new form of time series representation for graph-level classification tasks in MTS classification.\n3. We designed a novel representation learning approach for time series spatial correlations in MTS classification. Graph-level tasks include graph classification, graph regression, and graph matching, all of which require models to learn graph representations.\n4. A large number of experiments on publicly available datasets demonstrate the effectiveness of our method.\nThe remainder of this paper is structured as follows. Section 2 introduces the related work. Section 3 describes our method in detail. The experimental results are presented in Section 4, and our conclusions are provided in Section 5."}, {"title": "2. Related work", "content": "MTS classification aims to predict the class label of an unlabeled time series as accurately as possible. Recently, many research studies have been devoted to MTS classification, and they have proposed a large number of MTS classification methods. These methods can be divided into traditional machine learning-based methods and deep-learning-based methods.\nTraditional machine learning-based methods usually extract features through tedious feature engineering or data preprocessing. Many machine learning-based methods have been proposed, and they can be generally classified into two categories [23]: 1) distance-based methods, which use the distance between time series as a similarity metric [24]. For example, G\u00f3recki et al. [25] proposed PDDTW, which combines the dynamic time warping (DTW) distance between MTS with the DTW distance between derivatives of MTS for classification. 2) Feature-based methods classify time series based on several features [1, 26] such as structural features and statistical features. Sch\u00e4fer and Leser [27] proposed WEASEL+MUSE, which uses a bag-of-symbolic-Fourier-approximations model for MTS classification. Baydogan and Runger [28] introduced SMTS, which uses a code book to capture the local relationships between different dimensions for MTS classification.\nDeep-learning-based methods aim to unfold the internal representational hierarchy of time series, which helps to capture the intrinsic correlations among the representations [29]. In contrast to traditional machine learning-based methods, deep-learning-based methods successfully solve the problem of mining raw low-dimensional time series to create high-dimension features, and feature extraction can be performed in an end-to-end form.\nConsequently, this study focuses on deep-learning-based MTS classification methods because of their excellent ability to capture relationships. Several representative deep-learning-based methods are described in following subsection."}, {"title": "2.2. Deep-learning-based methods", "content": "In recent years, increasingly more researchers have classified MTSs through deep learning. Chen et al. [30] used a multi-level discrete wavelet decomposition to decompose an MTS into a group of sub-MTSs to extract multi-level time-frequency representations. In [30], a convolutional neural network (CNN) was developed for each level to learn level-specific nonlinear features, and a metric learning layer was added on the top of the network to learn the semantic similarity of MTSs. Zheng et al. [31] first learned features from individual time series in each channel, and then combined features from all channels to achieve classification. Tripathi and Baruah [32] used an attention-based CNN to encode information across multiple time stamps to learn the temporal features of an MTS. Huang et al. [33] proposed FDESN, which is a novel bi-level approach for optimizing parameters. FDESN uses temporal and spatial aggregations for MTS classification. Zhang et al. [34] designed a random group permutation method combined with a CNN to learn the features.\nIn an MTS, some correlations among features exist. Moreover, the local attributes in the time series classification problem are ordered, which makes this task clearly different from traditional classification problems. In essence, it is irrelevant whether the attributes in time series are ordered in time or not; what matters is the possible presence of order-dependent discriminative features in time series [35]. It is also crucial to discover the intrinsic correlations among features extracted from different positions. Various deep-learning methods have their own priorities, and we focus on two methods that are relevant to this study in the two following subsections."}, {"title": "2.3. Spatial and causal correlation-based methods", "content": "There is a certain relationship between different dimensions of an MTS. Spatial correlations can be viewed as dependencies between MTSs, and they can be obtained by several quantitative methods. Zuo et al. [36] modeled spatial-temporal dynamic features to demonstrate that the temporal dependency and evolution of the spatial interactions are important for MTS classification. However, this method does not have an explicit spatial structure.\nCausal correlations between MTS dimensions can be viewed as a subset of spatial correlations. Yang et al. [37] proposed a method that approximates the time series dynamics and explicitly learns the causal correlation-based relationships among multiple variables. Duan et al. [10] combined a GNN and an encoder-decoder-based variational graph pooling module to create adaptive centroids for graph coarsening. Zha et al. [38] represented time series classification as a node-level classification problem in a graph, where nodes correspond to each time series and links correspond to similarities between time series that are used in the final time series classification."}, {"title": "2.4. Local correlation-based methods", "content": "Local correlation-based methods usually contain a feature extraction network and a relationship network [35]. The relationship network focuse on extracting the relationships among the features that are obtained by the feature extraction network. The relationship networks can be a Transformer-based model, a self-attention-based model or a long short-term memory (LSTM)-based model.\nThe model proposed by Xiao et al. [35] included a temporal feature network to extract local features extraction and an LSTM-based attention network to mine the intrinsic correlations among the features. Liu et al. [39] proposed the GTN, an extension of the current Transformer with a gate. The GTN can model channel-wise and step-wise correlations individually. Karim et al. [40] used an LSTM and a FCN with a squeeze-and-excitation (SE) block [41] to capture feature correlations. Chen et al. [42] used a sparse self-attention (SSA)-based network to extract local features and correlations. Hao et al. [43] introduced a temporal attention mechanism to extract long- and short-term dependencies cross all time steps. Yu et al. [6] designed a traffic incident classifier based on an LSTM. This classifier was trained on time series feature vectors from both normal and collusion attack scenarios, and hence it can recognized dynamic traffic parameter patterns. Hong et al. [44] proposed LMGRU, which can obtain the local correlations of time series effectively. Zhang et al. [45] used a temporal attention encoder for extracting global temporal features and convolution for extracting local temporal features. Their method proved the effectiveness of hybrid global-local temporal attention features for MTS classification."}, {"title": "3. Proposed method", "content": "In this section, we first briefly introduce the overall structure of CaLoNet, and then detail the local correlation network, causal correlation network, and the specifics of the completed classification task."}, {"title": "3.1. Overview", "content": "The overall architecture of CaLoNet is shown in Fig. 2. CaLoNet consists of four main steps:\n\u2022 Causal correlation matrix construction. As shown in the upper left of Fig. 2, this step obtains the causal graph matrix between dimensions with the help of transfer entropy, as detailed in Section 3.2.\n\u2022 Local correlation extraction. As shown in the bottom left of Fig. 2, this step obtains the correlations among local features through a local correlation network. Each colored dot represents the local correlation features of each dimension, as detailed in Section 3.3.\n\u2022 Node embedding. This step obtains the node embedding using a GNN based on the causal graph matrix and node features of the local correlations, as presented in Section 3.4.\n\u2022 Prediction. This step predicts the class labels using a multi-layer perceptron (MLP), as described in Section 3.5."}, {"title": "3.2. Causal correlation matrix construction", "content": "Transfer entropy is used to calculate how much information is reduced in an observed system, and CaLoNet constructs the causal-based spatial correlation graph with the help of transfer entropy.\nGranger causality analysis [46, 47] is one of the best-known methods for quantitatively characterizing time series causality. However, as a linear model, Granger causality analysis cannot handle the possible nonlinear correlations of an MTS well. Therefore, transfer entropy [48] was proposed to perform a causality analysis that can handle the nonlinear cases. The study [48] first introduced transfer entropy as an information-theoretic-based causality measure. The transfer entropy from dimension Y to dimension X is defined in the following equation. In this equation, xt and yt denote the values of the time series at time t, and they represent the past state; Xt+1 and yt+1 represent the future state. In addition, $x^{(k)}_{t} = [x_t, x_{t-1}, ..., x_{t-k+1}]$ and $y^{(l)}_{t} = [y_t, y_{t-1}, ..., y_{t-l+1}]$.\n$TE_{y \\rightarrow x} = \\sum P(x_{t+1}, x^{(k)}_{t}, y^{(l)}_{t}) log_2 \\frac{P(x_{t+1} | x^{(k)}_{t}, y^{(l)}_{t})}{P(x_{t+1}/x^{(k)}_{t})}$\n$= \\sum P(x_{t+1}, x^{(k)}_{t}, y^{(l)}_{t}) log_2 P(x_{t+1} | x^{(k)}_{t}, y^{(l)}_{t}) - \\sum P(x_{t+1}, x^{(k)}_{t}) log_2 P(x_{t+1} | x^{(k)}_{t})$\n$= H(X_{t+1} | X_{t}) \u2013 H(X_{t+1} | X_{t}, Y_{t}),$\n(1)\nIn Eq.(1), H (Xt+1 | Xt) H (Xt+1 | Xt, Yt) is the conditional entropy, which means that the transfer entropy from Y to X represents the reduction of uncertainty in the value of X when the past value of X is known. Here, Xt and Y represent the past state, whereas Xt+1 and Yt+1 represent the future state. Conditional entropy H(XY) represents the amount of information in X given the known condition of Y. Moreover, $p(x_{t+1}, x^{(k)}_{t}, y^{(l)}_{t})$ represents the joint probability distribution of future state xt+1 given past states $x^{(k)}_{t}$ and $y^{(l)}_{t}$; $P(X_{t+1} | x^{(k)}_{t}, y^{(l)}_{t})$ represents the conditional probability distribution of future state xt+1 given past states $x^{(k)}_{t}$ and $y^{(l)}_{t}$); $p(x_{t+1} | x^{(k)}_{t})$ represents the conditional probability distribution of future state xt+1 given past state $x^{(k)}_{t}$);\nH (Xt+1 | Xt) represents the conditional entropy of future state xt+1 given past state Xt.\nFor two dimensions X and Y, the conditional entropy is defined as follows, where x denotes all possible values in dimension X and y denotes all possible values in dimension Y.\n$H(XIY) = - \\sum\\sum p(x, y) log_2(p(xy))$  (2)\nWhen the transfer entropy TEx\u2192y is larger than the transfer entropy TEy\u2192x, a causal correlation between the two variables is established. The causal correlation between X and Y can be further defined as follows.\n$C_{X,Y} = TE_{X \\rightarrow Y} - TE_{y \\rightarrow X}$ (3)\nIf Cx,y is greater than 0, we say that X affects Y.\nFinally, the causal correlations TE of the MTS are constructed in a matrix M. The dimensions of M are n \u00d7 n (where n is the number of dimensions in T). The value Mi,j in the i-th row and j-th column of M can be calculated as follows, where Ti represents the i-th dimension of MTS T, Tj represents the j-th dimension of T, and c is the threshold value used to determine whether the causal correlation is significant. An example of a causal correlation matrix is shown in Fig.3, in which a 6 \u00d7 6 causal correlations matrix has been obtained for an MTS with six dimensions using this procedure.\n$M_{ij} =\n\\begin{cases}\nC_{T_i, T_j} & C_{T_i, T_j} > c \\\\\n0, & otherwise\n\\end{cases}$\n(4)\nTransfer entropy is a method used to quantify the causal relationships between MTSs. It is based on the concept of information theory and measures the \"transfer\" or \"propagation\u201d of information from one time series to another. Transfer entropy helps identify causal dependencies between time series, i.e., how the values of one time series influence the values of another. By computing transfer entropy, we can quantify the strength of causal relationships between time series. A higher value of transfer entropy indicates a stronger influence of X on Y, suggesting a stronger causal relationship. Conversely, a value close to zero indicates a weaker influence of X on Y, implying a lack of obvious causality.\nTransfer entropy can be used to compute a causal relationship matrix between MTSs and plays an important role in the GNN. Transfer entropy can be used to construct causal relationship graphs by computing the causal relationships between time series. In the GNN, these causal relationship graphs can be represented as a graph structure, where nodes represent time series and edges represent causal relationships between time series. For instance, in Section 3.4, the causal correlations matrix has the graph structure of A graph isomorphism network (GIN). Such causal relationship graphs capture complex dependencies between time series and provide a foundation for subsequent analysis and prediction tasks. The causal relationship matrix obtained from transfer entropy calculation can serve as one of the inputs to the GNN. In the GNN, node features are typically used to represent attributes of time series, whereas edges represent relationships between time series. The causal relationship matrix can be used to define the adjacency matrix of the graph, explicitly representing the causal relationships between time series. Thus, a GNN can learn the causal propagation patterns between time series, thereby improving the accuracy of prediction and analysis.\nIn summary, the application of transfer entropy in a GNN helps model and capture causal relationships between MTSs, enhancing the predictive and analytical capabilities of the models. By combining the strengths of transfer entropy and the GNN, we can better understand the dynamic characteristics and interactions of time series data, thereby advancing research and applications in related fields."}, {"title": "3.3. Local correlation extraction", "content": "CaLoNet extracts local correlations as shown in Fig.2. The following main processes are used to extract local correlations:\n\u2022 EMBED. First, MTS is partitioned. We define the initial embedding of an MTS as T (with D dimensions, where each dimension has a length of L). CaLoNet uses four non-overlapping neighboring timestamps to obtain $\\frac{L}{4}$ time chunks. Each time chunk is then flattened and projected into a 4D-dimensional embedding. Finally, we obtain an$\\frac{T}{4} \\times 4D$-dimensional embedding, denoted as E, where Xembed has a dimension of 4D and the length of each dimension is $\\frac{T}{4}$\n\u2022 CBAM. The convolutional block attention module (CBAM) is a network used for feature refinement. We describe CBAM in Section 3.3.1.\n\u2022 SSA. The SSA layer is a network used for extracting local correlations. We describe SSA in Section 3.3.2.\n\u2022 LN. The layer normalization(LN) layer normalizes the hidden layers in the network to a standard normal distribution to speed up training and accelerate convergence.\n\u2022 MLP. The MLP layer functions as a fully connected layer.\n\u2022 SHIFT. The above process, except for the embedding layer, is repeated twice in the local correlation layer. However, a shift layer is added in the second pass [49], which moves the temporal blocks within the window. This solves the problem of global features being restricted to the local window partition. Each patch can then interact with other patches in a new window."}, {"title": "3.3.1. CBAM layer", "content": "Effective feature capture plays a crucial role in the MTS classification task. The CBAM layer [50] shares similarities with the SE block [40, 41], as both leverage attention mechanisms for feature refinement. The attention mechanism facilitates the refinement of features, whereas the self-attention mechanism establishes connections between different positions of a time series to derive relationships at specific positions. The CBAM layer generates attention maps along two independent dimensions: channel and spatial. These attention maps are then multiplied by the input feature map to adaptively refine features.\nFirst, MTS embedding is performed through the CBAM layer, which is a module designed to exploit spatial and channel attention mechanisms to focus on more discriminative features. As shown in Fig.5, channel attention is used to obtain E' by employing average pooling and maximum pooling operations, ultimately aggregating the spatial information of the feature map. The channel attention is calculated as follows, where W\u2081 and Wo represent the shared parameters of the MLP and E denotes the features from the EMBED layer.\n$E' = sigmoid(MLP(AvgPool(E)) + MLP(MaxPool(E)))$\n$= sigmoid(W_1(W_0(E_{avg})) + W_1(W_0(E_{max}))) $   (5)\nThe spatial attention mechanism is used to obtain the spatial attention E\". $E'_{avg}$ and $E'_{max}$ represent the average pooling operation and the maximum pooling operation applied along the channel axis. Then, the spatial attention feature map is generated by the one-dimensional CNN, it is computed as follows, where $f^a$ denotes a one-dimensional convolution operation with filter size a, and E' are features from the channel attention layer.\n$E\" = sigmoid(f^a([AvgPool(E'); MaxPool(E')]))$\n$= sigmoid(f^a([E'_{ avg}; E'_{ max}])) $   (6)"}, {"title": "3.3.2. SSA layer", "content": "The SSA layer, which is necessary for effective local correlation extraction, is used by CaLoNet to efficiently extract local correlations, resulting in significant time savings. For instance, [49, 51] explored both long-term and short-term dependencies in the data, thereby enhancing relationship extraction. The incorporation of the CBAM into the self-attention structure theoretically enables the extraction of more time-sensitive features and higher sensitivity to global changes in non-periodic data than fully connected networks.\nAs depicted in Fig. 6, the dot product score is calculated by multiplying Q and K to obtain the computed weight coefficient and compute the similarity first.\nNext, the SSA mechanism is used. The heads of each layer may focus on different types of features, as described by [52], when stacking Transformers. For example, for a time series generated monthly, the heads of each layer will focus on the features of a particular week. Therefore, we restrict the cells to two adjacent layers. That is, we only allow each layer of cells to participate in the dot product operation for those cells with previous exponential steps. Thus, the SSA layer only needs to compute the O(log L) (L is the time series length) dot product in each layer, as shown in Fig. 7.\nFurthermore, to handle the SSA layer's memory bottleneck problem, the space complexity of the normal transformer grows quadratically with the increase in time series length L. The time complexity of O(L\u00b2) results in huge memory consumption [53]. Therefore, to compute the self-attention dot product more efficiently, we adopt a sparse strategy [52] to design the attention layer. We introduce a sparse bias matrix M in the self-attention model with a log-sparse mask, as shown in Eq.(8) and Fig.7. In this approach, only the O(log L) dot product of each cell in each layer must be computed, thus reducing the computational complexity from O(L\u00b2) to O(L log L).\nThen, the softmax activation function is applied to the score obtained through the incorporated SSA mechanism. Finally, the softmax dot product V is calculated to obtain the weighted score for each input vector.\nThe query, key, and value matrices are matrices obtained by extracting features from the CBAM layer. Specifically, embedding E (Eembed) is represented using matrices Q, K, and V, as shown in Eq.(7). The dot product for the self-attention mechanism is then calculated using Eq.(8), as shown in Fig.6. In Fig.6, first, the dot product score is calculated by multiplying Q and V to obtain the computed weight coefficients and similarity. Then, the softmax activation function is applied to the score through the incorporated SSA mechanism. Finally, the softmax dot product V is used to obtain the weighted score for each input vector.\n$Q = CBAM_Q(E_{embed})$\n$K = CBAM_K(E_{embed})$\n$V = CBAM_V(E_{embed})$  (7)\n$F = SoftMax(\\frac{Q K^T}{\\sqrt{d_k}} + B)V $ (8)\nIn Eq.(7), CBAM represents the function of the CBAM layer, which is used to obtain the query, key, and value matrices. In Eq.(8), SoftMax is a function commonly used to compute a probability distribution that takes a vector as input and converts it to a vector of probability distributions. It is often used in classification tasks to represent the probability of different categories. In addition, $\\sqrt{d_k}$ is used for score normalization, $K^T$ represents the transpose of K, and B is a sparse bias matrix based on Fig. 7."}, {"title": "3.4. Node embedding", "content": "After obtaining the graph structure as described in Section 3.2 and the node features as described in Section 3.3, the method updates the node features to obtain the final node representations (right panel in Fig. 8). Using the different working mechanisms in the two approaches, namely local correlations and spatial correlations, we explore the possibility of connecting these two approaches to jointly model MTS learning representations. However, existing deep-learning methods cannot explicitly represent MTS spatial correlations. The input MTS is converted into a feature matrix \u0397 \u2208 Fnxd, where d is the calculated number of features, as introduced in Section 3.2. Matrix H can be viewed as a graph feature matrix with n nodes, which correspond to the six variables of the MTS. The adjacency of nodes in the graph structure is determined by causal matrix T. For this graph structure, a GNN can be directly applied to the node embeddings. Inspired by the GIN model [54], the following propagation mechanism is used to compute the forward-passing update for a node represented by v. GIN updates the node representations as shown in Eq. (9).\nAfter processing with the GIN model, the feature vectors hk) hv for each node are computed. The GIN model updates node features by iteratively applying MLP operations MLPk. An MLP is a multi-layered neural network that uses nonlinear transformations to process node features. For each node v, its feature vector is updated at each layer k to capture information about the node at that specific layer.\nWhen updating node features, the GIN model incorporates a tunable parameter ek specific to each layer k. This parameter adaptively controls the influence of the previous layer's features h(k-1) on the current layer's features hk). By adjusting the value of ek, the model can balance the propagation and aggregation of features across different layers.\nDuring the computation of node features, the GIN model also leverages information from the node's neighbors. N(v) represents the set of neighbors of node v, which includes neighboring nodes u. For each node v, the GIN model aggregates the feature vectors h(k-1) of its neighboring nodes by summing them. This aggregation of neighbor information helps the synthesis of node features and contextual modeling.\n$h_v^{(k)} = MLP_k ((1+e). h_v^{(k-1)} + \\sum_{u \\in N(v)} h_u^{(k-1)} ) $ (9)\nIn Eq. (9), hk) represents the feature vector of node v at layer k\u2014this vector captures the information about the node at that particular layer; MLPk denotes the MLP applied to the features at layer k, where the MLP is a feed-forward neural network with multiple layers of neurons that transform the input features; ek is an updatable parameter specific to layer k that allows the model to adaptively control the influence of the previous layer's features h(k-1) on the current layer's features h(k); N(v) represents the neighborhood of node v, which consists of neighboring nodes u; hu(k-1) represents the feature vector of the neighboring node u at layer k \u2212 1. The sum is taken over all neighboring nodes to aggregate their features."}, {"title": "3.5. Classification", "content": "Finally, CaLoNet classifies the final features X using an MLP. In the MLP, X is converted to predicted class labels using a fully connected layer. We trained the MLP using the loss function shown in Eq.(10), where N represents the number of samples in the training set, M is the number of labels, Yi,j denotes the true label of the i-th sample belonging to the j-th class, and \u0177i,j denotes the probability that the model predicts that the i-th sample belongs to the j-th class. The loss function is in the form of cross entropy, which measures the difference between the true label y and the predicted probability \u0177.\n$l(X) = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} y_{i,j} log p(y_{i,j})$  (10)"}, {"title": "4. Experiment", "content": ""}, {"title": "4.1. Experimental setting", "content": ""}, {"title": "4.1.1. Datasets", "content": "Twenty-one multivariate datasets from the UEA archive\u00b9 were used for our experiments. Table 1 lists their detailed information."}, {"title": "4.1.2. Experimental running environment", "content": "Our experiments were conducted on a Windows computer using Python 3.9 and PyTorch 1.10. Minimization was performed using a cross-entropy loss function and the ADAM optimizer. The results were obtained after 50 iterations for all datasets."}, {"title": "4.1.3. Reproducibility", "content": "For reproducibility, we released our code and parameters on GitHub.2 The experimental results can be independently replicated."}, {"title": "4.2. Comparison with baselines", "content": "CaLoNet was compared with 16 baselines: SMATE [36", "40": "WEASEL+MUSE [27", "42": "MR-PETSC [55", "56": "DTW-1NN-I [56"}, {"56": "ED-1NN(norm) [56"}, {"56": 11}, {"56": "RT(100%) [13", "13": "MF-Net [57", "34": "and MTSC_FF [58"}]}