{"title": "Bayesian Optimization for Hyperparameters Tuning in Neural Networks", "authors": ["Gabriele Onorato"], "abstract": "This thesis is the culmination of an internal project, equivalent to an internship, that was undertaken during the final year of my degree under the supervision of Prof. Giampaolo Liuzzi.\nBayesian Optimization is a branch of Mathematical Programming that involves the maximization or minimization of a function by selecting input values from a feasible set where membership is easily determined.\nThis method is commonly employed to black-box functions that require a significant amount of time to evaluate, the number of evaluations that can be performed is quite limited and derivatives are neither used nor available.\nFor this project, we have chosen Ax as our environment. Ax serves as a wrapper for BOTorch, a framework specifically designed for efficient Bayesian Optimization that utilizes GPU acceleration to enhance performance.\nThe algorithm devised in this project is engineered to find the global minimum or maximum points of a given function. Initially, it is applied to a selected set of test functions. Subsequently, it is utilized to fine-tune the hyperparameters of a Convolutional Neural Network (CNN). This CNN, trained using PyTorch, is purposed for image classification tasks on the CIFAR10 dataset.", "sections": [{"title": "Introduction", "content": "Mathematical Optimization (also known as Mathematical Programming) is a subfield of Applied Mathematics that consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function.\nOptimization problems arise in all quantitative disciplines from computer science and engineering to operations research and economics.\nMathematical Optimization encompasses several types, including but not limited to:\nLinear Programming: This involves problems in which the objective function and the constraints are linear.\nNonlinear Programming: This involves problems where the objective function or the constraints or both contain nonlinear parts.\nInteger Programming: In this type, some or all of the variables are required to be integers.\nStochastic Programming: This type considers optimization in the presence of uncertainty.\nMulti-Objective Programming: This involves optimization problems with multiple conflicting objectives, with solutions that may be evaluated based on Pareto efficiency.\nIndeed, only Linear and Nonlinear Programming are mutually exclusive. However, we will encounter problems that simultaneously fall into the categories of Nonlinear, Integer, and Stochastic programming."}, {"title": "Linear Programming", "content": "Linear Programming, henceforth referred to as LP, is a method used for optimizing a linear objective function that is subject to linear equality and inequality constraints. The feasible region of this method is a convex polytope.\nLP problems are typically articulated in the following standard form:\n$\\begin{cases}\\min c^T x \\\\Ax = b\\\\x \\geq 0 \\end{cases}$ (1.1)\nNumerous practical issues in fields such as Economics and Production can be effectively represented as LP problems, as illustrated in Chapter 3 of Massimo Roma's notes [1], It's also common to find that when modeling a problem, it becomes simpler to represent it in a format known as the \"general form\" as shown below.\n$\\begin{cases}\\min c^T x \\\\Ax \\geq b \\end{cases}$ (1.2)\nAs outlined in Chapter 6.1 of the referenced notes [1], this problem is equivalent to 1.1. Henceforth, we will refer to the latter problem without lack of generality.\nBefore analysing the Simplex Method, we will illustrate below one of the most important theorem of LP.\nTheorem 1.2.1 (Fundamental Theorem of Linear Programming).\nConsider the LP problem 1.2, if the polyhedron $P = \\{x \\in R^n| Ax > b\\}$ does not contain any line, then only and only one of the subsequent statement is true:\nThe problem 1.2 has no solution/is infeasible, that is, the polyhedron P is empty.\nThe problem 1.2 is unbounded.\nThe problem 1.2 has an optimal solution, and at least one of them is a vertex of the polyhedron P.\nThe Simplex Method exploits this theorem by sequentially visiting the vertices of P. It applies the sufficient condition for optimality, or determines that the problem is unbounded.\nDelving into the details of how the Simplex Method operates is not a minor discussion. It is extensively covered in Chapter 6 of Massimo Roma's notes [1], as well as in numerous other university-level textbooks.\nIn the following subchapter, we will examine the efficiency of this algorithm from a computer scientist's perspective. This analysis will undoubtedly be beneficial for future comparisons with other algorithms, including the one we designed."}, {"title": "Efficiency of the Simplex Algorithm", "content": "The question of whether a pivoting rule exists within the Simplex Method (or Algorithm) that solves the problem in polynomial time remains unresolved. We will commence our analysis by examining the fundamental question: How challenging is a Linear Programming problem?\nIt has been established that problems characterized by linear inequality constraints and a linear objective function belong to the class P (Polynomial Time). This classification signifies that these problems can be solved in polynomial time with respect to their input size. A notable method for solving such problems is Karmarkar's Algorithm [2], which provides a polynomial-time solution.\nContrarily, the Simplex Algorithm, when employing Bland's rule of pivoting (which guarantees finite termination), has a scenario where it traverses all vertices of the polytope before termination. This process, in fact, exhibits exponential time complexity because the number of vertices V of a polytope P of dimension n with k facets is given by McMullen's Upper Bound Theorem [3]: $O(k^{|n/2|})$\nWhy, then, is the Simplex Algorithm the most used utilized in practice? This is due to the fact that, on average, the Simplex Algorithm functions within a range of 3V to 4V, which is significantly lower, however, the existence of a pivoting rule that ensures polynomial termination in worst-case scenarios remains an unresolved issue in the field of Linear Optimization, and comprehensive examination can be found in V. Klee and G.L. Minty [4]."}, {"title": "Nonlinear Programming", "content": "Nonlinear Programming Problems are the most comprehensive type and do not make any assumptions about the linearity of either the objective function or the constraints.\nNonlinear Programming (NLP) Problems are typically modeled as follows:\n$\\begin{cases}\\min f(x) \\\\h(x) = 0\\\\(g(x) \\leq 0\\end{cases}$ (1.3)\nwhere $f: R^n \\rightarrow R$, $h : R^n \\rightarrow R^p$, and $g : R^n \\rightarrow R^m$ are at least once continuously differentiable in the region of interest, and at least twice in case of second order conditions.\nNonlinear Programming is an extensive field. While we don't aim to provide a detailed explanation, we will present the most crucial and recognized findings in this area, the topic is deeply covered in M. Sciandrone's Book [5].\nConsider now the unconstrained problem:\n$\\begin{cases}\\min f(x) \\\\x \\in R^n\\end{cases}$ (1.4)\nOptimality conditions for 1.4 are extensively covered in G. Liuzzi's notes [6] and in M. Sciandrone's book [5].\nNumerous tools utilized in Machine Learning belong to this category. For instance, loss functions serve as a measure to assess the accuracy of your algorithm in modeling your dataset. Specifically, the Mean Squared Error, a nonlinear convex function, is employed to adjust weights in Linear Regression. Additionally, constraints are incorporated to prevent overfitting through regularization.\nWe will only briefly touch upon the most crucial aspects. A key presumption we can make is that tuning hyperparameters for a Neural Network undoubtedly constitutes a Nonlinear Programming problem, considering that the function is not analytically accessible and is highly sensitive to alterations.\nWe are ready to analyse and discuss the Mean Squared Error and the subsequent approaches to achieve results of the following problem:\n$\\begin{cases}\\min L(w) = \\frac{1}{2}||Xw - y||^2 \\\\X \\in R^{n \\times p}\\\\w \\in R^p\\\\y\\in R^n\\end{cases}$ (1.5)\nIn this context, n denotes the total number of samples within the dataset, while w signifies a vector consisting of p weights. The term y relates to our target which our model must predict, and our objective is to minimize the error associated with it."}, {"title": "Integer Programming", "content": "Our discussion on Integer Programming will be concise, given its comprehensive treatment in M. Roma's Notes [1]. Integer programming encompasses a range of problems where variables are restricted to discrete values, rather than continuous ones. This can apply to scenarios such as determining the number of workers in a factory, the daily production quantities of sellable goods, or the total number of factories. Specifically, in our context, it pertains to setting the number of neurons per layer in a Neural Network.\nWe will now delve into the concept known as the relaxation of an Integer Programming problem, formalized as follows:\n$\\begin{cases}\\min f(x) \\\\x \\in D\\subset R^n\\\\x \\in Z^n\\end{cases}$ (1.6)\nThe Continuous Relaxation of this problem removes the integer constraint, meaning the condition $x \\in Z^n$ is omitted. This adaptation opens the door to various insights and analytical perspectives, which are thoroughly discussed in Chapter 9.2 of M. Roma's Notes [1].\nOne of the quintessential examples of Integer Programming, specifically within the linear scope, is the Binary Knapsack Problem. It has been established that there is a polynomial-time reduction from the SAT (Boolean satisfiability problem) to the Binary Knapsack Problem, thereby classifying it within the NP-Complete category. Consequently, as this specific Integer Programming challenge is NP-Complete, the entire class is deemed NP-Hard.\nThe range of methods for solving Integer Programming (IP) problems varies significantly. At one end of the spectrum is Total Enumeration, which entails assessing every possible feasible solution an approach that falls squarely within the EXPTIME (exponential time) complexity class. At the other end are Branch and Bound techniques, which segment the original problem into manageable subsets. For each subset, a permissible solution is sought, enabling the elimination of subproblems that cannot yield superior outcomes. This strategy significantly reduces the required evaluations. However, it's important to note that in the worst-case scenario, the computational time can still be exponential.\nThe final widespread method involves obtaining an approximate solution via solving the Continuous Relaxation of the original problem and subsequently rounding it to the closest admissible integer solution. It has been established that, particularly in scenarios with specific constraints (even if linear), this strategy can result in solutions that are infeasible or significantly worse than the optimal, deviating considerably from the intended approximation. Therefore, this method is only viable under certain regularity conditions of the problem's bounds. We plan to adopt this strategy within our Bayesian Optimization algorithm, provided these conditions are met.\nThese methods are extensively treated in Chapter 10 of M. Roma's Notes [1]."}, {"title": "Overview", "content": "In this chapter, we delve into the essence of Neural Networks as applied in Machine Learning, aiming to establish a solid foundation by exploring their fundamental principles, architecture, and capabilities. Subsequently, we will explore their broad applications and discuss their implementation in PyTorch [12]."}, {"title": "Machine Learning", "content": "Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform specific tasks without explicit instructions. It relies on patterns and inference derived from data to make decisions and predictions. Essentially, machine learning processes vast datasets to 'learn' and evolve based on experience. Its applications are wide-ranging and transformative, encompassing areas like speech and image recognition, as well as predictive analytics."}, {"title": "Historical Context", "content": "The idea of neural networks dates back to the 1940s when Warren McCulloch and Walter Pitts presented a mathematical model of a neural network. In the 1950s and 1960s, pioneers like Frank Rosenblatt and Bernard Widrow further developed the concept with models like the Perceptron and Adaline, which laid the foundation for modern neural networks. Despite early enthusiasm, limitations in technology and theoretical understanding led to periods of reduced interest, known as \"AI winters\". The resurgence of neural networks in the late 20th and early 21st centuries has been fueled by advances in computational power, the availability of large datasets, and improvements in algorithms, leading to the current era of deep learning."}, {"title": "Basic Structure of Neural Networks", "content": "A neural network consists of layers of interconnected nodes or neurons, where each node represents a specific output function called an activation function. The simplest structure includes three layers: an input layer, at least one hidden layer, and an output layer. Neurons in each layer connect to every neuron in the subsequent layer, forming a dense network through which data flows and is processed."}, {"title": "Input Layer", "content": "The input layer serves as the gateway for raw data, with each neuron representing a distinct input feature. In our scenario, where the input is an image, the number of neurons will be determined by the formula height \u00d7 width \u00d7 channels, reflecting the image's dimensions and color depth. The exact number of channels is contingent on the input format."}, {"title": "Hidden Layers", "content": "Hidden layers execute computations on inputs from the preceding layer, employing weights (parameters) and biases that are refined throughout the training process by minimizing a loss function. The complexity and depth of patterns a network can learn are significantly influenced by the number of hidden layers and the neurons within each layer. Our Bayesian Optimization algorithm will utilize the number of neurons per layer as an input parameter."}, {"title": "Output Layer", "content": "The output layer produces the final prediction or classification result, depending on the task the network is designed to perform."}, {"title": "Learning Process", "content": "Neural networks learn through a process known as backpropagation. During training, the network makes predictions on the input data, calculates the error of its predictions compared to the actual target values, and adjusts its weights and biases to minimize this error. The optimization of weights is typically performed using gradient descent or variations thereof."}, {"title": "Types of Neural Networks", "content": "There are various types of neural networks, each suited to different tasks and data types. Some of the most common include:\nFeedforward Neural Networks (FNN): The simplest type, where connections between the units do not form a cycle as illustrated in (Figure 2.1).\nConvolutional Neural Networks (CNN): Exceptionally suited for image-related tasks, CNNs employ convolutional layers to extract spatial features, use max pooling for dimensionality reduction, and subsequently channel the processed data through a series of fully connected layers for classification purposes.\nRecurrent Neural Networks (RNN): Designed for sequential data, such as time series or natural language, with connections forming directed cycles that uses the recently generated data to predict the next output."}, {"title": "Applications", "content": "Autoencoders: Used for unsupervised learning tasks, such as dimensionality reduction or feature learning, by learning to compress and then reconstruct the input data.\nGenerative Adversarial Networks (GANs): Composed of two networks, a generator and a discriminator, competing against each other to generate new data samples."}, {"title": "Computer Vision", "content": "CNNs have been fundamental to the advancement of computer vision technologies, enabling groundbreaking applications such as facial recognition, object detection, and image classification. Our project focuses on tuning hyperparameters for a CNN designed explicitly for image classification, with the objective of elevating its precision and efficiency through the application of various metrics."}, {"title": "Natural Language Processing", "content": "RNNs and the more recent Transformer architectures have significantly advanced machine understanding and generation of human language. These technological strides have opened new horizons in machine translation, text summarization, and sentiment analysis. Among the most prominent Large Language Models (LLMs), ChatGPT utilizes these sophisticated technologies, enhancing productivity across various industrial sectors and daily activities."}, {"title": "Activation Functions", "content": "An activation function in neural networks serves as a crucial mathematical tool that injects non-linearity into the network's learning mechanism. It essentially takes the input signal and transforms it into an output signal for the subsequent layer. This step is vital for the network's ability to digest and learn from complex data patterns, something that mere linear operations cannot accomplish.\nWithout the introduction of non-linearity, a neural network with many layers would be equivalent to a single-layer perceptron, limited to understanding only linear relationships. Simply put, layering multiple linear transformations together would still yield a linear outcome. Non-linear activation functions shatter this barrier, equipping neural networks with the capability to capture and model the complex, nuanced relationships found within various types of data, such as images, audio, and text.\nThe selection of activation functions is diverse, and choosing the appropriate type for each layer is a critical decision. This decision significantly influences the network's performance and is an example of a hyperparameter that can be tuned using our Bayesian Optimization Algorithm.\nLet's plot and analyze some well-known activation functions, examining their features and gradients."}, {"title": "The Problem of Exploding and Vanishing Gradients", "content": "As mentioned in the previous chapters, the training phase involves the systematic updating of weights via backpropagation. This process is fundamentally about computing the gradient of the loss function with respect to each weight, employing the chain rule. These computed gradients are subsequently utilized by the selected optimizer to refine the model's parameters.\nA critical challenge often encountered is the vanishing gradient problem, which is particularly prevalent in deep neural networks comprising numerous layers. The essence of this problem lies in the gradients of the network's weights diminishing towards zero. As a result, the network struggles to effectively adjust its weights, which drastically slows down the training process or may even stall it altogether. This problem is typically exacerbated by the use of activation functions such as the Sigmoid or Hyperbolic Tangent, which tend to saturate.\nTo mitigate this issue, we have opted to incorporate the Rectified Linear Unit (ReLU) as our activation function within the neural network. The ReLU function is known for its ability to maintain a non-zero gradient, thereby facilitating a more robust and continuous learning process during our Bayesian Optimization Loop. This strategic choice aims to enhance the efficiency of our training process and improve the overall performance of the neural network."}, {"title": "Loss Functions and Optimizers", "content": "To prevent exploding gradients, methods such as the use of gradient rescaling and L1 and L2 penalty functions are employed, drawing on concepts similar to those found in Lagrange multipliers, as shown below.\nRegularization alters the optimization domain by imposing constraints that shift the position of the minimum point within the solution space. For instance, L1 regularization tends to drive smaller weights toward zero, influencing the model's complexity and robustness.\nThese methods are also effective in preventing overfitting. For a detailed analysis, refer to the research by Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio [10].\nIn training a neural network, the loss function plays a pivotal role by evaluating the discrepancy between the network's current predictions and the actual ground truth within the dataset at each step. This evaluation guides the estimation of necessary adjustments. To optimize this process, first-order derivative methods, such as SGD (Alg 2) are commonly employed. These methods leverage backpropagation to calculate the gradient of the loss function accurately, followed by to adjusting the weights accordingly. For our implementation, we have opted for the Adam Optimizer [7], as implemented in PyTorch [12], due to its proven efficiency and effectiveness in handling such optimization tasks.\nIndeed, the choice of an optimizer is crucial for developing an efficient neural network. And it stands as a selectable hyperparameter within our Bayesian Optimization Algorithm.\nThe selection of the Loss Function is typically made on a case-by-case basis, influenced primarily by engineering considerations tailored to the specific application at hand. We will outline the most commonly used methods, but for a more detailed discussion on loss functions, refer to the initial subchapters of each section in the work of Juan Terven et al. [11]"}, {"title": "Loss Functions for Regression", "content": "We will analyze the two most common loss functions for regression: Mean Squared Error (MSE) and Mean Absolute Error (MAE), assuming that n is the number of predictions, x are the features in input, h(x) represents the vector of predictions, and y denotes the ground truth.\nRegarding the MSE which has been briefly introduced in 1.5, it is defined as follows:\n$MSE(x) = \\frac{1}{n}||h(x) - y||^2$ (2.1)\nHere, the MSE is characterized by its differentiability, allowing for easy computation of gradients due to its lack of discontinuities. In contrast, the MAE has a gradient that is undefined at zero, making it less straightforward for gradient-based optimization methods.\n$MAE(x) = \\frac{1}{n}\\sum_{i=1}^n |h(x_i) - y_i|$ (2.2)\nBoth of them are widely used, with MSE particularly emphasizing larger errors due to its squared term."}, {"title": "Loss Functions for Classification", "content": "In classification tasks, it's more challenging to establish a straightforward scale for evaluating error compared to regression tasks. For instance, in a simple binary classification task, the outcome is either correct or incorrect. So, how can we define the error in such cases? Let's explore the most commonly used methods starting from binary classification:\nBinary Cross-Entropy Loss (BCE), also known as Log Loss, quantifies the divergence between the predicted probability, p, of a class being 1 and the actual label, y, where y can be either 0 or 1. This measure is used to evaluate the accuracy of a binary classification model. Typically, a threshold of 0.5 is employed to decide whether the predicted probability is categorized as class 1 or class 0.\n$BCE(y,p) = -(y log(p) + (1 - y) log(1 - p))$ (2.3)"}, {"title": "Performance Metrics", "content": "BCE can be readily adapted to incorporate weights, particularly useful in scenarios where the sample distribution is imbalanced. This adaptation involves assigning a specific weight to each sample as follows:\n$BCE(y,p, w) = -w_i(y log(p) + w_1(1 - y) log(1 - p))$ (2.4)\nWeighted Binary Cross-Entropy Loss (WBCE) adjusts the model's focus by assigning greater importance to the under-represented class in imbalanced datasets. This weighting approach enhances the model's sensitivity to minority classes, improving performance on challenging datasets.\nBinary Cross-Entropy Loss (BCE) can be effectively extended to multi-class classification tasks by introducing Categorical Cross-Entropy Loss (CCE):\n$CCE(p, y) = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^C Y_{i,j} log(P_{i,j})$ (2.5)\nHere, N represents the number of samples, and C denotes the number of classes involved.\nThis adaptation enables the model to handle multiple classes by calculating the loss for each sample based on the discrepancy between the predicted probabilities and the actual class labels, and then averaging this loss across the entire dataset.\nTypically, the true label y is represented as a one-hot encoded vector, although in some implementations, the classes may be denoted by integers.\nFor our objectives, we utilized the Categorical Cross-Entropy Loss, as implemented in PyTorch. [12]\nPerformance metrics are essential for assessing and fine-tuning neural networks, providing insights into various aspects of model accuracy and effectiveness.\nThe most used metrics for classification purposes are:\nAccuracy: measures the proportion of correct predictions made by the model across all classes, offering a high-level view of performance.\nSpecificity and Sensitivity: Specificity measures the capacity to correctly detect true negatives, whereas sensitivity assesses the model's ability to correctly identify true positives.\nPrecision and Recall: are critical for tasks where the distinction between different types of errors is significant; precision measures the accuracy of positive predictions, while recall assesses how well the model identifies actual positives.\nF1 Score: combines these two metrics, giving a balanced measure of a model's precision and recall, especially useful in imbalanced datasets. The formula is $F1 = \\frac{2 \\times Precision \\times Recall}{Precision+Recall}$"}, {"title": "CIFAR-10 Dataset", "content": "For regression tasks or models predicting continuous outputs, Mean Squared Error (2.1) and Mean Absolute Error (2.2) are used to quantify the average magnitude of the errors in predictions.\nWe will focus on maximizing Accuracy as our objective function in the Bayesian Optimization Loop. For an extensive discussion of these metrics, see the work of Juan Terven, et al. [11].\nThe CIFAR-10 dataset is a widely used benchmark in the field of machine learning for evaluating image recognition algorithms. It consists of 60,000 RGB color images, each of size 32x32 pixels, categorized into 10 classes with 6,000 images per class. The classes represent everyday objects such as airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. The dataset is divided into a training set of 50,000 images and a test set of 10,000 images. Developed by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton [13]. CIFAR-10 is popular for its manageable size and complexity, making it an excellent resource for testing algorithms quickly and effectively in computer vision research.\nOur neural network is trained on this dataset with the goal of maximizing the average accuracy on the test set. Although our project focuses primarily on this metric, Ax [20] also supports multi-objective optimization and outcome constraints, allowing for consideration of metrics such as F1 score and AUC as well as class-specific accuracy, even though they are not utilized in our implementation. We will delve into these details later when discussing our Bayesian Optimization Algorithm."}, {"title": "Overview", "content": "We now explore the core optimization technique utilized in our project: Bayesian Optimization (BOpt). BOpt is a probabilistic, model-based approach designed to find the optimal solution of expensive-to-evaluate functions. It effectively handles stochastic noise in function evaluations by creating a surrogate model for the objective function and quantifying the uncertainty in that surrogate using Gaussian process regression, a Bayesian machine learning technique. This surrogate model is then used to derive an acquisition function that determines the next sampling points. BOpt is particularly suited for solving problems such as the following:\n$\\begin{cases} max \\\\x \\in A \\end{cases} f(x)$ (3.1)\nTypically, the following properties are desired:\nThe input $x \\in R^d$ should not be excessively large; typically, a dimensionality of $d \\leq 20$ is most suitable for practical applications. We will later discuss how the input dimensionality impacts the efficiency of the optimization process.\nThe feasible set A is a simple set in which it is easy to assess membership, such as a hyper-rectangle or a d-dimensional simplex. This assumption can be relaxed, but it is not within the scope of our work to cover more complex sets.\nMore importantly, f(x) lacks known special structures such as convexity or linearity, is expensive to evaluate, and is at least continuous.\nWhen we choose the next point at which f(x) will be evaluated, we only observe its value. Neither first nor second-order derivatives are used, giving it a 'derivative-free' property. Additionally, the function can be obscured by stochastic noise.\nWe will now present the most prominent and useful insights from the analysis in Peter I. Frazier's tutorial paper [14]."}, {"title": "Gaussian Process Regression", "content": "Gaussian Process (GP) Regression is a Bayesian statistical approach for modeling functions. It is based on two key concepts: the GP posterior on the objective function and the Acquisition Function."}, {"title": "GP Posterior on the Objective Function", "content": "The GP posterior provides a probabilistic framework for estimating the objective function. It combines prior beliefs with observed data to create a Gaussian posterior distribution, offering both a mean estimate and an uncertainty measure (variance). This approach allows us to incorporate prior knowledge and update our understanding as more data points are observed."}, {"title": "Acquisition Function", "content": "The acquisition function is a crucial component in GP regression. It guides the selection of the next points for evaluation by balancing exploration and exploitation. The function uses the posterior distribution to determine where to sample next, in order to find the global optimum of the objective function efficiently."}, {"title": "Mean Function and Kernel", "content": "In Gaussian Process (GP) Regression, the choice of the mean function and kernel (also known as the covariance function) is crucial as they define the properties of the functions we aim to model.\nThe mean function represents the expected value of the function we are modeling. There are several types of mean functions that can be chosen based on the prior knowledge about the function.\nConstant Mean A constant mean function, $\\mu_0(x) = \\mu$, assumes that the function has a constant mean across its domain. This is the simplest form of mean function and is often employed when there is no prior information suggesting a different mean structure, making it a common choice.\nParametric Mean Function A parametric mean function uses a parametric form to model the mean of the function, allowing the incorporation of prior knowledge about the general trend of the function. It is typically expressed as:\n$\\mu_0(x) = \\mu + \\sum_{i=1}^p \\beta_i \\psi_i(x)$\nwhere i are parametric functions, usually low-order polynomials. This approach provides flexibility in representing more complex mean structures.\nThe kernel defines the covariance between function values at different points and encodes assumptions about the function's smoothness, periodicity, and other properties. Two commonly used kernels in GP regression are the Gaussian kernel and the Mat\u00e9rn kernel.\nGaussian (RBF) Kernel The Gaussian kernel, also known as the Radial Basis Function (RBF) kernel, assumes that the function is infinitely differentiable, leading to a very smooth function. It is defined as:\n$\\Sigma_\\theta(x_i, x_j) = exp(\\frac{-\\|x_i - x_j\\|^2}{2l^2})$\nwhere l is the length-scale parameter that controls the smoothness of the function. The Gaussian kernel is widely used due to its simplicity and the strong smoothness assumption it imposes."}, {"title": "Acquisition Functions", "content": "Acquisition functions are essential in Bayesian Optimization, directing the selection of the next points for evaluating the objective function. Their evaluations are computationally inexpensive, allowing the use of optimization methods such as Gradient Descent and BFGS, which utilize first-order derivatives. In the following, we discuss some commonly used acquisition functions.\nExpected Improvement (EI) is one of the most popular acquisition functions. It builds upon the concept of improvement, which is defined as the amount by which a new observation f(x) exceeds the current best observation f(x+), if it does:\n$I(x) = max(f(x) - f(x^+), 0)$\nEI calculates the expected value of this improvement, taking into account both the mean and the uncertainty of the predictions. The El at a point x is defined as:\n$EI(x) = E[I(x)] = E[max(f(x) - f(x^+), 0)]$\nThe closed form of EI, under the common assumption that the prediction follows a Gaussian distribution, is as follows:\n$EI(x) = (\\mu(x) - f(x^+))\\Phi(\\frac{\\mu(x) - f(x^+)}{\\sigma(x)}) + \\sigma(x)\\phi(\\frac{\\mu(x) - f(x^+)}{\\sigma(x)})$\nwhere $\\mu(x)$ is the predicted mean, $\\sigma(x)$ is the predicted standard deviation, $\\Phi(.)$ is the cumulative distribution function, and $\\phi(\u00b7)$ is the probability density function of the standard normal distribution.\nBy considering the expected improvement, the EI tends to favor points with high uncertainty if the potential improvement is significant, effectively balancing exploration and exploitation.\nThe gradient of EI, can be derived as follows. Let $\\mu(x)$ and $\\sigma(x)$ be the predicted mean and standard deviation at point x, respectively. The El gradient is given by:\n$\\nabla EI(x) = \\nabla \\mu(x) ( \\frac{(f(x^+) - \\mu(x))\\phi(z) + \\sigma(x)\\Phi(z)}{\\sigma(x)})$\nwhere $\\phi(z)$ and $\\Phi(z)$ are the probability density function and the cumulative distribution function of the standard normal distribution, respectively, and $z = \\frac{f(x^+)-\\mu(x)}{\\sigma(x)}$.\nThe terms $\\nabla \\mu(x)$ and $\\nabla \\sigma(x)$ are the gradients of the predicted mean and standard deviation, respectively.\nThe gradient of EI is available in closed form when the Gaussian Process (GP) model used has a closed-form expression for the mean and covariance functions. This typically occurs when the kernel function of the GP is chosen so that its derivatives are analytically tractable. Common kernels like the Gaussian (RBF) kernel and the Mat\u00e9rn kernel with specific parameters often allow for such closed-form solutions, enabling efficient computation of the EI gradient."}, {"title": "q-Expected Improvement (q-EI) in Parallel Evaluations", "content": "In many practical scenarios, it is desirable to evaluate multiple points in parallel to speed up the optimization process. Expected Improvement can be extended to support parallel evaluations through various strategies.\nOne common approach is the use of the \"q-Expected Improvement\" (q-EI), which generalizes EI to the setting where q points are evaluated simultaneously. The q-EI is defined as the expected improvement over the best observed value, considering the joint distribution of improvements at q points. This joint consideration helps in capturing the dependencies between the evaluations, providing a more robust parallel evaluation strategy. The q-EI at a set of points $x_{1:q}$ is defined as:\nq-EI($x_{1:q}$) = E $max_{i=1,...,q} [ max(f(x_i) - f(x^+), 0)]$"}, {"title": "Probability of Improvement (PI)", "content": "Probability of Improvement (PI) focuses on the likelihood of improving over the current best observation. It is defined as:\n$PI(x) = P(f(x) > f(x^+))$\nPI tends to exploit more by selecting points with a high probability of improving the current best value, but it may overlook regions of high uncertainty, making it more suited for local search."}, {"title": "Upper Confidence Bound (UCB)", "content": "The Upper Confidence Bound (UCB) acquisition function balances exploration and exploitation by considering both the mean and the variance of the predictions. It is defined as:\nUCB(x) = $\\mu(x) + \\kappa \\sigma(x)$\nwhere \u03ba is a parameter that controls the balance between exploration and exploitation. Higher values of \u03ba encourage exploration by giving more weight to the uncertainty term.\nThe gradient of the UCB function can be derived as follows:\n$\\nabla$UCB(x) = $\\nabla \\mu(x) + \\kappa \\nabla \\sigma(x)$\nwhere $\\nabla \\mu(x)$ and $\\nabla \\sigma(x)$ are the gradients of the predicted mean and standard deviation, respectively. This gradient is useful for optimization algorithms that rely on gradient information to efficiently explore the search space."}, {"title": "Model Initialization", "content": "Our algorithm will primarily utilize UCB, and alternatively EI, as their gradients are available in closed form for sequential evaluations. Since our environment involves the task of training a neural network, which is inherently sequential, this allows us to significantly speed up the process. Additionally, we will discuss other scenarios where parallel evaluations are highly advantageous.\nAn efficient method for quasi-random sequence generation is the use of Sobol' Sequences. These sequences are utilized by Ax [20] during the initial phase of the optimization loop. In the following section, we will explain how Sobol' Sequences generate numbers."}, {"title": "Sob"}]}