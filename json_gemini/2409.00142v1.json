{"title": "Dynamic Depth Decoding: Faster Speculative Decoding for LLMs", "authors": ["Oscar Brown", "Zhengjie Wang", "Andrea Do", "Nikhil Mathew", "Cheng Yu"], "abstract": "The acceleration of Large Language Models (LLMs) with speculative decoding provides a significant runtime improvement without any loss of accuracy. Currently, EAGLE-2 is the state-of-the-art speculative decoding method, improving on EAGLE with a dynamic draft tree. We introduce Dynamic Depth Decoding (DDD), which optimises EAGLE-2's tree drafting method using a dynamic depth. This extends the average speedup that EAGLE-2 achieves over EAGLE by 44%, giving DDD an average speedup of 3.16x.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Brown et al., 2020) (Touvron et al., 2023) have demonstrated impressive performance over various tasks. However, their large number of parameters causes inference speed to be too slow for many applications.\nSpeculative Decoding (Leviathan et al., 2023) addresses this to accelerate an LLM, known as the target model. For each forward pass, the algorithm uses a much smaller draft model to generate a sequence of tokens to be inputted to the target model. Running the target model once is sufficient to verify the tokens until one is incorrect and generate the token that should follow the correct sequence. This gives a speedup by generating more tokens per forward pass of the target model. Notably, speculative decoding methods are lossless since every token is verified as correct by the target model.\nExtrapolation Algorithm for Greater Language-model Efficiency (EAGLE) (Li et al., 2024b) is the state of the art speculative decoding method, with it's key feature being the construction of a draft model using the embedding layer and LM head of the target model with a single trainable head in between. On its first release, EAGLE used a method of generating a tree of tokens from the draft model and adjusting the target model's attention mask to allow the entire tree to be inputted simultaneously into the target model. This tree has the structure shown in Figure 2, with the best tokens generated from each previous token being on the left. Although this tree chooses the tokens with the highest draft logprobs outputted after each token, its structure is static with no dependence on the draft model output.\nEAGLE-2 (Li et al., 2024a) improves on this"}, {"title": "2 Dynamic Depth Decoding", "content": "DDD is an implementation of dynamic depth with EAGLE-2. We use the sum of the probabilities of all the sequences in the beam as a heuristic with a required threshold to continue draft generation. EAGLE-2 processes the draft model output with a logsoftmax function to produce logprobs for deciding the next beam. We can thus calculate the heuristic from the logprobs as in Equation 1 for a beam width w where the sum of logprobs of each sequence in the beam is logprobsum[i].\n\n$H = log(\\sum_{i=0}^{w} exp(logprobsum[i]))$ \n\nWhile running the draft model, EAGLE-2 never uses data-dependent control flow, allowing the entire process to be lazy evaluated, providing significant optimisations. To determine whether to continue draft generation based on the probability sum, all the drafting must be immediately evaluated up to the current step. Hence, each time the heuristic is checked, there is a significant slowdown. To partially avoid this, we do not check the heuristic every step of the drafting process. Refer to Algorithm 1 for details in this method."}, {"title": "3 Experiments", "content": "3.1 Setup\nWe compare the speedup of DDD to both EAGLE and EAGLE-2 on a single NVIDIA A40 GPU. We test with temperature 0 on MT-bench. We use the same base and draft models as EAGLE-2 (Li et al., 2024a) with Vicuna-7B, Vicuna-13B, LLaMA2-Chat 7B and LLaMA2-Chat 13B. We measure the speedup ratio against vanilla autoregressive decoding. We do not measure accuracy as every speedup method we test is lossless. We also do not measure token acceptance rate, since it is always better at a greater depth and does not test the effectiveness of"}, {"title": "3.2 Parameters", "content": "All parameters are kept constant across all our experiments. We use the optimal parameters for both EAGLE and EAGLE-2 from their papers (Li et al., 2024b) (Li et al., 2024a), including the draft tree shown in Figure 2 for EAGLE and a depth of 6 for EAGLE-2. We have empirically found that DDD is optimal with a maximum of 11 draft model calls (n = 11), with heuristic checks after the 5th, 7th and 9th steps (S = {5, 7, 9}) and a minimum log-prob threshold of x = -0.3. Both EAGLE-2 and DDD are run with beam width w = 10."}, {"title": "3.3 Results", "content": "On average over Table 1, EAGLE-2 outperforms EAGLE by 8%, and DDD outperforms EAGLE-2 by 4%. However, EAGLE-2 is recorded to outperform EAGLE by 31% in the same experiments (Li et al., 2024a). We therefore hypothesize that our hardware causes the gaps in speedup between decoding algorithms is reduced. We would encourage the developers of EAGLE to evaluate DDD on their hardware to provide a comparison under the same conditions that EAGLE and EAGLE-2 were evaluated. We note that the parameters of DDD may need to be optimised for their hardware."}, {"title": "3.4 Lazy Evaluation", "content": "To determine the actual algorithmic advantage of DDD over EAGLE-2, we perform EAGLE-2's method as usual with depth 6, but we break lazy evaluation between every call to the draft model by calling torch.cuda.synchronize(). We compare this to DDD with S = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, where the heuristic is checked every step, also breaking lazy evaluation. We include this in Table 1 as \"E-2 + Strict\" and \"DDD + Strict\". We find that in this case, where lazy evaluation is broken every step, an average 5% advantage can be achieved by DDD, across all the models."}, {"title": "4 Conclusion", "content": "In this work, we introduce Dynamic Depth Decoding, an optimisation of EAGLE-2's decoding algorithm that increases the speedup of the current state-of-the-art speculative decoding method. We discover an opportunity to use the draft model's confidence to determine whether to continue drafting. Since the heuristic check breaks lazy evaluation, we find that it is optimal to check the heuristic only a few times. We also compare our decoding algorithm to EAGLE and EAGLE-2 over a variety of models. Future work on speculative decoding that significantly improves on the speedup of EAGLE-2 will most likely focus on optimising the draft model and the verification process, rather than the drafting algorithm."}, {"title": "5 Limitations", "content": "We implement DDD with a series of breaks in lazy evaluation that causes a slowdown. Discovery of a way to perform an algorithm similar to DDD without the losses from breaking lazy evaluation would theoretically provide a significant advantage. Also, the results we observe on our hardware is significantly different to the published results of EAGLE-2 (Li et al., 2024a). Our model would be more easily compared with other methods if tested on their hardware."}]}