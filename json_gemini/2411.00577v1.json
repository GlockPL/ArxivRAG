{"title": "WLPlan: Relational Features for Symbolic Planning", "authors": ["Dillon Z. Chen"], "abstract": "Scalable learning for planning research generally involves juggling between different programming languages for handling learning and planning modules effectively. Interpreted languages such as Python are commonly used for learning routines due to their ease of use and the abundance of highly maintained learning libraries they exhibit, while compiled languages such as C++ are used for planning routines due to their optimised resource usage. Motivated by the need for tools for developing scalable learning planners, we introduce WLPlan, a C++ package with Python bindings which implements recent promising work for automatically generating relational features of planning tasks. Such features can be used for any downstream routine, such as learning domain control knowledge or probing and understanding planning tasks. More specifically, WLPlan provides functionality for (1) transforming planning tasks into graphs, and (2) embedding planning graphs into feature vectors via graph kernels. The source code and instructions for the installation and usage of WLPlan are available at tinyurl.com/42kymswc", "sections": [{"title": "Introduction", "content": "Learning to plan has regained significant interest in recent years due to advancements of machine learning (ML) approaches across various fields. An aim of learning to plan involves designing automated, domain-independent algorithms for learning domain knowledge from small training problems used for scaling up planning to problems of very large sizes (Toyer et al. 2018, 2020; Dong et al. 2019; Shen, Trevizan, and Thi\u00e9baux 2020; Karia and Srivastava 2021; St\u00e5hlberg, Bonet, and Geffner 2022; Mao et al. 2023; Chen, Thi\u00e9baux, and Trevizan 2024). A recent state-of-the-art approach involves learning heuristic functions from relational features automatically extracted from graph representations of planning tasks for both classical (Chen, Trevizan, and Thi\u00e9baux 2024) and numeric (Chen and Thi\u00e9baux 2024a) planning. The approach involves (1) transforming planning tasks into graphs, and (2) embedding such graphs into feature vectors. Its performance can be attributed to its evaluation speed and expressive power over previous methods.\nWe introduce the WLPlan package which implements various transformations of planning tasks into graphs, and graph kernels for embedding such graphs. Figure 1 illustrates these routines. The underlying algorithms are implemented in C++ for optimised runtime and memory usage, and Python bindings are provided for easy prototyping and training of ML models. Most learning for planning architectures are either written in Python and heavily unoptimised or conversely written entirely in C++ and difficult to extend and prototype with. Furthermore, WLPlan can be extended to support new graph representations of planning tasks and graph kernels. This paper serves several purposes:\n(1) We provide the general algorithm, functionalities and design principles of the WLPlan package for streamlining learning for planning research.\n(2) We describe currently implemented graph representations and graph kernels in WLPlan, some of which have not been published in the papers it was built upon.\n(3) We outline various practical uses of WLPlan, including data visualisation, distinguishability testing, and learning heuristic functions, with experiments and analysis.\nThe structure of the remainder of the paper is as follows. Sec. 2 highlights the features and design choices of the current version of WLPlan, summarised in Figures 1 and 2. Sec. 3 provides the necessary technical background required for the remainder of the paper. Sec. 4 describes the currently implemented graph representations of planning tasks in the WLPlan package. Sec. 5 presents the currently implemented graph kernels for converting graphs into feature vectors. Sec. 6 illustrates various uses of WLPlan and experiments in learning for planning."}, {"title": "2 WLPlan", "content": "State-of-the-art ML techniques for planning primarily consist of graph learning approaches due to their ability to handle relational information in planning tasks, as well as their ability to operate on arbitrary task sizes. Such approaches summarily follow a three step process:\n(a) Convert a planning task to a graph.\n(b) Pass the graph through a graph learning model.\n(c) Learn some form of domain knowledge for planning.\nDespite this simple three step process, performing research in the field is arduous due to the burden of implementing both learning and planning modules. Both modules are inherently nontrivial to handle concurrently as researchers have to balance code optimisation, extensibility, and communication across modules. For example, Python is the go-to programming language for ML research due to its versatility and the existence of large collection of open-source ML libraries accessible via Python (Raschka, Patterson, and Nolet 2020; GitHub 2022), but planning in Python is slow and inefficient. Conversely, planners are implemented in C++ for optimised runtime and memory usage, but support for and implementation of ML models in C++ is limited and difficult. Lastly, models trained in Python have to be serialised and compatible with C++ code.\nWLPlan bridges this gap by providing a C++ package with Python bindings for automatically generating feature vectors of planning tasks. Feature vectors are easy to handle across programming languages as well as learning and planning modules as they are lightweight and agnostic to any downstream task. In contrast, deep learning models require more implementation effort across various modules and are restricted to the task they are designed for. Recent work by Chen, Trevizan, and Thi\u00e9baux (2024) also showed that for (b), it is sufficient to restrict our attention to classical ML approaches involving feature generators compared to deep learning approaches such as graph neural networks (GNNs), as the former are superior to the latter across various learning and planning metrics. WLPlan exploits this observation and focuses on the construction and design of cheap ML models that can be implemented easily in both learning and planning modules."}, {"title": "2.1 What WLPlan brings to the table", "content": "We highlight the core functionalities WLPlan aims to offer for both learning and planning, complemented with example code snippets in both Python and C++. It handles the feature generation and serialisation aspects of a typical learning for planning pipeline as illustrated in Figure 2.\nGraph representations of planning tasks WLPlan includes implementations of graph transformations of planning tasks with a simple graph interface that can be accessed from both C++ and Python. WLPlan graphs exhibit the base graph structure of nodes and edges, as well as node and edge features. WLPlan can also be extended to support new graph representations of planning tasks."}, {"title": "2.2 What WLPlan does not aim to support", "content": "Conversely, we also highlight the functionalities WLPlan does not aim to support and provide.\nData extraction Surveys and studies (Press 2016; Roh, Heo, and Whang 2021) have shown that in the context of machine learning routines, a significant amount of time (greater than 50%) is spent on collecting and organising datasets for training. WLPlan only focuses on the modelling side of learning routines and thus is not concerned with data collection and organisation. Nevertheless, WLPlan makes use of the pddl (Favorito, Fuggitti, and Muise 2024) package for automatically parsing planning tasks and converting them into a format fit for the WLPlan interface.\nLearning To keep WLPlan simple in terms of dependencies and functionality, the package does not provide any learning algorithms. We note that vectors and graphs generated from WLPlan can be used with any downstream classical ML and deep learning model, for which there exist plenty of highly maintained, optimised and open-source libraries.\nPlanning Similarly, WLPlan also does not provide the functionalities required for a complete planner, such as a successor generator or search algorithms. Various planning libraries and systems already exist, with which WLPlan can be integrated for a complete planning system.\nIndeed, we refer the reader to the GOOSE planner for a complete learning for planning system which combines WLPlan and other learning and planning components."}, {"title": "3 Background", "content": "This section provides the formal definitions required for understanding the algorithms and technical details of WLPlan. For the sake of brevity, we only focus on deterministic planning representations, but note that WLPlan is state-centric, meaning that it is agnostic to the transition model, and thus can also handle probabilistic or non-deterministic actions."}, {"title": "3.1 Planning Task", "content": "Let $[n]$ denote the set of integers $\\{1,...,n\\}$. A planning task can be understood as a state transition model (Geffner and Bonet 2013) given by a tuple $\\Pi = (S, A, S_0, G)$ where $S$ is a set of states, $A$ a set of actions, $s_0 \\in S$ an initial state, and $G \\subset S$ a set of goal states. Each action $a \\in A$ is a function $a: S \\rightarrow S \\cup {\\bot}$ where $a(s) = \\bot$ if $a$ is not applicable in $s$, and $a(s) \\in S$ is the successor state when $a$ is applied to $s$. A solution for a planning task is a plan: a sequence of actions $\\pi = a_1, ..., a_n$ where $s_i = a_i(s_{i-1}) \\neq \\bot$ for $i\\in [n]$ and $s_n \\in G$. A state $s$ in a planning task $\\Pi$ induces a new planning task $\\Pi' = (S, A, s, G)$. A planning task is solvable if there exists at least one plan."}, {"title": "3.2 Numeric Planning", "content": "Numeric planning, formalised in PDDL2.1 (Fox and Long 2003), is a compact, lifted representation of a planning task using predicate logic and relational numeric variables. More specifically, a numeric planning task is a tuple $\\Pi = (\\mathcal{O}, \\Sigma_p, \\Sigma_f, A, s_0, G)$, where $\\mathcal{O}$ denotes a set of objects, $\\Sigma_p/\\Sigma_f$ a set of predicate/function symbols, $A$ a set of action schemata, $s_0$ the initial state, and $G$ now the goal condition. As mentioned above, understanding of the transition system induced by $A$ is not required, so we instead focus on the representation of states and the goal condition next.\nStates Each symbol $\\sigma \\in \\Sigma_p \\cup \\Sigma_f$ is associated with an arity $ar(\\sigma) \\in \\mathbb{N} \\cup \\{0\\}$. Predicates and functions take the form $p(x_1,...,x_{ar(p)})$ and $f(x_1,..., x_{ar(f)})$ respectively, where the $x_i$s denote their arguments. Propositional and numeric variables are defined by substituting objects into predicate and function variables. More specifically, given $\\sigma \\in \\Sigma_p \\cup \\Sigma_f$, and a tuple of objects $o = (o_1,..., o_{ar(\\sigma)})$, we denote $\\sigma(o)$ as variable defined by substituting $o$ into arguments of $o$. A state $s$ is an assignment of values in $\\{T, \\bot\\}$ (resp. $\\mathbb{R}$) to all possible propositional (resp. numeric) variables in a state. Following the closed world assumption, we can equivalently represent a state as a set of true propositions and numeric assignments. Let $X_p(s)$ denote the set of propositional variables that are true in $s$, $X_n(s)$ the set of numeric variables, and $X(s) = X_p(s) \\cup X_n(s)$.\nGoal Condition A propositional condition is a positive (resp. negative) literal $x = T$ (resp. $x = \\bot$) where $x$ is a propositional variable. A numeric condition has the form $\\xi \\triangleright 0$ where $\\xi$ is an arithmetic expression over numeric variables and $\\triangleright \\in \\{>, \\geq, =\\}$. We let $[x]_s$ (resp. $[\\$\\xi]_s$) denote the value of a numeric variable $x$ (resp. expression $\\xi$) in a state $s$, and $V(g)$ for the set of numeric state variables in $\\xi$. The goal condition $G$ is a set of propositional and numeric conditions which we denote $G_p$ and $G_n$, respectively. A state $s$ satisfies the goal condition $G$ if $s$ satisfies all its conditions.\nDomain A planning domain is a set of planning tasks which share the same set of predicates, functions and action schemata. Constant objects are named objects that occur in all planning tasks in a domain.\nExample As a running example, we consider the Capacity Constrained Blocksworld (ccBlocksworld) domain by Chen and Thi\u00e9baux (2024a). ccBlocksworld is a numeric extension of Blocksworld where there are a finite number of bases on which blocks can be stacked, and each base has a capacity constraint on the number of blocks that can be stacked on it. The objective of ccBlocksworld is to rearrange blocks from an initial to goal tower configuration. The left image of Figure 1 illustrates a numeric ccBlocksworld planning task where there are 3 bases each with a capacity constraint of 3. In the 4-operations encoding of ccBlocksworld, an optimal plan requires 16 actions, while only 10 actions if the bases have no capacity constraints."}, {"title": "3.3 Graphs", "content": "We denote a graph with categorical and continuous node features and edge labels by a tuple $G = (V, E, F_{cat}, F_{con}, L)$."}, {"title": "4 Graph Representation", "content": "The first component of the WLPlan package involves the transformation of planning tasks into graphs. Graphs with edge features are viewed as 'relational structures' in other communities, from which we can derive relational features with various sorts of algorithms. In this section, we describe the numeric Instance Learning Graph (vILG) for representing numeric planning tasks (Chen and Thi\u00e9baux 2024a), which generalises the Instance Learning Graph (ILG) for classical planning tasks (Chen, Trevizan, and Thi\u00e9baux 2024). The ILG is the primary graph in the WLPlan package but the package can be extended to support arbitrary graph representations of planning tasks, such as those in the literature (Toyer et al. 2018; Shen, Trevizan, and Thi\u00e9baux 2020; Silver et al. 2021; St\u00e5hlberg, Bonet, and Geffner 2022; Chen, Trevizan 2024; Hor\u010d\u00edk and \u0160\u00edr 2024).\nThe middle image in Figure 1 illustrates a subgraph of the vILG encoding of the numeric ccBlocksworld planning task in the left image. Nodes of the vILG represent the objects (blue), goal conditions (yellow), and state information of the planning task, with colours encoding the semantics of nodes. From the closed world assumption, only true propositional variables (green) are encoded as nodes, alongside all numeric variables (red). Edges connect objects to variables that are predicates or functions instantiated with the object, as well as numeric goal conditions to their numeric variables. Edge labels encode the location of predicates and functions in which objects are instantiated. In the image, blue/orange edges connect variable nodes to the object that is instantiated in the first/second argument.\nDefinition 4.1. The numeric Instance Learning Graph (vILG) of a numeric planning task $\\Pi = (\\mathcal{O}, \\Sigma_p, \\Sigma_f, A, s_0, G)$ is a graph with categorical and continuous node features and edges labels $G = (V, E, F_{cat}, F_{con}, L)$ with\n$\\bullet$ nodes $V = \\mathcal{O} \\cup X(s_0) \\cup G$ where we assume w.l.o.g. that $V(g) \\subset X(s_0)$ for all $g \\in G_n$.\n$\\bullet$ edges $E = \\bigcup_{\\sigma(o) \\in X(s_0) \\cup G} \\{(p, o_i) \\mid i \\in [ar(\\sigma)]\\} \\cup \\bigcup_{\\xi \\triangleright 0 \\in G_n} \\{(\\xi, v) \\mid v \\in V(\\xi)\\}$.\n$\\bullet$ categorical node features $F_{cat} : V \\rightarrow \\Sigma_V$ defined by\n$\\begin{aligned} F_{cat}(u) = \\begin{cases} objt(u) & \\text{if } u \\in \\mathcal{O} \\\\ (pred(u), apg) & \\text{if } u \\in X_p(s_0) \\cap G_p \\\\ (pred(u), upg) & \\text{if } u \\in G_p \\setminus X_p(s_0) \\\\ (pred(u), apn) & \\text{if } u \\in X_p(s_0) \\setminus G_p \\\\ func(u) & \\text{if } u \\in X_n(s_0) \\\\ (comp(u), achv(u)) & \\text{if } u \\in G_n \\end{cases} \\end{aligned}$\nwhere $objt(u) = u$ if $u$ is a constant object and object otherwise, $pred(u)/func(u)$ denote the predicate/function symbol of a propositional/numeric variable $u$, $comp(u) \\in \\{\\geq,>,=\\}$ encodes the comparator type of the numeric goal condition $u$, and $achv(u) \\in \\{ung, ang\\}$ encodes whether $s_0$ satisfies $u$. We note that object, alongside $apg, upg, apn, ang, ung$ are constant categorical node features.\n$\\bullet$ continuous node features $F_{con} : V \\rightarrow \\mathbb{R}$ defined by\n$\\begin{aligned} F_{con}(u) = \\begin{cases} [u]_{s_0} & \\text{if } u \\in X_n (s_0) \\\\ [\\xi]_{s_0} & \\text{if } u = (\\xi \\triangleright 0) \\in G_n, \\xi \\triangleright 0 \\\\ 0 & \\text{otherwise} \\end{cases} \\end{aligned}$\n$\\bullet$ edge labels $L : E \\rightarrow \\mathbb{N} \\cup \\{0\\}$ defined by\n$\\begin{aligned} L(e) = \\begin{cases} i & \\text{if } e = (p, o_i) \\text{ for } p = \\sigma(o) \\in X (s_0) \\cup G_p \\\\ 0 & \\text{otherwise, i.e. } e = (\\xi, v) \\text{ for } (\\xi > 0) \\in G_n \\end{cases} \\end{aligned}$\nIn general, the number of categorical node features for a domain with predicates $\\Sigma_p$ and functions $\\Sigma_f$ is $|\\Sigma_V| = 1+ |constant\\_objects|+3|\\Sigma_p|+|\\Sigma_f|+ 4$. Continuous node features indicate the value of numeric variables in the state and the error in the expression of numeric goals if it has not been achieved in $s_0$, and are set to zero for all other nodes."}, {"title": "5 Feature Generation", "content": "The second component of the WLPlan package involves transforming graph representations of planning tasks into feature vectors for use with any downstream task. The go-to example is learning heuristic functions, although it is also possible to perform all sorts of other tasks as we will discuss later in Section 6.\nThe algorithms for feature generation of graphs are generally some extension of the colour refinement algorithm, a special case of the general k-Weisfeiler-Leman algorithm (Weisfeiler and Leman 1968; Cai, F\u00fcrer, and Immerman 1989). In this section, we begin with describing the colour refinement, or 1-Weisfeiler-Leman (WL) algorithm, followed by how the algorithm is used for constructing feature vectors of graphs. Lastly, we outline extensions of WL that been have implemented thus far in WLPlan."}, {"title": "5.1 The WL Algorithm", "content": "The underlying concept of the WL algorithm is to iteratively update node colours based on the colours of their neighbours. The original WL algorithm was designed for graphs without edge labels. We present the WL algorithm which can support edge labels (Barcel\u00f3 et al. 2022) in Algorithm 1. The algorithm's input is a graph with node features and edge labels as described in Section 3.3, alongside a hyperparameter $L$ determining how many WL iterations to perform. The output of the algorithm is a canonical form for the graph that is invariant to node orderings.\nLine 1 initialises node graph colours as their categorical node features. Lines 2 and 3 iteratively update the colour of each node $v$ in the graph by collecting all its neighbours and the corresponding edge label $(u, l)$ into a multiset. This multiset is then hashed alongside $v$'s current colour with an injective function to produce a new refined colour. In practice, the injective hash function is built lazily, where every time a new multiset is encountered, it is mapped to a new, unseen hash value. After $L$ iterations, the multiset of all node colours seen throughout the algorithm is returned."}, {"title": "5.2 Feature Vectors from WL", "content": "The WL algorithm has been used to generate features for the WL graph kernel (Shervashidze et al. 2011). Each node colour constitutes a feature, and the value of a feature for a graph is the count of the number of nodes that exhibit or has exhibited the colour. Then given a set of colours $C$ known a priori, the WL algorithm can return a fixed sized feature vector of size $|C|$ for every input graph. We first describe how to collect the colours $C$ from a set of given planning tasks, followed by how to embed arbitrary graphs into fixed sized feature vectors from such colours.\nCollecting colours We can construct $C$ from a given set of graph representations of planning tasks $G_1, ..., G_m$ by running the WL algorithm, with the same HASH function and number of iterations $L$, on all of them and then taking the set union of all multiset outputs, i.e. $C = \\bigcup_{i \\in [m]} WL(G_i)$.\nEmbedding graphs Now suppose we have collected a set of colours and enumerated them by $C = \\{c_1,..., c_{|C|} \\}$. Then given a graph $G$ and its multiset output from the WL algorithm $M$, we can define an embedding of the graph into Euclidean space by the feature vector\n$\\begin{aligned} [COUNT(M, c_1), ..., COUNT(M, c_{|C|})] \\in \\mathbb{R}^{|C|}, \\end{aligned}$\nwhere $COUNT(M, c_i)$ is an integer which counts the occurrence of the colour $c_i$ in $M$. We note importantly that any colours returned in $M$ that are not in $C$ are defined as unseen colours and are entirely ignored in the output."}, {"title": "5.3 WL Extensions", "content": "The WL algorithm is the canonical graph kernel baseline for graph learning tasks due to the theoretical result that it upper bounds distinguishing power of the message passing GNN architecture (Morris et al. 2019; Xu et al. 2019). It is also an efficient algorithm that runs in low polynomial time in the input graph and considered the first choice to apply to graphs as described in the extensive graph kernel survey by Kriege, Johansson, and Morris (2020). Nevertheless, the graph learning community has proposed various extensions of the WL algorithm and corresponding GNN architectures that have provably more distinguishing power than the WL algorithm, and yet are still computationally feasible.\nMost notably, it is well known that the $(k + 1)$-WL algorithm is strictly more powerful than the $k$-WL algorithm for $k \\geq 2$ but its runtime scales exponentially in $k$. Thus, many extensions of the WL algorithm and corresponding GNNs have been proposed to either approximate or achieve orthogonal expressiveness of higher order WL algorithms (Morris, Rattan, and Mutzel 2020; Morris et al. 2022; Zhao, Shah, and Akoglu 2022; Wang et al. 2023; Alvarez-Gonzalez, Kaltenbrunner, and G\u00f3mez 2024). Furthermore, graph kernels have been proposed that also handle graphs with continuous node attributes (Chen and Thi\u00e9baux 2024a). We have implemented some of these WL extensions alongside completely new graph kernels in the current version of WLPlan which we outline as follows. Figure 3 illustrates the expressivity hierarchy of mentioned WL algorithms.\n2-WL The k-WL algorithms are a suite of incomplete graph isomorphism algorithms which have a one-to-one correspondence to k-variable counting logics (Cai, F\u00fcrer, and Immerman 1989). However, the k-WL algorithms scale exponentially in k, with the 2-WL algorithm exhausting memory limits on medium sized graph datasets. We describe and implement the 2-WL algorithm and refer to (Cai, F\u00fcrer, and Immerman 1989, Section 5) for the general k-WL algorithm. The idea of the 2-WL algorithm, outlined in Algorithm 2, is to now assign and refine colours to ordered pairs of nodes.\nThe algorithm begins in Lines 1-3 by assigning all possible ordered pairs of nodes a tuple of the node colours as well as the edge label between them. If there is no edge between a pair of nodes, a special $|$ value is used as the edge label. Lines 4-5 then iteratively refine the colour of each node pair by defining the neighbour of a pair $(v, u)$ to be a set of tuple of pairs $((w, u), (v,w))$ where $w$ ranges over all nodes in the graph. Then the algorithm applies the colouring function of the current iteration to all node pairs to create a multiset of tuples of colours which are then hashed alongside the current node pair's colour. Finally, the algorithm returns the multiset of all node pair colours seen throughout the algorithm in Line 6. To generalise to the k-WL algorithm, one replaces node pairs with node k-tuples.\n2-LWL The k-LWL algorithms (Morris, Kersting, and Mutzel 2017) provide efficient approximations of the k-WL algorithms but still have the same worst case computational complexity. The main approximations made are that node tuples are converted to node sets, reducing the number of possible node tuples to consider per iteration by a constant factor $(n^k \\rightarrow (\\substack{n\\\\ k}))$, and relaxing the definition of neighbours of node tuples. Now the neighbour for a 2-sets of nodes $\\{v, u\\}$ in 2-LWL is defined by the set of set of 2-sets $\\{\\{w, u\\}, \\{v, w\\}\\}$ where $w$ now ranges over the union of neighbours of $u$ and $v$, instead of over all nodes. Algorithm 3 outlines the 2-LWL algorithm and Figure 3 illustrates the different neighbour definitions of 2-WL and 2-LWL.\niWL We introduce an expressive WL algorithm extension inspired by Identity-aware GNNs (ID-GNNs) (You et al. 2021), orthogonal to the k-WL algorithms. ID-GNNs run a GNN $|V|$ times on a graph which uses different parameters for embedding updates on a selected, individualised node on each GNN run. We kernelise the ID-GNN algorithm into what we call the individualised WL algorithm, presented in Algorithm 4 and Figure 3.\nWe have a single outer loop iterating over all nodes $w \\in V$ in a graph in Line 1, and within each inner loop, all nodes are assigned their initial node colour, except for $w$ which is assigned a special, individualised colour that is not in $\\Sigma_V$ of $F_{cat}$. The remainder of the algorithm is equivalent to the WL algorithm, except that iWL now returns $|V|$ more colours in the output multiset due to the outer loop.\nccWL Lastly, we outline a WL algorithm which can handle both categorical and continuous node features, as all WL algorithms covered thus far only handle categorical node features. This is the ccWL algorithm used for numeric planning introduced by Chen and Thi\u00e9baux (2024a). Although there exist other WL algorithms that have been proposed to handle continuous node features (Morris et al. 2016; Togninalli et al. 2019), they lose the semantics of numbers required for reasoning in numeric planning. Also differently to the aforementioned WL algorithms, the ccWL algorithm returns both a multiset of colours and a function $F_{clr}$ which maps colours to continuous features. The collection of output continuous features can then be concatenated to the embeddings of multisets described in Equation (1).\nThe algorithm is outlined in Algorithm 5 and is equiv-"}, {"title": "6 Example Uses", "content": "In this section, we outline various ways of using WLPlan with corresponding experiments and results."}, {"title": "6.1 Data Visualisation", "content": "Several theoretical frameworks have been developed to better understand the behaviour of planning domains such as novelty width (Lipovetzky and Geffner 2012), correlation complexity (Seipp et al. 2016), the river measure (Dold and Helmert 2024b), and methods to bound such measures (Dold and Helmert 2024a). Automatic tools include Hoffmann's (2011) Torchlight for testing the difficulty of planning domains based on the existence of local minima in the search space with respect to $h^+$ (Hoffmann 2005), and hand-crafted planning features for constructing planning portfolios (Ferber and Seipp 2022) that can be analysed to understand what features of a planning task are well suited for a specific planner. In light of tools for probing and understanding planning domains without any knowledge of them a priori, we propose using Principal Component Analysis (PCA) visualisations of WLPlan embeddings to understand certain structures of planning tasks.\nSetup PCA is a dimensionality reduction technique that projects high-dimensional data into lower-dimensional space through maximising the variance of projected features, see (Bishop 2007, Sec. 12.1) for more details. Thus, we can use PCA to visualise WL embeddings of planning states and the distribution of $h^*$ values. Given a set of $n$ planning states from optimal training plan traces of the Learning Track of the 2023 International Planning Competition (IPC23LT) (Taitler et al. 2024), we collect all colours and embed the states into Euclidean space. The embedded states can be represented by a matrix $X \\in \\mathbb{R}^{n \\times d}$ with rows corresponding to states and columns WL features. We can then run PCA on $X$ to get a projected matrix $X' \\in \\mathbb{R}^{n \\times 2}$, where columns correspond to the two principal components. Figure 4 illustrates $X'$ for each domain in the IPC23LT, colouring each point by the state's $h^*$ value.\nResults Visualising PCA projections of WL embeddings can give us an idea on the relationship between planning features and $h^*$ values, as well as the informativeness of the training set. Indeed, we see that some domains appear to convey a linear relationship with the $h^*$ values such as in Blocksworld, Ferry, and Miconic, which correlates with the high performance of learned linear heuristics on these domains (Chen, Trevizan, and Thi\u00e9baux 2024). Conversely, we note that projected embeddings on some domains exhibit no clear pattern such as in Rovers and Sokoban which suggests that the features may not be informative enough to learn a good heuristic function. The sparsity of points in these domains as well as in Childsnack may also suggest the need for more training data. For example the training set for Childsnack consists of at most one child allergic to gluten, whereas this number if arbitrary in the testing set."}, {"title": "6.2 Distinguishability Tests", "content": "In line with tools for probing and understanding planning tasks, we make use of distinguishability tests that are commonly performed in ML literature (Kriege, Johansson, and Morris 2020; Abboud et al. 2021; Balcilar et al. 2021; Feng et al. 2022; Zhao et al. 2022; Wang et al. 2023; Bouritsas et al. 2023; Hor\u010d\u00edk and \u0160\u00edr 2024; Drexler et al. 2024) for testing whether GNN architectures and WL algorithms are expressive enough to distinguish various graph representations of planning tasks with different $h^*$ values. The WLPlan package allows for quick implementations of such setups by making use of the theoretical fact that WL algorithms subsume GNN architectures for graph isomorphism testing (Morris et al. 2019; Xu et al. 2019).\nSetup To perform a distinguishability test, we collect all colours in a given set of $n$ planning states from optimal plan traces of training tasks for each IPC23LT domain, with which we use to embed all such states into Euclidean space. Next, we check how many of the $(\\substack{n\\\\ 2})$ pairs of vectors are distinguished by observing whether their outputs are different. The metric used to evaluate the tests is to count how many pairs of graphs are not distinguished by a model, with 0 being the best achievable score. We perform experiments with the WL, 2-LWL and iWL algorithm and $L = 2$. We do not display results for 2-WL as it uses significantly more memory compared to other considered WL algorithms.\nResults From the results in Table 1, we note that the WL algorithm is able to distinguish all pairs of training states for 7 out of 10 domains. The 2-LWL algorithm ran out of memory for 4 out of 10 domains while collecting colours, and the iWL algorithm on 1 domain. Despite using significantly more computation, we note that the more expressive WL algorithms are able to distinguish more pairs of training states on one out of 10 domains they were able to terminate on."}, {"title": "6.3 Learning Heuristic Functions", "content": "The most practical usage of learning for planning is to actually use learning to plan. Our last set of experiments involve reimplementations and reproducibility checking of work by the author with WLPlan. Specifically, we use WLPlan to reimplement the learning and usage of heuristic functions for classical planning in (Chen, Trevizan, and Thi\u00e9baux 2024) and the submission to NeurIPS-24 before acceptance for numeric planning (Chen and Thi\u00e9baux 2024a).\nSetup For the interest of space, we refer to the original papers for full setup details of the experiments, and only summarily highlight the results of the original papers and of the WLPlan reimplementations here. Table 2 presents results for both classical and numeric encodings of planning domains from the IPC23LT. $WL_{ICAPS-24}$ and $GNN_{ICAPS-24}$ refer to the original implementations of learned WL and GNN heuristics with GBFS and $WL_{WLPlan}$ refers to the WLPlan reimplementation. Planner baselines include the hFF heuristic with GBFS, and LAMA.\nNumeric encodings of IPC23LT domains are either equivalent to the original domain's semantics, or are made more difficult with additional numeric constraints. Floortile and Sokoban were omitted as there was no obvious benefit or extension to the domain that can be derived with a numeric encoding. $ccWL_{NeurIPS-24-a}$ refers to the implementation submitted to NeurIPS-24 before acceptance, and $ccWL_{WLPlan}$ the WLPlan reimplementation. Planner baselines shown are $h_{mrp}$ with GBFS (Scala et al. 2020) and the state-of-the-art numeric planner M(3h || 3n) (Chen and Thi\u00e9baux 2024"}]}