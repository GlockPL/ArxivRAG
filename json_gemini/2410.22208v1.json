{"title": "Drone Acoustic Analysis for Predicting Psychoacoustic Annoyance via Artificial Neural Networks", "authors": ["Vaiuso Andrea", "Righi Marcello", "Coretti Oier", "Apicella Moreno"], "abstract": "Unmanned Aerial Vehicles (UAVs) have become widely used in various fields and industrial\napplications thanks to their low operational cost, compact size and wide accessibility. However,\nthe noise generated by drone propellers has emerged as a significant concern. This may affect\nthe public willingness to implement these vehicles in services that require operation in proximity\nto residential areas. The standard approaches to address this challenge include sound pressure\nmeasurements and noise characteristic analyses. The integration of Artificial Intelligence models\nin recent years has further streamlined the process by enhancing complex feature detection\nin drone acoustics data. This study builds upon prior research by examining the efficacy of\nvarious Deep Learning models in predicting Psychoacoustic Annoyance, an effective index for\nmeasuring perceived annoyance by human ears, based on multiple drone characteristics as\ninput. This is accomplished by constructing a training dataset using precise measurements\nof various drone models with multiple microphones and analyzing flight data, maneuvers,\ndrone physical characteristics, and perceived annoyance under realistic conditions. The aim of\nthis research is to improve our understanding of drone noise, aid in the development of noise\nreduction techniques, and encourage the acceptance of drone usage on public spaces.", "sections": [{"title": "I. Introduction", "content": "Unmanned Aerial Vehicles (UAVs), commonly known as \"drones\", have emerged as versatile tools with a wide\nrange of applications in various sectors, including transportation, communication, agriculture, disaster management,\nand environmental conservation [1]. Moreover, in industrial applications, UAVs perform vital monitoring tasks, such as\nStructural Health Monitoring (SHM) for hard-to-reach locations [2]. Moreover, in the field of delivery services, UAVs\nare proposed as a more environmentally friendly alternative, potentially reducing greenhouse gas emissions and other\nenvironmental impacts [3].\nDespite their numerous advantages, the rapid adoption of drones for these applications faces challenges related to\npublic acceptance. Safety concerns and disruptions to personal environments, particularly noise pollution, are among\nthe primary issues. Recent studies have demonstrated that people find drone noise more disturbing than noise from\nconventional ground vehicles and full-size aircraft [4]. This noise aversion extends to wildlife as well [5]. This increased\nannoyance can be attributed to the specific aerodynamic characteristics of drones, such as blade and body design, rotor\nspacing, blade passing frequency (BPS), and all the various interactions between the rotors and the drone structure.\nSmall UAVs using all-electric propulsion are expected to be quieter than traditional aircraft, but as they operate closer to\nthe public, noise annoyance becomes a concern. Legislators and stakeholders require efficient tools to predict and assess\ndrone noise levels without relying solely on field testing, which can be inconvenient and lacks preemptive assessment\ncapabilities. This has led to the development of effective tools for predicting drone noise exposure in complex urban and\nextra-urban settings."}, {"title": "A. Featured works and goal", "content": "Efforts have been made to measure and understand the noise generated by drones, including studies that involve\nsound pressure measurements and the characterization of noise emission patterns. These studies have contributed to our\nknowledge of drone noise in various scenarios and have laid the foundation for predicting and managing noise exposure.\nSinibaldi et al. [6] explored ways to optimize rotor design to reduce noise impact. However, this study primarily focused\non propeller noise, without considering the aerodynamic interactions within a full drone structure. Cabell, et al. [7]\ndocumented sound pressure levels throughout drone flights, offering an angle-dependent emission model. Alexander\net. al. [8, 9] meticulously documented sound pressure measurements during drone operations, whether hovering or\nnavigating at low speeds above grassy terrains. Their studies involved the strategic placement of microphones on\nacoustically rigid ground plates. In parallel, Besnea [10] harnessed the power of microphone arrays for pinpointing the\nsources of acoustic emissions, yet encountered hurdles in discerning source strength. Furthermore, recent research by\nGallo et al. [11] aims to correlate several Sound Quality Metrics (SQM) with perceived annoyance (PA) by analyzing\ndrone flight maneuvers. By using multiple microphones in both an anechoic chamber and real-world settings, this work\nbuilds upon previous studies by Torija et al. [12], which compared SQM and PA between drones, airplanes, and ground\nvehicles, highlighting the specific frequency characteristics that make drone noise more bothersome.\nArtificial Intelligence (AI), and in particular Deep Neural Networks (DNN), has been used in recent years to predict\nand analyze complex data related to drone sound characteristics. Its remarkable capabilities have been showcased in\nvarious tasks, including fault diagnosis [13, 14] and drone presence detection [15]. The multifaceted nature of drone\noperations demands a sophisticated understanding of their interactions with the environment, and AI algorithms excel in\ndiscerning intricate patterns within vast datasets, enabling enhanced insights into the dynamics of drone behavior.\nOur study is based on the approach in [11] and [12], extending previous researches by exploring correlations between\ndifferent drone's specifications, flight data, and maneuvers performed, and attempting to predict noise annoyance factors\nunder realistic conditions. In our study, Psychoacoustic Annoyance (PA) is assessed by using four different AI-based and\nclassic regression models, whose performance is compared and evaluated. The objective of this study is to demonstrate\nthe applicability of this type of models in solving problems involving acoustics and psychoacoustics. This approach aims\nto improve our understanding of drone noise and its impacts using an AI-based model, facilitating the development of\nnoise mitigation strategies and improving public acceptance. The section II of this paper aims to examine the methods\nused to collect and process audio data. Subsequently, the section III will deal with the analysis of the methodology used\nfor collecting data, the calculation of Sound Quality Metrics (SQM) and Perceived Annoyance (PA), and the comparison\nof these metrics with variations in the drones, microphones, and maneuvers used. A correlation map is presented and\nanalyzed to better understand which variables influence each other. Finally, section IV, we examine the methodology\nused for the implementation of four different regression models: Linear, Support Vector Machine (SVM), Deep Neural\nNetwork (DNN), and Convolutional Neural Network (CNN). Our analysis shows that DNN is the most effective model"}, {"title": "II. Data Collection Methodology", "content": "The process of measuring drones is far from being standardized. This lack of uniformity becomes even more\npronounced when we focus on the acoustic measurement of drones during maneuvers. The ISO 5305 standard [16] has\nserved as a valuable reference during the creation of this methodology for measuring drone noise. The primary goal\nin adhering to these initial recommendations from ISO is to ensure that the results obtained can be as comparable as\npossible. Consequently, it becomes necessary to establish a custom-defined procedure to conduct these measurements.\nThe conceptual definition of the Methodology involve four main aspects: The selection of the drones, the definition of\nmanoeuvres, the design of the microphone setup (or microphone array) and the definition of a measuring procedure.\nAll the measurements were carried out in the Riedbachweg flight area in Winterthur, Switzerland, a quiet location\nwhere drone sounds could be recorded with relatively minimal background noise. The choice to record in an open space,\nrather than an anechoic chamber, was deliberate to facilitate the examination of emission, propagation, and human\nperception of the noise in a setting that closely resembles real-world conditions. This approach takes into account all the\nattenuation effects and anomalies caused by atmospheric conditions, including factors such as wind, thermal inversion,\nand turbulence. Table 1 and 2 include technical details concerning the setup configurations, drone specifications, and\nenvironmental conditions during the data collection phase."}, {"title": "A. Drones selection and maneuver definition", "content": "In this study, we examined four drones, selected based on a compromise between availability and diversity in\nterms of size and configuration. Details outlining the key characteristics of the drones used can be found in Table\n1. This selection includes two commercially available drones, a self-assembled Quadcopter (Holybro S500), and a"}, {"title": "B. Microphone setup", "content": "It's essential that the setup must be adjusted for each drone size. While this presents practical challenges, it ensures\nthat the results can be compared consistently across different drones, while also guaranteeing that all microphones are\nsituated in the acoustic far-field. For each drone, a distinct setup configuration is defined, yet these setups share common\ncharacteristics, with minor adjustments in hovering altitude (Hr) and microphone spacing (d, hs, hi). The primary\ndeviations between the proposed scheme and the ISO 5305 recommendation are as follows:\n\u2022 The structure incorporates seven microphones, including three of the four microphones specified in ISO 5305\n(\"m1,\" \"m4,\" \"m6\"), along with the addition of four extra microphones (\"m2,\" \"m3,\" \"m5\").\n\u2022 In place of the fourth microphone suggested by ISO 5305 to measure noise over the drone, a microphone (\"m7\") is\npositioned at a significantly lower height. This change was implemented for practical reasons, reducing the overall\nheight of the support structure.\nThe primary structure for microphone placement is based on a 50x50mm Item profile and divided into four 2-meter-long\nsegments for ease of transport. The microphones are affixed to adjustable supports and then secured to brackets fixed to\nthe profile. This arrangement allows precise positioning of the microphones according to the predefined plan, ensuring\nthey are oriented toward the hovering position. Microphone orientation is established prior to the structure's assembly\nby replicating the measurement conditions on the floor. The microphone setup is represented on Table 2 for each drone\nselected.\nBehringer ECM-8000 microphones were chosen. Calibration of the microphones is also conducted at this stage."}, {"title": "C. Measuring procedure", "content": "Acquiring the acoustic measurements from all seven inputs, as utilized in the Audio Interface system, was efficiently\nachieved using the Performer Lite program. However, the subsequent interpretation and analysis of this data introduced\ntwo main complexities to be taken into account.\nFirst, the decision to amalgamate all seven distinct maneuvers into a single flight introduced a level of intricacy.\nWhile this approach considerably streamlined the measurement process, it necessitated segmenting and filtering the\nmeasurements to isolate the noise during each maneuver. To address this, videos were recorded to capture the initial\nmoments when measurements began and the specific times at which each maneuver was executed.\nSecondly, maintaining accurate positional records throughout the maneuvers was essential. As previously mentioned,\nthe positioning accuracy of each measurement plays a key role in generating valuable acoustic data. The position data,\nstored by the drones themselves, required thorough filtering and analysis. To account for minor deviations in the take-off\nand landing positions, which are geometrically identical, a small linear correction was applied."}, {"title": "III. Data analysis", "content": "The data analysis was divided into several phases:\n1) Data Preprocessing: All the data was saved in a hard disk memory, and standardized with a sampling rate of\n22050 Hz."}, {"title": "2) Data Alignment with Flight Logs", "content": "The aim of this phase is to align all data collected through a synchronization\nprocess. This involved matching the RPM variation peaks observed in the available flight logs with the acoustic\nspectrum of the audio data. Afterwards, all signals were linear interpolated to match the sound sampling rate.\nThis alignment was easily achieved using the clock data within the logs."}, {"title": "3) Maneuver Segmentation", "content": "With the aid of videos, it was possible to accurately isolate the various maneuvers\nperformed by all the drones: at the end of this phase, it became feasible to analyze, for each drone, microphone,\nand maneuver, both the acoustic and electrophysical propulsion data."}, {"title": "4) Frequency features analysis", "content": "Frequency signals and spectrograms were calculated for all audio signals, and\nexamined in paragraph III.A."}, {"title": "5) Calculation of Acoustic Quality Metrics (SQM)", "content": "Metrics such as \"loudness,\" \"sharpness,\" \"roughness,\" and\n\"fluctuation strength\" were computed. These metrics are explained and analyzed in paragraph III.B."}, {"title": "6) Calculation of Psychoacoustic Annoyance (PA) based on SQM", "content": "An overall annoyance index was calculated.\nThis calculation is explained and analyzed in paragraph III.\u0421."}, {"title": "7) Drone acoustic features comparison", "content": "An example of SQM signals and PA values, which were the outcomes of\nthe preceding steps, are depicted in paragraph III.D. Subsequently, a comparative analysis of all the drones and\nmicrophones used in the experiment was conducted, considering their SQM and PA values."}, {"title": "A. Acoustic spectra and spectrograms", "content": "The Fourier transform of the signals and spectrograms is calculated for all drones, all microphones, and all\nmaneuvers. To calculate the frequency signals and spectrograms, the Python library Librosa [17] is used. Specifically,\nthe Short-Time Fourier Transform (STFT) algorithm is employed for signal in frequency calculation, where each\naudio frame is windowed using a window of length 2048. Scaling is applied according to the algorithm defined in\n\"librosa.amplitude_to_db\". For spectrograms, Matplotlib library is used with NFFT = 2048 and \"symlog\" as a\nscaling factor."}, {"title": "B. Sound Quality Metrics", "content": "Sound Quality Metrics (SQM) is a set of indices calculated by algorithms specific to the field of psychoacoustics [18].\nSQM are employed to establish the connection between the physical attributes of sound and the subjective impression\nperceived by the human ear. These algorithms attempt to analyze parameters like sound pressure level, frequency, and\nmodulation depth, and link them to human auditory perception. Sound quality encompasses several algorithms, such\nas ISO 532B stationary loudness, time-varying loudness, roughness, sharpness, tonality, and fluctuation strength. To\ncompute Psychoacoustic Annoyance, calculations were performed for time-varying loudness, roughness, sharpness, and\nfluctuation strength."}, {"title": "\u2022 Loudness (sone)", "content": "Loudness is a measure for how humans perceive the volume of sound. In the definition of\nloudness, it is specified that 1 sone, the unit of loudness, is equivalent to a 1 kHz tone at 40dB. The algorithm for\nloudness computation, also known as ISO 532B loudness or Zwicker loudness, is in compliance with ISO 532B,"}, {"title": "\u2022 Roughness (asper)", "content": "Roughness is a different metric employed to assess the subjective evaluation of sound quality.\nIt is associated with how noticeable or bothersome a sound is when perceived by the human ear. To be more\nprecise, roughness refers to an auditory sensation linked to fluctuations in loudness occurring at frequencies too\nhigh to be individually distinguished, typically involving modulation frequencies exceeding 30 Hz. In particular,\nroughness formula is defined as:\n$R = c_{al}\\int_{z=0}^{24}f_{mod} \\Delta L dz$\nWhere $f_{mod}$ represents the modulation frequency, detected by peak-picking a frequency-domain representation of\nthe acoustic loudness, $c_{al}$ is a constant used to maintain the reference signal's roughness at a constant value of\none, and $ \\Delta L$ is the modulation depth, that is a measure of the strength of amplitude modulation in a signal and can\nbe calculated by finding the envelope of filtered signals and taking the difference between maximum and minimum\nvalues of the envelope. In other words, it measures how much the amplitude of the signal varies due to modulation."}, {"title": "\u2022 Sharpness (acum)", "content": "Sharpness is a metric that captures the hearing sensation associated with frequency. It\ncharacterizes the perception of a sharp, piercing, high-frequency sound and is based on the comparison of the\namount of high-frequency energy to the overall energy. The sharpness algorithm derives sharpness from the\nsound pressure signal waveform, the 1/3-octave band spectrum computed across the frequency range of 25 Hz\nto 12.5 kHz, or the specific loudness. The algorithm that computes sharpness normalizes the specific loudness\nspectrum by the total loudness and adjusts the spectrum based on frequency. The result, a frequency-weighted\nvalue representing specific sharpness in relation to critical band rate, is then integrated to yield the sharpness\nmeasurement. Typically, the presence of higher frequency components in the signal leads to higher sharpness\nreadings. Sharpness formula is defined by:\n$S = k \\int_{z=0}^{24} L'(z) g(z) z dz$\nWhere L is the loudness, L' is the specific loudness in sones/Bark, the function g(x) and the scaling factor k\ndepend on the specified weighting method in compilance with DIN 45692 [19], Von Bismark and Aures. For our"}, {"title": "\u2022 Fluctuation Strength (vacil)", "content": "Fluctuation strength is a metric that tries to capture the hearing sensation associated\nwith loudness variations at low frequencies that can be individually perceived. It employs a similar approach to the\n\"roughness versus time\" analysis but is specifically focused on fluctuations with very low modulation frequencies.\nThe fluctuation strength metric assesses the energy within 47 overlapping barks, calculates and filters the envelope\nof the signal in each band, measures the amplitude modulation of each envelope, and adjusts the level in each band\nusing a frequency-dependent weighting function. The algorithm provides the result as the fluctuation strength\nspectrum relative to critical band rate and then integrates this spectrum to quantify the fluctuation strength. The\nanalysis encompasses modulations between 0 to 30 Hz, with a particular emphasis on those occurring near 4 Hz.\nFluctuation Strength formula is defined by:\n$F = \\frac{0.008 \\int_{z=0}^{24} AL dz}{\\frac{f_{mod}}{4} + \\frac{4}{f_{mod}}}$\nWhere, again, $f_{mod}$ represents the modulation frequency and $ \\Delta L$ is the perceived modulation depth\nAll of the metrics were calculated on Matlab software, using the Audio Toolbox package."}, {"title": "C. Psychoacoustic Annoyance", "content": "The Psychoacoustic Annoyance (PA) model, as devised by Zwicker and Fastl [18], provides a framework for\nunderstanding the connection between several auditory sensations, including loudness, sharpness, fluctuation strength,\nand roughness. This model is expressed through the following mathematical formulation:\n$PA = N5(1 + w_s^2 + w_{FR}^2)$\nWhere N5 is the 5th percentile of the loudness (in sones), ws and WFR are functions of S, F and R where:\n$W_S = \\begin{cases} (S - 1.75) \\cdot 0.25 log_{10}(N_5 + 10) & \\text{if } S > 1.75 \\\\ 0 & \\text{if } S \\le 1.75 \\end{cases}$\n$W_{FR} = \\frac{2.18}{N_5^{0.4}} (0.4 \\cdot F + 0.6 \\cdot R)$\nThe PA model offers valuable insights into how these specific psychoacoustic attributes collectively contribute to the\nperception of annoyance when exposed to various sound stimuli. It serves as a tool to analyze and predict the impact\nof sound on human annoyance, facilitating a deeper understanding of the subjective reactions to different acoustic\nenvironments."}, {"title": "IV. Data prediction Methodology", "content": "DL has revolutionized several fields, including computer vision, speech recognition, and text analysis, due to its\nremarkable success in classification and regression tasks [20]. In this section, we describe the methodology used for the\ndeveloping of a data-driven regression model for calculating psychoacoustic annoyance (PA) of a drone, based on a\nsubset of features extracted from our dataset. The purpose of developing this model is to understand the relationships\nbetween PA and various physical parameters of the drones."}, {"title": "A. Feature Selection", "content": "The dataset analyzed in section II was first processed to select the most relevant features for the prediction task. We\ncreated a Pandas DataFrame in which we derived the following subset of features, which are mostly independent of each"}, {"title": "B. Model architecture", "content": "We developed and tested several AI and non-AI based architectures and tried to understand which model better fits\nour problem, i.e. Linear Regression, Support Vector Machine (SVM), Deep Neural Network (DNN), and Convolutional\nNeural Network (CNN) using both TensorFlow's Keras and scikit-learn Python API. Hyperparameter tuning was\nimplemented using Nested Cross Validation (NCV). NCV estimates the generalization error of the underlying model\nand its hyperparameter search. The use of NCV is strongly advised in small datasets where test and train splitting can be\nnoisy, indeed for methods that do not sufficiently control overfitting, such as K-fold cross-validation with small sample\nsizes, could produce biased performance estimates [21]."}, {"title": "1. Custom Validation", "content": "Due to the relatively small amount of data in the dataset, we developed a secondary validation test to assess which is\nthe best model that has higher generalization capabilities. For this reason, we trained and optimized all four different\nmodels according to two main loss indices, namely RMSE (Full) and RMSE (Validation), which refers to the Root\nMean Square Error (RMSE) in predicting the PA of a drone. This prediction is calculated as the mean of the predicted\nannoyance for each microphone (using the variable $Micdist$) and again averaged over all maneuvers of that drone. In\nthe first case, the RMSE (Full) is the RMSE calculated from a model trained on the Training - Validation 0.2 splitted\nfull dataset (with all examples available for all drones). RMSE (Validation), on the other hand, is calculated from four\nsub-models derived from each regression model (4 sub-models of 4 models for a total of 16 models). Specifically, for"}, {"title": "2. Prediction model", "content": "The model architecture consists of multiple densely connected layers, and it was designed to predict PA based on the\nselected features performing a regression task. The model architecture is designed as follows:\n\u2022 Input Layer: Comprising 5 neurons, corresponding to the selected features described in Section IV.A.\n\u2022 Hidden Layers: 3 hidden layers, all utilizing the Rectified Linear Unit (ReLU) as activation function.\nLayer dense: 256 units\nLayer dense: 16 units\nOutput layer: 1 unit\nThe dense architecture structure was based and optimized using Nested Cross Validation (NCV) for hyperparameter\nsearch, using different optimizer, layers, units and activation functions.\n\u2022 Output Layer: A single neuron in the output layer, as this is a regression task aimed at predicting a the value (PA).\nFor our model compilation, we utilized the Adam optimizer, as suggested from the result of the hyperparameter research,\nand the mean squared error (MSE) loss function was implemented to calculate the model error. This specific loss\nfunction proves to be appropriate for regression tasks."}, {"title": "C. Model testing and results", "content": "The training history of the model is an important indicator of its learning progress. We monitored the validation\nloss during training to assess the model's performance, and the history trend of the best model at the end of the NCV\nprocess is shown in Figure 10 (a). The model's predictions during the RMSE (Validation) phase, closely align with the\nground truth values, indicating its ability to capture the relationships between the input features and PA. These results\ndemonstrate the model's effectiveness in predicting PA. With this assumption, we can easily calculate and analyze the\npredicted correlation between drone's physical features and the PA. As we can see in Figure 10 (b), the increasing of\nPerceived Annoyance could be assessed to both overall drone's Weight and it's aspect ratio $Larm/Lrot$. This result can\nbe a starting point for a shape optimization process with the goal of building more efficient drones that produce less\nnoise pollution."}, {"title": "V. Conclusions", "content": "Our study extends current research by examining multiple drone models with multiple microphones in a realistic\nflying condition. This contributes to a better understanding of how design choices in drone manufacturing can affect\nnoise generation. We conducted a thorough analysis of flight data collection and manipulation following the ISO\nstandard, analyzing various flight maneuvers, drone physical characteristics, and facing the calculation and prediction of\nSQM and PA in real-world scenarios.\nOur study can provide useful insights for the development of drone optimization strategies. There are several"}]}