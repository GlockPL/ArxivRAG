{"title": "GenAgent: Build Collaborative AI Systems with Automated Workflow Generation - Case Studies on ComfyUI", "authors": ["Xiangyuan Xue", "Zeyu Lu", "Di Huang", "Wanli Ouyang", "Lei Bai"], "abstract": "Much previous AI research has focused on developing monolithic models to maximize their intelligence and capability, with the primary goal of enhancing performance on specific tasks. In contrast, this paper explores an alternative approach: collaborative AI systems that use workflows to integrate models, data sources, and pipelines to solve complex and diverse tasks. We introduce GenAgent, an LLM-based framework that automati-cally generates complex workflows, offering greater flexibility and scalabil-ity compared to monolithic models. The core innovation of GenAgent lies in representing workflows with code, alongside constructing workflows with collaborative agents in a step-by-step manner. We implement GenAgent on the ComfyUI platform and propose a new benchmark, OpenComfy. The results demonstrate that GenAgent outperforms baseline approaches in both run-level and task-level evaluations, showing its capability to generate complex workflows with superior effectiveness and stability. The project page of this work is available at https://xxyqwq.github.io/GenAgent.", "sections": [{"title": "1 Introduction", "content": "The recent evolution of artificial intelligence is defined by the growing importance of collaborative AI systems, which integrate multiple models and tools to work together as a whole collaborative system. The success of ChatGPT Plus (OpenAI, 2024) presents the possibility of integrating a wide range of tasks, such as web browsing, image generation, and code execution, into a single conversational agent. Unlike traditional Al models that function as single entities, collaborative AI systems integrate multiple AI components, each contributing unique capabilities to solve complex problems. This shift towards integration has become crucial for achieving state-of-the-art results, as it leverages the combined strengths of diverse AI functionalities within a unified framework (Zaharia et al., 2024).\nUnfortunately, the design space and optimization of collaborative systems often require significant human expertise. For instance, various collaborative systems have distinctly different approaches: AlphaCode 2 (AlphaCode Team, 2024) generates up to 1 million solutions for a coding problem and then filters and scores them. AlphaGeometry (Trinh et al., 2024) iteratively suggests constructions in a geometry problem via LLM and checks deduced facts produced by a symbolic engine. ComfyUI (2023), though designed to be user-friendly, still requires users to have a certain level of expertise in understanding the principles of various generative model components and in meticulously designing"}, {"title": "2 Related Work", "content": "2.1 LLM Agents\nLLM agents are an emerging class of autonomy systems that leverage LLMs to interact with external tools and solve real-world problems (Xi et al., 2023; Wang et al., 2024a; Sumers et al., 2023). On one hand, LLM agents exploit the powerful capabilities of LLMs in cognition and creativity, so that they can adapt to various novel tasks without relying on human intervention (Li et al., 2023; Mei et al., 2024). On the other hand, LLM agents have access to a wide range of external tools such as search engines and databases, which makes up for the lack of specialized knowledge in LLMs (Ge et al., 2024; Shen et al., 2024).\nWith the rapid development of LLM agents, researchers have proposed various methods to improve their performance, including chain-of-thought prompting (Wei et al., 2022), self-consistency reasoning (Wang et al., 2022a), retrieval-augmented generation (Lewis et al., 2020; Gao et al., 2023), memory mechanism (Zhong et al., 2024; Zhang et al., 2024), reinforcement learning (Ge et al., 2024; Li et al., 2024; Ouyang et al., 2022; Bai et al., 2022; Kaufmann et al., 2023), etc. These methods have been proven effective in enhancing the capabilities of LLM agents in different tasks, thus hastening a common architecture for novel LLM agents.\nBased on well-designed module architectures and interaction mechanisms, LLM agents can be applied to a wide range of complex tasks, such as web navigation (Nakano et al., 2021; Thoppilan et al., 2022; Yao et al., 2022; Zhou et al., 2023; Koh et al., 2024), interface operation (Yang et al., 2023b; Xie et al., 2023; 2024b; Cao et al., 2024; Wu et al., 2024), code generation (Wang et al., 2022b; Yin et al., 2022; Yang et al., 2024a; Wang et al., 2024c), and other specified tasks (Huang et al., 2024; Thakur et al., 2023; Yang et al., 2023a). With the power of code generation, LLM agents can also be used for reasoning, planning, and decision making (Wei et al., 2022; Yao et al., 2024; Besta et al., 2024; Song et al., 2023; Shen et al., 2024; Ge et al., 2024; Xie et al., 2024a; Wang et al., 2024b; Yang et al., 2024b). These tasks, previously considered to require expert knowledge and human intervention, can now be accomplished by LLM agents with high efficiency and accuracy."}, {"title": "2.2 LLM-based Collaborative AI Systems", "content": "LLMs can be instructed to perform general tasks, leading to an intense focus on the capa-bilities of the base models. However, Zaharia et al. (2024) points out that state-of-the-art AI results are increasingly obtained by collaborative systems instead of monolithic models, which has been verified many times (AlphaCode Team, 2024; Trinh et al., 2024; Lewis et al., 2020; Nori et al., 2023; Team et al., 2023). Yuan et al. (2024) even designs a multi-agent framework that incorporates several advanced visual AI agents to replicate generalist video generation demonstrated by Sora (Brooks et al., 2024). The advantages of collaborative systems are prominent. On one hand, dynamic systems make it possible to fetch external knowledge with components such as search and retrieval. On the other hand, improving control and trust is easier with systems, so the tasks can be better solved.\nCollaborative AI systems pose new challenges in design, optimization, and operation compared to AI models. Many novel paradigms are emerging to tackle these challenges. Using composition frameworks is the most trending paradigm, which builds applications out of calls to AI models and other components (LangChain, 2023; LlamaIndex, 2023; AutoGPT, 2023; Hong et al., 2023; Willard & Louf, 2023; LMQL, 2023; Zheng et al., 2023; Wu et al., 2023; Chen et al., 2023). System optimization is another approach. Khattab et al. (2023) proposed the DSPy framework, which automatically tunes the pipeline by creating prompt instructions, few-shot examples, and other parameter choices to maximize end-to-end performance. GPTSwarm (Zhuge et al., 2024) supports building LLM agents in the form of graphs and optimizing the parameters and connections as a whole system."}, {"title": "3 Method", "content": "3.1 Workflow as the High-Level Abstract of the Collaborative AI System\nA collaborative AI system is a specific operational framework that addresses complex tasks by coordinating multiple interacting components, including AI models, retrievers, and external tools. A collaborative, low-level, and specific AI system corresponds to a high-level and abstract operational logic diagram, defined as the workflow of the collaborative AI system. Once we establish the workflow for a complex AI system at an abstract level, we can compile it into a specific AI system and execute this system by the interpreter reliably. Therefore, by focusing on building the workflows, we can effectively construct diverse collaborative AI systems.\nTypically, a workflow can be described as a directed acyclic graph (DAG), which necessarily contains vertices and directed edges. Here, we define:\n\u2022 Vertex: A vertex represents a particular component in the system for information processing (e.g. AI model, retriever, and external tool), which contains necessary input and output interfaces connected by directed edges.\n\u2022 Directed Edge: A directed edge represents the flow of information. The incoming edges of a vertex represent the external information received by it, while the out-going edges represent the information output from it. A vertex can have multiple incoming and outgoing edges, representing different information paths.\nBy constructing the DAG, we can efficiently build and edit the workflow of the collaborative Al system. ComfyUI (ComfyUI, 2023) is a typical example, which atomizes all modules and standardizes their data interfaces within a workflow DAG, thus providing an ideal platform for building collaborative AI systems in such a paradigm."}, {"title": "3.2 Representing Workflows with Code", "content": "Workflows are widely used across various applications, with diverse representations: flow graph, JSON, element list, and code.\nFlow graph is one of the most intuitive and user-friendly representations of workflow DAGs for humans, but not LLMs. As shown in Figure 2a, a flow graph illustrates a default text-to-image workflow in ComfyUI (ComfyUI, 2023), precisely mirroring how the ComfyUI frontend interacts with users. However, since flow graphs are inherently visual, they are not easily understood by standard LLMs, which cannot interpret visual data. While Vision-Language Models (VLMs) can read flow graphs, they struggle to modify them due to their limited vision-generation capabilities.\nJSON is a popular way for LLMs to represent structured information, but not good for workflow DAGs. Zhang et al. (2023) propose using JSON to represent some simple workflows for data analysis. For complex workflows, our experiments indicate that LLMs struggle to understand and generate it using JSON, as shown in Table 2. This difficulty may stem from the inability of large language models (LLMs) to fully comprehend the complex Directed Acyclic Graph (DAG) relationships encoded in JSON. This challenge is further exacerbated by the substantial redundant information typically found in JSON format, which poses significant issues due to the limited context length of LLMs.\nElement list is a more natural representation for LLMs to grasp workflows compared to flow graphs or JSON files, but not enough. The element list is more compact and closer to natural language, making it more accessible to LLMs compared to flow graphs or JSON files. However, element lists emphasize symbolic representation, which disrupts the intuitive dependencies within the workflow. They do not explicitly reveal the topological order of the flow graph, leading to significant bottlenecks for LLMs in understanding and generating workflows in this format once the workflows become relatively complex, as shown in Table 2. Li et al. (2024) propose using the CoRE language to represent workflows, which can be viewed as a verbal version of element lists. The CoRE language does not resolve the issues inherent in element lists. Moreover, it requires additional fine-tuning of LLMs.\nCode is a stable representation for LLMs to understand and create workflow DAGs. Compared to previous methods, we assert that utilizing code offers a superior workflow representation, as illustrated in Figure 2d. Programming languages are Turing complete, allowing any computationally feasible workflow to be represented equivalently. Addi-tionally, they provide sufficient semantic information, enhancing comprehensibility and learnability, which leverages the potential of LLMs in code generation. In this work, we manage to represent workflows with code. A dedicated programming language for accurate workflow description is of vital importance, ensuring that they can be precisely translated into corresponding code snippets and vice versa. To minimize the cost of developing a new language, we prompt LLMs to use a subset of Python-like syntax, restricting it to basic operations and control structures while prohibiting advanced features. The development of a workflow-oriented language is left for future work."}, {"title": "3.3 Generate Workflows with GenAgent: A Collaborative-Agents System", "content": "Through code representation, we can prompt LLMs to generate workflows for collaborative AI systems following our task instructions. In practice, traditional prompt-based methods, such as chain-of-thoughts (CoT) (Wei et al., 2022) and retrieval-augmented generation (RAG) (Lewis et al., 2020), fail to generate correct results stably with the complexity of workflows increasing, as shown in Table 1. To achieve the goal of generating complex workflows, we propose the GenAgent framework where the agents collaborate to complete the workflow generation task. GenAgent is mainly composed of three independent modules: Memory, PlanAgent, and Action.\nMemory mainly stores the recent state of the agent, building on psychological theo-ries (Atkinson, 1968). Here, memory refers to working memory (Baddeley, 1992), which reflects the agent's current circumstances: it stores the agent's recent history behaviors, results from intermediate, external reference knowledge, and internal reasoning. We mainly formulate these into three parts:\n\u2022 History: The history primarily stores recent actions taken by PlanAgent. PlanAgent can review past actions through the history to plan subsequent activities.\n\u2022 Reference: References predominantly comprise the most salient information re-trieved by RetrieveAgent from the knowledge database for analysis. RetrieveAgent typically retrieves the most pertinent information based on the current memory state, subsequently updating the references.\n\u2022 Workspace: The workspace stores the current workflow and its natural language description, keeping the current working status complete and comprehensible.\nPlanAgent is the core of GenAgent, responsible for the global planning of workflows under the task instruction. At each step, PlanAgent generates a high-level plan with an action decision based on the current memory and task instruction.\nActions represent different activities that PlanAgent can select, and the goal of each action is to modify the current memory. There are five possible actions for PlanAgent:\n\u2022 Init: PlanAgent selects an existing workflow to initialize the memory, and gives a thorough multi-step plan based on the task instruction.\n\u2022 Combine: There is a dedicated CombineAgent, responsible for executing the combine action. CombineAgent chooses a specific workflow from references and combines it into the current workflow code. After combining, a RefineAgent will refine the output of the CombineAgent and update it in the workspace.\n\u2022 Adapt: AdaptAgent is a specialized agent responsible for carrying out the adapt ac-tion. AdaptAgent adapts the details (e.g. hyperparameters) of the current workflow based on the memory and instructions. After adaptation, a RefineAgent refines the output of the AdaptAgent and updates it in the workspace.\n\u2022 Retrieve: A dedicated RetrieveAgent is responsible for executing the retrieve action. RetrieveAgent retrieves the most relevant information from the knowledge database and updates the references accordingly.\n\u2022 Finish: PlanAgent evaluates the completion status of the task instructions based on the current workflow in the workspace. Upon task completion, the PlanAgent executes the finish action to prevent the occurrence of infinite loops.\nGenAgent stably generates workflow code through multiple iterations of planning and action, based on the task instruction. The code is then converted into a standardized workflow, enabling the construction of a collaborative AI system."}, {"title": "4 Experiments", "content": "4.1 Implementation Details\nTheoretically, GenAgent can be applied to any platform that solves problems by designing and executing workflows. In this work, we implement GenAgent on the ComfyUI platform as a proof of concept. ComfyUI is initially designed for stable diffusion models, where multiple modules are connected to form a pipeline, allowing for flexible and efficient generation. With the joint efforts of the open-source community, ComfyUI has been extended to support various models and tools, making it possible to solve a wide range of generation tasks. ComfyUI uses workflows to describe the generation pipelines. A typical ComfyUI workflow consists of tens of nodes and links, which are connected to form a complex DAG. In contrast, the benchmark tasks in previous works, such as NCEN-QA (Zeng et al., 2023) and OpenAGI (Ge et al., 2024), can be solved by tool chains within several steps. Therefore, the automatic design of ComfyUI workflows is more valuable and challenging.\nThe bidirectional conversion between workflow and code is one of the key features of GenAgent. Naive ComfyUI workflows are stored in JSON format. We implement a parser to extract the entire DAG described in the structured JSON file. The nodes on the graph are viewed as function calls, while the links pass the variables between functions. According to the topological order, the DAG is converted into a sequence of assignments and function calls, which exactly forms the code we mentioned in Section 3.2. The code follows a simplified Python syntax, only involving class instantiation and function calls. Although the code cannot be executed directly, it enhances the readability and compresses the context length. Similarly, a reverse parser is implemented to convert the code back to the structured JSON file, so that the generated workflows can be executed in ComfyUI.\nConsidering the difficulty of generating ComfyUI workflows, a powerful backbone LLM is required. In this work, we choose the latest version of GPT-40, where different agents, such as the PlanAgent, CombineAgent, and AdaptAgent, are implemented as different roles played by GPT-40. On one hand, GPT-40 has been fed with updated online knowl-edge, which makes the latest tools more understandable. On the other hand, it has strong multimodal capabilities in both natural language and programming language, which is an essential feature benefiting the GenAgent framework. Practice shows that GPT-40 can maximize the potential of GenAgent without any additional fine-tuning."}, {"title": "4.2 Benchmark Evaluation", "content": "To evaluate the effectiveness of GenAgent, we propose a benchmark, OpenComfy, based on the ComfyUI platform. OpenComfy contains 20 different tasks, covering various types of generation tasks, such as image editing, video generation, and even some compound tasks. All the tasks are verified feasible with native nodes or common extensions in ComfyUI, of which the most challenging one requires 24 nodes to build the expected workflow. We provide complete documentation for every node to help agents better understand the inner logic. We also provide a set of examples, containing 12 basic workflows with manual annotations, so that agents can learn from these examples. The specific tasks are listed in Appendix B. We implement 4 competitive agents as baselines, which are listed as follows:\n\u2022 Zero-shot Agent follows the zero-shot learning paradigm, which is fed with the task requirement and directly generates the workflow.\n\u2022 Few-shot Agent follows the few-shot learning paradigm. which learns from the fixed examples and generates the workflow according to the task requirement.\n\u2022 CoT Agent follows the Chain-of-Thought (CoT) (Wei et al., 2022) paradigm, which involves multiple intermediate steps to generate the workflow.\n\u2022 RAG Agent follows the Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) paradigm, which learns from the retrieved examples and generates the workflow.\n\u2022 GenAgent is our proposed framework, where multiple agents collaborate to learn and generate the workflow according to the task requirement."}, {"title": "4.3 Ablation Studies", "content": "In Section 3.2, we argue that code is the most suitable representation for workflows compared to other formats. In this section, we conduct an ablation study to verify this claim. We implement three variants of GenAgent, where the workflows are represented in JSON, element list, and code, respectively. Note that flow graph is not implemented because it is not feasible for language models. Each variant is still evaluated on the OpenComfy benchmark, and each task is tested 5 times. We report the same metrics as in Section 4.2. The results are shown in Table 2.\nWe can see that representing workflows with code achieves the best performance in terms of all the metrics. The JSON representation shows poor performance as expected, because understanding and generating complex JSON structures is extremely difficult for LLMs. The element list representation fails to achieve satisfactory performance, because of the"}, {"title": "4.4 Generation Examples", "content": "Besides the quantitative evaluation, we also present some qualitative results to demonstrate the effectiveness of GenAgent. We present the generation results of two different tasks selected from the OpenComfy benchmark, which can intuitively show that GenAgent can generate complex ComfyUI workflows and complete various generation tasks.\nFigure 5 shows the first generation example, where the task provides a photo of a girl playing the guitar and requires generating an image of an old man in the forest, playing the guitar with the same pose as the girl. Besides, the expected style and resolution are specified in the task requirement. Technically, a possible solution is to use a pose estimator together with a ControlNet model to inject the pose information as conditions. It turns out that GenAgent learns these techniques from human experience and generates a simple\nworkflow with 13 nodes. The resulting image is generated by executing the workflow in ComfyUI, which satisfies the task requirement in terms of both parameters and semantics.\nFigure 6 shows the second generation example. The task is more complex compared to the first one, which requires generating an image of London following the style of the given photo of Budapest and converting it into a video. Considering the resolution and frame rate are limited by a single model, the task also involves upscaling and interpolation to form a high-quality video. To solve this task, GenAgent generates a workflow with 22 nodes and complicated connections, utilizing multiple models such as SVD, ESRGAN, and RIFE. The workflow finally generates a video with the expected content and quality. The complete workflows and corresponding images or videos are presented on our Project Page."}, {"title": "5 Conclusion", "content": "In this work, we introduce the trend of building collaborative AI systems to solve more complex and diverse tasks systematically. We point out that the automatic design of collaborative AI systems may be the key to achieving high-level machine intelligence. To realize this possibility, we propose the GenAgent framework, which enables LLM agents to automatically design collaborative systems by generating workflows. The core idea of GenAgent lies in two aspects: representing workflows with code and building workflows with collaborative agents. We argue that a dedicated programming language is the most suitable way to describe workflows, which is compact, Turing complete, semantically rich, and friendly for LLMs. We illustrate the entire architecture of GenAgent, where multiple agents collaborate to generate workflows in a step-by-step manner.\nTo verify the effectiveness of our GenAgent framework, we implement and apply it on the ComfyUI platform. We also propose the OpenComfy benchmark to conduct comprehensive evaluations. The results show that GenAgent outperforms all the baselines in both run-level and task-level evaluations, which demonstrates that GenAgent is capable of generating complex workflows with better effectiveness and stability. In addition, we present some qualitative examples of the generated workflows and final results. We believe GenAgent can be applied to design any collaborative AI system as long as unified interfaces are provided, which provides a potential solution for completing general tasks and realizing AGI."}, {"title": "A Prompt Template", "content": "In this section, we provide the prompt templates of different agents in the GenAgent framework. Generally, we first introduce the ComfyUI platform as the background, then present the useful information for the current task, and finally specify the answer format. To extract the expected answers correctly and stably, we follow the prompting strategy in CodeAct (Wang et al., 2024b), which uses tags to enclose the specific answers.\nThe prompt template for AnalyzeAgent is presented as follows:\nPrompt Template of AnalyzeAgent\nComfyUI uses workflows to create and execute Stable Diffusion pipelines so that users can design their own workflows to generate highly customized artworks. ComfyUI provides many nodes. Each node represents a module in the pipeline. Users can formulate a workflow into Python code by instantiating nodes and invoking them for execution. You are an expert in ComfyUI who helps users to design their own workflows.\nNow you are required to create a ComfyUI workflow to finish the following task:\n{query}\nBased on the description, point out the key points behind the requirements (e.g. main object, specific style, target resolution, etc.) and the expected paradigm of the workflow (e.g., text-to-image, image-to-image, image-to-video, etc.).\nYou are not required to provide the code for the workflow. Please make sure your answers are clear and concise within a single paragraph.\nThe prompt template for PlanAgent is presented as follows:\nPrompt Template of PlanAgent\n## Task\nComfyUI uses workflows to create and execute Stable Diffusion pipelines so that users can design their own workflows to generate highly customized artworks. ComfyUI provides many nodes. Each node represents a module in the pipeline. Users can formulate a workflow into Python code by instantiating nodes and invoking them for execution. You are an expert in ComfyUI who helps users to design their own workflows.\nNow you are required to create a ComfyUI workflow to finish the following task:\n{query}\nThe key points behind the requirements and the expected paradigm of the workflow is analyzed as follows:\n{analysis}\n## Reference\nAccording to the analysis, we have retrieved relevant workflows. Here are some example workflows that may be helpful:\n{reference}\n## History"}, {"title": "## Workspace", "content": "The code and description of the current workflow you are working on are presented as follows:\n{workspace}\n## Action\nBased on your previous plan and the current workflow, you should first think about what functions have been implemented and what modules remain to be added. Your thought should be enclosed with \"<thought>\" tag. For example: <thought> The text-to-image pipeline has been implemented, but an upscaling module is needed to improve the resolution. </thought>.\nAfter that, you should update your step-by-step plan to further modify your workflow.\nYour plan should contain at most 5 steps, but fewer steps will be better. Make sure that each step is feasible to be converted into a single action.\nYour plan should be enclosed with \"<plan>\" tag. For example: <plan> Step 1: I will refer to example_name to add a new node. Step 2: I will finish the task since all the functions are implemented. </plan>.\nFinally, you should choose one of the following actions and specify the arguments (if required), so that the updated workflow can realize the first step in your new plan.\nYou should provide your action with the format of function calls in Python. Your action should be enclosed with \"<action>\" tag. For example: <action> combine (name=\"example_name\") </action>, <action> adapt (prompt=\"Change the factor to 0.5 and rewrite the prompt.\") </action>, and <action> finish() </action>.\n`load: Load the specified example workflow into your workspace to replace the current workflow, so that you can start over. Arguments:\n`name`: The name of the example workflow you want to load.\n`combine`: Combine the current workflow with the specified example workflow, so that you can add necessary modules (e.g. adding a upscaling module to the text-to-image pipeline). Arguments:\nname: The name of the example workflow you want to combine.\n`adapt: Adapt some of the parameters in the current workflow, so that you can better realize the expected effects. Arguments:\n`prompt`: The prompt to specify which parameters you want to adapt and how to adapt them.\n`retrieve`: Retrieve a new batch of example workflows, so that you may find useful references. Arguments:\n`prompt`: The prompt to describe the expected features of example workflows you want to retrieve.\n`finish: Finish the task since the current workflow is capable of realizing the expected effects.\nRefer to the history before making a decision. Here are some general rules you should follow:\n1. You may choose the `load` action only when the history is empty (i.e. the current workflow may be unrelated to the task).\n2. If you choose the `load or `combine` action, make sure that the example workflow exists in the reference. Otherwise, you should try to update the reference list by the `retrieve` action.\n3. You should not choose the `adapt` action twice in a row, because they can be simplified into a single action."}, {"title": "4. If you choose the adapt or retrieve` action, make sure that your prompt is concise and contains all the necessary information.", "content": "5. You should choose the finish action before the steps are exhausted.\nNow, provide your thought, plan and action with the required format."}, {"title": "Prompt Template of CombineAgent", "content": "## Task\nComfyUI uses workflows to create and execute Stable Diffusion pipelines so that users can design their own workflows to generate highly customized artworks. ComfyUI provides many nodes. Each node represents a module in the pipeline. Users can formulate a workflow into Python code by instantiating nodes and invoking them for execution. You are an expert in ComfyUI who helps users to design their own workflows.\nNow you are required to create a ComfyUI workflow to finish the following task:\n{query}\nThe key points behind the requirements and the expected paradigm of the workflow is analyzed as follows:\n{analysis}\n## Reference\nThe code and description of the example workflow you are referring to are presented as follows:\n{reference}\n## Workspace\nThe code and description of the current workflow you are working on are presented as follows:\n{workspace}\n## Combination\nBased on the current working progress, your step-by-step plan is presented as follows:\n{planning}\nNow you are working on the first step of your plan. Later steps should not be considered at this moment. In other words, you should merge the example workflow with the current workflow according to your plan, so that their functions can be combined.\nFirst, you should provide your Python code to formulate the updated workflow. Your code must meet the following rules:\n1. Each code line should either instantiate a node or invoke a node. You should not invoke a node before it is instantiated.\n2. Each instantiated node should be invoked only once. You should instantiate another node if you need to reuse the same function.\n3. Avoid reusing the same variable name. For example: \"value_1 = node_1(value_1)\" is not allowed because the output \"value_1\" overrides the input \"value_1\".\n4. Avoid nested calls in a single code line. For example: \"output_2 = node_2(input_1, node_1())\" should be separated into \"output_1 = node_1() and output_2 = node_2(input_1, output_1)\".\nYour code should be enclosed with \"<code>\" tag. For example: <code> output_1 = node_1() </code>.\nAfter that, you should provide a brief description of the updated workflow and the expected effects as in the example. Your description should be enclosed with \"<description>\" tag. For example: <description> This workflow uses the text-to-image pipeline together with an upscaling module to generate a high-resolution image of a running horse. </description>.\nNow, provide your code and description with the required format."}, {"title": "Prompt Template of AdaptAgent", "content": "## Task\nComfyUI uses workflows to create and execute Stable Diffusion pipelines so that users can design their own workflows to generate highly customized artworks. ComfyUI provides many nodes. Each node represents a module in the pipeline. Users can formulate a workflow into Python code by instantiating nodes and invoking them for execution. You are an expert in ComfyUI who helps users to design their own workflows.\nNow you are required to create a ComfyUI workflow to finish the following task:\n{query}\nThe key points behind the requirements and the expected paradigm of the workflow is analyzed as follows:\n{analysis}\n## Workspace\nThe code and description of the current workflow you are working on are presented as follows:\n{workspace}\n## Adaptation\nBased on the current working progress, your step-by-step plan is presented as follows:\n{planning}\nNow you are working on the first step of your plan. Later steps should not be considered at this moment. In other words, you should adjust some of the parameters in the current workflow according to your plan, so that you can better realize the expected effects. Your expected modification is specified as follows:\n{adaptation}\nFirst, you should provide your Python code to formulate the updated workflow. Your code must meet the following rules:\n1. Each code line should either instantiate a node or invoke a node. You should not invoke a node before it is instantiated.\n2. Each instantiated node should be invoked only once. You should instantiate another node if you need to reuse the same function.\n3. Avoid reusing the same variable name. For example: \"value_1 = node_1(value_1)\" is not allowed because the output \"value_1\" overrides the input \"value_1\".\n4. Avoid nested calls in a single code line. For example: \"output_2 = node_2(input_1, node_1())\" should be separated into \"output_1 = node_1() and output_2 = node_2(input_1, output_1)\".\nYour code should be enclosed with \"<code>\" tag. For example: <code> output_1 = node_1() </code>.\nAfter that, you should provide a brief description of the updated workflow and the expected effects as in the example. Your description should be enclosed with \"<description>\" tag. For example: <description> This workflow uses the text-to-image pipeline together with an upscaling module to generate a high-resolution image of a running horse. </description>.\nNow, provide your code and description with the required format."}, {"title": "Prompt Template of RefineAgent", "content": "## Task\nComfyUI uses workflows to create and execute Stable Diffusion pipelines so that users can design their own workflows to generate highly customized artworks. ComfyUI provides many nodes. Each node represents a module in the pipeline. Users can formulate a workflow into Python code by instantiating nodes and invoking them for execution. You are an expert in ComfyUI who helps users to design their own workflows.\nNow you are required to create a ComfyUI workflow to finish the following task:\n{query"}, "nThe key points behind the requirements and the expected paradigm of the workflow is analyzed as follows:\n{analysis}\n## Reference\nAccording to the analysis, we have retrieved relevant workflows. Here are some example workflows that may be helpful:\n{reference}\n## Workspace\nThe code and description of the current workflow you are working on are presented as follows:\n{workspace}\n## Refinement\nBased on the current working progress, your step-by-step plan is presented as follows:\n{planning}\nTo realize the first step of your plan, the code and description of your updated workflow are presented as in the workspace. However, an error occurred when running your code. This may be caused by nested calls, missing parameters, or other issues. The detailed error message is presented as follows:\n{refinement}\nFirst, Try to explain why this error occurred. Your explanation should be enclosed with \"<explanation>\" tag. For example:  The error occurred because the input parameter of node_1 is missing. .\nAfter that, correct the error and provide your Python code again. Your code must meet the following rules:\n1. Each code line should either instantiate a node or invoke a node. You should not invoke a node before it is instantiated.\n2. Each instantiated node should be invoked only once. You should instantiate another node if you need to reuse the same function.\n3. Avoid reusing the same variable name. For example: \"value_1 = node_1(value_1)\" is not allowed because the output \"value_1\" overrides the input \"value_1\".\n4. Avoid nested calls in a single code line. For example: \"output_2 = node_2(input_1, node_1())\" should be separated into \"output_1 = node_1() and output_2 = node_2(input_1, output_1)\".\nYour code should be enclosed with \"<code>\" tag. For example: <code> output_1 = node_1()"]}