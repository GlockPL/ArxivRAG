{"title": "AI-Based Beam-Level and Cell-Level Mobility Management for High Speed Railway Communications", "authors": ["Wen Li", "Wei Chen", "Shiyue Wang", "Yuanyuan Zhang", "Michail Matthaiou", "Bo Ai"], "abstract": "High-speed railway (HSR) communications are pivotal for ensuring rail safety, operations, maintenance, and delivering passenger information services. The high speed of trains creates rapidly time-varying wireless channels, increases the signaling overhead, and reduces the system throughput, making it difficult to meet the growing and stringent needs of HSR applications. In this article, we explore artificial intelligence (AI)-based beam-level and cell-level mobility management suitable for HSR communications, including the use cases, inputs, outputs, and key performance indicators (KPI)s of AI models. Particularly, in comparison to traditional down-sampling spatial beam measurements, we show that the compressed spatial multi-beam measurements via compressive sensing lead to improved spatial-temporal beam prediction. Moreover, we demonstrate the performance gains of AI-assisted cell handover over traditional mobile handover mechanisms. In addition, we observe that the proposed approaches to reduce the measurement overhead achieve comparable radio link failure performance with the traditional approach that requires all the beam measurements of all cells, while the former methods can save 50% beam measurement overhead.", "sections": [{"title": "I. INTRODUCTION", "content": "HIGH speed railway (HSR) provides people with safe, fast, comfortable and economical modes of transportation, and has been widely developed around the world. Nevertheless, mobile communication systems are pivotal for ensuring rail safety, operations, maintenance, and delivering passenger information services. Emerging applications, such as railway multimedia dispatching, high-definition video monitoring on board and trackside, have posed stricter requirements on the reliability, data rate, and spectrum efficiency of the next-generation, dedicated mobile communication systems for HSR. However, the high speed of trains entails formidable challenges due to the time-varying nature of wireless channels in HSR communications. These challenges include the increased signaling overhead and reduced system throughput.\nThanks to the abundant spectrum available at higher frequencies, communication systems using the millimeter wave (mmWave) technology avail of faster data transmission speeds and reduced latency compared to traditional low-frequency communications. For HSR communications, mmWave technology underpins increased data transmission rates that meet the demands of onboard applications and services, and provides low latency that is indispensable for real-time communication requirements. However, mmWave communication systems suffer from significant path loss, which restricts the effective range of transmission. To tackle this problem, beam-forming with large-scale antenna arrays can be employed to generate finely directional beams, mitigating propagation loss and expanding signal coverage. However, the high mobility of trains entails pressing challenges in mobility management for HSR communications. While being connected, a train has to address two types of mobility, i.e., the cell-level and beam-level mobility as specified in the 3rd Generation Partnership Project (3GPP) Release 17 for 5G systems [1]. Beam-level mobility involves dynamically switching beams in which the train establishes a link path to a new beam within the same cell, ensuring communication with high performance. On the other hand, cell-level mobility focuses on cell handover between different cells. As trains move from one coverage zone to another, it is crucial to have a smooth cell handover to sustain continuous connectivity experience.\nThe management of mobility at the beam and cell level in 5G is based on the use of periodic channel state information reference signals (CSI-RS) and synchronization signal blocks (SSBs). Unfortunately, the resulting large latency and high overhead are prohibitive for HSR communications, and, hence, innovative approaches are urgently needed to address these limitations. Deep neural networks (DNNs) have found extensive applications in areas, such as computer vision and natural language processing, owing to their exceptional ability to extract features from intricate and high-dimensional data. In recent years, DNNs have also proven their effectiveness in capturing the complex and nonlinear characteristics of wireless channels to facilitate beam-level mobility management. For example, a multi-layer long short term memory (LSTM) network was utilized to predict the optimal beam change for future instances [2]. Studies on advanced artificial intelligence (AI)-based cell-level mobility management are relatively few. For example, [3] utilized convolutional neural network (CNN)-based auto-encoders for dimension reduction and LSTM networks to predict the appropriate base station (BS) for cell handover. Recently, the 3GPP 5G NR organization has launched studies on AI-based cell-level mobility management for highly reliable handovers in Release 19. However, these AI-based mobility management methods fail to exploit the specific characteristics in HSR communications. For instance, trains typically maintain stable high-speed motion on fixed tracks and have different speed levels to meet different needs. Mobility management methods for HSR communications are expected to leverage case-specific characteristics in the design phase or, alternatively, learn from data to yield improved performance in a data-driven manner.\nIn this article, we present the latest progress in AI-based beam-level and cell-level mobility management suitable for HSR communications. In Section II, we reveal the potential of AI-based spatial-temporal beam prediction for HSR communications. Particularly, in comparison to the traditional down-sampling spatial beam measurements, we show that the compressed spatial multi-beam measurements via compressive sensing (CS) can lead to improved spatial-temporal beam prediction in HSR communications. In Section III, we investigate AI-based cell-level mobility management for HSR scenarios, including use cases, the input/output of the AI model, evaluation indicators, and enhanced schemes. Particularly, we demonstrate the performance gains of AI-assisted cell handover over traditional cell handover mechanisms for HSR scenarios. Section V introduces the challenges and opportunities encountered in AI-based mobility management, followed by our conclusions in Section VI."}, {"title": "II. AI-BASED BEAM-LEVEL MOBILITY MANAGEMENT FOR HSR COMMUNICATIONS", "content": "Beam-level mobility enables a user equipment (UE) to communicate with a BS via the optimal beam pair, which involves low-level beam operations rather than handover. In HSR communications, with some coarse-grained beam quality measurements of the serving cell, AI can be utilized to estimate the finely-grained optimal beam pair in the current instance or predict the optimal beam pair in future instances, which leads to reduced measurement overhead and enhanced throughput.\nThere are two primary use cases for AI models applied in beam-level mobility management, i.e., temporal beam prediction and spatial beam prediction. For temporal beam prediction, where the beam quality information of previous instances is utilized to predict the quality of beams in one or more future instances, most AI-based methods employ recurrent neural networks (RNNs) and their variants, such as LSTM and gated recurrent unit (GRU) models. This is driven by the ability of RNN models to identify temporal patterns in time series data, allowing them to gather extensive information. A two-layer convolutional LSTM network can be used for predicting the beam quality across spatial and temporal dimensions as detailed in [4]. The combination of a CNN and LSTM network was proposed in [5] for predicting the optimal beam ID across temporal dimensions. A multi-layer LSTM network was applied in [6] for forecasting the optimal future beam. Spatial beam prediction exploits spatial correlations among adjacent beams to estimate the quality of all beams using incomplete beam data. CNNs and networks with fully connected (FC) layers have been commonly employed for this purpose [7]\u2013[9]. Due to the high mobility in HSR communications, the extensive signaling overhead and the time required for frequent beam sweeping become prohibitive. For these reasons, temporal beam prediction offers greater benefits in terms of resource conservation and stable performance in HSR communications."}, {"title": "B. Inputs and Outputs in HSR Communications", "content": "Different tasks in beam-level mobility management require distinct input/output into/of the AI models in HSR communications. According to the ongoing discussions within 3GPP, there are two vital concepts for beam management, namely Set A and Set B. Set A comprises all narrow beams from the predefined codebook, while Set B encompasses the beams that have been measured and are utilized as inputs into the AI models. Various alternative methods exist with respect to the relationship between Set A and Set B. For instance, the AI model could receive the input of reference signal received power (RSRP) measurements of partial narrow beams, making Set Bas a subset of Set A. This is the most commonly used method in HSR communications, which greatly reduces the measurement overhead. Another option is to utilize RSRP measurements of wide beams to estimate the quality of narrow beams, where Set B and Set A are distinct. In addition, Set B can be the same as Set A, whenever the RSRP measurements of all narrow beams are the inputs into the AI models.\nNote that the first two types of inputs are employed in temporal and spatial beam prediction models, while the last is only for temporal beam prediction. Instead of using RSRP, the complex received signal can be used as input into the AI model, which will be discussed in the CS-based method in the next subsection. In addition, supplementary auxiliary information can be incorporated to improve the prediction accuracy for HSR communications. In [10], the position of the UE, its mobility vector, the signal-to-noise ratio (SNR), and the beam index currently in use were utilized to forecast the state of the link connection. Furthermore, measurements from light detection and range (LIDAR) and various sensors, images of the environment taken by cameras, and information from low-frequency channels could also be employed as extra inputs into the AI model to enhance the prediction accuracy.\nThere are two types of outputs in the AI model design for beam-level mobility management. The most common outputs are weights associated with different beams, where a higher weight suggests a better beam. Additionally, line-of-sight (LoS) is the primary mode of signal propagation in HSR communications and beam-selection is easier in the LoS setting. Therefore, the AI model can be utilized to output the estimated state of each path, i.e., LoS or non-LoS, to enhance the beam prediction [9], [11]."}, {"title": "C. AI Enhancement for HSR Communications", "content": "Beam-level mobility management enhancement involves three types of beam search methods: traditional methods including exhaustive search and hierarchical search, advanced numerical methods, and AI-based methods. The exhaustive beam search method examines all the candidate beams in the predefined codebook one by one to obtain beam quality information. The hierarchical beam search method provides a more efficient alternative, which first searches within a wide beam range and then further searches for a narrow beam within the angular range of the selected wide beam. However, traditional beam search algorithms suffer from a large training overhead, which is not suitable for HSR communications which require frequent beam switching. In addition, enhancement techniques, based on numerical methods have been proposed. More specifically, [12] exploited the tree search method for efficient beam training based on the hierarchical codebook, whilst [13] proposed an optimal beam training model based on Markov's decision process. These numerical approaches mainly target general communication environments, making it challenging to tailor them for particular communication scenarios.\nIn HSR communications, every train follows a predetermined path and performs the same routes periodically. Given sufficient training data, AI models are expected to perform exceptionally well in managing beam-level mobility owing to this intrinsic regularity. For instance, AI-based super-resolution techniques can be employed for spatial beam prediction to reduce the measurement overhead in HSR communications. Neural networks can receive inputs in the form of low-resolution beam measurements, consisting of partial narrow beams or wide beams, and output high-resolution spatial narrow beam predictions. The super-resolution-based spatial beam prediction can be implemented using a sub-pixel convolutional layer paired with an up-sampling layer [4], or a combination of CNN and FC [7].\nNevertheless, these existing beam prediction solutions rely on the premise that the assessed beams are selected from a predefined subset of all the beams, akin to the down-sampling process. One limitation of beam measurement through down-sampling is that it does not capture any information from the beams that are not sampled, leading to inevitable performance loss. Rather than discarding certain beams outright, a potential approach to reduce this performance loss is to use CS that projects the channel into a lower dimensional space. Given some CS-based beam measurements, deep learning (DL) has been exploited for spatial beam prediction [14].\nHere, we investigate DL-based spatial-temporal beam prediction using compressed multi-beam channel measurements. DL is employed to output the beam quality in the future slots using historical multi-beam channel measurements in the compressed domain, where the CS measurement matrix can be learned jointly. In the training phase, the complete beam channel information from historical instances serves as input into the model. The CS matrix serves as the first layer, namely the linear compression layer. The compressed channel information is fed into the remaining neural network to reconstruct the complete RSRP information of all candidate beams for a future instance. As a learnable parameter, the CS measurement matrix is jointly optimized during the training of the model. In the inference phase, the trained measurement matrices are applied at the transceivers and the compressed measurements are fed into the trained neural network to conduct beam prediction. Compared with the traditional sweeping method, the multi-beam compression offers to the beam prediction network richer information with the same number of beam measurements. The structure of the proposed Al model for beam-level mobility management, namely CSAI, is illustrated in Fig. 2(d).\nNow, we compare the performance of AI-based spatial-temporal beam prediction methods, whose structures are shown in in Fig. 2. The AI-based methods in Fig. 2(a) [6] and Fig. 2(b) [7] employ the traditional LSTM and CNN architecture, respectively, to predict the high-resolution beams. Compared with the CSAI in Fig. 2(d), the AI-based method in Fig. 2(c) [4] has the same network structure except for the beam measurement module. In the experiments, we consider the UMa scenario for outdoor 30 GHz mmWave communications, while the UE follows a straight-line trajectory which aligns with the HSR scenario. Four distinct datasets, representing UE speeds of 60km/h, 120km/h, 350km/h and 500km/h, are utilized in the evaluation. These datasets comprise 40,000 training samples, 5,000 validation samples, and 5,000 test samples. The batch size is set to 256, and the training runs for a maximum of 800 epochs. The learning rate changes dynamically from $10^{-3}$ to $10^{-8}$, utilizing the adaptive moment estimation optimizer. We consider the prediction accuracy for the best beam with a compression ratio\u00b9 of $\\frac{1}{2}$. The simple non-AI approach straightforwardly uses the best one of the partially measured present beams as the prediction. As shown in Fig. 3, the AI-based spatial-temporal beam prediction methods outperform significantly the traditional non-AI method. As the speed of the movement increases, all methods generally show an anticipated decrease in the prediction accuracy. The CSAI demonstrates superior performance for all the cases. At the speed of 500km/h, the CSAI is still able to yield a prediction accuracy of 83.76%."}, {"title": "III. AI-BASED CELL-LEVEL MOBILITY MANAGEMENT IN HSR COMMUNICATIONS", "content": "Cell-level mobility management deals primarily with cell handovers, initiating the transition of a user's session to a new\n\u00b9 The compression ratio is defined as the ratio of the number of input beam measurements to the number of all beam measurements."}, {"title": "A. Use Cases in HSR Communications", "content": "cell when the signal strength from the serving cell falls below a predefined threshold. Given the rapid movement in HSR communications, cell handovers occur frequently, and failures in these handovers would unavoidable compromise the user experience. As specified in 3GPP, the cell handover procedure relies on concrete criteria, such as event A3, which will be triggered when the difference between the layer 3 RSRP (L3-RSRP)\u00b2 measurement of the target cell and the L3-RSRP measurement of the source cell exceeds a predefined offset. If the above criteria are met during the time to trigger (TTT) duration, the UE reports the measurements and the source cell initiates the handover preparation phase. Afterwards, the cell handover procedure initiates the execution after the UE receives the handover command message, whose task is to provide radio resource control (RRC) reconfiguration message to the target cell. The specific cell handover process is shown in Fig. 4. During state 1, the UE must sweep across all beams to acquire the L3-RSRP of adjacent cells for the TTT duration, leading to signal quality deterioration in the service cell, which are even more pronounced in HSR scenarios.\nTo maintain a stable communication quality for HSR communications, the most common use case for AI-based cell-level mobility management is to predict the RSRP of all adjacent cells in future TTT instances. The AI-based cell handover process is illustrated in Fig. 4, where option 1 and option 2 represent different deployment methods of AI models. In this case, one option for the source cell is to immediately execute a cell handover based on the AI model's predicted target cell. Alternatively, the source cell can monitor the signal quality of both the target and source cells over a period of time and initiate the handover if some handover conditions are still met. This approach can effectively reduce the ping-pong handovers and enhance the communication stability, while it requires more handover time for signal detection. In HSR scenarios, where trains move in straight lines and the probability of ping-pong handovers is low, the delay introduced by extended handover time, resulting in significant communication quality degradation, becomes intolerable. Therefore, the first approach is better suited for cell-level mobility management in HSR communications."}, {"title": "B. Inputs, Outputs, and KPIs in HSR Communications", "content": "For the HSR scenario, we explore various model inputs and outputs tailored to its specific requirements. One input option for Al models is the L3-RSRP of each cell at historical instances, with the output being either the L3-RSRP of each cell or the L3-RSRP of the top K cells at future TTT instances. The dimension of L3-RSRP information is lower than that of L1-RSRP. However, acquiring L3-RSRP, which requires averaging data over multiple time slots, introduces extra latency which is not desired in HSR communications. Another viable approach involves using layer 1 RSRP (L1-RSRP)\u00b3 as both the input and the output of the AI models.\n\u00b2L3-RSRP represents the cell-level RSRP, obtained by averaging or maximizing the RSRP of all beam pairs in each cell.  \u00b3L1-RSRP represents the beam-level RSRP, which is the RSRP of each beam pair of each cell.\nThe AI model output is then filtered to derive the L3-RSRP, which is used to execute cell handovers. For this option, the output L1-RSRP of AI model can be applied to both beam-level and cell-level mobility management. Nevertheless, L1-RSRP encompasses extensive information that exceeds the requirements for handover, while the correlation between the input and output L1-RSRP is intricate, thereby amplifying the learning challenge for the AI models. Moreover, leveraging L1-RSRP to forecast L3-RSRP via AI can be an advantageous strategy. This method reaps the benefits of the above two approaches, making use of the abundant data in L1-RSRP to enhance the AI model's prediction accuracy, while also streamlining the mapping between the model's input and output.\nFor the evaluation of AI-based cell-level mobility management, we should not only focus on the prediction accuracy of RSRP, such as the normalized mean squared error (NMSE) or RSRP difference between the actual RSRP and the predicted RSRP, but we also need to pay more attention to the following system-level KPIs which are important for HSR communications.\nRLF rates: Radio link failure (RLF) occurs when the signal-to-interference-plus-noise ratio (SINR) stays consistently small for an extended duration. In the HSR scenarios, the ultra-high moving speed necessitates timely handovers to mitigate SINR attenuation and reduce RLF rates. The RLF rates can be used to evaluate the timeliness of the cell handover during train operation.\nHandover failure (HOF) rates: Handover failure usually happens due to too late handover, too early handover and handover to a wrong cell. The HOF rates is calculated as the total number of HOFs divided by the sum of the total number of HOFs and the total number of successful handover. The HOF rates is one indicator on the reliability of the HSR communication system.\nData interruption: The time interval between the transmission of the last data packet from the source BS and the reception of the first data packet from the target BS is referred to as data interruption. The low data interruption guarantees the timeliness of signal quality for high-speed trains."}, {"title": "C. AI Enhancement for HSR Communications", "content": "The traditional cell handover mechanism including 3 states is described in Fig. 4. In state 1, the UE measures the beam quality and sends the measurement report to the source BS. In this state, the UE remains connected to the source cell. When the mobility of the UE is low or the UE is moving within a macro cell, the above handover mechanism can sustain good performance. However, when the mobility of UE is high or the UE is moving across micro cells of high density, the traditional handover mechanism would result in unintended events, e.g., HOF, RLF, ping-pong phenomenon, throughput loss and too early/late handover. In HSR communications, the trains move at high speed, resulting in a sharp decline of the communication quality of the link between the UE and the source BS during state 1. Leveraging its robust environmental awareness and learning abilities, AI/ML technology can address the aforementioned issues.\nOne approach for AI-based cell-level mobility management is to exploit the beam-level outputs of different cells. In particular, Al models are deployed at the network side, and each cell employs a unique AI model to predict the future RSRP of this cell. The shortcomings of this cell-level mobility management approach lie in the high deployment and management overhead of AI models. Another approach is to use an AI model to predict the future RSRP of multiple adjacent cells of the same type based on RSRP measurement information. In this context, [15] utilized a multi-layer LSTM architecture to predict the optimal BS index as the candidate for handover. Moreover, [3] developed a CNN-based auto-encoder to reduce the data dimension, and then a LSTM network was used to conduct link quality prediction for each cell. In literature, the input into these AI models is the RSRP of beam pairs of all cells, leading to significant measurement overhead and computational complexity.\nTo this end, we consider two approaches to reduce the measurement overhead for cell-level mobility management. For the first approach, all beams from partially selected cells are used to predict the L3-RSRP for all cells. For the second approach, partially selected beams of each cell are used to predict the L3-RSRP for all cells. We utilize the partial L1-RSRP from the past six time instances to predict the L3-RSRP in the next four time instances. We employ CNN-based and LSTM-based models as shown in Fig. 5. The LSTM-based model comprises four LSTM layers and each layer has 168 hidden units. The features extracted by the LSTM network undergo further processing via an FC layer and a sigmoid function for feature mapping. To enhance the performance of the LSTM network, we adopt four identical networks to predict L3-RSRP for four future instances separately. For the CNN-based model, a set of regular 2D convolution layers with 12, 24 and 32 kernels are employed before feeding it into two FC layers. To avoid overfitting during the training process, max pooling layers are incorporated after each CNN layer.\nThe channel used in our experiments is generated by QuaDRiGa, where we consider the UMa scenario and center frequency 30 GHz. We deploy 7 BSs, and each BS is equipped with three half-wavelength spaced uniform plane arrays (UPAs) with sector orientation of 30, 150 and -90 degrees. We generate four datasets corresponding to various UE speeds of 60 km/h, 120 km/h, 350 km/h, and 500 km/h. Each dataset comprises 50,000 samples, which are split into training, validation, and test sets in a 3:1:1 ratio. For the proposed two approaches with equidistant sampling, L1-RSRP of 50% beams in each cell or all beams in 50% cells are selected as input to the AI model. The batch size is 256 and the maximum epoch is 800. The learning rate dynamically changes from $10^{-3}$ to $10^{-8}$, utilizing the adaptive moment estimation optimizer. The RSRP difference is one evaluation metric of prediction accuracy for cell-level mobility management, which reflects the disparity between the predicted L3-RSRP and the actual L3-RSRP of all cells. In addition, we also consider the RLF rates as the system-level evaluation metric to illustrate the overall handover performance of the communication system.\nThe RSRP difference between the AI predictions and the actual measurements is shown in Fig. 6. The experimental results show that both CNN-based and LSTM-based models show excellent cell-level temporal prediction performance. Table II shows the significantly reduced RLF rates in AI-assisted cell handover processes. Remarkably, the proposed two approaches to reduce the measurement overhead for cell-level beam management, labeled as Part_Cell and Part_Beam, achieve comparable RLF performance with the traditional approach using all beam measurements of all cells, labeled as All_Beam_Cell, while we can save 50% beam measurement overhead for HSR communications. Furthermore, as shown in Table II, the flops and total parameters of the CNN-based model are much lower than those of the LSTM-based model."}, {"title": "IV. OPPORTUNITIES AND CHALLENGES OF AI-BASED MOBILITY MANAGEMENT FOR HSR COMMUNICATIONS", "content": "Due to the high mobility in HSR communications, the randomness of inter-cell interference constitutes a crucial challenge. Specifically, mmWave beams from neighboring cells directed at local terminals can cause severe inter-cell interference, while frequent cell handovers and beam switching exacerbate the randomness of this interference. Therefore, agile interference management is crucial to ensure reliable service in HSR communications. As a potential solution, graph neural networks (GNNs) can intrinsically match the topology of wireless networks and can be used to effectively handle dynamic interference management.\nIn HSR environments, the rapid movement necessitates more adaptable mobility management strategies to effectively handle frequent environmental changes. Relying solely on RSRP variations is insufficient for ensuring communication stability during handover decisions. Therefore, a multi-criteria decision-making mechanism is imperative. This mechanism could integrate factors, such as signal interference levels, network load conditions, terminal velocity, and direction of movement, along with RSRP variations as inputs for AI models to obtain better handover decisions. This would greatly improve the accuracy and reliability, ensuring seamless communication for high-speed trains.\nIn practical scenarios, the AI-assisted mobility management methods require high energy dissipation due to the high computing complexity, so it is more meaningful to deploy these AI models on the network side. In this case, the UE reports measurement data to the source BS, which feeds the measurements into the AI/ML models for mobility management. Moreover, the transmission entails extra latency while the unavoidable transmission errors, occurring in reporting these measurements, affect the performance of AI models. Then, it would be beneficial to deploy AI models on the UE side. We can consider using CS or training lightweight AI models to reduce the computational complexity and hardware requirements for UEs.\nAI/ML models designed for specific scenarios would usually have poor generalization performance. Altering the settings in communication scenarios, e.g., BS height, channel model, cell size and UE speed, might lead to a significant deterioration of the model performance. Therefore, one challenge for AI-based mobility management for HSR communications is the design of AI models with good scenario generalization properties. Generally, there exists a trade-off between model generalization and model performance. Improving the generalization of the model, e.g., training the model with a mixed datasets from multiple scenarios, will introduce difficulties for the training of AI/ML models, as more complex and diverse knowledge are embedded in the data. As a result, the balance between model generalization and prediction accuracy according to the actual application scenario needs to be thoroughly examined in future discussions."}, {"title": "V. CONCLUSION", "content": "In this article, we investigated the fundamental issue of AI-based beam-level and cell-level mobility management for HSR communications, including the use cases, inputs, outputs and KPIs of AI models. For beam-level mobility management in HSR communications, a hybrid approach, combining AI and CS, demonstrated superior performance compared to conventional AI-based methods with the same measurement overhead. For cell-level mobility management in HSR communications, we unveiled the significant gain of AI-based methods for cell handover, while the measurement overhead can be reduced by using partial beam measurements, without compromise of the RLF rates. At last, the opportunities and challenges on AI-based mobility management are briefly discussed for further studies."}]}