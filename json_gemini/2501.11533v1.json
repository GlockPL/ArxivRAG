{"title": "The impact of intrinsic rewards on exploration in Reinforcement Learning", "authors": ["Aya Kayal", "Eduardo Pignatelli", "Laura Toni"], "abstract": "One of the open challenges in Reinforcement Learning (RL) is the hard exploration problem in sparse reward environments. Various types of intrinsic rewards have been proposed to address this challenge by pushing towards diversity. This diversity might be imposed at different levels, favouring the agent to explore different states, policies or behaviours (State, Policy and Skill level diversity, respectively). However, the impact of diversity on the agent's behaviour remains unclear. In this work, we aim to fill this gap by studying the effect of different levels of diversity imposed by intrinsic rewards on the exploration patterns of RL agents. We select four intrinsic rewards (State Count, Intrinsic Curiosity Module (ICM), Maximum Entropy, and Diversity is all you need (DIAYN)), each pushing for a different diversity level. We conduct an empirical study on MiniGrid environment to compare their impact on exploration considering various metrics related to the agent's exploration, namely: episodic return, observation coverage, agent's position coverage, policy entropy, and timeframes to reach the sparse reward. The main outcome of the study is that State Count leads to the best exploration performance in the case of low-dimensional observations. However, in the case of RGB observations, the performance of State Count is highly degraded mostly due to representation learning challenges. Conversely, Maximum Entropy is less impacted, resulting in a more robust exploration, despite being not always optimal. Lastly, our empirical study revealed that learning diverse skills with DIAYN, often linked to improved robustness and generalisation, does not promote exploration in MiniGrid environments. This is because: i) learning the skill space itself can be challenging, and ii) exploration within the skill space prioritises differentiating between behaviours rather than achieving uniform state visitation.", "sections": [{"title": "1 Introduction", "content": "The sparsity of rewards is a major hurdle for RL algorithms [1, 2]. With infrequent feedback, the probability of the agent randomly discovering a rewarding sequence of actions becomes low. Therefore, a large number of samples is needed to explore and stumble into a successful sequence of actions leading to the desired outcome [3]. This is known as the hard exploration problem [1]. Classical exploration strategies, e.g., epsilon-greedy and Boltzmann distribution [4] fail to explore the environment efficiently enough to find the optimal solution when the feedback is sparse [5]. Among the possible solutions to address this limitation [1, 6, 7], intrinsic rewards [8, 9] have been proposed. They are a part of the larger notion of intrinsic motivation defined by Ryan and Deci [10] as the tendency to \u201cseek out novelty and challenges, to extend and exercise one's capacity, to explore, and to learn\". Intrinsic rewards are often categorised in the literature into: knowledge-based and competence-based [7, 11-13]. The first category encourages the agent to gain new knowledge about the environment. It compares the agent's experiences to its existing knowledge, and rewards the agent for encountering unexpected situations. This includes methods that reward novelty in states or state transitions [14-17], the prediction error [18] or the information gain [19]. The second category, also called \u201cskill learning\" in Aubret et al. [7, 20], rewards the agent for learning a diverse repertoire of skills in an unsupervised way. It mainly includes goal-conditioned RL approaches, which generate and achieve their own goals to explore the environment [21-23]. In Colas et al. [12], a detailed survey on goal-conditioned RL is presented, highlighting the different types of goal representations and goal-sampling strategies.\nThis categorisation uncovers a potential link between diversity and exploration, where intrinsic rewards promote diverse agent behaviours to efficiently explore the environment. While diversity is acknowledged as crucial in RL, it has mainly been explored in relation to robustness, generalisation, hierarchical learning or generation tasks [24-31]. However, its role in driving effective exploration remains underexplored and not empirically validated yet. In this work, we take an initial step toward understanding whether mechanisms that encourage diversity through skill discovery can also drive more effective exploration. To address this gap, we propose a rigorous methodology to empirically compare knowledge-based and competence-based intrinsic rewards, which has not been thoroughly investigated in prior research. Our work focuses on examining how different levels of diversity impact exploration, driven by the need to address the following open questions: i) What is, in practice, an effective exploration in environments with low- and high-dimensional state spaces? ii) How does the level of diversity imposed by intrinsic rewards affect exploration performance across different scenarios? iii) Does behavioural diversity through skill discovery, known to help robustness and fast adaptation [26, 27], also helps exploration?\nOur contributions are twofold:\n1. We design an empirical study that categorises intrinsic rewards based on their level of diversity and assesses how these categories impact exploration using various metrics in a controlled setting."}, {"title": "2 Related Works", "content": "While numerous intrinsic reward formulations have been proposed to address complex sparse-reward tasks, a comprehensive understanding of their comparative advantages and challenges remains elusive, leaving this an open question in the field. Here, we review previous works that have attempted to categorise or empirically compare intrinsic rewards. Existing surveys [1, 6, 7, 12, 13, 20, 33] offer slightly different taxonomies of intrinsic rewards, often using varied terminology. However, most include two broad categories: one focused on increasing knowledge about the environment (e.g., prediction error, information gain, learning progress, and state novelty), and another focused on learning diverse skills. Yet, these surveys lack empirical validation and none of them explore the different levels of diversity that these intrinsic rewards can introduce within each category. In this work, we build on the categorisation proposed by [12], which clearly distinguishes between knowledge-based and competence-based intrinsic rewards, and we further subdivide them into different diversity levels (state/dynamics/policy).\nWe are now interested in the works provided in the literature aimed at benchmarking different intrinsic rewards. A few studies have compared methods within the knowledge-based category. For instance, Andres et al. [34] compared State Count [35], Random Network Distillation (RND) [16], Intrinsic Curiosity Module (ICM) [18], Reward Impact Driven Exploration (RIDE) [36] on MiniGrid environment. The study aimed to evaluate the impact that weighting intrinsic rewards has on performance, as well as the effect of using different neural network architectures. Another study by Taiga et al. [37] evaluated pseudo-counts [14], RND, ICM and Noisy Networks [38] within the Arcade Learning Environment (ALE) [39], and suggested that none of these methods outperform the epsilon-greedy exploration. A more recent work by Yuan et al. [40] introduced RLeXplore, a comprehensive plug-and-play framework that"}, {"title": "3 Methodology", "content": "In the following, we sub-classify the knowledge and competence-based intrinsic reward methods according to the level of diversity they impose on the agent's exploration (Section 3.1). Then, we select four intrinsic rewards, one for each level (Section 3.2), and we test them empirically on MiniGrid environment, explained and motivated in Section 3.3. Section 3.4 outlines the experimental protocol used in the study, while Section 3.5 details the model architecture. Finally, Section 3.6 introduces the evaluation metrics.\nWe systematise the type of diversity imposed by intrinsic rewards into four levels: State level diversity encourages exploration of unseen states, pushing the agent towards areas where its knowledge is most limited. State + Dynamics level diversity also focuses on diverse states, but additionally considers the novelty of the dynamics between those states for a more comprehensive exploration. Policy level diversity explores the impact of different actions from given states, while Skill level diversity explores the effectiveness of diverse skills (policy-goal association) in achieving goals [12]. For a more detailed description of these diversity levels, please refer to Appendix A.\nWe augment the task reward with an intrinsic reward such that the total reward becomes: $r_{total} = r_{ext} + \\beta * r_{int}$, where $r_{ext}$ is the extrinsic reward, $r_{int}$ is the intrinsic reward and $\\beta$ is the intrinsic reward coefficient [1]. The best-performing $\\beta$ values, either sourced from the literature [34] or determined through a grid search (details provided in Appendix C), are presented in Table 3, also located in Appendix C. We select four different intrinsic reward methods, each representative of one of the four diversity levels:\nState Count (State level diversity) builds an intrinsic reward inversely proportional to the state visitation count [35]. For a transition $(s_t, a_t, s_{t+1})$, where $s_t$ is the current state, $a_t$ is the current action and $s_{t+1}$ is the next state, $r_{int}(t) = \\frac{1}{\\sqrt{N(s_{t+1})}}$ with $N(s_{t+1})$ being the number of times the state $s_{t+1}$ has been visited so far during training. This algorithm considers only discrete, low dimensional state space. However, for RGB observations, where the state space is much larger and State Count is not feasible, we use SimHash [15] to hash states before counting them. SimHash maps the pixel observations to hash codes according to the following equation, with h as the hashing function: $h(s_{t+1}) = sgn(A * \\phi(s_{t+1})) \\in \\{-1,1\\}^k$. Here, $\\phi$ is an embedding function, A is a matrix with i.i.d entries drawn from a standard normal distribution, k is the sise of the hashed key, and $sgn(\\cdot)$ maps a number to its sign. Then, the same intrinsic reward formula is applied but using the hashed observation: $r_{int}(t) = \\frac{1}{\\sqrt{N(h(s_{t+1}))}}$.\nIntrinsic Curiosity Module (ICM) (State + Dynamics level diversity) uses curiosity as intrinsic reward. Curiosity is formulated as the error in the agent's ability"}, {"title": "4 Experimental results and discussion", "content": "We discuss the following three questions to analyse the performance of the exploration algorithms:\nRQ1: Do different intrinsic rewards lead to different return performance/sample efficiency for both grid encodings and RGB partial observations?\nRQ2: What are the characteristics (strengths/weaknesses) of each intrinsic reward method, and what are the practical recommendations to select intrinsic rewards?\nRQ3: How different intrinsic rewards impact efficiency in discovering the sparse reward? Is there any link with credit assignment?\nIn terms of episodic return, State Count has the best performance with low-dimensional observations (grid encodings) on all environments (See column 1 of Figure 3). It converges to the maximum return with the least number of frames. In the case of DoorKey 16x16, where many algorithms, including PPO, Max Entropy, and DIAYN, struggle to solve the task, State Count emerges as the top performer, successfully obtaining the key and attaining the highest return. Following closely, ICM"}, {"title": "5 Conclusions", "content": "In this work, we have re-interpreted intrinsic reward techniques in the literature using a diversity perspective (State, State + Dynamics, Policy and Skill levels of diversity). We conducted empirical studies on MiniGrid, to understand the differences between these diversity levels in a partially observable and procedurally generated framework. We found that the homogeneity of the state coverage imposed by State Count (representing State level diversity) has led to the best sample efficiency on many MiniGrid tasks. State level diversity improves the convergence speed in strategical tasks, covers well the state space and leads to a fast decrease of policy entropy and intrinsic reward. This decreasing rate of the intrinsic reward aligns well with finding the optimal policy which avoided the dominance of the intrinsic reward. However, State level diversity is fragile and requires good state representations, while entropy maximisation seems to be slightly more robust when dealing with image-based observations. Learning good state representations is challenging, so entropy maximisation is a practical alternative. Moreover, DIAYN (representing Skill level diversity) struggles with exploration in MiniGrid due to the difficulty of learning the skill space and exploring within it, in a procedurally generated partially observable setting."}, {"title": "5.1 Limitations and Future works", "content": "This study serves as an initial exploration into the relationship between exploration and diversity imposed by intrinsic rewards. While we provide insights into this relationship, several limitations remain to be addressed in future work. Firstly, we examine only one representative intrinsic reward method for each level of diversity. This choice may not capture the full range of behaviours within each category, potentially limiting the generalisability of our findings. Expanding this work to benchmark a broader selection of intrinsic reward methods would improve the applicability of our results. Additionally, the effectiveness of intrinsic rewards is closely tied to the environment in which they are applied. Our experiments are restricted to the MiniGrid environment, specifically using grid encodings and RGB observations. Future studies could benefit from exploring more complex and varied environments, such as Mujoco [61], Atari [39], MiniHack [47], and MiniMax (Autocurricula) [62], where the impacts of different diversity levels might yield more distinct behaviours. Some intrinsic reward methods may excel in certain environments but perform poorly in others. Thus, identifying conditions under which each intrinsic reward method performs best across diverse environments would be a valuable contribution. Moreover, while diversity can enhance exploration, it may also impede performance as discussed in [63] in a phenomenon names the curse of diversity. Therefore, pinpointing the conditions under which diversity aids rather than hinders performance or developing strategies to counterbalance the potential negative effects of diversity-remains an open research question. For the competence-based category, we employed DIAYN, a method that learns a skill space autonomously. Other goal-conditioned approaches, such as those learning different goal representations [64], predefining goal abstractions [65], or employing"}, {"title": "Appendix A Diversity levels Categorisation", "content": "We divide intrinsic rewards into two categories: \"Where to explore\" and \"How to explore?\", as described in the following and shown in Figure 7."}, {"title": "Appendix A.1 \u201cWhere to explore?\u201d", "content": "In this subcategory, we collect all the works which encourage the exploration of unseen states. The most common method is \"State Count\", which stores the visitation count of each state, and gives high intrinsic rewards to encourage revisiting states with low counts [35, 69, 70]. While counting works well in tabular cases, it becomes difficult in vast state spaces. Several methods were proposed to extend State Count to large or continuous state spaces, such as pseudo-counts [14] and hashing [15]. Besides count-based methods, features prediction error can be used as a measure of the state novelty. For example, in [16], authors assessed state novelty by distilling a fixed randomly initialised neural network (target network) into another neural network (predictor network) trained on the data collected by the agent. This technique is called Random Network Distillation (RND), and the main motivation behind it is that the prediction error should be small for frequently visited states. Similarly, the NovelD algorithm [71] uses RND as a measure of state novelty but it defines the intrinsic reward as the difference in RND prediction errors at two consecutive states $s_t$ and $s_{t+1}$ in a trajectory."}, {"title": "Appendix A.2 \u201cHow to explore?\u201d", "content": "Algorithms in this subcategory aim to explore diverse actions from the same state. What makes it different from the State + Dynamics algorithms introduced in Appendix A.1 is that the previous category uses knowledge about states and dynamics of the environment, and pushes for exploring the areas where the agent knows the least (high uncertainty). In contrast, this level of diversity considers the previous exploration behaviour represented by the policy (how the agent has explored) and pushes it to explore differently, inducing diversity on the policy learned. For example, in Max-imum Entropy RL (Max Entropy), the aim is to learn the optimal behaviour while acting as randomly as possible. The objective function becomes the sum of expected rewards and conditional action entropy [84]. Soft-Actor Critic (SAC) [50] is a pop-ular RL algorithm implementing the Max Entropy RL framework. Diversity-driven exploration strategy [85] is another exploration technique that encourages the agent to behave differently in similar states. It maximises the divergence between the cur-rent policy and prior polices. Similarly, Adversarially Guided Actor-Critic (AGAC)"}, {"title": "Appendix B MiniGrid Environments", "content": "We use the following MiniGrid environments shown in Figure 8:\n1. Empty: This is an empty grid, where the agent is always placed in the corner opposite to the goal. The task is to get to the green goal square. We use the regular variant \u201cMiniGrid-Empty-16x16-v0\u201d.\n2. DoorKey: This is a sparse reward environment which requires a certain order of visiting the states to solve the task; the agent needs to pick up the key, open the door then get to the green goal square. It does not get any reward after picking up the key or unlocking the door; it gets rewarded just at the end of the task. We use \u201cMiniGrid-DoorKey-16x16-v0\" in the case of grid encodings and \"MiniGrid-DoorKey-8x8-v0\" in the case of RGB observations."}, {"title": "Observation and action spaces:", "content": "The observations are egocentric and partially observable. We considered first the grid encoding observations of size 7 \u00d7 7 \u00d7 3. The first two dimensions (7 \u00d7 7) compose the tile set, and the last dimension encode the object type (wall, door, ...), the object colour (red, green, \u2026) and the object status (door open, door closed, door locked). Specifically, object type \u2208 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, object colour \u2208 {0,1, 2, 3, 4, 5}, and object status \u2208 {0,1,2}. Then, we used partial RGB visual observations of size 56 x 56 x 3 (7 tiles of 8 \u00d7 8 pixels each) to increase the complexity of the task since agents must extract features directly from the images. There are 7 actions available to the agent: turn left, turn right, move forward, pick up an object, drop an object, toggle and done. Some of these actions are unused in certain tasks."}, {"title": "Appendix C Hyperparameters", "content": "For State Count, and ICM, we use the hyperparameters of the previous study [34]. Since Max Entropy + PPO and DIAYN were not tested before on MiniGrid, we run a grid search over $\u03b2\u2208 [0.1,0.01, 0.001, 0.0005]$ and pick the best values of \u03b2 which result in the highest return during training. The chosen values of \u03b2 are summarised in Table 3. For DIAYN, we choose to train 10 skills, which is the number used in the study by [90], and we use a discriminator learning rate of 3 \u00d7 e-4 following the implementation of DIAYN paper [21] (Table 2). Note that we reused the same hyperparameters for the second part where we tested on RGB observations."}, {"title": "Appendix D Additional experimental results", "content": ""}, {"title": "Appendix D.1 Grid Encoding Observation space", "content": ""}, {"title": "Appendix D.2 RGB Observation space", "content": ""}, {"title": "Appendix E DIAYN Extrinsic", "content": "Initially, we evaluated DIAYN combined with extrinsic rewards, but it did not perform well because of the imbalance between discriminability and reward maximisation"}]}