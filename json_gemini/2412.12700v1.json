{"title": "ParMod: A Parallel and Modular Framework for Learning Non-Markovian Tasks", "authors": ["Ruixuan Miao", "Xu Lu", "Cong Tian", "Bin Yu", "Zhenhua Duan"], "abstract": "The commonly used Reinforcement Learning (RL) model, MDPs (Markov Decision Processes), has a basic premise that rewards depend on the current state and action only. However, many real-world tasks are non-Markovian, which has long-term memory and dependency. The reward sparseness problem is further amplified in non-Markovian scenarios. Hence learning a non-Markovian task (NMT) is inherently more difficult than learning a Markovian one. In this paper, we propose a novel Parallel and Modular RL framework, ParMod, specifically for learning NMTs specified by temporal logic. With the aid of formal techniques, the NMT is modulaized into a series of sub-tasks based on the automaton structure (equivalent to its temporal logic counterpart). On this basis, sub-tasks will be trained by a group of agents in a parallel fashion, with one agent handling one sub-task. Besides parallel training, the core of ParMod lies in: a flexible classification method for modularizing the NMT, and an effective reward shaping method for improving the sample efficiency. A comprehensive evaluation is conducted on several challenging benchmark problems with respect to various metrics. The experimental results show that ParMod achieves superior performance over other relevant studies. Our work thus provides a good synergy among RL, NMT and temporal logic.", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning (RL) is a well-known paradigm for autonomous decision-making in complex and unknown environments Sutton & Barto (2018). It is used to help agents learn a policy, actually a strategy in pursuit of goals, from trial-and-error experiences towards maximizing long-term rewards. RL, or deep RL, achieves remarkable success in a wide range of tasks, from games Silver et al. (2016) to real-world context Kalashnikov et al. (2018). An important mathematical foundation for RL is Markov Decision Processes (MDPs). In MDPs, the accumulation of rewards relies only on the current state and action. However, there are many real-world tasks whose rewards are in response to more complex behaviors that is reflected over historical states and actions. Such rewards are often known as non-Markovian rewards, and tasks with these rewards are called non-Markovian tasks (NMTs). For instance, an autonomous taxi will receive reward for picking up passengers and subsequently delivering them to their destinations. An RL agent that attempts to learn NMTs without realizing that they are non-Markovian will display sub-optimal behaviors.\nLearning on NMTs has gained a lot of attractions recently, and a number of different solution methods are explored. NMTs are common in real-world applications, while state-of-the-art RL algorithms have limited success on them due to the non-Markovian feature. We address the challenge of ensuring non-Markovian requirements in RL from a formal methods perspective. To this end, we adopt temporal logic as an unambiguous specification language of what non-Markovian mean.\nIn this paper, we propose a novel RL framework for parallel training NMTs, introducing a task modularization unit into the traditional RL setting. Sub-tasks are computed up-front from the given task specification by the unit. In addition, it is essential to design a suitable and dense reward, especially for NMTs. To deal with this challenging problem, our work devises an automatic reward shaping technique to guide the agent towards the fulfilment of task specification.\nThe remainder of the paper starts with a detailed discussion of related work. Section 3 gives a review of the basic concepts and notations used in this paper. Section 4 elaborates our approach, mainly in how to decompose a non-Markovian task and how to train the task in parallel. Section 5 demonstrate the optimality and correctness of our approach. In Section 5, we provide experimental evaluations on benchmark problems to show the performance of our approach. Finally, we draw conclusions and puts forward the future research directions. All of our codes and data are open-sourced and provided in the GitHub repository\u00b9."}, {"title": "2. Related work", "content": null}, {"title": "2.1. Solutions for Non-Markovian Tasks", "content": "The work Camacho et al. (2018) presents a means of specifying non-Markovian rewards, expressed in LTLf (Linear Temporal Logic over finite traces) De Giacomo & Vardi (2013). Non-Markovian reward functions are put into automata representations, and reshaped automata-based rewards are exploited by off-the-shelf MDP planners to guide search. This approach is quite limited since it cannot be extended to continuous domains. A more general form of NMTs is proposed in Icarte et al. (2022), called reward machines which are inherently a special kind of finite state automaton. Different methodologies based on Q-learning and hierarchical RL, to cooperate with reward machines, are presented, including automated reward shaping, task decomposition, and counterfactual reasoning for data augmentation. The authors find a primary drawback that reward shaping in their approach does not help in continuous domains. Focusing on specifying NMTs with multiple sub-objectives and safety constraints, Jothimurugan et al. (2019) propose a simpler language for formally specifying NMTs and an algorithm called SPECTRL to learn policies of NMTs. Based on quantitative semantics of their specification language, SPECTRL is able to automatically generate reward function and reshape rewards. Although experiments demonstrate that SPECTRL can outperform the state-of-the-art baselines at the time, the complex reward shaping mechanism makes it difficult to extend to other RL algorithms.\nHasanbeig et al. (2020) investigate a deep RL method for policy synthesis in continuous-state/action unknown environments under requirements expressed in LTL. An LTL specification is converted to a Limit Deterministic B\u00fcchi Automaton (LDBA) and synchronised on-the-fly with the agent/environment. A modular Deep Deterministic Policy Gradient (DDPG) architecture is proposed to generate a low-level control policy that maximizes the probability of the given LTL specification. The synchronisation process automatically modularizes a complex global task into sub-tasks in order to deal with the sparse reward problem. Voloshin et al. (2022) study the problem of policy optimization with LTL constraints. A learning algorithm is derived based on a reduction from the product of MDP and LDBA to a reach-ability problem. This approach enjoys a sample complexity analysis for guaranteeing task satisfaction and cost optimality. However, continuous state and action spaces are beyond the capabilities of this approach. An RL framework is presented in Bozkurt et al. (2020) to synthesise a control policy from a product of LDBA and MDPs. The primary contribution of the framework is to introduce a novel rewarding and discounting scheme based on the B\u00fcchi acceptance condition. Motion planning case studies solved by a basic Q-learning implementation are given to show the optimization of the satisfaction probability of the B\u00fcchi objective.\nBy studying the satisfaction of temporal properties on unknown stochastic processes with continuous state space, Kazemi & Soudjani (2020) propose a sequential learning procedure on the basis of a path-dependent reward function. The positive standard form of the LTL specification forms the reward function which is maximized via the learning procedure. Oura et al. (2020) present a method for the synthesis of a policy satisfying a control specification described by an LTL formula, which is converted to an augmented Limit-Deterministic Generalized B\u00fcchi Automaton (LDGBA). A product of augmented LDGBA and MDP is generated, based on which a reward function is defined to relax the sparsity of rewards. Moreover, LTL is considered as strict and hard constraints to ensure safe RL Konda & Tsitsiklis (1999); Cohen et al. (2023)."}, {"title": "2.2. Distributed Reinforcement Learning", "content": "Nair et al. (2015) present the first massively distributed architecture Gorila for deep RL, which distributes DQN (Deep Q-Network) Mnih et al. (2013) agents across multiple machines, and uses asynchronous SGD for a centralized Q function learning. The distribution of learning process is intensively studied in A3C Mnih et al. (2016), which introduces a conceptually simple and lightweight framework, enabling multiple actor-learners compute gradients and update shared parameters asynchronously. Inspired by A3C, Espeholt et al. (2018) develop a highly scalable distributed framework IMPALA, which scales to a substantial number of machines without sacrificing data efficiency or resource utilisation. IMPALA achieves better performance than previous works with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach. Heess et al. (2017) explore how a rich environment help to promote the learning of complex behavior. The authors propose DPPO, a distributed version of PPO (Proximal Policy Optimization) Schulman et al. (2017), for training various locomotion skills in a diverse set of environments. Ape-X is a value-based distributed architecture similar to Gorila Horgan et al. (2018). It aims to reduce variance and accelerate convergence, and benefits from importance sampling and prioritized experience replay. Built upon Ape-X, R2D2 adapts recurrent experience replay for RNN-based DQN agents Kapturowski et al. (2018). Li et al. (2023) propose a Parallel Q-Learning scheme for massively parallel GPU-based simulation, including data collection, policy function learning, and value function learning on GPU. Based on a novel abstraction of RL training data flows, a scalable, efficient, and extensible distributed RL system ReaLly Scalable RL is developed Mei et al. (2023)."}, {"title": "3. Preliminaries", "content": null}, {"title": "3.1. Linear Temporal Logic over Finite Traces", "content": "LTL is a variant of LTL concentrating on expressing temporal properties over finite traces. Let AP be a set of atomic propositions. The syntax of LTLf is defined as follows:\n$\\varphi::= p | \\neg \\varphi| \\varphi_1 \\wedge \\varphi_2 | \\bigcirc \\varphi | \\varphi_1U\\varphi_2$\nwhere $p \\in AP$ is an atomic proposition, $\\bigcirc$(next) and U(until) are temporal operators. Propositional binary connectives $\\vee, \\rightarrow, \\leftrightarrow$ and boolean values $\\top, \\bot$ can be derived in terms of basic operators. Other temporal operators can also be expressed. For example, $\\Diamond$(eventually) and $\\Box$(always) are defined by: $\\Diamond \\varphi = \\top U\\varphi$, $\\Box\\varphi = \\neg \\Diamond \\neg \\varphi$. $\\bigcirc \\varphi$ denotes that $\\varphi$ holds in the next state which must exist. $\\varphi_1U\\varphi_2$ indicates that $\\varphi_1$ holds until $\\varphi_2$ is true. $\\Diamond \\varphi$ means that $\\varphi$ will eventually hold before or right in the last state. $\\Box \\varphi$ represents that $\\varphi$ holds along the whole trace.\nA state $s$ is a subset of AP that is true, while atomic propositions in AP \\ s are assumed to be false. LTLf formulae are interpreted over finite traces of states $\\sigma = s_0...s_n$. The semantics of LTLf is defined as follows. We say that $\\sigma$ satisfies $\\varphi$, written as $\\sigma \\models \\varphi$, when $\\sigma,0 \\models \\varphi$."}, {"title": "3.2. MDP and NMRDP", "content": "As one of the theoretical foundations of RL, an MDP M = (S, A, R, P, \u03b3, s\u2080) serves as a model for an agent\u2019s sequential decision-making process. Here, S is a set of states, A is a set of actions, R: S \u00d7 A \u00d7 S \u2192 R is a reward function, P(st+1|st, at) \u2208 [0, 1] is a transition probability distribution over the set of next states, given that the agent takes action at in state st at step t and reaches state st+1, \u03b3 is the discount factor, s\u2080 \u2208 S is the initial state.\nMDPs are limited in expressiveness due to its memoryless feature, while Non-Markovian Reward Decision Processes (NMRDPs) are more powerful by extending MDPs with non-Markovian rewards, which is suitable for modeling NMTs Thi\u00e9baux et al. (2006). An NMRDP is a tuple NM = (S, A, R, P, \u03b3, s\u2080), where S, A, P, \u03b3, s\u2080 are the same as those in an MDP. The only difference is the reward function R, which is defined as (S \u00d7 A)* \u2192 R. This function indicates that non-Markovian rewards are determined by a finite sequence of states and actions (memory). Consequently, LTL is a perfect match with non-Markovian rewards, which is defined in terms of a pair R = (\u03c6, r). \u03c6 is an LTLf formula (reward formula) and r \u2208 R is the associated reward.\nSince the standard training model for RL is based on MDP, advanced RL algorithms cannot be directly used to learn NMTs modeled by NMRDP. To overcome this problem, analogous to priori researches, we apply a synchronisation technique to generate a product of an LTLf formula (in fact a DFA) and NMRDP in a on-the-fly fashion. Given an NMRDP NM = \u27e8S, A, R = (\u03c6, r), P, \u03b3, s\u2080\u27e9 and a DFA A = (Q, \u03a3, \u03b4, q\u2080, F) with respect to the reward formula \u03c6, the converted MDP is a product M\u00ae = \u27e8S\u00ae, A, R\u00ae, P\u00ae, \u03b3, s\u00ae\u27e9, where S\u00ae = S \u00d7 Q and s\u00ae = (s\u2080, q\u2080). Given s\u00ae = (s, q) \u2208 S\u00ae, s\u00ae is referred to as a product state, s is the state ingredient of s\u00ae, and q is the DFA ingredient of s\u00ae. The reward function is defined as:\n$R^{\\otimes}(s_t, a_t, s_{t+1}) = \\begin{cases} r, & q_{t+1} \\in F \\\\ 0, & otherwise\\end{cases}$\nwhich means that the agent receives a reward r only when the DFA ingredient of s is an accepting state. The transition probability $P^{\\otimes}(s_{t+1}^{\\otimes} | s_t^{\\otimes}, a_t)$ equals to P(st+1 | st, at) if there exists a possible DFA transition enabled by st from the DFA ingredient of $s_t^{\\otimes}$ to that of $s_{t+1}^{\\otimes}$, and 0 otherwise, which is formalized below:\n$P^{\\otimes}(s_{t+1}^{\\otimes}|s_t^{\\otimes}, a_t) = \\begin{cases} P(s_{t+1}|s_t, a_t), & s_t^{\\otimes} = (s_t, q_t), s_{t+1}^{\\otimes} = (s_{t+1}, q_{t+1}), \\\\ & \\exists v \\in \\Sigma: (q_t, v, q_{t+1}) \\in \\delta \\text{ and } s_t = v \\\\ 0, & otherwise\\end{cases}$"}, {"title": "4. Parallel and Modular RL Framework", "content": "In this section, the Parallel and Modular RL framework, referred to as ParMod, is illustrated in detail. The intuition behind our approach is that, by modularizing the NMT into sub-tasks, multiple agents are simultaneously created to learn sub-tasks, with each agent focusing on one sub-task. A comprehensive policy will be synthesised from sub-policies ultimately. Figure 2 gives an overview of the proposed framework. The NMT specified by an LTLf formula is transformed to a DFA. The training environment is modeled as a product MDP via synchronising the DFA with the original environment. Modularizing a task essentially means dividing that task into different phases (sub-tasks) based on the classification of DFA states, determined before training. Each task phase corresponds to a category of DFA states. For example, we assume there are three categories of the DFA on the left hand side of Figure 2, i.e., {q1},{q2, q3}, {q4}, dividing the task into three phases. q2 and q3 are in the same category since they both need two steps to reach the accepting state q5. In ParMod, task phases are trained in parallel. An agent is responsible to handle a task phase by a separate deep network. Totally, the three agents corresponding to the three categories work in collaboration in Figure 2.\nMost of the task phases are located at intermediate and different positions of a global task. Therefore, a task phase should start from an initial state that may differ from other task phases. To this end, each agent maintains its own initial state buffer (e.g., three initial state buffers at the bottom right in Figure 2). It randomly selects an initial state from its buffer at the beginning of an episode and interacts with the environment. The update course for a network is not only influenced by the sampled experiences, but also connected with other networks. In addition, the reward function is reshaped base on the classification result in order to provide appropriate reward signals for each task phase."}, {"title": "4.1. Modularizing Non-Markovian Tasks", "content": "A modular RL approach is presented in Hasanbeig et al. (2020) (Modular DDPG). Aiming at synthesising policies of infinite behaviors, the authors construct a similar product MDP with LDBA which results from standard LTL. Each state of LDBA is roughly considered as a \u201ctask divider\u201d, and each transition jumping from one state to another as a \"sub-task\". Taking Figure 3 as an example, Modular DDPG modularizes the entire task into four task dividers q1, q2, q3, q4, and four sub-tasks, i.e., reaching g, r, r \u2227 \u00acg, g \u2227 \u00acr, with current DFA state being q1, q3, q1, q3 respectively.\nHowever, the efficiency and scalability of Modular DDPG will be severely affected as the size of LDBA increases. Inspired by the idea of Modular DDPG, we design a subtler mechanism for identifying task dividers. DFA states are classified into multiple categories, and each category is viewed as a \u201ctask phase\u201d which is akin to a task divider."}, {"title": "4.2. Parallel Training for Modular Task Phases", "content": "Modular DDPG makes use of multiple networks to solve complex NMTs. Unfortunately, it adopts a sequential update mechanism for networks. That is, only one agent explores the state space of the environment and only one network updates at a time. We are of the opinion that this does not help the sample efficiency, and instead, will make training time become the bottleneck to a great extent.\nIn our framework, accomplishing an NMT is equivalent to facilitate an accepting DFA trace to break into several segments. The framework ensures efficiency, in this way, even with an extremely large number of DFA states, we can still find appropriate number of categories to prevent excessive space occupation and low efficiency issues of networks.\nGiven a product MDP $M^{\\otimes} = \\langle S^{\\otimes}, A, R^{\\otimes}, P^{\\otimes}, \\gamma, s_0^{\\otimes}\\rangle$, and the number of categories N. In order to improve sample efficiency, ParMod utilizes a group of agents $\\mathcal{A} = \\{\\zeta_0,..., \\zeta_{N-1}\\}$ to train task phases in parallel. Each agent is responsible for training a task phase with a network. In particular, agent $\\zeta_i$ is bound with a unique initial state buffer $B_i$ which is ready to store suitable initial states for $\\zeta_i$. Define all initial state buffers as ISB = {$B_0$, ..., $B_{N-1}$}. A product state $s = (s, q) \\in S$ generated during the training process will have a chance to be put in the corresponding initial state buffer $B_{rank(q)}$. Agent $\\zeta_i$ randomly samples an initial state from $B_i$ when resetting the environment.\nFor preventing an agent\u2019s exploration to other task phases (deviating from its own task phase), an additional termination condition is introduced: task phases are different between two adjacent product states of $M^{\\otimes}$, i.e., $P((s_{t+1},q_{t+1}) | \\langle s_t, q_t\\rangle, a_t) \\neq 0$ and $rank(q_t) \\neq rank(q_{t+1})$. Suppose the current product state and the next one are respectively $(s_t, q_t)$ and $(s_{t+1}, q_{t+1})$. If the task phase changes, agent $\\zeta_{rank(q_t)}$ will store $(s_{t+1}, q_{t+1})$ in $B_{rank(q_{t+1})}$ to initialize the environment for agent $\\zeta_{rank(q_{t+1})}$. As an illustration, the agent inside the red dashed box in Figure 2 is responsible for training the first task phase. When an episode is terminated and the current product state is of the form $(s_t, q_2)$ or $(s_t, q_3)$, the agent needs to store $(s_t, q_2)$ or $(s_t, q_3)$ in the second initial buffer for initializing the second task phase (green dashed box)."}, {"title": "5. Correctness and Optimality", "content": "In this section", "f_1": "S \\times Q \\rightarrow S$ and $f_2 : S^{\\otimes"}, "rightarrow S$ such that\n1. \u2200s \u2208 S : f\u2082(f\u2081(s)) = s.\n2. \u2200s\u2081, s\u2082 \u2208 S, \u2200s\u00ae \u2208 S\u00ae and \u2200a \u2208 A : if f\u2082(s\u00ae) = s\u2081 and P(s\u2082 | s\u2081, a) > 0, there exits a unique s\u00ae \u2208 S\u00ae such that f\u2082(s\u00ae) = s\u2082 and $P^{\\otimes}(s^{\\otimes} | s^{\\otimes}, a) = P(s\u2082 | s\u2081, a)$.\n3. For any feasible trajectory s\u2080a\u2080s\u2081a\u2081 ... sn of NM and $s_0^{\\otimes}a_0s_1^{\\otimes}a_1...s_n^{\\otimes}$ of M\u00ae, such that f\u2081(s\u2080) = $s_0^{\\otimes}$ and si = f\u2082(s\u00ae) for 0 \u2264 i \u2264 n, we have R(s\u2080 ... si+1, ai) = $R^{\\otimes}(s_i^{\\otimes}, a_i, s_{i+1}^{\\otimes})$ for 0 < i < n.\nTheorem 1. An NMRDP NM = (S, A, R = (\u03c6, r), P, \u03b3, s\u2080) is equivalent to the product MDP M\u00ae = (S\u00ae, A, R\u00ae, P\u00ae, \u03b3, s\u00ae), which is the product of NM and the DFA A = (Q, \u03a3, \u03b4, q\u2080, F) with respect to the reward formula \u03c6.\nProof. Recall that every s\u00ae \u2208 S has the form (s, q). The reward function of NM is defined in terms of a pair R = (\u03c6, r), where the agent get reward r only when the LTLf formula is satisfied, i.e.\n$R(s_0...s_{n+1}, a_n) = \\begin{cases} r, & \\varphi \\text{ is satisfied} \\\\ 0, & otherwise\\end{cases}$\nBesides that, the transition probability in M\u00ae is formalized below:\n$P^{\\otimes}(s_{t+1}^{\\otimes}|s_t^{\\otimes}, a_t) = \\begin{cases} P(s_{t+1}|s_t, a_t), & s_t^{\\otimes} = (s_t, q_t), s_{t+1}^{\\otimes} = (s_{t+1}, q_{t+1}), \\\\ & \\exists v \\in \\Sigma: (q_t, v, q_{t+1}) \\in \\delta \\text{ and } s_t = v \\\\ 0, & otherwise\\end{cases}$\nThe reward function in M\u00ae is defined as\n$R^{\\otimes}(s_t^{\\otimes}, a_t, s_{t+1}^{\\otimes}) = \\begin{cases} r, & s_{t+1}^{\\otimes} = (s_{t+1}, q_{t+1}), q_{t+1} \\in F \\\\ 0, & otherwise\\end{cases}$\nFirst we define f\u2081(s) = <s, q\u2080> and f\u2082(s\u00ae) = f\u2082((s, q)) = s. Then condition 1 is easily verifiable by inspection. For condition 2, let $s^{\\otimes} = \\langle s_2, \\delta(q_1, s_1)\\rangle$, then the first condition of Equation 1 can be satisfied, we have $P^{\\otimes}(s^{\\otimes} | s^{\\otimes}, a) = P(s_2 | s_1, a)$ and f\u2082(s\u00ae) = s\u2082. The above proves the existence of s\u00ae, in the following, we demonstrate the uniqueness. Assume that $s^{\\otimes} = \\langle s^+, \\delta(q_1, s_1)\\rangle$ where $s^+ \\neq s_2 \\in S$ and $s^+ \\in S$, it cannot hold that $\\forall s_1, s_2 \\in S: f_2(s^{\\otimes}) = s_1 \\text{ and } f_2(s^{\\otimes}) = s_2$. Assume that $s^{\\otimes} = \\langle s_2, q^+\\rangle$ where $q^+ \\neq \\delta(q_1, s_1)$, then $P^{\\otimes}(s^{\\otimes} | s^{\\otimes}, a) = 0 \\neq P(s_2 | s_1, a)$ based on Equation 1. Hence we can conclude the uniqueness.\nFor condition 3, since f\u2081(s\u2080) = $s_0^{\\otimes}$ and si = f\u2082(s\u00ae) for 0 \u2264 i \u2264 n, we have $s_{i+1}^{\\otimes} = (s_{i+1}, \\delta(q_i, s_i))$ according to condition 2. Then a DFA trace q\u2080... qn can be induced by state trace s\u2080... sn. We can discuss condition 3 in two cases. When \u03c6 is satisfied after the trajectory s\u2080a\u2080s\u2081a\u2081 ... sn, reward r is received in NM based on R = (\u03c6, r). Namely, we have the DFA trace q\u2080, q\u2081,..., qn+1 such that qn+1 \u2208 F. Hence the reward also r in M\u00ae, according to Equation 2. When the task is not completed, the rewards of NM and M\u00ae are both zero.\nNext, we discuss the equivalence of the optimal policies between M and NM. Let \u03c0\u00ae(a | s\u00ae) be a policy of M\u00ae, which determines the probability of an action a \u2208 A at each state s \u2208 S. It is easy to define the equivalent policy of NM, i.e., \u03c0(a | s\u2080... sn). Given any feasible trajectory s\u2080a\u2080s\u2081a\u2081... sn of NM and DFA trace q\u2080 ... qn+1 induced by state trace s\u2080... sn, we have \u2200a \u2208 A : \u03c0(a | s\u2080... sn) = $\\pi^{\\otimes}(a | \\langle s_n, q_n\\rangle)$.\nTheorem 2. Given a product MDP M\u00ae, let $n\u2019$ be the optimal policy of M\u00ae. Then, the policy $n\u2019$ that is equivalent to \u03c0\u00ae* is optimal for NM.\nProof. We prove the following lemma firstly.\nLemma 1. For any feasible trajectory s\u2080a\u2080s\u2081a\u2081 ... sn of NM and $s_0^{\\otimes}a_0s_1^{\\otimes}a_1...s_n^{\\otimes}$ of M\u00ae, such that f\u2081(s\u2080) = $s_0^{\\otimes}$ and si = f\u2082(s\u00ae) for 0 \u2264 i \u2264 n, we have\n$\\mathbb{E} \\left[\\sum_{k=0}^{n-1} \\gamma^k R(s_0 ... s_{k+1}, a_k)\\right"], "\u2014": "enotes no success policy has been found. It is worth to mention that", "s": "n$\\pi^{\\otimes*} = argmax_{\\pi^{\\otimes*} \\in \\mathcal{D}^{\\otimes}} \\mathbb{E}_{D^{\\otimes}} \\left[\\mathbb{E} \\left[\\sum_{k=0}^{T-1} \\gamma^k (s_k^{\\otimes}, a_k, s_{k+1}^{\\otimes})\\right]\\right]$\n$\\pi^{\\otimes*} = argmax_{\\pi^{\\otimes*} \\in \\mathcal{D}^{\\otimes}} \\mathbb{E}_{D^{\\otimes}} \\left[L R^{\\otimes}(s, a_{k+1})\\right]$\n$\\pi^{\\otimes*} = argmax_{\\pi^{\\otimes*} \\in \\mathcal{D}^{\\otimes}} P(s)\\gamma^L r$\nwhere L is the steps to satisfying \u03c6 and P(s) indicates the probability of satisfying \u03c6. Since reward is only given when \u03c6 is satisfied, when \u03b3 < 1, maximizes the expected accumulated rewards for \u03c0\u00ae* is equivalent to minimizing the steps according to Equation 4. When \u03b3 = 1, then \u03c0*(s) = P(s)r, hence \u03c0\u00ae* maximizes the probability of satisfying \u03c6.\nFinally, we discuss whether ParMod can converge to the optimal strategy. Due to the strong generalizability, ParMod can set various algorithms as baselines, so the convergence depends on the specifics of the baseline algorithm. Here, we demonstrate this with ParMod and Q-learning as an example. Similar proofs can be referenced when other algorithms are used in ParMod as baselines.\nTheorem 4. Given a product MDP M\u00ae = (S\u00ae, A, R\u00ae, P\u00ae, \u03b3, s) and the DFA A = (Q, \u03a3, \u03b4, q\u2080, F), ParMod with tabular Q-Learning converges to an optimal policy as long as every possible state-action pair is visited infinitely often.\nProof. In ParMod, each modular Q-function Qk is updated by the rule\n$Q'(s^{\\otimes}, a_t) = Q_k(s^{\\otimes}, a_t) + \\alpha_t(s^{\\otimes}, a_t) \\cdot \\left[R(s^{\\otimes}, a_t, s_{t+1}^{\\otimes}) + \\gamma max_{b \\in A} Q_{k'}(s_{t+1}^{\\otimes}, b) - Q_k(s^{\\otimes}, a_t)\\right]$\nwhere $s^{\\otimes} = (s_i, q_i), k = rank(q_i) \\text{ and } k' = rank(q_{i+1})$.\nSimilarly, Q-learning has the following update format (referring to Melo (2001))\n$Q'(s^{\\otimes}, a_t) = Q(s^{\\otimes}, a_t) + \\alpha_t(s^{\\otimes}, a_t) \\cdot \\left[R(s^{\\otimes}, a_t, s_{t+1}^{\\otimes}) + \\gamma max_{b \\in A} Q(s"}