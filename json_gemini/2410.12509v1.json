{"title": "Benchmarking Defeasible Reasoning with Large Language Models - Initial Experiments and Future Directions", "authors": ["Ilias Tachmazidis", "Sotiris Batsakis", "Grigoris Antoniou"], "abstract": "Large Language Models (LLMs) have gained prominence in the AI landscape due to their exceptional performance. Thus, it is essential to gain a better understanding of their capabilities and limitations, among others in terms of nonmonotonic reasoning. This paper proposes a benchmark that corresponds to various defeasible rule-based reasoning patterns. We modified an existing benchmark for defeasible logic reasoners by translating defeasible rules into text suitable for LLMs. We conducted preliminary experiments on nonmonotonic rule-based reasoning using ChatGPT and compared it with reasoning patterns defined by defeasible logic.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have caught people's attention recently due to the exceptional performance these systems achieved in various language related tasks since they are the underlying technology behind chat bots such as ChatGPT\u00b9. Large Language models such as LaMDA (Thoppilan et al. 2022) and GPT (OpenAI 2023) are based on training deep neural networks with billions of parameters using huge lexical datasets and often employing human judgment in a semi-supervised (e.g., reinforcement learning) training setting (Lambert et al. 2022; Ouyang et al. 2022). The exceptional - often human level- performance of LLMs in various tasks has led to a widespread discussion about the potential benefits and dangers of such technologies in various areas and human society in general including petitions to pause research on more capable LLMs (Letters 2023). For example GPT-4 achieved human lever performance in various academic and professional exams including a score in the top 10% of test takers in the Uniform Bar Examination, this performance is attributed to a large degree to scaling LLMs to larger training datasets and more complex models with larger number of parameters (OpenAI 2023). Despite the impressive performance of Large Language Models, including their ability to demonstrate an emerging intelligent behaviour and reasoning capabilities, leading to the point of considering them forerunners of Artificial General Intelligence (Bubeck et al. 2023) several issues related to LLMs have been identified, such as the energy cost of training LLMs (Luccioni, Viguier, and Ligozat 2022; Strubell, Ganesh, and McCallum 2019), difficulty to control their behaviour (Luccioni and Viviano 2021), ensuring conformity with stakeholders requirements and norms, as well as interpreting their functionality (Bowman 2023). The interpetability of LLMs is a crucial issue since neural network based LLMs appear to be 'black boxes', in contrast to logic based systems, and although various attempts exist to deal with this problem, including the use of LLMs to interpret LLMs (Bills et al. 2023), this is still an unresolved issue. In addition, since LLMs are trained on vast amounts of raw text they tend to replicate their input rather that apply robust reasoning (Bender et al. 2021). LLMs trained on raw text instead of structured knowledge bases integrating machine readable semantics contribute to the difficulty of achieving efficient reasoning and this is an issue examined in various works such as (Zhang et al. 2022) and surveyed in (Huang and Chang 2022). Various attempts to integrate Knowledge Graphs (KGs) to LLMs have been proposed (Zhen et al. 2022; Yin et al. 2022) as a solution to the last issue, but recent advances in LLMs capabilities, including high performance on academic and professional exams (OpenAI 2023), illustrated the need for an updated evaluation of the reasoning capabilities of LLMs. This updated evaluation should take into account the recent developments in the field, including the deployment of systems such as ChatGPT employing the benefits of scalability (Kaplan et al. 2020) and the LLMs demonstrated ability to adjust to new tasks given just a small number of examples (Brown et al. 2020). Furthermore LLMs capabilities with respect to important formalisms such as defeasible reasoning have not been examined in detail yet. This kind of reasoning is important for cases where knowledge is incomplete and conflicting, which is the case in many application areas including law and healthcare. In previous work (Antoniou and Batsakis 2023) preliminary experiments on LLM defeasible reasoning have been performed, but a systematic analysis involving benchmark construction containing several examples of different reasoning patters is missing.\nThis work is an initial step towards developing a deep understanding of reasoning capabilities of LLMs with emphasis of nonmonotonic reasoning. In order to achieve this we propose a benchmark for LLMs by modifying an existing benchmark for defeasible logic reasoners. The pro-"}, {"title": "2 Background", "content": "A defeasible theory D is a triple (F,R,>) where F is a finite set of facts (literals), R a finite set of rules, and > a superiority relation (acyclic relation upon R).\nA rule r consists (a) of its antecedent (or body) A(r) which is a finite set of literals, (b) an arrow, and, (c) its consequent (or head) C(r) which is a literal. There are three types of rules: strict rules, defeasible rules and defeaters represented by a respective arrow $$\\rightarrow$$, $$\\Rightarrow$$ and $$\\hookleftarrow$$. Strict rules are rules in the classical sense: whenever the premises are indisputable (e.g., facts) then so is the conclusion. Defeasible rules are rules that can be defeated by contrary evidence. Defeaters are rules that cannot be used to draw any conclusions; their only use is to prevent some conclusions.\nGiven a set R of rules, we denote the set of all strict rules in R by Rs, and the set of strict and defeasible rules in R by Rsd. R[q] denotes the set of rules in R with consequent q. If q is a literal, ~q denotes the complementary literal (if q is a positive literal p then ~q is \u00acp; and if q is \u00acp, then ~q is p). A conclusion of D is a tagged literal and can have one of the following four forms:\n\u2022 +Aq, meaning that q is definitely provable in D.\n\u2022 - Aq, meaning that we have proved that q is not definitely provable in D.\n\u2022 +$\\delta$q, meaning that q is defeasibly provable in D.\n\u2022 -$\\delta$q, meaning that we have proved that q is not defeasibly provable in D.\nProvability is defined below. It is based on the concept of a derivation (or proof) in D = (F, R, >). A derivation is a finite sequence P = P(1), ..., P(n) of tagged literals satisfying the conditions shown below. The conditions are essentially inference rules phrased as conditions on proofs. P(1..2) denotes the initial part of the sequence P of length i. For more details on provability and an explanation of the intuition behind the conditions below, see (Maher 2004).\n+$\\Delta$: We may append P(i + 1) = +$\\Delta$q if either\nq\u2208 F or\n\u2203r \u2208 R[q] \u2200\u03b1 \u2208 A(r): +$\\Delta$\u03b1 \u2208 P(1..i)\n-$\\Delta$: We may append P(i + 1) = -$\\Delta$q if\nq \u2209 F and\n\u2200r \u2208 R[q] \u2203\u03b1 \u2208 A(r): -$\\Delta$\u03b1 \u2208 P(1..i)\n+\u03b4: We may append P(i + 1) = +\u03b4q if either\n(1) +$\\Delta$q \u2208 P(1..i) or\n(2) (2.1) \u2203r \u2208 Rsd[q] \u2200\u03b1 \u2208 A(r): +\u03b4\u03b1 \u2208 P(1..i) and\n(2.2) -$\\sim$q \u2208 P(1..i) and\n(2.3) \u2200s \u2208 R[$$\\sim$$q] either\n(2.3.1) \u2203\u03b1 \u2208 A(s): -\u03b4\u03b1 \u2208 P(1..i) or\n(2.3.2) \u2203t \u2208 Rsd[q] such that\n\u2200\u03b1 \u2208 A(t): +\u03b4\u03b1 \u2208 P(1..i) and t > s\n-\u03b4: We may append P(i + 1) = -\u03b4q if\n(1) -$\\Delta$q \u2208 P(1..i) and\n(2) (2.1) \u2200r \u2208 Rsd[q] \u2203\u03b1 \u2208 A(r): -\u03b4\u03b1 \u2208 P(1..i) or\n(2.2) +$\\sim$q \u2208 P(1..i) or\n(2.3) \u2203s \u2208 R[$$\\sim$$q] such that\n(2.3.1) \u2200\u03b1 \u2208 A(s): +\u03b4\u03b1 \u2208 P(1..i) and\n(2.3.2) \u2203t \u2208 Rsd[q] either\n\u2203\u03b1 \u2208 A(t): -\u03b4\u03b1 \u2208 P(1..i) or t \u226f s"}, {"title": "3 Dataset", "content": "We propose a dataset of scalable test theories which is inspired by (Maher et al. 2001). In (Maher et al. 2001) authors focused on evaluating the efficiency of existing defeasible reasoning systems. Here, we focus on a translation of rules into text suitable for LLMs. The proposed dataset is focused on typical defeasible inference patterns, allowing a comparison between inputs for reasoning systems and LLMs.\nEmpty. First, we skip the empty() theory as it contains no facts, rules or priorities. The empty() theory serves as a baseline for reasoning systems. However, there is no meaningful evaluation of a LLM in the absence of text.\nChain. Our first theory is chain(n), where a0 is at the end of a chain of n rules ai+1 $$\\Rightarrow$$ ai, with a single fact an initiating the chain of inference (no priorities defined). For chain(2), the defeasible rules are as follows:\n>> A0000002\nr1: A0000002 => A0000001\nr2: A0000001 => A0000000\nNote that \">> A0000002\" denotes a fact following the syntax of SPINdle (Rohaninezhad, Arif, and Noah 2015). Based on the fact A0000002 rule rl infers that A0000001 is deafeasibly provable, while rule r2 infers that A0000000 is deafeasibly provable as well. In this work, the structure of the theories is aimed at determining through logical inference whether A0000000 is provable or not. Subsequently, the translation of chain(2) into plain text is as follows:\nA0000002 is an Arkon.\nIf A0000002 is an Arkon, then typically A0000001 is an Arkon.\nIf A0000001 is an Arkon, then typically A0000000 is an Arkon.\nNotice the pattern, facts are expressed as statements, while rules are expressed as if-then statements with the keyword \"typically\" denoting the defeasible nature of the rule. Given the already identified affinity of ChatGPT to use other background knowledge when predicates and atoms are real-world entities, we use imaginary names of species on an imaginary planet, following (Ford and Billington 2000). Here, we use \"Arkon\" in order to ask ChatGPT:\nIs A0000000 an Arkon?\nThe theory chains(n), is a version of chain(n) with strict rules. For chains(2), the defeasible rules are as follows:\n>> A0000002\nr1: A0000002$\\rightarrow$ A0000001\nr2: A0000001$\\rightarrow$ A0000000"}, {"title": "4 Experimental Results", "content": "The proposed dataset is scalable, namely increasingly larger theories can be generated for increasing values of parameters n and k. However, as a first step in this work, we focus on relatively small and readable theories in order to assess empirically the inference patterns of ChatGPT. We used GPT-40 in order to assess theories: chain(8), chains(8),\ncircle(8), circles(8), dag(3,2), levels-(5), levels(5), hierarchies(2,4). ChatGPT was given the following instructions for each theory:\nYou are an expert on defeasible\nreasoning. Your task is to make\nlogical conclusions based on\nprovided knowledge (delimited with\nXML tags).\nIn addition, each prompt was based on the following template (namely, \"{theory}\" was substituted with each evaluated theory):\nBased on the following knowledge alone:\n<knowledge>\n''{theory}''\n</knowledge>\nIs A0000000 an Arkon?\nLet's think step by step.\nEach generated theory, such as chain(8), was evaluated over four settings:\n\u2022 A0000000 is not an Arkon with statements provided in random order (-d-rand),\n\u2022 A0000000 is an Arkon with statements provided in random order (+d-rand),\n\u2022 A0000000 is not an Arkon with statements provided in sequential order (-d-seq),\n\u2022 A0000000 is an Arkon with statements provided in sequential order (+d-seq).\nWe evaluated statements provided in random order first in order to observe any differences in generated responces when the same theory is provided in sequential order. Notice that ChatGPT conversations provided as links in footnotes contain the four settings in the following order: --rand, +d-rand, -d-seq, +d-seq. Results are summarised in Table 2.\nFor theory chain(8) we notice that +d-rand and +d-seq have similar inference patterns, namely starting from provided facts, each rule is applied until a final conclusion is reached (interestingly, statements provided in random order do not change the inference sequence). We also notice that while -d-seq evaluates all rules sequentially (even\nafter encountering A1111113), the inference of -d-rand\nmoves backwards once A1111113 is encountered. Theory\nchains(8) follows similar inference patterns based on if-then\nstatements.\nFor theory circle(8) we notice that for both -d-rand and\n-d-seq the circle is identified and no conclusion can be\ndrawn. For +d-rand and +d-seq we introduced a fact that\nproves the circle, inference started from the given fact lead-\ning to the inference of A0000000. Theory circles(8) follows\nsimilar inference patterns based on if-then statements. How-\never, for -d-rand the circle was not explicitly identified as\njustification for inference.\nFor theory dag(3,2), -d-rand exhibited unusual patterns\n(i.e. a potential hallucination), namely A1111114 was re-\nplaced with A0000004 (incorrectly), leading to the infer-\nence of A0000000 (while a rule based on A0000005 and\nA0000003 leading to A0000002 was not given as input).\nInterestingly, for -d-seq the inference pattern was correct\nwith A1111114 breaking the chain of inference (this indi-\ncates that the sequence of statements can have an effect on\nthe inference process, which is not the case for standard\nreasoners). The inference pattern for +d-rand was correct,\neven though it is unclear why the \u201cChain of Reasoning\" did\nnot include the conjunction of two premises. The inference\npattern for +d-seq was correct, with a well formed \u201cChain\nof Reasoning\".\nTheory levels-(5) introduces conflicting rules, with expla-\nnations provided not matching expected inference for de-\nfeasible reasoning. This might be attributed to the fact that\nChatGPT starts from A0000000 working backwards, while\nthe lack of priorities over conflicting rules introduces confu-\nsion. It is worth pointing out that -d-seq introduced the\nstatement \"A0000002 is typically an Arkon.\" (a fact not\ngiven as input), i.e. a potential hallucination.\nTheory levels (5) contains priority rules, which provide\nsome clarity. However, for \u2013d-rand, since there are no pri-\nority rules for A0000002 the inference does not match deaf-\neasible reasoning (it seems that ChatGPT does not follow\nthe notion of defeasible reasoning where both A0000002\nand -A0000002 might not be provable). Conversely, for\n+d-rand the combination of priorities and rules with fail-\ning premises (namely, the resolution of A0000002 knowing\nthat A0000003 is not an Arkon) leads to conclusions that fol-\nlow defeasible reasoning. Interestingly, the inference steps\nof-d-seq are well structured, closely resembling defeasible"}, {"title": "5 Conclusion", "content": "This work is a first step towards gaining a better understanding of reasoning capabilities of LLMs with respect to non-monotonic reasoning. We proposed a benchmark tailored to LLMs through the modification of an existing benchmark for defeasible logic reasoners. A range of reasoning patterns was covered by the proposed benchmark. Preliminary experiments indicated encouraging results for monotonic reasoning as well as certain challenges in the context of non-monotonic rule-based reasoning. Future work will focus on expanding our exploration of reasoning patterns that might pose a challenge to LLMs. Furthermore, while this work was focused on small and readable theories, future efforts will examine the effect of increasingly larger theories on the reasoning process of LLMs."}]}