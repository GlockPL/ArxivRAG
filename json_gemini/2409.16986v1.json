{"title": "HARNESSING DIVERSITY FOR IMPORTANT DATA SELECTION IN PRETRAINING LARGE LANGUAGE MODELS", "authors": ["Chi Zhang", "Huaping Zhong", "Kuan Zhang", "Chengliang Chai", "Rui Wang", "Xinlin Zhuang", "Tianyi Bai", "Jiantao Qiu", "Lei Cao", "Ye Yuan", "Guoren Wang", "Conghui He"], "abstract": "Data selection is of great significance in pre-training large language models, given the variation in quality within the large-scale available training corpora. To achieve this, researchers are currently investigating the use of data influence to measure the importance of data instances, i.e., a high influence score indicates that incorporating this instance to the training set is likely to enhance the model performance. Consequently, they select the top-k instances with the highest scores. However, this approach has several limitations. (1) Computing the influence of all available data is time-consuming. (2) The selected data instances are not diverse enough, which may hinder the pre-trained model's ability to generalize effectively to various downstream tasks. In this paper, we introduce Quad, a data selection approach that considers both quality and diversity by using data influence to achieve state-of-the-art pre-training results. In particular, noting that attention layers capture extensive semantic details, we have adapted the accelerated iHVP computation methods for attention layers, enhancing our ability to evaluate the influence of data, i.e., its quality. For the diversity, Quad clusters the dataset into similar data instances within each cluster and diverse instances across different clusters. For each cluster, if we opt to select data from it, we take some samples to evaluate the influence to prevent processing all instances. To determine which clusters to select, we utilize the classic Multi-Armed Bandit method, treating each cluster as an arm. This approach favors clusters with highly influential instances (ensuring high quality) or clusters that have been selected less frequently (ensuring diversity), thereby well balancing between quality and diversity.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, large language models(LLM) have significantly advanced the field of artificial intelligence (Zhao et al., 2023; Hadi et al., 2023; Minaee et al., 2024). Different from typical machine learning approaches, LLMs scale up the model parameters, unsupervised dataset size, and computing resources, empowering LLMs to successfully tackle a broad spectrum of downstream tasks through the use of prompts. In practice, for LLM pre-training, computational resources are invariably limited, determining both the model size and the volume of training data employed. In this situation, judiciously selecting train datasets is essential for producing well-performed LLMs (Brown, 2020; Du et al., 2022; Gururangan et al., 2020; Hoffmann et al., 2022; Raffel et al., 2020), considering the diverse quality of the vast available corpus. Also, the LLaMA-3.1 report (Dubey et al., 2024) emphasizes that the use of high quality data in later training stages can greatly improve model performance."}, {"title": "2 RELATED WORK", "content": "High-quality data selection has been widely studied in large language models."}, {"title": "2.1 HEURISTIC DATA SELECTION", "content": "Manual intuition In the early stages, researchers often relied on intuition to design hand-crafted heuristics, such as those discussed in (Soldaini et al., 2024) and (Penedo et al., 2023), aimed at improving data quality. Deduplication is another standard approach to pretraining data selection, with methods such as (Penedo et al., 2023) and SemDedup (Abbas et al., 2023) using keyword-based and semantic deduplication, respectively. Although these methods effectively filter out noise and redundant data from web sources, more precise techniques are required to identify the most desirable examples from the candidate data pool.\nHigh-performance LLMs While large models like GPT-4 exhibit exceptional semantic extraction abilities and can efficiently evaluate data quality, the metrics people employ for scoring data frequently depend excessively on human intuition. This often leads to a discrepancy between the data chosen by these methods and the data actually needed by the model. Significant contributions in this area include Qurating (Wettig et al., 2024), Finweb (Penedo et al., 2024), AutoDS (Zhang et al., 2024), and (Gunasekar et al., 2023).\nSurrogate Models In DeepSeekMath (Shao et al., 2024), an active learning strategy was employed to train a web data classifier. Similarly, in MATES Yu et al. (2024), a surrogate model was developed to estimate the influence scores of the data samples. RHO-1 (Lin et al., 2024) used a surrogate model trained in high-quality data to perform token-level data filtering. However, these techniques usually require significant GPU resources for surrogate model training, and classifiers tend to be domain-specific, limiting their adaptability across various domains."}, {"title": "2.2 MODEL-AWARE DATA SELECTION", "content": "Perplexity Perplexity serves as a metric for selecting high-probability data under a language model. In (Chen et al., 2024; Marion et al., 2023; Muennighoff et al., 2024; Wenzek et al., 2019), perplexity (PPL) is utilized to filter data. Similar to Qurating, we observed that this method often incorporates a significant amount of simple and redundant data, since it is easy for the model to predict.\nInfluence Function (Grosse et al., 2023; Choe et al., 2024) demonstrated that influence functions can uncover the impact of training data on the performance of large models. Consequently, LESS (Xia et al., 2024) and MATES (Yu et al., 2024) utilized influence functions for selecting data during the SFT and pretraining phases, respectively. For large models, computing influence functions is extremely resource-demanding (Grosse et al., 2023). However, given the large amount of data handled during pretraining, directly implementing LESS methods (Xia et al., 2024) for data selection at this stage poses considerable difficulties. To overcome this, MATES (Yu et al., 2024) employed a proxy model to approximate the influence score across the full dataset. Nonetheless, the limited capacity of this proxy model hinders its ability to grasp detailed data information, resulting in inaccurate influence scores. Additionally, basing data selection solely on influence often leads to a lack of diversity in the chosen data. Therefore, both MATES (Yu et al., 2024) and Qurating (Wettig et al., 2024) use heuristic methods, such as Gumbel sampling, to ensure a balance between diversity and quality.\nMulti-Armed Bandit Multi-armed bandit algorithms (Vermorel & Mohri, 2005) represent a simple yet effective approach in reinforcement learning. In ODM (Albalak et al., 2023), the loss incurred during the pre-training phase is utilized as a reward signal to dynamically adjust the data distribution throughout the training process of the model. However, employing the loss as a reward signal in this approach will lead to imprecise data scoring. Furthermore, we argue that using an entire domain,"}, {"title": "3 METHODS", "content": "Initially, we present our problem statement in \u00a73.1.Next, in \u00a73.2, we explain how our method strikes a balance between quality and diversity in selecting pretraining data. Finally, in \u00a73.3, we describe the enhancements made for computing influence accuracy."}, {"title": "3.1 PROBLEM DEFINITION", "content": "To enhance the capabilities of large models in a specific aspect, it is necessary to retrieve relevant data from a vast data pool and perform further training on the large model. Formally, given a candidate data pool \\(D_{e}\\) and a reference set \\(D_{r}\\), a subset of data \\(D_{s}\\) is selected from \\(D_{e}\\) to fine-tune the large model \\(M\\), with the aim of optimizing the performance of the updated model \\(M'\\) in the target dataset."}, {"title": "3.2 STRIKE A BALANCE BETWEEN QUALITY AND DIVERSITY", "content": "The overall process of this approach is illustrated in Figure 2. Our method can be divided into the following four steps: First, we sample the topk clusters with the highest Cluster Score (CS). Then we calculate the influence scores for the samples in each cluster. At this point, we select the highest scoring data that exceed a defined threshold and use the calculated influence scores to update the Cluster Score (CS) for each cluster.\nCalculate influence score as cluster reward In the online process, the issue of data selection can be recast as a multi-armed bandit (MAB) problem. Here, each cluster represents an arm of the MAB, and during each iteration, a subset of data \\(B_{i}\\) is sampled from the respective cluster. The enhancement in model performance derived from this subset is viewed as the reward \\(I_{i}\\) for that cluster during that iteration.:\n\\(I_{i} = \\sum_{z' \\in B_{i}} I_{M}(D_{r}, z')\\)"}, {"title": "Spread reward to neighbour clusters", "content": "Figure 1b illustrates that neighboring clusters possess comparable content and impact. Additionally, relying solely on influence scores for sampling clusters can diminish data diversity. Consequently, we modify cluster rewards by factoring in their proximity and similarity to adjacent clusters and then distribute these modified rewards. In particular, we employ the Wasserstein distance as a metric to assess the separation between clusters, where the distance between two clusters \\(C_{i}\\) and \\(C_{j}\\) is defined as:\n\\(d(C_{i}, C_{j}) = W (p_{i}, p_{j}) = || \\mu_{i} - \\mu_{j} ||_{2} + tr(\\Sigma_{i} + \\Sigma_{j} - 2(\\Sigma_{i}\\Sigma_{j})^{1/2})\\)\nLet \\(N(C_{i})\\) represent the adjacent clusters to \\(C_{i}\\), encompassing all clusters within a distance of \\(\\tau\\) from \\(C_{i}\\). Denote the reward for cluster \\(C_{i}\\) as \\(I_{i}\\). If \\(I_{i} > 0\\) (or \\(I_{i} < 0\\)), then the clusters \\(C_{j}\\) are correspondingly rewarded (or penalized). The precise formula for the reward \\(I_{j}\\) is:\n\\(I_{j} = \\begin{cases} I_{i} \\times (1 - d(C_{i}, C_{j})/\\tau) & \\text{if } C_{j} \\in N(C_{i}) \\\\ 0 & \\text{if } C_{j} \\notin N(C_{i}) \\end{cases}\\)\nWhen a cluster i is selected or receives a reward as a neighboring cluster:\n\\(R(C_{i})_{+} = I_{i}, \\quad T(C_{i})_{+} = 1\\)\nwhere \\(T(C_{i})\\) denotes the total reward accumulated by cluster i over several iterations, while \\(T(C_{i})\\) signifies the count of instances where the current cluster has received a reward.\nUpdate cluster score Given that the data preferences of a large model may vary at different stages of training, after choosing a subset of data \\(D_{i} = \\sum B_{i}\\) through the multi-armed bandit method, the large model continues to be trained to update its parameters. As a result, the reward scores for each cluster will evolve. To balance the reward scores for each cluster between previous and current rounds, the reward score \\(I_{i}\\) for cluster i can be expressed as:\n\\(I = \\frac{R(C_{i})}{T(C)}\\)\nThen, update the Cluster Scores(CS) for all clusters:\n\\((CS)_{i} = \\hat{I_{i}} + \\alpha \\sqrt{\\frac{2 \\ln \\sum T(C)}{T(C_{i})}}\\)\nwhere \\(\\sum T(C)\\) denotes the cumulative count of rewards assigned to all clusters. In this way, in the next round, the cluster with the highest Cluster Score(CS) score among all clusters can be selected for sampling, and the influence score of the sampled data can be calculated as the reward score for that cluster. Using Cluster Scores (CS), a good balance between exploration and exploitation can be achieved."}, {"title": "3.3 ENHANCEMENT IN INFLUENCE CALCULATION", "content": "To assess the impact of each data sample on the model, one could retrain the large model with each data sample from \\(D_{r}\\). However, this approach is computationally prohibitive. Instead, the impact of each data sample point \\(z'\\) on the model \\(M\\) can be estimated by calculating the influence function for each sample point.\n\\(I = -\\nabla L(\\theta, D_{r})(H + \\mu I)^{-1} \\nabla L(\\theta, z')\\)\nIn the above expression, since the training of the large model often does not converge completely, resulting in a non-invertible Hessian matrix \\(H\\), a regularization term \\(\\mu I\\) is introduced.Bae et al. (2022)\nThe process of computing the influence function always involves two key steps:\n1. Approximate the inverse Hessian matrix \\(H^{-1}\\) and the gradient of the validation set \\(\\nabla L(\\theta, D_{r})\\) using the inverse Hessian vector product (iHVP)."}]}