{"title": "A Survey of Deep Learning for Group-level Emotion Recognition", "authors": ["Xiaohua Huang", "Jinke Xu", "Wenming Zheng", "Qirong Mao", "Abhinav Dhall"], "abstract": "With the advancement of artificial intelligence (AI) technology, group-level emotion recognition (GER) has emerged as an important area in analyzing human behavior. Early GER methods are primarily relied on handcrafted features. However, with the proliferation of Deep Learning (DL) techniques and their remarkable success in diverse tasks, neural networks have garnered increasing interest in GER. Unlike individual's emotion, group emotions exhibit diversity and dynamics. Presently, several DL approaches have been proposed to effectively leverage the rich information inherent in group-level image and enhance GER performance significantly. In this survey, we present a comprehensive review of DL techniques applied to GER, propos-ing a new taxonomy for the field cover all aspects of GER based on DL. The survey overviews datasets, the deep GER pipeline, and performance comparisons of the state-of-the-art methods past decade. Moreover, it summarizes and discuss the fundamental approaches and advanced developments for each aspect. Furthermore, we identify outstanding challenges and suggest potential avenues for the design of robust GER systems. To the best of our knowledge, thus survey represents the first comprehensive review of deep GER methods, serving as a pivotal references for future GER research endeavors.", "sections": [{"title": "I. INTRODUCTION", "content": "EMOTIO exhibits a profound influence on human perception, attention, memory, and decision-making, directly impacting both physical and mental well-being [1]. Consequently, the comprehension and perception of emotions not only enhance interpersonal communication but also implicitly contribute to the regulation of human physical health. With the evolution of big data technology, the broadening of ap-plication scenarios, and the progress in artificial intelligence technology, group-level emotion recognition (GER) has become a focal point for researchers [2, 3]. Unlike individual-level emotion recognition, which analyzes the emotions of individuals through facial expressions, speech, and postures, GER focuses on identifying the collective emotions of a group of people. Existing GER technology integrates diverse information, encompassing facial expressions, postures, social interactions, etc., to predict the emotions of a group. Taking facial expression data as an example, GER typically involves three key steps: first, given an image, detecting and extracting information such as faces and postures from images or videos; secondly, employing handcrafted descriptor or neural networks to extract facial features and other relevant information; finally, inputting these features as sequential data into another model like recurrent neural network to classify the group-level emotion. However, GER presents three key challenges. Firstly, the groups and contexts involved in GER may exhibit diversity and complexity. Secondly, the labeling of group-level emotion demands more nuanced considerations for the emotions expressed by the primary group, introducing additional complexities compared to individual-level emotion labeling. Lastly, due to the involvement of multiple individuals and diverse contexts, the recognition process becomes more complex than individual-level emotion recognition. Despite these challenges, GER remains an essential and formidable research area. It would be utilized to analyze emotion changes of a group of people, detecting abnormal behaviors and potential dangers in a timely manner in surveillance videos, or understanding students' learning status in collaborative learning [4].\nThis article provides a comprehensive survey of deep learning approaches for GER. Different from existing review [5], this paper elaborates on the methods of GER with a unique focus on deep learning architecture, providing insights into the current state and technical challenges associated with deep learning method in GER. In the beginning,we commence by providing an in-depth analysis of groups and emotions from a social perspective, offering a concise concept for GER. Subsequently, we present a thorough description of the image-based and video-based group-level emotion databases currently available for GER. Moreover, we explore recently developed deep learning methods for GER and scrutinize their technical challenges. In conclusion, we discuss the development trends of GER, offering valuable insights and guidance for future research and applications."}, {"title": "II. GROUP-LEVEL EMOTION", "content": "Emotions, traditionally viewed as individual-level phenomena in common parlance, have increasingly received attention in the field of social psychology as being potentially group-level [6]. Niedenthal and Brauer offer a broad definition of group-level emotions, characterizing them as emotions experienced by individuals on behalf of a group with which they identify [7]. This perspective suggests that group-level emotions are inherently more complex than their individual-level counterparts. Adopting both sociological and psychological perspectives, we will proceed by elucidating the concepts of groups and emotions before providing a precise definition of group emotions.\nThe fundamental disparity between group-level emotion and individual-level emotion arises from the distinction between the concepts of a group and an individual. While an individual pertains to an singular, independent entity, a group transcends mere aggregation, constituting a social phenomenon that amalgamates individuals into a collective entity. Various perspectives and definitions of the concept of a group have been proposed in scholarly literature [8\u201311]. Shaw et al. [8] defines a group as comprising two or more individuals engaged in mutual interaction and influence, emphasizing the significance of interaction among group members. Szilagi and Wallance [9] characterize a group as a collection of two or more individuals who interact and depend on each other to achieve a common goal. Additionally, Barron et al. [10] further elaborate on this definition by conceptualizing a group as individuals connected by some kind of bond, exhibiting varying degrees of cohesion. Moreover, Dasgupta et al. [11] describe a group as a group of people closely interconnected in some manner. According to these studies, a group consists of multiple individuals with a shared objective and direct or indirect interactions between them during a certain period of time. Thus, only when these conditions are met can a combination of multiple individuals be deemed a group.\nNumerous scholars have investigated the concept of emotion within the sociological and psychological field. Schachter et al. [12] proposed that emotion encompasses both a physiological arousal state and a cognitive state adapted to this physiological state. Kleinginna et al. [13] conducted a comprehensive review categorizing 92 different definitions of emotion from various literature sources. Ekman in [14] suggested that \u201cemotions evolve from the adaptive value of human beings in handling basic life tasks\u201d, indicating that emotions arise as adaptive responses to different situations encountered during social activities. Averill regards emotion as a complex of impulsive motivation in higher cognition, involving various psychological processes and physiological responses [15]. Cabanac defines emotion as any psychological experience with high intensity and high pleasure content, emphasizing the relationship between emotion and psychological experience [16]. Barrett posits that emotions are constructs of the brain's interpretation of external and internal stimuli [17]. Scherer views emotion as a biopsychological phenomenon resulting from the interaction of specific neural systems and physiological responses, cognitive assessments, and social-cultural factors [18]. Therefore, emotions can be understood as physiological and psychological responses to stimuli in our environment.\nBased on the foregoing elucidation of group and emotion, group-level emotion denotes the physiological and psychological responses elicited by multiple interacting individuals over a defined period. Scholars have proffered diverse definitions of group-level emotion in their studies. In the beginning, Hatfield et al. [19] characterize group-level emotion as the process whereby group members reciprocally influence each other through emotional contagion and empathy, culminating in emotional synchronization and consistency. Essentially, group-level emotion entails the reciprocal transmission and influence of emotions among individuals, leading to emotional consistency. Furthermore, group emotion is perceived as the emotional state propagated among members of a social group, cultivated and diffused through social interaction and shared experiences [20]. Bars\u00e4de and Gibson stressed the importance for researchers in the social science community to approach group-level emotions from both a \u201ctop-down approach\u201d and a \u201cbottom-up approach\u201d [21]. A \u201ctop-down approach\u201d suggests that emotion exhibited by group is represented at the group level and is felt by individual members, while the \u201cbottom-up approach\u201d highlights the unique compositional effects of individual-level group member emotions. According to the framework [21], Kelly and Bars\u00e4de further proposed that group-level emotion comprises affective compositional effects and affective context [22]. In essence, group-level emotion emerges from a combination of individual-level affective factors posed by group members and group-level factors that shape the emotional experience of the group. Additionally, Bars\u00e4de and Gibson further explore group-level emotion as the emotional state disseminated and shared among members of an organization [23]."}, {"title": "III. GROUP-LEVEL EMOTION DATASET", "content": "The rise of social media platforms has led to a surge in the volume of uploaded photos and videos, driving advancements in big data technologies and affective computing domains, es-pecially GER. In recent years, numerous group-level emotion datasets are established, attracting attention from researchers in the domains of affective computing and computer vision. However, the quality of annotation on images and videos plays an critical role in determining the efficacy of GER models. Thus, this section aims to provide an exhaustive examination of existing group-level emotion datasets, classifying them into two main types: image-based and video-based types.\nIn contrast to individual-level emotion datasets, group-level emotion datasets necessitate annotation for a group. Image-based datasets began to emerge since 2013, with notable representations including MultiEmoVA [24], Happiness Im-age database (HAPPEI) [25], Group-level Affect database (GAF) [2, 26, 27], Group cohesion dataset [28], GroupE-moW [29], and SiteGroEmo [3]. Among these database, HAPPEI, GAF, and the Group cohesion dataset, which were"}, {"title": "C. Dataset summary", "content": "The specific comparisons of the existing group emotion datasets are presented in Table I. Despite all datasets collecting images or videos from diverse real-world scenarios, their sample size is comparably small when compare with com-prehensive datasets like AffectNet [35]. This limited data size impedes the robust learning of group-level features. Recently, in the domain of micro-expression recognition, a composite dataset consolidating various micro-expression databases has become popular [36]. This approach corroborates the generalization capacity of the method across datasets with disparate characteristics, mitigating the issue of data scarcity. Such a strategy holds promise for GER by potentially augmenting data volumes, particularly for video-based datasets, and en-hancing the generalization ability of GER methods.\nState-of-the-art approaches, particularly those showcased in the EmotiW challenge, primarily undergo evaluation using image-based databases. All databases adopt an annotation strategy categorizing images into three valence-level cate-gories, as certain nuanced emotions such as fear and contempt pose challenges in data collection, resulting in limited samples for these categories, which are insufficient for robust learning.\nAs seen from Table I, except SiteGroEmo, each database has a balanced distribution of data across each class. Furthermore, in practical experiments, apart from the HAPPEI and Multi-EmoVA databases, other databases follow an official protocol where the train, validation, and test sets remain strictly fixed without random validation. Such rigid dataset partitioning may not be facilitate accurate assessments of GER method performance."}, {"title": "IV. INPUT MODALITY", "content": "Recognizing group-level emotions poses significant chal-lenges due to the diversity in group dynamic, individual emo-tion expressions, and limited data availability, deep learning (DL) methods, while promising, exhibit varied performance depending on the various modalities utilized. In this section, we describe the diverse information cues based on static image or video sequence utilized for GER and outline their respectively strengths and limitations."}, {"title": "A. Static image", "content": "Due to the abundant availability of facial images online such as AffectNet database [35], numerous existing studies in FER are conducted on static images. Leveraging the ef-ficiency demonstrated by FER with static images, numerous researchers have extended their efforts to GER, incorporating various additional cues such as face, scene, pose, even ob-jects. Convolutional Neural Networks (CNNs), notably VGG, ResNet and their variants, are commonly employed to analyze static images in GER research.\nCue aggregation as input. Given the flexible nature of group sizes, GER faces the challenge of effectively aggregating features from multiple individuals within a group to derive an overall emotional state. Existing methods are broadly catego-rized into two approaches. The first category involves averaging or weighted sum all individuals' emotion scores [39\u201342], which are typically outputted by a classifier such as Support Vector Machine (SVM). For example, Rassadin et al. [39] normalized detected faces and fed them into VGGFace and ImageNet. They constructed an ensemble of four Random Forest classifier trained on the features outputted by VGGFace, ImageNet, and landmark features. Finally, they employed a weighted sum approach to fuse the score outputted by the random forest classifiers. The second approach employs some machine learning algorithms such as bag-of-words and clustering to aggregate all individuals' features into a single feature vector. Balaji et al. [43], for example, utilized CNNS to extract face information. Subsequently, Fisher vector and VLAD encoding techniques were applied to compress all individual features in the image, yielding bottom-up features capable of representing group emotions. This method effec-tively compresses features, reduces computational complexity.\nMultimodality as input. Group are often considered as \u201cemotional entities and a rich source of varied manifestations of affect\u201d. Earlier discussions by Bars\u00e4de and Gibson in [23] emphasized the necessity for GER researchers to adopt both a \u201ctop-down approach\u201d and a \u201cbottom-up approach\u201d. Consequently, GER research has concentrated on integrating both"}, {"title": "B. Dynamic image sequence", "content": "As emotions are temporal in nature, such automatic tools in environments like factories, companies, and offices can help identifying interventions to maintain a healthy work culture. Facial expressions, being dynamic cues, evolve and change, revealing their effective signals over time. The visual information captured in videos plays a pivotal role in discerning the emotions depicted within them [52]. The temporal variations across video frames provides additional information to be exploited, albeit encoding these variations introduces complexity to emotion recognition. Training a network to comprehend the overall affect of a group of people shown across frames poses challenges. In this subsection, we delineate various dynamic inputs.\nTemporal information as input. Temporal information encapsulate the dynamics of an entire video sequence in a single instance. The temporal information, modeled by using algorithms like LSTM, has been successfully employed in GER to model scene dynamics and appearance in a video [32]. Similarly, active image encapsulated spatial and temporal information from video sequences into a single instance by estimating and accumulating changes in each pixel component. Sun et al. [53] utilized temporal segment networks to extract RGB information, optical flow frame, and warped optical flow frame for each video, incorporating a temporal shift module to model dynamic scene information. Quach et al. [33] introduced a fusion mechanism called Non Volume Preserving Fusion (NVPF) to better model spatial relationships between facial emotions in each frame, effectively addressing emotional ambiguity caused by insufficient facial resolution or undetectable emotions.\nFrame aggregation as input. Dynamic image sequence collected in the wild often include complex scene background. Petrova et al. [54] designed a method based on VGG-19 frame-work for each frame, capturing global emotions, followed by using score averaging and accumulation across all frames. Li et al. [55] proposed leveraging multi-task learning theory to aggregate frame features, while Liu et al. [56] employed four aggregation methods including maximum, minimum, average, and standard deviation to consolidate all individual's face feature in a group image.\nMultimodality as input. In analyzing group affect, audio features play a crucial role alongside facial image, as relying solely on facial expressions may lead to inaccuracies in estimating overall group affect. Pitch, speech rate, and duration, etc. have been found relevant to affect analysis. In group settings, these features are vital for distinguishing between situations like arguments and discussions, where the visual model may falter. Visual-audio fusion models have been proposed to enhance visual-based models [32, 55\u201358]. For example, Wang et al. [57] introduced a network called K-injection audiovisual network, which employs a multi-head cross-attention mechanism to jointly model audio and video data, integrating previous emotion knowledge to improve the model's generalization ability. Recent study indicate that human gesture can convery emotion [59]. Several researchers have incorporated human gesture features fused with scene and face cues for video-level GER [53, 56]. For example, Sun et al. [53] utilized CenterNet for human detection and pose estimation, followed by ResNetSt for extracting body feature. For prediction, the average probability of each frame's prediction was calculated, yielding the video's class prediction."}, {"title": "C. Discussion", "content": "The primary challenge faced by these methods lies in establishing relationships between modalities and effectively fuse them. As evident from the discussed methods, GER has increasingly emphasized mining the dynamic information present in videos. This trend is expected to drive further advancements in Recurrent Neural Networks (RNN) and its derivatives in group-level emotion recognition. Additionally, these studies have ventured beyong single-modal information, paving the way for research to better comprehend and leverage multimodal information. This broader perspective holds promise for enrich the understanding and utilization of diverse sources of information in group-level emotion recognition tasks."}, {"title": "V. DEEP NETWORKS FOR GER", "content": "Since the inception of artificial neurons in the 1940s, deep learning has been undergone extensive exploration and imple-mentation. Evolving from single-layer perceptrons to multi-layer neural networks, Convolutional neural networks (CNNs), Recurrent neural networks (RNNs), Cascade networks, Graph convolutional networks (GCNs), attention mechanisms, and beyond, deep learning has witnessed rapid evolution. So far, various deep learning approaches have emerged to discern the collective emotions of a group of people, leveraging diverse information such as facial expression, gestures, social interactions, and more."}, {"title": "A. Basic Network Block", "content": "Before describing the network architecture, Figure 2 first introduce basic network block widely used in GER, including CNN, RNN, GCN.\nCNN. Due to the limitations of traditional machine learning in addressing complex environments, many researchers have explored more in-depth research methods. LeCun et al. [60] first proposed a convolutional neural network model based on the backpropagation algorithm. However, due to hardware limitations at the time, the research progress of CNN was relatively slow. It was not until 2012 when Krizhevsky et al. [61] proposed the AlexNet CNN model in the ImageNet Large Scale Visual Recognition Challenge, which significantly surpassed traditional machine learning methods in accuracy and promoted the development of deep learning in the field of computer vision. Since then, various models based on CNN have been proposed, such as VGGNet, GoogLeNet, ResNet, DenseNet, and MobileNet. Meanwhile, CNN models have also shown their superiority in GER.\nRNN. In Convolutional Neural Networks (CNNs), each input and output are independent of each other, but this ignores the relationship between them. Although CNNs can extract good features when processing image datasets, they are not ideal for datasets with time series data, such as speech, audio, and video. Rumelhart et al. [62] proposed a method of Recurrent Neural Networks (RNN) that learns and trains through the backpropagation algorithm, and applied it to process data with time series. RNN has the characteristic of introducing a recurrent structure, allowing the network to remember and utilize previous information, thereby extracting better time series features. Various improved network architectures based on RNN have also been widely used, such as Simple Recurrent Neural Network (SRNN), Bidirectional Recurrent Neural Network (BRNN), Long Short-Term Memory Network (LSTM), and Gated Recurrent Unit (GRU), which have achieved good results based on RNN. Normally, RNN is always combined with CNN, leading to the cascade network for GER.\nGCN. Graph data has a wide presence in the real world, such as social networks, biological networks, recommendation networks, and chemical molecules. However, previous deep learning models based on CNN and RNN mainly deal with vector and matrix data, which neglects the topological structure of graphs and the relationships between nodes, leading to possible information loss and performance degradation. In order to solve this problem, Scarselli et al. [63] first proposed a new neural network model, namely, the graph neural network model, which extends existing neural network methods to handle data represented in the graph domain. Recent studies demonstrate that the effectiveness of graph convolutional networks (GCNs) in modeling semantic relationships, making them valuable for facial expression recognition (FER) tasks [64]."}, {"title": "B. Network architecture", "content": "The efficacy of FER neural units depends on how multiple networks are integrated. GER methods typically adopt one of five network architectures: single-stream, multi-stream, cas-cade, graph convolutional network, and attention mechanism. In this section, we delve into the specifics of each architecture.\n1) Single-stream networks: Typical deep GER methods adopt single CNN with individual input. In single-stream 2D CNNs, the primary input is facial images, while single-stream 3D CNNs directly extract spatial and temporal features from video sequences. Many studies [39, 40, 66] employ transfer learning strategy on deep networks pretrained on large-scale face datasets to mitigate the overfitting issues. For example, Rassadin et al. [39] employ a pre-trained VGGFace model on detected faces, followed by a weighted sum to obtain the final result using a random forest classifier. Similarly, Lu et al. [40] utilize a VGG model pretrained on the VGGFace dataset to extract facial features for GER.\nIn addition to transfer learning methods, several works design cascade network [67, 68] or kernel methods [69] on single-stream shallow CNNs. Sun et al. [67] explore various handcrafted feature (LBP), AlexNet, Reduced AlexNet and ResNet, followed by group-expression model or LSTM for group-level happiness intensity estimation. Building on this, Wei et al. [68] extend the group-level intensity estimation with VGGFace pretrained VGG-Face dataset. Furthermore, GER with ResNet18, ResNet34, MobileNet, DenseNet, Resnet50, Inception, GoogleNet, and VGG19 pretrained on Imagenet for scene-level information is explored in [41, 54]. The results demonstrate that VGG surpasses other architectures in GER and excels at distinguishing the complex hidden information in data.\nWhile the aforementioned works are based on 2D CNN with image input, several works employ 3D CNN variants [53, 53, 58] or cascade network [70] to directly extract spatial and temporal features from video sequences. Inflated ResNet-3D [58], Temporal shift module (TSM) [53], and Temporal Binding Network (TBN) [53] are introduced in GER. Additionally, a end-to-end cross-attention cascade network [70] combined the idea of ClipBERT [71] modules with the temporal sequence to enhance the representation in spatial and temporal dimensions.\n2) Multi-stream Networks: Single-stream model represent a basic structure in GER, extracting features solely from a single viewpoint such as the face or scene within group-level image. However, since group-level images encompass diverse and rich information, a single view may not provide sufficient insight. As we discussed in Section IV, employing various inputs from different perspectives can effectively explore spatial and temporal information. Hence, multi-stream networks have been adopted in GER to extract features through multiple inputs. Generally, multi-stream networks can be categorized into networks with two inputs, more than two inputs, and handcrafted features.\nMulti-stream networks with two inputs. According to [72], the face plays a crucial role in expressing the emotion. Therefore, among multi-stream networks with local and global inputs, the face remains a primary input. Several studies incorporated the face as local information and the scene as global information. They combined a convolution neural network for face while another neural network on scene descriptors for GER [42, 44, 45, 66, 73]. Experiment results of these studies demonstrate that models based on face outperforms those on other methods and other modality is still competitive and useful to GER. Moreover, the multi-stream networks have"}, {"title": "3) Cascade network:", "content": "For GER, dealing with varying num-bers of face between group-level images poses a significant challenge. As discussed in Section IV-A, current cascade networks can be categorized to two types. The first category utilizes variants of CNN such as ResNet, VGG, followed by decision-level score fusion. In contrast, the second category combines various CNN and RNNs to address the inconsistency in the number of faces between two group-level image. In this section, we will discuss the details of the second category [67, 70, 76, 80, 81].\nIn GER, a critical issue is how to effectively model the variability in the number of faces. LSTM is a primary method. Sun et al. [67] were the first to investigate the combination of CNNs and LSTM. They explored the use of AlexNet, Reduced AlexNet, and ResNet to extract the individual faces in a group. Subsequently, a weighted LSTM was used to assign different weights to each facial feature based on factors such as the size of the face and the distance between them. Furthermore, visual attention mechanisms were incorporated in LSTM for GER [70, 76]. In [70], a cascade network containing CNN and LSTM was employed to extract image-level and audio-level features, respectively. An attention mechanism was then introduced to compute important features at each time step. Additionally, Li et al. [82] employed a similar architecture at the face-level for GER. However, in contrast to [67], skeletons and scene features were directly fused at the end. Moreover, skeletons information extracted by OpenPose toolkit was considered in cascade network proposed by Slogrove et al. [80]. In their approach, the coordinates and confidence information of all individual key points were fed into an LSTM as a sequence for modeling, resulting in a group-level emotion classification. Additionally, speech signals were investigated for GER in [81]. They proposed a method incorporating a cascade network with multi-task learning for GER, using deep spectral features on speech signal. The cascade network based on CNN and RNN, was designed to extract discriminative deep spectral features, while multi-task learning combines emotion recognition and speaker identification tasks during the model training process."}, {"title": "4) GCN based network:", "content": "Individuals within a group often exhibit diverse social relationships with others. To highlight this social aspect, several works [3, 29] have utilized graph convolutional networks (GCNs) to model both visual features and social context within group-level images. In these approaches, the emotional states of individuals are treated as node features, while the interactions between individuals are represented as graph edges, thus forming a graph structure. Guo et al. [29] introduced a group-level emotion recognition method based on four cues, where faces, bodies, objects, and the entire image are transformed into a graph structure. This graph represents the relationships within the group based on these four cues, facilitating group-level emotion recognition. Additionally, Wang et al. [3] proposed a context-consistent cross-graph neural network to mitigate emotional biases resulting from different cues in multi-cue emotion recognition."}, {"title": "5) Attention based network:", "content": "In order to prioritize important character or object features that play a pivotal role in group emotions, five studies have incorporated attention mechanisms. Gupta et al. [83] detect local facial emotions by employing an attention mechanism to concentrate on more pertinent local information on the face, generating probability attention weights through the Softmax function. The weighted sum of facial features is then calculated based on these attention weights to produce a single facial feature vector representation. Additionall, Guo et al. [76] and Khan et al. [79] also integrated attention mechanisms. They introduced a novel region attention network (RAN) to detect and extract crucial features in facial regions. The region attention mechanism comprises"}, {"title": "C. Fusion stage and scheme", "content": "In GER, the fusion stage plays a crucial role in integrating information from multiple cues to improve the accuracy of emotion recognition. It encompass various methods for com-bining features extracted from different modalities, such as facial expressions, scene, skeleton etc. One common fusion approach is score-level, as used in studies like [47, 49, 76]. In their approaches, the output scores from individual modalities are combined using techniques like averaging or mean voting.\nAlternatively, feature-level fusion used in studies like those by [33, 48, 78], integrates raw feature representations extracted from each modality before feeding them into a classifier. This approach allows the model to learn more complex relationships between different modalities but may be more computationally intensive. Besides score-level and feature-level fusion, kernel-based [69, 88] and loss function-based [50, 74] fusion are two alternative ways to fuse multi-modality. For example, [74]"}, {"title": "D. Loss function", "content": "Different from classical methods, where the feature extraction and classification are independent, deep networks can perform end-to-end classification through loss functions by penalizing the deviation between predicted and true labels during training. Most GER works directly apply the commonly used softmax cross-entropy loss [39]. The softmax loss is typically effective at correctly classifying known categories. However, in practical classification tasks, the classification of unknown samples is also essential. Therefore, to achieve better generalization ability, it is crucial to further enhance inter-class difference and reduce intra-class variation, especailly for data scarcity. Metric learning techniques, such as contrastive loss [89], have been developed to ensure intra-class compactness and inter-class separability by measuring the relative distances between inputs. Wang et al. [90] proposed a contrastive learning-based self-attentive network. In this approach, different features are embedded into a vector space, and the similarity and difference of features are learned by en-hancing the similarity between samples of the same class and reducing the similarity between samples of different classes. Then, adaptive weight calculation and weighted average fusion are performed to adaptively fuse features at different levels. Although the above two methods have achieved good performance, they are still limited to static images. Additionally, metric learning loss often requires effective sample mining strategies for robust recognition performance. Metric learning alone may not suffice for learning a discriminative metric space for GER. To address these challenges, Zhang et al. [74] proposed a semi-supervised group-level emotion recognition framework based on contrastive learning to learn efficient features from both labeled and unlabeled images. To alleviate the uncertainty of given pseudo-labels, they introduce Weight Cross-Entropy Loss (WCE-Loss) to suppress the influence of samples with unreliable pseudo-labels in the training process.\nIn summary, although most current GER approaches utilize the standard softmax cross-entropy loss, only a limited number of studies have explored alternative loss functions such as contrastive learning loss or introduced novel loss functions to enhance inter-class separability, intra-class compactness, and achieve well-balanced learning. Looking ahead, investigating more effective loss functions targeting discriminative repre-sentations for group-level emotion features holds significant promise as a direction for future research in GER."}, {"title": "VI. EXPERIMENTS", "content": "The standard evaluation metric for GER typically involves using accuracy for group-level emotion recognition and mean square error for group happiness estimation. Accuracy assesses the proportion of correct predictions relative to the total number of evaluated samples, providing a measure of the model's overall performance in correctly identifying group-level emotion. Conversely, mean square error quantifies the"}, {"title": "B. Model Evaluation Protocols", "content": "Cross-validation stands as a widely utilized protocol for evaluating GER performance. This protocol involves dividing the dataset into train, validation, and test sets, ensuring fair verification of deep learning architectures on group emotion datasets. Cross-validation in the GER contains fixed partition validation and K-fold cross validation. The first kind of cross-validation is commonly used as the official evaluation method in the competitions such as the Emotion Recognition in the wild challenge (EmotiW) [27]. Here, the train, validation, and test sets are pre-determined and remain fixed throughout the competition, eliminating randomization. Participants receive the train and validation sets at the competition's outset for model development, while the test set is revealed later for final performance assessment and ranking. On the other hand, K-fold cross-validation protocol is also prevalent in GER research. This protocol involves randomly dividing the dataset into k equally sized parts, with each part serving as a test set in turn while the remaining portions constitute the training data. The process repeats k times, with each partition serving as the test set once. The choice for k, typically 4 or 5 in GER, can significantly impact evaluating time while ensuring robust performance assessment."}, {"title": "C. Performance analysis", "content": "Table II reports the performance of various deep learning models for GER as reported in EmotiW since 2016. With the exceptions of [54, 56", "91": "all methods fused more than two cues. Among these, face, pose/skeleton, and object are regarded as local component, while scene information serves as global information. This approach aligns with the concept of bottom-up and top-down components mentioned in group emotion theory. Furthermore, widely recognized network blocks such as VGG, Xception, ResNet and AlexNet have found extensive use in GER due to their promising performance in image and face recognition tasks. Additionally, the fusion of various networks enables better exploitation of complementary information between them. Common fusion schemes include Average and Feature concatenation, which provide straightforward solutions to the fusion problem. However, some researchers have proposed novel approaches to fuse multi-modality features. For example, Guo et al. [29", "50": "proposed a uncertain-aware learning to extract more robust representation from face, object, and scene modalities for GER. Moreover, unlike competitions, recently proposed GER methods have been evaluated across various databases, such as GroupEmoW, GAF, and GECV-GroupImg to access their generalization ability. With the introduction of databases like VGAF and GECV-GroupVid, researchers have started exploring the spatiotemporal GER.\nIn general, modality fusion can yield promising results across all datasets. Different modalities contribute diverse information, allowing for more comprehensive exploration of limited GER samples. Since the combined inputs offer robust GER solutions, multi-stream networks are recommended to effectively learn representations from available modalities. In contrast, single-modality approaches perform worse due to limited information and redundancy.\nFrom Tables III and IV, it is clear that fusion of scores and features is a common approach in integrating multiple modalities. Additionally, there is a growing trend towards using loss functions for multi-modality fusion. Fusion schemes like cross-attention, GCN, ECL, and NVPF have demonstrated state-of-the-art results across all databases. This is likely due to the challenges posed bythat the limited number of GER samples and group sizes, making leveraging additional data sources as reasonable and effective solution.\nPresently, scene information based on LSTM and averaging features across all faces are commonly employed in video-based GER studies. This is likely because flexible group sizes pose the main challenge for video-based GER. Re-cently, Quach et al. [33"}]}