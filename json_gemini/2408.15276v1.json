{"title": "A Survey of Deep Learning for Group-level Emotion Recognition", "authors": ["Xiaohua Huang", "Jinke Xu", "Wenming Zheng", "Qirong Mao", "Abhinav Dhall"], "abstract": "With the advancement of artificial intelligence (AI) technology, group-level emotion recognition (GER) has emerged as an important area in analyzing human behavior. Early GER methods are primarily relied on handcrafted features. However, with the proliferation of Deep Learning (DL) techniques and their remarkable success in diverse tasks, neural networks have garnered increasing interest in GER. Unlike individual's emotion, group emotions exhibit diversity and dynamics. Presently, several DL approaches have been proposed to effectively leverage the rich information inherent in group-level image and enhance GER performance significantly. In this survey, we present a comprehensive review of DL techniques applied to GER, proposing a new taxonomy for the field cover all aspects of GER based on DL. The survey overviews datasets, the deep GER pipeline, and performance comparisons of the state-of-the-art methods past decade. Moreover, it summarizes and discuss the fundamental approaches and advanced developments for each aspect. Furthermore, we identify outstanding challenges and suggest potential avenues for the design of robust GER systems. To the best of our knowledge, thus survey represents the first comprehensive review of deep GER methods, serving as a pivotal references for future GER research endeavors.", "sections": [{"title": "I. INTRODUCTION", "content": "EMOTIO exhibits a profound influence on human perception, attention, memory, and decision-making, directly impacting both physical and mental well-being [1]. Consequently, the comprehension and perception of emotions not only enhance interpersonal communication but also implicitly contribute to the regulation of human physical health. With the evolution of big data technology, the broadening of application scenarios, and the progress in artificial intelligence technology, group-level emotion recognition (GER) has become a focal point for researchers [2, 3]. Unlike individual-level emotion recognition, which analyzes the emotions of individuals through facial expressions, speech, and postures, GER focuses on identifying the collective emotions of a group of people. Existing GER technology integrates diverse information, encompassing facial expressions, postures, social interactions, etc., to predict the emotions of a group. Taking facial expression data as an example, GER typically involves three key steps: first, given an image, detecting and extracting information such as faces and postures from images or videos; secondly, employing handcrafted descriptor or neural networks to extract facial features and other relevant information; finally, inputting these features as sequential data into another model like recurrent neural network to classify the group-level emotion. However, GER presents three key challenges. Firstly, the groups and contexts involved in GER may exhibit diversity and complexity. Secondly, the labeling of group-level emotion demands more nuanced considerations for the emotions expressed by the primary group, introducing additional complexities compared to individual-level emotion labeling. Lastly, due to the involvement of multiple individuals and diverse contexts, the recognition process becomes more complex than individual-level emotion recognition. Despite these challenges, GER remains an essential and formidable research area. It would be utilized to analyze emotion changes of a group of people, detecting abnormal behaviors and potential dangers in a timely manner in surveillance videos, or understanding students' learning status in collaborative learning [4].\nThis article provides a comprehensive survey of deep learning approaches for GER. Different from existing review [5], this paper elaborates on the methods of GER with a unique focus on deep learning architecture, providing insights into the current state and technical challenges associated with deep learning method in GER. In the beginning,we commence by providing an in-depth analysis of groups and emotions from a social perspective, offering a concise concept for GER. Subsequently, we present a thorough description of the image-based and video-based group-level emotion databases currently available for GER. Moreover, we explore recently developed deep learning methods for GER and scrutinize their technical challenges. In conclusion, we discuss the development trends of GER, offering valuable insights and guidance for future research and applications."}, {"title": "II. GROUP-LEVEL EMOTION", "content": "Emotions, traditionally viewed as individual-level phenomena in common parlance, have increasingly received attention in the field of social psychology as being potentially group-level [6]. Niedenthal and Brauer offer a broad definition of group-level emotions, characterizing them as emotions experienced by individuals on behalf of a group with which they identify [7]. This perspective suggests that group-level emotions are inherently more complex than their individual-level counterparts. Adopting both sociological and psychological perspectives, we will proceed by elucidating the concepts of groups and emotions before providing a precise definition of group emotions.\nThe fundamental disparity between group-level emotion and individual-level emotion arises from the distinction between the concepts of a group and an individual. While an individual pertains to an singular, independent entity, a group transcends mere aggregation, constituting a social phenomenon that amalgamates individuals into a collective entity. Various perspectives and definitions of the concept of a group have been proposed in scholarly literature [8\u201311]. Shaw et al. [8] defines a group as comprising two or more individuals engaged in mutual interaction and influence, emphasizing the significance of interaction among group members. Szilagi and Wallance [9] characterize a group as a collection of two or more individuals who interact and depend on each other to achieve a common goal. Additionally, Barron et al. [10] further elaborate on this definition by conceptualizing a group as individuals connected by some kind of bond, exhibiting varying degrees of cohesion. Moreover, Dasgupta et al. [11] describe a group as a group of people closely interconnected in some manner. According to these studies, a group consists of multiple individuals with a shared objective and direct or indirect interactions between them during a certain period of time. Thus, only when these conditions are met can a combination of multiple individuals be deemed a group.\nNumerous scholars have investigated the concept of emotion within the sociological and psychological field. Schachter et al. [12] proposed that emotion encompasses both a physiological arousal state and a cognitive state adapted to this physiological state. Kleinginna et al. [13] conducted a comprehensive review categorizing 92 different definitions of emotion from various literature sources. Ekman in [14] suggested that \"emotions evolve from the adaptive value of human beings in handling basic life tasks\", indicating that emotions arise as adaptive responses to different situations encountered during social activities. Averill regards emotion as a complex of impulsive motivation in higher cognition, involving various psychological processes and physiological responses [15]. Cabanac defines emotion as any psychological experience with high intensity and high pleasure content, emphasizing the relationship between emotion and psychological experience [16]. Barrett posits that emotions are constructs of the brain's interpretation of external and internal stimuli [17]. Scherer views emotion as a biopsychological phenomenon resulting from the interaction of specific neural systems and physiological responses, cognitive assessments, and social-cultural factors [18]. Therefore, emotions can be understood as physiological and psychological responses to stimuli in our environment.\nBased on the foregoing elucidation of group and emotion, group-level emotion denotes the physiological and psychological responses elicited by multiple interacting individuals over a defined period. Scholars have proffered diverse definitions of group-level emotion in their studies. In the beginning, Hatfield et al. [19] characterize group-level emotion as the process whereby group members reciprocally influence each other through emotional contagion and empathy, culminating in emotional synchronization and consistency. Essentially, group-level emotion entails the reciprocal transmission and influence of emotions among individuals, leading to emotional consistency. Furthermore, group emotion is perceived as the emotional state propagated among members of a social group, cultivated and diffused through social interaction and shared experiences [20]. Bars\u00e4de and Gibson stressed the importance for researchers in the social science community to approach group-level emotions from both a \u201ctop-down approach\u201d and a \"bottom-up approach\u201d [21]. A \"top-down approach\u201d suggests that emotion exhibited by group is represented at the group level and is felt by individual members, while the \"bottom-up approach\" highlights the unique compositional effects of individual-level group member emotions. According to the framework [21], Kelly and Bars\u00e4de further proposed that group-level emotion comprises affective compositional effects and affective context [22]. In essence, group-level emotion emerges from a combination of individual-level affective factors posed by group members and group-level factors that shape the emotional experience of the group. Additionally, Bars\u00e4de and Gibson further explore group-level emotion as the emotional state disseminated and shared among members of an organization [23]."}, {"title": "III. GROUP-LEVEL EMOTION DATASET", "content": "The rise of social media platforms has led to a surge in the volume of uploaded photos and videos, driving advancements in big data technologies and affective computing domains, especially GER. In recent years, numerous group-level emotion datasets are established, attracting attention from researchers in the domains of affective computing and computer vision. However, the quality of annotation on images and videos plays an critical role in determining the efficacy of GER models. Thus, this section aims to provide an exhaustive examination of existing group-level emotion datasets, classifying them into two main types: image-based and video-based types."}, {"title": "A. Image-based datasets", "content": "In contrast to individual-level emotion datasets, group-level emotion datasets necessitate annotation for a group. Image-based datasets began to emerge since 2013, with notable representations including MultiEmoVA [24], Happiness Image database (HAPPEI) [25], Group-level Affect database (GAF) [2, 26, 27], Group cohesion dataset [28], GroupEmoW [29], and SiteGroEmo [3]. Among these database, HAPPEI, GAF, and the Group cohesion dataset, which were utilized in the EmotiW sub-challenge, have received significant attention and adoption by researchers in the fields of computer vision and affective computing.\nMou et al. [24] introduced a multi-dimensional group emotion image dataset, namely MultiEmoVA. This dataset was primarily constructed by scouring various social media platforms for real-life photos. Initially, 400 color images were collected, and after manually filtering images with ambiguous emotional expressions, 250 images meeting the criteria were remained. Furthermore, these images were categorized into positive, neutral, and negative. Additionally, Mou et al. also incorporated arousal-level annotation into the dataset, providing more explicit representations of the intensity for each emotion category.\nThe HAPPEI database contains images captured from social media platform and categorizes group-level emotions into neutral, small smile, big smile, smile, big laugh, and thrilled, totaling 2,638 images. In contrast, the GAF datbase delineates three emotion categories along the valence dimension: positive, neutral, and negative. The data collection process involved web search using keywords corresponding to various scenarios, such as weddings, birthday parties, and sports events, etc., to obtain images depicting group-level emotions in those contexts. Subsequently, these images were annotated by 2 to 3 experts in affective computing domain. The GAF database has undergone three iterations, namely GAF [26], GAF2.0 [2], and GAF3.0 [27]. The initial version, GAF includes 504 images [26], which is relatively small in scale and has been limited usage by researcher for evaluating the performance of deep GER models. However, the subsequent iterations, GAF2.0 and GAF3.0, witnessed a substantial increase in data size, featuring 6,467 [2] and 17,172 images [27], respectively.\nIn addition to emotion category, the cohesiveness of a group serves as a crucial indicator of the emotional state, structure and success of a group of individuals. Group Cohesion database was derived from GAF3.0 by adding cohesion labels [28]. Each image in this database was annotated with a cohesion score ranging from 0 to 3 by five annotators, where 0 represents no cohesion and 3 means strong cohesion. This database was utilized in the Emotiw2019 challenge [30], with 9,300 images allocated for training, 4,244 images for validation and 2,899 images for testing purposes.\nBesides the above-mentioned databases, another databases have recently proposed by Guo et al. [29] and Wang et al. [3], that is GroupEmoW and SiteGroEmo. GroupEmoW was created with a stringent criterion mandating that each image contains 2 to 9 individuals engaged in a specific activity, thereby forming distinct groups. According to this criterion, they collected 15,894 images from the internet, creating a diverse dataset with varying image resolutions and in-the-wild. Subsequently, these images were categorized into positive, neutral, and negative. On the other hand, the SiteGroEmo dataset diverges from existing database by capturing image across tourism scenes worldwide. This dataset not only contains rich geographic information and scene variations but also randomly captures the facial and body movements of individuals at a specific moment. Comprising a total of 10,034 images, the dataset is labeled with valence to denote emotions, specifically categorized as negative, neutral, and positive.\nIt is important to note that the aforementioned image-based datasets predominantly rely on internet keywords searches for data collection. While this method may expedite the establishment of satisfactory datasets, the existing datasets, with the exception of the MultiEmoVA database, solely employ valence-level annotations for images, lacking arousal-level annotations and more specific emotion categories such as anger and surprise. This limitation may arise from the diverse expression exhibited by participants in a group, making it challenging to annotate group-level images with a comprehensive emotion taxonomy. Additionally, manual annotation may introduce discrepancies in labeling due to cultural differences. Moreover, many images in these datasets may suffer from quality issues such as poor lighting, incomplete capture of facial expressions, or obstructions obscuring certain individuals. Finally, given their static nature, these images lack information about emotional dynamics. In the domain of emotion recognition research, scholars have emphasized that dynamic changes in facial expressions can offer crucial clues for both humans and computers to discern emotions or emotional processes [31]. Therefore, these uncontrollable circumstances and the absence of dynamic information may have a discernible impact on the accuracy of GER based on image."}, {"title": "B. Video-based datasets", "content": "In contrast to image-based datasets, video-based datasets not only capture the temporal dynamics of emotional expressions but also provide additional contextual information, facilitating a more comprehensive and accurate portrayal of changes in emotional states. However, due to the demanding collection and annotation process, only two video-based datasets have emerged recently since 2019: the VGAF dataset introduced by Sharma et al. [32] and the GECV dataset by Quach et al. [33].\nThe VAGF dataset comprises videos sourced from the YouTube platform, each featuring a varying number of individuals forming groups of different sizes. The dataset is partitioned into training, validation, and test sets, encompassing 2,661, 766, and 756 samples, respectively. Alongside valence annotation, the dataset includes cohesion metrics among individuals in the group. The corresponding dataset has also been used in EmotiW2023 [34]. Conversely, the GECV dataset contains videos captured in leisure and crowded scenes, amounting to a total of 627 videos. This dataset is further subdivided into three subsets: GEVC-SingleImg, GEVC-GroupImg, and GEVC-GroupVid. The latter two subsets are tailored to capture group emotions more effectively by showcasing various various group behaviors across different scenarios.\nWhile video-based databases offer richer semantic and contextual information compared to image-based databases, facilitating more discriminant criteria for data annotation, they are sourced from media platforms featuring complex scenes that often depict real-life scenarios. Nonetheless, they present limitations such as the absence of physiological signals akin to multi-modal emotion recognition, thereby posing challenges in data collection."}, {"title": "C. Dataset summary", "content": "The specific comparisons of the existing group emotion datasets are presented in Table I. Despite all datasets collecting images or videos from diverse real-world scenarios, their sample size is comparably small when compare with comprehensive datasets like AffectNet [35]. This limited data size impedes the robust learning of group-level features. Recently, in the domain of micro-expression recognition, a composite dataset consolidating various micro-expression databases has become popular [36]. This approach corroborates the generalization capacity of the method across datasets with disparate characteristics, mitigating the issue of data scarcity. Such a strategy holds promise for GER by potentially augmenting data volumes, particularly for video-based datasets, and enhancing the generalization ability of GER methods.\nState-of-the-art approaches, particularly those showcased in the EmotiW challenge, primarily undergo evaluation using image-based databases. All databases adopt an annotation strategy categorizing images into three valence-level categories, as certain nuanced emotions such as fear and contempt pose challenges in data collection, resulting in limited samples for these categories, which are insufficient for robust learning.\nAs seen from Table I, except SiteGroEmo, each database has a balanced distribution of data across each class. Furthermore, in practical experiments, apart from the HAPPEI and Multi-EmoVA databases, other databases follow an official protocol where the train, validation, and test sets remain strictly fixed without random validation. Such rigid dataset partitioning may not be facilitate accurate assessments of GER method performance."}, {"title": "IV. INPUT MODALITY", "content": "Recognizing group-level emotions poses significant challenges due to the diversity in group dynamic, individual emotion expressions, and limited data availability, deep learning (DL) methods, while promising, exhibit varied performance depending on the various modalities utilized. In this section, we describe the diverse information cues based on static image or video sequence utilized for GER and outline their respectively strengths and limitations."}, {"title": "A. Static image", "content": "Due to the abundant availability of facial images online such as AffectNet database [35], numerous existing studies in FER are conducted on static images. Leveraging the efficiency demonstrated by FER with static images, numerous researchers have extended their efforts to GER, incorporating various additional cues such as face, scene, pose, even objects. Convolutional Neural Networks (CNNs), notably VGG, ResNet and their variants, are commonly employed to analyze static images in GER research.\nCue aggregation as input. Given the flexible nature of group sizes, GER faces the challenge of effectively aggregating features from multiple individuals within a group to derive an overall emotional state. Existing methods are broadly categorized into two approaches. The first category involves averaging or weighted sum all individuals' emotion scores [39\u201342], which are typically outputted by a classifier such as Support Vector Machine (SVM). For example, Rassadin et al. [39] normalized detected faces and fed them into VGGFace and ImageNet. They constructed an ensemble of four Random Forest classifier trained on the features outputted by VGGFace, ImageNet, and landmark features. Finally, they employed a weighted sum approach to fuse the score outputted by the random forest classifiers. The second approach employs some machine learning algorithms such as bag-of-words and clustering to aggregate all individuals' features into a single feature vector. Balaji et al. [43], for example, utilized CNNS to extract face information. Subsequently, Fisher vector and VLAD encoding techniques were applied to compress all individual features in the image, yielding bottom-up features capable of representing group emotions. This method effectively compresses features, reduces computational complexity.\nMultimodality as input. Group are often considered as \"emotional entities and a rich source of varied manifestations of affect\". Earlier discussions by Bars\u00e4de and Gibson in [23] emphasized the necessity for GER researchers to adopt both a \u201ctop-down approach\u201d and a \u201cbottom-up approach\u201d. Consequently, GER research has concentrated on integrating both bottom-up and top-down components. Typically, bottom-up refers to individual emotions, while top-down encompasses contextual factors such as background information in an image. Recognizing the benefits of incorporating both bottom-up and top-down components, several studies [24, 43-50] have explored to fuse features from various cues in group-level images. For example, in Garg's work [46], a deep convolutional neural network is utilized to identify facial expressions within an image, while a Bayesian network leverages scene descriptors to extract visual features of the image content, thereby inferring the overall emotion of the image. In addition to these modalities, body information [24, 51] and object [49, 50] have also been incorporated into certain studies. In summary, GER research has commonly explored one or more cues extracted from face, pose/skeleton information, object, and scene context. Cues combination offers the advantage of enabling successful recognition of group-level emotions in the challenging environment when one cue is lacking. However, even employing multiple cues, a key concern arises regarding how to effectively establish connection among these cues to enhance the robustness of GER in real-world scenarios."}, {"title": "B. Dynamic image sequence", "content": "As emotions are temporal in nature, such automatic tools in environments like factories, companies, and offices can help identifying interventions to maintain a healthy work culture. Facial expressions, being dynamic cues, evolve and change, revealing their effective signals over time. The visual information captured in videos plays a pivotal role in discerning the emotions depicted within them [52]. The temporal variations across video frames provides additional information to be exploited, albeit encoding these variations introduces complexity to emotion recognition. Training a network to comprehend the overall affect of a group of people shown across frames poses challenges. In this subsection, we delineate various dynamic inputs.\nTemporal information as input. Temporal information encapsulate the dynamics of an entire video sequence in a single instance. The temporal information, modeled by using algorithms like LSTM, has been successfully employed in GER to model scene dynamics and appearance in a video [32]. Similarly, active image encapsulated spatial and temporal information from video sequences into a single instance by estimating and accumulating changes in each pixel component. Sun et al. [53] utilized temporal segment networks to extract RGB information, optical flow frame, and warped optical flow frame for each video, incorporating a temporal shift module to model dynamic scene information. Quach et al. [33] introduced a fusion mechanism called Non Volume Preserving Fusion (NVPF) to better model spatial relationships between facial emotions in each frame, effectively addressing emotional ambiguity caused by insufficient facial resolution or undetectable emotions.\nFrame aggregation as input. Dynamic image sequence collected in the wild often include complex scene background. Petrova et al. [54] designed a method based on VGG-19 framework for each frame, capturing global emotions, followed by using score averaging and accumulation across all frames. Li et al. [55] proposed leveraging multi-task learning theory to aggregate frame features, while Liu et al. [56] employed four aggregation methods including maximum, minimum, average, and standard deviation to consolidate all individual's face feature in a group image.\nMultimodality as input. In analyzing group affect, audio features play a crucial role alongside facial image, as relying solely on facial expressions may lead to inaccuracies in estimating overall group affect. Pitch, speech rate, and duration, etc. have been found relevant to affect analysis. In group settings, these features are vital for distinguishing between situations like arguments and discussions, where the visual model may falter. Visual-audio fusion models have been proposed to enhance visual-based models [32, 55\u201358]. For example, Wang et al. [57] introduced a network called K-injection audiovisual network, which employs a multi-head cross-attention mechanism to jointly model audio and video data, integrating previous emotion knowledge to improve the model's generalization ability. Recent study indicate that human gesture can convery emotion [59]. Several researchers have incorporated human gesture features fused with scene and face cues for video-level GER [53, 56]. For example, Sun et al. [53] utilized CenterNet for human detection and pose estimation, followed by ResNetSt for extracting body feature. For prediction, the average probability of each frame's prediction was calculated, yielding the video's class prediction."}, {"title": "C. Discussion", "content": "The primary challenge faced by these methods lies in establishing relationships between modalities and effectively fuse them. As evident from the discussed methods, GER has increasingly emphasized mining the dynamic information present in videos. This trend is expected to drive further advancements in Recurrent Neural Networks (RNN) and its derivatives in group-level emotion recognition. Additionally, these studies have ventured beyong single-modal information, paving the way for research to better comprehend and leverage multimodal information. This broader perspective holds promise for enrich the understanding and utilization of diverse sources of information in group-level emotion recognition tasks."}, {"title": "V. DEEP NETWORKS FOR GER", "content": "Since the inception of artificial neurons in the 1940s, deep learning has been undergone extensive exploration and implementation. Evolving from single-layer perceptrons to multi-layer neural networks, Convolutional neural networks (CNNs), Recurrent neural networks (RNNs), Cascade networks, Graph convolutional networks (GCNs), attention mechanisms, and beyond, deep learning has witnessed rapid evolution. So far, various deep learning approaches have emerged to discern the collective emotions of a group of people, leveraging diverse information such as facial expression, gestures, social interactions, and more. Figure 1 illustrates literature spanning database, emotion competition, method, and survey categories of academic papers employing deep learning based methods for GER over the past decade. It is noteworthy that the publication trend has exhibited a noticeable increase, especially attributed to the EmotiW competition. In this section, we delve into the approaches from the perspectives of specialized blocks, network architecture, fusion stage and scheme, and loss function."}, {"title": "A. Basic Network Block", "content": "Before describing the network architecture, Figure 2 first introduce basic network block widely used in GER, including CNN, RNN, GCN.\nCNN. Due to the limitations of traditional machine learning in addressing complex environments, many researchers have explored more in-depth research methods. LeCun et al. [60] first proposed a convolutional neural network model based on the backpropagation algorithm. However, due to hardware limitations at the time, the research progress of CNN was relatively slow. It was not until 2012 when Krizhevsky et al. [61] proposed the AlexNet CNN model in the ImageNet Large Scale Visual Recognition Challenge, which significantly surpassed traditional machine learning methods in accuracy and promoted the development of deep learning in the field of computer vision. Since then, various models based on CNN have been proposed, such as VGGNet, GoogLeNet, ResNet, DenseNet, and MobileNet. Meanwhile, CNN models have also shown their superiority in GER.\nRNN. In Convolutional Neural Networks (CNNs), each input and output are independent of each other, but this ignores the relationship between them. Although CNNs can extract good features when processing image datasets, they are not ideal for datasets with time series data, such as speech, audio, and video. Rumelhart et al. [62] proposed a method of Recurrent Neural Networks (RNN) that learns and trains through the backpropagation algorithm, and applied it to process data with time series. RNN has the characteristic of introducing a recurrent structure, allowing the network to remember and utilize previous information, thereby extracting better time series features. Various improved network architectures based on RNN have also been widely used, such as Simple Recurrent Neural Network (SRNN), Bidirectional Recurrent Neural Network (BRNN), Long Short-Term Memory Network (LSTM), and Gated Recurrent Unit (GRU), which have achieved good results based on RNN. Normally, RNN is always combined with CNN, leading to the cascade network for GER, as shown in Figure 2c.\nGCN. Graph data has a wide presence in the real world, such as social networks, biological networks, recommendation networks, and chemical molecules. However, previous deep learning models based on CNN and RNN mainly deal with vector and matrix data, which neglects the topological structure of graphs and the relationships between nodes, leading to possible information loss and performance degradation. In order to solve this problem, Scarselli et al. [63] first proposed a new neural network model, namely, the graph neural network model, which extends existing neural network methods to handle data represented in the graph domain. Recent studies demonstrate that the effectiveness of graph convolutional networks (GCNs) in modeling semantic relationships, making them valuable for facial expression recognition (FER) tasks [64], as depicted in Figure 2d. With the success of GCNS in FER, [65] introduced GCNs for GER, aiming to enhance performance by capturing inter-individual relationships."}, {"title": "B. Network architecture", "content": "The efficacy of FER neural units depends on how multiple networks are integrated. GER methods typically adopt one of five network architectures: single-stream", "networks": "Typical deep GER methods adopt single CNN with individual input. In single-stream 2D CNNs", "66": "employ transfer learning strategy on deep networks pretrained on large-scale face datasets to mitigate the overfitting issues. For example", "39": "employ a pre-trained VGGFace model on detected faces", "40": "utilize a VGG model pretrained on the VGGFace dataset to extract facial features for GER.\nIn addition to transfer learning methods", "68": "or kernel methods [69", "67": "explore various handcrafted feature (LBP)"}, {"68": "extend the group-level intensity estimation with VGGFace pretrained VGG-Face dataset. Furthermore", "54": ".", "58": "or cascade network [70"}, {"58": "Temporal shift module (TSM) [53", "53": "are introduced in GER. Additionally", "70": "combined the idea of ClipBERT [71", "Networks": "Single-stream model represent a basic structure in GER", "72": "the face plays a crucial role in expressing the emotion. Therefore", "73": ".", "74": "presented a semi-supervised group-level emotion recognition (SSGER) framework based on contrastive learning", "75": "proposed branches for both video and audio", "23": ".", "76\u201379": "have investigated combinations of more than one top-down component and bottom-up components. Guo et al. [51", "76": "further used visual attention attention mechanism to fuse face", "78": "proposed a two-stage architecture for GER. The first stage performs binary classification based on facial expression to distinguish \"Positive\" labels", "56": "investigated multi-stream network for dynamic GER. Spatio-temporal features and static features were exploited by Sun et al. [53"}, {"56": ".", "68": "combined deep features for face-level and handcrafted features for scene-level to leverage the low-level and high-level information for robust GER.\n3) Cascade network: For GER", "81": ".", "67": "were the first to investigate the combination of CNNs and LSTM. They explored the use of AlexNet", "76": ".", "70": "a cascade network containing CNN and LSTM was employed to extract image-level and audio-level features", "82": "employed a similar architecture at the face-level for GER. However"}, {"67": "skeletons and scene features were directly fused at the end. Moreover", "80": ".", "81": ".", "network": "Individuals within a group often exhibit diverse social relationships with others. To highlight this social aspect, several works [3, 29", "29": "introduced a group-level emotion recognition method based on four cues, where faces, bodies, objects, and the entire image are transformed into a graph structure. This graph represents the relationships within the group based on"}]}