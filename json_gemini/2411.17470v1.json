{"title": "Towards Precise Scaling Laws for Video Diffusion Transformers", "authors": ["Yuanyang Yin", "Yaqi Zhao", "Mingwu Zheng", "Ke Lin", "Jiarong Ou", "Rui Chen", "Victor Shea-Jay Huang", "Jiahao Wang", "Xin Tao", "Pengfei Wan", "Di Zhang", "Baoqun Yin", "Wentao Zhang", "Kun Gai"], "abstract": "Achieving optimal performance of video diffusion transformers within given data and compute budget is crucial due to their high training costs. This necessitates precisely determining the optimal model size and training hyperparameters before large-scale training. While scaling laws are employed in language models to predict performance, their existence and accurate derivation in visual generation models remain underexplored. In this paper, we systematically analyze scaling laws for video diffusion transformers and confirm their presence. Moreover, we discover that, unlike language models, video diffusion models are more sensitive to learning rate and batch size-two hyperparameters often not precisely modeled. To address this, we propose a new scaling law that predicts optimal hyperparameters for any model size and compute budget. Under these optimal settings, we achieve comparable performance and reduce inference costs by 40.1% compared to conventional scaling methods, within a compute budget of 1e10 TFlops. Furthermore, we establish a more generalized and precise relationship among validation loss, any model size, and compute budget. This enables performance prediction for non-optimal model sizes, which may also be appealed under practical inference cost constraints, achieving a better trade-off.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in Diffusion Transformers (DiT) [23, 35, 41, 49] have significantly enhanced video generation capabilities over previous convolutional-based neural models [3, 6, 15, 24, 47], demonstrating superior quality and diversity in generated video content [5, 14, 29, 34, 36, 39, 62, 64]. The success of video DiT can be largely attributed to the scalability of transformer architecture, where simply scaling up the model size often yields improved performance. Currently, the largest video generation model like Movie Gen [42], has reached a parameter count of 30 billion, which is significantly larger than the initial video DiT around 700M [36, 64]. However, due to limited training budgets, infinitely scaling up transformer size is not feasible. Consequently, determining the optimal DiT size within a given budget has become a critical challenge. Early research on transformer-based language models [1, 12, 19, 25, 28] revealed that model performance (measured by validation loss L), the model size N, and the given compute budget C, exhibit a power-law relationship, known as scaling laws. This discovery enables the use of smaller models for experiments to predict the performance of larger models, thereby enabling the selection of the optimal model size under real-world constrains. In the field of computer vision, researchers have primarily made empirical attempts in the scaling properties [31, 35, 38, 41, 44] of image DiT, seldom modeling precise scaling laws as done in language models. In the domain of video generation, such research is virtually nonexistent. Given the complexity and importance of video models-which must not only model visual structures but also simulate and understand complex real-world dynamics over time dimension-they are often trained at very high costs; for example, Movie Gen was trained using 6,144 H100 GPUs [42]. Therefore, developing accurate and practical scaling laws for large-scale video generation models is particularly important, as selecting the correct model size and training configurations can greatly reduce costs. This study aims to investigate the scaling laws of video diffusion transformers by addressing the following key question: Given a specific video transformer architecture and training budget, how can we predict the optimal model size, along with its corresponding performance and optimal hyperparameters-specifically, batch size and learning rate? We began our investigation by applying a commonly used scaling law from large language models [25] to video diffusion transformers. However, we found that due to the"}, {"title": "2. Setup", "content": "Notations. We will investigate scaling laws for video diffusion transformers from three critical perspectives: selecting the optimal batch size and learning rate to optimize performance, predicting the performance, and determining the optimal model/data allocation strategy to balance performance and compute budgets. To facilitate analysis in subsequent sections, we introduce the following notations: \u2022 L- the loss function for the video DiT on the validation set. \u2022 N - the number of model parameters, excluding all vocabulary and positional embeddings. \u2022 $n_{ctx}$ - the context window length, defined as $n_{ctx} = f h w$, where f is the number of frames, and h and w are the height and width of each frame, respectively. \u2022 $C_{token} = N(7 + \\frac{d}{n_{ctx}})$: The compute cost per token during training, where d represents the model dimension. \u2022 T - the number of training tokens, representing the dataset size measured in tokens. \u2022 $C \u2248 C_{token} T$ an estimate of the total non-embedding training compute. A more detailed per-operation parameter and compute"}, {"title": "3. Scaling Laws for Hyperparameters", "content": "Previous studies, such as [17, 37, 45, 48], have offered empirical insights into the batch size and learning rate relationship in large language model training. However, these investigations generally rely on heuristic methods, lacking rigorous theoretical foundations for optimal hyperparameter selection. Recent studies [2, 57] primarily focus on hyperparameter transfer, where hyperparameters are indirectly tuned on smaller models and then transferred to full-sized models without further tuning. However, most scaling law research ignores the impact of selecting hyperparameters for different model sizes, which can significantly affect fitting precision. Specifically, these works exhibit the following limitations when transfer to video DiT: \u2022 OpenAI's Scaling Law [28] suggests that smaller batch sizes are more computationally efficient but require more update steps to converge. Our experiments show that in video generation, scaling laws are highly sensitive to hyperparameters like batch size and learning rate. As shown in Figure 1, fixed suboptimal settings (gray points) lead to higher validation loss, while optimal settings (red points) align with the fitted loss curve, ensuring accurate predictions. Notably, these optimal hyperparameters vary with model size and training tokens. This demonstrates that there exists an optimal combination of hyperparameters that minimizes training loss and accurately captures scaling behavior. Our theoretical analyses further support the existence of these optimal hyperparameters. \u2022 Chinchilla's Scaling Law [25] establishes the relationship between the optimal model size $N_{opt}$ and compute"}, {"title": "3.1. Impact of model size on hyperparameters", "content": "We first investigate whether model size should be considered an independent variable in the scaling laws of batch size and learning rate. To optimize the objective function $L(\\theta)$, we need to compute its gradient $G(\\theta) = \\nabla L(\\theta)$. Since the objective function $L(\\theta)$ is complex, we approximate its gradient using the stochastic function. At iteration k, we randomly sample a mini-batch of data $\\{\\xi_k^{(b)}\\}_{b=1}^B$ from the training data distribution $p$. Using these samples, we compute an estimated gradient $g_k$ as:\n$g_k = \\frac{1}{B} \\sum_{b=1}^B G_{est}(\\theta_k; \\xi_k^{(b)})$  (1)\nwhere $\\xi_k^{(b)} \\sim p$ represents a random data sampled from a distribution $p$ at iteration k and $G_{est} (\\theta_k; \\xi_k^{(b)})$ is the stochastic gradient estimate for a single sample $\\xi_k^{(b)}$. To facilitate convergence analysis, we assume that, conditioned on the filtration $\\mathcal{G}_B$,\n$\\mathcal{G}_B = \\{\\theta_k, \\{g_k\\}, \\{\\xi_k^{(b)}\\}_{b=1}^B, \\{\\xi_{0:k-1}^{(b)}\\}_{b=1}^B, \\{\\theta_0\\}$  (2)\nthe mini-batch gradient $g_k$ is an unbiased estimate on $\\nabla L(\\theta_k) = G(\\theta_k)$, and the variance is bounded by $\\sigma^2$:\n$\\mathbb{E}_{\\xi_k \\sim p}[g_k | \\mathcal{G}_B] = G(\\theta_k)$  (3)\n$\\mathbb{E}_{\\xi_k \\sim p}[||g_k - G(\\theta_k)||^2 | \\mathcal{G}_B] \\le \\frac{\\sigma^2}{B}$  (4)"}, {"title": "3.2. Proposed $B_{opt}(N,T)$ equation", "content": "Firstly, we examine the variance of the gradient estimate in Mini-Batch SGD, formulated as,\n$\\mathbb{E}_{\\xi \\sim p}[g_k] = \\frac{1}{B} \\Sigma_k$  (6)"}, {"title": "Scaling law for optimal batch size", "content": "Based on the aforementioned analyses, we posit that under a fixed compute budget, there exists an optimal batch size $B_{opt}$ that achieves the best trade-off between gradient noise and the number of update steps, with the selection of the optimal batch size primarily influenced by the model size N and the total training tokens T. Referring to [1], we provide a concise formulation:\n$B_{opt} = A_B T^{\\beta_B} N^{\\gamma_B}$  (11)"}, {"title": "3.3. Proposed $\\eta_{opt}(N,T)$ equation", "content": "To determine an effective learning rate, we first consider the constraints that ensure each update step reduces the loss. Specifically, the expected change in loss at the k-th step, $\\Delta L_k = \\mathbb{E}[L(\\theta_{k} - \\eta g_k)] - L(\\theta_{k-1})$, must satisfy $\\Delta L_k < 0$. From this, we derive the following condition for the learning rate:\n$\\eta(B) < \\frac{2||G(\\theta_k)||^2}{G(\\theta_k)^T H_k G(\\theta_k) + \\frac{\\text{tr}(H_k \\Sigma_k)}{B}}$  (12)\nAs training progresses, the gradients become smaller, leading to tighter bounds on the learning rate to ensure effective updates."}, {"title": "Scaling law for optimal learning rate", "content": "Similar to the optimal batch size, the optimal learning rate aims to minimize L(\u03b8) under a fixed number of training tokens T by balancing the number of effective steps with the gain per step. Thus, we express the optimal learning rate as $\\eta_{opt} \\propto (\\frac{T}{B_{opt}})^{k_1} N^{k_2}$. Based on Equation (11), we derive:\n$\\eta_{opt} = \\alpha_{\\eta} T^{\\beta_{\\eta}} N^{\\gamma_{\\eta}}$  (14)"}, {"title": "4. Scaling Laws in Video DiT", "content": "In this section, we will predict the optimal model size under a fixed compute budget, utilizing optimal hyperparameters. Concurrently, we will fit the validation loss for any model size and training tokens under optimal hyperparameters. This not only allows us to accurately predict the validation loss at the optimal model size but also enables us to predict validation loss for any fixed suboptimal model size and compute budget, which significantly enhances the practical value of the fitting results as we always forgo the op-"}, {"title": "4.1. Scaling law for optimal model size", "content": "In this section, we explore how to optimally allocate model size N and training tokens T within a given compute budget C to maximize model performance. Building on the approach proposed in [1], we leverage the optimal hyperparameters defined in Equations (11) and (14) for our experiments. For each compute budget [3e17, 6e17, 1e18, 3e18, 6e18], we conducted five experiments to ensure reliable results. As shown in Figure 6a, we fit a parabola to each compute budget's results to identify the minimum loss point, which corresponds to empirical optimal model size under that specific budget.\n$N_{opt} = 1.5787 C^{0.4146}$  (15)\nWe will validate the accuracy of our results by extrapolating the validation loss in the subsequent sections."}, {"title": "4.2. Scaling law for performance", "content": "[19, 25] modeled validation loss as a function of model size N and training steps S or data size D, but did not detail hyperparameter settings, leaving it unclear if models achieved optimal performance across different compute budgets. [1] introduced a simplified power-law relationship between loss L and compute budget C, assuming optimal hyperparameters and model size. However, in practice, the optimal model size is often not used due to hardware limitations or the need to balance performance with model size. Thus, fitting the loss only at the optimal model size is insufficient for practical needs. To address these limitations, we propose a more general and accurate performance prediction method, capable of estimating loss across any model size and compute budget while using optimal hyperparameters. Similar to [25],"}, {"title": "4.3. Uncovering the advantages", "content": "In this section, we validate the advantages of our scaling laws under optimal hyperparameters. We demonstrate that our scaling laws precisely predict the optimal model size"}, {"title": "4.3.1 More parameter-efficient model sizes", "content": "We first validate our fitting results by extrapolating the compute budget to $5.85 \\times 10^{20}$. Under this condition, the optimal model size predicted by Equation (15) is 0.64B. Due to discrete layer selection, we conducted an experimental validation using a 14-layer model with 719.3M parameters. According to equations Equations (11) and (14), we set the batch size to 832 and the learning rate to $1.6 \\times 10^{-4}$. The resulting loss closely aligned with the prediction from Equation (16) (see Figure 5b), further validating our conclusions. To validate the optimal hyperparameters yield more precise fitting results, we conducted experiments with fixed suboptimal hyperparameters to fit the relationship between optimal model size and compute budget (Appendix C). As shown in Figure 6, using fixed suboptimal hyperparameters overestimates the optimal model size, and this discrepancy grows with increasing compute budgets. At a compute budget of $1 \\times 10^{10}$ TFlops, the scaling law based on optimal hyperparameters predicts a model that saves 39.9% of the parameters while achieving a similar validation loss. This occurs because, when computational resources are abundant relative to data, scaling the model size near the optimal size"}, {"title": "4.3.2 More precise validation loss predictions", "content": "To evaluate the precision of our scaling laws, we conducted experiments with both optimal and fixed suboptimal hyperparameters to fit the validation loss $L(N,T)$. With fixed suboptimal hyperparameters, the mean squared error (MSE) between the observed validation loss and the fitted curve is $4.31 \\times 10^{-7}$. Under optimal hyperparameters, however, the MSE decreases significantly to $2.35 \\times 10^{-7}$, representing a 45.5% reduction. This demonstrates the superior predictive capability of our scaling laws when using optimal hyperparameters. Additionally, we validate the accuracy of our predictions by fitting the predicted optimal model size based on the loss function derived under both optimal and fixed suboptimal hyperparameters (Table 3 and Appendix C). We then compare the predicted optimal model size with empirical optimal model size to assess the accuracy of the loss function. Specifically, we introduce a constraint incorporating com-"}, {"title": "5. Related Work", "content": "Text-to-Video Generation Models. Text-to-Video (T2V) generation has evolved from early GAN-based and VAE-based methods to modern architectures like diffusion models, DiT-based frameworks, and auto-regressive models. Early GAN approaches [11, 43, 51, 53, 61] struggled with temporal coherence, causing frame inconsistencies. Video diffusion models (VDMs) adapted text-to-image diffusion techniques to enhance frame continuity. DiT-based architectures [14, 34, 36, 40] introduced spatio-temporal attention mechanisms, improving the capture of complex video dynamics and frame coherence [4, 10, 16, 18]. Auto-regressive models [26, 30, 33, 52, 55, 56] use token-based methods to capture temporal dependencies, excelling in long-form video generation [21, 50, 54, 60, 63, 65] and video-to-video translation [27, 58, 59]. Scaling Laws. Scaling laws are crucial for understanding large neural networks' performance as the increase in model and data size. While well-established for LLMs [20, 22, 25, 28], scaling laws in the generative domain remains insufficient. In image generation, [31, 38] are mostly empirical, with precise scaling relationships still unclear, limiting our understanding of compute budgets, model size, dataset size, and performance. A concurrent work [32] attempt to establish precise scaling laws but was limited by smaller batch sizes and impractical hyperparameter configurations, reducing its applicability. Additionally, limited research has explored scaling laws in the video domain. The temporal complexity in video data suggests that scaling behaviors in video models may differ from other modalities, necessitating further investigation into video-specfic scaling laws."}, {"title": "6. Conclusion, Limitation and Future Work", "content": "In this work, we draw several key conclusions. First, we confirmed that video DiT models exhibit scaling laws, providing foundational theoretical support for the scalability of these models and establishing a basis for future applications with larger datasets and model size. Second, we establish scaling laws for optimal hyperparameters in video diffusion transformers that can predict the optimal hyperparameters"}], "supplementary_material": [{"title": "A. Experimental Settings and Main Results", "content": "In our experiments, we employ the Cross-DiT architecture [7], an efficient diffusion transformer model that incorporates cross-attention module to integrate text conditions. This architecture is optimized for high-quality image/video generation at reduced computational costs. Our model setup includes: \u2022 VAE[64] for encoding, PixArt-XL-2-512x512 [7] for initializing, and T5 for text encoding. \u2022 Input sequences of 17 frames with a resolution of 256x256."}, {"title": "B. Proof of Key Results", "content": "For convenience, we restate the two assumptions here:\n$\\mathbb{E}[g_k | \\mathcal{G}_B] = G(\\theta_k)$  (25)\n$\\mathbb{E}[||g_k - G(\\theta_k)||^2 | \\mathcal{G}_B] \\le \\frac{\\sigma^2}{B}$  (26)\nThe assumptions indicates that, the stochastic gradient $g_k$ is an unbiased estimate of $G(\\theta_k)$, and the variance is bounded by $\\sigma^2$. Using these two assumptions we get:\n$\\mathbb{E}[||g_k||^2 | \\mathcal{G}_B]$\n$= \\mathbb{E}[||g_k - G(\\theta_k) + G(\\theta_k)||^2 | \\mathcal{G}_B]$\n$= ||G(\\theta_k)||^2 + \\mathbb{E}[||g_k - G(\\theta_k)||^2 | \\mathcal{G}_B]$\n$\\le ||G(\\theta_k)||^2 + \\frac{\\sigma^2}{B}$  (27)\nwhere the second equality holds due to Equation (25) and the last inequality holds due to Equation (26)."}, {"title": "B.2. Convergence Rate of Mini-Batch SGD", "content": "Since $L(\\theta_k)$ is L-smooth,we have\n$\\mathbb{E}[L(\\theta_{k+1}) | \\mathcal{G}_B]$\n$\\le L(\\theta_k) + \\mathbb{E}[(G(\\theta_k), \\theta_{k+1} - \\theta_k) | \\mathcal{G}_B]$\n$\\frac{L}{2} \\mathbb{E}[|| \\theta_{k+1} - \\theta_k||^2| \\mathcal{G}_B]$\n$= L(\\theta_k) - \\eta \\mathbb{E}[(G(\\theta_k), g_k) | \\mathcal{G}_B]$\n$\\frac{L\\eta^2}{2} \\mathbb{E}[||g_k||^2| \\mathcal{G}_B]$\n$\\le L(\\theta_k) - \\eta (1 - \\frac{L\\eta}{2}) ||G(\\theta_k)||^2 + \\frac{L\\eta^2 \\sigma^2}{2B}$  (28)"}, {"title": "By taking expectations over the filtration $\\mathcal{G}_B$", "content": "we have\n$\\mathbb{E}[L(\\theta_{k+1})] \\le \\mathbb{E}[L(\\theta_k)] - \\eta (1 - \\frac{L\\eta}{2}) \\mathbb{E}[||G(\\theta_k)||^2] + \\frac{L\\eta^2 \\sigma^2}{2B}$  (29)\nThis is equivalent to\n$\\eta (1 - \\frac{L\\eta}{2}) \\mathbb{E}[||G(\\theta_k)||^2] \\le (\\mathbb{E}[L(\\theta_k)] - \\mathbb{E}[L(\\theta_{k+1})]) + \\frac{L\\eta^2 \\sigma^2}{2B}$  (30)\nTaking the average over $k = 0, 1, ..., K$, we have\n$\\frac{\\eta}{K+1} \\sum_{k=0}^K \\mathbb{E}[||G(\\theta_k)||^2] \\le \\frac{2(L(\\theta_0) - L^*)}{\\eta(K + 1)} + \\frac{L\\eta \\sigma^2}{B}$  (31)"}, {"title": "B.3. Stepwise Loss of Mini-Batch SGD", "content": "We approximate $G(\\theta_k)$ using a batch of B samples:\n$g_k = \\frac{1}{B} \\sum_{b=1}^B G_{est}(\\theta_k; \\xi_k^{(b)})$  (32)"}, {"title": "Equation (38) differs from Equation (39) by an absolute error of 0.1581", "content": "or 30.26%, indicating a significant discrepancy, larger than the one observed in the fitting results under optimal hyperparameters. This discrepancy stems from the poor fit of Equation (40), particularly for lower compute budgets."}]}