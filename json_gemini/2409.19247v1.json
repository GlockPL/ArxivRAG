{"title": "Edit-Constrained Decoding for Sentence Simplification", "authors": ["Tatsuya Zetsu", "Yuki Arase", "Tomoyuki Kajiwara"], "abstract": "We propose edit operation based lexically constrained decoding for sentence simplification. In sentence simplification, lexical paraphrasing is one of the primary procedures for rewriting complex sentences into simpler correspondences. While previous studies have confirmed the efficacy of lexically constrained decoding on this task, their constraints can be loose and may lead to sub-optimal generation. We address this problem by designing constraints that replicate the edit operations conducted in simplification and defining stricter satisfaction conditions. Our experiments indicate that the proposed method consistently outperforms the previous studies on three English simplification corpora commonly used in this task.", "sections": [{"title": "Introduction", "content": "Lexically constrained decoding (Anderson et al., 2017; Post and Vilar, 2018; Hu et al., 2019) allows to explicitly apply human knowledge in language generation, which has been employed in various tasks. Applications include machine translation using a bilingual dictionary of technical terms as constraints (Chatterjee et al., 2017; Hokamp and Liu, 2017), data augmentation using references as constraints (Geng et al., 2023b), style transfer using style-specific vocabulary as constraints (Kajiwara, 2019), knowledge-grounded generation (Choi et al., 2023), and commonsense reasoning using common concepts as constraints (Lu et al., 2021). The notable advantage of lexically constrained decoding is its direct applicability to decoders without the need for model (re)training.\nLexically constrained decoding is particularly appealing for sentence simplification, where the transformation of complex tokens to simpler ones is crucial. Laufer (1989) points out that learners of a foreign language need to know 95% of tokens in the input text to comprehend the text message. A recent evaluation metric for sentence simplification (Heineman et al., 2023) also focuses on word-level edit quality. Previous studies employed negative and positive lexical constraints, i.e., to suppress the output of difficult tokens (negative constraints) (Dehghan et al., 2022) and to encourage the output of their corresponding simpler alternatives (positive constraints) (Zetsu et al., 2022). These studies showed that such simple constraints significantly improve the quality of simplification. However, their naive constraints are not tight enough, which leads to sub-optimal simplification satisfying easier constraints yet failing on difficult ones.\nTo address this problem, we design constraints more directed to simplification associated with stricter satisfaction conditions. Sentence simplification conducts three edit operations to rewrite sentences, i.e., insertion, deletion, and substitution of tokens. We expand NeuroLogic decoding (Lu et al., 2021) to perform these edit operations via constrained decoding. Figure 1 illustrates our method that constrains generation using a substitution operation during beam search.\nWe evaluate our method using standard sentence"}, {"title": "Related Work", "content": "Sentence Simplification\nSentence simplification (Shardlow, 2014; Alva-Manchego et al., 2020b) paraphrases complex sentences into simpler forms. The primary approaches can be categorised into three types: (a) translation-based, (b) edit-based, and (c) hybrid approaches. The translation-based approach, e.g., (Nisioi et al., 2017; Zhang and Lapata, 2017; Kriz et al., 2019; Surya et al., 2019; Sheang and Saggion, 2021; Martin et al., 2022; Ansch\u00fctz et al., 2023), formalises sentence simplification as monolingual machine translation from complex to simple sentences. This approach learns rewriting patterns from corpora and thus allows flexible rewriting; however, the infrequent nature of simplification operations hinders a model from learning necessary operations. As a result, it tends to maintain complex tokens intact and end up in too conservative rewriting (Zhao et al., 2018; Kajiwara, 2019).\nIn contrast, the edit-based approach (Alva-Manchego et al., 2017; Dong et al., 2019; Kumar et al., 2020; Mallinson et al., 2020; Omelianchuk et al., 2021) rewrites a source sentence by editing, i.e., deleting, inserting, and replacing, its tokens. This approach can address the conservativeness problem owing to explicit token-by-token edits. However, it lacks the flexibility to rewrite an entire sentence to change its structure (Zetsu et al., 2022).\nFinally, the hybrid approach gets the best of both worlds by applying lexical constraints to translation-based models. Agrawal et al. (2021) biases a non-autoregressive simplification model by setting an initial state of decoding, considering the lexical complexity of a source sentence. Kajiwara (2019) and Dehghan et al. (2022) apply a negative lexical constraint using lexically constrained"}, {"title": "Enhanced Constrained Decoding", "content": "Earlier studies on lexically constrained decoding have focused on computational efficiency (Post and Vilar, 2018; Hu et al., 2019; Miao et al., 2019; Sha, 2020). Since its prevalence in text generation tasks, recent studies have expanded the types of constraints considered. Bastan et al. (2023) enhanced NeuroLogic decoding to consider syntactic constraints, namely, dependency types up to two entities. Similarly, Geng et al. (2023a) proposed constrained decoding by grammar and McCarthy et al. (2023) proposed finite-state constraints. Transformation of syntactic structure is another primary factor in simplification (Niklaus et al., 2019). In our method, structure transformation is implicitly handled by the base translation-based model. Our constraints are lexical but expand the simple concepts of positive and negative constraints to replicate edit operations conducted in sentence simplification."}, {"title": "Edit-Constrained Decoding", "content": "We expand NeuroLogic Decoding (Lu et al., 2021) to realise edit-based constraints, for its efficacy and computational efficiency when applied to sentence simplification (Zetsu et al., 2022)."}, {"title": "Preliminary: NeuroLogic Decoding", "content": "We briefly review NeuroLogic Decoding. Conceptually, it seeks for a hypothesis with the highest generation likelihood and constraint satisfaction:\n$\\underset{Y\\in\\mathcal{Y}}{\\text{arg max}} P_{\\theta}(Y|X) \\text{ s.t. } \\underset{i=1}{\\overset{L}{\\cap}} C_i = L,$\nwhere $P_{\\theta}(X|Y)$ is the likelihood to generate a sequence $Y = \\{Y_0, Y_1, \\dots, Y_n\\}$ given an input sequence $X = \\{x_0, x_1, \\dots, x_M\\}$, $\\mathcal{Y}$ is the possible generation space, and $C_i$ is $i$-th positive or negative constraint (in total $L$ constraints). Specifically, NeuroLogic Decoding achieves this by expanding the beam search to conduct pruning, grouping, and selecting processes at every timestep.\nNeuroLogic Decoding first prunes candidates by discarding hypotheses confirmed as non-satisfying constraints and then filtering out less"}, {"title": "Edit-based Constraints", "content": "Inspired by the edit operations conducted in sentence simplification, we define edit constraints of insertion, denoted as $C_I$, deletion, denoted as $C_D$, and substitution, denoted as a tuple $C_R = (C_R^i, C_R^o)$ where $C_R^i$ is replaced by $C_R^o$. These edit constraints are associated with satisfaction and dissatisfaction conditions. Furthermore, instead of a binary weight, we use a numerical score $s$ to allow flexibility in appreciating different types of constraints. Therefore, the second term in Equation (1) changes to maximise the total edit score:\n$\\sum_{i=0}^{t} \\gamma_i(s),$\nwhere $\\gamma_i(s)$ denotes the score that the $i$-th output token receives and $t$ is the current timestep. Note that $\\gamma_i$ corresponds to a node $n_i$ in the lattice of beam search; we use $\\gamma_i$ and $n_i$ interchangeably in the following for notation simplicity.\nInsertion operation is equivalent to the positive constraint. If the node $n_i$ is equivalent to the $j$-th insertion constraint $C_I^j$, the $n_i$ receives the score $s$ as follows.\n$s = \\begin{cases}\n\\lambda^I & \\text{if } n_i = C_I^j, \\\\\n0 & \\text{otherwise,}\n\\end{cases}$\nwhere $\\lambda^I \\in \\mathbb{R}_{+}$ is a weight of insertion operation.\nDeletion operation conceptually corresponds to the negative constraint. Previous studies (Kajiwara, 2019; Dehghan et al., 2022; Zetsu et al., 2022) naively defined negative constraints, i.e., they regard a negative constraint as satisfied if a certain word is simply avoided, without further conditions. However, we argue that the deletion operation, or negative constraint, requires more careful consideration to be properly handled. Otherwise, we may end up with a sub-optimal generation that satisfies only negative constraints $C_D$ at early timesteps. This is because the size of the vocabulary is typically much larger than that of deletion constraints; their satisfaction is far easier than the positive counterpart. As a result, hypotheses that have high likelihoods and exclude negative constraints at early timesteps remain in a beam, which struggles to satisfy harder positive constraints later on. For example, in Figure 2, the second beam candidate 'are old.' can be the best hypothesis because it satisfies all positive ('old') and negative (exclusion of 'remain' and 'aged') constraints and has high likelihood due to its shorter length.\nTo address this problem, we design a stricter condition for the satisfaction of deletion constraint. Intuitively, the deletion operation is regarded as satisfied by selecting sibling nodes other than the one equivalent to the constraint. Specifically, the node $n_i$ is regarded as satisfying the deletion constraint $C_D^j$ and avoids a penalty (negative score) if and only if its sibling node $n_k \\in \\pi$ is equivalent to $C_D^j$, where $\\pi$ denotes the set of sibling nodes of $n_i$ and $n_k$ in the lattice. These nodes receive the score $s$:\n$s = \\begin{cases}\n-\\lambda^D & \\text{if } \\exists n_k = C_D^j, \\\\\n& \\forall n_i \\in \\{\\pi \\backslash n_k = C_D^j\\}, \\\\\n0 & \\text{otherwise,}\n\\end{cases}$\nwhere $\\lambda^D \\in \\mathbb{R}_{+}$ is a weight of deletion operation. Note that no penalty is given when no node in siblings is equivalent to $C_D^j$, i.e., the score $s = 0$.\nSubstitution operation replaces a token $C_R^i$ to $C_R^o$. You may think that this operation can be realised by a combination of the insertion and deletion operations (or positive and negative constraints); however, it would lead to sub-optimal generation due to the lack of a mechanism to ensure the simultaneous satisfaction of both constraints like in (Zetsu et al., 2022). Therefore, similar to the"}, {"title": "Algorithm", "content": "Our method conducts the same pruning, grouping, and selecting processes as NeuroLogic Decoding at each timestep of beam search. The only difference is the pruning conditions. Namely, our method drops candidates whose edit scores (Equation (2)) are less than $\\delta \\in \\mathbb{R}_{+}$ from the top score."}, {"title": "Experiment Settings", "content": "We conducted two kinds of evaluations. The first evaluation (\u00a75) aims to compare the performance of lexically constrained decoding with different constraint designs. To make the effects of methodological differences clearer, this evaluation used oracle constraints extracted from references. The second evaluation (\u00a76) follows to observe the performance under a practical setting with predicted constraints. Preceding these evaluations, this section describes the common experiment settings."}, {"title": "Datasets", "content": "We measured the performance on three sentence simplification corpora, namely, Turk (Xu et al., 2016), ASSET (Alva-Manchego et al., 2020a), and AutoMeTS (Van et al., 2020). Turk and ASSET are commonly used public evaluation corpora in simplification. In addition, we also included AutoMeTS, aiming to investigate the effects of our method in\na domain where technical terms are prevalent, and thus, lexical paraphrasing is crucial.\nTurk focuses on lexical paraphrasing, i.e., replacing difficult tokens with simpler synonyms. Turk was created by crowd-sourcing, where annotators were instructed to rewrite sentences by reducing the number of difficult tokens or idioms.\nASSET expanded the Turk corpus to encompass a variety of rewriting patterns, not only lexical paraphrasing but also sentence splitting and deleting unimportant information. ASSET uses the same source sentences with Turk and crowdsourced these dynamic rewrites.\nAutoMeTS is a simplification dataset in the medical domain. It extracted pairs of expert-oriented and simpler sentences from the sentence-aligned English Wikipedia corpus (Kauchak, 2013) using the medical term dictionary selected from the Unified Medical Language System (Bodenreider, 2004). This dataset contains technical terms frequently that are expected to be properly rephrased. We discarded pairs whose source and reference were identical, which reduced the number of pairs from what was reported in the original paper.\nAs Turk and ASSET miss a training set, we employed Wiki-Auto (Jiang et al., 2020) following the convention. All of these corpora originate from English Wikipedia. Table 1 lists the numbers of training, validation, and test sets used."}, {"title": "Evaluation Metrics", "content": "As the primary evaluation metric to measure the quality of sentence simplification, we used SARI (Xu et al., 2016). SARI evaluates the success rates of addition, keeping, and deletion of tokens by comparing model-generated simplifications with the source sentence and references. The final score is computed by averaging F1 scores of addition, keep, and deletion operations. For fine-grained analysis of the effects of our edit-based constraints,"}, {"title": "Baselines", "content": "We compared our method to previous studies also using lexically constrained decoding, namely, Kajiwara (2019) and Zetsu et al. (2022). While lexically constrained decoding theoretically applies to any base models using decoders, we limited ourselves to a basic pretrained sequence-to-sequence model, namely BART (Lewis et al., 2020), expecting to make the analysis simpler and easier. Exploration of the best possible performance by applying to superior base models, such as large language models (LLMs) (Kew et al., 2023) or models trained with large corpus (Martin et al., 2022) constitutes our future work. We also compare our method to Agrawal et al. (2021) as the strong previous study that takes the same hybrid approach to simplification with us (see \u00a72). In addition, the performance of naive beam search on the fine-tuned BART-base was also examined as the bottom line."}, {"title": "Implementation", "content": "The baseline methods were replicated using the codes released by the authors. For methods using lexically constrained decoding, we fine-tuned BART-base with an AdamW (Loshchilov and Hutter, 2019) optimiser using the corresponding training set of each corpus. The fine-tuning was conducted for 5 epochs at maximum with early stopping based on validation SARI score. For inference, we set the beam size to 20.\nThe proposed method was implemented utilising the released codes of NeuroLogic Decoding. Its hyper-parameters, namely, $\\lambda^I, \\lambda^D, \\lambda^R$, and $\\delta$, were searched using Optuna (Akiba et al., 2019) to maximise validation SARI score. The $\\lambda$ values were searched in the range of [0, 1], while the $\\delta$ value"}, {"title": "Evaluation with Oracle Constraints", "content": "We first measure the performance of the proposed method under a setting where highly reliable constraints are given. This setting eliminates possible performance variations depending on the quality of constraints and clarifies how different lexically constrained decoding mechanisms contribute to sentence simplification."}, {"title": "Oracle Constraint Extraction", "content": "We extracted highly reliable constraints from references. Specifically, we conducted word alignment between the source and reference sentences in a corpus using the state-of-the-art word aligner (Lan et al., 2021). The alignment results were converted to constraints using simple heuristics below, which are referred to as \u2018oracle' constraints hereafter.\n\u2022 Source tokens of null alignment were set as deletion operations (the proposed method) or negative constraints (baselines).\n\u2022 Source tokens aligned to the identical tokens were set as insertion operations (the proposed method) or positive constraints (baselines).\n\u2022 Source tokens aligned to ones with different surfaces were regarded as substitution operations (the proposed method) or a set of negative and positive constraints (baselines).\nWhile Turk and ASSET provide multiple references, we used the every first reference of each source sentence for extracting the oracle constraints to keep the constraints and reference consistent. Therefore, the evaluation metrics were computed with these first references to meet the condition of oracle constraint extraction."}, {"title": "Results", "content": "Table 2 shows results on the test sets under the oracle setting. The proposed method largely outperforms the previous studies. Breakdown scores of\nSARI indicate that our method improved all operations. We conjecture that this is because the stricter satisfaction conditions allowed proper handling of edit-based constraints.\nAmong the methods employing lexically constrained decoding, Kajiwara (2019) uses only negative constraints. Table 2 indicates that this method generated significantly longer sentences than others. As we observed their outputs, sentences were often disfluent with repeated tokens. We conjecture that this is an adverse effect using multiple negative constraints, which may have corrupted the decoder.\nIn contrast, Zetsu et al. (2022) consider both positive and negative constraints, which may enable it to avoid this corruption problem. As we discussed in \u00a73.2, their conditions of constraint satisfaction are looser than ours. Table 3 shows the percentages of constraints satisfied by Zetsu et al. (2022) and the proposed method, where our method achieved significantly higher constraint satisfaction rates. We further investigated the differences between Zetsu et al. (2022) and our method by error analysis of their simplification outputs. We sampled 100 source sentences from the ASSET corpus"}, {"title": "Evaluation with Predicted Constraints", "content": "In this section, we evaluate the proposed method under a practical setting, where constraints are predicted by a simple neural model."}, {"title": "Constraint Prediction", "content": "We employed the simple constraint prediction model developed by Zetsu et al. (2022). It first predicts which tokens in a source sentence should be inserted, deleted, and substituted as a token classification problem. Specifically, we fine-tuned BERT-base (Devlin et al., 2019) using the oracle constraints extracted in \u00a75.1 as described in the original paper.\nFor tokens predicted as substitution, replacing tokens were determined using a lexical translation table, which was assembled from each training set using the Moses toolkit (Koehn et al., 2007)."}, {"title": "Results", "content": "Table 6 shows results on test sets where constraints were predicted. To ensure side-by-side comparison with Table 2, the evaluation metrics were computed using the same single references on Turk and ASSET (i.e., every first reference). \u00a7A.2 provides the results computed on multiple references, which show the same trends with Table 6. While the gains became smaller, the proposed method consistently outperformed the baselines on SARI. The results of Agrawal et al. (2021) and Kajiwara (2019) revealed their instability; they can be comparable to or even worse than the naive beam search, depending on the corpus. In contrast, our method achieved consistent improvements over the baselines.\nObviously, the quality of simplification by these"}, {"title": "Summary and Future Work", "content": "In this study, we proposed the edit-constrained decoding for sentence simplification. Our constraints directly replicate the edit operations involved in simplification and are associated with strict satisfaction conditions. These features allow us to avoid the sub-optimal generation observed in previous studies. The evaluation with oracle constraints confirmed that our method largely outperforms the previous methods using lexically constrained decoding. Furthermore, the evaluation with a simple constraint prediction model confirmed consistent improvement by our method over the previous studies even with imperfect constraints.\nIn future work, we will apply our edit-constrained decoding to LLMs to further enhance the sentence simplification technology. As Valentini et al. (2023) revealed, LLMs still lack the ability to control lexical complexities. The proposed method is promising as complementation on this issue. We will also improve the constraint prediction model, which should boost the quality of simplification by a large margin."}, {"title": "Limitations", "content": "To derive the full potential of our method for sentence simplification, we should improve the quality of constraint prediction as discussed in \u00a76.2. Employment of LLMs is promising given their high"}, {"title": "Additional Experiment Details", "content": null}, {"title": "Implementation Details", "content": "All the experiments were conducted on NVIDIA RTX A6000 (48GB memory) GPUs installed on a Ubuntu server. The main memory size was 1TB, and the central processor was AMD EPYC CPU.\nWe used a fine-tuned BART-base as the sequence-to-sequence generation model. For experiments targeting ASSET, we reused the model fine-tuned employing Turk's validation set (for early stopping) to save computational costs. Given that"}]}