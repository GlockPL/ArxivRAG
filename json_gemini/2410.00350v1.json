{"title": "Efficient Training of Large Vision Models via Advanced Automated Progressive Learning", "authors": ["Changlin Li", "Jiawei Zhang", "Sihao Lin", "Zongxin Yang", "Junwei Liang", "Xiaodan Liang", "Xiaojun Chang"], "abstract": "The rapid advancements in Large Vision Models (LVMs), such as Vision Transformers (ViTs) and diffusion models, have led to an increasing demand for computational resources, resulting in substantial financial and environmental costs. This growing challenge highlights the necessity of developing efficient training methods for LVMs. Progressive learning, a training strategy in which model capacity gradually increases during training, has shown potential in addressing these challenges. In this paper, we present an advanced automated progressive learning (AutoProg) framework for efficient training of LVMs. We begin by focusing on the pre-training of LVMs, using ViTs as a case study, and propose AutoProg-One, an AutoProg scheme featuring momentum growth (MoGrow) and a one-shot growth schedule search. Beyond pre-training, we extend our approach to tackle transfer learning and fine-tuning of LVMs. We expand the scope of AutoProg to cover a wider range of LVMs, including diffusion models. First, we introduce AutoProg-Zero, by enhancing the AutoProg framework with a novel zero-shot unfreezing schedule search, eliminating the need for one-shot supernet training. Second, we introduce a novel Unique Stage Identifier (SID) scheme to bridge the gap during network growth. These innovations, integrated with the core principles of AutoProg, offer a comprehensive solution for efficient training across various LVM scenarios. Extensive experiments show that AutoProg accelerates ViT pre-training by up to 1.85x on ImageNet and accelerates fine-tuning of diffusion models by up to 2.86x, with comparable or even higher performance. This work provides a robust and scalable approach to efficient training of LVMs, with potential applications in a wide range of vision tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "RECENT developments of Large Vision Models (LVMs) demonstrate the importance of model scale, dataset scale, and training scale. Two streams of models represent the development of LVMs, the representative discriminative models, Vision Transformers (ViTs), and the representative generative model, diffusion models. With powerful high model capacity and large amounts of data, ViTs have dramatically improved the performance on many tasks in computer vision (CV) [1], [2]. The pioneering ViT model [3], scales the model size to 1,021 billion FLOPs, 250\u00d7 larger than ResNet-50 [4]. Through pre-training on the large-scale JFT-3B dataset [5], the ViT model, CoAtNet [6], reached remarkable performance, with about 8\u00d7 training cost of the original ViT. For generative models, the recently popular Diffusion Transformer (DiT) [7] achieves superior performance on the ImageNet class-conditional generation task. Its training requires 950 V100 GPU days on 256\u00d7256 images, and 1733 V100 GPU days on 512\u00d7512 images, as estimated by [8]. The rapid growth in the training scale of LVMs inevitably leads to higher environmental costs. As shown in Tab. 1, recent breakthroughs of ViTs have come with a considerable growth of carbon emissions. Therefore, it is crucial to make LVM training sustainable in terms of computational and energy consumption.\nIn mainstream deep learning training schemes, all the network parameters participate in every training iteration. However, we empirically found that training only a small part of the parameters yields comparable performance in early training stages of ViTs. As shown in Fig. 1, smaller ViTs converge much faster in terms of runtime (though they would be eventually surpassed given enough training time).\nThe above observation motivates us to rethink the efficiency bottlenecks of LVM training: does every parameter, every input element need to participate in all the training steps?\nThe lottery ticket hypothesis [14] in the field of network pruning believes that a randomly-initialized, dense neural network contains a sub-network that can reach the perfor-mance of the full network after training for at most the same number of iterations. Here, we make the Growing Ticket Hypothesis of LVMs: the performance of a Large Vision Model, can be reached by first training its sub-network, then the full network after properly growing (or unfreezing), with the same total training iterations. The proper growing (or unfreezing) schedule is the Growing Ticket we need to find. This hypothesis generalizes the lottery ticket hypothesis [14] by adding a finetuning procedure at the full model size, changing its scenario from efficient inference to efficient training. By iteratively applying this hypothesis to the sub-network, we have the progressive learning scheme.\nRecently, progressive learning has started showing its capability in accelerating model training. In the field of NLP, progressive learning can reduce half of BERT pre-training time [15]. Progressive learning also shows the ability to reduce the training cost for convolutional neural networks (CNNs) [16]. However, these algorithms differ substantially from each other, and their generalization ability among architectures is not well studied. For instance, we empirically observed that progressive stacking [15] could result in significant performance drop (about 1%) on ViTs.\nTo this end, we take a practical step towards sustainable deep learning by generalizing and automating progressive learning on LVMs, including ViTs and diffusion models. To cover both the pre-training and fine-tuning of LVMs, we study them separately by using ViTs and diffusion models as the case study, respectively.\nWe focus on the efficient pre-training of ViTs as a represen-tative case of LVMs. To begin, we establish a robust manual baseline for progressive learning in ViTs by developing a growth operator. To evaluate the optimization process of this growth operator, we introduce a uniform linear growth schedule that operates along two critical dimensions of ViTs:"}, {"title": "2 RELATED WORK", "content": "Progressive Learning. Early works on progressive learn-ing [18], [19], [20], [21], [22], [23], [24], [25] mainly focus on circumventing the training difficulty of deep networks. Recently, as training costs of modern deep models are becoming formidably expensive, progressive learning starts to reveal its ability in efficient training. Net2Net [26] and Network Morphism [27], [28] studied how to accelerate large model training by properly initializing from a smaller model. In the field of NLP, many recent works accelerate BERT pre-training by progressively stacking layers [15], [29], [30], dropping layers [31] or growing in multiple network dimensions [32]. Similar frameworks have also been proposed for efficient training of other models [33], [34]. As these algorithms remain hand-designed and could perform poorly when transferred to other networks, we propose to automate the design process of progressive learning schemes.\nAutomated Machine Learning. Automated Machine Learn-ing (AutoML) aims to automate the design of model struc-tures and learning methods from many aspects, including"}, {"title": "3 AUTOMATED PROGRESSIVE PRE-TRAINING OF VISION TRANSFORMERS", "content": "3.1 Progressive Learning for Efficient Pre-training of Vision Transformers\nIn this section, we aim to develop a strong manual baseline for progressive learning of ViTs. We start by formulating progressive learning with its two main factors, growth schedule and growth operator in Sec. 3.1.1. Then, we present the growth space that we use in Sec. 3.1.2. Finally, we explore the most suitable growth operator of ViTs in Sec. 3.1.3.\nNotations. We denote scalars, tensors and sets (or sequences) using lowercase, bold lowercase and uppercase letters (e.g., $n$, $x$ and $\\Psi$). For simplicity, we use $\\{x_n\\}_{n=1}^l$ to denote the set $\\{x\\}_{i=1}^l$ with cardinality $|n|$, similarly for a sequence $(x_n)_{n=1}^I$. Please refer to Tab. 2 for a vis-to-vis explanation of the notations we used.\n3.1.1 Progressive Learning\nProgressive learning gradually increases the training over-load by growing among its sub-networks following a growth schedule $\\Psi$, which can be denoted by a sequence of sub-networks with increasing sizes for all the training epochs $t$.\nIn practice, to ensure the network is sufficiently optimized after each growth, it is a common practice [16], [30], [32] to divide the whole training process into $|k|$ equispaced stages with $T = |t|/|k|$ epochs in each stage. Thus, the growth schedule can be denoted as $\\Psi = (\\psi_k)_{k=1}^{|k|}$; the final one is always the complete model. Note that stages with different lengths can be achieved by using the same $\\psi$ in different numbers of consecutive stages, e.g., $\\Psi = (\\psi_{\\alpha}, \\psi_{\\iota}, \\psi_{\\b})$, where $\\psi_{\\alpha}$, $\\psi_{\\iota}$ are two different sub-networks.\nWhen growing a sub-network to a larger one, the param-eters of the larger sub-network are initialized by a growth operator $\\varsigma$, which is a reparameterization function that maps the weights $w_{k-1}$ of a smaller network in the $k - 1$ stage to $w_k$ of a larger one in stage $k$ by $w_k = \\varsigma(w_{k-1})$.\nLet $L$ be the target loss function, and $T$ be the total runtime; then progressive learning can be formulated as:\n$\\min_{\\omega, \\Psi, \\varsigma} \\{L(\\omega, \\Psi, \\varsigma), T(\\Psi)\\},$ (1)\nwhere $\\omega$ denotes the parameters of sampled sub-networks during the optimization. Growth schedule $\\Psi$ and growth op-erator $\\varsigma$ have been explored for language Transformers [15], [32]. However, ViTs differ considerably from their linguistic counterparts. The huge difference on task objective, data distribution and network architecture could lead to drastic difference in optimal $\\Psi$ and $\\varsigma$. In the following parts of this section, we mainly study the growth operator $\\varsigma$ for ViTs by fixing $\\Psi$ as a uniform linear schedule in a growth space $\\Omega$, and leave automatic exploration of $\\Psi$ to Sec. 3.2."}, {"title": "3.1.2 Growth Space in Vision Transformers", "content": "The model capacity of ViTs are controlled by many factors, such as number of patches, network depth, embedding di-mensions, MLP ratio, etc. In analogy to previous discoveries on fast compound model scaling [84], we empirically find that reducing network width (e.g., embedding dimensions) yields relatively smaller wall-time acceleration on modern GPUs when comparing at the same flops. Thus, we mainly study number of patchs ($n^2$) and network depth ($l$), leaving other dimensions for future works.\nNumber of Patches. Given patch size $p \u00d7 p$, input size $r \u00d7 r$, the number of patches $n \u00d7 n$ is determined by $n^2 = r^2/p^2$. Thus, by fixing the patch size, reducing number of patches can be simply achieved by down-sampling the input image. However, in ViTs, the size of positional encoding is related to $n$. To overcome this limitation, we adaptively interpolate the positional encoding to match with $n$.\nNetwork Depth. Network depth ($l$) is the number of Trans-former blocks or its variants (e.g., Outlooker blocks [9]).\nUniform Linear Growth Schedule. To ablate the optimiza-tion of growth operator, we fix growth schedule $\\Psi$ as a uniform linear growth schedule. To be specific, \u201cuniform\u201d means that all the dimensions (i.e., $n$ and $l$) are scaled by the same ratio $r_t$ at the $t$-th epoch; \u201clinear\u201d means that the ratio $r_t$ grows linearly from $r_1$ to 1. This manual schedule has only one hyper-parameter, the initial scaling ratio $s_1$, which is set to 0.5 by default. With this fixed $\\Psi$, the optimization of"}, {"title": "3.1.3 On the Growth of Vision Transformers", "content": "Fig. 3 (a)-(c) depict the main variants of the growth operator $\\varsigma$ that we compare, which cover choices from a wide range of the previous works, including RandInit [22], Stacking [15] and Interpolation [56], [85]. More formal definitions of these schemes can be found in the supplementary material. Our empirical comparison (in Sec. 5.4) shows Interpolation growth is the most suitable scheme for ViTs.\nUnfortunately, growing by Interpolation changes the original function of the network. In practice, function perturbation brought by growth can result in significant performance drop, which is hardly recovered in subsequent training steps. Early works advocate for function-preserving growth operators [26], [27], which we denote by Identity. However, we empirically found growing by Identity greatly harms the performance on ViTs (see Sec. 5.4). Differently, we propose a growth operator, named Momentum Growth (MoGrow), to bridge the gap brought by model growth.\nMomentum Growth (MoGrow). In recent years, a grow-ing number of self-supervised [86], [87], [88] and semi-supervised [89], [90] methods learn knowledge from the historical ensemble of the network. Inspired by this, we propose to transfer knowledge from a momentum network during growth. During training of the last stage (stage $k - 1$), this momentum network has the same architecture with $\\psi_{k-1}$ and its parameters $\\omega_{k-1}$ are updated with the online parameters $\\varepsilon_{k-1}$ in every training step by:\n$\\tilde{\\omega}_{k-1} = m\\tilde{\\omega}_{k-1} + (1 - m)\\omega_{k-1},$ (3)\nwhere $m$ is a momentum coefficient set to 0.998. As the the momentum network usually has better generalization ability and better performance during training, loading parameters from the momentum network would help the model bypass the function perturbation gap. As shown in Fig. 3 (d), MoGrow is proposed upon Interpolation growth by maintaining a momentum network, and directly copying"}, {"title": "3.2 Automated Progressive Learning for Pre-training", "content": "In this section, we focus on optimizing the growth schedule $\\Psi$ by fixing the growth operator as $\\varsigma_{MoGrow}$. We first formulate the multi-objective optimization problem of $\\Psi$, then propose our solution, called AutoProg, which is introduced in detail by its two estimation steps in Sec. 3.2.2 and Sec. 3.2.3.\n3.2.1 Problem Formulation\nThe problem of designing growth schedule $\\Psi$ for efficient training is a multi-objective optimization problem [91]. By fixing $\\varsigma$ in Eq. (1) as our proposed $\\varsigma_{MoGrow}$, the objective of designing growth schedule $\\Psi$ is:\n$\\min_{\\omega, \\Psi} \\{L(\\omega, \\Psi), T(\\Psi)\\},$ (5)\nNote that multi-objective optimization problem has a set of Pareto optimal [91] solutions which can be approximated using customized weighted product, a common practice used in previous Auto-ML algorithms [37], [92]. In the scenario of progressive learning, the optimization objective can be defined as:\n$\\min_{\\omega, \\Psi} L(\\omega, \\Psi) \\cdot T(\\Psi),$ (6)"}, {"title": "3.2.2 Automated Progressive Learning by Optimizing Sub-Network Architectures", "content": "Denoting $|\\Lambda|$ the number of candidate sub-networks, and $|k|$ the number of stages, the number of candidate growth schedule is thus $|\\Lambda|^{|k|}$. As optimization of Eq. (6) contains optimization of network parameters $\\omega$, to get the final loss, a full $|t|$ epochs training with growth schedule $\\Psi$ is required:\n$\\Psi^* = \\underset{\\Psi}{\\text{argmin }} C(\\omega^*(\\Psi); x) \\cdot T(\\Psi), \\\\ \\text{s.t. } \\omega^*(\\Psi) = \\underset{\\omega}{\\text{argmin }} L(\\Psi, \\omega; x).$ (7)\nThus, performing an extensive search over the higher level factor $\\Psi$ in this bi-level optimization problem has complexity $O(|\\Lambda|^{|k|} \\cdot |t|)$. Its expensive cost deviates from the original intention of efficient training.\nTo reduce the search cost, we relax the original objective of growth schedule search to progressively deciding whether, where and how much should the model grow, by searching the optimal sub-network architecture $\\psi_k$ in each stage $k$. Thus, the relaxed optimal growth schedule can be denoted as $\\Psi^* = (\\psi_k^*)_{k=1}^{|k|}$.\nWe empirically found that the network parameters adapt quickly after growth and are already stable after one epoch of training. To make a good tradeoff between accuracy and training speed, we estimate the performance of each sub-network $\\psi_k$ in each stage by their training loss after the first two training epochs in this stage. Denoting $\\omega^*$ the sub-network parameters obtained by two epochs of training, the optimal sub-network can be searched by:\n$\\psi_k^* = \\underset{\\psi_k \\in \\Lambda_k}{\\text{argmin }} C(\\omega^*(\\psi_k); x) \\cdot T(\\psi_k)^\\alpha, \\\\ \\text{where } \\Lambda_k = \\{\\psi \\in \\Omega \\mid |\\omega(\\psi)| \\ge |\\omega(\\psi_{k-1}^*)|\\}.$ (8)\n$\\Lambda_k$ denotes the growth space of the $k$-th stage, containing all the sub-networks that are larger than or equal to the previous optimal sub-network $\\psi_{k-1}^*$ in terms of the number of parameters $|\\omega|$."}, {"title": "3.2.3 One-shot Estimation of Sub-Network Performance via Elastic Supernet", "content": "Though we relax the optimization problem with significant search cost reduction, obtaining $\\omega^*$ in Eq. (8) still takes $2|\\Lambda_k|$ epochs for each stage, which will bring huge searching overhead to the progressive learning. The inefficiency of loss prediction is caused by the repeated training of sub-networks weight $\\omega$, with bi-level optimization being its nature. To circumvent this problem, we propose to share and jointly optimize sub-network parameters in $\\Lambda_k$ via an Elastic Supernet with Interpolation.\nElastic Supernet with Interpolation. An Elastic Supernet $\\Phi(\\tilde{\\omega})$ is a weight-nesting network parameterized by $\\tilde{\\omega}$, and is able to execute with its sub-networks $\\{\\psi\\}$. Here, we give the formal definition of weight-nesting:\nDefinition 3.1. (weight-nesting) For any pair of sub-networks $\\psi_{\\alpha}(\\omega_{\\alpha})$ and $\\psi_{\\b}(\\omega_{\\b})$ in supernet $\\Phi$, where $|\\omega_{\\alpha}| < |\\omega_{\\b}|$, if $\\omega_{\\alpha} \\subseteq \\omega_{\\b}$ is always satisfied, then $\\Phi$ is weight-nesting.\nIn previous works, a network with elastic depth is usually achieved by using the first layers to form sub-networks [60], [67], [68]. However, using this scheme after growing by Interpolation or MoGrow will cause inconsistency between expected sub-networks after growth and sub-networks in $\\Phi$.\nTo solve this issue, we present an Elastic Supernet with Interpolation, with optionally activated layers interpolated in between always activated ones. As shown in Fig. 4, beginning from the smaller network in the last stage $\\psi_{k-1}^*$, sub-networks in $\\Lambda_k$ are formed by inserting layers in between the original layers of $\\psi_{k-1}^*$ (starting from the final layers), until reaching the largest sub-network in $\\Lambda_k$.\nTraining and Searching via Elastic Supernet. By nesting parameters of all the candidate sub-networks in the Elastic supernet $\\Phi$, the optimization of $\\omega$ is disentangled from $\\psi$. Thus, Eq. (8) is further relaxed to:\n$\\psi_k^* = \\underset{\\psi_k \\in \\Lambda_k}{\\text{argmin }} L(\\psi^*; x) \\cdot T(\\psi_k)^\\alpha, \\\\ \\text{s.t. } \\psi^* = \\underset{\\Psi}{\\text{argmin }} E_{\\psi_k \\in \\Lambda_k} [L(\\psi_k, \\tilde{\\omega}; x)],$ (9)\nwhere the optimal nested parameters $\\tilde{\\omega}^*$ can be obtained by one-shot training of $\\Phi$ for two epochs. For efficiency, we train $\\Phi$ by randomly sampling only one of its sub-networks in each step (following [68]), instead of four in [64], [65], [67]. After training all the candidate sub-networks in the Elastic Supernet $\\Phi$ concurrently for two epochs, we have the adapted supernet parameters $\\tilde{\\omega}^*$ that can be used to estimate the real performance of the sub-networks (i.e. performance when trained in isolation). As the sub-network grow space $\\Lambda_k$ in each stage is relatively small, we can directly perform traversal search in $\\Lambda_k$, by testing its training loss with a small subset of the training data. We use fixed data augmentation to ensure fair comparison, following [93]. Benefiting from parameter nesting and one-shot training of all the sub-networks in $\\Lambda_k$, the search complexity is further reduced from $O(|\\Lambda_k| \\cdot |k|)$ to $O(|k|)$."}, {"title": "4 AUTOMATED PROGRESSIVE FINE-TUNING OF DIFFUSION MODELS", "content": "In this section, we set to solve the problem of efficient fine-tuning via automated progressive learning. We use diffusion models as a case study. We start by developing a strong manual baseline for progressive fine-tuning. Then, we present AutoProg-Zero for efficient fine-tuning.\n4.1 Progressive Learning for Efficient Fine-tuning of Diffusion Models\n4.1.1 Progressive Fine-tuning\nCurrent computer vision tasks benefits a lot from adapting large pre-trained models through fine-tuning. Progressive learning introduced previously can only be applied on the pre-training phase of vision models. The efficiency issue of fine-tuning large pre-trained models remains unsolved. In this section, we set to solve this issue by generalizing progressive learning to efficient fine-tuning.\nAs introduced previously, progressive pre-training grad-ually increases the training overload by growing among its sub-networks. However, training by routing through a sub-network during fine-tuning can significantly harm the performance of a pre-trained LVM. To achieve progressive fine-tuning, we can prune or remove part of the network by ranking the importance of the pre-trained parameters and then reverse this process by progressive growing. However, such a process could still harm the performance of the pre-trained network and may increase the overall training overload due to the extra evaluation and ranking process of the pre-trained parameters. Instead of progressive growing, we seek a simpler yet more efficient way inspired by parameter-efficient fine-tuning schemes, namely progressive unfreezing.\nProgressive Unfreezing. Progressive efficient fine-tuning gradually increases the training overload by first freezing all the parameters in the pre-trained models, then gradu-ally unfreezing the parameters in the model following a unfreezing schedule $\\Psi$, in analogy to the growth schedule in progressive growing for efficient pretraining. Each status of the unfreezing schedule can be denoted as a network with different learnable settings, $\\epsilon \\in \\{\\text{learnable, frozen}\\}$ for all the parameters. Similar to progressive pre-training, we divide the whole training process into $|k|$ equispaced stages. The unfreezing schedule $\\Psi$ can then be denoted by a sequence"}, {"title": "4.1.2 Bridging the Gap in Progressive Fine-tuning with Unique Stage Identifier", "content": "During progressive fine-tuning, when switching unfreezing stages, the input resolution and learnable sub-networks are changed substantially. Such a large distribution shift of input and optimization gap caused by unfreezing network parameters may lead to large fluctuations in the training dynamics during fine-tuning. Inspired by the subject-driven customization of diffusion models [97], we propose the Unique Stage Identifier (SID) to bridge the gap between different growing stages by customizing diffusion models in each unfreezing stage.\nUnique Stage Identifier (SID). Our goal is to minimize the fluctuations of the original \"dictionary\u201d of the diffusion model when switching training stages by adding a new (stage identifier [SID], training stage $k$) pair into the diffusion model's \u201cdictionary\u201d. For text-to-image diffusion models, we label all input images corresponding to the training stage as \u201ca [class noun], [SID] stage\u201d, where [SID] is a unique identifier associated with the training stage, and [class noun] is a class descriptor relevant to the input sample (e.g., flowers, birds, etc.). The class descriptor is usually given by the label or caption in the dataset. Class-conditional diffusion models take a class embedding, instead of the text, as the condition. The class embedding for each class can be denoted as $C \\in \\mathbb{R}^{1 \\times dim}$, where $dim$ is the hidden dimension of the class embeddings. For these models, a learnable stage embedding $\\text{SID} \\in \\mathbb{R}^{1 \\times dim}$ is added to the class embedding as the Unique Stage Identifier. For both the two types of models, we switch to a new SID at the beginning of each training stage.\nIncorporating a stage identifier in the condition (text or class condition) across different training stages helps anchor the model's prior knowledge of the class learned in earlier stages. This approach effectively maps the unique distribution specific to the current stage to its corresponding"}, {"title": "4.2 Automated Progressive Fine-tuning", "content": "In this section, we focus on the automatic optimization of the unfreezing schedule $\\Psi$ in Progressive Fine-tuning. We first formulate the multi-objective optimization problem of $\\Psi$ on the task of diffusion models fine-tuning, then propose our solution, called AutoProg-Zero, by generalizing AutoProg on fine-tuning and then introduce a novel zero-shot evaluation scheme for the unfreezing schedule $\\Psi$.\n4.2.1 Automated Progressive Fine-tuning by Zero-shot Metrics\nSimilar to the case for pre-training, the optimization of the unfreezing schedule $\\Psi$ for efficient fine-tuning is a multi-objective bi-level optimization problem following Eq. (5). As optimization of Eq. (5) contains optimization of network parameters $\\omega$, a full $|s|$ steps training is needed to get the optimal $\\omega^*$ for each unfreezing schedule $\\Psi$. Similar to the case in AutoProg-One, performing such extensive search over the higher level factor $\\Psi$ in this bi-level optimization problem has complexity $O(|\\Lambda|^{|k|} \\cdot |s|)$. We reduce the search cost to $O(|\\Lambda| \\cdot |k|)$ by relaxing the original objective of unfreezing schedule search to progressively optimize sub-network architecture $\\psi_k^*$ in each stage $k$, following the relaxation in Sec. 3.2.2."}, {"title": "Limitation of One-shot Schedule Search", "content": "In AutoProg-One for efficient pre-training, we optimize sub-network architec-ture $\\psi_k^*$ through their evaluation loss after one-shot training with Elastic Supernet. Through this, the search cost is further reduced to $O(|k|)$ and then zero through weight recycling, in Sec. 3.2.3. However, this approximation is not suitable for fine-tuning and progressive unfreezing. In progressive unfreezing, all candidate learnable configurations have the same forward function and evaluation performance after one-shot training. As an alternative, we seek to estimate the future performance of different unfreezing schedules through analysis of the backward pass and gradients. In sparse training and efficient Auto-ML algorithms, it is a common practice to estimate future ranking of models with current parameters and their gradients [98], [99], or with parameters after a single step of gradient descent update [46], [47], [54]. These methods are not suitable for AutoProg-One for progressive pre-training, as the network function is drastically changed and is not stable after growing. In contrast, during progressive fine-tuning, the network function remains unchanged after unfreezing. This approach enables the possibility of a zero-shot search for the candidate unfreezing schedule.\nAutoProg-Zero. Let $H$ be the zero-shot evaluation metric to predict the final loss of each learnable sub-network. Denoting $\\omega$ the zero-shot sub-network parameters directly inherited from the parameters of previous training, the optimal sub-network can be searched by:\n$\\psi_k^* = \\underset{\\psi_k \\in \\Lambda_k}{\\text{argmin }} \\{H(\\omega^*(\\psi_k)), T(\\psi_k)\\},$ (10)\nwhere $\\Lambda_k$ denotes the unfreezing search space of the $k$-th stage. By directly performing zero-shot evaluation on the inherited parameters, the bi-level optimization problem is relaxed to a single-level one. The process of AutoProg-Zero is shown in Fig. 5 and Algorithm 2.\nOverall, by relaxing the original bi-level optimization problem in Eq. (7) to Eq. (10), we avoid the cost of optimiz-ing the parameters $\\omega$ of the candidates. Thus, the search complexity is reduced drastically from $O(|\\Lambda|^{|k|} \\cdot |s|)$ to zero."}, {"title": "4.2.2 Zero-shot Proxy for Automated Progressive Learning", "content": "During progressive unfreezing, all the candidate choices of unfreezing schedules have the same model function (forward pass). Therefore, their loss and other performance are the same. We can not use loss at the current step as a proxy for the loss of the target step at the end of the current training stage. We design zero-shot metrics $H$ to measure the trainability, convergence rate, and generalization capacity of candidate learnable sub-networks at the beginning of each stage to predict their performance after the training of this stage. By doing this, we estimate the future ranking of models with current parameters and their gradients.\nSuppose we train a diffusion model denoiser function $\\epsilon(\\mathbf{x}, k)$ with parameters $\\omega$ using dataset $\\{\\mathbf{x}\\}$. The forward diffusion process progressively perturbs a training sample $\\mathbf{x}_0$ to a noisy version $\\mathbf{x}_k \\in [0, 1]$ by adding Gaussian noise. In the reverse process, diffusion model progressively denoises the noisy sample for $k$ steps from $\\mathbf{x}_1$ to recover the original sample. At each denoising timestep $k$, the noise $\\hat{\\epsilon}$ is predicted by a diffusion denoiser network $\\epsilon(\\mathbf{x}, k)$. The optimization"}, {"title": "5 EXPERIMENTS", "content": "5.  1 Implementation Details\n5.  1. 1 Implementation Details for Efficient Pre-training\nDatasets. We evaluate our method on a large scale image classification dataset, ImageNet-1K [106] and two widely used classification datasets, CIFAR-10 and CIFAR-100 [107], for transfer learning. ImageNet contains 1.2M train set images and 50K val set images in 1,000 classes. We use all the training data for progressive learning and supernet training, and use a 50K randomly sampled subset to calculate training loss for sub-network search.\nArchitectures. We use two representative ViT architectures, DeiT [1] and VOLO [9] to evaluate the proposed AutoProg. Specifically, DeiT [1] is a representative standard ViT model; VOLO [9] is a hybrid architecture comprised of outlook attention blocks and transformer blocks.\nTraining Details. For both architectures, we use the original training hyper-parameters, data augmentation and regular-ization techniques of their corresponding prototypes [1], [9]. Our experiments are conducted on NVIDIA 3090 GPUs. As the acceleration achieved by our method is orthogonal to the acceleration of mixed precision training [108], we use it in both original baseline training and our progressive learning. Grow Space $\\Omega$. We use 4 stages for progressive learning. The initial scaling ratio s1 is set to 0.5 or 0.4; the corresponding grow spaces are denoted by $\\Omega_{0.50}$ and $\\Omega_{0.40}$. By default, we use $\\Omega_{0.50}$ for our experiments, unless mentioned otherwise. The grow space of $n$ and $l$ are calculated by multiplying the value of the whole model with 4 equispaced scaling ratios $s \\in \\{0.5, 0.67, 0.83, 1.0\\}$, and we round the results to valid integer values. We use Prog to denote our manual progressive baseline with uniform linear growth schedule as described in Sec. 3.1.2."}, {"title": "5.  1. 2 Implementation Details for Efficient Fine-tuning", "content": "Datasets. We evaluate our method on 7 downstream image generation tasks, including class-conditional generation on ArtBench-10 [109], CUB-200-2011 [110], Oxford Flowers [111] and Stanford Cars [112], text-to-image generation on CUB-200-2011 [110] and Oxford Flowers [111] and customized text-to-image generation on DreamBooth dataset [97]. To use classification datasets without text annotation on text-to-image generation task, we constructed a standardized text prompt for training: \u201ca <class name>.\u201d. For customized text-to-image generation, we use the first 4 images of the class \u201cDOG6\u201d in DreamBooth dataset.\nArchitectures. In our experiments, we employed DiT-XL/2, which achieved a remarkable FID score of 2.27 on the ImageNet 256x256 dataset after 7 million training iterations. For the text-to-image generation task using Stable Diffusion, we utilized the pre-trained Stable Diffusion model, known for its strong performance in generating high-quality images from textual descriptions. Additionally, for the DreamBooth framework, we followed the same experimental setup as DiffFit, using the Stable Diffusion model.\nTraining Details. In our experiments with DiT, we adhered to the DiffFit settings, using a constant learning rate of 1e-4 for our proposed method. We configured the classifier-free guidance to 1.5 during evaluation and 4.0 for visualization, ensuring methodological consistency and enabling direct comparisons across different approaches. Our experiments were conducted on 8 A800 GPUs, utilizing a total batch"}, {"title": "5.2 Efficient Pre-training", "content": "5.  2. 1 Efficient Pre-training on ImageNet\nWe first validate the effectiveness of AutoProg-One for efficient pre-training on ImageNet. As shown in Tab. 3, AutoProg-One consistently achieves remarkable efficient training results on diverse ViT architectures and training schedules. First, our AutoProg-One achieves significant training acceleration over the regular training scheme with no performance drop. Generally, AutoProg-One speeds up ViTs training by more than 45% despite changes on training epochs and network architectures. In particular, VOLO-D1 trained with AutoProg $\\Omega_{0.40}$ achieves 85.1% training acceleration, and even slightly improves the accuracy (+0.1%). Second, AutoProg-One outperforms the manual baseline, the uniform linear growing (Prog"}]}