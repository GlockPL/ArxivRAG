{"title": "Kolmogorov-Arnold Transformer", "authors": ["Xingyi Yang", "Xinchao Wang"], "abstract": "Transformers stand as the cornerstone of mordern deep learning. Traditionally, these models rely on multi-layer\nperceptron (MLP) layers to mix the information between channels. In this paper, we introduce the Kolmogorov-Arnold\nTransformer (KAT), a novel architecture that replaces MLP layers with Kolmogorov-Arnold Network (KAN) layers to\nenhance the expressiveness and performance of the model. Integrating KANs into transformers, however, is no easy\nfeat, especially when scaled up. Specifically, we identify three key challenges: (C1) Base function. The standard B-spline\nfunction used in KANs is not optimized for parallel computing on modern hardware, resulting in slower inference speeds.\n(C2) Parameter and Computation Inefficiency. KAN requires a unique function for each input-output pair, making the\ncomputation extremely large. (C3) Weight initialization. The initialization of weights in KANs is particularly challenging\ndue to their learnable activation functions, which are critical for achieving convergence in deep neural networks. To\novercome the aforementioned challenges, we propose three key solutions: (S1) Rational basis. We replace B-spline functions\nwith rational functions to improve compatibility with modern GPUs. By implementing this in CUDA, we achieve faster\ncomputations. (S2) Group KAN. We share the activation weights through a group of neurons, to reduce the computational\nload without sacrificing performance. (S3) Variance-preserving initialization. We carefully initialize the activation weights\nto make sure that the activation variance is maintained across layers. With these designs, KAT scales effectively and readily\noutperforms traditional MLP-based transformers. We demonstrate the advantages of KAT across various tasks, including\nimage recognition, object detection, and semantic segmentation. It consistently enhances performance over the standard\ntransformer architectures of different model sizes. Our code is openly available at https://github.com/Adamdad/kat.", "sections": [{"title": "Introduction", "content": "Transformers have become the de facto architecture in deep learning, widely adopted in computer vision and\nnatural language processing. At their core, transformers are built upon two fundamental components: attention\nmodules and multi-layer perceptrons (MLPs). Although significant research has focused on replacing the traditional\nattention mechanism with alternative operations, these variants still lean heavily on MLPs.\nSurprisingly, there have been relatively few efforts aimed at enhancing MLPs themselves.\nOpening up the box, MLPs are composed of stacked linear layers coupled with non-linear activations. What makes\nit so popular is that, theoretically, they can approximate any function, assuming that there are enough neurons avail-\nable.\nHowever, despite their versatility, MLPs face limitations in modeling complex functions. For example, when using ReLU-like\nactivation, a two-layer MLP may struggle to fit periodic functions. Moreover, employing gradient descent to train these\nnetworks often results in prolonged convergence times for high-frequency components. These\nchallenges have led researchers to explore alternative, perhaps more expressive architectures than MLPs.\nRecently, Kolmogorov-Arnold Networks (KANs) emerged as a powerful alternative. KANs are noted for their theoretical\nparameter efficiency, potentially requiring fewer parameters to model complex functions. They are particularly\nsuitable for mathematical or symbolic regression tasks. The key to such success is the learnable\nbase function in each input-output pair. Those functions are often parameterized by B-spline curves. This\ndesign allows KANs to approximate more intricate functions through a summation of spline bases.\nGiven its potential, integrating KAN layers into transformers becomes an exciting topic. Such integration\nmay boost the expressiveness and efficiency of transformers, enhancing their competitiveness across a wide range of\napplications.\nUnfortunately, this ambition has been met with limited success. In particular, KANs have been reported to be \"10\u00d7 slower\nthan MLPs, given the same number of parameters\". Initial attempts to apply KANs to vision recognition tasks have yielded\ndisappointing results. Even on a small scale, these studies have consistently fallen short of matching, let alone surpassing,\nthe performance of traditional architectures. This lack of improvement is often attributed to the limited computational\nresources and ongoing scalability problems.\nIn a preliminary experiment, we attempted to replace MLP layers in the Vision Transformer (ViT) with KAN layers. It\ncreates a model, which we call ViT+KAN. However, as shown in Figure 1 (Right), this straightforward substitution led to\nsignificant challenges when performing ImageNet-scale training, resulting in poor performance. Scalability, therefore,\nremains a significant obstacle for KAN-based models.\nMotivation and Challenges. Through dedicated analysis, we have identified several key challenges that hinder the\neffectiveness of KANs in large-scale applications, ultimately limiting their scalability.\n\u2022 (C1) Base function. The standard B-spline functions in KANs are not ideal for parallel computing architectures typical\nof modern GPUs. B-splines require recursive computation, which significantly slows down even the most optimized\nimplementations.\n\u2022 (C2) Parameter and Computation Inefficiency. Each unique input-output pair in a KAN requires a distinct set of\nparameters and base functions. This necessity causes an exponential growth in the number of parameters as the\nnetwork's hidden size increases, resulting in substantial computational overhead and scalability issues.\n\u2022 (C3) Weight initialization. The weight initialization in KANs is similar to that in MLPs, but it does not meet KANs'\nneeds for convergence. This mismatch can lead to instability and degraded performance during the training process.\nOur Approach. In this paper, we introduce Kolmogorov-Arnold Transformer (KAT), which successfully integrates KANs\ninto transformers for large-scale training scenarios such as ImageNet. Beyond simple replacement, We have developed\nthree key innovations (S1-S3) to address these challenges (C1-C3) respectively.\n\u2022 (S1) Rational activation. We employ rational function as our base function and provide full CUDA implementation. It\naligns better with modern GPU architectures, enhancing computational efficiency and compatibility.\n\u2022 (S2) Group KAN. We share function coefficients and base functions among groups of edges. This strategy reduces\ncomputational load significantly without sacrificing performance.\n\u2022 (S3) Variance-preserving initialization. We carefully initialize weights to maintain consistent variance in activations\nacross the model's layers. This ensures stability during training and improves the model's learning dynamics."}, {"title": "Preliminary", "content": "The Kolmogorov-Arnold representation theorem states that any multivariate continuous function f, defined on a\nbounded domain, can be expressed as a finite composition of continuous univariate functions and addition. Specifically, for\na smooth function $f : [0, 1]^n \\rightarrow \\mathbb{R}$, it can be represented as:\n$$f(x_1,...,x_n) = \\sum_{q=1}^{2n+1} \\Phi_q(\\sum_{p=1}^n \\phi_{q,p}(x_p)$$\nHere, each function $\\phi_{q,p} : [0, 1] \\rightarrow \\mathbb{R}$ and $\\Phi_q : \\mathbb{R} \\rightarrow \\mathbb{R}$ are continuous. This means that the (2d+1)(d+1) univariate functions\n$\\Phi_q$ and $\\phi_{q,p}$ are enough for an exact representation of a d-variate function.\nThis theorm can be written in matrix form as follows:\n$$f(x) = \\Phi_{out} \\phi_{in} \\circ x$$\nwhere $\\phi_{in}$ and $\\Phi_{out}$ are defined as:\n$$\\phi_{in} = \\begin{bmatrix}\n\\phi_{1,1}(.) & ... & \\phi_{1,n}(.)\\\\\n: & : & :\\\\\n\\phi_{2d+1,1}(.) & ... & \\phi_{2d+1,d}(.)\n\\end{bmatrix}$$\n$$\\Phi_{out} = [\\Phi_1(.) \\qquad ... \\qquad \\Phi_{2d+1}(.)]$$\nThis decomposition illustrates how f can be built from simpler functions, showcasing an essential property of multivariate\ncontinuous functions."}, {"title": "Kolmogorov-Arnold Networks", "content": "Inspired by the Kolmogorov-Arnold representation theorem, define a generalized Kolmogorov-Arnold layer\nto learn univariate functions on edge, in the form of activation function. Formally, a Kolmogorov-Arnold layer with\n$d_{in}$-dimensional inputs and $d_{out}$-dimensional outputs is illustrated as\n$$f(x) = \\Phi \\circ x = [\\sum_{i=1}^{d_{in}} \\phi_{i,1}(x_i) \\qquad ... \\qquad \\sum_{i=1}^{d_{in}} \\phi_{i,d_{out}}(x_i)], \\text{ where } \\Phi =\\begin{bmatrix}\n\\phi_{1,1}(.) & ... & \\phi_{1,d_{in}}(.)\\\\\n: & & :\\\\\n\\phi_{d_{out},1}(.) & ... & \\phi_{d_{out},d_{in}}(.)\n\\end{bmatrix}$$"}, {"title": "Why original KAN fails to scale?", "content": "This section examines the scalability issues of KAN. We will explore three key factors: the choice of base function,\nredundant parameters and computation, and initialization problems. These design choices make the vanilla version of\nKAN resource-intensive and difficult to apply to large-scale models.\nThe use of B-spline functions in KAN layers introduces challenges when implemented\non GPUs. First, B-splines are not standard functions within CUDA. Implementing them using pure PyTorch and NumPy\nresults in slower performance on modern GPU devices due to the lack of optimized CUDA support. Second, the localized\nnature of B-Spline computations complicates their use in parallel GPU processes. Typically, each control point influences\nonly a small adjacent area of the curve. This leads to sparse or recursive computations, a type of operation that GPUs\nmanage less efficiently. Although there are efficient implementation for cubic B-Spline, scaling\nthese methods to higher orders is not straightforward.\nUnlike standard neural networks, KAN employs a learnable base func-\ntion for each pair of input-output channels. This design inherently leads to an increased parameter count and higher\ncomputational demands, especially when scaling up the width and depth of a neural network.\nIn the standard configuration of KAN, a layer with $d_{in}$ input and $d_{out}$ output channels incorporates an B-spline function for\neach input-output pair, of order K on G intervals. This results in the network having a total of $(d_{in}\\times d_{out})\\times(G+K+3)+d_{out}$\nlearnable parameters. In contrast, a typical MLP only needs $(d_{in} \\times d_{out}) + d_{out}$ parameters.\nIn terms of computation, the FLOPs for one sample in B-spline with De Boor-Cox Iterative formulation is\n$\\{FLOPS_{\\text{non-linear function}} \\times d_{in} + (d_{in} \\times d_{out}) \\times [9K \\times (G + 1.5K) + 2G \u2013 2.5K + 3]\\}$. Meanwhile, the FLOPs for an\nequivalent MLP layer is merely $(FLOPS_{\\text{non-linear function}} \\times d_{out} + 2 \\times (d_{in} \\times d_{out})\\}$.\nOverall, the parameter size and computational effort of KAN are on the order of O(G + K) and O(GK) times greater than\nthose of a conventional MLP, respectively. This significant increase in complexity is a primary reason why KAN struggles\nto scale effectively.\nDeep learning heavily relies on good weight initialization to enable trainability\nand convergence. A fundamental principle is to ensure variance-preserving, meaning that the variance of the signal should\nremain constant as it propagates through multiple layers, whether forward or backward. This\nprinciple ensures that the activation and gradient maintain stability across layers.\nHowever, in the KAN paper, the initialization strategy deviates from this principle. Specifically, the B-spline coefficients\n$c_i$ are initialized as $N(0, \\sigma^2)$ with $\\sigma = 0.1$, and $w_s = 1$ and $w_b \\sim U[\\frac{-6}{\\sqrt{d_{in}+d_{out}}}, \\frac{6}{\\sqrt{d_{in}+d_{out}}}]$, are initialized according to the\nXavier initialization. The combined output variance of the model can be expressed as:\n$$Var[\\varphi(x)] = Var[w_s \\text{silu}(x)] + Var[w_b \\text{spline}(x)] = \\mathbb{E}[\\text{silu}^2(x)] + \\mathbb{E}[\\text{spline}^2(x)]$$\nIf we assume the input x is normally distributed, $x \\sim N(0, \\epsilon)$ and consider a zero-th order spline, the variance of spline(x)\nat any point x is simply:\n$$\\mathbb{E}[\\text{spline}^2(x)] = \\sum_i c_i^2 Var[B_i(x)] = \\sigma^2 Var[B_i(x)] = \\sigma^2 = 0.01$$"}, {"title": "Kolmogorov-Arnold Transformer", "content": "As discussed earlier, the standard KAN faces three major challenges that limit its use in large, deep neural networks. In this\nsection, we refine its design to better suit modern transformers, allowing us to replace MLP layers with KANs."}, {"title": "Overall Architecture", "content": "Just as its name imply, Kolmogorov-Arnold Transformer (KAT) replaces the MLPs in vision transformer with\nKAN layers.\nSpecifically, for a 2D image $x \\in \\mathbb{R}^{H\\times W \\times C}$, we first flatten it into a 1D sequence, apply patch embedding and positional\nencoding, and then pass it through a series of KAT layers. At layer l, the following operations are performed:\n$$x_o^{(l)} = MSA(LN(x_e^{(l-1)})) + x_e^{(l-1)}, \\qquad l=1,...,L$$\n$$x_e^{(l)} = MLP(LN(x_o^{(l)})) + x_o^{(l)}, [Transformer]$$\n$$x_e^{(l)} = KAN(LN(x_o^{(l)})) + x_o^{(l)}, [KAT]$$\nwhere $x_e$ stands for the output feature sequence at the $l$ layer. As illustrated, we replace all two-layer MLPs with\ntwo-layer KANs while keeping the attention layers unchanged. Although similar efforts have been made in specific\ndomains, a simple replacement is not enough to achieve scalability in large models.\nMost importantly, here, we introduce a special kind Group-Rational KAN. We use rational functions as the base function for\nKAN and share parameters between a group of edges. We also specify the weight initialization\nscheme to ensure stable training. Together, these enhancements make KAT more scalable and improve\nperformance."}, {"title": "Rational Base Functions", "content": "In our method, we use the rational function as the base function for the KAN layer, instead\nof the B-spline.\nSpecifically, we parameterize the function $\\varphi(x)$ on each edge as rational over polynomials $P(x), Q(x)$ of order m, n.\n$$\\varphi(x) = wF(x) = w \\frac{P(x)}{Q(x)} = w \\frac{a_0 + a_1x + ... + a_mx^m}{b_0 + b_1x + ... + b_nx^n}$$\nan and bm are coefficient of the rational function and w is the scaling factor. This function is said to have degree m/n. We\nhope to learn those an, bm and w through end-to-end backpropagation.\nTo avoid instability caused by poles, where $Q(x) \\rightarrow 0$ and $\\varphi(x) \\rightarrow \\pm\\infty$, we employ a Safe Pad\u00e9 Activation Unit\n(PAU) as our basis, which is a modified form of the standard rational function\n$$F(x) = \\frac{a_0 + a_1x + ... + a_mx^m}{1 + |b_1x + ... + b_nx^n|}$$\nWhy use Rational Function? There are practical and theoretical reasons for selecting rational functions as our base\nfunctions.\nFirst, from an efficiency perspective, evaluating polynomials involves simple operations that are highly suitable for parallel\ncomputing. This makes rational functions computationally efficient for large-scale models.\nSecond, from a theoretical perspective, rational functions can approximate a wider range of functions\u2014including those\nwith singularities or sharp variations-more efficiently and accurately than polynomials. Since B-splines\nare essentially sums of local polynomials, rational functions offer a theoretical advantage over B-splines for modeling\ncomplex behaviors.\nThird, from a practical perspective, rational activations have already been successfully used as activation functions in\nneural networks.\nGiven these reasons, we adopt rational functions as the base functions in our KAN layers to enhance the model's\nexpressiveness, stability, and computational efficiency.\nWith the rational function, a core contribution in this paper is to implement it\nefficiently on parralized devices like GPU. In stead of using pytorch with automatic differentiation, we implement it fully\nwith CUDA."}, {"title": "Group KAN", "content": "Instead of learning a unique base function for each input-output pair, we can share their parameters within a group of edges.\nIt reduces the number of parameters and computation. This kind of parameter sharing and group-wise\ncomputation have been key techniques in neural network design.\nSpecifically, we divide the input channels din into g groups, sharing parameters among din/g input channels within each\ngroup. Unlike MLPs,\nwhich employ non-learnable activations, KAN assigns a unique function to each input-output pair. Group KAN reduces\nthe number of parameters by sharing these functions among a group of edges.\nWe combine the rational function of Section 4.2 with group-wise parameters to implement our\nGroup-Rational KAN (GR-KAN). In practice, we share the parameter for the rational function F for each group; however,\neach edge retains a unique scalar w.\nSuppose i is the index of the input channel. With g groups, each group contains $d_g = d_{in}/g$ channels, where $[i/d_g]$ is the\ngroup index. The operation of GR-KAN on input vector x can be expressed as\n$$GR\\text{-}KAN(x) = \\Phi \\circ x = [\\sum_{i=1}^{d_{in}} w_{i,1} F_{[i/d_g]}(x_i) \\qquad ... \\qquad \\sum_{i=1}^{d_{in}} w_{i,d_{out}} F_{[i/d_g]}(x_i)]$$\nWith a simple rewrite, this can be expressed in matrix form as the product of a weight matrix $W \\in \\mathbb{R}^{d_{in}\\times d_{out}}$ and a input-wise\nrational function F\n$$GR\\text{-}KAN(x) = WF(x) = \\begin{bmatrix}\nW_{1,1} & ... & W_{1,d_{in}}\\\\\n: & & :\\\\\nW_{d_{out},1} & ... & W_{d_{out},d_{in}}\n\\end{bmatrix} \\times [F_{[1/d_g]}(x_1) \\qquad ... \\qquad F_{[d_{in}/d_g]}(x_{d_{in}})]$$\nAs such, we can implement this GR-KAN layer as a group-wise rational function F followed by a linear layer\n$$GR\\text{-}KAN(x) = \\text{linear}(\\text{group\\_rational}(x))$$\nIn this form, sharing parameters across each input channel allows direct application of the rational function to the input\nvector, equivalently applying it across each grouped edge. In this way, GR-KAN functions as a specialized MLP, with 1)\nlearnable non-linear functions, 2) activation preceding the linear layer, and 3) unique activation functions tailored for each\ngroup of edges.\nIn experiments, we notice that for rational function, we share the denominator coefficient bn among all groups and use\ndifferent am for each group. It gets better performance.\nThe original KAN requires $d_{in} \\times d_{out}$ unique activation functions. Through our\ngrouping strategy, only g unique functions are needed, reducing the parameter count to a constant overhead compared to a\nstandard MLP.\nExcept the saving on parameter number, this grouping also reduces computational demands. Each input channel computes\nthe activation function $\\phi$ once, shared across all corresponding output channels. In contrast, the original KAN requires that\neach output channel j to independently compute $\\phi_{i,j}$. This results in significant computational savings."}, {"title": "Variance-Preserving Initialization", "content": "In this section, we aim to initialize the values for $a_m, b_n$ and w in Group-Rational KAN to ensure variance-preserving\nbehavior across the network. At its core, we prevent the growth or reduction of activation magnitudes throughout the\nlayers, thereby maintaining stability.\nWe revisit the analysis from and adapt it to KANs. For a GR-KAN layer, the computation for each output y; is\ngiven by $y_j = \\sum_{i=1}^{d_{in}} \\varphi(x_i) = \\sum_{i=1}^{d_{in}} (w_{i,j}F(x_i)) + b_j$. We assume that all $x_i$ are mutually independent and uniformly\ndistributed. Here, $w_{i,j}$ follows a normal distribution $N(0, \\sigma_o^2)$ and $b_j$ is initialized to zero. The variance of $y_j$ can then be\ndescribed as:\n$$Var[y] = d_{in} Var[wF(x)]$$\n$$Var[y] = d_{in}Var[w]\\mathbb{E}[F(x)^2]$$\nwhere x, y, and w represent the random variables of each element in $x_i, y_j,$ and $w_{ij}$ respectively. When layers are stacked,\nwe aim for the variance of the input-output activations to remain consistent, expressed as:\n$$Var[x] = d_{in}Var[w]\\mathbb{E}[F(x)^2]$$\nSince F(x) is the rational function containing coefficients am and bn, the initialization of w and these coefficients are\ninterdependent-the form of F(x) influences the appropriate initialization of w. The crucial step is to calculate $\\frac{Var[x]}{\\mathbb{E}[F(x)^2]}$\nand adjust w to maintain consistent activation scaling.\nFor our rational function defined in Equation 12, computing $\\mathbb{E}[F(x)^2]$ involves evaluating:\n$$\\mathbb{E}[F(x)^2] = \\int_{-\\infty}^{+\\infty} F(x)^2 f(x)dx = \\int_{-\\infty}^{+\\infty} (\\frac{a_0 + a_1x + ... + a_mx^m}{1 + |b_1x + ... + b_nx^n|})^2 f(x)dx$$\nwhere f(x) is the density function of x. Unlike activation functions such as ReLU, for which $\\mathbb{E}[F(x)^2] = \\frac{1}{2} Var[x]$,\ncomputing $\\mathbb{E}[F(x)^2]$ for the rational function is challenging due to the lack of a closed-form solution.\nTo make the process manageable, Instead of sampling w, a, and b jointly, we\nproceed sequentially. Initially, we determine a and b such that F fits established activations like ReLU, GELU, and Swish.\nOnce a and b are set, we estimate the gain $ \\alpha = \\frac{\\mathbb{E}[F(x)^2]}{Var x} $ numerically, assuming $x \\sim N(0, 1)^2$. The calculated gains, a, are\ndocumented in Table 3. We use the gain value to initialize w from $N(0, \\frac{\\alpha}{d_{in}})$.\nIn addition to random weight initialization, we can also transfer weights from a pre-trained ViT\nto our KAT model. This transfer is straightforward for most layers, as KAT can replicate the micro-architecture of ViT,\nexcept for the KAN layer."}, {"title": "Experiments", "content": "We modify the original ViT architecture by substituting its MLP layers with GR-KAN layers. By default, these\nKAN layers employ a rational function with parameters m = 5 and n = 4, and are organized into groups of 8 (g = 8). Each\ntransformer block contains 2 KAN layers. The first GR-KAN layer's am and b\u2081 are initialized to fit the identity function,\nwhile the second is initialized to mimic the Swish function. The attention layers are initialized with Mimetic\nInitialization. The remainder of the architecture remains unchanged. We intentionally do not use hierarchical\narchitectures for simplicity.\nWe select the configurations of KAT to be identical with those used in ViT, as summarized in\nTable 4. All variants use an input patch size of 16 \u00d7 16."}, {"title": "Image Recognition", "content": "We do experiments on ImageNet-1K image classification benchmark. ImageNet-1K is one of the\nmost widely-used datasets in computer vision which contains about 1.3M images of 1K classes on training set, and 50K\nimages on validation set.\nWe mainly follow the hyper-parameters of DeiT. Specifically, models are trained for 300 epochs at 2242\nresolution. The patch size is set to 16. Data augmentation and regularization techniques include RandAugment, Mixup, CutMix, Random Erasing, weight decay, Label Smoothing and Stochastic\nDepth. We adopt AdamW optimizer with batch size of 1024.\nWe compare with ViT and DeiT, as we share the same architecture, except for the channel mixer. We\nalso report the results of ViT + KAN, that simply replacing MLP with standard KAN.\nOur experimental results demonstrate that the KAT models consistently outperform their counterparts on the\nIN-1k dataset, as shown in Table 5. Firstly, the integration of GR-KAN in the transformer architectures demonstrates\nsuperior performance over traditional MLP-based mixers. For instance, the KAT-S model achieved an accuracy of 81.2%,\noutperforming the DeiT-S model by 2.4%. This improvement underscores the potential of KAN mixers to enhance model\nefficacy when properly integrated.\nSecondly, the vanilla KAN layer faces scalability issues. ViT-T/S + KAN only achieved an accuracy of around 63%, even\nwith a much higher computational cost. ViT-L + KAN fails to converge, resulting in NAN error. We addressed these scaling\nchallenges as detailed in Section 3, enabling our KAT models to scale successfully."}, {"title": "Object Detection and Instance Segmentation", "content": "We evaluate our approach on the MS-COCO2017 dataset, a standard benchmark for object\ndetection and instance segmentation. In our setup, the KAT is employed as the backbone within a ViTDet-based\nMask R-CNN model, initialized with weights pre-trained on ImageNet. We followed the standard 3\u00d7 training\nschedule, which consists of 36 epochs. The training images were resized to 800 \u00d7 1333 pixels. The AdamW optimizer was used with a learning rate of 0.0001 and a total batch size of 16. Our implementation was based on the PyTorch and\nMMDetection libraries, and we use FP16 precision to reduce training costs. The experiments were carried out\non 4 NVIDIA H100 GPUs.\nKAT consistently outperformed other models,\nparticularly in object detection, where it achieved a 3.0 Apbox gain on the S-sized model and a 1.4 Apbox gain on the L-sized\nmodel compared to ViTDet. The improvements were most pronounced in smaller models, where computational cost\nincreased by only 1 GFLOPs. This shows that KAT offers better accuracy with minimal overhead."}, {"title": "Semantic Segmentation", "content": "We evaluated our KAT model on the ADE20K dataset. This dataset comprises 150 semantic\ncategories with 20,000 images in the training set and 2,000 in the validation set. For our experiments, we utilized KAT\nas the backbone for the UperNet framework, initializing it with ImageNet pre-trained weights. The training\nwas conducted using the AdamW optimizer with a learning rate of 0.0001 and a batch size of 16, across 160,000\niterations. Our implementation was carried out using the PyTorch and mmsegmentation libraries, and the experiments\nwere performed on two NVIDIA H100 GPUs. For comparison, we evaluated UperNet with other backbones, including\nDeiT, Swin Transformer, and ConvNeXt."}, {"title": "Ablation Study and Analysis", "content": "As GR-KAN can be considered as a special kind of MLP with group rational function, we do an\nablation study to consider different types of activation for MLP and compare with our GR-KAN. Superficially, we replace\nthe activation function in MLP in ViT-Ti/16 to different kinds, including GELU, ReLU, SiLU,\nPRELU and PAU, and comparing them with KAT.\nThe results indicate that traditional activation functions like ReLU and GELU perform similarly. Learnable activations like\nPReLU and PAU show an improvement. Notably, Our KAT-T achieves the highest accuracy at 74.6%, outperforming GELU\nby 1.9%. This suggests that GR-KAN, as used in KAT-T, can significantly enhance the expressiveness of MLPs in vision\ntransformers.\nIn addition to accuracy, we analyzed the computational cost of different activations by measuring throughput and peak\nmemory usage on an NVIDIA A5000 GPU. All activation functions showed similar peak memory usage. However,\nour method (KAT-T) showed slightly lower throughput compared to the baseline activations (e.g., ReLU, GELU, and SiLU),\nwhich are more efficient. This suggests that while KAT-T offers substantial accuracy improvements, there is a trade-off in\ncomputational efficiency, which may be attributed to the increased complexity of rational function computations."}, {"title": "Benefit of CUDA Implementation", "content": "To evaluate the efficiency improvements introduced by our CUDA implementation\ndiscussed in Section 4.2, we conducted experiments to measure both forward pass speed and peak memory usage. Specifically,\nwe compared our CUDA implementation against two alternative methods. The first is called Torch Looped, which loops over\neach channel group, applies the rational function, and then concatenates the results. The second is called Torch Vectorized.\nIn this method, the input tensor is reshaped according to the channel groups, the rational function is applied in a vectorized\nmanner, and the tensor is reshaped back to its original form. We compare these three implementation on A5000 GPU,\nunder 1) different group number $g \\in \\{1, 2, 4, 8, 16\\}$. 2) different input dim $D\\in \\{128, 256, 512, 1024, 2048\\}$"}, {"title": "Rational Initialization", "content": "We tested our KAT-T model with different initializations of the rational functions when training\nfrom scratch. As shown in Table 10, the \u201cIdentity - Swish\u201d initialization achieves the best performance, which we have\nadopted as our default setting."}, {"title": "Visualization of Trained Functions", "content": "An important aspect to examine is the behavior of the trained rational functions. As shown in Figure 7, we plot the\nfunctions for KAT-S with g = 8 across all 12 layers. The results indicate that within each layer, the rational functions\nexhibit similar trends, while the functions across different layers tend to differ from one another."}, {"title": "Conclusion and Future Work", "content": "In this work, we introduced the Kolmogorov-Arnold Transformer (KAT), a novel architecture that successfully integrates\nKolmogorov-Arnold Networks (KANs) into transformers, addressing key challenges associated with large-scale training\nscenarios. Our proposed Group-Rational KAN (GR-KAN) variant, with its rational activation functions, group-based\nparameter sharing, and variance-preserving initialization, demonstrated significant improvements in computational\nefficiency and scalability. Through extensive experiments on vision tasks, including image recognition, object detection,\nand semantic segmentation, KAT outperformed traditional MLP-based transformers, achieving superior accuracy on\nImageNet1K while maintaining comparable computational demands.\nOur study highlights KAT's potential as a good alternative to MLP-based transformers, especially in large-scale\nvision tasks. This integration introduces exciting opportunities for broad applications. For example, employing KAT\narchitectures might help development of language models.\nHowever, KAT is not without its challenges. A primary concern is running speed. Even with the CUDA optimized code, the\nrational function is still slower than plain activation like ReLU and GELU. Another issue is the stability when using rational\nfunctions in neural networks. The higher order gradients for am and bn can become unstable because of their dependence\non the input power. Integrating these functions into the backpropagation process could introduce complications.\nAdditionally, it is important to acknowledge that our GR-KAN represents a hybrid model. On the one hand, GR-KAN\nis a KAN layer with shared edges and a rational base function. On the other hand, it can be interpret as MLP with a\nredesigned activation placed before the linear layer. It leverages the computational simplicity of MLPs but maintains some\ncharacteristics of KANs. However, GR-KAN is not a pure KAN model. Instead, it merges advantages from both systems to\nenhance overall functionality.\nThere are multiple directions of KAT for future research. One potential area of exploration is to find alter-\native base functions to further improve computational efficiency and compatibility with emerging hardware architectures.\nCurrently, rational functions serve as one option, but other possibilities exist. These include Fourier transformations [Noe24], Wavelet transforms [BC24b], and Gaussian radial bases [Li24].\nAdditionally, expanding the applicability of KAT to other domains beyond vision tasks, such as natural language processing\nor reinforcement learning, could unlock new opportunities for performance gains. Further research could also investigate\nhybrid models , or adaptive mechanisms for dynamically selecting between KAN and MLP layers based on\nthe complexity of the task, thereby optimizing resource utilization. Finally, addressing the remaining scalability challenges,\nparticularly in terms of memory footprint and inference speed, will be crucial for deploying KAT in real-world applications\nat scale."}]}