{"title": "Exploiting Pre-trained Models for Drug Target Affinity Prediction with Nearest Neighbors", "authors": ["Qizhi Pei", "Lijun Wu", "Zhenyu He", "Jinhua Zhu", "Yingce Xia", "Shufang Xie", "Rui Yan"], "abstract": "Drug-Target binding Affinity (DTA) prediction is essential for drug discovery. Despite the application of deep learning methods to DTA prediction, the achieved accuracy remain suboptimal. In this work, inspired by the recent success of retrieval methods, we propose kNN-DTA, a non-parametric embedding-based retrieval method adopted on a pre-trained DTA prediction model, which can extend the power of the DTA model with no or negligible cost. Different from existing methods, we introduce two neighbor aggregation ways from both embedding space and label space that are integrated into a unified framework. Specifically, we propose a label aggregation with pair-wise retrieval and a representation aggregation with point-wise retrieval of the nearest neighbors. This method executes in the inference phase and can efficiently boost the DTA prediction performance with no training cost. In addition, we propose an extension, Ada-kNN-DTA, an instance-wise and adaptive aggregation with lightweight learning. Results on four benchmark datasets show that kNN-DTA brings significant improvements, outperforming previous state-of-the-art (SOTA) results, e.g, on BindingDB IC50 and Ki testbeds, kNN-DTA obtains new records of RMSE 0.684 and 0.750. The extended Ada-kNN-DTA further improves the performance to be 0.675 and 0.735 RMSE. These results strongly prove the effectiveness of our method. Results in other settings and comprehensive studies/analyses also show the great potential of our kNN-DTA approach.", "sections": [{"title": "1 Introduction", "content": "Drug discovery has been more and more important, which is a long and expensive process that typically takes tens of years and billions of dollars. Therefore, Computer-Aided Drug Discovery (CADD) plays an important role to help accelerate the journey,"}, {"title": "2 Related Work", "content": "Drug-Target binding Affinity (DTA) Prediction aims to estimate the strength of drug-target interaction. The experimental assay [22] is the most reliable method, but it is labor-intense with high cost. Hence, computational methods have been applied, which can be divided into structure-based and structure-free methods. For structure-based ways, molecular docking [59, 62] and molecular dynamics simulations [49] are typical ones. For structure-free methods, machine learning ways include Random Forest [50], kernel-based works [5], and gradient boosting machines [19]. Thanks to the increased available affinity data, deep learning models [21, 39, 40, 42, 43] are now dominating the DTA prediction, which takes different neural networks (e.g., GNNs) for representation learning. Similarity-based Virtual Screening is commonly adopted in classical binding prediction, which usually generates drug and target similarity matrices [1, 8, 23, 48, 51, 52, 57]. These similarity matrices serve as features to be integrated into different methods, such as kernel regression [67], matrix factorization [3], gradient boosting machine [56, 58], neural network classifiers [2, 52], and so on. Several works also utilize the drug/target similarity to integrate the affinity labels [36, 60]. SVM-KNN [69] is a work that combines the kNN with an SVM classifier for prediction, but it differs a lot from ours on motivation and process.\nNearest Neighbor Learning and Memory Networks. Recently, kNN retrieval is popular in Natural Language Processing (NLP) [4, 16, 26, 33, 72]. [29] is among the first work that successfully combines the language model with kNN retrieval method. Later, [28] shares a similar idea to apply kNN retrieval to machine translation. After that, the kNN-based methods are widely spread in different"}, {"title": "3 Method", "content": "In this section, we first define the DTA prediction task and necessary notations. Next, we introduce our kNN-DTA with proposed two aggregation and retrieval ways. Then, we present the extension Ada-kNN-DTA. Finally, we give some discussions.\nPreliminary. Let $D = \\{(D, T, y)\\}_{i=1}^N$ denotes a DTA dataset, where $(D, T, y)$ is a triplet sample and N is the dataset size. Here D/T is one drug/target from the dataset, and y (a floating number) is the label measuring the binding affinity strength (e.g., IC50, Ki, Kd) between the drug-target pair. The DTA prediction is then a regression task that aims to predict the affinity score between the drug-target pair. Mathematically, the goal is to learn a mapping function $F: D \\times T \\rightarrow y$. A drug D can be represented by different formats, such as simplified molecular-input line-entry system (SMILES) [63], or graph with nodes (atoms) and edges (bonds), or a 3D conformation where the coordinates of all atoms are available. Similarly, a target T can be represented by amino acid sequences or a 3D conformation. In this work, we take the SMILES strings for drug and amino acid sequences for target. Due to the superior performance of Transformer [61], we use two Transformer encoders $M_D$ and $M_T$ to encode the drug D and target T and obtain $R_D$ and $R_T$. The $R_D$ and $R_T$ are fed into a prediction module P to get the predicted affinity $\\hat{y}$."}, {"title": "3.1 Retrieval-based KNN-DTA", "content": "Our kNN-DTA incorporates two retrieval methods and aggreation ways, which are label aggregation with pair-wise retrieval and representation aggregation with point-wise retrieval."}, {"title": "3.1.1 Label Aggregation with Pair-wise Retrieval", "content": "Intuitively, similar drug-target pairs possibly have similar binding affinity scores. Hence, we propose label aggregation with pair-wise retrieval, which is to aggregate the ground-truth affinity scores from k nearest neighbors retrieved by the embeddings of drug-target pair. Shortly speaking, we first build a key-value memory datastore that contains the encoded representations of all drug-target pairs and their corresponding labeled affinity values, which can be quickly done through a single forward pass of the pre-trained DTA model. Then, the kNN retrieval is performed when evaluating on test samples.\nDatastore. The memory datastore is constructed offline with a set of key-value pairs $(k_i, v_i)$. Since the affinity score y corresponds to a specific drug-target pair $(D_i, T_i)$ instead of one drug or target only, the key $k_i$ in our datastore is the concatenated representation of $R_{D_i}$ and $R_{T_i}$, that is $[R_{D_i}; R_{T_i}]$, and the value $v_i$ is the ground-truth affinity score $y_i$. This is the why we call pair-wise retrieval. The datastore (K, V) is created by the key-value pairs for all the samples in dataset D,\n$(K, V) = \\{([R_{D_i}; R_{T_i}], y_i)|((D_i, T_i), y_i) \\in D\\}.$\n(1)\nNoting that we only need a single forward pass of the pre-trained DTA model to obtain (K, V), which can be quickly done.\nPair-wise Retrieval, Label Aggregation, Affinity Prediction. Given a test sample $(D_t, T_t)$, we first encode the data through encoder $M_D$ and $M_T$ to obtain representations $R_{D_t}$ and $R_{T_t}$. Then the concatenated $[R_{D_t}; R_{T_t}]$ is used as query to retrieve the k nearest neighbors $N = \\{(k_i,v_i)\\} = \\{([R_{D_i}; R_{T_i}], y_i)\\}$ from the datastore. The retrieval depends on specific similarity measurement s(, ) between query and the datastore, such as L2 distance. With the retrieved nearest neighbor set, we then do label aggregation among the labeled affinity from the neighbors in an attentive way. That is, a softmax is performed on the similarities $s([R_{D_t}; R_{T_t}], k_i)$ and we aggregate the retrieved affinity values $y_i$ by the attention weights to be $y'$. Mathematically,\n$y' = \\sum_{(k_i,v_i) \\in N} A_i * y_i, A_i = \\frac{\\exp (s([R_{D_t}; R_{T_t}], k_i)/\\tau)}{\\sum_{(k_j,v_j)\\in N} \\exp (s([R_{D_t}; R_{T_t}], k_j)/\\tau)},$\n(2)\nwhere $\\tau$ is the temperature, and $y_i$ equals to $v_i$ in above equations. The integrated affinity score $y_t$ is supposed to produce a good prediction with the help of retrieved neighbors. Since the pre-trained model P can also produce a prediction $\\hat{y}_t$, we can further aggregate the aggregated affinity score $y'$ and the model prediction $\\hat{y}_t$ as the final one, $y_t = \\lambda * \\hat{y}_t + (1 - \\lambda) * y'$, where $\\lambda$ is the coefficient."}, {"title": "3.1.2 Representation Aggregation with Point-wise Retrieval", "content": "Apart from above label aggregation that directly affects the predicted affinity scores through the label space, we also introduce another representation aggregation with point-wise retrieval to leverage the nearest neighbors from the embedding space. This is related to the similarity-based VS methods. Different from the above pair-wise retrieval, here we use separate point-wise retrieval for $k_D$ nearest drug representations and $k_T$ nearest target representations. Generally speaking, we build separate datastores for drugs and targets, with only the key (drug and target representations) saved in the datastore since the values we need is the same as the keys (also the drug/target representations). Then kNN retrieval is performed on test drug and target to aggregate representations.\nDatastore, Point-wise Retrieval, Representation Aggregation, Affinity Prediction. We build a datastore $K_D$ for drugs and a $K_T$ for targets. Instead of the key-value pairs, these datastores only save keys $k_D$, and $k_T$. That is, the encoded drug/target representation $R_{D_i}/R_{T_i}$ is stored in $K_D/K_T$. Noting that the $R_{D_i}$ and $R_{T_i}$ are the same as that in above pair-wise retrieval method. Thus $K_D = \\{R_{D_i}|D_i \\in D\\}, K_T = \\{R_{T_i} |T_i \\in D\\}$, where $D_i$ and $T_i$ are the unique drugs and targets.\nAt test time, given the test sample$(D_t, T_t)$, we use $R_{D_t}/R_{T_t}$ as query to retrieve nearest representations from $K_D/K_T$ with similarity metric s(, ). The retrieved sets are $N_D = \\{R_{D_i} \\}$ and $N_T = \\{R_{T_i} \\}$. The kNN retrieval is also based on similarity metric s(, ) between query representation and the ones in datastore. With the retrieved sets $N_D$ and $N_T$, attentive representation aggregation is conducted. Same as the label aggregation, the representation aggregation is\n$R'_{D_t} = \\sum_{R_{D_i} \\in N_D} \\alpha * R_{D_i}, \\alpha = \\frac{\\exp (s(R_{D_t}, R_{D_i})/\\tau_D)}{\\sum_{R_{D_j} \\in N_D} \\exp (s(R_{D_t}, R_{D_j})/\\tau_D)}$ and"}, {"title": "3.1.3 Unified Framework", "content": "Each of the above aggregation methods can be used to enhance DTA prediction. In order to make the best use of above two ways, we systematically combine them in a unified framework, which is shown in Figure 1. Given the test sample $(D_t, T_t)$, the whole test process is as follows. (1) Use encoders $M_D$ and $M_T$ to obtain the representations $R_{D_t}$ and $R_{T_t}$; (2) Concatenate $R_{D_t}$ and $R_{T_t}$ and use it as a query to retrieve the nearest samples from (K, V). The label aggregation is performed to the retrieved neighbors affinity values to obtain $y'$; (3) Use $R_{D_t}/R_{T_t}$ as query to separately retrieve the nearest drug/target representations from $K_D/K_T$, and aggregate retrieved representations and the query representations to obtain $R'_{D_t}/R'_{T_t}$, then get model prediction $\\hat{y}_t = P(R'_{D_t}, R'_{T_t})$; (4) The $y'$ are then combined with the predicted $\\hat{y}_t$ to produce the final affinity prediction $y_t = \\lambda * \\hat{y}_t + (1 - \\lambda) * y'.$"}, {"title": "3.2 Extension: Adaptive Retrieval-based Ada-kNN-DTA", "content": "The above kNN-DTA only requires retrieving nereast neighbors in the inference phase, and the calculation of the aggregation is parameter-free and training-free. Though efficient, the coefficients for aggregation, e.g., $\\lambda/\\lambda_D$, are manually designed hyperparameters in current kNN-DTA and shared for all the test data, without considering the aggregation quality for each specific sample. Hence, to further exploit the power of kNN-DTA and reduce the manually tuning cost of these hyperparameters, we propose an adaptive learning extension Ada-kNN-DTA.\nIn Ada-kNN-DTA, some lightweight modules are introduced to meta-learn the aggregation weights, e.g., $\\alpha/\\alpha_D$, and the coefficients, e.g., $\\lambda/\\lambda_D$. Concretely, the embedding distances between the query and neighbors s(,) are fed into a light meta-network to learn the weights/coefficients and then perform the aggregation. Take the label aggregation as an example, these k distances are put as a vector (denoted as $S = [s_1, ..., s_k]$) and then fed into a FFN with softmax to output the aggregation weights and coefficient,\n$y_t = \\alpha_0 * \\hat{y}_t + \\sum_{i=1}^k \\alpha_i * y_i, \\alpha_i = softmax(FFN(S))_i,$\n$FFN(S) = max(0, SW_1 + b_1) W_2 + b_2,$\n(3)\nwhere W and b are the learnable parameters. Specially, the output dimension of FFN is k + 1. After softmax over k + 1 values, the first $\\alpha_0$ is the coefficient $\\lambda$. In this way, we automatically learn the coefficient $\\lambda$ and adaptive weights $\\alpha$ for aggregation. Noting that the training is only conducted on the valid set and then the trained meta-network is directly applied on the test set."}, {"title": "3.3 Discussion", "content": "We put some clarification and discussion here. (1) For nearest neighbor retrieval, chemists/biologists usually utilize the data-specific"}, {"title": "4 Experiments", "content": "To evaluate our kNN-DTA, we first pre-train a neural DTA model as test model, and then perform the kNN retrieval. We introduce the experiments with different settings in this section. If not specified, the pre-trained model, the datastore creation, and the testset are all from the same domain."}, {"title": "4.1 Datasets and Pre-trained DTA Models", "content": "We evaluate on four well-known DTA benchmarks, including BindingDB IC50 and Ki [37], DAVIS [7], and KIBA [55]. Besides, there are four generalization testsets for zero-shot transfer learning. The statistics of these datasets are in the Appendix A.1."}, {"title": "4.2 Parameters of kNN-DTA and Evaluation Metrics", "content": "To find the best hyperparameters for kNN-DTA, we do search on each valid set. We tune k, $k_D$, $k_T$ in $[21, 22, ...27]$, $\\tau$, $\\tau_D$ and $\\tau_T$, in $[10^1, 10^2, ..., 10^5]$, $\\lambda$, $\\lambda_D$ and $\\lambda_T$ in [0.1, 0.2, ..., 1.0]. When searching neighbors, we use FAISS [25], which is a library for efficient nearest neighbor search in high-dimensional spaces. The parameters for the best valid performance are applied to the test set. For training Ada-kNN-DTA, the hidden dimension of the meta-network is 32 and we take no more than 5k steps training on one GPU on the valid data.\nWe follow previous works [21, 27, 42] to evaluate the performance. Specifically, (a) root-mean-square error (RMSE) and (b) Pearson Correlation coefficient (R) [1] are used to evaluate on BindingDB IC50 and Ki datasets, (c) mean-square error (MSE) and (d) Corcondance Index (CI) [15] are on DAVIS and KIBA datasets."}, {"title": "4.3 Results on BindingDB Benchmark", "content": "The RMSE and Pearson Correlation results of BindingDB IC50 and Ki are shown in Table 1. For comparison, we take several works and existing best models as baselines, including Random Forest [27], DeepAffinity [27], DeepDTA [42], MONN [35], BACPI [34], and SSM-DTA [45]. These baseline results are reported from original papers (Random Forest is reported in DeepAffinity and DeepDTA is reported in MONN). From Table 1, we can see: (1) Comparing with existing works, our pre-trained DTA models achieve strong performances (e.g., 0.717 RMSE), which already outperform the previous best BACPI [34] on both RMSE and R. (2) After combining with our kNN-DTA, the performances can be further improved by a large margin. For instance, RMSE results on IC50 and Ki benchmarks are improved to 0.684 and 0.750, which significantly overpass the pre-trained models by 0.033 and 0.035 RMSE. (3) With Ada-kNN-DTA, the performances are further improved. The RMSE is reduced to 0.675 and 0.735. Therefore, these numbers can clearly demonstrate the effectiveness of our kNN-DTA and also the adaptive learning of Ada-kNN-DTA."}, {"title": "4.4 Results on DAVIS and KIBA Benchmarks", "content": "We then evaluate the performance on DAVIS and KIBA datasets, and the results are presented in Table 2. Compared with BindingDB datasets, DAVIS and KIBA are relatively small-scale. The baseline methods are KronRLS [44], GraphDTA [39], DeepDTA [42], DeepPurpose [21], DeepCDA [1], Affinity2Vec [57], WGNN-DTA [24], and SSM-DTA [45]. Again, we see that our pre-trained DTA models obtain good performances compared to previous best works, e.g. 0.205 and 0.162 MSE on DAVIS and KIBA respectively. By applying our kNN-DTA, MSE is reduced to 0.190 and 0.146. However, Ada-kNN-DTA performs similarly to the kNN-DTA. We then study the reason behind and find the shape of the probability density"}, {"title": "4.5 Retrieval from Other Datastore", "content": "Apart from above experiments, we further verify whether adding other/external datastore for retrieval is beneficial. In this experiment, we take the pre-trained model on DAVIS. Besides the DAVIS training set as datastore, we also add BindingDB training data in the datastore, hence the datastore is from two different datasets. Note that part of the targets in the DAVIS are also in BindingDB, so this actually enlarge the retrieval datastore. The evaluation is performed on DAVIS testset and the results are presented in Table 3. We compare the kNN-DTA retrieval on DAVIS datastore, and DAVIS+BindingDB, and Ada-kNN-DTA on DAVIS+BindingDB. It can be seen that retrieval method benefits from additional data and improves the DTA performance, e.g., MSE is reduced from 0.189 to 0.168 when comparing the retrieval from DAVIS only with DAVIS+BindingDB. This experiment shows the easy adoption of our method and also the great potential in real applications."}, {"title": "5 Study", "content": "To better understand our work, we conduct extensive studies. Without specific mention, we take BindingDB K\u012f as testbed."}, {"title": "5.1 Ablation", "content": "We first conduct ablation study to investigate the effect of our two aggregation ways. We remove the label aggregation and representation aggregation from our kNN-DTA separately and check the performance effect. In Table 4, we can see that (1) removing each of the two aggregation methods hurt the prediction performance. (2) Besides, both aggregation methods benefit the DTA prediction (each of the removed settings still outperforms pre-trained model). (3) Comparing these two methods, we can conclude that label aggregation contributes more to the success of kNN-DTA, e.g., the performance drop when removing label aggregation (0.748 v.s. 0.762) is more than removing representation aggregation (0.748 v.s. 0.753)."}, {"title": "5.2 Comparison with Biological Similarity", "content": "In drug discovery, a widely adopted way to retrieve similar molecule/protein is to use chemical/biological similarity measures, e.g., the 2D/3D structure similarity of the molecules/proteins. The most popular similarity measurement for molecules is Tanimoto similarity [13] based on the fingerprint. For protein, the common method is the normalized score of the Smith-Waterman (SW) alignment of the amino acid sequence [67], which compares segments of all possible lengths of the protein sequence and optimizes the similarity measure. Hence, we first make a study to compare the retrieval cost by Tanimoto similarity and Smith-Waterman (SW) alignment score with our embedding similarity. For robustness, we use several drugs/targets as the queries and then count the average retrieval cost on the whole datastore (i.e. unique drugs/targets set of the whole set). We use the widely used toolkits, RDKit [32] and Biopython [6] to do molecule/protein similarity searches, and the fingerprint is RDKit topological fingerprint with 2048 bits. To ensure a fair comparison, we use the CPU version for all packages, and the fingerprints of drugs are calculated and saved in advance (like our embedding datastore). Averagely speaking, for each drug, the time"}, {"title": "5.3 kNN-DTA for other Backbone models", "content": "Generally speaking, our kNN-DTA is model agnostic and it does not depend on specific architecture or what kind of pre-trained DTA model. Hence, in this subsection, we evaluate different pre-trained DTA models. Besides the Transformer network that is used as the DTA model in this paper, we also apply our kNN-DTA to graph neural network (GNN)-based DTA model prediction. Specifically, we first take the 12-layer pre-trained molecule/protein encoders and finetune them on DTA. We also take the 4-layer Transformer encoders that were trained from scratch for DTA prediction. Above two DTA models are still based on Transformer architecture but with different performances. Besides, we take the recent best GNN work, BACPI [34], as the DTA backbone model. Then we apply kNN retrieval on these different pre-trained DTA models to evaluate the performance on BindingDB Ki test set. The results are shown in Table 6. For the two Transformer-based DTA models, applying our kNN-DTA can consistently improve the model performance as we show in our main experiments. For our reproduced BACPI, it achieves RMSE score 0.815 and Pearson Correlation 0.856, with kNN-DTA, the results are improved to 0.797 RMSE and 0.863 Pearson Correlation. These comparisons show the universal effectiveness of our kNN retrieval method. The method can improve performance not only on different model architectures but also on pre-trained DTA models with different performances."}, {"title": "5.4 Effect of The Size of Retrieval Datastore", "content": "We further do another study to see the effect of the size of retrieval datastore. We conduct this study on BindingDB Ki dataset, and we vary the datstore size from the full training data to half and quarter of the full set, then we evaluate the performance of the valid and test sets. The results are shown in Table 7. From the table, we can clearly observe that the datastore size indeed impacts the final performance, but they all surpass the original model (without kNN retrieval). Generally, the larger the datastore is, the more possible that we can retrieve for similar drug-target pairs, and the larger performance improvement we can get."}, {"title": "5.5 Results on Zero-shot Transfer", "content": "Experiments in Section 4 build datastores from the training set used for pre-trained model, and the testset is from same domain, which intuitively ensure the similarity between the datastore and the testset. To evaluate the generalization ability of our kNN-DTA, we conduct the following two additional experiments.\nFirst, we conduct experiment on the BindingDB Ki dataset, where the test set is segmented into four parts based on the visibility (seen or unseen) of drugs and targets during training. The RMSE results are shown in Table 8. Notably, kNN-DTA achieves consistent improvement over these four settings. This evidences kNN-DTA's potential in completely novel targets or drugs.\nWe further conduct a zero-shot transfer experiment on BindingDB generalization testsets. Specifically, the targets in ER/Ion Channel/GPCR/Tyrosin Kinase are hold out before data splitting, which are unseen and increasingly different from the BindingDB training set. Thus, we take the model pre-trained on BindingDB, build the datastore on BindingDB training set, and then apply kNN-DTA to evaluate on these four testsets. The results of RMSE and Pearson Correlation are reported in Table 9. We can see that though these testsets are much different from the data in datastore, kNN-DTA also improves the performance on some specific sets. For instance, on Tyrosin Kinase IC50 and Ki, the RMSE reduced 4 and 3 points. Noting this zero-shot transfer is extremely hard. Thus, our method has potential towards the generalization ability. From these experiments, we can see this hard setting should be an important direction for future works."}, {"title": "5.6 Embedding Visualization", "content": "Our retrieval-based method is mainly based on the assumption that for one drug-target pair, other drugs that are similar to the specific query drug may have similar affinity binding scores (e.g.,"}, {"title": "5.7 Case Study of Nearest Neighbors", "content": "We finally provide some retrieved cases (more in the Appendix B) to better understand the effect of our method. The study is performed on pair-wise retrieval for simplicity. We randomly choose one sample that improves after applying our kNN-DTA. Then we look into their retrieved nearest pairs for study. We plot the paired cases with their drug (PubChem ID, graph visualization), target (UniProt ID, 3D visualization), and also their ground-truth binding affinity score (Ki), the pre-trained DTA predicted score and our kNN-DTA predicted score. For the retrieved neighbors of drug-target pairs (k = 32), we show the graph visualization, PubChem ID of the drugs for clear understanding, and the UniProt ID of targets, also the affinity scores. For the specific drug-target pair and their retrieved neighbors, we have several findings. (1) For the retrieved neighbors, all of the pairs are with the same target, and the differences are from the drugs. This is reasonable since multiple drugs can be used for one target. We can also see from the visualized graphs that the retrieved drugs are in the similar structure, which further demonstrates that similar drugs can benefit our method for DTA prediction. (2) Our kNN-DTA model indeed helps the predicted affinity score to be closer to the ground-truth value, specifically for some out-of-distributed pairs.In Figure 2, the ground-truth values of the test samples are far different from the neighbors. The predictions from our pre-trained model are based on the training data so the predictions are also far from the ground-truth. With the help of neighbors by our kNN-DTA, the predicted values are pushed to be much closer to the ground-truth. This is interesting and demonstrates the value of kNN-DTA."}, {"title": "6 Conclusions", "content": "In this paper, we propose an embedding-based non-parametric retrieval method, kNN-DTA and its extension Ada-kNN-DTA, for drug-target binding affinity prediction so as to further exploit the potential upon an existing DTA model with no or light cost. Through a label aggregation with pair-wise embedding retrieval and a representation aggregation with point-wise embedding retrieval, kNN-DTA greatly benefits DTA prediction from these retrieved neighbors. We verify the effectiveness of kNN-DTA on four benchmark sets (BindingDB IC50 and Ki, DAVIS, KIBA), and obtain significant improvements over previous best models. Comprehensive studies and experiments prove the great potential/practicality of our work. In the future, we will improve our method for better efficiency and also extend it to other applications for drug discovery."}, {"title": "A Experimental Settings", "content": "A.1 Dataset Details\nThe datasets we used for evaluation are BindingDB IC50, Ki, KIBA, DAVIS, also the BindingDB generalization testsets. Besides, BindingDB Kd dataset is used for out-of-domain datastore creation in Section 4.5. For BindingDB IC50 and Ki, we randomly split them into train/valid/test with 6:1:3 as in [27]. For KIBA and DAVIS, train/valid/test sets are 7:1:2 as in [21]. We give the detailed statistics of these datasets in Table 10 and BindingDB generalization testsets in Table 11, including the number of drug-target pairs, the unique molecules and proteins. To better show the label information, we further give the label distribution plots of BindingDB IC50, Ki, DAVIS and KIBA datasets in Figure 5. We can see that the affinity distributions of BindingDB are like normal distribution. However, the data distributions of DAVIS and KIBA are different, where the shape is sharp and the values are centered around specific area. This somehow hinders the learning ability of the Ada-kNN-DTA and affects the further performance gain of Ada-kNN-DTA on them.\nA.2 Model Configurations\nTo evaluate our method, we first pre-train DL-based DTA models for each dataset. We show the architecture of our DTA model in Figure 4. We use two Transformer encoders for molecule encoder MD and protein encoder MT respectively, and each follows RoBERTa [38] architecture and configuration that consists of 16 layers. The first 12 layers of both encoders are initialized from the pre-trained molecule model and pre-trained protein model respectively. Specifically, the pre-trained molecule model is from a Transformer-based encoder that trained on molecules from PubChem [30] dataset, and the pre-trained protein model is the same as the one in TAPE [47] trained on proteins from Pfam [12] dataset (but we re-trained using Fairseq [41]). As commonly done, both encoders take the masked language modeling objective for pre-training. The remained last 4 Transformer layers are randomly initialized for MD and MT. Then, the total 16 layer encoders and an upper prediction module P are combined for DTA model training, which is the \"Pre-trained DTA\" that we used for later kNN retrieval. The embedding/hidden size and the dimension of the feed-forward layer are 768 and 3, 072 respectively. The max lengths for molecule and protein are 512 and 1,024 respectively. The regression prediction head P is 2-MLP layers with tanh activation function and the hidden dimension is 768. During training, to save the computational cost, the first two pre-trained 12-layer molecule and protein encoders are fixed and used as feature extractors, and only the last 4 Transformer layers and 2-MLP layers are learnable for DTA prediction. The implementation is based on Fairseq toolkit\u00b9. The model is optimized by Adam [31] with a learning rate of 0.0001. The dropout and attention dropout of two encoders are 0.1. The learning rate is warmed up in the first 5% update steps and then linearly decayed. The batch size is 32 and we accumulated the gradients 8 times during training."}, {"title": "B More Case studies", "content": "We provide more cases about the retrieved nearest neighbors by the pair-wise retrieval method. We randomly choose some cases that benefit from our kNN-DTA method w.r.t the prediction performance. In Figure 6, we plot the paired cases with their drug (PubChem ID, graph visualization), target (UniProt ID, 3D visualization), and also their ground-truth binding affinity score (Ki), the pre-trained DTA predicted score and our kNN-DTA predicted score. For the retrieved neighbors of drug-target pairs (k = 32), we show the graph visualization, PubChem ID of the drugs for clear understanding, and the UniProt ID of targets, also the affinity scores. From these cases, we have several interesting findings. (1) For the retrieved neighbors, almost all of the pairs are with the same target, and the differences are from the drugs. This is reasonable since multiple drugs can be used for one target, and these pairs can help for the test sample. For instance, in case 1 (Figure 2) in the main paper, the target is adenosine receptor A1 and it shares with all retrieved neighbors. We can also see from the visualized graphs that the retrieved drugs are in the similar structure. (2) Our kNN-DTA model indeed helps the predicted affinity score to be closer to the ground-truth value, specifically for some out-of-distributed pairs. For example"}]}