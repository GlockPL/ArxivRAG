{"title": "NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM", "authors": ["Zihan Wang", "Yaohui Zhu", "Gim Hee Lee", "Yachun Fan"], "abstract": "Vision-and-Language Navigation (VLN) is an essential skill for embodied agents, allowing them to navigate in 3D environments following natural language instructions. High-performance navigation models require a large amount of training data, the high cost of manually annotating data has seriously hindered this field. Therefore, some previous methods translate trajectory videos into step-by-step instructions for expanding data, but such instructions do not match well with users' communication styles that briefly describe destinations or state specific needs. Moreover, local navigation trajectories overlook global context and high-level task planning. To address these issues, we propose NavRAG, a retrieval-augmented generation (RAG) framework that generates user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical scene description tree for 3D scene understanding from global layout to local details, then simulates various user roles with specific demands to retrieve from the scene tree, generating diverse instructions with LLM. We annotate over 2 million navigation instructions across 861 scenes and evaluate the data quality and navigation performance of trained models. The code and dataset is available at https://github.com/MrZihan/NavRAG", "sections": [{"title": "1 Introduction", "content": "Vision-and-Language Navigation (VLN) (Anderson et al., 2018; Krantz et al., 2020; Qi et al., 2020; Zhu et al., 2021) requires the agent to understand natural language instructions and navigate to the described destination in 3D environments. The immense semantic space and diverse forms of language instructions require massive data to train a VLN agent capable of generalizing across different scenarios. However, the high cost of manual annotation has seriously hindered this field, driving efforts to develop instruction generators for automating data generation.\nAs shown in Figure 1 (a), many previous approaches train a navigation instruction generator that takes video or images from Web or simulators as input and produces step-by-step instructions. Leveraging large-scale generated navigation data, this strategy has delivered outstanding results in some navigation tasks using trajectory-based instructions, such as R2R (Anderson et al., 2018) and REVERIE (Qi et al., 2020). However, such instruction generators still remain some shortcomings: 1) These instruction generators are trained on small-scale and domain-specific datasets, the generated instructions lack diversity; 2) Such step-by-step instructions are limited to local navigation trajectories overlooking the global context and high-level task planning; 3) These instructions don't match well with users' natural expressions that describe destinations or state specific needs.\nTo tackle these challenges, this work proposes NavRAG, an instruction generation method leveraging retrieval-augmented LLM, as illustrated in Figure 1(b). Specifically, for each 3D scene, NavRAG constructs a scene description tree in a bottom-up manner for hierarchical scene representations. This hierarchical tree comprises multiple layers of language descriptions: the instance layer captures descriptions, attributes, and functionalities"}, {"title": "2 Related Work", "content": "Vision-and-Language Navigation (VLN) (Anderson et al., 2018; Krantz et al., 2020; Qi et al., 2020; Zhu et al., 2021) enables embodied agents to navigate to the destination described by the language instructions. Early VLN researches focus on discrete environments within 90 scenes of Matterport3D (Chang et al., 2017), which uses a predefined navigation graph, the agent observes panoramic RGB and depth images, teleporting between graph nodes to follow natural language instructions. Under this setting, the datasets include the step-by-step instruction dataset R2R (Anderson et al., 2018), the multilingual instruction dataset RxR (Ku et al., 2020) with longer trajectories, the Remote Embodied Visual Referring Expression (REVERIE) (Qi et al., 2020) dataset, and the Scenario Oriented Object Navigation (SOON) (Zhu et al., 2021) task. Although efficient for training in discrete environments, these datasets lack real-world applicability. To address this, R2R-CE (Krantz et al., 2020) introduce continuous environments (Savva et al., 2019) with instructions from the R2R dataset, where agents navigate freely in 3D spaces using low-level actions (e.g., turn 15\u00b0, move 0.25m) in the Habitat simulator (Savva et al., 2019). In this work, we focus on generating large-scale, high-quality navigation instructions, for simplicity and efficiency, our NavRAG is currently validated in the discrete environments, while the annotated data remains easily transferable to continuous settings.\nNavigation Instruction Generation is an effective approach to addressing the scarcity of training data for VLN. Speaker-follower (Fried et al., 2018) and Env-Drop (Tan et al., 2019) use the LSTM-based instruction generator to generate the offline augmented instructions. VLN-Trans (Zhang and Kordjamshidi, 2023) propose a translator module that enables the navigation agent to generate more concise sub-instructions, leveraging recognizable and distinctive landmarks. AutoVLN (Chen et al., 2022a), MARVAL (Kamath et al., 2023) and ScaleVLN (Wang et al., 2023c) leverage multiple foundation models (Cheng et al., 2022; Radford et al., 2019; Zhao et al.; Koh et al., 2023) and use more 3D scenes to annotate instructions, such as HM3D (Ramakrishnan et al.) and Gibson (Xia et al., 2018). Recently, more works focus on designing more powerful instruction generator, such as a joint structure for instruction following and generation (Wang et al., 2023a), Knowledge enhanced speaker (Zeng et al., 2023), LLM instruction generator with chain of thought prompting (Kong et al., 2025), and LLM instruction generator with BEV perception (Fan et al., 2025). However, these methods are limited to identifying landmarks in navigation trajectories and generating low-level instructions, making it difficult to integrate global context, match user demands, and plan high-level tasks. NavRAG will generate navigation instructions better tailored to the application scenario by considering the global context and user demands through"}, {"title": "3 Method", "content": "scene description trees and retrieval-augmented LLM.\nRetrieval-Augmented Generation (RAG) (Lewis et al., 2020) was initially introduced to enhance LLMs by retrieving relevant document chunks, thereby providing domain-specific knowledge for better answer. Over time, several innovations have expanded on this idea, including techniques like iterative knowledge retrieval (Shao et al., 2023), and the incorporation of knowledge graphs (Edge et al., 2024). Furthermore, adapting RAG to the field of robotics, some works (Xie et al., 2024; Booker et al., 2024) attempt constructing non-parametric memory or scene graphs for 3D scenes, and utilize retrieval-augmented LLM for question answering or navigation. However, traditional RAG methods for scene graph retrieval struggle to balance global context with local details and interpret the environment layout. NavRAG leverages the scene description tree and hierarchical retrieval strategy, achieve better scene understanding."}, {"title": "3.1 Navigation Setups", "content": "In the vision-and-language navigation (VLN) setting, the navigation connectivity graph $G = \\{V,E\\}$ is provided by the Matterport3D simulator (Chang et al., 2017), where V represents navigable nodes and & denotes the edges connecting them. The agent is equipped with RGB cameras and a GPS sensor. Starting from a starting node and following natural language instructions, the agent must explore the navigation connectivity graph G and move to the destination node. The instruction is represented by a sequence of word embeddings $W = \\{w_i\\}_{i=1}^K$, where L is the number of words. At each time step t, the agent can perceive a panoramic RGB observation $R_t = \\{r_{t,k}\\}_{k=1}^K$ at current node Vt, consisting of K view images. The RGB observation of nodes can be obtained through the Matterport3D simulator, so each annotated navigation sample only needs a navigation instruction and an optimal path from the starting node to the destination node for training or evaluation."}, {"title": "3.2 Constructing the Scene Description Tree", "content": "Before generating instructions, it is essential to first represent and understand the environment. As illustrated in Figure 2, we propose a bottom-up, hierarchical approach for constructing a scene description tree. At the view and object levels, each object is described with fine-grained details, including its category, attributes, functionality. The spatial relations among objects is summarized in view-level description. The viewpoint level aggregates multiple views surrounding each navigable viewpoint and summarize the spatial layout around this viewpoint. The zone level integrates multiple viewpoints to define large functional areas (e.g., a bedroom) within the 3D scene. Finally, the house level encompasses multiple zones, offering a high-level abstraction of the overall spatial layout and functional partitioning of the whole scene.\nNavigation Graph. We introduce 800 training scenes from HM3D (Ramakrishnan et al.) and 61 training scenes along with 11 validation scenes from Matterport3D (Chang et al., 2017) for scene tree construction. Obtaining the navigation graphs of these scenes is the first step. Although MP3D already has manually annotated navigation graphs, the navigation graphs of HM3D still remains to construct. Following ScaleVLN (Wang et al., 2023c), we use a heuristic method to build high-quality navigation graphs for HM3D scenes, ensuring high space coverage, fully traversable edges, and well-positioned nodes, which samples dense viewpoints using Habitat Simulator (Savva et al., 2019)'s navigable position function, ensuring over 0.4m geodesic separation. The Agglomerative Clustering (1.0m threshold) is utilized to centralize nodes and form an initial graph by randomly connecting viewpoints within 5.0m, capping node edges at five. Finally, the graph is refined for full connectivity and traversal, producing graphs for 800 scenes.\nView and Object-level Annotation. To capture detailed information about objects within a specific viewpoint of the navigation graph, we utilize the Habitat simulator (Savva et al., 2019) to uniformly sample six views (each with an image resolution of 480x480) from every viewpoint in the navigation graph. These views are then input into a multi-modal LLM (i.e., GPT-40-mini (Hurst et al., 2024)) to generate descriptions of each view, objects, their attributes, and functionalities.\nViewpoint-level Annotation. Integrating descriptions and object information from multiple views, the LLM generates a comprehensive description of the environment surrounding the viewpoint. This description encompasses the area type, spatial layout, and relationships among objects, providing a holistic understanding of panorama.\nZone Partitioning and Annotation. To enhance the comprehension of the scene's spatial layout"}, {"title": "3.3 User Demand Instruction Generation", "content": "As shown in Figure 4 and Figure 5, after constructing the scene description tree, NavRAG leverages the scene-level description, user information, and demands to generate a rough instruction for the navigation agent, such as \"Walk to the warm hall and set the wooden table for lunch\". Subsequently, NavRAG performs a top-down, hierarchical retrieval of potential destinations from the scene tree and integrates the retrieved environmental descriptions at different levels into the LLM, to refine rough instruction into precise and comprehensive instruction, such as \"Walk to the warm hall featuring elegant wooden accents and set the large wooden table with candles and napkins for a lovely dinner ambiance\".\nUser Demands Simulation. To further improve the diversity of generated instructions and meet the user demands, NavRAG integrates texts of user information and demands, enabling the instruction generator to simulate specific roles to generate tailored instructions. A sample of user profile and demands is as follows:\nWe manually annotate 20 user profiles for different roles. For each role, the prompt guides the LLM in simulating the role's behavior with a given scene description tree, generating the records of 50 navigation instructions sent to the agent during one day of this role.\nRetrieval-Augmented Generation. As illustrated in Figure 4, Figure 5 and Figure 6, NavRAG performs layer-by-layer retrieval of texts at different levels based on the scene description tree, progressively localizes the navigation destination. Initially,"}, {"title": "4 Experiments", "content": "a trajectory generator which samples the starting viewpoint and calculate the shortest path to the destination, we randomly sample 5 trajectories per instruction, yielding over 10 million navigation trajectories in total. To evaluate model performance, we also annotate 7,396 instruction-trajectory pairs across 11 unseen scenes, forming the NavRAG Val Unseen benchmark for performance evaluation.\nEvaluation Metrics. Four main metrics are used for navigation: 1) Navigation Error (NE): the mean of the shortest path distance between the agent's final position and the destination. 2) Oracle Success Rate (OSR): the percentage that the agent has reached a position within 3 meters of the destination. 3) Success Rate (SR): the percentage of the predicted stop position being within 3 meters from the destination. (3) Success rate weighted Path Length (SPL) that normalizes the success rate with trajectory length."}, {"title": "4.2 VLN Models", "content": "To evaluate our NavRAG dataset, multiple VLN models are used in the experiments, as shown in Table 2 and Table 3.\nDUET (Dual-scale Graph Transformer) (Chen et al., 2022b) is a VLN model that dynamically builds a topological map for efficient global exploration while integrating fine-grained local observations and coarse-grained global encoding through graph transformers.\nHAMT (History Aware Multimodal Transformer) (Chen et al., 2021) is a VLN model that integrates long-horizon history using a hierarchical vision transformer, which efficiently encodes past panoramic observations and combines text, history, and current views to predict navigation actions.\nNavGPT (Zhou et al., 2024) is a purely LLM-based instruction-following navigation agent, which performs zero-shot sequential action prediction, demonstrating abilities such as high-level planning, sub-goal decomposition, commonsense integration, and navigation progress tracking.\nMapGPT (Chen et al., 2024) is a LLM-based VLN agent that integrates an online linguistic-formed map to enable global exploration. By incorporating node information and topological relationships into prompts, MapGPT understands spatial environments and features an adaptive planning mechanism for multi-step path planning."}, {"title": "4.3 Limitations of the Existing Training Data.", "content": "Table 2 evaluates the zero-shot performance of multiple VLN methods on NavRAG and REVERIE benchmarks, and also shows the performance of models trained on NavRAG datasets. As shown in rows 1-3 of Table 2, models trained on previously generated large-scale datasets (i.e., AutoVLN and ScaleVLN) perform poorly on the NavRAG benchmark, whereas LLM-based methods (rows 4-6) demonstrate relatively strong performance.\nNavRAG leverages the scene description tree and retrieval-augmented LLM, resulting in a larger semantic space of instructions with more diverse sentence structures, meanwhile, better aligned with human expression. LLM-based models effectively comprehend these instructions. In contrast, instructions in ScaleVLN and AutoVLN are generated by a pre-trained instruction generator trained on a small-scale manually annotated dataset (i.e. REVERIE and R2R), restricting the semantic space and diversity, and further hindering the generation ability. Thus, models trained on them struggle with NavRAG benchmark and real-world applications."}, {"title": "4.4 Generalization Ability of NavRAG", "content": "Notably, the performance of the LLM-based method on the NavRAG benchmark surpasses the human-annotated REVERIE benchmark (NE, OSR and SR metrics), due to NavRAG's longer, more detailed, and accurate instructions (shown in Table 1). This finding further validates the quality of instructions generated by our NavRAG.\nAs shown in the last two rows of Table 2, the models trained on the NavRAG dataset achieves competitive performance on both NavRAG Val Unseen and REVERIE Val Unseen benchmarks, and even outperforms LLM-based methods (i.e., NavGPT and MapGPT), showing the ability of NavRAG dataset to enhance model generalization.\nFurthermore, Figure 7 illustrates that NavRAG consistently improves the performance of the VLN model as the pre-training data scale increases, underscoring the potential and value of large-scale generated navigation data."}, {"title": "4.5 Comparison with SOTA Methods", "content": "The last row of Table 3 presents the performance of DUET pre-trained on the NavRAG dataset and fine-tuned on the REVERIE dataset, which is comparable to the SOTA approaches with LLM.\nPrevious methods use manually annotated object bounding boxes of REVERIE datasets to extract visual features for model inputs. However, this strategy restricts the model's applicability in real-world deployment, since the real world does not have ground-truth object information. NavRAG removes the reliance on annotated object bounding boxes, making it more suitable for real-world deployment. For a fair comparison, we also evaluate the performance of other generated datasets after removing the object bounding box information from REVERIE, in this setting, NavRAG shows superior performance. This suggests that, despite NavRAG having a larger domain gap with the REVERIE dataset compared to AutoVLN and ScaleVLN, pretraining on more diverse instructions of the NavRAG dataset enables the model to achieve strong generalization, even leading to better fine-tuning performance surpasses domain-specific generated data."}, {"title": "4.6 Ablation Study", "content": "Table 4: The ablation study of NavRAG, evaluating the effectiveness of the components. To reduce costs, only 100 scenes are annotated for DUET training.\nRetrieval-Augmented Generation: NavRAG vs. GraphRAG. To validate the superiority of our scene description tree-based retrieval over traditional RAG methods (e.g., GraphRAG (Edge et al., 2024)), we also annotate 100 scenes through GraphRAG to evaluate instruction quality. Specifically, GraphRAG replaces the scene description tree with a knowledge graph built from view-level descriptions. During instruction generation, it retrieves relevant text fragments from the knowledge graph, integrates them into a prompt, and feeds them to the LLM to generate instructions and navigation destinations. Comparing the first and last rows of Table 4 shows that the model trained with GraphRAG-annotated data performs poorly on its validation set, indicating low annotation quality.\nZone Partitioning Algorithm. Row 2 of Table 4 evaluates the instruction quality using zones from hierarchical clustering (Xie et al., 2024). Compared to our proposed zone partitioning algorithm, hierarchical clustering relies solely on the distance between different viewpoints, disregarding the spatial layout of the environment (e.g., wall partitions) and lacking environmental semantic understanding.\nRole Simulation and User Demands. To enhance the diversity of instructions and better match user demands, we design prompts that guide the LLM to simulate a user with a specific role profile and generate instructions to the agent in everyday scenarios. As shown in rows 3 and 4 of Table 5, we analyze the impact of role simulation and user demands on the quality of NavRAG-generated instructions. When user demands are not utilized for training data generation, performance significantly decreases in validation data with diverse user demands (Table 5, row 3). However, if user demands are included in the training data but removed from the validation data, the model still maintains strong performance. The experimental results indicate that enhancing the diversity of generated instructions by simulating user roles and incorporating user demands is feasible. Moreover, more diverse instructions can provide the model with stronger generalizability and performance."}, {"title": "5 Conclusion", "content": "In this work, we propose NavRAG, a user demand-oriented navigation data generation method through retrieval-augmented LLM. Unlike previous works that use trajectory-based instruction generators to translate navigation videos into step-by-step instructions, our NavRAG utilizes the environmental representations from a hierarchical scene description tree. By retrieving descriptions of different levels in a top-down manner and introducing the user demands, NavRAG effectively enhances the quality of instructions generated by the LLM."}, {"title": "6 Limitations", "content": "1) Although the strong navigation performance shows the quality of the NavRAG dataset, no effective method exists to evaluate the correctness of generated instructions. Previous approaches evaluate instruction generators by comparing generated instructions with human-annotated instructions (e.g., using metrics like Bleu, SPICE, and CIDEr). However, our experiments show that small-scale human annotations lack diversity and are insufficient for accurately evaluating dataset quality. 2) The navigation targets annotated by NavRAG are limited to the viewpoint-level, failing to precisely locate specific target objects and their positions, which restricts its applicability in object-centered tasks such as mobile manipulation."}]}