{"title": "Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making", "authors": ["Siyu Wu", "Alessandro Oltramari", "Jonathan Francis", "C. Lee Giles", "Frank E. Ritter"], "abstract": "Resolving the dichotomy between the human-like yet constrained reasoning processes of Cognitive Architectures and the broad but often noisy inference behavior of Large Language Models (LLMs) remains a challenging but exciting pursuit, for enabling reliable machine reasoning capabilities in production systems. Because Cognitive Architectures are famously developed for the purpose of modeling the internal mechanisms of human cognitive decision-making at a computational level, new investigations consider the goal of informing LLMs with the knowledge necessary for replicating such processes, e.g., guided perception, memory, goal-setting, and action. Previous approaches that use LLMs for grounded decision-making struggle with complex reasoning tasks that require slower, deliberate cognition over fast and intuitive inference-reporting issues related to the lack of sufficient grounding, as in hallucination. To resolve these challenges, we introduce LLM-ACTR, a novel neuro-symbolic architecture that provides human-aligned and versatile decision-making by integrating the ACT-R Cognitive Architecture with LLMs. Our framework extracts and embeds knowledge of ACT-R's internal decision-making process as latent neural representations, injects this information into trainable LLM adapter layers, and fine-tunes the LLMS for downstream prediction. Our experiments on novel Design for Manufacturing tasks show both improved task performance as well as improved grounded decision-making capability of our approach, compared to LLM-only baselines that leverage chain-of-thought reasoning strategies.", "sections": [{"title": "Introduction", "content": "Large-capacity neural foundation models, such as Large Language Models (LLMs), have gained considerable popularity for a wide range of prediction and decision-making tasks, spanning applications, such as robotics and control, neural question-answering, scene understanding, code generation, mathematical reasoning, etc. LLMs are trained on massive datasets, can be used both as discriminative scoring functions as well as generative models, and their capacity allows them to accumulate and retain vast amounts of knowledge (Brown et al. 2020; Andreas 2022; Dong et al. 2022; Francis et al. 2022; Hu et al. 2023). On the surface, typical usage of LLMs mirrors \u2018system-1 reasoning processes\u2019 (Sloman 1996; Hagendorff, Fabi, and Kosinski 2023), from the dual-process theory of human cognition (Wason and Evans 1974; Kahneman 2011), which provide fast, intuitive, and automatic reasoning-underpinning tasks like navigating daily environments and making quick decisions. Advancements in multi-agent LLM frameworks as well as emergent capabilities such as in-context learning (Coda-Forno et al. 2024; Dong et al. 2022) have enabled LLMs to employ more sophisticated reasoning strategies, such as 'chain-of-thought' reasoning (CoT) (Bhattamishra et al. 2023). These capabilities facilitate LLMs' pursuit of 'system-2 processes' (Tversky and Kahneman 1974), which involve slower, deliberate cognition and critical thinking for complex tasks (Brown et al. 2020; Webb, Holyoak, and Lu 2022)-essential for decision-making in realistic settings. While LLMs have shown promise in this area, key concerns remain, e.g., over discrepancies between LLM inference behavior and human reasoning (Binz and Schulz 2023; Liu et al. 2024), in analyses showing that LLMs prioritize fast and intuitive \u201csystem-1\u201d thinking over slower and deliberate analysis (Hagendorff, Fabi, and Kosinski 2023), and over issues of insufficient grounding such as hallucination (Chakraborty, Ornik, and Driggs-Campbell 2024). These issues raise potential concerns about deployment settings where LLMs are left to perform inference, without having been first grounded on reliable knowledge sources or decision processes (Yao et al. 2023).\nTo alleviate these issues, we propose LLM-ACTR, which shows improved decision-making capabilities over LLMs by integrating intermediate representations extracted from a well-establish neuro-symbolic system: the ACT-R cognitive architecture (Anderson 2009; Ritter, Tehranchi, and Oury 2023). ACT-R offers an integrated theory of the mind encompassing perception, memory, goal-setting, and action - and has been pivotal in developing synthetic agents for learning and training (Anderson et al. 2019). The representation extracted from ACT-R cognitive models serves as domain knowledge, infusing LLMs with decision-making augmentation. LLM-ACTR uses ACT-R models to represent human repeated decision-making with learning. We infuse ACT-R model's intermediate representations with the last hidden layers of open source LLM, and add a top classification layer for fine-tuning. The architecture is then deployed in unseen decision-making tasks. The LlaMa model family (Touvron et al. 2023) was selected for this research, due to its full accessibility to network architecture, including"}, {"title": "Related Work", "content": "Cognitive Decision Making The exploration of human cognition and decision-making processes has been a long-standing pursuit in cognitive science and artificial intelligence research (Gluck and Pew 2006). Two prominent frameworks for cognitive modeling are ACT-R (Anderson 2009; Bothell 2017) and Soar (Laird 2012): these frameworks serve as robust tools for simulating human behavior across various cognitive tasks. They are referred to as Cognitive architectures (CAs) (Laird 2012; Anderson 1998), reflecting a set of intertwined mechanisms to model human behavior and aiming for a unified representation of mind (Newell 1994). CAs use task-specific knowledge to generate behavior. They represent various types of knowledge, including declarative (factual), procedural (how-to), and even recent advancements in perception and motor skills. This knowledge allows CAs to not only simulate behavior but also to explain it, both through direct examination and by tracing the reasoning steps involved in real-time (concurrent protocol). ACT-R encompasses perception, memory, goal-setting, and action, and it has been widely applied in decision-making tasks across psychology and computer science e.g.,(Best and Lebiere 2003; Wu et al. 2023; Gonzalez, Lerch, and Lebiere 2003). ACT-R was chosen for this study to provide the intermediate representations of real time reasoning steps. However, the ACT-R model does not accept natural language as input and cannot generalize across different tasks, even within the same domain, which limits its flexibility for decision-making. In contrast, LLM-ACTR combines the strengths of both LLMs and ACT-R models by leveraging the natural language processing and generative capabilities of LLMs, and making decisions that are grounded by those of ACT-R models.\nIntegration of Cognitive Architectures and LLMS CAS face limitations due to domain restrictions, which have hampered their broader application. They are incapable of processing natural language, they are limited to areas that can be described by logical rules, and they require a significant number of pre-defined rules to function. Intriguingly, LLMs (Brown et al. 2020) offer potential solutions to mitigating these weaknesses. LLMs can process a variety of text inputs and are more flexible than rule-based systems. Additionally, they seem to learn rules implicitly, through pre-training, eliminating the need for manual rule creation. Hence, the notion of integrating CAs and LLMs is attractive, for leveraging the strengths of both approaches and thereby creating a more robust unified theory of computational models. This integration can take several forms, however, e.g., leveraging the implicit world knowledge of LLMs to replace the CAs' declarative knowledge mechanisms or to enhance their traditional symbolic mechanisms for procedural knowledge (Wray, Kirk, and Laird 2024; Kirk et al. 2024). Additional research explores how principles from cognitive architectures can guide the design of LLM-based agent frameworks (Sumers et al. 2023), demonstrating a comprehensive integration effort that spans from knowledge representation to interaction with the environment. However, to our knowledge, unlike these previous efforts that incorporate LLMs into CAs, there is currently no research focusing on assimilating the advantages of CAs into LLMs. In this paper, we leverage a cognitive architecture to ground the reasoning process and outputs of LLMs; by assimilating a neural representation of ACT-R model within LLMs, we aim to enhance LLMs' human alignment and explainability."}, {"title": "Problem Definition: Design for Manufacturing", "content": "We define the terminology that constitutes our problem. The problem setting is a prototypical manufacturing production-line workflow, from supplier to customer, for which there exists a Value Stream Map (VSM; see Figure 2), which allows for tracking the efficiency at different sectors of the process and abstracts the overall problem for mathematical modeling and optimization. Key sectors include: Body Production, Pre-Assembly, Assembly, Honing, Washing, Testing, and Packaging. Early sectors pose potential efficiency problems in the workflow and may warrant optimization (triangles), while later stages are governed by First-In-First-Out (FIFO) processes. The metrics at each stage include Cycle Time (CT), Overall Equipment Effectiveness (OEE), and Mean Absolute Error (MAE); the flow progresses through each stage, aiming for efficient operation, performance monitoring, and error minimization to ensure high-quality production output and timely customer delivery.\nFocused on maintaining stable output for manufacturing plants, we consider plant managers' feedback alongside the VSM structure to define two decision-making problems that aim to reduce Total Assembly Time (TAT) while minimizing Total Defect Rate (TDR). An agent G is a predictive model that takes a natural language question Q as a prompt, along with N snapshots of the sector-wise production flow data {CT, OEE, MAE}. In a single-facet decision-making problem, G outputs a binary decision (0 or 1) on which of two sectors, pre-assembly or assembly, requires a time reduction. In a more-challenging multi-faceted decision-making problem, G should output the same binary decision as before, about which sector should be the optimization target, along with an optimization strategy S. Here, S is a strategy defined by one of several decision-making personas that govern manufacturing process management, which we refer to in the manuscript as 'novice', 'intermediate', and 'expert'."}, {"title": "Cognitive LLMs: Hybrid Architectures for Human-Aligned Decision Making", "content": "We start by providing a brief background on the central components of the ACT-R Cognitive model, before providing details about our proposed Cognitive LLM framework, LLM-ACTR. Our approach demonstrates essential characteristics, derived from ACT-R cognitive model, which are crucial for augmenting decision-making using foundation models with cognitive reasoning.\nLLM-ACTR relies on an ACT-R cognitive model capable of (1) executing tasks from problem definition using decision-making behaviors observed in humans and retrieving knowledge representations similarly, (2) integrating personas ranging from novice to intermediate and expert levels, and (3) simulating the reinforcement learning processes of decision-makers as they transition from novice to expert.\nWe released VSM-ACTR 1.0 (see Background: the VSM-ACTR 1.0 Model; Authors (2024)), which is a rule-based ACT-R cognitive decision-making model for manufacturing decision-making that implements multiple problem-solving strategies, through a combination of production rules. We now provide a new version of this model, VSM-ACTR 2.0, which has incorporated the meta-cognitive processes that reflect on and evaluate the progress of chosen strategies\u2014with an emphasis on headcount cost evaluation, through a reward structure that enables a process akin to reinforcement learning. This system allows the model to dynamically assess the impact of headcount costs on decision-making outcomes, computing a reward or penalty for each decision cycle. These rewards or penalties then propagate back to the initial production rule that initiated the decision cycle, thereby dynamically adjusting the utility of each decision-making strategy.\nVSM-ACTR 2.0 integrates the prototypical decision process with insights into how cognitive models represent different levels of expertise (Blessing and Anderson 1996; Martin, Gonzalez, and Lebiere 2004), categorizing users into three levels of expertise: novices, intermediates, and experts. Novices engage in decision-making using intuitive deliberative chunks. Intermediates can manage key metrics such as CT and OEE but struggle with the systematic analysis of intertwined variables. Experts, on the other hand, make judgments systematically. The cognitive model employs three types of knowledge chunks: decisions, decision merits, and goals. The 'decision chunk' encodes eight slots including reduction time (goal), decision-making state (novice, intermediate, expert), and related variables. The 'decision merits chunk' holds information on sector weights, defect increases by sector, and comparative defect rate increases. The 'goal chunk' captures the initial production conditions and the ultimate goal of achieving the optimal decision. In addition, the model uses 18 procedural rules driven by goal-focused objectives across 20 states, covering actions such as choosing strategies, actions, working memory management, decisions, and evaluations.\nDopaminergic signals are believed to transmit reinforcement information to the corpus striatum (Sch\u00f6nberg et al. 2007), traditionally signaling reward-related activities. However, these signals are now understood to represent the error signal in the temporal difference (TD) algorithm from reinforcement learning (Sutton and Barto 1999), which is applied in"}, {"title": null, "content": "ACT-R's learning mechanism. As expressed in Eqn. 1, Each production rule in the ACT-R model has a utility-a value or strength-associated with it, which is updated using the TD algorithm:\nEqn. 1: \\(U_i(n) = U_i(n - 1) + \\alpha [R_i(n) + U_i(n - 1)]\\),\nwhere \\(U_i(n)\\) represents the value or utility of some item i (e.g., a production) after its n-th occurrence, and \\(R_i(n)\\) represents the reward received on the n-th occurrence. The parameter \\(\\alpha\\) (0 < \\(\\alpha\\) < 1) controls the learning rate.\nA key strength of the TD algorithm is its ability to propagate rewards back to earlier critical productions, through a chain of productions, influencing their utilities. This mechanism is tied to the widely-used 'softmax' function, which is also integral to ACT-R's production selection, as expressed in Equation 2. After propagation, if multiple productions compete with expected utility values \\(U_j\\), the probability of of selecting production i is given by:\nEqn. 2: \\(Probability(i) = \\frac{e^{U_i/\\sqrt{2s}}}{\\sum_j e^{U_j/\\sqrt{2s}}}\\)\nwhere the summation over j is over all the productions that currently have their conditions satisfied; and s is the noise.\nTo understand the dynamics of the learning mechanism, consider a scenario involving penalties within a decision-making process. The reward function R(s, f(x)) calculates the reward at the end of one decision-making round. This function takes two parameters: S, representing the strategy used, and f (x), the outcome of the cost analysis, resulting in either a reward or a penalty. In one decision round, a penalty of -2 is computed due to the use of a novice strategy coupled with an inefficient cost. Factoring in the memory retention effect after a 0.5 time step, the subsequent penalty calculation modifies the impact of the decision:\nR(S, x) - 0.5 time-steps = -2.5\nU(7) = U(6)+\\alpha [R(S, x) \u2013 0.5 time step + U(6)] = \u22121.02\nU(7) represents the utility of novice strategy production at the seventh occurrence of firing. While U(6) represents the utility at sixth occurrences; \\(\\alpha\\) is set at 0.2, based on the learning rule from Wagner and Rescorla (1972). This framework allows penalties to retroactively influence previous decisions, thus shaping the model's strategic choices in subsequent rounds.\nWe ran the VSM-ACTR model across 2012 decision-making trials and 32 problem sets to analyze its behavior (Ritter et al. 2011). Each model run comprised 15-16 trials until reach a more stable expert behavior. We encoded decision types as 0, 1, and 2 for novice, intermediate, and expert strategies, respectively.\nTo assess learning, individual differences, and progression, we initially used descriptive statistics to chart the average progression of decision types over 16 trials. We then employed a mixed linear model to evaluate the influence of trial numbers on decision types, incorporating repeated measures and random effects to account for individual variance."}, {"title": null, "content": "Additionally, an ordered logistic regression analyzed the relationship between the number of trials and the learning progression from novice to expert.\nThe results of the descriptive statistics demonstrate a significant positive impact of trial exposure on decision-making progression, evidenced by a coefficient of 0.086 (P < 0.05). A mixed linear model regression confirms the effect of trials on decision-making and further reveals a variance of 0.007 in the random group effects. This indicates that while there are differences between groups, these differences are relatively small, suggesting that the trials themselves predominantly explain the variability in decision type.\nThreshold analysis using ordered logistic regression reveals significant transition thresholds. The transition from novice to intermediate has a significant threshold of 0.88 (P < 0.05), indicating a challenging progression to higher decision-making skills. In contrast, the transition from intermediate to expert shows a significantly lower threshold of 0.1 (P = 0.021), suggesting it is easier to progress from intermediate to expert than from novice to intermediate.\nComparing these results to the earlier VSM-ACTR 1.0 version, it's find that the transition from intermediate to expert has become significantly more pronounced. This change is attributed to enhancements in headcount cost evaluation factors that have reinforced the progression from intermediate to expert levels.\nFigure 1 illustrates the approach to creating LLM-ACTR, which begins with the collection of task data and documentation. The task procedures are then modeled using ACT-R, employing stochastic simulations to analyze these tasks on a large scale. After the simulation phase, the generated synthetic data is semi-automatically distilled and combined with prompt requests. This data is subsequently used to infuse into an open source LLM through fine-tuning, resulting in a type of cognitive LLM, named LLM-ACTR.\nThe decision-making process demonstrates cognitive reasoning through VSM-ACTR's traces, which capture the reasoning steps in real-time using a concurrent protocol (see Example ACT-R Decision Trace in the Appendix). These traces log the cognitive operations executed by various modules at each decision point, including the activation of the goal module to drive decisions, the use of the imaginal buffer for accessing working memory, procedural memory matching and firing, and utility updating driven by reinforcement learning, along with the decision actions.\nReserving information from ACT-R model's decision-making traces poses challenges. A single decision-making round can generate a vast number of lines of traces, each timestamped as frequently as every 5 milliseconds. Deciding which lines to select or whether to preserve all lines-requires a balance between minimizing information loss and reducing computational costs. The rationale for choosing outputs from specific modules as reliable sources within the decision representation lies in their clear correspondence to deterministic cognitive processes. The ratio-"}, {"title": null, "content": "nale for preserving all traces involves processes of semantic embedding extraction and dimensional reduction.\nThe information used to augment decision-making in this study focuses on distilling macro-level cognitive processes related to executive function(Gilbert and Burgess 2008), capturing the evolution of decision-making results across trials and how decisions adapt through learning and experience. Furthermore, the decision actions are categorized into strategy levels (novice, intermediate, expert), reflecting the learning phases. Neurologically, as cognitive strategies evolve from novice to expert, there is a corresponding increase in the efficiency and effectiveness of neural circuits in the prefrontal cortex and basal ganglia in humans (see paragraphs: Implementing a reinforcement-learning mechanism in a production system framework).\nThe next step involves converting the traces into tensors that the LLM can process. This study explores two approaches: one uses selected traces, and another uses full traces.\nThe selected traces are components distilled from macro-level cognitive processes related to executive function. This process requires human involvement to log decision results and strategy traces, which are then numerically encoded. For instance, '0' represents a decision for reduced time in pre-assembly section, and '1' for assembly. These data are subsequently fed into the neural network as single vectors.\nIn contrast, the holistic traces approach (see Figure 3a) retains both macro- and micro-level cognitive processes, with the latter including metacognition(Nelson and Narens 1994). Metacognition involves an awareness and understanding of one's own cognitive processes, as exhibited through model traces that demonstrate the use of the imaginal buffer for accessing working memory, procedural memory matching and firing, headcount cost analysis, and the assessment of strategy effectiveness.\nThe investigation begins with the transformation of full traces from VSM-ACTR, representing both cognitive and metacognitive processes, into a format that balances information retention with computational efficiency. Cognitive"}, {"title": null, "content": "reasoning traces for each task are processed through a sentence transformer to obtain semantic embeddings for each timestamp. A Sum of Ranked Explanatory Effects (SREE) analysis is then applied to determine the number (N) of principal components that account for at least 70% of the variance. Finally, these embeddings are reduced to N dimensions using Principal Components Analysis (PCA)(Abdi and Williams 2010).\nWith the VSM-ACTR model, which represents human-like cognitive reasoning in repeated decision-making tasks, this section outlines the experimental settings for fine-tuning of the LLM-ACTR framework. Fine-tuning, sometimes referred to as transfer learning, involves optimizing all model weights for the given task. The process includes parsing consistent template prompts that reflect the decision making task into an open-source LLM, aligning the task for the cognitive model Using the LLM as the base model to access the last hidden layer and obtain masked embeddings, constructing a classification layer with softmax activation on top of the base model, using targets containing the salient decision representations of the cognitive model and features from the masked embeddings of the base LLM, and fine-tuning the LLM for classification using the LORA method (see Additional Figures and Illustrations). The key points are: (1) The targets decode the salient decision information from the cognitive model. (2) Use the final layer of contextualized embeddings in transformer-based LLMs, generated through the attention block mechanism. The attention block, a key feature of transformers, distinguishes them from other architectures like recurrent neural networks (Graves 2012). It creates embeddings that capture the in-context meaning of tokens by recombining them with other tokens' embeddings. Successive attention blocks further refine these embeddings, producing multiple layers of abstraction. The final layer, a blend of these refined embeddings, is used in this pipeline because it offers the richest semantic information while balancing minimal information loss and reduced computational costs for fine-tuning. (3) Use Low-Rank Adaptation (LoRa)"}, {"title": "Experiments", "content": "As an instantiation of the problem definition, above, our manufacturing line has two sections with potential defect sources: pre-assembly and assembly. Pre-assembly takes 40 seconds with an OEE rate of 88%, while assembly takes 44 seconds with an OEE rate of 80.1%. To reduce total assembly time by 4, we must identify which section can be shortened with minimal defect increase. We note that reducing cycle time will also lead to an increase in headcount costs.\nThe LlaMa-2 13B model was chosen as the foundation for this research because of its demonstrated effectiveness and efficiency in NLP tasks (Huang et al. 2024). As a state-of-the-art large language model, LlaMa has been trained on trillions of tokens from publicly available datasets. Unlike other transformer-based models such as the GPT family, which can only be accessed at the user's end, LlaMa's architecture, including its pre-trained weights, is fully accessible. Furthermore, its proven capability to extract the last hidden layer for predicting behavioral discrepancies has been provided (Binz and Schulz, 2024). These attributes collectively establish LlaMa-2 13B as an optimal choice for this study.\nTo determine the dataset size that can effectively perform the task while balancing efficacy and resource limitations, we referred to Kumar, Sharma, and Bedi (2024), who showed evidence that LlaMa-2 13B achieves F1 scores above 0.9 in resource-limited text classification tasks, with datasets as 1,000 rows per class. Based on this, we developed the dataset size for fine-tuning as N (number of classes) * 1,000. The ACT-R dataset for binary decision-making classification contains 2,012 decision-making trials, Obtained by running the developed ACT-R model across 32 problem sets, each ACT-R persona was run for 15-16 trials until more stable expert behavior was achieved (Ritter et al. 2011).\nThis study compared the goodness-of-fit and prediction accuracy of the resulting models using holdout data against two baselines: a random guess model and LlaMa without fine-tuning, obtained by reading out log-probabilities of the pre-trained LlaMa.\nA random guess model serves as the most basic form of chance level baseline and represents the simplest hypothesis for model comparison. In psychological interdisciplinary experiments, control conditions often employ random responses to distinguish the effects of treatment from chance (Gaab et al. 2019). This approach allows assessing the extent to which decisions are influenced by knowledge versus being purely stochastic.\nOn the other hand, using LlaMa without fine-tuning as a baseline provides a reference point to measure the impact of fine-tuning on the model's performance. This comparison"}, {"title": null, "content": "reveals how much the model 'learns' from the fine-tuning process compared to its generic, pre-trained state.\nBased on our framework's components, we identify a set of research questions that we answer through experiments.\nRQ1: What are the properties of a useful neural network representation of the decision-making process in Cognitive Architectures?\nAnswering this question sets the groundwork for developing a context-aware domain knowledge base for augmenting decision-making in LLMs.\nRQ2: What level of complexity in behavior representation can LLMs effectively capture?\nPrevious research has used LLM conceptual embeddings to predict human behavior based on past behavioral studies (Binz and Schulz 2024), confirming LLMs' ability to replicate known human patterns. However, high costs and extensive data collection efforts limit this method. By incorporating cognitive model simulations, the study seeks to address these limitations and broaden the investigation to determine the extent to which LLMs can reproduce decision-making knowledge. This will, in turn, help define the depth of decision-making domain knowledge that can be effectively integrated with the innate learning capabilities of LLMs.\nRQ3: Can we inform the LLM with knowledge about the reasoning process of the cognitive architecture?\nInspired by previous works on knowledge-injection (Oltramari et al. 2021; Ma et al. 2019), answering this question offers insights into knowledge transfer from domain-specific bases to LLMs and evaluates its impact on performance in holdout tasks. The method for addressing RQ1 was introduced in the first two sections of our approach framework.\nTo answer RQ2: What level of complexity in behavior representation can LLMs effectively capture? Building on previous research that used conceptual embeddings from LLMs to predict human behavior with historical behavioral data (Binz and Schulz 2024), we adopted the same method of LLM feature extraction for behavior prediction (Hussain et al. 2023). We created datasets consisting of last contextual embeddings as features and the corresponding different levels of VSM-ACTR decision actions representations as targets. We obtained embeddings by passing prompts that included all the information that VSM-ACTR had access to on a given trial through LlaMa and then extracting the hidden activations of the final layer, as shown in Figure 3b. The first dataset used features extracted from prompts (see LLM System Prompt Templates) identical to the VSM-ACTR task, with targets being the VSM-ACTR decision-making results, where '0' indicates reduced time in preassembly and '1' indicates assembly. The second dataset's prompt template added an explanation of the strategy adopted by VSM-ACTR and used compound targets comprising both the decision-making results and the strategies reflecting the"}, {"title": null, "content": "learning trajectory (novice, intermediate, and expert). The targets were encoded as follows: 0, 1, and 2 for preassembly choices using novice, intermediate, and expert strategies, respectively, and 3, 4, and 5 for assembly choices following the same pattern. With these two datasets, we fitted a regularized logistic regression model using 10-fold cross-validation for dataset 1 and multinomial regression using 10-fold cross-validation with L2 regularization for dataset 2. Model performance was assessed by measuring the goodness of fit through negative log-likelihood (NLL) and the predictive accuracy of hold-out data.\nTo answer RQ3: whether LLMs can be informed with knowledge about the reasoning processes of cognitive architecture-we use the fine-tuning approach of LLM-ACTR Framework. The fine-tuning process employs Cross-Entropy as the loss function and uses Adam optimization. Training involves a train test split of 0.2 and uses a batch size of 5 for both training and validation phases. The learning rate is set to le-5, with the training spanning across 10 epochs. To ensure regularization and prevent overfitting, a weight decay of 0.01, and a dropout of 0.5 are applied, and gradient accumulation is set to 2. Last but not least, gradient clipping is employed to maintain a maximum gradient norm of 1.0 for gradient explosion control. We evaluate the model fitting and generalization quality using training loss and validation loss across epochs, then compare the goodness of fit and prediction accuracy of the hold-out data against the baseline models.\nThe approach of distilling macro-level cognitive processes related to executive function captures the evolution of decision-making results across trials and how decisions adapt through learning and experience, all represented as a sequential single vector. This format facilitates ease of use for downstream tasks involving knowledge transfer. However, this method retains only partial cognitive decision-making knowledge.\nIn contrast, the holistic semantic preservation approach encompasses both macro and micro-level cognition processes. However, the embeddings produced vary in shape due to the individual differences in traces originating from stochastic simulations. They cannot be directly fed into neural networks for downstream tasks. Nevertheless, the first two principal components of the reduced embeddings, which correspond to the semantic mapping of ACT-R's components including procedural, imaginal, goal knowledge, utility updating, and decision-making-are detailed in Figure 6.\nThe MANOVA analysis was conducted to assess the overall effect of the independent variables, which include label categories or ACT-R components, on the combined dependent variables-components of reduced embeddings. This analysis reveals a significant relationship with the semantic mapping of ACT-R's components. For instance, the extremely low Wilks' lambda value (0.0004) suggests that the label or ACT-R component categories explain nearly all the variance in the dependent variables, indicative of a strong group effect. The statistical tests applied-Wilks' lambda, Pillai's trace, Hotelling-Lawley trace, and Roy's greatest root-all demonstrate strong significance, as evidenced by the extremely low p-values across all tests. These findings highlight that the principal components retained in the PCA successfully capture the essential variance related to these cognitive processes.\nThis result validates that our semantic abstraction method has the potential to retain the maximum semantics of neural symbolic representations at a minimal computational cost. However, further work is required to address the issue of ragged tensors for downstream tasks.\nIn a preliminary experiment, we addressed the issue of ragged tensors by employing padding with value imputation. We then integrated the 240 full cognitive reasoning traces from the VSM-ACTR model with LLM using embedding concatenation and conducted feature extraction for behavior prediction. Specifically, we transposed the reduced embeddings from each cognitive model run into a (1, X) dimension tensor and subsequently concatenated this with the LLM's last contextual embedding from the same prompt. These concatenated embeddings served as resources for predicting decision-making within the VSM-ACTR model. The prediction targets were multifaceted, including both the decision-making results and the strategies used. The results showed no significant improvement in prediction accuracy with concatenated embeddings compared to using LLM embeddings alone. Further details, illustrations, discussions on potential reasons, and suggestions for improvements, are provided in Additional Experiments, in the Appendix."}, {"title": null, "content": "Table 1 shows that LLM-ACTR captures a single facet of decision-making, achieving an average accuracy of 0.64 across 10 validation folds in the hold-out task. When decision-making targets involve multiple facets-encompassing both choices and strategies that shape the learning trajectory-the accuracy decreases to 0.42. While this reduction suggests that capturing complex decision-making processes is less accurate, the results still show promise in handling these complexities. However, the Negative Log-Likelihood (NLL) reveals greater predictive uncertainty for multifaceted decision-making processes, as evidenced by a significantly higher NLL of 1.18 compared to 0.65 in single-facet scenarios."}, {"title": null, "content": "We first report training and validation losses, across 10 epochs, to reveal the fine-tuned model's learning and generalization behavior. Initially, the training loss begins at approximately 0.73, with a slight fluctuation observed in subsequent epochs, peaking around epoch 2 and showing a notable dip at epoch 7. In contrast, the validation loss starts at around 0.64 and remains remarkably stable throughout the epochs. This consistency in validation loss, coupled with a generally downward trend in training loss after its initial variations, suggests that the model is learning effectively. The overall trend indicates an improvement in model performance over time, reflecting its capability to generalize well on unseen data.\nWe then report the comparison of the LLM-ACTR with the baseline models on goodness of fit using negative log likelihood (NLL) and accuracy score for hold-out data. The LLM-ACTR model demonstrates significantly better performance across all metrics compared to the LlaMa-only model, highlighting its effectiveness in decision-making tasks involving sequential cognitive reasoning. Additionally, the LlaMa-only model performs worse than the chance-level model. This underscores the necessity of fine-tuning pre-trained language models like LlaMa to adapt them to specific human-aligned repeated decision-making tasks."}, {"title": "Discussion and Conclusion", "content": "Resolving the dichotomy between the human-like yet constrained reasoning processes of CAs and the broad, often noisy inference behavior of LLMs remains a challenging but exciting pursuit. This is crucial for enabling reliable machine reasoning capabilities in production systems. This study introduces LLM-ACTR, a novel neuro-symbolic architecture designed to enhance human-aligned and versatile decision-making by integrating the ACT-R model's cognitive process with LLMs. Our framework extracts and embeds ACT-R model's internal decision-making processes as latent neural representations based on using traces of its performance, then injects this information into trainable LLM adapter layers, and finally fine-tunes the LLMs for downstream prediction tasks. LLM-ACTR addresses the data scarcity issue often encountered in research aimed at aligning LLMs with human reasoning. Our approach demonstrates improved grounded decision-making capabilities compared to LLM-only baselines that leverage chain-of-thought reasoning strategies.\nWe explore distilling latent representations. The findings show that distilling macro-level cognitive processes preserves high-level neural symbolic knowledge, aiding downstream tasks but only partially capturing decision-making knowledge. A holistic semantic preservation approach, covering both cognitive and metacognitive processes, better retains full neural symbolic semantics with low computational costs. However, challenges with ragged tensors in downstream tasks require further research. We then use a VSM-ACTR cognitive model, developed for a manufacturing design task, to distill its macro-level cognitive processes as domain knowledge. This knowledge was then employed in both a feature extraction for behavior prediction method and a fine-tuning pipeline to investigate the LLM's capabilities in (1) capturing the complexity of behavioral representations and (2) determining whether and how the LLM can be informed by the reasoning processes inherent in the"}]}