{"title": "In-Dataset Trajectory Return Regularization for Offline Preference-based Reinforcement Learning", "authors": ["Songjun Tu", "Jingbo Sun", "Qichao Zhang", "Yaocheng Zhang", "Jia Liu", "Ke Chen", "Dongbin Zhao"], "abstract": "Offline preference-based reinforcement learning (PbRL) typically operates in two phases: first, use human preferences to learn a reward model and annotate rewards for a reward-free offline dataset; second, learn a policy by optimizing the learned reward via offline RL. However, accurately modeling step-wise rewards from trajectory-level preference feedback presents inherent challenges. The reward bias introduced, particularly the overestimation of predicted rewards, leads to optimistic trajectory stitching, which undermines the pessimism mechanism critical to the offline RL phase. To address this challenge, we propose In-Dataset Trajectory Return Regularization (DTR) for offline PbRL, which leverages conditional sequence modeling to mitigate the risk of learning inaccurate trajectory stitching under reward bias. Specifically, DTR employs Decision Transformer and TD-Learning to strike a balance between maintaining fidelity to the behavior policy with high in-dataset trajectory returns and selecting optimal actions based on high reward labels. Additionally, we introduce an ensemble normalization technique that effectively integrates multiple reward models, balancing the trade-off between reward differentiation and accuracy. Empirical evaluations on various benchmarks demonstrate the superiority of DTR over other state-of-the-art baselines.", "sections": [{"title": "Introduction", "content": "Designing complex artificial rewards in reinforcement learning (RL) is challenging and time-consuming (Skalse et al. 2022; Wang et al. 2024a). Preference-based reinforcement learning (PbRL) addresses this by leveraging human feedback to guide policies, demonstrating success in aligning large language models (Ouyang et al. 2022) and robot control (Liang et al. 2022). Recently, considering the growing utilization of offline data in aiding policy optimization via offline RL (Fang et al. 2022; Chen, Li, and Zhao 2024), offline PbRL (Shin, Dragan, and Brown 2023) has gained attention. This approach involves training a reward model with limited human feedback, annotating rewards for a reward-free offline dataset, and applying offline RL to learn policies.\nDespite significant advancements in offline PbRL, learning an accurate step-wise reward model from trajectory-wise preference feedback remains inherently challenging due to limited feedback data (Zhang et al. 2024b), credit assignment (Kim et al. 2023) and neural network approximation errors (Zhu, Jordan, and Jiao 2023). The introduced reward bias adds potential brittleness to the pipeline, leading to suboptimal performance (Yu et al. 2022; Hu et al. 2023). To mitigate this issue, some studies have aimed to enhance the robustness of the reward model (Shin, Dragan, and Brown 2023; Gao et al. 2024), yet ignoring the potential influence of reward bias in offline RL. Alternatively, approaches that bypass reward modeling and directly optimize policy using preference (An et al. 2023; Hejna et al. 2024) struggle to achieve out-of-distribution (OOD) generalization, limiting their ability to outperform the dataset (Xu et al. 2024).\nFor the policy learning, most of offline PbRL methods (Christiano et al. 2017; Yuan et al. 2024; Gao et al. 2024) apply the learned reward function directly to downstream TD-Learning (TDL) based offline RL algorithms, such as CQL (Kumar et al. 2020) and IQL (Kostrikov, Nair, and Levine 2022). However, these TDL-based methods do not account for potential bias in the predicted rewards. The introduced reward bias, especially overestimated rewards, can lead to optimistic trajectory stitching and undermine the pessimism towards OOD state-action pairs in offline TDL algorithms (Yu et al. 2022). To minimize the impact of reward bias, it is necessary to consider being pessimistic about overestimated rewards during the offline policy learning phase (Zhan et al. 2024). Apart from TDL-based offline algorithms, another methodology, conditional sequence modeling (CSM) (Emmons et al. 2022) such as Decision Transformer (DT) (Chen et al. 2021), has not yet been investigated in offline PbRL. This type of imitation-based approach learns a maximum return policy with in-dataset trajectories by assigning appropriate trajectory reweighting."}, {"title": "Related Works", "content": "Offline PbRL. To enhance the performance of offline PbRL, some works emphasize the importance of improving the robustness of reward model, such as improving the credit assignment of returns (Early et al. 2022; Kim et al. 2023; Verma and Metcalf 2024; Gao et al. 2024), utilizing data augmentation to augment the generalization of reward model (Hu et al. 2024b). Other approaches modify the Bradley-Terry model to optimize preferences directly and avoid reward modeling, such as DPPO (An et al. 2023) and CPL (Hejna et al. 2024). A related SOTA work is FTB (Zhang et al. 2024b), which optimizes policies based on diffusion model and augmented trajectories without TDL. In contrast, our approach balances trajectory-wise DT and step-wise TD3 (Fujimoto, Hoof, and Meger 2018) to mitigate the risk of reward bias and leads to enhanced performance.\nIn theory, (Zhu, Jordan, and Jiao 2023) proves that the widely used maximum likelihood estimator (MLE) converges under the Bradley-Terry model in offline PbRL with the restriction to linear reward function. (Hu et al. 2023) stresses that pessimism about overestimated rewards should be considered in offline RL. (Zhan et al. 2024) relaxes the assumption of linear reward and provides theoretical guarantees to general function approximation. Based on these results, we further provide theoretical suboptimal bound guarantees for offline PbRL under the estimated value function.\nImproving the Stitching Ability of CSM. Due to the transformer architecture and the paradigm of supervised learning, CSM has limited trajectory stitching ability. Therefore, some works aggregate the input of the transformer-based policy or modify its structure to adapt to the characteristics of multimodal trajectories (Shang et al. 2022; Zeng et al. 2024; Kim et al. 2024). The alternative approach leverages Q value to enhance trajectory stitching capability such as CGDT (Wang et al. 2024b), QDT (Yamagata, Khalil, and Santos-Rodr\u00edguez 2023), Q-Transformer (Chebotar et al. 2023) and recent QT (Hu et al. 2024a) for offline RL with GT rewards."}, {"title": "Preliminaries", "content": "Learning Rewards from Human Feedback\nFollowing previous studies (Lee, Smith, and Abbeel 2021; Kim et al. 2023), we consider trajectories of length H composed of states and actions, defined as $\\sigma = {s_k, a_k, ..., s_{k+H}, a_{k+H}}$. The goal is to align human preference y between pairs of trajectory segments $\\sigma^0$ and $\\sigma^1$, where y denotes a distribution indicating human preference, captured as $y \\in {1, 0, 0.5}$. The preference label y = 1 indicates that $\\sigma^0$ is preferred to $\\sigma^1$, namely, $\\sigma^0 > \\sigma^1$, y = 0 indicates $\\sigma^1 > \\sigma^0$, and y = 0.5 indicates equal preference for both. The preference datasets are stored as triples, denoted as $D_{pref}: (\\sigma^0, \\sigma^1, y)$.\nThe Bradley-Terry model (Bradley and Terry 1952) is frequently employed to couple preferences with rewards. The preference predictor is defined as follows:\n$P_\\varphi[\\sigma^0 > \\sigma^1] = \\frac{exp(\\sum_{t=1}^H r_\\varphi(s_t, a_t))}{\\sum_{\\tau\\in{0,1}} exp(\\sum_{t=1}^H r_\\varphi(s_t, a_t))}$   (1)\nwhere $r_\\varphi$ is the reward model to be trained, and $\\varphi$ is its parameters. Subsequently, the reward function is optimized using the cross-entropy loss, incorporating the human ground-truth label y and the preference predictor $P_\\varphi$:\n$L_{CE} = -E_{(\\sigma^0,\\sigma^1,y) \\sim D_{pref}} {(1 - y) log P_\\varphi[\\sigma^0 > \\sigma^1] + y log P_\\varphi[\\sigma^1 > \\sigma^0]}$ (2)\nIn the offline PbRL, we assume there exists a small dataset $D_{pref}$ with preference labels along with a much larger unlabeled dataset D without rewards or preference labels. We"}, {"title": "Methods", "content": "In this section, we first introduce the problem of reward bias in offline PbRL using a toy example and analyze the TDL and CSM methods in such scenarios to further emphasize our motivation. Then, we provide a detailed pipeline and the overall framework for the DTR method. Finally, we prove that extracting policies with in-dataset trajectory return regularization leads to provable suboptimal bounds."}, {"title": "In-dataset Regularization: Training and Inference", "content": "In the offline policy training phase, we aim to utilize the DT and TD3 to achieve a balance between maintaining fidelity to the behavior policy with high in-dataset trajectory returns and selecting optimal actions with high reward labels. Specifically, we sample a mini-batch trajectories from D, and use DT as the policy network to rollout the estimated actions {$a_i$}$_{i=t-H+1}$. Due to the powerful capability of autoregressive generative model, we also rollout estimated states {$\\hat{s}_i$}$_{i=t-H+1}$ to strengthen policy representation (Liu et al. 2024). Considering the trajectory $\\tau_t = {..., R_i, s_i, a_i, ..., R_t, s_t, a_t}$, the self-supervised loss of DT can be expressed as:\n$L_{DT} = E_{\\tau\\in D} \\sum_{i=t-H+1}^t ||\\hat{s}_i - s_i||^2 + ||\\hat{a}_i - a_i||^2  (3)\nAdditionally, we train the Q function to select optimal actions with high reward labels. A natural idea is to use the estimated trajectory sequence for n-step TD bootstrapping.\nFollowing (Fujimoto, Hoof, and Meger 2018) and (Hu et al. 2024a), the Q target and loss are defined as follows:\n$Q(s_i, a_i) = \\sum_{j=i}^{t-1} \\gamma^{j-i} r_j + \\gamma^{t-i} min_{i=1,2} Q_{\\theta'}(s_t, a_t)$ (4)\n$L_Q = E_{\\tau\\in D} \\sum_{i=t-H+1}^t (Q_\\phi(s_i, a_i) - Q(s_i, a_i))^2$ (5)\nTo prevent failures from incorrect trajectory stitching, as highlighted in the toy example, we dynamically integrate the policy gradient to train the policy network. Meanwhile, in Theorem 1, we will show that extracting policies from in-dataset trajectories leads to provable suboptimal bounds. In the early training stage, the policy loss $L_\\pi$ primarily consists of supervised loss $L_{DT}$ to ensure that the policy learns the trajectories of in-dataset optimal return and the corresponding Q function. Subsequently, we gradually increase the proportion of policy gradient to facilitate trajectory stitching near the optimal range within the distribution. In summary, we minimize the following policy loss:\n$L_\\pi = L_{DT} - \\lambda(step) E_{\\tau \\in D} \\sum_{i=t-H+1}^t Q_\\phi(s_i, a_i)$  (6)\nwhere $\\lambda(step)$ considers the normalized Q value and increases linearly with the training steps:\n$\\lambda(step) = \\frac{\\eta}{E_{(s,a)\\sim\\tau \\in D}Q_\\phi(s,a)|} = \\frac{step \\times N_{max}}{max\\_step}$ (7)"}, {"title": "Relabel Offline Data By Ensemble Normalization", "content": "During the annotation reward phase, we introduce a simple but effective normalization method to highlight reward differentiation, and further improve the performance of downstream policies. Specifically, we train N-ensemble MLP reward models {$r_\\psi$}$_{i=1}^N$ from the offline preference dataset $D_{pref}$ with the loss function defined in Equation 2, then annotate the offline data D with predicted rewards.\nWe observe that the trained reward model labels different state-action pairs with only minor differences (as detailed in the Experiments Section and Figure 4a), and these indistinct reward signals may lead to exploration difficulties and low sample efficiency (Jin et al. 2020). While direct reward normalization can amplify these differences, it also increases uncertainty, resulting in inaccurate value estimation and high variance performance (Gao et al. 2024).\nTo balance these two aspects, we propose ensemble normalization, which first normalizes the estimates of each ensemble and then averages them:\n$\\hat{r} = Mean(Norm(r_{\\psi_1}), ..., Norm(r_{\\psi_N}))$   (10)\nNotably, ensemble normalization is a plug-and-play module that can enhance reward estimation without any modifications to reward training."}, {"title": "Theoretical Analysis", "content": "Suppose the preference dataset $D_{pref}$ with $N_p$ trajectories of length $H_p$ and the reward-free offline dataset $D$ with $N_o$ trajectories of length $H_o$. Then, consider the following general offline PbRL algorithm:\n1. Construct the Reward Confidence Set:\nFirst, estimate the parameters $\\psi \\in R^d$ of reward model via MLE with Equation 2. Then, construct a confidence set $\\Psi(\\zeta)$ by selecting reward models that nearly maximize the log-likelihood of $D_{pref}$ to a slackness parameter $\\zeta$.\n2. Bi-Level Optimization of Reward and Policy:\nIdentify the policy $\\pi$ that maximizes the estimated policy value $V_\\psi$ under the least favorable reward model $\\psi$ with both $D_{pref}$ and $D$:\n$\\psi = arg\\underset{\\psi \\in \\Psi(\\zeta)}{min} V_\\psi - E_{\\tau \\sim \\mu_{ref}}[r_\\psi(\\tau)], \\pi = arg\\underset{\\pi}{max} V_\\psi$\nAnd we have the following theorem:\nTheorem 1 (Informal) Suppose that: (1) the dataset $D_{pref}$ and $D$ have positive coverage coefficients $C^{\\dagger}$ and $C^{\\circ}$; (2) the underlying MDP is a linear MDP; (3) the GT reward $\\psi^* \\in G_\\psi$; (4) $0 \\le r_\\psi(\\tau) \\le r_{max}$, and $|\\psi|_3 \\le d$ for all $\\psi \\in G_\\psi$ and $\\tau \\in T$; and (5) $\\pi$ is any mesurable function of the data $D_{pref}$. Then with probability 1 - 2$\\delta$, the performance bound of the policy $\\pi$ satisfies for all s $\\in$ S,\n$SubOpt(\\pi; s) \\le \\frac{cC^\\dagger C^\\circ (G_\\psi, \\pi^*, \\mu_{ref})k^2 log(N_{G_\\psi}/N_p d)}{N_p} + \\frac{2cr_{max}}{(\\frac{1-\\gamma}{d})^2}\\frac{d^3\\xi_{\\delta}}{N_oH_oC^\\circ}$ (11)\nwhere $\\xi_{\\delta} = log(\\frac{4d(N_pH_p+N_oH_o)}{(1-\\gamma)\\delta})$, and other variables involved are not related to with $N_p$ or $N_o$. For detailed definitions, see Appendix A.\nRemark 1 The theorem extends offline RL theory specifically to offline PbRL, leading to a theoretical upper bound for offline PbRL. In order to guarantee this bound, the learned policy $\\pi$ is any measurable function of $D_{pref}$ should be satisfied. In other words, if $\\pi \\in D_{pref}$, the assumption naturally holds. A relaxed condition is $\\pi \\in D$, since trajectories in $D_{pref}$ are often sampled from $D$. This theoretical analysis guides us to make more use of in-dataset trajectories for policy optimization to ensure marginal performance improvements. Accordingly, our DTR method utilizes CSM for optimizing in-dataset trajectories to establish reliable performance bound, while employing TDL to enhance the utilization of out-dataset trajectories. We elaborate on this finding with experiments in Appendix E7."}]}