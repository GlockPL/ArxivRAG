{"title": "On Exact Bit-level Reversible Transformers Without Changing Architectures", "authors": ["Guoqiang Zhang", "J.P. Lewis", "W. Bastiaan Kleijn"], "abstract": "In the literature, various reversible deep neural networks (DNN) models have been proposed to reduce memory consumption or improve data-throughput in the train- ing process. However, almost all existing reversible DNNs either are constrained to have special structures or are constructed by modifying the original DNN ar- chitectures considerably to enable reversibility. In this work, we propose exact bit-level reversible transformers\u00b9 without changing the architectures in the infer- ence procedure. The basic idea is to first treat each transformer block as the Euler integration approximation for solving an ordinary differential equation (ODE) and then incorporate the technique of bidirectional integration approximation (BDIA) (see [26] for BDIA-based diffusion inversion) into the neural architecture together with activation quantization to make it exactly bit-level reversible, referred to as BDIA-transformer. In the training process, we let a hyper-parameter \u03b3 in BDIA- transformer randomly take one of the two values {0.5, -0.5} per transformer block for averaging two consecutive integration approximations, which regular- izes the models for improving the validation accuracy. Light-weight side infor- mation per transformer block is required to be stored in the forward process to account for binary quantization loss to enable exact bit-level reversibility. In the inference procedure, the expectation \u0395(\u03b3) = 0 is taken to make the resulting architectures of BDIA-transformer be identical to transformers up to activation quantization. Empirical study indicates that BDIA-transformers outperform their original counterparts notably due to the regularization effect of the \u03b3 parameter.", "sections": [{"title": "1 Introduction", "content": "Nowadays, one active research trend in deep learning intends to scale up the size of both the deep neural networks (DNNs) and training data, aiming to obtain universal machine-learning models that are capable of accomplishing various tasks. One typical example is the large language models (LLMs) such as GPT-4 [1] and Llama 2 [20] that can, for example, have informative and friendly conversations with humans, solve mathematical problems, or produce high-quality source codes for programming tasks One main bottleneck for training those large DNNs is that they often require large on-chip memory and large inter-chip communication bandwidth across many chips\nIn fact, by following the same principle in this paper, we can also design exact bit-level reversible ResNet without changing its architecture in the inference procedure. We focus on transformer in this paper due to its popularity in the field of generative artificial intelligence (AI)."}, {"title": "2 Related Works", "content": "In recent years, various quantization strategies [24; 23; 22] have been proposed in the training and/or inference processes of DNN models over low-precision devices. For instance, the work [23] success- fully performed quantization on DNN weights, activations, gradients and errors in the training and inference processes and obtained promising results. In summary, it is found in the literature that the obtained validation performance after applying those quantization operations in the DNN models is competitive to the original ones. In our work, we only need to apply activation quantity to enable exact bit-level reversibility in training BDIA-transformers."}, {"title": "3 Preliminary", "content": "Diffusion sampling via solving ODE: Recently, the work [26] proposed the BDIA technique to enable diffusion inversion for effective round-trip image editing. From a high level point of view,"}, {"title": "On reversibility of BDIA:", "content": "The update expression (3) is carefully designed in [26] to enable diffusion inversion for round-trip image editing. By reformulating (3), $z_{i-1}$ can be easily computed in terms of $(z_i, z_{i+1})$. We note that due to the nature of float-point datatype, there might be error accumulation in round-trip image reconstruction, where the corresponding diffusion states in the forward and reverse process are not identical. In practice, error accumulation of BDIA in diffusion inversion is not a big issue [26] due to the fact that the number of timesteps is generally set to be small (e.g., 50 timesteps in either forward or reverse process) to make the time complexity reasonable.\nOne main difference between diffusion inversion and reversible Transformers is that no gradient is needed to be back-propagated in diffusion inversion for updating the DNN model. As a result, even if there is error accumulation in diffusion inversion, it is less severe than in reversible trans- formers where error accumulation in online back-propagation would slow down the training speed or even make the training fail especially for very deep models like LLMs. In next section, we will explain how to design exact bit-level reversible transformers in the training process to avoid any error-accumulation while at the same time, maintains the architectures of the transformer in the inference procedure."}, {"title": "4 Exact Bit-Level Reversible Transformers via BDIA", "content": "In this section, we first briefly review the transformer update expressions from the ODE viewpoint. We then propose the BDIA-transformer that enable exact bit-level reversibility with activation quan- tization. Specially, we will study the impact of the two \u03b3 values {-0.5, -0.5} in BDIA-transformer. In addition, we explain why additional light-weight side-information is required to be stored to ac- count for the binary quantization loss."}, {"title": "4.1 Revisiting transformer update expression", "content": "A typical transformer block consists of the attention function (denoted as $f_k(\\cdot)$) and the function of feed-forward network (FFN) (denoted as $g_k(\\cdot)$), of which the trainable parameters are generally different for different block indices. Accordingly, the output $x_{k+1}$ of the $k$th transformer block can be mathematically represented in terms of the input $x_k$ as\n$$x_{k+1} = x_k + f_k(x_k) + g_k(x_k + f_k(x_k)),\\ \\ \\ \\ \\ \\ h_k (x_k)$$\nwhich involves two residual connections. For simplicity, we ignore the pre-normalisation operations in (4), which in fact does not affect the design of BDI-transformers later on.\nIt is well known from the literature [4] that the $k$th forward step in (4) can be viewed as the Euler integration approximation of an ODE at timestep $t_k$:\n$$x_{k+1}=x_k+h_k(x_k)=x_k+\\frac{d(x_k, t_k)}{dt}(t_{k+1} - t_k) \\ \\ \\ \\ (t_k \\rightarrow t_{k+1} | x_k)$$\n$$\\approx x_k + \\int_{t_k}^{t_{k+1}} \\frac{d(x,t)}{dt}dt,$$\nwhere $\\frac{d(x_k, t_k)}{dt}$ denotes the gradient vector with $(x_k, t_k)$ as the input. Both $\\frac{d(x_k, t_k)}{dt}$ and $(t_{k+1} - t_k)$ are implicitly learned via the composite function $h_k(x_k)$, which is alternatively denoted as $\\Delta(t_k \\rightarrow t_{k+1} | x_k)$."}, {"title": "4.2 BDIA-transformer without quantization", "content": "In this subsection, we first derive the update expressions of BDIA-transformer without quantization, and then study the impact of the \u03b3 values in {0.5, -0.5}. When $k = 0$, $x_1$ can be computed as\n$$x_1 = x_0 + h_0(x) = x_0 + \\Delta(t_0 \\rightarrow t_1|x_0),$$\nThe update expression for $x_{k+1}$ in the training process, $N-1 > k \\geq 1$, can be obtained by utilizing (2)-(3). In particular, we let\n$$\\Delta(t_k \\rightarrow t_{k-1}|x_k) = -h_k(x_k)$$\n$$\\Delta(t_k \\rightarrow t_{k+1}|x_k) = h_k(x_k).$$\nBy combining (7)-(8) and (2)-(3) with $i = k$, the update expression $x_{k+1}$, $N - 1 \\geq k \\geq 1$, can be represented as\n$$x_{k+1} = x_{k-1} + (1 - \\gamma)(x_k - x_{k-1}) + (1 + \\gamma)h_k(x_k),$$\n$$= \\gamma x_{k-1} + (1 - \\gamma)x_k + (1 + \\gamma)h_k(x_k),$$\nwhere in general, \u03b3 is recommended to be randomly selected from {0.5, -0.5} with equal probabil- ity per training sample per transform block. This is different from the work of BDIA-based diffusion inversion [26] where \u03b3 has to be positive.\nIn the inference stage, $E(\\gamma) = 0$ is taken to replace \u03b3 in (10), which leads to a simpler update expression that only involves $(x_k, x_{k+1})$:\n$$x_{k+1} = x_k + h_k(x_k),$$\nwhich is identical to the original transformer update expression (4).\nImpact of \u03b3 parameter: We now study the impact of the \u03b3 parameter in (10). When \u03b3 = 0.5, it follows from (2)-(3) and (7)-(8) that\n$$\\int_{t_{k-1}}^{t_k} \\frac{d(x, t)}{dt}dt \\approx 0.5(x_k - x_{k-1}) + 0.5 h_k(x_k)$$\n$$\\int_{t_k}^{t_{k+1}} \\frac{d(x, t)}{dt}dt \\approx h_k(x_k).$$\nOn the other hand,"}, {"title": "4.3 On exact bit-level reversibility of BDIA-transformer with quantization", "content": "As explained in Section 1, one strategy for reducing memory consumption in training DNN models like LLMs is to perform online back-propagation. That is, the intermediate activation outputs from the top neural block until the bottom one are computed online when performing back-propagation to update the DNN model. In this subsection, we first discuss the reversibility issue of the update expression (10. We then consider performing activation quantization to enable exact bit-level re- versibility. We note that light-weight side information is required to be stored per transformer block for lossless online back-propagation.\nLimitation of the reversibility of (10): The update expression (10 is theoretically reversible. That is, $x_{k-1}$ can be computed in terms of $(x_k, x_{k+1})$ as\n$$x_{k-1} = \\frac{x_{k+1}}{\\gamma} - \\frac{(1 - \\gamma)}{\\gamma}x_k - \\frac{(1 + \\gamma)}{\\gamma}h_k(x_k).$$\nHowever, in practice, the setup $\\gamma \\in {0.5, -0.5}$ would lead to non-negligible accumulation error especially for very deep DNN models. The factor $\\frac{1}{\\gamma} = \\pm 2$ in front of $x_{k+1}$ would amplify the error when k decreases from N \u2013 1 to 1, making the online back-propagation unstable. Fig. 1 illustrates that the reconstruction error indeed increases when applying the online-back-propogation from the top block until the bottom one.\nBDIA-transformer with quantization: To allow for lossless online back-propagation, we propose to perform activation quantization. In particular, we use $Q_l[\\cdot]$ to denote the quantization operation"}, {"title": "5 Experiments", "content": "We evaluated the BDIA training technique for both ViT-small by utilizing the open-source repository, \u00b2 and nano-GPT2 by using the repository. \u00b3 It is found that the BDIA training technique produces promising performance in both tasks."}, {"title": "5.1 On training ViT-small", "content": "In this experiment, we consider training ViT-small over CIFAR10. When implementing the BDIA- training technique, the y parameter was drawn from {-0.5,0.5} with equal probability per train- ing sample per transformer block. The hyper-parameter l for quantization was set to l = 9. In addition, we utilize the SET-Adam optimizer [25] in the training process with the configuration (\u03b7\u03bf, \u03b21, \u03b22, \u20ac) = (1e - 4,0.9, 0.999, 1e-18), where no denotes the initial learning rate. The remain- ing training setups follow directly from the original open source. Three experimental repetitions were performed per training setup to mitigate the effect of randomness."}, {"title": "5.2 On training nano-GPT2", "content": "In this experiment, we consider training nano-GPT2 over the dataset of openwebtext. For illustration purpose, we only took a small subset from the entire training dataset when training the model. The parameter l for quantization was set to l = 6.\nFig. 3 summarizes the training and validation curves for three different training configurations. It is clear from the figure that the BDIA training technique makes the training procedure slightly slower. On the other hand, the resulting validation curve with BDIA is slightly better than without BDIA. The above results are consistent with those of Fig. 2 for training ViT-small.\nAnother interesting property in Fig. 3 is that GPT2 with and without quantization have very simi- lar training and validation performance. That is, the impact of activation quantization in terms of performance is negligible."}, {"title": "6 Conclusions", "content": "In this work, we have proposed the BDIA training technique to assist the training procedure of trans- formers. In particular, BDIA attempts to average every two consecutive integration approximations as a regularizer via the random variable \u03b3 \u2208 {0.5, -0.5}. Exact bit-level reversibility for lossless online back-propagation can be achieved by performing activation quantization and storing light- weight side information. Experiments on training ViT-small and nano-GPT2 show that BDIA leads to better validation performance."}]}