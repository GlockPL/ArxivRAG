{"title": "On Exact Bit-level Reversible Transformers Without Changing Architectures", "authors": ["Guoqiang Zhang", "J.P. Lewis", "W. Bastiaan Kleijn"], "abstract": "In the literature, various reversible deep neural networks (DNN) models have been proposed to reduce memory consumption or improve data-throughput in the train-ing process. However, almost all existing reversible DNNs either are constrained to have special structures or are constructed by modifying the original DNN ar-chitectures considerably to enable reversibility. In this work, we propose exact bit-level reversible transformers\u00b9 without changing the architectures in the infer-ence procedure. The basic idea is to first treat each transformer block as the Euler integration approximation for solving an ordinary differential equation (ODE) and then incorporate the technique of bidirectional integration approximation (BDIA) (see [26] for BDIA-based diffusion inversion) into the neural architecture together with activation quantization to make it exactly bit-level reversible, referred to as BDIA-transformer. In the training process, we let a hyper-parameter \u03b3 in BDIA-transformer randomly take one of the two values {0.5, -0.5} per transformer block for averaging two consecutive integration approximations, which regular-izes the models for improving the validation accuracy. Light-weight side infor-mation per transformer block is required to be stored in the forward process to account for binary quantization loss to enable exact bit-level reversibility. In the inference procedure, the expectation \u0395(\u03b3) = 0 is taken to make the resulting architectures of BDIA-transformer be identical to transformers up to activation quantization. Empirical study indicates that BDIA-transformers outperform their original counterparts notably due to the regularization effect of the \u03b3 parameter.", "sections": [{"title": "1 Introduction", "content": "Nowadays, one active research trend in deep learning intends to scale up the size of both the deep neural networks (DNNs) and training data, aiming to obtain universal machine-learning models that are capable of accomplishing various tasks. One typical example is the large language models (LLMs) such as GPT-4 [1] and Llama 2 [20] that can, for example, have informative and friendly conversations with humans, solve mathematical problems, or produce high-quality source codes for programming tasks One main bottleneck for training those large DNNs is that they often require large on-chip memory and large inter-chip communication bandwidth across many chips\nIn fact, by following the same principle in this paper, we can also design exact bit-level reversible ResNet without changing its architecture in the inference procedure. We focus on transformer in this paper due to its popularity in the field of generative artificial intelligence (AI)."}, {"title": "2 Related Works", "content": "In recent years, various quantization strategies [24; 23; 22] have been proposed in the training and/or inference processes of DNN models over low-precision devices. For instance, the work [23] success-fully performed quantization on DNN weights, activations, gradients and errors in the training and inference processes and obtained promising results. In summary, it is found in the literature that the obtained validation performance after applying those quantization operations in the DNN models is competitive to the original ones. In our work, we only need to apply activation quantity to enable exact bit-level reversibility in training BDIA-transformers."}, {"title": "3 Preliminary", "content": "Diffusion sampling via solving ODE: Recently, the work [26] proposed the BDIA technique to enable diffusion inversion for effective round-trip image editing. From a high level point of view,"}, {"title": "On reversibility of BDIA:", "content": "The update expression (3) is carefully designed in [26] to enable diffusion inversion for round-trip image editing. By reformulating (3), $z_{i\u22121}$ can be easily computed in terms of $(z_i, z_{i+1})$. We note that due to the nature of float-point datatype, there might be error accumulation in round-trip image reconstruction, where the corresponding diffusion states in the forward and reverse process are not identical. In practice, error accumulation of BDIA in diffusion inversion is not a big issue [26] due to the fact that the number of timesteps is generally set to be small (e.g., 50 timesteps in either forward or reverse process) to make the time complexity reasonable.\nOne main difference between diffusion inversion and reversible Transformers is that no gradient is needed to be back-propagated in diffusion inversion for updating the DNN model. As a result, even if there is error accumulation in diffusion inversion, it is less severe than in reversible trans-formers where error accumulation in online back-propagation would slow down the training speed or even make the training fail especially for very deep models like LLMs. In next section, we will explain how to design exact bit-level reversible transformers in the training process to avoid any error-accumulation while at the same time, maintains the architectures of the transformer in the inference procedure."}, {"title": "4 Exact Bit-Level Reversible Transformers via BDIA", "content": "In this section, we first briefly review the transformer update expressions from the ODE viewpoint. We then propose the BDIA-transformer that enable exact bit-level reversibility with activation quan-tization. Specially, we will study the impact of the two y values {-0.5, -0.5} in BDIA-transformer. In addition, we explain why additional light-weight side-information is required to be stored to ac-count for the binary quantization loss."}, {"title": "4.1 Revisiting transformer update expression", "content": "A typical transformer block consists of the attention function (denoted as $f_k(\u00b7)$) and the function of feed-forward network (FFN) (denoted as $g_k(\u00b7)$), of which the trainable parameters are generally different for different block indices. Accordingly, the output $x_{k+1}$ of the kth transformer block can be mathematically represented in terms of the input $x_k$ as\n$$x_{k+1} = x_k + f_k(x_k) + g_k(x_k + f_k(x_k)),$$\n$$h_k(x_k)$$\nwhich involves two residual connections. For simplicity, we ignore the pre-normalisation operations in (4), which in fact does not affect the design of BDI-transformers later on.\nIt is well known from the literature [4] that the kth forward step in (4) can be viewed as the Euler integration approximation of an ODE at timestep $t_k$:\n$$x_{k+1}=x_k+h_k(x_k)=x_k+d(x_k, t_k)(t_{k+1} - t_k)$$\n$$\u2248 x_k + \\int_{t_k}^{t_{k+1}} d(x,t)dt,$$\nwhere $d(x_k, t_k)$ denotes the gradient vector with $(x_k, t_k)$ as the input. Both $d(x_k, t_k)$ and $(t_{k+1} - t_k)$ are implicitly learned via the composite function $h_k(x_k)$, which is alternatively denoted as $\u25b3(t_k \u2192 t_{k+1}|x_k)$."}, {"title": "4.2 BDIA-transformer without quantization", "content": "In this subsection, we first derive the update expressions of BDIA-transformer without quantization, and then study the impact of the \u03b3 values in {0.5, -0.5}. When k = 0, $x_1$ can be computed as\n$$x_1 = x_0 + h_0(x_0) = x_0 + \u25b3(t_0 \u2192 t_1|x_0),$$\nThe update expression for $x_{k+1}$ in the training process, N-1 > k \u2265 1, can be obtained by utilizing (2)-(3). In particular, we let\n$$\u25b3(t_k \u2192 t_{k-1}|x_k) = -h_k(x_k)$$\n$$\u25b3(t_k \u2192 t_{k+1}|x_k) = h_k(x_k).$$\nBy combining (7)-(8) and (2)-(3) with i = k, the update expression $x_{k+1}$, N \u2013 1 > k \u2265 1, can be represented as\n$$x_{k+1} = x_{k-1} + (1 \u2212 \u03b3)(x_k \u2212 x_{k\u22121}) + (1 + \u03b3)h_k(x_k),$$\n$$= \u03b3x_{k\u22121} + (1 \u2212 \u03b3)x_k + (1 + \u03b3)h_k(x_k),$$\nwhere in general, y is recommended to be randomly selected from {0.5, -0.5} with equal probabil-ity per training sample per transform block. This is different from the work of BDIA-based diffusion inversion [26] where y has to be positive.\nIn the inference stage, E(\u03b3) = 0 is taken to replace y in (10), which leads to a simpler update expression that only involves ($x_k$,$x_{k+1}$):\n$$x_{k+1} = x_k + h_k(x_k),$$\nwhich is identical to the original transformer update expression (4).\nImpact of \u03b3 parameter: We now study the impact of the y parameter in (10). When \u03b3 = 0.5, it follows from (2)-(3) and (7)-(8) that\n$$\\int_{t_{k-1}}^{t_k} d(x, t)dt \u2248 0.5(x_k - x_{k\u22121}) + 0.5h_k(x_k)$$\n$$\\int_{t_k}^{t_{k+1}} d(x, t)dt \u2248 h_k(x_k).$$"}, {"title": "4.3 On exact bit-level reversibility of BDIA-transformer with quantization", "content": "As explained in Section 1, one strategy for reducing memory consumption in training DNN models like LLMs is to perform online back-propagation. That is, the intermediate activation outputs from the top neural block until the bottom one are computed online when performing back-propagation to update the DNN model. In this subsection, we first discuss the reversibility issue of the update expression (10. We then consider performing activation quantization to enable exact bit-level re-versibility. We note that light-weight side information is required to be stored per transformer block for lossless online back-propagation.\nLimitation of the reversibility of (10): The update expression (10 is theoretically reversible. That is, $x_{k-1}$ can be computed in terms of ($x_k$, $x_{k+1}$) as\n$$x_{k-1} = x_{k+1}/\u03b3 \u2212 (1 \u2212 \u03b3)/\u03b3x_k \u2212 (1 + \u03b3)/\u03b3h_k(x_k).$$$\nHowever, in practice, the setup \u03b3\u2208 {0.5, \u22120.5} would lead to non-negligible accumulation error especially for very deep DNN models. The factor $\\frac{1}{\u03b3} = \u00b12$ in front of $x_{k+1}$ would amplify the error when k decreases from N \u2013 1 to 1, making the online back-propagation unstable. Fig. 1 illustrates that the reconstruction error indeed increases when applying the online-back-propogation from the top block until the bottom one.\nBDIA-transformer with quantization: To allow for lossless online back-propagation, we propose to perform activation quantization. In particular, we use $Q_l[\u00b7]$ to denote the quantization operation"}, {"title": "5 Experiments", "content": "We evaluated the BDIA training technique for both ViT-small by utilizing the open-source repository, 2 and nano-GPT2 by using the repository. 3 It is found that the BDIA training technique produces promising performance in both tasks."}, {"title": "5.1 On training ViT-small", "content": "In this experiment, we consider training ViT-small over CIFAR10. When implementing the BDIA-training technique, the y parameter was drawn from {-0.5,0.5} with equal probability per train-ing sample per transformer block. The hyper-parameter l for quantization was set to l = 9. In addition, we utilize the SET-Adam optimizer [25] in the training process with the configuration (\u03b7\u03bf, \u03b21, \u03b22, \u20ac) = (1e - 4,0.9, 0.999, 1e-18), where no denotes the initial learning rate. The remain-ing training setups follow directly from the original open source. Three experimental repetitions were performed per training setup to mitigate the effect of randomness."}, {"title": "5.2 On training nano-GPT2", "content": "In this experiment, we consider training nano-GPT2 over the dataset of openwebtext. For illustration purpose, we only took a small subset from the entire training dataset when training the model. The parameter l for quantization was set to l = 6.\nFig. 3 summarizes the training and validation curves for three different training configurations. It is clear from the figure that the BDIA training technique makes the training procedure slightly slower. On the other hand, the resulting validation curve with BDIA is slightly better than without BDIA. The above results are consistent with those of Fig. 2 for training ViT-small.\nAnother interesting property in Fig. 3 is that GPT2 with and without quantization have very simi-lar training and validation performance. That is, the impact of activation quantization in terms of performance is negligible."}, {"title": "6 Conclusions", "content": "In this work, we have proposed the BDIA training technique to assist the training procedure of trans-formers. In particular, BDIA attempts to average every two consecutive integration approximations as a regularizer via the random variable \u03b3 \u2208 {0.5, -0.5}. Exact bit-level reversibility for lossless online back-propagation can be achieved by performing activation quantization and storing light-weight side information. Experiments on training ViT-small and nano-GPT2 show that BDIA leads to better validation performance."}, {"title": "in the bit-level precision of $2^{\u2212l}$:", "content": "$$Q_l[y] = round[y/2^{-l}]2^{-l}.$$\nUpon introducing $Q_l[\u00b7]$, the new update expression for BDIA-transformer can be represented as\n$$x_0 \u2190 Q_l[x_0],$$\n$$x_1 = x_0 + Q_l[h_0(x_0)]$$\n$$s_{k-1}[m] =\\begin{cases}1 & \\text{if mod}(x_{k-1}[m]/2^{-l},2) = 1 \\\\0 & \\text{otherwise}\\end{cases}k\u2265 1$$\n$$x_{k+1} = Q_l[\u03b3(x_{k-1} + s_{k-1}2^{-l})] + Q_l[(1 \u2212 \u03b3)x_k + (1 + \u03b3)h_k(x_k)] k\u2265 1,$$\nwhere \u03b3\u2208 {0.5, -0.5}, $x_{k\u22121}[m]$ denotes the mth element of $x_{k\u22121}$. The mth element $s_{k-1}[m]$ indicates if the integer value $x_{k\u22121}[m]/2^{\u2212l}$ is odd or not. It is immediate from (18)-(21) that $x_k = Q_l[x_k]$ for all N > k \u2265 0. That is, all the intermediate activation outputs $\\{x_k\\}_{k=0}^N$ have fix-point precision of $2^{\u2212l}$.\nAgain in the inference procedure, we replace \u03b3 in (21) by E(\u03b3) = 0. As a result, the update expression (21) can be simplified to be\n$$x_{k+1} = Q_l[x_k + h_k(x_k)] k\u2265 1.$$\nThe only difference of (22) w.r.t. the original transformer update expression (4) is that the quantiza-tion operation $Q_l[\u00b7]$ is performed for each activation output."}, {"title": "On reversibility of (21) by storing light-weight side information:", "content": "We now consider recovering $x_{k-1}$ from $(x_k, x_{k+1})$ by utilizing (20)-(21). By using the fact that \u03b3\u2208 {0.5, -0.5} and the definition for $s_{k\u22121}$, we can easily conclude that the quantity $Q_l[\u03b3(x_{k-1} + s_{k-1}2^{-l})]$ in (21) can be alternatively represented as\n$$Q_l[\u03b3(x_{k-1} + s_{k-1}2^{-l})] = \u03b3(x_{k-1} + s_{k-1}2^{-l}).$$\nThat is, the quantization operation has no effect on $\u03b3(x_{k-1} + s_{k-1}2^{\u2212l})$. This is because the vector $s_{k-1}$ essentially captures the 1-bit quantization loss of $Q_l[\u03b3x_{k\u22121}]$ per element.\nSuppose in each forward pass in the training process, all the side information $\\{s_{k-1}\\}_{k=1}^{N-1}$ are stored in the memory. When we perform online back-propagation, each $x_{k-1}$ can be reconstructed losslessly in the form of\n$$x_{k-1}=\u2212\\frac{1}{\u03b3}x_{k+1} \u2212 \\frac{1\u2212\u03b3}{\u03b3}x_k \u2212\\frac{1}{\u03b3}Q_l[(1 \u2212 \u03b3)x_k + (1+ \u03b3)h_k(x_k)] \u2212 s_{k-1}2^{\u2212l} k\u2265 1.$$\nConsequently, the computed gradient in the online back-propagation will not be drifted from the ground truth, which is desirable in very deep transformer models."}]}