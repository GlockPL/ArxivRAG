{"title": "Contrastive Token-level Explanations for Graph-based Rumour Detection", "authors": ["Daniel Wai Kit Chin", "Roy Ka-Wei Lee"], "abstract": "The widespread use of social media has accelerated the dissemination of information, but it has also facilitated the spread of harmful rumours, which can disrupt economies, influence political outcomes, and exacerbate public health crises, such as the COVID-19 pandemic. While Graph Neural Network (GNN)-based approaches have shown significant promise in automated rumour detection, they often lack transparency, making their predictions difficult to interpret. Existing graph explainability techniques fall short in addressing the unique challenges posed by the dependencies among feature dimensions in high-dimensional text embeddings used in GNN-based models. In this paper, we introduce Contrastive Token Layerwise Relevance Propagation (CT-LRP), a novel framework designed to enhance the explainability of GNN-based rumour detection. CT-LRP extends current graph explainability methods by providing token-level explanations that offer greater granularity and interpretability. We evaluate the effectiveness of CT-LRP across multiple GNN models trained on three publicly available rumour detection datasets, demonstrating that it consistently produces high-fidelity, meaningful explanations, paving the way for more robust and trustworthy rumour detection systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Motivation. Social media platforms have revolutionized communication, enabling rapid information sharing but also amplifying the spread of misinformation, including rumours and fake news [1]. Crises like the the Russia-Ukraine war highlight the susceptibility of users to such content [2]. The unchecked dissemination of rumours can cause significant harm [3], emphasizing the need for automated detection methods that mitigate the spread of rumours [4]. To meet this challenge, it is essential to develop trustworthy tools that not only detect rumours effectively but also provide clear, interpretable explanations for their predictions.\nEarly rumour detection methods relied on text mining and handcrafted features [5]\u2013[7]. While these approaches laid a foundation, their reliance on manually engineered features limited scalability. Deep learning methods, such as Recurrent Neural Networks (RNNs) [8] and Long Short-Term Memory (LSTM) networks [9], improved detection by capturing temporal dependencies in rumour propagation. However, these models fail to incorporate the structural information unique to rumours, prompting the development of approaches that leverage propagation structures through kernel models [10], Recursive Neural Networks (RvNN) [11], and Graph Neural Networks (GNN) [12]\u2013[14]. GNNs, in particular, have demonstrated strong performance and computational efficiency, making them effective for both rumour detection and broader misinformation challenges [15], [16].\nGraph explainability techniques, widely used in domains such as molecular chemistry [17], [18], citation networks [19], [20], and scene graphs [21], [22], have seen limited application in misinformation detection. For GNN-based rumor detection, enhancing explainability is crucial for improving model trust and reliability. Techniques can be broadly categorized as gradient-based, decomposition-based, perturbation-based, and surrogate-based [23]. While perturbation-based and surrogate-based methods provide powerful insights, they are computationally intensive and lack generalizability in dynamic rumor contexts [23]. In contrast, gradient-based and decomposition-based approaches offer efficiency and scalability by leveraging model internal mechanisms.\nHowever, current explainability methods often provide only node or edge-level insights, which fail to capture critical dependencies in high-dimensional text embeddings used as node features in GNN-based models [12]\u2013[14]. To address this limitation, explanations must go beyond coarse representations to consider individual textual components, offering finer granularity and higher fidelity.\nResearch Objectives. We propose Contrastive Token Layerwise Relevance Propagation (CT-LRP), a novel framework that addresses the limitations of existing GNN explainability techniques by providing fine-grained, token-level explanations for rumour detection models. CT-LRP combines Layerwise Relevance Propagation (LRP) with an explanation space partitioning strategy, enabling it to isolate class-specific and task-relevant textual components. This token-level granularity captures dependencies in high-dimensional text embeddings, offering nuanced insights into model predictions that surpass traditional node and edge-level explanations.\nTo rigorously evaluate CT-LRP, we extend existing explanation metrics to support token-level resolution, ensuring fidelity and interpretability. Experiments on three public rumour detection datasets demonstrate that CT-LRP consistently produces reliable, high-quality explanations, setting a new standard for"}, {"title": "II. RELATED WORK", "content": "In this section, we discuss the relation of our work to existing research in GNN explainability and text explainability."}, {"title": "A. GNN Explainability", "content": "Explainability techniques for GNNs can be broadly cate-gorized into two groups: (i) methods adapted from Convolutional Neural Networks (CNNs) and (ii) methods specifically designed for GNNs. The first category includes gradient-based and decomposition-based methods such as LRP [24], Grad-CAM [25], Excitation Backpropagation (EB) [26], and Sensitivity Analysis (SA) [27], as well as perturbation-based approaches like LIME [28] and SHAP [29]. These techniques, originally developed for CNNs, generalize well to GNNs by treating graphs as lattice-shaped structures, where nodes represent pixels or features, and convolution filters act as subgraph kernels. However, these methods typically treat nodes like pixels with node features analogous to colour channels which fail to capture the nuance within the text represented by the node.\nThe second category includes techniques specifically de-signed for GNNs, such as GNNExplainer [30], PGExplainer [31], ZORRO [32], and GraphLIME [33]. These methods provide insights into node, edge, or subgraph-level attributions, offering explainability for graph-specific tasks like node classification or link prediction. However, their reliance on perturbation or surrogate models often limits their general-izability to unseen data, particularly in dynamic applications like rumour detection. Gradient-based methods, while more scalable, lack the granularity needed to interpret latent textual features represented in high-dimensional node embeddings."}, {"title": "B. Text Explainability", "content": "Explainability methods originally developed for text clas-sification tasks can be categorized into gradient-based, decomposition-based, and perturbation-based approaches. Gradient-based and decomposition-based methods, including LRP [24], Saliency Maps [34], and Guided Backpropagation [35], trace backpropagated gradients or relevance scores to determine the contribution of individual tokens to the model's decision. Perturbation-based techniques such as LIME [28], SHAP [29], and Occlusion [36] identify influential tokens by substituting input elements and observing changes in model output. These methods often struggle with latent feature in-terpretability, particularly when applied to high-dimensional embeddings.\nOur approach recasts the explainability problem for GNN-based rumor detection as a text explanation task. By combin-ing GNN explainability methods to accurately attribute node features with text explainability techniques to generate token-level explanations, CT-LRP bridges the gap between node feature attribution and interpretable token-level insights. This hybrid approach ensures that latent text features are effectively explained, offering a novel solution to the challenges faced by existing methods."}, {"title": "III. PRELIMINARIES", "content": "In rumour detection, event propagation on social media is modelled as a graph-level classification task. Let $G = (V, E)$ represent an event propagation graph, where $V$ is the set of nodes corresponding to posts made during the event, and $E$ is the set of edges capturing interactions between these posts. Each node $v \\in V$ is associated with a feature vector $x_v \\in R^{|D|}$, typically a text embedding derived from the post content. The feature matrix $X \\in R^{|V|\\times|D|}$ aggregates the embeddings for all posts in the event. The notations used are summarized in Table I."}, {"title": "IV. PROPOSED FRAMEWORK", "content": "Our approach addresses the low resolution of existing GNN explainability methods by reframing the explanation process to align with techniques used in text classification models. Instead of attributing relevance to entire sentences or posts represented as nodes in the graph, we aim to pinpoint the specific tokens that drive the model's predictions for a given class. This fine-grained approach enhances both the granularity and interpretability of the explanations. In this subsection, we first formalize the task of explaining GNN-based rumour detection models within the context of event propagation graphs. We then introduce our framework, which delivers detailed attributions at both the GNN-encoder and token levels, bridging the gap between high-dimensional embeddings and actionable insights."}, {"title": "A. Task Formulation", "content": "Building on the graph representation outlined in Section III, we expand the attributed event propagation graph $G = (V, E, X)$ to incorporate tokenized text content $P$ associated with the nodes. Here, $V = \\{0,1,...,n - 1\\}$ is the set of nodes, $E$ is the set of edges, and $X \\in R^{|V|\\times|D|}$ is the node feature matrix. The tokenized text $P \\in Z+^{|V|\\times|T|}$ is mapped to the feature matrix $X$ via a text embedding function $f_{text}(P_{text}) = X$, where $T$ represents the set of tokens for each node. The rumour detection model is implemented as a GNN $f(G|\\Theta) = y$, parameterized by $\\Theta$. Given $G$, the model outputs logits $y \\in R^{|C|}$, where the predicted class $\\hat{y}$ is obtained as $\\hat{y} = argmax(y)$. The goal of explainability is to generate a token-level attribution map $Z^{(\\hat{y})} = [z_0^{(\\hat{y})},...,z_v^{(\\hat{y})},..., z_{n-1}^{(\\hat{y})}]$, where $z_v \\in R^{|T|}$ quantifies the contribution of each token $t \\in T$ in node $v$ to the prediction $\\hat{y}$.\nFramework Overview. Fig. 1 illustrates the flow of in-formation through our framework. The process involves a forward pass and a backward pass. In the Forward Pass, tokenized text $P$ is transformed into the feature matrix $X$ via $f_{text}$. The GNN processes $X$ along with the adjacency matrix $A \\in \\{0, 1\\}^{|V|\\times|V|}$, producing logits $y$. In the Backward Pass, relevance scores are calculated for the predicted class $\\hat{y}$ by masking all other logits. These scores are backpropagated through the GNN to generate node-level attributions $R_{node}$. Finally, $R_{node}$ is backpropagated through $f_{text}$ to produce token-level attributions $Z^{(\\hat{y})}$.\nMotivation for Token-Level Attribution. As discussed in Section III, the feature matrix $X$ comprises latent text embeddings, which are challenging to interpret directly. Tra-ditional gradient and decomposition-based methods provide node-level attributions, but they fail to attribute relevance to individual tokens in $P$. By leveraging these methods to compute intermediate node-level attributions and extending them through relevance backpropagation, our framework pro-duces fine-grained, interpretable token-level explanations. This approach bridges the gap between high-dimensional latent features and actionable insights. The complete steps for gen-erating token-level attributions are detailed in Algorithm 1,"}, {"title": "B. GNN explanation", "content": "We select LRP [24] as our base method due to its strong attribution conservation properties, making it well-suited for generating interpretable explanations. Given our focus on token-level explanations, we prioritize node feature attribu-tions while treating the structure of the input graph $G$ as static. As a result, the importance of edges is implicitly accounted for through LRP's attribution propagation.\nFor the graph convolutional and classifier layers, we em-ploy the epsilon-stabilized LRP rule, which generates sparser explanations by focusing on the most salient features. This property is particularly valuable in large graphs, where spar-sity enhances both interpretability and usability. The epsilon-stabilized LRP rule is defined as:\n$r_j = \\sum_k \\frac{A_{jk}w_{jk}}{\\epsilon + \\sum_{j'} |a_{j'}w_{j'k}|} r_k$ (1)\nHere, $r_j$ and $r_k$ denote the relevance scores of neurons $j$ and $k$, respectively, $a_j$ represents the activation of neuron $j$, $w_{jk}$ is the weight of the connection between neurons $j$ and $k$, and $\\epsilon$ is a small stabilizing constant. By applying this rule during the backward pass, we obtain the node feature attribution map $R \\in R^{|V|\\times|D|}$ for the target class, with dimensions corresponding to the original node feature matrix $X$. In this attribution map, positive values indicate components that con-tribute to an increase in the model's output for the target class, while negative values indicate components that decrease the output for the target class. This node-level attribution serves as an intermediate representation, bridging the gap between GNN explanations and token-level attributions."}, {"title": "C. Token-level explanation", "content": "To generate token-level attributions, we backpropagate the node feature attribution map $R$ through the embedding func-tion. The embedding process is conceptualized as a two-stage operation. In the first stage, each token $t$ in node $v$ is embedded into the text embedding space via the function $f_{embed}$, producing the token vector $x_{v,t}$. In the second stage, these token vectors are aggregated to form the node vector using a pooling function $f_{pool}$. The pooling function $f_{pool}$ may involve simple operations like mean or max pooling, or more complex architectures such as a Multi-Layer Perceptron (MLP) or a Transformer network.\nFor simplicity, we first consider the case of mean pooling. The node vector for mean pooling is defined as:\n$x_{v,d} = \\frac{1}{|T_v|} \\sum_{t_v \\in T_v} x_{t_v,d}$ (2)\nwhere $T_v$ is the set of tokens in node $v$, and $x_{v,d}$ and $x_{t_v,d}$ represent the $d^{th}$ dimension of the node vector and token vector, respectively.\nApplying the epsilon-stabilized LRP rule (Eq. 1), the rele-vance propagation for mean pooling is given by:\n$r_{t_v,d} = \\frac{x_{t_v,d}}{\\epsilon + x_{v,d}} r_{v,d}$ (3)\nFor max pooling, the node vector and its relevance propa-gation rule are defined as:\n$x_d = max_{t_v \\in T_v} (x_{t,d})$ (4)\n$r_{t_v,d} = \\begin{cases} x_{t_v,d} r_{v,d}, & \\text{if } x_{t_v,d} = max_{t_v \\in T} (x_{t_v,d}) \\\\ 0, & \\text{otherwise} \\end{cases}$ (5)\nWhen $f_{pool}$ is implemented using an MLP or a more complex network, the epsilon-stabilized LRP rule (Eq. 1) is applied to the individual layers within the network as needed."}, {"title": "D. Contrastive Token-Level Explanation", "content": "Using the framework described above, we compute token attribution maps for each class, denoted as $Z^{(c)}$ for $c \\in C$. To refine the attribution map for the predicted class $\\hat{y}$, we compare it against the attribution maps of all other classes. Specifically, we identify tokens with positive attribution values in both the predicted class and any other class.\nFor each shared token, we construct a perturbed graph input $G' = G - x_t$, where the token vector $x_t$ is removed before the node feature aggregation step. The logits for the perturbed input are then calculated as $f(G') = y'$. The token's influence is determined by the difference between the original logits and the perturbed logits. If the influence on the predicted class satisfies $y_{\\hat{y}} - y_{\\hat{y}}' > y_c - y_c', \\forall c \\neq \\hat{y}$, then the token contributes more strongly to the predicted class and is retained in the explanation. Otherwise, it is excluded.\nTo finalize the explanation, we generate a mask matrix to eliminate shared tokens that have a greater influence on other classes and tokens with negative attribution values. Multiplying the original attribution map by this mask yields an exclusive set of tokens that are specific to the predicted class, effectively disambiguating tokens that contribute positively to multiple class outputs."}, {"title": "V. EXPERIMENTS", "content": "We validate CT-LRP through quantitative experiments on three representative GNN-based models trained on publicly available rumour detection datasets. These experiments assess the framework's effectiveness and propose a novel paradigm for evaluating explainability in GNN-based models. This section outlines the models, datasets, and preprocessing steps, followed by the baselines and evaluation metrics. We conclude with a presentation and discussion of the experimental results."}, {"title": "A. Models and Datasets", "content": "Models: We evaluate CT-LRP using three representative GNN-based models, chosen for their distinct architectures and approaches to event propagation:\n\u2022 Bi-Directional Graph Convolution Network (BIGCN) [12]: This model represents event propagation as bipar-tite top-down and bottom-up directed graphs, processed by two separate Graph Convolution Networks (GCNs). Each GCN comprises two convolution layers, with mean pooling applied to node representations to generate graph-level embeddings. The embeddings are concatenated to form the final input for classification.\n\u2022 Edge-enhanced Bayesian Graph Convolutional Net-work (EBGCN) [13]: Similar to BiGCN, this model uses bipartite directed graphs but introduces an edge consistency module to dynamically learn and synchronize edge weights between top-down and bottom-up GCNs. The graph aggregation strategy mirrors that of BiGCN.\n\u2022 Claim-guided Hierarchical Graph Attention Network (ClaHi-GAT) [14]: Using an undirected graph struc-ture, this model incorporates a Graph Attention Network (GAT) with two convolution layers. It also employs"}, {"title": "B. Data Preprocessing and Model Training", "content": "Data Preprocessing: For each dataset, we use the post metadata to construct the top-down graph representing the event propagation structure. We then reverse the direction of the edges to create the bottom-up graph and combine both graphs to form the undirected version of the graph. To standardize text preprocessing across all three datasets, we use a pre-trained multilingual BERT model [38] and its associated tokenizer to tokenize the post text and generate token em-beddings. We opted for a multilingual BERT model because the PHEME dataset includes events in multiple languages. Additionally, using the same multilingual BERT model across datasets ensures that the generated explanations are consistent and not influenced by differences in text embedding models that could arise if different monolingual BERT models were used. We used mean pooling for the main experiments as they demonstrated the best overall model performance on the rumour detection task. We include the results in Appendix A.\nModel Training: Following the task setup in the original papers for our three selected models [12]\u2013[14], we train each model using a five-fold cross-validation split for the Twitter and Weibo datasets, and a nine-fold event-wise cross-validation split for the PHEME dataset. We adopt the hyper-parameter settings specified in the original papers for each model and train them for 200 epochs, with early termination triggered if the loss plateaued for 10 consecutive epochs."}, {"title": "C. Baselines and Evaluation Metrics", "content": "Baselines:\n\u2022 LRP [24]: A decomposition-based method that utilizes deep Taylor approximation and relevance conservation principles to explain the individual feature dimensions of nodes for the class of interest.\n\u2022 Grad-CAM [25]: A gradient-based method that weights"}, {"title": "D. Quantitative Study", "content": "We evaluate fidelity at fixed sparsity levels for each model using k-fold cross-validation. At a given sparsity level, ele-ments highlighted by the explanation are removed in decreas-ing order of importance until the sparsity limit is reached. For node-level baselines, at a sparsity of 0.8, the most salient nodes are removed until 20% of the total nodes in the graph remain. Similarly, for CT-LRP, at the same sparsity level, the most salient tokens are removed until 20% of the total tokens in the graph remain. To prove the effectiveness of the token class disambiguation step, we also test LRP at the token level to provide a comparison. The results are summarized in Tables II and III, with the top results in bold and the second-best marked with an asterisk (*).\nThe results show that our framework provides superior explanations of model behaviour, as evidenced by the average fidelity scores. Both CT-LRP and token-level LRP achieve the highest fidelity scores across all models and datasets. Among the two, CT-LRP performs better, showing the importance of the class membership disambiguation step, with an average of 25.77% increase in performance over token-level LRP.\nOn sparsity, c-EB achieves the sparsest explanations due to its WTA process. However, its low fidelity severely limits its utility in understanding model behaviour. In contrast, the CT-LRP provide the second-highest sparsity scores while maintaining high fidelity. CT-LRP demonstrates superior per-formance over the baselines in achieving an optimal balance between fidelity and sparsity, as evidenced by the Fidelity-Sparsity scores in Table IV, with an average 66.98% increase in score over the next best baseline.\nFig. 3 shows a detailed analysis of CT-LRP and other baselines at different sparsity levels for each model. Generally, as sparsity increases, CT-LRP and token-level LRP experience a notable drop in fidelity although CT-LRP exhibits a great drop in fidelity only at higher sparsities. This suggests that the disambiguation of token class membership successfully removed elements that have a greater impact on other class outputs, thereby ensuring the remaining tokens in the expla-nation are those which are most salient to the predicted class. On the other hand, the baselines show little change in fidelity at higher sparsity levels, likely due to the smoothing effect of node-level saliency. In such cases, removing all tokens from a node impacts the output more uniformly across classes."}, {"title": "VI. DISCUSSION", "content": "CT-LRP represents a significant advancement in explain-ability for GNN-based rumour detection models by providing token-level explanations that are both class-specific and task-relevant. Our experiments reveal that existing GNN explain-ability methods, which primarily focus on node-level expla-nations, fall short of capturing the intricate decision-making processes underlying these models. In contrast, CT-LRP's token-level granularity offers a more nuanced understanding of model behaviour, as demonstrated by the substantial im-provements in fidelity across all models and datasets. This granularity is essential for rumour detection tasks, where unseen data and dynamic content make accurate interpretation of predictions particularly challenging.\nKey findings from our application of CT-LRP to the Twitter, Weibo, and PHEME datasets underscore its ability"}, {"title": "VII. CONCLUSION", "content": "In this study, we introduced CT-LRP, a novel framework for enhancing the explainability of GNN-based rumour detection"}]}