{"title": "Towards An Automated AI Act FRIA Tool That Can Reuse GDPR's DPIA", "authors": ["Harshvardhan J. Pandit", "Tytti Rintam\u00e4ki"], "abstract": "The AI Act introduces the obligation to conduct a Fundamental Rights Impact Assessment (FRIA), with the\npossibility to reuse a Data Protection Impact Assessment (DPIA), and requires the EU Commission to create of an\nautomated tool to support the FRIA process. In this article, we provide our novel exploration of the DPIA and\nFRIA as information processes to enable the creation of automated tools. We first investigate the information\ninvolved in DPIA and FRIA, and then use this to align the two to state where a DPIA can be reused in a FRIA.\nWe then present the FRIA as a 5-step process and discuss the role of an automated tool for each step. Our work\nprovides the necessary foundation for creating and managing information for FRIA and supporting it through an\nautomated tool as required by the AI Act.", "sections": [{"title": "1. Introduction", "content": "The rapid proliferation of artificial intelligence (AI) technologies presents novel challenges for creating\nnew regulations to guide responsible progress and prevent harmful impacts. The EU's recently published\nAI Act [1], which is the world's first legal framework to regulate AI, tackles this challenge through a\nrisk based approach where AI systems are classified as \u2018high-risk' based on their potential impact to\nfundamental rights. To guide organisations and authorities in detecting where such impacts might arise,\nthe AI Act's Article 27 lays down the obligation for high-risk AI systems to conduct a 'Fundamental\nRights Impact Assessment' (FRIA) that identifies \u201crisks of harm likely to have an impact\" on people or\ngroups of people affected by the use of the AI system.\nThis framework of using impact assessments is based on and similar to the EU's General Data Protec-\ntion Regulation (GDPR) [2], a globally impactful regulation that regulates the processing of personal\ndata and includes an impact assessment obligation called the \u2018Data Protection Impact Assessment'\n(DPIA). Under Article 35 of the GDPR, organisations are required to conduct a DPIA when their data\nprocessing activities are likely to result in a high risk to the rights and freedoms of individuals. This\nrequirement to conduct a DPIA can be based on the list of use-cases defined in GDPR, or through lists\npublished by Data Protection Authorities (DPAs) responsible for enforcing the GDPR.\nIn our prior work [3], we found there are 94 distinct conditions for which a DPIA can be required\nbased on the GDPR or the country involved, and that of the 25 high-risk conditions in AI Act's Annex\nIII, GDPR applies and a DPIA is necessary in 22 clauses, and could conditionally be required in the\nremaining 3. Thus there is an inherent synergy between the GDPR and the AI Act. While there will be\nuse-cases where AI systems do not involve personal data, there will also be a large number of AI systems\nusing personal data - either as inputs when developing the underlying AI models and components\nor as inputs to the system and by producing outputs that can be considered as personal data. This\nnecessitates a combined implementation of GDPR and the AI Act.\nThese findings reveal the wide overlap between GDPR and AI Act and establish the importance of\nexploring the synergy between their respective impact assessments, namely the DPIA in GDPR and the"}, {"title": "2. State of the Art", "content": "FRIA in AI Act as both require the assessment of risks to rights and freedoms and utilising the outputs\nto avoid harms to people. The drafting of the AI Act indeed acknowledges this by stating in Art.27-5\nthat \"If any of the obligations\nis already met through the DPIA conducted pursuant to Article 35\nof GDPR, the FRIA ... shall complement that DPIA\u201d. However, the AI Act does not expand upon this\nstatement in any other article or recital, thereby raising the question of how and where can a DPIA be\nused with a FRIA.\nIn addition to this, the AI Act's Article 27-5 states \u201cThe AI Office shall develop a template for a\nquestionnaire, including through an automated tool, to facilitate deployers in complying with their\nobligations under this Article in a simplified manner\u201d. This means the enforcement of the AI Act\nrequires creating an information system that will assist with the obligations associated with FRIA, and\nthat such an information system will use automation to simplify and support stakeholders in their\ncompliance related activities. In the past, similar approaches have existed for conducting DPIA under\nthe GDPR - though in the form of guidance from DPAs. For example, CNIL, the French DPA, has a tool\nthat supports DPIA processes by utilising a risk assessment approach to collecting and documenting\ninformation [4].\nWhile this is a progressive and positive step in terms of regulations acknowledging and requiring the\nuse of technologies to aid in simplifying and enforcing compliance, the implications of the AI Act in\nthis regard are not completely clear. For example, there are several possibilities for what the \u201cautomated\ntool\" might refer to - it could be a simple checklist that ascertains where and when a FRIA is needed,\nor it could be a tool to conduct the FRIA itself, or it could be a tool to support the notification of the\nFRIA to stakeholders (including authorities). There are also necessary questions that must be raised as\nto how such a tool can be made to support the variety and diversity of use-cases that exist, and how\nits information can be made to be interoperable so that other tooling can be used to address tasks not\ncovered by the tool. And how such a tool, if it is intended to, facilitate the reuse of a DPIA for FRIA.\nUsing this as the motivation, we outline the following research questions:\nRQ1 How to interpret DPIA obligations in a manner that facilitates its reuse for a FRIA (Section 3)\nRQ2 Where can automation assist with FRIA obligations? (Section 4)\nBy exploring these questions, we provide the first discussion on the implementation of Article 27 of\nthe AI Act regarding how to approach the FRIA from an information systems and knowledge engineering\nperspective, the intersection and reuse of GDPR\u2019s DPIA with the FRIA, and the implications of these\non the creation and use of automated tools to support both DPIA and FRIA processes. Our work has\nimportant implications for all stakeholders - for AI authorities and organisations using AI it provides a\nframework for how to approach the implementation of FRIA, how to interpret the FRIA information in\na manner that aids in the creation of automated tooling for different use-cases, and how the DPIA can\nbe integrated in various ways to support the FRIA.\nFor the GDPR authorities and organisations processing personal data, our work provides a way to\nadapt existing DPIA processes and tooling to address the additional requirements of a FRIA. Our work\nalso has implications on the combined enforcement of GDPR and AI Act and the co-operation of their\nrespective authorities as it outlines what informational overlaps exist in the obligations for DPIA and\nFRIA, and how these can be combined and used to support common obligations."}, {"title": "2.1. Al Act and FRIA", "content": "The AI Act and the FRIA are both recent developments. Therefore the state of the art on this topic is\nminimal, though it is expected to be a growing discussion as the AI Act starts its enforcement activities.\nMantelero [5] discusses the AI Act and FRIA as a legal processes and compares it to the approaches\ntaken by GDPR's DPAs regarding the use of AI. Calvi and Kotzinos [6] discuss the FRIA as part of a wider\nanalysis of impact assessments across GDPR, AI Act, and Digital Services Act (DSA). Similarly, Malgieri\nand Santos [7] also analyse impact assessments across these regulations and proposal a rights-based"}, {"title": "3. How to use a DPIA for a FRIA?", "content": "To answer the question of how to interpret DPIA obligations in a manner that facilitates its reuse for a\nFRIA (RQ1), with the eventual goal of creating an automated tool (RQ2 and RQ3), we first establish\nin Section 3.1 what information is involved in a DPIA. We perform a similar exercise in Section 3.2\nregarding the FRIA process. In Section 3.3 we explore the different roles a DPIA can take within the\nFRIA process - namely as an ex-ante input where an existing DPIA supports a new FRIA, or as an\nex-post activity where both the DPIA and the FRIA are conducted simultaneously. We then use these to\nalign the DPIA process with the FRIA process in Section 3.4 based on the information involved."}, {"title": "3.1. Information Requirements for DPIA", "content": "A DPIA under GDPR is an ex-ante activity that is carried out before the processing of personal data to\nascertain whether the processing is \u201clikely to result in a high risk to the rights and freedoms of natural\npersons\" (GDPR Art.35). The first step in the DPIA process is therefore ascertaining whether a DPIA is\nrequired. The GDPR states that assessing whether a DPIA is required should be done \u201cin particular"}, {"title": "3.2. Information Requirements for FRIA", "content": "A FRIA under the AI Act is also an ex-ante activity that is carried out before deploying a high-risk AI\nsystem as determined under the conditions outlined in Art.6-2, and is intended to be \u201can assessment of\nthe impact on fundamental rights that the use of such system may produce\u201d (Art.27-1). Similar to the\nDPIA process, the first step in the FRIA process is to assess whether a FRIA is required based on the\nclassification of an Al system as high-risk, and the obligations and exceptions established in Articles 6\nand 27. For the scope of this work, we only focus on the cases where a FRIA is necessary and do not\ndiscuss the exceptions for entities and use-cases that do not require a FRIA.\nOnce the obligation to conduct a FRIA has been established, the second step in the FRIA process is\nto identify and collect the relevant information to carry out the impact assessment on fundamental\nrights - which the AI Act states as being \u201chealth, safety, fundamental rights as enshrined in the Charter\nof Fundamental Rights of the European Union\u201d (Recital 1). And where the 'risks' as per Art.27-1d are\nspecific to the \u201crisks of harm likely to have an impact on the categories of natural persons or groups\nof persons\u201c. Thus the broader interpretation of the FRIA is to assess the risk of harmful impacts to\nfundamental rights as enshrined within the Charter.\nThe relevant information required to conduct a FRIA involves \u201ca description of the deployer's\nprocesses in which the AI system will be used in line with its intended purpose\u201d (Art.27-1a), the duration"}, {"title": "3.3. DPIA as an Ex-Ante and Ex-Post input to FRIA", "content": "In the previous two sections, we explored the DPIA and FRIA processes in terms of the information\nrequired to conduct them as well as the outputs produced and their use with in the respective scopes of\nGDPR and AI Act. In this section, we discuss where a DPIA may be used with FRIA based on the AI\nAct's statements regarding the existence of prior impact assessments in Art.27-2 and the functioning of\nthe AI value chain in Art.25."}, {"title": "3.4. Reuse of DPIA Information in FRIA", "content": "For both cases whether the DPIA is an ex-ante input to the FRIA or an ex-post output, as outlined in the\nprevious section, we need to identify the commonality in information requirements for both so as to\nbuild technical tools that can support the completion of FRIA obligations based on information present\nin/through a DPIA. To do this, we take the information concepts identified in Section 3.1 regarding\nDPIA and Section 3.2 regarding FRIA and create a mapping between them to identify similarities and\noverlaps. To start with, we first align the clauses in GDPR Art.35 and AI Act Art.27 to understand the\ncommonality in legal obligations, and then explore the alignment in information required to meet each\nobligation. A detailed exploration of such an alignment between the obligations and information across\nboth regulations requires further work and is beyond the scope of this current article (we explicitly\nmention this later as future work).\nSystematic Description: GDPR Art.35-7a and AI Act. 27-1a and Art.27-1b both require a systematic\ndescription of the respective systems as inputs to the impact assessment process. This includes purposes,\ntechnical operations, involved data, human (data or AI) subjects, and context such as scale of data\nand operations, cross-border implications, controls for entities (e.g. consent, oversight), whether the\nactivities involve profiling, scoring, or decision making, and the environment (e.g. public area).\nBy comparing the systematic descriptions required for a DPIA (Section 3.1) with that for a FRIA\n(Section 3.2), we find that both require identifying common elements such as the purposes, data - and\nwhether it is personal and special category under GDPR, human subjects (whether as data subjects\nor AI subjects), technical descriptions of activities such as the processing operations over data, scale\nand scope of data, processing, and subjects, location of operations, contextual considerations such as\nthe involvement of profiling and decision making, the specific technical and organisational measures\nused, data quality and governance measures implemented, and the \u2018necessity and proportionality' of\nthe operations against the (intended) purposes.\nWe also find that there are some items which do not share an overlap - such as the interpretation of\nlegal bases in the DPIA process under GDPR, or the involvement of non-personal data under the AI Act."}, {"title": "4. How to use an \u2018Automated Tool' in FRIA?", "content": "Further, the AI Act has more information requirements which do not correspond to requirements for\nthe DPIA - such as the specific use of data in training and validation, user interfaces to manage the\nAl systems and explain its outputs, provenance of the AI system and its development, and planned\nchanges to the Al system.\nHowever, where these concepts involve personal data, they become relevant for a DPIA as part of\ndescribing the processing operations (e.g. personal data used to train AI systems) or as part of specific\nmeasures (e.g. explanation of AI outputs can be useful as a measure for automated processing). Thus,\nsystematic description of an AI system as required in a FRIA can also aid and compliment the DPIA\nprocess. This means both ex-ante and ex-post uses of DPIA as described in the previous section are\nfeasible and can be facilitated through the same automated tool being built to support the FRIA process.\nConversely, this also means existing DPIA tools can be adapted to support the FRIA process.\nProportionality and Necessity: Though the AI Act does not directly use these terms, these concepts\nare well established in EU law [19] and are a necessary tool in impacts assessments. For GDPR,\nproportionality and necessity (GDPR Art.35-7b) refer to the processing of personal data for specific\npurposes. For AI Act, proportionality and necessity can be interpreted to refer to the use of AI systems to\nachieve the intended purpose (AI Act Art.27-1a) - a crucial concept which is the basis for interpretation\nof other important concepts such as reasonably foreseeable misuse (Art.3-13), instructions for use (Art.3-\n14), and performance of an Al system (Art.3-18). Where AI systems involve the processing of personal\ndata, there is therefore an overlap in the assessment of proportionality and necessity assessments as\nboth GDPR and the AI Act assess this based on their respectively defined purposes.\nRisks to Rights and Freedoms: GDPR\u2019s DPIA refers to \u201crisks to the rights and freedoms of data\nsubjects\" (Art.35-7c), while the AI Act's FRIA refers to \u201cassessment of the impact on fundamental rights\"\n(Art.27-1d) and \u201csignificant risk of harm to the health, safety or fundamental rights of natural persons\"\n(Art.6-3). The DPIA thus has a broader scope regarding impacts to rights as it includes both fundamental\nand other rights, whereas the FRIA specifically focuses on fundamental rights. Both DPIA and FRIA\nrequire an assessment of likelihood and severity of risks and use this information to identify a risk\nlevel to decide on further obligations. Further, both require an assessment of whether risks are likely to\nimpact rights (in the case of AI Act - fundamental rights).\nWhile GDPR does not explicitly describe how to assess \u2018impacts', it defines \u2018material or non-material\ndamage' (Art.82) as a criteria for compensation. The AI Act uses the phrasing 'risk of harms' to narrow\nthe categories of impacts to events that can be considered as 'harms' under EU law, and also provides\nexplicit examples including damage and disruption (Recital 155). Thus, while the initial process of risk\nassessment is similar in both DPIA and FRIA, the assessment and expression of impacts have differing\nlegal terminology and criteria that depend on their respective regulations.\nRisk Mitigation Measures: Both DPIA (GDPR Art.35-7d) and FRIA (AI Act Art.27-1f) require iden-\ntifying specific measures in response to risk through risk assessment and management processes.\nWhile the GDPR includes specific technical and organisational measures (Art.32), these are focused on\nconfidentiality, integrity, availability regarding the processing of personal data regarding (Art.35-7d).\nIn contrast, the AI Act provides a detailed framework consisting of risk management (Art.9), data\ngovernance practices (Art.10), technical documentation (Art.11, Anx.IV), human oversight (Art.14),\nensuring accuracy, robustness, and cybersecurity (Art.15), and establishing a quality management\nsystem (Art.17).\nFrom this, we can see that the DPIA's notion of risk mitigation measures is limited as compared\nto the AI Act, though both have the same ultimate goal of using these measures to mitigate risks\nregarding impacts on rights. Based on where and how an AI system involves and processes data, and\nthe interpretation of where the scope of GDPR starts in this process, the measures described in AI act\nalso have a bearing on the implementation of the GDPR as they will be considered as technical and\norganisational measures that safeguard the processing of personal data and address risks to rights.\nSimilarly, the specific technical and organisational measures intended to safeguard the processing of\npersonal data, and used within a DPIA, would also be relevant to support the AI Act's obligations\nregarding risk management based on the involvement of personal data within the AI system and its\nlifecycle."}, {"title": "4.1. FRIA as a 5-step process", "content": "The AI Act Art.27-5 requires the AI Office to develop a questionnaire and an automated tool to \u201cto\nfacilitate deployers in complying with their obligations ... in a simplified manner\". Recital 96 further\nclarifies that this requires the AI Office to \u201cdevelop a template for a questionnaire in order to facilitate\ncompliance and reduce the administrative burden for deployers\". From this, it is clear that the question-\nnaire is to be filled in by the deployer (interpreting the term 'template'), and that the outputs of the\nquestionnaire are to support obligations outlined in the AI Act, with the automated tool reducing the\nefforts involved in some form. Further ahead in this section, we outline our interpretation of stages in\nthe FRIA process along with an overview for what information is involved and how the questionnaire\nand tool could be used for that stage.\nHowever, in order to discuss tools from an information system and software development perspective,\nthe information provided in the AI Act is vague and not sufficient for creating a tool. For example, what\ndoes", "as": "systematic description, proportionality and necessity assessment,\nrisks to rights, and risk mitigation. These 4 categories, in the specified order, provide a structured\napproach to collecting and using the information within a FRIA process. To complete the obligations in\nArt.27-3 regarding deployers notifying market surveillance authorities the FRIA outputs through the\nfilled-out template, we add a fifth category of information corresponding to the last stage in Section\n4.1 regarding notifying authorities. The FRIA questionnaire and automated tool can thus work in the\nfollowing manner for each stage."}, {"title": "4.2. Stage 1: Determining FRIA necessity", "content": "In the first stage, the tool helps decide whether a FRIA will be needed. The obligation a tool can help\nwith here is Art.27-1 and Art.6-2 in which an entity determines whether it is required to conduct a FRIA\nbased on its role under the AI Act (deployer or provider), the categorisation of the AI system as high-risk\n(Art.6, Anx.I, and Anx.III), and whether there is a valid exception or exemption for its use-case (Art.6\nand Art.27). The tool can vary from being a completely manual process that only aids in identifying the\nrequirements to assess whether a DPIA is required (e.g. a checklist) to being a completely automated\nprocess where a decision for whether a FRIA is needed is provided as an output.\nIn this stage, The tool takes as input information regarding the AI system being assessed, such as\nwhether it is being developed and put on the market and/or it is being deployed, whether it satisfies any\nhigh-risk criteria established in Art.6 or Art.27, and whether it is subject to any exception or exemption\nregarding the classification of high-risk and the requirement for a FRIA. If the tool is intended to\nsupport such classifications through automation, it can utilise existing approaches which take specific\ninformation categories and determine the risk level based on logical reasoning [9]. The final output of\nthe tool is a binary result that indicates whether a FRIA is necessary or not.\nWe do not provide a detailed description of the information required for the tool in this first stage\nassociated with deciding whether a FRIA is needed as this requires a lengthy analysis of the AI Act as a\nwhole to define the role of entities and the classification of high-risk in Annex I, but we assume it is a\npre-requisite step that must be completed before any of the other stages are initiated."}, {"title": "4.3. Stage 2: Reusing DPIAS", "content": "The second stage is an optional process where the tool helps reuse an existing DPIA as outlined in\nSection 3.3. The inputs for this process will be a completed DPIA, and the tool will help with obligations\nin Art.27-2 and Art.27-4. Its outputs consist of producing the information in Section 3.2, and integrating\nit in the FRIA process as required in Section 4.2. For the tool to reuse a DPIA in this manner, the tool\ncan either ask the user to input the DPIA outputs (information about the processing of personal data,\nrisks identified, mitigations identified or applied, impact to rights) and then integrate it within the FRIA\ninputs, or use some developed interoperable specification or technical mechanism through which it can\nautomatically extract the relevant information."}, {"title": "4.4. Stage 3: Information gathering", "content": "In the third stage, the tool helps with collecting the inputs defined in Section 3.2 required for the FRIA\nprocess, and helps with obligations in Art.27-1. The tool can vary from being completely manual\n(e.g. a form) or completely automated (e.g. extracting or inferring technical information from existing\ndocuments or pre-defined use-cases).\nThe input information required here relates to the AI system and its operations such as its intended\npurpose, relevant entities and their roles, involved data, and specific requirements for operation. The tool\ncan either directly ask the user to provide this information, or use automation to derive the information.\nBased on the categories identified in Section 3.2, the following additional inputs are also needed.\n1. Purpose compatibility: an assessment of whether the purposes of the AI system being deployed\nare compatible with the intended purposes for which the AI system was developed and put on the\nmarket (Art.27-1a). The answer to this must include a binary value in the form of compatible or\nnot-compatible. For internal compliance management purposes, the tool should also aim to collect\nadditional information such as when this assessment was performed, by whom, whether the AI\nsystem has had any change since then, and recording links or references to any documentation\nthat must be associated with this process. If the tool is designed to support performing an\nassessment of purpose compatibility, it can use approaches such as those defined in literature [9]\nwhich provide a heuristic to flag situations requiring an assessment and provide a methodology\nto perform it based on specific information categories already available to the organisation.\n2. Affected Persons: the direct or indirect involvement of natural persons and groups of persons\nwhich is categorised based on whether they are the user/operator or subject of the AI system,\nwhether they are intended or unintended, active or passive, informed or uninformed, and whether\nthey have a relationship with the deployer - such as through a contract for service provision. In\naddition to this, information on what controls, if any, are available to such persons in the context\nof the Al system is also necessary - such as whether they can view the output, correct it, and can\nopt-in/out. It is also essential to consider whether such affected persons can be considered to be a\n'vulnerable group' by virtue of their nature (e.g. minors) or social vulnerability (e.g. belonging to\na protected group). It is also essential to not only consider persons being identified as directly\ninvolved subjects, but also as potentially being excluded - which is important for determining\nimpacts such as discrimination in later stages.\n3. Personal Data and Special Category Personal Data: a binary (yes/no) assessment of whether\nthe AI system uses or produces or is capable of producing personal data, and whether such data\nbelongs to special categories under GDPR, and whether any inputs or outputs of the AI system\ncan be classified as profiling or decisions. It is also essential to enquire regarding the quality\nof data in terms of specific characteristics relevant to its role e.g. completeness for training, or\naccuracy for output and verification. This information can be collected directly by the tool or\nretrieved from a \u2018datasheet' if present.\n4. Technical and operational metrics: pertinent details regarding the performance, robustness,\nand cybersecurity of the AI system (Anx.IV) that are necessary to identify risks in use of the AI\nsystem. This includes the ability to understand the operations and outputs (explainability) in"}, {"title": "4.5. Stage 4: Producing outputs", "content": "In the fourth stage, the tool helps with producing the outputs from the FRIA process - namely the risks\nof harmful impacts on health, safety, and fundamental rights, and helps with obligations in Art.27-1."}, {"title": "4.6. Stage 5: Notifying authorities", "content": "In the fifth stage, the tool helps with the notification of the FRIA to the authorities as required in Art.27-3.\nThe input to this tool will be a completed FRIA along with relevant metadata such as timestamps and\nentity information, and the output could consist of an acknowledgement from the authority including\ntimestamps and identifiers. The tool could assist here in creating the documentation to be sent in the\nnotification, or can also act as a communication tool to directly notify the authority with the relevant\ninformation. As before, the tool can do these operations manually (i.e. the user decides when and what\nto notify) or automatically (i.e. the tool automatically sends notifications for specific conditions)."}, {"title": "5. Conclusion & Future Work", "content": "In this article, we interpreted the GDPR\u2019s DPIA and the AI Act's FRIA as processes involving information\nfor inputs and outputs. Based on this, we identified the commonality in both to identify where a DPIA\nmight be reused to create a FRIA, or based how a DPIA and a FRIA could be developed concurrently.\nWe then used this understanding to express the FRIA as a 5-step process, including the reuse of a DPIA\nand the notification obligation. Through this, we have outlined a methodological approach for the\nimplementation of the FRIA questionnaire and automated tool, as required in AI Act Article 27, and\nprovided the first discussions on how and where such an automated tool could be used, and what\ninformation it would require, and the varying uses of automation in the FRIA process. Our work has\nsignificance in the developing interpretations of the AI Act, particularly regarding the novel clauses\nassociated with FRIA, and provides a practical approach for the information and documentation that is\nneeded along with how technological tools can support in tasks associated with it.\nThe work presented in this article represents the preliminary stages of a larger approach towards\nreusing DPIAs in FRIA, and creating automated tools to support the FRIA processes. Both the DPIA and\nthe FRIA are complex legal processes which require varying information depending on the use-case\ninvolved, and therefore will also require information management tools that can support it. We therefore\nidentify the necessity to develop approaches that facilitate the representation of information associated\nwith DPIA and FRIA processes in a structured, machine-readable, and interoperable manner so as\nto enable the creation of useful tools and information services that can work across use-cases and\nfor different stakeholders. We plan to use and extend the Data Privacy Vocabulary (DPV) [21] for\nrepresenting DPIA and FRIA information as the DPV enables representing information for both GDPR\nand AI Act, and provides a large collection of taxonomies that aid representing real-world use-cases.\nWe also identify the need to further investigate the requirements for conducting DPIA and FRIA\nbased on the legal obligations defined in GDPR and AI Act respectively, as these are crucial requirements\nthat form the first stage in both assessments. For this, we aim to represent the information required\nto assess whether a DPIA and a FRIA is needed using the DPV, and to then use this as the basis to\ndevelop a tool that can automate the assessment for when a DPIA or a FRIA is needed, and which can\nbe further enhanced to support the individual in expressing their use-case and discovering risks and\nimpacts through logical reasoning methods."}]}