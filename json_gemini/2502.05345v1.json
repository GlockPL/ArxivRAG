{"title": "Estimating Voltage Drop: Models, Features and Data Representation Towards a Neural Surrogate", "authors": ["Yifei Jin", "Dimitrios Koutlis", "Hector Bandala", "Marios Daoutis"], "abstract": "Accurate estimation of voltage drop (IR drop) in modern Application-Specific Integrated Circuits (ASICs) is highly time and resource demanding, due to the growing complexity and the transistor density in recent technology nodes. To mitigate this challenge, we investigate how Machine Learning (ML) techniques, including Extreme Gradient Boosting (XGBoost), Convolutional Neural Network (CNN), and Graph Neural Network (GNN) can aid in reducing the computational effort and implicitly the time required to estimate the IR drop in Integrated Circuits (ICs). Traditional methods, including commercial tools, require considerable time to produce accurate approximations, especially for complicated designs with numerous transistors. ML algorithms, on the other hand, are explored as an alternative solution to offer quick and precise IR drop estimation, but in considerably less time. Our approach leverages ASICs' electrical, timing, and physical to train ML models, ensuring adaptability across diverse designs with minimal adjustments. Experimental results underscore the superiority of ML models over commercial tools, greatly enhancing prediction speed. Particularly, GNNs exhibit promising performance with minimal prediction errors in voltage drop estimation. The incorporation of GNNs marks a groundbreaking advancement in accurate IR drop prediction. This study illustrates the effectiveness of ML algorithms in precisely estimating IR drop and optimizing ASIC sign-off. Utilizing ML models leads to expedited predictions, reducing calculation time and improving energy efficiency, thereby reducing environmental impact through optimized power circuits.", "sections": [{"title": "I. INTRODUCTION", "content": "Prediction of IR drop is an important problem faced today often by ASIC designers. As the current (I) flows through the Power Distribution Network (PDN), a part of the applied voltage inherently drops across the current path, which is, in simple terms, the definition of IR drop. This voltage deviation from the expected original value (of supply voltage) can either happen to the power supply (VDD), which results in voltage drop, or to the grounding (GND), which results in a ground bounce. These two effects are called together Power Supply Noise (PSN) [1], as illustrated in Figure 1 below.\n\nThe deviation of voltage level needs to be restricted because it may prevent the circuit from meeting its timing target and function properly, leading to a compromised performance [1, 2]. In addition, excessive IR drop can affect the reliability of the signals in the ASIC and increase the negative impact of the noise in the circuit. Cross-talk between signals in the PDN may become severe with the presence of a high IR drop. Commercial tools typically use electrical and geometrical features of the ASIC design as an input to localize and estimate IR drop in the layout.\n\nExisting commercial tools can achieve very accurate results in terms of localizing IR drop values and their locations within the circuit of an ASIC. An example of a typical workflow of the commercial tool can be seen in Figure 2. Engineers analyze the design of the circuit according to the results, and then an Engineer Change Order (ECO) is applied if required. Thus, a new round of simulations is required for verification. This process is a standard routine in every ASIC design and manufacturing process, and it is defined as the \"sign-off\" process.\n\nWith the transition to larger density integration of transis- tors, the number of connection layers and interconnections have increased exponentially over the last decades, driven by the more complex designs needed to deliver high-demand applications, and at the same time, this also leads to stricter power integrity parameters [3]. As a result, while commercial tools are trying to keep up with the up-scaling demand, they may need a considerable amount of time (even days for large-scale commercial designs) to examine every single node of connection for voltage drop. Machine learning (ML) algorithms, through learning the heuristics of the time & cost expensive simulation result, can have the potential to deliver a real-time surrogate for IR drop estimation that can shorten the design phase by minimising the time needed for estimating the voltage drop."}, {"title": "II. RELATED WORK", "content": "When we focus on approaches that leverage ML algorithms to approximate IR drop, we see they rely on the use of linear regression, Support Vector Machines (SVM), XGBoost or neural-network-based ones such as Convolutional Neural Networks (CNN). For example, in [4], they use a linear re- gression technique for IR drop prediction of each cell instance. They describe a method that creates linear correlation models between the average power and IR drop of a few selected patterns and then they use that to pinpoint the design that exhibited an excessive IR drop. They evaluate their technique against benchmark circuits (614), (b17), (b18) and (619) of ITC'99 [11] to validate their method, with high accuracy. However, such method represents data in a local cell for its limited receptive field, leading to a native generalization crisis across cells. Thus, their approach requires a significant time investment in computing a local model for every cell instance of large benchmarks. On the contrary, our work proposes the formulation of a graph neural network,which required graph data structure being expressive towards IR- drop related feature. Such expressiveness contributes to a faster convergence than CNN model that learns from the matrix data representation.\n\nXGBoost is another popular machine learning algorithm used for regression and classification problems, as it is known for its scalability, speed and accuracy. A model was trained in [5], based on data captured before ECO to predict the voltage drop after ECO. There can be 17 input features to the ML model, consisting of power, timing, and geometrical information in order to be more accurate during the prediction phase. After defining the area around a cell that they wanted to test, they extracted features of the target cell, such as cell type, loading capacitance, toggle rate, and peak current of a cell during switching time. A table was used for data representation, similar to what have been used in other cases of XGBoost simulation. They also imported information from neighboring cells in the form of a density map. Specifically, they selected cell instances that have the same timing window as the target cell, and they constructed density maps of average and peak current, toggle rate, and power consumption. The use of a fast ML algorithm like XGBoost, resulted in simulation time of 2 minutes, 10 \u00d7 faster compared to a commercial tool.\n\nPowerNet [10] is an example of the use of CNN in IR drop prediction, and specifically, the aim was to detect locations of IR drop hot spots. Figure 3a shows the ML model of PowerNet and Figure 3b the CNN model used.\n\nAs it can be seen, two types of maps are generated, based on cell power information. The first type includes four power maps, which do not include any time information, and the second one includes a power map that goes through time and space decomposition. Time decomposition means that every power map $P_t[j]$ corresponds to one time instant j * t, so only cells that can switch at the same instance are used in the ML model. These time decomposition and total power maps are perceived as an attributed matrix by CNN and based on these, PowerNet [10] finds the one that results in the highest IR drop at each grid by processing all of these maps in parallel with the help of a CNN model. By combining general power and time-related power for each instant, PowerNet [10] can predict the dynamic IR drop hot spots more accurately. However, compared with a less complicated CNN model, processing so many attributed matrices in"}, {"title": "B. Features Extraction", "content": "Feature selection is crucial for IR drop analysis, both for ML models and heuristic algorithms in simulation, as it influences the accuracy of the ML models and final results and the duration of the simulation. The first step for the commercial tool is to generate a \u201cnetlist\u201d of the design under test, which contains information about the components of the circuit, their connections, and the power rails of the circuit [15]. Then, the software extracts geometrical information from the layout, including parasitics [15]. The three main areas from which these features are extracted are power, timing, and physical information. Power features may include the target cell's toggle rate (TR), average current drawn by a cell instance ($I_{avg}$) or total power consumption ($P_{total}$), which includes switching, leakage and internal power. Timing features may include the switching timing of the target cell minimum and/or maximum rising/falling time. Finally, physical features are related to a cell instance's actual (x, y) coordinate on physical layout or their distances from the nearest via.\n\nAn interesting selection of input features suitable for ML model training was developed in PowerNet [10]. As mentioned above in Section II, features in PowerNet are represented as power maps, which include features such as internal, switching, and leakage power and a time power map, which contributed to the model by ensuring that only cells that could switch at the same time were considered in the training of the model. Similar to PowerNet [10], Chen et al. imported two types of features: raw features and density map features [16]. The first one is not much different compared to similar studies as they also included features about (x, y) coordinates of a cell, power, and current consumption. However, the density map features used in their study include information about the target cell and its neighboring cells around it."}, {"title": "III. METHOD OVERVIEW", "content": "The first dataset we used is called CircuitNet [17]. It is an open-source dataset dedicated to ML applications in Electronic Design Automation (EDA), and it consists of approximately 10,000 samples [17] from versatile runs of commercial design tools. The information on the layout was converted into attributed matrix based on tiles of sizes 1.5um \u00d7 1.5um, and they make up the main part of CircuitNet. The features (1) and (2) and labels (3) that were used to generate the power maps are described below:\n\n1) Instance Power: Include the instance level internal, switching and leakage power along with the toggles rate from a vector-less power analysis.\n2) Signal Arrival Timing Window: Is the possible switch- ing time domain of the instance in a clock period from a static timing analysis for each pin. The clock period is decomposed evenly into 20 parts, and the cell contributes to the power map of total power only in the parts that it is switching."}, {"title": "B. Feature Selection", "content": "Feature selection is important when building the ML model for IR drop downstream tasks. Based on the features used, the model will be trained on these to predict accurate IR drop values when unseen data are used during test time. The features used for our model are described in Table I.\n\nWe cherry-picked two sets of feature combinations to de- termine the performance impact by feature selection. SET A includes features regarding the net name, path resistance R, total power consumption Ptotal, peak current of a cell instance $I_{peak}$, average current consumption $I_{average}$ and the x, y coordinates. SET B includes the features of SET A, plus the timing characteristics of the transient simulation (rise and fall time of cells switching) and the response time RC."}, {"title": "C. GNN architecture", "content": "Data representation in GNNs is not the same as for CNNs because the former is designed to process non-grid structures, such as graphs or networks, where nodes are connected to other nodes in a complex pattern. Such formulations have proven to outperform image-like grid representation in many complex systems. One major superiority that GNN offers is its message-passing (MP) procedures to propagate information between the nodes of the graph. MP provides more ad-hoc- ness than the convolutional kernel, as it takes the native graph structure, often containing rich domain knowledge, to associate features in different components that can be distant in Euclidean space.\n\nAbstracting the complex system into graph representation is always the first task for GNN applications. Graph represen- tation consists of nodes and edges, representing component information and their inter-relation. In the circuit domain, IR drop, as a result of routing length, is dominated by the relative Manhattan distance (i.e., Equ. 1) between netnames on the design plane.\n\nMANHATTAN DISTANCE = |X1 - X2| + |Y1 - Y2| (1)\n\nWe formulate a graph representation G(V, E) for each design plane, where node set V for net names and edge set E is the proximity between the net names. Manhattan distance Y is represented as an edge feature that associates different nodes in graph representation. Other impactful features and factors, including total path resistance or peak current of a cell instance, are addressed as node features X, where we conduct an ablation study towards different sets of feature selection as shown in Figure 8. Finally, the IR drop label is formulated as a node-level label Z on a per-net-id basis.\n\nWe introduce a hyper-parameter to threshold the inter- distance between node representations. The motivation of such practice is to enable a sufficient level of connectivity in the graph representation to facilitate GNN performance. The Degree Rank Plot can be used to visualize the graph representation connectivity of selecting different threshold values. This plot shows the degree of each node in descending order. The degree of a node in a graph is the number of edges that are incident to that node. The x-axis represents the rank of each node by degree, and the y-axis illustrates the degree of the node. In the case of Figure 7a, the plot follows a power-law distribution, which indicates that the graph has a few highly connected nodes, and that results in inaccurate predictions for the IR drop. On the other hand, Figure 7c, shows that most of the nodes have a high degree of connectivity, hence this value was decided to be used to develop the GNN model further.\n\nThe next step was to define the architecture of the model, similar to what has been done for the CNN regarding input and output layers. Algorithm 1 below shows a pseudo-code as a common recipe for GNN models addressed.\n\nThe pseudo-code in Algorithm 1 defines a Message-passing Graph Neural Network (MP-GNN) model which we have developed in the PyTorch framework, which consists mainly of three message-passing layers. At the input layer, the model takes in a feature matrix representing each node's feature vectors in the graph. All the convolutional layers are imple- mented using the GCNConv class. The first convolutional layer takes in the feature matrix and the edge index matrix, which represent the edges in the graph. This layer then outputs a new feature matrix, called hidden-channels, which is a hyper- parameter that determines the number of output channels in the layer. Also, ReLu activation function has been used to improve model performance. The final output layer outputs a final feature matrix. The learning rate of the model was set to 0.0001 and the decaying factor to 0.001. Weight decay is a"}, {"title": "D. Architecture of other ML models", "content": "In order to evaluate the performance of our GNN models, we needed to employ other ML models and solutions, specifi- cally based on XGBoost and CNN. XGBoost is a fast and easy algorithm that we have seen can be used to build an ML model for voltage drop prediction [20]. The prediction of the IR value can be formulated as a regression problem, so any appropriate ML algorithm could theoretically be applied. Most of the machine learning algorithms, including XGBoost, typically follow a similar basic structure, which involves splitting the data into training, validation, and test sets. More specifically:\n\n1) The data (features and labels) must be loaded into the model accordingly. Specifically, they are usually split into three categories:\n\na) Training set contains the data the model will use during the training rounds. For this model, 70% of the input data is used for training.\n\nb) Validation set: which contains the data that the model will use during the training to validate its performance. For this model, 10% of the input data is used for validation.\n\nc) Test set contains the data the model will use during the testing. For this model, 20% of the input data is used for testing. The test set is always selected in a way that ensures that it has not been seen by the model during training, so the data are completely new to the model. Different combinations of the above values were tried out before cherry-picking the 70/10/20 (training, validation, testing) as the final split. A combination of 80/10/10 was eval- uated, but the accuracy of the prediction was very close to the previous one, while the training time was increased by 15% until convergence.\n\n2) The data are transformed in an appropriate format for the ML model chosen, for example, in DMATRIX format, which is explained below.\n3) After setting up the required model parameters, the"}, {"title": "IV. RESULTS", "content": "Two different sets have been used, SET A and SET B. Table II, illustrates the performance of XGBoost, CNN, and GCN on features of SET \u0410.\n\nBefore commenting on the above results, it is worth men- tioning some basic characteristics of the test circuit according to commercial tools. The voltage supply of the test circuit VDD is set at 800 (mV) during the simulation. Dynamic"}, {"title": "B and observe the % difference in the evaluation metrics.", "content": "The aim is to validate whether or not, the use of timing characteristics can result in better IR drop prediction results. Prediction results of SET B can be seen in Table III below.\n\nIt is obvious that the use of timing features results in better predictions for all ML models. It is noticeable that MAE was decreased by 75% for XGBoost and by approximately 40% for GCN. Additionally, the mean and max IR drop predicted values were dramatically improved with the use of GCN (90% and 80% respectively), making it a promising solution in the task of IR drop prediction. However, the task of IR drop prediction is very delicate since big variations can lead to chip malfunction. Hence, it is important to take a closer look into the MaxE across the different ML models (Figure 10), because it gives information about the single worst error from the model used.\n\nAccording to Figure 10, MaxE is lower in all models for SET B. However, in XGBoost model and GCN models, even the worst case scenario (MaxE=50 (mV) and 38 (mV) respectively) are below the 10%threshold of the supply voltage, which is equal to 80 (mV) (red line). This observation means that the CNN model might need further improvement (for example, more convolutional layers) to decrease the MaxE value. On the other hand, the prediction of maximum error of IR drop is improved by more than 75% for GCN between SET A and SET B. Due to the fact the MaxE focuses on the worst-case scenario, it is vital also to show the same plot for MAE. MAE in Figure 11, gives a more general idea of the magnitude of the errors as it is less sensitive to outliers and it shows that XGBoost performance is the best in both SET A and SET B."}, {"title": "A. Ablation Results of GNN Architecture", "content": "Given such graph representation, we consider a few different forms of MP-GNN, such as GAT and GIN, for an ablation study against different GNN architectures. It was observed that the training time of the GAT model was higher by more than 6 times compared to GIN or GCN (40-55 seconds for GCN, GIN and 270 seconds for GAT). This happens because, in the case of the last two models, the operations for each node are rel- atively simple as they focus on aggregating information from neighboring nodes. For GAT, learnable pair-wise attention is applied when computing the message embedding MN(v) to weigh the relevance of each adjacent node, according to their node attributes, at the cost of higher training time. Figure 12 depicts the performance of all three models of GNN.\n\nFigure 12 illustrates the decrease of both NRMSE and MaxE as a more advanced model (GIN) is used. On the left y- axis, values of NRMSE are shown in % (blue line), and on the right y-axis, the values of MaxE (orange line) and MAE (grey line) in (mV) are displayed. On the x-axis, the three different models are defined, and as can be seen, there is a significant decrease in NRMSE between GCN and GAT. Additionally, MaxE is decreased from 38.43 (mV) for GCN to 33.1 (mV) for GIN. Finally, the decrease of MAE between the three models is not so significant compared to the other two evaluation metrics, but it is still an acceptable result that can lead to safe IR drop prediction by using an ML model. The slightly improved results of GIN over GCN and GAT were expected because it incorporates all the neighboring node features through a sum aggregation followed by an MLP to update the node representation. Additionally, GIN"}, {"title": "B. Run-time comparison", "content": "The correct and early prediction of high voltage drop can allow the designer to correct the design and evaluate the circuit's performance. However, when a complicated design with hundreds of thousands of cells is considered, the time for voltage drop simulation is very high. A schematic repre- sentation of such workflow can be illustrated in Fig. 13, where one can see the ML models greatly outperform the heuristic- based commercial tool by offering real-time results for IR-drop estimation.\n\nTwo important criteria to decide whether or not to use the ML model are the time and the accuracy of it. Any of the proposed models can result in very accurate IR drop estimation, especially when the correct set of electrical features is used. Regarding the last one, it is important to include in any ML model, the timing features of the ASIC design, because during the switching time more power is needed by the circuit, hence the potential danger of IR drop is higher. In general, the correct combination of power, timing, and physical features can lead to a more accurate and faster prediction of voltage drop."}, {"title": "V. CONCLUSIONS", "content": "In this work, we propose a machine learning-based IR drop prediction approach based on Graph Neural Networks, which significantly outperforms traditional methods as well as other ML models, yielding highly accurate and fast predictions. Our contribution lies not just in the act of feature selection but in innovating a unique and information-rich data representation tailored for this novel domain. We have designed it specifically to harness the strengths of GNNs for the complex problem at hand. Remarkably, GNNs have not been previously applied in this context, opening new avenues for research in IR drop prediction. This study showcases the potential and novelty of GNNs as a valuable tool in addressing the complex challenges of modern integrated circuit design."}]}