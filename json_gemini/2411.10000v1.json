{"title": "DuSEGO: Dual Second-order Equivariant Graph Ordinary Differential Equation", "authors": ["YINGXU WANG", "NAN YIN", "MINGYAN XIAO", "XINHAO YI", "SIWEI LIU", "SHANGSONG LIANG"], "abstract": "Graph Neural Networks (GNNs) with equivariant properties have achieved significant success in modeling\ncomplex dynamic systems and molecular properties. However, their expressiveness ability is limited by: (1)\nExisting methods often overlook the over-smoothing issue caused by traditional GNN models, as well as the\ngradient explosion or vanishing problems in deep GNNs. (2) Most models operate on first-order information,\nneglecting that the real world often consists of second-order systems, which further limits the model's\nrepresentation capabilities. To address these issues, we propose the Dual Second-order Equivariant Graph\nOrdinary Differential Equation (DuSEGO) for equivariant representation. Specifically, DuSEGO apply the dual\nsecond-order equivariant graph ordinary differential equations (Graph ODEs) on graph embeddings and node\ncoordinates, simultaneously. Theoretically, we first prove that DuSEGO maintains the equivariant property.\nFurthermore, we provide theoretical insights showing that DuSEGO effectively alleviates the over-smoothing\nproblem in both feature representation and coordinate update. Additionally, we demonstrate that the proposed\nDuSEGO mitigates the exploding and vanishing gradients problem, facilitating the training of deep multi-layer\nGNNs. Extensive experiments on benchmark datasets validate the superiority of the proposed DuSEGO\ncompared to baselines.", "sections": [{"title": "1 INTRODUCTION", "content": "GNNs [1-4] with equivariant properties [5-7] have become essential tools in modeling complex dy-\nnamic systems [8-11] and molecular properties [12, 13] due to their ability to capture symmetrical\nrelationships within data. Equivariant GNNs are particularly effective in scenarios when maintain-\ning consistency under transformations (e.g., rotations, reflections) is crucial. The consistency of\nequivariant representations allows these models to accurately reflect the inherent properties of the\nsystems they are designed to study, leading to more reliable and insightful predictions, which now\nis an essential topic in machine learning [14-18].\nDespite their successes, existing Equivariant GNNs face notable limitations that constrain their\nperformance and expressiveness. First, the over-smoothing problem [19-22]. The classical Equivariant\nGNNs (such as EGNN [5] and SEGNN [12]) typically utilize the GNNs [1, 23-25] as a backbone,\nas well as cooperating coordinates for equivariant representation. However, GNNs usually suffer\nfrom the over-smoothing problem, leading to the indistinguishable representation of features and\ncoordinates, which is shown in Fig. 1. Second, gradient explosion or vanishing problems [26-28].\nMotivated by the notion that larger models are usually more expressive [29], we tend to explore\nthe expressiveness ability by enlarging the model scale. However, as the network depth increases,\ngradients can either become excessively large or diminish to near zero, which limits the scalability\nand depth of GNNs. Third, real-world simulation [30]. Most Equivariant GNNs [5, 12, 15, 31-33]\nprimarily focus on first-order information. However, many physical systems are governed by\nsecond-order laws [30]. The first-order methods overlook the higher-order interactions that are\ncritical in real-world systems, and limit the representational ability. Even though some works [9, 34]\ntry to introduce second-order information into equivariant learning, they simply focus on the\nsimulation the position in dynamic physical system, ignoring the expressiveness representation for\nmolecular properties prediction.\nTo address these challenges, we take an insight into the second-order method in Equivariant GNNS\nand a framework named Dual Second-order Equivariant Graph Ordinary Differential Equation\n(DuSEGO). The proposed DuSEGO is a universal framework that transfer the first-order Equivariant\nGNNs to second-order, which aims to enhance Equivariant GNNs expressiveness and address the\nissues of over-smoothing and gradient stability. First, we prove that the proposed DuSEGO maintains"}, {"title": "2 RELATED WORK", "content": "Equivariant Graph Neural Networks. GNNs [1, 35-37] with equivariant properties have demon-\nstrated their effectiveness in various tasks, especially for 3D molecular graphs and physical sym-\nmetry of systems [38]. The Tensor field networks (TFNs) [15] method was first proposed for 3D\npoint clouds, which only leverage the positional information of nodes without any features. SE(3)-\ntransformers [31] deploy attention mechanism into TFNs to improve the model performance. EGNN\n[5], a simple but effective form of equivariant graph neural network, was proposed and has been\nthe backbone of the following research. SEGNN [12] incorporates geometric and physical prop-\nerties into the message-passing framework, significantly enhancing the accuracy and predictive\ncapabilities of EGNN in understanding three-dimensional molecular structures. However, these\nmethods focus on first-order velocity information, despite the fact that many physical systems are\ngoverned by second-order motion laws. GMN [9] first introduced the second-order information\ninto equivariant learning, and SEGNO [34] further applied the second-order ODE for solving the\nphysical dynamic system. However, these methods only target the description of positional state\ninformation in the physical world and cannot be applied to molecular feature representation learn-\ning. The proposed DuSEGO utilizes the second-order ODE to evaluate the evolution of equivariant\nrepresentation and position, making it suitable for applications in dynamic physical systems and\nmolecular properties prediction.\nGraph Ordinary Differential Equations. Graph Ordinary Differential Equations (Graph ODEs)\n[39] draw inspiration from Neural Ordinary Differential Equations [40] and provide a sophisticated\nmechanism for modeling dynamic changes within graph structures, showing great potential in\ngraph learning [39, 41-43]. A series of works propose to deploy first-order or second-order ODE\ninto GNNs to enhance their expressiveness ability. LG-ODE [44] learns the dynamics of continuous\nsystems by using models that can handle irregular sampling and incomplete data, improving\nprediction accuracy and robustness. CG-ODE [45] uses coupled ordinary differential equations on\ngraphs to learn and model the dynamics of interacting systems effectively. HOPE [46] leverages\nsecond-order differential equations to capture complex interactions and dynamic behaviors within\ngraph structures. GraphCon [47] proposes a novel framework that integrates the dynamics of\ncoupled oscillators with graph theory and analyzes the stability across complex networked systems.\nHowever, the above works do not consider the equivariance contained in the graph structure, which\nis hard to extend into 3D molecular graphs and the physical symmetry of systems."}, {"title": "3 PRELIMINARY", "content": "Equivariance Let $T_g: X \\rightarrow X$ be a set of transformations on $X$ for the abstract group $g \\in G$. We say\na function $\\phi: X \\rightarrow Y$ is equivariant to $g$ if there exists an equivalent transformation on its output"}, {"title": "4 METHOD", "content": "In this section, we introduce the proposed DuSEGO. In the dynamic systems and molecular proper-\nties prediction, we utilize the second-order ODE to simulate the representation and trajectory."}, {"title": "4.1 Second-order Graph ODE", "content": "To model high-order correlations in long-term temporal trends, [47] first propose the second-order\ngraph ODE, which is represented as:\n$X\" = \\sigma(F_\\theta(X, t)) - \\gamma X - \\alpha X^{\\prime},$\\nwhere $(F_\\theta(X, t))_{ij} = F_\\theta(X_i(t), X_j(t), t)$ is a learnable coupling function with parameters $\\theta$. $\\sigma$ is the\nmapping function, which is implemented with multiplayer perceptions. Due to the unavailability of\nan analytical solution for Eq. 5, GraphCON [47] addresses it through an iterative numerical solver\nemploying a suitable time discretization method. GraphCON utilizes the IMEX (implicit-explicit)\ntime-stepping scheme, an extension of the symplectic Euler method [?] that accommodates systems\nwith an additional damping term.\n$Y^n = Y^{n-1} + \\Delta t [\\sigma(F_\\theta(X^{n-1}, t^{n-1})) - \\gamma X^{n-1} - \\alpha Y^{n-1}],$\\n$X^n = X^{n-1} + \\Delta tY^n, n = 1,\\cdots, N,$\\nwhere $\\Delta t > 0$ is a fixed time-step and $Y^n, X^n$ denote the hidden node features at time $t_n = n\\Delta t$."}, {"title": "4.2 DuSEGO", "content": "To efficiently evaluate the graph representation and the coordinate information, we present the\nframework in a graph dynamical system by following the second-order ODEs:\n$X\" = \\sigma(F_\\theta(X, H, t)) - \\gamma_1 X - \\alpha X^{\\prime}, H\" = \\sigma(F_{\\theta_0}(X, H, t)) - \\gamma_2 H - \\alpha H^{\\prime},$\\nwhere $X$ is the coordinate embeddings and $H$ is the node features. Similar with Eq. 6, we have:\n$Y^n = Y^{n-1} + \\Delta t [\\sigma(F_\\theta(X^{n-1}, H^{n-1}, t^{n-1})) - \\gamma_1 X^{n-1} - \\alpha Y^{n-1}], X^n = X^{n-1} + \\Delta tY^n, n = 1,\\cdots, N,$\\nand\n$U^n = U^{n-1} + \\Delta t [\\sigma(F_\\theta(X^{n-1}, H^{n-1}, t^{n-1})) - \\gamma_2 H^{n-1} - \\alpha U^{n-1}], H^n = H^{n-1} + \\Delta tU^n, n = 1,\\cdots, N,$\nTo keep the Equivariant properties of DuSEGO, the proposed method allows for any Equivariant\nGNNs to be used as $F_\\theta$.\nChoice of function $F_\\theta$. Our framework allows for any learnable equivariant GNNs to be used\nas $F_\\theta$. In this paper, we focus on the two particularly popular choices:\nChoice 1: Equivariant Graph Neural Networks (EGNNs):\n$F_\\theta(X^t, H^t, t^t) = h^{t+1}_i, x^{t+1}_i = \\phi_h(h^t_i, m_i), x^t_i + \\sum_{j\\neq i}(x^t_j - x^t_i) \\phi_x(m_{ij}),$\nwhere $m_{ij} = \\phi_e(h^t_i, h^t_j, ||x_i - x_j||^2, a_{ij})$ and $m_i = \\sum_{j\\neq i} m_{ij}$. $\\phi_e$ and $\\phi_h$ are the edge and node\noperations respectively which are commonly approximated by Multilayer Perceptrons (MLPs).\nNodes $j$ are the neighbors of node $i$.\nChoice 2: Steerable E(3) Equivariant Graph Neural Networks (SEGNN):\n$F_\\theta(X^t, H^t, t^t) = \\phi_h(h^t_i, \\sum_{j\\in N(i)} m_{ij}, a_i), m_{ij} = \\phi_m(h^t_i, h^t_j, ||x_j - x_i||^2, a_{ij}),$\nwhere $||x_j - x_i||^2$ is the squared relative distance between two nodes $v_i$ and $v_j$, $\\phi_m$ and $\\phi_h$ are O(3)\nsteerable MLPs, $a_{ij}$ and $a_i$ are steerable edge and node attributes, $a_i = \\frac{1}{|N(i)|} \\sum_{j\\in N(i)} a_{ij}$. $N(i)$\ndenotes the neighbors of node $v_i$."}, {"title": "5 DUSEGO PROPERTIES ANALYSIS", "content": "To gain some insights into the functioning of DuSEGO, we start by setting the hyperparameter\n$\\gamma = 1$ and assuming that the 1-neighborhood coupling $F_\\theta$ is given by either the GAT or GCN type\ncoupling functions. In this case, the underlying ODEs take the following node-wise form,\nPROPOSITION 1. Suppose the backbone Equivariant GNN $F_\\theta$ of DuSEGO is E(3) equivariant and\ntranslation- invariant, the output $H$ and $X$ is E(3)-equivariant.\nProof: Due to $F_\\theta$ is E(3) equivariant, then we have:\n$F_\\theta(X + g, H + g) = F_\\theta(X, H) + g, F_\\theta(QX, QH) = QF_\\theta(X, H).$\nFor the proposed DuSEGO, we ignore the $\\sigma$ and set $\\gamma_1 = 1, \\gamma_2 = 1$ for simplification,\n$F_\\theta(X + g, H + g) - (X + g) - \\alpha((X + g)') = F_\\theta(X, H) + g - X - g - \\alpha X' = F_\\theta(X, H) - X - \\alpha X' = X\",$\n$F_\\theta(X + g, H + g) - (H + g) - \\alpha((H + g)') = F_\\theta(X, H) + g - H - g - \\alpha H' = F_\\theta(X, H) - H - \\alpha H' = H\".$\nSimilarly, we have:\n$F_\\theta(QX, QH) - QX - \\alpha QX' = QF_\\theta(X, H) - QX - \\alpha QX' = QX\",$\n$F_\\theta(QX, QH) - QH - \\alpha QH' = QF_\\theta(X, H) - QH - \\alpha QH' = QH\".$\nTherefore, the output $H$ and $X$ also follows E(3) equivariant.\nPROPOSITION 2. Let $X_n, Y_n$ be the node features, and $H_n, U_n$ be the node coordinate, generated\nby DuSEGO. We assume that $\\Delta t \\ll 1$ is chosen to be sufficiently small. Then, the gradient of the loss\nfunction $L$ with respect to any learnable weight parameter $w_i$, for some $1 \\leq k \\leq v$ and $1 \\leq l \\leq N$ is\nbounded as:\n$\\frac{\\partial L}{\\partial w^k_i} < \\frac{D \\Delta t}{\\upsilon} \\max_{x} |\\sigma'(x)| \\cdot (1 + l N \\Delta \\Gamma_x) \\cdot (\\max_{1 < i < v} |X_i| + |Y_i|)$\n$+ \\max_h |\\sigma'(h)| \\cdot (1 + l N \\Delta \\Gamma_h) \\cdot (\\max_{1 < i < v} |H_i| + |U_i|)$\n$+ \\frac{D \\Delta t}{\\upsilon} \\max_{x} |\\sigma'(x)| \\cdot (1 + l N \\Delta \\Gamma_x) \\cdot (\\max_{1 < i < v} |X_i| + |\\sigma(x)| \\sqrt{N \\Delta t})$\n$+ \\max_h |\\sigma'(h)| \\cdot (1 + l N \\Delta \\Gamma_h) \\cdot (\\max_{1 < i < v} |H_i| + |\\sigma(h)| \\sqrt{N \\Delta t}),$"}, {"title": "6 EXPERIMENTS", "content": "To verify the effectiveness of DuSEGO in enhancing the performance of Equivariant GNN, we\nfollow the experimental settings in [5] and evaluate it on three different tasks, i.e., modeling a\ndynamical system, learning unsupervised representations of graphs in a continuous latent space,\nand predicting molecular chemical properties."}]}