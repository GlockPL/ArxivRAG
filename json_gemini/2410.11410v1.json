{"title": "PMMT: Preference Alignment in Multilingual Machine Translation via LLM Distillation", "authors": ["Shuqiao Sun", "Yutong Yao", "Peiwen Wu", "Feijun Jiang", "Kaifu Zhang"], "abstract": "Translation is important for cross-language communication, and many efforts have been made to improve its accuracy. However, less investment is conducted in aligning translations with human preferences, such as translation tones or styles. In this paper, a new method is proposed to effectively generate large-scale multilingual parallel corpora with specific translation preferences using Large Language Models (LLMs). Meanwhile, an automatic pipeline is designed to distill human preferences into smaller Machine Translation (MT) models for efficiently and economically supporting large-scale calls in online services. Experiments indicate that the proposed method takes the lead in translation tasks with aligned human preferences by a large margin. Meanwhile, on popular public benchmarks like WMT and Flores, on which our models were not trained, the proposed method also shows a competitive performance compared to SOTA works.", "sections": [{"title": "Introduction", "content": "Recently, translation methods are improved with technological advancements. However, most of them pursue the accuracy criteria and neglect the vivid and nuanced expressions of languages (Costa-juss\u00e0 et al. 2022; Fan et al. 2021).\nBy analyzing the dialogues from our online services, we noticed that \u201chuman preferences\u201d in expression vary drastically among users. For example, some of them want to translate \"Can you\" to \"\u00bfPodr\u00eda\" instead of \"\u00bfPuede\" to provide a more polite tone. This is hard to achieve using traditional quality estimation (QE) filters or the intervention techniques because the Spanish word varies in different condition and the translation preference is conflict to the semantic mapping.\nAlthough many researches are conducted to address the problem of preference alignment (He et al. 2024; Xu et al. 2024b; Wu et al. 2018), challenges remain in aligning these specific customized preferences to translation besides aligning to common rules like the 3H criteria (helpfulness, harmless, and hallucination).\nFirstly, despite the abundance of datasets for general translation (He et al. 2023; Michel and Neubig 2018), there is a paucity of customized corpora (Xu et al. 2023, 2024a). Possible reasons are: 1) preferences vary significantly across applications and are hard to gather, and 2) constructing large-scale parallel corpora is costly for diverse preferences.\nSecondly, due to cost, latency, and stability concerns, using small models for online translation services is more practical. However, the translation from a pre-trained small model of a given source text can hardly be changed by prompts. As a result, when a new preference is required, update the models can be costly due to training corpus re-generating and manual model re-training.\nTo address these challenges, we propose an automatic data generation method that can be easily generalized to large-scale parallel corpus production with specific preferences. Firstly, a small seed dataset is built through multiple training resources, on which a translation LLM is trained to generate more candidate translations from the new source texts. Meanwhile, a reward model (RM) is trained on another small dataset which is aligned to human preferences. Finally, the candidate translations are filtered by the Best-of-N (BoN) strategy (Nakano et al. 2021). With the growth of source text scale, ceaseless high-quality parallel data will be produced through the pipeline automatically.\nTo relieve the cost of alignment to fickle preferences, we propose an approach to automatically distill knowledge from LLMs to smaller models. In this pipeline, only few examples that represent new preferences are required to update the seed preference data and all small translation models will be updated automatically.\nEvaluation are conducted on customized dataset and public benchmarks including WMT23 (Kocmi et al. 2023) and flores (Costa-juss\u00e0 et al. 2022). Experimental results show that the proposed method can generate first-class translations in general tasks while takes the lead by a large-margin in tasks with specific human preferences. Figure. 1 illustrates an overview of the proposed method and details are introduced in the following sections.\nIn General, the main contributions of this paper are:\n\u2022 Proposing a method to generate high-quality, large-scale parallel translation corpora that are aligned to customized human preferences.\n\u2022 Developing an approach to distill knowledge from LLMs to smaller models through automatic model update."}, {"title": "Related Work", "content": "Multilingual Machine Translation (MMT) has been extensively researched and numerous methods are proposed. Among them, M2M (Fan et al. 2021) is one of the most representative works, which achieves translation between two arbitrary languages without using English as a bridge. Later, NLLB (Costa-juss\u00e0 et al. 2022) was proposed to expand the translation capabilities on low-resource languages.\nRecently, LLMs are widely leveraged by MMT methods. Some of them target in achieving MMT through prompt engineering (He et al. 2023) while others prefer to improve the translation accuracy through model training. For example, the English-based model LLaMA-13B (Touvron et al. 2023a) is augmented on Chinese corpora for a bilingual-central translation (Yang et al. 2023b). However, multilingual parallel corpus can be scarce sometimes, so a two-stage training method (Xu et al. 2023) is proposed, where the model is firstly trained on large-scale monolingual corpora to learn the expression in single languages, and then use small-scale human-annotated parallel corpora to learn the mapping between languages. Although these models have demonstrated powerful capabilities in MMT tasks, aligning them with human preferences is expensive due to dataset generation and training cost.\nThe quality and quantity of the parallel corpus determine the accuracy upper bound of a translation model (Xu et al. 2024c) and many efforts have been made to generate large-scale high-quality datasets.\nFor example, He et. al attempt to utilize the multilingual ability of LLMs with multi-step prompt engineering approaches for data generation (He et al. 2023). MTNT (Michel and Neubig 2018) produces manual-proofreading high-quality parallel corpora through pre-filtering and noise identification. SentAlign (Steingr\u00edmsson, Loftsson, and Way 2023) utilizes a variant of the Dijkstra algorithm and the LaBSE model semantically align the source and target texts, thereby improving the accuracy and scalability when processing large-scale data. Works are also done to extract high-quality parallel corpora from noisy translation datasets following a few-shot prompt with Chain-of-Thought (CoT) reasoning (Bolding et al. 2023). Many QE method are also proposed for data cleaning (Alves et al. 2024; Xu et al. 2023; Blain et al. 2023).\nAlthough these methods can provide high-performant results, some challenges remain. First of all, most of them only target on general linguistic rules and is clumsy for aligning with capricious customized human preferences. Meanwhile, considering the large quantity of the raw data, directly use large QE models for data cleaning is not practical in most cases.\nReinforcement Learning (RL) has been proved to be effective in numerous preference alignment applications, including machine translation (Wu et al. 2018). Methods are proposed to elevate the BLEU score (Post 2018) of translation models with strategies such as leveraging a distributed policy gradient algorithm (Donato et al. 2022).\nAdditionally, several works have been conducted based on Reinforcement Learning with Human Feedback (RLHF) (Ouyang et al. 2022) to achieve preference alignment according to higher human standards. An MBR decoder with DPO (Rafailov et al. 2023) is proposed (Yang et al. 2023a) to refine translations based on human feedback, and a CPO method for reference-free translation fine-tuning (Xu et al. 2024a). Additionally, more subsequent methods (He et al. 2024; Xu et al. 2024b) have been proposed to align higher-level human preferences.\nHowever, preferences defined in the above-mentioned methods still mainly refer to the translation accuracy under general applications, while neglecting higher-order human preferences such as faithfulness, expressiveness, and elegance"}, {"title": "Approach", "content": "The proposed method mainly consists of two parts: the cold-start stage and the regular update stage.\nDuring the first stage, a small-scale seed dataset is built by leveraging multiple translation resources. The translations are cleaned before training the LLM (named as PMMT-P, where \"P\" stands for \"Producer\u201d), which is used to generate the candidate translations for following steps. At the same time, an RM (named as PMMT-J, where \"J\" stands for \"Judge\") is trained on a small customized preference dataset, and is thereafter used to select translations that match specific preferences.\nDuring the second stage, numerous parallel data are generated by PMMT-P and PMMT-J with the update of the source texts, which contain both the translation norms learned by PMMT-P and the preferences learned by PMMT-J. Then, smaller models (named as PMMT-B) are automatically trained on the generated corpora and thus distill the knowledge from LLMs into the translation.\nThe seed dataset is used to train the PMMT-P and the source texts are from our online e-commerce conversations (see Figure. 1 for reference). Multiple translation resources are utilized to maintain the diversity of outputs from PMMT-P: translation APIs (Google translation and Damo translation), sophisticated AI models (Brown et al. 2020; OpenAI 2023), and human annotators.\nTo elevate the accuracy of the seed dataset, a series of data cleaning strategies are designed and applied to multiple stages of PMMT. The main strategies are introduced below and they are applied in a descending order with respect to computational complexity to reduce the total cost.\nEmpirically, the length of the translation usually approximately matches that of the source text. Therefore, a length difference $f_e$ is calculated to filter the translations which are much longer or shorter than the source text:\n$f_e(S,T, \\alpha) = \\max(\\frac{l(S)}{l(T)}, \\frac{l(T)}{l(S)}) < \\alpha$ (1)\nwhere $l()$ calculates the length of the source text S and the target text T, and $\\alpha$ is the threshold determined according to the language cluster: $\\alpha = 2$ for texts within a same language cluster (e.g., English and Spanish), and $\\alpha = 3$ for texts from different clusters (e.g., English and Chinese).\nTranslation resources might produce incorrect translations in the form of wrong languages. We use two python language detection tools to check the translation languages: lingua and langid. A translation is considered correct when it passes any one of the tools.\nContent hallucinations from translation resources such as LLMs can be filtered by certain keywords such as \"-\u00bf\", \"to target language\", \"translat\", \"my sentence is\", etc. When a \"danger word\" is detected in the translation, the whole translation (along with source text) will be removed from the corpus. For non-English target languages, the outputs will first be translated into English using a relatively reliable method for word checking. Additionally, to avoid destroying icons and emojis during the process, emojis will also be verified using an emoji list.\nThe presence of inaccurate numbers in the training corpus could significantly affect the performance of the translation models trained on this dataset, which can be fatal to applications like e-commerce, where order numbers are pivotal to providing satisfactory services. As a result, we specifically checked for number consistency between S and T using regex match rules. If the numbers equal to each other, then the translation T is regarded to pass the number check.\nSemantic consistency between S and T is checked by a similarity score $f_{sim}$ defined by Equation. 2, where T is the translation process:\n$f_{sim}(S,T(T)) = cos(M(S), M(T(T)))$ (2)\nwhere M is the Minilmv2 model (Wang et al. 2021) used for embedding computation, cos is the cosine similarity function, and T represents the translation process.\nThe final seed dataset contains over 28 languages, and Figure. 2 (a) illustrates an overview of the language distribution.\nThe PMMT-P model is an LLM trained on the seed dataset. Its mission is to produce raw candidate multilingual translations in multiple tones and styles. Those translations that matches the customized human preference will be chosen by the judge model PMMT-J in the following pipeline. The PMMT-P model in this paper is fine-tunned from the pre-trained LLaMA2-13B (Touvron et al. 2023b) model using the loss function $L(\\theta)$:\n$L(\\theta) = -\\sum_{t=1}^T \\log p(y_t|Y_{<t}, x; \\theta)$ (3)\nwhere $\\theta$ represents the model parameters, and $p(y_t|Y_{<t}, x; \\theta)$ denotes the probability of the correct token $y_t$, given the preceding tokens $y_{<t}$ and the input x.\nThe PMMT-J model is trained on a small-scale seed preference dataset. In this paper, we use human preferences for colloquial conversations with politeness for online e-commerce conversations as an example, but the proposed method can be easily expanded to accommodate other customized preferences.\nThe seed preference dataset is built from two sources: the seed dataset and human translators. For data from the seed dataset, GPT-4 (OpenAI 2023) is used to label the chosen and rejected answers following these rules:\nThe translation should be correct in grammar and orthography. In instances where the source text contains minor typos, the translation is still expected to convey the correct meanings.\nThe translation should be in a polite tone, taking into account different cultures. For example, a seller might address the customer as \"\u4eb2\u201dor\"\u4eb2\u4eb2\u201d in Chinese, which should be translated to \"dear\" or \"dear customer\" in English, rather than the literal \"kiss\" or \"kiss kiss\u201d (which represents the apparent meaning of the source text).\nFurthermore, some countries may adhere to stricter guidelines regarding formalities and expect the seller to address the customer in a more respectful manner rather than using an overly friendly tone. For example, a Japanese customer would prefer to be addressed as \"\u304a\u5ba2\u69d8\u201d instead of \"\u9867\u5ba2\u201d since it is more polite and courteous.\nThere can be some customized translation requirements. For example, \"may I\" and \"can I\" in English should be translated into \"\u00bfPuedo\" or \"\u00bfPuedes\" in Spanish generally. However, as is mentioned above, some users would prefer to rewrite them into \"\u00bfPodr\u00eda\" to convey a more polite tone.\nAll these preferences are considered while building the seed preference dataset. Additional preferences can also be added by introducing a small amount of new parallel data which can reflect the new preferences. The seed preference dataset and the PMMT-J model are updated only when the preference varies to maintain a more coherent filtering rule and save cost.\nThe PMMT-B model is trained on cleaned large-scale parallel multilingual corpora with human preferences, which are generated by PMMT-P model and filtered by the PMMT-J model. The cleaned multilingual corpora contain about 1 billion distinct entries, and the main constitution is shown in Figure. 2 (b). The model structure of PMMT-B is the same as that described in (Fan et al. 2021), which is a standard encoder-decoder model."}, {"title": "Experiments", "content": "All models are trained on A100 GPUs using PyTorch (Paszke et al. 2019). The PMMT-B models are trained with the conventional training approach for standard transformer models (Wolf et al. 2020) while PMMT-P and PMMT-J are trained within an LLM training framework (Zheng et al. 2024).\nThe PMMT-P model is trained on the seed dataset of all language pairs Simultaneously, where one language pair refers to translations from a specific language (X) to English and vice versa. The model is only updated when new languages are added or reaches a preset update time (which is 1 month in our practice). The model is trained for 5 epochs on the seed dataset at a start learning rate of 10-5.\nThe PMMT-J model is trained on the seed preference dataset, where each source text is paired with multiple translation outputs that reflect different preferences. During the model training, the value head of PMMT-J scores each output generated from the base LLM. The objective of the training process is to maximize the gaps between chosen and rejected translations (Ouyang et al. 2022):\n$L_{JM}(\\theta) = E [\\log (\\sigma (r_{\\theta}(S,T^+) -r_{\\theta}(S,T^-)))]$ (4)\nwhere $\\sigma$ stands for the sigmoid function, $r_{\\theta}(S, T)$ is the scalar output of PMMT-J with parameters $\\theta$. $T^+$ denotes the chosen translations, and $T^-$ stands for the rejected translations.\nTo ensure translation stability and the update flexibility of the online serving, all PMMT-B models are trained on individual language pairs, meaning that each PMMT-B model can only translate between English and another language. Language pairs that are not covered by direct translation PMMT-B models will be translated using English as the bridge.\nThe models are trained for 15-20 epochs on the generated corpus, and the number of epochs is determined by the dataset scale empirically. The loss function $L(\\theta)$ used to train the PMMT-B models is defined as follows (Fan et al. 2021):\n$L(\\theta) = \\frac{1}{NT}\\sum_{i=1}^N \\sum_{t=1}^{T_i} \\log P(y_{i,t}|Y_{i,<t}, x_i; \\theta)$ (5)\nwhere $\\theta$ denotes the model parameters, N is the number of training samples, $T_i$ indicates the length of the i-th target sequence, $y_{i,t}$ stands for the t-th token in the i-th target sequence, $Y_{i,<t}$ refers to the sequence of tokens in the i-th target sequence preceding the t-th token, $x_i$ is the i-th source sequence, and $P(y_{i,t}|Y_{i,<t}, x_i; \\theta)$ denotes the predicted probability of the token $y_{i,t}$ given the preceding tokens $Y_{i,<t}$ and the source sequence $x_i$, parameterized by $\\theta$.\nThe PMMT-B models are regularly updated at a preset frequency using new parallel corpora generated by PMMT-P and PMMT-J from added source texts. This practice ensures that the deployed PMMT-B models are aware of new words and preferences.\nAs is mentioned above, our method is proposed to provide accurate translations with human preferences in expressions like tones, styles, etc. Therefore, evaluation of the proposed method is conducted in two fields:\n\u2022 Public benchmarks: To evaluate the models' basic translation ability like accuracy and fluency.\n\u2022 Customized test set: To evaluate the models' alignment quality under specific human preferences.\nThe customized test set contains two parts: 1) real-world sentences derived from online dialogues (2k for each translation direction) that are translated by GPT4 and filtered using a same cleaning strategy described above; and 2) sentences reflecting the same human preferences as the seed preference dataset in English and Spanish (1k for each language) that are translated by human annotators. The source texts in the customized test set are also from our online services but the source texts are de-duplicate from the training set.\n\u2022 WMT23 (Kocmi et al. 2023): WMT is an annually updated benchmark that contains text from various resources (webpages, documents, etc.) and are translated by multiple resources. The latest WMT23 benchmark contains 7 languages: Czech, German, Hebrew, Japanese, Russian, Ukrainian, and Chinese.\n\u2022 Flores200 (Costa-juss\u00e0 et al. 2022): The Flores dataset contains parallel data with over 200 languages and the source text are from diverse resources including books, literatures, news, social media, etc.\nWe use COMET (Rei et al. 2022a) and BLEURT (Sellam, Das, and Parikh 2020) for evaluation and the COMET score is computed by wmt22-comet-da (Rei et al. 2022b) with reference."}, {"title": "Ablation Study", "content": "Ablation studies are conducted to test the effectiveness of our training strategy by comparing SFT with DPO (Rafailov et al. 2023) and PPO (Schulman et al. 2017). We chose Spanish-English translation task as an example and experiments are performed on the Flores dataset and our customized test set since WMT23 does not have Spanish. To evaluate the influence of general translation and translation with human preferences, we split the customized dataset into two parts:\n\u2022 common samples (trainset, 256k) and common_data (test set, 30k): samples randomly selected from the seed dataset;\n\u2022 preference samples (trainset, 20k) and pref_data (test set, 2k): samples randomly selected from the seed preference dataset.\nTraining details of each model is listed below:\n\u2022 SFT Model 1 (PMMT-P strategy): Simultaneously trained on common samples and preference samples for 5 epochs.\n\u2022 SFT Model 2: Successively trained on common samples for 5 epochs and then fine-tuned on preference samples for 1 epoch.\n\u2022 SFT Model 3: Trained on common samples for 5 epochs.\n\u2022 DPO Model: Trained on preference samples for 1 epoch after SFT using DPO strategy.\n\u2022 PPO Model: Trained on preference samples for 1 epoch after SFT using PPO strategy.\nIn general, although DPO performs best in all test sets, the performance gap between strategies is very small. Considering that the training cost of SFT is much less than DPO\nAn ablation study was conducted to determine the optimal data scale for PMMT-J training. Specifically, we trained the PMMT-J model with 7B, 13B, and 70B parameters on datasets with scales ranging from 1k to 100k entries in English and Spanish. As shown in Figure. 4, different model scales follow a similar trend while there is an obvious plateau between data scales of 5k and 20k, where accuracy improvements are minimal despite the increasing data scale. Consequently, a data scale of 10k was selected to train the PMMT-J model with 13B parameters to balance training cost and performance."}, {"title": "Conclusion", "content": "In this paper, a novel method is proposed to align customized human preferences in multilingual translation, and knowledge from LLMs is distilled to smaller models automatic for more effective online serving. First, a small-scale seed dataset is produced to train the translation LLM for generating candidate translations with various styles. Then, an RM is trained on another seed preference dataset and is used to select translations that match human preferences from the candidates generated by the LLM. Finally, large-scale multilingual corpora are generated and used to train the smaller translation analyzing models, which can be updated effectively with the proposed pipeline. Experimental results prove that our method can not only align with human preferences"}, {"title": "Reproducibility Checklist", "content": "Unless specified otherwise, please answer \u201cyes\u201d to each ques-tion if the relevant information is described either in the paper itself or in a technical appendix with an explicit reference from the main paper. If you wish to explain an answer further, please do so in a section titled \"Reproducibility Checklist\" at the end of the technical appendix.\nThis paper:\nIncludes a conceptual outline and/or pseudocode descrip-tion of AI methods introduced (yes)\nClearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes)\nProvides well marked pedagogical references for less-familiare readers to gain background necessary to replicate the paper (yes)\nDoes this paper make theoretical contributions? (yes)\nIf yes, please complete the list below.\nAll assumptions and restrictions are stated clearly and formally. (yes)\nAll novel claims are stated formally (e.g., in theorem state-ments). (yes)\nProofs of all novel claims are included. (yes)\nProof sketches or intuitions are given for complex and/or novel results. (yes)\nAppropriate citations to theoretical tools used are given. (yes)\nAll theoretical claims are demonstrated empirically to hold. (yes)\nAll experimental code used to eliminate or disprove claims is included. (yes)\nDoes this paper rely on one or more datasets? (yes)\nIf yes, please complete the list below.\nA motivation is given for why the experiments are con-ducted on the selected datasets (yes)\nAll novel datasets introduced in this paper are included in a data appendix. (partial: We public the customized test set)\nAll novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (yes)\nAll datasets drawn from the existing literature (potentially including authors' own previously published work) are ac-companied by appropriate citations. (yes)\nAll datasets drawn from the existing literature (potentially including authors' own previously published work) are pub-licly available. (yes)\nAll datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing. (yes)\nDoes this paper include computational experiments? (yes)\nIf yes, please complete the list below.\nAny code required for pre-processing data is included in the appendix. (no: This is trade secret. But we have make it clear enough for reproducing the method including methods and tools that we used.).\nAll source code required for conducting and analyzing the experiments is included in a code appendix. (no. The inference code has been described in the paper, which is publicly available. The metrics used are stadard critera and computation method is also publicly available. Checkpoints of our models are trade secret.)\nAll source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (partial)"}]}