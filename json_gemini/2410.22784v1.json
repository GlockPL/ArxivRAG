{"title": "Contrastive Learning and Adversarial Disentanglement for Privacy-Preserving Task-Oriented Semantic Communications", "authors": ["Omar Erak", "Omar Alhussein", "Wen Tong"], "abstract": "Task-oriented semantic communication systems have emerged as a promising approach to achieving efficient and intelligent data transmission, where only information relevant to a specific task is communicated. However, existing methods struggle to fully disentangle task-relevant and task-irrelevant information, leading to privacy concerns and subpar performance. To address this, we propose an information-bottleneck method, named CLAD (contrastive learning and adversarial disentanglement). CLAD leverages contrastive learning to effectively capture task-relevant features while employing adversarial disentanglement to discard task-irrelevant information. Additionally, due to the lack of reliable and reproducible methods to gain insight into the informativeness and minimality of the encoded feature vectors, we introduce a new technique to compute the information retention index (IRI), a comparative metric used as a proxy for the mutual information between the encoded features and the input, reflecting the minimality of the encoded features. The IRI quantifies the minimality and informativeness of the encoded feature vectors across different task-oriented communication techniques. Our extensive experiments demonstrate that CLAD outperforms state-of-the-art baselines in terms of task performance, privacy preservation, and IRI. CLAD achieves a predictive performance improvement of around 2.5-3%, along with a 77-90% reduction in IRI and a 57-76% decrease in adversarial accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "In conventional communication systems, the primary objective has been to ensure reliable transmission of data, focusing on delivering bit sequences across noisy channels without considering the meaning, context, or purpose of the data being transmitted. Shannon's mathematical theory of communication focuses on optimizing metrics such as data rate, error rate, and bandwidth efficiency, whilst being agnostic to the ultimate purpose and relevance of the transmitted information [1]. This approach has been widely successful and effective for general communication needs thus far. However, next-generation communication systems, beginning with 6G, require more intelligent and task-aware communication methods to enable emerging applications [2], [3], such as computer vision [4], autonomous driving [5], extended reality (XR) [6], and generative artificial intelligence (AI) [7].\nAs we move towards these advanced systems, there is a growing recognition that communication should not merely be about transmitting raw data, but about understanding and leveraging the underlying meaning and purpose of the data. This shift towards task-oriented semantic communications represents a fundamental change in the design of communication networks [8], [9]. Instead of focusing solely on the accurate and efficient transmission of bits, these new approaches aim to ensure that the information most relevant to the specific task or decision-making process is prioritized and delivered with minimal delay. For example, in a smart city environment [10], rather than transmitting all sensor data from traffic cameras, task-oriented communications focuses on sending only the information necessary to identify and respond to potential hazards or optimize traffic flow in real time.\nWith the growing success and popularity of deep learning (DL) in various wireless communication applications [11], many emerging task-oriented communication systems have adopted DL approaches to encode task-relevant information to improve task performance and efficiency of the communication system [12]\u2013[14]. Nevertheless, most proposed schemes do not focus quantifying or benchmarking the amount of information that the encoded features retain about the input, primarily due to the computational difficulty of estimating mutual information and the lack of a unified methodology that provides fair and reproducible results. This omission is critical, as understanding the mutual information is key to developing systems with improved bandwidth and latency. Moreover, there has been little emphasis on explicitly discarding task-irrelevant information, often leading to a trade-off between task performance and information compression.\nFurthermore, most current approaches rely on maximizing mutual information between the encoded features and the target using variational approximations based on the cross-entropy loss [12], [14], [15]. However, deriving a maximization for the mutual information between the encoded feature vector and the targets based on contrastive learning [16] remains unexplored for task-oriented communication systems.\nTo address the aforementioned challenges, we develop a task-oriented communication system based on contrastive learning [16] and disentangled representation learning [17] and we devise a new metric to compute comparative values for a proxy of mutual information between the encoded features and the inputs across different systems, rather than computing the exact mutual information. More specifically, our major contributions are as follows:\n\u2022 We derive a lower bound for the mutual information between the encoded features and the target using con-"}, {"title": "II. RELATED WORK", "content": "DL-based communication systems have shown success in recent years. DeepJSCC (Deep Joint Source-Channel Coding) is a recent advancement in the field of wireless communications that utilizes deep learning to jointly optimize source and channel coding, which are traditionally treated as separate tasks [13]. Unlike conventional methods that rely on separate compression and error-correction codes, DeepJSCC uses neural networks to directly map source data to channel symbols, allowing for an end-to-end optimization of the communication system. DeepJSCC can be trained on a classification task by minimizing the cross-entropy loss, ensuring task-specific performance; however, it does not inherently ensure that only task-relevant features are transmitted.\nBuilding on that, Shao et al. [12] proposed a task-oriented communication system for edge inference by leveraging the information bottleneck (IB) theory [18], and variational approximations [15] to balance a trade-off between the minimality of the transmitted feature vector and the task performance. Their results demonstrate improved latency and classification accuracy. Another work focuses on improving the aforementioned IB framework for task-oriented communication systems by introducing an information bottleneck framework that ensures robustness to varying channel conditions [14].\nWang et al. formulate a privacy-utility trade-off to develop an IB-based privacy-preserving task-oriented communication system against model inversion attacks [19]. This is achieved by striking a balance between the traditional IB-based loss functions similar to the work discussed above and a mean squared error (MSE) based term that aims at maximizing reconstruction distortion. Their results demonstrate improved privacy with minimal impact to task performance.\nDespite the successful results achieved by the aforementioned task-oriented communication systems, there remain several key areas that warrant further investigation and improvement. One significant limitation in the existing literature is the lack of results and comprehensive benchmarking on the mutual information between the encoded features and the input, given a particular task-oriented communication system. In task-oriented communication systems, mutual information plays a critical role in determining the efficiency of the system, particularly regarding the preservation of information during transmission.\nFurthermore, while task-specific performance, specifically classification accuracy, has been improved by these advancements, there is still room for enhancing performance further. Another key challenge in these systems is balancing multiple trade-offs, such as task performance, informativeness, minimality and privacy. These trade-offs are typically managed through careful tuning of hyperparameters. Reducing the dependency on hyperparameters would make these systems more robust and easier to deploy in real-world scenarios.\nContrastive learning has gained significant attention in recent years, particularly for its success in unsupervised learning [16], [20]\u2013[22]. By leveraging the concept of instance discrimination, contrastive learning methods aim to pull together positive pairs for example, augmentations of the same image, while pushing apart negative pairs for example, different images, thus learning meaningful representations of data without the need for labels. More recently, contrastive learning has also shown exceptional results in supervised learning scenarios. By incorporating label information into the contrastive loss function, methods such as supervised contrastive learning (SupCon) [23] have improved upon traditional cross-entropy loss. Contrastive learning has not been investigated for task-oriented communication systems as an alternative to cross-entropy based mutual information approximation techniques.\nDisentangled representation learning has been widely studied in recent years. Prominent examples include the B-VAE [24], that extends the variational autoencoder (VAE) by introducing a regularization term that encourages disentanglement, and FactorVAE [25], that further improves this disentanglement by encouraging the representations' distribution to be factorial and therefore independent across the dimensions. Other works [26]-[28], explored disentangling through adversarial-based objectives [29]."}, {"title": "III. SYSTEM MODEL AND NOTATIONS", "content": "Throughout this paper, we use the following notational conventions. Random variables are denoted by uppercase letters, such as X, Y, and Z. Their corresponding realizations (i.e., specific instances) are denoted by lowercase bold letters, such as x, y, and z. The space from which these random variables are drawn is represented by calligraphic letters, such as X, \u0423 and Z. We denote entropy of a random variable X by H(X). The mutual information between two random variables X and Y, is denoted by I(X; Y). We use I(Z; X|Y) to denote the conditional mutual information between Z and X given Y. We use the expectation notation E[.], which refers to the average value of a random variable over a distribution.\nWe consider a communication system with a transmitter that includes a feature extractor and a joint source-channel coding (JSCC) encoder. We collectively refer to these components as the task-relevant encoder (TRE). The TRE encodes an input image x \u2208 X into a lower-dimensional feature vector z\u2208 Z. Encoded vector z is then transmitted to a receiver over a noisy wireless channel. The primary objective is to transmit a minimal yet informative representation (z), by discarding task-irrelevant information while ensuring that z contains only the essential information for accurate downstream classification at the receiver.\nThe overall transmission and decoding process can be described by the following Markov chain:\nY \u2192 X \u2192 Z \u2192 2 \u2192 \u0176,\nwhere X is the random variable representing the input images, Z is the random variable representing the encoded feature vectors, 2 is the noisy signals received by the receiver, Y is the random variable representing the labels of the input images, and \u0176 is the random variable representing the predicted labels at the receiver.\nAt the transmitter, the TRE encodes input image x \u2208 RN, where N represents the number of pixels in the image (Height \u00d7 Width x Color Channels). The encoder maps this input into a lower-dimensional feature vector z \u2208 Rd, where d is the dimension of the encoded feature vector. The encoding function, denoted by fo : RN \u2192 Rd, is parameterized by 0, and the encoding process can be expressed as\nz = fo(x)\nFeature vector z is then prepared for transmission over the wireless channel by being mapped to channel input symbols. The role of the TRE is twofold: encoding the input data into feature representations and preparing them as channel symbols suitable for transmission. The encoded feature vector z \u2208 Rd is transmitted over a wireless channel, which is modeled as an additive white Gaussian noise (AWGN) channel. The channel introduces noise and distortion, and the received signal 2 \u2208 Rd at the receiver is expressed as\n2 = z + n,\nwhere n ~ N(0,021) is the additive Gaussian noise with variance 02. The noise variance o\u00b2 is related to the channel's signal-to-noise ratio (SNR), which quantifies the channel quality. The SNR in decibels (dB) is given by\nSNRdB = 10log10 (E[||z||2]/\u03c32)\nAt the receiver, a classifier, denoted by q\u00a2 : Rd \u2192 RM, where M is the number of labels, is parameterized by 4. The classifier maps the received noisy signal 2 to predicted label \u0177\u2208 RM as\n\u0177 = q (2).\nThe classifier is trained to minimize the loss between the predicted label (y) and the true label (y). Since true posterior distribution p(yz) is intractable, q serves as an approximation based on the received noisy signal."}, {"title": "IV. PROBLEM DESCRIPTION", "content": "In this section, we identify the primary challenges that arise when transmitting features over a communication channel and utilizing them for a downstream classification task. Our goal is to ensure that the transmitted features contain only the minimum necessary information required for the downstream task to preserve privacy and maximize efficiency. Furthermore, we argue that it is necessary to have a fair, reproducible, and unified method to obtain comparative values that act as a proxy for mutual information between the encoded features and the input data to allow effective benchmarking of different task-oriented communication systems."}, {"title": "A. Minimum Necessary Information", "content": "Following [30], the Minimum Necessary Information (MNI) criterion for an ideal representation 2 must satisfy the following key principles:\n\u2022 Informativeness: Representation 2 should contain all the necessary information to predict Y, requiring us to maximize the mutual information I(2; Y).\n\u2022 Necessity: Representation 2 should contain the necessary amount of information in order to perform well in the downstream task, any less information would mean that Z has discarded task-relevant information. Necessity can be defined as\nI(X; Y) \u2264 I(Y; 2)\n\u2022 Minimality: Among all possible representations Z that satisfy the task of predicting Y, we seek the one that encodes the least amount of information about X beyond what is strictly necessary for the task. This can be formulated as\nmin I(2; X) subject to I(X; Y) = I(2; Y)\nAny more information than that would result in Z having redundant information about X that is unnecessary for predicting Y.\nGiven the above we conclude that in an optimal case we must have\nI(2; X) = I(2; Y) = I(X; Y),\nthis implies that 2 contains exactly the amount of information necessary to perform the task of predicting Y from X, no more and no less. At the MNI point, 2 captures all the relevant information needed for the task, while discarding any irrelevant or redundant information about X."}, {"title": "B. Privacy Concerns and Task-Irrelevant Information", "content": "The second challenge is privacy concerns due to the leakage of task-irrelevant information from X into 2. If 2 retains information about X that is not relevant to predicting Y, this may inadvertently expose sensitive or private data, and could make the system more vulnerable to different attacks such as attribute inference attacks and model inversion attacks [31], [19]. Therefore, disentangling task-irrelevant information ensures that 2 does not encode unnecessary or sensitive information that is not directly relevant to the downstream task, which minimizes privacy risks."}, {"title": "C. Quantifying Information Retention", "content": "In task-oriented communication systems, it is critical to have an understanding of the mutual information I(2; X) as it provides insights into how much of the original input information X is encoded in 2 and can directly affect latency, bandwidth and privacy. However, a significant challenge arises because the estimation of I(2; X) varies drastically depending on the estimation method used. Indeed, multiple works have reported widely different I(2; X) for the same task-oriented approach [15], [27]. This makes it difficult to arrive at reliable conclusions about the amount of information being retained in 2.\nGiven these discrepancies, we argue that it is crucial to devise a method that yields consistent, reliable and fair comparative estimates of information retention, even if the exact value of I(2; X) is intractable. Instead of absolute precision, a method that provides relative and comparable estimates across different systems would greatly enhance the ability to evaluate and optimize different task-oriented communication systems."}, {"title": "D. Limitations of Variational Information Bottleneck (VIB)", "content": "The variational information bottleneck (VIB) has been the de facto method for many task-oriented communication systems. VIB tries to minimize the following objective:\nLvIB = \u1e9eI(2; X) \u2013 I(2; Y),\nwhere I(2; X) measures the amount of information retained from the input X, and I(2;Y) represents the informativeness of Z for predicting Y. Hyperparameter \u03b2 controls the trade-off between preserving task-relevant information and discarding irrelevant information. To maximize I(\u017d;Y), a cross-entropy based loss is used, and the Kullback-Leibler divergence is used to to minimize I(2; X) [12], [15].\nHowever, VIB-based task-oriented communication systems presents several challenges:\n\u2022 Limitation of cross-entropy based loss: The majority of task-oriented communication systems rely on the cross-entropy loss as a vraitaional approximation to maximize"}, {"title": "V. PROPOSED METHOD: CLAD", "content": "We propose CLAD, a comprehensive task-oriented communication scheme designed to achieve strong task performance and improved privacy, through effective disentanglement of task-relevant and task-irrelevant information. The method utilizes two key encoders: the TRE, which maps the input into the task-relevant channel codeword Z1, and the task-irrelevant encoder (TIE), which maps the input into task-irrelevant channel codeword Z2. The task performance is optimized through contrastive learning, which aims to maximize the mutual information I(\u0179\u2081;Y), ensuring that 2\u2081 captures the most informative features for downstream classification. The disentanglement is achieved through reconstruction learning to capture task-irrelevant information in 22, and adversarial training is utilized to minimize the mutual information I(21; 22), thus promoting independence between the two feature representations. The different components and training stages of CLAD are visualized in Fig. 3. These components are optimized together to ensure both high task accuracy and effective disentanglement of information, corresponding to the following maximization objective:\nLCLAD = I(21;Y) + I(22; X|Y) \u2013 I(21; 22).\nHere, the objective consists of three key terms:\n\u2022 I(\u0179\u2081; Y) maximizes the mutual information between task-relevant features 2\u2081 and the label Y using contrastive learning;\n\u2022 I(22; X|Y) ensures that 22 captures the residual information in X that is not covered by Y by utilizing a reconstruction loss;\n\u2022 I(21; 22) minimizes the information overlap between 21 and 22, encouraging disentanglement via an adversarial loss.\nReflecting back on Fig. 1, we can see that our new objective maximizes the blue region (task-relevant information) and minimizes the pink region (task-irrelevant information) and avoids the conflicting objectives of VIB. We explain each of the components of our loss function in detail below, accompanied by their mathematical formulations, implementation details, and training strategy."}, {"title": "A. Contrastive Loss for Task-Relevant Features", "content": "To maximize I(21;Y), we adopt a supervised contrastive learning framework similar to [23]. First, we apply an augmentation function, which applies different augmentations such as"}, {"title": "B. Reconstruction for Task-Irrelevant Features", "content": "To maximize I(22; X|Y), we use a reconstruction-based objective that ensures X is reconstructed from both Y and 22, where 22 captures the information in X that is not already captured by Y. Let gn : RN \u2192 Rd represent the task-irrelevant encoder, parameterized by y, which maps input x \u2208 RN to encoded task-irrelevant representation 22 \u2208 Rd. The encoder approximates the posterior distribution of the latent variable 22 given x, which we denote by q(22x). This encoder is responsible for capturing features unrelated to the task, i.e., the features not directly useful for predicting y. Mathematically, the encoded task-irrelevant representation is given by\nZ2 = 8\u03b7 (x),\nwhere d represents the dimensionality of the task-irrelevant feature space.\nNext, we introduce the reconstructor rw : Rd \u00d7 RM \u2192 RN, parameterized by w. The reconstructor r takes as input both noisy task-irrelevant features 22 and task-relevant label y and attempts to reconstruct the original input x. The objective is to minimize the reconstruction error, ensuring that 22 focuses solely on task-irrelevant information. The reconstruction loss is defined as\nLrecon = Ep(x,y) [||rw(22, y) \u2013 x||2] .\nTo justify this approach, we show how this reconstruction-based loss provides an approximation for the mutual information I(22; X|Y). Using a variational encoder and reconstruction model ru (x|22, y), we can approximate I(22; X|Y) as follows,\nI(22; X|Y) \u2265 Ep(x,y)q(22|x) [log p(x|y, 22)]\n- Ep(x,y) [log p(x|y)].\nThe first term, Ep(x,y)q(22|x) [log p(x|y, 22), represents the expected log-likelihood of reconstructing x given both y and 22. The second term, Ep(x,y) [log p(x|y)], represents the expected log-likelihood of reconstructing x based solely on y, independent of the task-irrelevant features.\nMinimizing the reconstruction loss Lrecon effectively approximates the maximization of the first term in the mutual information expression, thereby increasing I(22; X|Y). By optimizing both the encoder gn and the reconstructor \u03b3\u03c9, we ensure that 22 captures task-irrelevant information while leveraging y for the reconstruction of task-relevant features in x."}, {"title": "C. Adversarial Disentanglement", "content": "To approximate the minimization of the mutual information I(21; 22), we employ adversarial training, following the approach in [26]. This ensures that task-relevant features in 2\u2081 and task-irrelevant features in 22 are disentangled. The mutual information I(21; 22) quantifies the dependence between 21 and 22. It is formally defined as\nI(21;22) = \u222b\u222b p(21, 22) log (p(21, 22)/P(21)p(22)) d21d22.\nMinimizing it promotes independence between these two representations. However, directly computing I(21;22) is intractable since it requires access to the underlying joint distribution p(21, 22) and the product of the marginals p(21)p(22).\nTo circumvent this, we approximate the minimization using a discriminator to distinguish between samples drawn from the joint distribution p(21,22) and samples drawn from the product of the marginals p(21)p(22).\nTo approximate the joint distribution, we sample pairs (21,22) from the encoder's output for the same input data point, which represents samples from p(21,22). For the marginal distribution, we shuffle 22 across the batch, generating (21,22), where 22 is a shuffled version of 22 from a different data point. This ensures that 21 and 22 are independent, approximating the product of the marginals p(21)p(22).\nLet D, represent the discriminator parameterized by v, trained to distinguish between joint samples (21,22) and marginal samples (21,22). The adversarial loss is defined as\nLadv = Ep(21,22) [log Dv(21, 22)]+ Ep(21)p(22) [log (1 \u2013 D, (21, 22))] .\nThis loss encourages D, to assign high probabilities to true joint samples (21,22) and low probabilities to independent (shuffled) samples (21, 22).\nTo promote disentanglement in the encoder, we add an adversarial penalty to the encoder's loss. The encoder is trained to fool the discriminator by making the joint distribution p(21,22) indistinguishable from the product of the marginals p(21)p (22). The encoder's loss for disentanglement is defined as\nLenc = Ep(21,22) [log(1 \u2212 Dy(21, 22))] .\nMinimizing Lenc encourages the encoder to make 21 and 22 as independent as possible, thereby minimizing the mutual information I(21; 22). This ensures that the latent representations 21 and 22 are disentangled, with 21 capturing task-relevant information and 22 capturing task-irrelevant information."}, {"title": "D. Classification Task", "content": "The final downstream task is classification, where the goal is to predict the label Y from the encoded features 21. We use a simple feed-forward neural network classifier q(y|21), parameterized by \u00f8, and trained with a cross-entropy loss. The classifier takes as input the task-relevant features 21 and is optimized to minimize the following cross-entropy loss:\nLclass = -Ep(x,y) \u2211 Eye log q4 (yc|21)\nc=1\nwhere C is the number of classes, and ye is the ground truth one-hot encoded label for class c."}, {"title": "E. Training Procedure", "content": "Training a complex system with many different components and loss function must be performed carefully to ensure that each stage achieves its goal without interfering with other objectives and that the gradients flow appropriately. The training is done in multiple stages, each targeting a different part of the system. Below, we outline the step-by-step procedure used to train our model and the associated algorithms.\nStage 1: Training the Task-Irrelevant Encoder: In the first stage, we train the task-irrelevant encoder gn by pairing it with a reconstructor rw. The reconstructor rw, learns to reconstruct an image by using the encoded representations from gn as well as the label informationy. This encourages gn to focus on capturing the parts of the input that are not necessary for the downstream classification task by minimizing the reconstruction loss. The reconstructor is discarded, and the parameters of gn are frozen after training to preserve the task-irrelevant features for later use. This procedure is outlined in Algorithm 1.\nStage 2: Training the Task-Relevant Encoder with Contrastive Loss and Discriminator: After freezing gn, the task-relevant encoder fe is trained in this stage. We use both a contrastive loss to ensure that fo captures class-discriminative features and an adversarial loss to enforce disentanglement between the task-relevant encoder fe feature vector and the task-irrelevant encoder gn feature vector. The task-relevant encoder is trained using augmented views of the input for contrastive learning and through adversarial training with the discriminator Dy. The training process alternates between updating the contrastive loss and updating the discriminator and encoder to ensure disentanglement. The details of this stage are described in Algorithm 2.\nStage 3: Training the Classifier on the Frozen Task-Relevant Encoder: Once disentanglement is achieved, we discard the projection head hy, the discriminator D,, and the task-irrelevant encoder gn, leaving only the frozen task-relevant encoder fe. In this final stage, we train the classifier q on top of fe for the downstream classification task. The classifier is trained with the cross-entropy loss, ensuring that it can utilize the task-relevant features fe for accurate classification. The classifier training procedure is outlined in Algorithm 3."}, {"title": "F. Information Retention Index Across Different Methods", "content": "To assess how much information 2 retains about input X, we estimate I(2; X), which quantifies the informativeness of the latent representation 2 for reconstructing the original input X. Since direct computation of mutual information is intractable, we adopt a reconstruction-based proxy [33] to compute the IRI across different methods.\nAssume that the reconstruction loss Lrecon (x2), parameterized by a reconstructor ry(\u00b7) with parameters y, denotes the expected error for reconstructing x from the latent representation 2. The mutual information I(2;X) can be bounded as follows:\nI(2; X) = H(X) \u2013 H(X|2) \u2265 H(X) \u2013 Ep(x,t) [Lrecon(x/2)],\nwhere H(X) represents the entropy of the input, and Lrecon (x2) is the reconstruction loss. Therefore, one can compute I(2; X) by minimizing the reconstruction error as follows:\nI(2; X) \u2265 H(X) \u2013 min Lrecon(x|2).\nIn practice, for each task-oriented communication method we evaluate, the corresponding encoder parameters are frozen, and a reconstructor ry is trained to minimize the reconstruction loss Lrecon (x2). The reconstructor is trained using mean squared error (MSE) as the loss function, and we evaluate"}, {"title": "VI. EXPERIMENTAL EVALUATIONS AND DISCUSSION", "content": "In this section, we present the experimental setup used to evaluate CLAD. We start by describing the datasets used in our experiments, followed by a discussion of the baseline methods, neural architectures, and the experimental setup. Finally, we present detailed evaluations and analysis of the results. For brevity, we omit results on other datasets, such as CIFAR-10; however, these can be found in the accompanying code repository\u00b9.\n1 The source code, models and results are available at https://github.com/OmarErak/CLAD"}, {"title": "A. Experimental Setup", "content": "1) Datasets: The Colored MNIST and Colored FashionMNIST datasets are extensions of the standard MNIST [35] and FashionMNIST [36] datasets, each consisting of 60,000 28x28 grayscale images. In Colored MNIST, handwritten digits (0-9) are overlaid on colored backgrounds, while in Colored FashionMNIST, clothing items from 10 categories (e.g., T-shirts, coats, shoes) are similarly displayed on colored backgrounds. The introduction of background colors adds additional task-irrelevant information, creating a more challenging setup for the model to disentangle task-relevant features relevant to classifying digits or clothing items from background-related attributes. Furthermore, incorporating background color labels enables the evaluation of attribute inference attacks, where an adversary is trained to predict background color. This setup provides insight into the model's ability to protect against such attacks while maintaining disentanglement between task-relevant and task-irrelevant features.\n2) Neural Network Architectures: In this work, we employ a deep neural network (DNN) architecture for the classification tasks across both Colored MNIST and Colored FashionMNIST datasets. The architecture consists of convolutional and fully connected layers, with a latent dimension d = 64. This architecture, detailed in Table I, serves as the backbone for"}, {"title": "B. Baselines", "content": "In our experiments, we compare the proposed method CLAD against two baselines: DeepJSCC [13] and Variational Information Bottleneck (VIB) [12], [15]. These methods provide a benchmark for task-oriented communication systems, helping to evaluate the effectiveness of our approach in disentangling task-relevant and task-irrelevant features, and improving predictive accuracy through contrastive learning.\n1) DeepJSCC: DeepJSCC is a neural network-based approach that optimizes the encoding of data for transmission over noisy channels. For our task-oriented scenario, DeepJSCC is trained with cross-entropy loss for classification rather than reconstruction, and it does not explicitly discard task-irrelevant information. As a result, it serves as a baseline for how well the encoded representation performs without feature disentanglement.\n2) Variational Information Bottleneck (VIB): The variational information bottleneck (VIB) framework aims to compress the input X into latent representation Z while retaining sufficient information for predicting Y. VIB balances I(Z; X) and I(Z; Y) via hyperparameter \u03b2. In our experiments, we provide results for different values of \u03b2 to illustrate how varying the trade-off between compression and task relevance impacts task performance, IRI, and privacy.\nBy comparing our method to these two baselines, we demonstrate how CLAD improves task-relevant feature extraction, minimizes task-irrelevant information, achieves better performance in downstream classification tasks and provides better privacy."}, {"title": "C. Evaluation Metrics", "content": "To evaluate the effectiveness of CLAD, we utilize three key metrics: task performance through classification accuracy, IRI, and attribute inference attack accuracy. These metrics provide insights into how well the method performs in terms of classification accuracy, information retention, and privacy preservation. Additionally, we assess all methods at different channel SNRs to measure performance under various dynamic transmission conditions.\n1) Task Performance (Accuracy): The primary evaluation metric for task performance is classification accuracy. It measures the ability of classifier to predict the label y from 2. Accuracy is calculated as the ratio of correctly classified instances to the total number of instances:\nAccuracy = 1/N \u03a3 fi(\u0177i = yi),\nwhere N is the total number of samples, \u0177 is the predicted label, and yi is the ground truth label.\n2) Information Retenetion Index (IRI): To quantify the amount of information retained in the encoded representation 2, we use our proposed method to compute the IRI. This measures how much information from the input X is present in the encoded representation 2, which helps assess the compression of the representation. By comparing IRI across different methods, we can evaluate how effectively each method discards task-irrelevant information.\n3) Attribute Inference Attack: In addition to task performance and information retention, we also evaluate privacy by comparing the vulnerability of different methods to attribute inference attacks. An attribute inference attack aims to recover sensitive or irrelevant information about the input, such as background color, from the encoded representation 2.\nGiven the encoded representation 2, the adversary seeks to predict the background color of the image. If 2 contains significant task-irrelevant information, the adversary will be able to classify the background color with high accuracy. To evaluate this, we train a background color classifier (with the"}, {"title": "D. Results and Analysis", "content": "In this subsection we thoroughly analyze and discuss the performance of the proposed method CLAD against the two baselines, DeepJSCC and VIB, on both Colored MNIST and Colored FashionMNIST datasets.\n1) Colored MNIST Results: Fig. 4a shows the accuracy as a function of the SNR for each method. Here, it is evident that CLAD outperforms all the baselines at different channel conditions. This is a strong indication that maximizing I(2; Y) through contrastive learning yields better feature extraction and classification performance, even with undesirable and high noise conditions. As seen in Table III, CLAD outperforms both DeepJSCC and VIB, achieving the highest classification accuracy of 98.42% at SNR = 12 dB. This represents a 0.46% improvement over DeepJSCC (97.96%) and a 0.52% improvement over VIB at \u03b2 = 0.001 (97.90%). The higher accuracy of CLAD can be attributed to its ability to learn a more disentangled and task-relevant latent space using contrastive learning and adversarial disentanglement.\nFrom Table III, CLAD achieves a very low IRI value of 0.039, drastically outperforming DeepJSCC (0.608) and VIB at \u03b2 = 0.001 (0.1931). This substantial difference shows that CLAD is effective at disentangling and removing task-irrelevant information from its encoded representations. This is further supported by the significantly lower adversarial accuracy of CLAD compared to DeepJSCC and VIB at \u03b2 = 0.001. From Fig. 6a, we can visually see that image reconstructions from CLAD excel in retaining task-relevant information and preserving the core structure of the digits while discarding irrelevant information such as background color and other style features such as thickness or slant in the digit. This contrasts with DeepJSCC and VIB, which retain unnecessary information such as background color and thickness and slant of the digit.\nInterestingly, from Table III, VIB at lower \u03b2 values (e.g., \u03b2 = 0.01 and \u03b2 = 0.1) achieves IRI values that are closer to CLAD. However, the major drawback of VIB is that, while it improves privacy, informativeness and minimality, it comes at the cost of accuracy, especially as \u03b2 increases, as shown in the table. For example, at \u03b2 = 0.1, VIB achieves even lower IRI,"}, {"title": "VII. CONCLUSIONS", "content": "In this work, we have proposed a new task-oriented communication system, CLAD, based on contrastive learning and adversarial disentanglement. The proposed framework effectively disentangles task-relevant and task-irrelevant features, leading to enhanced performance in downstream tasks while simultaneously improving privacy as demonstrated using attribute inference attacks. Additionally, by disentangling task-relevant and task-irrelevant features, we reduce number of information bits that need to be transmitted. By leveraging contrastive learning, we enforce the model to capture discriminative task-relevant features, while adversarial disentanglement ensures that task-irrelevant features are separated, minimizing the mutual information between task-relevant and irrelevant representations. Furthermore, we present an approach"}]}