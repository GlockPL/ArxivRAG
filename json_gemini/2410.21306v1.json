{"title": "Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges", "authors": ["Farid Ariai", "Gianluca Demartini"], "abstract": "Natural Language Processing is revolutionizing the way legal professionals and laypersons operate in the legal field. The considerable potential for Natural Language Processing in the legal sector, especially in developing computational tools for various legal processes, has captured the interest of researchers for years. This survey follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses framework [89], reviewing 148 studies, with a final selection of 127 after manual filtering. It explores foundational concepts related to Natural Language Processing in the legal domain, illustrating the unique aspects and challenges of processing legal texts, such as extensive document length, complex language, and limited open legal datasets. We provide an overview of Natural Language Processing tasks specific to legal text, such as Legal Document Summarization, legal Named Entity Recognition, Legal Question Answering, Legal Text Classification, and Legal Judgment Prediction. In the section on legal Language Models, we analyze both developed Language Models and approaches for adapting general Language Models to the legal domain. Additionally, we identify 15 Open Research Challenges, including bias in Artificial Intelligence applications, the need for more robust and interpretable models, and improving explainability to handle the complexities of legal language and reasoning.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, advancements in Natural Language Processing (NLP) have significantly impacted the legal domain by simplifying complex tasks such as Legal Document Summarization (LDS), enhancing legal text comprehension for laypersons, and improving Legal Question Answering (LQA) and Legal Judgment Prediction (LJP) [24, 42, 50, 52, 63, 93, 98]. These improvements are primarily attributed to advancements in neural network architectures, such as transformer models [118]. NLP techniques now enable machines to generate text, answer legal questions, drafting a regulation, and simulate legal reasoning, which can revolutionize legal practices [50]. Applications like contract review [45, 76, 77, 117] and case prediction [85, 120] have been automated to a large extent, speeding up processes, reducing human error, and cutting operational costs. Additionally, the use of NLP allows lawyers and legal professionals to reduce their workload, enhance efficiency, and minimize errors in decision-making processes [98]. Despite the rapid development of NLP, challenges remain in processing lengthy documents,"}, {"title": "2 BACKGROUND AND FOUNDATIONAL CONCEPTS", "content": "In this section, we will establish an essential understanding of how NLP intersects with the legal domain. Initially, we explore the characteristics of legal documents, which are fundamental to this intersection. Legal texts are known for their complex structure, specialized words. Recognizing these attributes is essential as they significantly influence the development and application of AI technologies in legal practices.\n2.1 Legal Documents\nLegal documents are typically written in a descriptive language and presented in an unstructured text format. They have unique features that set them apart from other fields. These documents cover a broad array of texts essential"}, {"title": "2.1.1 Legal language and its characteristics", "content": "Legal language is characterized by its unique features that set it apart from everyday language, primarily due to its function in the legal system. One prominent feature is its formality, where legal language often employs a more formal vocabulary and syntax to ensure precision and avoid ambiguity [43]. This formality is critical, as the precise meaning of terms can have significant legal effects. Legal texts also typically utilize passive constructions and complex sentence structures to provide detailed and comprehensive descriptions [43]. These constructions help clarify responsibilities and outcomes without directly attributing actions or intentions to specific parties.\nAnother distinctive aspect of legal language is its reliance on specialized words and phrases. This includes terms that have specific meanings within legal contexts, archaic words that are not commonly used in everyday language, and standardized phrases that have been historically embedded in legal tradition [43]. This can make legal documents less accessible to non-specialists, requiring legal professionals to interpret the content accurately.\nFurthermore, legal language is heavily intertextual, meaning it frequently references other legal texts, such as statutes, regulations, and case law. This characteristic ensures that legal arguments are grounded in and supported by existing legal frameworks and previous cases. The dense use of citations and references in legal documents not only supports the arguments made but also connects the document to a broader legal discourse. Such intertextuality demands that legal professionals not only understand the texts themselves but also the"}, {"title": "2.2 Legal NLP", "content": "2.2.1 Introduction to Legal NLP. Legal NLP involves the application of NLP techniques to legal texts. This field is crucial as it helps automate and enhance the analysis of complex legal documents, improving efficiency and accuracy in legal research, compliance, and decision-making processes. The foundation of NLP is text, and the legal domain primarily consists of textual data [2], including statutes, case law, contracts, and regulations. Given the text-intensive nature of the legal field, NLP offers significant potential to transform how legal professionals interact with and utilize this vast amount of information. By leveraging advanced algorithms and Machine Learning (ML) models, Legal NLP aims to make legal texts more accessible, interpretable, and actionable.\n2.2.2 Why NLP is a game-changer for the legal section. LLMs, as a part of NLP applications, such as ChatGPT [88], have had a significant impact since their public debut in November 2022. The legal sector, however, has been exploring the potential of AI for a longer period, applying it practically. NLP applications in the legal field are extensive, ranging from drafting client briefs and producing complex analyses from large document sets to enabling smaller firms to compete with larger ones [78]. NLP is very important in doing thorough checks when companies merge and greatly helps in legal education and learning in fast-changing fields [78].\nA notable demonstration of the capabilities of NLP applications in the legal sector occurred when GPT-4 passed the Uniform Bar Exam, underscoring the technology's potential [56]. Lawyers and law students are keenly aware of this potential, as evidenced by a LexisNexis survey released in August 2023 [62]. The survey revealed that about half of all lawyers believe that LLMs will significantly transform legal practice, with nearly all (92 percent) expecting some impact. Additionally, 77 percent of respondents believe LLMs will increase the efficiency of lawyers, paralegals, and law clerks, while 63% foresee changes in how law is taught and studied. Moving forward, we will introduce specific NLP tasks in the legal domain, exploring their applications and impacts they are expected to have on the legal profession."}, {"title": "2.2.3 Basic foundations of NLP", "content": "The integration of NLP in the legal domain relies on several foundational techniques that enable the effective analysis and manipulation of legal texts. These fundamental methods provide the building blocks for more complex applications, transforming unstructured legal documents into structured, actionable information. By leveraging these NLP techniques, legal professionals can enhance their efficiency and accuracy, making it easier to manage and interpret vast amounts of legal data. The following section provides essential definitions of terms related to the basic foundations of NLP:\n\u2022 Tokenization: Tokenization is the process of breaking down a sequence of text into smaller units, typically words or sub-words, known as tokens. This is a fundamental step in NLP as it allows for the structured analysis of text. In the legal domain, tokenization helps in processing and understanding lengthy documents by segmenting them into manageable pieces.\n\u2022 Word Embeddings: Word Embeddings are continuous vector representations that encode the semantic meanings of words or tokens in a high-dimensional space. These representations allow models to convert individual tokens into a format suitable for neural network processing. During training, LMs develop embeddings that capture the relationships between words, such as synonyms."}, {"title": "Transformers", "content": "Transformers [118] are a neural network architecture designed to convert input sequences into output sequences by understanding the context and relationships among the elements of the sequence. For instance, given the input \"What is the color of the sky?\", a transformer model internally processes and identifies connections among the words 'color', 'sky', and 'blue'. This understanding enables it to produce the output: \u201cThe sky is blue\" [3]. Transformer models enhance this process through a self-attention mechanism. This mechanism allows the model to analyze different parts of the sequence simultaneously instead of sequentially, helping it identify which parts are most significant.\n\u2022 PLMs: PLMs are trained on extensive corpora in a self-supervised manner, which involves tasks like recover-ing incomplete input sentences or auto-regressive language modeling. These models, such as Bidirectional Encoder Representations from Transformers (BERT) [29] and RoBERTa [70], are initially trained on large-scale general text datasets. After pre-training, they can be fine-tuned for specific downstream tasks in the legal domain, adapting them to comprehend and process legal language for applications like document classification and information extraction.\n\u2022 Question Answering (QA):QA system is a type of NLP solution designed to answer questions posed in natural language. These systems take a user's query and, by extracting relevant information from a dataset provide a relevant and informative response.\n\u2022 NER: NER is the task of identifying mentions of specific entities within a text that belong to predefined categories such as persons, locations, organizations, and more. NER is a fundamental component for many NLP applications, including question answering, text Summarization, and machine translation [64].\n\u2022 Information Retrieval (IR): IR involves the process of obtaining relevant information from large collections of unstructured legal texts, such as case laws, statutes, contracts, and regulations. The goal of IR is to provide users with the most relevant documents and data in response to specific queries.\n\u2022 Multi-task Learning (MTL): MTL is an approach in ML where a model is trained on multiple related tasks simultaneously or utilizes auxiliary tasks to enhance performance on a specific task. By learning from diverse tasks, MTL enables models to capture generalized and complementary knowledge, improving robustness and addressing data scarcity, particularly for low-resource tasks. MTL's ability to share implicit knowledge across tasks often leads to performance gains and more efficient models, making it a valuable strategy for building robust and adaptable systems in NLP and other domains [22].\n\u2022 Parameter-Efficient Fine-Tuning (PEFT): PEFT is a method for adapting PLMs that involves freezing the majority of the model's parameters and only updating a small subset. This approach significantly reduces the computational resources and time required for fine-tuning, making it particularly effective in resource-limited scenarios, while still achieving competitive performance in tasks like text generation [65].\n\u2022 Retrieval-Augmented Generation (RAG): RAG is an advanced AI framework that enhances text creation by merging traditional information retrieval systems, with the generative power of LLMs. This integration allows the AI to access additional knowledge sources while utilizing its advanced language capabilities."}, {"title": "2.2.4 Key Publications and Conferences in Legal NLP", "content": "This section highlights the key journals, conferences, and workshops that serve as platforms for sharing advancements and insights in the intersection of NLP and the legal domain. These resources provide good opportunities for researchers to engage with cutting-edge research in Legal NLP.\nSeveral leading journals focus on the intersection of AI, NLP, and the legal domain. \u201cArtificial Intelligence and Law\", published by Springer, is a leading journal that features research articles on legal reasoning, legal IR, and legal knowledge representation. Additionally, the \u201cJournal of Law and Information Technology\", published by Taylor & Francis, focuses on the application of information technology in law, including research AI.\nConferences significantly advance research and promote collaboration in Legal NLP. The International Confer-ence on Artificial Intelligence and Law (ICAIL) is a biennial event that showcases advances in AI applications in"}, {"title": "3 METHODOLOGY", "content": "This survey follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) frame-work [89]. It ensures a transparent and comprehensive assessment of research to NLP tasks within the legal sector.\n3.1 Search Strategy\nWe performed a systematic search across two academic databases to identify relevant studies, including: Google Scholar and IEEE Xplore. Then, search queries were crafted to capture studies that focused on the application of NLP to legal tasks. The search was defined by the following queries:\n\u2022 Query 1: (\"Natural Language Processing\u201d OR \u201cNLP\u201d) AND (\u201cLegal\u201d OR \u201cLaw\")\n\u2022 Query 2: (\u201cLegal\u201d AND (\u201cNamed Entity Recognition\u201d OR \u201cNER\u201d OR \u201cDocument Summarization\u201d OR \u201cText Classification\u201d OR \u201cDocument Classfication\u201d OR \u201cJudgment Prediction\u201d OR \u201cQuestion Answering\" OR \u201cCorpus\u201d OR \u201cLanguage Model\u201d))\nOur search covered publications with the following date ranges for each specific NLP task: LQA from 2020-2024, LJP from 2017-2024, LTC from 2018-2023, LDS from 2016-2024, Legal NER from 2010-2022, legal corpora from 2021-2024, and Legal LMs from 2020-2024. This ensured that we included recent advancements. Peer-reviewed journal articles and high-quality conference proceedings were prioritized, along with a secondary consideration of non-peer-reviewed sources, such as arXiv articles, where relevant.\n3.2 Study Selection\nA total of 148 studies were initially identified from the database search. To refine this list, we applied a manual reviewing and. This process involved:\n(1) Title and Abstract Screening: We reviewed the titles and abstracts of all retrieved studies to assess their relevance to the predefined legal NLP tasks. Studies unrelated to the core legal NLP and its tasks were excluded at this stage.\n(2) Full-Text Review: Articles that passed the initial screening went through a more detailed full-text review. This was done to confirm their relevance, quality, and alignment with the inclusion criteria. During this phase, a careful study of the literature review sections of each included research and survey was conducted. This helped ensure that the studies not only contributed original findings but also reflected a comprehensive understanding of the existing research landscape in legal NLP.\n(3) Final Selection: From the original 148 studies, 127 were included in the final list, selected based on their direct relevance to key NLP tasks within the legal sector and their methodological quality, as well as their engagement with existing literature in the field."}, {"title": "3.3 Eligibility Criteria", "content": "To determine which studies were included in the final synthesis, we established the following criteria:\n\u2022 Inclusion Criteria:\n\u2013 The study must focus on at least one of the following NLP tasks: Legal Question Answering, Legal Named Entity Recognition, Legal Judgment Prediction, Legal Document Summarization, Legal Text Classification, Legal Language Models, or legal corpora.\n\u2013 The study must present empirical research or significant methodological contributions to legal NLP.\n\u2013 Both peer-reviewed and non-peer-reviewed (e.g., arXiv) studies were considered if they provided valuable insights.\n\u2022 Exclusion Criteria:\n\u2013 Studies focused exclusively on unrelated areas such as information retrieval methods, pattern mining, information extraction, or similarity detection without a clear application to the specific legal NLP tasks mentioned.\n\u2013 General NLP studies without a focus on legal applications.\n\u2013 Editorials, opinion pieces, or other non-research articles.\n\u2013 Papers that did not meet basic methodological standards were not included in the final analysis."}, {"title": "4 NLP TASKS, DATASETS, AND LARGE CORPORA IN LEGAL DOMAINS", "content": "NLP tasks in the legal domains cover a range of specialized applications that address the unique challenges and requirements of legal texts. These tasks leverage advanced NLP techniques to process, analyze, and extract meaningful information from legal documents. Legal NLP tasks help make legal research and decision-making more efficient and accurate. In this section, we explore various NLP tasks that are tailored to the legal domain and show their impact on the legal section. Additionally, we will discuss existing works and research related to each task and provide an overview of the current state of the art in this field.\nFurthermore, the success of these NLP tasks heavily depends on the availability and quality of domain-specific datasets. Therefore, we will also examine the datasets commonly used in legal NLP research, exploring their characteristics and the role they play in advancing the development of NLP models for the legal domain."}, {"title": "4.1 Legal Question Answering", "content": "4.1.1 Task. LQA involves responding to queries about law. This task is typically done by legal professionals skilled in the relevant legal field. This process requires a comprehensive review of existing laws, careful interpretation of statutes and regulations, and the application of legal principles and past cases to the relevant facts. LQA seeks to offer precise advice on legal matters. It helps people and businesses in navigating the legal landscape and addressing legal challenges. Recently, DL has been leveraged in LQA to employ neural network models that train on extensive datasets to identify complex patterns and relationships. These models evaluate the questions posed, identify relevant legal topics, and produce suitable answers based on the patterns they have learned.\nModern ML approaches for LQA, particularly DL, rely on neural networks to understand natural language. Popular architectures include Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), and Convo-lutional Neural Networks (CNN), which can be fine-tuned for specific tasks such as QA. These models generate accurate responses, adapt to new patterns, understand context, and provide relevant answers. Transformer-based models, such as BERT [29] and ChatGPT [88], have proven particularly effective in NLP tasks. These models use the transformer architecture and self-attention mechanisms to learn the context of the text and understand the meaning of words. This allows them to provide relevant answers by effectively weighing the importance of different parts of the input. In the following, we will study the existing LQA works in legal domain."}, {"title": "4.2 Legal Judgment Prediction", "content": "4.2.1 Task. LJP is an important task within legal NLP, especially prevalent in civil law systems where judgments are determined based on case facts and statutory articles [138]. This task involves predicting legal outcomes from the descriptions of cases and relevant legal statutes, and is essential in countries like France, Germany, Japan, and China [138]. LJP has garnered significant attention from AI researchers and legal professionals due to its potential to assist judges, lawyers, and legal scholars in predicting case outcomes based on historical data [23]."}, {"title": "4.3 Legal Text Classification", "content": "4.3.1 Task. LTC is an important task within the domain of NLP that involves categorizing legal documents based on their content, a foundational aspect of building intelligent legal systems. With the exponential growth of legal documents, it has become increasingly challenging for legal professionals to efficiently locate relevant rulings in similar cases for argumentation. LTC addresses this challenge by automatically associating legal texts with predefined categories, such as criminal, civil, or administrative cases, thereby simplifying legal research and decision-making processes.\nIn the legal field, this process is often referred to as predictive coding, where ML algorithms are trained through supervised learning to classify documents into specific categories. The broader task of text classification in NLP involves assigning one or multiple categories to a document from a set of predefined options, and it can take various forms, including binary classification, multi-class classification, and multi-label classification. Legal document classification often falls under large multi-label text classification, where the label space can consist of thousands of potential categories, adding significant complexity to the task [101]. This subsection explores the methodologies and advancements in this area.\nDL typically requires extensive data to yield effective results, but MTL offers a potential solution to the data scarcity problem. Elnaggar et al. [34] leverage transfer learning and MTL to perform tasks like translation and multi-label classification within legal document corpora. They utilize the MultiModel algorithm [53], which uses a fully convolutional sequence-to-sequence architecture integrating different modality nets. Their model processes legal texts through a unified embedding, enabling efficient task switching and promoting generalization across different legal tasks, effectively tackling data scarcity in the legal field."}, {"title": "4.4 Legal Document Sumarrization", "content": "4.4.1 Task. LDS is a specialized branch of automatic summarization that focuses on condensing legal texts, such as court judgments, into clear and informative summaries. Unlike general text summarization, which extracts key details without following specific formatting rules, LDS must account for the distinct structure and specialized content of legal documents. These documents often include complex details like article numbers, statutory language, and citations that are critical for presenting the legal arguments and decisions accurately. The natural complexity of legal texts, characterized by their extensive length and detailed internal structures such as sections, articles, and paragraphs in statutes-demands tailored summarization techniques. This need is further emphasized by the hierarchical importance of documents based on their judicial origin, where the interpretation of texts can vary significantly between higher and lower court opinions [55].\nLDS can be approached through extractive and abstractive methods. Extractive summarization techniques in LDS focus on identifying and extracting the most critical sentences or phrases directly from the text, maintaining the original wording and meaning. In contrast, abstractive summarization involves generating new sentences that paraphrase the most important information, aiming for conciseness and coherence while ensuring that the essence of the legal text is preserved. This subsection will explore existing approaches to LDS and show the small differences that set it apart from more general summarization methods and discussing the challenges and solutions specific to the legal domain."}, {"title": "4.5 Legal Named Entity Recognition", "content": "4.5.1 Task. NER is a fundamental task in NLP that involves identifying specific segments of text and classifying them into predefined categories such as 'organization', 'person', and 'location' [59]. In the legal domain, NER extends to specialized recognition tasks that focus on extracting entities unique to legal texts, such as laws, legal norms, and procedural terms. This specialized form of NER is crucial for structuring legal documents and enhancing legal IR systems. Unlike general NER systems that handle common entity types, legal NER must navigate the complex language and structured format of legal documents, underscoring the need for systems and methodologies specifically tailored to the legal context.\nDozier et al. [32] present pioneering work in NER within legal documents such as US case law and pleadings, employing three methodologies: lookup, contextual rules, and statistical models to detect entities like judges, attorneys, and legal terms. Their system adapts these approaches to the specialized context of legal texts, processing various types of documents and extracting legal entities. This work highlights the challenges and necessary adaptations for deploying NER in the legal domain, where the specialized language and high accuracy are required for successful legal analysis."}, {"title": "4.6 Large Legal Corpora", "content": "The foundational step in training an LLMs is the use of extensive corpora. For the development of a sophisticated LLMs that effectively addresses a wide range of legal NLP tasks, it is crucial to have access to large-scale legal corpora. These corpora must meet several critical criteria to ensure their effectiveness and ethical utility. First, they should be transparent in their sourcing and composition, allowing users to understand the origins and types of included data. Additionally, the privacy of individuals should be safeguarded, preventing any potential invasion of personal data. It is also imperative to minimize toxicity and bias within the corpora to promote fairness and accuracy in model outcomes. By following these principles, we can enhance the capabilities of LLMs in the legal domain. This ensures they are both powerful and reliable tools for legal analysis. In this section, we will explore the existing large legal corpora and online databases."}, {"title": "5 LEGAL LANGUAGE MODELS AND METHODS FOR LEGAL DOMAIN ADAPTATION", "content": "In the fast-moving field of NLP, LLMs have become a key tool for processing and understanding large amounts of unstructured text data. These models, initially trained on broad datasets like Wikipedia, have shown great skill across various language tasks. Building on this success, the legal technology community is increasingly interested in using these powerful models for Legal NLP applications. This involves adapting these general-domain models to legal texts and further training them on specialized legal documents. Such efforts aim to reduce the domain gap and customize the models to better understand the complex language used in legal documents. In this section, we will explore how these models are being adapted and applied within the legal domain to enhance Legal NLP applications.\nIn this section, following the methodology of this survey, we studied all peer-reviewed LMs or methods. However, due to the significant challenges present in the legal domain, there are many legal LMs that have not undergone peer review. Given the scarcity of adequate peer-reviewed resources, our research has focused on the investigation of, in order of priority, the peer-reviewed sources, then the most well-known and widely used non-peer-reviewed legal LMs. Despite their lack of formal peer review, these models have gained considerable attention and usage in the field."}, {"title": "5.1 Language Models", "content": "Chalkidis et al. [18] present an in-depth analysis of applying BERT [29], a pre-trained language model, in the legal domain, showcasing the need for domain-specific adaptation to enhance performance on legal NLP tasks. They explore three strategies: using standard BERT directly, further pre-training on legal corpora, and pre-training from scratch with legal-specific data. Their study found that both further pre-training and pre-training from scratch generally outperform the use of BERT directly. They introduce LEGAL-BERT, a specialized family of models optimized for legal text, which includes versions for varied computational capacities and demonstrates competitive performance with a lower environmental impact.\nXiao et al. [126] introduce Lawformer, a Longformer-based [10] language model adapted for Chinese legal texts, designed to handle extensive document lengths common in legal data. Recognizing the limitation of standard PLMs with shorter token capacities, Lawformer employs a unique combination of sliding window, dilated sliding window, and global attention mechanisms to efficiently process long texts, making it suitable for legal AI tasks like judgment prediction and LQA. Pre-trained on a vast corpus of Chinese legal documents segmented into criminal and civil cases, Lawformer integrates complex sequential dependencies across tokens using these attention techniques, enhancing model performance for legal-specific tasks.\nIn the development of specialized NLP tools for Arabic legal texts, a model specifically tailored to the unique linguistic features of Arabic jurisprudence was designed, introducing AraLegal-BERT [1] midway through this innovation. This model enhances NLP applications within the legal field by adapting BERT [29] technology to Arabic's specific content needs, involving pre-training BERT from scratch using a broad range of legal documents, including legislative materials and contracts.\nColombo et al. [25] introduce SaulLM-7B, a novel LLM specifically designed for legal text comprehension and generation, built on the 7 billion parameter Mistral [51] architecture. This model is trained on an extensive English legal corpus, designed to meet the unique challenges of legal syntax and terms. SaulLM-7B uses a two-tier training approach: continued pre-training on a carefully curated 30 billion token legal dataset and an innovative instruction fine-tuning method, incorporating both generic and legal-specific instructions to enhance the model's performance on legal tasks."}, {"title": "5.2 Methods for Improving In-Domain Adaptability of Legal Language Models", "content": "Li et al. [63] explore a novel adaptation of LMs for the legal domain by integrating domain-specific unsupervised data from public legal forums to optimize prefix domain adaptation, a parameter-efficient learning approach that trains only about 0.1% of the model's parameters. They introduce a training methodology where a deep prompt is specifically tuned using a domain-adapted prefix from legal forums and then utilized in various legal tasks, demonstrating improved few-shot performance compared to full model tuning methods like LEGAL-BERT [18]. This approach significantly reduces computational overhead while maintaining or exceeding performance metrics across multiple legal tasks, suggesting an efficient and scalable model for legal NLP applications.\nMamakas et al. [75] explore strategies for adapting pre-trained transformers to cope with the challenges of long legal texts within the LexGLUE benchmark, focusing on extending input capabilities and enhancing efficiency. They modify Longformer [10], originally extending up to 4,096 sub-words, to process up to 8,192 sub-words by reducing local attention window size and incorporating a global token at the end of each paragraph to facilitate information flow across longer texts. Additionally, they adapt LEGAL-BERT [18] to employ TF-IDF representations to manage longer documents effectively, introducing variants like TF-IDF-SRT-LegalBERT, which deduplicates and sorts sub-words by TF-IDF scores, and TF-IDF-EMB-LegalBERT, which incorporates a TF-IDF embedding layer. These adaptations aim to combine the robust capabilities of transformers with the practical requirements of handling extensive legal documents, surpassing the performance of traditional linear classifiers while maintaining computational efficiency."}, {"title": "6 OPEN RESEARCH CHALLENGES", "content": "Despite researchers' efforts in the this interdisciplinary field and extensive advancements in AI techniques, Open Research Challenges (ORCs) still exist. In this section, we identify the ORCs and provide advice to overcome these challenges.\nORC 1: Bias and Fairness. Bias and fairness are crucial concerns in the field of AI, especially at the intersection with the legal domain where decisions can deeply impact individuals' lives. The scarcity of unbiased data in legal domains such as case law complicates the training of AI models, as these models often learn from historical decisions that may reflect existing human biases [33, 114]. This reliance on biased datasets can lead to unfair and biased outcomes in classification and prediction tasks. Addressing these issues is critical to ensure that AI-driven legal decisions uphold the standards of impartiality and fairness required for justice.\nORC2: Interpretability and Explainability. Interpretability and explainability are crucial across various applications in legal NLP, yet these aspects remain underexplored in many studies. The ability to trace and comprehend the decision-making process of AI systems is essential for identifying and mitigating biases. Transparent and understandable AI systems help build trust and ensure they are used responsibly, which is particularly important in legal contexts where decisions can significantly impact people's lives. Improving these aspects of AI models is necessary to their ethical use, ensuring they meet the high standards of fairness required in legal proceedings.\nORC3: Transparent Annotation. Transparently annotated datasets are rare in the field of legal NLP. Often, studies mention the involvement of expert annotators for tasks such as classification, question answering, or prediction,"}, {"title": "7 CONCLUSION", "content": "Advances in AI and NLP have improved Legal NLP techniques and models. These improvements help better meet the needs of laypersons in legal matters and ease the workload of legal professionals. This survey provides a comprehensive overview of the advancements in NLP techniques used in the legal domain. Additionally, we discussed the unique characteristics of legal documents. We also reviewed existing datasets and LLMs tailored for the legal domain. Legal NER research spans multiple languages and utilizes diverse methods, from rule-based to BERT-based models. LDS has largely focused on extractive and abstractive methods, including TF-IDF and transformer-based models. In LTC, multi-class classification tasks dominate, with deep learning architectures like CNNs and Bi-LSTMs widely used. LJP primarily focuses on Chinese datasets with deep learning approaches like CNNs. LQA often leverages information retrieval techniques such as BM25, with a significant focus on statutory law. Finally, we explored key ORCs, such as the need for domain-specific fine-tuning strategies, addressing bias and fairness in legal datasets, and the importance of interpretability and explainability. Other challenges include the development of more robust pre-processing techniques, handling multilingual capabilities, and integrating ontology-based methods for more accurate legal reasoning."}]}