{"title": "Explainable Reinforcement Learning for Formula One Race Strategy", "authors": ["Devin Thomas", "Junqi Jiang", "Avinash Kori", "Aaron Russo", "Steffen Winkler", "Stuart Sale", "Joseph McMillan", "Francesco Belardinelli", "Antonio Ragot"], "abstract": "In Formula One, teams compete to develop their cars and achieve the highest possible finishing position in each race. During a race, however, teams are unable to alter the car, so they must improve their cars' finishing positions via race strategy, i.e. optimising their selection of which tyre compounds to put on the car and when to do so. In this work, we introduce a reinforcement learning model, RSRL (Race Strategy Reinforcement Learning), to control race strategies in simulations, offering a faster alternative to the industry standard of hard-coded and Monte Carlo-based race strategies. Controlling cars with a pace equating to an expected finishing position of P5.5 (where P1 represents first place and P20 is last place), RSRL achieves an average finishing position of P5.33 on our test race, the 2023 Bahrain Grand Prix, outperforming the best baseline of P5.63. We then demonstrate, in a generalisability study, how performance for one track or multiple tracks can be prioritised via training. Further, we supplement model predictions with feature importance, decision tree-based surrogate models, and decision tree counterfactuals towards improving user trust in the model. Finally, we provide illustrations which exemplify our approach in real-world situations, drawing parallels between simulations and reality.", "sections": [{"title": "1 Introduction", "content": "Formula One (F1) is a class of motorsport, often described as its pinnacle, with the average annual cost of running a team in the hundreds of millions of pounds.\u00b9 Teams are thus in constant pursuit of the slightest gains in time, achieved by recruiting the best drivers and making their cars faster through ground-breaking engineering. Once a race begins, however, teams are unable to make changes to their cars or drivers but can optimise their selection of the tyre compounds used through the race. Tyres degrade and become slower as they are used and the different tyre compounds are selected to provide a trade-off between speed and rate of tyre degradation.\nThis optimisation problem is the crux of race strategy, which primarily consists of choosing which tyre compounds to select and when to make pitstops, i.e. leaving the race to change tyres. The optimal race strategies are far from simple, however, given that live race situations must be taken into account, and thus have become a critical factor in determining the cars' finishing positions. This has been accentuated by F1's move to a single tyre manufacturer for all teams, rather than multiple competing tyre manufacturers, which has levelled the playing field between teams and increased the importance of their strategic decisions.\u00b2"}, {"title": "2 Background and Related Work", "content": "RL. First, we give a brief overview of the RL model used in the paper. Q-learning is an off-policy Temporal Difference (TD) control method that uses a behavioural policy to control the exploration of the environment, whilst the target policy is updated. Actions are selected based on their respective Q-values \u2013 the value of taking the action in the current state. Meanwhile, a deep Q-network (DQN) [18] is an RL agent that combines Q-Learning with a deep convolutional neural network. A DQN works by minimising the TD error, given the weights of the DQN. Finally, the deep recurrent Q-network (DRQN) [9], builds upon the DQN by utilising a recurrent neural network to predict the Q-values instead of a regular convolutional neural network. In doing so, the model utilises previous states in its understanding and predictions of the current state, unlike the DQN which only considers the current state. The DRQN was found to parallel the performance of the traditional DQN and outperforms it in the Atari games Frostbite and Double Dunk [9].\nRL in Motorsport. Many motorsport categories other than F1, such as Formula E, WEC, Indycar and GT Racing, also require effective race strategies. Within Formula E, Liu et al. [14, 15] explore race strategy using distributed deep deterministic policy gradients, neural networks and Monte Carlo Tree Search.\nIn GT Racing, Boettinger and Klotz [5] implement a DQN to optimise strategy decisions over a limited action space. They find that a baseline Q-Learning model is significantly beaten by the DQN model. Meanwhile, Wurman et al. [26] employ a quantile regression soft actor-critic model to drive GT cars in the Gran Turismo game. Their model outperformed the world's top drivers and demonstrated driving strategies such as different corner overtakes, highlighting its ability to learn human behaviours.\nIn F1, Heilmeier et al. [11] introduce a virtual strategy engineer comprising two separate neural networks that predict whether or not a pitstop is taken based on given race inputs. They discuss as future work the possibility of taking an RL-based approach, as we introduce in this paper.\nFinally, Heine and Thraves [12] explore race strategy through dynamic programming (DP) and later incorporate random events such as safety car periods through stochastic DP. They find that both models can be solved to optimality. The DP approach allows the answering of questions such as \"If there is a safety car period in the current lap, is it worth making a pit stop?\" and \"If yes, which tyre compound should we change to?\". Comparatively, the stochastic DP approach considers delaying pitstops in favour of benefiting from future safety car periods, which improves upon the DP model when randomness is increased during simulations.\nNone of the works mentioned here, in F1 or otherwise, apply RL, coupled with explainability techniques, to optimise race strategy.\nXAI for RL. XAI has gained prominence due to the growing role of Al in society, and thus, it is increasingly important that humans comprehend the predictions made by these systems. Milani et al. [17] give a useful review of different explainability techniques and present a novel taxonomy. Methods such as DAGGER [20], VIPER [2], and PIRL [24] convert RL policies into interpretable formats such as decision trees, whereas reward decomposition [23] is used to decompose the rewards into a set of additive terms with semantic meaning. Some methods directly generate explanations. For example, natural language explanations employed by Hayes and Shah [10] utilise templates for the agent to fill in. Saliency maps are also used by Greydanus et al. [8], though they have been considered insufficient for explaining RL due to the subjective conclusions that can be drawn from their explanations [17].\nAs for the methods we use in this paper, we first note that SHAP [16] is a method to explain individual predictions based on Shapley values [21]. SHAP explains the prediction of an instance by computing the contribution of each feature to the prediction using computed Shapley values. TimeSHAP was then introduced by Bento et al. [3] and extends the use of Shapley values into sequential domains. TimeSHAP is a model-agnostic recurrent explainer that can compute feature-level attributions, highlighting the importance of features of the current time step to a prediction.\nAnother method we use is VIPER [2], an imitation learning method that builds upon the data aggregation method of DAGGER [20]. As a surrogate model, VIPER builds decision trees that closely mimic the behaviour of a pre-trained oracle. VIPER leverages the fact that"}, {"title": "3 Race Strategy Reinforcement Learning", "content": "We now cover the architecture and implementation of our approach."}, {"title": "3.1 Problem Formalisation", "content": "State Space. Selecting the state space, S, is critical to ensure that the model can execute successful strategies without being overloaded with features. The chosen features are outlined in Table 1. It should be noted that there are three tyre compounds used: soft, medium and hard. The soft tyres are initially the fastest but they wear down and become slower more quickly than the other tyres. On the other hand, the hard tyres are longer-lasting but slow, whilst the medium tyres give a compromise between the two. Meanwhile, the deployment of RL introduces a new challenge in data preprocessing since we do not have a predefined training dataset that dictates the limits of each feature. Theoretically, some features have no limit in their values. For instance, the gap to the leader \\(s_{gl}\\), i.e. the time in seconds between the leader of the race and RSRL's car, can be arbitrarily large if the driver takes a pitstop every lap, subsequently making them fall further and further back from the leader. We thus created a dataset with value ranges determined by observations from a reasonable number of simulations to estimate their working ranges. From this set of values, we created custom scaling functions that linearly scale each feature.\nAction Space. At each lap, RSRL decides whether to change the tyres by executing a pitstop. Subsequently, the action space at any timestep, t, consists of four actions, \\(a^{t} \\in \\{\\textit{no pit}, \\textit{pit soft}, \\textit{pit medium}, \\textit{pit hard}\\}\\), representing taking no pitstop or taking a pitstop to one of the three tyre compounds, respectively. The tyre that the model begins the race on is chosen by the Monte Carlo race simulator. Furthermore, the number of times each action can be selected during a race simulation is limited, since the available number of each tyre compound is specified in the F1 regulations.\nReward Function. Rewards of 1 are given for normal steps where the model progresses to the next lap. This can be done by not pitting, or by taking the first valid pitstop (i.e. a pitstop to a different tyre, a requirement in F1). A penalty of -10 is given for extraneous pitstops beyond the first valid pitstop. A terminal reward is also given based on the final finishing position which equates to 100 times the points the driver would receive in F1 for finishing in that position, where the points for P1 to P10 are 25, 18, 15, 12, 10, 8, 6,\nThe reward function at any timestep t, \\(R^{t}\\), is formally defined below, where \\(P = [25, 18, 15, 12, 10, 8, 6, 4, 2, 1]\\):\n\\begin{equation*}\n\\begin{aligned}\nR^{t} =\\begin{cases}\n-1000 & \\text{if } a^{t} = \\text{pit soft } \\land \\neg s_{\\text{soft}}\\\\\n-1000 & \\text{if } a^{t} = \\text{pit medium } \\land \\neg s_{\\text{medium}}\\\\\n-1000 & \\text{if } a^{t} = \\text{pit hard } \\land \\neg s_{\\text{hard}}\\\\\n-1000 & \\text{if } \\neg S_{\\text{terminal}} \\land \\neg s_{\\text{of}}\\\\\n-10  & \\text{if } a^{t} \\neq \\text{ no pit } > s_{\\text{of}}\\\\\n100 * P[s_{\\text{pos}} - 1] & \\text{if } s_{\\text{pos}} \\leq 10 \\land S_{\\text{terminal}}\\\\\n0   & \\text{if } s_{\\text{pos}} > 10 \\land S_{\\text{terminal}}\\\\\n1   & \\text{otherwise}\n\\end{cases}\n\\end{aligned}\n\\end{equation*}\nIn developing RSRL, we considered various architectures. The original DQN was motivated by Boettinger and Klotz's [5] approach. However, this model does not effectively capture the nature of partial observability whereby the agent only receives partial observations of the underlying state space. Consequently, we decided to utilise the DRQN architecture as outlined by Hausknecht and Stone [9]. We motivate this architecture through its ability to capture temporal dynamics which are important in this setting. For example, the model needs to understand whether it is catching the car ahead or falling behind it. This same issue motivated Heilmeier et al. [11] to introduce new features representing these dynamics."}, {"title": "3.2 System Architecture", "content": "The design of the system architecture lends itself well to many features and is illustrated in Figure 1. Central to the training loop is the black-box Monte Carlo race simulator. Since it handles proprietary data definitions, it is important to abstract our state and action space away from these definitions. By defining a custom UnifiedRaceState and UnifiedRaceStrategy classes with a translator layer in between, the model maintains its interoperability with the race simulator and decouples RSRL from the black-box. Furthermore, the translator layer improves portability, allowing for the use of different data sources such as live race data or the F1 23 game. By creating a similar translator layer to convert live race data into the UnifiedRaceState, the model can be trained using the Monte Carlo race simulator, but deployed during a race. Such a method is currently not employed in industry, which defaults to the hard-coded race strategies. Portability with different data sources also enables the ability to refine the model using different information. For example, the model can be fine-tuned using the Driver-In-The-Loop Simulator to produce feasible strategies depending on a driver's racing style. The definition of the UnifiedRaceStrategy class also allows us to flexibly change the definition of a 'race strategy' in the future, without significantly changing existing components. This"}, {"title": "4 Evaluation", "content": "In this section, we evaluate the performance of RSRL, analyse how well it generalises and examine the fidelity and comprehensibility of the implemented XAI techniques."}, {"title": "4.1 Model Performance", "content": "To test the performance of a trained RSRL model, we take random seeds and run identical Monte Carlo simulations for RSRL and two baselines, namely a Fixed Strategy baseline and the industry state-of-the-art (SOTA) model provided by Mercedes, and compare their results. To begin with, we start with one track for both training and testing: Bahrain 2023 with a driver with a pace equivalent to an expected finish of P5.5 (i.e. in the third fastest team for Bahrain).\nThe Fixed Strategy model randomly employs one of the possible strategies defined on the F1 website for the 2023 Bahrain Grand"}, {"title": "4.2 Generalisability", "content": "To understand how well RSRL performs when trained across a different number of tracks, we trained three separate instances on 1, 2 and 9 tracks, named RSRL-1, RSRL-2 and RSRL-9, respectively. The training tracks represent a variety of expected strategies and exhibit diverse track characteristics. To determine how well these"}, {"title": "4.3 Explanations", "content": "To evaluate the efficacy of the explanations, we undertake (high-level) assessments of their fidelity, i.e. how closely the explanations replicate the models, and comprehensibility, i.e. how comprehensible the explanations are to users. To demonstrate the use of the explanations in practice, we also exemplify how each of the explanations highlights the motivations behind the decisions of RSRL (from Section 4.1) in a race simulation. To do so, we analyse RSRL's decision of no pit on lap 10 of the 2023 Bahrain Grand Prix.\nTimeSHAP Feature Importance. To determine the fidelity of the feature importance, we measure the Mean Absolute Error (MAE) for 100 timesteps across 10 simulations. RSRL achieved an average error of 124.39. Compared to the maximum reward of 2500, this represents a 5% normalised MAE, demonstrating that the importance values are accurate and consistent with the actual rewards, thus reducing the likelihood of large errors when predicting the feature importance.\nFeature importance is easy to comprehend, whereby larger bars are considered more important to a model's predictions. Even without explicit SHAP values, it is trivial to understand the importance of each feature to the other features [13]."}, {"title": "VIPER Decision Tree Surrogate Model", "content": "The fidelity of the VIPER decision tree surrogate model is demonstrated with a confusion matrix generated from 100 random simulations, seen in Table 4. We note that the accuracy is 0.926, however, there are considerably more no pit decisions being taken through the 100 simulations. The F1 score of 0.910 represents a good balance between precision and recall, indicating that RSRL is good at identifying both positive and negative cases for each class. Furthermore, Figure 5 depicts the change in testing accuracy of the VIPER model over 50 iterations during training. A peak testing accuracy of 0.97 is achieved against the oracle, supporting the strong fidelity of the model.\nTrained decision trees vary in comprehensibility depending on their depth. In [2], the authors generate trees with hundreds of nodes which becomes very difficult to comprehend. From depth testing the tree, we found that a maximum depth of 4 to 6 maintains good accuracy and looks to be sufficiently comprehensible. Figure 6 demonstrates how we reach a plateau around this range. However, with a larger input state space, this value is likely to increase as the tree has to split the dataset even further to achieve high accuracy.\nLooking at the decision path for the no pit decision on lap 10 of the 2023 Bahrain Grand Prix (Table 5), we see that predictions are influenced by the tyres, race progress and the gaps. Similarly to the feature importance, we see that the race progress being less than 26% (15 laps) is an important factor. The first node decision on the current fitted tyre indicates how the model has different strategies based on the starting tyre, which is randomly chosen by the race simulator. The last lap to reference, \\(s_{llr}\\) feature value being \u2264 1.016 indicates that the decision tree is also looking at the"}, {"title": "Decision Tree Counterfactuals", "content": "To measure the fidelity of counterfactuals, we examine the proximity of generated counterfactuals to the original instance by counting the number of feature changes required to reach alternate decisions. By changing fewer features, the counterfactual demonstrates high fidelity because it suggests that the counterfactual closely aligns with the model's decision boundaries and feature importance. Taking 100 random simulations, and computing the closest counterfactual for a random alternate action, the average number of features requiring changing for the closest counterfactual was 1.630 whilst the average distance was 0.069. These low figures represent how these counterfactuals are sufficiently close to the real prediction, only requiring small but critical changes to result in a different action. This is in line with Molnar and Dandl's [7] criteria for good counterfactuals, whereby they must be diverse and contain likely features.\nDecision Tree Counterfactuals are presented in a textual format, listing exactly what changes are required to predict the alternate"}, {"title": "5 Conclusions and Future Work", "content": "We have introduced RSRL, an RL model for F1 Race Strategy. RSRL achieves an average finishing position of P5.33, outperforming the Fixed Strategy Model's P5.63 and Mercedes' SOTA model's P5.86 at the 2023 Bahrain Grand Prix. Generalisability testing highlighted how when more tracks are used in training, the model performs worse on seen tracks due to a lack of state space exploration, but better on unseen tracks. Finally, we concluded that the explainability methods support RSRL's predictions and help to explain its decisions, as evident through our examples. Drawing parallels between the simulations and real-world phenomena with these explanations indicates how these models are deployable in practice. This work also extends beyond F1, providing a framework for implementing portable and extendable RL systems through an abstraction architecture and deploying methods for explaining RSRL's decisions in real-time strategy applications.\nThis work opens up numerous avenues for future work, not least controlling multiple cars to explore cooperative strategies presents an interesting venture towards discovering new strategies. Additionally, we would like to to improve strategy predictions based on a driver's driving style by fine-tuning models to suggest tailored and personalised strategies. Finally, given that we have shown what can be achieved in learning these strategies from scratch, we would like to improve this process by incorporating human-in-the-loop feedback processes to utilise race strategists' priceless expertise."}]}