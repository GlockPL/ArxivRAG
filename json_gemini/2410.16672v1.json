{"title": "DEAN: DEACTIVATING THE COUPLED NEURONS TO MITIGATE FAIRNESS-PRIVACY CONFLICTS IN LARGE LANGUAGE MODELS", "authors": ["Chen Qian", "Dongrui Liu", "Jie Zhang", "Yong Liu", "Jing Shao"], "abstract": "Ensuring awareness of fairness and privacy in Large Language Models (LLMs) is critical. Interestingly, we discover a counter-intuitive trade-off phenomenon that enhancing an LLM's privacy awareness through Supervised Fine-Tuning (SFT) methods significantly decreases its fairness awareness with thousands of samples. To address this issue, inspired by the information theory, we introduce a training-free method to DEActivate the fairness and privacy coupled Neurons (DEAN), which theoretically and empirically decrease the mutual information between fairness and privacy awareness. Extensive experimental results demonstrate that DEAN eliminates the trade-off phenomenon and significantly improves LLMs' fairness and privacy awareness simultaneously, e.g., improving Qwen-2-7B-Instruct's fairness awareness by 12.2% and privacy awareness by 14.0%. More crucially, DEAN remains robust and effective with limited annotated data or even when only malicious fine-tuning data is available, whereas SFT methods may fail to perform properly in such scenarios. We hope this study provides valuable insights into concurrently addressing fairness and privacy concerns in LLMs and can be integrated into comprehensive frameworks to develop more ethical and responsible Al systems. Our code is available at https://github.com/ChnQ/DEAN.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, as LLMs increasingly permeate sensitive areas such as healthcare, finance, and education (Li et al., 2023b; Yuan et al., 2023; Al-Smadi, 2023), concerns regarding their fairness and privacy implications have become critically important (Liu et al., 2023; Sun et al., 2024a). For instance, when queried for sensitive information such as a social security number, we would expect the LLM to refuse to provide such information. Similarly, a desirable LLM should avoid producing unfair or discriminatory content, as shown in Figure 1(a).\nIn this paper, we focus on LLMs' awareness of fairness and privacy concerns, i.e., their ability to recognize and appropriately respond to requests involving sensitive information (Li et al., 2024; Sun et al., 2024a). A well-recognized challenge is the trade-off between addressing fairness and privacy-related concerns (Bagdasaryan et al., 2019; Mangold et al., 2023; Agarwal, 2021) in traditional Deep Neural Networks (DNNs). As a result, many studies have emerged attempting to reconcile this trade-off, proposing techniques to balance these conflicting objectives (Lyu et al., 2020; Cummings et al., 2019). This prompts us to explore an intriguing question: Does trade-off also exist between the awareness of fairness and privacy in the era of LLMs?\nInterestingly, our preliminary experimental results indicate that enhancing privacy awareness through SFT methods decreases the fairness awareness of LLMs, as shown in Figure 1(b). Specifically, we fine-tune LLMs on limited-data conditions (thousands of samples) with Full-parameter Fine-Tuning (FFT) (Devlin et al., 2019) and Parameter-Efficient Fine-Tuning (PEFT) methods (Hu et al., 2022; Liu et al., 2024b; Wu et al., 2024), due to challenges in acquiring large volumes of high-quality fine-tuning data in real-world scenarios (Xu et al., 2024; Sun et al., 2024b). Please see Appendix C for more discussions. Such a trade-off phenomenon can be partially explained by the neuron semantic superposition (Elhage et al., 2022; Bricken et al., 2023; Templeton, 2024), i.e., neurons are polysemantic and exist a subset of neurons closely related with both fairness and privacy awareness. In this way, fine-tuning LLMs inadvertently affects these coupled neurons and may introduce a conflicting optimization direction for fairness and privacy, leading to the trade-off phenomenon. Therefore, an effective operation for decoupling fairness and privacy-related neurons is likely to mitigate the above trade-off phenomenon.\nInspired by the information theory (Ash, 2012; Yang & Zwolinski, 2001) that removing the common components of two variables can reduce their mutual information and thus decouple these variables, we propose a simple and effective method, namely DEAN, to decouple LLMs' awareness of fairness and privacy by DEActivating the coupled Neurons (Figure 1(c)). Specifically, we first identify a sparse set of neurons closely related to fairness and privacy awareness, respectively. Then, the intersection of these two sets of neurons can be considered as coupled neurons. In this way, deactivating these coupled neurons decouples the awareness of fairness and privacy, i.e., decreasing the mutual information between fairness-related and privacy-related representations. The decreasing mutual information potentially mitigates the trade-off phenomenon.\nExtensive experimental results demonstrate the advantages of training-free DEAN. Firstly, DEAN can simultaneously improve both fairness and privacy awareness of the LLM without compromising the LLM's general capabilities, e.g., improving the Qwen2-7B-Instruct's (Yang et al., 2024a) fairness awareness by 12.2% and privacy awareness by 14.0%. Secondly, training-free DEAN performs effectively under limited annotated data conditions, e.g., a few hundred data samples, thereby reducing the reliance on extensive annotation and computational resources.\nNotably, DEAN maintains strong performance even when only malicious fine-tuning data (e.g., unfair queries with unfair responses) is available, whereas previous studies (Qi et al., 2024; Yang et al., 2024b; Halawi et al., 2024) have shown that using such data for fine-tuning could significantly degrade model performance. These effectivenesses are attributed to the focus on identifying and deactivating relevant neurons rather than directing the model to learn from the dialogue data via fine-tuning, which also enjoys better interpretability. We do not expect that DEAN alone can fully address fairness and privacy concerns in LLMs without FFT and SFT methods. In contrast, we consider that DEAN can be flexibly integrated into a comprehensive framework to further contribute to the development of more ethical and responsible AI systems in the era of LLMs."}, {"title": "2 RELATED WORK", "content": "Fairness and privacy-related concerns in DNNs. The concerns surrounding fairness and privacy in deep neural networks (DNNs) have garnered significant attention in recent years (Mehrabi et al., 2021; Caton & Haas, 2024; Mireshghallah et al., 2020; Liu et al., 2020). Fairness research spans various topics (Verma & Rubin, 2018), including but not limited to individual fairness (Dwork et al., 2012; Kusner et al., 2017), which emphasizes treating similar individuals similarly; and group fairness (Dwork et al., 2012; Kusner et al., 2017), which aims to ensure that different demographic groups receive equal treatment. In privacy, topics such as differential privacy (Dwork et al., 2006; Mireshghallah et al., 2020), which ensures that the removal or addition of a single individual's data does not significantly affect the output of the model; and membership inference resistance (Shokri et al., 2017; Mireshghallah et al., 2020), which prevents attackers from determining whether a particular data instance was part of the training set, are widely explored. While traditional DNNs are primarily designed for discriminative tasks, LLMs focus more on open-ended generative scenarios in various real-world applications, which shifts the emphasis on fairness and privacy concerns. As mentioned before, we emphasize LLMs' awareness of fairness and privacy, where a more formal definition can be found in Section 4.\nPEFT methods for LLMs. PEFT aims to reduce the expensive fine-tuning cost of LLMs by updating a small fraction of parameters. Existing PEFT methods can be roughly classified into three categories. The first category is Adapter-based methods, which introduce new trainable modules (e.g., fully-connected layers) into the original frozen DNN (Houlsby et al., 2019; Karimi Mahabadi et al., 2021; mahabadi et al., 2021; Hyeon-Woo et al., 2022). The second category is Prompt-based methods, which add new soft tokens to the input as the prefix and train these tokens' embedding (Lester et al., 2021; Razdaibiedina et al., 2023). LoRA-based methods (Hu et al., 2022; Zhang et al., 2023; Liu et al., 2024b; Renduchintala et al., 2023) are the third category of PEFT. LoRA-based methods utilize low-rank matrices to represent and approximate the weight changes during the fine-tuning process. Prior to the inference process, low-rank matrics can be merged into the original model without bringing extra computation costs. In this study, we discover that PEFT methods lead to the trade-off phenomenon between the awareness of fairness and privacy in LLMs.\nIdentifying task-related regions in LLMs. Attributing and locating task-related regions in DNNs is a classic research direction in explainable artificial intelligence (Tjoa & Guan, 2020; Liu et al., 2024a; Ren et al., 2024). Previous studies aim to interpret and control DNNs, by identifying task-specific regions and neurons. Springenberg et al. (2015); Sundararajan et al. (2017); Shrikumar et al. (2017); Michel et al. (2019); Maini et al. (2023); Wang et al. (2023a); Wei et al. (2024); Liu et al. (2024c) measure the importance score for weights in DNNs based on back-propagation gradients. Probing-based methods are another perspective for identifying the layers and regions, where the task-related knowledge is encoded in LLMs (Adi et al., 2016; Hewitt & Liang, 2019; Zou et al., 2023). Specifically, training a probe classifier based on the model's feature representations on some task-related samples, including truthfulness (Li et al., 2023a; Qian et al., 2024), toxicity (Lee et al., 2024), and knowledge Burns et al. (2023); Todd et al. (2023) in LLMs."}, {"title": "3 METHOD: DEACTIVATING THE COUPLED NEURONS TO MITIGATE FAIRNESS-PRIVACY CONFLICTS", "content": "As demonstrated in Figure 1(b), common SFT techniques tend to introduce a trade-off between LLMs' awareness of fairness and privacy. In this section, we propose our training-free method DEAN for addressing the trade-off issue. We begin by establishing the theoretical foundation based on information theory (3.1), followed by a detailed description of our proposed DEAN (3.2). Finally, we provide experimental analysis to verify that DEAN achieves the expected outcomes derived from the theoretical foundation (3.3)."}, {"title": "3.1 INSPIRATION FROM INFORMATION THEORY", "content": "As discussed in Section 1, one potential explanation for the trade-off between LLMs' awareness of fairness and privacy is the neuron semantic superposition hypothesis (Elhage et al., 2022; Bricken et al., 2023; Templeton, 2024). This means that certain neurons may simultaneously contribute to both fairness-related and privacy-related representations. Therefore, fine-tuning LLMs may leads to conflicting optimization directions in these coupled representation spaces, causing the observed trade-off phenomenon. To understand the interplay between fairness and privacy-related representations in LLMs, we first leverage concepts from information theory, particularly focusing on mutual information between different representations."}, {"title": "Theorem 1", "content": "Let X, Y, and Z be random variables, then we have:\n$I[X; Y] \\le I[(X, Z); (Y, Z)]$,\nwhere $I[X; Y]$ denotes the mutual information between variables X and Y, and $I[(X, Z); (Y, Z)]$ denotes the mutual information between the joint variables (X, Z) and (Y, Z)."}, {"title": "Remark 1", "content": "Theorem 1 indicates that the presence of coupled variable Z contributes to a larger mutual information between X and Y, i.e., $I[X; Y] < I[(X, Z); (Y, Z)]$. In this way, deactivating and eliminating the coupled variable Z decreases the mutual information between (X, Z) and (Y, Z). In the context of this study, let (X, Z) and (Y, Z) denote the fairness-related and privacy-related representations in the original LLM, respectively. Therefore, deactivating or eliminating Z can potentially decouple X and Y, i.e., decreasing $I[X; Y]$. Building on this insight, we have the following proposition with respect to the LLM's application."}, {"title": "Proposition 1", "content": "Let $\\psi(\\cdot)$ denote the representation extraction function of original LLM, and $\\phi(\\cdot)$ denote the representation extraction function of LLM where fairness and privacy-related representations are decoupled. Let $Q_b$ and $Q_p$ represent query sets related to fairness and privacy awareness, respectively. For specific queries $q_u \\in Q_b$ and $q_p \\in Q_p$, we have:\n$I[E_{q_b \\sim Q_b}\\phi(q_b); E_{q_p \\sim Q_p}\\phi(q_p)] \\le I[E_{q_b \\sim Q_b}\\psi(q_b); E_{q_p \\sim Q_p}\\psi(q_p)]$.\nRemark 2. Proposition 1 indicates that by removing representations associated with both fairness and privacy (i.e., modify $\\psi(\\cdot)$ to obtain the $\\phi(\\cdot)$), the mutual information between fairness and privacy representations would reduce, thereby potentially facilitating their decoupling to mitigate the trade-off. In practical terms, we can achieve this goal by identifying and deactivating the neurons that contribute to both fairness-related and privacy-related representations, thereby reducing the coupled information."}, {"title": "3.2 DECOUPLING FAIRNESS AND PRIVACY VIA NEURON DEACTIVATION", "content": "Building on the theoretical insights, we propose a method for decoupling the awareness of fairness and privacy in LLMs: deactivating neurons associated with both fairness and privacy semantics. Specifically, we first identify neurons related to fairness and privacy semantics, then deactivate those neurons that are coupled across both representations.\nComputing importance scores for neurons. We begin with an activation dataset D, where each data sample s consists of a query-response pair (xquery, yanswer). Let $W_{module}$ denote the weight matrix corresponding to a specific target module (e.g., Multi-Head Attention (MHA) or Multi-Layer Perceptron (MLP)) within the layer l of the LLM. For simplicity, we omit layer and module subscripts in the subsequent discussion. Then the importance score matrix $I_W$ for the weight matrix W is computed as follows (Michel et al., 2019; Wang et al., 2023a; Wei et al., 2024):\n$I_W = E_{s \\sim D} |W \\odot \\nabla_W L(s)|$.\nHere, $L(s) = -log p(y_{answer} | x_{query})$ represents the negative log-likelihood loss in generative settings, and $\\odot$ denotes the Hadamard product. For a neuron located at the i-th row and j-th column of W, the importance score\n$I_{W(i, j)} = E_{s \\sim D} |W_{(i,j)} \\nabla_{W(i,j)} L(s)|$\nserves as a first-order Taylor approximation of the change in the loss function when $W(i, j)$ is set to zero (Wei et al., 2024). Intuitively, the magnitude of $I_{W (i, j)}$ reflects the relative importance of the neuron with respect to the dataset D. That is, a larger value of $I_{W}(i, j)$ indicates that the neuron at this position has a stronger association with the dataset D. In practice, we compute $I_W$ by taking the expectation over the activation dataset D following Michel et al. (2019); Wei et al. (2024). The computation of these importance scores serves as a foundation for the subsequent processes of locating and deactivating relevant neurons.\nLocating the Coupled Neurons. Given activation datasets $D_f$ and $D_p$ related to fairness and privacy awareness, respectively, we perform the following steps to locate fairness and privacy coupled neurons within a specific layer and functional module. First, we compute the corresponding importance score matrices $I_W^f$ and $I_W^p$ based on Eq. (3). For example, larger values in $I_W^f$ indicate that the corresponding neurons are more closely related to fairness awareness. Thus, the method for locating the fairness and privacy coupled neurons is intuitive: if a neuron at a specific position (i, j) has both high $I_W^f (i, j)$ and high $I_W^p(i, j)$, we consider it a coupled neuron. Specifically, to allow for computational flexibility, we select the top-r fraction of neurons based on the importance score matrices $I_W^f$ and $I_W^p$ to form the neuron subsets $N_b$ and $N_f$, respectively, where $r \\in (0, 1]$ denotes the extraction ratio. Then, we compute the set of coupled neurons $N_{coupled} = N_b \\cap N_f$. Note that to avoid degrading the model's general performance, we further remove neurons in $N_{coupled}$ that are related to general model capabilities, drawing insights from (Wei et al., 2024).\nDeactivating the Coupled Neurons. Once the coupled neurons $N_{coupled}$ are identified, we proceed to deactivate them as discussed in Remark 2. Specifically, the deactivation is performed by setting the corresponding weights of these neurons to zero (Wei et al., 2024; Liu et al., 2024c). In this way, the operation effectively removes the influence of these neurons during the model's inference process, helping to reduce the mutual information between fairness representations and privacy representations (verified in Section 3.3).\nThe above procedure is summarized in Algorithm 1. By default, this procedure is applied to all layers and modules within the LLM (more detailed ablation studies are provided in Section 4.4). Extensive experimental results in Section 4 demonstrate that such operation effectively alleviates the trade-off between LLM's fairness awareness and privacy awareness."}, {"title": "3.3 DEAN REDUCES THE MUTUAL INFORMATION BETWEEN FAIRNESS-RELATED AND PRIVACY-RELATED REPRESENTATIONS", "content": "Recalling in Section 3.1, we propose that identifying and deactivating coupled neurons (i.e., the proposed DEAN) could decrease the mutual information between fairness-related representations and privacy-related representations (Proposition 1 and Remark 2). In this subsection, we aim to verify that DEAN achieves the goal of Proposition 1.\nExperimental setup. We conduct experiments to compare the mutual information between fairness-related and privacy-related representations in the final layer of LLMs, both before and after applying DEAN. Specifically, we use subsets of fairness and privacy-related questions (see Section 4.1 for details) from Salad-bench (Li et al., 2024) as inputs to the LLMs to extract the corresponding representations. We focus on the last layer due to higher layers typically containing more semantic information (Zou et al., 2023; Rimsky et al., 2024) and being closest to the final text output. Our experiments include several representative LLMs, i.e., Qwen2-7B-Instruct (Yang et al., 2024a), Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), Llama2-7B-Chat (Touvron et al., 2023), and Vicuna-7B-v1.5 (Chiang et al., 2023). Following Ma et al. (2020); Qian et al. (2024), we employ HSIC (Gretton et al., 2005) (please see Definition 1, and we discuss the practical implementation of HSIC in Appendix B.) to estimate mutual information due to the challenges associated with accurate computation in high dimensions (Kraskov et al., 2004; Poole et al., 2019)."}, {"title": "Definition 1", "content": "HSIC is the Hilbert-Schmidt norm of the cross-covariance operator between the distributions in Reproducing Kernel Hilbert Space (RKHS). Formally, HSIC(X, Y) is defined as:\n$HSIC(X, Y) = E_{xyx'y'} [k_x(X, X') k_y(Y, Y')] + E_{xx'} [k_x(X, X')] E_{yy'} [k_y(Y, Y')] - 2E_{xy} [E_{x'} [k_x(X, X')] E_{y'} [k_y(Y, Y')]]$,\nwhere X', Y' are independent copies of X, Y, respectively, and $k_x$, $k_y$ are kernel functions.\nExperimental results. Figure 2 indicates that applying DEAN decreases mutual information between fairness-related and privacy-related representations across all four models. This decrease suggests that DEAN effectively decouples fairness awareness and privacy awareness at the representation level, thereby validating Proposition 1. In following Section 4, extensive experiments will validate that such a decrease in mutual information could help mitigate the trade-off between fairness awareness and privacy awareness in LLMs."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct comprehensive experiments to validate the effectiveness of DEAN. We first introduce the experimental setup (4.1), then showing DEAN's main results in mitigating the trade-off between LLMs' awareness of fairness and privacy (4.2). Next, we further examine the characteristics of DEAN through case studies (4.3), and finally present the ablation studies (4.4)."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Datasets. To identify the coupled neurons in LLMs (Section 3.2) and to fine-tune LLMs, we require datasets in the (query, answer) format. For fairness awareness and privacy awareness datasets, we utilize the open-source preference dataset BeaverTails (Ji et al., 2023) to extract training samples via sensitive phrase matching (Wang et al., 2023b; Qi et al., 2024). For general capabilities datasets, we follow Qi et al. (2024); Wei et al. (2024) to adopt the refined version of the Alpaca (Taori et al., 2023) dataset. Further details regarding these datasets are provided in Appendix B.\nModels. To evaluate the effectiveness and generalization ability of DEAN, we conduct experiments on three representative model families, specifically including Qwen2 model series (Yang et al., 2024a), Mistral-v0.2 model series (Jiang et al., 2023), Vicuna model series (Chiang et al., 2023), and Llama2 model series (Touvron et al., 2023).\nBaselines. To validate the effectiveness of DEAN, we compare it with following baselines: FFT (Devlin et al., 2019), where all model parameters are updated for maximum adaptability, though at the cost of significant computational resources; LoRA (Hu et al., 2022), which only updates low-rank matrices while freezing the large fraction of model parameters for efficiency; DoRA (Liu et al., 2024b), which decomposes model weights into magnitude and direction, and updates only the directional component in LoRA to enhance learning capacity; ReFT (Wu et al., 2024), a representation-based fine-tuning approach that applies task-specific interventions on hidden representations instead of updating model weights. Recalling the experimental results in Figure 1(b), employing SFTs method to enhance the LLM's awareness of privacy leads to a significant decrease in model's fairness awareness. To mitigate this trade-off, we incorporate an equal amount of fairness awareness data into the fine-tuning dataset for these SFT methods. More details are provided in Appendix B."}, {"title": "Evaluation.", "content": "1) Awareness of fairness and privacy. As discussed in the introduction, we focus on LLM's awareness of fairness and privacy under the generative scenarios. Here, we formalize the evaluation process as follows."}, {"title": "Evaluating LLMs' awareness of fairness and privacy", "content": "1. Input Space Definition: Let Q represent the input space of all possible queries, and let A represent the output space of all possible responses generated by the LLM.\n2. Evaluation Function: Define an evaluation function $g : Q \u00d7 A \\rightarrow \\{0,1\\}$, where $g(q, a) = 1$ if the response a to query q demonstrates that the LLM is aware of fairness or privacy issues and provides an appropriate response. Otherwise, $g(q, a) = 0$.\n3. Performance Metric: For a given query set $Q \u2286 Q$ and its corresponding response set $A \u2286 A$, we define the awareness ratio $r_a = \\frac{\\sum_{(q, a) \\in (Q, A)} g(q,a)}{\\left| Q \\right|}$, which measures the proportion of responses that demonstrate awareness and provide appropriate handling of fairness or privacy-related issues. A higher $r_a$ indicates a greater level of awareness by the LLM regarding fairness and privacy issues.\nSpecifically, we conduct our evaluation using Salad-bench (Li et al., 2024), a safety benchmark specifically designed to evaluate LLMs in generative tasks. From Salad-bench, we extract query subsets under the predefined categories of \u201cunfair representation\u201d and \u201cprivacy infringement\u201d to construct fairness awareness query set $Q_f$ and privacy awareness query set $Q_p$, respectively. We then employ MD-judge (Li et al., 2024) as the evaluator g to assess the LLM's generated responses regarding $Q_f$ and $Q_p$. 2) General capabilities. To evaluate LLMs' general capabilities, we select several widely used benchmark, i.e., HellaSwag (Zellers et al., 2019), Race (Lai et al., 2017), MMLU (Hendrycks et al., 2021), GPQA (Rein et al., 2024), OpenBookQA (Mihaylov et al., 2018), BoolQ (Clark et al., 2019), and Perplexity (Chen et al., 1998). We utilize the lm-evaluation-harness library (Gao et al., 2023) with default evaluation settings to conduct the evaluation."}, {"title": "4.2 MAIN RESULTS", "content": "DEAN enhances LLM's awareness of fairness and privacy simultaneously without compromising general capabilities. Table 1 demonstrates that DEAN significantly improves the LLM's awareness of both fairness and privacy across all four model families. In contrast, the SFT methods often demonstrate a tradeoff between these two aspects, i.e., models typically show a tendency to enhance privacy awareness while experiencing a notable decline in fairness awareness. In Llama2-7B-Chat, we observe a decrease in both fairness and privacy awareness with SFT methods, which may be caused by the conflicts in model's internal optimization. Additionally, as indicated by the general capabilities performance in Table 3, DEAN effectively maintains LLMs' general capabilities. These characteristics highlight DEAN's potential in real-world scenarios, especially in fields like healthcare and finance, where it is crucial to balance the LLMs' awareness of fairness and privacy.\nDEAN maintains its effectiveness across multiple LLM sizes. While Table 1 primarily explores DEAN's performance on 7B-parameter LLMs, we further validate its generalization capability by conducting experiments on three models of different parameter scales, i.e., Qwen2-0.5B-Instruct, Qwen2-1.5B-Instruct, and Llama2-13B-Chat. Table 2 shows that, when applied to LLMs of varying sizes, DEAN can still significantly enhance models' awareness of both fairness and privacy."}, {"title": "4.3 CASE STUDY", "content": "DEAN remains robust even when only malicious fine-tuning data is available. Typically, enhancing the performance of an LLM in specific domains requires helpful fine-tuning with data relevant to the target task. For instance, to improve an LLM's awareness of fairness, we often need helpful fine-tuning data in the form of unfair query + fair response. In contrast, using malicious fine-tuning data (e.g., unfair query + unfair response) for model training can potentially degrade the model's capabilities (Qi et al., 2024; Yang et al., 2024b; Halawi et al., 2024). Then, how does DEAN perform when using malicious fine-tuning data? Interestingly, Figure 3 shows that across three test LLMs, DEAN consistently enhances both fairness and privacy awareness even with malicious fine-tuning data. We analyze that this robustness stems from DEAN's reliance on the data to identify \"coupled\" neurons, rather than requiring training the model to learn to follow the dialogues within the data. Consequently, DEAN maintains robustness against variations in the form of fine-tuning data. This highlights DEAN's strength in improving LLM performance without necessitating carefully curated training data, thereby minimizing the risk of degrading fairness and privacy awareness under data scarce scenarios."}, {"title": "DEAN remains robust when the data size is reduced.", "content": "In Figure 4, we investigate the effects of decreasing the dataset size on the performance of DEAN and several training-based methods. As shown in Figure 4, DEAN consistently maintains stable performance as the dataset size decreases, consistently enhancing the model's awareness of both fairness and privacy. In comparison, SFT methods still exhibit a trade-off between fairness and privacy awareness. Specifically, when fine-tuning data is severely limited, such as in scenarios with only 100 data samples, both fairness and privacy awareness are compromised. Interestingly, we also observe that as the dataset size decreases under the SFT methods, the model's awareness of fairness tends to increase, while its awareness of privacy shows a general decline, which further dynamically illustrates the trade-off between these two aspects. We leave the more in-depth analysis of this phenomenon for future work.\nDEAN encourages the model to produce more cautionary language related to fairness and privacy. To further analyze how our approach enhances the model's awareness of fairness and privacy in generative tasks, we first identify a set of key words or phrases based on sensitive phrase matching (Wang et al., 2023b; Qi et al., 2024), which are closely associated with a heightened awareness of these issues. We then measure the frequency of these terms in model responses before and after applying DEAN. In Figure 5, we compare the normalized frequency of fairness-related (left) and privacy-related (right) words or phrases in responses from the original model and the model applying DEAN, revealing distinct patterns in language use. Specifically, Figure 5 shows that\n\u2022 The model applying DEAN tends to employ more disclaimers and cautionary expressions, such as \"I'm sorry\" and \"I cannot,\u201d across both fairness and privacy evaluation scenarios, indicating a stronger focus on avoiding potential issues.\n\u2022 For fairness, the model applying DEAN emphasizes terms like \u201cdiverse,\u201d \u201call individuals,\u201d and \u201cis a stereotype\" more frequently than the original model, reflecting greater attention to fairness and diversity.\n\u2022 Similarly, in the privacy-related analysis, the model applying DEAN shows a significant increase in the use of phrases such as \"respect people's privacy,\" \"not have access to,\u201d and \u201cpersonal information,\" underscoring its commitment to privacy protection."}, {"title": "4.4 ABLATION STUDY", "content": "In this subsection, we investigate how changes in the extraction ratio and the choice of target modules (Section 3.2) affect DEAN's performance in terms of LLMs' fairness awareness, privacy awareness, and general capabilities. Specifically, we vary the extraction ratio within the range of ($1 \u00d7 10^{-7}, 1 \u00d7 10^{-3}$) and select MHA, MLP, and ALL (both MHA and MLP) as the target modules. From Figure 6, we can obtain the following observations.\nPerformance degradation with increasing extraction ratio. When the target module is set to either ALL or MLP, an increasing extraction ratio generally leads to a decline in performance across all three capacities. However, we observe a slight performance improvement when the extraction ratio increased within the range of $1 \u00d7 10^{-7}$ to $1 \u00d7 10^{-6}$. We hypothesize that this initial improvement may be due to a more precise deactivation of the target neurons when the extraction ratio is small. As the extraction ratio continues to increase beyond this range, the introduction of significant noise from deactivating more neurons may inadvertently disrupt neurons crucial for essential functionalities, leading to the overall performance decline.\nPerformance stability with MHA module. Interestingly, when the target module is set to MHA, the model's performance of three tasks remains relatively stable across varying extraction ratios. Moreover, the impact on fairness and privacy awareness is negligible. This observation suggests that neurons associated with fairness and privacy awareness are predominantly encoded within MLP modules. This observation aligns with previous studies (Geva et al., 2021; Dai et al., 2022; Meng et al., 2022; Luo & Specia, 2024), which indicate that the MLP modules in transformer-based language models are more focused on the storage and processing of knowledge.\nBased on these observations, we conclude that for practical applications, selecting ALL or MLP as the target module and setting a lower extraction ratio can help achieve a desirable model, i.e., maintaining general capabilities while simultaneously enhancing awareness of fairness and privacy. We also hope that our work will encourage further fine-grained exploration of the target modules, thereby contributing to a deeper understanding of LLMs."}, {"title": "5 CONCLUSION", "content": "In this work, we introduce a training-free method DEAN to mitigate the trade-off between fairness and privacy awareness that arises in SFT methods. Building on theoretical insights from information theory, DEAN deactivates the coupled neurons responsible for both fairness and privacy in LLMs. Extensive experiments demonstrate that DEAN effectively mitigates the trade-off, leading to simultaneous enhancements in both fairness and privacy awareness of LLMs. Notably, DEAN exhibits robust performance with limited annotated data or with only malicious fine-tuning data, whereas conventional SFT methods typically fail in these challenging scenarios. We expect that DEAN can be seamlessly integrated into broader frameworks, contributing to the development of more responsible and ethical AI systems. We hope this study provides meaningful insights into the simultaneous handling of fairness and privacy LLMs and inspires further related research."}, {"title": "ETHICS STATEMENT", "content": "This research focuses on mitigating the trade-off between fairness and privacy awareness in LLMs. The proposed DEAN is intended to enhance the ethical handling of fairness and privacy concerns in Al systems. Our experiments were conducted on publicly available benchmark datasets"}]}