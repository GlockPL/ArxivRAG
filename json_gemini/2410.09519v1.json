{"title": "Pic@Point: Cross-Modal Learning by Local and Global Point-Picture Correspondence", "authors": ["Vencia Herzog", "Stefan Suwelack"], "abstract": "Self-supervised pre-training has achieved remarkable success in NLP and 2D vision. However, these advances have yet to translate to 3D data. Techniques like masked reconstruction face inherent challenges on unstructured point clouds, while many contrastive learning tasks lack in complexity and informative value. In this paper, we present Pic@Point, an effective contrastive learning method based on structural 2D-3D correspondences. We leverage image cues rich in semantic and contextual knowledge to provide a guiding signal for point cloud representations at various abstraction levels. Our lightweight approach outperforms state-of-the-art pre-training methods on several 3D benchmarks.", "sections": [{"title": "1. Introduction", "content": "Point clouds are the preferred 3D representation for many applications, including autonomous driving, robotics, AR/VR, and various sensor technologies (LIDAR, SFM, Kinect Structured Light, etc). However, the annotation of point clouds is associated with high costs, and labeled 3D scans are scarce. This poses an obstacle to the development of robust, scalable deep learning models for point cloud analysis.\nRecent progress in self-supervised learning has led to significant advances in the areas of image recognition and natural language processing (NLP). By using parts of the input data itself as a guiding signal, self-supervised learning bypasses the annotation bottleneck associated with training large neural networks. This strategy reflects a shift from traditional task-specific training towards pre-training general-purpose representations that are applicable across a wide range of tasks. Extending these advances to 3D data remains challenging due to factors such as irregular information density and the unordered nature of point clouds.\nGenerative modeling has seen a number of adaptations to 3D data using transformer-style architectures (Yu et al., 2022; Pang et al., 2022; Zhang et al., 2022a). However, these approaches have yet to replace traditional architecture-based methods (Ma et al., 2022; Qian et al., 2022a). Two factors that contribute to the underperformance on point clouds are the lack of inductive biases in Standard Transformers and the use of point set similarity metrics (e.g., Chamfer Distance) in reconstruction, which are imprecise and hard to optimize (Wu et al., 2021; Huang et al., 2023).\nContrastive learning approaches, on the other hand, typically aim at learning invariances to transformations or augmentations. They have been shown to be highly competitive to generative modeling (Oord et al., 2018; Chen et al., 2020). However, the quality of learned representations is highly dependent on the complexity and informative value of the contrastive task (Goyal et al., 2019), and many contrastive models only consider global views, neglecting local relationships (Qi et al., 2023).\nTo address these limitations, we propose leveraging 2D-3D correspondences for contrastive representation learning on point clouds. We present the Pic@Point model, which aims to learn point cloud representations by exploiting structural features of pictures at various point cloud abstraction levels. Specifically, we extract 3D and 2D features at both global and local scales using a generic 3D backbone and a pre-trained 2D backbone, respectively. We then employ global and local, pose-conditioned projection heads to project these features into a common, representation-invariant latent space, as illustrated in Figure 1. This structural 2D-3D contrastive learning approach offers several advantages over existing methods:\n\u2022 We effectively leverage features from pre-trained vision foundation models. In contrast, generative cross-modal methods (Zhang and Hou, 2023; Wang et al., 2023) generate images from input point clouds using a custom 2D generator atop a 3D backbone, as opposed to integrating a powerful vision model.\n\u2022 In addition, our method is very lightweight while being highly effective, as it uses no decoder and employs a frozen 2D backbone. Figure 2 shows the size of different pre-training models in relation to linear accuracy on ModelNet40, showcasing the efficiency of our approach.\n\u2022 Unlike existing contrastive methods (Afham et al., 2022; Wu et al., 2023) which only learn global shape correspondences, Pic@Point provides guidance on a structural level. By exploiting local correspondences, it provides pose-aware, positional guidance while benefiting from a larger number of contrastive samples."}, {"title": "2. Related Work", "content": "Point Cloud Analysis Early efforts to apply neural networks to points clouds involved converting the input into structured forms such as voxel grids (Wu et al., 2015; Maturana and Scherer, 2015) or multi-view images (Su et al., 2015). Sparse voxel-based methods (Riegler et al., 2017; Choy et al., 2019) remain a prevalent approach for large-scale scene analysis.\nWith the introduction of PointNet (Qi et al., 2017a), point-based methods began to apply neural architectures directly to raw point sets without any conversion or preprocessing steps. A key concern is ensuring permutation invariance, often addressed by aggregating information using symmetric functions. Deep set architectures can be divided into three categories: MLP-methods (Qi et al., 2017a,b; Wang et al., 2019; Ma et al., 2022), convolution-based methods (Li et al., 2018; Thomas et al., 2019), and attention-based methods (Yu et al., 2022; Pang et al., 2022; Zhang et al., 2022a).\nSelf-supervised Point Cloud Pre-training Pre-training is a widely used transfer learning approach in which a model is initially trained on a pretext task and subsequently fine-tuned on a downstream task. Self-supervised pre-training does not use any labels during the pre-training phase, which enables a wide scope of available pre-training data.\nPretext tasks are typically either generative or contrastive. A common generative approach is autoencoding, where the input is recovered under some form of corruption (Chen et al., 2021) or masking (Wang et al., 2021). Transformer architectures (Devlin et al., 2018; He et al., 2022) combine masked reconstruction with multi-head self-attention mechanisms. Point-BERT (Yu et al., 2022) and Point-MAE (Pang et al., 2022) adapt language and vision Transformers to point clouds. Generative models typically reconstruct directly in point cloud space, which is computationally expensive and hard to optimize (Wu et al., 2021; Huang et al., 2023).\nContrastive learning uses available pairs of similar and dissimilar data points to learn an embedding space where the distance between data points reflects a measure of their similarity. This is done by contrasting a sample with augmented versions (He et al., 2020; Chen et al., 2020) or by capturing the relationship between local features and their global context (Oord et al., 2018; Hjelm et al., 2018).\nContrastive methods benefit from diverse, informative data to mitigate overfitting issues and counterbalance representation deficits. To this end, we propose a cross-modal method leveraging image cues rich in structural and semantic context.\nCross-Modal 3D Representation Learning The potential of joint 2D-3D learning has been recognized by previous works. One line of work adapts pre-trained vision and language models for 3D point cloud analysis using strategies such as prompt tuning (Zhang et al., 2022b; Wang et al., 2022), 2D-to-3D architectural modification (Xu et al., 2022; Qian et al., 2022b) and knowledge distillation (Dong et al., 2022; Qi et al., 2023) techniques. These methods cannot be directly applied to generic 3D backbones because they require specialized architectures or adaptation processes, often having large memory requirements and difficulties with 3D extensibility.\nA different approach is to design cross-modal pretext tasks directly on 3D models, allowing the use of generic 3D backbones for downstream tasks. PointVST (Zhang and Hou, 2023) and TAP (Wang et al., 2023) employ a generative cross-modal approach that generates images from point clouds at specified camera views, but they lack knowledge integration from pre-trained vision models. CrossPoint (Afham et al., 2022) performs cross-modal and inter-modal contrastive learning to align point clouds with 2D renderings and with augmented versions. They leverage only global correspondences, resulting in a coarse guiding signal. Conversely, point-pixel level correspondence methods such as Tran et al. (2022) require costly upsampling layers and loss computation, while being unnecessarily fine-grained for learning meaningful contextual relationships."}, {"title": "3. Proposed Method", "content": "We propose structural 2D-3D correspondence learning for self-supervised pre-training of point cloud representations. The importance of incorporating structural knowledge has been demonstrated in prior uni-modal research (Hjelm et al., 2018; Oord et al., 2018; Rao et al., 2020). Our novel cross-modal approach enriches point cloud information with structured, semantic image cues to provide a comprehensive guiding signal for 3D understanding."}, {"title": "3.1. Overview", "content": "Let $D = \\{(P_i, I_i)\\}_{i=1}^n$ denote an unlabeled dataset of point clouds $P_i \\in \\mathbb{R}^{N\\times 3}$, where N is the number of points, and shape renderings $I_i \\in \\mathbb{R}^{H\\times W\\times 3}$ of size $H \\times W$. Rendering $I_i$ is captured from a random camera view point with view matrix $M_{view} \\in \\mathbb{R}^{3\\times 3}$ and projection matrix $M_{proj} \\in \\mathbb{R}^{3\\times 3}$. Figure 3 depicts the overall architecture of our proposed Pic@Point model. It consists of the following modules: 1) a 3D Backbone extracts local and global point cloud features, obtained as top-level positional features $f^{3d}$ and final shape embeddings $A(f^{3d})$, after a global pooling function A, 2) a frozen 2D Backbone returns top-level local and global image features as extracted by a pre-trained vision model (e.g.,"}, {"title": "3.2. Projection into Shared Latent Space", "content": "To unify knowledge and produce rich, transferable representations, we project the modal features into a shared latent space, where a contrastive loss is applied between projected features within the mini-batch. The global projection heads $H_{glb}^{3d}, H_{glb}^{2d}$ consist of simple multi-layer perceptrons (MLPs) with two layers. The local projection heads $H_{loc}^{3d}, H_{loc}^{2d}$ use Conv1d and Conv2d layers with kernel size 1 and stride 1, respectively, to apply transformations in the channel dimension while preserving the spatial dimensions. All projected features are L2-normalized.\nPose Encoding We facilitate spatially aware correspondence between local point cloud and image features by integrating a pose encoding into the local point cloud projection head $H_{loc}^{3d}$. This is necessary because while it is assumed that a complete shape can be uniquely identified in a representation of any modality, this may not hold true for a local shape region. For instance, symmetric features like the red circled plane wing tip depicted in Figure 1 could potentially belong to any of the image snippets depicting a plane wing tip, if no additional pose information is given. The pose encoding is calculated via a two-layer MLP that transforms $M_{view}$ into a 64-dimensional vector that is concatenated to the output of the first convolutional layer in $H_{loc}^{3d}$."}, {"title": "3.3. Cross-Modal Correspondence Task", "content": "Point-to-Pixel Mapping For the local cross-modal correspondence task, we begin by establishing the ground truth mapping from points to image positions using projective transformations with $M_{view}, M_{proj} \\in \\mathbb{R}^{4\\times 4}$. Given a local point cloud embedding $z_l$, let $(u, v) \\in [0,1]^2$ denote the image position projected from its center point. With top-level downsampled image regions of size 7 \u00d7 7, such as produced by ResNet (He et al., 2016), the indexed position (i, j) of the corresponding image region embedding $q_{ij}$ is determined as $(i, j) = [\\lfloor u\\cdot 7, \\upsilon \\cdot 7 \\rfloor]$.\nSubsequently, a contrastive learning objective can be applied to the cross-modal correspondences. For each of the L local point cloud region embeddings $\\{z_l\\}_{l=1,...,L}$, we pull close the corresponding image region embedding $q^+$ of the same object, while pushing away all other image region embeddings $\\{q_k \\neq q^+\\}_{k=1}^m$ within the mini-batch of size m. To achieve this, we apply the InfoNCE loss function (Oord et al., 2018) with temperature hyper-parameter \u03c4:\n$\\mathcal{L}_{lcl} = \\frac{1}{L}\\sum_l \\mathcal{l}_{lcl}$ with\n$\\mathcal{l}_{lcl} = -log\\frac{exp(z_l^Tq^+/\\tau)}{\\sum_{i,j,k} exp(z_l^Tq_{ij}^k/\\tau)}$\nSimilarly, for global point cloud embedding z, we pull close the global image embedding $q^+$ of the same object, while pushing away all other global image embeddings $\\{q^k \\#q^+\\}_{k=1,...,m}$,\n$\\mathcal{L}_{glb} = - log\\frac{exp(z^Tq^+/\\tau)}{\\sum_k exp(z^Tq^k/\\tau)}.$\nThe overall loss function is given by $L = L_{lcl} + L_{glb}$."}, {"title": "4. Experiments", "content": "In the following, we present our experimental setups and results of Pic@Point pre-training using different point cloud backbones. We conduct experiments on four standard benchmarks: ModelNet40 (Wu et al., 2015) and ScanObjectNN (Uy et al., 2019) for object classification, ShapeNetPart (Yi et al., 2016) for part segmentation, and S3DIS (Armeni et al., 2016) for semantic scene segmentation."}, {"title": "4.1. Pre-Training Setup", "content": "Pre-training Dataset Following common practice, we pre-train on the ShapeNet (Chang et al., 2015) dataset, which consists of more than 50 000 CAD models from 55 semantic categories. We obtain point clouds by randomly sampling 1024 points from each object. Furthermore, we obtain renderings of size 224 \u00d7 224 from 20 different view points placed in a regular dodecahedron around the object, saving projection and camera matrices $M_{proj}, M_{view}$. We randomly select a single rendering per object at each iteration and apply random rotation as augmentation."}, {"title": "4.2. Downstream Tasks", "content": "In the following, we present experimental results on object classification, part segmentation and scene segmentation."}, {"title": "4.2.1. OBJECT CLASSIFICATION", "content": "To evaluate the effectiveness of our proposed Pic@Point method for object classification, we conduct experiments on the synthetic dataset ModelNet40 (Wu et al., 2015) and the real-world scanned dataset ScanObjectNN (Uy et al., 2019). ModelNet40 consists of 12331 3D CAD models from 40 object categories. ScanObjectNN contains 15 categories of real-world indoor scans with 2903 unique object instances. We evaluate on all three common variants of the the ScanObjectNN dataset: OBJ_BG contains complete object scans, OBJ_ONLY uses background cropping, and PB_T50_RS contains perturbed versions of the scans. We sample 1024 points on each object for both training and testing.\nWe use two transfer learning protocols to evaluate the effectiveness of our proposed Pic@Point model: linear probing with an SVM and fine-tuning on the downstream dataset.\nLinear SVM Results We test the representation capability of Pic@Point by fitting a linear SVM on the features extracted from the pre-trained point cloud backbone. In Table 1 we report our linear classification results on ModelNet40 and ScanObjectNN (OBJ_BG). The upper part of the table shows existing self-supervised methods employing specialized point cloud architectures, including transformer-style methods. These methods focus primarily on point cloud encoding architectures rather than on the design of pretext tasks. The lower part of the table shows architecture-agnostic pre-training methods, which are compared using a DGCNN backbone.\nPic@Point significantly outperforms competing methods on a DGCNN backbone with 92.5% and 85.7% linear classification accuracy on ModelNet40 and ScanObjectNN (OBJ_BG), respectively. By incorporating normals as additional input, we further improve to 92.9% accuracy on ModelNet40. We achieve margins of +0.4% and +4.0% over the second best methods PointVST (Zhang and Hou, 2023) and CrossPoint (Afham et al., 2022), respectively. This underscores the effectiveness of our proposed structural 2D-3D correspondence learning over existing cross-modal approaches such as PointVST, a generative method, and CrossPoint, a contrastive method that relies solely on global correspondences.\nNotably, the improvements over existing methods are more pronounced on ScanObjectNN than on ModelNet40. This may be attributed to Pic@Point's enhanced generalization ability through learning modality-invariant features, making it more robust on real-world data with irregular sampling and noise. We outperform methods employing larger transformer-style architectures on ScanObjectNN, achieving a margin of +1.6% over the second best method Point-M2AE (Zhang et al., 2022a), which uses a large multi-scale Transformer architecture.\nFine-tuning Results Next, we perform extensive fine-tuning experiments on all three variants of the ScanObjectNN dataset, which has established itself as a favored classification benchmark (Qian et al., 2022a; Ma et al., 2022). The results are shown in Table 2. The top part of the table shows supervised 3D models trained from scratch. The middle part shows 2D-to-3D methods that use large-scale vision foundation models with specialized architectures and 3D downstream adaptations such as visual prompt tuning or multi-stage knowledge distillation. We note that our model does not directly compete with these methods due to their use of specialized architectures with large memory requirements and 2D-to-3D adaptations. Nevertheless, we include them to provide a comprehensive overview of related"}, {"title": "4.2.2. PART SEGMENTATION", "content": "ShapeNetPart (Yi et al., 2016) is a widely used dataset for object part segmentation. It contains 16881 pre-aligned CAD models from 16 object classes and has a total of 50 part categories. We use the same pre-training setup as for classification, pre-training on 1024 randomly sampled points per object on the ShapeNet dataset. For downstream fine-tuning on ShapeNetPart, we train and test on 2048 points. Table 4 reports the mean intersection"}, {"title": "4.2.3. SCENE SEGMENTATION", "content": "Semantic segmentation on large 3D scenes challenges the understanding of contextual relationships and coherent semantic interpretation. S3DIS (Armeni et al., 2016) is a scene segmentation benchmark consisting of 6 types of large scanned indoor areas with 13 semantic categories. Following common practice, we test on the largest area, Area 5, and fine-tune on the remaining areas. For training, the point clouds are downsampled with a voxel size of 0.04m and sub-sampled to 24000 points. Testing is conducted on the entire scene.\nPic@Point consistently improves performance over training from scratch, increasing mIoU by +0.7% and mAcc by +0.6%. It outperforms leading transformer-style methods by a margin of +2.8% mIoU. The achieved scene segmentation results indicate that Pic@Point is superior in performing dense prediction tasks through its integration of rich semantic cues and structural, pose-aware guidance."}, {"title": "5. Conclusions", "content": "This paper presents Pic@Point, a self-supervised pre-training method that leverages 2D-3D correspondences at local and global scales. Our proposed method uses a simple contrastive learning framework to integrate point cloud and image features across various abstraction levels, providing a guiding signal that is rich in semantic and structural knowledge. Pic@Point pre-training significantly outperforms existing self-supervised pre-training methods, including those based on Transformer architectures, in various 3D understanding tasks. Its lightweight, architecture-agnostic design offers distinct practical advantages and benefits from future advancements in point cloud technologies."}]}