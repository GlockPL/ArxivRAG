{"title": "MAMBAPEFT: EXPLORING PARAMETER-EFFICIENT FINE-TUNING FOR MAMBA", "authors": ["Masakazu Yoshimura", "Teruaki Hayashi", "Yota Maeda"], "abstract": "An ecosystem of Transformer-based models has been established by building large models with extensive data. Parameter-efficient fine-tuning (PEFT) is a crucial technology for deploying these models to downstream tasks with minimal cost while achieving effective performance. Recently, Mamba, a State Space Model (SSM)-based model, has attracted attention as a potential alternative to Transformers. While many large-scale Mamba-based models have been proposed, efficiently adapting pre-trained Mamba-based models to downstream tasks remains unexplored. In this paper, we conduct an exploratory analysis of PEFT methods for Mamba. We investigate the effectiveness of existing PEFT methods for Transformers when applied to Mamba. We also modify these methods to better align with the Mamba architecture. Additionally, we propose new Mamba-specific PEFT methods that leverage the distinctive structure of Mamba. Our experiments indicate that PEFT performs more effectively for Mamba than Transformers. Lastly, we demonstrate how to effectively combine multiple PEFT methods and provide a framework that outperforms previous works. To ensure reproducibility, we will release the code after publication.", "sections": [{"title": "INTRODUCTION", "content": "Modern large-scale models, also known as Foundation Models, are heavily based on Transformers (Vaswani, 2017). Transformer-based pre-trained models span diverse domains such as language, vision, and multi-modal applications. Despite their widespread use, Transformers have a notable drawback: their computational inefficiency with long sequences. The computational complexity of the attention module scales with the square of the sequence length.\nTo address this fundamental drawback, Gu & Dao (2023) proposed Mamba, a linear-time sequence model that leverages the strengths of State Space Models (SSMs). While Transformers are constructed from attention modules, Mamba is based on the SSM architecture, allowing it to handle long sequences more efficiently. Additionally, Mamba has been shown to perform better than Transformers with the same number of parameters on major tasks such as natural language processing (NLP) (Gu & Dao, 2023) and computer vision (CV) (Zhu et al., 2024). This fact makes Mamba stand out from other post-Transformer architectures with sub-square time complexity (Peng et al., 2023; Sun et al., 2023). Mamba-based models have been proposed across a wide range of domains (Behrouz & Hashemi, 2024; Liang et al., 2024; Zhang et al., 2024b; Li et al., 2024). We believe that Mamba has the potential to go beyond the Transformer ecosystem.\nParameter-efficient fine-tuning (PEFT) is essential for adapting such large-scale models to downstream tasks. Fine-tuning all parameters of these models results in high computational costs. PEFT enables additional training for large-scale Transformers with limited computing resources and data. Early examples of PEFT include the fine-tuning of pre-trained language models for NLP tasks (Houlsby et al., 2019; Hu et al., 2021). Subsequently, it has been extensively adopted across a wide range of applications (Yeh et al., 2024; Wang et al., 2023; 2024a). While many PEFT methods have been extensively studied for Transformers (Lialin et al., 2023; Han et al., 2024), research on PEFT methods for Mamba remains limited."}, {"title": "RELATED WORK AND PRELIMINARY", "content": "In this section, we review related work and present the motivation behind our research. We begin with an introduction of Mamba. Subsequently, we discuss PEFT methods for Transformers. Finally, we highlight that research on PEFT for Mamba is limited."}, {"title": "MAMBA", "content": "SSMs are powerful methods for modeling time-dependent dynamic systems. It was developed and analyzed in detail to address the signal processing technique classically known as the Kalman filter (Kalman, 1960) and has been used to date in a wide range of fields, such as control engineering (Maciejowski & Huzmezan, 2007). An SSM (Gu et al., 2021a;b) takes an input Xt, converts it to a hidden state Ht, and then outputs Yt. This procedure at each time step t is\n$H_t = A_tH_{t-1}+B_tX_t, Y_t = C_tH_t + D_tX_t.$\n(1)\nAs this model describes the relationship between continuous quantities, it must be described to deal with discrete quantities such as NLP. During step size d, the input is assumed to be invariant and the well-known theory of zero-order hold is applied. Putting\n$\\overline{A} := (I - d/2 \\cdot A)^{-1}(I + d/2 \\cdot A), \\overline{B} := (I - d/2 \\cdot A)^{-1}dB, \\overline{C} := C, \\overline{D} := D$\n(2)\nallows us to obtain the final discrete representation:\n$H_t = \\overline{A}H_{t-1}+\\overline{B}X_t, Y_t = \\overline{C}H_+ + \\overline{D}X_t.$\n(3)"}, {"title": "PARAMETER-EFFICIENT FINE-TUNING FOR TRANSFORMER", "content": "PEFT methods have been actively studied due to the large model size of Transformers extensively in both the NLP and CV communities. These studies can be categorized into four methods; partial, additive, reparametrization, and hybrid according to Lialin et al. (2023); Han et al. (2024).\nPartial-tuning methods. While usual fine-tuning updates all parameters in the network, Partial-tuning updates a part of them. BitFit (Zaken et al., 2021) is a successful method in this category which tunes only the bias terms of the linear layers in Transformers. However, most of the linear layers in Mamba do not have the bias terms, which implies that it cannot be applied a priori.\nAdditive methods. Additive methods add a small number of additional parameters or small networks to be fine-tuned. They can be further divided into additive adapter and additive token methods. The former adds adapter modules whose rank r is reduced from the input dimension d (r << d) by a bottleneck structure. Adapter (Houlsby et al., 2019) attaches the adapter in series with the feed-forward network (FFN) module in Transformers. In the following methods, Adapter+ (Steitz & Roth, 2024) improves the position of the input feature acquisition to the adapter. ParallelAdapter (He et al., 2022) and AdaptFormer (Chen et al., 2022) improve performance by inserting the adapter in parallel to the FFN. We adopt ParallelAdapter for Mamba because it is successful in both language (He et al., 2022) and vision (Chen et al., 2022) tasks. It attaches the adapter\n$x_e = FFN(x_e) + s \\cdot ReLU(x_e \\cdot W_{down}) \\cdot W_{up},$\n(5)\nwhere the second term is the adapter with additional parameters $W_{down} \\in \\mathbb{R}^{d \\times r}$ and $W_{up} \\in \\mathbb{R}^{r \\times d}$, and a scaling hyperparameter s. In the latter category, Prompt-tuning (Liu et al., 2021) (resp. Prefix-tuning (Li & Liang, 2021)) adds learnable soft tokens to the input of the network (resp. soft tokens inside each Attention layer). Successive methods improve how to embed soft tokens (Liu et al., 2022; Razdaibiedina et al., 2023; Zhang et al., 2023). Though prompt-tuning for ViT adds learnable tokens at the beginning of the input (Jia et al., 2022; Wang et al., 2024b), it is unclear where the token should be added for Mamba because the order of tokens makes sense in SSM. In fact, Vim (Zhu et al., 2024) adds a class token in the middle of the input sequence. Hence, we investigate multiple choices of soft token insertion.\nReparameterization methods. Although reparameterization methods add low-rank adapters to Linear layers similar to the additive adapter methods, they adopt adapters that can be merged into the weights of the original Linear layer at the inference time. Due to this restriction, activation cannot be used in the adapter, while it eliminates the inference time overhead. The most notable one is LORA (Hu et al., 2021), a method that reparameterizes the pre-trained Linear weights W as $W' = W + s \\cdot W_{down} \\cdot W_{up}$. While subsequent studies have improved the efficiency by finding ways to reparameterize the weights (Hayou et al., 2024; Liu et al., 2024a; Jiang et al., 2024), we first investigate how many ranks of the adapter should be attached to which linear weights in Mamba. In this regard, we will use a simple LoRA with few hyperparameters.\nHybrid methods. Hybrid methods use multiple PEFT methods. While many approaches manually combine these methods (Mao et al., 2021; He et al., 2022; Karimi Mahabadi et al., 2021), several"}, {"title": "PARAMETER-EFFICIENT FINE-TUNING FOR MAMBA", "content": "In contrast to the case of Transformers, there are limited studies on PEFT for Mamba. To the best of our knowledge, the only study in this context is Halloran et al. (2024), which applied the same rank of LoRA to all the linear layers in Mamba. With this motivation, we conduct an exploratory investigation and provide benchmarks of PEFT methods for Mamba."}, {"title": "INVESTIGATION OF PEFT METHODS FOR MAMBA AND BEYOND", "content": "In this section, we explore PEFT methods for the Mamba architecture. First, we discuss how existing PEFT methods for Transformers can be adapted to Mamba. Next, we present methods that have been improved and modified based on the characteristics of Mamba. Finally, we propose new PEFT methods specifically for Mamba and hybrid PEFT methods to search optimal combinations efficiently."}, {"title": "SIMPLE ADAPTATION OF EXISTING PEFT METHODS TO MAMBA", "content": "Some PEFT methods can be directly applied to Mamba because they are network architecture-independent. For example, ParallelAdapter attaches a parallel adapter to each FFN in Transformers. The counterpart of the FFN in Mamba is the out_proj layer, and hence we attach it to the out_proj.\nLoRA is also a network architecture-independent method and can be applied directly to Mamba. However, it is still unclear which Linear layer should be targeted and what hyperparameter values should be used. We individually investigate LoRA on each module as LoRA(embedding), LoRA(in_proj), LoRA(x_proj), LoRA(dt_proj), and LoRA(out-proj) to clarify the appropriate way to apply LoRA to Mamba (see Figure 1)."}, {"title": "RE-DESIGNING AND IMPROVING EXISTING PEFT METHODS FOR MAMBA", "content": "Partial LoRA. Mamba is a network characterized by numerous intermediate features with diverse properties, such as X, Z, dt, B, C, and so on (see Figure 1). In the previously mentioned normal LORA, the inputs for multiple intermediate features are compressed together in the narrow rank of LoRA, even though they have different properties. Therefore, we now develop Partial LoRAs LoRAp(X), LoRAp(Z), LoRAp(dt), LoRAp(B), and LoRAp(C), where we apply LoRA to only a part of the weights in the linear layer according to the output features. Since the dimensions of linear layers vary widely in Mamba (e.g. from 16 to 2048 in Mamba-1.3B), we also investigate whether there is an optimal dimension per layer or not to attach LoRA.\nPrompt-tuning. Prompt-tuning can be applied directly to Mamba. As with Transformer, we simply add soft prompts to the input sequence. In normal Prompt-tuning, a prompt is inserted at the beginning of the input sequence. For Vim, however, we adjust the insertion position. This is because, unlike ViT, Mamba is a time-series model and works differently depending on where the prompt is inserted. In fact, Vim improved accuracy by inserting the class token in the middle of the sequence rather than the beginning. In addition, Mamba Register (Darcet et al., 2024) found it is preferable to insert the register tokens at equal intervals. Hence, in this paper, we construct three prompt types; prefix (resp. infix, suffix) type, inserting prompt tokens at the beginning (resp. with equal intervals, at the end)."}, {"title": "NEW PEFT METHODS DESIGNED FOR MAMBA", "content": "Affix-tuning. We cannot apply Prefix-tuning directly to time series models such as Mamba, since it is designed for the Attention mechanism based on query, key, and value. As shown in Figure 2, by adding soft tokens to only K and V, Prefix-tuning successfully adds new information to all tokens in the input sequence without breaking the positional relationship of the original input sequence. Motivated by these observations, we propose a PEFT method that adds soft tokens before input to the SSMs and discards the output tokens of the inserted position (see Figure 2). This preserves the positional relationship between input and output tokens even when tokens are added. Furthermore, for Vim, we adjust the insertion position in the same way as for Prompt-tuning. Since the name Prefix-tuning can be misleading, we generalize by calling it Affix-tuning.\nPartial-tuning. BitFit (Zaken et al., 2021) is effective among Partial-tuning methods because it fine-tunes bias terms with low degrees of freedom and they do not break the pre-trained model. Different situations for the Partial-tuning in Mamba is that, unlike Transformers, it does not utilize bias terms in linear layers without dt_proj, while it has various other parameters with low degrees of freedom such as the parameters A, D, and the weights of the causal convolution whose dimension is one. Based on this situation, we construct Partial-tuning for Mamba, named Bias-tuning, A-tuning, D-tuning, and Conv1d-tuning, which fine-tune the previously mentioned parameters. For Vim, we also make positional embedding and class tokens learnable. In addition, we enhance our method from conventional Partial-tuning by modifying the weight decay. Specifically, instead of the usual weight decay of $|W|^2$, we propose applying a weight decay of $|W \u2013 W_{pretrain}|^2$ to preserve the pre-trained model.\nAdditional-scan. We propose a new efficient PEFT with fewer parameters by leveraging the SSM architecture. By increasing the state dimension of the SSM, we aim to store information in it to adapt to new tasks, called Additional-scan. It corresponds to additive methods. Specifically, we increase the state dimension of the input parameters of SSMs, A, B, and C, by N'. Eq. 3 then becomes as follows:\n$A^{[T,L,N+N']} = exp(-{\\Delta}^{[T,L,1]} A^{[1,L,N+N']}), B^{[T,L,N+N']} = {\\Delta}^{[T,L,1]}\u3002B^{[T,1,N+N']},$\n(6)\nwhere T, L, and N are the token length, hidden size, and state dimension of the SSM, and $P^{[d_1,d_2,d_3]}$ represents $P\\in \\mathbb{R}^{d_1\\times d_2 \\times d_3}$. Then, the core recurrent computation in SSMs becomes\n$H^{[L,N+N']}_t = A^{[L,N+N']}_t \\circ H^{[L,N+N']}_{t-1} + B^{[L,N+N']}_t \\cdot X^{[D,1]}_t,\\newline Y^{[L,1]}_t = H^{[L,N+N']}_t \\cdot (C^T)^{[N+N',1]} + D^{[L,1]}.\n(7)"}, {"title": "HYBRID PEFT SEARCH", "content": "We have introduced the primary seven methods as follows: ParallelAdapter, LoRA, Partial LoRA, Prompt-tuning, Affix-tuning, Partial-tuning, and Additional-scan. Specifically, there are five variations of LoRA, five variations of Partial LoRA, and six variations of Partial-tuning, making a total of 20 methods. This variety motivated us to propose a hybrid PEFT for Mamba, which combines multiple PEFT methods to enhance performance. There are three key elements to consider in our hybrid PEFT: the combination of PEFT methods, the hyperparameters of each PEFT method (e.g., a rank of LoRA), and the training hyperparameters for each method (e.g., learning rate). Exploring them simultaneously results in an enormous search space compared to the search space of previous works which target only several PEFT methods. Furthermore, the hyperparameters have a conditional search space, meaning they are only considered if the corresponding PEFT method is active. Thus, we propose a two-step approach that explicitly divides the problem into two parts to improve efficiency in the search process.\nIn the first step, we restrict the search space to combinations of PEFT methods with minimal trainable parameters within each method, i.e., we explore whether to activate each PEFT method with fixed hyperparameters with minimal trainable parameters. This phase is based on our observation that performance degrades when too many PEFT methods or trainable parameters are added. In the second step, based on the PEFT method combination suggested by the first step, we greedily search the hyperparameters of both PEFT methods and their training. Since this phase inevitably increases the number of trainable parameters, we also include an option to remove a specific PEFT method. This approach helps to prevent excessive parameter growth and allows us to allocate more parameters to the more critical methods. We utilize the TPE algorithm (Bergstra et al., 2011) implemented in Optuna (Akiba et al., 2019) for the search in each step. Detailed algorithm is provided in Appendix B."}, {"title": "EXPERIMENTS AND DISCUSSION", "content": "In this section, we present our experiments on 7 PEFT methods with 20 variations. We conduct experiments on both image and language tasks. Our methods are compared with state-of-the-art methods for Transformers. We provide detailed results and analysis based on these comprehensive experiments."}, {"title": "EXPERIMENTAL SETTINGS", "content": "Models. We use Vim-S (Zhu et al., 2024) as a base model in our experiments. We also experiment with ViT (Dosovitskiy et al., 2020) for comparison. We adopt pre-trained weights trained with ImageNet-1k (Deng et al., 2009) dataset using DeIT (Touvron et al., 2021) training framework in all models.\nDataset. We conduct our evaluation on the VTAB-1k image classification dataset (Zhai et al., 2019). This dataset contains tasks from 19 domains. These tasks are grouped into three categories: Natural, Specialized, and Structured. For each task, 1000 images are used for training.\nBaselines. We adopt LoRA (Hu et al., 2021), Adaptformer (Chen et al., 2022), FacT-TK (Jie &\nDeng, 2023), and Adapter+ (Steitz & Roth, 2024) as existing PEFT methods for Transformers. For Mamba, we evaluate the 20 PEFT methods explained in Section 3. As a baseline for Hybrid PEFT, we use a combination of all PEFT methods in the experiments.\nImplementation Details. We follow the setup of Jie & Deng (2023) in our experiments, using AdamW optimizer (Loshchilov & Hutter, 2017) and training the model for 100 epochs. The learning"}, {"title": "RESULTS FOR INDIVIDUAL PEFT METHODS", "content": "Overall Results. Table 1 shows the performance with hyperparameters that maximize accuracy. Many PEFT methods for Vim outperform the state-of-the-art ones for ViT. Interestingly, the PEFT methods for ViT start over-fitting early when the number of trainable parameters is increased and are unable to improve accuracy. On the other hand, Vim continues to improve its accuracy even with more trainable parameters. This is probably due to the highly modular structure of Mamba, which prevents its pre-trained memory from being corrupted by additional parameters. In addition, Vim has a greater performance improvement margin between full fine-tuning and PEFT than ViT has. These results suggest that PEFT is more effective for Mamba than Transformer.\nLoRA. In our experiments, LoRA turns out to be one of the most effective PEFT methods with Vim. As shown in Table 1, PEFT methods perform well regardless of where LoRA is applied. The best performance is achieved with the partial LoRA, LoRAp(X). This result stems from our detailed investigation, indicating that the effectiveness of LoRA depends on the nature of the output features.\nAnother notable finding regarding LoRA is that when it is attached to a linear weight with a small dimension, the accuracy continues to increase even if the rank of LoRA exceeds the full-rank, as shown in Figure 3a. Full-rank refers to the smaller value of the input dimension or output dimension. This phenomenon is likely because the effective degrees of freedom are bounded by the full-rank. It follows that adding more parameters does not cause overfitting. Instead, it positively contributes"}, {"title": "Affix-tuning and Prompt-tuning", "content": "The ablation studies on affix-tuning are shown in Table 2a. First, our Affix-tuning has multiple choices about where to add soft tokens because it is architecture-independent compared to Prefix-tuning for transformers, as shown in Figure 2. We test two positions, before or after the in_proj layer. If before, the output tokens are discarded just before the out_proj layer. If after, they are discarded after SSM. We find that it is important to affect additional tokens only on SSM by adding after the in_proj layer. Moreover, the best performance is achieved by adding an affix at the beginning of the input tokens, just as with Prefix-tuning for Transformer, and different from how to handle additional tokens for Vim in Zhu et al. (2024); Darcet et al. (2024). This should be because it is impossible to learn which positions are additional tokens rather than patch tokens unless they are trained from scratch. A detailed analysis of the positions and numbers of inserted tokens is provided in the Appendix A.1.\nNote that these methods have the option of applying embedding projection to the prompts, which significantly increases the number of parameters. However, it can be merged during inference, resulting in zero computational cost increase. Our results show that embedding projection is effective in improving accuracy, and hence it is recommended to use it if memory allows."}, {"title": "PEFT methods designed for Mamba", "content": "We perform experiments on two newly proposed PEFT methods for Mamba. The results of Additional-scan are presented in Table 1. Additional-scan shows competitive performance with the best settings of LoRA and Affix-tuning, with fewer trainable parameters. Figure 3b and Table 2b list the experiments with different initialization for Additional-scan. It shows that our proposed initialization consistently outperforms the original S4D used in Mamba. The results of Partial-tuning are presented in Table 1. Performance highly depends on which components are made learnable. The highest performance is achieved by making convld learnable. Our proposed modified weight decay works effectively around 1e-3 strength (Table 2c). We analyze the number of trainable parameters and hyperparameters in Appendix A.2 in detail."}, {"title": "RESULTS OF HYBRID PEFT", "content": "The bottom of Table 1 shows the effectiveness of our two-step optimization approach. Combining all PEFT methods results in lower performance compared to using a single method. In contrast, our two-stage search method achieves higher performance with fewer parameters. These results indicate that selecting a preferred combination of PEFT methods is crucial.\nWe find that the high-performing single methods are not necessarily selected among the optimal combinations. An example of this combination is provided in Table 6 in Appendix B.2. We hypothesize that this phenomenon is related to the importance of ensuring model diversity in ensemble methods (Dietterich, 2000). Combining PEFT methods with different mechanisms can offset individual errors, mitigate overfitting, and enhance generalization performance. Detailed verification of this aspect remains a direction for future research."}, {"title": "LANGUAGE TASKS", "content": "In addition to the image tasks, we evaluate our method on language tasks using the vanilla Mamba Gu & Dao (2023). We experiment with a commonsense reasoning task, following the setup and dataset of Hu et al. (2023). This task consists of eight sub-tasks, which are evaluated on each task after training on a dataset that integrates training data from all tasks. We use Pythia (Biderman et al., 2023), which is pre-trained with the same dataset as Mamba, as a Transformer baseline. For a PEFT method for Mamba, we compare with SLL LORA (Halloran et al., 2024). To the best of our knowledge, SLL LORA is the only PEFT method for Mamba.\nIn the language tasks, the proposed Additional-scan is found to work as an efficient PEFT. We hypothesize that this is related to the amount of data. This experiment uses 170k datasets, in contrast to the 1k used for VTAB-1k. We hypothesize that learning a selective mechanism requires a relatively large amount of data, and Additional-scan works powerfully in such cases. In Affix-tuning, we find that, as the size of the base model increases, it achieves sufficient accuracy without the embedding projection. This is a valuable discovery because reducing memory costs is crucial for larger models."}, {"title": "LIMITATIONS", "content": "This paper focuses on an exploratory investigation. Evaluating the applicability of our findings to larger models, such as vision models trained with ImageNet-21K (Ridnik et al., 2021) or large language models with billions of parameters (Touvron et al., 2023), is an area for future work. We will release the code after publication to stimulate future research from communities."}, {"title": "CONCLUSION", "content": "We conducted a comprehensive investigation of PEFT for Mamba. Our investigation started by applying existing PEFT methods for Transformers to Mamba. We proposed Affix-tuning and Partial LORA as modifications of existing PEFT methods and introduced Partial-tuning and Additional-scan as new Mamba-specific PEFT methods. We also provided an effective framework for combining them. These comprehensive experiments on image and language tasks led to several findings.\nThe experiments demonstrate that Mamba benefits from PEFT more than Transformers. It improves accuracy without overfitting, even with a larger number of parameters, especially when fine-tuning data is limited. LoRA-based methods, particularly LoRAp(X), are effective with limited data. In contrast, the proposed Additional-scan method excels with larger datasets. We found that initialization significantly impacts performance when adding an additional dimension to A, despite Mamba's robustness to initialization compared to other SSMs. Moreover, the proposed Affix-tuning was effective especially for large Mamba models. As to adapters on small linear weights, interestingly, over-parameterizing the original weight rather than decomposing it to a lower rank can be beneficial, which should holds for other architectures than Mamba. In hybrid PEFT, simply combining individually high-performance methods is inefficient. It is crucial to find the appropriate combination of PEFT methods. We believe that these results make a valuable contribution to the Mamba research community."}, {"title": "A MORE DETAILED ABLATION STUDIES", "content": "In the experiments, we need to determine the appropriate parameters for each method. To this end, we investigate the optimal settings by varying the degrees of freedom for each PEFT method. Specifically, the rank of LoRA (r), the number of additional tokens for Affix-tuning and Prefix-tuning (n), the number of dimensions for Additional-scan, and the strength of the proposed weight decay for Partial-tuning. Same as the ablation study in Table 2 and Figure 3 in the main text, we evaluate the averaged accuracy across six tasks: CIFAR-100, Sun397, Camelyon, Retinopathy, Clevr-Count, and sNORB-Elev.\nFirst, each LoRA applied to large weights is evaluated. As shown in Figure 4d, the maximum accuracy with ViT can be obtained with approximately r = 32, whereas that with Vim can be obtained with a higher rank, as shown in Figure 4c. One reason is that the pre-trained Vim is less prone to collapse even when a large LoRA is added. Interestingly, the in_proj layer itself is prone to collapse and begins to degrade in accuracy at r > 32, while LoRAp(X) and LoRAp(Z), which apply LoRA to partial weights of the in_proj layer, are able to continue improving accuracy up to r = 64. By dividing LoRA for each output feature, the collapse is suppressed, and further accuracy improvement is possible with large parameters in LoRA. We have already discussed LoRA on small linear weights in the main text (Figure 3a)\nFor Prompt-tuning, adding soft tokens to both the beginning and end is the most effective, as shown in Figure 4b. Figure 4a also shows that the number of affixes improves performance up to 3 tokens. However, 4 or more affix tokens degrade performance, due to over-fitting caused by increasing the number of parameters."}, {"title": "EFFICIENCY OF PEFT METHODS FOR MAMBA", "content": "In Table 1, we present a benchmark with the hyperparameter settings that maximize accuracy. As discussed in Appendix A.1, Vim is less prone to collapse and benefits more from increasing the number of training parameters in PEFT modules. Consequently, the number of trainable parameters for Vim in Table 1 is larger than that for ViT. Hence, we analyze the trade-off between accuracy and the size of PEFT modules. Specifically, all methods evaluated in Figure 3 and Figure 4 are plotted in Figure 5. The results indicate that the superior PEFT methods for Mamba achieve high accuracy even with a lower number of parameters and exhibit a better trade-off compared to those for ViT."}, {"title": "PER-TASK EVALUATION ON VTAB-1K", "content": "We present the results for each task in VTAB-1k, including different base model sizes and training data. Specifically, we also report the accuracy on other base model architectures such as Vim-tiny (Zhu et al., 2024), ViT-tiny (Touvron et al., 2021), and ViT-B (Dosovitskiy et al., 2020), as well as different pre-training datasets such as ImageNet-21k (Deng et al., 2009) or using the training framework of (Dosovitskiy et al., 2020). The compared PEFT methods for Transformers are LoRA (Hu et al., 2021), Adaptformer (Chen et al., 2022), Adapter+ (Steitz & Roth, 2024), FacT-TK (Jie &\nDeng, 2023), Head2Toe (Evci et al., 2022), VQT (Tu et al., 2023), BitFit (Zaken et al., 2021), and LOSA (Mercea et al., 2024). The results are shown in Table 4.\nMany PEFT methods for Vision Transformers have benchmarked with ViT-B pre-trained on ImageNet-21K. First, we clarify the accuracy change due to differences in our experimental settings and their settings. By using ImageNet-1K instead of ImageNet-21K, accuracy decreases by"}, {"title": "Ablation studies for vision tasks", "content": "a shows the relationship between the number of tokens and performance in Affix-tuning with projection. b shows the relationship between the number of tokens and performance in Prompt-tuning. c shows the relationship between the rank of LoRA and performance in Mamba. d shows the relationship between the rank of LoRA and performance in ViT. Performance consistently decreases as the rank increases up to 64, contrary to the case in Mamba."}, {"title": "A4 ABLATION STUDIES ON LANGUAGE TASKS", "content": "We conduct ablation studies for language tasks. Figure 6c shows that the optimal learning rate varies for each method. In contrast, as shown in Figure 6c and Figure 6d, the optimal learning rate and the optimal number of trainable parameters remain relatively stable despite changes in model size. Hence, the number of trainable parameters is fixed regardless of the model size, and the learning rate is coarsely tuned for evaluation in Table 3. The detailed settings are described in Appendix B.1.\nThe trade-off between the number of trainable parameters and accuracy for Additional-scan, Affix-tuning (w/o proj), and LoRAp(X) is described in Figure 6b. Each method excels in certain aspects and should be chosen based on the specific application. LoRAp(X) is effective when sufficient memory is available, Affix-tuning (w/o proj) maintains accuracy with the fewest trainable parameters, and Additional-scan falls in between. Since even PEFT may not fit into GPU memory with large-scale models, it is crucial to select the appropriate method based on the application and environment."}, {"title": "HYBRID PEFT SEARCH", "content": "Since optimizing over all 19 tasks is computationally expensive, we optimize over the average accuracy of 5 tasks in the representative subset of VTAB-1k (Zhai et al., 2019): Caltech101, EuroSAT, Pets, Camelyon, and Resisc45. By processing five tasks in parallel on one A100 GPU, one trial can be completed in 20 to 30 minutes, with minimal dependency on the type and size of the applied PEFT methods.\nAs mentioned in the Section 3.4, the first step of our search verifies the effectiveness of each PEFT method and seeks the preferable combination with minimum trainable parameters. Specifically, we search for 100 trials using TPE algorithm (Bergstra et al., 2011) about whether to use each PEFT method with the fixed hyperparameters shown in step 1 of Table 6. As a result, it turns out that the combination of eight PEFT methods, CLS-token-tuning, A-tuning, Affix-tuning, LoRA(out_proj), LoRA(in_proj), LoRA(dt_proj), LoRAp(X), and LoRAp(B), is the best."}]}