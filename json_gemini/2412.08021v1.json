{"title": "CAN A MISL FLY? ANALYSIS AND INGREDIENTS FOR\nMUTUAL INFORMATION SKILL LEARNING", "authors": ["Chongyi Zheng", "Jens Tuyls", "Joanne Peng", "Benjamin Eysenbach"], "abstract": "Self-supervised learning has the potential of lifting several of the key challenges\nin reinforcement learning today, such as exploration, representation learning, and\nreward design. Recent work (METRA (Park et al., 2024)) has effectively argued\nthat moving away from mutual information and instead optimizing a certain Wasser-\nstein distance is important for good performance. In this paper, we argue that the\nbenefits seen in that paper can largely be explained within the existing framework\nof mutual information skill learning (MISL). Our analysis suggests a new MISL\nmethod (contrastive successor features) that retains the excellent performance of\nMETRA with fewer moving parts, and highlights connections between skill learn-\ning, contrastive representation learning, and successor features. Finally, through\ncareful ablation studies, we provide further insight into some of the key ingredients\nfor both our method and METRA.", "sections": [{"title": "1 INTRODUCTION", "content": "Self-supervised learning has had a large impact on areas of machine learning ranging from audio\nprocessing (Oord et al., 2016; 2018) or computer vision (Radford et al., 2021; Chen et al., 2020) to\nnatural language processing (Devlin et al., 2019; Radford & Narasimhan, 2018; Radford et al., 2019;\nBrown, 2020). In the reinforcement learning (RL) domain, the \"right\" recipe to apply self-supervised\nlearning is not yet clear. Several self-supervised methods for RL directly apply off-the-shelf methods\nfrom other domains such as masked autoencoding (Liu et al., 2022), but have achieved limited\nsuccess so far. Other methods design self-supervised routines more specifically built for the RL\nsetting (Burda et al., 2019; Pathak et al., 2017; Eysenbach et al., 2019; Sharma et al., 2020; Pong\net al., 2020). We will focus on the skill learning methods, which aim to learn a set of diverse and\ndistinguishable behaviors (skills) without an external reward function. This objective is typically\nformulated as maximizing the mutual information between skills and states (Gregor et al., 2016;\nEysenbach et al., 2019), namely mutual information skill learning (MISL). However, some promising\nrecent advances in skill learning methods build on other intuitions such as Lipschitz constraints (Park\net al., 2022) or transition distances (Park et al., 2023). This paper focuses on determining whether the\ngood performance of those recent methods can still be explained within the well-studied framework\nof mutual information maximization.\nMETRA (Park et al., 2024), one of the strongest prior skill learning methods, proposes maximizing\nthe Wasserstein dependency measure between states and skills as an alternative to the idea of mutual\ninformation maximization. The success of this method calls into question the viability of the MISL\nframework. However, mutual information has a long history dating back to Shannon (1948) and\ngracefully handles stochasticity and continuous states (Myers et al., 2024). These appealing properties\nof mutual information raises the question: Can we build effective skill learning algorithms within the\nMISL framework, or is MISL fundamentally flawed?\nWe start by carefully studying the components of METRA both theoretically and empirically. For\nrepresentation learning, METRA maximizes a lower bound on the mutual information, resembling"}, {"title": "2 RELATED WORK", "content": "Through careful theoretical and experimental\nanalysis, we develop a new mutual informa-\ntion skill learning method that builds upon con-\ntrastive learning and successor features.\nUnsupervised skill discovery. Our work builds\nupon prior methods that perform unsupervised\nskill discovery. Prior work has achieved this\naim by maximizing lower bounds (Tschannen\net al., 2020; Poole et al., 2019) of different mu-\ntual information formulations, including diverse\nand distinguishable skill-conditioned trajecto-\nries (Li et al., 2023; Eysenbach et al., 2019;\nHansen et al., 2020; Laskin et al., 2022; Strouse\net al., 2022), intrinsic empowerment (Mohamed\n& Jimenez Rezende, 2015; Choi et al., 2021),\ndistinguishable termination states (Gregor et al.,\n2016; Warde-Farley et al., 2019; Baumli et al.,\n2021), entropy bonus (Florensa et al., 2016; Lee\net al., 2019; Shafiullah & Pinto, 2022), predictable transitions (Sharma et al., 2020; Campos et al.,\n2020), etc. Among those prior methods, perhaps the most related works are CIC (Laskin et al., 2022)\nand VISR (Hansen et al., 2020). We will discuss the difference between them and our method in\nSec. 5. Another line of unsupervised skill learning methods utilize ideas other than mutual informa-\ntion maximization, such as Lipschitz constraints (Park et al., 2022), MDP abstraction (Park & Levine,\n2023), model learning (Park et al., 2023), and Wasserstein distance (Park et al., 2024; He et al., 2022).\nOur work will analyze the state-of-the-art method named METRA (Park et al., 2024) that builds on\nthe Wasserstein dependency measure (Ozair et al., 2019), provide an alternative explanation under\nthe well-studied MISL framework, and ultimately develop a simpler method."}, {"title": "3 PRELIMINARIES", "content": "Mutual information skill learning. The MISL problem typically involves two steps: (1) unsupervised\npretraining and (2) downstream control. For the first step, we consider a Markov decision process\n(MDP) without reward function defined by states $s \\in S$, actions $a \\in A$, initial state distribution\n$P_o \\in \\Delta(S)$, discount factor $\\gamma \\in (0,1]$, and dynamics $p : S \\times A \\rightarrow \\Delta(S)$, where $\\Delta(\\cdot)$ denotes\nthe probability simplex. The goal of unsupervised pretraining is to learn a skill-conditioned policy\n$\\pi: S \\times Z \\leftrightarrow \\Delta(A)$ that conducts diverse and discriminable behaviors, where Z is a latent skill\nspace. We use $\\beta : S \\times Z \\leftrightarrow \\Delta(A)$ to denote the behavioral policy. We select the prior distribution\nof skills as a uniform distribution over the d-dimensional unit hypersphere $p(z) = UNIF(S^{d-1})$ (a\nuniform von Mises\u2013Fisher distribution (Wikipedia, 2024)) and will use this prior throughout our\ndiscussions to unify the theoretical analysis.\nGiven a latent skill space Z, prior methods (Eysenbach et al., 2019; Sharma et al., 2020; Laskin\net al., 2022; Gregor et al., 2016; Hansen et al., 2020) maximizes the MI between skills and states\n$I^{\\pi} (S; Z)$ or the MI between skills and transitions $I^{\\pi} (S, S'; Z)$ under the target policy. We will focus\non $I^{\\pi} (S, S'; Z)$ but our discussion generalizes to $I^{\\pi} (S; Z)$. Specifically, maximizing the MI between\nskills and transitions can be written as\n$\\max_\\pi I^{\\pi} (S, S'; Z) \\triangleq \\max_\\pi E_{z\\sim p(z),s\\sim p^{\\pi} (s_+=s|z)} [\\log p^{\\pi} (z | s, s')]$\nwhere $p^{\\pi} (s_+ = s | z)$ is the discounted state occupancy measure (Ho & Ermon, 2016; Nachum et al.,\n2019; Eysenbach et al., 2022; Zheng et al., 2024) of policy $\\pi$ conditioned on skill z, and $p^{\\pi} (s' | s, z)$\nis the state transition probability given policy $\\pi$ and skill z. This optimization problem can be casted\ninto an iterative min-max optimization problem by first choosing a variational distribution $q(z | s, s')$\nto fit the historical posterior $p^{\\beta}(z | s, s')$, which is an approximation of $p^{\\pi}(z | s, s')$, and then\nchoosing policy $\\pi$ to maximize discounted return defined by the intrinsic reward $\\log q(z | s, s')$:\n$q_{k+1} \\leftarrow arg \\max_q E_{p^{\\beta} (s,s',z)} [\\log q(z | s, s')]$.\n$\\pi_{k+1} \\leftarrow arg \\max_\\pi E_{p^{\\pi} (s,s',z)} [\\log q_k (z | s, s')]$,\nwhere k indicates the number of updates. See Appendix A.1 for detailed discussion.\nFor the second step, given a regular MDP (with reward function), we reuse the skill-conditioned\npolicy $\\pi$ to solve a downstream task. Prior methods achieved this aim by (1) reaching goals in a\nzero-shot manner (Park et al., 2022; 2023; 2024), (2) learning a hierarchical policy $\\pi_h : S \\leftrightarrow \\Delta(Z)$\nthat outputs skills instead of actions (Eysenbach et al., 2019; Laskin et al., 2022; Gregor et al., 2016),\nor (3) planning in the latent space with a learned dynamics model (Sharma et al., 2020)."}, {"title": "4 UNDERSTANDING THE PRIOR METHOD", "content": "In this section we reinterpret METRA through the lens of MISL, showing that:\n1.  The METRA representation objective is nearly identical to a contrastive loss (which maxi-mizes a lower bound on mutual information). See Sec. 4.1.\n2.  The METRA actor objective is equivalent to a mutual information lower bound plus an extraterm. This extra term is related to an information bottleneck (Tishby et al., 2000; Alemiet al., 2017) and our experiments will show it is important for exploration. See Sec. 4.2.\nSec. 5 will then introduce a new mutual information algorithm that combines these insights to match\nthe performance of METRA while (1) retaining the theoretical grounding of mutual information and\n(2) being simpler to implement.\n4.1 CONNECTING METRA'S REPRESENTATION OBJECTIVE AND CONTRASTIVE LEARNING\nOur understanding of METRA starts by interpreting the representation objective of METRA as a\ncontrastive loss. This interpretation proceeds by two steps. First, we focus on understanding the\nactual representation objective of METRA, aiming to predict the convergent behavior of the learned\nrepresentations. Second, based on the actual representation objective, we draw a connection between\nMETRA and contrastive learning. In Sec. 6, we conduct experiments to verify that METRA learns\noptimal representations in practice and that they bear resemblance to contrastive representations.\nSec. 3 mentioned that the Lagrangian $L(\\phi, \\lambda)$ used as the METRA representation objective does not\ncorrespond to the constrained optimization problem in Eq. 4, raising the following question: What is"}, {"title": "4.2 CONNECTING METRA'S ACTOR OBJECTIVE WITH AN INFORMATION BOTTLENECK", "content": "This section discusses the actor objective used in METRA. We first clarify the distinction between\nthe actor objective of METRA and those used in prior methods, helping to identify a term that\ndiscourages exploration. Removing this anti-exploration term results in covering a larger proposition\nof the state space while learning distinguishable skills. We then relate this anti-exploration term\nto estimating another mutual information, drawing a connection between the entire METRA actor\nobjective and a variant of the information bottleneck (Tishby et al., 2000; Alemi et al., 2017).\nWhile prior work (Eysenbach et al., 2019; Gregor et al., 2016; Sharma et al., 2020; Hansen et al.,\n2020; Campos et al., 2020) usually uses the same functional form of the lower bound on (Eq. 2\n& 3) different variants of the mutual information to learn both representations and skill-conditioned\npolicies (see Appendix A.5 for details), METRA uses different objectives for the representation and\nthe actor. Specifically, the actor objective of METRA $J(\\pi)$ (Eq. 6) only encourages the similarity\nbetween the difference of transition representations $\\phi(s') - \\phi(s)$ and their skill z (positive pairs),\nwhile ignoring the dissimilarity between $\\phi(s') - \\phi(s)$ and a random skill z (negative pairs):\n$J(\\pi) = LB(\\phi) = LB_{\\phi}^{\\beta} - LB_{\\phi}^{\\pi}$,\nwhere $LB_{\\phi}^{\\pi}$, $LB_{\\phi}^{\\beta}$, and $LB(\\phi)$ are under the target policy $\\pi$ instead of the behavioral policy\n$\\beta$. The SOTA performance of METRA and the divergence between the functional form of the actor\nobjective (positive term) and the representation objective (positive and negative terms) suggests that\n$LB_{\\phi}^{\\pi}$ may be a term discouraging exploration. Intuitively, removing this anti-exploration term\nboosts the learning of diverse skills. We will empirically study the effect of the anti-exploration term\nin Sec. 6.3 and provide theoretical interpretations next.\nOur understanding of the anti-exploration term $LB_{\\phi}^{\\pi}$ relates it to a resubstituion estimation of the\ndifferential entropy $h^{\\pi}(\\phi(S') - \\phi(S))$ in the representation space (see Appendix A.4 for details), i.e.,\n$LB_{\\phi}^{\\pi} = \\hat{h}^{\\pi} (\\phi(S') - \\phi(S))$. Note that this entropy is different from the entropy of states $h^{\\pi} (S)$,\nindicating that we want to minimize the entropy of difference of representations $\\phi(s') - \\phi(s)$ to\nencourage exploration. There are two underlying reasons for this (seemly counterintuitive) purpose:\nMETRA aims to (1) constrain the expected L2 distance of difference of representations $\\phi(s') - \\phi(s)$\n(Eq. 5) and (2) push difference of representations $\\phi(s') - \\phi(s)$ towards skills z sampled from\n$UNIF(S^{d-1})$. Nonetheless, this relationship allows us to further rewrite the anti-exploration term\n$LB_{\\phi}^{\\pi}$ as an estimation of the mutual information $I^{\\pi} (\\phi(S') -\\phi(S'); S, S')$, connecting the METRA\nactor objective to an information bottleneck:"}, {"title": "5 A SIMPLIFIED ALGORITHM FOR MISL VIA CONTRASTIVE LEARNING", "content": "In this section, we derive a simpler unsupervised skill learning method building upon our understand-\ning of METRA (Sec. 4). This method maximizes MI (unlike METRA), while retaining the good"}, {"title": "5.1 LEARNING REPRESENTATIONS THROUGH CONTRASTIVE LEARNING", "content": "Based on our analysis in Sec. 4.1, we use the contrastive lower bound on $I^{\\beta} (S, S'; Z)$ to optimize the\nstate representation directly. Unlike METRA, we obtain this contrastive lower bound within the MISL\nframework (Eq. 2 & 3) by employing a parameterization of the variational distribution $q(z | s, s')$\nmentioned in prior work (Poole et al., 2019; Song & Kingma, 2021). Specifically, using a scaled\nenergy-based model conditioned representations of transition pairs (s, s'), we define the variational\ndistribution as\n$q(z|s, s') \\triangleq \\frac{p(z)e^{(\\phi(s')-\\phi(s))^Tz}}{E_{p(z')} [e^{(\\phi(s')-\\phi(s))^Tz'}]}$.\nPlugging this parameterization into Eq. 2 produces\n$q_{k+1} \\leftarrow argmax_{\\Phi} E_{p^{\\beta} (s,s',z)} [(\\phi(s') - \\phi(s))^Tz] - E_{p^{\\beta} (s,s')} [log E_{p(z')} [e^{(\\phi(s')-\\phi(s))^Tz'}]]$,\nwhich is exactly the contrastive lower bound on $I^{\\beta} (S, S'; Z)$. This contrastive lower bound allows us\nto learn the state representation $\\phi$ while getting rid of the dual gradient descent procedure (Eq. 5)\nadopted by METRA. In practice, we find that adding a fixed coefficient $\\xi = 5$ to the second term of\nEq. 10 helps boost performance. We include further discussions of $\\xi$ in Appendix A.6 and ablation\nstudies in Appendix C.3.\nIn the same way that the METRA actor objective excluded the anti-exploration term (Sec. 4.2), we\npropose to construct the intrinsic reward by removing the negative term from our representation\nobjective (Eq. 10), resulting in the same RL objective as $J(\\pi)$ (Eq. 6):\n$\\pi_{k+1} \\leftarrow arg max_{\\pi} E_{p^{\\pi} (s,s',z)} [r_k(s, s', z)], r_k(s, s', z) \\triangleq (\\phi_k(s') - \\phi_k(s))^Tz$\nWe use this RL objective as the update rule for the skill-conditioned policy $\\pi$ in our algorithm."}, {"title": "5.2 LEARNING A POLICY WITH SUCCESSOR FEATURES", "content": "To optimize the policy (Eq. 11), we will use an actor-critic method. Most skill learning methods use\nan off-the-shelf RL algorithm (e.g., TD3 (Fujimoto et al., 2018), SAC (Haarnoja et al., 2018)) to fit\nthe critic. However, by noting that the intrinsic reward function $r(s, s', z)$ is a linear combination\nbetween basis $\\phi(s') - \\phi(s) \\in R^d$ and weights $z \\in Z \\subset R^d$, we can borrow ideas from successor\nrepresentations to learn a vector-valued critic. We learn the successor features $\\Psi^{\\pi} : S \\times A \\times Z \\leftrightarrow R^d$:\n$\\Psi^{\\pi} (s, a, z) \\triangleq E_{s_+\\sim p^{\\pi} (s_+=s|z),s'\\sim p(s'|s,a)} [\\phi(s') - \\phi(s)]$,\nwith the corresponding skill-conditioned policy $\\pi$ in an actor-critic style:\n$\\Upsilon_{k+1}(s, a, z) \\leftarrow arg min_{\\psi} E_{(s,a,z)\\sim p^{\\beta} (s,a,s',z),a'\\sim\\pi(a'|s',z)} \\big[((\\s, a, z) - V_k^{s,s', a', z)})^2\\big]$,\nwhere $V_k^{s, s', a', z)} \\triangleq \\phi_k(s') - \\phi_k(s) + \\gamma\\Psi_k(s', a', z)$,\n$\\pi_{k+1} \\leftarrow arg max_{\\pi} E_{(s,z)\\sim p^{\\beta} (s,z),a\\sim\\pi(a|s,z)} [\\Upsilon_k(s, a, z)^Tz]$,\nwhere $\\varepsilon$ is an estimation of $\\Upsilon^{\\pi}$. In practice, we optimize $\\psi$ and $\\pi$ for one gradient step iteratively."}, {"title": "6 EXPERIMENTS", "content": "The aims of our experiments(1) verifying the theoretical analysis in Sec. 4 experimentally, (2)\nidentifying several ingredients that are key to making MISL algorithms work well more broadly,\nand (3) comparing our simplified algorithm CSF to prior work. Our experiments will use standard\nbenchmarks introduced by prior work on skill learning. All experiments show means and standard\ndeviations across ten random seeds."}, {"title": "6.1 METRA CONSTRAINS REPRESENTATIONS IN EXPECTATION", "content": "Sec. 4.1 predicts that the optimal METRA representation satisfies its constraint\n$E_{(s, s')} [||\\phi^*(s') - \\phi^*(s)||^2] = 1$ strictly (Prop. 1). We study whether this condition holds\nafter training the algorithm for a long time. To answer this question, we conduct didactic experiments\nwith the state-based Ant from METRA (Park et al., 2024) navigating in an open space. We set the\ndimension of $\\phi$ to d = 2 such that visualizing the learned representations becomes easier. After\ntraining the METRA algorithm for 20M environment steps (50K gradient steps), we analyze the\nnorm of the difference in representations $|\\phi(s') - \\phi(s)||$.\nWe plot the histogram of $|\\phi(s') - \\phi(s)||^2$ over 10K transitions randomly sampled from the replay\nbuffer (Fig. 2a). The observation that the empirical average of $|\\phi(s') - \\phi(s)||^3$ converges to 0.9884\nsuggests that the learned representations are feasible. Stochastic gradient descent methods typically\nfind globally optimal solutions on over-parameterized neural networks (Du et al., 2019), making us\nconjecture that the learned representations are nearly optimal (Prop. 1). Furthermore, the spreading of\nthe value of $|\\phi(s') - \\phi(s)||^2$ implies that maximizing the METRA representation objective will not\nlearn state representations $\\phi$ that satisfy $|\\phi(s') - \\phi(s)||^2 \\leq 1$ for every $(s, s') \\in S_{adj}$. These results\nhelp to explain what objective METRA's representations are optimizing."}, {"title": "6.2 METRA LEARNS CONTRASTIVE REPRESENTATIONS", "content": "We next study connections between representations learned by METRA and those learned by con-\ntrastive learning empirically. Our analysis in Sec. 4.1 reveals that the representation objective of\nMETRA corresponds to the contrastive lower bound on $I^{\\beta} (S, S'; Z)$. This analysis raises the question\nwhether representations learned by METRA share similar structures to representations learned by\ncontrastive losses (Gutmann & Hyv\u00e4rinen, 2010; Ma & Collins, 2018; Wang & Isola, 2020).\nTo answer this question, we reuse the trained algorithm in Sec. 6.1 and visualize two important\nstatistics: (1) the conditional differences in representations $\\phi(s') - \\phi(s) \\cdot z$ and (2) the normalized\nmarginal differences in representations $(\\phi(s') - \\phi(s))/||\\phi(s') - \\phi(s)||^2$. The resulting histograms\n(Fig. 2b & 2c) indicate that the conditional differences in representations $\\phi(s') - \\phi(s) \\cdot z$ converges"}, {"title": "6.3 ABLATION STUDIES", "content": "We now study various design decisions of both METRA and our simplified method, aiming to identify\nsome key factors that boost these MISL algorithms. We will conduct ablation studies on Ant again,\ncomparing coverage of (x, y) coordinates of different variants.\n(1) Contrastive learning recovers METRA's representation objective. Our analysis (Sec. 4.1)\nand experiments (Sec. 6.2) have shown that METRA learns contrastive representations. We now test\nwhether we can retain the performance of METRA by simply replacing its representation objective\nwith the contrastive lower bound (Eq. 8). Results in Fig. 3 (Left) suggest that using the contrastive\nloss (METRA-C) fully recovers the original performance, circumventing explanations building upon\nthe Wasserstein dependency measure.\n(2) Maximizing the information bottleneck is important. In Sec. 4.2, we interpret the intrinsic\nreward in METRA as a lower bound on an information bottleneck. We conduct ablation experiments\nto study the effect of maximizing this information bottleneck over maximizing the mutual information\ndirectly, a strategy typically used by prior methods (Eysenbach et al., 2019; Mendonca et al., 2021;\nHansen et al., 2020). Results in Fig. 3 (Center) show that CSF failed to discover skills when only\nmaximizing the mutual information (i.e. including the anti-exploration term). These results indicate\nthat using the information bottleneck as the intrinsic reward may be important for MISL algorithms.\n(3) Parameterization is key for CSF. When optimizing a lower bound on the mutual information\n$I^{\\pi} (S, S'; Z)$ using a variational distribution, there are many ways to parametrize the critic $f(s, s', z)$.\nIn Eq. 9, we chose the parameterization $(\\phi(s') - \\phi(s))^Tz$, but there are many other choices. Testing\nthe sensitivity of this choice of parameterization allows us to determine whether a specific form of\nthe lower bound is important. In Fig. 3, we study several variants of CSF that use (1) a monolithic"}, {"title": "6.4 CSF MATCHES SOTA FOR BOTH EXPLORATION AND DOWNSTREAM PERFORMANCE", "content": "Our final set of experiments compare CSF to prior MISL algorithms, measuring performance on both\nunsupervised exploration and solving downstream tasks.\nExperimental Setup. We evaluate on the same five tasks as those used in Park et al. (2024)\nplus Robobin from LEXA (Mendonca et al., 2021), though we will only focus HalfCheetah\nand Humanoid in the main text. For baselines, we also use a subset from Park et al. (2024)\n(METRA (Park et al., 2024), CIC (Laskin et al., 2022), DIAYN (Eysenbach et al., 2019), and\nDADS (Sharma et al., 2020)) along with VISR (Hansen et al., 2020). See Appendix B.2 for details.\nExploration performance. To measure the inherent exploration capabilities of each method without\nconsidering any particular downstream task, we compute the state coverage by counting the unique\nnumber of (x, y) coordinates visited by the agent. Fig. 4 (left) shows CSF matches METRA on both\nHalfCheetah and Humanoid. For the full set of exploration results, please see Appendix B.3.\nZero-shot goal reaching. In this setting the agent infers the right skill given a goal without further\ntraining on the environment. We evaluate on the same set of six tasks and defer both the goal sampling\nand skill inference strategies to Appendix B.4. We report the staying time fraction, which is the\nnumber of time steps that the agent stays at the goal divided by the horizon length. In Fig. 4 (middle),\nwe find all methods to perform similarly on HalfCheetah, while METRA and CSF perform best\non Humanoid, with METRA performing slightly better on the latter. For the full set of zero-shot\ngoal reaching results, please see Appendix B.4.\nHierarchical control. We train a hierarchical controller $\\pi_h(z | s)$ that outputs latent skills z as\nactions for every fixed number of time steps to maximize the discounted return in two downstream\ntasks from Park et al. (2024), one of which requires to reach a specified goal (HumanoidGoal)\nand one requires jumping over hurdles (HalfCheetahHurdle). The results in Fig. 4 (right) show\nCSF and METRA are the best performing methods, showing mostly similar performance. For further\ndetails as well as the full set of results on all tasks, please see Appendix B.5.\nTaken together, CSF is a competitive MISL algorithm that matches the current SOTA. On the full set\nof results (Appendices B.3, B.4, and B.5) we find that CSF continues to perform roughly on par with\nMETRA on most tasks, though there are some tasks where CSF performs better and vice versa."}, {"title": "7 CONCLUSION", "content": "In this paper, we show how one of the current strongest unsupervised skill discovery algorithms\ncan be understood through the lens of mutual information skill learning. Our analysis allowed the"}]}