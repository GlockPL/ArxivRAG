{"title": "Real-World Cooking Robot System from Recipes Based on Food State Recognition Using Foundation Models and PDDL", "authors": ["Naoaki Kanazawa", "Kento Kawaharazuka", "Yoshiki Obinata", "Kei Okada", "Masayuki Inaba"], "abstract": "Although there is a growing demand for cooking behaviours as one of the expected tasks for robots, a series of cooking behaviours based on new recipe descriptions by robots in the real world has not yet been realised. In this study, we propose a robot system that integrates real-world executable robot cooking behaviour planning using the Large Language Model (LLM) and classical planning of PDDL descriptions, and food ingredient state recognition learning from a small number of data using the Vision-Language model (VLM). We succeeded in experiments in which PR2, a dual-armed wheeled robot, performed cooking from arranged new recipes in a real-world environment, and confirmed the effectiveness of the proposed system.", "sections": [{"title": "Introduction", "content": "The demand for cooking behaviour is increasing as one of the expected tasks for robots. In cooking, it is necessary to follow recipes, which are environment and agent independent task descriptions, and to execute cooking operations based on the conditions of the surrounding environment. There have been various studies on cooking behavior by robots, such as planning and executing robot actions based on recipes described in natural language [1-6], manipulation skills for cooking tasks [7-10], improving cooking quality through feedback from sensors like taste sensors [11\u201313], and recognizing the state of foodstuffs [14-17].\nIn recent years, Large Language Models (LLMs) have emerged and have been applied to robot be-haviour planning [18\u201321]. This has enabled robots to plan actions that can respond to natural language instructions at a higher level than previous rule-based processing. However, it has not yet been realized a series of cooking behaviours where the robot interprets new unknown recipes and recognises food state changes, which is the purpose of the cooking process. Therefore, this study proposes a robot system for cooking from recipe descriptions that takes into account changes of food ingredients' states. The proposed system solves two important issues, \u201creal-world executable action planning\" and \"foodstuff state change recognition,\u201d by using the foundation model and classical planning, and realises a series of real-world cooking behaviours based on recipe descriptions by the robot.\nThe first issue, \u201creal-world executable action planning,\" is that in order for robots to execute cooking based on recipe descriptions in the real world, it is necessary to complement actions that connect the state of the actual environment with the target state of each cooking action described in the recipe and plan action procedures that can be executed. As pointed out by [6], behaviors that need to be comple-mented include actions that are not described in the recipe because people do them unconsciously, but are actually necessary. For example, in order to \"pour the eggs in the bowl into the pan,\u201d it is necessary"}, {"title": "Problem Setting for the Real-World Robot Cooking from Recipes in This Study", "content": "In this study, we consider a problem setting in which a robot cooks basic egg dishes in a real kitchen environment based on recipes. Egg dishes are mostly made from the same single ingredient, the egg, and have many variations depending on the cooking method, such as boiling or pan frying. Therefore, egg dishes are a simple cuisine but a genre in which a variety of major cooking methods appear. This is a suitable subject for research on robot systems that cook from recipes. In this study, three basic egg"}, {"title": "Real-World Executable Robotic Cooking Action Planning from Recipe", "content": "We propose a real-world executable robot cooking behavior planning method from natural language recipe descriptions (Fig. 5). First, a cooking function sequence generation from a recipe with the LLM (Section 3.1) is used to convert the recipe description into a sequence of cooking functions that can be interpreted by the program. Next, classical planning using the description of PDDL [23] complements the actions omitted in the recipe and the actions necessary to execute the target cooking behavior based on the current situation of the environment and the robot, and transforms the converted cooking function sequences into action procedures that can be executed in the real world (Section 3.2). Details of each of these are explained in the following sections."}, {"title": "Cooking Function Sequence Conversion from Recipe with Large Language Model", "content": "Convert a cooking recipe written in natural language into a representation of a sequence of cooking functions that can be interpreted by robots using inference of the Large Language Model (LLM). Three pairs of language descriptions of known recipes and function sequence representations annotated with cooking functions defined in Section 2 are used as examples. As shown in the first half of Fig. 5, the function sequence for a given recipe description is inferred by the few-shot prompting of one of the LLMs, GPT-4 [22]. The black text in the figure shows the actual prompts, and its last recipe section depends on the natural language description of the recipe to be converted. The blue part is the result of the conversion that the LLM outputs. The version of GPT-4 used in this study is gpt-4-0613."}, {"title": "Executable Cooking Action Procedure Planning with PDDL", "content": "The action procedure is planned by classical planning using PDDL descriptions. It plans procedures based on cooking function sequences transformed from recipes, supplemented with the basic actions that are necessary to execute them in the real world. While the content of Section 3.1 was an agent- and environment-independent transformation, this one is intended for action planning tailored to the situation of the real-world environment and the agent.\nFirst, each step of the cooking function sequence transformed by Section 3.1 is converted to the de-scription of predicate in PDDL. Each cooking function is transformed into predicate of the corresponding action's effect, and a sequence of target state descriptions is obtained by adding the default condition at the end of each step. The default condition predicate is the condition that the robot has nothing. By adding this condition, the behavior of the robot becomes natural, similar to the way a human cooks. Similarly, we add an end condition predicate that states that after all steps, the robot has nothing in its hands and that the stove and water are all off.\nNext, planning is performed for the obtained sequence of PDDL predicates. For each step of the sequence, classical planning in the PDDL description is solved to connect the start state and the target state by actions. This yields the necessary action steps and the final state at the end of each step. This state is used as the start state of the next step. This classical planning is repeated for all steps of the sequence to plan the entire executable action procedure (the latter part of Fig. 5)."}, {"title": "Food State Recognition Learning from Small Data using Vision-Language Model", "content": "Regarding state, which is the target state of each cooking action proposed in Section 2, the robot needs to be able to recognize whether the target state has been realized or not during the operation. Therefore, we propose a method for learning to recognize foodstuff states from a small amount of data by few-shot learning of the Vision-Language Model (VLM) (Fig. 6). As a method for few-shot learning, we use the linear-probe discussed in the article [24] of CLIP, which is one of the VLM. In the linear-probe, a linear discriminator on the features output by CLIP's image-encoder is trained on a supervised learning basis to classify images.\nFirst, a person visually specifies the time when a state change occurs in the previously acquired time series data. Next, a linear-probe is used to train a foodstuff state recognizer by labeling images before the time specified by the human as pre-change (0) and images after the annotation time as post-change (1). During inference, the state identification is performed in real time on the input images, and the timing of the state change is defined as the point in time when the first post-change label is estimated.\nThe learning part of this method is learning a discriminator for the image features, and not a single time-series-specific tuning method as previously proposed, such as [17] and [25]. This allows us to learn by adding new data acquired each time an experiment is performed. Therefore, the robustness of the method can be improved with each new cooking session, and it is considered to be a more stable method for recognizing the state of food ingredients.\nThe use of VLM for object state recognition has been actively studied in recent years and is a very effective method. [26] uses CLIP to detect object states by calculating the degree of association between predefined text representing object states and images. In [17, 25] we fine-tune the VLM output to a state-changing time series for the data time series. While these methods require multiple language descriptions of the target in advance, the method used in this study is independent of the accuracy of the language descriptions because it uses the image features of CLIP's image-encoder as is. In the field of computer vision, research has also been conducted on state recognition using VLMs [27\u201329]. These studies mainly include generalization of recognition ability to different objects or situations and extraction of relevant frames from noisy videos. Since the objective in this study is to incorporate stable state change point recognition into the system, we employed a very simple linear-probe of CLIP, but there is a possibility that the recognizable range of state changes can be increased by applying the methods of these studies in the future."}, {"title": "Experiments for the Real World Robot Cooking from Recipes", "content": "Experiments were conducted to confirm the validity of the Section 3.2 using known recipes. The pro-posed method was used to plan the action procedures for three egg dishes based on the sequence of function representations. In this experiment, the initial conditions were set up in the kitchen with the necessary ingredients and utensils for each dish. However, it is also possible to plan actions based on the condition that each ingredient or tool is set in a predetermined location, or based on the current situation recognized by the robot in some other way.\nThe result of the action planning for the poached egg is shown in Fig. 9. Since the initial condition is that there is no water in the pot, the action plan is completed with the following actions: getting a measuring cup, getting water from the tap, pouring water into the pot, and so on. We also confirmed that the desired action plans for scrambled egg (Fig. 10) and sunny-side up (Fig. 11) were also created by complementing the actions such as moving, holding and placing objects, and turning on and off the stove.\nAn experiment was conducted to plan actions with PDDL under different initial conditions than in the previous experiment (Fig. 12). For the three known recipes, we also planned actions for the initial condition in which all the tools and ingredients are in the kitchen spot, and confirmed that the action planning was appropriate, with more movement steps than in the previous examples."}, {"title": "Experiments of Food State Recognition Learning from Small Data using Vision-Language Model", "content": "To evaluate the performance of the recognizer, we collected data on three food state changes in unknown recipes: \"heat(butter, melt)\u201d(Butter melting), \"boil(water, boiled-water)\u201d(Water boiling), and \"cook(egg, cooked-egg)\"(Egg cooking), and conducted experiments to apply the proposed method (Section 4). In all experiments, image time series data were collected at 10hz, the rate at which CLIP can reason in real time."}, {"title": "Real-World Experiments with Robot Cooking Execution from Recipes", "content": "Experiments (Fig. 19 and Fig. 21) were conducted in which the robot cooked an arranged unknown recipe in the real world, and the effectiveness of the proposed system was confirmed. The proposed system plans cooking actions that can be executed in the real world based on natural language recipe descriptions (Section 3), and executes them in sequence while recognizing changes in the state of food ingredients during heating using the Vision-Language model(Section 4). In this experiment, the robot performed the motions that were created by a human by using direct teach, etc. Please refer to the attached movie for the actual cooking process as seen from the robot's camera.\nA new recipe for sunny-side up, arranged to use butter instead of oil, was cooked by the robot in the real world using the proposed system. First, as shown in Fig. 18, the robot planned actions that could be executed in the real world from the given natural language recipe using the method proposed in Section 3, converting it into a cooking function sequence using LLM, and then planning PDDL from the sequence. Since the starting condition was a situation in which butter and eggs were placed in bowls, actions such as holding the bowls containing the butter and eggs and turning on the stove were complemented.\nThe robot executed the planned cooking actions while recognizing the state of the food ingredients using the method proposed in Section 4. The robot experimenting with cooking autonomously is shown in Fig. 19. The robot's recognition of whether the eggs were cooked or not ended immediately because the frying pan was quite hot when the eggs were poured and it took time to put the bowl down after the eggs were poured. However, when the sunny-side up was transferred from the frying pan to a plate im-mediately after the robot finished cooking, it was confirmed that the sunny-side up was cooked properly without collapsing.\nThe cooking was executed in the same way for the unknown recipe of boiled and sauteed broccoli. Real-world action planning from the recipe was performed as shown in Fig. 20. The robot autonomously executes the planned action steps while recognizing changes in the state of the food ingredients as shown in Fig. 21. Although there are some points to be improved, such as the fact that the robot uses a net ladle for the experimental circumstance, the real-world behavior is a little irregular in this part, and the time until the robot judges that the butter is melted is short, and it seems that the butter was not yet completely melted when the robot judged. However, we were able to actually cook broccoli.\nThe robot's cooked dishes served on plates are shown in Fig. 22. The robot executed the cooking using real ingredients in the real world, and confirmed that the finished dishes were tasty and human-eatable by eating them."}, {"title": "Discussion", "content": "This section mainly describes the performance and limitations of this study. First, the action plan part is described. The range of recipes that can be covered by this setup is limited due to the small number of cooking functions prepared, and it is considered possible to handle cocktails, smoothies and other juices, salads, soups, and other dishes on the assumption that the ingredients have been prepared. In order to handle more recipes, it is necessary to add more cooking functions, and although our method can handle complex recipes with many steps, further efforts are needed to keep the accuracy of the action plan high even when the recipes are complex. In interpreting complex recipes, it may be difficult to maintain continuity of the object indicated by the function arguments between steps of the function sequence. It would be effective to express the arguments as variables with indices and check whether the entire functional expression is convertible into a valid graph by referring to the previous study on the function expression of cooking [32, 33], and to re-plan by feeding back error messages and other results to the LLM by referring to the previous study on the use of LLM [21]. In addition, in cooking, it is possible to execute multiple tasks simultaneously using multiple stoves, such as simultaneous cooking of multiple dishes or simultaneous cooking of pasta sauce and noodles, and PDDL itself can also execute multiple tasks simultaneously. Therefore, the action planning method combining LLM and PDDL in this study is expected to be effective in the future development of cooking robot systems, including more complex recipes and simultaneous execution of multiple tasks.\nNext, we will discuss the part of the recognition of state change of food ingredients. In cooking, various state changes occur, and it is necessary to recognize state changes in situations where there are multiple ingredients in a pot or pan. Our state recognition method is able to deal with such situations by learning from a small amount of the relevant data. For situations where there is no data for the same situation, there is a possibility that state recognition for unknown state changes and unknown situations can be improved by referring to the research on generalization of state change recognition to different objects [29] for further improvement, which should be verified in the future. On the other hand, the method in this study is also considered to be very effective for stable state change recognition in the presence of a small number of data, and a system that uses two different methods for unknown and known states may be effective. It will be also important to recognize whether the food has become soft or not by tactile sensation, to sense the state of burnt food and temperature by sense of smell and temperature, and to evaluate the final dish by these senses together with taste.\nFinally, we discuss the motion execution part. In this research, cooking was performed by execut-ing motion trajectories predefined by a human. Finally, we discuss the motion execution part. In this research, cooking was performed by executing motion trajectories predefined by humans. However, in order for robots to be able to perform more complex movements and manipulation of foods with indi-vidual differences, it will be necessary to implement adaptive motion execution using machine learning. Efficient use of imitation learning [34], which learns movement skills from human piloted data, in com-bination with image recognition or visual information extraction using the Foundation Models, etc., is considered effective."}]}