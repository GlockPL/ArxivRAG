{"title": "VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning", "authors": ["Jingkun Ma", "Runzhe Zhan", "Derek F. Wong", "Yang Li", "Di Sun", "Hou Pong Chan", "Lidia S. Chao"], "abstract": "Although previous research on large language models (LLMs) and large multi-modal models (LMMs) has systematically explored mathematical problem-solving (MPS) within visual contexts, the analysis of how these models process visual information during problem-solving remains insufficient. To address this gap, we present VisAidMath, a benchmark for evaluating the MPS process related to visual information. We follow a rigorous data curation pipeline involving both automated processes and manual annotations to ensure data quality and reliability. Consequently, this benchmark includes 1,200 challenging problems from various mathematical branches, vision-aid formulations, and difficulty levels, collected from diverse sources such as textbooks, examination papers, and Olympiad problems. Based on the proposed benchmark, we conduct comprehensive evaluations on ten mainstream LLMs and LMMs, highlighting deficiencies in the visual-aided reasoning process. For example, GPT-4V only achieves 45.33% accuracy in the visual-aided reasoning task, even with a drop of 2 points when provided with golden visual aids. In-depth analysis reveals that the main cause of deficiencies lies in hallucination regarding the implicit visual reasoning process, shedding light on future research directions in the visual-aided MPS process.", "sections": [{"title": "1 Introduction", "content": "The formulation of mathematical problems contains numerous elements hidden in the textual dimension. For instance, the transformation between visual and text modality effectively enlarge the decision space of large language models (LLMs), thereby increasing the complexity of mathematical problem-solving (MPS).\nAlthough general-purpose mathematical reasoning remains a weakness of LLMs or large multi-modal models (LMMs) [1, 2, 3, 4], multi-modal MPS is"}, {"title": "2 VisAidMath", "content": ""}, {"title": "2.1 Data Creation", "content": ""}, {"title": "Principles", "content": "A typical problem within our VisAidMath benchmark comprises four parts: Visual Context (C), Question (Q), Visual Aids (V), and Answer (A). The main task involves prompting the model to generate visual aids that assist in mathematical reasoning. While the visual context may be optional, spatial descriptions are included as essential data elements within the question. Given that many text-based LLMs lack image understanding or generation capabilities, we have additionally annoatated precise captions for both the visual context and the visual aids through annotation. This allows us to extend the evaluation scenarios to models that are constructed with limited modality. By providing these detailed captions, we are able to accommodate models with varying modality capabilities."}, {"title": "Data Sources", "content": "In accordance with the aforementioned principles, the VisAidMath benchmark has been manually collected and annotated using a diverse and balanced set of data sources. Through an extensive search and careful examination on a case-by-case basis, we discovered that the Chinese community offers a larger pool of mathematical problems with visual aids across various complexity levels and mathematical branches compared to other communities. As a result, we primarily collected data from Chinese sources and subsequently performed machine translation. To ensure a range of difficulty levels, we categorized the data samples based on their sources into the following categories: 1) Easy: e.g., High School Entrance Examination, 2) Medium: e.g., College Entrance Examination, and 3) High: e.g., Mathematical Olympiad. Additionally, metadata has been included for further in-depth analysis, which will be discussed in Appendix B.1."}, {"title": "Mathematical Categories", "content": "To ensure diversity and balance, we manually collected and annotated a range of categories within the benchmark. The questions primarily belong to four mathematical branches: Plane Geometry, Solid Geometry, Analytical Geometry, and Calculus and Function.\nFor visual aids, we introduce the following solution elements: Auxiliary Line, Plane Geometry Graph, Solid Geometry Graph, Function Graph, Plane Coordinate System, and Three-Dimensional Coordinate System. Examples from different categorizations can be found in Appendix B.3."}, {"title": "Construction Pipeline", "content": "As depicted in Figure 8, We propose a construction pipeline for the VisAidMath dataset, which incorporates multi-round verification and dynamic quality control based on feedback. The dataset creation pipeline involves four key roles:\n\u2022 Administrator: This role assigns daily collection tasks based on the progress and previous annotation feedback.\n\u2022 Collector: The collector searches for data that satisfies the assigned collection tasks. The collected data should be in PDF format and later transformed into LATEX files using OCR.\n\u2022 Annotator: The annotator first validates and refines the IATEX files by comparing the original PDF files provided by the collector with the transformed LATEX files. Then, the annotator performs interactive labeling using our designed tool. To ensure a balanced distribution across different categories, the annotator regularly analyzes the data distribution and provides feedback on the current progress and any annotation issues to the collector and administrator.\n\u2022 Verifier: The verifier is responsible for validating the categorization and data quality. If labels are not appropriate, they adjust the annotated captions of the context and visual aids."}, {"title": "2.2 Benchmark Attributes", "content": "The distribution of data sources is presented in Figure 3, providing a comprehensive overview of the dataset's origins. Additionally, the mathematical branches within the dataset exhibit a well-balanced distribution, as depicted in Appendix B.3.1. This balance enables a broader exploration of diverse mathematical knowledge. It is worth noting that the distribution of visual aids labels is slightly imbalanced. This occurs because mathematics problems often involve overlapping types, and basic elements such as auxiliary lines are frequently incorporated as necessary components within more complex visual aids. Further details on the distribution of visual aid types can be found in Appendix B.3.2. In terms of question types, VisAidMath encompasses three main formats: Multiple Choice, True/False, and Free-form. The answer formats encompass integers, decimals, fractions, and choices, ensuring unambiguous evaluation criteria."}, {"title": "2.3 Task Definition", "content": "VisAidMath introduces the concept of visual aids as a crucial component in bridging the gap between text-centric reasoning and visual-aided reasoning. As a result, a series of novel tasks are proposed that involve generating or leveraging visual aids alongside mathematical reasoning to achieve the correct answers. These tasks mainly are categorized into two formulations:"}, {"title": "Task 1: Vanilla Visual-Aided Reasoning", "content": "In this task, the model is required to understand the visual context (C) and the question (Q). It then generates visual aids (V) and deduces the answer (A) with the assistance of the generated visual aids. We denote this reasoning process as CQ2VA (Visual Context + Question \u2192 Visual Aids + Answer)."}, {"title": "Task 2: Reasoning with Provided Visual Aids", "content": "In this task, the model is provided with visual aids (V) along with the visual context (C) and the question (Q). The model uses this information to deduce the final answer (A). We denote this reasoning process as CQV2A (Visual Context + Question + Visual Aids \u2192 Answer).\nWe illustrate the distinctions between these tasks and general visual reasoning in Figure 4. To accommodate language models that only accept textual inputs, we transcribe the visual input into textual descriptions as a substitute for visual information. For all transcribed or provided information, we prefix the corresponding notation with \u201cp\u201d. For example, \u201cpV\u201d denotes the visual aids provided to the model, and \"pC\" represents the description of the input image."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Models", "content": "We conducted experiments on two types of tasks mentioned above, considering various input modalities. We explored the visual-aided reasoning capabilities of both open-source and closed-source LLMs and LMMs. The models evaluated on the VisAidMath benchmark include: 1) Open-source LLMs: Llama-2-7B [8], Mistral-7B-Instruct-v0.2 [9]; 2) Closed-source LLMs: GPT-3.5-turbo [10], GPT-4-turbo [11]; 3) Open-source LMMs: LLaVA-Next-Mistral-7B [12], InternLM-XComposer2-VL [13]; 4) Closed-source LMMs: Qwen-VL-Plus [14], Gemini-Pro-Vision [15], Claude-3-Sonnet [16], GPT-4-Vision [17]. For all open-source models, a temperature of 1.0 was set for decoding. Given that the Gemini-Pro-Vision model requires both text query and image input, thus we utilized the Gemini-Pro model to handle reasoning in scenarios without visual context."}, {"title": "3.2 Evaluation", "content": "Given that VisAidMath comprises mathematics problems with deterministic answers, we evaluate the correctness of problem-solving using the Accuracy score as a fundamental metric. To accurately extract the final answer from model outputs, we follow the approach of Lu et al. (2023) [7] and employ SOTA LLMs as answer extractors. Specifically, we selected GPT-4 as the model for answer extraction, as it demonstrated a high success rate in"}, {"title": "3.3 Main Results", "content": "The accuracy results for Task 1: Visual-Aided Reasoning are presented in Table 1. While the GPT-4 series outperforms most models across all three modality settings in this task, some open-source LLMs and LMMs perform below the random choice baseline on certain tasks (24.42% accuracy). Specifically, Llama-2-7B achieves only 23.25% and 24.08% accuracy on the pCQ2A and pCQpV2A tasks, respectively, and LLaVA-Next-Mistral-7B attains only 23.08% accuracy on the pCQ2VA task. Furthermore, except for GPT-4 and GPT-4V, all other models achieve lower performance than the most frequent choice baseline, indicating the significant challenge presented by the collected math problems. Notably, only Llama-2-7B and GPT-3.5 perform better on the (p)CQ2VA task compared to the other tasks, highlighting the deficiency of most models in visual-aided reasoning."}, {"title": "Reasoning Quality", "content": "Figure 5a illustrates the low similarity between general reasoning and visual-aided reasoning answers. This suggests that the task we designed differs significantly from general reasoning tasks, and the visual-aided reasoning capability remains a crucial bottleneck for some models. GPT-4V not only achieves high accuracy on the CQ2VA task but also demonstrates distinct reasoning steps compared to the general mathematical problem-solving process. This indicates its excellent comprehension of the visual-aided reasoning task. Figure 5b shows how well each model deduces the visual aids. We observe that while GPT-4V achieves high accuracy, its reasoning steps for visual aids deviate from the references. On the other hand, models like Gemini-Pro-Vision, InternLM, and Claude-3-Sonnet are more likely to produce visual-aided reasoning steps similar to the references, but their final answers are incorrect. These findings prompt us to analyze the specific errors in the reasoning steps and how they lead to incorrect answers."}, {"title": "4 Analysis", "content": ""}, {"title": "4.1 Quantitative Analysis", "content": "To uncover the extent to which current models overlook visual aids in the reasoning process, we perform an analysis using the vanilla visual-aided reasoning setting (CQ2VA). We randomly sample 200 model outputs with correct answers and analyze the causes that led to the final resolution. We categorize and annotate the reasoning causes as follows:\n\u2022 General: Correct reasoning without relying on visual aids.\n\u2022 Arithmetic: Correct reasoning using pure arithmetic methods.\n\u2022 Visual-Aided: Correct reasoning incorporating the use of visual aids.\n\u2022 Backward: Correct reasoning derived from provided choices or the final conclusion.\n\u2022 Hallucination.\nAs shown in Figure 6a, only 3.0% of the resolved questions benefit from generating visual aids, indicating a negligible inclination towards visual-aided reasoning. Additionally, 19.3% of the cases directly perform general reasoning steps without utilizing any visual aids. The majority of cases tend to resolve problems using arithmetic methods (41.1%) or through false inference with hallucination (33.2%). This distribution highlights the significant tendency of models to proceed with reasoning along a text-only trajectory, disregarding the potential benefits of visual aids."}, {"title": "Failure Analysis of Visual Aids Generation", "content": "Previous experimental results have shown low quality in terms of generating visual aids. Therefore, we conducted a fine-grained analysis to determine the causes of poor visual-aided reasoning steps in the CQ2VA task. We randomly sampled 200 outputs"}, {"title": "4.2 Other Analysis", "content": "To provide a comprehensive and fine-grained analysis of the model capability in different aspects, we present experimental results within various categories. We conduct an analysis of result distribution for the math category (see Appendix G.2.1), complexity (see Appendix G.2.2), and visual-aid type (see Appendix G.2.3). Each category includes comprehensive visualizations for different tasks. In all tasks, GPT-4V outperforms other models in each aspect, demonstrating a significant advantages in visual-aided reasoning."}, {"title": "5 Related Work", "content": ""}, {"title": "Benchmark", "content": "Numerous benchmarks have been developed to evaluate mathematical reasoning abilities in both textual and multi-modal tasks. These benchmarks primarily rely on textual inference as the reasoning tool. Regarding the text-only task, arithmetic problems with pure numerical expressions [19] and MPS (Mathematical Problem Solving) [20] have been extensively explored. On the multi-modal side, benchmarks focus on mathematical problems within the geometry category to foster research on spatial understanding and properties deduction [21, 22]. Other multi-modal benchmarks concentrate on general visual contexts, such as bar charts [6]. More recently, [7] established a comprehensive benchmark that incorporates different visual contexts. However, these benchmarks primarily rely on textual reasoning to solve mathematical problems, limiting the comprehensive mathematical decision space to a singular text dimension. In contrast, humans tend to combine visual and textual reasoning to exploit latent properties and ease the complexity of reasoning. Therefore, we propose the VisAidMath benchmark, which incorporates visual aids in the reasoning process."}, {"title": "LLMs for Mathematical Reasoning", "content": "LLMs have not achieved satisfactory performance on mathematical reasoning benchmarks under zero or few-shot settings [23]. To further enhance these models, chain-of-thought reasoning is introduced for step-wise reasoning, and various prompt engineering methods are provided to improve generation control [24]. In the multi-modal setting, LLMs can leverage rich information from visual contexts for subsequent mathematical reasoning. Recent studies [12, 17] explore reasoning over diverse figures that contain abundant numerical and spatial information. Interaction with external tools [25] and downstream instruction tuning [26] are also widely employed to improve overall reasoning quality. Another relevant idea proposed by [13] explores inter-connected text-vision reasoning by providing text content with contextually relevant images sourced from a pre-defined dataset. However, contrary to our essential idea, these output images are deliberately generated to enhance the readability of the output content, rather than improving reasoning capabilities."}, {"title": "6 Conclusion", "content": "In this paper, we lay the groundwork for mathematical problem solving using multi-modal reasoning steps. We introduce VisAidMath, a benchmark designed to investigate the visual-aided reasoning capabilities of both large language models and large multi-modal models. Experiments on mainstream models demonstrate significant deficiencies in deducing visual aids and the corresponding textual reasoning steps. The best performing LMM, GPT-4V, achieves only 45.33% accuracy in the visual-aided reasoning task, indicating insufficient progress in this research direction. Most models exhibit even lower performance when tasked with visual aids. Furthermore, GPT-4V lacks adequate visual-aid generation capability, correctly inferring only 35% of samples with valid visual aids. We conduct fine-grained quantitative and qualitative analyses to reveal disparities in visual-aid inference and text-vision interconnected reasoning. These findings highlight a substantial research opportunity in improving foundational models, data, and evaluation methodologies."}, {"title": "A Future Direction", "content": "Spatial Capability Despite the predominant emphasis on the construction and fitting of extensive datasets, mainstream works are confined to inference tasks within textual dimension. LLM exhibits exceedingly poor performance in providing visual reasoning step, revealing deficiencies in spatial understanding, imagination, and more other aspects. To address mathematical reasoning with visual-aid inference, future investigation could be directed to specifically enhance LLM's adaptation to visual reasoning tasks, devise new methods for better integration of textual and visual reasoning, or design specific architectures for multimodal reasoning steps inference.\nMathematical Image Generation Preliminary experiments find mainstream models exhibit poor mathematical image generation performance, thus further captioning each mathematical images to explore visual reasoning step inference. Primary model deficiencies fall in: mathematical image caption comprehension, spatial relationships apprehension, lack of numerical precision, significant stylization discrepancies in the images, and more. Generate image drawing code can increase the overall drawing precision, while suffering from plenty of code errors. There lies a long research road in mathematical image generation before fully exploration of textual-visual interconnected inference.\nEvaluation Metrics Reasoning non-uniqueness enhances evaluation complexity of visual aids generation. Different viewing angle, relative element size, and styles can alter perceptual features instead of semantic feature. Visual-aid can be captioned by multiple correct expressions with semantic remains stable. Therefore, future evaluation metrics research for visual-aid should be directed toward semantic-based method."}, {"title": "B Dataset Analysis", "content": ""}, {"title": "B.1 Metadata", "content": "We list the manually annotated metadata for each sample in 2. \u201cvisual context\" is optional depending on whether image is provided along with the problem. \"choices\" is not empty when question form belongs to multiple choices or true/false. \"language\" stamp the original language of each problem. We also record the Chinese version text before machine translation with \"zh_\" prefix at the bottom of each data sample."}, {"title": "B.2 Data Source", "content": "We analyze the problem complexity of 16 data source following three difficulty levels: 1) Easy: Chinese High school entrance examination 2) Medium: Chinese College Entrance Examination 3) High: Chinese Mathematical Olympiad. The complete complexity categorization of each source is listed in 3. Particularly, since \"AP Calculus\" consists of both easy and medium level mathematical problems, we consider questions satisfying one of the following conditions as medium level: 1) involve coordinate axes rotation 2) cannot be resolved in one step leveraging Newton-Leibniz formula."}, {"title": "B.3 Examples for Different Categorizations", "content": ""}, {"title": "B.3.1 Math Branch", "content": "ID: 1114\nQuestion: determine that the sum of the squares of the lengths of the three medians of a triangle is equal to the sum of the squares of the lengths of the three sides"}, {"title": "ID: 1006", "content": "Visual Context:Two congruent triangles are glued together to obtain a hexahedron with all the dihedral angles equal, and the length of the shortest prong of the hexahedron is 2.\nQuestion: Given that two congruent triangular pheons are glued together to obtain a hexahedron with all the dihedral angles equal, and that the shortest prong of the hexahedron is 2, the distance between the two farthest vertices is"}, {"title": "ID: 619", "content": "Visual Context:In the trigonometry $A BCD$, $AB \\perp AD$, $BC \\perp BD$, the plane $ABD \\perp$ the plane $BCD$, points $E, F$(E do not coincide with $A, D$) are on the prisms AD, BD and EF BD respectively.\nAD EF //Plane ABC\nQuestion: In the trigonal pheasant $A \u2013 BCD$, $AB \\perp AD$, $BC \\perp BD$, plane $ABD \\perp$ plane $BCD$, points E, F(E and $A, D$ do not coincide) on the prong $AD$, $BD$ respectively, and $EF \\perp AD$.EF// plane $ABC$ determine whether $AD \\perp AC$."}, {"title": "ID: 939", "content": "Question: Can the surface of a container formed by rotating the curve $y = Cx$about the vertical axis Oycause the liquid surface to fall uniformly as it flows from the bottom of the container?"}, {"title": "B.3.2 Visual Aid Type", "content": "ID: 10\nVisual Context:In Rt \u2206ABC, ABAC = 90\u00b0, point D is the midpoint of BC, points E, F are points on AB, AC respectively, and ED | FD.\nQuestion: As shown in the figure, in Rt \u2206ABC,\u2220BAC = 90\u00b0, the point D is the midpoint of BC, the point E, F is the point on AB, AC, and ED | FD . Can a triangle be formed with the segment BE, EF, FC as its side? If so, determine the shape of the triangle."}, {"title": "ID: 719", "content": "Question: Given that the vector $\\vec{OA},\\vec{OB}$ satisfies $|\\vec{OA}| = 1,\\vec{OA}\\perp \\vec{OB},\\vec{OC} = \\lambda\\vec{OA} + \\mu \\vec{OB}(\\lambda,\\mu \\in R)$, if $M$ is the midpoint of $AB$and $|MC| = 1$ then the maximum value of $\\lambda + \\mu$ is ()"}, {"title": "ID: 1099", "content": "Visual Context:A square $ABCD \u2013 A_1B_1C_1D_1$ has prism length 1,001 is the incircle of the square $ABCD$, $O_2$ is the outer circle of the square $ADD_1 A_1$, and P, Q are the points on $0_1, 0_2$ respectively.\nQuestion: As shown in the figure, the prisms of the square $ABCD \u2013 A_1B_1C_1D_1$ have the lengths 1,$001$ as the tangent circle of the square $ABCD$, $O_2$ as the outer circle of the square $ADD_1A_1$, and $P, Q$ as the point on $0_1, 0_2$. Find the range of the length of PQ."}, {"title": "ID: 723", "content": "Question: Given that the plane vector a, b, c satisfies the angle between $|a| = 1, |b| = \\sqrt{3}, a \u00b7 b = 0, c \u2212 a$ and-bis, then the maximum value of $\u0109 \u00b7 (b \u2212 a)$ is"}, {"title": "ID: 220", "content": "Visual Context:In the square $SG_1G_2G_3$, E, F are the midpoints of $G_1G_2, G_2G_3$ respectively, and D is the midpoint of EF, connecting EF, SE, SF, $SG_2$.\nQuestion: As shown in the figure, in the square $SG_1G_2G_3$, E, F is the midpoint of $G_1G_2, G_2G_3$ and D is the midpoint of EF respectively. If this square is folded into a tetrahedron along SE, SF and EF so that the three points of $G_1, G_2, G_3$ coincide, and the point of coincidence is labeled G, then which of the edges of the tetrahedron SEFG are perpendicular to the faces of the tetrahedron?"}, {"title": "ID: 394", "content": "Question: Through the parabola $C : y\u00b2 = 2px(p> 0)$ The focus of F, as the slope of $2\\sqrt{2}line$ land parabola Cintersect at two points A, B, If $AF = \\FB$, x = ( )"}, {"title": "B.3.3 Complexity", "content": ""}, {"title": "ID: 30", "content": "Visual Context:Figure: In \u2206ABC, $BD = DC = AC$, AE is the center line of ADC.\nQuestion: It is known that, as shown in the figure: \u25b3ABC, $BD = DC = AC$, AE is the center line of ADC : judge if: $AB = 3AE$ ."}, {"title": "ID: 650", "content": "Visual Context:\u00c5B is a vertical pole standing on the horizontal ground BCD, where \u2220CBD = 90\u00b0\nQuestion: In the figure, AB is a vertical pole standing on the horizontal ground BCD, where \u2220CBD = 90\u00b0. If the angle between the plane ACD and the horizontal ground is 0, then tan \u03b8"}, {"title": "ID: 1024", "content": "Visual Context:In the right triangular prism $ABC \u2013 \u0410_1\u0412_1\u0421_1$, $\u0410\u0412 = AC = 5$, D, E are the midpoints of BC, $BB_1$ respectively, and the quadrilateral $B_1 BCC_1$ is a square with side length 6.\nQuestion: As shown in the figure, in the rectangular triangular prism $ABC \u2013 A_1B_1C_1AB$ = AC = 5, D, E are the midpoints of BC, $BB_1$ and the quadrilateral $B_1BCC_1$ is a square of side 6. determine that: A1B// is in the plane of $AC_1 D$."}, {"title": "C Dataset Collection Detail", "content": ""}, {"title": "C.1 Caption Writing Templates", "content": "Mathematical graphs are consists of shapes and elements bound with specific relation or theorem. To reduce manual annotation work and enhance caption consistency, we standardize the caption writing for visual context and visual aids by defining templates for certain visual elements. The annotators should caption image referring to these templates as listed in 17."}, {"title": "D Dataset Preparation", "content": ""}, {"title": "D.1 Machine Translation", "content": "Since we have collected most data from Chinese sources and annotated captions in Chinese, we need to uniformly translate all Chinese text into English. Open source machine translation (MT) models"}, {"title": "D.2 Data Process", "content": "We conduct further data process upon annotated data, adapting to model input interface and evaluation uniqueness. Since many models accept only one image in single generation round, we leverage tool [27] to merge images of visual context and visual aid respectively if multiple images exist in single data. We convert decimal answers to three decimal places, and transform fractional answers into the form with '/' as separator, where the integer is the numerator and the denominator. Mathematical problems with free form answers are reshaped as multiple choices or true/false problem, with particular choice as the answer. We also provide a final validation and modification round focus on the caption of visual context and visual aid, regarding description completeness."}, {"title": "D.3 Formalization", "content": "Each sample data is finalized into certain dir with one folder for visual context images, one folder for visual aid images, and one \u201cdata.json\" file. Each image are named as the order in which it appears in problem or reasoning (eg. 1.png, 2.png). The Merged image is named as \u201cconcatenate.png\". \"data.json\" file stores all text and metadata, including untranslated text within specific data. And we finally formalized the dataset under CC-BY-SA-4.0 license."}, {"title": "E Limitations and Social Impact", "content": "The limitation of VisAidMath is three-folded. First, dataset is restricted to 1200 samples since both collection, annotation and verification of mathematical problems acquire heavy manual work to satisfy dataset principles. Such mathematical problems with visual aids cost more human efforts to understand each segment before judgment. Secondly, deficiency of mainstream machine translation systems in mathematical domain could introduce various translation errors, thus enhancing complexity for problem solving and subsequent evaluation. Thirdly, we cannot conduct comprehensive analysis of visual-aided reasoning with image generation, since current LMMs remain significant deficiency in mathematical image generation. No negative social impact will be provided from our math-centric work, expecting only to enhance further understanding of LLM reasoning."}, {"title": "F Detail Experiment Settings", "content": ""}, {"title": "F.1 Hyperparameters", "content": "We utilize the default inference settings for each LLMs and LMMs in our experiments. Only specific hyperparameters that are necessary to clarify are listed in Table 18 and 19. We conduct open source model inference based on [28]."}, {"title": "F.2 Reasoning Prompter", "content": "We list the ICL prompts for assigning different models to perform reasoning under six task settings in mathematical domain: 1) CQ2A 2) CQ2VA 3) CQpV2A 4) pCQ2A 5) pCQ2VA 6) pCQpV2A. Task instructions for each task are listed in Table 20. ICL examples can be found at our code open sourced after acceptance."}, {"title": "F.3 Instruction across Answer Types", "content": "To facilitate accuracy evaluation, each sample is bound with with non-ambiguous result across integer, fraction, decimal and choice. We define choice as answer type for multiple choice and true/false problems. The specific task instruction for each output answer type is shown in Table 21."}, {"title": "F.4 Visual Aid Extraction", "content": "Reasoning prompter instruct models to provide visual aids, reasoning and answer following certain format. We can directly extract visual aids and followup reasoning from output generated by LLM and LMMs with sufficient zero-shot reasoning capability (eg. GPT-4, GPT-4V). However, few weaker models (eg. llama2, LLaVA-Next-Mistral-7B) fail to provide standardized output. We regard these output as both visual-aid and followup reasoning for later answer extraction and similarity evaluation respectively."}, {"title": "F.5 Answer Extraction Prompter", "content": "Answer extraction is conducted upon the extracted reasoning text from model output. We provide five ICL examples instructing GPT-4 to extract answer from varied model outputs. Examples are shown in Table 22."}, {"title": "G More Experimental Results", "content": ""}, {"title": "G.1 Results of other tasks", "content": ""}, {"title": "G.2 Quantitative Analysis", "content": ""}, {"title": "G.2.1 Performance across Math Branches", "content": "Figure 11, 12, and 13 illustrate the accuracy scores of LMMs upon three tasks with image input across four math branches. GPT4V outperforms other models in problems within \"plane geometry\", \"analytic geometry\u201d, and \u201ccalculus and functions\u201d branches. Gemini-Pro-Vision achieves the highest score on solid geometry problems. Notably Claude-3-Sonnet and InternLM-XComposer2-VL both achieves comparable results toward GPT4V in \u201csolid geometry\u201d branch when reason with provided visual-aided, exhibiting robustness and enhanced capabilities in spatial understanding and visual reasoning under \u201csolid geometry\u201d. GPT4V underperforms in visual-aided reasoning, exhibiting significant deficiency processing implicit visual information. Gemini-Pro-Vision performs better at \"analytic geometry\u201d and \u201ccalculus and functions\u201d with provided visual aids, demonstrating better understanding of visual context within these mathematical branches."}, {"title": "G.2.2 Performance across Complexities", "content": ""}, {"title": "G.2.3 Performance across Visual Aids", "content": "With Image as input across different visual aids required to generate, the accuracy scores of mainstream LMMs under three tasks are listed in Figure 23, 24, and 25. GPT-4V outperforms other models on problems with visual aids \u201cauxiliary line\u201d and \u201cfunction graph\" across all three tasks. In text-only reasoning task, GPT-4V achieves leading performance in text-only reasoning except for problems with rectangular three-dimensional coordinate system as visual aids. After introduce visual aids, Gemini-Pro-Vision significantly outperform other models on problems with solid geometry graph and three-dimensional rectangular coordinate system as visual aids. Gemini-Pro-Vision also achieves comparable result to GPT-4 in problem solving provided with auxiliary line. Comparing generated and provided visual-aids, overall average accuracy are enhanced saliently on \u201cauxiliary line\", \"plane coordinate system\u201d, and \u201cfunction graph\", exhibiting higher sensitivity in reasoning towards these visual aids."}, {"title": "H In-depth Model Analysis", "content": ""}, {"title": "H.1 General Reasoning Tendency", "content": "Question: As shown in the figure, the lateral prongs of the square triangular pheasant SABC are equal to the lengths of the sides of the base, and if E, F is the midpoint of SC, AB, then the angle between EF and SA is equal to ().\nChoices: A. 90\u00b0 \u0412. 60\u00b0 C. 45\u00b0 D. 30\u00b0\nReasoning: Connect SE and SF, E, F are midpoints of SC, AB respectively, and given that SABC is a square triangular pheasant with lateral prongs equal to the base, we know SE = SF = SC = AB. Since AB | BC and SC | BC, SE | SF, and triangle SEF forms a right triangle with EF as the hypotenuse. Since SE = SF, ASEF is also an isosceles right triangle, so \u2220SEF = 45\u00b0. Since SA I AB and SEI SF, plane SAE is perpendicular to plane SEF. Therefore, the angle between EF and SA is also 45\u00b0. So, the final answer is C. 45\u00b0.."}, {"title": "ID: 466", "content": "Question: Given that the plane vector a", "is\"\nChoices": "A.$ \\sqrt{10"}, "B.$ \\sqrt{6}$C.$ \\sqrt{5}$D.$ \\sqrt{2}$\nReasoning: To solve the problem, we need to analyze the conditions given and deduce the maximum value of c. **Given Conditions:** 1. |@| = $\\sqrt{2}$ 2. |6| = 1 3. \u0101\u00b7 b = \u22121 4. Angle between a \u2013 \u0109and b \u2013 cis $\\frac{\\pi}{4}$ **Reasoning:** - From a \u00b7 b = \u22121, we use the formula for dot product a \u00b7"]}