{"title": "RKadiyala at SemEval-2024 Task 8: Black-Box Word-Level Text Boundary\nDetection in Partially Machine Generated Texts", "authors": ["Ram Mohan Rao Kadiyala"], "abstract": "With increasing usage of generative models\nfor text generation and widespread use of ma-\nchine generated texts in various domains, being\nable to distinguish between human written and\nmachine generated texts is a significant chal-\nlenge. While existing models and proprietary\nsystems focus on identifying whether given text\nis entirely human written or entirely machine\ngenerated, only a few systems provide insights\nat sentence or paragraph level at likelihood of\nbeing machine generated at a non reliable ac-\ncuracy level, working well only for a set of\ndomains and generators. This paper introduces\nfew reliable approaches for the novel task of\nidentifying which part of a given text is ma-\nchine generated at a word level while compar-\ning results from different approaches and meth-\nods. We present a comparison with proprietary\nsystems, performance of our model on unseen\ndomains' and generators' texts. The findings\nreveal significant improvements in detection ac-\ncuracy along with comparison on other aspects\nof detection capabilities. Finally we discuss\npotential avenues for improvement and impli-\ncations of our work. The proposed model is\nalso well suited for detecting which parts of\na text are machine generated in outputs of In-\nstruct variants of many LLMs.", "sections": [{"title": "Introduction", "content": "With rapid advancements and usage of AI models\nfor text generation, being able to distinguish ma-\nchine generated texts from human generated texts\nis gaining importance. While existing models and\nproprietary systems like GLTR (Gehrmann et al.,\n2019), ZeroGPT (ZeroGPT), GPTZero (Tian and\nCui, 2023), GPTKit (GptKit), Open AI detector\netc.. focus on detecting whether a given text\nis entirely AI written or entirely human written\n, there was less advancement in detecting which\nparts of a given text are AI written in a partially ma-\nchine generated text. While some of the above\n,\nmentioned systems provide insights into which\nparts of the given text are likely AI generated,\nthese are often found to be unreliable and having\nan accuracy close or worse than random guess-\ning. There is also a rise in usage of AI to spread\nfake news and misinformation along with using\nAI models to modify Wikipedia articles (Vice,\n2023). Our proposed model focuses on detecting\nword level text boundary in partially machine gen-\nerated texts as part of the SemEval shared task\n: Multi-generator, Multi-domain, and Multilin-\ngual Black-Box Machine-Generated Text Detec-\ntion(Wang et al., 2024b). This paper also discusses\nimplications of findings, comparisons with dif-\nferent models and approaches, comparison with\nexisting proprietary systems with relevant metrics,\nother findings regarding AI generated texts. The of-\nficial submission is DeBERTa-CRF, several other\nmodels have been tested for comparison. With\nnew, better, and diverse AI models coming into\nexistence, having a model that can make accurate\npredictions on unseen domains and unseen genera-\ntor texts can be useful for practical scenarios."}, {"title": "Dataset", "content": "The dataset used is part of M4GT-bench\nDataset(Wang et al., 2024a) consisting of texts\neach of which are partially human written and par-\ntially machine generated sourced from PeerRead\nreviews and outfox student essays (Koike et al.,\n2023) all of which are in English. The genera-"}, {"title": "Baseline", "content": "The provided baseline uses finetuned Longformer\nover 10 epochs. The baseline classifies tokens indi-\nvidually as human or machine generated and then\nmaps the tokens to words to identify the text bound-\nary between machine generated and human written\ntexts. The final predictions are the labels of words\nafter whom the text boundary exists. The detection\ncriteria is first change from 0 to 1 or vice versa.\nWe have tried one more approach by considering\nthe change only if consecutive tokens are the same.\nThe baseline model achieved an MAE of 3.53 on\nthe Development set which consists of same source\nand generator as the training data. The model had\nan MAE of 21.535 on the test set which consists of\nunseen domains and generators."}, {"title": "Proposed Model", "content": "We have built several models out of which\nDeBERTa-CRF was used as the official submis-\nsion. We have finetuned DeBERTa(He et al.,\n2023), SpanBERT(Joshi et al., 2020), Long-\nformer(Beltagy et al., 2020), Longformer-pos\n(Longfomer trained only on position embeddings),\neach of them again along with Conditional Random\nFields (CRF)(McCallum, 2012) with different text\nboundary identification logic by training on just the\ntraining dataset and after hyperparameter tuning,\nthe predictions have been made on both develop-\nment and test sets. CRFs have played a vital role\nin improving the performance of the models due\nto their architecture being well suited for pattern\nrecognition in sequential data. The primary metric\nused was Mean Average Error (MAE) between pre-\ndicted word index of the text boundaries and the\nactual text boundary word index. However Mean\nAverage Relative Error (MARE) too was used for\na better understanding which is the ratio of MAE\nand text lenght in words. Some of the plots and in-\nformation couldn't be added due to page limits and\nare available here. 1 along with the code used. 2.\na hypothetical example in Figure 1 demonstrates\nhow the model works. The tokens are classified at\nfirst and mapped to words. In cases where part of\na word is predicted as human and rest as machine\n(in case of longer words), the word as a whole is\nclassified as machine generated."}, {"title": "Our system", "content": "We have used 'deberta-v3-base' along with CRF\nusing Adam(Kingma and Ba, 2017) optimizer over\n30 epochs with a learning rate of 2e-5 and a\nweight decay of le-2 to prevent overfitting. other\nmodels that have been used are 'Spanbert-base-\ncased', 'Longformer-base-4096', 'Longformer-\nbase-4096-extra.pos.embd.only' which is similar to\nLongformer but pretrained to preserve and freeze\nweights from RoBERTa(Liu et al., 2019) and train\non only the position embeddings. The large vari-\nants of these have also been tested however the\nbase variants have achieved better performance on\nboth the development and testing datasets. pre-\ndictions have been made on both the development\nand testing datasets by training on just the training\ndataset. Two approaches were used when detecting\ntext boundary 1) looking for changes in token pre-\ndictions i.e from 1 to 0 or 0 to 1. and 2) looking\nfor change to consecutive tokens i.e 1 to 0,0 or 0\nto 1,1. Approach 2 achieved better results than ap-\nproach 1 in all the cases and was used in the official\nsubmission."}, {"title": "Results", "content": "The results from using different models with the\ntwo approaches on the development set and the test\nset can be seen in Table 2. These models have been\ntrained over 30 epochs and the best results were\nadded among the several attempts with varying hy-\nperparameters. The provided baseline however has\nbeen trained on just through approach I over 10\nepochs using base variant of Longformer. These\nmodels have then been used to make predictions\non the test set without further training or changes\nusing the set of hyperparameters that produced the\nbest results for each on the development set. How-\never MAE which is the primary metric of the task\ndoesn't take length of the text into consideration,\nHence MARE (Mean Average Relative Error) was\nalso calculated for a better understanding."}, {"title": "Comparison with proprietary systems", "content": "Some of the proprietary systems built for the pur-\npose of detecting machine generated text provide\ninsights into what parts of the text input is likely\nmachine generated at a sentence / paragraph level.\nMany of the popular systems like GPTZero, GP-\nTkit, etc.. are found to to less reliable for the task\nof detecting text boundary in partially machine gen-\nerated texts. Of the existing models only ZeroGPT\nwas found to produce a reliable level of accuracy.\nFor the purpose of accurate comparison percentage\naccuracy of classifying each sentence as human /\nmachine generated is used as ZeroGPT does detec-\ntion at a sentence level."}, {"title": "Results comparison", "content": "Since the comparison is being done at a sentence\nlevel, In cases where actual boundary lies inside\nthe sentence, calculation of metrics is done on the\nremaining sentences, and when actual boundary\nis at the start of a sentence, all sentences were\ntaken into consideration. With regard to predic-\ntions, A sentence prediction is deemed correct only\nwhen a sentence that is entirely human written is\npredicted as completely human written and vice\nversa. The two metrics used were average sentence\naccuracy which is average of percentage of sen-\ntences correctly calculated in each input text, and\noverall sentence accuracy which is percentage of\nsentences in the entire dataset accurately classified.\nThe results on the development and test sets are as\nshown in Table 3. Since its difficult to do the same\non 12000 items of the test set, a small section of\n500 random samples were used for comparison and\nwere found to perform similar to the development\nset with a 15-20 percent lower accuracy than the\nproposed models. Since ZeroGPT's API doesn't\ncover sentence level predictions, they have been\nmanually calculated over the development set and\ncan be found here. 3."}, {"title": "Conclusion", "content": "The metrics from Table 3 demonstrate the proposed\nmodel's performance on both seen domain and gen-\nerator data (dev set) along with unseen domain and\nunseen generator data (test set), hinting at wider\napplicability. While there was a drop in accuracy\nat a word level, there was an increase in sentence\nlevel accuracy."}, {"title": "Strengths and Weaknesses", "content": "It was observed that the proprietary systems used\nfor comparison struggled with shorter texts. i.e\nwhen the input text has fewer sentences, the predic-\ntions were either that the input text is fully human\nwritten or fully machine generated leading to com-\nparatively low accuracy.\nThe average accuracy of sentence level classifica-\ntion for each text length of our model and ZeroGPT\ncan be seen in Figure 2, Figure 3 respectively. the\nproposed model overcomes this issue by providing\nmore accurate results even on short text inputs.\nThe sentence level accuracy did vary consider-\nably while comparing cases where the actual text\nboundary is at the end of sentence and those where\nit is mid sentence. The results can be seen in Ta-\nble 4.\nSince the source and generators of texts individ-\nually wasn't made available, the comparison be-\ntween in-domain and out-of-domain texts couldn't\nbe made."}, {"title": "Possible Improvements", "content": "DeBERTa performed better when text boundaries\nare in the first half of the given text, while Long-\nformer had better performance when the text bound-\nary is in the other half as seen in Figure 4 and\nFigure 5. In cases where there was a significantly\nbigger MAE, atleast one of two (DeBERTa and\nLongformer) had made a very close prediction.\nThere is a possibility that an ensemble of both"}, {"title": "Limitations and potential for misuse", "content": "While this novel task of detecting text boundaries in\npartially machine generated texts achieves a high\naccuracy where one change from human to ma-\nchine occurs. Being able to handle the cases of\nmultiple changes from human to machine and vice\nversa is vital. Since having a completely machine\ngenerated text and rewriting a few sentences in be-\ntween or vice versa isn't covered by this work or\nother existing models, there is a possibility that\ndetection can be evaded this way. There is also\na potential for misuse by learning what features\nand texts caused errors using the proposed mod-\nels to create texts that can evade detection. The\ncurrent study covers only two kinds of LLMs i.e\nGPT and LLaMa. The performance on other types\nof LLMs is still to be tested. With wide range of\navailable LLMs, training the models over wider\nrange of LLMs might improve performance. The\ncurrent work focuses on just English texts, however\nit can be extended to other languages by replacing\nDeBERTa with mDeBERTa and training on a mul-\ntilingual corpus. However not all languages are\ncovered by mDeBERTa, this can be a potential is-\nsue when dealing with multilingual texts. Another\nkind of texts that need to be tested upon is where\nmachine generated portions are generated by differ-\nent generators, and the cases where it is completely\nmachine generated but by different generators. The\ncurrent corpus used to trained the models is sourced\nfrom academic platforms and academic essays, It\nis necessary to have models to work over a wide\nvariety of texts including cases where it can be in\na casual tone. While the current work only consid-\ners the first 512 tokens, the longformer version did\nachieve the same results on unseen generator texts.\nIt is worth looking into how well chunking would\nwork on the deberta model to process longer texts."}, {"title": "Possible Extensions and Applications", "content": "The need to detect AI generated content is also\nprevalent over all languages. While the current\nmodel utilizes just English language data, gath-\nering multilingual data and having a multilingual\nmodel might also be of great use. With the growth\nof misinformation and fake news using bots on so-\ncial media handles(Zellers et al., 2019), being able\nto detect AI generated texts is of great importance.\nAs most of the texts i.e posts, comments etc.. are\nshorter in length and difficult to detect, An exten-\nsion of the current work by training on social media\ndata may yield a good result as demonstrated in Fig-\nure 2 and Figure 3. The dataset mostly consisted\nof texts which are academic related while there is\na need to detect machine generated texts in other\nfields too. Also, It is worth testing the performance\nof paraphrased data along with the existing data.\nSince, usage of additional data was prohibited, data\naugmentation wasn't used in training the current\nmodels. It is likely that having more data to cover\nthe cases of pre and post POS tags that weren't\npresent in the training dataset may improve the per-\nformance of the models. Some of the other findings\nare available in Appendix A."}, {"title": "Other Plots and information", "content": "Some of the information that couldn't be covered\ndue to page limitations along with details for sys-\ntem replication have been added here."}, {"title": "POS tag usage : human vs machines", "content": "It can be seen from Figure 11, Figure 12 and Fig-\nure 10 that machine generated texts had higher\nshare of certain POS tags in the machine gener-\nated parts compared to the human written parts.\nThis was observed in all 3 sets, the train and dev\nhad similar distributions as a result of using same\ngenerators i.e ChatGPT and the test had a bit of\na variation due to multiple different generators i.e\nLLaMA2 and GPT4. Although the percentile com-\nparison did vary from train, dev and test sets, it\nwas minimal."}, {"title": "MAE characteristics : DeBERTa vs\nLongformer", "content": "As discussed in the paper, there were some in-\nstances where one model performed significantly\nbetter than the other as seen in Figure 8 and Fig-\nure 9 hinting that an ensemble of both's predictions\nmight yield better results."}, {"title": "System Description", "content": "DeBERTa-CRF was the official submission,\nlongformer.pos-CRF had almost the same perfor-\nmance on the test set. i.e 18.538 and 18.542.\nOther models that have been tested but were\nfound to have a big margin of performance with\nabove listed models\nDue to time and computational resources lim-\nitation, only a part of hyperparameter space was\nexplored."}, {"title": "Effect of Text boundary location on\nperformance", "content": "The location of text boundaries with respect to\nlength of the text samples are varying over the\ntraining and testing set as seen in Figure 13 and\nFigure 14. Despite training on samples where the\ntext boundaries are in the first half in most of the\ncases, the models did perform well on the testing\nset where there is a good amount of samples with\ntext boundaries in later half. This is an area where\nthe proprietary systems struggled."}]}