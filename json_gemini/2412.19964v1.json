{"title": "Single-view and Multi-view Fused Depth Estimation with Mamba", "authors": ["Zelin Meng", "Zhichen Wang"], "abstract": "Multi-view depth estimation has achieved impressive performance over various benchmarks. However, almost all current multi-view systems rely on given ideal camera poses, which are unavailable in many real-world scenarios, such as autonomous driving. In this work, we propose a new robustness benchmark to evaluate the depth estimation system under various noisy pose settings. Surprisingly, we find current multi-view depth estimation methods or single-view and multi-view fusion methods will fail when given noisy pose settings. To tackle this challenge, we propose a two-branch network architecture which fuses the depth estimation results of single-view and multi-view branch. In specific, we introduced mamba to serve as feature extraction backbone and propose an attention-based fusion methods which adaptively select the most robust estimation results between the two branches. Thus, the proposed method can perform well on some challenging scenes including dynamic objects, texture-less regions, etc. Ablation studies prove the effectiveness of the backbone and fusion method, while evaluation experiments on challenging benchmarks (KITTI and DDAD) show that the proposed method achieves a competitive performance compared to the state-of-the-art methods.", "sections": [{"title": "Introduction", "content": "Depth estimation from images remains a critical challenge in computer vision with extensive applications, particularly in autonomous driving systems where understanding the 3D environment is crucial. Traditional methods for depth estimation are broadly categorized into multi-view and single-view approaches. Although recent single-view methods have achieved significant improvements in estimation accuracy, like diffusion model-based Marigold [5] and DepthAnything [4] which is trained by huge amount of training samples. Existing problems such as scale ambiguity still influence the upper-bound of estimation results. To overcome the disadvantages of single-view methods, some multi-view methods rely on epipolar geometry and triangulation, which necessitate precise camera calibration and accurate pose estimation. These methods, while providing high-quality depth information under ideal conditions, struggle with dynamic objects,\ntexture-less regions, and scenarios where cameras are stationary, such as when vehicles stop at traffic lights or perform turns. Conversely, single-view methods offer robustness to these challenges by utilizing semantic understanding and perspective cues, yet they grapple with scale ambiguity, leading to inferior performance compared to multi-view methods.\nRecent efforts to fuse these approaches have sought to leverage their respective strengths. However, existing fusion systems often assume ideal camera poses and can perform worse than single-view methods in the presence of noisy poses. To address these limitations, we propose a novel adaptive fusion network that integrates the benefits of multi-view and single-view depth estimation techniques while mitigating their disadvantages. Our approach utilizes a two-branch network, where each branch extracts image features using the designed Pyramid Mamba module. These extracted feature maps, which represent different scales, are then used to construct the corresponding cost volume. Following this, we perform feature fusion using the proposed Adaptive Fusion module. This module, featuring a cascade structure, fuses features at different scales in a coarse- to-fine manner to achieve accurate depth estimation. The main contributions of the proposed method can be summarized as follows:\nWe introduce a single-view and multi-view fusion network where each branch utilizes the Pyramid Mamba module to extract multi-scale image features. These features are used to construct variance volume, which is then processed by the adaptive fusion module.\n\u2022 We propose an adaptive fusion module which applies the calculated attention weights on variance volume, resulting in improved depth estimation accuracy even in challenging scenarios. Our approach overcomes the limitations of traditional methods by providing robust depth prediction results that is less affected by noisy camera poses and dynamic environments.\n\u2022 We conduct extensive experiments and demonstrate various experimental results on benchmark datasets. And it shows our proposed method achieves a competitive performance on benchmark datasets including KITTI and DDAD."}, {"title": "Related Works", "content": ""}, {"title": "Single-view Based Methods", "content": "Single-view depth estimation remains a central challenge in computer vision, with recent advancements largely driven by convolutional neural network (CNN)-based approaches. Traditionally, these methods tackle the problem through per-pixel classification or regression frameworks. Eigen et al. [6] firstly proposed a two-stage neural network-based model, which shows the potential of CNN-based methods on tasks of single-view depth estimation. To enhance performance, recent strategies have included aggregating more robust visual features, developed novel loss functions, and utilized mix-data training techniques. [12] presented a compact but effective model for self-supervised single-view depth estimation, which somehow overcomes the limitation of obtaining accurate per- pixel ground-truth depth data. In [17], the authors proposed a feature metric loss which enables the learning of feature representations in a self-supervised manner. To reduce the increasing complexity of depth estimation networks, [8] reformulate the training of depth estimation network as an ordinal regression problem, and adopt a muti-scale network structure to avoid the unnecessary spatial pooling. [12] propose a network architecture that utilizes novel local planar guidance layers to enhance the performance of encoder- decoder structure on depth prediction tasks. [2] proposed a transformer-based block to that divides the depth range into bins whose center value is estimated adaptively per image, which leads to a significant improvement in monocular depth prediction. [22] propose a canonical camera space transformation module which enables monocular models to be trained in an extremely mixed-large dataset and obtaining excellent zero- shot generalization abilities. Despite these innovations and continuous improvements on various benchmarks, the accuracy achieved by single-view depth estimation methods still lag behind that of multi-view geometry-based methods. Considering this, our work integrates a single-view depth estimation module, leveraging its notable robustness to low-texture regions and dynamic objects, to address some of the limitations inherent in current approaches."}, {"title": "Multi-view Based Methods", "content": "Multi-view depth estimation has significantly advanced through leveraging multiple camera views with known intrinsics and poses. Initial approaches, such as those introduced by [24], demonstrated the potential of feature learning in multi-view stereo, albeit using traditional aggregation methods for matching costs. Building on this, [MVSNet] pioneered the use of a differentiable cost volume combined with 3D CNNs for"}, {"title": "3. Proposed Methods", "content": "The primary works of the proposed method is to perform depth estimation from images using a multi-view stereo approach within a neural network framework. The forward pass extracts features from both a reference image and multiple source images, constructs a 3D volume representation based on these features, and calculates depth predictions using attention mechanisms and regression techniques. The code facilitates the modeling of depth across multiple perspectives, leveraging convolutional neural networks to manage both feature extraction and the depth regression process, ultimately outputting depth maps along with their confidence scores. The network is designed with a modular architecture that incorporates multiple layers to extract features at various scales. Each layer is responsible for capturing different levels of detail, from low-level textures to high-level semantic information. Utilizing Vmamba, the network integrates multiple scales of feature extraction. This is achieved through parallel paths that process the input at"}, {"title": "3.1 Feature Extraction", "content": "The proposed method for depth estimation utilizes a multi-view stereo approach which is implemented by a two-branch neural networks. The reference image is processed through a feature extractor to obtain a series of multi-scale feature maps.\nThe network is designed with a modular architecture that incorporates multiple layers to extract features at various scales. Each layer is responsible for capturing different levels of detail, from low-level textures to high-level semantic information. Utilizing proposed depth-mamba backbone, the network integrates multiple scales of feature extraction. This is achieved through parallel paths that process the input at different resolutions, allowing for a more comprehensive understanding of the data [25, 26]. The architecture includes a stack of depth mamba modules that can enhance the focus on relevant features. The design is flexible, allowing for adjustments in layer depth and width to suit different datasets or tasks. Besides, inspired by [26], we also propose local feature blocks to further enhance the ability of extracting local features because mamba is not good at capturing the spatial information of input images compared to other CNN-based methods."}, {"title": "3.2 Attention Volume Construction", "content": "In our proposal, we construct attention volume for the depth regression. For the branch of reference image, we firstly construct the group-wise correlation (GwC)volume by utilizing both reference features and source features captured through depth-mamba backbone. After that, we apply a multi-scale attention module and a 3D hourglass network on the GwC volume to obtain attention weights. For the branch of source images, we construct variance volume by utilizing the reference feature and source feature. Finally, we multiply the constructed variance volume and attention weights to obtain the attention volume."}, {"title": "3.3 Depth Value Regression", "content": "Depth predictions are generated through a depth value regression process applied to the feature maps, resulting depth maps and confidence maps. Finally, the depth and confidence maps are up-sampled to the original input resolution using nearest neighbor interpolation, and the results are organized into the outputs dictionary, which encapsulates the method's ability to deliver accurate depth estimation from multi-view images."}, {"title": "3.4 Loss Functions", "content": "In the training process, we adopt mean absolute error (MAE) loss function as it is commonly used in regression tasks to measure the accuracy of a model. The loss function is defined as follows:\n$L = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|$\nWhere N denotes the total number of pixels, $y_i$ indicates the ground truth depth value for the ith pixel, $\\hat{y}$ indicates the predicted depth value for ith pixel."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1 Dataset", "content": "In stereo depth estimation task, high-quality datasets are significantly essential for training and evaluating depth estimation models. Two prominent datasets in this field are the KITTI and DDAD datasets, each providing valuable resources for developing and benchmarking stereo vision systems.\nThe KITTI dataset, introduced by the Karlsruhe Institute of Technology and Toyota Technological Institute, is one of the most widely used benchmarks for stereo depth estimation. It offers a diverse set of stereo image pairs captured from a vehicle-mounted stereo camera rig in various urban and rural environments. The KITTI dataset includes a range of scenes, from city streets to highways, with annotations for ground truth disparity"}, {"title": "4.2 Evaluation Metrics", "content": "The evaluation metrics used for quantitative evaluation are defined as follows:\n$AbsRel = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{|y_i - y_i^{pred}|}{y_i}$\n$SqRel = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{(y_i - y_i^{pred})^2}{y_i}$\n$RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - y_i^{pred})^2}$\nWhere N denotes the total number of pixels with ground truth labels in the depth map."}, {"title": "4.3 Implementation Details", "content": "We implement the proposed methods with PyTorch [16] and conduct evaluation experiments utilizing NVIDIA RTX 4090 GPUs. We adopt AdamW optimizer [14] and schedule the learning rate utilizing one-cycle policy [31] with lrmax = 1.0\u00d7 10\u20134. We trained 40 epochs on KITTI [6] and 30 epochs on DDAD [10]. During the training process, the proposed network architecture takes consecutive 3 frames as input, i.e, n=3. Regarding the details of multi-view setup, we set the depth hypothesis number of planes as 128, besides, the weight of the frames from different views is the same."}, {"title": "4.4 Comparison Experiment Results", "content": "To reveal the competitive performance of the proposed methods, we conduct evaluation experiments on KITTI [6] and DDAD [10] benchmark.\nThe KITTI dataset [9] and the KITTI Eigen split [6] serve as key benchmarks for"}, {"title": "4.5 Ablation Studies", "content": "To demonstrate the effectiveness of each component of the proposed method, we conducted ablation studies on KITTI. In this section, the following variants are discussed to evaluate the effectiveness of the proposed method:"}, {"title": "4.5.1 Ablation Studies on Feature Extraction Backbone", "content": "Based on the two-branch multi-view depth estimation architecture, we employ three different kinds of backbone, including ConvNeXt-T, VMamba, and our proposed Depth- mamba to verify the effectiveness of the proposed backbone towards this depth estimation task.\nThe results of ablation experiments are summarized in Table.2. The quantitative results reveal the effectiveness of our proposed mamba-based backbone. The proposed backbone outperform the ConvNeXt-T and VMamba."}, {"title": "4.5.2 Ablation Studies on Fusion Module", "content": "To evaluate the effectiveness and performance of the proposed fusion module, we conduct ablation studies on KITTI. Based on the basic architecture of two-branch muti-view depth estimation architecture, we implemented three variants. For the case of directly concatenating, we directly concatenate the two cost volumes utilizing a convolution layer which are constructed on single-view branch and multi-view branch, respectively. For the case of utilizing cross attention, we apply a cross-attention layer to fuse the cost volumes that are constructed on single-view and multi-view branch. For the details of our proposed fusion method, please refer to section 3.2.\nAblation on proposed backbone. The comparison results are shown in Table 2. Compared with the commonly adopted 'ConvNeXt-T', applying the proposed depth- mamba can extract more robust features from the two-branch inputs leading to the AbsRel error of the depth prediction reduced by 27.5%. Compared to VMamba backbone, the proposed method reduces the AbsRel error by 22.2%, which reveals the competitive performance of the proposed fusion method.\nEffectiveness of the proposed feature extraction backbone. As shown in Table 2, the evaluation metrics indicate the effectiveness and robustness of the proposed depth- mamba for tackling the task of multi-view depth prediction."}, {"title": "Conclusions", "content": "In this paper, we propose a two-branch neural networks which fuses the most reliable depth estimation results obtained from the single-view and multi-view branch, to improve the robustness of depth estimation results under the challenging conditions. In specific, we select the most robust depth estimation results from the two branches by utilizing an attention-based method. Besides, we introduce the mamba-based feature extraction backbone to further enhance the feature representation ability of the overall networks. The ablation studies certificate the effectiveness of the proposed fusion method and mamba-based backbone. Evaluation experiments show the competitive performance of our proposed method on both DDAD and KITTI benchmarks."}]}