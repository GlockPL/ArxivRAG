{"title": "VR Based Emotion Recognition Using Deep Multimodal Fusion With Biosignals Across Multiple Anatomical Domains", "authors": ["Pubudu L. Indrasiri", "Bipasha Kashyap", "Chandima Kolambahewage", "Bahareh Nakisa", "Kiran Ijaz", "Pubudu N. Pathirana"], "abstract": "Emotion recognition is significantly enhanced by integrating multimodal biosignals and IMU data from multiple domains. In this paper, we introduce a novel multi-scale attention-based LSTM architecture, combined with Squeeze-and-Excitation (SE) blocks, by leveraging multi-domain signals from the head (Meta Quest Pro VR headset), trunk (Equivital Vest), and peripheral (Empatica Embrace Plus) during affect elicitation via visual stimuli. Signals from 23 participants were recorded, alongside self-assessed valence and arousal ratings after each stimulus. LSTM layers extract features from each modality, while multi-scale attention captures fine-grained temporal dependencies, and SE blocks recalibrate feature importance prior to classification. We assess which domain's signals carry the most distinctive emotional information during VR experiences, identifying key biosignals contributing to emotion detection. The proposed architecture, validated in a user study, demonstrates superior performance in classifying valance and arousal level (high / low), showcasing the efficacy of multi-domain and multi-modal fusion with biosignals (e.g., TEMP, EDA) with IMU data (e.g., accelerometer) for emotion recognition in real-world applications.", "sections": [{"title": "I. INTRODUCTION", "content": "MOTION recognition has emerged as a critical research area in recent years, driven by advancements in deep learning and the increasing availability of multimodal data sources [1], [2], [3], [4]. Affect recognition is central to various applications in human-computer interaction (HCI), healthcare, gaming, and adaptive systems, where an accurate understanding of emotional states can significantly enhance user experience and system responsiveness. Traditionally, emotion detection has relied on unimodal data streams, such as physiological signals, motion data, or facial expressions [5]. However, human emotions' inherent complexity and dynamic nature necessitate more sophisticated approaches, prompting a shift toward multimodal systems that integrate data from multiple sources to improve classification accuracy and robustness.\nPhysiological signals such as electrocardiograms (ECG), electrodermal activity (EDA), and heart rate (HR)-have demonstrated strong correlations with emotional arousal, stress, and other affective states [2], [6], [1], [7]. Similarly, motion data captured by inertial measurement units (IMUs) has been employed to infer emotions through analysis of body movements [8], [9]. Eye-tracking, which provides insight into gaze behavior and visual focus, further contributes contextual understanding of emotional states [10], [11]. Despite the strengths of these individual modalities, each has inherent limitations when used in isolation. For example, while physiological signals capture autonomic nervous system activity, they do not account for contextual or environmental factors that may be captured through motion data or gaze tracking.\nTo address these limitations, multimodal data integration has gained significant attention [3], [11], [1]. Multimodal systems leverage the complementary strengths of different data streams, providing a more comprehensive understanding of emotional states. Recent advances in deep learning have enabled the efficient fusion of high-dimensional data from multiple modalities, offering significant improvements over traditional machine learning techniques in feature extraction and pattern recognition.\nIn this work, we propose a novel emotion recognition framework that integrates data from three distinct devices, each corresponding to a different physiological and motion domain: the trunk, head, and peripheral. The devices employed include a wearable vest (trunk domain), a virtual reality (VR) headset (head domain), and a wrist-worn device (peripheral domain). Each device contributes unique data modalities-physiological signals (e.g., ECG and HR) from the vest, motion data from the wrist via accelerometers, and feature-extracted data from the VR headset. By incorporating these multimodal inputs, we aim to capture emotional states with greater precision, leveraging the complementary nature of data across the trunk, head, and peripheral domains.\nThis multi-domain approach addresses several key challenges in affect recognition. By capturing physiological signals from the trunk, motion data from the wrist, and gaze behaviour from the head, we construct a more holistic view of emotional states, thus overcoming the limitations of single-modality systems. Additionally, the fusion of these modalities enables a"}, {"title": "II. RELATED WORKS", "content": "In recent years, affect recognition has garnered significant attention, particularly with the rise of multimodal data sources and advanced deep learning techniques [12], [13], [14]. Numerous studies have explored various approaches for recognizing affective states using physiological signals and additional modalities such as motion or environmental sensors."}, {"title": "A. Emotion Detection Using Physiological Signals", "content": "Physiological signals have been extensively explored in the field of affective computing for emotion detection, given their strong correlation with emotional states. Among the most commonly utilized signals is the electrocardiogram (ECG), which provides insights into heart rate variability (HRV), a key indicator of autonomic nervous system activity and emotional arousal [2], [6], [7]. Electrodermal activity (EDA), also referred to as galvanic skin response (GSR), measures skin conductance and reflects sympathetic nervous system responses to emotional stimuli, making it a vital tool for detecting arousal and stress [1], [7]. Additionally, blood volume pulse (BVP), typically measured via photoplethysmography (PPG), has been widely used to assess heart rate and peripheral blood flow, contributing to the detection of emotional responses related to stress and anxiety [1]. Other crucial signals include respiration rate (RR), which varies with emotional states such as fear or relaxation, and electromyography (EMG), which captures facial muscle movements linked to expressions of emotion [15]. Moreover, electroencephalogram (EEG) has been extensively employed to monitor brainwave activity, offering insights into the valence and arousal dimensions of emotion [2], [7], [3], [16]. Finally, skin temperature (ST) and heart rate (HR) have also proven useful in detecting stress and emotional arousal, further contributing to a holistic understanding of affective states [1]. These signals, often used in combination, provide a rich, multimodal approach to accurately capturing and classifying complex emotional experiences."}, {"title": "B. Emotion Detection Using Motion Signals", "content": "Emotion recognition research based on human motion has attracted significant attention within the field of human-computer interaction, particularly with the advancements in IMU sensors, which have facilitated efficient and seamless interaction between humans and devices [8], [9], [17], [18]. IMU sensors, such as accelerometers (ACC) and gyroscopes (GYRO), capture crucial motion-related data that can be used to infer emotional states. Recent studies have explored the use of wearable inertial devices attached to various body parts to collect emotional information through human movement. Hashmi et al. [8] employed a smartphone worn on the chest to record human gait signals and used spectro-temporal features from stride signals for emotion recognition, identifying six basic emotions using Random Forest and SVM classifiers. Similarly, Gravina et al. [9] leveraged sensor-level and feature-level fusion to monitor in-seat activities, which reflect psychological and emotional states, though the handcrafted features required labour-intensive design. Chang et al. [19] used intelligent inertial sensors to recognize emotions during badminton play, though prolonged activity resulted in physical fatigue that affected the accuracy of emotion detection. Furthermore, recent advancements such as those proposed by Feng et al. [20] have integrated motion and emotion recognition into intelligent wearable systems using multi-sensor fusion, enabling real-time interaction and feedback in virtual environments via Digital Twin technology, significantly enhancing user experience and emotion recognition accuracy. These studies highlight the increasing potential of IMU-based motion signals in emotion recognition, though challenges such as feature extraction complexity and physical fatigue remain areas of ongoing research."}, {"title": "C. Emotion Detection Using Eye Tracking Data", "content": "Humans interact with their environment, with each elicited emotion being closely tied to its specific context [21]. Ptaszynski et al. [22] highlighted the importance of incorporating contextual analysis into emotion processing. Eye-tracking involves identifying a user's gaze point or focus on a specific visual stimulus. An eye-tracker, a device designed for this purpose, measures eye position and movements [23]. As a sensor technology, eye-tracking is versatile, applicable in diverse configurations and uses, as demonstrated by Singh et al. [24]. Furthermore, other studies explore eye movements as potential indicators for recognizing emotions [10], [11]."}, {"title": "D. Multimodal Emotion Detection", "content": "Multimodal fusion has captured significant interest across various research domains due to its broad applicability in areas such as emotion recognition, event detection, image segmentation, and video classification [25]. Fusion strategies are traditionally categorized by the fusion level, including: 1) feature-level fusion (early fusion), 2) decision-level fusion (late fusion), and 3) hybrid multimodal fusion. With advancements in deep learning, an increasing number of researchers are leveraging deep learning frameworks to enhance multimodal fusion approaches."}, {"title": "E. Emotion Detection Using Multi-Domain Sensors", "content": "Emotion recognition using IMU sensors and biosensors in VR environments has emerged as a critical area of research, with a significant focus on analyzing signals from isolated domains such as head, trunk, and peripheral sensors. Historically, emotion detection efforts have been domain-specific [5]. For example, head-mounted sensors, predominantly integrated into VR headsets, capture signals like eye movement and facial EMG to infer emotional states. Trunk-mounted sensors have primarily focused on physiological signals such as HRV and respiration, while peripheral devices, particularly wrist-worn sensors, monitor EDA, skin temperature, and accelerometry. While these single-domain approaches have demonstrated effectiveness in capturing emotional cues, they inherently limit the understanding of how emotions are represented across the body in different regions [1], [5].\nHowever, a critical gap in the current literature is the lack of comparative studies that examine the relative effectiveness of these domains (head, trunk, and peripheral) in emotion recognition tasks. Specifically, no substantial research has been conducted to systematically compare the emotional information captured by sensors in these distinct body regions within the same experimental setup. Furthermore, the literature has not adequately explored the potential benefits of multimodal sensor fusion, where signals from multiple domains are combined to exploit complementary features and improve emotion detection accuracy. This omission leaves a gap in understanding the holistic contribution of multi-domain signals in representing complex emotional states, particularly in immersive VR settings."}, {"title": "F. Motivations and Contributions", "content": "Building on the identified gaps in single-domain emotion recognition, this study aims to leverage the potential of multi-domain sensor fusion in VR environments. By systematically comparing signals from the head, trunk, and peripheral regions, we seek to determine whether the combination of signals from these diverse domains can provide a more comprehensive and accurate representation of emotional states. Additionally, this work explores the specific contributions of each domain and evaluates the benefits of integrating these signals for more precise emotion detection.\nThis study advances multi-domain inference in VR emotion recognition by presenting a framework, within the same experimental setup, that:\nIdentifies Key Emotional Domains: Although emotion originates from the central nervous system, emerging research supports the idea that physiological signals from various body domains such as the head [10], [11], trunk [2], [6], [7], and peripheral [?], [19] systems, offer unique and complementary emotional insights. When these signals are integrated, they significantly enhance the accuracy and robustness of emotion recognition systems. Our comprehensive analysis of these domains within the same experimental setup elucidates which signals contribute most effectively to emotional inference during VR experiences, ultimately advancing the precision of emotion recognition in VR environments.\nPerforms Biosignal Importance Analysis: Using advanced techniques such as deep learning, we pinpoint specific biosignals within the most informative domain that contribute most significantly to emotion recognition, enhancing model interpretability and optimization.\nPropose a novel deep learning-based multi-modal architecture that leverages multi-scale attention mechanisms and Squeeze-and-Excitation (SE) blocks. It captures fine-grained dependencies across modalities and dynamically recalibrates feature representations to enhance accuracy and robustness in emotion detection.\nEnables Multimodal Data Fusion: We assess the performance of a fused multi-domain model to evaluate whether combining data across domains yields superior accuracy for emotion detection compared to single-domain approaches.\nThis multi-domain framework provides a scalable and precise tool for VR emotion recognition research, with implications for enhanced affective computing, user experience analysis, and behavioural studies in immersive environments."}, {"title": "III. EXPERIMENTAL PROTOCOL", "content": "Thirteen 360-degree videos were sourced from the public database described in [32], specifically chosen for their ability to evoke targeted emotional responses. Due to the varied lengths of these original videos, three domain experts-two psychologists and one computer scientist\u2014selected specific intervals within each video that best represented the intended emotional state for viewers [30], [31]. This standardization ensured uniform stimulus duration across all videos, reducing variability in the experimental setup and maintaining an efficient overall study duration.\nTable I lists the videos used, detailing the selected intervals and associated emotional quadrants (LALV = low arousal, low valence; LAHV = low arousal, high valence; HALV = high arousal, low valence; HAHV = high arousal, high valence). Video clips were categorized by valence and arousal levels, grouping clips of similar emotional tones within the same quadrant. Participants were presented with a printed Self-Assessment Manikin (SAM) scale reference, ratings < 4 as low valence (LV) and low arousal (LA), and ratings > 4 as high valence (HV) and high arousal (HA) and instructed to rate each video group on these dimensions after viewing, allowing for consistent assessment of emotional responses (Figure 1)."}, {"title": "B. Participants", "content": "In this study, a cohort of 23 healthy participants was recruited through digital and physical advertisements displayed across social media platforms and within the university campus. A substantial portion of the sample (75.0%, n=18) comprised male participants, while the remaining 25.0% (n=6) were female. The age of participants ranged from 21 to 52 years, yielding a mean age of 28.9 years with a standard deviation of 6.4 years. All participants were screened to confirm the absence of any pre-existing cardiovascular or psychological conditions, ensuring a baseline of physiological and psychological health across the sample."}, {"title": "C. Experimental Setup", "content": "Three commercially available devices-the Equivital Vest, Empatica Embrace Plus Watch, and Meta Quest Pro VR headset were employed to collect a range of physiological and behavioural measurements from participants. The Equivital Vest (specifically, Equivital EQ02+ LifeMonitor) was utilized to capture electrocardiogram (ECG) data along with accelerometer readings, while the Empatica Watch (specifically, Empatica E4 Watch recorded EDA, skin temperature, heart rate, and accelerometer data. The Equivital GSR add-on accessory was used to capture GSR (Galvanic Skin Response) activity. The Meta Quest Pro VR headset, in addition to delivering immersive 3D 360-degree video stimuli, provided critical eye-tracking data, capturing participant gaze patterns and reactions in real-time. Within the same experimental set-up, sensory data from the trunk, peripheral (wrist), and head domains were captured using the Equivital Vest, Empatica Embrace Plus Watch, and Meta Quest Pro VR headset, respectively. Figure 2.(b) provides a schematic overview of the data collection architecture.\nTo achieve temporal precision across all devices in the experimental setup, the Host Computer and Meta Quest Pro were synchronized via a shared Network Time Protocol (NTP) server. This alignment ensured uniform timekeeping, critical for data accuracy. Additionally, the control applications for the Equivital Vest and Empatica Watch, running on the host computer, upheld the same time synchronization protocol, reinforcing the cohesion across the system. Data recording on both the vest and watch commenced automatically upon activation and continued until device removal and shutdown. User control over the Meta Quest Pro was enabled through a wireless keyboard, while the research team monitored the participant's perspective in real-time through the device's live video streaming capability. A custom-built VR video player and data logger application developed using Unity and C#, facilitated seamless, real-time data capture from the headset's integrated sensors. Figure 2.(c) illustrates the architecture of this custom VR application.\nThis multi-modal data collection framework, which incorporated physiological signals, immersive environments, and eye-tracking data, provided a robust platform for evaluating participant responses and behaviours under controlled conditions.\nThe VR application architecture is structured into three core modules to optimize immersive and interactive data collection: the VR Environment, Data Acquisition, and Data Logger"}, {"title": "D. Data Collection", "content": "Upon arrival, participants were briefed on the study's objectives and protocol. Researchers ensured accurate equipment fitting after providing informed consent and completing a demographic survey.\nParticipants watched thirteen videos, completing the SAM Manikin Questionnaire after each video to rate their emotional responses on a scale from 1 to 7 for both valence and arousal. This process of viewing videos and completing the questionnaire was repeated across all video sets. Researchers then assisted with equipment removal, ensuring standardized data collection for robust, synchronized multi-modal analysis. The data collection process is illustrated in Fig 1."}, {"title": "E. Data Analysis and Preprocessing", "content": "The system accepts raw data from Empatica Embrace Plus Watch, Equivital Vest and Meta Quest Pro VR Head Set as inputs.Through the data preprocessing module, the heterogeneous inputs can be formatted into specific representations, which can be effectively used in the following feature extraction network module.\nThe signals from the Empatica EmbracePlus and Equivital Vest can be categorized into two primary types: physiological signals and inertial measurement unit (IMU) data. Physiological signals encompass measurements such as heart rate (ECG), skin conductance (EDA), blood volume pulse (BVP), and skin temperature (TEMP). The ECG, or electrocardiogram, records the electrical activity of the heart over time, providing detailed information about the heart's rhythm and rate by detecting the depolarization and repolarization of cardiac muscles. Blood volume pressure (BVP), measured via a photoplethysmography (PPG) sensor embedded in the watch, reflects the pulse wave of the heart and the volume of blood flowing through vessels, offering a non-invasive insight into cardiovascular dynamics. EDA, or electrodermal activity, captures variations in the skin's electrical conductance, which is influenced by sweat gland activity and serves as a physiological marker for emotional and arousal states. This signal is monitored by applying a low-level current between two electrodes on the skin, providing valuable data about the body's electrodermal response to external stimuli. Skin temperature (TEMP) is another critical metric, measured using a thermopile infrared sensor, which helps monitor the body's thermoregulatory response. Vascular responses, such as vasodilation or vasoconstriction, result in corresponding changes in skin temperature, which can indicate the body's reaction to various environmental and physiological factors. In contrast, IMU data involves motion-related signals captured by accelerometers, which measure acceleration along three axes x, y, and z on the watch, providing insights into movement dynamics. In addition to the Empatica EmbracePlus data, the Equivital Vest offers complementary accelerometer measurements, including the lateral accelerometer, which measures side-to-side movement, the longitudinal accelerometer, which captures forward and backward motion, and the vertical accelerometer, which tracks up-and-down movements. Together, these physiological and motion-related signals provide a comprehensive understanding of the user's physical and emotional states.\nThe VR headset predominantly tracks eye movement, with left and right eye positions (LEyeposition, REyeposition) measured in three-dimensional space (x, y, z coordinates). These eye position signals provide precise spatial coordinates at specific time points, enabling us to capture gaze dynamics and spatial orientation during each experimental task.\nFor peripheral domain signal preprocessing, we applied tailored filtering techniques to ensure high-quality data. Accelerometer signals were filtered using a 0.5-20 Hz band-pass filter to eliminate low-frequency drift and high-frequency noise, preserving relevant motion information. Regarding EDA preprocessing, upsampling from 4 to 64 Hz was performed to make both signals at the equal sampling frequency [33]. A Butterworth filter with a cutoff frequency of 0.5 Hz is often used. Apply a bandpass filter between 0.5 Hz and 4 Hz to remove noise for BVP signal processing. Use a moving average to reduce the noise from the temperature readings.\nFor trunk domain data, ECG signals, a 0.5-45 Hz band-pass filter was used to retain key waveform features (P, QRS, T waves) while removing artifacts and noise. Same as the watch, accelerometer signals are filtered using a 0.5-20 Hz band-pass filter. However, GSR data were unreliable for five participants; removing them would cause data insufficiency. Thus, GSR was excluded to maintain consistency across participants.\nFor each physiological, IMU, and eye-tracking data input, we first applied z-score normalization to standardize the data, ensuring that all signals were on a comparable scale. Following this, we selected the last 40 seconds of data from both the peripheral and trunk domains. This selection was based on an analysis of the signals, which indicated that the final segment of the video stimuli most consistently elicited emotional responses. The extraction of this uniform 40-second segment was necessary to maintain equal input lengths across modalities, a critical requirement for the proposed deep learning architecture.\nAfter segmentation, we applied a sliding window technique with a 2-second window and a 50% overlap to the physiological and IMU signals. This windowing strategy effectively partitioned the input data stream into multiple overlapping windows, ensuring fine-grained temporal resolution while preserving temporal context. Each segmented window was then represented as a 2D array with dimensions T \u00d7 F, where T is the number of time windows (time stamps), and F represents the size of the data (features) in one window, determined by the sampling frequency and window duration. Specifically, for signals from the Empatica Embrace Plus, F is set to 128, based on its sampling rate of 64 Hz, while for the Equivital vest, F is 512, reflecting its sampling rate of 256 Hz.\nFor the Meta Quest Pro VR headset data, we extracted the final 2000 coordinates from each axis of both the left and right eye positions (x, y, z for each eye). The data was then segmented into overlapping windows using a sliding window approach, with 50% overlap, dividing the 2000 coordinates into windows of F=200 coordinates each. This segmentation process provided structured 2D inputs for the proposed architecture, similar to the representation used for the physiological and IMU data, ensuring compatibility across all input modalities."}, {"title": "IV. SYSTEM ARCHITECTURE", "content": "The high-level structure of our proposed emotion recognition system, EMO-MSASE: Emotion Detection using Multi-Scale Attention and Squeeze-and-Excitation is depicted in Fig. 3, highlighting only the best-performing signals from each"}, {"title": "A. Multi-Scale Attention LSTM Feature Extraction", "content": "In this stage, we employed an LSTM architecture augmented with a multi-scale attention mechanism to act as a high-level feature extractor. The LSTM model [34], a specialized variant of recurrent neural networks (RNNs), is adept at capturing long-term dependencies within sequential data. The core elements of the LSTM structure include the cell state and gating mechanisms; the cell state functions as a \u201cmemory\" that carries information across time steps, while the gating structures control which information is retained or discarded. Specifically, the LSTM comprises three gate types: the forget gate, input gate, and output gate. The forget gate selectively removes irrelevant information from prior cell states, the input gate updates the cell state with new data from the current input, and the output gate produces the final output based on the updated cell state. Utilizing the LSTM enables us to capture temporal relationships within each input modality, yielding a detailed representation.\nAdditionally, recognizing that not all sub-samples contribute equally to the final recognition task (for example, not all frames in a video are equally informative), we incorporated an attention mechanism [35]. This approach assigns relative importance weights to different sub-samples, allowing for the fusion of these weighted sub-samples into a final informative feature vector. The attention mechanism thereby enhances our ability to capture both temporal and spatial dependencies by differentially weighting each sub-sample, while the fused feature vector, being more compact, also reduces training time requirements [36].\nThe proposed architecture is structured around a multi-domain, multi-modality framework, incorporating three distinct domains: the trunk, peripheral (wrist), and head, represented by the Equivital Vest, Empatica Embrace Plus Watch, and Meta Quest Pro VR headset, respectively. We denote these domains as Dv (Equivital Vest), Dw (Empatica Embrace Plus), and DH (Meta Quest Pro). Each domain-specific device has multiple modalities to capture diverse physiological and motion-related signals across these domains. For the watch domain, the modalities are denoted as $X_w^i$, where i indexes the different modalities, for example, accelerometer data in the x, y, and z directions. Similarly, the modalities for the vest domain are represented as $X_v^j$ and for the VR headset as $X_H^k$, with j and k indexing the respective modalities within each domain.\nEach modality follows a consistent feature extraction process using an LSTM + multi-scale attention (MSA) architecture as shown in Fig. 4. Let $X \\in R^{T \\times F}$ represent the input signal for a given modality, where T is the number of timesteps and F is the number of features per timestep. The LSTM processes this sequence and outputs a sequence of hidden states $H \\in R^{T \\times H}$, where H is the size of the hidden state. Mathematically, the output of the LSTM can be represented as:\n$H = LSTM(X) = \\begin{bmatrix}\nh_1\\\\ ... \\\\\nh_t\n\\end{bmatrix}, h_i \\in R^H$\nHere, h denotes the hidden state vector at each timestep, capturing features extracted by the LSTM.\nThe multi-scale attention (MSA) mechanism is then applied to the LSTM output, operating at three different temporal scales: short-term, medium-term, and long-term. The attention weights for each scale are generated by comparing the hidden states to a learnable context vector, $u \\in R^H$, which is initialized randomly and trained to prioritize the most informative timesteps for each modality. The attention score ai for each hidden state hi is computed as:\n$a_i = \\frac{exp(u^Th_i)}{\\sum\\limits_{t=1}^{T}{exp(u^Th_t)}}$\nThe learnable context vector u helps highlight important information by guiding the focus on particular timesteps based on the context of the task.\nFor the short-term attention, attention weights $\\alpha_i$ are computed over all T timesteps, producing a weighted sum of the hidden states:\n$V_{short} = \\sum\\limits_{i=1}^{T}{\\alpha_ih_i}, V_{short} \\in R^H$.\nFor the medium-term attention, adjacent timesteps are merged, reducing the sequence length to Tmedium = 17, and attention is applied over the merged sequence with weights $\\beta_i$:\n$V_{medium} = \\sum\\limits_{i=1}^{T_{medium}}{\\beta_ih_{merged, i}}, V_{medium} \\in R^H$.\nSimilarly, for long-term attention, the sequence is further reduced to Tlong = 17, and attention is applied over this coarser representation with weights $\\gamma_i$:\n$V_{long} = \\sum\\limits_{i=1}^{T_{long}}{\\gamma_ih_{merged, i}}, V_{long} \\in R^H$.\nThe outputs from the short-term, medium-term, and long-term attention mechanisms are concatenated to form the final feature vector, called the Concatenated Attention Vector (CAV) for the modality:\n$CAV = V_{modality} = [V_{short}, V_{medium}, V_{long}], V_{modality} \\in R^{3H}$.\nWithin each domain, all modality-specific feature vectors are concatenated to form a domain-specific feature vector. For example, in the watch domain:\n$V_w = [v_w^1, v_w^2,....v_w^{M_w}], V_w \\in R^{M_w \\times 3H}$,"}, {"title": "B. Deep Learning Setup For the Proposed Architecture", "content": "For feature extraction, each device utilizes a dedicated Attention-LSTM network. The LSTM network comprises two layers, each with 128 hidden units. The model is trained using binary cross-entropy loss with the AdamW optimizer for effective weight updates, a learning rate of 0.001, and a batch size of 16. Training is conducted over 50 epochs, with early stopping based on validation performance to prevent overfitting. All data analytics and deep learning model building and testing were conducted on a 13th-generation i7 workstation equipped with an RTX 4080 GPU and 32GB of RAM."}, {"title": "V. RESULTS", "content": "In this section, we present the performance of EMO-MSASE. Our study leverages data from three distinct domains, head (Meta Quest Pro VR headset), trunk (Equivital Vest), and peripheral (Empatica Embrace Plus watch) to advance emotion recognition. These domains contribute multiple modalities, including IMU data, ECG, temperature, EDA, BVP, and eye movements as in Table II, which are utilized to assess valence and arousal. While these devices offer a wide array of signals, we selected only the most relevant modalities for our analysis, focused on their significance in emotion detection. We collected a total of 299 samples for each modality from 23 participants. Each participant watched thirteen videos, completing the SAM Manikin Questionnaire after each video to rate their emotional responses on a scale from 1 to 7 for both valence and arousal. As indicated in Table I, we classified ratings < 4 as low valence (LV) and low arousal (LA), and ratings > 4 as high valence (HV) and high arousal (HA), forming the first ground truth (G1) for valence (G1_V) and arousal (G1_A) respectively. The second ground truth (G2) of valence (G2_V) and arousal (G2_A) levels is derived from external ratings (three domain experts, two psychologists and one computer scientist) based on established criteria from previous research [30]-[32]. In constructing the deep learning model, four distinct cases of label assignment were systematically applied to the collected dataset: (1) the actual response of each participant for each video was used as the label (general), (2) for each video, a final label was determined by the majority response across all 23 participants (majority) as in Table I, (3) only the responses from male participants were used (males only), and (4) the label values provided by the selected reference papers (G2_V and G2_A) for each video to assign labels for each video. Cases (1), (2), and (3) are subclasses of the general labels (G1_V and G1_A).\nThe efficacy of our proposed deep learning architecture for valence and arousal detection was rigorously evaluated through two distinct cross-validation techniques: (1) Group K-Fold cross-validation and (2) Leave-One-Subject-Out (LOSO) cross-validation. In Group K-Fold cross-validation, participants were divided into two distinct groups for training and testing, ensuring that data from the same participant was not used in both the training and testing sets to prevent data leakage. This approach maintains subject independence during model evaluation. In LOSO cross-validation, the model was trained on data from all but one participant, with the left-out participant's data used for testing. This process was repeated for each participant, allowing for a robust assessment of the model's generalizability across individuals. For Analysis V-A, V-B, V-C, we employed Group K-Fold cross-validation with 5 folds. The mean accuracy across the 5 folds was computed to provide a robust and comprehensive evaluation of the model's performance. To further enhance the evaluation, Analysis V-C was additionally assessed using (LOSO) cross-validation."}, {"title": "A. Uni-modality Performance Analysis", "content": "We began by independently evaluating the performance of each signal modality within each domain, as shown in Table II, for the detection of valence and arousal. Fig 5 illustrates the results of individual signal performance for both valence (a) and arousal (b) cross four distinct cases. In the context of valence detection, the model demonstrates robust performance across almost all signals from the three domains when using G_2 labels, consistently achieving over 70% accuracy. Notably, LAT_ACC from the trunk, EDA and TEMP from the peripheral domain, and L_EP_Y and R_EP_Y from the head domain perform particularly well"}, {"title": "B. Single Domain Performance Analysis", "content": "In this experiment, we conducted an independent evaluation of each domain's performance using their respective modalities for valence and arousal detection. For the peripheral domain, we initially considered four key modalities: IMU data (accelerometer readings along the X, Y, and Z axes), BVP, EDA, and TEMP. These six raw signals across four modalities yielded 64 possible combinations. Table III showcases the top four combinations for both valence and arousal detection. For the trunk domain, we analyzed two key modalities: electrocardiogram (ECG) and IMU data. The ECG data was captured from two leads, referred to as ECG1 and ECG2, while the IMU data included lateral acceleration (LAT_ACC), longitudinal acceleration (LONG_ACC), and vertical acceleration (VERT_ACC). The top five performing combinations for valence and arousal detection are summarized in Table IV. For the eye-tracking data, under the eye modality, we utilized six features: LEyeposition_x, LEyeposition_y, LEyeposition_z, REyeposition_x, REyeposition_y, and REyeposition_z. The results of the top-performing combinations are presented in Table V.\nAs shown in Tables III and IV, higher accuracy values for both valence and arousal were consistently observed with G2_V and G2_A labeling, except in the trunk domain. For arousal detection, the trunk domain achieved the highest accuracy under majority labeling. In contrast, when using G1_V and G1_A labels, valence accuracy was moderate across all three cases (general, majority, and males) in both the peripheral and trunk domains. Overall, majority labeling yielded the highest accuracy for both valence and arousal in the trunk and peripheral domains when using G1_V and G1_A labels from the collected data. In analyzing the top-performing signals combinations for the watch, the combination of ACC_Z, EDA, and TEMP consistently demonstrated superior performance"}, {"title": "C. Multi-Domain Fusion Performance Analysis", "content": "The results presented in Table VI clearly demonstrate the superiority of multi-domain fusion over single-domain approaches in both valence and arousal detection."}, {"title": "D. Ablation Study", "content": "Table VII presents the results of the ablation experiments conducted on the proposed multimodal fusion framework. In these experiments, LSTMSA denotes the use of self-attention with LSTM for feature extraction, LSTMMSA represents the application of a multi-scale attention mechanism with LSTM in the fusion layer, and proposed architecture, EMO-MSASE indicates the integration of both multi-scale attention and a Squeeze-and-Excitation (SE) block for each domain prior to domain fusion.\nThe results provide clear evidence of the benefits introduced by the EMO-MSASE architecture compared to the LSTMSA and LSTMMSA models. Across all domain combinations and both valence and arousal detection, EMO-MSASE consistently outperforms the other models. For valence detection, the fusion of trunk and head signals achieves the highest accuracy of 87.14% for G2_V, significantly surpassing the best results from LSTMMSA (84.28%) and LSTMSA (80.56%). Similarly, in arousal detection, EMO-MSASE delivers superior performance, with 65.15% accuracy for G1_A and 63.53% for G2_A when combining all domains, again outperforming LSTMMSA and LSTMSA. The addition of the multi-scale attention mechanism, along with the Squeeze-and-Excitation (SE) blocks, enhances the model's capacity to capture complex patterns across domains, leading to the observed improvements in accuracy. Notably, the combination of head and peripheral domains in EMO-MSASE also shows strong performance, particularly in valence detection, highlighting the importance of multi-modal fusion in this task. These results underline the efficacy of integrating domain-specific SE blocks and multi-scale attention, positioning EMO-MSASE as the most effective model in the"}]}