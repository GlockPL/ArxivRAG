{"title": "Model editing for distribution shifts in uranium oxide morphological analysis", "authors": ["Davis Brown", "Cody Nizinski", "Madelyn Shapiro", "Corey Fallon", "Tianzhixi Yin", "Henry Kvinge", "Jonathan H. Tu"], "abstract": "Deep learning still struggles with certain kinds of scientific data. Notably, pretraining data may not provide coverage of relevant distribution shifts (e.g., shifts induced via the use of different measurement instruments). We consider deep learning models trained to classify the synthesis conditions of uranium ore concentrates (UOCs) and show that model editing is particularly effective for improving generalization to distribution shifts common in this domain. In particular, model editing outperforms finetuning on two curated datasets comprising of micrographs taken of U3O8 aged in humidity chambers and micrographs acquired with different scanning electron microscopes, respectively.", "sections": [{"title": "1. Introduction", "content": "The morphological features of nuclear materials can provide information about the processing conditions used to produce the materials and the history of the materials after their production [18]. For uranium ore concentrates (UOCs) the precipitation chemistry and calcination conditions have been shown to impart certain morphological features that allow particles observed by scanning electron microscopy (SEM) to be correlated back to the synthesis conditions [27]. Aging studies have looked at how UOC properties and particle morphologies change over time during exposure to different storage conditions, particularly humidity and temperature [8, 9, 23, 25, 28, 29, 31, 33].\nA number of analytical methods have been developed for morphological analysis of nuclear materials, including qualitative descriptions [30], quantitative measurement of particles by image segmentation [22] or other particle sizing methods, texture analysis [4, 5], and machine learning. Deep learning models for morphological analysis have been trained in supervised [17], self-supervised [13], and unsupervised settings [6]. The robustness of these models to out-of-distribution (OOD) data has been investigated [21], along with methods for uncertainty quantification (UQ) or calibration of prediction confidence [7] and source-free domain adaptation (SFDA) for covariate shifts [16].\nComputer vision models for predicting the provenance of nuclear materials must be robust to statistic-level variations (i.e., the appearance of images taken with various microscopes and in varying settings), and to some extent, feature-level variations (i.e., changes to the actual morphological characteristics). Whereas SFDA has only been demonstrated for the former, model editing methods [2, 20] could potentially be used for both types of distribution shifts and can be applied to existing validated models incrementally as new kinds of distribution shifts are discovered with lower computational needs than other methods."}, {"title": "Contributions", "content": "Our contributions can be summarized as follows:\n\u2022 Model editing methods generalize well on feature-level distribution shifts caused by aging uranium oxides under diel cycling humidity and temperature conditions.\n\u2022 We compare editing methods and find that low-rank editing generally outperforms surgical finetuning.\n\u2022 Both editing and surgical finetuning outperform full-model finetuning."}, {"title": "2. Description of data", "content": "The training and validation datasets consist of scanning electron microscope (SEM) images collected of UOCs representing five precipitation pathways ammonium diuranate (ADU), ammonium uranyl carbonate (AUC), magnesium diurante (MDU), sodium diuranate (SDU), and uranyl peroxide (UO4.H2O) that have been converted to three uranium oxides uranium trioxide (UO3), triuranium octoxide (U3O8), and uranium dioxide (UO2). The micrographs were collected using an FEI Nova NanoSEM 630 scanning electron microscope with the through lens detector (TLD) operating in secondary electron (SE) mode. More complete descriptions of the material synthesis and data collection can be found elsewhere [1, 11, 17, 27]. Previous work has shown that classification models trained on earlier"}, {"title": "3. Methods for model updating", "content": "Model editing makes tweaks to model weights to incorporate new facts [10, 19, 20], semantics [2], and behaviors [12]. Such methods have been used to patch model errors in the form of spurious correlations [26] and confusion [24] with only a single input example. We focus on the setting of natural distribution shifts, where we want to adapt models to inherent variations in the data largely in the form of input/statistical-level variations (the detector editing task) and feature-level variations (the aging editing task).\nLet f be a neural network defined as a composition of layers f = fL\u00b0 fL\u22121 \u00b0\u00a8\u00a8\u00a8\u00b0 f2 \u00b0 f1, where f\u2081(x) = \u03c3 (W\u2081x) and Wi is an n\u2081 \u00d7 ni\u22121 matrix for each l = 1,..., L with an activation function \u03c3. For any layer, let f<1 denote the composition of the first l layers: fio f\u0131\u22121\u00b0\u00a8\u00a8\u00a8\u00b0 f20f1. In contrast with other work, e.g. [2, 19, 24], we do not use exemplar pairs (x, x', y) of transformed images x' with a fixed label y to update a model so that f<1(x) \u2248 f<\u0131 (x'). Instead, we update our model on new pairs of datapoints x' and labels y' from either the aging shift or the detector shift and use stochastic gradient descent (SGD) so that f(x') \u2248 y'. This is a fairly significant difference from some of the editing methods discussed above, however we keep the editing terminology because our methods modify single-layers with constrained updates and/or a limited number of examples.\nWe next describe the two model update methods we use in this work. We compare these update methods to a baseline of full finetuning, where all model weights W1,..., WL, are updated with stochastic gradient descent (SGD) to minimize the mean squared error (MSE) of f(x') and y'.\nLow-rank model editing: the edit is an update W\u2081 \u2190 W\u2081 + UVT where U and V are low-rank matrices learned via SGD to minimize the MSE of f(x') and y'; in our experiments we fix the rank tor = 2. The model weights W1,..., WL, are kept frozen. We use the implementation"}, {"title": "4. Experiments", "content": "For both datasets, our goal is to adapt the model for the new domain while maintaining the model's performance on the original dataset. On the aging dataset, this means updating a model to be accurate on the set of images and labels (x', y') that have undergone the D 24-hour cycles of aging, while maintaining performance on the original unaged datapoints (x, y). Similarly, for the detector dataset the model is updated to be accurate on datapoints taken from both the T2 SE detector and the original detector.\nWe use a ConvNeXt-Small model [15] trained with SGD on the original SEM image dataset with a validation accuracy of 97.9%. For each of the model updating methods, we perform a coarse grid search over learning rates for each convolutional layer, and then a finer-grained grid search over the two best performing layers for each method. To choose the best performing hyperparameters for an update, we employ the hyperparameter selection strategy from [2], where we select 50% of the dataset examples (for a given age D for the aging dataset or for the images taken from the T2 SE detector) to learn the update and to select the best hyperparameters and the remaining 50% to test performance. A hyperparameter run is dropped if it causes the model performance on the original SEM validation set to drop below a given threshold (e.g., a threshold of 1.5% drop for Fig. 1 and a substantially more permissive 7% threshold for Fig. 2)."}, {"title": "4.1. Aging Experiments", "content": "The aging editing task comprises seven different material aging time steps. We use this dataset structure to evaluate how well models edited with images of age D generalize to other durations of particle aging. In other words, we ask how well a model update captures the relevant morphology of aging as opposed to the particulars of the collected dataset/aging timing. For example, we examine how well a material aged for D = 43 days performs on unaged control samples (0 days), earlier ages (aging for 14, 24, and 36 days, respectively) and later ages (54 and 60 days).\nFig. 1 summarizes the results of the aging experiments. The diagonal elements capture the performance of a model on the targeted aging duration whereas off-diagonal elements capture the generalization of those models to other aging durations. The performance of the original model (without any updates) is plotted in Fig. 1c. Because all full-model finetuning runs incurred substantial accuracy drops on the base (non-aged) validation set, we also plot performance for a more permissive threshold in Fig. 2. Note that all of the model update methods surgical finetuning, low-rank editing, and even the baseline full-model finetuning for the permissive threshold exhibit partial generalization across different ages. The generalization to shorter aging durations is stronger than for longer ones.\nComparing rows of Fig. la to those of Fig. 1b, we see that low-rank editing outperforms surgical finetuning on the targeted aging duration (e.g., editing a model on 43-day aging examples and testing on held-out 43-day aging examples) as well as on other aging datasets (e.g., editing a model on 43-day aging examples and testing on hold-out data for D\u2260 43). For example, in the aging tasks for 43 days and 54 days, low-rank editing has better editing performance for the target editing task by 15% for 43 days and 44% for 54 days of aging. For the off-target editing generalization to other aging sets, low-rank editing outperforms surgical fine-tuning as much as 27%. Both update methods increase model aging accuracy over the baseline model in Fig. 1c and over full finetuning in Fig. 2, and are targeted, i.e., do not pay substantial performance costs on the original SEM dataset. We expect that the better targeting of editing is due to regularization, where we update only a single-layer for surgical finetuning and single-layer with a low-rank restriction for low-rank editing.\nFinally, we note that all update methods have worse generalization on the 60-day aging data. While the updates on the 60-day aging duration achieve reasonable performance on the held-out 60-day set, they do not generalize well to D\u2260 60. Likewise, updating a model for D \u2260 60 leads to relatively minimal increases in accuracy for D = 60. We hypothesize that this poor performance is due to slightly different collection conditions for the 60-day micrographs, where a different scientist collected the SEM samples. This suggests that the success of model updates is sensitive to the image collection parameters; rather than editing for the changes in surface morphology seen in D = 60 materials, edits were made on the style (e.g., brightness, contrast) of the image. We expect that editing performance will increase when using editing prototypes that better reflect the diversity in SEM images, however we leave this to future work."}, {"title": "4.2. SEM Detector Experiments", "content": "The detector editing task captures input-level and statistical changes in the distribution of SEM images due to using a different scanning electron microscope. In general, this task is more challenging for the editing methods under consideration. While editing does provide performance improvement over the baseline unedited model, the accuracy increases are modest. Box plots of model accuracy across"}, {"title": "5. Conclusions and Future Work", "content": "We find that we can update models trained to predict the processing conditions of uranium ore concentrates to better handle a common domain-specific distribution shifts. Model editing methods perform particularly well on the set of aging distribution shifts, representing feature-level morphological changes in the SEM images. This has the potential to be particularly useful for designing future aging studies and incorporating their data, where model edits for a duration D can be reliably expected to generalize to shorter durations (and to a lesser but still notable extent, longer ones).\nThe detector distribution shift, reflecting lower-level variation in the statistics of input samples, proved more difficult. We are particularly optimistic about two directions for future work. First, we think that more targeted mixtures of image examples used for editing may better capture relevant directions of variation. For example, using exemplars from multiple detectors may allow for better generalization on a held-out detectors. Second, strong results in this do-"}]}