{"title": "Order-Optimal Projection-Free Algorithm for Adversarially Constrained Online Convex Optimization", "authors": ["Yiyang Lu", "Mohammad Pedramfar", "Vaneet Aggarwal"], "abstract": "Projection-based algorithms for constrained Online Convex Optimization (COCO) face scalability challenges in high-dimensional settings due to the computational complexity of projecting iterates onto constraint sets. This paper introduces a projection-free algorithm for COCO that achieves state-of-the-art performance guarantees while eliminating the need for projections. By integrating a separation oracle with adaptive Online Gradient Descent (OGD) and employing a Lyapunov-driven surrogate function, while dynamically adjusting step sizes using gradient norms, our method jointly optimizes the regret and cumulative constraint violation (CCV). We also use a blocked version of OGD that helps achieve trade- offs betweeen the regret and CCV with the number of calls to the separation oracle. For convex cost functions, our algorithm attains an optimal regret of $O(\\sqrt{T})$ and a CCV of $O(\\sqrt{T}\\log T)$, matching the best-known projection-based results, while only using $\\tilde{O}(T)$ calls to the separation oracle. The results also demonstrate a tradeoff where lower calls to the separation oracle increase the regret and the CCV. In the strongly convex setting, we further achieve a regret of $O(\\log T)$ and a CCV of $O(\\sqrt{T}\\log T)$, while requiring $O(T^2)$ calls to the separation oracle. Further, tradeoff with the decreasing oracle calls is studied. These results close the gap between projection-free and projection-based approaches, demonstrat- ing that projection-free methods can achieve performance comparable to projection-based counterparts.", "sections": [{"title": "1 Introduction", "content": "Online Convex Optimization (OCO) has emerged as a foundational framework for sequen- tial decision-making under uncertainty, with applications spanning resource allocation, real- time control systems, and adversarial robustness in machine learning (Shalev-Shwartz et al., 2012). A critical variant of this framework, Constrained Online Convex Optimization (COCO), requires agents to minimize cumulative costs while adhering to time-varying con- straints. In this problem, the learner interacts with an environment over T rounds, where at round t, the learner selects a decision $x_t \\in \\mathcal{K}$. The adversary then reveals a convex cost function $f_t: \\mathcal{K} \\rightarrow \\mathbb{R}$ and k convex constraint functions $g_{t,i} : \\mathcal{K} \\rightarrow \\mathbb{R}, i \\in [k]$. Let $\\mathcal{K}^* \\subseteq \\mathcal{X}$ denote the set of feasible decisions satisfying all constraints $g_{t,i}(x) \\leq 0$ for $t \\in [T], i \\in [k]$. We assume $\\mathcal{K}^* \\neq \\emptyset$ to ensure that at least one feasible solution exists. Then, we use the following performance metrics:\n\n\n$\\begin{equation}  \\text{Regret}(T) = \\sum_{t=1}^T f_t(x_t) - \\min_{x^* \\in \\mathcal{K}} \\sum_{t=1}^T f_t(x^*)  \\tag{1} \\end{equation}$\n\n$\\begin{equation}  \\text{CCV}(T) = \\max_{i \\in [k]} \\sum_{t=1}^T (g_{t,i}(x_t))_+. \\tag{2} \\end{equation}$\nThe goal is to design an algorithm that achieves sublinear $\\text{Regret}(T)$ and $\\text{CCV}(T)$ si- multaneously. COCO's applications span diverse domains, including portfolio optimization under dynamic risk constraints (Redeker and Wunderlich, 2018), dynamic resource alloca- tion for time-varying workloads (Doostmohammadian et al., 2022), real-time dynamic pric- ing with inventory constraints (Li and Zheng, 2024), and collision-free trajectory planning under safety constraints (da Silva Arantes et al., 2019).\nWhile traditional COCO algorithms often rely on computationally expensive projection operations to maintain feasibility, scaling these methods to high-dimensional settings, where projections become intractable, remains a significant challenge. This paper addresses this gap by introducing a projection-free algorithm that achieves state-of-the-art performance in adversarial COCO, matching the regret and constraint violation bounds of projection-based methods while eliminating the need for costly projections.\nWe summarize the key related works in Table 1. We note that most works require solv- ing a constrained convex optimization problem on each round (CONV-OPT), or requires Euclidean projection operation on the convex set (Projection). Both operations are compu- tationally complex, rendering these methods impractical for modern large-scale problems. While some projection-free algorithms, such as those based on Frank-Wolfe or linear opti- mization oracles, avoid projections, they historically suffer from suboptimal regret or CCV guarantees, particularly under adversarial constraints. The state of the art projection free approach for adversarial constraints is studied in (Garber and Kretzu, 2024), where the re-"}, {"title": "2 Related Works", "content": "COCO: COCO problems have been widely studied, since they can handle complex action set (Mahdavi et al., 2012). Table 1 provides some of the key works for regret and CCV guar- antees in the area. (Guo et al., 2022) proposed an algorithm that solves a constrained convex optimization problem in each round, and achieves $O(\\sqrt{T})$ regret with $O(T^{3/4})$ CCV. This result is a special case of the result in (Yi et al., 2023). Recently, Sinha and Vaze (2024) pro- posed a projection-based algorithm that achieves regret and CCV of $\\tilde{O}(T^{1/2})$. The results have been further extended to strongly convex cost functions, where all these works show a regret of $\\tilde{O}(1)$ with a CCV of $\\tilde{O}(T^{1/2})$. However, performing a constrained convex opti- mization or doing a orthogonal projection is computationally complex (Garber and Kretzu, 2022), and thus we provide a projection-free algorithm in this paper.\nProjection-free Algorithms for OCO: Frank-Wolfe based approaches have been pro- posed for OCO, and achieve a regret of $O(T^{3/4})$ for convex cost function (Daniely et al., 2015; Hazan and Kale, 2012) and $O(T^{2/3})$ for strongly convex cost functions (Kretzu and Garber, 2021). These approaches do not scale well with the dimensional spaces due to the use of lin- ear optimization oracle. Garber (2021) proposed the concept of infeasible projection which was implemented with linear optimization oracle to replace the traditional orthogonal pro- jection oracles, and used that for online linear optimization. Garber and Kretzu (2022) gave an approach to implement infeasible projection via separation oracle, which achieves an op- timal regret of $O(\\sqrt{T})$ using online gradient descent based approach with constant step size. Their results for the case where infeasible projection is implemented via linear optimization oracle does not achieve order-optimal regret. This motivates the choice of use of infeasible projection using separation oracle in our work. We also note that (Pedramfar and Aggarwal, 2024) studied a variant of the approach to implement infeasible projection via separation oracle in (Garber and Kretzu, 2022), and applied that to online gradient descent with con- stant step size. The key difference is that the approach of (Pedramfar and Aggarwal, 2024) queries only in the constraint set, which is not necessary in (Garber and Kretzu, 2022), which motivates us to use the approach in (Pedramfar and Aggarwal, 2024).\nProjection-free Algorithms for COCO: Projection-free algorithms for online learn- ing were proposed in response to the growing prevalence of high-dimensional data, which led to increased computational challenges for projection operators (Hazan and Kale, 2012; Hazan and Minasyan, 2020; Garber and Kretzu, 2022; Lee et al., 2023). Recent work in (Garber and Kretzu, 2024) have studied projection-free algorithms for COCO, but do not achieve order-optimal regret and CCV (See Table 1). In contrast, this paper provides the algorithm that achieves order optimal regret and CCV."}, {"title": "3 Problem Setup", "content": "To begin with, we formally describe the problem that our algorithm aims to solve. Consider an agent is playing a consecutive online game with an adversary. At each step t, the agent chooses an action $x_t$, and the adversary chooses a cost function $f_t$ and k constraint functions"}, {"title": "4 Projection Free Algorithm for OCO: BPFAdaOGD", "content": "In order to solve the COCO problem, we will use surrogate functions that combine the objectives and constraints. For this surrogate function optimization, we need a projection- free OCO algorithm, which will be provided in this Section.\nThe proposed algorithm is based on Online Gradient Descent with adaptive stepsize (AdaOGD). We note that although (Garber and Kretzu, 2022) proposed a projection free OGD using an infeasible projection oracle (Algorithm 1, Lemma 4), there are no blocks and the step size were considered to be constant. However, the guarantees of COCO are obtained with a blocked version that uses adaptive step-sizes, which is why we will analyze the variant of projection-free OGD described in Algorithm 1.\nThe proposed algorithm, Blocked Projection-Free Adaptive OGD (BPFAdaGD), is given in Algorithm 1. Given action set $\\mathcal{K}$, time horizon T, $c \\in \\text{relint}(\\mathcal{K})$, $r = r_{\\mathcal{K},c}$, block size K, shrinking parameter $\\delta \\in [0,r)$, diameter of the action set D, an instance of IP-SO algorithm $\\mathcal{P}$implemented by Algorithm 3, and in the strongly-convex case the strongly- convex parameter $\\theta$. We initiate the program by choosing $x_1 \\in \\mathcal{K}$, and pass $\\mathcal{K}, c, r, d$ to $\\mathcal{P}$ to initiate algorithm instances. Within each block m, there are K iterations, represented by $\\mathcal{T}_m$. At each step t, we play action $x_t$, and observe the cost function $f_t$ adversarially selected and revealed by the adversary, which we use to compute $\\nabla_t$, the gradient of $f_t$ at $x_t$. After K iterations, we compute the average gradient in the block m, $\\nabla_m = \\frac{1}{K} \\sum_{t \\in \\mathcal{T}_m} \\nabla_t$, and perform a gradient descent update, $x_{m+1} = \\mathcal{P}\\left(x_m - \\eta_m \\nabla_m\\right)$, with adaptive step-size $\\eta_m = \\frac{D}{\\sqrt{\\epsilon + \\sum_{m'=1}^m ||\\nabla_{m'}||^2}}$ in the convex case, or $\\eta_m = \\frac{1}{m\\theta}$ in the strongly convex case. The gradient descent update $x_m - \\eta_m \\nabla_m$ is passed to the IP-SO instance $\\mathcal{P}$ to obtain next action, $x_{m+1}$. The following result bounds the regret of Algorithm 1."}, {"title": "5 Projection-free algorithm for Adversarial COCO: BPFAdaOGD-Sur", "content": "In this section, to solve Online Convex Optimization with Adversarial Constraint (Adver- sarial COCO), we will propose Algorithm 2, blocked Projection-Free Adaptive OGD with Surrogate function (BPFAdaOGD-Sur), and analyze its regret and CCV guarantees. This algorithm creates a surrogate function $\\bar{f}_t$ which combines the cost function and the cumu- lative constraint violation, and applies the OCO algorithm in Section 4 on the surrogate function. While Algorithm 2 is given for both convex and strongly convex cost functions, this section studies the convex cost function case, while the strongly convex cost function will be studied in Section 6.\nFor computational convenience, we introduce a processing parameter $\\gamma$ and we let $\\bar{f}_t = \\gamma f_t, \\bar{g}_{t,i} = \\gamma (g_{t,i})_+$. To track the cumulative constraint violation, we let $Q_t = Q_{t-1} + \\bar{g}_t(x_t)$, and $Q_0 = 0$. Since our objective is to make the cumulative constraint violation small, we introduce a potential function $\\Phi : \\mathbb{R}_+ \\rightarrow \\mathbb{R}_+$ that is non-decreasing, differentiable, convex, and satisfies $\\Phi(0) = 0$, and we call $\\Phi(\\cdot)$ Lyapunov function from here forward. We create the surrogate cost function to be $\\hat{f}_t = V f_t + \\Phi'(Q_t) \\bar{g}_t$ where V is a real number and $\\Phi'(Q_t)$ is the gradient of Lyapunov function $\\Phi$ at $Q_t$. V is introduced as a regularization parameter to balance between cost and constraint violation.\nThe algorithm use the inputs: action set $\\mathcal{K}$, shrinking parameter $\\delta$, block size K, di- ameter of the action set D, time horizon T, the common Lipschitz continuous constant $M_1$ for the cost function and constraints, an instance of Algorithm 1 A, processing parameter $\\gamma$, regularization parameter V, and Lyapunov function $\\Phi$. We initiate the algorithm by"}, {"title": "6 Projection-free COCO for Strongly Convex Cost Function", "content": "We note that the regret bounds can improve when the cost functions are strongly convex. In this section, we describe the proposed algorithm when the cost functions are strongly convex, and analyze the Regret and CCV guarantees for the proposed algorithm.\nLet the cost functions $f_t$ be $\\theta$-strongly convex while the constraint functions are consid- ered to be convex, not necessarily strongly convex. Let them all be $M_1$-Lipschitz continuous. In this case, we still apply BPFAdaOGD-Sur (Algorithm 2), but with different parameters. More precisely, we choose $\\gamma = 1$, $V = \\frac{8M_1^2 K \\log(T \\epsilon / K)}{\\theta}$, and $\\Phi(\\cdot) = (\\cdot)^2$. Further, the BP- FAdaOGD instance A uses step size $\\eta_m = \\frac{1}{m \\theta}$. Having studied the regret for OCO for strongly convex functions in Section 4, we now use the result to study the regret and CCV for the adversarial COCO problem for strongly convex costs, with similar approach to the proofs in Section 5. The results are given in Theorem 8."}, {"title": "7 Conclusion", "content": "In this paper, we proposed a novel projection-free adaptive online gradient descent (OGD) algorithm for adversarially Constrained Online Convex Optimization (COCO). Our method eliminates the need for costly projection steps while achieving regret and cumulative con- straint violation (CCV) bounds that match state-of-the-art projection-based methods. By leveraging a infeasible projection approach and adaptive step-size updates, we achieve order- optimal performance in both convex and strongly convex settings."}, {"title": "Appendix A. Useful lemmas", "content": "Here we introduce some technical lemmas that are used in our proofs.\nLemma 9 (Lemma 4.13 in (Orabona, 2019)) Let $a_0 \\geq 0$ be a real number, $N \\geq 1$ be an integer, and $(a_t)_{t=1}^N$ be a sequence of non-negative real numbers. For any non-increasing function $f : [0,\\infty) \\rightarrow [0, \\infty)$, we have\n\n$\\frac{\\sum_{t=1}^N a_t}{\\sum_{t=0}^N \\big(a_0 + \\sum_{i=1}^t a_i\\big)} \\leq \\int_{a_0}^{\\sum_{t=1}^N a_t} f(x)dx.$\nLemma 10 Assume f is $\\theta$-strongly convex and g is convex. Let a and b be some non- negative real number. Then $af + bg$ is a$\\theta$-strongly convex."}, {"title": "Appendix B. Infeasible Projection via a Separation Oracle", "content": "Here we introduce the details of IP-SO algorithm and an important Lemma 11."}, {"title": "Appendix C. Proof of Theorem 1", "content": "Proof Let $x^* \\in \\text{argmin}_{x \\in \\mathcal{K}} \\sum_{t=1}^T f_t(x)$ and $\\bar{x} = (1 - \\frac{\\delta}{r})x^* + \\frac{\\delta}{r} c \\in \\mathcal{K}_\\delta$. Thus, the regret of the proposed algorithm is given as\n\n$\\begin{equation}   \\text{Regret}(T) = \\sum_{t=1}^T (f_t(x_t) - f_t(x^*)) = \\sum_{t=1}^T (f_t(x_t) - f_t(\\bar{x})) + \\sum_{t=1}^T (f_t(\\bar{x}) - f_t(x^*)).   \\tag{11} \\end{equation}$\n\nWe bound the two terms separately.\nFirst we consider $\\sum_{t=1}^T (f_t(x_t) - f_t(\\bar{x}))$. From Lemma 11, we have for $\\bar{x} \\in \\mathcal{K}_\\delta$,\n\n$||x_{m+1} - \\bar{x}||^2 \\leq ||x_m - \\eta_m \\nabla_m - \\bar{x}||^2  <  ||x_m - \\bar{x}||^2 + \\eta_m^2 ||\\nabla_m||^2 - 2\\eta_m \\langle \\nabla_m, x_m - \\bar{x} \\rangle$.\n\nRearranging, we have\n\n$\\begin{equation}  \\langle \\nabla_m, x_m - \\bar{x} \\rangle \\leq \\frac{1}{2\\eta_m} (||x_m - \\bar{x}||^2 - ||x_{m+1} - \\bar{x}||^2) + \\frac{\\eta_m}{2} ||\\nabla_m||^2.  \\tag{12} \\end{equation}$\n\nSumming this over m from 1 to T/K, we have\n\n$\\sum_{m=1}^{T/K} \\langle \\nabla_m, x_m - \\bar{x} \\rangle \\leq \\frac{1}{2\\eta_1} ||x_1 - \\bar{x}||^2 + \\sum_{m=2}^{T/K} \\frac{1}{2} \\big( \\frac{1}{\\eta_m} - \\frac{1}{\\eta_{m-1}} \\big) ||x_m - \\bar{x}||^2 + \\sum_{m=1}^{T/K} \\frac{\\eta_m}{2} ||\\nabla_m||^2$\n\n$\\leq \\frac{D^2}{2\\eta_1} + D^2 \\sum_{m=2}^{T/K}  \\big( \\frac{1}{2\\eta_m} - \\frac{1}{2\\eta_{m-1}} \\big) + \\sum_{m=1}^{T/K} \\frac{\\eta_m}{2} ||\\nabla_m||^2$\n\n$\\quad = \\frac{D^2}{2\\eta_1} + \\frac{D^2}{2\\eta_{T/K}} + \\sum_{m=1}^{T/K} \\frac{\\eta_m}{2} ||\\nabla_m||^2 ,$\n\nwhere we used the facts that $||x_m - \\bar{x}|| < D$ and that $\\eta_m$ is non-increasing. By the convexity of $f_t$, we have $f_t(x_t) - f_t(\\bar{x}) \\leq \\langle \\nabla_t, x_t - \\bar{x} \\rangle$. Summing this over t, we have\n\n$\\sum_{t=1}^T f_t(x_t) - f_t(\\bar{x}) \\leq \\sum_{t=1}^T  \\langle \\nabla_t, x_t - \\bar{x} \\rangle = \\sum_{m=1}^{T/K}  \\sum_{t \\in \\mathcal{T}_m} \\langle \\nabla_t, x_t - \\bar{x} \\rangle =  \\sum_{m=1}^{T/K}  \\langle K \\nabla_m, x_m - \\bar{x} \\rangle$\n\n$\\quad \\leq \\frac{KD^2}{2\\eta_{T/K}} + \\frac{K}{2} \\sum_{m=1}^{T/K} \\eta_m ||\\nabla_m||^2 , \\tag{13} \\end{equation}$"}, {"title": "Appendix D. Proof of Theorem 3", "content": "Proof Let $x^* \\in \\text{argmin}_{x \\in \\mathcal{K}} \\sum_{t=1}^T f_t(x)$ and $\\bar{x} = (1 - \\frac{\\delta}{r})x^* + \\frac{\\delta}{r} c \\in \\mathcal{K}_\\delta$. We may decompose the regret in the same manner as Equation (11) to see that"}]}