{"title": "Using Images to Find Context-Independent Word Representations in Vector Space", "authors": ["Harsh Kumar"], "abstract": "Many methods have been proposed to find vector representation for words, but most rely on capturing context from the text to find semantic relationships between these vectors. We propose a novel method of using dictionary meanings and image depictions to find word vectors independent of any context. We use auto-encoder on the word images to find meaningful representations and use them to calculate the word vectors. We finally evaluate our method on word similarity, concept categorization and outlier detection tasks. Our method performs comparably to context-based methods while taking much less training time.", "sections": [{"title": "1. Introduction", "content": "Using vector space representations for words is a widely studied area in natural language processing. Many methods have been proposed to finding semantically meaningful vector representations for words, including the bag-of-words model, skip-grams (Mikolov et al., 2013), character n-grams (Bojanowski et al., 2017), matrix-factorization methods (Lund & Burgess, 1996), (Rohde et al., 2005), (Bullinaria & Levy, 2007), (Lebret & Lebret, 2013) and RNN architectures (Luong et al., 2013b).\nHowever, nearly all these methods treat word tokens as individual entities and attempt to form semantic similarity between their representations by capturing some context from the text. As a result, the meaning of any given word becomes limited to the diversity of contexts seen in the dataset. In other words, the learned embedding of a word is a static point in the vector space, and any semantic relationship between the word vectors is limited to the contexts seen in the dataset.\nThis work presents our study on representing words based on their dictionary definitions. We believe that (potentially multiple) definitions of a term can fully explain its meaning across contexts. This is consistent with traditional methods in pedagogy where students are often learn definitions of new terms along with their example usage.\nSince there are a limited number of words in any vocabulary, other words are needed to explain the meaning of a given word. The \"definition\" is thus a recursive relationship between words. To deal with this issue, we employ the pedagogical approach of using images to describe word meanings. For this, we create a custom image dataset for words encountered in the definitions of vocabulary words. We then use auto-encoders to find semantic representations for these images in a sequence to get the final word representation.\nAs we shall see, our method gives a comparable performance with context-based methods while being computationally cheaper, taking only ten hours for training.\nWe list our contributions as follows:\n\u2022 We advocate the usage of definitions for finding word vector representations instead of relying on diverse contexts to capture their meanings.\n\u2022 We use images to represent the definition terms for the words and use auto-encoders to find hidden representations for these images. The hidden representations of these images are then appended together in a sequence to find the final representation of the original word.\n\u2022 We release our custom dataset which consists of 5,00,000+ images for 1,15,000+ terms occurring in vocabulary and their definitions.\nThe rest of the paper has been organized as follows. In Section 2, we list previous works done in the field of word representations. In Section 3, we explain our approach including the custom dataset preparation and the architecture of auto-encoder used. In Section 4, we provide training details and the set of tasks on which we evaluate our method. In Section 5, we explain our observations and finally, in Section 6, we conclude our paper."}, {"title": "2. Related Work", "content": "The representation of words in the machine learning literature has a long history. Early methods represented words as one-hot vectors. This method did not capture any semantic relationship between words. A related method was the bag-of-words (BoW) representation. The bag-of-words approach created a vocabulary by tokenizing all the text, followed by assigning a vector of vocabulary length to each sentence. Each element in the vector corresponds to the frequency of words in the sentence. The TF-IDF improved upon the one-hot encoding by weighting the importance of words in documents based on their frequency in the text. However, it still treated words independently without capturing any semantic relationship.\n(Bengio et al., 2003) proposed neural-network architecture where a feedforward neural network with a linear projection layer was used to learn jointly the word vector representation and a statistical language model. (Mikolov et al., 2009) proposed another neural network architecture where the word vectors learned using a hidden layer are used to train the language model.\n(Mikolov et al., 2013) take the neural-network-based representation a step further and propose model architectures for computing continuous vector representations (embeddings) that capture semantic relationships. They propose two models: CBOW and Skip-grams. CBOW predicts a word given its context and Skip-gram predicts the context given the word. (Pennington et al., 2014) propose a global log-bilinear model that combines the matrix factorization and local context window methods. It creates word embeddings by leveraging word-word co-occurrence statistics from a text.\n(Bojanowski et al., 2017) propose a representation of word vectors as a sum of character n-grams to preserve the morphology of words. Their approach works better on a dataset of rare words for word similarity and analogy tasks. (Luong et al., 2013a) propose a novel RNN architecture capable of building representations for morphologically complex words from their morphemes. (Chen, 2017) propose to represent a document as a simple average of word embeddings to capture the semantic meaning of documents during learning. (Roy et al., 2016) use word vectors to deal with information retrieval. They represent documents and queries as sets of word-embedded vectors for indexing and scoring documents. (O et al., 2018) calculate word vector representations using a knowledge-based graph to generate the context of an ambiguous word. This improves the performance of the learned vector representations in word-sense disambiguation tasks.\n(Maas et al., 2011) argue that vector-based approaches to semantics can model rich lexical meanings but fail to capture sentiment information. Their proposal alleviates this problem. (Erk & Pad\u00f3, 2008) propose a structured vector space model to compute word vector representations to identify the meaning of word occurrences which can vary widely according to context. (Vilnis & McCallum, 2015) advocate for using density-based distributed embeddings for word representations instead of mapping words as a point in vector space.\n(Klein et al., 2015) deal with the problem of associating a sentence with an image using Fisher vectors. (Lauren et al., 2017) use auto-encoders based on extreme learning to find word representations on a word-context matrix."}, {"title": "3. Proposed Solution", "content": "Our proposed solution is based on a simple approach employed in pedagogy. Visual examples are often observed to be effective when teaching a new term or concept to students and students the meaning of a concept is often related with some visual representation.\nThe meaning of a word is often explained using a sequence of other words (definitions). For clarity, we call the words appearing in the definition of a word as definition-terms set. Each word in the definition-terms also needs an explanation, which is another sequence. For a limited vocabulary, this inevitably leads to circular dependencies among the words.\nword - (meaning) \u2192 sequence of words\nTo avoid this issue, we use images to represent the words found in the definitions. We call the list of these images as that word's image-set. The collection of all image-sets for all the words in the vocabulary leads to a custom image dataset. We train an auto-encoder model on this image dataset to find meaningful representations. Representations for each image in the image-set are appended to get the final representation of the word.\nWe explain the dataset preparation and the auto-encoder architecture in the following sections."}, {"title": "3.1. Dataset Preparation", "content": "We use the pre-trained BERT model's vocabulary as our base vocabulary. The meanings for these words are obtained using definitions for them available in dictionaries at Project Gutenberg. Any word found in the definitions, but not in the base vocabulary are also added to our final list of words. The final set of words forms our final vocabulary.\nOur dataset consists of images for terms in the final vocabulary in such a way that the image-set of a given word consists of images for the words in its definition-terms set. The images in the image-set is organized in the same sequence as the words in the definition-terms. The overall approach is shown in Figure 1 and is explained next.\n\u2022 For each term in the base vocabulary, we find its definition using a dictionary. We use the dictionaries available at Project Gutenberg and Wiktionary for the same. The new terms encountered in the definition are included in the final vocabulary. Our final vocabulary has 1,15,458 terms.\n\u2022 We use the CommonCrawl dump image links and DuckDuckGo's image search API to search for openly available images for terms in the final vocabulary. The image set for each term consists of the images of the given term and images for the terms in the definition in the same sequence as they appear.\nFor example, if the definition of a given word word is given by the terms w1 w2 w3 w4 w5, then the image-set of word consists of list of images of word, w1, w2, w3, w4 and w5 in the exact sequence. We carefully select five images for each term to account for the fact that a given term can have multiple meanings.\n\u2022 We limit the definition for each word to a maximum of 19 terms. We append empty tokens (represented by blank images) to the definition if their size is below the limit. As such, each word is represented by a set of (19 + 1) * 5 = 100 images in a specific sequence.\nPlease note that common occurring words such as is, are, also etc. are not considered. However, the emphatic words, question words, conjunctions and punctuations are not removed because these may change the tone and meaning of a sentence."}, {"title": "3.2. Auto-Encoder", "content": "We train an autoencoder model to find meaningful representations for the images in the dataset. The model consists of a five-layer encoder-decoder architecture with a latent state size of 32. Each encoder layer consists of a Convolution-ReLU block with convolution having a kernel size of 3x3, stride = 1, and padding = 1. Each decoder layer consists of a ConvTranspose2d-ReLU pair (the last layer has a sigmoid instead of the ReLU activation) with a kernel size of 3x3, stride = 1, and padding = 1 for the ConvTranspose2d layer. The overall architecture is shown in Figure 2.\nThe input images are reshaped to 32x32 and the model finds a 32-dimensional vector representation. The vector representations of all the images in the word's image-set are appended to calculate the final 3200-dimensional vector representation for the given word."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Training details", "content": "We trained our auto-encoder model on a custom dataset of 5,77,290 images corresponding to 1,15,458 terms of our final vocabulary. Each image is resized to 32x32 pixels. We use binary-cross-entropy loss to measure the image reconstruction error.\nThe autoencoder model is initialized with random weights and trained for 25 epochs. We use the Adam optimizer, with an initial learning rate of 0.00215. The learning rate is halved every 5 epochs. The training takes 10 hours on an NVIDIA Ampere GPU with 8GB GDDR6X."}, {"title": "4.2. Evaluation Methods", "content": "We conduct extensive evaluation experiments on various word semantic similarity, Concept Categorization, and Outlier word detection tasks. We evaluate our proposed method on several benchmarks whose details are listed as follows:"}, {"title": "4.2.1. WORD SEMANTIC SIMILARITY", "content": "The task of word semantic similarity is based on the idea that the distances between the words in the embedding space follow closely to the actual semantic distances evaluated using human judgments. We use the cosine distance between the word vectors to represent the semantic distance between their meanings. Our benchmarks include WordSim-353 (Finkelstein et al., 2002), MC-30 (Miller & Charles, 1991), RG-65 (Rubenstein & Goodenough, 1965), RW (Luong et al., 2013b), SimVerb-3500 (Gerz et al., 2016), SimLex-999 (Hill et al., 2014), MTurk-287 (Radinsky et al., 2011), Verb-143 (Baker et al., 2014)"}, {"title": "4.2.2. OUTLIER WORD DETECTION", "content": "The task is to identify a semantically anomalous word in an already-formed cluster. We evaluate our method on 8-8-8 (Camacho-Collados & Navigli, 2016) and WordSim-500 (Blair et al., 2016) datasets."}, {"title": "4.2.3. CONCEPT CATEGORIZATION", "content": "The task is to evaluate the quality of word representations clustered into subsets belonging to different categories. For example, the words chair, bench, bed should be in a distinct cluster as compared to car, bus and truck. We evaluate our method on the Battig and Montague (Battig & Montague, 1969), Almuhareb and Poesio (Almuhareb & Poesio, 2004), BLESS (Baroni & Lenci, 2011), and ESSLLI-2008 (Icard & Muskens, 2010) benchmarks."}, {"title": "5. Results", "content": "We compare our proposed method with other word-embedding models in Table 1. A similarity score is obtained using the cosine similarity. We compute spearman's rank correlation coefficient between the score and the human judgments. The data for other models were taken from (Wang et al., 2019) since their study uses the Wiki2010 dataset for training (number of tokens in this dataset is comparable to number of images in our custom dataset). We observe that our approach gives comparable performance to other models while taking less training time.\nTable 2 lists down the accuracy for outlier word detection datasets. We observe comparable performance with other models.\nWe model the concept categorization as a clustering task and report v-measure scores for the benchmarks. The v-measure score is the harmonic mean between the homogeneity and completeness metrics and is independent of the labels assigned to the clusters. Table 3 lists down v-measure scores for concept categorization datasets. We observe decent v-measure scores for all the datasets which indicates that the word representations help achieve a good clustering quality."}, {"title": "6. Conclusion and Future Work", "content": "In this work, we discussed a simple yet effective approach to finding semantically sound word vector representations. We employed an auto-encoder to find latent representations for images corresponding to words and their definition terms and appended those latent representations sequentially to obtain the find word vector representation.\nWe also observed finding such vector representations take a relatively small amount of time due to the smaller size of the auto-encoder model. At the same time, the obtained word vectors show decent performance on word similarity, concept categorization, and outlier detection tasks.\nWe want to point out that the main challenge with this approach is the creation of a custom dataset and picking relevant images for each term in the vocabulary. As such, one should expect the quality of the word vectors to be dependent on the selected images to represent the word. At the same time, it can be argued that this approach is independent of finding appropriate length context windows and using large text datasets which is pervasive with other models.\nAn interesting work would be to evaluate the effectiveness of our approach on machine translation tasks because a given object may be described using different words across different languages, however, images for those objects remain the same."}]}