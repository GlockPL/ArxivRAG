{"title": "HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic Evaluation Using a Vision Language Model", "authors": ["Masanari Ohi", "Masahiro Kaneko", "Naoaki Okazaki", "Nakamasa Inoue"], "abstract": "Vision-language models (VLMs) have shown impressive abilities in text and image understanding. However, existing metrics for evaluating the text generated by VLMs focus exclusively on overall quality, leading to two limitations: 1) it is challenging to identify which aspects of the text need improvement from the overall score; 2) metrics may overlook specific evaluation criteria when predicting an overall score. To address these limitations, we propose HarmonicEval, a reference-free evaluation metric that aggregates criterion-wise scores to produce the overall score in a bottom-up manner. Furthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE) dataset, which comprises 18,000 expert human judgments across four vision-language tasks. Our experiments demonstrate that HarmonicEval achieves higher correlations with human judgments than conventional metrics while providing numerical scores for each criterion.", "sections": [{"title": "1 Introduction", "content": "Automatic evaluation of text generated by vision-language models (VLMs) is essential for enhancing their robustness and reliability in tasks such as image captioning and visual question answering (Hessel et al., 2021; Lee et al., 2024b). As VLMs advance rapidly, the need for more informative and accurate metrics aligned with human perception has grown. However, most existing metrics focus solely on measuring the overall quality of texts generated by VLMs, lacking two critical aspects expected in automatic evaluation: 1) numerical explainability and 2) comprehensiveness.\nFirst, evaluation metrics that provide only overall scores (Papineni et al., 2002; Zhang et al., 2020; Hessel et al., 2021) cannot identify specific deficiencies in aspects of the text, such as clarity or fluency, as illustrated in Figure 1(a). While recent metrics for vision-language tasks (Chan et al., 2023; Lee et al., 2024b) offer free-form textual explanations to complement scores, these explanations lack the granularity and consistency that come from explicit numerical breakdowns. As a result, providing a numerical explanation that links specific aspect qualities to overall scores remains a challenge in the automatic evaluation of vision-language tasks.\nSecond, as discussed in previous studies (Kasai et al., 2022; Fabbri et al., 2021), existing metrics can fail to consider specific evaluation criteria when predicting overall scores. For instance, criteria such as correctness and completeness might be prioritized while conciseness is overlooked, as illustrated in Figure 1 (b). This limitation can become more pronounced when handling a range of vision-language tasks beyond image captioning, where comprehensive evaluation is crucial to achieve better alignment with human judgments."}, {"title": "2 Method", "content": "This section introduces HarmonicEval, a reference-free evaluation metric that integrates criterion-wise evaluation scores through harmonic weighting. Figure 2 shows an overview of HarmonicEval. First, a VLM is prompted to generate raw evaluation scores for the input text on each criterion independently (\u00a72.1). Second, score smoothing is applied to obtain robust scores by leveraging the output token probabilities assigned by the VLM (\u00a72.2). Finally, the individual scores are aggregated to produce an overall score through harmonic weighting (\u00a72.3).\n2.1 Prompts for criterion-wise evaluation\nGiven a text t (e.g., a caption for IC, or a question-answer pair for VQA) and an image x as inputs, this step produces raw evaluation scores $s_c = f([p_c, t], x)$ for each criterion c \u2208 C. Here, f is a VLM, $p_c$ is a pre-defined prompt, and C is a set of criteria. Based on a comprehensive literature survey of natural language generation (Asano et al., 2017; Kryscinski et al., 2019; Fabbri et al., 2021; Freitag et al., 2021; Song et al., 2024) and vision-\n2.2 Score smoothing\nTo improve alignment with human judgment, this step applies score smoothing. Inspired by previous studies (Liu et al., 2023b; Lee et al., 2024b), the first-order statistics of the output token probability distribution is utilized. Specifically, the smoothed scores $s_c$ are expected scores calculated as follows:\n$s_c = \\sum_{r=1}^{5} r p_c(r),$\n2.3 Score aggregation\nGiven a set of criterion-wise scores {$s_c : c \u2208 C$}, this step aggregates $s_c$ using harmonic weighting, which calculates the overall score s as\n$s = \\sum_{c \\in C} w_c s_c$,\nwhere 0 < $w_c$ \u2264 1 are weighting coefficients. The weighting coefficients are determined based on the second-order statistics of $p_c$ as\n$w_c = \\frac{\\frac{1}{\\sigma_c^{2(1-\\gamma)}}}{\\sum_{c \\in C} \\frac{1}{\\sigma_c^{2(1-\\gamma)}}} = \\frac{\\frac{1}{\\sigma_c^{2(1-\\gamma)}}}{H},$\nwhere $\\sigma_c$ is the standard deviation of the output token probabilities given by\n$\\sigma_c = \\sqrt{\\sum_{r=1}^{5} (r - s_c)^2 p_c(r)},$\nand $H = \\sum_{c \\in C} \\sigma_c^{-2(1-\\gamma)}$ is the harmonic mean of the variances with a hyperparameter \u03b3. Note that smaller values of $\\sigma_c$ can be interpreted as indicating higher confidence in the evaluation of c.\nThe role of hyperparameter \u03b3 is to bridge three weighting strategies: uniform weighting, inverse variance weighting and selective weighting. Below, we detail each strategy and discuss the hyperparameter setting.\nUniform weighting. When \u03b3 = 1, harmonic weighting reduces to uniform weighting $w_c$ = 1/|C|. This is effective when all criterion-wise scores are equally reliable in determining overall scores. However, this may not be the best estimator as observed variances are ignored in the aggregation process.\nInverse variance weighting. When \u03b3 = 0.5, harmonic weighting reduces to inverse variance weighting $w_c$ \u221d $\u03c3_c^{-2}$. This provides the best linear unbiased estimator under the assumption that the observed variance is due to statistical fluctuations. However, this assumption is not always reasonable, as each criterion may have its own variance.\nSelective weighting. When \u03b3 \u2192 0, only the score $s_c$ with the smallest variance is selected as the overall score. This approach is used in experiments to show the necessity of aggregation."}, {"title": "3 Dataset", "content": "This section presents the MMHE dataset, which consists of 18,000 human expert judgments covering four vision-language tasks and five evaluation criteria. This dataset promotes research towards automatic evaluation that aligns with human evaluation in a fine-grained manner. Example human judgment scores are shown in Figure 3.\n3.1 Vision-language tasks\nTo assess the multifaceted capabilities of VLMs, we selected four vision-language tasks: REG, VQA, VDU, and IC.\nReferring expression generation (REG) aims to generate textual expressions that uniquely identify a specific object marked by a bounding box. To evaluate the expression quality, the VLM is given an expression and an image with the relevant bounding box highlighted in red.\nVisual question answering (VQA) aims to answer a question posed in natural language based on the content of an image. To evaluate the answer quality, the VLM is provided with a triplet consisting of an image, a question, and an answer.\nVisual document understanding (VDU) focuses on interpreting and extracting meaningful information from visually presented documents. Quality evaluation follows a question-answering manner. Specifically, the VLM is provided with a document image, along with a question-answer pair related to the content.\nImage captioning (IC) aims to generate captions for given images. To evaluate caption quality, the VLM is provided with a text-image pair.\n3.2 Dataset construction\nThe dataset construction process consists of three steps: 1) Source selection, selecting input texts and images for tasks from datasets; 2) Target generation, creating target texts from sources using state-of-the-art VLMs; and 3) Human expert evaluation, assessing the quality of the generated texts.\nSource selection. We selected the following four datasets: RefCOCO (Kazemzadeh et al., 2014) for REG, OK-VQA (Marino et al., 2019) for VQA, VisualMRC (Tanaka et al., 2021) for VDU, and MSCOCO (Lin et al., 2014) for IC. We randomly\n3.3 Data Analysis\nFigure 4 presents the score distributions for each criterion across the four tasks. The correctness and completeness criteria exhibit diverse score distributions for most tasks, suggesting that even the"}, {"title": "4 Experiments", "content": "4.1 Settings\nBaselines. We implemented eight baseline metrics for the experiments on MMHE. Four of these are traditional n-gram-based metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). The remaining three metrics leverage neural network-based approaches: BERTScore (Zhang et al., 2020), BARTScore (Yuan et al., 2021), and CLIPScore (Hessel et al., 2021). Additionally, we employ FLEUR (Lee et al., 2024b), a VLM-based evaluation metric that provides a single overall score and its textual explanation. Details of these baselines are provided in Appendix E.\nPerformance measures. To assess the performance of evaluation metrics on MMHE, we use accuracy (%) for overall evaluations and Kendall's tau correlation coefficient \ud835\udf0f for criterion-wise evaluations.\n4.2 Main results\nOverall performance. Table 2 compares HarmonicEval with eight metrics in terms of overall performance on MMHE. HarmonicEval achieves the highest accuracy on REG (66.6), VQA (76.4), VDU (73.4), and IC (77.0). While FLEUR also achieves the highest score of 76.4 on VQA, it falls short on VDU. These results underscore the effectiveness of HarmonicEval across multiple tasks.\nCriterion-wise performance. Table 3 shows the correlations between predicted scores and human judgments for each evaluation criterion. HarmonicEval evaluates numerical explainability by reporting correlations between its predicted criterion-wise scores and human judgment scores, whereas other metrics report correlations between their predicted overall scores and criterion-wise human judgment scores. As shown, HarmonicEval achieves the highest correlation across most criteria, highlighting its numerical explainability. FLEUR emerges as the second-best metric, a trend also observed in overall comparisons.\nTask-wise trends in baseline metrics. To gain deeper insights into the evaluation behavior of baseline metrics, we examine the task-wise correlations. Specifically, we explore which evaluation criteria baseline metrics tend to prioritize or deprioritize for each task, potentially hindering comprehensive evaluation. In Table 3, black underlines denote the most correlated criterion, while gray underlines indicate the least correlated criterion for each metric within a task.\nFor REG, completeness shows the highest correlation across most metrics. This is understandable, as REG requires explicit expressions to identify a unique object by distinguishing it from surrounding objects. For VQA, most metrics are more strongly correlated with conciseness but less so with completeness. This indicates that existing metrics deprioritize completeness, potentially leading to inaccurate evaluations of insufficient answers. A similar trend is observed in VDU, where completeness is also deprioritized by existing metrics. For IC, fluency exhibits low correlations for most metrics, suggesting a tendency to assign high scores even to texts lacking natural flow. A qualitative example illustrating this tendency is provided in Section 4.3.\n4.3 Analysis\nQualitative examples. We present a qualitative comparison between HarmonicEval and FLEUR. To generate textual explanations for HarmonicEval, we prompt the VLM with \u201cWhy? Tell me the reason.\", following Lee et al. (2024b).\""}, {"title": "5 Related Work", "content": "Automatic evaluation metrics. Traditional metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) were originally proposed for automatic evaluation of machine translation and"}, {"title": "Limitations", "content": "This section discusses limitations from five perspectives: theory, modality, criteria, dataset, and model.\nTheory. The proposed harmonic weighting provides a statistically reliable way to aggregate individual scores, as discussed in Appendix A, with justification for using 0.5 < \u03b3 < 1.0 based on clarified assumptions underlying inverse variance weighting and uniform weighting. However, there is no guarantee that \u03b3 = 0.75 is the optimal setting for any tasks and VLMs. Further, since score distributions are approximated by the output token probabilities, there is a possibility of unintended bias. Notably, evaluation bias in LLM-based evaluation metrics has been documented in several studies (Zheng et al., 2023; Liu et al., 2023b; Ohi et al., 2024). As such, further research into evaluation bias in VLM-based evaluation metrics is essential for future work.\nModality. This study focused primarily on evaluating text quality in vision-language tasks because most state-of-the-art VLMs output only text. This leaves image quality evaluation underexplored. Given that several recent image generation models, such as DALL-E 3, are integrated into conversational systems using VLMs, the automatic evaluation of both generated image and text quality would be a promising next step toward the development of more user-friendly systems.\nCriteria. We carefully selected five general criteria that are considered effective across various vision-language tasks as detailed in Appendix B. These criteria were useful for discussions spanning the four vision-language tasks in this study. To further expand research to include a greater number of criteria and tasks, analyzing the relationship between task- or domain-specific criteria and these general criteria would also be necessary in future work.\nDataset. As the number of criteria increased, it became difficult even for experts to maintain annotation consistency, leading to greater time requirements for data collection. MMHE extracted one hundred images from each task, which was considered to be a statistically reliable number, and each target text was evaluated by three annotators. While large-scale crowdsourcing was attempted to scale up the dataset, obtaining human judgments that adhered accurately to each rating scale was challenging because careful explanation of the rating process through direct communication was required. This leaves scaling up the number of tasks and images challenging.\nModel. Improving VLMs to generate better text based on the evaluation results remains future work. In particular, achieving high scores across all criteria within learning frameworks such as in-context learning or reinforcement learning would be an intriguing direction for further exploration."}, {"title": "Ethics Statement", "content": "Data collection. We created and publicly release a new dataset as part of this research. The data collection process was conducted with careful consideration for ethical guidelines. All annotators were informed about the purpose of this dataset and provided consent before participation. Any personally identifiable information has been removed to ensure privacy protection. The dataset was reviewed to minimize harmful content, offensive language, or biases that could negatively impact downstream applications. However, some inherent biases might still be present due to the nature of the data sources including natural images.\nReproducibility. All code necessary to reproduce the experimental results is provided in the supplementary material and will be made publicly available. All experiments have been conducted as deterministically as possible by fixing random seeds and setting the temperature hyperparameters to zero."}, {"title": "Appendix", "content": "A Mathematical background\nThis section provides a detailed mathematical background of harmonic weighting.\nA.1 Notations and setting\nLet C be a finite discrete set and $s_c (c \\in C)$ be independent estimators for the overall score s. Harmonic weighting assumes that each $c \\in C$ has an intrinsic variance $\\varsigma^2$ and the empirical distributions of individual scores $\\hat{s_c} = s_c + \\varsigma(s - \\hat{s_c})$, $\\hat{s_c}$ ~ $p_\\varsigma$ are observable, where $p_\\varsigma : R \\rightarrow R^+$ is a probability density function that can be approximated by the output token probability distributions. Then, the following combined estimator \u015d is used to predict the overall score:\n$\\hat{s} = \\sum_{c \\in C} w_c \\hat{s_c},$\nwhere 0 < $w_c$ \u2264 1 are weighting coefficients, satisfying $\\sum_{c \\in C} w_c = 1$.\nA.2 Best linear unbiased estimator\nHarmonic weighting chooses the weight coefficients such that the combined estimator is the best linear unbiased estimator when all estimators $\\hat{s_c}$ are unbiased, and the variance V[$s_c$] = $r^2$ is finite. Since the best linear unbiased estimator minimizes the variance V[${\\hat{s}}$], the weighting coefficients are obtained by solving the following minimization problem:\nmin V[${\\hat{s}}$] subject to $\\sum_{c \\in C} w_c = 1$.\nHere, since we have\nV[${\\hat{s}}$] =  $\\sum_{c \\in C} w_c^2 \\sigma_c^2,$\nthe solution is given by\n$w_c =  \\frac{\\frac{A_c}{\\sigma_c^2}}{\\sum_{c \\in C} \\frac{A_c}{\\sigma_c^2}} =  \\frac{A_c}{\\sigma_c^2},$  ($\\forall c \\in C$).\nA.3 Inverse variance weighting\nInverse variance weighting is a statistical technique used to combine multiple independent estimates, which assigns larger weights to reliable estimates having smaller variances. We have Proposition 1 under Assumption 1.\nAssumption 1. The observed variance is entirely due to statistical fluctuations, i.e., $\\varsigma_c = 1$.\nProposition 1. Harmonic weighting reduces to inverse variance weighting under Assumption 1. The proof is trivial from Eq. (8). In practice, the distribution p is approximated by the output token probability distribution $p_c$, thereby $\\sigma_c$ is approximated as $\\sigma_c \\sim \\hat{\\sigma_c}$ where $\\hat{\\sigma_c}$ is an observation given by\n$\\hat{\\sigma_c} = \\sqrt{\\sum_{r=1}^{5} (r - \\hat{s_c})^2 p_c(r)},$\n$\\hat{s_c} = \\sum_{r=1}^{5} r p_c(r).$\nA.4 Uniform weighting\nFor uniform weighting, we have Proposition 2 under Assumption 2.\nAssumption 2. The observed variance is entirely intrinsic to each $c \\in C$ i.e., $\\varsigma_c \\propto \\sigma_c$.\nProposition 2. Harmonic weighting reduces to uniform weighting under Assumption 2.\nProof. Since a constant \u03ba > 0 exists such that $\\varsigma_c = \u03ba\u03c3_c$, we have\n$\\frac{A_c}{\\sigma_c^2} =  \\frac{(\\kappa \\sigma_c)^2}{\\sigma_c^2} = \\kappa^2$.\nTherefore, we have uniform weighting coefficients:\n$w_c =  \\frac{\\frac{A_c}{\\sigma_c^2}}{\\sum_{c \\in C} \\frac{A_c}{\\sigma_c^2}} =  \\frac{\\frac{\\kappa^2}{1}}{\\sum_{c \\in C} \\frac{\\kappa^2}{1}} =  \\frac{1}{|C|}$.\nA.5 Harmonic weighting\nConsidering that both Assumptions 1 and 2 make strong assumptions, it is reasonable to consider an intermediate case where part of the observed variance is due to statistical fluctuations and part is intrinsic to each c\u2208 C. If this hypothesis is correct, it is optimal to estimate $s_c$ using a hyperparameter \u03b3 such that it represents a balance between inverse variance weighting and uniform weighting. Harmonic weighting formulates this with linear weighting in a log scale as\n$log \\sigma_h^2 = \\gamma  \\log \\sigma^{(1)2} + (1 - \\gamma) \\log \\sigma^{(2)2}$\nwhere $\\sigma_c^{(1)} = 1$ is from Assumption 1 and $\\sigma_c^{(2)} = \\sigma_c$ is from Assumption 2. In summary, harmonic"}, {"title": "C Implementation details", "content": "This section provides details of full prompts for HarmonicEval and vision-language models used in the present study.\nC.1 Full prompts\nCorrectness (Crt). The correctness is the degree to which the target text accurately reflects the content of the input image and text. The following is the full prompt designed for the correctness criterion.\nYour task is to rate the candidate caption for the given image on a scale of 1 to 5 on the following criterion and rating scale.\nEvaluation Criterion:\nCorrectness: How accurately does the caption describe the image?\nRating Scale:\n1 Very Low Correctness: The caption is mostly or entirely incorrect; it fails to accurately describe the image or misses key elements.\n2 Low Correctness: The caption includes some correct information but misses major aspects or includes incorrect details.\n3 Moderate Correctness: The caption is generally accurate, capturing a basic understanding of the image but may omit some details or include minor inaccuracies.\n4 High Correctness: The caption is mostly accurate, including key elements and details of the image with minimal inaccuracies.\n5 Extremely High Correctness: The caption perfectly captures all relevant and discernible details of the image with complete accuracy.\nCompleteness (Cmp). The completeness is the extent to which the target text captures all relevant and significant details of the input image and text. The following is the full prompt designed for the completeness criterion.\nYour task is to rate the candidate caption for the given image on a scale of 1 to 5. Output the evaluation score first\nClarity (Clr). The clarity is the ease with which the reader can understand the target text. The following is the full prompt designed for the comprehensiveness criterion.\nYour task is to rate the text on a scale of 1 to 5. Output the evaluation score first based on the following criteria and rating scale.\nEvaluation Criteria:\nClarity: How clear is the text?\nRating Scale:\n1 Ambiguous: The text is very unclear and can be interpreted in multiple ways, leading to significant confusion about its intended meaning.\n2 Somewhat ambiguous: The text includes phrases or structures that make it non-definitive, such as using 'or' or\nFluency (Flu). The fluency is the grammatical accuracy and natural flow of the target text. The following is the full prompt designed for the fluency criterion.\nYour task is to rate the text on a scale of 1 to 5. Output the evaluation score first based on the following criteria and rating scale.\nEvaluation Criteria:\nFluency: How well the text is written in terms of grammar, punctuation, and phrasing.\nRating Scale:\n1 Disfluent: The text contains numerous errors, making it difficult to understand.\n2 Somewhat disfluent: The text has several noticeable errors that make it sound unnatural.\n3 Moderately fluent: The text is generally understandable but contains errors that cause some discomfort while reading.\n4 Fluent: The text flows well and is easy to understand with only minor imperfections.\n5 Very fluent: The text is perfectly constructed with no grammatical errors or awkward phrasing.\nConciseness (Cnc). The conciseness is the efficiency of the target text in conveying information without unnecessary verbosity. The following is the full prompt designed for the conciseness criterion.\nYour task is to rate the text on a scale of 1 to 5. Output the evaluation score"}, {"title": "Visual Question Answering (VQA)", "content": "Visual question answering (VQA). To generate answers for the VQA task, the following prompt is used:\n[QUESTION] Please give an answer and explain it in two or three sentences.\nwhere [QUESTION] is a placeholder for input questions.\nVisual document understanding (VDU). Because the VDU task is performed in a question-answering format, the same prompt as for VQA is used:\n[QUESTION] Please give an answer and explain it in two or three sentences.\nwhere [QUESTION] is a placeholder for input questions.\nImage captioning (IC). For caption generation, the following prompt is used:\nPlease provide a one-sentence caption for the provided image.\nC.3 Vision language models\nThe target texts are generated by the following VLMs.\n LLaVA-1.5-7B (Liu et al., 2023a) is an open-source VLM based on the transformer architecture obtained by fine-tuning Vicuna on instruction data generated by GPT. The version is 1.5 and the number of parameters is 7 billion.\n LLaVA-1.5-13B (Liu et al., 2023a) is an extended version of LLaVA-1.5, featuring 13 billion parameters.\n InstructBLIP-Vicuna-7B (Dai et al., 2023) is an VLM that combines the BLIP-2 model with the Vicuna-7B language model by an instruction-aware Q-former module. This model is trained on a collection covering 11 tasks and 26 publicly available datasets.\n InstructBLIP-Vicuna-13B (Dai et al., 2023) is an extended version of InstructBLIP-Vicuna using Vicuna-13B.\n Qwen-VL (Bai et al., 2023) is an VLM that incorporates the vision transformer into the Qwen language model through cross-attention. This model is trained on a large-scale web-crawled dataset, including English and Chinese language subsets of LAION-5B.\n Qwen2-VL-Instruct-7B/72B (Wang et al., 2024a) is the instruction-tuned version of the Qwen2-VL model.\n CogVLM-Chat (Wang et al., 2024b) is a conversational VLM fine-tuned for dialogue-based tasks. This model is based on CogVLM-17B trained on publicly available datasets including LAION-2B and COYO-700M. Fine-tuning is performed on VQA and instruction-tuning datasets.\n GPT-4o (OpenAI, 2024) is a VLM provided by OpenAI. Specifically, we used the model version gpt-4o-2024-08-06.\n GPT-4o-mini is a lightweight variant of GPT-4o, optimized for efficiency and speed.\nC.4 HarmonicEval\nWe use the GPT-4o model as the backbone of HarmonicEval. Specifically, we utilize gpt-4o-2024-08-06 as the model version in the API calls. For the criteria of Clarity, Fluency, and Conciseness, we input only the target text into the model, as these criteria are designed to assess the quality of the target text independently, without considering the source or the image. All experiments are conducted in a single attempt."}, {"title": "MMHE", "content": "This section provides details of MMHE.\nD.1 Source data\nWe select input texts and images for tasks from existing datasets. English is the language used for all datasets. Referring expression genera-tion (REG). We adopt RefCOCO (Apache-2.0 license) (Kazemzadeh et al., 2014) as a source dataset. This dataset contains 142,209 referring expressions for 50,000 objects in 19,994 images. Our usage of the data complies with the terms of its license.\nVisual question answering (VQA). We adopt OK-VQA (Apache-2.0 license) (Marino et al., 2019) as a source dataset. This dataset contains 14,055 open-ended questions. Our usage of the data complies with the terms of its license.\nVisual document understanding (VDU). We adopt VisualMRC (Tanaka et al., 2021) as a source dataset. This dataset contains over 30,000 question-answer pairs associated with more than 10,000 document images. Our usage of the data complies with the terms defined by its license.\nImage Captioning (IC). We adopt MSCOCO (CC BY 4.0) (Lin et al., 2014) as a source dataset. Specifically, we utilized the 2017 Val split, which contains approximately 5,000 images and captions. Our usage of the data complies with the terms of its license.\nD.2 Human Annotation\nFive annotators who are English native speakers and experts in proofreading were hired for this process. The instructions provided to the annotators were almost identical to the prompts given to the VLM in HarmonicEval. On average, the annotators were paid $20 per hour. Annotators were instructed on how the annotated data would be utilized.\nD.3 Overall judgment\nWhen calculating the overall judgment accuracy in Table 2, 4, and 6, we extract and utilize pairs of judgments from best-of-three triplets."}, {"title": "Baselines", "content": "Below are details of the eight baselines implemented on the MMHE dataset.\n Bilingual evaluation understudy (BLEU) (Papineni et al., 2002) is a metric for machine translation evaluation that measures the ngram overlap between candidate and reference texts.\n Recall-oriented understudy for gisting evaluation (ROUGE) (Lin, 2004) is a metric for text summarization and machine translation evaluation that measures the overlap of n-grams, skip-bigrams, and weighted longest common subsequences between candidate and reference texts.\n Metric for evaluation of translation with explicit ordering (METEOR) (Banerjee and Lavie, 2005) is a metric for machine translation evaluation that assesses translations by explicit word-to-word matches between candidates and references.\n Consensus-based image description evaluation (CIDEr) (Vedantam et al., 2015) is a metric for image captioning that measures the consensus between candidate and reference captions by weighting n-grams based on their"}, {"title": "Computational Budget", "content": "We run all the experiments on ABCI (https://abci.ai/), Compute Node(A), whose CPUs are two Intel Xeon Platinum 8360Y, and GPUs are eight NVIDIA A100 SXM4. The approximate total processing time is 40 hours."}]}