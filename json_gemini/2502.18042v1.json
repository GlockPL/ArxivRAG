{"title": "VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion", "authors": ["Pei Liu", "Haipeng Liu", "Haichao Liu", "Xin Liu", "Jinxin Ni", "Jun Ma"], "abstract": "Abstract-Human drivers adeptly navigate complex scenarios by utilizing rich attentional semantics, but the current autonomous systems struggle to replicate this ability, as they often lose critical semantic information when converting 2D observations into 3D space. In this sense, it hinders their effective deployment in dynamic and complex environments. Leveraging the superior scene understanding and reasoning abilities of Vision-Language Models (VLMs), we propose VLME2E, a novel framework that uses the VLMs to enhance training by providing attentional cues. Our method integrates textual representations into Bird's-Eye-View (BEV) features for semantic supervision, which enables the model to learn richer feature representations that explicitly capture the driver's attentional semantics. By focusing on attentional semantics, VLM-E2E better aligns with human-like driving behavior, which is critical for navigating dynamic and complex environments. Furthermore, we introduce a BEV-Text learnable weighted fusion strategy to address the issue of modality importance imbalance in fusing multimodal information. This approach dynamically balances the contributions of BEV and text features, ensuring that the complementary information from visual and textual modality is effectively utilized. By explicitly addressing the imbalance in multimodal fusion, our method facilitates a more holistic and robust representation of driving environments. We evaluate VLM-E2E on the nuScenes dataset and demonstrate its superiority over state-of-the-art approaches, showcasing significant improvements in performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous driving has witnessed remarkable progress in recent years [1]\u2013[3], with significant advancements in key areas such as perception [4]\u2013[6], motion prediction [7]\u2013[9], and planning [10], [11]. These developments have laid a solid foundation for achieving more accurate and safer driving decisions. Among these, end-to-end (E2E) autonomous driving has emerged as a transformative paradigm, leveraging large-scale data to demonstrate impressive planning capabilities. By directly mapping raw sensor inputs to driving actions, E2E approaches bypass the need for handcrafted intermediate modules, enabling more flexible and scalable solutions. However, Despite these advancements, traditional end-to-end autonomous driving approaches predominantly predict future trajectories or control signals directly, without explicitly considering the driver's attention to critical information such as traffic dynamics and navigation cues. E2E systems often struggle in complex and ambiguous scenarios due to their limited ability to reason about high-level semantics and contextual cues, such as traffic rules, driver attentions, and dynamic interactions. In contrast, human drivers rely on an attentional decision-making process, where attention to both the surrounding traffic environment and navigation guidance plays a critical role [12]\u2013[14]. For instance, when approaching an intersection, human drivers naturally prioritize traffic signals, pedestrian movements, and lane markings, dynamically adjusting their focus based on the evolving scene.\nThis limitation has spurred the integration of Vision-Language Models (VLMs) [15]\u2013[18] into autonomous driving frameworks. Trained on vast multimodal datasets, VLMs excel at tasks requiring high-level semantic reasoning, such as interpreting complex scenes, predicting dynamic interactions, and generating contextual descriptions. Their ability to leverage commonsense knowledge makes them particularly well-suited for addressing challenges in autonomous driving, such as understanding traffic rules, identifying vulnerable road users, and making safe decisions in ambiguous scenarios. By generating text-based descriptions of critical driving cues, VLMs can explicitly capture and prioritize regions of interest that align with human driver attention. This capability enables more humanlike decision-making, particularly in safety-critical scenarios where attentional focus is paramount.\nMotivated by these challenges, we propose VLM-E2E (illustrated in Fig. 1), a novel framework designed to enhance autonomous driving systems by incorporating a deeper understanding of driver attentional semantics. Our approach addresses three key questions:\nHow to integrate VLMs with E2E models? While most existing methods integrate VLMs with decision-making modules [19] or other high-level components [20]\u2013[22], leveraging their semantic understanding capabilities to enhance decision processes, our approach introduces a novel integration strategy. Instead of limiting VLMs to decision modules, we combine them directly with the BEV module, which is widely used to represent and process spatial information from multiple perspectives in autonomous driving. By integrating VLMs into the BEV module, we enable BEV representations to incorporate both visual and textual features, resulting in richer and more semantic-aware spatial understanding. This integration allows the model to not only perceive geometric structures but also reason about high-level driver attentional semantics.\nHow to fuse vision and text representations? Existing methods for fusing vision and text representations predominantly rely on attention-based mechanisms [23], [24], such as cross-attention or co-attention modules, to align and enhance interactions between modalities. While effective, these approaches often rely on predefined attention mechanisms that lack flexibility in adapting to varying task requirements and time and memory consuming. To address this issue, we propose a BEV-Text learnable weighted fusion strategy, where the importance of each modality is dynamically determined through a learnable weighting mechanism. This approach allows the model to adaptively emphasize visual or textual features based on their relevance to the task, leading to a more robust and context-aware multimodal representation. For instance, in scenarios requiring precise localization such as lane keeping, the model can prioritize BEV features, while in scenarios requiring high-level reasoning such as red lights, it can emphasize text features.\nHow to represent driver attentional environment? To effectively model driver attentional environment, we propose a multimodal framework that leverages vision-language representations. First, we utilize front-view images captured from the driving scene as input to BLIP-2 [25] to generate initial textual descriptions of the environment. These descriptions provide a semantic understanding of key objects and events within the driver's vision scope. To address the hallucination problem of VLMs, we further refine these textual representations using ground truth annotations and high-level maneuvering intentions. This refinement ensures that the generated text is not only accurate but also contextually aligned with the driving task. Finally, the refined text is encoded into a dense representation using a pre-trained CLIP [26] model, which aligns the textual information with visual features in a shared embedding space. This textual representation enables the model to capture driver attentional cues, such as focusing on pedestrians near crosswalks or traffic signals at intersections, leading to more human-like decision-making and better safety performance.\nWe evaluate VLM-E2E on the widely used nuScenes dataset, a comprehensive benchmark for autonomous driving research. Our experimental results demonstrate significant improvements over the baseline methods, highlighting the effectiveness of our approach in enhancing perception, decision-making, and overall driving performance. The key contributions of this work can be summarized as follows:\n\u2022 We propose VLM-E2E, a novel framework that leverages VLMs to enrich the training process with attentional understanding. By integrating semantic and contextual information, VLM-E2E explicitly captures driver attentional semantics, which enables more human-like decision-making in complex driving scenarios.\n\u2022 We introduce a BEV-Text learnable weighted fusion strategy that dynamically balances the contributions of BEV and textual modalities. This adaptive fusion mechanism is computationally efficient, which requires minimal additional overhead while significantly enhancing the model's adaptability and robustness.\n\u2022 To address the hallucination problem of VLMs, we incorporate semantic refinement of text descriptions generated from front-view images. By leveraging ground truth (GT) labels and high-level maneuvering intentions, we ensure that the textual representations are both accurate and highly relevant to the driving task, enhancing the model's ability to reason about critical driving cues.\n\u2022 Extensive experiments on the nuScenes dataset demonstrate the superiority of VLM-E2E over existing methods. Our framework achieves significant improvements in handling complex driving scenarios, showcasing its ability to integrate geometric precision with high-level semantic reasoning for safer and more interpretable autonomous driving."}, {"title": "II. RELATED WORK", "content": "A. BEV Representation from Multi-view Cameras\nBEV representation has emerged as a natural and ideal choice for planning and control tasks in autonomous driving [19], [27]\u2013[29]. Unlike perspective views, BEV avoids issues such as occlusion and scale distortion while preserving the 3D spatial layout of the scene, making it highly suitable for tasks like path planning, object detection, and motion prediction. While LiDAR and HD maps can be easily represented in BEV, projecting vision inputs from camera views into BEV space remains a challenging problem due to the inherent complexity of perspective-to-BEV transformation [30].\nEarly approaches to BEV generation relied on geometric methods [31], [32], such as IPM [33], which assumes a flat ground plane to project 2D image pixels into BEV space. However, these methods struggle in complex environments where the ground plane assumption does not hold, such as uneven terrains or dynamic scenes. To address these limitations, learning-based methods have gained prominence. For instance, some works [34], [35] implicitly project image inputs into BEV using neural networks. However, the quality of these projections is often limited due to the lack of ground truth BEV data for supervision. Loukkal et al. [36] proposed an explicit projection method using homography between the image and BEV plane, but this approach is sensitive to calibration errors and environmental variations.\nRecent advancements have introduced more sophisticated techniques for BEV generation. Methods like [5], [37] acquire BEV features through spatial cross-attention with pre-defined BEV queries, enabling end-to-end learning of perspective-toBEV transformations. Notably, [4] and [38] have demonstrated impressive performance by leveraging estimated depth and camera intrinsics to perform the projection. These methods explicitly model the 3D geometry of the scene, resulting in more accurate and robust BEV representations.\nB. Multi-modal Information Fusion Mechanism\nIn recent years, attention-based fusion mechanisms and learnable fusion strategies have emerged as dominant paradigms for multi-modal information fusion, addressing the challenges of modality heterogeneity and imbalance. These approaches have demonstrated remarkable success in capturing cross-modal interactions and dynamically adapting to the relevance of each modality, making them particularly suitable for complex tasks such as autonomous driving and robotics.\nAttention-based fusion mechanisms leverage the power of attention to model dependencies between modalities, enabling the model to focus on the most informative features. Transformer-based architectures [23], [24] have become a cornerstone of this approach, utilizing self-attention and crossattention mechanisms to fuse features from different modalities. For instance, TransFuser [39] employs transformers to integrate visual and LiDAR features, achieving state-of-the-art performance in 3D object detection and scene understanding. Similarly, cross-modal attention networks [40] use attention to weigh the importance of visual and textual features, enhancing tasks such as image-text matching and visual question answering. These methods excel at capturing long-range dependencies and complex interactions between modalities. However, they often require significant computational resources, limiting their applicability in real-time systems.\nOn the other hand, learnable fusion mechanisms have gained traction for their ability to dynamically adjust the contribution of each modality based on task-specific requirements. These methods introduce learnable parameters, such as weights or coefficients, to adaptively fuse features during training. For example, Modality-Aware Fusion [41] proposes learnable coefficients to balance the importance of visual and LiDAR features, improving robustness in autonomous driving tasks. Another notable approach is Dynamic Fusion Networks [42], which use gating mechanisms to selectively combine modalities based on their relevance to the current context. These strategies are particularly effective in handling modality imbalance, where one modality may dominate due to its inherent information richness or task-specific importance. By dynamically adjusting the fusion process, learnable mechanisms ensure that all modalities contribute meaningfully to the final output, enhancing both performance and interpretability.\nC. End-to-end Autonomous Driving\nEnd-to-end autonomous driving systems have demonstrated significant improvements in overall performance by jointly training all modules under a unified objective, thereby minimizing information loss across the pipeline. In recent years, unified frameworks such as ST-P3 [43] and UniAD [2] have pioneered vision-based E2E systems that seamlessly integrate perception, prediction, and planning modules, achieving stateof-the-art results in complex driving scenarios. Building on these advancements, subsequent research such as VAD [1] and VADv2 [44] introduced vectorized encoding methods to enhance the efficiency and scalability of scene representation, enabling more robust handling of dynamic environments.\nMore recently, methods such as Ego-MLP [45], BEVPlanner [46], and PARA-Drive [47] have explored novel design spaces within modular stacks, focusing on self-state modeling and innovative architectural designs to further enhance driving performance. These approaches have pushed the boundaries of E2E systems by incorporating richer representations of the ego vehicle's state and its interactions with the environment.\nIn this work, we build upon ST-P3 by integrating driver attentional text information into the framework. By leveraging natural language descriptions of critical driving cues such as pedestrian crossing ahead or red traffic light, we enable the model to explicitly capture and prioritize regions of interest that align with human driver attention. This enhancement not only improves the interpretability of the system but also ensures that the model's decisions are more closely aligned with human-like reasoning, particularly in safety-critical scenarios.\nD. Vision Language Models in Autonomous Driving\nThe integration of VLMs into autonomous driving systems has garnered significant attention due to their inherent capabilities in common sense knowledge, advanced reasoning, and interpretability. These attributes effectively address the limitations of traditional E2E models, making VLMs a promising avenue for enhancing driving systems. Recent research has explored various methodologies to harness VLMs for driving tasks, demonstrating substantial progress in this domain. For instance, Drive-with-LLMs [48] employs a Transformer network to encode ground-truth perception data into a latent space, which is subsequently processed by a Large Language Model (LLM) to predict future trajectories. Similarly, DriveGPT4 [49] leverages VLMs to interpret frontcamera video inputs, generating planning control signals and providing natural language explanations for decision-making processes. Further advancements include DriveMLM [50], which validates the efficacy of VLM-based planning in closedloop simulation environments [51], and ELM [52], which introduces large-scale pre-training of VLMs using cross-domain video data. These studies collectively underscore that the incorporation of diverse data sources and task-specific training significantly enhances VLM performance in driving-related tasks.\nMoreover, several works have proposed specialized data collection strategies and datasets tailored for autonomous driving [53]\u2013[56], further accelerating the development and application of VLMs in this field. A notable contribution is DriveVLM [57], the first framework to seamlessly integrate VLMs with E2E models. In this approach, VLMs predict lowfrequency trajectories, which are subsequently refined by the E2E model to generate the final planning trajectory. Additionally, Senna [20] generates high-level planning decisions in"}, {"title": "III. METHODOLOGY", "content": "In this section, we provide a detailed introduction to VLME2E, as illustrated in Fig. 2. The input scene information includes multi-view image sequences, GT, maneuvering, and user prompts. The front-view image, maneuvering, and user prompts are fed into the VLM-based Text Annotation Generation module to generate descriptive text annotations, while the multi-view images are processed by the visual encoding layer to produce BEV features. These text annotations are then passed to the Text Interaction Guidance Module, where they are encoded into text features using a pre-trained CLIP model. Subsequently, the BEV and text features are fused to support downstream tasks such as perception, prediction, and decision-making. In Section III-A, we introduce the design of VLM-based Text Annotation Generation in detail. Sections III-C and III-B focus on the design of the Text Interaction Guidance Module and Vision-based End-to-End Architecture, respectively.\nA. VLM-based Text Annotation Generation\n1) Text Annotation: Fig. 2 depicts the proposed pipeline for extracting driver attentional information from visual inputs, leveraging the reasoning capabilities of a pre-trained VLM. The semantic annotation extraction process can be formulated as follows:\n$T = BLIP2(P, I_{front})$\nwhere $BLIP2(\u00b7)$ denotes the visual language model BLIP-2, $P$ represents the task-specific prompts, $I_{front}$ is the visual input from the ego vehicle's front camera, and $T$ is the generated textual description providing detailed environment-related information. The goal of this process is to utilize task-specific prompts alongside real-time visual inputs to extract actionable and attentional information from BLIP-2. This approach not only emphasizes critical elements such as pedestrians, traffic signals, and dynamic obstacles but also filters out irrelevant scene details, ensuring that the outputs directly support driving decisions.\nIn our work, we employ a state-of-the-art vision language model BLIP-2 [25], capable of performing complex reasoning over visual contexts, to generate precise and contextually relevant descriptions. The model interprets visual scenarios guided by prompts and outputs textual descriptions. This method enhances the dataset's richness by providing driver attentional annotations, thereby improving the understanding and decision-making capabilities of downstream driving models.\nWe encountered a challenge in determining the visual input. That is, selecting the right images from multiple cameras that can cover 360 degrees of the ego vehicle. Considering that we want to capture the driver's attentional semantics when driving, the front view images usually contain the most relevant information required for most driving tasks. All-view images contain more distracting information that affects the system's decision-making, so we choose to use only the frontview images to extract the attentional information. In addition, considering that the ego vehicle and its surroundings are in dynamic motion and the hallucination problem inherent in large models, we use the GT and maneuvering to refine the annotations of dynamic objects.\nB. Text Interaction Guidance Module\nThe driver's attentional text descriptions preserve rich visual semantic cues. It is complementary to the BEV features that mainly represent the 3D geometric information. Hence, BEVText fusion is proposed for comprehensive scene understanding from the BEV perspective.\n1) Text Encoder: Given a text input $T$ that provides semantic features to guide the BEV-Text fusion network toward achieving a specified fusion result, the text encoder and embedding within the text interaction guidance architecture are responsible for transforming this input into a text embedding. Among various VLMs, we adopt CLIP [26] due to its lightweight architecture and efficient text feature extraction capabilities. Compared to other VLMs, CLIP is computationally less demanding and produces text embeddings with a relatively small feature dimension of 77, which significantly enhances the efficiency of subsequent BEV-Text feature fusion. We freeze the text encoder from CLIP to preserve its consistency and leverage its pretrained knowledge. This process can be formally expressed as:\n$f_t = CLIP(T)$\nwhere $CLIP \u2208 R^{N\u00d7L}$ denotes the CLIP model with wights frozen. $f_t$ is the text semantic representations.\nIn different but semantically similar texts, the extracted features should be close in the reduced Euclidean space. Furthermore, we use the MLP F to mine this connection and further map the text semantic information and the semantic parameters. Therefore, it can be obtained:\n$v_m = F_m(f_t), \\\\ \u03b2_m = F_m(f_t)$\nwhere $Fl$ and $F2$ are the chunk operations to form the text semantic parameters.\n2) BEV-Text Fusion: In the semantic interaction guidance module, semantic parameters interact through feature modulation and fusion features $s_t$, to obtain the effect of guidance. The feature modulation consists of scale scaling and bias control, which adjust the features from two perspectives, respectively. In particular, a residual connection is used to reduce the difficulty of network fitting, inspired by [58]. For simplicity, it can be described as:\n$x_t = (1 + \u03b1_m) \\cdot s_t + \u03b2_m$\nwhere $\u00b7$ denotes the Hadamard product. $x_t$ denotes the BEV feature of BEV-Text fusion, and $s_t$ denotes the BEV feature defined in Section III-C1.\nC. Vision-based End-to-end Model\n1) Spatial Temporal BEV Perception: In our framework, the BEV representation is constructed from multi-camera images. The input multi-camera images {$I_1$,\u2026\u2026,$I_n$}, n = 6 at time t are first passed through a shared backbone network, EfficientNet-b4 [59], to extract high-dimensional feature maps. For each camera image k at time t, we get its encoder features $e^t \u2208 R^{C\u00d7H_e\u00d7W_e}$ and depth estimation $d \u2208 R^{D\u00d7H_e\u00d7W_e}$ with C denotes the number of feature channels, D is the number of discrete depth values and $(H_e, W_e)$ depicts the spatial feature size. Implicit depth estimation is applied to infer the depth information for each pixel, enabling the construction of a 3D feature volume. Since the depth values are estimated, we take the outer product of the features with the depth estimation.\n$e^t_d = e^t \u2297 d$\nwhere $e^t_d \u2208 R^{C\u00d7D\u00d7H_e\u00d7W_e}$. Then, to transform the 2D perspective features into a 3D space, we employ a feature lifting module. This module uses camera intrinsic and extrinsic parameters to project the 2D features into a 3D voxel space. The 3D feature volume is then collapsed into a 2D BEV representation by aggregating features along the vertical axis to form the BEV view features $b_t \u2208 R^{C\u00d7H\u00d7W}$, with (H, W) denotes the spatial size of BEV feature. This is achieved through attention-based aggregation, which preserves the most salient features while maintaining spatial consistency. The resulting BEV map provides a top-down view of the scene, encapsulating both geometric and semantic information.\nIn addition to the BEV construction pipeline described above, we further incorporate temporal modeling to enhance the dynamic understanding of the scene. Specifically, given the current timestamp t and its h historical BEV features {$b_{t-h}$,..., $b_{t-1}$,$b_t$}, we first align the historical features to the current frame's coordinate system using a temporal alignment module. This process leverages the relative transformation and rotation matrix $M_{t-it} \u2208 R^{4\u00d74}$ between adjacent frames. The past BEV feature $b_{t-i}$ is then spatially transformed as:\n$b_{t-i} = W(b_{t-i}, M_{t-it}), i = 1,2$\nwhere $W(\u00b7)$ denotes the pose-based BEV feature warping operation, and $\u00d4_{t\u2212i}$ represents the aligned historical features. Subsequently, the aligned BEV features from the h frames are concatenated to form the spatiotemporal input $b = [b_{t-h}, ..., b_{t-1}, \u00d4_t] \u2208 R^{h\u00d7C\u00d7H\u00d7W}$. To capture longterm dependencies in dynamic scenes, we use a spatiotemporal transformer module $F_s$.\n$s_t = F_s(b_{t-h}, \u2026\u2026,b_{t-1}, b_t)$\nwhere $s_t \u2208 R^{h\u00d7C\u00d7H\u00d7W}$ is the spatiotemporally fused BEV feature. $F_s$ is a spatiotemporal convolutional unit with crossframe self-attention. Our spatial-temporal BEV representation explicitly models the static and dynamic evolution of the scene, enabling the BEV representation to encode geometric structures and temporal continuity simultaneously.\n2) Semantic Occupancy Prediction: The future prediction model is a convolutional gated recurrent unit network taking as input the current state $s_t$ and the latent variable $n_t$ sampled from the future distribution during training, or the present distribution $P$ for inference. It recursively predicts future states {$Y_{t+1}$,\u2026, $Y_{t+1}$} with l denotes the prediction horizon.\nTo model the inherent uncertainty in multi-modal future trajectories, we employ a conditional variational framework inspired by [60]. Present distribution P(z|xt) is conditioned solely on the current state xt. Future distribution $P_f(z|x_t, Y_{t+1:t+l})$ is augmented with ground-truth future observations $(Y_{t+1},\u00b7\u00b7\u00b7, Y_{t+l})$. This distribution is parameterized as diagonal Gaussian with learnable mean $\u00b5\u2208 R^M$ and variance $\u03c3^2 \u2208 R^M$, where M is the latent dimension.\n$P(z|xt) = N(\u00b5_{pres}, \u03c3^2_{pres})$\n$P_f(z|xt, Y_{t+1:t+l}) = N(\u00b5_{fut}, \u03c3^2_{fut})$\nIn the training phase, to ensure prediction consistency with observed futures while preserving multi-modal diversity, we sample $n_t$ from $P_f(z|x_t, Y_{t+1:t+l})$ and then optimize a modecovering KL divergence loss.\n$L_{KL} = D_{KL}(P_f(z|X_t, Y_{t+1:t+F})||P(z|xt))$\nwhich encourages $P(z|x_t)$ to encompass all plausible futures encoded in $P_f$. In the inference phase, future trajectories are generated by sampling from the present distribution $n_t \u223c P(z|x_t)$, where each sample $n_t$ represents a distinct future hypothesis.\nThis probabilistic formulation enables our model to generate diverse yet physically plausible futures while maintaining temporal consistency, crucial for handling ambiguous scenarios like unprotected left turns or pedestrian interactions.\nThe fusion features $x_t$ are processed by a multi-task decoder $D_p$ to generate instance-aware segmentation masks and motion predictions. The decoder outputs four key predictions: semantic segmentation, instance centerness, instance offset, and future instance flow, which collectively enable robust instance detection, segmentation, and tracking. The semantic segmentation head predicts pixel-wise semantic categories through a convolutional classifier. This provides a dense understanding of the scene layout and object categories. For instance segmentation, we adopt a hybrid center-offset formulation [61]. The instance centerness head outputs a heatmap $H_t \u2208 R^{H\u00d7W}$ indicating the likelihood of instance centers. During training, a Gaussian kernel is applied to suppress ambiguous regions and focus on high-confidence centers. The instance offset head predicts a vector field $O_t \u2208 R^{2\u00d7H\u00d7W}$, where each vector points to its corresponding instance center. At inference, instance centers are extracted via non-maximum suppression (NMS) on $H_t$. The future instance flow head predicts a displacement vector field $F_t \u2208 R^{2\u00d7H\u00d7W}$ encoding the motion of dynamic agents over a future horizon l. This flow field is used to propagate instance centers across timesteps, ensuring temporal consistency. Specifically, detected instance centers {$c^i_t$} are flow-warped to t + 1 via $c^{i}_{t+1} = c^i_t + F_t(c^i_t)$. The warped centers {$c^{i}_{t+1}$} are then matched to detected centers $c^{i+1}$ at t+1 using the Hungarian algorithm [62], which solves for optimal assignments based on pairwise IoU. This flowbased matching enables robust cross-frame association even under occlusions or abrupt motion changes.\nD. Attention Guided Future Planning\nThe primary objective of the proposed motion planner is to generate trajectories that ensure safety, comfort, and efficient progress toward the goal. To achieve this, we employ a motion planner that generates a set of kinematically feasible trajectories, each of which is evaluated using a learned scoring function, inspired by [43], [63]\u2013[65].\nOur scoring function incorporates a probabilistic dynamic occupancy field, which is crucial for encoding the safety of the potential maneuvers. This field encourages cautious driving behaviors by penalizing trajectories that enter occupied regions or get too close to these regions, thus maintaining a safe distance from surrounding obstacles. Additionally, we utilize the probabilistic layers from our online map to inform the scoring function. These layers provide important information, ensuring that the self-driving vehicle (SDV) remains within the drivable area, stays close to the center of the lane, and moves in the correct direction. Particularly in regions of uncertainty, where occupancy and road structure are less predictable, the planner takes extra care to drive cautiously. Moreover, the planner ensures that the vehicle progresses toward the goal specified by the input high-level command, whether it is to continue forward, make a turn, or navigate other maneuvers.\nThe planner evaluates all sampled trajectories in parallel. Each trajectory \u03c4 is assessed based on the scoring function f, which considers several input factors, including the map M, occupancy O, and the motion V. The trajectory selection process is formulated as:\n$\u03c4^* = argmin f (\u03c4, M, O, V, w)$\nwhere $\u03c4^*$ denotes the optimal trajectory, $f(\u03c4, M, O, V, w)$ is the learned scoring function, w are the learnable parameters of the model.\nThe scoring function evaluates each trajectory concerning multiple criteria, such as the safety of the maneuver avoiding obstacles, the comfort of the ride such as maintaining smooth motion, and progress toward the goal, as guided by the highlevel command. By combining these factors, the motion planner efficiently selects the trajectory that best satisfies all safety, comfort, and progress criteria, ensuring the SDV navigates complex environments in a manner that is both effective and cautious.\nThe output of the motion planner is a sequence of vehicle states, which defines the desired motion of the SDV within the planning horizon. In each iteration of the planning process, a set of candidate trajectories is generated and evaluated using the cost function described in (11). The output of the motion planner is a sequence of vehicle states, which defines the desired motion of the SDV within the planning horizon. The trajectory with the minimum cost is then selected for execution.\nTo ensure real-time performance, the set of sampled trajectories must remain sufficiently small. However, this set must also represent various possible maneuvers and actions to avoid encroaching obstacles. To strike this balance, we employ a sampling strategy that is aware of the lane structure, ensuring that the sampled trajectories effectively capture a diverse range of driving behaviors while remaining computationally feasible. In particular, we follow the trajectory sampling method proposed in [66], [67], where trajectories are generated by combining longitudinal motion with lateral deviations relative to specific lanes, such as the current SDV lane or adjacent lanes. This approach allows the planner to sample trajectories that adhere to lane-based driving principles while incorporating variations in lateral motion. These variations enable the motion planner to handle a wide array of traffic scenarios.\nTo ensure the planned trajectory adheres to driver attention on traffic regulations and route, we utilize a temporal refinement module that dynamically integrates traffic regulations. Leveraging front-view camera features $e_{front}$ from the encoder, we initialize a GRU-based refinement network to iteratively adjust the initially selected trajectory. The frontview features explicitly encode traffic regulations semantics, enabling the model to halt at red lights or proceed through green signals. The recurrent architecture ensures smooth transitions between trajectory points, mitigating abrupt steering or acceleration changes."}, {"title": "IV. EXPERIMENTAL SETTINGS", "content": "A. Dataset\nWe evaluate our method on the nuScenes dataset [68], a large-scale autonomous driving benchmark comprising 1,000 diverse driving scenes, each spanning 20 seconds with annotations provided at 2 Hz. The dataset features a 360\u00b0 multicamera rig composed of six synchronized cameras (front, front-left, front-right, back, back-left, back-right) with minimal field-of-view overlap. Precise camera intrinsic and extrinsic are provided for each frame to ensure accurate spatial alignment. The BEV occupancy labels {$Y_{t+1}$,\u2026, $Y_{t+1}$} are generated by projecting 3D bounding boxes of dynamic agents onto the BEV plane, creating a spatiotemporal occupancy grid. All labels are transformed into the ego vehicle's reference frame using GT future ego-motion, ensuring temporal consistency across frames.\nB. Metrics\nThe perception performance is assessed using the Intersection over Union (IoU), which quantifies the overlap between predicted and GT object bounding boxes. This metric is commonly used to evaluate the accuracy of object detection and tracking in autonomous driving systems.\n$IoU = \\frac{A\u2229B}{AUB}$\nwhere A represents the predicted segmentation, B represents the ground truth segmentation.\nFor prediction evaluation, we employ three key metrics. Panoptic Quality (PQ) evaluates the overall quality of both semantic segmentation and instance detection, accounting for both the accuracy of object classification and the correctness of instance segmentation. Recognition Quality (RQ) measures the ability of the model to correctly recognize and classify objects within the scene. Segmentation Quality (SQ) focuses on the accuracy of the predicted segmentation masks, comparing them with the GT to evaluate the precision of the object segmentation.\n$PQ = \\frac{\\sum_{(p,g) \\in TP} IOU (p, g)}{\\sqrt{(|TP| + |FP|) \\cdot (|TP| + |FN|)}} = SQ \\cdot RQ$\nwhere TP, FP, and FN refer to true positives, false positives, and false negatives, respectively. p, g are the predicted and GT instances.\nFor evaluating the planning performance, we consider two primary metrics. L2 Distance measures the Euclidean distance between the SDV's planned trajectory and real human-driven trajectories. A lower L2 distance indicates that the model is better able to replicate human-like driving behavior, which is important for ensuring natural and comfortable motion. Collision Rate quantifies the percentage of time steps during a trajectory in which the SDV's predicted path collides with the ground truth bounding box of other agents. It provides an indication of the safety of the planner's decisions.\n$CR = \\frac{1}{T} \\sum_{t=1}^T I(\u03c4^*_t \\cap o_t \u2260 0)$\nwhere $\u03c4^*$ is the planned trajectory and o is the other agents' ground-truth occupancy.\n$L2 = \\frac{1}{T} \\sum_{t=1}^T ||\u03c4^*_t - Thuman_t ||^2$\nwhere is the Euclidean distance between the SDV's planned trajectory at time t and the corresponding human trajectory at the same time.\nC. Implementation Details\nOur model utilizes a temporal context of 1.0 seconds of past information to predict the future trajectory over a 2.0-second horizon. In the nuScenes dataset, this corresponds to 3 frames of past context and 4 frames into the future, operating at a frequency of 2 Hz.\nAt each past timestep, the model processes 6 camera images, each with a resolution of 224 \u00d7 480 pixels. The BEV spatial is 100m \u00d7 100m area at a pixel resolution of 50cm in both the x and y directions. This results in a BEV video with spatial dimensions of 200 \u00d7 200 pixels.\nTraining is performed using the Adam optimizer with a constant learning rate of 2.0 \u00d7 10-3. The model is trained for 20 epochs with a batch size of 6, distributed across 4 Tesla A6000 GPUs. To optimize memory usage and accelerate computation, mixed precision training is employed. Additionally, both our model and ST-P3 are trained without depth guidance, ensuring a fair comparison and highlighting the effectiveness of our approach in leveraging semantic and attentional cues for improved performance."}, {"title": "V. RESULTS", "content": "A. Quantitative Results\n1) Perception: Table I presents the perception performance of various methods across four key categories: Drivable Area, Lane, Vehicle, and Pedestrian. Our proposed VLM-E2E model demonstrates significant improvements over existing approaches, achieving best results in three out of four categories. Specifically, VLM-E2E outperforms ST-P3 in lane detection with a 2.24% relative improvement, vehicle detection with an 0.75% increase, and Pedestrian detection with a 24.40% boost on the nuScenes validation set. While IVMP achieves the highest score in drivable area detection, VLM-E2E closely follows with a score of 74.69, demonstrating competitive performance. These results highlight the effectiveness of our endto-end approach in enhancing perception accuracy, particularly driving, especially in scenarios that demand reliable long-term predictions and effective collision avoidance.\nB. Qualitative Analysis\nFig. 3 demonstrates the generated outputs, including instance segmentation, instance center, instance offset, and future flow. Fig. 3(b) features a heatmap highlighting detected objects, while Fig. 3(c) displays the instance segmentation results, where each segment is color-coded to represent different objects. The offset is a vector pointing to the center of the instance in Fig. 3(d). The future flow Fig. 3(e) is a displacement vector field of the dynamic agents. These visualizations enhance the understanding of spatial relationships and the distribution of elements within the environment, underscoring the model's capability to accurately perceive and segment critical features essential for autonomous driving applications.\nFig. 4 illustrates examples of planning scenarios. In the upper scene, the model accurately predicts the route when provided with turning instructions, effectively navigating through crowded environments in a manner similar to human demonstrations. The bottom scene demonstrates the model's predictions when instructed to proceed straight at an intersection, further"}]}