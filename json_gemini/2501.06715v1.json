{"title": "ZNO-Eval: Benchmarking reasoning capabilities of large language models in Ukrainian", "authors": ["Mykyta V. Syromiatnikov", "Victoria M. Ruvinskaya", "Anastasiya S. Troynina"], "abstract": "As the usage of large language models for problems outside of simple text understanding or generation increases, assessing their abilities and limitations becomes crucial. While significant progress has been made in this area over the last few years, most research has focused on benchmarking English, leaving other languages underexplored. This makes evaluating the reasoning and robustness level of language models in Ukrainian particularly challenging.\n The purpose of this work is to establish a comprehensive benchmark for the reasoning capabilities evaluation of large language models in the Ukrainian language. This paper presents the ZNO-Eval benchmark based on real exam tasks from Ukraine's standardized educational testing system: the External Independent Evaluation and the National Multi-subject Test. With single-answer options, multiple-choice, matching, and open-ended questions from diverse subjects, including Ukrainian language, mathematics, history, and geography, this dataset paves the way toward a thorough analysis of reasoning capabilities across different domains and complexities.\n Evaluation of several well-known language models, such as GPT-3.5-Turbo, GPT-40, GPT-4-Turbo, Mistral Large, Claude 3 Opus, and Gemini-1.5 Pro on this benchmark demonstrated the superiority of GPT-40 in both common knowledge reasoning and intricate language tasks. At the same time, Gemini Pro and GPT-4 Turbo excelled in the arithmetic domain, leading in single-answer and open-ended math problems. While all models were close to max performance in text-only common knowledge tasks like history and geography, there still is a gap for Ukrainian language and math, thus highlighting the importance of developing specialized language benchmarks for more accurate assessments of model capabilities and limitations across different languages and contexts.\n This research introduced ZNO-Eval, an effective benchmark for evaluating reasoning capabilities, and thoroughly explored the abilities and limitations of modern solutions in the Ukrainian language. Future research should aim to expand the scope of ZNO-Eval to other modalities like images commonly used for exam problem description.", "sections": [{"title": "Introduction", "content": "The recent advancements in language modeling have revolutionized the field of natural language processing, dramatically improving the context understanding in automated customer service and content generation tasks. Moreover, it was observed that large language models (LLM) could be successfully applied to solve other real-world problems outside NLP, such as making discoveries in mathematical sciences [1] or planning and executing operations with robots for a given task description in natural language [2]. Although general-purpose benchmarks for language understanding or generation with narrow homogeneous tasks are essential in baseline performance measurement and cross-model comparisons, they can barely help understand actual reasoning, complete scene-understanding capabilities, and evaluate hallucinations in complex real-world tasks where the language model makes plausible but incorrect statements.\n Nowadays, while LLMs' reasoning capabilities are being actively studied, much of the current investigation and benchmarking focus disproportionately on widely spoken languages like English, leaving significant gaps in understanding how these models perform in less commonly represented languages. Ukrainian, with its rich linguistic features and growing digital presence, despite presenting a unique challenge and opportunity for such evaluation, has not received the same level of attention."}, {"title": "Related works", "content": "This work aims to establish a robust tool in the form of a benchmark dataset with content-rich tasks bearing real-world complexity designed to assess the reasoning capabilities and encompass the full spectrum of language understanding of publicly available large language models in the Ukrainian language.\n Over the past few years, a simple increase of the trainable parameters, the so-called scaling law, ascended language models to the critical milestone of human-level performance on common general-purpose benchmarks like GLUE [3]. However, further discoveries later revealed that these metrics have a limited correlation with real-world performance and barely provide researchers with a satisfactory comprehension of actual model capabilities and limitations [4]. Since then, the benchmarks and datasets tailored to assess various understanding and reasoning tasks have significantly advanced and played a pivotal role in driving progress in language modeling.\n Let us take a deeper look at widely used benchmarks focusing on reasoning capabilities:\n MMLU (Massive Multitask Language Understanding) benchmark designed to test a model's knowledge and reasoning abilities in specific subject areas includes over 57 tasks, covering multiple academic disciplines such as mathematics, history, literature, and science;\n GSM8K (Grade School Math 8K) is a standard benchmark for assessing language models' multi-step reasoning and arithmetic skills, featuring 8,000 school-level math problems;\n\u2013 BIG-Bench (Beyond the Imitation Game Benchmark) is a large-scale benchmark with over 200 tasks such as logic, mathematics, common sense reasoning, and language generation, challenging models to demonstrate deeper understanding and reasoning across different cognitive tasks [5].\n While the aforementioned multitask and multilingual benchmarks empower researchers with a thorough evaluation, their primary focus remains mainly on English, limiting their applicability in assessing performance across different languages, including Ukrainian. Most existing diagnostical datasets for the Ukrainian language target narrow problems like text classification or question answering. However, a good starting point for reasoning capability evaluation was recently established by the Shared Task on Fine-Tuning Large Language Models for Ukrainian. This task contains almost 4,000 machine-readable questions and answers from the Ukrainian External Independent Evaluation (EIE) exam, covering two subjects: the history of Ukraine and the Ukrainian language and literature [6]. Compared to other benchmarks, this dataset better reflects real-world complexity since its origin serves as a critical measure of academic proficiency for school graduates across the nation. However, this benchmark has three key disadvantages: it only contains questions with one correct answer; it does not evaluate arithmetic skills; and tasks are not grouped by tests, thus making comparison with human performance harder.\n This paper introduces ZNO-Eval, a comprehensive benchmark designed to assess the reasoning capabilities of large language models in the Ukrainian language. ZNO-Eval is inspired by the structure and content of the standardized Ukrainian educational testing system, the External Independent Evaluation. By leveraging the question format defined by the ZNO dataset, a dataset with diverse subjects such as language, mathematics, history, and geography was created."}, {"title": "Datasets", "content": "The Ukrainian language and literature dataset consists of 49 exams administered over the past ten years, containing 2.746 questions in total. To evaluate Ukrainian language proficiency, we selected four National Multi-subject Test (NMT) exams, each containing 30 tasks, resulting in 120 questions. These exams were chosen intentionally, as in contrast to EIE, postponed after 2021, its temporary replacement \u2013 NMT, does not contain open-ended tasks requiring subject matter expert assessment. Fig. 2 presents a graph showing the average test scores achieved by each model.\nEach graph's \"max\" bar represents the distribution of exam points per task type. In contrast, the \"max solvable\" bar can be interpreted as a practical ceiling for these evaluations, as it only considers tasks possible to solve with text-only input. For each single correct answer task, four or five answer options are presented, of which only one is correct. Each matching task consists of information indicated by numbers and letters. To complete the task correctly, matching the information marked with numbers and letters (to form logical pairs) is necessary.\n A dataset of 3 EIE and 6 NMT math exams was collected for arithmetic reasoning evaluation. Combining single correct answer tasks, matching, and open-ended questions requiring multi-step reasoning results in 230 entries. Due to the same reason as for the Ukrainian language evaluation, the four NMT exams with 30 questions each were selected. Additionally, all formulas in question and answer options have been converted to either plain text or LaTeX formatting, depending on the formula complexity. This adjustment was necessary as the original MathML format significantly increased the total number of input tokens. Math evaluation results are demonstrated in Fig. 3.\nThe History of Ukraine dataset was compiled from NMT and EIE exams, resulting in 48 tests and 2.640 questions, covering key historical periods, significant events, and influential figures that have shaped the country. Fig. 4 demonstrates the evaluation result, for which a subset of the dataset with four NMT tests and 120 questions was selected.\nIn addition to single correct answer and matching questions used in math and language exams, the history tests introduce two more types. For sequencing tasks, a list of events must be arranged chronologically, while tasks with three correct answers offer seven answer options.\nThe Geography dataset was carefully assembled from 32 External Independent Evaluation tests, totaling 1,788 questions. This dataset covers a broad spectrum of geographical topics, providing a well-rounded assessment of common knowledge. Fig. 5 presents the evaluation results for a subset of this dataset, encompassing three exams with 54 tasks each and 162 questions in total."}, {"title": "Conclusion", "content": "The evaluation of large language models using the proposed ZNO-Eval benchmark reveals that GPT-40 performed the best overall, demonstrating strong reasoning and comprehension capabilities across various subjects. However, Gemini-1.5 Pro and GPT-4 Turbo outperformed the leader in handling complex, unstructured arithmetic reasoning tasks, showing distinct strengths in specific areas. The models performed well on the NMT subsets for history and geography, highlighting their proficiency in factual recall and common knowledge. They also demonstrated strong capabilities in handling structured tasks and schemas in a zero-shot manner. However, the limitations in understanding more nuanced language and terms led to struggles with single-answer and matching questions for Ukrainian language and math exams, while these tasks are ordinary for most examinees.\n These findings underscore the importance of creating language-specific benchmarks to ensure that LLMs can perform effectively across diverse languages and domains. The results from the ZNO-Eval benchmark provide valuable insights into the current state of LLMs' reasoning abilities and limitations in the Ukrainian language, while the established benchmark [8] itself contributes to the broader goal of building general-purpose AI systems that can serve diverse communities."}]}