{"title": "EMPOWER: Embodied Multi-role Open-vocabulary Planning with Online Grounding and Execution", "authors": ["Francesco Argenziano", "Michele Brienza", "Vincenzo Suriani", "Daniele Nardi", "Domenico D. Bloisi"], "abstract": "Task planning for robots in real-life settings presents significant challenges. These challenges stem from three primary issues: the difficulty in identifying grounded sequences of steps to achieve a goal; the lack of a standardized mapping between high-level actions and low-level commands; and the challenge of maintaining low computational overhead given the limited resources of robotic hardware. We introduce EMPOWER, a framework designed for open-vocabulary online grounding and planning for embodied agents aimed at addressing these issues. By leveraging efficient pre-trained foundation models and a multi-role mechanism, EMPOWER demonstrates notable improvements in grounded planning and execution. Quantitative results highlight the effectiveness of our approach, achieving an average success rate of 0.73 across six different real-life scenarios using a TIAGO robot.", "sections": [{"title": "I. INTRODUCTION", "content": "A plan is a finite sequence of actions that one or more agents must execute to achieve a specific goal within a given environment. When this environment is a real-world setting, whether controlled or not, the already inherent difficulty of this task increases even more. This happens because the actions need to be grounded in the environment, meaning a mapping is needed between the high-level actions of the plan - described either in natural language or in some formal language - and the low-level actions that the agent can execute in the physical world, in terms of joint motions.\nWhen those agents are real robotic systems, they typically face limitations in computational resources due to their hardware, unlike simulated agents that can run on more powerful machines. As a result, a trade-off is necessary between performance and computational overhead, which can harm the system, especially if fast execution is required. Thus, enabling an embodied agent to accurately plan and act in the world is a significant challenge in this context.\nIn recent years, we have witnessed the emergence of Foundation Models (FMs), i.e. machine learning models trained on a vast, Internet-scale amount of data adaptable for a wide range of downstream applications [1]. Although initially designed for Natural Language Processing (NLP) tasks, these models have found utility across a variety of cross-domain applications. This happened because as a consequence of being trained on such a broad amount of data, FMs have incorporated the commonsense knowledge which is proper"}, {"title": "II. RELATED WORK", "content": "In this section, we review the most relevant literature on the topic. We first discuss the applications of LLMS in the field of robotics (Section II-A). Next, we examine the benefits of prompt engineering, particularly in multi-role and multi-agent scenarios (Section II-B). Finally, we analyze the use of open-vocabulary detectors in complex scenarios (Section II-C)."}, {"title": "A. LLMs and robotics", "content": "Large Language Models (LLMs) [4], [5], [11], [6], and Visual Language Models (VLMs) [12], [13], [14] have emerged as some of the most significant achievements in Artificial Intelligence (AI) in recent years. Their integration with robotics has rapidly advanced, driven by the benefits"}, {"title": "B. Multi-role prompting", "content": "Designing prompts to follow specific patterns has proven to be very effective, leading to better performance compared to free-form text (prompt engineering) [3]. Notably, Wei et al. [2] showed that asking an LLM to explicitly describe its reasoning process significantly improves the quality of the results, a technique known as chain-of-thought reasoning. Building on this concept, other prompting strategies have been developed, such as tree-of-thoughts [30], which is particularly relevant in planning applications where multi-step reasoning is required. Despite these advancements, LLMs can still suffer from issues such as hallucinations or other undesirable outputs. A highly effective solution to this problem is leveraging multi-agent and multi-role systems. By following the \"divide et impera\" paradigm, breaking down"}, {"title": "C. Open-vocabulary systems", "content": "Recent advancements in open-vocabulary systems have contributed to bridging the semantic gap between textual descriptions and visual content, freeing models from the limitations of a fixed, closed vocabulary. Transformers have enabled open-vocabulary architectures in various tasks, including image segmentation [36], [37], [38], object detection [39], [40], [41] and scene parsing [42], [43], [44]. However, this level of generality comes at a cost: these models often require significant computational resources even during inference, making them less suitable for embedded applications. In our work, we use on-the-edge models like YOLO-World [45] and EfficientViT-SAM [46] to achieve an open-vocabulary architecture that does not compromise the overall system's performance."}, {"title": "III. METHODOLOGY", "content": "In this section, we describe the architecture of our system. Section III-A details the planner component, which utilizes a multi-role subdivision of tasks. Section III-B explains the NLP processing technique we used for grounding concepts in the scene and the open-vocabulary tools we adopted. Section III-C describes how we map the high-level grounded plan obtained in previous steps to low-level actions executable by the robot."}, {"title": "A. Multi-role planner", "content": "The multi-role planner module consists of three agents, chosen accordingly to [10]: a Semantic-Knowledge Miner Agent (SMK) that is specialized in obtaining semantic relationships between objects in the scene; a Grounded-Knowledge Miner Agent (GMK) that is focused on describing the environment by grounding the objects relevant to the task; a Planner Agent (P) which is responsible for generating the final sequence of actions needed to complete the task.\nFor our implementation, the SMK and GMK roles are performed by GPT-4V [12], while the Planner role is handled by[11]. We selected these models due to their effectiveness in preliminary tests and their ability to perform efficiently without overloading the robotic system.\nThe agents work together to devise plans executable by a"}, {"title": "B. Open-vocabulary grounding", "content": "The open-vocabulary grounding component enables semantic grounding and discrimination of the objects necessary to achieve the desired task. In fact, given that multiple instances of the same object type might be present, it is crucial to distinguish their locations and roles w.r.t. the task.\nThe SMK agent's response includes a graph of objects and their spatial relations. This information helps identify instances of objects and their grounding.\nTo determine object classes seen by the robot, we apply a Part-of-Speech (POS) tagging technique to extract nouns, filtering out adjectives that describe objects. We have chosen the nouns that respect the condition of central nouns, nominal subjects, or direct objects of the sentence. For instance, for an object described as a recycling bin for paper, we only need to know that the object is a bin to detect it: at this stage, every other information is not relevant. Therefore, the POS-tagger allows us to obtain for the object only the label that we need (bin). We use spaCy [48] as our POS-tagger of choice. Objects not previously encountered are added to a list that contains all the classes needed to solve the task (Algorithm 1). We use Word2Vec similarity embeddings to check whether an object is already in the list. The use of semantic similarity is necessary because several words can describe the same object in the scene. To compute the semantic similarity between two objects, we cross-check each word of the object obtained by the POS-tagger."}, {"title": "C. Plan Actuator", "content": "The Plan Actuator component enables the robot to execute in the world the actions specified in the high-level plan. At this stage, we perform two key tasks: i. we refine high-level actions by mapping them with low-level primitives that the agent can perform; ii. we ground these actions, namely, we determine which objects the robot should interact with based on the output of the previous module. Since we are using a TIAGo robot with a gripper end-effector, we have identified a set of 5 low-level primitives directly executable by the robot: 1. NAVIGATE (to a location), 2. GRAB (an object), 3. DROP (an object), 4. PULL (an object and move back), 5. PUSH (an object with the base). All possible action names that can appear in the plan's description are mapped to one of these 5 primitives so that at lower levels we have more control over what action is going"}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "This section details the experiments conducted in our laboratory using the robot."}, {"title": "A. Hardware specification", "content": "Our framework operates on the TIAGO robot, which is well-suited for indoor manipulation tasks due to its 7 degrees-of-freedom arm. The robot is equipped with a 640x480 RGB-D camera and uses a Parallel Gripper end-effector for enhanced grabbing capabilities. It is powered by 16GB of RAM, an Intel i7 processor, 512 GB of storage, and notably, it lacks a GPU, meaning all computations are carried out on the CPU.\nWe designed six representative use cases to evaluate the LLMs' planning capabilities for indoor tasks involving multi-step reasoning and manipulation. These use cases are: sort object by their height, grab a jacket on the coat rack, throw the objects in the right recycle bins, order the shelf to have 2 objects per level, order the shelf depending on the objects' material, exit the room."}, {"title": "B. Qualitative Results", "content": "We conducted 10 runs for each use case. shows representative results from these runs. In particular, it includes the results of the grounding components, the semantic point cloud, their reprojections in the scene, and the plans"}, {"title": "C. Quantitative Results", "content": "To evaluate our system quantitatively, we used the Success Rate (SR) metric, which measures the percentage of successful plans across 10 runs. A plan is deemed successful if the goal is achieved at any point and is not undone by subsequent actions; on the contrary, it is considered unsuccessful if subsequent actions nullify the achieved goal. In Figg. 3, 4 display the results. Our comparison study shows that the multi-role architecture achieves an average SR of 73%, compared to 34% for the single-role architecture, highlighting the effectiveness of the multi-role approach for complex reasoning tasks. An analysis of the average number of steps per plan is also shown in Fig. 5, and it indicates that plans generated by the multi-role setup are generally shorter and more accurate."}, {"title": "D. Temporal Analysis", "content": "State-of-the-art systems for open-vocabulary object detection and phrase grounding, such as GLIP [51], require CUDA acceleration and thus cannot be used on systems without. Our pipeline, however, provides comparable qualitative performance on CPU-only robots and operates efficiently on embodied systems.\nWe conducted a temporal analysis of our framework, examining the average execution times for the NLP pipeline, YOLO-World, and EfficientViT-SAM The NLP pipeline takes on average in 0.8 seconds, YOLO-World takes 0.7 seconds, and EfficientViT-SAM processes one mask in 0.9 seconds. Thus, the total time for a full execution ranges from"}, {"title": "V. DISCUSSION", "content": "This paper offers contributions both in engineering and research, advancing robot embodiment and highlighting how improved spatial and semantic awareness can enhance planning, especially in unfamiliar environments.\nFrom an engineering perspective, the EMPOWER framework integrates pre-trained models with a multi-role planning structure, enabling real-time, open-vocabulary grounding and planning on systems like the TIAGo robot. This approach demonstrates practical, CPU-based solutions for complex tasks, making it suitable to a wide range of robotic platforms.\nResearch-wise, this work takes a first step toward more advanced robot embodiment, where robots gain a deeper semantic understanding of their environment. The multi-role architecture supports online execution, showing that enhanced awareness helps robots adapt and plan effectively, even in new settings. This shift towards open-vocabulary planning aids in better handling unexpected scenarios.\nThis paper underscores the value of combining spatial and semantic awareness in robotics, and we hope to pave the way for future developments in autonomous, intelligent systems."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we introduced EMPOWER, a framework designed for open-vocabulary, online grounding, and planning in embodied agents, specifically aimed at addressing real-world robotic planning challenges. By leveraging efficient pre-trained foundation models within a multi-role structure, we developed a multi-role planning system that, to the best of our knowledge, is the first to support online execution in real-time settings.\nOur approach enables embodied agents to plan effectively in an open-vocabulary environment, utilizing semantic understanding derived from NLP techniques applied to elements within the scene.\nWe evaluated our framework at multiple levels, first demonstrating the enhanced high-level planning capabilities provided by our multi-role VLM-based architecture compared to a single-role approach. We then showcased the system's overall planning capabilities, from high-level strategy to low-level execution, by testing the Success Rate of a TIAGo robot performing tasks in real environments.\nThe evaluation was conducted across six real-life scenarios, including tasks that require interpretative skills, which are particularly challenging for robots. For example, sorting and recycling waste by material type involves distinguishing between objects with similar appearances but different semantic meanings.\nQuantitative results highlight the effectiveness of our approach, achieving an average Success Rate of 0.73 across the selected challenging use cases performed by the TIAGO robot."}]}