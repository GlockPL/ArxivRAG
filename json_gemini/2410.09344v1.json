{"title": "DARE the Extreme \u00dd: Revisiting Delta-Parameter Pruning\nFor Fine-Tuned Models", "authors": ["Wenlong Deng", "Yize Zhao", "Vala Vakilian", "Minghui Chen", "Xiaoxiao Li", "Christos Thrampoulidis"], "abstract": "Storing open-source fine-tuned models separately introduces redundancy and increases response times\nin applications utilizing multiple models. Delta-parameter pruning (DPP), particularly the random drop\nand rescale (DARE) method proposed by Yu et al., addresses this by pruning the majority of delta param-\neters-the differences between fine-tuned and pre-trained model weights-while typically maintaining\nminimal performance loss. However, DARE fails when either the pruning rate or the magnitude of the delta\nparameters is large. We highlight two key reasons for this failure: (1) an excessively large rescaling factor\nas pruning rates increase, and (2) high mean and variance in the delta parameters. To push DARE's limits,\nwe introduce DAREX (DARE the eXtreme), which features two algorithmic improvements: (1) DAREx-q,\na rescaling factor modification that significantly boosts performance at high pruning rates (e.g., > 30%\non COLA and SST2 for encoder models, with even greater gains in decoder models), and (2) DAREx-L2,\nwhich combines DARE with AdamR, an in-training method that applies appropriate delta regularization be-\nfore DPP. We also demonstrate that DAREx-q can be seamlessly combined with vanilla parameter-efficient\nfine-tuning techniques like LoRA and can facilitate structural DPP. Additionally, we revisit the application\nof importance-based pruning techniques within DPP, demonstrating that they outperform random-based\nmethods when delta parameters are large. Through this comprehensive study, we develop a pipeline for\nselecting the most appropriate DPP method under various practical scenarios.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) like BERT (Devlin et al., 2018), GPT (Floridi & Chiriatti, 2020), and\nLlama (Touvron et al., 2023) excel in various language-modeling tasks. Finetuning these for specific\ndownstream tasks enhances personalized experiences (Sanh et al., 2021; Labrak et al., 2024) and has become\na standard practice in natural language processing (Dodge et al., 2020; Zhao et al., 2023). Indeed, most\nentries on the OpenLLM Leaderboard involve full-parameter finetunes or their combinations (Liu et al.,\n2024), underscoring the widespread adoption and availability of fine-tuned models online. Decomposing\nfine-tuned model weights into the original parameters of the pre-trained model yields delta parameters\n(DP) (Yu et al., 2023a; Liu et al., 2024; Yao & Klimovic, 2023). Reducing the size of DPs, which are as large\nas the base model and can number in the hundreds of millions of parameters for LLMs, could significantly\nenhance communication efficiency in federated learning, minimize task conflicts in model merging, accelerate\nmulti-task serving, and decrease storage needs for new fine-tuned models (see Related Work).\nDelta-parameter pruning (DPP) drops a fraction $p$ of the DPs towards realizing these benefits. Naturally,\nDPP can be seen as an instance of generic model-parameter pruning, which compresses neural networks\nby eliminating weights that contribute minimally, resulting in sparse architectures (LeCun et al., 1989; Han\net al., 2015b). Traditional pruning methods typically remove weights post-training based on importance\ncriteria like weight magnitude or activation levels. While these techniques could naturally extend to DPP,\ntheir integration into this context remains largely unexplored.\nRandom-based pruning strategies, which provide more flexibility and efficiency in implementation, also offer\na competitive alternative. For instance, Random Drop and Rescale (DARE) (Yu et al., 2023a), a recently\nintroduced randomized DPP method, reduces DP size through random pruning followed by rescaling. DARE\nhas been quickly adopted across various applications, including model merging libraries (Goddard et al.,\n2024), state-of-the-art medical LLMs (Labrak et al., 2024), and Japanese LLMs (Akiba et al., 2024).\nHowever, as we demonstrate in this paper, DARE struggles when the pruning rate is high or when DPs are\nlarge. This observation prompts several key questions:\nWhat are the key factors contributing to DARE's failure modes? Can these issues be addressed to\npush the limits of effective random-based DPP? Additionally, can importance-based model pruning\ntechniques be applied to DPP in ways that compete with random-based methods?"}, {"title": "1.1 Contributions", "content": "We address these questions through\nprincipled analysis and extensive\nexperimentation on large-scale lan-\nguage models and datasets. Our\ncontributions are summarized as fol-\nlows:\n\u2022 Analysis of DARE: By examining\nthe absolute change in intermediate\noutput model representations result-\ning from the application of DARE\nto the DPs, we identify two primary\nfactors that influence this change: (a)\na large pruning rate ($p$), which re-\nsults in an excessively high rescaling\nfactor of $1/(1 - p)$, and (b) a high\nmean and variance of the DPs rel-\native to input activations. Through\nexperiments on both controlled se-\ntups and LLMs, we show that the\nabsolute change in intermediate rep-\nresentations is a reliable proxy for\ntest performance (see Fig. 1). Thus, these two factors emerge as the main contributors to DARE's failure\nmodes. This analysis inspires two new algorithms that significantly improve DARE's performance, as follows."}, {"title": "2 Related work", "content": "Delta-parameter pruning. DPs represent the parameter changes induced by fine-tuning and are critical to\nvarious operations. In Federated and Distributed Learning, DPs are continuously exchanged between a central\nserver and distributed clients during training (Li et al., 2021, 2020; Khirirat et al., 2018; Chen et al., 2021).\nIn model merging, the goal is to merge multiple task-specific models, fine-tuned from the same pretrained\nbackbone, by combining their DPs into a single model with diverse capabilities (Wortsman et al., 2022;\nIlharco et al., 2022). Similarly, in multi-task serving, DPs are processed separately while the base model\nweights are computed once during the forward pass (Yao & Klimovic, 2023; Liu et al., 2024). (Yu et al.,\n2023a; Liu et al., 2024; Yao & Klimovic, 2023) focus on reducing the size of DPs, and recently (Yu et al.,\n2023a) has popularized DARE as a poerful random-based DPP method. However, DARE fails with large\nDPs or high pruning rates. We propose techniques that, applied during both fine-tuning and post-hoc stages,\nrestore performance and significantly improve upon vanilla DARE. Our results demonstrate, for the first\ntime, that DPP remains effective even at extreme pruning rates, reducing up to 99% of the model parameters.\nThanks to our modification, we also show that random-based DPP can be combined with LoRA and can be\nused for structured pruning.\nModel parameter pruning. There are two primary approaches to model parameter pruning: regularization-\nbased and importance-based methods; however, an exhaustive review is beyond the scope of this paper."}, {"title": "3 Delta parameter pruning", "content": "DPP aims to efficiently store a fine-tuned model $\\Theta_F$ initialized from a pre-trained base model $\\Theta_P$ by pruning\nthe difference known as the \"delta parameter\u201d (DP), defined as $\\Delta_{\\Theta} = \\Theta_F - \\Theta_P$ between the fine-tuned and\nthe base model. Successful DP pruning (DPP) removes parameters from $\\Delta$, resulting in reduced storage\nrequirements while minimally compromising performance. Sec. 3.1 presents the theoretical model used\nto analyze various DPP methods. Sec. 3.2 and Appendix A provide a theoretical analysis of randomized\nand importance-based DPP methods, respectively. These theoretical insights are tested through a detailed\ncontrolled experimental analysis using a two-layer neural network in App. B, and, more importantly, applied\nin Sec. 4 to guide the design of new algorithms that significantly improve LLM performance in Sec. 5."}, {"title": "3.1 Formulation", "content": "To gain analytical insights, we consider the application of DPP to perceptron operations in a neural network.\nThis focus is partly motivated by the observation that multi-layer perceptron (MLP) operations, both in the\nprojection layers of attention mechanisms and in feed-forward networks, constitute the majority of operations\nin LLMs. Consider a perceptron layer, where $x \\in \\mathbb{R}^n$ is the input activation vector connecting to $m$ output\nneurons, $W \\in \\mathbb{R}^{m \\times n}$ is the hidden-layer weight matrix, and $\\sigma$ is a Lipschitz activation non-linearity (e.g.,"}, {"title": "3.2 Randomized DPP: Random Drop and Rescale (DARE)", "content": "DARE randomly sets each element of $[P(\\Delta w)]_{ij}$ to zero (dropped) with probability $p$, and rescales non-zero\nelements to $[P(\\Delta w)]_{ij} = [\\Delta w]_{ij}/(1-p)$ (Yu et al., 2023a). While DARE has been tested on a wide range\nof fine-tuned models across various tasks, it exhibits performance degradation under high pruning rates, as\nshown in Table 7 and Table 8. We seek the underlying causes of these limitations and explore potential\nimprovements. Our approach involves quantifying the difference vector $h^{diff}$ in relation to key variables: the\ndropout rate, the DPs, and the input data. As per Eq. 1, the effect of DARE on the output becomes\n$h^{diff}_{i} = \\sum_{j\\in[n]} \\Delta W_{ij}x_j - \\sum_{j\\in[n]} \\frac{1}{1-p}d_{ij} \\Delta W_{ij} x_j,$\nwhere $p$ is the drop rate, $\\{d_{ij}\\}_{i\\in[m],j\\in[n]}$ are iid Bernoulli$(1 - p)$ random variables, and $1/(1 - p)$ is the\nrescaling factor. We denote $\\Delta W_{ij}$ the $(i, j)$ entry of $\\Delta w \\in \\mathbb{R}^{m \\times n}$.\nIt is easy to see that $\\mathbb{E}[h^{diff}] = 0$ (expectation over $d_{ij}$), and Yu et al. (2023a) use this as justification for\nDARE's effective performance. However, this alone overlooks the sources of DARE's failure modes, which\nwe reveal by instead establishing bounds on the absolute output changes $|h^{diff}|$. The following theorem\naddresses this by providing a high-probability bound on these changes, with respect to DARE's randomness,\nwhich arises from the Bernoulli variables. See App. E.1 for the proof."}, {"title": "3.3 Importance-based DPP", "content": "Using the same analytical framework, we extend the application of importance-based pruning methods,\nspecifically magnitude pruning and WANDA, to the DPP setting. Due to space constraints, we provide a\ndetailed discussion in App. A. In brief, importance-based DPP can achieve strong performance when the\ndistribution of the coefficients $c_{ij}$ exhibits light tails and sharp-peakedness. We provide empirical validation\nof these findings in Sec. C.4 and Appendix B."}, {"title": "4 Algorithms", "content": "To extend the applicability of DPP at high pruning rates and in cases where DPs of fine-tuned models exhibit\nundesirable statistics, we propose here two strategies based on the theoretical insights discussed in Sec. 3."}, {"title": "4.1 Adjusting the rescaling factor", "content": "Motivation. Recall from Thm. 3.1 as $p$ increases, the absolute difference in outputs $|h^{diff}|$ grows at a rate of\n$O((1-p)^{-\\frac{1}{2}})$. Setting the rescaling factor to $q = 1 - p$ eliminates the mean of $h^{diff}$, but does not minimize\nits magnitude. Through empirical analysis, we demonstrate in Fig. 1 how various values of $q$ influence model\noutputs in terms of both mean output change (i.e., the average of $|h^{diff}_{i}|$ across all last-layer neurons) in\nFig. 1a, and, test performance in Fig. 1b across different datasets. For 99% pruning rate (1 p = 0.01)\nwe observe these quantities over a range of $q$ values from 0.005 to 0.04. The results depicted in Fig. 1a for\nmean output change, indicate a convex-like trend, with minimum points (marked with asterisks) achieved\nat $q$ values higher than the vanilla $1 p$. E.g., the optimal $q$ for COLA and SST2 is \u2248 0.018 and 0.033,\nrespectively. Recall now that in our analysis, the amount of change in output activations is used as a proxy for\ntest performance: smaller mean output change corresponds to better performance. Fig. 1b validates this: the\ntest-performance curves show a concave-like shape, with the peak performance also occurring at $q > 1 - p$\nand numerically consistent with the values of that minimize the mean absolute output change.\nDrop and rescale with 1/q (DAREx-q). Motivated by these observations, we propose two variants of\nDAREx-q: tuning based on (1) maximizing performance on a labeled validation set, and (2) minimizing mean\noutput change over unlabeled data.\n\u2022 DAREx-q with labeled validation data (1/qv): We use a validation dataset $\\{x_v, y_v\\} \\in V$ to determine the\nbest rescaling factor 1/qv that maximizes test performance (eqv. minimizes test error) on the validation set.\nSpecifically, we select $q_v = \\arg \\min_q P_v(f_q(x_v) \\neq y_v)$, where $f_q$ represents the pruned model rescaled by\n$1/q$. We then randomly drop a faction $p$ of the model DPs and rescale those that are not dropped with $1/q_v$.\nFurther details can be found in Algorithm 2 (1) in the Appendix.\n\u2022 DAREx-q with unlabeled data (1/qe): This method selects q by optimizing an unsupervised objective\nmeasuring the mean output difference $|h^{diff}_{i}|$ across all last-layer neurons of the model. This approach is\nbased on the observation in Fig. 1 that mean output difference is as an effective unsupervised proxy for test"}, {"title": "4.2 AdamR finetuning", "content": "Motivation. While DAREx-q addresses the issue of large rescaling factors at high pruning rates, we've seen\nthat for absolute output changes $|h^{diff}_{i}|$ to remain small, the first- and second-order averages of the influence\nfactors $\\{c_{ij}\\}_{j\\in[n]}$ must also be small. These factors directly depend on the magnitude of the DPs $\\Delta W_{ij}$,\nwhich are determined during fine-tuning. We propose an in-training method that fine-tunes the model to keep\nthese statistics small, thereby guaranteeing that the post-hoc application of DARE maintains performance.\nAdamR-L2. Our approach is to fine-tune the model with L2 regularization on the DPs $\\Delta_{\\Theta} = \\Theta_F - \\Theta_P$.\nAs mentioned, the motivation behind this is penalizing $|\\Delta_{\\Theta}||^2$ directly translates to smaller total second-\norder averages $\\sum_{j\\in[n]} c_{ij}$, which, as previously noted, capture the extent of change in the output layer\npost-finetuning and factor into the bound in Thm. 3.1. To implement this, we replace the weight decay of\nAdamW-the standard choice for training LLMs-with our custom regularization. Specifically, we adjust the\nregularization strength based on the gradient norm, as Xie et al. (2024) recently found that weight decay can\ncause large gradient norms during the final phase of training. Our modified decay step of AdamW is:\n$\\Theta_t = \\Theta_{t-1} - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} m_t - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} \\lambda(\\Theta_{t-1} - \\Theta_P),$\nwhere $\\Theta_t$ represents model parameters at iteration $t$, $m_t$ and $\\hat{v_t}$ are the first and second moments of gradients,\nand $\\bar{v}$ is the mean of the second moment used to adjust regularization. See also Algorithm 1 in Appendix.\nAdamR-L1. We extend our approach to fine-tune the model in preparation for importance-based pruning,\nrather than random-based pruning, post-training. In this case, we regularize using the L\u2081 norm of the DPs.\nThis is again motivated by the analysis in Sec. 3.3. The detailed algorithm, referred to as AdamR-L1, is\nprovided in Algorithm 1 in the Appendix."}, {"title": "5 Experiments", "content": "We validate the effectiveness of our analysis and proposed algorithms through comprehensive experiments.\nDatasets and models for Encoder-based and Decoder-based LMs. For Encoder-based LMs, we utilize\nfour datasets-sentence acceptability dataset COLA (Warstadt et al., 2019), sentiment detection dataset SST-\n2 (Socher et al., 2013), paraphrase dataset MRPC (Dolan & Brockett, 2005), and sentence similarity dataset\nSTS-B (Cer et al., 2017). For the selection of pretrained backbones, we choose BERT-base-uncased (Devlin\net al., 2018) and RoBERTa-base (Liu et al., 2019), fine-tuning them on these task-specific datasets to\nobtain supervised finetuned models. For Decoder-based LMs, we focus on mathematical reasoning tasks.\nDue to constraints on resources for fine-tuning, we opted to fine-tune a smaller Qwen2-0.5B (Yang et al.,\n2024) model on the MetaMath dataset (Yu et al., 2023b). Additionally, we utilized publicly available\nmathematical reasoning models, including the MetaMath-llema-7B (Yu et al., 2023b), MetaMath-7B (Yu\net al., 2023b), WizardMath-7B (Luo et al., 2023) and Abel-7B (Chern et al., 2023), all based on the Llama2-7B\narchitecture (Touvron et al., 2023). We then use the GSM8K (Cobbe et al., 2021) to test these models.\nEvaluation metrics. Following (Yu et al., 2023a), we evaluate performance using the Matthews correlation\ncoefficient for CoLA, accuracy for SST-2, the average of accuracy and F1 score for MRPC, the average of\nPearson and Spearman correlation for STS-B, and zero-shot accuracy for GSM8K."}, {"title": "5.1 Rescaling-parameter modification", "content": "We evaluate our DAREx-q algorithm. From Sec. 4.1, recall the four proposed variations (i.e., 1/qv, 1/qe, 1/q\u03c5\nand 1/qe) for choosing the rescaling factor along two orthogonal axes: (a) whether to use labeled validation\ndata or not (1/qu or 1/qe, respectively); and (b) global tuning versus per-layer tuning (1/q vs 1/q).\nEncoder models. Table 2 compares DAREx-q (all four variants) with vanilla DARE (with rescaling factor"}, {"title": "6 Conclusions and limitations", "content": "Our systematic study of DPP methods, provides theoreti-\ncal insights, extensive experiments in controlled environ-\nments, and demonstrates practical applications on large-\nscale models and datasets. Starting with an in-depth analysis\nof random-based DPP methods, we identify key factors that\ndegrade their performance. Based on these insights, we pro-\npose: (i) DAREx-q, a post-training rescaling modification\nstrategy that significantly outperforms the state-of-the-art\nat high pruning rates and performs at least as well across\nother rates; and (ii) AdamR finetuning to enhance existing\nmodels that struggle with DPP, ensuring the production of\nhighly prunable DPs. Our results, including dramatic gains\nover baseline methods shown in Table 1, suggest that these\nstrategies warrant further investigation across different tasks\nand modalities. Additionally, preliminary results suggest\nthat DAREx-q can be effectively combined with parameter-efficient finetuning methods like LoRA and\nutilized for structural DPP, both of which highlight the potential for further research. Finally, while finetuning\nLLMs typically results in small DPs favoring random-based DPP, we demonstrate that importance-based\nDPP methods can serve as robust alternatives for larger DPs. This highlights the need for further exploration\nof alternatives. In conclusion, our comprehensive study of DPP methods provides a framework for selecting\nappropriate DPP methods under different practical scenarios, which we summarize in Fig. 4.\nFramework for DPP method selection: We outline the procedure for selecting appropriate DPP methods in\npractice. As illustrated in Fig 4, if fine-tuning is not permitted and the data points (DPs) have large statistics,\nwe recommend using WANDA for DPP (see Sections 5.3). If the DPs are not large, DAREx-q with 1/qv\nshould be applied when a validation set is available, otherwise, DAREx-q with 1/qe is recommended (see\nSec. 4.1). If the existing DPs are insufficient and fine-tuning is allowed, we suggest using AdamR-L2 or L1\nwith appropriate regularization weights to generate highly prunable DPs for DAREx-q and MP, respectively\n(see Sec. C.6). Among the two, AdamR-L2+DAREx-q (DAREx-L2-q) should be prefered due to flexibility\nand better performance as shown in Table 9 and Table 5.\nBroader impact: As discussed in Section 1, DPP offers potential advantages for online serving, gradient\ncommunication in Federated Learning, and storage saving. Our systematic study of DPP methods and the\nproposed strategies that improve performance of existing methods can positively impact the usage of LLMs.\nWe also analyzed the impact of our approach on the efficiency of the machine learning system in App. C.12."}, {"title": "A Importance-based DPP", "content": "In this section, we adapt importance-based pruning, which is traditionally popular for model parameter\npruning, e.g. Han et al. (2015b); Li et al. (2018); Sun et al. (2023), into DPP.\nMagnitude pruning (MP). Magnitude pruning Li et al. (2018) drops model weights according to their\nmagnitudes: less \"important\" weights with smaller magnitudes are dropped. To extend its application to\npruning delta parameters we evaluate importance in terms of the magnitudes of delta parameters, represented\nas $\\triangle S_{ij} := |\\triangle W_{ij}|$.\nPruning based on both weights and activations (WANDA). Recently, WANDA was introduced by Sun et al.\n(2023) and was empirically found to achieve state-of-the-art pruning performance for LLMs. It evaluates the\nimportance of each model weight using a score metric that scales the magnitude of the weight by the Euclidean\nnorm of the input feature, thus accounting for the input activations. We extend WANDA to encompass delta\nparameters by adjusting the importance score to $\\triangle S_{ij} = |\\triangle W_{ij}|\\cdot ||x_j||_2$.\nFor their respective scores, MP and WANDA (and any other importance-sampling method adapted to\nDPP) maintain the top-k parameters in $\\Delta w$ with the highest scores. Let $S_k \\subset [m] \\times [n]$ be the set of indices\n$(i, j)$ that correspond to weights with the top-k scores $\\triangle S_{ij}$. Moreover, let $k := (1 - p)mn$ so that the\nmethod retains the same number of delta parameters as DARE does in expectation. Accordingly, denote the\nset of selected weights with respect to $p$ as $S_p := S_k$. Then, putting in Eq. (1), the change for each output\nactivation $i \\in [m]$ can be expressed as:\n$h^{diff}_{i} = \\sum_{\\{j:(i,j)\\notin S_p\\}} \\triangle W_{ij} x_j,$\nwhere the summation extends over all input dimensions $j$ for which the $(i, j)$ entry of $\\Delta w$ is dropped due to\na low score. An importance sampling method can perform effectively even for large $p \\approx 1$ if the cumulative\ncontributions from the summation are approximately zero out. This is guaranteed when the distribution\nof the entries $[\\triangle W_{ij}x_j]_{j\\in[n]}$ has a light tail and high peakedness. We validate this in Secs. C.4 and B and\naccordingly propose AdamR-L1 fine-tuning to enhance their pruning performance (algorithm detailed in in\nSec. C.6)"}, {"title": "B Analysis on two-layer neural-network", "content": "Having gained analytical insights into the key factors that influence the performance of DPP methods in\nSec. 3.2 and Sec. A, we now explore in detail how these factors influence DPP in a two-layer neural network.\nConcretely, for an input $x \\in \\mathbb{R}^n$, the model output is $f(x) = W_oN(\\phi(W_1N(x) + b_1)) + b_o$. Here, $N\ndenotes layer normalization (RMSnorm Zhang & Sennrich (2019) in our case), $\\phi$ is the ReLU activation\nfunction, $W_o/ b_o$ and $W_1/ b_1$ are the weights / biases of the output and the hidden layer respectively, and\nare all trainable (during both pre-training and fine-tuning). In our experiments, we pre-train the model on the\nCIFAR-10 dataset and use the SVNH dataset Sermanet et al. (2012) for the supervised fine-tuning task.\nInfluence of variance and mean statistics on DARE (Fig. 5a): Thm. 3.1 establishes how DARE's\nperformance, approximated by the magnitude of $h^{diff}$, is influenced by the mean and variance statistics of\n$\\{c_{ij}\\}$. Specifically, smaller values are favorable for performance. To verify this, we compare performance of\nDARE when the model is fine-tuned using L\u2081/L2 regularization with respect to the pre-trained parameters.\nFormally, recalling that $\\Delta \\Theta = \\Theta_F - \\Theta_P$ denotes the delta parameter of all trainable weights, we use\nregularization that penalizes the following: $|\\Delta_{\\Theta}||_r, r\\in \\{1,2\\}$. L2 regularization directly corresponds to\na smaller total energy $(\\sum_{ij} c_{ij}^2)$ of the $\\{c_{ij}\\}$ parameters, which recall capture the degree of change in the\noutput layer after fine-tuning. This enters the bound in Thm. 3.1 and suggests that L2 regularization improves"}, {"title": "C Additional details", "content": "C.1 Additional related Work\nSupervised fine-tuning of language models. Supervised fine-tuning (SFT) of pre-trained LLMs is designed\nto enhance their capabilities by training on task-specific data, establishing a standard in natural language\nprocessing Dodge et al. (2020); Zhao et al. (2023). SFT can be categorized into full fine-tuning Devlin\net al. (2018); Liu et al. (2019) and parameter-efficient fine-tuning (PEFT) Ding et al. (2023); Li & Liang\n(2021); Hu et al. (2021); Deng et al. (2024). However, recent studies Chen et al. (2022); Lu et al. (2023);\nYao & Klimovic (2023) suggest that PEFT methods may not yet achieve the model quality of full parameter\nfine-tuning, particularly in high-resource tasks Chen et al. (2022). Additionally, Liu et al. (2024) indicates\nthat most models on the Open LLM Leaderboard are derived from full parameter fine-tunes or their merges.\nConsequently, this paper focuses on full model fine-tuning.\nSparse finetuning. An orthogonal approach to DPP is sparse fine-tuning, which reduces the size of delta\nparameters by modifying the fine-tuning process itself. This is achieved through techniques such as iterative\nmasking and fine-tuning to create sparse DP (Guo et al., 2020; Liao et al., 2023; Fu et al., 2023). In contrast,\nDPP methods like DARE Yu et al. (2023a) are primarily post-hoc procedures that focus on pruning the\ndelta weights of models that have already been fine-tuned, making DPP particularly valuable for the many\nfine-tuned models available on platforms like Hugging Face.\nFollow-up work on DARE. The recent development of the DARE (Drops delta parameters And REscales)\ntechnique has significantly advanced the efficiency of finetuned-model pruning. This method sets most\ndelta parameters to zero, maintaining the efficacy of Supervised Fine-Tuning without loss of performance.\nHighlighted in Goddard et al. (2024) within Arcee's MergeKit, a toolkit for merging large LMs, DARE\nhas shown great potential enhancing model merging processes and has various practical applications. It\nhas been successfully implemented in projects such as Prometheus Kim et al. (2024), an open-source LM\nfor evaluating other LMs, and in medical LMs like Biomistral Labrak et al. (2024), which develops LMs\nfor medical applications. The technique also supports specialized domains, as seen in Akiba et al. (2024),\nwhich focuses on a Japanese LLM with advanced reasoning capabilities. These implementations highlight\nthe broad applicability and effectiveness of DARE in enhancing model merging strategies, thus making our\nimprovements particularly relevant and practical."}, {"title": "C.2 Implementation details", "content": "For decoder LLMs, following Yu et al. (2023a), we set the temperature to 0.0 for greedy decoding and\nlimit the maximum number of generated tokens to 1,024 on GSM8K. For encoder-based LMs, we fine-tune\nBERT-base-uncased and RoBERTa-base for 10 epochs using a warmup strategy and a learning rate of le-4.\nExperiments are conducted on NVIDIA A100 GPUs."}, {"title": "C.3 Controlling pruning with regularizaiton", "content": "In this section, we demonstrate the magnitude of regularization weight can control the degree of pruning that\nis required.\nFig 6 (a-d) illustrates the AdamR-L2 fintuned RoBERTa model with different regularization weights\non SST2, MRPC, COLA and STS-B datasets respectively and we use pruning rate $p\\in [0,0.9, 0.99, 0.999]$\nto prune the delta parameters. To separate different pruning rates in the figure, we set log scaled pruning\n$\\frac{-\\log(1 - p)}{3} = [0,0.33, 0.67, 1.00]$ in x axis. With moderate regularization the model has a close\nperformance to models without regularizaiton (weight 0) (even better i.e.weight 0.01 in Fig 6 (a)). This\nindicate adding moderate regularization won't hurt the model performance. To achieve moderate pruning\nperformance $p = 0.99$, we can choose moderate regularization. For example, in fig 6 (c), choose weight 0.05\nachieves the best performance when pruning rate $p = 0.99$ and outperforms the no regularization by more\nthan 40%. When there is a need for extreme delta parameters pruning, it is recommended to increase the\nregularization weight. As shown in Figure 6 (d), weight 0.1 achieves the best performance when pruning"}, {"title": "C.4 AdamR-L\u2081 finetuning", "content": "In this section, we focus on MP (Han et al., 2015a) and WANDA (Sun et al., 2023), utilizing AdamR-L1 to\nenhance pruning performance for importance-based methods.\nMP: We demonstrate AdamR-L1 results in Table 7: the pruning rate consistently increased on datasets when\napplying magnitude pruning on L\u2081 fintuned models. This indicates AdamR-L1 is an effective strategy in\nproducing highly prunbale DPs for MP.\nWANDA: As shown in Table 7, AdamR-L\u2081 improved WANDA's performance in most datasets. However,\nin the SST2 and STS-B datasets (Table 7), adding AdamR-L\u2081 negatively impacted pruning performance,\nas indicated by the numbers in red. This is because WANDA considers outlier activations Dettmers et al.\n(2022) in LLMs and uses $\\triangle S_{ij} = |W_{ij}|\\cdot|x_j|$ as its metric. Thus, some important DPs may be pruned\nwhen AdamR-L1 is applied due to their small $|x_j|$. This suggests that important DPs are not always aligned\nwith outlier activations, and that AdamR-L\u2081 may not be the optimal strategy for improving WANDA. For\nfurther analysis of outlier activations, see Appendix D."}, {"title": "C"}]}