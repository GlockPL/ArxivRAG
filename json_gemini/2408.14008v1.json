{"title": "LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models", "authors": ["Qihang Ge", "Wei Sun", "Yu Zhang", "Yunhao Li", "Zhongpeng Ji", "Fengyu Sun", "Shangling Jui", "Xiongkuo Min", "Guangtao Zhai"], "abstract": "The explosive growth of videos on streaming media platforms has underscored the urgent need for effective video quality assessment (VQA) algorithms to monitor and perceptually optimize the quality of streaming videos. However, VQA remains an extremely challenging task due to the diverse video content and the complex spatial and temporal distortions, thus necessitating more advanced methods to address these issues. Nowadays, large multimodal models (LMMs), such as GPT-4V, have exhibited strong capabilities for various visual understanding tasks, motivating us to leverage the powerful multimodal representation ability of LMMs to solve the VQA task. Therefore, we propose the first Large Multi-Modal based Video Quality Assessment (LMM-VQA) model, which introduces a novel spatiotemporal visual modeling strategy for quality-aware feature extraction. Specifically, we first reformulate the quality regression problem into a question and answering (Q&A) task and construct Q&A prompts for VQA instruction tuning. Then, we design a spatiotemporal vision encoder to extract spatial and temporal features to represent the quality characteristics of videos, which are subsequently mapped into the language space by the spatiotemporal projector for modality alignment. Finally, the aligned visual tokens and the quality-inquired text tokens are aggregated as inputs for the large language model (LLM) to generate the quality score as well as the quality level. Extensive experiments demonstrate that LMM-VQA achieves state-of-the-art performance across five VQA benchmarks, exhibiting an average improvement of 5% in generalization ability over existing methods. Furthermore, due to the advanced design of the spatiotemporal encoder and projector, LMM-VQA also performs exceptionally well on general video understanding tasks, further validating its effectiveness. Our code will be released at https://github.com/Sueqk/LMM-VQA.", "sections": [{"title": "I. INTRODUCTION", "content": "As video consumption continues to surge across a variety of streaming media platforms, including YouTube, TikTok, Facebook, Netflix, etc, accurately evaluating the perceptual quality of videos is crucial for these video-enabled applications and services to monitor video quality throughout the entire video processing procedures and perceptually optimize video procession algorithms (e.g. video compression and enhancement), ultimately providing better quality of experience (QoE) for end-users. Towards this goal, several video quality assessment (VQA) methods [38] have been proposed in recent years, which can be categorized into reference-based VQA, including full-reference [2, 53] and reduced-reference VQA [15, 51], and no-reference VQA, also known as blind VQA (BVQA) [21-23, 46, 52, 54, 55, 58, 59, 61, 63, 64, 69, 70, 79]. In this paper, we focus on BVQA\u00b9 as it does not require any reference video information and has a broader range of application scenarios.\nIn the literature, BVQA is typically studied from two perspectives: the knowledge-driven approach [21, 39, 58, 59] and the data-driven approach [22, 24, 52, 61, 63, 64, 70]. Knowledge-driven BVQA methods leverage prior knowledge of visual quality perception to extract corresponding handcrafted features for quality regression. For example, natural scene statistics (NSS) [41] have been proven to be sensitive to synthetic distortions and are the most frequently used features in knowledge-driven blind image and video quality assessment studies [39, 40, 45]. Other visual descriptors, such as texture [8], noise [13], contrast [14], blockiness [80], color [48], motion vector [4], optical flow [37], etc., are also considered quality-related features and have been explored in previous studies.\nIn the following paper, we also use VQA to refer to BVQA for simplicity."}, {"title": "Towards this goal, we propose the first large multimodal-based video quality assessment (LMM-VQA) model by instruction tuning a spatiotemporal enhanced LMM model on the VQA datasets. Given that the functionality of LMMs is defined by instruction prompts, we design a rule-based approach to automatically generate a set of question-and-answer (Q&A) pairs for the VQA datasets. As shown in Fig. 2, the question template, termed quality prompts, consists of one system prompt, two instruction prompts, and two response restrictions to ensure the LMM comprehends the VQA task and produces the correct output format. The answering template contains the video quality score and the quality level, serving as the ground truth outputs for fine-tuning the LMM-VQA model.", "content": "In contrast, data-driven BVQA methods automatically learn the quality-aware feature representation by training a carefully designed deep neural network (DNN) in a learning-based manner. Typically, there are three kinds of architectures for quality-aware feature extraction: 1) utilizing a 2D network to extract frame-wise features and then employing a sequence model (e.g. GRU [6]) to fuse them into video-level features [24, 69]; 2) utilizing a 2D network to extract key frame features and a 3D network to extract the motion features from video chunks, which are then fused into video-level features by the concatenating operator or a sequence model [52, 64]; 3) directly utilizing a 3D network to extract video-level features from video chunks [63]. The 2D and 3D backbones can be pre-trained from other computer vision tasks [16, 71] or fine-tuned on VQA datasets via an end-to-end manner.\nDespite that significant efforts have been made in the field of BVQA, existing methods still face several challenges. First, knowledge-driven BVQA methods have better explainability but perform poorly on in-the-wild videos, as handcrafted features cannot model the complex in-the-wild distortion diverse video content. Second, although data-driven BVQA methods perform well on specific VQA datasets, they struggle to maintain the same level of performance during out-of-distribution (OOD) evaluations (as shown in Fig. 1), which is more critical for real-world applications. This limitation arises because current VQA datasets contain a relatively small scale of videos, which are insufficient to cover the vast high-dimensional spaces of video content and distortions, thereby restricting the ability of trained BVQA models to generalize to OOD videos.\nTo overcome this problem, a straightforward approach is to perform subjective experiments to construct a larger-scale VQA dataset that encompasses a wider variety of video samples. However, subjective VQA experiments are highly expensive and time-consuming, making the construction of such a large-scale VQA dataset challenging. Another routine is to leverage as many existing visual understanding models as possible to enhance the feature representation of BVQA models. For example, some BVQA studies [32, 55, 61, 73] combine diverse visual task models and quality assessment models to boost their representation capacity. Recently, the emergence of large multimodal models (LMMs) [1, 31, 68] opens the door for vision foundation models to handle general-purpose visual tasks. Some work also reveals that LMMs have strong abilities to understand low-level visual characteristics [30, 65, 66, 75]. These encouraging advances inspire us to leverage LMMs to address the BVQA task, thus benefiting from their strong general-purpose visual representation capabilities. However, our empirical studies reveal that directly utilizing video LMMs as the quality evaluator lead to very poor performance. As shown in Table I, the SRCC and PLCC values of the best-performing model, VILA-1.5, do not exceed 0.6 on five VQA benchmarks. Therefore, how to adapt LMMs to the VQA task and leverage their strong visual representation capabilities to develop effective and robust VQA model remains an open challenge."}, {"title": "Our LMM-VQA model consists of three modules: a spatiotemporal enhanced visual encoder for video feature extraction, a spatiotemporal visual project for vision-language alignment, and a large language model (LLM) for understanding video quality characteristics. For the spatiotemporal enhanced visual encoder, we avoid the sparse sampling frame strategy commonly used in most video LMMs [26, 28, 68] to encode video features, as temporal distortions in videos are typically reflected in continuous frames. Instead, we treat the video features as two distinct parts: spatial features and temporal features, where the spatial features are extracted by a 2D visual encoder (e.g., CLIP ViT-L/14 [44]) from sparse key frames, while the temporal features are extracted by a 3D visual encoder (e.g., SlowFast [11]) from continuous frames. This approach empowers LMMs to capture enough spatial and temporal distortion cues in videos for quality analysis, thereby enhancing prediction performance for the VQA task. Subsequently, the spatial and temporal features are inputted to the spatiotemporal projector to map visual tokens into the language space for modality alignment. Finally, the aligned visual tokens and quality prompt tokens are aggregated as input for the LLM to generate the final answers, including the video quality scores and levels. Experimental results reveal that LMM-", "content": "VQA achieves state-of-the-art (SOTA) performance across different datasets. Specifically, we apply in-sample, OOD, and fine-tuning evaluations to demonstrate its effectiveness in VQA tasks.\nOur contributions are summarized as follows:\n\u2022\n\u2022 We develop an instruction tuning pipeline to adapt LMMs for the VQA task, effectively leveraging their powerful visual representation capabilities for high-performance and robust video quality evaluation.\n\u2022 To address tricky temporal distortion issues in videos, we propose a spatiotemporal enhanced visual encoder that extracts the spatial and temporal features separately. The temporal features enhance the perception of temporal distortions and significantly improve the performance of the proposed LMM-VQA.\n\u2022 LMM-VQA achieves superior performance on five VQA benchmarks in both fine-tuning and OOD evaluation settings. What's more, LMM-VQA also perform well on general video understanding benchmarks. These experimental results comprehensively demonstrate the effectiveness of LMM-VQA."}, {"title": "II. RELATED WORK", "content": "Knowledge-driven BVQA models primarily utilize hand-crafted features derived through prior knowledge of visual quality perception to evaluate video quality [21, 39, 58, 59]. For instance, NIQE [41] and BRISQUE [39] first introduce natural scene statistics (NSS) for general quality predictions. Subsequently, V-BLIINDS [45] and VIIDEO [40] extend NSS to the temporal domain to address the BVQA task. TLVQM [21] integrates two-level features-spatial high-complexity and temporal low-complexity features to measure complex spatial and temporal distortions. VIDEVAL [58] ensembles diverse handcraft features sourced from classical BI/VQA models to create a strong feature representation for VQA. In summary, knowledge-driven BVQA models typically incorporate temporal-related features into BIQA models or extend the 2D quality features into 3D, demonstrating the critical importance of temporal information in video quality evaluation. However, due to insufficient understanding of human perception in quality assessment, knowledge-driven BVQA models tend to underperform on in-the-wild videos.\nData-driven BVQA models have gained popularity with the advancement of DNNs, aiming to automatically learn quality-aware features from labeled video data [22, 24, 52, 61, 63, 64, 70].For example, Li et al. [24] use GRU [6] to capture the temporal relationship among the semantic features extracted by ResNet-50 [16]. Liu et al. [34] jointly optimize the feature extractor and the regressor for quality assessment and compression distortion classification. Wang et al. [61] utilizes the compression level, video content, and distortion type derived from independent networks to evaluate the quality of user-generated videos. Li et al. [22] combine the CNN-based temporal model and RNN-based temporal model to"}, {"title": "III. METHODS", "content": "For an LMM based video quality assessment model, we define X as the video input, T as the text input, and Q as the video quality, including quality scores or levels output. Q is an approximation of the ground-truth video quality Q. Then, we define the assessment model F as follows:\n$\\mathcal{F}(X, T)$.\nLarge instruction-tuned multimodal models (i.e., fine-tuned to respond to instructions) showcase an impressive ability for new tasks. Instruction prompt generation is a method for inducing instruction following capabilities with minimal human labeled data [43]. Its effectiveness has been validated through extensive instruction tuning experiments [62]. In our work, we construct Q&A instruction prompts using videos and their MOS, as illustrated in Fig. 2. The question part includes quality prompts and videos. Quality prompts can be divided into three parts: system prompts, instruction prompts, and response restrictions. System prompts delineate the role of the LMM in this task, ensuring that the LMM has a clear understanding of the task. Instruction prompts pose a direct question to the LMM, as a conversational distillation of the task description. Response restriction defines the desired format for the model's answers. A typical example of these prompts is shown in Fig. 2.\nSpecifically, we construct pair-wised Q&A instruction prompts based on the VQA benchmark datasets [17, 50, 60, 70, 72]. The basic analysis of these datasets is shown in Table II. Note that each video-score sample in VQA datasets results in two Q&A instruction prompts, separately for the quality score regression task and the quality classification task. For the quality score regression task, we take scores as the answer in Q&A pairs. For the classification task, we equally divide dataset samples into three levels\u2014'good', 'fair', and 'poor'\u2014 based on their quality scores, and take these levels as the classification responses. To ensure the diversity of the training dataset, we generate 2,000 templates of instructions by GPT-4 such as: \"Would you mind calculating the video quality score with the help of these frames?\u201d. Meanwhile, the visual tokens (image-i) extracted from video chunks and key frames are inserted followed by quality prompts during our training.\nTo balance quality assessment efficacy and algorithmic complexity, we perform spatiotemporal downsampling operations during video preprocessing. Consider a video $X = {x_i}_{i=1}^{N-1}$"}, {"title": "A. Problem Setting", "content": "For an LMM based video quality assessment model, we define X as the video input, T as the text input, and Q as the video quality, including quality scores or levels output. Q is an approximation of the ground-truth video quality Q. Then, we define the assessment model F as follows:\n$\\mathcal{F}(X, T)$.\nLarge instruction-tuned multimodal models (i.e., fine-tuned to respond to instructions) showcase an impressive ability for new tasks. Instruction prompt generation is a method for inducing instruction following capabilities with minimal human labeled data [43]. Its effectiveness has been validated through extensive instruction tuning experiments [62]. In our work, we construct Q&A instruction prompts using videos and their MOS, as illustrated in Fig. 2. The question part includes quality prompts and videos. Quality prompts can be divided into three parts: system prompts, instruction prompts, and response restrictions. System prompts delineate the role of the LMM in this task, ensuring that the LMM has a clear understanding of the task. Instruction prompts pose a direct question to the LMM, as a conversational distillation of the task description. Response restriction defines the desired format for the model's answers. A typical example of these prompts is shown in Fig. 2.\nSpecifically, we construct pair-wised Q&A instruction prompts based on the VQA benchmark datasets [17, 50, 60, 70, 72]. The basic analysis of these datasets is shown in Table II. Note that each video-score sample in VQA datasets results in two Q&A instruction prompts, separately for the quality score regression task and the quality classification task. For the quality score regression task, we take scores as the answer in Q&A pairs. For the classification task, we equally divide dataset samples into three levels\u2014'good', 'fair', and 'poor'\u2014 based on their quality scores, and take these levels as the classification responses. To ensure the diversity of the training dataset, we generate 2,000 templates of instructions by GPT-4 such as: \"Would you mind calculating the video quality score with the help of these frames?\u201d. Meanwhile, the visual tokens (image-i) extracted from video chunks and key frames are inserted followed by quality prompts during our training."}, {"title": "B. Q&A Instruction Prompts", "content": "Large instruction-tuned multimodal models (i.e., fine-tuned to respond to instructions) showcase an impressive ability for new tasks. Instruction prompt generation is a method for inducing instruction following capabilities with minimal human labeled data [43]. Its effectiveness has been validated through extensive instruction tuning experiments [62]. In our work, we construct Q&A instruction prompts using videos and their MOS, as illustrated in Fig. 2. The question part includes quality prompts and videos. Quality prompts can be divided into three parts: system prompts, instruction prompts, and response restrictions. System prompts delineate the role of the LMM in this task, ensuring that the LMM has a clear understanding of the task. Instruction prompts pose a direct question to the LMM, as a conversational distillation of the task description. Response restriction defines the desired format for the model's answers. A typical example of these prompts is shown in Fig. 2.\nSpecifically, we construct pair-wised Q&A instruction prompts based on the VQA benchmark datasets [17, 50, 60, 70, 72]. The basic analysis of these datasets is shown in Table II. Note that each video-score sample in VQA datasets results in two Q&A instruction prompts, separately for the quality score regression task and the quality classification task. For the quality score regression task, we take scores as the answer in Q&A pairs. For the classification task, we equally divide dataset samples into three levels\u2014'good', 'fair', and 'poor'\u2014 based on their quality scores, and take these levels as the classification responses. To ensure the diversity of the training dataset, we generate 2,000 templates of instructions by GPT-4 such as: \"Would you mind calculating the video quality score with the help of these frames?\u201d. Meanwhile, the visual tokens (image-i) extracted from video chunks and key frames are inserted followed by quality prompts during our training."}, {"title": "C. Video Preprocessing", "content": "To balance quality assessment efficacy and algorithmic complexity, we perform spatiotemporal downsampling operations during video preprocessing. Consider a video $X = {x_i}_{i=1}^{N-1}$"}, {"title": "D. Model Structure", "content": "Videos, comprised of sequences of frames, depict motion scenes that differ from static images [35]. Therefore, the characteristics of both spatial and temporal dimensions are crucial for understanding videos [33]. Previous works have relied on discrete frames of videos to extract visual features, such as Video-ChatGPT [36]. While discrete frames provide a fundamental basis for understanding videos, they do not make use of the temporal features of continuous frames, leading to overlooked issues such as jitter, lagging, flickering, etc. Thus, to address spatial and temporal distortion issues in videos, we propose a novel spatiotemporal enhanced encoder a two-branch encoder designed to enhance the capture of spatial and temporal information in videos."}, {"title": "Algorithm 1 LMM-VQA", "content": "Input: video chunks V, key frames Y, pair-wised instruction text prompts: T\n1: for y in {y\u2070,y\u00b9,\u2026\u2026\u2026,y\u1d37\u207b\u00b9} do\n2:\tF\u209b\u209a = f\u209b\u209a(y)\n3: end for\n4: All spatial vision embeddings: F\u209b\u209a\u2208 \u211d\u1d3a\u1d56\u02e3\u1d9c\u02e2\u1d56\n5: for v\u2071 in {v\u2070, v\u00b9,...,v\u1d37\u207b\u00b9} do\n6:\tF\u209c\u209a = f\u209c\u209a(v\u2071)\n7: end for\n8: All temporal vision embeddings: F\u209c\u209a \u2208 \u211d\u1d37\u02e3\u00b9\u02e3\u1d9c\u1d57\u1d56\n9: Alignment for spatial token:\n10:\tZ\u209b\u209a = f\u1d65\u1d62\u209c (F\u209b\u209a)\n11: Alignment for temporal token:\n12:\tZ\u209c\u209a = Mean (f\u2098\u2097\u209a (F\u209c\u209a))\n13: Aggregated input token:\n14:\tZ\u2090\u2097\u2097 = f\ua700\u2090\u209c (Z\u209b\u209a, Z\u209c\u209a, Z\u209c\u2091\u2093\u209c)\n15: Generate quality score or level by LLM decoder:\n16:\tZ\u1d60\u1d64\u2090\u2097\u1d62\u209c\u1d67 = D (Z\u2091 | Z\u2090\u2097\u2097, Z<\u2091)\nOutput: Z\u1d60\u1d64\u2090\u2097\u1d62\u209c\u1d67\nTo detail the soundness of our technical design, we compare our LMM-VQA with two similar approaches, LLaVA [31] and Video-LLaVA [28] respectively. As shown in Fig. 4, LLaVA only takes the images as input without other external modalities. Different from LLaVA, Video-LLaVA takes images and discrete video frames as input and uses different vision encoders for them. With a small modality gap between images and discrete video frames, Video-LLaVA performs well with the shared V-L translator in video understanding tasks. In our task, temporal features are quite different from pixel-level video representation. Thus, we propose a spatiotemporal encoder with"}, {"title": "where $x^{i} \\in \\mathbb{R}^{H \\times W \\times 3}$ represents the i-th frame with a resolution of H \u00d7 W, and N is the total number of frames. As illustrated in Fig. 3, we aim to extract video chunks $V\\in\\mathbb{R}^{K \\times \\tau \\times H \\times W \\times 3}$ for temporal feature extraction and key frames $Y\\in\\mathbb{R}^{K \\times H \\times W \\times 3}$ for spatial feature extraction. Thus, we slice the video in the temporal domain to derive non-overlapping consecutive video chunks $V = {v^j}_{j=0}^{K-1}$. Each video chunk has the same frame length \u03c4 and $K = [N/\\tau]$. [\u00b7] is the floor function. So we define the j-th video chunk $v^{j}$ as:", "content": "Videos, comprised of sequences of frames, depict motion scenes that differ from static images [35]. Therefore, the characteristics of both spatial and temporal dimensions are crucial for understanding videos [33]. Previous works have relied on discrete frames of videos to extract visual features, such as Video-ChatGPT [36]. While discrete frames provide a fundamental basis for understanding videos, they do not make use of the temporal features of continuous frames, leading to overlooked issues such as jitter, lagging, flickering, etc. Thus, to address spatial and temporal distortion issues in videos, we propose a novel spatiotemporal enhanced encoder a two-branch encoder designed to enhance the capture of spatial and temporal information in videos."}, {"title": "$\\nu^j = \\lbrace x^{(\\tau \\cdot j + 1)} \\rbrace_{i=0}^{\\tau - 1},\\quad j = 0$", "content": "Videos, comprised of sequences of frames, depict motion scenes that differ from static images [35]. Therefore, the characteristics of both spatial and temporal dimensions are crucial for understanding videos [33]. Previous works have relied on discrete frames of videos to extract visual features, such as Video-ChatGPT [36]. While discrete frames provide a fundamental basis for understanding videos, they do not make use of the temporal features of continuous frames, leading to overlooked issues such as jitter, lagging, flickering, etc. Thus, to address spatial and temporal distortion issues in videos, we propose a novel spatiotemporal enhanced encoder a two-branch encoder designed to enhance the capture of spatial and temporal information in videos."}, {"title": "Then, we select the first frame $v_i^j$ as the j-th key frame $y^j$:", "content": "Videos, comprised of sequences of frames, depict motion scenes that differ from static images [35]. Therefore, the characteristics of both spatial and temporal dimensions are crucial for understanding videos [33]. Previous works have relied on discrete frames of videos to extract visual features, such as Video-ChatGPT [36]. While discrete frames provide a fundamental basis for understanding videos, they do not make use of the temporal features of continuous frames, leading to overlooked issues such as jitter, lagging, flickering, etc. Thus, to address spatial and temporal distortion issues in videos, we propose a novel spatiotemporal enhanced encoder a two-branch encoder designed to enhance the capture of spatial and temporal information in videos."}, {"title": "$y^j = x_{\\tau \\cdot j},$", "content": "Videos, comprised of sequences of frames, depict motion scenes that differ from static images [35]. Therefore, the characteristics of both spatial and temporal dimensions are crucial for understanding videos [33]. Previous works have relied on discrete frames of videos to extract visual features, such as Video-ChatGPT [36]. While discrete frames provide a fundamental basis for understanding videos, they do not make use of the temporal features of continuous frames, leading to overlooked issues such as jitter, lagging, flickering, etc. Thus, to address spatial and temporal distortion issues in videos, we propose a novel spatiotemporal enhanced encoder a two-branch encoder designed to enhance the capture of spatial and temporal information in videos."}, {"title": "B. Model Structure", "content": "Videos, comprised of sequences of frames, depict motion scenes that differ from static images [35]. Therefore, the characteristics of both spatial and temporal dimensions are crucial for understanding videos [33]. Previous works have relied on discrete frames of videos to extract visual features, such as Video-ChatGPT [36]. While discrete frames provide a fundamental basis for understanding videos, they do not make use of the temporal features of continuous frames, leading to overlooked issues such as jitter, lagging, flickering, etc. Thus, to address spatial and temporal distortion issues in videos, we propose a novel spatiotemporal enhanced encoder a two-branch encoder designed to enhance the capture of spatial and temporal information in videos."}, {"title": "Subsequently, video chunks V and key frames Y are processed by the vision encoder and projector to obtain visual tokens that serve as input to the LLM decoder.", "content": "Videos, comprised of sequences of frames, depict motion scenes that differ from static images [35]. Therefore, the characteristics of both spatial and temporal dimensions are crucial for understanding videos [33]. Previous works have relied on discrete frames of videos to extract visual features, such as Video-ChatGPT [36]. While discrete frames provide a fundamental basis for understanding videos, they do not make use of the temporal features of continuous frames, leading to overlooked issues such as jitter, lagging, flickering, etc. Thus, to address spatial and temporal distortion issues in videos, we propose a novel spatiotemporal enhanced encoder a two-branch encoder designed to enhance the capture of spatial and temporal information in videos."}, {"title": "$\\begin{aligned}F_{\\mathrm{sp}} &= f_{\\mathrm{sp}}(Y), F_{\\mathrm{sp}} \\in \\mathbb{R}^{K \\times N_p \\times C_{\\mathrm{sp}}}, \\end{aligned}$", "content": "where $N_p = H/p \\times W/p$ indicate the number of image patches and and $C_{\\mathrm{sp}}$ is the number of spatial embedding dimensions, respectively. For the temporal branch, a 3D visual encoder (SlowFast in Fig. 5) extracts temporal features $F_{\\mathrm{tp}}$:"}, {"title": "where $N_p = H/p \\times W/p$ indicate the number of image patches and and $C_{\\mathrm{sp}}$ is the number of spatial embedding dimensions, respectively. For the temporal branch, a 3D visual encoder (SlowFast in Fig. 5) extracts temporal features $F_{\\mathrm{tp}}$:", "content": "where $N_p = H/p \\times W/p$ indicate the number of image patches and and $C_{\\mathrm{sp}}$ is the number of spatial embedding dimensions, respectively. For the temporal branch, a 3D visual encoder (SlowFast in Fig. 5) extracts temporal features $F_{\\mathrm{tp}}$:"}, {"title": "$F_{\\mathrm{tp}} = f_{\\mathrm{tp}}(V), F_{\\mathrm{tp}} \\in \\mathbb{R}^{K \\times 1 \\times C_{\\mathrm{tp}}},$", "content": "where $C_{\\mathrm{tp}}$ is the number of temporal embedding dimensions. To address the disparity between vision and text modalities, we design a spatiotemporal projector to map visual tokens into language space for modality alignment, as illustrated in Fig. 5. Specifically, we introduce a spatial projector which comprises a single-layer vision transformer (ViT) and a one-layer MLP, to map spatial features into the language space. Following this, we use a one-layer MLP as a temporal projector to align temporal features with the language space. We formulate these processes as:"}, {"title": "where $C_{\\mathrm{tp}}$ is the number of temporal embedding dimensions. To address the disparity between vision and text modalities, we design a spatiotemporal projector to map visual tokens into language space for modality alignment, as illustrated in Fig. 5. Specifically, we introduce a spatial projector which comprises a single-layer vision transformer (ViT) and a one-layer MLP, to map spatial features into the language space. Following this, we use a one-layer MLP as a temporal projector to align temporal features with the language space. We formulate these processes as:", "content": "where $C_{\\mathrm{tp}}$ is the number of temporal embedding dimensions. To address the disparity between vision and text modalities, we design a spatiotemporal projector to map visual tokens into language space for modality alignment, as illustrated in Fig. 5. Specifically, we introduce a spatial projector which comprises a single-layer vision transformer (ViT) and a one-layer MLP, to map spatial features into the language space. Following this, we use a one-layer MLP as a temporal projector to align temporal features with the language space. We formulate these processes as:"}, {"title": "$\\begin{aligned}Z_{\\mathrm{sp}} &= f_{\\mathrm{viT}} (F_{\\mathrm{sp}}), \\\\Z_{\\mathrm{tp}} &= \\operatorname{Mean} (f_{\\mathrm{MLP}} (F_{\\mathrm{tp}})).\\end{aligned}$", "content": "A text decoder generates scalable text tokens Ftext from quality prompts T (i.e., LLM embedding layer in Fig. 5). Spatial tokens $Z_{\\mathrm{sp}}$, temporal tokens $Z_{\\mathrm{tp}}$ and text tokens $Z_{\\text{text}}$ are concatenated in the context length dimension $f_{\\text{cat}}$, defined as $Z_{\\text{all}}$:"}, {"title": "A text decoder generates scalable text tokens Ftext from quality prompts T (i.e., LLM embedding layer in Fig. 5). Spatial tokens $Z_{\\mathrm{sp}}$, temporal tokens $Z_{\\mathrm{tp}}$ and text tokens $Z_{\\text{text}}$ are concatenated in the context length dimension $f_{\\text{cat}}$, defined as $Z_{\\text{all}}$:", "content": "A text decoder generates scalable text tokens Ftext from quality prompts T (i.e., LLM embedding layer in Fig. 5). Spatial tokens $Z_{\\mathrm{sp}}$, temporal tokens $Z_{\\mathrm{tp}}$ and text tokens $Z_{\\text{text}}$ are concatenated in the context length dimension $f_{\\text{cat}}$, defined as $Z_{\\text{all}}$:"}, {"title": "$Z_{\\text{all}} = f_{\\text{cat}} (Z_{\\mathrm{sp}}, Z_{\\mathrm{tp}}, Z_{\\text{text}}).$", "content": "These text-guided visual tokens and quality prompt tokens are aggregated to generate the final output tokens, which represent the predicted quality scores or levels. \nWe utilize the Llama-3 model [9] as the language decoder and output the predicted answers in an auto-regressive manner:"}, {"title": "These text-guided visual tokens and quality prompt tokens are aggregated to generate the final output tokens, which represent the predicted quality scores or levels. \nWe utilize the Llama-3 model [9] as the language decoder and output the predicted answers in an auto-regressive manner:", "content": "These text-guided visual tokens and quality prompt tokens are aggregated to generate the final output tokens, which represent the predicted quality scores or levels. \nWe utilize the Llama-3 model [9] as the language decoder and output the predicted answers in an auto-regressive manner:"}, {"title": "$Z_{\\text{quality}} = D (Z_e | Z_{\\text{all}}, Z_{<e}),$", "content": "where $Z_{\\text{quality}} = {z_l}_{l=0}^{L-1}$ is the output text sequence follows the input tokens $Z_{\\text{all}}$ and D(\u00b7) is the Llama-3 decoder. $z_l \\in {0, 1}^{|S|}$ and S denote the vocabulary set. With well-trained LMM-VQA, $Z_{\\text{quality}}$ represents an approximation Q to the"}, {"title": "where $Z_{\\text{quality}} = {z_l}_{l=0}^{L-1}$ is the output text sequence follows the input tokens $Z_{\\text{all}}$ and D(\u00b7) is the Llama-3 decoder. $z_l \\in {0, 1}^{|S|}$ and S denote the vocabulary set. With well-trained LMM-VQA, $Z_{\\text{quality}}$ represents an approximation Q to the", "content": "where $Z_{\\text{quality}} = {z_l}_{l=0}^{L-1}$ is the output text sequence follows the input tokens $Z_{\\text{all}}$ and D(\u00b7) is the Llama-3 decoder. $z_l \\in {0, 1}^{|S|}$ and S denote the vocabulary set. With well-trained LMM-VQA, $Z_{\\text{quality}}$ represents an approximation Q to the"}, {"title": "true perceptual quality Q. The training process uses a cross-entropy loss:", "content": "where $Z_{\\text{quality}} = {z_l}_{l=0}^{L-1}$ is the output text sequence follows the input tokens $Z_{\\text{all}}$ and D(\u00b7) is the Llama-3 decoder. $z_l \\in {0, 1}^{|S|}$ and S denote the vocabulary set. With well-trained LMM-VQA, $Z_{\\text{quality}}$ represents an approximation Q to the"}, {"title": "true perceptual quality Q. The training process uses a cross-entropy loss:", "content": "where $Z_{\\text{quality}} = {z_l}_{l=0}^{L-1}$ is the output text sequence follows the input tokens $"}]}