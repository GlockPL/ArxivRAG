{"title": "LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models", "authors": ["Qihang Ge", "Wei Sun", "Yu Zhang", "Yunhao Li", "Zhongpeng Ji", "Fengyu Sun", "Shangling Jui", "Xiongkuo Min", "Guangtao Zhai"], "abstract": "The explosive growth of videos on streaming media platforms has underscored the urgent need for effective video quality assessment (VQA) algorithms to monitor and perceptually optimize the quality of streaming videos. However, VQA remains an extremely challenging task due to the diverse video content and the complex spatial and temporal distortions, thus necessitating more advanced methods to address these issues. Nowadays, large multimodal models (LMMs), such as GPT-4V, have exhibited strong capabilities for various visual understanding tasks, motivating us to leverage the powerful multimodal representation ability of LMMs to solve the VQA task. Therefore, we propose the first Large Multi-Modal based Video Quality Assessment (LMM-VQA) model, which introduces a novel spatiotemporal visual modeling strategy for quality-aware feature extraction. Specifically, we first reformulate the quality regression problem into a question and answering (Q&A) task and construct Q&A prompts for VQA instruction tuning. Then, we design a spatiotemporal vision encoder to extract spatial and temporal features to represent the quality characteristics of videos, which are subsequently mapped into the language space by the spatiotemporal projector for modality alignment. Finally, the aligned visual tokens and the quality-inquired text tokens are aggregated as inputs for the large language model (LLM) to generate the quality score as well as the quality level. Extensive experiments demonstrate that LMM-VQA achieves state-of-the-art performance across five VQA benchmarks, exhibiting an average improvement of 5% in generalization ability over existing methods. Furthermore, due to the advanced design of the spatiotemporal encoder and projector, LMM-VQA also performs exceptionally well on general video understanding tasks, further validating its effectiveness. Our code will be released at https://github.com/Sueqk/LMM-VQA.", "sections": [{"title": "I. INTRODUCTION", "content": "AS video consumption continues to surge across a variety of streaming media platforms, including YouTube, TikTok, Facebook, Netflix, etc, accurately evaluating the perceptual quality of videos is crucial for these video-enabled applications and services to monitor video quality throughout the entire video processing procedures and perceptually optimize video procession algorithms (e.g. video compression and enhance-ment), ultimately providing better quality of experience (QoE) for end-users. Towards this goal, several video quality assessment (VQA) methods [38] have been proposed in recent years, which can be categorized into reference-based VQA, including full-reference [2, 53] and reduced-reference VQA [15, 51], and no-reference VQA, also known as blind VQA (BVQA) [21-23, 46, 52, 54, 55, 58, 59, 61, 63, 64, 69, 70, 79]. In this paper, we focus on BVQA\u00b9 as it does not require any reference video information and has a broader range of application scenarios.\nIn the literature, BVQA is typically studied from two perspectives: the knowledge-driven approach [21, 39, 58, 59] and the data-driven approach [22, 24, 52, 61, 63, 64, 70]. Knowledge-driven BVQA methods leverage prior knowledge of visual quality perception to extract corresponding handcrafted features for quality regression. For example, natural scene statistics (NSS) [41] have been proven to be sensitive to synthetic distortions and are the most frequently used features in knowledge-driven blind image and video quality assessment studies [39, 40, 45]. Other visual descriptors, such as texture [8], noise [13], contrast [14], blockiness [80], color [48], motion vector [4], optical flow [37], etc., are also considered quality-related features and have been explored in previous studies."}, {"title": "II. RELATED WORK", "content": "Knowledge-driven BVQA models primarily utilize hand-crafted features derived through prior knowledge of visual quality perception to evaluate video quality [21, 39, 58, 59]. For instance, NIQE [41] and BRISQUE [39] first introduce natural scene statistics (NSS) for general quality predictions. Subsequently, V-BLIINDS [45] and VIIDEO [40] extend NSS to the temporal domain to address the BVQA task. TLVQM [21] integrates two-level features-spatial high-complexity and temporal low-complexity features to measure complex spatial and temporal distortions. VIDEVAL [58] ensembles diverse handcraft features sourced from classical BI/VQA models to create a strong feature representation for VQA. In sum-mary, knowledge-driven BVQA models typically incorporate temporal-related features into BIQA models or extend the 2D quality features into 3D, demonstrating the critical importance of temporal information in video quality evaluation. However, due to insufficient understanding of human perception in quality assessment, knowledge-driven BVQA models tend to underperform on in-the-wild videos.\nData-driven BVQA models have gained popularity with the advancement of DNNs, aiming to automatically learn quality-aware features from labeled video data [22, 24, 52, 61, 63, 64, 70].For example, Li et al. [24] use GRU [6] to capture the temporal relationship among the semantic features extracted by ResNet-50 [16]. Liu et al. [34] jointly optimize the feature extractor and the regressor for quality assessment and compression distortion classification. Wang et al. [61] utilizes the compression level, video content, and distortion type derived from independent networks to evaluate the quality of user-generated videos. Li et al. [22] combine the CNN-based temporal model and RNN-based temporal model to"}, {"title": "III. METHODS", "content": "For an LMM-based video quality assessment model, we define X as the video input, T as the text input, and Q as the video quality, including quality scores or levels output. Q is an approximation of the ground-truth video quality Q. Then, we define the assessment model F as follows:\n$F(X, T).$\nLarge instruction-tuned multimodal models (i.e., fine-tuned to respond to instructions) showcase an impressive ability for new tasks. Instruction prompt generation is a method for inducing instruction following capabilities with minimal human labeled data [43]. Its effectiveness has been validated through extensive instruction tuning experiments [62]. In our work, we construct Q&A instruction prompts using videos and their MOS, as illustrated in Fig. 2. The question part includes quality prompts and videos. Quality prompts can be divided into three parts: system prompts, instruction prompts, and response restrictions. System prompts delineate the role of the LMM in this task, ensuring that the LMM has a clear understanding of the task. Instruction prompts pose a direct question to the LMM, as a conversational distillation of the task description. Response restriction defines the desired format for the model's answers. A typical example of these prompts is shown in Fig. 2.\nSpecifically, we construct pair-wised Q&A instruction prompts based on the VQA benchmark datasets [17, 50, 60, 70, 72]. The basic analysis of these datasets is shown in Table II. Note that each video-score sample in VQA datasets results in two Q&A instruction prompts, separately for the quality score regression task and the quality classification task. For the quality score regression task, we take scores as the answer in Q&A pairs. For the classification task, we equally divide dataset samples into three levels\u2014'good', 'fair', and 'poor'\u2014based on their quality scores, and take these levels as the classification responses. To ensure the diversity of the training dataset, we generate 2,000 templates of instructions by GPT-4 such as: \"Would you mind calculating the video quality score with the help of these frames?\u201d. Meanwhile, the visual tokens (image-i) extracted from video chunks and key frames are inserted followed by quality prompts during our training.\nTo balance quality assessment efficacy and algorithmic complexity, we perform spatiotemporal downsampling operations during video preprocessing. Consider a video X = {x}$=1,"}, {"title": "D. Model Structure", "content": "Videos, comprised of sequences of frames, depict motion scenes that differ from static images [35]. Therefore, the characteristics of both spatial and temporal dimensions are crucial for understanding videos [33]. Previous works have relied on discrete frames of videos to extract visual features, such as Video-ChatGPT [36]. While discrete frames provide a fundamental basis for understanding videos, they do not make use of the temporal features of continuous frames, leading to overlooked issues such as jitter, lagging, flickering, etc. Thus, to address spatial and temporal distortion issues in videos, we propose a novel spatiotemporal enhanced encoder a two-branch encoder designed to enhance the capture of spatial and temporal information in videos.\nAlgorithm 1 illustrates the pipeline of LMM-VQA, starting from key frames and video chunks. Specifically, as shown in, for the spatial branch, a 2D visual encoder (ViT-L/14 [44] in Fig. 5) transforms the input key frames Y into spatial features Fsp:\n$F_{sp} = f_{sp}(Y), F_{sp} \\in \\mathbb{R}^{K\\times N_p\\times C_{sp}},$\nwhere Np = H/p\u00d7 W/p indicate the number of image patches and and Csp is the number of spatial embedding dimensions, respectively. For the temporal branch, a 3D visual encoder (SlowFast in Fig. 5) extracts temporal features Ftp:\n$F_{tp} = f_{tp}(V), F_{tp} \\in \\mathbb{R}^{K\\times 1 \\times C_{tp}},$\nwhere Ctp is the number of temporal embedding dimensions.\nTo address the disparity between vision and text modalities, we design a spatiotemporal projector to map visual tokens into language space for modality alignment, as illustrated in Fig. 5. Specifically, we introduce a spatial projector which comprises a single-layer vision transformer (ViT) and a one-layer MLP, to map spatial features into the language space. Following this, we use a one-layer MLP as a temporal projector to align temporal features with the language space. We formulate these processes as:\n$Z_{sp} = f_{ViT} (F_{sp}),$\n$Z_{tp} = Mean (f_{MLP} (F_{tp})).$\nA text decoder generates scalable text tokens Ftext from quality prompts T (i.e., LLM embedding layer in Fig. 5). Spatial tokens Zsp, temporal tokens Ztp and text tokens Ztext are concatenated in the context length dimension fcat, defined as Zall:\n$Z_{all} = f_{cat} (Z_{sp}, Z_{tp}, Z_{text}).$\nThese text-guided visual tokens and quality prompt tokens are aggregated to generate the final output tokens, which represent the predicted quality scores or levels.\nWe utilize the Llama-3 model [9] as the language decoder and output the predicted answers in an auto-regressive manner:\n$Z_{quality} = D (Z_e | Z_{all}, Z_{<e}),$\nwhere Zquality = {z}=0\u00b9 is the output text sequence follows the input tokens Zall and D(\u00b7) is the Llama-3 decoder. z\u03b9 \u2208 {0,1}|s and S denote the vocabulary set. With well-trained LMM-VQA, Zquality represents an approximation Q to the"}, {"title": "IV. EXPERIMENT", "content": "We utilize Llama-3-8b-Instruct [9] as the decoder, clip-vit-large-L/14 [44] and SlowFast [11] as the spatial and temporal encoders, respectively. We utilize paired Q&A instruction prompts as our training data. During the training phase, the parameters of both the decoder and the vision encoders are frozen. We only train the spatial and temporal projectors to align modalities between visual and text tokens. Training experiments are conducted on eight NVIDIA A800 GPUs, while inference is performed on two NVIDIA A6000 GPUs. The batch size is set as 32, with a learning rate of 0.001, and training epochs are set as 6 to achieve convergence. The video input resolution H \u00d7 W is 224 \u00d7 224 and the patch size p is typically set to 14 for the ViT-L/14 backbone. The length of video chunk is configured to equal the video frame rate, i.e., one video chunk and one key frame are sampled per second.\n We select Spearman Rank-Order Correlation Coefficient (SRCC), and Pearson Linear Correlation Coefficient (PLCC) as metrics for performance evaluation, which indicate the prediction monotonicity and prediction accuracy. It should be noted that SRCC and PLCC range from 0 to 1.0, where larger values indicate better results.\n For instruction tuning and evaluation, we construct our Q&A instruction prompts based on five datasets: LSVQ [70], KoNViD-1k [17], YouTube-UGC [60], LIVE-VQC [50], LIVE-YT-Gaming [72], with details provided in Table II. LSVQ, the largest VQA dataset in our study, comprises 38,811 videos, covering a wide range of resolutions and frame rates. KoNViD-1K consists of 1,200 videos of fixed resolution sourced from the Internet, primarily focusing on spatial distortions. LIVE-VQC includes 585 videos captured directly by various mobile cameras, showcasing a broad range"}, {"title": "V. CONCLUSION AND FUTURE DISCUSSIONS", "content": "In this paper, we present LMM-VQA, an LMM-based VQA model that aims at better modeling spatial and temporal relationships to predict accurate scores and levels for VQA. LMM-VQA takes video frames as input, and generates the responses of quality scores with the quality levels. The process initiates with two vision encoders that separately transform input frames into spatial and temporal features. To better capture temporal distortions and align different modalities, we design two spatial and temporal projectors to integrate visual tokens into the language space for modality alignment. We utilize Llama-3 as our language decoder, the aligned visual tokens and quality prompt tokens are aggregated as input for Llama-3 to generate the final answers. The proposed LMM-VQA has reached SOTA performance on five popular VQA datasets and achieved excellent generalization ability among them."}]}