{"title": "Doubly Stochastic Adaptive Neighbors Clustering via the Marcus Mapping", "authors": ["Jinghui Yuan", "Chusheng Zeng", "Fangyuan Xie", "Zhe Cao", "Rong Wang", "Feiping Nie", "Xuelong Li"], "abstract": "Clustering is a fundamental task in machine learning and data science, and similarity graph-based clustering is an important approach within this domain. Doubly stochastic symmetric similarity graphs provide numerous benefits for clustering problems and downstream tasks, yet learning such graphs remains a significant challenge. Marcus theorem states that a strictly positive symmetric matrix can be transformed into a doubly stochastic symmetric matrix by diagonal matrices. However, in clustering, learning sparse matrices is crucial for computational efficiency. We extend Marcus theorem by proposing the Marcus mapping, which indicates that certain sparse matrices can also be transformed into doubly stochastic symmetric matrices via diagonal matrices. Additionally, we introduce rank constraints into the clustering problem and propose the Doubly Stochastic Adaptive Neighbors Clustering algorithm based on the Marcus Mapping (ANCMM). This ensures that the learned graph naturally divides into the desired number of clusters. We validate the effectiveness of our algorithm through extensive comparisons with state-of-the-art algorithms. Finally, we explore the relationship between the Marcus mapping and optimal transport. We prove that the Marcus mapping solves a specific type of optimal transport problem and demonstrate that solving this problem through Marcus mapping is more efficient than directly applying optimal transport methods.", "sections": [{"title": "Introduction", "content": "Clustering data with complex structure is an essential problem in data science, the goal of which is to divide the given samples into different categories. Graph-based clustering has been attracting increasing attention due to its ability to capture the intrinsic relationships between data points, allowing for more accurate and meaningful clustering results (Ju et al. 2023). It has been also extensively applied in classification, segmentation, protein sequences analysis and so on (Huang et al. 2023; Barrio-Hernandez et al. 2023). Spectral clustering (SC) is a typical graph-based clustering method whose core idea is to construct a similarity graph and partition the samples by cutting this graph. Over the past decades, there have been many significant works on spectral clustering (Nie et al. 2011; Bai and Liang 2020; Zong et al. 2024). According to different normalization approach, SC could be divided into ratio cut (Rcut) and normalized cut (Ncut). However, if the similarity matrix is doubly stochastic, the results of Rcut and Ncut are actually the same. The doubly stochastic affinity graph could greatly benefit clustering performance, which could also contributes to dimensionality reduction (Van Assel et al. 2024), transformers (Sander et al. 2022) and reaction prediction (Meng et al. 2023). Meanwhile, (Zass and Shashua 2006) highlight the positive impact of doubly stochastic matrices on clustering results and subsequent tasks. (Ding et al. 2022) establishes conditions under which projecting an affinity matrix onto doubly stochastic matrices improves clustering by ensuring ideal cluster separation and connectivity. Consequently, extending clustering algorithms to incorporate doubly stochastic matrices have become a hot topic. The key issue lies in how to obtain a doubly stochastic matrix. Some researchers have proposed several methods to address this issue. For instance, (Zass and Shashua 2005) propose a method to transform a non-negative matrix into a doubly stochastic matrix. They believe that by iterating the process $K^{(t+1)} \\leftarrow D^{-1/2}K^tD^{-1/2}$ with $D = diag(K^t\\mathbb{1})$, the sequence converges to a doubly stochastic matrix. However, this method suffer from slow convergence, numerical instability, sensitivity to initialization, and high computational complexity for large matrices. (Zass and Shashua 2006) present a Frobenius-optimal doubly stochastic normalization method via Von-Neumann successive projection and apply it into SC. (Marcus and Newman 1961) propose a similar theorem, stating that any positive symmetric matrix can be diagonalized by multiplying it on the left and right with the same diagonal matrix. Therefore, it is valuable to explore and extend Marcus's theory, providing good conditions and algorithms for this transformation. Optimal transport theory has gained significant attention in recent years due to its wide applicability in various fields. (Villani 2003) laid the foundational mathematical framework for optimal transport, focusing on the cost of transporting mass in a way that minimizes the overall transportation cost. (Cuturi 2013) revolutionized the field by introducing the Sinkhorn algorithm (Sinkhorn and Knopp 1967), which employs entropy regularization to make the computation of optimal transport more efficient and scalable to high-dimensional data. (Peyr\u00e9, Cuturi et al. 2019) further developed algorithms for computational optimal transport, enhancing its practicality for large-scale problems. However, the relationship between clustering and optimal transport has been relatively unexplored (Yan et al. 2024). Introducing the concept of optimal transport into clustering is valuable and meaningful. (Nie et al. 2016) proposed the rank constraint, which states that if the Laplacian matrix of the learned similarity matrix has a rank of $n-c$, then the similarity matrix has exactly $c$ connected components. The rank constraint clustering has also applied in (Nie et al. 2017; Wang et al. 2022). However, the learned matrix is generally not a doubly stochastic symmetric matrix, which means it differs from a probability matrix. In this paper, we propose the Adaptive Doubly Stochastic Clustering algorithm (ADCMM) based on our proposed Marcus mapping. Our method can learn symmetric doubly stochastic similarity matrix, also known as probability matrices, which naturally have exactly $c$ connected components, allowing direct determination of clustering results without post-processing. Besides, we extend Marcus theorem by introducing a more relaxed constraint, proposing the Marcus mapping and proving that it can transform certain sparse non-negative matrices into symmetric doubly stochastic matrices through diagonal matrices. Additionally, we explore the relationship between the Marcus mapping and optimal transport, proving that the Marcus mapping solves a specific optimal transport problem more efficiently than directly using optimal transport methods. We summarize our main contributions below:\n\u2022 We propose the Marcus mapping theorem, the conditions for which have been rigorously proven. An iterative method to compute the Marcus mapping theorem is also provided. This extends the Marcus theorem by relaxing its requirement for positive matrices.\n\u2022 We explore the relationship between the Marcus mapping algorithm and optimal transport, proving that the Marcus mapping algorithm is a special case of optimal transport. We demonstrate that computations using the Marcus mapping algorithm are more efficient compared to those using optimal transport.\n\u2022 We propose the Doubly Stochastic Adaptive Neighbors Clustering method based on the Marcus Mapping (ANCMM) and design an optimization algorithm to solve this problem. The convergence of proposed method are proven theoretically and experimentally. We validate the effectiveness of our method through extensive comparative experiments on synthetic and real-world datasets."}, {"title": "Methodology", "content": "In this section, we first introduce the notations used in the paper, then proceed to describe and prove the generalized Marcus mapping and adaptive neighbors. Finally, we present the optimization problem to be solved."}, {"title": "Notations", "content": "Throughout this paper, data matrix are denoted as $X \\in \\mathbb{R}^{n\\times d}$, where $n$ is the number of samples, and $d$ is the number of features $x_i \\in \\mathbb{R}^d$ is the $i$-th samples and also the transpose of the $i$-th row of $X$. $\\mathbb{1}_n$ or $\\mathbb{I}$ denotes a vector in the space $\\mathbb{R}^n$ with all the elements being 1. $\\pi$ represents a full permutation. For example, given a permutation rule $\\pi$, {$\\pi(1)$, $\\pi(2)$, ..., $\\pi(n)$ } is just {1, 2, ..., n} but $\\pi(1)$, $\\pi(2)$, ..., $\\pi(n)$ and 1, 2, ..., n are in a different order. For matrix $S$, $diag(S, 1)$ represents the elements on the superdiagonal, i.e., the diagonal shifted up by one position. $diag(S, 2)$ represents the elements on the second superdiagonal, i.e., the diagonal shifted up by two positions. $Tr(S)$ represents the trace of $S$, and $||S||_F$ represents the Frobenius norm of $S$."}, {"title": "Marcus Mapping", "content": "In this section, we will introduce the Marcus mapping. The Marcus mapping is a generalized form of the Marcus theorem, which is shown below.\nMarcus theorem (Marcus and Newman 1961) If S is symmetric and has positive entries there exists a diagonal matrix D with positive main diagonal entries such that DSD is doubly stochastic. In other words, given a symmetric matrix $S \\in \\mathbb{R}^{n\\times n}$, we could find a diagonal matrix D such that\n$M(S) = DSD$   (1)\nand the $M(S) \\in {Z\\in \\mathbb{R}^{n\\times n}|Z\\mathbb{1}_n = \\mathbb{1}_n, Z_{ij} \\geq 0, Z = Z^T}$. (Ron Zass and Amnon Shashua 2005) has a similar idea, they believe that any non-negative symmetric matrix S can be transformed into a doubly stochastic matrix by iterative left and right multiplication with a degree matrix $D^{-\\frac{1}{2}}$ where $D = diag(S\\mathbb{1}_n)$. However, this idea is flawed. In fact, we can provide an example where the matrix S, as shown below, cannot be transformed into a doubly stochastic matrix regardless of the choice of any diagonal matrix D.\n$S = \\begin{pmatrix} 0 & 1 & 1 \\\\ 1 & 0 & 1\\\\ 1 & 1 & 0 \\end{pmatrix}$   (2)\nTherefore, we introduce a weaker condition here and utilize the Sinkhorn theorem to prove that under this weaker condition, the Marcus theorem holds true. We also provide an iterative algorithm for solving the Marcus mapping. We will prove that the algorithm we propose satisfies this very weak condition and explain the Marcus mapping through optimal transport theory.\nTheorem 1.(Marcus mapping theorem) For a symmetric no-negative matrix S, if its subdiagonals $diag(S, 1) \\neq 0$ and second superdiagonals $diag(S, 2) \\neq 0$, it can be transformed into a doubly stochastic matrix $M(S) = DSD \\in {Z\\in \\mathbb{R}^{n\\times n}|Z\\mathbb{1}_n = \\mathbb{1}_n, Z_{ij} \\geq 0, Z = Z^T}$ using the Marcus mapping by a diagona matrix D. To prove this theorem, we first need to introduce the definition of $\\pi$-diagonals and the Sinkhorn theorem."}, {"title": "Definition 1.(\u03c0-diagonals)", "content": "If $S \\in \\mathbb{R}^{n\\times n}$ and $\\pi$ is a permutation of {1,2,...,n}, then the sequence of {$s_{1,\\pi(1)}$, $s_{2,\\pi(2)}$, ..., $s_{n,\\pi(n)}$ } is called the $\\pi$-diagonal of S corresponding to $\\pi$. S is said to have total support if S \u2260 0 and if every positive element of Z lies on a positive $\\pi$-diagonal. A non-negative matrix that contains a positive $\\pi$-diagonal is said to have support. By definition, we know that if $\\pi$ is the identity, the $\\pi$-diagonal is called the main diagonal. The content of the Sinkhorn theorem is shown as follows.\nTheorem 2.(Sinkhorn theorem) If $S\\in \\mathbb{R}^{n\\times n}$ is nonnegative, a necessary and sufficient condition that there exist a doubly stochastic matrix $\\hat{S}$ of the form $\\hat{S} = D_1SD_2 \\in {Z\\in \\mathbb{R}^{n\\times n}|Z\\mathbb{1}_n = \\mathbb{1}_n, Z_{ij} \\geq 0, Z = Z^T}$, where $D_1$ and $D_2$ are diagonal matrices with positive main diagonals is that S has total support. If $\\hat{S}$ exists, then it is unique. Now we will prove the Marcus mapping theorem. The core of the proof lies in demonstrating that a no-negative matrix satisfying condition $diag(S, 1) \\neq 0$ and $diag(S, 2) \\neq 0$ is also total support.\nProof For a matrix S with positive elements along the $diag(S, 1)$ and $diag(S, 2)$, we first prove that it is a support matrix: If n is even, we can choose the following permutation:\n$\\pi(i) = i - 1, i = 2k$   (3)\n$\\pi(i) = i + 1, i = 2k + 1$\nwe choose the $\\pi$-diagonal such that\n$s_{1,\\pi(1)}, s_{2,\\pi(2)}, ..., s_{n-1,\\pi(n-1)}, s_{n,\\pi(n)}$\n$= s_{1,2}, s_{2,1}, ..., s_{n-1,n}, s_{n,n-1}$   (4)\nFor n being odd, we can choose another type of $\\pi$-diagonal\n$\\pi(1) = 2, \\pi(2) = 3, \\pi(3) = 1$\n$\\pi(i) = i + 1, (i = 2k and i > 3)$   (5)\n$\\pi(i) = i - 1, (i = 2k - 1 and i > 3)$\nand so that\n$s_{1,\\pi(1)}, s_{2,\\pi(2)}, s_{3,\\pi(3)}, s_{4,\\pi(4)}, s_{5,\\pi(5)}, ..., s_{n-1,\\pi(n-1)}, s_{n,\\pi(n)}$\n$= s_{1,2}, s_{2,3}, s_{3,1}, s_{4,5}, s_{5,4}, \u2026, s_{n-1,n}, s_{n,n-1}$   (6)\nThus, we have shown that regardless of whether n is odd or even, there exists a positive $\\pi$-diagonal, implying that the matrix is definitely support. Considering\n$\\forall s_{i,j} \\geq 0 \\neq diag(S, 1) \\cup diag(S, 2)$   (7)\nwe will prove that $s_{p,q}$ appears on a positive $\\pi$-diagonal. In fact, whether n is odd or even, we can find such a permutation $\\pi$ such that $s_{p,q}$ appears on the $\\pi$-diagonal. Without loss of generality, we assume p, q, and n are even. Previously proved that regardless of being odd or even, there exists a $\\pi$ permutation that places them on the positive $\\pi$-diagonal. Specifically, for the odd parts, construct the permutation $\\pi$ according to Eq.(5), and for the even parts, construct the permutation $\\pi$ according to Eq.(3). Then combine the two disjoint sub-permutations to form the complete permutation. Since S is symmetric, this $\\pi$-diagonal must be positive. Therefore, the matrix S is total support,so it can be doubly stochasticized by two diagonal matrices. Given that S is symmetric, these two diagonal matrices $D_1 = D_2$. Under the conditions that satisfy the Marcus mapping theorem(S is no-negative and $diag(S, 1) \\neq 0$, $diag(S, 2) \\neq 0$), the Marcus mapping must exist. We propose the following Marcus algorithm to compute the results obtained from the Marcus mapping."}, {"title": "Algorithm 1: Marcus mapping algorithm", "content": "Input: Symmetric matrix $S \\in \\mathbb{R}^{n\\times n}$\nOutput: Doubly stochastic $M(S)$\n1: Initialize $u \\in \\mathbb{R}^{n\\times 1}$\n2: while not converge do\n3: $u = 1./(Su)$\n4: end while\n5: $D = diag(u)$\n6: $S = DSD$\n7: $M(S) = S/(\\Sigma_i S_{i:})$"}, {"title": "Adaptive Local doubly stochastic Structure Learning", "content": "In graph-based learning, an important aspect is to find a low-dimensional local manifold representation, as high-dimensional data generally contains low-dimensional manifolds. Therefore, selecting an appropriate similarity graph is a crucial requirement for graph-based learning. A reasonable objective function for graph learning is shown below (Nie, Wang, and Huang 2014).\n$\\min_{S\\in \\mathbb{R}^{n\\times 1}} \\sum_{i,j}||x_i - x_j||_1^2 S_{ij} + \\alpha||S||_*$\ns.t. $\\forall i, S^T\\mathbb{I} = \\mathbb{I}, 0 \\leq S_{ij} \\leq 1$   (8)\nIn the research process, $s_{ij}$ is usually considered the probability that $x_i$ and $x_j$ belong to the same class. However, the solution to this problem is not a doubly stochastic matrix, which cannot reasonably express the concept of probability."}, {"title": "Therefore, we impose a stronger constraint on problem (8)", "content": "$\\min_{S\\in \\mathbb{R}^{n\\times 1}} \\sum_{i,j}||x_i - x_j||_1^2 S_{ij} + \\alpha||S||_*$\ns.t. $\\forall i, S^T\\mathbb{I} = \\mathbb{I}, 0 \\leq s_{ij} \\leq 1, S = S^T$   (9)\nTypically, the learned probability matrix S does not have c connected components, requiring further clustering using methods like K-means. Due to the following theorem, we introduce the Laplacian rank constraint.\nTheorem 3. For a matrix S, if the rank of the Laplacian matrix $L_s = n \u2013 c$, then S has exactly c connected components. Hence, the Laplacian rank constraint is introduced in, and the final form of the loss function becomes:\n$\\min_{S\\in \\mathbb{R}^{n\\times 1}} \\sum_{i,j}||x_i - x_j||_1^2 S_{ij} + \\alpha||S||_*$\ns.t. $S^T\\mathbb{I} = \\mathbb{I}, 0 \\leq s_{ij} \\leq 1, S = S^T, rank(L_s) = n - c$   (10)\nProblem (10) is our objective function. We have designed an optimization algorithm to solve this problem."}, {"title": "Optimization Algorithm", "content": "Optimizing problem (10) is more challenging because the matrix constraints are coupled. However, we can cleverly solve this problem by iteratively solving the relaxed problem and using the Marcus mapping."}, {"title": "Clustering", "content": "Assume $\\sigma_i(L_s)$ represents the i-th smallest eigenvalue of $L_s$. Since $L_s$ is a positive semidefinite matrix, $\\sigma_i(L_s) \\geq 0$. Because solving for the constraint $rank(L_s) = n c$ is too difficult, we relax it to $\\sum_{i=1}^{c} \\sigma_i(L_s) = 0$, which means $\\sigma_i(L_s) = 0 \\forall i = 1...c$. According to Ky Fan's Theorem (ky Fan et al.1949), we have\n$\\sum_{i=1}^{c} \\sigma_i(L_s) = \\min_{F\\in \\mathbb{R}^{n\\times c}, F^T F=I} Tr (F^T L_sF)$   (11)\nThen problem (10) is equivalent to the following problem\n$\\min_{S\\in \\mathbb{R}^{n\\times 1}} \\sum_{i,j}||x_i - x_j||_1^2 S_{ij} + \\alpha||S||_* + 2\\lambda Tr(F^T L_sF)$\ns.t. $\\forall i, S^T\\mathbb{I} = \\mathbb{I}, 0 \\leq s_{ij} \\leq 1, S = S^T, F^T F = I$   (12)\nWe selected a sufficiently large $\\lambda$, so that in the optimization process, $\\sum_{i=1}^{c} \\sigma_i(L_s) = 0$ can be obtained, thereby solving the relaxed constraint."}, {"title": "Fix S, update F", "content": "Once S is fixed, updating F only requires solving the following problem\n$\\min_{F\\in \\mathbb{R}^{n\\times c}, F^T F=I} Tr (F^T L_sF)$   (13)\nThe optimal solution for F is the eigenvectors corresponding to the smallest c eigenvalues of $L_s$. This is part of Ky Fan's theorem."}, {"title": "Fix F, update S", "content": "Due to the following important equation\n$\\sum_{i,j}|| f_i - f_j|| S_{ij} = 2Tr(F^T L_sF)$   (14)\nwe are essentially required to solve the following problem:\n$\\min_{S\\in \\mathbb{R}^{n\\times 1}} \\sum_{i,j}(||x_i - x_j||_1^2 + \\lambda||f_i - f_j||_1^2)s_{ij} + \\alpha||S||_*$\ns.t. $\\forall i, S^T\\mathbb{I} = \\mathbb{I}, 0 \\leq s_{ij} \\leq 1, S = S^T$   (15)\nSolving this problem is very challenging. Our key idea is to first solve the relaxed problem and then map it to a symmetric doubly stochastic matrix using the Marcus mapping.In this section, we first consider the relaxed problem (15).\n$\\min_{S\\in \\mathbb{R}^{n\\times 1}} \\sum_{i,j}(||x_i - x_j||_1^2 + \\lambda||f_i - f_j||_1^2)s_{ij} + \\alpha||S||_*$\ns.t. $\\forall i, S^T\\mathbb{I} = \\mathbb{I}, 0 \\leq s_{ij} \\leq 1$   (16)\nDenote $m_{ij} = ||x_i - x_j||_1^2 + \\lambda||f_i - f_j||_1^2$ and $m_i \\in \\mathbb{R}^{n\\times 1}, (m_i)_j = m_{ij}$,Because we relaxed the constraints and decoupled the relationships between rows and columns, we can solve each row independently,Expanding the original problem and simplifying, we ultimately reduce it to the following form, which has a closed-form solution.\n$\\min_{S_i} ||\\frac{1}{2 \\alpha}S_i + M_i ||_2^2$\ns.t. $S^T\\mathbb{I} = \\mathbb{I}, 0 \\leq s_{ij} \\leq 1$   (17)\nBy solving this problem, we obtain the optimal solution S* after relaxation. We transform it into a symmetric matrix using the following symmetrization to meet the conditions for the Marcus mapping.\n$S \\leftarrow \\frac{S^* + S^{*,T}}{2}$   (18)\nWe will later prove two things: 1. Prove that it is easy to satisfy the conditions of the Marcus mapping by choosing $\\alpha$. 2. Prove that the solution obtained through Marcus mapping is sufficiently close to the true solution of problem (14)."}, {"title": "Fix F, update M(S)", "content": "At this step, we utilize the Marcus mapping to compute M(S). Specifically, we input $\\hat{S}^*$ into the Marcus mapping and aim to obtain\n$S \\leftarrow M(\\hat{S}^*) = D\\hat{S}D = D\\frac{S^* + S^{*,T}}{2}D$   (19)\nAfter calculating $S = M(\\hat{S}^*)$, we substitute it into the first step and iterate through various steps until convergence. We summarize our optimization algorithm in Algorithm 2."}, {"title": "Select \u03b1 based on sparsity", "content": "In this part, we will demonstrate how the selection of $\\alpha$ affects the sparsity of the probability matrix S. Additionally,"}, {"title": "Algorithm 2: ANCMM", "content": "Input: Data matrix $X \\in \\mathbb{R}^{n\\times d}$, clusters c\nParameter: $\\alpha$ and $\\lambda$\nOutput: Desired similarity graph $S\\in \\mathbb{R}^{n\\times n}$ with c connected components\n1: Initialize S\n2: while not converge do\n3: Fix S and update F by Eq.13\n4: Fix F and update S by Eq.15\n5: Symmetrize matrix S by Eq.16\n6: Update S by Marcus mapping using Algorithm 1\n7: end while\nwe will prove that by simply choosing $\\alpha$ we can satisfy the conditions required by the Marcus mapping. We still consider the relaxed problem16 first because if the relaxation problem's $\\alpha$ can control the sparsity of the matrix S, the non-relaxed one can too. Since we update each row individually, (Nie et al.2016) has proven that the closed-form solution has the following form.\n$S_{ij} = (\\frac{m_{ij}}{\\tau} - \\xi)_+$   (20)\nwhere $\\phi = \\frac{1}{\\tau} + 2 \\sum_{j=1}^{k} m_{ij}$ (Nie, Wang, and Huang 2014). That $x_i$ have k neighbours $\\rightarrow S_{ij} > 0, \\forall 1 \\leq j \\leq k$ and $S_{i,k+1} = 0$. Here, we assume $s_{ij}$ is sorted in descending order with respect to j. According to (20) and the value of $\\phi$ we have\n$\\xi = \\frac{1}{2 \\tau}(\\frac{1}{k} \\sum_{j=1}^{k} m_{ij} < \\alpha < \\frac{1}{k+1} \\sum_{j=1}^{k} m_{ij})$   (21)\nwhere $(m_{ij})_i = M_{i1}, M_{i2}, \u2026, M_{in}$ are also sorted in descending order with respect to j. This means that by controlling $\\alpha_i$ for each row, we can precisely control the sparsity of each row. Now we demonstrate that we can easily satisfy the conditions required by the Marcus mapping by selecting $\\alpha$. First, we choose $k \\geq 2$, meaning each row has at least two or more elements that are not zero, which means\n$\\alpha > \\max_{i\\in{1...n}} \\alpha_i = \\max_{i\\in{1...n}} (\\frac{M_{i,1} + M_{i,2}}{2}) \t{ }$   (22)\nBy choosing $\\alpha$, we can ensure that each row of $S^*$ has at least two or more elements that are not zero. Since each element of $S^*$ is greater than zero, $\\hat{S}^* = \\frac{S + S^T}{2}$ also has at least two or more elements that are not zero in each row. Next, we perform simultaneous row and column exchanges on the symmetric matrix $\\hat{S}^*$(a congruence transformation of the matrix), ensuring that the superdiagonal $diag(\\hat{S}^*, 1)$ and second superdiagonal $diag(\\hat{S}^*, 2)$ elements are not zero. Note that this only changes the order of the samples and does not affect the clustering results. Through row and column exchanges, we can satisfy the conditions required by the Marcus mapping."}, {"title": "Approximation Analysis", "content": "In this section, we will demonstrate that $M(\\hat{S}^*)$ is sufficiently close to the true solution of Problem (15). For problem (8), due to the Karush-Kuhn-Tucker (KKT) conditions, there exist 0 and $\\gamma$ such that:\nM + 2\\theta S^* + \\gamma \\mathbb{I}^T = 0   (23)\nwhere $M_{ij} = m_{ij}, \\theta \\geq 0$ and $\\gamma \\in \\mathbb{R}^n$, Similarly, considering the transpose problem of Problem (8), we have:\nM + 2\\theta S^{*T} + I\\gamma^T = 0   (24)\nWe know that\nM(\\hat{S}^*) = D(\\frac{S^* + S^{*,T}}{2})D$\n$\\frac{DSD + DSD^T}{2} = \\frac{1}{2} (DSD + DS^TD)$   (25)\nIf $DS^*D, DS^{*,T}D$ are the optimal solutions to Problem (8) and the transpose Problem, respectively, then according to Eq.(23) and Eq.(24), $M(\\hat{S}^*)$ is the optimal solution to Problem (9). However, this is usually not the case. Nevertheless, we can have the following estimation:\n$||M(\\hat{S}^*) \u2013 \\hat{S}^*|| \\leq \\frac{1}{2}(||DSD - S|| + ||DS^TD - S^T||) $   (26)\n$= ||DSD \u2013 S^*||< \\epsilon$\nUsually, $\\epsilon$ is very small relative to $||S^*||$, making it an acceptable approximation. Time Complexity Analysis\nTime complexity comparison. Indeed, in some cases, defining the degree matrix as $D = diag(S\\mathbb{1}_n)$. and continuously performing the following operations(Ron Zass et al.2005)\nS\\leftarrow DS^{ -\\frac{1}{2}}DS^{ -\\frac{1}{2}}$   (27)\ncan also transform S into a doubly stochastic matrix. However, through this method, we need to perform $n^2$ additions, n square roots, n divisions, and $2n^2$ multiplications each round, far exceeding the $n^2$ additions, $n^2$ multiplications and n divisions of the Marcus mapping. If we only consider multiplication, the Marcus mapping takes about half the time compared to the above method. Time complexity analysis. When updating F by Eq. (13), c smallest eigenvalues need to be computed, Using the Lanczos algorithm can achieve a time complexity that is linear with respect to O(n). Then, the corresponding eigenvectors need to be computed, resulting in a time complexity of $O(k_2n^2)$, $k_2$ represents the number of iterations. When updating S by the Eq. (17), only n linear equations need to be solve and a matrix addition needs to be calculated, resulting in a time complexity of O($n^2$). Then we calculate Marcus mapping by Algorithm 1, which only requires computing the multiplication of a matrix by a vector and the division of a scalar by a vector, resulting in a time complexity of $O(k_1n^2)$. Here, $k_1$ represents the number of iterations. Therefore, the overall time complexity is $O(k(k_1 + k_2)n^2)$, k represents the total number of iterations."}, {"title": "Connection with Optimal Transport", "content": "In this section, we will point out the connection between the Marcus mapping and optimal transport. Donate the Marcus mapping for a non-negative matrix S as $M(S)$, $\\log(S)$represents the natural logarithm of each element of S, and we require $\\log(0) = -\\infty$ and $e^{-\\infty} = 0$. Consider the optimal transport problem with entropy regularization:\n$\\min_{P \\in \\Omega(\\mathbb{I}, \\mathbb{I})}  < P, -log(S) > - \\frac{1}{\\omega} \\sum P_{ij}log(P_{ij})$\ns.t.   (28)\nwhere $\\Omega(\\mathbb{I}, \\mathbb{I}) = {Z|Z_{ij} \\geq 0, Z\\mathbb{I} = \\mathbb{I}, Z^T\\mathbb{I} = \\mathbb{I}}$ The Lagrangian function of the optimal transport problem is given by\n$L(P, \\phi, \\xi) = \\sum -P_{ij}log(S_{ij}) - \\frac{1}{\\omega} P_{ij}log(P_{ij}) + \\phi^T (P\\mathbb{I}_n \u2013 \\mathbb{I}_n) + \\xi^T (P^T\\mathbb{I}_n - \\mathbb{I}_n)$   (29)\nBy taking the partial derivative with respect to P, we can obtain that the optimal solution satisfies:\nP_{ij} (\\omega) = e^{ - \\omega \\phi_i } e^{ - \\omega \\xi_i}   (30)\nIn particular, when w is 1, the form of the optimal transport solution is consistent with the Marcus mapping.\nP_{ij}|_{\\omega=1} = e^{-\\phi_i} S_{ije}^{-\\xi_i}   (31)\nSince S is symmetric, because\n$\\min_{P \\in \\Omega(\\mathbb{I}, \\mathbb{I})} < P, -log(S) > - \\frac{1}{\\omega} \\sum P_{ij}log(P_{ij}))$   (32)\nis also symmetric, so P has the form $P = DSD$. This demonstrates that the Marcus mapping is solving a special optimal transport. The transformation of $-\\log(S)$ is crucial as it can convert zeros in the probability matrix to positive values, thereby satisfying the conditions for optimal transport. However, computing through the Marcus mapping is much more efficient than directly using optimal transport, primarily because optimal transport involves additional logarithmic computations, and expressing +\u221e in a computer is challenging."}, {"title": "Experiments on toy datasets", "content": "To demonstrate the superiority of our algorithm, we first constructed a toy dataset in the shape of double moons. We set the sample size to 200, with a random seed of 1 and noise level of 0.13. We performed clustering using both the CAN and ANCMM algorithms, both of which incorporate rank constraints and are adaptive neighbors algorithms. We compared their accuracy and the configuration of adaptive neighbors. We used the learned probability matrix weights as the edge weights connecting each pair of points.Additionally, we demonstrate the similarity matrix obtained solely from"}, {"title": "Experiments on real datasets", "content": "Experimental Settings Datasets. We conducted extensive experiments on ten datasets", "dataset": "Compared Methods. To demonstrate the superiority of our algorithm ANCMM, we compare it with various state-of-the-art algorithms. We chose the K-Means algorithm, Spectral Clustering(SC) algorithm, CAN algorithm(Nie et al. 20"}]}