{"title": "Doubly Stochastic Adaptive Neighbors Clustering via the Marcus Mapping", "authors": ["Jinghui Yuan", "Chusheng Zeng", "Fangyuan Xie", "Zhe Cao", "Rong Wang", "Feiping Nie", "Xuelong Li"], "abstract": "Clustering is a fundamental task in machine learning and data science, and similarity graph-based clustering is an important approach within this domain. Doubly stochastic symmetric similarity graphs provide numerous benefits for clustering problems and downstream tasks, yet learning such graphs remains a significant challenge. Marcus theorem states that a strictly positive symmetric matrix can be transformed into a doubly stochastic symmetric matrix by diagonal matrices. However, in clustering, learning sparse matrices is crucial for computational efficiency. We extend Marcus theorem by proposing the Marcus mapping, which indicates that certain sparse matrices can also be transformed into doubly stochastic symmetric matrices via diagonal matrices. Additionally, we introduce rank constraints into the clustering problem and propose the Doubly Stochastic Adaptive Neighbors Clustering algorithm based on the Marcus Mapping (ANCMM). This ensures that the learned graph naturally divides into the desired number of clusters. We validate the effectiveness of our algorithm through extensive comparisons with state-of-the-art algorithms. Finally, we explore the relationship between the Marcus mapping and optimal transport. We prove that the Marcus mapping solves a specific type of optimal transport problem and demonstrate that solving this problem through Marcus mapping is more efficient than directly applying optimal transport methods.", "sections": [{"title": "Introduction", "content": "Clustering data with complex structure is an essential problem in data science, the goal of which is to divide the given samples into different categories. Graph-based clustering has been attracting increasing attention due to its ability to capture the intrinsic relationships between data points, allowing for more accurate and meaningful clustering results (Ju et al. 2023). It has been also extensively applied in classification, segmentation, protein sequences analysis and so on (Huang et al. 2023; Barrio-Hernandez et al. 2023).\nSpectral clustering (SC) is a typical graph-based clustering method whose core idea is to construct a similarity graph and partition the samples by cutting this graph. Over the past decades, there have been many significant works on spectral clustering (Nie et al. 2011; Bai and Liang 2020; Zong et al. 2024). According to different normalization approach, SC could be divided into ratio cut (Rcut) and normalized cut (Ncut). However, if the similarity matrix is doubly stochastic, the results of Rcut and Ncut are actually the same. The doubly stochastic affinity graph could greatly benefit clustering performance, which could also contributes to dimensionality reduction (Van Assel et al. 2024), transformers (Sander et al. 2022) and reaction prediction (Meng et al. 2023).\nMeanwhile, (Zass and Shashua 2006) highlight the positive impact of doubly stochastic matrices on clustering results and subsequent tasks. (Ding et al. 2022) establishes conditions under which projecting an affinity matrix onto doubly stochastic matrices improves clustering by ensuring ideal cluster separation and connectivity. Consequently, extending clustering algorithms to incorporate doubly stochastic matrices have become a hot topic. The key issue lies in how to obtain a doubly stochastic matrix. Some researchers have proposed several methods to address this issue.\nFor instance, (Zass and Shashua 2005) propose a method to transform a non-negative matrix into a doubly stochastic matrix. They believe that by iterating the process $K^{(t+1)} \\leftarrow D^{-1/2}K^{t}D^{-1/2}$ with $D = diag(K^{t}1)$, the sequence converges to a doubly stochastic matrix. However, this method suffer from slow convergence, numerical instability, sensitivity to initialization, and high computational complexity for large matrices. (Zass and Shashua 2006) present a Frobenius-optimal doubly stochastic normalization method via Von-Neumann successive projection and apply it into SC. (Marcus and Newman 1961) propose a similar theorem, stating that any positive symmetric matrix can be diagonalized by multiplying it on the left and right with the same diagonal matrix. Therefore, it is valuable to explore and extend Marcus's theory, providing good conditions and algorithms for this transformation.\nOptimal transport theory has gained significant attention in recent years due to its wide applicability in various fields. (Villani 2003) laid the foundational mathematical framework for optimal transport, focusing on the cost of transporting mass in a way that minimizes the overall transportation cost. (Cuturi 2013) revolutionized the field by introducing the Sinkhorn algorithm (Sinkhorn and Knopp 1967), which employs entropy regularization to make the computation of optimal transport more efficient and scalable to high-dimensional data. (Peyr\u00e9, Cuturi et al. 2019) further developed algorithms for computational optimal transport, enhancing its practicality for large-scale problems. However, the relationship between clustering and optimal transport has been relatively unexplored (Yan et al. 2024). Introducing the concept of optimal transport into clustering is valuable and meaningful.\n(Nie et al. 2016) proposed the rank constraint, which states that if the Laplacian matrix of the learned similarity matrix has a rank of $n - c$, then the similarity matrix has exactly c connected components. The rank constraint clustering has also applied in (Nie et al. 2017; Wang et al. 2022). However, the learned matrix is generally not a doubly stochastic symmetric matrix, which means it differs from a probability matrix. In this paper, we propose the Adaptive Doubly Stochastic Clustering algorithm (ADCMM) based on our proposed Marcus mapping. Our method can learn symmetric doubly stochastic similarity matrix, also known as probability matrices, which naturally have exactly c connected components, allowing direct determination of clustering results without post-processing. Besides, we extend Marcus theorem by introducing a more relaxed constraint, proposing the Marcus mapping and proving that it can transform certain sparse non-negative matrices into symmetric doubly stochastic matrices through diagonal matrices. Additionally, we explore the relationship between the Marcus mapping and optimal transport, proving that the Marcus mapping solves a specific optimal transport problem more efficiently than directly using optimal transport methods. We summarize our main contributions below:\n\u2022 We propose the Marcus mapping theorem, the conditions for which have been rigorously proven. An iterative method to compute the Marcus mapping theorem is also provided. This extends the Marcus theorem by relaxing its requirement for positive matrices.\n\u2022 We explore the relationship between the Marcus mapping algorithm and optimal transport, proving that the Marcus mapping algorithm is a special case of optimal transport. We demonstrate that computations using the Marcus mapping algorithm are more efficient compared to those using optimal transport.\n\u2022 We propose the Doubly Stochastic Adaptive Neighbors Clustering method based on the Marcus Mapping (ANCMM) and design an optimization algorithm to solve this problem. The convergence of proposed method are proven theoretically and experimentally. We validate the effectiveness of our method through extensive comparative experiments on synthetic and real-world datasets."}, {"title": "Methodology", "content": "In this section, we first introduce the notations used in the paper, then proceed to describe and prove the generalized Marcus mapping and adaptive neighbors. Finally, we present the optimization problem to be solved."}, {"title": "Notations", "content": "Throughout this paper, data matrix are denoted as $X \\in \\mathbb{R}^{n\\times d}$, where n is the number of samples, and d is the number of features $x_i \\in \\mathbb{R}^{d}$ is the i-th samples and also the transpose of the i-th row of X. $I_n$ or I denotes a vector in the space $\\mathbb{R}^{n}$ with all the elements being 1. \u03c0 represents a full permutation. For example, given a permutation rule \u03c0, {\u03c0(1), \u03c0(2), ..., \u03c0(n)} is just {1, 2, ..., n} but \u03c0(1), \u03c0(2), ..., \u03c0(n) and 1, 2, ..., n are in a different order.\nFor matrix S, diag(S, 1) represents the elements on the superdiagonal, i.e., the diagonal shifted up by one position. diag(S, 2) represents the elements on the second superdiagonal, i.e., the diagonal shifted up by two positions. Tr(S) represents the trace of S, and $||S||_F$ represents the Frobenius norm of S."}, {"title": "Marcus Mapping", "content": "In this section, we will introduce the Marcus mapping. The Marcus mapping is a generalized form of the Marcus theorem, which is shown below.\nMarcus theorem (Marcus and Newman 1961) If S is symmetric and has positive entries there exists a diagonal matrix D with positive main diagonal entries such that DSD is doubly stochastic.\nIn other words, given a symmetric matrix $S \\in \\mathbb{R}^{n\\times n}$, we could find a diagonal matrix D such that\n$M(S) = DSD$  (1)\nand the $M(S) \\in \\{Z\\in \\mathbb{R}^{n\\times n}|ZI_n = I_n, Z_{ij} \\geq 0, Z = Z^T\\}$. (Ron Zass and Amnon Shashua 2005) has a similar idea, they believe that any non-negative symmetric matrix S can be transformed into a doubly stochastic matrix by iterative left and right multiplication with a degree matrix $D^{-\\frac{1}{2}}$ where $D = diag(SI_n)$. However, this idea is flawed. In fact, we can provide an example where the matrix S, as shown below, cannot be transformed into a doubly stochastic matrix regardless of the choice of any diagonal matrix D.\n$S=\\begin{pmatrix}\n0 & 1 & 1\\\\\n1 & 0 & 1\\\\\n1 & 1 & 0\n\\end{pmatrix}$ (2)\nTherefore, we introduce a weaker condition here and utilize the Sinkhorn theorem to prove that under this weaker condition, the Marcus theorem holds true. We also provide an iterative algorithm for solving the Marcus mapping. We will prove that the algorithm we propose satisfies this very weak condition and explain the Marcus mapping through optimal transport theory.\nTheorem 1.(Marcus mapping theorem) For a symmetric no-negative matrix S, if its subdiagonals $diag(S, 1) \\neq 0$ and second superdiagonals $diag(S, 2) \\neq 0$, it can be transformed into a doubly stochastic matrix $M(S) = DSD \\in \\{Z\\in \\mathbb{R}^{n\\times n}|ZI_n = I_n, Z_{ij} \\geq 0, Z = Z^T\\}$ using the Marcus mapping by a diagona matrix D.\nTo prove this theorem, we first need to introduce the definition of \u03c0-diagonals and the Sinkhorn theorem."}, {"title": "\u03c0-diagonals", "content": "Definition 1.(\u03c0-diagonals) If $S \\in \\mathbb{R}^{n\\times n}$ and \u03c0 is a permutation of {1,2,...,n}, then the sequence of $(S_{1,\\pi(1)}, S_{2,\\pi(2)}, ..., S_{n,\\pi(n)})$ is called the \u3160-diagonal of S corresponding to \u03c0. S is said to have total support if S \u2260 0 and if every positive element of Z lies on a positive \u03c0-diagonal. A non-negative matrix that contains a positive \u03c0-diagonal is said to have support.\nBy definition, we know that if \u03c0 is the identity, the \u03c0-diagonal is called the main diagonal. The content of the Sinkhorn theorem is shown as follows.\nTheorem 2.(Sinkhorn theorem) If $S\\in \\mathbb{R}^{n\\times n}$ is non-negative, a necessary and sufficient condition that there exist a doubly stochastic matrix $\\hat{S}$ of the form $\\hat{S} = D_1SD_2 \\in \\{Z\\in \\mathbb{R}^{n\\times n}|ZI_n = I_n, Z_{ij} \\geq 0, Z = Z^T\\}$, where $D_1$ and $D_2$ are diagonal matrices with positive main diagonals is that S has total support. If $\\hat{S}$ exists, then it is unique.\nNow we will prove the Marcus mapping theorem. The core of the proof lies in demonstrating that a no-negative matrix satisfying condition diag(S, 1) \u2260 0 and diag(S, 2) \u2260 0 is also total support.\nProof For a matrix S with positive elements along the diag(S, 1) and diag(S, 2), we first prove that it is a support matrix: If n is even, we can choose the following permutation:\n$\\pi(i) = i - 1, i = 2k$ (3)\n$\\pi(i) = i + 1, i = 2k + 1$\nwe choose the \u3160-diagonal such that\n$S_{1,\\pi(1)}, S_{2,\\pi(2)}, ..., S_{n-1,\\pi(n-1)}, S_{n,\\pi(n)}$\n$= S_{1,2}, S_{2,1}, ..., S_{n-1,n}, S_{n,n-1}$ (4)\nFor n being odd, we can choose another type of \u03c0-diagonal\n$\\pi(1) = 2, \\pi(2) = 3, \\pi(3) = 1$\n$\\pi(i) = i + 1, (i = 2k and i > 3)$ (5)\n$\\pi(i) = i - 1, (i = 2k - 1 and i > 3)$\nand so that\n$S_{1,\\pi(1)}, S_{2,\\pi(2)}, S_{3,\\pi(3)}, S_{4,\\pi(4)}, S_{5,\\pi(5)}, ..., S_{n-1,\\pi(n-1)}, S_{n,\\pi(n)}$\n$= S_{1,2}, S_{2,3}, S_{3,1}, S_{4,5}, S_{5,4}, \u2026, S_{n-1,n}, S_{n,n-1}$ (6)\nThus, we have shown that regardless of whether n is odd or even, there exists a positive \u03c0-diagonal, implying that the matrix is definitely support. Considering\n$\\sqrt{S_{i,j}} \\geq 0 \\neq diag(S, 1) \\cup diag(S, 2)$ (7)\nwe will prove that $S_{p,q}$ appears on a positive \u03c0-diagonal. In fact, whether n is odd or even, we can find such a permutation \u03c0 such that $S_{p,q}$ appears on the \u03c0-diagonal. Without loss of generality, we assume p, q, and n are even. We previously proved that regardless of being odd or even, there exists a \u03c0 permutation that places them on the positive \u03c0-diagonal.\nSpecifically, for the odd parts, construct the permutation \u03c0according to Eq.(5), and for the even parts, construct the permutation \u03c0 according to Eq.(3). Then combine the two disjoint sub-permutations to form the complete permutation.\nSince S is symmetric, this \u03c0-diagonal must be positive. Therefore, the matrix S is total support,so it can be doubly stochasticized by two diagonal matrices. Given that S is symmetric, these two diagonal matrices $D_1 = D_2$.\nUnder the conditions that satisfy the Marcus mapping theorem(S is no-negative and diag(S, 1) \u2260 0, diag(S,2) \u2260 0), the Marcus mapping must exist. We propose the following Marcus algorithm to compute the results obtained from the Marcus mapping."}, {"title": "Marcus mapping algorithm", "content": "Algorithm 1: Marcus mapping algorithm\nInput: Symmetric matrix $S \\in \\mathbb{R}^{n\\times n}$\nOutput: Doubly stochastic M(S)\n1: Initialize $u \\in \\mathbb{R}^{n\\times 1}$\n2: while not converge do\n3: \t$u = 1./(Su)$\n4: end while\n5: D = diag(u)\n6: $S = DSD$\n7: $M(S) = S/(\\Sigma_{i=1}^{n} S_{ii})$"}, {"title": "Adaptive Local doubly stochastic Structure Learning", "content": "In graph-based learning, an important aspect is to find a low-dimensional local manifold representation, as high-dimensional data generally contains low-dimensional manifolds. Therefore, selecting an appropriate similarity graph is a crucial requirement for graph-based learning. A reasonable objective function for graph learning is shown below (Nie, Wang, and Huang 2014).\n$\\underset{S_{i} \\in \\mathbb{R}^{n \\times 1}}{min}\\sum_{i, j}||x_{i}-x_{j}||^{2} S_{i j}+\\alpha||S||_{F}^{2}$ (8)\ns.t. $\\forall i, S^{T} \\mathbb{I}=\\mathbb{I}, 0<S_{i j} \\leq 1$\nIn the research process, $s_{ij}$ is usually considered the probability that $x_i$ and $x_j$ belong to the same class. However, the solution to this problem is not a doubly stochastic matrix, which cannot reasonably express the concept of probability."}, {"title": "Optimization Algorithm", "content": "Therefore, we impose a stronger constraint on problem (8)\n$\\underset{S_{i} \\in \\mathbb{R}^{n \\times 1}}{min}\\sum_{i, j}||x_{i}-x_{j}||^{2} S_{i j}+\\alpha||S||_{F}^{2}$ (9)\ns.t. $\\forall i, S^{T} \\mathbb{I}=\\mathbb{I}, 0<S_{i j} \\leq 1, S=S^{T}$\nTypically, the learned probability matrix S does not have c connected components, requiring further clustering using methods like K-means. Due to the following theorem, we introduce the Laplacian rank constraint.\nTheorem 3. For a matrix S, if the rank of the Laplacian matrix $L_s = n \u2013 c$, then S has exactly c connected components.\nHence, the Laplacian rank constraint is introduced in, and the final form of the loss function becomes:\n$\\underset{S_{i} \\in \\mathbb{R}^{n \\times 1}}{min}\\sum_{i, j}||x_{i}-x_{j}||^{2} S_{i j}+\\alpha||S||$ (10)\ns.t. $S^{T} \\mathbb{I}=\\mathbb{I}, 0<S_{i j} \\leq 1, S=S^{T}, rank(L_{s})=n-c$\nProblem (10) is our objective function. We have designed an optimization algorithm to solve this problem."}, {"title": "Optimization Algorithm", "content": "Optimizing problem (10) is more challenging because the matrix constraints are coupled. However, we can cleverly solve this problem by iteratively solving the relaxed problem and using the Marcus mapping."}, {"title": "Clustering", "content": "Assume $\\sigma_{i}(L_s)$ represents the i-th smallest eigenvalue of $L_s$. Since $L_s$ is a positive semidefinite matrix, $\\sigma_{i}(L_s) \\geq 0$ . Because solving for the constraint $rank(L_s) = n - c$ is too difficult, we relax it to $\\sum_{i=1}^{c} \\sigma_{i}(L_s) = 0$, which means $\\sigma_{i}(L_s) = 0 \\forall i = 1...c$. According to Ky Fan's Theorem (ky Fan et al.1949), we have\n$\\sum_{i=1}^{c} \\sigma_{i}(L_s) = \\underset{F \\in \\mathbb{R}^{n \\times c}, F^{T} F=I}{min} Tr \\left(F^{T} L_{s} F\\right)$ (11)\nThen problem (10) is equivalent to the following problem\n$\\underset{S_{i} \\in \\mathbb{R}^{n \\times 1}}{min}\\sum_{i, j}||x_{i}-x_{j}||^{2} S_{i j}+\\alpha||S||+2\\lambda Tr\\left(F^{T} L_{s} F\\right)$ (12)\ns.t. $\\forall i, S^{T} \\mathbb{I}=\\mathbb{I}, 0<S_{i j} \\leq 1, S=S^{T}, F^{T} F=I$\nWe selected a sufficiently large \u03bb, so that in the optimization process, $\\sum_{i=1}^{c} \\sigma_{i}(L_s) = 0$ can be obtained, thereby solving the relaxed constraint.\nFix S, update F Once S is fixed, updating F only requires solving the following problem\n$\\underset{F \\in \\mathbb{R}^{n \\times c}, F^{T} F=I}{min} Tr \\left(F^{T} L_{s} F\\right)$ (13)\nThe optimal solution for F is the eigenvectors corresponding to the smallest c eigenvalues of $L_s$. This is part of Ky Fan's theorem."}, {"title": "Time Complexity Analysis", "content": "Fix F, update S Due to the following important equation\n$\\sum_{i, j}||f_{i}-f_{j}|| S_{i j}=2 T r\\left(F^{T} L_{s} F\\right)$ (14)\nwe are essentially required to solve the following problem:\n$\\underset{S_{i} \\in \\mathbb{R}^{n \\times 1}}{min}\\sum_{i, j}\\left(||x_{i}-x_{j}||^{2}+\\lambda||f_{i}-f_{j}||^{2}\\right) s_{i j}+\\alpha||S||$ (15)\ns.t. $\\forall i, S^{T} \\mathbb{I}=\\mathbb{I}, 0<S_{i j} \\leq 1, S=S^{T}$\nSolving this problem is very challenging. Our key idea is to first solve the relaxed problem and then map it to a symmetric doubly stochastic matrix using the Marcus mapping.In this section, we first consider the relaxed problem (15).\n$\\underset{S_{i} \\in \\mathbb{R}^{n \\times 1}}{min}\\sum_{i, j}\\left(||x_{i}-x_{j}||^{2}+\\lambda||f_{i}-f_{j}||^{2}\\right) s_{i j}+\\alpha||S||$ (16)\ns.t. $\\forall i, S^{T} \\mathbb{I}=\\mathbb{I}, 0<S_{i j} \\leq 1$\nDenote $m_{i j}=\\|x_{i}-x_{j}\\|^{2}+\\lambda\\|f_{i}-f_{j}\\|^{2}$ and $m_{i} \\in \\mathbb{R}^{n \\times 1},(m_{i})_{j}=m_{i j}$,Because we relaxed the constraints and decoupled the relationships between rows and columns, we can solve each row independently,Expanding the original problem and simplifying, we ultimately reduce it to the following form, which has a closed-form solution.\n$\\underset{S_{i}}{min}\\left\\|\\frac{1}{2\\alpha} S_{i}+M_{i}\\right\\|_{F}^{2}$ (17)\ns.t. $S^{T} \\mathbb{I}=\\mathbb{I}, 0<S_{i j} \\leq 1$\nBy solving this problem, we obtain the optimal solution $S^{*}$ after relaxation. We transform it into a symmetric matrix using the following symmetrization to meet the conditions for the Marcus mapping.\n$S=\\frac{S^{*}+S^{*, T}}{2}$ (18)\nWe will later prove two things: 1. Prove that it is easy to satisfy the conditions of the Marcus mapping by choosing \u03b1. 2. Prove that the solution obtained through Marcus mapping is sufficiently close to the true solution of problem (14).\nFix F, update M(S) At this step, we utilize the Marcus mapping to compute M(S). Specifically, we input $\\hat{S}^{*}$ into the Marcus mapping and aim to obtain\n$S=M\\left(\\hat{S}^{*}\\right)=D \\hat{S} D=D \\frac{S^{*}+S^{*, T}}{2} D$ (19)\nAfter calculating $S = M(\\hat{S}^*)$, we substitute it into the first step and iterate through various steps until convergence. We summarize our optimization algorithm in Algorithm 2."}, {"title": "Time Complexity Analysis", "content": "Select \u03b1 based on sparsity\nIn this part, we will demonstrate how the selection of \u03b1 affects the sparsity of the probability matrix S. Additionally,"}, {"title": "ANCMM", "content": "Algorithm 2: ANCMM\nInput: Data matrix $X \\in \\mathbb{R}^{n \\times d}$, clusters c\nParameter: a and \u03bb\nOutput: Desired similarity graph $S\\in \\mathbb{R}^{n \\times n}$ with c connected components\n1: Initialize S\n2: while not converge do\n3: \tFix S and update F by Eq.13\n4: \tFix F and update S by Eq.15\n5: \tSymmetrize matrix S by Eq.16\n6: \tUpdate S by Marcus mapping using Algorithm 1\n7: end while\nwe will prove that by simply choosing \u03b1 we can satisfy the conditions required by the Marcus mapping.\nWe still consider the relaxed problem16 first because if the relaxation problem's \u03b1 can control the sparsity of the matrix S, the non-relaxed one can too. Since we update each row individually, (Nie et al.2016) has proven that the closed-form solution has the following form.\n$S_{i j}=\\left(\\frac{M_{i j}}{\\eta i}-\\varphi\\right)_{+}$ (20)\nwhere $\\varphi=\\frac{1}{n}+\\frac{2}{\\eta i} \\sum_{j=1}^{k} M_{i j}$ (Nie, Wang, and Huang 2014). That $x_i$ have k neighbours $\\Rightarrow S_{i j}>0, \\forall 1 \\leq j \\leq k$ and $S_{i, k+1}=0$. Here, we assume sij is sorted in descending order with respect to j. According to (20) and the value of \u03c6 we have\n$\\frac{1}{k} \\sum_{j=1}^{k} m_{i j}<\\varphi<\\frac{1}{k+1} \\sum_{j=1}^{k+1} m_{i j}$ (21)\nwhere $\\left(m_{i j}\\right)=M_{i 1}, M_{i 2}, \\dots, M_{i n}$ are also sorted in descending order with respect to j. This means that by controlling \u03b1 for each row, we can precisely control the sparsity of each row.\nNow we demonstrate that we can easily satisfy the conditions required by the Marcus mapping by selecting \u03b1.\nFirst, we choose k \u2265 2, meaning each row has at least two or more elements that are not zero, which means\n$\\alpha>\\underset{i \\in\\{1 \\dots n\\}}{max}\\left(\\frac{M_{i, 3}}{M_{i, 1}+M_{i, 2}}\\right)$ (22)\nBy choosing \u03b1, we can ensure that each row of $S^*$ has at least two or more elements that are not zero. Since each element of $S^{*}$ is greater than zero, $\\hat{S}^{*}=\\frac{S+S^{T}}{2}$ also has at least two or more elements that are not zero in each row.\nNext, we perform simultaneous row and column exchanges on the symmetric matrix $\\hat{S}^{*}$(a congruence transformation of the matrix), ensuring that the superdiagonal $d i a g\\left(\\hat{S}^{*}, 1\\right)$ and second superdiagonal $d i a g\\left(\\hat{S}^{*}, 2\\right)$ elements are not zero. Note that this only changes the order of the samples and does not affect the clustering results. Through row and column exchanges, we can satisfy the conditions required by the Marcus mapping."}, {"title": "Approximation Analysis", "content": "In this section, we will demonstrate that $M(\\hat{S}^{*})$ is sufficiently close to the true solution of Problem (15).\nFor problem (8), due to the Karush-Kuhn-Tucker (KKT) conditions, there exist 0 and y such that:\n$M+2 \\theta S^{*}+y \\mathbb{I}^{T}=0$ (23)\nwhere $M_{i j}=m_{i j}, \\theta \\geq 0$ and $y \\in \\mathbb{R}^{n}$. Similarly, considering the transpose problem of Problem (8), we have:\n$M^{T}+2 \\theta S^{T}+I y^{T}=0$ (24)\nWe know that\n$M\\left(\\hat{S}^{*}\\right)=D\\left(\\frac{S^{*}+S^{*, T}}{2}\\right) D=\\frac{1}{2}\\left(D S^{*} D+D S^{*, T} D\\right)$ (25)\nIf DSD, DS*,T D are the optimal solutions to Problem (8) and the transpose Problem, respectively, then according to Eq.(23) and Eq.(24), M($\\hat{S}^{*}$) is the optimal solution to Problem (9). However, this is usually not the case. Nevertheless, we can have the following estimation:\n$\\left\\|M\\left(\\hat{S}^{*}\\right)-S^{*}\\right\\|<\\frac{1}{2}\\left(\\left\\|D S^{*} D-S^{*}\\right\\|+\\left\\|D S^{*, T} D-S^{T}\\right\\|\\right)$ (26)\n$=\\left\\|D S^{*} D-S^{*}\\right\\|<\\epsilon$\nUsually, \u03f5 is very small relative to $||S^{*}||$, making it an acceptable approximation.\nTime Complexity Analysis\nTime complexity comparison. Indeed, in some cases, defining the degree matrix as $D=d i a g\\left(S \\mathbb{I}\\right)$. and continuously performing the following operations(Ron Zass et al.2005)\n$S \\leftarrow D^{-\\frac{1}{2}} S D^{-\\frac{1}{2}}$ (27)\ncan also transform S into a doubly stochastic matrix. However, through this method, we need to perform $n^{2}$ additions, n square roots, n divisions, and $2 n^{2}$ multiplications each round, far exceeding the $n^{2}$ additions, $n^{2}$ multiplications and n divisions of the Marcus mapping.\nIf we only consider multiplication, the Marcus mapping takes about half the time compared to the above method.\nTime complexity analysis. When updating F by Eq. (13), c smallest eigenvalues need to be computed, Using the Lanczos algorithm can achieve a time complexity that is linear with respect to O(n). Then, the corresponding eigenvectors need to be computed, resulting in a time complexity of $O(k^2 n^2)$, k2 represents the number of iterations.\nWhen updating S by the Eq. (17), only n linear equations need to be solve and a matrix addition needs to be calculated, resulting in a time complexity of O($n^2$). Then we calculate Marcus mapping by Algorithm 1, which only requires computing the multiplication of a matrix by a vector and the division of a scalar by a vector, resulting in a time complexity of O($k_1n^2$). Here, $k_1$ represents the number of iterations.\nTherefore, the overall time complexity is O($k(k_1$ + $k_2)n^2$), k represents the total number of iterations."}, {"title": "Connection with Optimal Transport", "content": "In this section, we will point out the connection between the Marcus mapping and optimal transport.\nDonate the Marcus mapping for a non-negative matrix S as M(S), log(S)represents the natural logarithm of each element of S, and we require log(0) = \u2212\u221e and $e^{-\\infty}$ = 0. Consider the optimal transport problem with entropy regularization:\n$\\underset{P}{min} <P,-\\log (S)>-\\frac{1}{\\omega} \\sum P_{i j} \\log \\left(P_{i j}\\right)$ (28)\ns.t. $P \\in \\Omega(\\mathbb{I}, \\mathbb{I})$\nwhere $\\Omega(\\mathbb{I}, \\mathbb{I})=\\left\\{Z \\mid Z_{i j} \\geq 0, Z \\mathbb{I}=\\mathbb{I}, Z^{T} \\mathbb{I}=\\mathbb{I}\\right\\}$\nThe Lagrangian function of the optimal transport problem is given by\n$\\mathcal{L}(P, \\phi, \\xi)=\\sum\\left(-P_{i j} \\log \\left(s_{i j}\\right)+\\phi^{T}\\left(P \\mathbb{I}_{n}-\\mathbb{I}_{n}\\right)+\\xi^{T}\\left(P^{T} \\mathbb{I}_{n}-\\mathbb{I}_{n}\\right)\\right)$ (29)\nBy taking the partial derivative with respect to P, we can obtain that the optimal solution satisfies:\n$P_{i j}(\\omega)=e^{-\\phi_{i}} e^{-\\omega \\xi_{i}} e^{-\\omega \\log \\left(s_{i j}\\right)}$ (30)\nIn particular, when w is 1, the form of the optimal transport solution is consistent with the Marcus mapping.\n$P_{i j \\mid \\omega=1}=e^{-\\phi_{i}} S_{i j} e^{-\\xi_{j}}$ (31)\nSince S is symmetric, because\n$P=\\underset{P \\in \\Omega(\\mathbb{I}, \\mathbb{I})}{arg min}\\left(<P,-\\log (S)>-\\frac{1}{\\omega} \\sum P_{i j} \\log \\left(P_{i j}\\right)\\right)$ (32)\nis also symmetric, so P has the form $P=DSD$. This demonstrates that the Marcus mapping is solving a special optimal transport.\nThe transformation of -log(S) is crucial as it can convert zeros in the probability matrix to positive values, thereby satisfying the conditions for optimal transport. However, computing through the Marcus mapping is much more efficient than directly using optimal transport, primarily because optimal transport involves additional logarithmic computations, and expressing $+ \\infty$ in a computer is challenging."}, {"title": "Experiments on toy datasets", "content": "To demonstrate the superiority of our algorithm, we first constructed a toy dataset in the shape of double moons. We set the sample size to 200, with a random seed of 1 and noise level of 0.13. We performed clustering using both the CAN and ANCMM algorithms, both of which incorporate rank constraints and are adaptive neighbors algorithms. We compared their accuracy and the configuration of adaptive neighbors. The dataset and results are shown in Figure 2.\nWe used the learned probability matrix weights as the edge weights connecting each pair of points.Additionally, we demonstrate the similarity matrix obtained solely from the Gaussian kernel function. Both CAN(Nie.et 2010) and ANCMM learned similarity matrix with two connected components. However, as shown in the figure, CAN misclassified one more point compared to ANCMM. Table 1 shows the accuracy(ACC), normalized mutual information(NMI), and purity (PUR) metrics for the clustering results."}, {"title": "Experiments on real datasets", "content": "Experimental Settings\nDatasets. We conducted extensive experiments on ten datasets, including TR41,ORL,ARsmall,LetterRecognition, warpPIE10P, Wine,Feret,Movement, Ecoil, Yeast. The Table 2"}]}