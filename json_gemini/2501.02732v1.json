{"title": "AFed: Algorithmic Fair Federated Learning", "authors": ["Huiqiang Chen", "Tianqing Zhu", "Wanlei Zhou", "Wei Zhao"], "abstract": "Federated Learning (FL) has gained significant attention as it facilitates collaborative machine learning among multiple clients without centralizing their data on a server. FL ensures the privacy of participating clients by locally storing their data, which creates new challenges in fairness. Traditional debiasing methods assume centralized access to sensitive information, rendering them impractical for the FL setting. Additionally, FL is more susceptible to fairness issues than centralized machine learning due to the diverse client data sources that may be associated with group information. Therefore, training a fair model in FL without access to client local data is important and challenging. This paper presents AFed, a straightforward yet effective framework for promoting group fairness in FL. The core idea is to circumvent restricted data access by learning the global data distribution. This paper proposes two approaches: AFed-G, which uses a conditional generator trained on the server side, and AFed-GAN, which improves upon AFed-G by training a conditional GAN on the client side. We augment the client data with the generated samples to help remove bias. Our theoretical analysis justifies the proposed methods, and empirical results on multiple real-world datasets demonstrate a substantial improvement in AFed over several baselines.", "sections": [{"title": "I. INTRODUCTION", "content": "With the increasing use of machine learning algorithms in critical decision-making areas such as credit evaluation, loan applications, and healthcare, there are concerns about potential bias and discrimination in trained models. For instance, COMPAS, a software used by courts to aid judges in pretrial detention and release decisions, has been found to have a substantial bias against African-Americans when comparing false positive rates of African-American offenders to Caucasian offenders.\nAs a result, fairness research has gained prominence, with group fairness [18] and individual fairness [13] being two broad classifications for approaches to fairness. We refer to these two concepts as algorithmic fairness because the definition of fairness is expanded in the context of FL, such as accuracy parity [24], good-intent fairness [31, 12]. This paper presents an algorithmic fair FL framework emphasizing group fairness since clients tend to belong to specific demographic groups in FL [20].\nVarious algorithms have been proposed to train an algorithmic fair model in centralized machine learning [46, 47, 45, 40], including pre-processing, in-processing and post-processing [11]. Although the details differ, all of these algorithms assume centralized access to data. In FL, however, training data is not centrally accessible by either the server or the clients. Therefore, one cannot simply apply centralized fair learning algorithms in FL tasks. This begs the question: How to train an algorithmic fair FL model without centralized access to data?\nThis problem is challenging due to restricted access and data heterogeneity. FL models are trained on all clients' local data, with only gradients/model parameters transmitted to the server during each round. Debiasing on the server side is not feasible since the server is prohibited from directly accessing clients' local data. On the other hand, a single client can't accurately picture the global distribution due to limited data [29]. Worse yet, data distributions vary between clients [20]. There is no guarantee that the model debiased with local data will generalize to the global distribution. In addition, it has been long observed that fairness conflicts with accuracy [49, 3]. A trivial model that makes constant predictions is perfectly fair but useless.\nTo address these challenges, we proposed a novel framework named AFed to achieve algorithmic fair FL. AFed aims to learn a fair model satisfying group fairness (e.g., demographic parity [18]) in FL. The core idea is to learn global data distribution and then disseminate that knowledge to all clients to help with local debiasing. To this end, two algorithms are proposed. Our first algorithm, AFed-G, trains a conditional generator to learn clients' data distribution on the server side. AFed-G enjoys less computation and communication overhead. The second algorithm, AFed-GAN, trains a conditional GAN on the client side, which is more effective in learning data distribution. An auxiliary classification head is designed to help extract informative features that benefit the generator training. The knowledge about global distribution is then shared among all clients to help debiasing the model. We mix the true training data with generated data to debiase the model.\nIn summary, the contributions of this paper are listed below.\n\u2022 We tackle the challenge of training a fair model in FL without accessing clients' data. We bypass the restricted centralized data access by introducing a conditional generator/GAN.\n\u2022 We design an auxiliary classification head to help extract more informative features, which benefits the conditional generator/GAN training.\n\u2022 We provide theoretical analysis to justify the robustness of the proposed method. Our empirical results demonstrate substantial improvement of the proposed methods over baselines on several real-world datasets."}, {"title": "II. RELATED WORKS", "content": "At a high level, algorithmic fairness notions adopted in centralized machine learning can be categorized as two different families: the statistical notion and individual notion [6]. The statistical notions require specific statistical measures to be comparable across all of these groups [22, 5]. The individual notions aim at fairness between specific pairs of individuals: Give similar predictions to similar individuals [13]. Individual notions offer a meaningful fairness guarantee but need to make significant assumptions, some of which present non-trivial challenges in fairness. We adopt statistical notions, more specifically, demographic parity [18] as fairness metric in this paper.\nDebiasing methods can be classified as pre-processing, in-processing, and post-processing, respectively [11]. Pre-processing improves fairness by eliminating bias present in the training dataset. The model is subsequently trained and applied on the modified data [15, 21]. For instance, Kamiran et al. [21] propose reweighting samples in the training dataset to correct for biased treatment. Xu et al. [37] proposed FairGAN, which generates fair data from the original training data and uses the generated data to train the model. In-processing achieves fairness by adding constraints during training. Berk et al. [3] integrated a family of fairness regularizers into the objective function for regression issues. Zhang et al. [42] borrowed the ideal of adversarial training to restrict a model's bias. The general methodology of post-processing methods is to adjust the decision thresholds of different groups following specific fairness standards [5, 18, 4]. However, existing methods assume centralized access to the data, which is invalid in FL. Each client can only access its own data, and the server does not know the local data distribution. As such, it remains challenging to train a fair model in FL."}, {"title": "B. Fairness in Federated Learning", "content": "Fairness can be defined from different aspects in FL. A body of work focuses on accuracy disparity, i.e., trying to minimize the performance gaps between different clients [24]. A model with a more uniform performance distribution is considered fairer. The goal, in this case, is to address any statistical heterogeneity during the training phase by using techniques like data augmentation [50], client selection [38] and reweighting [19], multi-task learning [35], etc. Another area of focus is good-intent fairness [31], ensuring that the training procedure does not overfit a model to any client at the expense of others. In this instance, the objective is to train a robust model against an unknown testing distribution.\nHowever, these two lines of work don't address the algorithmic fairness issues in FL. To date, there are limited works about algorithmic fairness in FL. Du et al. [12] formulated a distributed optimization problem with fairness constraints. G\u00e1lvez et al. [16] cast fairness as a constraint to an optimization problem. To protect privacy, the constraint problem is optimized based on statistics provided by clients. Chu et al. [8] and Cui et al. [10] took a similar path, imposing fairness constraints on the learning problem. To avoid intruding on the data privacy of any client, the fairness violations are locally calculated by each client. Zhang et al. [43] focused on client selection, using a team Markov game to select clients to ensure fairness. Ezzeldin et al. [14] proposed a fairness-aware aggregation method that amplifies local debiasing by weighting clients whose fairness metric aligns well with the global one. Wang et al. [36] explore the relationship between local group fairness and propose a regularized objective for achieving global fairness. Zeng et al. [41] extend FairBatch [33], a fair training method in centralized machine learning, to FL. In their design, each client shares extra information about the unfairness of its local classifier with the server. Chu et al. [7] propose estimating global fairness violations by aggregating the fairness assessments made locally by each client. They then constrain the model training process based on the global fairness violation."}, {"title": "C. Discussion of Related Works", "content": "Currently, most FL research focuses on accuracy disparity, which is more closely related to data heterogeneity rather than the potentially unfair decisions the model may make. Comparatively, less research has been conducted regarding algorithmic fairness in FL. The difficulty of ensuring algorithmic fair FL stems from the privacy restriction of FL, i.e., the data never leaves the device. In the presence of client-specific data heterogeneity, however, local debiasing may fail to provide acceptable performance for the entire population. Existing works have attempted to circumvent this restriction by providing server statistics or calculating fairness constraints locally. However, they fail to adequately address the non-i.i.d. challenge in FL. For example, in the presence of non-i.i.d. data, the estimated global fairness violation in [7] could be inaccurate. Our approach distinguishes itself from existing methods in two key ways: 1) Alternative methodology: We explore a different direction by leveraging local data augmentation to enhance debiasing efforts; 2) Enhanced practicality: We address the non-i.i.d. challenges inherent in FL when training fair models. Our local data augmentation technique effectively mitigates these non-i.i.d. issues."}, {"title": "III. BACKGROUND", "content": "This research focuses on addressing fairness issues in the context of binary classification tasks to illustrate our method. Note that our methods can be extended to the general multi-class classification problem since we didn't explicitly require the target label to be binary. Denote X \u2208 \u211d^{m} as the input space. Z \u2208 \u211d^{p} is the latent feature space with p < m. y = {0,1} is the output space, and A = {0,1} is the set of all possible sensitive attribute values. The classification model f parameterized by \u03b8_{f} := {\u03b8_{E}, \u03b8_{y}} consists of two parts: a feature extractor E: X \u2192 Z and a classification head h_{y}: Z \u2192 \u0394, where \u0394 stands for the probability simplex over Y. In addition, we train another classification head h_{a}: Z \u2192 \u0394_{A} parameterized by \u03b8_{a} to classify sensitive attribute a. In AFed-G, a conditional generator G parameterized by w_{G} is trained at the server side to synthesize fake representations conditional on a: A \u2192 Z. In AFed-GAN, we replace G with conditional GAN parameterized by w := {w_{G}, w_{D}}. The discriminator D: z \u222a x \u2192 [0, 1] is trained to distinguish fake samples from real ones."}, {"title": "B. Federated Learning", "content": "A FL system comprises N local clients and a central server. Each client possesses a specific dataset D_{k}, k \u2208 {1, 2, ..., N} and the i-th data sample is represented as z_{i} := {x_{i}, y_{i}, a_{i}}, where x_{i} is the features, y_{i} is the ground-true label, and a_{i} is the sensitive attribute. The amount of data of the k-th client D_{k} is denoted as n_{k}. The goal of FL is to minimize the empirical risk over the samples of all clients, i.e.,\n$\\theta^{f} = \\arg \\min_{\\theta^{f}} \\frac{1}{N} \\sum_{k=1}^{N} \\frac{1}{n_{k}} \\sum_{i=1}^{n_{k}} l_{k}(f(x_{i}; \\theta), y_{i}),$ (1)\nwhere l_{k} is the loss objective of the k-th client. Unlike central machine learning, which assumes centralized access to all train data D = \u222a_{k\u2208N}D_{k}, the client's training data is never exposed to the server. Only gradients/model parameters are transmitted to the server for aggregation.\nAt the t-th training round, the server randomly selects a set of clients S to train the current global model w^{t}. Selected clients first receive w^{t} from the server, then train it on their local dataset D_{k}, k \u2208 S,\n$\\theta_{k}^{t+1} = \\theta_{k}^{t} - \\eta \\nabla_{\u03b8_{k}} \\sum_{i\u2208B_{k}^{t}} l_{k}(f(x_{i}; \\theta_{k}^{t}), y_{i}),$ (2)\nwhere \u03b7 denotes the learning rate and B_{k}^{t} is a mini-batch of training data sampled from D_{k} in the t-th iteration. The local training process could run for multiple rounds. After that, the trained model \u03b8_{k}^{t} or equivalently gradients \u0394_{k}^{t} := \u03b8_{k}^{t+1} - \u03b8_{k}^{t} is send back to the server for aggregation,\n$\\theta^{t+1} = \\sum_{k\u2208S} w_{k} \\theta_{k}^{t+1},$ (3)\nwhere w_{k} = n_{k}/\u03a3_{k\u2208S}n_{k} is the aggregation weight. \u03b8^{t+1} is used as the initial model for the next training round. This interaction between clients and the server is repeated until a certain criterion (e.g., accuracy) is satisfied."}, {"title": "C. Fairness", "content": "Fairness in FL can be defined in different ways. This paper focuses on algorithmic fairness, especially group fairness, which is measured by the disparities in the algorithm decisions made for different groups determined by sensitive attributes, such as gender, race, etc. Here, we have opted for demographic parity as the fairness metric in this paper.\n$\\gamma_{0}(\\hat{Y}) = Pr(\\hat{Y} = 1 | A = 0) = E_{x~P_{0}}[f(x)]$\nDefinition 1 (Demographic parity [18]). Demographic parity (DP) requires that a model's prediction be independent of any sensitive attributes. The positive prediction rate for a demographic group a \u2208 A is defined as follows,\n$\\gamma_{1}(\\hat{Y}) = Pr(\\hat{Y} = 1 | A = 1) = E_{x~P_{1}}[f(x)]$\n$\\gamma_{1}(\\hat{Y}) = Pr(\\hat{Y} = 1 | A = 1) = E_{x~P_{1}}[f(x)]$\n$\\gamma_{1}(\\hat{Y}) = Pr(\\hat{Y} = 1 | A = 1) = E_{x~P_{1}}[f(x)]$\n$\n\\gamma_{1}(\\hat{Y}) = Pr(\\hat{Y} = 1 | A = 1) = E_{x~P_{1}}[f(x)]\n$"}, {"title": "D. GAN and Conditional GAN", "content": "GAN was first proposed by [17] as a min-max game between the generator G and discriminator D. The aim is to approximate a target data distribution so that the generated data can't be separated from the true data by D. The objective function of the generator is formed as\n$\\min_{G} \\max_{D} E_{p(x)}[log D(x) + log(1 - D(G(x)))],$ (7)\nwhere p(x) is the real data distribution.\nThere exist various kinds of generators to generate synthetic data for different purposes. Among these, we chose conditional GAN [30] to generate synthetic data conditional on the given input. The conditional GAN differs from the unconditional GAN by providing the generator and discriminator condition information. The learning objective of conditional GAN is formulated as follows:\n$\\min_{G} \\max_{D} E_{p(x)}[log D(x, a) + log(1 - D(G(x, a)))].$ (8)\nThe additional information a is fed to the generator and the discriminator during training. After convergence, the generator can generate fake data conditional on a."}, {"title": "IV. ALGORITHMIC FAIR FEDERATED LEARNING", "content": "The non-i.i.d. nature of local data distributions in a typical FL system presents a significant challenge for local debiasing. Certain groups may dominate a client's data, while others may have limited representation. To illustrate this challenge, we create a toy FL system with four clients, each with a unique local dataset having different distributions. We generated the data from a mixture of four Gaussian distributions and grouped them based on the attribute a into two categories. In this example, 85% of a client's data is sampled from one Gaussian distribution, while the remaining 15% is evenly distributed across the other three Gaussian distributions.\nIn this situation, local debiasing is futile because none of the clients completely understand the data distribution across all clients. For example, client 1 has very few samples from group 1 (i.e., a = 1), as shown in Fig. 1a. Even less is the proportion when all clients' data is considered. Therefore, it is difficult for client 1 to debias the classification model, given that client 1 has almost no knowledge of group 1. The same issue arises with the other clients. Our theoretical analysis in Section V confirms this. The performance of the local model on the global distribution depends on the quantity and similarity of"}, {"title": "B. Overview of AFed", "content": "This section details the proposed AFed framework, summarized in Algorithm 1. Fig.3 shows the general description of AFed, which involves two learning tasks. As shown on the left part of Fig.3, the first task is to extract knowledge about local distribution. To this end, we train a conditional generator/GAN to learn clients' local data distribution. We then aggregate the extracted distributions to form a global distribution embedded in a conditional generator G.\nIn the right part of Fig.3, our second task is to train a fair and accurate model f, consisting of a feature extractor and classification head. The accuracy loss L_{ace} is obtained on real samples. To debias the model, the generator produces a corresponding fake latent feature with a different attribute value for each latent feature. We mix the true features with the fake ones and require the model to make consistent predictions on the mixed samples, which gives the fairness loss L_{fair}. Each of these two tasks is discussed in more detail in the following."}, {"title": "C. Learning Global Data Distribution", "content": "Our first task is to learn the global data distribution. We draw inspiration from the field of knowledge extraction [51, 39]. We extract local data distribution information before aggregating it to form a global distribution. We chose to perform this task in the latent space Z \u2208 \u211d^{p} instead of input space X \u2208 \u211d^{d} for both privacy and practical concerns. Because 1) sample-specific information is compressed in the latent space, only class-related information is retained; and 2) the latent space Z is more compact than the input space X, making optimization more manageable. Note that the object of this paper is to train an algorithmic fair FL model without accessing client data, which is understudied in the literature. We leave both private and fair FL to our future work.\nThe feature extractor E trained with h_{y} will only retain the information relevant to y. While information about the attribute a may be compressed. To address this, we designed an auxiliary classification head h_{a} that can assist E in extracting informative latent features. The benefit of h_{a} is twofold: first, it provides feedback to the extractor E, allowing it to extract more informative latent features; second, h_{a} embeds the relationships between the latent feature z and the sensitive attribute a, i.e., P(A|Z). This knowledge is crucial for the server to learn a global data distribution in AFed-G.\nFor the k-th client, the training process of the feature extractor E(;\u03b8_{E}) and the classification head h_{a}(;\u03b8_{a}) is formulated as follows:\n$\\min_{\u03b8_{E}, \u03b8_{a}} J_{1}(\u03b8_{E}, \u03b8_{a}):= \\frac{1}{|D_{k}|} \\sum_{x_{i}\u2208D_{k}} l_{ce}[h_{a}(E(x_{i}; \u03b8_{E}); \u03b8_{a}), a_{i}],$ (9)\nwhere l_{CE} is the cross-entropy loss.\nA typical federated learning (FL) system faces the challenge of non-i.i.d. data, where each client possesses a distinct data"}, {"title": "V. ANALYSIS", "content": "This section presents our analysis. We begin by showing h_{a} is necessary for the extractor to learn informative features. Then, we analyze the drawbacks of AFed-G. In the third part of this section, we thoroughly analyzed the fairness guarantee of our framework. First, we demonstrate that debiasing with local data is futile. Next, we show that generated samples improve the local model's fairness performance on global data distribution. Since Rasou et al. [32] have proven the convergence of GAN training in FL with non-i.i.d. data, we omit the convergence analysis of AFed-GAN."}, {"title": "A. Feature Extractor Analysis", "content": "Fig. 4a shows the one classification head case. The resultant extractor E tends only to retain information pertinent to label y and compresses all other information. {x1,y1 = 0, a1 = 1} and {x2, y2 = 0, a2 = 0} are mapped to the same area due to the same label y = 1 despite having different attribute values a. This brings confusion when training conditional GAN in this feature space.\nWhen training conditional GAN, the generated sample should resemble the training data. In other words, the fake samples should be close to the real ones in the feature space. Near z2, within the area so, there are samples with attribute values a = 0 and a = 1. As a result, feeding the generator with attribute value a = 0 will produce samples similar to"}, {"title": "B. Generator Analysis", "content": "Our first method AFed-G trains a conditional generator G to approximate the P(Z|A) using the knowledge embedded in the classification head {\u03b8_{a}^{k}}_{k=1}^{N}, i.e. p_{k}(a|z). The second method, AFed-GAN, is more direct. A discriminator guides training to train a conditional GAN to learn the distribution p_{k}(z|a). The former enjoys simplicity (only a generator is added) and can be carried out on the server's side, which reduces the client's training and communication load. In comparison, the latter performs better in learning the local data distribution.\nSince introduced in [17], numerous researchers have successfully implemented GAN to learn the target distribution [27, 30]. In the context of FL, [32] demonstrates the convergence of GAN training in FL with non-i.i.d. data. Here, we focus on analyzing the shortcomings of the AFed-G approach.\nTheorem 1. Given the optimal classification head h_{y}, optimizing Eq. 12 is equivalent to\n$\\min_{Q_{A,Z}} KL(\\prod_{k=1}^{N} Q_{A,Z} || \\prod_{k=1}^{N} P_{A,Z}) - KL(\\prod_{k=1}^{N} Q_{Z} || \\prod_{k=1}^{N} P_{Z}) + H_{Q}(A||Z)$,\nwhere $P_{Z}^{k}$ and $\\prod_{k=1}^{N} P_{Z}^{k}$ are the joint probability distribution of clients' data. $H_{Q}(A||Z) = E_{a,z\\sim Q_{A,Z}} [log q(a|z)]$ is the conditional entropy of the generated samples.\nProof. Let's denote p_{k}(a|z) := p(a|z; \u03b8_{a}^{k}) the likelihood of client k's local data distribution, which is embedded in the classification head \u03b8_{a}^{k}. Eq. 12 aims to find the target distribution Q*. Note we lose k for simplicity.\n$\\max_{Q} E_{a\\sim Q(a)} E_{z\\sim Q(z|a)} \\sum_{k=1}^{N} log p_{k}(a|z)$\n$\\max_{Q} E_{a\\sim Q(a)} E_{z\\sim Q(z|a)} \\sum_{k=1}^{N} log \\frac{p_{k}(a, z)}{p_{k}(z)}$\n$\\max_{Q} E_{a\\sim Q(a)} E_{z\\sim Q(z|a)} \\sum_{k=1}^{N} log \\frac{q(a, z)}{q(a, z)}$\n$\\min_{Q} E_{a\\sim Q(a)} E_{z\\sim Q(z|a)} \\sum_{k=1}^{N} log q(a, z) - log p_{k}(a, z)$\n$\\sum_{k=1}^{N} log q(z) + log p_{k}(z) - log q(a|z)$\n$\\min_{Q} KL(\\prod_{k=1}^{N} Q_{A,Z} || \\prod_{k=1}^{N} P_{A,Z}) - KL(\\prod_{k=1}^{N} Q_{Z} || \\prod_{k=1}^{N} P_{Z}) + H_{Q}(A||Z)$,"}, {"title": "C. Fairness Analysis", "content": "1) Debiasing with local data is ineffective: Our analysis borrows the idea from the field of domain adaptation. Each client's data is associated with a source domain Dk from which local data is sampled. The goal is to train a model that performs well on the global (or target) domain, which is the ensemble of all source domains D = \u2211_{k=1}^{N} Dk.\nWe first introduce a metric to measure the local and global distribution differences. We use the H-divergence d_{H\u0394H} [1] to measure the difference between two probabilities,\n$d_{H\u0394H}(D_{k}, D_{g}) = 2 sup_{h\u2208H} |Pr_{D_{k}}[I(h)] - Pr_{D_{g}}[I(h)]|,$ (23)\nwhere H is the hypothesis space, I(h) is a set of samples x which is defined as I(h) = {x|h(x) = 1}.\nLemma 1. [2] Let H be a hypothesis space of VC dimension d, U and U' are samples of size m, sampled from D and D', respectively. d_{H\u0394H}^{\u02c6}(D,D') is the H-divergence between D and D' and d_{H\u0394H}(D,D') is the empirical distance computed on U and U'. Then for any \u03b4 \u2208 (0,1), with probability at least 1-\u03b4,\n$\\hat{d}_{H\u0394H}(D, D') \u2264 d_{H\u0394H}(D, D') + 4\\sqrt{d \\frac{4log(2m) + log(\\frac{4}{\u03b4})}{m}},$\n(24)\nNow we are ready to bound the fairness violation that a hypothesis h_{k} trained with the local model can have on global data, i.e., ADPD(hk). We borrow the idea from the field of domain adaptation. Each client's data can be viewed as a source domain Dk, and the goal is to train a global model that performs well on the global domain, which is the ensemble of all source domains D = \u2211_{k=1}^{N} Dk."}, {"title": "VI. EXPERIMENTS", "content": "In this section, we empirically verify the effectiveness of AFed. We compare the performance of our method with several baseline methods on four real-world datasets. After that, we demonstrate the performance gain of auxiliary classification head ha and the robustness of our method w.r.t different participant ratios."}, {"title": "B. Effectiveness of Proposed Method", "content": "Fig. 6 shows the trade-offs between accuracy and Adp on three different datasets. All curves are obtained by altering the hyperparameter \u03bb in Eq. 16, which controls the trade-off between accuracy and fairness. A smaller ADP means a fairer model. Only controlling ADP is trivial. However, our goal is to train a model that is both fair and accurate, which means we must control unfairness while still achieving acceptable accuracy performance. Therefore, we prioritize curves in the upper left corner over those in the lower right corner.\nWe present the averaged results across five runs in all experiments to mitigate the impact of randomness. Overall,"}, {"title": "D. Effects of Auxiliary Classification Head", "content": "We remove the auxiliary classification head h_{a} and rerun the experiment on the UTKface dataset. The results are shown in Fig. 9. The performance of both AFed-GAN and AFed-G drops after removing h_{a}. This confirms our previous analysis that h_{a} helps the feature extractor extract more informative representations and keep information about label y and attribute a. Which consequently benefits the generator training. Nevertheless, our methods outperform baseline methods without an auxiliary classification head."}, {"title": "VII. CONCLUSION", "content": "In this paper, we propose AFed-G and AFed-GAN, two algorithms designed to learn a fair model in FL without centralized access to clients' data. We first employ a generative model to learn the clients' data distribution, which is then shared with all clients. Subsequently, each client's data is augmented with synthetic samples, providing a broader view of the global data distribution. This approach helps derive a more generalized and fair model. Both empirical and theoretical analyses validate the effectiveness of the proposed method. One potential future application of our methods is the case of multiple sensitive attributes, where the data is divided into several groups. Additionally, the proposed framework could be extended to FL scenarios where each client uses a different model architecture, provided they share the same latent feature dimension."}]}