{"title": "CoLiDR: Concept Learning using Aggregated Disentangled Representations", "authors": ["Sanchit Sinha", "Guangzhi Xiong", "Aidong Zhang"], "abstract": "Interpretability of Deep Neural Networks using concept-based models offers a promising way to explain model behavior through human-understandable concepts. A parallel line of research focuses on disentangling the data distribution into its underlying generative factors, in turn explaining the data generation process. While both directions have received extensive attention, little work has been done on explaining concepts in terms of generative factors to unify mathematically disentangled representations and human-understandable concepts as an explanation for downstream tasks. In this paper, we propose a novel method CoLiDR - which utilizes a disentangled representation learning setup for learning mutually independent generative factors and subsequently learns to aggregate the said representations into human-understandable concepts using a novel aggregation/decomposition module. Experiments are conducted on datasets with both known and unknown latent generative factors. Our method successfully aggregates disentangled generative factors into concepts while maintaining parity with state-of-the-art concept-based approaches. Quantitative and visual analysis of the learned aggregation procedure demonstrates the advantages of our work compared to commonly used concept-based models over four challenging datasets. Lastly, our work is generalizable to an arbitrary number of concepts and generative factors making it flexible enough to be suitable for various types of data.", "sections": [{"title": "1 INTRODUCTION", "content": "The increasing proliferation of Deep Neural Networks (DNNs) has revolutionized multiple diverse fields of research such as vision, speech, and language [9, 38]. Given the black-box nature of DNNs, explaining DNN predictions has been an active field of research that attempts to impart transparency and trustworthiness in their decision-making processes. Recent research has categorized explainability into progressively increasing levels of granularity. The most fine-grained approaches attempt to assign importance scores to the raw features (e.g. pixels) extracted from the data, while less granular approaches assign importance scores to data points (sets of features). Explaining DNNs using concepts provides the highest level of abstraction, as concepts are high-level entities shared among multiple similar data points that are aligned with human understanding of the task at hand. This makes concept explanation much more global in nature. Many recent approaches for concept-based explanations have attempted to either 1) infer concepts post-hoc from trained models [15] or 2) design inherently explainable concept-based models [1], such as the concept bottleneck model (CBM) [18].\nA parallel field of research on disentanglement representation learning [3, 5, 11, 16] attempts to learn a low-dimensional data representation where each dimension independently represents a distinct property of the data distribution. These approaches learn mutually independent generative factors of data by estimating their probability distribution from observed data. Once the probability distribution of the generative factors is estimated, a given sample can be theoretically decomposed and re-generated from its generative factors. Due to their ability to uncover the underlying generative factors, disentanglement approaches are considered highly interpretable.\nPresent state-of-the-art approaches do not effectively unify disentangled representation learning with concept-based approaches. Approaches like GlanceNets [23] attempt to align concepts with disentanglement with strong assumptions, which are not valid for real-world datasets. To address the above issues, in this paper, we propose Concept Learning using Aggregated Disentangled Representations (CoLiDR), a self-interpretable approach that combines disentangled representation learning with concept-based explainability. Specifically, CoLiDR learns disentangled generative factors using a disentangled representation learning module, followed by the aggregation of learned disentangled representations into human-understandable concepts using a novel aggregation/decomposition module and subsequently a task prediction module that maps concepts to task labels. Our experiments show that interventions on learned concept representations can fix wrongly classified samples, which makes CoLiDR useful for model debugging."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Related Work on Disentangled Representation Learning", "content": "Disentangled representation learning has long been a fascinating study aimed at separating distinct informational factors of variations in real-world data [3, 21]. Due to its probabilistic framework and flexibility to customize training objectives, the variational autoencoder (VAE) [17] is a commonly used architecture in the study of disentangled representation learning, which can capture different factors of variation with its encoder and decoder. Based on the traditional VAE, Higgins et al. [11] proposed a variant, \u03b2-VAE, which introduces an additional hyperparameter to scale the importance of the regularization term. By changing the weight of the regularization term, \u03b2-VAE can control the trade-off between the reconstruction of inputs and the disentanglement of latent variables. Instead of optimizing the Kullback-Leibler (KL) divergence between the latent distribution and standard Gaussian prior for disentanglement as done by VAE and \u03b2-VAE, FactorVAE [16] and \u03b2-TCVAE [5] further decompose the regularization term and propose to directly penalize the total correlation between latent variables, which are shown to better disentangle the variables.\nWhile early studies in disentanglement representation learning attempted to learn independent latent variables by modifying the training objective of VAE [5, 11, 16], Locatello et al. [21] showed that it is impossible to learn identifiable disentangled latent variables without any supervision. A series of subsequent studies were then proposed to learn disentangled representations with better identifiability using different kinds of supervision [22, 33, 34]."}, {"title": "2.2 Related Work on Concept Based Explanations", "content": "Due to the black-box nature of deep learning models, various approaches have been explored to provide explanations for the outcome of deep neural networks [10, 13, 18, 26, 27]. One important direction is to interpret the models with intuitive and human-understandable concepts, which are usually high-level abstractions of the input features. Various attempts have been made to automatically learn the concepts for different tasks [7, 8, 15, 39, 40]. Among them, the concept bottleneck model (CBM) [18] is a commonly used approach to incorporate the learning of concepts into deep neural networks. By constructing a low-dimensional intermediate layer in the models, CBMs are able to capture the high-level concepts that are related to the downstream tasks. Numerous studies have been carried out to adapt CBMs for tasks in various domains [2, 12, 23, 28, 31]. The most relevant works to our proposed approach are Concept Bottleneck Models [18], Concept Embedding Models [42], CLAP [37] and GlanceNets [23], details of which are discussed in the next Section."}, {"title": "3 UNIFYING CONCEPT-BASED MODELING WITH VARIATIONAL INFERENCE", "content": "Problem setup. The concept learning problem is characterized as a two-step process [18] for a given training sample {x, y, C}. Given an observation x, a concept-based model learns a function to map the input to its associated human-understandable concepts C = {C1,\uff65\uff65\uff65, CN}. Subsequently, a predictor will project the concept embeddings to \u0177, which is the prediction for the label y of the sample in a downstream task.\nAssumptions. Following previous work in concept-based models [18, 23], concept models follow two fundamental assumptions as follows: 1) the data distribution in the input space can be accurately mapped to the distributions of concepts in the latent space, and 2) the concept scores are necessary and sufficient to predict labels of samples in downstream tasks."}, {"title": "3.1 Variational Inference-based Concept-learning for Greater Interpretablity", "content": "Utilizing disentanglement approaches to explain the data generation process has been well studied recently [11, 17]. A typical disentanglement system attempts to learn a fundamental representation of mutually independent generative factors (GFs) usually modeled as random variables from independent probability distributions. The Variational Inference (VI) process utilizes the learned generative factors to emulate the data generation procedure. Even though variational inference has been successfully utilized for controlled data generation with success [6, 11], utilizing them to improve the interpretability of concept-based models is still relatively underexplored. Our work attempts to fill this gap. Below, we list the advantages of using variational inference-based approaches for concept learning:\n\u2022 Provides a controlled data generation process: In addition to emulating the data generation process, VIs offer control via interventions over GFs - a desirable property for stakeholders of a concept-based model.\n\u2022 Captures the continuous space mappings: Several works [24, 35] have demonstrated that learning discrete mapping of input samples to concepts is susceptible to fragility. VI smoothens both the input and concept spaces to learn a continuous, well-trained and robust mapping function.\n\u2022 Easier to isolate confounds: Another well-documented problem in concept-based models is the encoding of non-related confounds in the concept representations [41]. VI methods are successful in isolating confounds - hence reducing the effect of encoding noise, spurious correlations, etc. in the concept representations."}, {"title": "3.2 Comparison with Existing Approaches", "content": "We begin the discussion by presenting an anti-causal [14] model to visualize the data generation process of our proposed approach and compare it to multiple comparable recent state-of-the-art approaches. Note that we represent the task labels as Y, the set of concepts as C, the disentangled generative factors as Z, and the data distribution as X. The single-edge arrows model the dependency between distributions while the double-ended arrows represent variational inference - modeling the generative process.\nComparisons to CBM/CEM. Concept Bottleneck Models [18] and Concept Embedding Models [42] do not incorporate variational inference. Nevertheless, they visualize the data-generative process as being conditioned on the annotated concepts and also assume that the task labels are entirely conditioned on the concepts.\nComparisons to CLAP. CLAP [37] is one of the first works utilizing variational inference to model the data generation process. CLAP considers a part of the disentangled space representing relevant GFs to be conditioned on the task labels and a part of it representing confounds to be conditioned on independent normal distributions. However, CLAP does not utilize human-annotated concepts - making the learned disentangled space Z unidentifiable (Refer [22]). This makes CLAP not comparable to our approach.\nComparisons to GlanceNets. GlanceNets [23], to our knowledge, is the only existing approach that attempts to bridge the gap between VI and concept learning. The GlanceNets approach incorporates a Variational Autoencoder based disentangling mechanism to learn the generative factors (GFs) of the distribution and formulate the concept learning problem as a one-to-one mapping between a subset of generative factors (or disentangled representations) and human-understandable concepts. As shown in Figure 2, the annotated concepts directly supervise a part of the learned disentangled space Z. However, in doing so it makes an implicit assumption that the human-understandable concepts act as generative factors of the data distribution. This is a strong assumption for two distinct reasons. Firstly, multiple generative factors can contribute as a constituent of a particular concept and multiple concepts can share the same generative factors. Secondly, human-annotated concepts are by definition, abstract and high-level and do not necessarily model the fine-grained data generation process. Utilizing abstract concepts to supervise the disentangling process may undermine the disentanglement procedure, as disentangled representations are supposed to capture the low-level GFs of the data distribution instead of high-level abstract human concepts. The model is shown to be effective on two synthetic datasets, dSprites and MPI-3D, which are procedurally generated using carefully curated GFs that correspond to human-understandable concepts. In addition, CoLiDR can effectively generalize to datasets where GFs are completely unknown. Unannotated concepts. Note that we model the concept set as a union of known, annotated concepts, and unannotated concepts. This modeling approach is common in unsupervised concept learning approaches like [1], but has not yet been studied for Variational Inference based concept learning. The assumption is that there exists some concepts relevant to the task prediction but are not manually annotated."}, {"title": "3.3 Drawbacks of Supervising GFs with Concepts", "content": "Although it might be tempting to utilize concepts as GFs like what GlanceNets [23] did, it is dangerous for a variety of reasons. In this section, we provide the drawbacks associated with this line of thought with a concrete and practical example. Let us consider as a sample from the CelebA dataset [20] which consists of facial photographs of celebrities. Each image in the dataset is annotated with binary concepts such as the presence of \u201cblack hair\u201d, \u201cblonde hair\", \"wavy hair\", \"straight hair\" which are easily understandable by humans - implying they are abstract. Consider the assumption (fundamental in [23]) that each of these concepts also align with the generative factors of the data distribution as well.\nLimited concept abstraction. As it can be seen, the assumption is flawed - two identical images with straight hair and different colors (Black and Blonde) would be sampled from completely different distributions despite sharing the same type of hair. Images of two people having the same hair color but with wavy and straight hair respectively would also be sampled from completely different distributions despite sharing the same color. Hence, there is no fine-grained control over the actual generative factors. Furthermore, as concepts such as \"attractiveness\" are very subjective and contain a large number of constituent underlying distributions, it is much more effective to understand constituent distributions than directly aligning attractiveness with a single disentangled representation.\nLimited intra-concept hierarchy modeling. By definition of disentanglement, generative factors and concepts would be independent of each other. Any hierarchical relationship cannot be encoded in such a space. For example, the concept \"black hair\" is ideally sampled from mutually independent distributions representing \"blackness\" and multiple other distributions constituting in construction of hair themselves. As such, \"blackness\" can also be combined with distributions modeling facial hairs for \"beard\" concepts.\nHence, instead of directly aligning the concepts with disentangled representations, it is more explainable and reasonable to understand the actual distributions constituting a concept. Relaxing the assumption of aligning disentangled latent space representations with concepts helps us to better capture and explain constituent distributions in each concept - in turn improving explainability."}, {"title": "4 METHODOLOGY", "content": "We first provide a broad overview of the proposed approach CoLiDR in Section 4.1. Section 4.2 details the architectures utilized for computing disentangled representations. Subsequently, Section 4.3 introduces the Aggregation/Decomposition module and the task prediction network. Finally, Section 4.4 describes the training procedure and additional Disentangled Representation (DR) Consistency loss which regularizes the Aggregation/Decomposition module to correctly learn representations to concept."}, {"title": "4.1 Overview of CoLiDR", "content": "CoLiDR consists of three distinct modules as shown in Figure 3. The first module learns the disentangled generative factors from the input data. The second module aggregates the disentangled representations and maps them to human-understandable concepts. The third module maps the concepts to the task label."}, {"title": "4.2 Disentangled Representations Learning (DRL) Module", "content": "The first step of the proposed approach involves learning disentangled representations corresponding to the various generative factors in the data. As depicted in Figure 1, the generative factors form the basis of the data generation process itself. Our DRL module learns the disentangled generative factors that can be used for data generation. Suppose z is the given embedding of the generative factors and x is the corresponding generated data. The underlying data generation process pe(x|z) can be estimated using a \u03b2-VAE, which estimates the maximum likelihood using variational inference. Following [17], we maximize the Evidence Lower Bound (ELBO) to model the posterior distribution q\u03c6(z|x) and the distribution of data generation p\u03b8(x|z) as detailed below.\nELBO = \u2212\u03b2 D_{KL}(q_\u03c6 (z|x) || p(z)) + E_{q_\u03c6 (z|x)} [log p_\u03b8 (x|z)] (1)\nThe E term in Formula 1 comprises the reconstruction loss between the input x and predicted reconstruction \u00ee, usually taken as Mean Square Error (MSE). The prior distribution p is usually taken as a standard Gaussian distribution which encourages the covariance matrix of the learned distribution to be diagonal, enforcing independence constrain [11, 17]. The tunable hyperparameter \u03b2 acts as a quantitative measure of the extent of disentanglement.\n\u03b2-VAE estimates q\u03c6 using an encoder that maps x from the input observation space in \\(R^d\\) to the disentangled representation space in \\(R^k\\), where d and k are the dimensions of the input and latent space respectively. The distribution p\u03b8 is estimated using a decoder that maps the disentangled representation space back to the observation space (\\(R^k\\) \u2192 \\(R^d\\)). Specifically, the encoder encodes the input as an estimated mean z\u03bc \u2208 \\(R^k\\) and a standard deviation vector z\u03c3 \u2208 \\(R^k\\), from which the latent representation z is sampled from the Gaussian multivariate distribution N(z\u03bc, diag(z\u03c3))."}, {"title": "4.3 Aggregation/Decomposition Module", "content": "While the DRL module learns disentangled generative factors in the latent space, the factors may be too fine-grained to be aligned with human understanding. However, these disentangled generative factors can be considered as an independent basis of human-understandable concepts as shown in Figure 1."}, {"title": "4.3.1 Aggregation of Generative Factors into Concepts", "content": "We propose the Aggregation module which aggregates the disentangled underlying generative factors of data into human-understandable concepts. Specifically, given a latent representation z = [z1,..., zk], each concept ci \u2208 {c_i}_{i=1}^N, can be considered as a combination of all disentangled factors z1,..., zk. To increase the expressiveness of our model in learning concepts from the generative factors while keeping each concept as a linear aggregation of different factors for interpretability, we propose to encode each generative factor zj (j\u2208 {1,..., k}) with an independent neural network aj, which learns the non-linear mapping from zj to z\u2019j for concept learning. Subsequently, our model learns the linear combinations of components in z' = [z\u20191,\u2026\u2026,z\u2019k]^T to represent high-level concepts c1,..., cN that are relevant to the downstream task. Given the posterior distribution q\u03c6(z|x) in \u03b2-VAE, the learned concepts can be formulated as\nc = f(A(z)), (2)\nwhere z ~ q\u03c6(z|x), f is a one-layer fully connected model, and A is defined as\nA(z) = [z\u2081,..., z_k] = [a\u2081(z\u2081), a\u2082(z\u2082),\u2026\u2026\u2026, a_k(z_k)]. (3)"}, {"title": "4.3.2 Concept Level Supervision", "content": "Given the annotation of n manually defined concepts, we can supervise the learning of the first n concepts to be aligned with human knowledge. Formally, given the estimated scores c1,..., cn and human annotations l1,..., ln, the supervision can be performed by minimizing\nL_{con} = \u2211_{i=1}^{n} BCE(c_i, l_i), (4)\nwhere BCE is the Binary Cross Entropy Loss. In addition to the supervised learning of human-annotated concepts, our model is designed to automatically capture the information of the remaining concepts cn+1, cn+2, cn from the input in an unsupervised manner as it is not possible to annotate a truly closed set of concepts. Specifically, the unsupervised learning of the unannotated concepts is performed through the training of concept decomposition for the reconstruction of generative factors, which are described in Sections 4.3.3 and 4.4."}, {"title": "4.3.3 Decomposing Concepts into Generative Factors", "content": "Corresponding to the aggregation module, we propose the decomposition module g which learns a mapping from the concept embedding c = [c1,\u2026,cN] back to transformed disentangled representations (z') which are further transformed back to the original disentangled representations z with decoders D = {d1,, dn}. Mathematically,\nz = D(g(c)) = [d_1(g(c)),\u2026\u2026\u2026, d_n (g(c))] (5)\nwhere z is the estimated generative factors from the given concepts c. Here the decomposition module acts as the inverse of the aggregation module and maps concepts back to the disentangled representations. Instead of using one decoder directly for the mapping from c to z, we use independent decoders to project each dimension in z' to the corresponding dimension in z, which mitigates the problem of concept leakage from concepts to unrelated generative factors [24]."}, {"title": "4.3.4 Task Prediction using Learned Concepts", "content": "Task prediction entails the mapping from learned concepts in \\(R^N\\) to the prediction of task labels in \\(R^m\\), where m is the number of categories in classification tasks. The model prediction can be formulated as\np(y|c) = Softmax(h(c)) (6)\nwhere the function h is a shallow neural network. Although h can be chosen up to task requirements, we utilize a linear polynomial to estimate h for model interpretability, with only human-annotated concepts as the input to see how well they can be used to explain the model prediction:\nh(c) = w_0 + \u2211_{i=1}^n w_i c_i, (7)"}, {"title": "4.4 Disentangled Representation Consistency and End-to-end Training", "content": "The three modules of CoLiDR (DRL, Aggregation/Decomposition, Task Learning) are trained end-to-end. To maintain the consistency of learned representations of generative factors, we enforce z and \u00bd to be as similar as possible. We propose to use a DR consistency loss (Ldrc), which can be formulated as:\nL_{drc} = ||z - z\u0302||\u2082 (9)\nFor a given set of disentangled generative factors z, there exists a family of surjective functions f - which map from z' to concepts c. Consequently, there exists a family of functions g inverse coupled with f, which maps from concepts c to z. As the function f is subjective, computing a direct inverse is intractable and hence requires enforced consistency through Ldrc. The function g is NOT a standalone family of functions as they are inverse of a surjective function f.\nIn addition, we also encourage sparsity on the transformed representations z' to identify the most important generative factors that are aggregated to compose the concepts and reduce the impact of non-relevant factors. The overall training objective can be given by:\nL = ELBO + \u03bb_1 L_{con} + \u03bb_2 L_{pred} + \u03bb_3 L_{drc} + \u03bb_4||z\u2019||\u2081. (10)"}, {"title": "5 EXPERIMENTAL SETUP", "content": ""}, {"title": "5.1 Dataset Descriptions", "content": "\u2022 D-Sprites [25]: D-Sprites consists of procedurally generated samples from six independent generative factors. Each object in the dataset is generated based on two categorical factors (shape, color) and four numerical factors (X position, Y position, orientation, scale). The six factors are independent of each other. The dataset consists of 737,280 images. We randomly split data into train-test sets in a 70/30 split.\n\u2022 Shapes3D [4]: Shapes3D consists of synthetically generated samples from six independent generative factors consisting of color (hue) of floor, wall, object (float values) and scale, shape, and orientation in space (integer values). The dataset consists of 480,000 images. We randomly split the data into train and test sets in a 70/30 split.\n\u2022 CelebA [20]: CelebA consists of about 200,000 178 \u00d7 218 sized RGB images of center-aligned facial photographs of celebrities. The faces are annotated with 40 binary concepts like hair color, smile, attractiveness, etc. Some of the features in the set are simple and observable like color (black, blonde) and style of hair (wavy, bangs). However, many concepts are abstract and subjective like attractiveness, heavy makeup, etc. We only consider the objective concepts for experiments. We center-crop the images to 148x148 and subsequently resize them to 64x64.\n\u2022 AWA2 [19]: Animals with Attributes-2 consists of 37,322 images of a combined 50 animal classes with 85 binary concepts like number of legs, presence of tail, etc. We remove certain subjective concepts such as \"eats fish\". AWA2 is neither centered nor cropped and consists of significant background noise, making it significantly harder to disentangle. We resize all images to 64x64 and combine the train and test splits. We use 70% of the data for training and 30% for testing."}, {"title": "5.2 Dataset Task Descriptions", "content": "Synthetic datasets: As dSprites and Shapes3D datasets are procedurally generated, they do not contain an inherent downstream task, hence we construct downstream tasks using combinations of GFs Similar to [23] and [29]. For each task, we consider two GFs at random and a sample has the label \"1\" when all factors satisfy a pre-defined criterion and. For categorical factors, we consider the presence of exact values as Truth, while for continuous factors we use a threshold. More details on task construction can be found in the Appendix.\nReal-world datasets: For the CelebA dataset, the downstream task is cluster assignment [23]. For AWA2, the downstream task is classification."}, {"title": "5.3 Model Implementation Details", "content": "Disentangled representations learning (DRL) module. We utilize two different Variational Autoencoder architectures for estimating disentangle representations in the DRL module. We utilize a standard VAE [17] and a \u03b2-VAE [11]. Even though similar in formulation, a \u03b2-VAE is parameterized by a tunable hyperparameter \u03b2 which controls the strength of disentanglement. For both VAEs, the encoder is a 5-layer CNN with BatchNorm and LeakyReLU as the activation function. The decoder is modeled symmetrically to the encoder with five Transpose Convolutional Layers. The size of the latent space k is set as 64 for d-Sprites and Shapes3D and 512 for CelebA and AWA2 datasets.\nAggregation/Decomposition module. The Aggregation module is composed of the set of transformation operations A and mapping function between transformed representations and concepts f. We model each of the k neural networks a\u00a1 for i \u2208 {0, 1, .., k} as a 3-layer network. For dSprites and Shapes3D the network consists of layers sized [64,64,1] and for CelebA and AWA2 the network consists of layers sized [512,512,1] with ReLU as the activation function. The function f is modeled as a single fully connected layer. Similarly, the Decomposition module is composed of the set of inverse transformation operations D and mapping function between concepts and transformed representations g. We model each of the k neural networks di for i \u2208 {0, 1, .., k} as a 3-layer network. For dSprites and Shapes3D the network consists of layers sized [1,64,64] and for CelebA and AWA2 the network consists of layers sized [1,512,512] with ReLU as the activation function. The function g is modeled as a single fully connected layer.\nTask prediction module. As proposed in [18] and [23], we utilize a single fully connected layer mapping from the concepts to predictions (h). The final output is passed through a softmax layer to compute probability scores for the label."}, {"title": "5.4 Evaluation Setup", "content": "Task and concept accuracy. For a Concept-Based Model to be deemed effective, it is required to be at par on performance with non-inherently explainable models (or black box models). We measure the task accuracy and compare it against a standard Deep Neural Network formed using the same encoder as the VAE and a fully connected layer as the classification head.\nNext, we compare the concept-accuracy of CoLiDR against CBMs as proposed in [18]. For datasets with binary concepts, accuracy is reported as 0-1 error (misclassification). For datasets with non-binary concepts, Root Mean Square Error (RMSE) is reported.\nDisentangled representation aggregation performance. Effective aggregation of disentangled representations serves as the most important desiderata for CoLiDR. However, it is not straightforward to understand the aggregation effect. Unlike supervised disentanglement where each dimension of the disentangled representations is forced to correspond to a concept, CoLiDR aggregates multiple dimensions into a concept. Hence, instead of identifying individually significant dimensions as concepts, it is important to consider a set of representative dimensions from the aggregation module.\nDue to VAE's inherent disentanglement procedure, we can safely assume the mutual independence of dimensions among the learned disentangled representations. Hence, the latent dimensions themselves can be thought of as features of the aggregation module. In effect, the problem of selecting a representative set of dimensions is identical to assigning importance scores to the most important features in a deep neural network. Multiple post-hoc interpretability methods are proposed [30, 36]. We utilize Integrated Gradients (IG)[36] to assign importance scores to each dimension. For an input x and a baseline x' (zero-vector), IG computes attribution scores for each feature i using the following path integral:\nIG(x) = (x_i - x\u2019_i ) \u222b_{\u03b1=0}^1 \\frac{\u2202 F(x\u2019 + \u03b1(x - x\u2019))}{\u2202 x_i} d\u03b1 (11)\nWe utilize Captum (https://captum.ai/) to compute attributions using IG. The values of the attributions are normalized and their absolute values are assigned as the final attributions. Once the attribution scores are computed, we utilize three distinct methods to evaluate the effects of the representative set of disentangled representations.\n\u2022 GradCAM [32] visualizations: For each dimension in the top-k most important dimensions, the GradCAM attribution visualization plots are plotted.\n\u2022 Latent space traversal: For concepts with one dominant dimension in the representative set, we linearly interpolate the normalized latent space between two terminal values to identify visual cues in each generated image\n\u2022 Oracle Classification: Even though the aforementioned visualizations provide qualitative evaluation, a robust quantitative evaluation is required to assuage the effects of confirmation bias. To achieve this, we train a Oracle Network to automatically classify images to the associated concepts with high accuracy. Subsequently, we compare heatmaps generated using CBMs and CoLiDR with the actual ground-truth annotated bounding boxes and report the average Intersection over Union (IoU) scores which measure the ratio of overlap to the combined are between two bounding boxes (Refer Appendix for mathematical formulation). We utilize a fine-tuned VGG-16 model as the backbone of the oracle for each dataset. For more details, refer to Appendix."}, {"title": "Concept decomposition performance (intervention)", "content": "Another desirable desideratum of a Concept-based model is its ease of debugging. For a misclassified sample, fixing the concept annotation by a domain expert should be able to correct the model predictions. Recall that the decomposition module decomposes concepts back into disentangled representations. We define an intervention to be successful for a wrongly classified sample, replacing the predicted concept score with the ground truth concept annotations changes the wrong prediction label to the correct ground truth label y."}, {"title": "5.5 Evaluation Metric Descriptions", "content": ""}, {"title": "5.5.1 Oracle Training", "content": "We utilize a fine-tuned VGG-16 which contains 5 blocks of convolutional layers followed by a max pooling layer at the end of each block. The final output is passed through a 3-layer fully connected network to perform concept prediction. The model is trained similarly to the concept network where each concept is weighted by its relative occurrence in the dataset."}, {"title": "5.5.2 Intersection over Union Calculation (IoU)", "content": "Intersection over Union is defined as the ratio of overlap between two spans of areas on an image. Assuming two bounding boxes A and B, IoU is defined as:\nIoU = \\frac{A \u2229 B}{A \u222a B} (12)\nWe calculate IoU's between heatmaps generated using GradCAM. The selected pixels above a threshold are selected (255 value grayscale CAM maps above 150). As the heatmaps can be irregular in shape, we consider each selected pixel as part of a set on a fixed image size."}, {"title": "6 RESULTS AND DISCUSSION", "content": ""}, {"title": "6.1 Task and Concept Accuracy", "content": "In Table 2, we compare our model against Concept Bottleneck Models (CBMs) [18], which are trained without disentanglement learning, and GlanceNets [23], which involves the learning of disentangled factors, on the average task performance and concept errors. We implement CBMs by replacing the VAE with a standard Autoencoder and the task learning module and the Aggregation/Decomposition module are replaced by identity functions. For GlanceNets, we provide supervision on not only the learned concepts but also a part of disentangled latent space (represented as Gy in [23]). Differently, we do not utilize the Open-Set Recognition Mechanism as we do not specifically study concept leakage.\nAs can be seen in Table 2, CoLiDR variants perform the best among all concept-based models with disentanglement learning and even outperforms CBM on 3 out of 4 datasets. The results show that CBM performs better than GlanceNet and CoLiDR on the AWA2 dataset (0.531). A possible reason for this observation stems from the fact that disentanglement performance on the AWA2 dataset is not effectively captured by VAEs (Refer to Appendix) due to less training data and significant noise in the dataset. Furthermore, the performance of GlanceNet and CoLiDR is at par on dSprites and Shapes3D implying that datasets that are 'easier' to disentangle yield similar performances across methods.\nFor the concept identification task, CoLiDR still achieves the best performance with the lowest concept errors among all models with disentanglement learning. Compared with the model without disentanglement learning, Table 2 shows that our model performs comparable and sometimes better than CBM on dSprtes, Shapes3D, and CelebA datasets. For the AWA2 datasets, CBM outperforms both CoLiDR and GlanceNet. We attribute this to the fact that CBM can learn concepts very flexibly without the regularization of disentanglement. However, this also leads to the problem of learning spurious correlation instead of real semantics of concepts, as shown in Appendix. Note that CoLiDR with no consistency regularization (Ldrc) performs the worst on concept learning - implying strong disentanglement performance is vital to concept learning."}, {"title": "6.2 Disentangled Representation Visualizations", "content": "Figures 4 and 5 respectively demonstrate the most important dimensions constituting a concept. In the Figure 4, we demonstrate the concept \u201cbrown_hair\u201d and its associated dimensions with scores calculated using IG. For example, in Figure 4 GradCAM heatmap visualizations on the two highest computed constituent dimensions for a correctly identified concept \"straight_hair\" (top) and \"wavy_hair\" are shown. We see that both dimensions correctly correspond to the distributions constituting the hair of the people in the image. Similarly, in Figure 5, the GradCAM visualizations corresponding to the concepts \"heart\" (left) and \"ellipse\" (right) are shown. In addition, we also provide a linear interpolation of the identified dimensions. As can be seen, the dimensions are indeed responsible for the shapes of hearts and ellipses."}, {"title": "6.3 Comparision with Oracle Network", "content": "We partition a subset of concepts that can be easily understood in the CelebA dataset and dub them 'simple' (Refer to Appendix for the exact splits). We report average IoU values for correctly classified concepts on both 'simple' and all concepts in Table 3. Column-2, 3 and 4 list the average IoUs for simple concepts"}]}