{"title": "TAGIFY: LLM-powered Tagging Interface for Improved Data Findability on OGD portals", "authors": ["Kevin Kliimask", "Anastasija Nikiforova"], "abstract": "Efforts directed towards promoting Open Government Data (OGD) have gained significant traction across various governmental tiers since the mid-2000s. As more datasets are published on OGD portals, finding specific data becomes harder, leading to information overload. Complete and accurate documentation of datasets, including association of proper tags with datasets is key to improving dataset findability and accessibility. Analysis conducted on the Estonian Open Data Portal, revealed that 11% datasets have no associated tags, while 26% had only one tag assigned to them, which underscores challenges in data findability and accessibility within the portal, which, according to the recent Open Data Maturity Report, is considered trend-setter. The aim of this study is to propose an automated solution to tagging datasets to improve data findability on OGD portals. This paper presents TAGIFY a prototype of tagging interface that employs large language models (LLM) such as GPT-3.5-turbo and GPT-4 to automate dataset tagging, generating tags for datasets in English and Estonian, thereby augmenting metadata preparation by data publishers and improving data findability on OGD portals by data users. The developed solution was evaluated by users and their feedback was collected to define an agenda for future prototype improvements.", "sections": [{"title": "1. Introduction", "content": "In recent times, governments around the world have developed Open Government Data (OGD) initiatives, merging the principles of open government and open data [1]. Governments around the globe publish governmental data on OGD portals that become available for a wide audience of users. Yet, users face difficulties in finding and discovering datasets related to their goals due to lack of sufficient descriptive metadata in open data catalogues [2]. As an increasing number of datasets become accessible, the challenge of information overload arises due to the difficulty in locating specific information [3]. A recommended approach to enhance the discoverability and shareability of published datasets involves employing expressive descriptors, such as tags, effectively [3]. Descriptors constitute a form of metadata, providing details about the content of a resource to assist in its discovery or comprehension [4]. Incomplete or inaccurate metadata inhibits consumers from discovering relevant data for their requirements, leading to the necessity of spending significant time manually searching through portals and the data itself to identify relevant datasets [3, 5].\nWhile tags may seem to be a trivial searching facet, the current practice shows that both their presence and relevance to the actual dataset tend to be a challenge for OGD portals, including the Estonian Open Data Portal, which, according to various open data rankings, is recognized among \u201ctrend-setters\" [20]. Through an analysis of the Estonian Open Data Portal conducted as part of this study (Section II.C), significant shortcomings were discovered in data tagging practices, i.e., among 1787 datasets published (as of April 2024), 11% have no associated tags, while for 26% datasets have only one tag assigned to them. These findings underscore challenges in data findability and accessibility within the portal.\nThe aim of this study is to address the challenges associated with dataset findability and metadata quality by developing a prototype that automates the tagging process by employing a large language model (LLM). The development of such a tool holds significant promise for both data publishers and consumers. Automating the tagging process for data publishers mitigates the risk of datasets lacking tags, a common occurrence on portals where their inclusion isn't mandatory, i.e., the design of the portal does not suggest that they are mandatory. Moreover, this automation minimises the association of datasets with incomplete or inaccurate tags. Consequently, it enhances the"}, {"title": "2. Background", "content": "This section defines the core concepts used in the paper, including OGD and FAIR principles with further determination of the problem this study attempts to resolve."}, {"title": "2.1.Open Government Data vs FAIR data", "content": "Initiatives aimed at fostering Open Government Data (OGD), including the establishment of OGD portals, have seen widespread adoption since the mid-2000s across governmental levels [8]. The Organisation for Economic Co-operation and Development (OECD) defines OGD as both a philosophy and a set of policies aimed at fostering transparency, accountability, and value generation by making government data accessible to the public [9]. To be recognized as OGD these data must be also compliant with principles set by the Open Data Charter, according to which they must be: open by default; timely and comprehensive; accessible and usable; comparable and interoperable; suitable for improved governance and citizen engagement, as well as for inclusive development and innovation [10]. By sharing data that public entities generate in substantial amounts, these data are seen to enhance transparency and accountability to citizens, while their use and reuse promotes the creation of businesses and innovative, citizen-centric services [9]. This movement has been joined by most countries globally.\nAnother concept closely related to OGD that aims to maximise the value and usability of data, albeit within another context, is FAIR. FAIR principles [12] are guiding principles that enable both machines and humans to find, access, interoperate and re-use data and metadata [13,14]. FAIR stands for Findability, Accessibility, Interoperability and Reusability. Findability is the principle, according to which, both humans and computers should encounter minimal difficulty in locating metadata and data resources, where machine-readable metadata plays a crucial role in facilitating automated discovery of datasets and services, thus constituting a fundamental aspect of the FAIRification process. Accessibility requires that after locating the desired data, the user must ascertain the methods for accessing them, which may involve considerations such as authentication and authorization processes. Interoperability sets prerequisites for data to be integrated with other datasets, making them capable of interoperating with various applications or workflows for purposes such as analysis, storage, and processing. Reusability, being the primary goal of FAIR, dictates the need to enhance the efficiency of data reuse. This entails ensuring that metadata and data are well-described so they can be reused in different settings [14].\nAs such, OGD initiatives and FAIR principles share common goals of maximising the value and usability of data by promoting principles of openness, accessibility, interoperability, and reusability. However, although both are related concepts, they serve slightly different purposes, where OGD initiatives focus specifically on making government data open and accessible to the public, while FAIR principles provide a broader framework for ensuring that data, regardless of its source, is findable, accessible, interoperable, and reusable (FAIR). As such, data can be compliant with the open (government) data principles, but not necessarily compliant with FAIR principles and vice"}, {"title": "2.2.Data Findability Issues", "content": "Data published on data portals is subject for search through several approaches, namely, text search that allows dataset search by their title, and faceted search that allows datasets search by facets such as publisher, file format, spatial/geographical coverage, time period keyword and tag-based, with keyword and tag-based search being prevalent [1,16]. While some facets used for dataset search can be automatically retrieved from the data associated with the publisher or the dataset, e.g., dataset format, some facets, such as spatial coverage and tags, are expected to be provided by the data publisher, where the quality of tags (completeness, accuracy etc.), depend directly on the data provided by publishers. These tags being thought of as \u201cexpressive descriptors\u201d [3] play a crucial role in facilitating efficient navigation through data portals [2, 3]. By associating datasets with relevant tags, users can locate datasets relating to specific topics of their interest [2, 3].\nEntering tags manually is slow and prone to human errors, such as tags not always being accurate or relevant to the actual dataset [3, 17]. Additionally, if the portal's design doesn't enforce mandatory tagging, publishers may overlook tagging entirely due to its time-consuming nature, which is one of barriers towards data opening [20]. Inadequate metadata, including descriptions or tags, renders both manual and automated searches ineffective in locating the dataset, thus making the dataset non-findable, inaccessible, interoperable, and consequently \u2013 non-re-usable [18].\nWhile tags may seem to be a trivial facet, the current practice shows that both their presence and relevance to the actual dataset tend to be a challenge for OGD portals, including the Estonian Open Data Portal, as found out in conversation with Estonian Open Data Portal representatives, regardless of the fact that, according to various open data rankings, is recognized among \u201ctrend-setters\u201d [20]. To this end, to assess the relevance of the topic of this study and Estonian open data as a domain of application, an analysis of datasets available on the Estonian Open Data Portal was conducted with the aim to examine the relevance of the issue in question, i.e., lack of or insufficient quality of tags associated with published datasets on Estonian Open Data Portal."}, {"title": "2.3.Analysis of datasets tags in Estonian Open Data Portal", "content": "To analyse the number of tags associated with each dataset on the portal as defined by data publishers, a Python scraping script was developed (see Fig. 1), the code of which is available in a Github repository\u00b9. The script operates as follows:\n\u2022\nlist of all datasets on the portal is retrieved using the get_datasets_list() function, which iterates over each dataset. get_datasets_list() fetches datasets from the API endpoint https://avaandmed.eesti.ee/api/datasets;\n\u2022\nfor each dataset, information is retrieved using the get_dataset(uuid) function, where the parameter uuid is the dataset's unique identifier. It fetches the detailed information from the API endpoint https://avaandmed.eesti.ee/api/datasets/{uuid};\n\u2022\nthe length of the datasets's tag/keywords field is determined. The count of tags for each dataset is then incremented in the counts dictionary;\n\u2022\nonce all the retrieved datasets are processed, the dictionary containing counts for each number of tags is printed out.\nThe analysis performed in accordance with this procedure, uncovered significant negative trends in datasets tagging practice, thereby confirming the relevance of the study objective. Out of the 1787 datasets published (as of 23.04.24), 190 datasets (11%) lacked any associated tags, while 457 (26%) had only one tag assigned to them. This infringes the principles of FAIR, i.e., if a dataset is lacking relevant metadata such as tags, it will be more difficult for interested parties to find it (infringes findability) and to integrate it with other datasets (infringes interoperability) [14]. As a result, lack of keywords/tags will make the dataset less likely to be reused, which infringes reusability [14]. This indicates potential areas for improvement in dataset tagging within the portal through augmentation of this process, which is a central objective of this study. As advancements in artificial intelligence technologies continue, they can be harnessed to enhance the discoverability of data through automated tagging of datasets. Moreover, automation elements are inherent to the FAIR vision [19]."}, {"title": "3. Implementation Strategy and Technological Framework", "content": "The objective of the study is achieved by automating dataset tagging, which, in turn, is achieved by employing a LLM that powers TAGIFY \u2013 a prototype of tagging interface. LLM is appropriate for this purpose, as it was found to be useful for predicting tags from partial content of a dataset [22]. As such, the following steps outline the process of automatically tagging a dataset:\n1. the LLM gets a system prompt describing to it which data it will receive, which task it has to do and how its response should be formatted. A system prompt is a message that can be used to specify the persona used by the model in its replies [23]. Instructions to the LLM are provided in English;\n2. then, the LLM is provided with the first rows of a dataset, including the dataset's header row. The number of rows provided to the LLM is 10. Experimentation has shown that this number of rows is one of the lowest that still allows the LLM to generate relevant tags. Moreover, every additional row provided for analysis would increase the computational resources required, thus making the process more expensive. Furthermore, this number of rows also fits inside the input token limit of the LLM, which determines the maximum length of the input string that the LLM can accept;\n3. after processing the input, the LLM outputs a list of relevant tags. The tags are in English. The number of tags to output can be chosen by the user, of which the LLM is informed through the initial system prompt;\n4. finally, in addition to the English tags generated by LLM at step 3, translation of the generated tags in Estonian, as the language of a portal with which it will be tested and to which it is planned to integrate it to, is returned"}, {"title": "3.1.Interfacing with the LLM", "content": "Communication with the LLM is achieved through a RESTful web service, which handles interfacing with the LLM's API. A RESTful web service is a web application that adheres to REST standards [24]. Web services enable various organisations or applications from diverse origins to interact without the necessity of exchanging sensitive data or IT infrastructure [25]. Developing the project as a web service has the benefit of not limiting the project to the Estonian Open Data Portal or OGD portals in general, thereby making it environment-agnostic, which will make it convenient to integrate the tagging service with other products.\nAdditionally, a basic graphical user interface (GUI) is developed to interface with the web service. This is done to allow for a more streamlined and user-friendly usability testing (section 5). In order to facilitate usability testing over distance, the application is deployed to the cloud. By implementing this approach, users will be spared the need to set up the application locally, thus alleviating the associated inconvenience. Furthermore, it ensures that sensitive API keys remain protected and do not need to be shared with users during the testing phase."}, {"title": "3.2.Technology Choices", "content": "This section presents technological choices made to develop an automated tagging service, with the reference to both LLM, web service framework, GUI framework, translation service and cloud provider.\n1) Large Language Model: Since the prototype under development is LLM-powered, the first technological choice concerned which LLM to use. The factors that determined the choice of LLM were performance, cost and ease of implementation. Several benchmarks have been developed to evaluate the performance of a LLM, such as HELM (Holistic Evaluation of Language Models), which is a research benchmark developed by the Stanford CRFM (Center for Research on Foundation Models) to assess performance across a variety of prediction and generation scenarios, Open LLM leaderboard by HuggingFace, which is a leaderboard for open source LLM evaluation across 4 benchmarks - MMLU, TruthfulQA, HellaSwag and A12 reasoning, and Chatbot Arena by LMSys, which is a benchmark utilising an Elo-derived ranking system, aggregated over pairwise battles [26]. However, there is no widely used benchmark for evaluating performance of LLMs as data annotators [26]. To this end, we referred to a technical report by Refuel [26] to find a LLM with the best trade-off between label quality and cost. The report evaluated the performance of 6 LLMs, namely Text-davinci-003, GPT-3.5-turbo, GPT-4, Claude-v1, FLAN-T5-XXL, PaLM-2 for labelling datasets. The report identified that the top-3 LLMs with the best trade-off between label quality and cost are FLAN-T5-XXL, PaLM-2 and GPT-3.5-turbo, which were further considered for the purpose of this study. An additional investigation of the three LLMs revealed that FLAN-T5-XXL requires self-hosting, which increases the complexity of developing the solution. PaLM-2 and GPT-3.5-turbo offer a paid API, which is easier to implement than a self-hosted LLM. As the performance of the 2 models is similar, where GPT-3.5-turbo generates better quality labels compared to PaLM-2 in 5 out of 10 datasets, while the cost per label of PaLM-2 is ~70% higher [21], the choice to use GPT-3.5-turbo was made.\nIn addition, during development, the decision to include GPT-4 as an additional option was made, incl. to better evaluate GPT-3.5-turbo.\n2) Web Service: A RESTful web framework was used to develop a HTTP-based API for accessing the web service. The choice of framework was FastAPI\u0399 a \"web framework for building APIs with Python 3.8+ based on standard Python type hints\" [27], as according to independent benchmarks by TechEmpower, FastAPI is considered as one of the fastest Python frameworks available, only below Starlette and Uvicorn [28]. FastAPI is built upon Starlette, which itself is built upon Uvicorn, which explains the differences in performance as this hierarchical architecture inherently introduces additional layers of abstraction, resulting in increased overhead [28]. But as an added benefit, FastAPI provides more features on top of Starlette, such as data validation and serialisation that are essential to building APIs [28]. By using a higher-level framework such as FastAPI, development time is saved and similar performance to a lower-level framework, such as Starlette, can be achieved as features missing in Starlette would have to be developed manually [28]. In addition, OpenAI (the company that offers the GPT-3.5-turbo and GPT-4 models) provides official Python bindings for using their models [29], which makes using a Python-based framework convenient."}, {"title": "3) Graphical User Interface", "content": "When making a decision about a graphical user interface, a choice in favour of one of two options should be made, namely a desktop application or a web application. A front-end web application as the graphical user interface was chosen to facilitate a more seamless user-testing experience. The decision was influenced by several factors. Notably, web applications offer the advantage of immediate accessibility without the need for installation, ensuring users can swiftly engage with the application across different devices and operating systems [30]. While it is acknowledged that web applications rely on an internet connection, which could be perceived as a limitation [30], usage of the LLM requires an Internet connection regardless. Therefore, this potential drawback becomes irrelevant in the context of this study.\nNode.js and React stand as two of the most used front-end web frameworks globally [31]. Node.js is an open-source JavaScript runtime environment that facilitates the development of servers and web applications [32]. Conversely, React is described as a \"library for web and native user interfaces\" [33]. Given that Node.js is predominantly tailored towards API creation, while React is renowned for its prowess in creating user interfaces [34], the decision to use React was made due to its better alignment with the project's requirements."}, {"title": "4) Machine Translation Service", "content": "For this project, the criteria for choosing a machine translation service was that it must be accurate and have an accessible API. According to research conducted by Intento [35], DeepL emerged as the top-performing neural machine translation service. DeepL offers a free, although limited, access plan to access their API. Additionally, the existence of an official Python library maintained by DeepL facilitates its convenient integration into the application. Considering these factors, the decision to use DeepL as the project's machine translation service was made. Although Google Translate was initially considered during the project's early stages, a comparative analysis revealed that DeepL consistently delivered more accurate translations. This performance disparity ultimately solidified DeepL as the preferred choice for the project."}, {"title": "5) Cloud Provider", "content": "When selecting a cloud provider, the primary criteria were cost-effectiveness and ease of application deployment. For this project Vercel was chosen. Vercel is a cloud-based platform specifically tailored for hosting static sites and serverless functions, offering developers a streamlined process in developing and launching web projects [37]. Vercel offers the ability to run back-end code as serverless functions [37]. A serverless function embodies business logic that operates without retaining data (stateless) and has a temporary lifespan, being created and then terminated [38]. These functions persist for short durations, mere seconds, and are intended to be triggered by a specific condition, such as an user making a request. Given that the web service does not need to retain data and only needs to run upon a request, the utilisation of serverless functions was deemed aligned with the project. In addition, Vercel offers a free tier and its straightforward deployment process further solidified its suitability."}, {"title": "4. Implementation", "content": "In this section, implementation of the prototype back-end is presented, with subsequent presentation of the front-end. Finally, the process of hosting the application is presented."}, {"title": "4.1.Back-end", "content": "The back-end of the developed prototype consists of 3 main modules: (1) API endpoint, (2) OpenAI service and (3) translator service, each playing its own crucial role:\n\u2022\nAPI endpoint accepts requests and validates received data from the user, which is expected to be a .csv file. The data consists of the first 10 rows of a to-be tagged dataset, including its header row;\n\u2022\nOpenAI service handles interfacing with OpenAI API. It creates a system prompt, appends data received from the API endpoint to a user prompt and sends both messages to the LLM. It, in turn, receives a response from the LLM with tags (generated by the LLM in English);\n\u2022\nTranslator service handles interfacing with DeepL API. It takes tags received from OpenAI service and translates them to another language, which within the context of the study is Estonian;\n\u2022\nConfig module handles loading environment variables into the application. The necessary environment variables for the back-end application to function are front-end url, OpenAI API key and DeepL API key.\nIn addition, the back-end project contains a requirements file for required Python packages and a Vercel configuration file for the prototype application deployment purposes. The required Python packages for the project are fastapi, pydantic, pydantic-settings, python-multipart, uvicorn, openai, deepl and all dependencies of the"}, {"title": "1) API Endpoint", "content": "The API endpoint accepts data sent via HTTP POST method. Furthermore, the endpoint is mapped to the \u201c/\u201d route, also known as the root route. As there are no other endpoints in the application, it is sufficient to accept requests only on the root route.\nReceived data is validated to prevent unexpected behaviours in the application. The API endpoint accepts a body consisting of a matrix, where the matrix represents data from a dataset. In addition, the endpoint accepts count and model as query parameters from the user. These determine how many tags the LLM should generate and which LLM model should be used, respectively. The default values for these parameters are 5 tags and GPT-3.5-turbo model. The validation logic sets the following rules for the received data:\n\u2022\nlength of data in the request body, which represents the number of rows of a dataset, must be a maximum of 10 lines;\n\u2022\ncount should be in the range of 3 to 10;\n\u2022\nmodel should be either GPT-3.5-turbo or GPT-4.\nIf any of the validations fail, a HTTP exception is returned as a response shown to the user, specifying the nature of the error (see Fig. 2)."}, {"title": "2) OpenAI Service", "content": "The OpenAI service defines a function handle_tagging that uses OpenAI API to generate tags for a dataset. Communication with OpenAI API is handled by OpenAI Python library. The function takes a list of records from a dataset, the number of tags to generate, and the model to use as input parameters, all provided by the user. The function builds messages to send to the OpenAI API, formats the data into a user message, sends the messages to the API, retrieves the generated tags, splits them into English tags, and then translates them into Estonian using translator service. Finally, it returns a dictionary containing both English and Estonian tags (see Fig. 3)."}, {"title": "3) Translator Service", "content": "To ensure tags are generated in a language other than English, such as Estonian, as is the case for this study, translator service is used. The service defines a function translate_text that uses DeepL API to translate tags originally generated by the LLM. Interfacing with DeepL API is handled by the DeepL Python package. The function accepts a list of tags, source language and destination language as input parameters, which in this case are English and Estonian, respectively. The function translates every string in the input list and returns the translated strings as a list (see Fig. 4)."}, {"title": "4) Config module", "content": "The config module defines a Settings class that inherits from BaseSettings provided by Pydantic - a library for data validation and settings management. It specifies the environment variables required for the application, namely frontend_url, chatgpt_api_key, and deepl_auth_key. Then, it creates an instance of the Settings class to load the values of these environment variables (see Fig. 5). This approach ensures that the application's settings are correctly loaded and validated from the environment. Additionally, this setup enables anybody to run the application and use their own environment variables seamlessly."}, {"title": "4.2.Front-end", "content": "The front-end architecture is centred around a single React component named App, functioning as the primary entry point for the application.\n1) Dependencies: The application's required packages are defined in a package.json file. For a React app to operate, the main dependencies are react, react-dom and react-scripts. In addition, the developed React application also makes use of the react-drag-drop-files package to handle file uploads through drag-and-drop functionality. When starting the app, the environment variable REACT_APP_BACKEND_URL must be defined to specify the backend server's URL.\n2) State Management: The useState hook from React is used to manage component state. The App component utilises the useState hook to manage states of tags, selectedNumberOfTags, selectedModel, error, and isLoading (see Fig. 6). These states are essential for tracking the uploaded file, selected parameters, error messages, and loading status."}, {"title": "3) File Upload and Tag Generation", "content": "The handleChange function is invoked upon uploading a file. It utilises the readCsv utility function to extract data from the uploaded file. The readCsv utility function parses the CSV file uploaded by the user, preparing the data for transmission to the backend. This function accepts a single parameter file, representing the uploaded CSV file, and returns a Promise resolving to an array containing the first 10 rows of the parsed CSV file data (see Fig. 7). Parsing CSV data in the front-end offers the advantage of bypassing the need to transfer large files to the back-end for processing. Consequently, this approach eliminates the need for a size limit on file uploads, with the maximum size being solely dictated by the browser, e.g., 4GB limit in Chrome."}, {"title": "4) User Interface", "content": "The handleChange function is invoked upon uploading a file. It utilises the readCsv utility function to extract data from the uploaded file. The readCsv utility function parses the CSV file uploaded by the"}, {"title": "4.3.Hosting", "content": "Vercel facilitates automatic deployments triggered by changes to the respective front-end or back-end folders within the main branch of the source code's repository. Both the front-end and back-end source code are hosted in a single repository. A repository that contains multiple projects, such as the back-end and front-end, is called a monorepo [39]. The deployment of the application on Vercel is separated into 2 different Vercel projects: gpt-tagger and gpt-tagger-frontend. This is Vercel's recommended approach to deploying applications that use a monorepo [40]."}, {"title": "5. Evaluation of the Prototype", "content": "The primary objectives of this testing were to assess the application's functionality, relevancy of generated tags, the quality of translations from English to Estonian, user-friendliness, and gather general feedback for further improvement of the prototype. In the following subsections, the methodology and results of the evaluation are presented."}, {"title": "5.1.Prototype Evaluation Methodology", "content": "The evaluation of the prototype application involved conducting usability testing through a Google Forms survey. The survey was designed with three sections, each aimed at assessing specific aspects of the prototype, which are described below. Before taking the survey, participants were introduced with a brief description of the survey purpose (incl., its objective, brief overview of the process, and the length) and prototype, informed about consent for further use of collected data, as well as specifying that datasets uploaded are processed according to OpenAI's enterprise privacy.\nThe first part of the survey aimed to evaluate tagging accuracy of the prototype with pre-defined sample datasets. Participants were provided with instructions on the prototype use and links to two sample datasets sourced from the Estonian Open Data Portal. Participants were asked to try out the prototype by following the instructions on its use provided within the survey, by uploading respective datasets to the prototype. In the survey, respondents were required to answer the same set of questions for each dataset provided.\nThe first question was \u201cHow relevant are the generated tags to the actual content of the datasets?", "not relevant at all": "nd 5 to \u201cvery relevant\u201d. If a low score was assigned, the participant was followed up with an additional question asking for a justification for this score. Then, evaluation of how the parameter \u201cnumber of keywords", "Yes, improves relevancy significantly": "Yes, improves relevancy slightly", "No, does not improve or worsen relevancy": "nd \u201cNo, rather worsens relevancy", "If tags relevancy worsens, how and at which number of keywords?": "Afterwards, the participants were asked which LLM produced more relevant better tags with options being \u201cGPT-3.5-turbo\u201d, \u201cGPT-4\u201d and \u201cBoth had results of similar relevancy\u201d. Finally, the respondents were asked to assess the combination of different parameters, with the question being \u201cWhich combination of the options \"number of keywords\" and \"model\" seemed to produce the most relevant results?\u201d. This question was open-ended. Additionally, Estonian speakers were asked to assess the accuracy of Estonian translations of tags. As being a native speaker of Estonian was not a mandatory prerequisite for participating in the survey, this question was optional.\nThe second section of the survey provided participants with the opportunity to try the prototype application with their own datasets. While this section was optional, participants were encouraged to test the application with a dataset of their own choice, while providing links to Estonian Open Data Portal and European Data Portal, from which open dataset could be selected by them. After testing their dataset, participants were asked to share any observations or feedback they have regarding the tagging process. This feedback was collected to map potential areas of the prototype for improvement."}, {"title": "5.2.Evaluation Results", "content": "The survey was distributed through social media, emailing to Estonian Open Data Portal representatives and personal channels, gathering in total 22 responses. The survey was targeted at individuals, who actively work or engage with datasets within their professional or personal domains.\nMost respondents found generated tags relevant to the actual content of the datasets with an average value being 4.4 of 5 points, i.e., predominantly relevant, with no 1 or 2 points received. In cases, where respondents found tags to be less relevant (3 to 4 points), reasoning behind low relevance score was justified by respondents through the fact that while most tags were relevant to the dataset, some were overly specific, failing to encapsulate the broader essence of the datasets.\nAbout 74% of respondents reported that changing the number of tags to be generated option improves relevancy with the largest share reporting that it improves changes slightly. From the obtained open-ended question seeking to find how and at which number of keywords tags relevancy worsens, a consensus emerged that increasing the number of tags generally enhanced accuracy or provided opportunities to discern more precise tags amid less accurate ones.\nThe majority of respondents (65%) highlighted that the best performing model was GPT-4, with the prevailing dominance of respondents highlighted that the combination of GPT-4 and utilizing 5 or more keywords appeared to consistently yield the most relevant outcomes for respondents.\nFinally, as regards the Estonian tag translations, most answers accumulated to the values of 4 and 5 with no respondents assessing it with 1 or 2.\nWhile participants found that GPT-4 generally outperforms GPT-3.5-turbo with generating tags, it was pointed out that in some rare cases the LLM returns incomprehensible output instead of relevant tags. Furthermore, several comments were made about the Estonian translations differing when using GPT-3.5-turbo and GPT-4, although these models were not used for translation, as a separate translation service was used to translate the English tags to Estonian (see Section 3 and 4).\nThe prototype was found by participants useful with 54% respondents rated the usefulness of the prototype with the score 4, and 27% participants found it to be very useful, thus giving it 5 of 5 points. The reasons for lower usefulness scores were commented by respondents to be due (1) the LLM has a hallucination problem, i.e., sometimes irrelevant tags are produced; (2) a tool is standalone, whereas it would be more useful if it was integrated into an open data portal; (3) multiple different combinations of \u201cnumber of keywords\u201d and \u201cmodel\u201d must be tried in order to find optimal tags."}, {"title": "6. Discussion", "content": "The prototype developed within this study received positive feedback from participants in several key areas. Firstly, respondents generally rated the relevance of the generated tags highly, with an average rating of 4.4 out of 5. This indicates that the prototype effectively captured the essence of the datasets. Moreover, a significant portion of participants reported that adjusting the \"number of keywords\" option improved tag relevancy, suggesting flexibility in fine-tuning the tagging process. Additionally, most respondents favored the \"GPT-4\" model for its superior performance in tag generation compared to GPT-3.5-turbo that was originally selected for its superiority over other LLMs (Section III.C). The combination of \"GPT-4\" with five or more keywords emerged as the most effective strategy for producing relevant tags consistently. Furthermore, Estonian speakers generally expressed satisfaction with the accuracy of the Estonian tag translations.\nDespite the positive reception, user feedback identified areas for improvement in the prototype. Notably, some participants encountered instances, where the application produced irrelevant tags or incomprehensible output. In addition, some feedback highlighted that certain tags were overly specific, failing to encapsulate the broader content adequately. Furthermore, feedback regarding user interface and functionality emphasized concerns, such as the need to reupload files when adjusting parameters, limitations in supported file types and the standalone nature of the tool.\nTo improve the usefulness of the application, issues with tagging accuracy (although pointed to by a minority of participants) must be addressed. These issues could be addressed by refining the initial system prompt provided to the LLM or by supplying more than 10 rows of dataset content for the LLM to analyze. Although experimentation has shown that 10 rows is one of the lowest thresholds that still allows the LLM to generate relevant tags, where every additional row provided for analysis would increase the computational resources required, thus making the process more expensive (Section III), increasing the amount of data the LLM processes allows it to make better generalizations based on the dataset.\nFurthermore, feedback regarding user interface and functionality emphasised concerns such as the need to reupload files when adjusting parameters, limitations in supported file types and the standalone nature of the tool. These recommended improvements to the user interface can be implemented in the future to enhance user experience, particularly focusing on the interaction with the file upload logic. This includes expanding the file type support to common formats such as JSON, HTML, XLS, XLSX, and XML, and ensuring parameter values can be changed dynamically without the need to re-upload the dataset. The latter, namely, stand-alone nature of the tool stressed by evaluators, however, is due to the fact that the evaluated artefact is a prototype, which was made publicly available by hosting it as a stand-"}]}