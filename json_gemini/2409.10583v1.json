{"title": "Reinforcement Learning with Quasi-Hyperbolic Discounting", "authors": ["S.R. Eshwar", "Mayank Motwani", "Nibedita Roy", "Gugan Thoppe"], "abstract": "Reinforcement learning has traditionally been studied with exponential discounting or the average reward setup, mainly due to their mathematical tractability. However, such frameworks fall short of accurately capturing human behavior, which has a bias towards immediate gratification. Quasi-Hyperbolic (QH) discounting is a simple alternative for modeling this bias. Unlike in traditional discounting, though, the optimal QH-policy, starting from some time t\u2081, can be different to the one starting from t2. Hence, the future self of an agent, if it is naive or impatient, can deviate from the policy that is optimal at the start, leading to sub-optimal overall returns. To prevent this behavior, an alternative is to work with a policy anchored in a Markov Perfect Equilibrium (MPE). In this work, we propose the first model-free algorithm for finding an MPE. Using a two-timescale analysis, we show that, if our algorithm converges, then the limit must be an MPE. We also validate this claim numerically for the standard inventory system with stochastic demands. Our work significantly advances the practical application of reinforcement learning.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement Learning (RL) [16], [3] looks at identifying a policy/strategy for an agent to optimally complete a task with sequential decisions. So far, a strategy 's optimality has been decided based on either the expected exponentially discounted sum or the long-term average of the sequence of rewards received under that strategy. That is, based on either $\\sum_{n=0}^{T-1} \\gamma^n r_n$ or $\\lim_{T\\rightarrow\\infty}\\frac{1}{n}\\sum_{n=0}^{T-1} \\Gamma^n$, where $r_n$ is the expected reward under policy at time $n$ and $\\gamma\\in [0,1)$. Exponential discounting is preferred when the agent has impatience, i.e., immediate gains have emphasis over future gains, with the emphasis level decided by the y value. In contrast, the average of the rewards is preferred when the present and future rewards are to be treated equally. However, evidence is now growing that these discounting ideas fail to model human behaviors accurately [7].\nHumans are known to be impatient over shorter horizons, but not so much over longer horizons. That is, we have a bias towards instant gratification. This can be understood from the famous example by Richard Thaler [18], who said, \"Most people would prefer one apple today to two apples tomorrow, but they prefer two apples in 51 days to one in 50 days.\" Observe that there is a reversal of preferences when the time frame shifts. This phenomenon is known as the common difference effect [7]. Such preference reversals cannot happen under a policy that is optimal with respect to either of the two traditional discounting models. This is because of their time-consistent nature [16], i.e, this optimal policy remains optimal even when reconsidered from some later time as well. This demonstrates the limitations of these discounting models in explaining human behaviors.\nHyperbolic discounting [13] is a leading candidate [1], [6], [8] for explaining the common difference effect. The value of a strategy \u4e93 under this discounting model is $\\sum_{n=0}^{100} b_n r_n$, where $r_n$ is defined as before and $b_n = (1 + \\kappa_1 n)^{-k_2/k_1}$ for some $\\kappa_1, \\kappa_2 > 0$. However, this form of discounting is quite complicated, making its study hard. This brings forth Quasi-Hyperbolic (QH) discounting [14], [12], which is a simpler and more tractable alternative. In QH discounting, $b_0 = 1$ and $b_n = \\sigma \\gamma^n$, $n > 1$, for some $\\sigma\\in [0,1]$ and $\\gamma\\in [0,1)$. The symbol o is the short-term discount factor, while \u03b3 is the long-term discount factor. Clearly, for \u03c3 = 1, QH discounting matches exponential discounting. \nUnder exponential discounting, the optimal policy * is deterministic and stationary, and has a greedy relationship with its Q-value function [16]. However, such properties do not always hold under QH-discounting [11]. This can lead to a complicated agent behavior, as we now illustrate. Consider the two-state Markov Decision Process (MDP) setup of Fig. 1b, which is taken from [11]. Clearly, there are only two deterministic stationary policies here: f = (f, f,\u2026) and \u011f \u2261 (g, g, . . .), where f maps state 1 to action a1, and g maps state 1 to action a2, and both map state 2 to action a1. For \u03c3 = 0.5 and y = 0.8, their Q-value functions under QH-discounting are given in Table 1c. For a policy \u3160, its QH Q-value function, denoted by $Q^\\pi$, is defined in the same way as in the exponential discounting case, but with discount factors 1, \u03b3, \u03b3\u00b2, . . . replaced by 1, \u03c3\u03b3, \u03c3\u03b32, ... In Table 1c, notice that f yields the highest returns from state 2. However, at state 1, neither f nor \u011f shares a greedy relationship with its QH Q-value function. This fact implies that gf (acting as gat n = 0 and f for n > 1) is the policy that yields the highest returns, starting from state 1. The optimal policies in the above example have three interesting dissimilarities compared to their counterparts in RL with exponential discounting or simple averaging. Firstly, the optimal policy varies depending on the initial state of the process. Secondly, gf is non-stationary. Thirdly, and significantly, both f and gf display time inconsistency. To elaborate the last point further, note that both gf and f advocate following f at any n \u2265 1. Now suppose, at time n = 1, the MDP is in state 1 and the agent re-evaluates the optimal policy from that time onward. Then, from Table 1c, the agent would again discover gf to be optimal, i.e., act as per gat n = 1 and revert to f thereafter. This behavior contradicts the one that is optimal from n = 0, highlighting the time inconsistency.\nWe now describe a complex agent behavior for the above setup, stemming from the time-inconsistent nature of the optimal policies. Consider the agent as a sequence of selves, each corresponding to a different time step n. Suppose each future self is naive, i.e., unaware of the time inconsistency in the optimal policy. Alternatively, suppose they all have self-control issues and a possibility to act contrary to their own interests. In both scenarios, the following scenario could unfold. Each time the MDP visits state 1, the naive selves recalculate the optimal strategy from that point on and decide to act as per g at that time step. Similarly, each self with control issues could also decide to act as per g because (i.) it is aware that if the subsequent selves act as per f, then it would receive higher returns, and (ii.) it presumes that the subsequent selves will act as per f. However, if all selves act as per g for all n \u2265 0, then Table 1c shows that the expected overall returns would be substantially lower (only 16.77).\nTo safeguard against the above pitfall, it is desirable to have a stationary (possibly stochastic) policy \u4e93 = (\u03c0, \u03c0, ...) from which there is no incentive for deviation. For such a policy, it would then be true that\n$Q^\\pi(\\delta, s) = \\sup_{\\nu:S\\rightarrow\\triangle(A)} Q^\\pi(\\delta, v)$, $s \\in S$, (1)\nwhere S (resp. A) is the MDP state space (resp. ac- tion space), \u0394(A) is the set of distributions on A, and $Q^\\pi(s,v) = \\sum_{a \\in A(s)} v(a|s)Q^\\pi(s, a)$ is the average of \u03c0's QH Q-values for a starting state distribution $v : S \\rightarrow \\Delta(A)$. Any stationary policy \u4e93* which satisfies (1) is called a Markov Perfect Equilibrium (MPE) [11]. For our two-state MDP example, Table 1c shows that the stationary policy h is an MPE. For a general MDP, MPEs neither need to exist nor be unique; so far, they have been found only using analytical techniques, and that too only for simple MDPs [11].\nOur goal in this work is to develop a model-free RL algorithm to identify an MPE in a finite state and action MDP. An MPE's existence here is guaranteed by the condi- tions outlined in [11]. However, finding it poses significant challenges, unlike finding an optimal policy in classical RL. Firstly, no known Bellman-type contraction mapping exists for which an MPE's Q-value function is a fixed point. Consequently, the traditional fixed-point-type methods cannot be used to find this value function. Moreover, even if this function were identified somehow, determining the MPE itself remains challenging as it lacks a straightforward relationship with its value function. Recall that the optimal policy in classical RL is greedy with respect to its Q-value function. Secondly, MPEs often are stochastic. This means the search space for an MPE encompasses all stochastic policies, which is infinitely large even for finite state and action MDPs. In contrast, for traditional discounting, the optimal policy search is confined to deterministic policies, which is a finite set (albeit growing combinatorially).\nOur main contributions are as follows. We propose the first model-free RL algorithm for finding an MPE. This algorithm is a two-timescale stochastic approximation and is inspired by the recently proposed critic-actor method for classical RL [4]. Unlike the actor update in [4], which follows a stochastic estimate of the value function's gradient, our method updates"}, {"title": "II. SETUP, GOAL, ALGORITHM, AND MAIN RESULTS", "content": "In this section, we describe our problem setup, our goal, and our key contributions: the first algorithm for finding an MPE and results that describe its asymptotic convergence.\nA. Setup and Goal\nLet A(U) denote the set of distributions over a set U. Our setup consists of an MDP M = (S,A,P,R,\u03c3, \u03b3), where S and A are finite state and finite action spaces, respectively, P: S\u00d7A \u2192 A(S) is the transition matrix, and r: S\u00d7 A\u2192 R is the instantaneous reward function. Further, \u03c3, \u03b3 \u0395 [0, 1) are the parameters of QH discounting. Within the above setup, our goal is to find an MPE, i.e., a stationary policy \u03c0\u2261 (\u03c0,\u03c0, ...) (henceforth denoted only by \u03c0) that satisfies the MPE relation given in (1).\nB. MPE-learning Algorithm\nFor a stationary policy \u03c0 : S \u2192 A(A), let $Q^\\pi$ denote the policy \u03c0'\u03c2 QH Q-value function. Formally, let\n$Q^\\pi(s, a) := r(s, a) + E_{\\begin{smallmatrix}S_n \\sim P(\\cdot|S_{n-1}, a_{n-1}) \\\\ a_n \\sim \\pi(\\cdot|S_n)\\end{smallmatrix}} \\sum_{n=1}^{\\infty} \\sigma\\gamma^n r(S_n, a_n),$\n(2)\nwhere $s_{n+1} \\sim P(\\cdot|s_n, a_n)$ and $a_{n+1} \\sim \\pi(\\cdot|s_{n+1})$. Also, for \u03b8\u2208 R|S|\u00b7|A| and s \u2208 S, let $\u03c0_\\theta(\\cdot|s) = softmax(\\theta(s,\\cdot))$. Our novel approach for finding an MPE is given in Algorithm 1. The symbol $\u03c0_{\\theta_n}$ parameterizes the policy rep- resenting our MPE estimate at time n \u2265 0, while $W_n$ is this policy's Q-value function estimate. Hence, we refer to the $\u03c0_{\\theta_n}$ update (Step 11) as the actor update, and to the $W_n$ update (Step 10) as the critic update. Throughout this work, we focus on the scenario where the stepsizes $\u03b1_n$ and $\u03b2_n$, used in the critic and actor updates, respectively, satisfy the relation $\\lim_{n\\rightarrow\\infty} \\frac{\u03b1_n}{\u03b2_n} = 0$. This ensures the critic updates are on a slower timescale compared to the actor. Consequently, Algorithm 1 falls under the category of critic-actor algorithms [4] (instead of actor-critic). We give a principled motivation for our algorithm in Section III.\nC. Main Results\nWe first state our assumptions.\nA1) Stepsizes: $(\u03b1_n)_{n\u22650}$ and $(\u03b2_n)_{n\u22650}$ are two sequences of monotonically decreasing positive real numbers such that (i) $\u03b1_0 \u2264 1,\u03b2_0 \u2264 1$; (ii) $\\sum_{n=0}^{\\infty} \u03b1_n = \\sum_{n=0}^{\\infty} \u03b2_n = \\infty$, but $\\sum_{n=0}^{\\infty}(\u03b1_n + \u03b2_n^2) < \\infty$; and (iii) $\\lim_{n\\rightarrow\\infty}(\u03b1_n/\u03b2_n) = 0$.\nA2) Bounded reward: There exists $r_{max} > 0$ such that $r(s, a) \u2264 r_{max}$ for all s \u2208 S and a \u2208 A.\nNext, we define a set-valued map. For W \u2208 R|S||A|, let X(W) be the (convex) set of stochastic policies given by\n$\\lambda(W) := \\{\\pi : S \\rightarrow \\triangle(A): \\sum_{\\alpha \\in A} \\pi(\\alpha|s) = 1 and supp(\\pi(s)) = arg max W(s,\\cdot)\\} \\forall s \\in S$.\\label{eq:mpe-properties} (3)\nOur main result can now be stated as follows. Let || \u00b7 || be the standard l\u221e norm.\nTheorem 1. Suppose A1 and A2 are true. Then the following statements hold for the iterates (Wn) and ($\u03b8_n$) obtained from Algorithm 1:\n(i) (Wn) is stable, i.e., $\\sup_n ||W_n|| < \\infty$ a.s.;\n(ii) If (Wn) held constant at W, then $\u03c0_{\\theta_n} \\rightarrow X(W)$; and\n(iii) If $(W_n, \u03c0_{\\theta_n}) \\rightarrow (W^*,\u03c0^*)$, then \u03c0* must be an MPE and W* must be the QH Q-value function of \u03c0*.\nRemark 2. Our first statement shows that our (Wn) iterates would be uniformly bounded. Next, for the case where the (Wn) sequence is (hypothetically) held constant at W, our second statement shows that the actor updates would lead to a policy that is greedy with respect to W. This result is important since the two-timescale nature of the algorithm, viz. the condition $\u03b1_n/\u03b2_n \\rightarrow 0$, implies the (Wn) iterates are quasi-static (or slowly changing) from the viewpoint of the $\u03b8_n$ updates. Our final statement shows that if our algorithm converges, then the limit must correspond to an MPE."}, {"title": "III. OUR ALGORITHM DESIGN", "content": "We now motivate the critic (Step 10) and actor (Step 11) update rules of our proposed algorithm and explain how they enable MPE estimation.\nOur critic update step is based on the temporal difference idea for minimizing the QH Bellman error at time n, i.e., the critic tries to find a W that minimizes the gap between $T^{\u03c0_{\\theta_n}} (W)$ and W for the given $\u03b8_n$. Hence, $W_n$ can be viewed as an estimate of the QH Q-value function of $\u03c0_{\\theta_n}$.\nOur actor or the $\u03b8_n$ update is approximately along the QH advantage function $A^{\u03c0_{\\theta_n}}$ of $\u03c0_{\\theta_n}$, where $A^\u03c0(s,a) = Q^\u03c0(s,a) \u2013 (\u03c0(\u00b7|s), Q^\u03c0(s,*))$ for any \u03c0,\u03c2, and a. We say approximately because $\u03b8_n(s,a)$ is actually updated along $W_n(s, a)-(\u03c0_{\\theta_n} (\u00b7|s), W_n(s, \u00b7))$ and $W_n$ is only an estimate of $Q^\u03c0$. Our main motivation to use the advantage function for updating on is to ensure that $\u03c0_{\\theta_n}$ is asymptotically greedy with respect to $W_n$; see Theorem 1.(ii). Even in RL with exponential discounting, the actor updates are along the corresponding advantage function estimate of the current policy [17]. However, the advantage function there aligns with the gradient of the state value function and enables discovery of the optimal policy. In QH discounting, though, this alignment does not hold, as we show next.\nLet $A^\u03c0(s, a) = Q^\u03c0(s, a) \u2212 (\u03c0(\u00b7|s), Q^\u03c0(s, \u00b7))$ be the policy \u03c0'\u03c2 advantage function under exponential discounting with y discount factor. Similarly, let $A^\u03c0(s, a)$ be the analogous \u03c3 = 0 expression. Now, for 0 \u2208 R|S|:|A|, if $\u2202_\u03b8^(\u03bc) := E_{s~\u03bc,a~\u03c0_\u03b8}[\u2202_\u03b8\u03c0_\u03b8(s,a)]$ denotes an averaging of $\u03c0_\u03b8$'\u03c2 QH Q-value function with respect to some fixed initial state distribution \u03bc, then we have that\n$\\frac{\u2202_\u03b8 Q_{\\theta}^\u03c0(\u03bc)}{\u2202\u03b8(s, a)} = (1 \u2212 \u03c3)\u03bc(s)\u03c0_\u03b8(as)A_\u03b8^\u03c0 (s, a) + \\frac{\u03c3}{1\u2212\u03b3}\u03bc(s)\u03c0_\u03b8(as) A_{\u03c0_\u03b8}^\u03c3 (s, a).$ (4)\nClearly, the RHS does not equal\n$A^{\u03c0_\u03b8}(s, a) = (1 \u2212 \u03c3)A_\u03b8^{\u03c0_\u03b8}(s, a) + \u03c3A_{\u03c0_\u03b8}^\u03c3 (s, a)$,\nwhich itself holds since\n$Q^\u03c0(s, a) = E_{\\begin{smallmatrix}S_n \\sim P(\\cdot|S_{n-1}, a_{n-1}) \\\\ a_n \\sim \\pi(\\cdot|S_n)\\end{smallmatrix}} (1 \u2212 \u03c3)r(s_0, a_0) + \\sum_{n=0}^{\\infty} \u03c3\u03b3r(s_n, a_n)  So = S,  \u03b1_0 = \u03b1$\nThis verifies our aforementioned claim that the advantage function does not align with the gradient. In fact, this also explains why our algorithm does not track the optimal policy, unlike the classical critic-actor method.\nFinally, we provide evidence for why our algorithm's convergence implies that the limit must be an MPE. Let T : Rd \u2192 2Rd be given by\n$T(W) := \\{T^\u03c0(W) : \u03c0 \\in \u03bb(W)\\}$,\n(5)\nwhere X is as in (3) and $T^\u03c0 : R^{|S||A|} \u2192 R^{|S||A|}$ is the QH Bellman operator for the policy \u03c0. That is,\n$T^\u03c0(W)(s,a) = r(s,a) + \u03b3\\sum_{s',a'}P(s'|s, a)\u03c0(a'|s') \u00d7 [\u2212(1\u2212 \u03c3)r(s', a') + W(s', a')].$ (6)\nThen, for the case where our algorithm's iterates converge, it can be shown ([4], [5], [19], [9]) by building upon Theorem 1.(ii) and the two-timescale nature of our algorithm that our (Wn) iterates asymptotically track the solution trajectories of the Differential Inclusion (DI) [2]\n$\\dot{W(t)} \\in T(\\dot{W(t)}) \u2013 W(t)$.\n(7)\nThe limiting set-valued dynamics arises because the same W can have multiple greedy policies.\nTracking the above DI is valuable for MPE estimation, and we demonstrate this in two ways. On the one hand, we present the following general result, showing that every MPE's QH Q-value function is a zero of this limiting DI.\nProposition 1. For any MPE \u03c0*, we have $\u03c0^* \\in (2)$ which, in turn, implies that 0 \u2208 T(Q) \u2013 Q.\nOn the other hand, we discuss this DI's vector field plot in Fig. 2 for the specific two-state MDP given in Fig. 1b, highlighting the complexities of MPE estimation and how this DI addresses them. The three subplots in"}, {"title": "IV. PROOF OUTLINES", "content": "We briefly sketch the arguments for our stated results.\nTheorem 1.(i) follows via a similar induction argument as in [10]. On the other hand, Theorem 1.(ii) follows from the fact that, for a state-action pair (s,a), when W(s, a) is larger (resp. smaller) than $(W(s,\\cdot), \u03c0_{\\theta_n} (\\cdot, s))$, then $\u03b8_{n+1}(s, a)$ is larger (resp. smaller) than $\u03b8_n(s, a)$. Note that $(W(s,\\cdot), \u03c0_{\\theta_n} (\\cdot, s))$ is W(s,\u00b7)'s average with respect to $\u03c0_{\\theta_n} (\\cdot|s)$. Thus, $\u03c0_{\\theta_{n+1}}$ assigns a relatively higher probability to actions where the W values are larger, from which the desired claim follows. For Theorem 1.(iii), we build upon the above results and the ideas in [5], [15], [19] and [4], which discuss the convergence of a stochastic approximation algo- rithm to a suitably defined limiting DI. Finally, Proposition 1 follows by exploiting the definition of an MPE."}, {"title": "V. EXPERIMENTS", "content": "We now demonstrate the effectiveness of our algorithm through a numerical example concerning the stochastic in- ventory control problem. This problem involves managing stock levels such as cars in a showroom-to meet uncertain daily demand while maximizing overall profits.\nFor our illustration, we consider an inventory system with a maximum capacity of M = 2. We suppose that the procurement (resp. holding\u00b3) cost per item is c = 500 (resp. h = 50), while the selling price is p = 900. Further, we suppose that the daily demand is a random variable taking values of 0,1, or 2 with probabilities 0.3, 0.2, and 0.5, respectively. At the start of day n, the inventory manager gets to see the current stock level sn and then decide on the number of new items an to (immediately) procure to meet the (uncertain) demand dn for that day; the capacity constraint implies that sn + an can be at most 2. Hence, the reward obtained for day n equals $r_n(S_n,a_n) = 900 \u00d7 min\\{s_n + a_n, d_n\\} - 500 \u00d7 a_n - 50 \u00d7 max\\{s_n + a_n dn, 0\\}$. In turn, starting with an initial stock of s and initial procurement of a, the QH Q-value function of a procurement policy \u3160 equals the expression on the RHS of (2). For our illustration, we suppose \u03c3 = 0.3 and y = 0.9.\nWe ran our proposed algorithm multiple times and it identified three different MPEs. These policies are given in Table I, and their corresponding QH Q-value functions are given in Table II. As can be seen, MPEs are not unique, and the overall profits associated with different MPEs can vary. For example, \u3160 yields the highest returns in our setup. Lastly, note that, at any state, each MPE assigns positive probabilities only to actions with the highest Q- values, although these probabilities need not be equal.\nSeparately, we also ran the variant of the classical critic- actor method for QH discounting, the one where the actor update is along the gradient of the state value function as described in (4) (with the initial state distribution being uniform). This yielded the policy \u03c0*, given in Table I. Its QH Q-value function is given in Table II.\nUnlike any of the MPEs, observe that \u03c0* does not share a greedy relation with its QH Q-value function. This implies that, e.g., starting from zero stock, the highest overall return is obtained if the stock is raised to 1 on day 0, and to 2 thereafter. Thus, there is an incentive for deviation, as explained in Section I. If the store manager is naive or has self-control issues and deviates from \u03c0* every day, then the resulting policy would be \u03c0\u2081 (see Table I). From its Q- values in Table II, it can be seen that \u3160\u2081 yields the lowest overall returns, compared to \u03c0* and, importantly, to any of the MPEs. We emphasize that if the manager and their future selves agree to follow an MPE, the current self would have no incentive for deviation."}, {"title": "VI. CONCLUSION AND FUTURE DIRECTIONS", "content": "In this work, we have developed the first model-free RL algorithm for finding an MPE in the QH-discounting setting. Our proposed algorithm, based on a two-timescale stochastic approximation, overcomes challenges posed by the lack of a Bellman-type contraction for an MPE's Q- value. We demonstrated its effectiveness in an inventory"}]}