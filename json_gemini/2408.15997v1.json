{"title": "Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need", "authors": ["Sijia Peng", "Yun Xiong", "Yangyong Zhu", "Zhiqiang Shen"], "abstract": "Time series forecasting requires balancing short-term and long-term dependencies for accurate predictions. Existing methods mainly focus on long-term dependency modeling, neglecting the complexities of short-term dynamics, which may hinder performance. Transformers are superior in modeling long-term dependencies but are criticized for their quadratic computational cost. Mamba provides a near-linear alternative but is reported less effective in time series long-term forecasting due to potential information loss. Current architectures fall short in offering both high efficiency and strong performance for long-term dependency modeling. To address these challenges, we introduce Mixture of Universals (MoU), a versatile model to capture both short-term and long-term dependencies for enhancing performance in time series forecasting. MoU is composed of two novel designs: Mixture of Feature Extractors (MoF), an adaptive method designed to improve time series patch representations for short-term dependency, and Mixture of Architectures (MoA), which hierarchically integrates Mamba, FeedForward, Convolution, and Self-Attention architectures in a specialized order to model long-term dependency from a hybrid perspective. The proposed approach achieves state-of-the-art performance while maintaining relatively low computational costs. Extensive experiments on seven real-world datasets demonstrate the superiority of MoU.", "sections": [{"title": "1 Introduction", "content": "Time series forecasting is crucial in various fields, such as climate prediction (Murat et al. 2018; Scher 2020; Haq 2022; Neumann et al. 2024), financial investment (Sezer, Gudelek, and Ozbayoglu 2020; Liu et al. 2023; Bieganowski and Slepaczuk 2024), and household power management (Bilal et al. 2022; Kim, Park, and Kim 2023; Cascone et al. 2023). However, achieving accurate forecasts is challenging as both short-term and long-term dynamics jointly influence future values (Chen et al. 2023).\nTo model short-term dependency, the patching strategy has gained attention for its ability to preserve contextual semantics. For instance, PatchTST (Nie et al. 2023) proposes grouping data points into patch tokens to retain local semantics, while Pathformer (Chen et al. 2017) employs a multi-scaled patching strategy to summarize short-term dependency in various resolutions. Although these approaches enhance short-term information by generating patch tokens, they still rely on uniform linear transformations for patch embedding. This approach neglects the divergence in feature affinity within different patches, which arises from their varying semantic contexts. As a result, important contextual information may be lost, limiting the accurate representation of short-term details.\nThis motivates us to explore adaptive methods that tailor the embedding process for different patch tokens, generating more informative representations. Such approaches have shown success in computer vision (Chen et al. 2020; Yang et al. 2019). For instance, Dynamic Convolution (Chen et al. 2020) and Conditional Convolution (Yang et al. 2019) adapt convolution kernel parameters for different input patches. However, we find that these methods perform poorly for time series patches, even worse than uniform linear transformation (for detailed explanation, please refer to Section 3.4). We attribute this failure to the increased tunable parameters relative to the small size of patch data, which may hinder the feature extractor's ability to learn robust representations.\nGiven the lack of suitable adaptive methods for time series patches, we propose Mixture of Feature Extractors (MoF). Inspired by the Mixture of Experts approach (Shazeer et al. 2017), which uses sparse activation to flexibly adjust model structure for various downstream tasks, MoF comprises multiple sub-extractors designed to handle divergent contexts"}, {"title": "2 Approach", "content": "2.1 Problem Setting and Model Structure\nThe aim of multivariate time series forecasting is to predict future values over T time steps, given historical data from the past L time steps. Specifically, given the historical values of a multivariate time series consisting of M variables, $X_{input} = [X^1, X^2,..., X^M] \\in \\mathbb{R}^{M\\times L}$, where each $X^i$ is a vector of length L, $X^i = [X_1,X_2, ..., X_L] \\in \\mathbb{R}^L$, our task is to predict the future values $X_{output} = [\\hat{X}^1,\\hat{X}^2,...,\\hat{X}^M] \\in \\mathbb{R}^{M\\times T}$, where each $\\hat{X}^i$ is a vector of length T, representing the predicted values for the i-th variable from L+1 to L+T. In this work, we adopt the variate independence setting as in PatchTST (Nie et al. 2023), simplifying our goal to learning a function F: X \u2192 X, which maps historical time series to predicted time series. In our work, F is our proposed MoU.\nFirst, we preprocess the time series using a patching strategy, which applies a sliding window of fixed size P with stride S to generate a sequence of N patch tokens:\n$X_p = Patch(X)$ (1)\nwhere X \u2208 RL is the raw time series, and $X_p \\in \\mathbb{R}^{N\\times P}$ is the resulting patched sequence, consisting of N patch tokens, each containing P data points.\nTo capture short-term dependencies within tokens, Xp is fed into our MoF module. The process of generating adaptive representations is described as:\n$X_{rep} = MoF(X_p),$ (2)\nwhere MoF(\u00b7) is our proposed Mixture of Feature Extractors, specifically designed for time series patch tokens. It"}, {"title": "2.2 Mixture of Feature Extractors", "content": "To account for the diverse feature affinities present within various patches, we introduce MoF in Figure 3 as an adaptive feature extractor specifically designed for time series patches. Different from other adaptive methods, MoF keeps minimum increment in activated net parameters by sparse activation. This facilitates a robust generation of adaptive representations, especially for time series patches with such small data size. Specifically, MoF consists of a set of sub-extractors {F1, F2, ..., Fc}, each representing an independent linear mapping. The representation for a patch token is generated by MoF as follows:\n$X_{rep} = MOF(X) = \\sum_{i=1}^{n}R_i(X_p)F_i(X_p)$ (5)\nwhere Ri() is an input-relevant router that generates a sparse vector with most elements set to zero, enabling the sparse activation of sub-extractors and ensuring minimal parameter increase. The router function $R(X_p)_i$ is calculated:\n$R(X_p) = Softmax(Top_k(H(X_p)_i,k))$ (6)\nwhere Softmax(\u00b7) normalizes the top k scores kept by $Top_k(\\cdot, k)$. $H(X_p)_i$ is a vector of scores for sub-extractors:\nH(Xp) = [H(Xp)1, \u0397(Xp)2, ...H(Xp)c], where H(Xp)i\ndenotes the score of i-th sub-extractor:\nH(Xp)i=(Xp.Wg);+SN.Softplus ((XpWnoise)) (7)\nwhere Wg contains the parameters of a linear function, and the second term injects tunable noise for load balancing, following the approach in MoE (Shazeer et al. 2017). Here, SN represents the standard normal distribution.\nThis mechanism effectively partitions the patch tokens into combinations of c different patterns, where c corresponds to the number of sub-extractors. Since the information of each pattern is processed by corresponding optimal feature extractor, MoF is capable of generating most representative embedding for patches with divergent contexts."}, {"title": "2.3 Mixture of Architectures", "content": "We propose MoA to capture comprehensive long-term dependencies. As shown in Figure 4, MoA is structured hierarchically with four layers, Mamba, FeedForward, Convolution, and Self-Attention layers, each captures a different aspect of long-term dependencies. Benefiting from its gradually expanding perspective, MoA is capable of modeling long-term dependencies with effectiveness and efficiency.\nMamba-layer in Time Series is first to select relevant data and learns time-variant dependencies with selective SSM.\nLet x denotes the $X_{rep}$, the process can be described as:\nx' = \u03c3 (Conv1D (Linear (x)))\nz = \u03c3 (Linear (x)) (8)\nwhere o is activation function SiLU. Then, x, x' and z are used to calculate output y by:\ny' = Linear(SelectiveSSM(x') \u25ca z)\ny = LayerNorm (y' + x) (9)\nwhere \u25ca denotes element-wise multiplication, and the SelectiveSSM can be further expanded as:\nSelectiveSSM(x) = yt\nyt = Cht, ht = \u0100ht\u22121 + Bx (10)\nSpecifically, ht is the latent state updated at time step t, while $Y_t$ is the output representation. The discrete matrices A, B and C are input-relevant and updated by the time-variant recurrent rule over the time:\nBt = SB(x), Ct = Sc (x1),\nAt = softplus (S\u25b3 (x)) (11)\nwhere SB, SC, and S\u25b3 are linear projection layers. We use following equation to obtain discrete parameters At and Bt:\nfa (\u2206t, A) = exp (AA)\nfb (\u2206t, A, Bt) = (\u2206tA)\u00af\u00b9(exp(\u2206tA) \u2013 \u0399)\u00b7 \u0394\u0392 (12)\n\u0100t = fA (\u2206t, A), Bt = fB (\u2206t, A, Bt)\nwhere fa and fB are discretization functions, while A,B,C and A are parameters.\nFeedForward-layer serves as the second transition layer, following the Transformer architecture's convention of enhancing non-linearity: $x_{ffn}$ = FeedForward (yt ; W1, \u03c3, W2), where w\u2081 and w\u2082 are parameters, o is activation function.\nConvolution-layer, the third layer, expands MoA's receptive field. It facilitates information exchange among tokens encapsulating partial long-term dependencies, allowing more comprehensive learning of dependencies: $x_{conv}$ = Conv(xffn; k, s, p, Cout), where k is the kernel size, s is the stride, p is the padding, and Cout is the number of output channels, with the output dimension kept unchanged.\nSelf-Attention-layer is the final layer, capturing comprehensive long-term dependencies with its global perspective:\n$X_{att}$ = FeedForward(Attention(Q, K, V))\n$Attention(Q, K, V) = Softmax(\\frac{QKT}{\\sqrt{dk}}) V$\nQ = XconvWQ, K = XconvWK, V = Xcony WV (13)\nwhere WQ,WK,Wv are parameters, and self-attention is implemented using a scaled dot product.\nPartial-to-global Design for Time Series. To capture long-term dependencies in time series, our MoA initially focusing on part of dependencies and gradually expanding into a comprehensive global perspective. It begins with the Mamba-layer, which selectively processes time-variant dependencies using SSM, focusing on partial dependencies to identify essential features. The FeedForward-layer adds non-linearity and transitions these partial dependencies into more complex representations. The Convolution-layer then broadens the receptive field, facilitating information exchange between tokens to enhance understanding of broader temporal relationships. Finally, the Self-Attention-layer provides a global perspective, integrating localized information into a complete understanding of long-term dependencies. This hierarchical design ensures the the capturing of intricate time series patterns while remaining computationally efficient."}, {"title": "2.4 Computational Complexity and Model Parameter", "content": "Given a sequence with T tokens, the computational cost of an MoU block that selects the top-k experts is:\n$C_{MOU} = kTxd^2+T\\times d^2 + T \\times d^2 + kTd^2+T^2 \\times d + T\\times d^2$ (14)\nwhere k is the kernel size in the convolutional layer, d is the dimension of the vector representations. In the Transformer block, we account for the complexity of both the linear transformation used to compute query (Q), key (K), and value (V) matrices, as well as the complexity of the Self-Attention layer. For comparison, the computational cost required by a three-layer Multi-Head Self-Attention (MHSA) is:\n$C_{mhsa} = 3 \u00d7 (T^2d + Td^2)$ (15)\nAs shown, except for the Transformer block, the complexity of our structure is linear, resulting in significantly lower computational requirements compared to pure Transformer models like PatchTST (Nie et al. 2023). Considering the model parameters, the linear layer primarily involves parameters of size d\u00b2 + bias for MoF and FFN layers, the convolutional layer is mainly determined by the kernel size of k \u00d7 d\u00b2, and the Self-Attention layer's parameters are mainly the mapping matrices for Q, K, V, each head of size d2. As shown in Figure 1, the total size of our model is 372MB, compared to PatchTST of 408MB and ModernTCN of 611MB."}, {"title": "3 Experiments", "content": "3.1 Datasets\nWe evaluate long-term forecasting performance of our proposed MoU on 7 commonly used datasets (Wu et al. 2021), including Weather, ILI and four ETT datasets (ETTh1, ETTh2, ETTm1 and ETTm2). Details are in Appendix A.\n3.2 Baselines and Setup\nTo evaluate the overall performance of MoU in time series long-term forecasting task, we conducted a comparative analysis against four state-of-art models, each representative of a distinct architectural paradigm. The included baselines are as follows:\n\u2022 Mamba-based Models (S-Mamba): S-Mamba model, known for reducing computational cost from quadratic to linear while maintaining competitive performance, has been effectively applied to time series forecasting, making it a relevant baseline for our study.\n\u2022 Linear-based Models (D-Linear): The D-Linear model, which challenges the dominance of Transformer-based models by achieving superior performance in time series forecasting, represents our chosen baseline for Linear-based models due to its simplicity and efficiency.\n\u2022 Convolution-based Models (ModernTCN): by expanding receptive field, demonstrates that convolutional models can achieve state-of-the-art performance in long-term time series forecasting, making it our selected baseline for Convolution-based models."}, {"title": "3.4 Ablation Study", "content": "Ablation for feature extractor design. In this section, we investigate the efficacy of MoF in capturing short-term dependencies within patch tokens. We compare four types of encoders: one using a uniform transformation approach (Linear) and three employing adaptive transformation methods, including our proposed MoF, SE-M\u00b9, and Dyconv (Chen et al. 2020). All methods take input $X_p \\in \\mathbb{R}^{N\\times P}$ and produce output $X_{rep} \\in \\mathbb{R}^{N\\times D}$\nFrom Table 2, we make the following observations:\n\u2022 MoF outperforms the uniform transformation method (Linear), demonstrating that adaptive feature extraction leads to better representation of patch tokens.\n\u2022 Dyconv, another adaptive method, fails to outperform Linear. We attribute Dyconv's poor performance to the huge increase in network parameters, making it unsuitable for small datasets like time series patches. This highlights our MoF's advantage in maintaining a small set"}, {"title": "3.5 Model Analysis", "content": "Does MoF actually learn contexts within patches? To assess whether MoF effectively learns distinct contexts within patches, we analyze the sub-extractor activations. After training MoF, we input a set of patches and record the activated sub-extractor for each. In cases where multiple sub-extractors are activated, we noted the one with the highest score, as detailed in Section 2.2. We then categorize the patches into C classes based on their sub-extractor activations. As shown in Figure 5, patches associated with the same sub-extractor exhibit similar wave patterns, while those linked to different sub-extractors show divergent dynamics. This confirms that MoF effectively learns distinct contexts within patches, leading to more representative embeddings. More details and more visualizations can be found in Appendix E.\nWhat is learned by the layers of MoA? As aforementioned, MoA is designed to capture long-term dependencies"}, {"title": "4 Related Work", "content": "Research into architectures for long-term time series forecasting recently has gained significant attention. For instance, ETSformer (Woo et al. 2022) proposes time series Transformers by integrating the principle of exponential smoothing. PatchTST (Nie et al. 2023) introduces a Transformer-based model using patching and channel-independent structures for time series forecasting. ModernTCN (Donghao and Xue 2024) optimizes the use of convolution in time series by proposing a DWConv and ConvFFN-based structure. TimeMachine (Ahamed and Cheng 2024) leverages Mamba, a state-space model, to capture long-term dependencies in multivariate time series while maintaining linear scalability and low memory usage. Mambaformer (Xu et al. 2024) combines Transformer and Mamba architectures for enhanced time series forecasting. Additionally, Time-LLM (Wang et al. 2024a) repurposes large language models (LLMs) of LLaMA (Touvron et al. 2023) for general time series forecasting by using LLMs as a reprogramming framework while keeping the backbone language models intact."}, {"title": "5 Conclusion", "content": "We have presented Mixture of Universals (MoU), a pioneering and advanced model designed for efficient and effective time series forecasting. MoU consists of two key components: Mixture of Feature Extractors (MoF), an adaptive method specifically designed to enhance time series patch representations for capturing short-term dependencies, and Mixture of Architectures (MoA), which hierarchically integrates multiple architectures in a structured sequence to model long-term dependencies from a hybrid perspective. The proposed approach achieves state-of-the-art performance across various real-world benchmarks while maintaining low computational costs. We hope that our work can bring new ideas and insights to the model structure design in the field of time series forecasting."}, {"title": "Appendix", "content": "In this Appendix, we provide details which are omitted in the main text. The outlines are as follows:\n\u2022 Section A: An introduction of datasets and their training set split configurations (in \u201cExperiments\u201d of main text).\n\u2022 Section B: A description of training configurations, including devices, optimization, training parameter settings and the visualization of predictions compared with baseline models (in \u201cExperiments\u201d of main text).\n\u2022 Section C: The details about model parameters configuration in our MoU (in \u201cExperiments\u201d of main text).\n\u2022 Section D: An introduction to the baseline models and an elucidation of their respective calculation processes (in \"Ablation Study\" of main text).\n\u2022 Section E: More details for behaviors of MoF, and a discussion about differences between MoF and other similar existing works (in \u201cModel Analysis\" of main text).\n\u2022 Section F: An illustration of metrics used to evaluate all models (in \u201cExperiments\u201d of main text).\n\u2022 Section G: The performance of MoU for univariate long-term forecasting, and analysis for impact of various look-back windows (in \"Main Results\" of main text).\n\u2022 Our code as well as the running scripts are available at https://github.com/lunaaa95/mou/."}, {"title": "A Datasets Details", "content": "We evaluate the performance of MoU on seven widely used datasets (Wu et al. 2021), including Weather, ILI, Electricity and four ETT datasets (ETTh1, ETTh2, ETTm1 and ETTm2). All datasets are publicly available.\n\u2022 ETT datasets contain two-year records of Electricity Transformer Temperature from two distinct regions in a Chinese province, labeled with suffixes 1 and 2. Each dataset includes seven variables with timestamps: HUFL, HULL, MUFL, LUFL, LULL, and OT. For univariate forecasting, OT (oil temperature) is the primary focus. The datasets are provided in two time resolutions: hourly ('h') and 15-minute intervals ('m').\n\u2022 Weather comprises 21 meteorological indicators from Germany for the year 2020, including variables like humidity, air temperature, recorded at 10-minute intervals.\n\u2022 ILI is a national dataset tracking influenza-like illness, including patient counts and the illness ratio. It contains 7 variables, with data collected weekly.\n\u2022 Electricity dataset records the hourly electricity consumption of 321 customers.\nDataset details are presented in Table 4. Following the setup in (Nie et al. 2023), we split each dataset into training, validation, and test sets. The split ratio is 6:2:2 for ETT"}, {"title": "B Implementation Details", "content": "Devices All the deep learning networks are implemented in PyTorch and conducted on NVIDIA V100 32GB GPU.\nOptimization Our training target is minimizing 12 loss, with optimizer of Adam by default. Then initial training rate is set to 2.5 \u00d7 10-3 on ILI dataset for quick searching, 2 \u00d7 10-4 for ETTm2 for better convergence, and 1 \u00d7 10-3 for other datasets by default.\nParameter Setting Instead of using a fixed look-back window, we rerun PatchTST, ModernTCN, and S-Mamba with varying look-back windows: L\u2208 {48,60, 104, 144} for the ILI dataset and L \u2208 {192, 336, 512, 720} for the other datasets. The best results are selected to generate strong baselines. For DLinear, we directly use the results from PatchTST (Nie et al. 2023), where the best results with varying look-back windows are already selected.\nVisualization of prediction We present a visualization of the future values predicted by MoU with the ground truth values for the ETTm2 dataset. The look-back window is set to 512 and predicted length is set to 336. The results are shown in Figure 7). We observe that the predicted values (orange line) are highly consistent with ground-truth (blue line), indicating our model is capable of making accurate forecasting. Besides, we also notice that the prediction repeats the periodic waves which have shown in historical series, indicating a long-term dynamics captured by model."}, {"title": "C Model Architecture and Parameter Details", "content": "By default, our MoU model includes a single MoA block. Our experimental setup uses a uniform patch length of 16 and a patch stride of 8 for generating patch tokens. The number of Sub-Extractors is set to four, with the top two selected in the MoF for generating patch representations. The Mamba layer employs an SSM state expansion factor of 21. In the FeedForward layer, the hidden size expansion rate is set to 2. The Convolution layer parameters are fixed with a kernel size of 3, a stride of 1, and padding of 1. In the Self-Attention layer, the number of heads is set to 4 for the ETTh1, ETTh2, and ILI datasets, and 16 for all other datasets.\nThe dimensionality of both short-term and long-term representations is set to 64 for the ETTh1 and ETTh2 datasets and increased to 128 for the remaining datasets for better performance. Within the MoA block, the input and output dimensions are kept consistent across layers, but can be adjusted by modifying the FeedForward layer's output dimension or the Convolution layer's kernel size and stride."}, {"title": "D Baseline Models Details", "content": "We present the detailed calculations of SE-M, W, and Dyconv, which are baseline models in Section 3.4 (main text). SE-M is a modified Squeeze-and-Excitation (Hu et al. 2019) method. The process of SE-M can be described as:\n$X_{rep} = SE-M(X_p) = (W_{se}X_p) \\Y$ (16)\nwhere parameter $W_{se} \\in \\mathbb{R}^{P\\times D}$, and Y denotes the gating vector, which can be describes as:\n$\\Y$ = Expand($\\sigma_2$ ($W_2$$\\sigma_1$($W_1Z$))) (17)\nwhere $\\sigma_1$ and $\\sigma_2$ are respectively ReLU function and Sigmoid function. Z is a average pooled vector which squeezes the information:\nZ = AvgPool($W_{se}X_p$) (18)\nTo avoid underestimating the ability of original Squeeze-and-Excitation method, we apply Squeeze-and-Excitation on $W_{se} X_p$ instead of vanilla $X_p$ to generate enriched representation. This modification is the difference between original Squeeze-and-Excitation and our baseline method SE-M.\nOverall, the parameters participated in once calculation are $W_{se} \\in \\mathbb{R}^{P\\times D}$, $W_1 \\in \\mathbb{R}^{D\\times (D/r)}$ and $W_2 \\in \\mathbb{R}^{(D/r)\\times D}$, where r is the reduction rate.\nW is a linear transformation. The process can be summarized as:\n$X_{rep} = Linear(X_p) = W_iX_p$ (19)\nDyconv is the method of Dynamic Convolution (Chen et al. 2020). The process can be described as:\n$X_{rep} = Dyconv(X_p) = Conv(X_p; K)$ (20)\nwhere K denotes the parameters of adaptive kernel, which is aggregated by a set of kernels {K} with attention weight of $\\pi_i(X_p)$, this process can be described as:\n$\\kappa = \\sum_{i=1}^{N} \\pi_i(X_p) K_i$ (21)\nWe calculate the weight of each kernel $\\pi_i(X_p)$ by:\n$\\pi_i (X_p) = Softmax($W_2(\\sigma(W_1(AvgPool(X_p))))$(22)\nwhere o is activation function of ReLU. The size of parameters in Dyconv is biggest among all listed methods.\nOverall, the parameters participated in once calculation are W1 \u2208 RP\u00d7D, W2 \u2208 RD\u00d7(D/r) and {K}."}, {"title": "E More Model Analysis", "content": "Behaviors of Sub-Extractors in MoF We conducted an experiment to extract the contextual information learned by the Sub-Extractors in MoF. The method is descried in Section 3.5 (main text), with the experimental outcomes presented in an integrated format in Figure 5 (main text). Specifically, we selected the top 10 patches with the highest scores to serve as representative examples for their respective Sub-Extractors.\nTo enhance clarity, each patch is individually displayed in Figure 9. Within each row of the figure, the patches are processed by the same Sub-Extractor. It is observable that the patches exhibit a consistent shape within rows, while there is a significant divergence in shapes across rows. This observation demonstrates the proficiency of the MoF in capturing contextual information."}, {"title": "F Metrics Illustration", "content": "We use mean square error (MSE) and mean absolute error (MAE) as our metrics for evaluation of all forecasting mod-"}]}