{"title": "Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need", "authors": ["Sijia Peng", "Yun Xiong", "Yangyong Zhu", "Zhiqiang Shen"], "abstract": "Time series forecasting requires balancing short-term and long-term dependencies for accurate predictions. Existing methods mainly focus on long-term dependency modeling, neglecting the complexities of short-term dynamics, which may hinder performance. Transformers are superior in modeling long-term dependencies but are criticized for their quadratic computational cost. Mamba provides a near-linear alternative but is reported less effective in time series long-term forecasting due to potential information loss. Current architectures fall short in offering both high efficiency and strong performance for long-term dependency modeling. To address these challenges, we introduce Mixture of Universals (MoU), a versatile model to capture both short-term and long-term dependencies for enhancing performance in time series forecasting. MoU is composed of two novel designs: Mixture of Feature Extractors (MoF), an adaptive method designed to improve time series patch representations for short-term dependency, and Mixture of Architectures (MoA), which hierarchically integrates Mamba, FeedForward, Convolution, and Self-Attention architectures in a specialized order to model long-term dependency from a hybrid perspective. The proposed approach achieves state-of-the-art performance while maintaining relatively low computational costs. Extensive experiments on seven real-world datasets demonstrate the superiority of MoU. Code is available at https://github.com/lunaaa95/mou/.", "sections": [{"title": "1 Introduction", "content": "Time series forecasting is crucial in various fields, such as climate prediction (Murat et al. 2018; Scher 2020; Haq 2022; Neumann et al. 2024), financial investment (Sezer, Gudelek, and Ozbayoglu 2020; Liu et al. 2023; Bieganowski and Slepaczuk 2024), and household power management (Bilal et al. 2022; Kim, Park, and Kim 2023; Cascone et al. 2023). However, achieving accurate forecasts is challenging as both short-term and long-term dynamics jointly influence future values (Chen et al. 2023).\nTo model short-term dependency, the patching strategy has gained attention for its ability to preserve contextual semantics. For instance, PatchTST (Nie et al. 2023) proposes grouping data points into patch tokens to retain local semantics, while Pathformer (Chen et al. 2017) employs a"}, {"title": "2 Approach", "content": null}, {"title": "2.1 Problem Setting and Model Structure", "content": "The aim of multivariate time series forecasting is to predict future values over T time steps, given historical data from the past L time steps. Specifically, given the historical values of a multivariate time series consisting of M variables, $X_{input} = [X^{1}, X^{2},..., X^{M}] \\in R^{M\\times L}$, where each $X^{i}$ is a vector of length L, $X^{i} = [x_{1}, x_{2}, ..., x_{L}] \\in R^{L}$, our task is to predict the future values $X_{output} = [\\hat{X^{1}},\\hat{X^{2}},...,\\hat{X^{M}}] \\in R^{M\\times T}$, where each $\\hat{X^{i}}$ is a vector of length T, representing the predicted values for the i-th variable from L+1 to L+T. In this work, we adopt the variate independence setting as in PatchTST (Nie et al. 2023), simplifying our goal to learning a function F: X \u2192 X, which maps historical time series to predicted time series. In our work, F is our proposed MoU.\nFirst, we preprocess the time series using a patching strat- egy, which applies a sliding window of fixed size P with stride S to generate a sequence of N patch tokens:\n$X_{p} = Patch(X)$   (1)\nwhere X \u2208 RL is the raw time series, and $X_{p} \\in R^{N\\times P}$ is the resulting patched sequence, consisting of N patch to- kens, each containing P data points.\nTo capture short-term dependencies within tokens, Xp is fed into our MoF module. The process of generating adap- tive representations is described as:\n$X_{rep} = MoF(X_{p})$,   (2)\nwhere MoF(\u00b7) is our proposed Mixture of Feature Extrac- tors, specifically designed for time series patch tokens. It"}, {"title": "2.2 Mixture of Feature Extractors", "content": "To account for the diverse feature affinities present within various patches, we introduce MoF in Figure 3 as an adap- tive feature extractor specifically designed for time series patches. Different from other adaptive methods, MoF keeps minimum increment in activated net parameters by sparse activation. This facilitates a robust generation of adaptive representations, especially for time series patches with such small data size. Specifically, MoF consists of a set of sub-extractors {$F_{1}, F_{2}, ..., F_{c}$}, each representing an indepen- dent linear mapping. The representation for a patch token is generated by MoF as follows:\n$X_{rep} = MOF(X) = \\sum_{i=1}^{n}R_{i}(X_{p})F_{i}(X_{p})$   (5)\nwhere Ri() is an input-relevant router that generates a sparse vector with most elements set to zero, enabling the sparse activation of sub-extractors and ensuring minimal pa- rameter increase. The router function R(Xp)i is calculated:\n$R(X_{p}) = Softmax(Top_{k}(H(X_{p})_{i},k))$ (6)\nwhere Softmax(\u00b7) normalizes the top k scores kept by Topk(, k). H(Xp) is a vector of scores for sub-extractors:"}, {"title": "2.3 Mixture of Architectures", "content": "We propose MoA to capture comprehensive long-term de- pendencies. As shown in Figure 4, MoA is structured hier- archically with four layers, Mamba, FeedForward, Convo- lution, and Self-Attention layers, each captures a different aspect of long-term dependencies. Benefiting from its grad- ually expanding perspective, MoA is capable of modeling long-term dependencies with effectiveness and efficiency. Mamba-layer in Time Series is first to select relevant data and learns time-variant dependencies with selective SSM. Let x denotes the Xrep, the process can be described as:\n$x' = \\sigma (Conv1D (Linear (x)))$ \n$z = \\sigma (Linear (x))$   (8)\nwhere o is activation function SiLU. Then, x, x' and z are used to calculate output y by:\n$y' = Linear(SelectiveSSM(x') \\bigotimes z)$ \n$y = LayerNorm (y' + x)$ (9)"}, {"title": "2.4 Computational Complexity and Model Parameter", "content": "Given a sequence with T tokens, the computational cost of an MoU block that selects the top-k experts is:\n$C_{MOU} = kTxd^{2}+T\\times d^{2} + T \\times d^{2} + kTd^{2}+T^{2} \\times d + T\\times d^{2}$  (14)\nwhere k is the kernel size in the convolutional layer, d is the dimension of the vector representations. In the Transformer block, we account for the complexity of both the linear trans- formation used to compute query (Q), key (K), and value (V) matrices, as well as the complexity of the Self-Attention layer. For comparison, the computational cost required by a three-layer Multi-Head Self-Attention (MHSA) is:\n$C_{mhsa} = 3 \\times (T^{2}d + Td^{2})$  (15)\nAs shown, except for the Transformer block, the complex- ity of our structure is linear, resulting in significantly lower computational requirements compared to pure Transformer models like PatchTST (Nie et al. 2023). Considering the model parameters, the linear layer primarily involves pa- rameters of size $d^{2}$ + bias for MoF and FFN layers, the convolutional layer is mainly determined by the kernel size of k \u00d7 $d^{2}$, and the Self-Attention layer's parameters are mainly the mapping matrices for Q, K, V, each head of size d2. As shown in Figure 1, the total size of our model is 372MB, compared to PatchTST of 408MB and ModernTCN of 611MB."}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Datasets", "content": "We evaluate long-term forecasting performance of our pro- posed MoU on 7 commonly used datasets (Wu et al. 2021), including Weather, ILI and four ETT datasets (ETTh1, ETTh2, ETTm1 and ETTm2). Details are in Appendix A."}, {"title": "3.2 Baselines and Setup", "content": "To evaluate the overall performance of MoU in time se- ries long-term forecasting task, we conducted a comparative analysis against four state-of-art models, each representative of a distinct architectural paradigm. The included baselines are as follows:\n\u2022 Mamba-based Models (S-Mamba): S-Mamba model, known for reducing computational cost from quadratic to linear while maintaining competitive performance, has been effectively applied to time series forecasting, mak- ing it a relevant baseline for our study.\n\u2022 Linear-based Models (D-Linear): The D-Linear model, which challenges the dominance of Transformer-based models by achieving superior performance in time series forecasting, represents our chosen baseline for Linear- based models due to its simplicity and efficiency.\n\u2022 Convolution-based Models (ModernTCN): by expand- ing receptive field, demonstrates that convolutional mod- els can achieve state-of-the-art performance in long-term time series forecasting, making it our selected baseline for Convolution-based models."}, {"title": "3.3 Main Results", "content": "Multivariate long-term forecasting results are shown in Ta- ble 1. Overall, MoU consistently outperforms other base- lines. Compared to ModernTCN, MOU achieves a 17.2% re- duction in MSE and a 9.5% reduction in MAE. When com- pared to PatchTST, MOU yields a 22.6% reduction in MSE and a 15.6% reduction in MAE. Additionally, MoU demon- strates significant improvements across all datasets over the Linear-based DLinear and Mamba-based S-Mamba models."}, {"title": "3.4 Ablation Study", "content": "Ablation for feature extractor design. In this section, we investigate the efficacy of MoF in capturing short- term dependencies within patch tokens. We compare four types of encoders: one using a uniform transformation ap- proach (Linear) and three employing adaptive transforma- tion methods, including our proposed MoF, SE-M\u00b9, and Dyconv (Chen et al. 2020). All methods take input Xp \u2208 RN\u00d7P and produce output Xrep \u2208 RN\u00d7D\nFrom Table 2, we make the following observations:\n\u2022 MoF outperforms the uniform transformation method (Linear), demonstrating that adaptive feature extraction leads to better representation of patch tokens.\n\u2022 Dyconv, another adaptive method, fails to outperform Linear. We attribute Dyconv's poor performance to the huge increase in network parameters, making it unsuit- able for small datasets like time series patches. This high- lights our MoF's advantage in maintaining a small set"}, {"title": "3.5 Model Analysis", "content": "Does MoF actually learn contexts within patches? To assess whether MoF effectively learns distinct contexts within patches, we analyze the sub-extractor activations. After training MoF, we input a set of patches and record the activated sub-extractor for each. In cases where multi- ple sub-extractors are activated, we noted the one with the highest score, as detailed in Section 2.2. We then catego- rize the patches into C classes based on their sub-extractor activations. As shown in Figure 5, patches associated with the same sub-extractor exhibit similar wave patterns, while those linked to different sub-extractors show divergent dy- namics. This confirms that MoF effectively learns distinct contexts within patches, leading to more representative em- beddings. More details and more visualizations can be found in Appendix E.\nWhat is learned by the layers of MoA? As aforemen- tioned, MoA is designed to capture long-term dependencies"}, {"title": "4 Related Work", "content": "Research into architectures for long-term time series fore- casting recently has gained significant attention. For in- stance, ETSformer (Woo et al. 2022) proposes time se- ries Transformers by integrating the principle of expo- nential smoothing. PatchTST (Nie et al. 2023) introduces a Transformer-based model using patching and channel- independent structures for time series forecasting. Mod-"}, {"title": "5 Conclusion", "content": "We have presented Mixture of Universals (MoU), a pioneer- ing and advanced model designed for efficient and effec- tive time series forecasting. MoU consists of two key com- ponents: Mixture of Feature Extractors (MoF), an adap- tive method specifically designed to enhance time series patch representations for capturing short-term dependencies, and Mixture of Architectures (MoA), which hierarchically integrates multiple architectures in a structured sequence to model long-term dependencies from a hybrid perspec- tive. The proposed approach achieves state-of-the-art per- formance across various real-world benchmarks while main- taining low computational costs. We hope that our work can bring new ideas and insights to the model structure design in the field of time series forecasting."}]}