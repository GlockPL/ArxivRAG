{"title": "OPTIMAL MULTI-OBJECTIVE BEST ARM IDENTIFICATION WITH FIXED CONFIDENCE", "authors": ["Zhirui Chen", "P. N. Karthik", "Yeow Meng Chee", "Vincent Y. F. Tan"], "abstract": "We consider a multi-armed bandit setting with finitely many arms, in which each arm yields an M-dimensional vector reward upon selection. We assume that the reward of each dimension (a.k.a. objective) is generated independently of the others. The best arm of any given objective is the arm with the largest component of mean corresponding to the objective. The end goal is to identify the best arm of every objective in the shortest (expected) time subject to an upper bound on the probability of error (i.e., fixed-confidence regime). We establish a problem-dependent lower bound on the limiting growth rate of the expected stopping time, in the limit of vanishing error probabilities. This lower bound, we show, is characterised by a max-min optimisation problem that is computationally expensive to solve at each time step. We propose an algorithm that uses the novel idea of surrogate proportions to sample the arms at each time step, eliminating the need to solve the max-min optimisation problem at each step. We demonstrate theoretically that our algorithm is asymptotically optimal. In addition, we provide extensive empirical studies to substantiate the efficiency of our algorithm. While existing works on pure exploration with multi-objective multi-armed bandits predominantly focus on Pareto frontier identification, our work fills the gap in the literature by conducting a formal investigation of the multi-objective best arm identification problem.", "sections": [{"title": "Introduction", "content": "Multi-armed bandit (MAB) (Thompson, 1933) is a sequential decision-making paradigm where an agent sequentially pulls one out of K finitely many arms and receives a corresponding reward at each time step, with widespread applications in clinical trials, internet advertising, and recommender systems (Lattimore and Szepesv\u00e1ri, 2020). In the classical MAB setup, the rewards from the arms are independent and identically distributed (i.i.d.), and real-valued (one-dimensional). In contrast, the multi-objective multi-armed bandit (MO-MAB) setup proposed by Drugan and Nowe (2013) allows for i.i.d. multi-dimensional (vector) rewards from the arms, with the reward of any given dimension (a.k.a the objective) being independent of the others or, more generally, a function of the rewards of the others. Defining the best arm of an objective as the arm with the largest mean reward corresponding to the objective, it is evident that in the MO-MAB setup, distinct objectives may possess distinct best arms, leading to the possibility of an arm being optimal for one objective and sub-optimal for another, thereby amplifying the complexity of identifying one or more best arms. In this paper, we study the problem of recovering the best arm of every objective in the shortest (expected) time, while ensuring that the probability of error is within a prescribed threshold (fixed-confidence regime).\nMotivation Consider the task of deploying advertisements from a candidate set of advertisements, on platforms such as YouTube and Twitch. Here, selecting an advertisement to launch on any given day is analogous to pulling an arm. The feedback obtained from deploying a specific advertisement is inherently multi-dimensional, comprising various video-specific metrics such as user engagement and view rates, as well as demography-specific metrics such as viewer age and gender. The task of identifying the optimal advertisement for different demographic segments, which is crucial for maximizing revenue, translates to finding the best arm for each objective (e.g., each age group).\nAs such, the problem of best arm identification (BAI) poses a non-trivial challenge, primarily owing to the inherent uncertainty associated with the true reward distribution of each arm. This challenge is further exacerbated when rewards are multi-dimensional (as in the MO-MAB setting). As delineated in prior works, numerous practical applications exhibit rewards that are multi-dimensional in nature, as opposed to being solely scalar, such as hardware design (Zuluaga et al., 2016), drug development and dose identification (Lizotte and Laber, 2016) in clinical trials, and electric battery control (Busa-Fekete et al., 2017). However, the existing works on pure exploration in MO-MAB settings are mainly focused on Pareto frontier identification. The identification of the best arm for each objective, an inherent task in MO-MAB scenarios, has not received comprehensive scholarly attention. This study seeks to fill the research gap in this domain."}, {"title": "Overview of Existing Works", "content": "Multi-objective bandits and fixed-confidence BAI have both been extensively investigated in the literature. This section highlights a collection of recent studies addressing these topics. For the problem of BAI in the fixed-confidence regime, Garivier and Kaufmann (2016) first proposed the well-known TRACK-AND-STOP (TAS) algorithm with two variants (C-Tracking and D-tracking), and demonstrated the optimality of these variants in the asymptotic limit of vanishing error probabilities. The basic premise for achieving asymptotic optimality, they showed, is to pull arms according to an oracle weight that is derived from the problem instance-dependent lower bound. Later, Degenne et al. (2020) and Jedra and Proutiere (2020) specialised the TAS algorithm to the linear bandit setting, while still maintaining asymptotic optimality. For more general structured bandits with non-linear structural dependence between the mean rewards of the arms, Wang et al. (2021) proposed an efficient TAS-type algorithm and a novel lower bound for the structured MAB setting, and further established the asymptotic optimality of their algorithm. Mukherjee and Tajer (2023) also proposed an efficient scheme for achieving asymptotic optimality without solving for the oracle weight at each time step. It is noteworthy that the aforementioned studies deal with a single objective, whereas our research deals more generally with multiple objectives.\nDegenne and Koolen (2019) explored the problem of fixed-confidence BAI with multiple correct answers (i.e., multiple best arms), with the objective of identifying any one of the correct answers. They proposed the STICKY TRACK-AND- STOP algorithm along the lines of C-Tracking and demonstrated its asymptotic optimality. While their setup appears to bear similarities with ours, it is worth noting that their work aims to identify one among several correct answers, whereas our study focuses on uncovering all correct answers (i.e., the best arm of every objective).\nDrugan and Nowe (2013) introduced the MO-MAB setting as well as two associated metrics-the Pareto regret and the scalarized regret. The authors proposed two UCB-like algorithms to optimize these two metrics. Subsequently, several factions of researchers have predominantly concentrated on the study of Pareto regret. Turgay et al. (2018) tackled the problem of Pareto regret minimization by introducing a similarity assumption regarding the means of arms, and designed an algorithm that achieves a regret upper bound of the order $\\tilde{O} (T^{(1+d_p)/(2+d_p)})$, where $d_p$ is the number of dimensions that is a function of the arm vectors and the environmental context. Xu and Klabjan (2023) defined the notion of Pareto regret in the context of adversarial bandits (Lattimore and Szepesv\u00e1ri, 2020, Chapter 11), and designed an algorithm achieving near-optimality up to a factor of log T in both adversarial and stochastic bandit environments.\nOn the topic of pure exploration in the MO-MAB setting, a popular line of work is Pareto optimal arm identification or Pareto frontier identification. Considering specifically the fixed-confidence regime, Auer et al. (2016) proposed a successive elimination (SE)-type algorithm to identify all the Pareto optimal arms. For any given confidence level, the upper bound on the sample complexity of their algorithm matches their lower bound up to a logarithmic factor that is a function of the sub-optimality gaps of the arms. Along similar lines, Ararat and Tekin (2023) present another SE-type algorithm to identify all Pareto (\u20ac, \u03b4)-PAC arms, a generalization of Pareto optimal arms. More recently, Kim et al. (2023) developed a framework for analysing the problem of Pareto frontier identification in linear bandits. Their proposed algorithm is nearly optimal up to a logarithmic factor involving the minimum of the arm sub-optimality gaps and the algorithm's accuracy parameter. While the best arm of each objective is notably also Pareto optimal, our work goes beyond merely identifying a subset of Pareto optimal arms, hence significantly advancing the state-of-the-art in multi-objective bandits."}, {"title": "Our Contributions", "content": "While existing works on pure exploration in multi-objective bandits primarily focus on Pareto frontier identification, we bridge the gap by investigating the problem of identifying the best arm of each objective under the fixed-confidence regime.\nWe provide an asymptotic lower bound for the problem and complement it with an algorithm that achieves the lower bound up to a multiplicative constant of 1 + \u03b7, where \u03b7 > 0 is a tuneable parameter that can be chosen arbitrarily close to 0. We show that the lower bound is characterised by the solution to a max-min optimisation problem that is reminiscent of fixed-confidence BAI problems. The basic premise upon which asymptotically optimal algorithms of the prior works such as D-Tracking (Garivier and Kaufmann, 2016) and Sticky TAS (Degenne and Koolen, 2019) operate is to compute the max-min optimisation at each time step to evaluate the oracle weight at each time instant, and to pull arms according to the (empirical) oracle weight in order to guarantee asymptotic optimality. However, these approaches may be inefficient, as the oracle weight, to the best of our knowledge, does not have a closed-form solution in multi-objective cases. Instead of following the (empirical) oracle weight, we propose a novel technique to sample the arms at each step, based on the idea of surrogate proportions. These surrogate proportions serve as proxy for the oracle weight and can be computed efficiently."}, {"title": "Preliminaries and Problem Setup", "content": "Let N denote the set of positive integers. For $n \\in N$, let $[n] := \\{1, ..., n\\}$. We consider a multi-armed bandit with K arms in which each arm is associated with M independent objectives. Pulling arm $A_t \\in [K]$ at time step t yields an M-dimensional reward $r_t = [r_{t,m} : m \\in [M]] \\in \\mathbb{R}^M$, where $r_{t,m} = \\mu_{A_t,m} + N_{t,m}$; here, $\\mu_{A_t,m} \\in \\mathbb{R}$ is the unknown mean corresponding to objective m of arm $A_t$, and $N_{t,m}$ is an independent standard normal random variable. Let $v = [\\mu_{i,m} : (i, m) \\in [K] \\times [M]]$ denote a problem instance in which $pi,m$ is the mean corresponding to objective m of arm i. Arm i is said to be the best arm of objective m if it has the highest mean in dimension m across all arms, i.e., $pi,m > \\mu_{j,m}$ for all $j \\neq i$. Without loss of generality, we assume that each objective has a unique best arm, and we write P to denote the set of all problem instances with a unique best arm for each objective. We write $I^*(v) = (i_1^*(v), i_2^*(v), ..., i_M^*(v))$ to denote the collection of best arms under instance v; here, $i_m^*(v)$ is the best arm of objective m.\nGiven an error probability threshold $\\delta \\in (0, 1)$, the goal is to identify the set of best arms $I^*(v)$ in the shortest time, while ensuring that the error probability is within \u03b4. Formally, an algorithm (or policy) for identifying the best arms is a tuple $\\pi = (\\mathcal{A}, \\tau, \\hat{I})$ consisting of the following components."}, {"title": "Lower bound", "content": "In this section, we present an lower bound on (2) for any instance $v \\in P$. We first introduce the notion of sub-optimality gaps under this instance. For any arm $i \\in [K]$ and objective $m \\in [M]$, we define the sub-optimality gap of the tuple (i, m) under the instance v as\n$\\Delta_{i,m}(v) := \\mu_{i_m^*(v),m} - \\mu_{i,m}(v)$,\nwhere, to recall, $i_m^*(v)$ is the best arm of objective m under instance v.\nProposition 3.1. Fix $\\delta \\in (0,1)$. For any \u03b4-PAC policy \u03c0,\n$\\mathbb{E}_v^{\\pi} [\\tau_{\\delta}] \\geq c^*(v) \\log \\left( \\frac{1}{\\delta} \\right)  \\quad \\forall v \\in \\mathcal{P}$", "equations": ["\\Delta_{i,m}(v) := \\mu_{i_m^*(v),m} - \\mu_{i,m}(v)", "\\mathbb{E}_v^{\\pi} [\\tau_{\\delta}] \\geq c^*(v) \\log \\left( \\frac{1}{\\delta} \\right)  \\quad \\forall v \\in \\mathcal{P}"]}, {"title": "Achievability: Proposed Method", "content": "In this section, we describe our computationally efficient algorithm named MO-BAI based on the idea of surrogate proportion for pulling arms at each time step, and we also provide the pseudocode in Algorithm 1 in the Appendix A. Before we present the algorithm formally, we introduce some notations. Let $\\mu_{i,m}(t)$ denote the empirical mean of rewards obtained from objective m of arm i up to time t, i.e., $\\mu_{i,m}(t) := \\frac{1}{N_{i,t}} \\sum_{s=1}^t \\mathbb{1}_{\\{A_s=i\\}} r'_{s,m}$, and $N_{i,t} := \\sum_{s=1}^t \\mathbb{1}_{\\{A_s=i\\}}$ denotes the number of times arm i is pulled until time t. Let $\\Delta_{i,m}(t) := \\mu_{i_m^*(t),m}(t) - \\mu_{i,m}(t)$ denote the empirical gap of the tuple (i, m) \u2208 [K] \u00d7 [M], where $i_m^*(t) \\in \\arg \\max_{l\\in[K]} \\mu_{l,m}(t)$ denotes the empirical best arm of objective m at time t. Let $g_v(\\cdot)$ be as defined in (7), and for all m \u2208 [M] and i \u2208 [K], let", "equations": ["\\mu_{i,m}(t) := \\frac{1}{N_{i,t}} \\sum_{s=1}^t \\mathbb{1}_{\\{A_s=i\\}} r'_{s,m}", "\\Delta_{i,m}(t) := \\mu_{i_m^*(t),m}(t) - \\mu_{i,m}(t)"]}, {"title": "Surrogate Proportion and Arm Selection Rule", "content": "Let $l_t := \\max_{k \\in \\mathbb{N}: 2^k < t} 2^k$, and let $\\hat{v}_t$ denote the empirical instance with means $[\\mu_{i,m}(t') : (i, m) \\in [K] \\times [M]]$ for $t\\in \\mathbb{N}$ and $t' = \\max\\{t - 1, K\\}$. The surrogate proportion at time step t, denoted $s_t$, is defined as", "equations": ["s_t := \\arg \\max_{s \\in \\Gamma^{(\\eta)}} h_{\\hat{v}_{l_t}} (\\omega_{\\cdot,t-1}, s)", "\\Gamma^{(\\eta)} := \\{ \\omega \\in \\Gamma : \\forall i \\in [K], \\omega_i \\geq \\frac{\\kappa}{(1+\\eta)K} \\}", "h_{\\Gamma} (\\omega, z) := \\min_{\\substack{m \\in [M] \\cr i \\in [K] \\setminus i_m^* (v) }} \\{ g_{\\hat{v}_{l_t}}^{(i,m)} (\\omega) + (\\nabla_{\\omega} g_{\\hat{v}_{l_t}}^{(i,m)} (\\omega), z - \\omega) \\}."]}, {"title": "Stopping and Recommendation Rules", "content": "We now delineate the stopping and recommendation rules employed in our algorithm. We adopt a variant of Chernoff's stopping rule (Kaufmann et al., 2016; Lattimore and Szepesv\u00e1ri, 2020); our design is particularly inspired by Chen et al. (2023). Specifically, let", "equations": ["Z(t) := \\min_{\\substack{m \\in [M] \\cr i \\in [K] \\setminus i_m^* (t) }} \\frac{N_{i,t} N_{i_m^* (t),t}}{\\mu_{i,t} N_{i_m^* (t),t}} \\frac{(\\mu_{i_m^* (t),t} - \\mu_{i,t})^2}{2(\\sqrt{N_{i,t}} + \\sqrt{N_{i_m^* (t),t}})}", "\\tau_{\\delta} = \\min\\{t > K : Z(t) > \\beta(t, \\delta)\\}", "\\beta(t, \\delta) := MK \\log(t^2 + t) + f^{-1}(\\delta)", "f(x) := \\sum_{i=1}^{MK} \\frac{x^{i-1} e^{-x}}{(i - 1)!}  \\quad x \\in (0,+ \\infty)."]}, {"title": "Performance of MO-BAI", "content": "In this section, we characterise the performance of MO-BAI with a fixed input parameter \u03b7. The first result below asserts that MO-BAI is \u03b4-PAC for any \u03b4 \u2208 (0, 1).\nProposition 4.1. Fix \u03b7 > 0 and \u03b4 \u2208 (0,1). Then, MO-BAI with parameter \u03b7 is \u03b4-PAC, i.e., \u2200v \u2208 P", "equations": ["\\mathbb{P}_v^{MO-BAI} (\\tau_{\\delta} < +\\infty) = 1  \\text{ and }", "\\mathbb{P}_v^{MO-BAI} (\\hat{I}_{\\delta} = I^* (v)) \\geq 1 - \\delta."]}, {"title": "Proof Sketch of Theorem 4.2", "content": "In this section, we outline the key ideas that go into the proof of Theorem 4.2. At a high level, the proof encompasses two important steps: (1) Deriving a bound on the limiting value of Z(t)/t, and (2) Deriving a upper bound on the stopping time 75 based on the limiting value of Z(t)/t. Throughout, we fix an underlying instance v.", "equations": ["C(\\upsilon, \\eta) := \\sup_{\\omega,\\gamma \\in \\Gamma^{(\\eta)}, \\gamma \\in (0,1), \\atop z = \\omega + \\gamma(y - \\omega)} \\frac{2}{\\gamma^2} \\left( h_{\\upsilon} (\\omega, z - \\omega) - g_{\\upsilon} (z) \\right)."]}, {"title": "Bounding the Limiting Value of Z(t)/t", "content": "The key to deriving a \u201cgood\u201d upper bound for the limiting value of Z(t)/t lies in establishing that the evaluations of $g_v$ at the oracle weights arising from MO-BAI (with input parameter \u03b7) match, in the long run, with the constant c* (v) appearing in the lower bound up to a factor of 1 + \u03b7. Towards this, we introduce the following auxiliary constant $\\theta_{\\eta} > 0$:"}, {"title": "Stopping time", "content": "Using the limiting value of Z(t)/t derived earlier, we upper bound the stopping time almost surely and in expectation. First, we introduce two auxiliary terms.", "equations": ["T_{gap} (\\upsilon, \\eta, \\epsilon) := \\min \\left\\{ t : \\frac{Z(t)}{t} - \\tilde{c} \\left(\\upsilon, \\eta \\right)^{-1} < \\epsilon \\quad \\forall t > T_{gap} (\\upsilon, \\eta, \\epsilon) \\right\\}", "T_{thres} (\\upsilon, \\eta, \\epsilon, \\delta) := 1 + \\left \\lceil \\frac{f^{-1}(\\delta)}{\\tilde{c} (\\upsilon, \\eta)^{-1} - \\epsilon \\tilde{c} (\\upsilon, \\eta)^{-1} - \\epsilon } + \\frac{MK}{2 \\left(\\tilde{c} (\\upsilon, \\eta)^{-1} - \\epsilon \\tilde{c} (\\upsilon, \\eta)^{-1} - \\epsilon \\right)^2} \\log \\left( \\frac{2 f^{-1}(\\delta)}{\\tilde{c} (\\upsilon, \\eta)^{-1} - \\epsilon} \\right)^2 + 1 \\right \\rceil."]}, {"title": "Numerical Study", "content": "We run experiments to validate the effectiveness of MO-BAI through empirical assessments on the SNW dataset (Zuluaga et al., 2016) and a synthetic dataset, and their detailed descriptions are presented in Appendix A.2. Specifically, we compare our algorithm against BASELINE, a multi-objective adaptation of the D-Tracking algorithm by Garivier and Kaufmann (2016) which was originally designed for a single objective (see details in Appendix A.5), and a Successive Elimination-based (Even-Dar et al., 2006) algorithm. As alluded to in Remark 3, the implementation of BASELINE involves solving an optimization problem, which, in scenarios with M > 1, may not be exactly resolved. Hence, we employ an iterative method (details in Appendix A and pseudo code of Algorithm 3), and the accuracy of the obtained solution depends on the number of iterations."}, {"title": "Conclusions and Future Work", "content": "This work considered a novel best arm identification setting in which a single pull of an arm yields an M-dimensional vector as its reward. The goal was to identify the M best arms, one corresponding to each dimension, under the fixed- confidence regime. We developed an efficient algorithm based on the original idea of surrogate proportions, that we proved is asymptotically optimal and computationally efficient. We conducted empirical studies on a synthetic dataset and the SNW datasets to substantiate the proposed algorithm's computational efficiency and asymptotic optimality. Our results are asymptotically optimal in the sense that the results are tight as the error probability 8 \u2193 0. It would be fruitful to investigate whether the ideas in non-asymptotic strengthenings of single-objective pure exploration problems (e.g., Degenne et al. (2019)) carry over to our multi-objective setting."}, {"title": "Proof of Proposition 3.1", "content": "Firstly, we introduce a useful lemma adapted from Kaufmann et al. (2016).\nLemma D.1. Fix \u03b4 > 0 and a \u03b4-PAC policy \u03c0 with stopping time 78. Let Frs = \u03c3({(XAt,m(t), At) : t \u2208 [T8], m\u2208 [M]}) denote the history of all the arm pulls and rewards seen up to the stopping time ts under the policy \u03c0. Then, for any pair of instances v, v' \u2208 P with arm means {pi,m : i \u2208 [K],m \u2208 [M]} and {pi,m : i \u2208 [K], m\u2208 [M]} respectively, and any Frs-measurable event E,", "equations": ["\\sum_{i=1}^K \\sum_{m=1}^M \\mathbb{E}_v^{\\pi} [N_{i,\\tau_{\\delta}}] \\frac{(\\mu_{i,m} - \\mu'_{i,m})^2}{2} \\geq d_{KL} (\\mathbb{P}_{v}^{\\pi} (E), \\mathbb{P}_{v'}^{\\pi} (E))", "\\mathbb{E}^{\\pi} [\\tau_{\\delta}] \\geq \\frac{\\log \\left( \\frac{1}{\\delta} \\right)}{\\sup_{\\omega \\in \\Gamma} \\inf_{v' \\in Alt(v)} \\sum_{m=1}^M \\sum_{i=1}^K \\omega_i \\frac{(\\mu_{i,m}(v) - \\mu_{i,m}(v'))^2}{2}}", "g_{\\upsilon} (\\omega) = \\inf_{v' \\in Alt(v)} \\sum_{i=1}^K \\omega_i \\sum_{m=1}^M \\frac{(\\mu_{i,m} - \\mu'_{i,m})^2}{2}", "\\inf_{v' \\in Alt(v)} \\sum_{m=1}^M \\sum_{i=1}^K \\omega_i \\frac{(\\mu_{i,m} - \\mu'_{i,m})^2}{2} = \\min_{\\substack{m \\in [M] \\cr i \\in [K] \\setminus i_m^* (v) }} \\inf_{v' : \\mu'_{i,m} > \\mu'_{i_m^*(v), m}} \\omega_i \\frac{(\\mu_{i,m} - \\mu'_{i,m})^2}{2}", "=\\min_{\\substack{m \\in [M] \\cr i \\in [K] \\setminus i_m^* (v) }} \\frac{\\omega_i}{2}  \\inf_{v' : \\mu'_{i,m} \\geq \\mu'_{i_m^*(v), m}}  (\\mu_{i,m} - \\mu'_{i,m})^2 \\frac{\\omega_i}{2} \\left( \\mu_{i_m^*(v), m} - \\mu_{i,m} \\right)^2", "=\\min_{\\substack{m \\in [M] \\cr i \\in [K] \\setminus i_m^* (v) }}  \\frac{\\omega_i}{2}  \\Delta_{i,m}^2(v)", "g_v (\\omega) = \\min_{\\substack{m \\in [M] \\cr i \\in [K] \\setminus i_m^* (v) }}  \\frac{\\omega_i \\Delta_{i,m}^2(v)}{2}", "\\mu'_{i_m^*(v), m} = \\mu_{i,m} + (\\mu_{i_m^*(v),m} - \\mu_{i,m}) \\frac{\\omega_i}{\\omega_i + \\omega_{i_m^*(v)}}", "\\mu'_{i,m} = \\mu_{i_m^*(v), m} - (\\mu_{i_m^*(v),m} - \\mu_{i,m}) \\frac{\\omega_{i_m^*(v)}}{\\omega_i + \\omega_{i_m^*(v)}}"]}, {"title": "Proof of Proposition 4.1", "content": "Below, we first record some useful results that will be used in the proof.\nLemma E.1. (Lattimore and Szepesv\u00e1ri, 2020, Lemma 33.8) Let Y1, Y2,... be independent Gaussian random variables with mean \u00b5 and unit variance. Let un := 1 =1 Yi. Then,", "equations": ["\\mathbb{P} (\\exists n \\in \\mathbb{N} : (\\mu_n - \\mu)^2 > \\log(1/\\delta) + \\log(n(n + 1))) \\leq \\delta."]}, {"title": "Multi-Objective Best Arm Identification", "content": "In this section, we show that the rank of multi-objective BAI problem with M independent objectives is equal to 2 for all values of M; notably, this is also the rank of the single-objective BAI problem. Before we present the formal arguments, we reproduce the definition of rank of a pure exploration problem from Kaufmann and Koolen (2021).", "equations": ["\\bigcap_{q=1}^Q \\mathcal{O} \\setminus \\mathcal{O}_p = \\left\\{ \\Lambda \\in \\mathbb{R}^d : \\exists (\\kappa_{p,q}, ..., \\lambda_{kp,Q}) \\quad \\Lambda_{\\kappa_{p,q}} \\in L_{p.a}\\right\\}", "\\mathcal{O} \\setminus \\mathcal{O}_p = \\bigcup_{q \\neq p} \\left\\{ \\Lambda \\in \\mathbb{R}^K : \\Lambda_q > \\Lambda_p \\right\\}"]}, {"title": "Numerical Study", "content": "In this section, we provide more details on our experimental implementation. Especially, we implement the algorithm of MO-BAI as shown in the pseudocode of Algorithm 1."}, {"title": "Simulation Environment", "content": "The experiments were executed on an Apple M1 Chip with 16 GB of memory, operating on Mac OS 14.2.1. The linear programming procedures outlined in Algorithm 1 and Algorithm 3 were executed using SciPy v1.11.3 within the Python 3.11.2 environment."}, {"title": "Descriptions of the Datasets", "content": "Synthetic Dataset: Our synthetic dataset is generated with parameters K = 20 and M = 10. For all pairs (i, m) where i \u2260 m, pi,m is uniformly chosen from the interval [0, 1]. For pairs (i, m) where i = m, pi,m is uniformly selected from [1.2, 2]. These values remain constant throughout the experiment. Let v = [\u00b5i,m : (i, m) \u2208 [K] \u00d7 [M]]. It is evident that im (v) = m for every m \u2208 [M]. Additionally, \u2206i,m(v) > 0.2 for all i \u2260 im (v).\nSNW Dataset: We adopt the SNW dataset introduced by Zuluaga et al. (2016), consisting of 206 distinct hardware implementations of a sorting network. Following the protocol outlined by Ararat and Tekin (2023), the objective values, represented by the negative of the area, serve as mean rewards for the designs, and Gaussian noises are added to the mean rewards in the bandit dynamic. Consequently, in this dataset, we have K = 206 and M = 2. To facilitate the simulation, we scale the rewards of each arm by a factor of 10."}, {"title": "Results for the SNW Dataset", "content": "We fix \u03b7 = 0.1 in our implementation of MO-BAI. We run three independent trials, and average the stopping times from these trials to obtain the values in Table 2. The tabulated results indicate the superior performance of our proposed MO-BAI algorithm over BASELINE on the SNW dataset. Notably, this dataset involves a greater number of decision variables compared to the synthetic dataset instance (i.e., 206 versus 20), posing increased difficulty in running the optimization routine of BASELINE. Consequently, to achieve comparable performance with MO-BAI, it is necessary to increase the number of iteration steps in BASELINE. This underscores the superiority of MO-BAI from a practical standpoint."}, {"title": "Curated Threshold for Simulations", "content": "It is customary in the fixed-confidence BAI literature to employ thresholds in simulations that differ from theoretical thresholds. Notably, in the single-objective case, Garivier and Kaufmann (2016) utilized $c_{t}^{\\text {empirical }}(\\delta)=\\log \\frac{\\left(1+\\log t\\right)}{\\delta}$ for empirical evaluation, a threshold that is (a log factor) smaller than the theoretical threshold $\\beta^{\\text {theoretical }}(t, \\delta)=\\log \\frac{\\left(C K t^{a}\\right)}{\\delta}$ employed in their D-Tracking algorithm. Here, a > 1 is a parameter of the D- Tracking algorithm, and CK is a universal constant that depends only on the number of arms K. In a recent work, Kaufmann and Koolen (2021) introduced the concept of \"rank\" for pure exploration problems. They demonstrated that for a problem with rank R, the threshold", "equations": ["c_{t}^{\\text {empirical }}(\\delta)=\\log \\frac{\\left(1+\\log t\\right)}{\\delta}", "\\beta^{\\text {theoretical }}(t, \\delta)=\\log \\frac{\\left(C K t^{a}\\right)}{\\delta}"]}, {"title": "The BASELINE Algorithm and its Implementation Details", "content": "The pseudo-code of BASELINE is presented in Algorithm 2. It is important to note that the approach proposed by Garivier and Kaufmann (2016) for solving the optimization problem in line 6 of BASELINE becomes impractical when M > 1 due to the various best arms across different objectives. In our implementation of BASELINE, we adopt the sub-routine in Algorithm 3 to solve the optimisation problem in Line 6 of Algorithm 2 by specifying the number of iterations steps ITER, and its convergence can be established by the idea of Lemma F.8. Figure 1 and Tab. 1 show respectively the stopping times and computation times (in ms) incurred under ITER \u2208 {5, 10, 20}.\nFurthermore, the threshold used in the implementation of BASELINE is equal to the single-objective empirical threshold of log((1+log t)/8) used in Garivier and Kaufmann (2016). Notably, this threshold remains independent of the number of objectives M. The rationale behind this choice stems from the fact that the \u201crank\u201d of a multi-objective BAI problem with M independent objectives is equal to the rank of the single-objective BAI problem for all values of M. See Appendix C for further details.\nAs such, the BASELINE algorithm, with any value of ITER < +\u221e, is not asymptotically optimal (though practically implementable), while asymptotically optimal but practically not implementable for ITER = +\u221e. A plausible scheme to achieve asymptotic optimality, while ensuring practical feasibility, is to let ITER grow with t. For e.g., if ITER(t) = O(log t), solving for \u1ff6.,t = argmaxw\u2208r 9\u00fbt (w) up to a 1/poly(t) error at time step t requires O(log t) iterations. However, quantifying the exact growth rate (e.g., ITER(t) = O(log(t)), O(\u221at), O(t), or O(exp(t))) that is necessary to achieve asymptotic optimality is a technically challenging task; the latter involves quantifying the approximation error of each subroutine as ITER grows with t, and ensuring that these errors amortize asymptotically as t \u2192 \u221e. Moreover, if ITER(t) growth as t (e.g., ITER(t"}]}