{"title": "CHUNK-DISTILLED LANGUAGE MODELING", "authors": ["Yanhong Li", "Jiawei Zhou", "Karen Livescu"], "abstract": "We introduce Chunk-Distilled Language Modeling (CD-LM), an approach to\ntext generation that addresses two challenges in current large language models\n(LLMs): the inefficiency of token-level generation, and the difficulty of adapting\nto new data and knowledge. Our method combines deep network-based LLMs\nwith a straightforward retrieval module, which allows the generation of multi-\ntoken text chunks at a single decoding step. Our retrieval framework enables\nflexible construction of model- or domain-specific datastores, either leveraging\nthe internal knowledge of existing models, or incorporating expert insights from\nhuman-annotated corpora. This adaptability allows for enhanced control over\nthe language model's distribution without necessitating additional training. We\npresent the CD-LM formulation along with performance metrics demonstrating\nits ability to improve language model performance and efficiency across a diverse\nset of downstream tasks. Code and data will be made publicly available.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) have become a crucial component of intelligent systems, but still\nsuffer from fundamental challenges to their efficiency and performance. LLMs are most commonly\nbased on autoregressive Transformers (Vaswani et al., 2017) and typically generate text sequences\none token at a time in a serial fashion, which limits their efficiency. Moreover, once pre-trained, up-\ndating the model parameters requires expensive data and computational resources, making it difficult\nto incorporate dynamic knowledge into the model.\nSeveral techniques have been proposed to improve the efficiency and performance of LLMs, such\nas speculative decoding (Leviathan et al., 2023; Chen et al., 2023; Miao et al., 2024; Spector & Re,\n2023) and retrieval-augmented generation (RAG) (Lewis et al., 2020; Guu et al., 2020; Borgeaud\net al., 2022). The former relies on a smaller model to speculate several tokens at a time to reduce\ninference runtime while retaining the same model distribution, while the latter combines parametric\nlanguage models with non-parametric memory to improve adaptability to dynamic knowledge but\noften without efficiency gains.\nThis work aims to alleviate both challenges via a fine-grained retrieval-augmented language model-\ning approach that focuses on text chunks, or contiguous spans of tokens that often appear together.\nThe intuition for this approach is that a substantial amount of linguistic or factual knowledge can\nbe expressed in text chunks spanning multiple contiguous tokens, such as named entities, multi-\nword expressions, and other common phrases. These sub-sentence structures tend to exhibit lower\nvariability compared to larger text units such as sentences, and are often memorized precisely by\nwell-trained LLMs.\nChunks conveying key content are\noften repeated verbatim across multiple decoding runs with similar contexts, and the LLM proba-\nbilities over token sequences show recurring plateaus of high probability within such multi-token\nchunks. By injecting memorized or novel chunks into the generation process, we may be able to\nimprove the models' ability to adapt to new domains or knowledge. In addition, if entire chunks can\nbe cached and retrieved during inference, we should also be able to speed up text generation."}, {"title": "BACKGROUND", "content": "While many attempts have been made to improve language modeling and generation efficiency,\nit remains a significant challenge to address both simultaneously. For example, non-parametric\napproaches like kNN-LM (Khandelwal et al., 2020) reduce LM perplexity in certain domains, but\ntends to require a sizable database for retrieval and adds latency during generation; specialized\ninference algorithms like speculative decoding (Spector & Re, 2023) speed up generation but keep"}, {"title": "LANGUAGE MODELING WITH CHUNK GENERATION", "content": "In this section, we introduce a general framework of language modeling that interleaves chunk gen-\nerations with tokens from a standard autoregressive LM. We then describe the operational details of\nthe chunk generation process with retrieval from a structured database in Section 4. Together, these\ntwo sections build the core ideas of CD-LM. Finally, we derive a tractable algorithm for computing\nsequence probabilities under CD-LM in Section 5."}, {"title": "PRELIMINARIES", "content": "An autoregressive language model assigns a probability to any given sequence of tokens\n(x1,x2,..., XN) as follows\n\\begin{equation}\nP_{\\theta}(x_1, x_2, ..., x_N) = \\prod_{n=1}^N P_{\\theta}(x_n | x_{<n})\n\\end{equation}\nwhere \u03b8 is the model parameters and x<n = (X1,X2,...,xn\u22121). Modern LLMs are usually param-\neterized by Transformer (Vaswani et al., 2017) architectures composed of stacks of self-attention"}, {"title": "TEXT CHUNK GENERATION MODELING", "content": "Instead of producing text one token at a time, we provide a mechanism that can directly generate\na span of multiple consecutive tokens, or chunks, with better efficiency and flexibility of injecting\nknowledge on fine-grained sub-sentence levels into the model distribution on the fly.\nFormally, we use n to index sequential token position,\nand t to index generation steps. For every step, we allow\ngeneration of either a single token from a base LM Me\nwith parameter \u03b8, or a text chunk from a different model\nG, which we call the chunk proposal model. Let lt de-\nnote the sequence length (i.e., the number of tokens) after\nt steps. Unlike typical token-based decoding, we have\nlt \u2265 t. In particular, the chunk proposal model G takes\nany prefix X<n and returns a possible text chunk contin-\nuation Cn = (Xn, Xn+1,..., Xn+Tn-1) with acceptance\nprobability qn \u2208 [0, 1], and in the length of the proposed\nchunk.2 We introduce a binary random variable zn that\ndenotes whether the generation at token position n uses\nthe chunk proposed by G or defaults to the single token\ngenerated by the LM, and p(n = 1) = qn. The chunk-"}, {"title": "CD-LM WITH FINE-GRAINED RETRIEVAL", "content": "In this section, we describe in detail our retrieval-based chunk proposal model G needed for step (3)\ndefined in Section 3.2, completing the chunk-interleaved generative process under CD-LM. These\ndetails include datastore representation of chunks (Section 4.1), chunk proposal process with re-\ntrieval (Section 4.2), and chunk sources that enable different applications (Section 4.3)."}, {"title": "CHUNK DATASTORE CONSTRUCTION", "content": "Given any text corpus C, suppose there is an expert model & (to be elaborated in Section 4.3) that\nidentifies text spans in C that we want to re-use for generation. These chunks often convey integral"}, {"title": "ADAPTIVE CHUNK RETRIEVAL FOR GENERATION", "content": "Given previously generated tokens X<n, we formulate the chunk proposal model G(x<n) \u2192\n(Cn, qn) as a chunk retrieval process to be interleaved with the LM generation. We use the infor-\nmation from the LM computation en route to the most recent token In\u22121 to derive plausible chunk\nproposals. Per Eq (2), right before generation of In\u22121, the context vector fe(X<n-1) provides a\nsummary of the context, which we use as the query for chunk retrieval. We use Xn-1 as the entry\ntoken to confine the chunk search to the corresponding trie Tan-1, leading to smooth chunk contin-\nuations (for instance, see the searched trie in Figure 3). This is crucial for improving the naturalness\nof the retrieved chunks combined with the previous context. In the meantime, using the entry token\ntrie to limit the search space also greatly enhances retrieval efficiency. Formally, the chunk proposal\nmodel G is given by\n\\begin{equation}\n(u^*, c_n) = \\underset{(u, s) \\in T_{x_{n-1}}}{\\arg \\max} \\{sim(f_{\\theta}(x_{<n-1}), f_{\\theta}(u))\\}\nq_n = g_{\\phi} (sim(f_{\\theta}(x_{<n-1}), f_{\\theta}(u^*)))\n\\end{equation}\nwhere u is the stored chunk context except entry token, \u2208 Txn\u22121 means searching for each node\nin trie, sim(\u00b7,\u00b7) is a vector similarity measure for which we use cosine similarity, and g$\\phi$(\u00b7) is a\nfunction parametrized by \u03c6 to convert the similarity scores to acceptance probabilities, which can\nbe tuned for different base LMs Me (implementation details described in Section 6)."}, {"title": "CHUNK EXTRACTION MODEL", "content": "Now we describe the expert model & that provides the chunks for the datastore. We categorize the\npossible knowledge sources into three major types intended for various CD-LM applications:\nKnowledge Distillation As suggested earlier in Figure 2, well-trained LLMs memorize text chunks\nwith high probabilities in contexts. This provides a natural source of automatically defined chunks\nfrom models' internal knowledge. Let Met denote the pre-trained model with parameter 07 that we\nderive chunks from. It is often a more powerful model that is larger or more specialized, which serves\nas a teacher to help adapt the distribution of the base LM Mo. Operationally, for chunk identification\nwe run Met on a text corpus C, and apply a thresholding heuristic to extract the longest chunks\nwhose token probabilities are all above a threshold y (see Appendix D.2 for an example). Formally,\nchunk s along with context r is extracted if \u2203r, s.t. po(xi|r,x<i) \u2265 y, \u2200(Xi, X<i) \u2208 s. Note that\nthe chunk datastore construction this way only needs one forward pass of M\u04e9\u0442 on C. We also run\na forward pass of the base model Me on the chunk contexts for their hidden vectors for retrieval"}, {"title": "PROBABILITY DISTRIBUTION UNDER CD-LM", "content": "Sampling text with the CD-LM generative process in Section 3.2 is fairly easy, but assigning proba-\nbilities to a given text sequence is non-trivial. This requires enumerating all possible chunk propos-\nals at different token positions to marginalize the zn variables, which is complicated due to variable\nchunk lengths and dependency structures shown in Figure 4. We derive a dynamic program sim-\nilar to a backward algorithm for computing sequence probabilities under CD-LM, allowing use to\nmeasure intrinsic language modeling performance with perplexity (PPL).\nFor any given sequence x1:N, the chunk proposals at every position (Cn, qn) from G(x*n) are deter-\nministic given the datastore D and thus can be pre-computed. CD-LM models the joint distribution\nof X: N, 22:N as\n\\begin{equation}\np(x_{1:N}, z_{2:N}) = p(x_1) \\prod_{n=2}^N [p(z_n | x^*_n, z_{<n})\\cdot p(x_{n:n+T_n-1} | x^*_{<n}, z_{<n})]\\mathbf{1}\\{n; z_{2:n}\\}\n\\end{equation}\nwhere the binary indicator function 1{n; 22:n} marks whether the token position n is inside of a\nsampled chunk based on the values of 22:n . To marginalize over 22:N, we define\nan = p(xn:N|Zn = 1, x*<n, z<n) = 1 {xn:n+Tn-1 = Cn}. [an+rnQn+\u03c4\u03b7 + \u03b2\u03b7+\u03c4\u03b7 (1 - qn+tn)]\n\u03b2n = p(x:Nzn = 0, x*<n, z<n) = Po(xm|x*<n) [an+19n+1 + \u03b2n+1(1 - qn+1)]\nwhere the function 1{xn:n+n-1 = Cn} indicates whether the proposed chunk en exactly matches\nthe given text segment, and pe is the probability from Me. By computing a and \u1e9e values backward\nfrom N to 2, we can get the marginal sequence probability under CD-LM as\nP(x1:N) = P(x\u2081) [a2q2+ \u03b22(1 \u2013 92)]"}, {"title": "EXPERIMENTS", "content": "We conduct experiments on multiple LMs and tasks. We formulate go in Eq (3) as a simple piece-\nwise linear function, where the maximum context matching similarity score only maps to a non-zero"}, {"title": "KNOWLEDGE DISTILLATION", "content": "1.1\nModel and Data We focus on two objectives: improving language modeling performance and\nenabling domain adaptation, using a weak pre-trained 137M GPT-2 small model as the base lan-\nguage model, Mo, for KCD-LM. For language modeling, we evaluate on the WikiText-103 dataset\nand the Dockerfile subset of the GitHub Code dataset. Dockerfile is a low-resource code language\nand the base model has poor PPL on the Dockerfile data. This setting allows us to explore the ef-\nfectiveness of KCD-LM in low-resource settings. For domain adaptation, we focus on adapting to\nmedical and legal domains. We use the Medical Instruction Dataset, which contains conversations\nbetween an AI assistant and patients during medical consultations, and the Federal Register subset\nof the Pile-of-Law (Henderson et al., 2022). For these tasks, we set as the teacher model \u041c\u04e9\u0442\neither a pretrained 1.5B GPT-2 XL model (for code) or an off-the-shelf domain-specific GPT-2 XL\nmodel (for WikiText, medical, and law). Chunk datastores are constructed from the corresponding\ntraining sets. Empirically, extracting chunks from WikiText-103 takes under an hour on four A4000\nGPUs for a small base model like GPT-2. Building the WikiText datastore takes up to 1.5 hours on\na single A4000 GPU, while other datastores are built within 30 minutes.\n1.2\nEvaluation We measure PPL computed from 512-token sequences on corresponding test sets. PPL\nis computed with our dynamic program derived under the CD-LM distribution in Section 5."}, {"title": "SELF DISTILLATION", "content": "1\nExperimental Setup We focus on practical scenarios where language models operate in environ-\nments with frequent repetition of similar or thematically related queries. In such contexts-common\nin customer support or domain-specific assistants\u2014building datastores for common topics and"}, {"title": "EXPERT DISTILLATION", "content": "1\nFACTUAL KNOWLEDGE INJECTION\nSetup We focus on knowledge-intensive question answering. We use Wikipedia hyperlinks as\nexpert-annotated entities and scrape all hyperlinks from Alan Turing's Wikipedia page, saving these\nentities as chunks in the datastore. We prompt ChatGPT to generate 5000 questions about Alan\nTuring (examples in Appendix G.1) and then have Me answer each question with a maximum\nof 200 tokens. The base models are GPT-2-xl-conversational, LLaMA-2-7b-chat, and Mistral-7B-\nInstruct-v0.2. Our metrics include: Average count (average number of accepted retrieved chunks),\nUnique entities (average number of unique entities in each generated sequence), and Generation\nfluency (evaluated by English experts from Upwork for both Base LM and CD-LM on 200 generated\nsequences). Additionally, we analyze the Entity distributions by comparing the log frequency of\neach entity versus its rank within the generated sequences (See Appendix G.2 for more details).\nWe also employ GPT-40-based LLM-as-a-judge evaluations to assess factual accuracy (detailed in\nAppendix G.8)."}, {"title": "PRIVATE INFORMATION INJECTION", "content": "2\nSetup We consider a senario where a user's personally identifiable information (PII) is stored in\nan external datastore. We create artificial user profiles containing user information, such as phone\nnumber and office address. When building the datastore, we collect common prefixes for each of\nthe information types. We use the common prefixes provided by (Huang et al., 2023) augmented\nwith GPT-4-generated prefixes. After constructing the datastore, we prompt GPT-4 to generate 1000\ndifferent queries asking about users' private information. We then use these queries to prompt GPT-\n2-xl-conversational, LLaMA-2-7b-chat, and Mistral-7B-Instruct-v0.2. To evaluate accuracy, we use\nregular expressions to extract all PII strings from the generated responses and compare them with the\nuser information in our datastore. See Appendices G.5, G.6, and G.7 for the user profile, example\ncommon prefixes, and example queries and generated responses.\nWe test three configurations: Base LM: The LM is prompted with questions about PII, but it does not\nhave any prior knowledge of the PII. Base LM + ICL (In-Context Learning): All PII is appended\nto the beginning of the prompt, and then the LM is asked to answer a question regarding the PII.\nECD-LM: The base LM is used, but it retrieves information only from the PII datastore."}, {"title": "CONCLUSION", "content": "We propose chunk-distilled language modeling (CD-LM) for adaptive and efficient language model-\ning and generation. Instead of generating a single token at a time, it integrates contiguous text chunk\ngenerations through fine-grained retrieval into any pre-trained LM, and augments the LM distribu-\ntions with flexible knowledge injection from either parametric LMs or nonparametric annotations.\nBy skipping token generation steps within chunks, CD-LM also achieves better efficiency at infer-\nence with saved LM forward runs. No training is needed and CD-LM is also lightweight to run by\nhaving sparse databases of chunks. Experiments on diverse applications demonstrate improvements\nwith CD-LM on both inference efficiency and language modeling performance. In this work, we do\nnot focus on optimizing the retrieval process. We leave further engineering efforts to reduce retrieval\noverhead-such as quantization, datastore pruning, or alternative search strategies for future work."}, {"title": "ETHICS STATEMENT", "content": "We follow the Code of Ethics for conducting and presenting our research. We propose an automatic\ninference time generation algorithm to improve language model generations and efficiency. Our ap-\nproach has potentially broad applications when text generation is involved, thus we share the general\nethical considerations of language modeling with deep learning such as fairness, bias, misinforma-\ntion, factual errors, among others. We conduct one simple human evaluation to measure our model's\ngeneration quality compared with base LM's generation. The human evaluators are presented with\nfull disclosure of our research and are aware of potential impacts. All other experiments are on\npublically available data and models, and no private or sensitive information is collected and used.\nNo harmful artifacts were created as a direct indication of our research."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We are committed to promoting transparency and ensuring reproducibility of our work by the re-\nsearch community, towards which we have made considerable efforts to provide all the details in our\nstudies. Our method involves probabilistic modeling of chunk interleaved generation and empirical\nevaluations in various settings. For mathematical formulation and theoretic derivations of practical\nsequence probability computation algorithms under CD-LM, we provide full details in Section 3,\nSection 4, Section 5, and also Appendix A for step-wise inspections and Appendix B for simple\nexamples of probability computation illustrations. For experimental studies, we provide compre-\\hensive documentation, including all data processing, model configurations, evaluation metrics, and\nruntimes resources used in our experiments. All the data and models we used are publicly avail-\nable, as noted in Section 6 in both the main text and various footnotes. Computational resources\nand runtime are also documented such as in Section 6.1. Full data examples, experimental setups,\nand human evaluation questionnaires are detailed in Appendix D for general experimental details,\nAppendix E for KCD-LM, Appendix F for SCD-LM, and Appendix G for ECD-LM. Additional\nexperimental results with full ablations are also presented in detailed tables and figures in corre-\nsponding appendix sections. Our method requires no training at all, thus no optimization parameters\nare involved. There are only two hyper-parameters that affects our inference algorithm: the chunk\nextraction threshold y and the equivalent chunk retrieval threshold \u03b7. We describe their selection\nand present full ablation of their effect in Section 6 and corresponding appendices, such as in Fig-\nure 5, Figure 6, Figure 10, and Figure 11. Text generation samples with varying \u03b7 are also shown in\nTable 7, among many others in Appendix.\nAll code and data we produced during the research will be made publicly available."}]}