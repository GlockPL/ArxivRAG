{"title": "TOWARDS GENERAL-PURPOSE MODEL-FREE REINFORCEMENT LEARNING", "authors": ["Scott Fujimoto", "Pierluca D'Oro", "Amy Zhang", "Yuandong Tian", "Michael Rabbat"], "abstract": "Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive general results across benchmarks but come at the cost of increased complexity and slow run times, limiting their broader applicability. In this paper, we attempt to find a unifying model-free deep RL algorithm that can address a diverse class of domains and problem settings. To achieve this, we leverage model-based representations that approximately linearize the value function, taking advantage of the denser task objectives used by model-based RL while avoiding the costs associated with planning or simulated trajectories. We evaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a single set of hyperparameters and show a competitive performance against domain-specific and general baselines, providing a concrete step towards building general-purpose model-free deep RL algorithms.", "sections": [{"title": "1 INTRODUCTION", "content": "The conceptual premise of RL is inherently general-purpose\u2014an RL agent can learn optimal behavior with only two basic elements: a well-defined objective and data describing its interactions with"}, {"title": "2 RELATED WORK", "content": "General-purpose RL. Although many traditional RL methods are general-purpose in principle, practical constraints often force assumptions about the task domain. For example, algorithms like Q-learning and SARSA (Watkins, 1989; Rummery & Niranjan, 1994) can be conceptually extended to continuous spaces, but are typically implemented using discrete lookup tables. In practice, early examples of general decision-making approaches can be found in on-policy methods with function approximation. For instance, both evolutionary algorithms (Rechenberg, 1978; Back, 1996; Rubinstein, 1997; Salimans et al., 2017) and policy gradient methods (Williams, 1992; Sutton et al., 1999;"}, {"title": "3 BACKGROUND", "content": "Reinforcement learning (RL) problems are described by a Markov Decision Process (MDP) (Bellman, 1957), which we define by a tuple (S, A, p, R, \u03b3) of state space S, action space A, dynamics function p, reward function R and discount factor \u03b3. Value-based RL methods learn a value function Q(s,a) := \u0395\u03c0[\u2211t=ort|so = s, ao = a] that models the expected discounted sum of rewards rt ~ R(st, at) by following a policy \u03c0which maps states s to actions a.\nThe true value function Q\u2122 is estimated by an approximate value function Qe. We use subscripts to indicate the network parameters \u03b8. Target networks, which are used to introduce stationarity in prediction targets, have parameters denoted by an apostrophe, e.g., Qer. These parameters are periodically synchronized with the current network parameters (\u03b8' \u2190 \u03b8)."}, {"title": "4 MODEL-BASED REPRESENTATIONS FOR Q-LEARNING", "content": "This section presents the MR.Q algorithm (Model-based Representations for Q-learning), a model-free RL algorithm that learns an approximately linear representation of the value function through"}, {"title": "4.1 THEORETICAL MOTIVATION", "content": "Consider a linear decomposition of the value function, where the value function Q(s, a) is represented by features zsa and linear weights w:\n$Q(s, a) = z_{sa}^T w$.\nOur primary objective is to learn features zsa that share an approximately linear relationship with the true value function Q\u201d. However, since this relationship is only approximate, we use these features as input to a non-linear function Q(zsa), rather than relying solely on linear function approximation.\nWe start by exploring how to find features that can linearly represent the true value function. Given a dataset D of tuples (s, a, r, s', a'), we consider two possible approaches for learning a value function Q: A model-free update based on semi-gradient TD (Sutton, 1988; Sutton & Barto, 1998):\n$w \u2190 w - \u03b1 \\mathbb{E}_D[\u2207_w (z_{sa}^T w - r + \u03b3z_{s'a'}^T w)^2]$.\nA model-based approach to learn wmb, based on rolling out estimates of the dynamics and reward:\n$w_{mb} := \\argmin \\sum_t w_p^t w_r^t$.\n$w_r := \\argmin \\mathbb{E}_D [(z_{sa}^T w - r)^2]$,\n$w_p := \\argmin \\mathbb{E}_D [(z_{sa}^T w - z_{s'a'})^2]$.\nClosely following Parr et al. (2008) and Song et al. (2016), we can show that these approaches converge to the same solution (proofs for this section can be found in Appendix A).\nTheorem 1. The fixed point of the model-free approach (Equation 4) and the solution of the model-based approach (Equation 5) are the same.\nFrom the insight of Theorem 1, we can connect the value error VE, the difference between an approximate value function Q and the true value function Q\u2122,\n$VE(s, a) := Q(s,a) \u2013 Q^* (s, a)$\nto the accuracy of reward and dynamics components of the estimated model (Theorem 2).\nTheorem 2. The value error of the solution described by Theorem 1 is bounded by the accuracy of the estimated dynamics and reward:\n$|VE(s, a)| \u2264 \\frac{1}{1- \u03b3} \\max_{(s,a)\u2208S\u00d7A} [|z_{sa}^T w_r \u2013 \\mathbb{E}_{s,a}[r]| + \\max|w_i|\\sum |z_{s,a}^T w_p \u2013 \\mathbb{E}_{s,a\u2032,s,a\u2032}[z_{s\u2032a\u2032}|] |$."}, {"title": "4.2 ALGORITHM", "content": "We now present the details of MR.Q (Model-based Representations for Q-learning). Building on the insights from the previous section, our key idea is to learn a state-action embedding zsa that is approximately linear with the true value function Q\u2122. To account for approximation errors, these features are used with non-linear function approximation to determine the value.\nThe state embedding vector zs is obtained as an intermediate component by training end-to-end with the state-action encoder. MR.Q handles different input modalities by swapping the architecture of the state encoder. Since zs is a vector, the remaining networks are independent of the observation space and use feedforward networks.\nGiven the transition (s, a, r, d, s') from the replay buffer:"}, {"title": "4.2.1 ENCODER", "content": "The encoder loss is based on unrolling the dynamics of the learned model over a short horizon. Given a subsequence of an episode (so, ao, r1, d1, 81, ..., rHEnc, dHEnc, sHEnc), the model is unrolled by encoding the initial state so, then by repeatedly applying the state-action encoder gw and linear MDP predictor m:\n$z_t, r_t, d_t := g_w (z_{t\u22121}, a_{t-1})m$, where $z^{0} := f_w(s_0)$.\nThe final loss is summed over the unrolled model and balanced by corresponding hyperparameters:\n$\\mathbb{L}_{Encoder}(f, g, m) := \\sum_{t=1}^{H_{enc}} \\lambda_{Reward} \\mathbb{L}_{Reward} (r_t) +  \\lambda_{Dynamics}  \\mathbb{L}_{Dynamics} (z_t) + \\lambda_{Terminal} \\mathbb{L}_{Terminal} (d_t)$."}, {"title": "4.2.2 VALUE FUNCTION", "content": "Value learning is primarily based on TD3 (Fujimoto et al., 2018). Specifically, we train two value functions and take the minimum output between their respective target networks to determine the value target. Similar to TD3, the target action is determined by the target policy \u03c0\u03c6', perturbed by small amount of clipped Gaussian noise:\n$\\{argmax_{a'} for discrete A, \\atop clip(a',-1,1) for continuous A,\\\\$, where $a' = \u03c0_{\u03c6'}(s') + clip(\u03f5, -c,c), \u03f5~N(0,\u03c3\u00b2)$.\nDiscrete actions are represented by a one-hot encoding, where the Gaussian noise is added to each dimension. Action noise and the clipping is scaled according the range of the action space.\nWe modify the TD3 loss in a few ways. Firstly, following numerous prior work across benchmarks (Hessel et al., 2018; Barth-Maron et al., 2018; Yarats et al., 2022; Schwarzer et al., 2023), we predict multi-step returns over a horizon HQ. Secondly, we use the Huber loss instead of mean-squared error to eliminate bias from prioritized sampling (Fujimoto et al., 2020). Finally, the target value is normalized according to the average absolute reward in the replay buffer:\n$\\mathbb{L}_{Value}(Q_i) := Huber Q_i, \\frac{1}{H_Q} \\sum_{t=0}^{H_Q-1} \u03b3^t r_t + \u03b3^{H_Q}Q_{\u03b8'}= r' \\min_{Q_{\u03b8'}} Q_{\u03b8'} (z_{s_{H_Q}a_{H_Q}})$.\nThe value r' captures the target average absolute reward, which is the scaling factor used to the most recently copied value functions Qe. This value is updated simultaneously with the target networks \u03b8' \u2190 r. Maintaining a consistent reward scale keeps the loss magnitude constant across different benchmarks, thus improving the robustness of a single set of hyperparameters."}, {"title": "4.2.3 POLICY", "content": "For both continuous and discrete action spaces, the policy is updated using the deterministic policy gradient (Silver et al., 2014):\n$\\mathbb{L}_{Policy} (a_\u03c0) := \u22120.5  \\sum_{i={1,2}} Q_i(z_{sa_\u03c0}) + \u03bb_{pre\u2212activ} z_\u03c0 ^2$, where $a_\u03c0 = activ(z_\u03c0)$.\nTo make the loss universal between action spaces, we use Gumbel-Softmax (Jang et al., 2017; Lowe et al., 2017; Cianflone et al., 2019) for discrete actions, and Tanh for continuous actions. A small regularization penalty is added to the square of the pre-activations z before the policy's final activation to help avoid local minima when the reward, and value, is sparse (Bjorck et al., 2021).\nFor exploration, Gaussian noise is added to each dimension of the action (or one-hot encoding of the action). Similar to Equation 18, the resulting action vector is clipped to the range of the action space for continuous actions. For discrete actions, the final action is determined by the argmax operation."}, {"title": "5 EXPERIMENTS", "content": "We evaluate MR.Q on four popular RL benchmarks and 118 environments, and compare its performance against strong domain-specific baselines, general model-based approaches, DreamerV3 (Hafner et al., 2023) and TD-MPC2 (Hansen et al., 2024), and a general model-free algorithm, PPO (Schulman et al., 2017). Rather than establish MR.Q as the state-of-the-art approach in any particular benchmark, our objective is to demonstrate its broad applicability and effectiveness across a diverse set of tasks with a single set of hyperparameters. The baselines use author-suggested default hyperparameters and are fixed across environments. Additional details can be found in Appendix B."}, {"title": "5.1 MAIN RESULTS", "content": "Aggregate learning curves are displayed in Figure 2, with full results displayed in Appendix C.\nGym - Locomotion. This subset of the Gym benchmark (Brockman et al., 2016; Towers et al., 2024) considers 5 locomotion tasks in the MuJoCo simulator (Todorov et al., 2012) with continuous actions and low level states. Agents are trained for 1M time steps without any environment preprocessing. We evaluate against three baselines: TD7 (Fujimoto et al., 2024), a state-of-the-art (or near) approach for this benchmark, as well as TD-MPC2, DreamerV3, and PPO. To aggregate results, we normalize using the performance of TD3 (Fujimoto et al., 2018).\nDMC - Proprioceptive. The DeepMind Control suite (DMC) (Tassa et al., 2018) is a collection of continuous control robotics tasks built on the MuJoCo simulator. These tasks use the proprioceptive states as the observation space, meaning that the input is a vector, and limit the total reward for each episode at 1000, making it easy to aggregate results. We report results on all 28 default tasks that were used by either TD-MPC2 or DreamerV3. Agents are trained for 500k time steps, equivalent to 1M frames in the original environment due to action repeat. For comparison, we evaluate against the same three algorithms as in the Gym benchmark, with TD-MPC2 considered state-of-the-art (or near) for this benchmark. We also include TD7 due to its strong performance in the Gym benchmark.\nDMC - Visual. The visual DMC benchmark includes the same 28 tasks as the proprioceptive benchmark, but uses image-based observations instead. Agents are trained for 500k time steps. For baselines, we include DrQ-v2 (Yarats et al., 2022), given its state-of-the-art (or near) performance in model-free RL, alongside TD-MPC2, DreamerV3, and PPO.\nAtari. The Atari benchmark is built on the Arcade Learning Environment (Bellemare et al., 2013). This benchmark uses pixel observations and discrete actions and includes the 57 games used by DreamerV3. We follow standard preprocessing steps, including sticky actions (Machado et al., 2018) (full details in Appendix B.3). Agents are trained for 2.5M time steps (equivalent to 10M frames), a setting which has been considered by prior work (Sokar et al., 2023). For comparison, we evaluate against three baselines: the model-based approach DreamerV3, as well as model-free approaches, DQN (Mnih et al., 2015), Rainbow (Hessel et al., 2018), and PPO. Results are aggregated by normalizing scores against human performance."}, {"title": "5.2 DESIGN STUDY", "content": "To better understand the impact of certain design choices and hyperparameters, we attempt variations of MR.Q, and report the aggregate results in Table 2."}, {"title": "6 DISCUSSION AND CONCLUSION", "content": "This paper introduces MR.Q, a general model-free deep RL algorithm that achieves strong performance across diverse benchmarks and environments. Drawing inspiration from the theory of model-based representation learning, MR.Q demonstrates that model-free deep RL is a promising avenue for building general-purpose algorithms that achieve high performance across environments, while being simpler and less expensive than model-based alternatives.\nOur work also reveals insights on which design choices matter when building general-purpose model-free deep RL algorithms and how common benchmarks respond to these design choices.\nModel-based and model-free RL. MR.Q integrates model-based objectives with a model-free backbone during training, effectively blurring the boundary between traditional model-based and model-free RL. While MR.Q could be extended to the model-based setting by incorporating planning or simulated trajectories with the state-action encoder, these components can add significant execution time and increase the overall complexity and tuning required by a method. Moreover, the performance of MR.Q in these common RL benchmarks demonstrates that these model-based components may be simply unnecessary\u2014suggesting that the representation itself could be the most valuable aspect of model-based learning, even in methods that do use planning. This argument is echoed by DreamerV3 and TD-MPC2, which rely on short planning horizons and trajectory generation, while including both value functions and traditional model-free policy updates. As such, it may be necessary to examine more complex settings, to reliably see a benefit from model-based search or planning, e.g., (Silver et al., 2016).\nUniversality of RL benchmarks. Our results demonstrate that there is a striking lack of positive transfer between benchmarks. For example, despite the similarities in tasks and the same underlying MuJoCo simulator, the top performers in Gym and DMC fail to replicate their success on the opposing benchmark. Similarly, although DreamerV3 excels at Atari, these performance benefits do not translate to continuous control environments, underperforming TD3 in Gym and outright failing to learn the Dog and Humanoid tasks in DMC (see Appendix C). These findings show the limitations of single-benchmark evaluations, indicating that success on one benchmark may not translate easily to others, and highlights the need for more comprehensive benchmarks.\nLimitations. MR.Q is only the first step towards a new generation of general-purpose model-free deep RL algorithms. Many challenges remains for a fully general algorithm. In particular, MR.Q is not equipped to handle settings such as hard exploration tasks or non-Markovian environments. Another limitation is our evaluation only considers standard RL benchmarks. Although this allows direct comparison with other methods, established algorithms such as PPO have demonstrated their effectiveness in highly unique settings, such as team video games (Berner et al., 2019), drone racing (Kaufmann et al., 2023), and large language models (Achiam et al., 2023; Touvron et al., 2023). To demonstrate similar versatility, new algorithms must undergo the same rigorous testing across a range of tasks that is beyond the scope of any single study.\nAs the community continues to push the boundaries of what is possible with deep RL, we believe that building simpler general-purpose algorithms has the potential to make this technology more accessible to a wider audience, ultimately enabling users to train agents with ease. Perhaps one day\u2014with just the click of a button."}, {"title": "A PROOFS", "content": "Theorem 1. The fixed point of the model-free approach (Equation 4) and the solution of the model-based approach (Equation 5) are the same.\nProof. Let Z be a matrix containing state-action embeddings zsa for each state-action pair (s, a) \u2208 S \u00d7 A. Let Z' be the corresponding matrix of next state-action embeddings zs'a'. Let R be the vector of the corresponding rewards r(s,a).\nThe linear semi-gradient TD update:\n$w_{t+1} := w_t \u2212 \u03b1Z^T (Zw_t \u2212 (R+ \u03b3Z'w_t))$\n$= w_t - \u03b1Z^T Zw_t + \u03b1Z^T R+ \u03b1\u03b3Z^T Z'w_t$\n$= (I \u2212 \u03b1(Z^T Z \u2013 \u03b3Z^T Z'))w_t + \u03b1Z^T R$\n$= (I \u2212 \u03b1A)w_t + \u03b1B,$\nwhere A := ZTZ \u2013 YZ\u2122 Z' and B := Z\u2122 R.\nThe fixed point of the system:\nwmf = (I\u2212 \u03b1A)wmf + \u03b1B\nwmf - (I \u2013 A)wmf = \u03b1B\nAwmf = \u03b1B\nwmf = A-1B."}, {"title": "B.1 HYPERPARAMETERS", "content": ""}, {"title": "B.2 NETWORK ARCHITECTURE", "content": "This section describes the networks used in our method using PyTorch code blocks (Paszke et al., 2019). The state encoder and state-action encoder are described as separate networks for clarity but are trained end-to-end as a single network. The value and policy networks are trained independently from the encoders."}, {"title": "B.3 ENVIRONMENTS", "content": "All main experiments were run for 10 seeds (the design study is based on 5 seeds). Evaluations are based on the average performance over 10 episodes, measured every 5k time steps for Gym and DM control and every 100k time steps for Atari.\nGym - Locomotion. For the gym locomotion tasks (Todorov et al., 2012; Brockman et al., 2016; Towers et al., 2024), we choose the five most common environments that appear in prior work (Fujimoto et al., 2018; 2024; Haarnoja et al., 2018; Kuznetsov et al., 2020). We use the -v4 version. No preprocessing is applied. When aggregating scores, we use normalize with the TD3 scores obtained from TD7 (Fujimoto et al., 2024):\n$TD3-Normalized(x) := \\frac{x - random score}{TD3 score - random score}$.\nDM Control Suite. For the DM control suite (Tassa et al., 2018), we choose the 28 default environments that appear either in the evaluation of TD-MPC2 or DreamerV3. We omit any custom environments included by the TD-MPC2 authors. The same subset of tasks are used in the evaluation of proprioceptive and visual control. Like prior work, for both observation spaces, we use an action repeat of 2 (Hansen et al., 2024). For visual control, the state (network input) is composed of the previous 3 observations which are resized to 84 \u00d7 84 pixels in RGB format (Tassa et al., 2018).\nAtari. For the Atari games (Bellemare et al., 2013; Brockman et al., 2016; Towers et al., 2024), we use the 57 games in the Atari-57 benchmark that appears in prior work (Hessel et al., 2018; Schrittwieser et al., 2020; Badia et al., 2020; Hafner et al., 2023). For DQN and Rainbow, two games (Defender and Surround) are missing from the Dopamine framework (Castro et al., 2018) and are omitted. We use the -v5 version. For MR.Q, we use the common preprocessing steps (Mnih et al., 2015; Machado et al., 2018; Castro et al., 2018), where an action repeat of 4 is used and the observations are grayscaled, resized to 84 \u00d7 84 pixels and set to the max between the 3rd and 4th frame. The state (network input) is composed of the previous 4 observations.\nConsider the 16 frame sequence used by a single state, where fi is the ith grayscaled and resized frame and oj is the jth observation set to the max of two frames\n$s_0=max(f_2,f_3)$\n$o_1=max(f_6,f_7)$\n$0_2=max(f_{10},f_{11})$\nWhen aggregating scores, we normalize with Human scores obtained from (Wang et al., 2016):\nHuman-Normalized(x) :=x - random score / Human score - random score"}, {"title": "C COMPLETE MAIN RESULTS", "content": ""}, {"title": "C.1 GYM", "content": ""}, {"title": "C.2 DMC - PROPRIOCEPTIVE", "content": ""}, {"title": "C.3 DMC - VISUAL", "content": ""}, {"title": "C.4 ATARI", "content": ""}, {"title": "D COMPLETE ABLATION RESULTS", "content": "In this section, we show a per-environment breakdown of each variation in the design study in Section 5.2. Each table reports the raw score for each environment. The [bracketed values] represent a 95% bootstrap confidence interval. The aggregate mean, median and interquartile mean (IQM) are computed over the the difference in the normalized score. We use TD3 to normalize for Gym, raw scores divided by 1000 for DMC and human scores to normalize for Atari (see Appendix B.3). Highlighting is used to designate the scale of the difference in normalized score:"}, {"title": "D.1 GYM", "content": ""}, {"title": "D.2 DMC - PROPRIOCEPTIVE", "content": ""}, {"title": "D.3 DMC - VISUAL", "content": ""}, {"title": "D.4 ATARI", "content": ""}]}