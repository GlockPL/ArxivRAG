{"title": "Personalized Federated Collaborative Filtering: A Variational AutoEncoder Approach", "authors": ["Zhiwei Li", "Guodong Long", "Tianyi Zhou", "Jing Jiang", "Chengqi Zhang"], "abstract": "Federated Collaborative Filtering (FedCF) is an emerging field focused on developing a new recommendation framework with preserving privacy in a federated setting. Existing FedCF methods typically combine distributed Collaborative Filtering (CF) algorithms with privacy-preserving mechanisms, and then preserve personalized information into a user embedding vector. However, the user embedding is usually insufficient to preserve the rich information of the fine-grained personalization across heterogeneous clients. This paper proposes a novel personalized FedCF method by preserving users' personalized information into a latent variable and a neural model simultaneously. Specifically, we decompose the modeling of user knowledge into two encoders, each designed to capture shared knowledge and personalized knowledge separately. A personalized gating network is then applied to balance personalization and generalization between the global and local encoders. Moreover, to effectively train the proposed framework, we model the CF problem as a specialized Variational AutoEncoder (VAE) task by integrating user interaction vector reconstruction with missing value prediction. The decoder is trained to reconstruct the implicit feedback from items the user has interacted with, while also predicting items the user might be interested in but has not yet interacted with. Experimental results on benchmark datasets demonstrate that the proposed method outperforms other baseline methods, showcasing superior performance.", "sections": [{"title": "Introduction", "content": "In the digital age, Recommendation Systems have become essential tools for filtering online information and helping users discover products, content, and services that match their preferences (Ko et al. 2022). Collaborative Filtering (CF) is widely recognized for its ability to generate personalized recommendations by analyzing the relationships between users and items based on user interaction data (Shen, Zhou, and Chen 2020). However, with the enforcement of data privacy laws like GDPR (Voigt and Von dem Bussche 2017), safeguarding privacy has become increasingly critical.Traditional CF methods typically require centralizing user data on servers for processing, a practice that is no longer viable in today's privacy-conscious environment.\nTo address this challenge, Federated Collaborative Filtering (FedCF) has emerged, combining the principles of federated learning (FL) and CF (Yang et al. 2020). FedCF enables models to be trained on users' devices, eliminating the need to upload private data to central servers, thus ensuring data privacy while still providing recommendation services (Ammad-Ud-Din et al. 2019). Existing work on FedCF (Chai et al. 2020; Lin et al. 2020b,a) is mostly based on matrix factorization (MF) (Koren, Bell, and Volinsky 2009), which is favored for its computational efficiency and strong interpretability. MF effectively captures the latent relationships between users and items, making it widely used in many practical applications and delivering strong performance in recommendation systems (Mehta and Rana 2017).\nHowever, the fundamental assumption of MF is that the relationships between users and items are linear, which may limit the model's ability to handle complex, non-linear relationships. The success of Neural Collaborative Filtering (NCF) (He et al. 2017) demonstrates the necessarity of building non-linear relationships among users and items. In addition to FedNCF (Perifanis and Efraimidis 2022) which is a federated NCF algorithm, some recent works proposed to preserve the personalization on neural models in federated settings. PFedRec (Zhang et al. 2023a) proposed using personalized item embeddings to capture each user's unique perspective about the relationships among items. FedRAP (Li, Long, and Zhou 2024) decomposes the item embedding to two additive components that preserve shared and personalized knowledge respectively. However, these methods rely on personalized item embedding that may lead to poor performance on model generalization, and maintaining a personalized item embedding is computation consuming.\nMain Contribution. To effectively preserve fine-grained personalization and non-linear user-item relationships, we propose a novel gating dual-encoder structure that transforms the user-item interaction vector into two separate latent subspaces. Specifically, through collaborative training across clients, the global encoder maps the user profile into a universal latent subspace shared among clients. Meanwhile, the local encoder remains on the client side, where it is trained to map the user profile into a user-specific latent subspace. Consequently, the global encoder retains shared knowledge across clients, while the local encoder preserves personalized knowledge. It is worth noting our proposed"}, {"title": "Related Work", "content": "Personalized Federated Learning. Standard federated learning methods, such as FedAvg (McMahan et al. 2017), learn a global model on the server while considering data locality on each device. However, these methods are limited in their effectiveness when dealing with non-IID data. PFL aims to learn personalized models for each device to address this issue (Tan et al. 2022), often necessitating server-based parameter aggregation (Arivazhagan et al. 2019; T Dinh, Tran, and Nguyen 2020; Collins et al. 2021). Several studies (Ammad-Ud-Din et al. 2019; Huang et al. 2020; T Dinh, Tran, and Nguyen 2020) accomplish PFL by introducing various regularization terms between local and global models. Meanwhile, some work focus on personalized model learning by promoting the closeness of local models via variance metrics (Flanagan et al. 2020), or enhancing this by clustering users into groups and selecting representative users for training (Li et al. 2021; Luo, Xiao, and Song 2022; Tan et al. 2023; Yan and Long 2023), instead of random selection. APFL (Deng, Kamani, and Mahdavi 2020) proposes adaptive PFL and derives the generalization bound for the mixture of local and global models, thereby achieving a balance between global collaboration and local personalization.\nFederated Collaborative Filtering. As privacy protection becomes increasingly important, many studies (Heged\u0171s, Danner, and Jelasity 2019; Chai et al. 2020; Zhang and Jiang 2021; Zhang et al. 2024a,c,b) have focused on FedCF. In this paper, we mainly focus on model-based personalized FedCF, which leverages users' historical behavior data, such as ratings, clicks, and purchases, to learn latent factors that represent user and item characteristics (Aggarwal and Aggarwal 2016). To mitigate the impacts caused by client heterogeneity, personalized FedCF has received considerable attention due to its ability to take into account personalized information for each user. Early works (Lin et al. 2020a; Liang, Pan, and Ming 2021; Zhu et al. 2022; Luo et al. 2023; Yuan et al. 2023; Zhang et al. 2023b) primarily focused on modeling user preferences. In contrast, PFedRec (Zhang et al. 2023a) and FedRAP (Li, Long, and Zhou 2024) have implemented dual personalization, which means they personalized both user preferences and item information. However, the nature of most existing work on matrix factorization limits their ability to model complex nonlinear relationships in data.\nVariational Autoencoders. VAEs have become increasingly important in recommendation systems due to their ability to model complex data distributions and handle data sparsity issues (Liang et al. 2024). VAEs learn latent representations of user-item interaction data by mapping high-dimensional data to a low-dimensional latent space, which effectively captures complex patterns in user preferences and item characteristics (Kingma and Welling 2013). This latent space enables efficient encoding of both user and item information, facilitating accurate predictions of future interactions (Shenbin et al. 2020). Recent research has focused on enhancing VAE models to improve recommendation performance, including combining VAEs with Generative Adversarial Networks (Lee, Song, and Moon 2017; Yu et al. 2019; Gao et al. 2020), integrating VAEs with more effective priors (Tomczak and Welling 2018; Klushyn et al. 2019; Shenbin et al. 2020; Tran and Lauw 2024), and enhancing VAEs with contrastive learning techniques (Aneja et al. 2021; Xie et al. 2021; Wang et al. 2022). These strategies have significantly boosted the performance of VAEs in recommendation tasks. HI-VAE (Nazabal et al. 2020) has demonstrated the effectiveness of VAE in density estimation and missing data imputation through experiments on heterogeneous data completion tasks. Mult-VAE (Liang et al. 2018) proves that using multinomial distribution as the likelihood function is more suitable for implicit feedback. FedVAE (Polato 2021) represents the first attempt to extend VAE to a federated setting; however, it aggregates gradients for all parameters on the server, which can lead to the loss of personalized information from individual clients."}, {"title": "Problem Formulation", "content": "Given n users and m items, let U and I be the sets of users and items respectively. We assume of knowing the users' implicit feedback, i.e., $R = [r_1, r_2,\u2026, r_n]^T \\in {0,1}^{n \\times m}$,"}, {"title": "Federated Collaborative Filtering (FedCF)", "content": "In recommendation tasks with implicit feedback, Collaborative Filtering (CF) methods typically rely solely on users' interaction patterns R to capture the relationships between users U and items I, allowing for generating item recommendations without the need for exogenous information about the users or items (Koren, Rendle, and Bell 2021). However, in the context of FL, the task of FedCF is to model the latent relationships between each user u and items I at their client using the observed portions of their interaction data ru locally. The goal is to generate predicted ratings ru while simultaneously ensuring the protection of user privacy. Thus, we have the following for recommendation:\n$\\min\\limits_{\\Phi_u} \\sum\\limits_{u=1}^{n} L_{recon} (\\hat{r}_u, r_u),$\\nwhere $L_{recon}$ measures the reconstruction loss between ru and $\\hat{r}_u$. By optimizing Eq. 1, we ensure that $\\hat{r}_u$ follows the same distribution as $r_u$, i.e., $\\hat{r}_u \\sim p(r_u) (u = 1, 2, . . ., n).$"}, {"title": "A VAE perspective for FedCF.", "content": "From the perspective of VAEs, we first sample a latent variable $z_u \\sim N(0, I_k)$, which is assumed to model the u-th user's decision logic of the preference in recommendation system. It is worth noting that the work (Liang et al. 2018) suggested that multi-nominal distribution might be an appropriate option for recommendation with implicit feedback.\nGiven the sampled variable zu, the user's preference probabilities for items can be generated by a nonlinear function $f_\\theta: \\mathbb{R}^k \\to \\mathbb{R}^m$, parameterized by $\\theta$, applied to $z_u$:\n$\\pi(z_u) = f_\\theta(z_u),$\ns.t. ru is drawn from $r_u \\sim Mult(m_u, \\pi(z_u))$. To measure the correctness of the predicted user preference, we use log-likelihood of ru conditioned on zu defined as follows:\n$\\log p_\\theta(r_u|z_u) = \\sum\\limits_{i=1}^{m} r_{ui} \\log \\pi_i(z_u).$\nWe apply variational inference (Jordan et al. 1999) to approximate the intractable posterior distribution p(zuru) using variational distribution to estimate @ for the function fo:\n$q_{\\Phi_u}(z_u|r_u) = N(\\mu_{\\Phi_u}(r_u), diag\\{\\sigma_{\\Phi_u}(r_u)\\})\\}$.\nGiven k << {n,m}, both of the vectors $\\mu_{\\Phi_u}(r_u)$ and $\\sigma_{\\Phi_u}(r_u)$ in Eq. (4) are k-dimensional outputs of the data-dependent function $g_{\\Phi_u}(r_u) = [\\mu_{\\Phi_u}(r_u), \\sigma_{\\Phi_u}(r_u)] \\in \\mathbb{R}^{2k}$, where the parameter $\\Phi_u$ is used to capture the representation of item features on the u-th client (u = 1, 2, . . ., \u03b7)."}, {"title": "Methodology", "content": "Fig. 1 shows the overall framework of FedDAE. For each client u, FedDAE inputs the interaction vector ru into the global encoder $q_\\varphi$, the local encoder $q_{\\varphi_u}$, and the gating network $h_{\\psi_u}$, separately. By multiplying the outputs of the dual encoders by the weights generated by $h_{\\psi_u}$ based on the user's interaction data and then summing them, Fed-DAE achieves adaptive additive personalization. The resulting output is then passed to the global decoder $p_\\theta$ to reconstruct the user's interaction data $r_u (u = 1, 2, . . ., n)$. During the training phase, the global encoder and global decoder are collaboratively trained across clients, while the local encoder and gating network are trained locally.\nDual Encoders\nWe propose a dual-encoder mechanism to separately preserve shared knowledge across clients and client-specific personalized knowledge. Specifically, the encoder $q_{\\Phi_u}$ in Eq. 4 is implemented through a dual-encoder structure, consisting of a global encoder $q_\\varphi$ and a local encoder $q_{\\varphi_u}$ for each user u. The parameters \u03c6 and $\\varphi_u$ capture the globally shared representation of item features and the personalized representation specific to user u, respectively. This enables FedDAE to achieve personalized item feature representations while maintaining shared information.\nGating Network\nTo effectively combine the global and local representations, we use a gating network $h_{\\psi_u}(r_u) = [w_{u1},w_{u2}] \\in \\mathbb{R}^2$, parameterized by $\\psi_u$, which dynamically assigns weights wu1 and wu2 to the outputs of $g_\\varphi$ and $g_{\\varphi_u}$ based on the data ru.\nLemma 1 (Additivity of Gaussian distributions) Given two independent Gaussian random variables X and Y, distributed as $N(\u03bc_1, \u03c3^2_1)$ and $N (\u03bc_2, \u03c3^2_2)$, respectively. Then Z = w1X + w2Y follows a new Gaussian distribution Z ~ N(w1\u03bc1 + w2\u03bc2, w^2_1\u03c3^2_1 + w^2_2\u03c3^2_2).\nConsidering Lemma 1, since the latent variable zu is drawn from $q_{\\Phi_u}(z_u|r_u)$ according to Eq. (4), the combined $\u03bc_{\\Phi_u}(r_u)$ and $\u03c3_{\\Phi_u}(r_u)$ are then defined as follows:\n$\\mu_{\\Phi_u}(r_u) = w_{u1} \\cdot \\mu_{\\varphi}(r_u) + w_{u2} \\cdot \\mu_{\\varphi_u}(r_u),$\n$\\sigma_{\\Phi_u}(r_u) = w^2_{u1} \\cdot \\sigma^2_{\\varphi}(r_u) + w^2_{u2} \\cdot \\sigma^2_{\\varphi_u}(r_u),$"}, {"title": "Reconstruction and Prediction", "content": "We define the VAE loss for the FedCF problem as follows. By combining Eq. (3) and Eq. (4) to form a VAE, the evidence lower bound (ELBO) on the u-th client is given by:\n$L_p(r_u; \\Phi_u, \\theta) = $\n$\\mathbb{E}_{q_{\\Phi_u}(z_u|r_u)}[\\log p_\\theta(r_u|z_u)] - \\beta\\cdot KL(q_{\\Phi_u}(z_u|r_u)||p_\\theta(z_u)),$\nwhere KL() is the Kullback-Leibler divergence, and \u03b2\u2208 [0,1] is a hyperparameter controlling the strength of regularization, following the B-VAE (Higgins et al. 2017). The first term in Eq. (6) is the reconstruction loss, which equals to $-L_{recon} (\\hat{r}_u, r_u)$, while the second term is for prior matching. By jointly maximizing Eq. (8) of all users, we obtain the reconstructed interactions $\\hat{r}_u$ for each user u (u = 1,2,...,n). However, relying solely on sharing @ is insufficient for effective information sharing across clients, which may result in suboptimal recommendation performance in a federated setting. To address this issue, we propose a novel personalized FedCF method called FedDAE.\nAfter sampling zu from $q_{\\Phi_u}(z_u|r_u)$, it is decoded using the decoder po defined in Eq. (3) to obtain the reconstructed interactions $\\hat{r}_u$. Combining Eq. (6), the objective of FedDAE is to maximize the ELBO to approximateas log p(ru) across all clients (u = 1, 2, . . ., n), which is defined as follows:\n$\\max\\limits_{\\varphi, \\{\\varphi_u\\}, \\{\\psi_u\\}, \\theta} \\sum\\limits_{u=1}^{n} \\alpha_u \\mathbb{E}_{q_{\\Phi_u}(z_u|r_u)} L_p(r_u; \\varphi, \\varphi_u, \\psi_u, \\theta),$\\nHere, au is the weight of the loss for the u-th client, used to balance the contribution of this client, satisfying $\\sum\\limits_{u=1}^{n} \\alpha_u = 1$. Once the objective function in Eq. (7) is optimized, we obtain the final prediction $\\hat{r}_u = \\mu_{\\Phi_u}(r_u)$, which will be used to personalize recommendations for the u-th user, suggesting items they might be interested in."}, {"title": "Algorithm", "content": "To optimize the objective function outlined in Eq. (7), we utilize an alternative optimization algorithm for training the FedDAE method. The overall workflow of this algorithm is summarized in several steps, as demonstrated in the Alg. 1. The process begin by initializing and 6 on the server, and all yu and Yu on their respective clients. For each communication round t, the server randomly selects a subset of clients to participate in the training, denoted as St. It is important to note that, as discussed in (Chai et al. 2020; Li, Long, and Zhou 2024), for privacy protection, no client should participate in training for two consecutive rounds.\nAccording to the line 4 in the Global Procedure, the latent variable zu is sampled from the normal Gaussian distribution using the reparameterization trick. After receiving and 0, the selected clients call the ClientUpdate function to update their local models with the learning rate \u03b7.\nIn the ClientUpdate function, The local model updates are performed over E epochs (line 13). Once the update"}, {"title": "Discussions", "content": "Matrix Factorization\nIn recommendation systems, MF-based CF methods typically decompose the interaction data matrix into an item embedding that preserves shared knowledge and a user embedding that retains personalized knowledge. In our proposed method, we model the data distribution of the interaction matrix and use the weights of the dual encoders to capture both the globally shared knowledge across clients and the personalized knowledge for each user. This allows"}, {"title": "Complexity Analysis", "content": "Given n users and m items, the input dimension of the encoder is m. Assuming each encoder on the client side consists of L fully connected layers with a hidden layer dimension of d >> k, the time complexity of the encoder is O(Lmd), and the space complexity is O(L(m + 1)d). Similarly, assuming the decoder has L' fully connected layers with the same hidden layer dimension d, the time and space complexities of the decoder are O(L'md) and O(L'(m +\n1)d) respectively. The reparameterization involves generating the mean and variance of the latent subspace zu and sampling using a standard normal distribution, with a time complexity of O(k). Additionally, the gating network on each client requires O(2m) time and space complexity. Thus, the overall time complexity is O(n(2L + L')md). Due to each client in FedDAE having an independent gating network, an independent encoder, a globally shared encoder, and a globally shared decoder, the overall space complexity is O(((n + 1)L + L')(m + 1)d)."}, {"title": "Privacy-preservation Enhancement", "content": "The proposed framework adopts a decentralized architecture from the FL scheme, which significantly reduces the risk of privacy leaks by maintaining data locality, which generally performs well in a trusted environment. Similar to most FedCF methods (Chai et al. 2020; Zhang et al. 2023a; Li, Long, and Zhou 2024), our approach only transmits the gradients of global model parameters, $\\nabla_{\\varphi} L_{B}$ and $\\nabla_{\\theta} L_{B}$, without sharing any raw data with third parties. Additionally, if in an untrusted environment, the proposed method can be easily combined with other advanced privacy-preserving techniques, such as differential privacy (Abadi et al. 2016) or randomly cropping gradients, to further strengthen user privacy guarantees. In our experiment, we perform an ablation study to evaluate the effectiveness of privacy protection."}, {"title": "Experiments", "content": "We performed an extensive experimental analysis to assess the performance of FedDAE on four widely utilized datasets: MovieLens-100K (ML-100K), MovieLens-"}, {"title": "Experimental Setting", "content": "We refer to previous work (He et al. 2017; Zhang et al. 2023a; Li, Long, and Zhou 2024), randomly selecting 4 negative samples for each positive sample in the training set, and using the leave-one-out strategy to validate the methods. In this work, we set $h_{\\psi_u}(r_u) = softmax(r_u \\Psi_u)$ and thus $w_{u1} + w_{u2}$ = 1. We perform hyper-parameter tuning for FedDAE and select the learning rate \u03b7 from {10i|i = -8,..., -1}. The Adam optimizer (Kingma and"}, {"title": "Evaluation", "content": "We evaluate the prediction performance in the experiments using two widely used metrics: Hit Rate (HR@K) and Normalized Discounted Cumulative Gain (NDCG@K). These criteria were formally defined in the work (He et al. 2015). In this study, we set K = 20 and repeat all experiments five times to report the mean and standard deviations."}, {"title": "Comparison Analysis", "content": "Table 2 presents the experimental results of all baseline methods and FedDAE on four widely used recommendation datasets, demonstrating that FedDAE outperforms all federated methods and achieves performance close to centralized methods. This is attributed to FedDAE's ability to adaptively and individually model each user's data distribution, retaining shared information about item features while better integrating user-specific item representations. This indicates that FedDAE has learned fine-grained personalized features, effectively leveraging user preferences.\nAdditionally, to study FedDAE's convergence, we evaluated its performance over 100 iterations on all datasets and visualized the HR@20 and NDCG@20 results, as shown in Fig. 2. Insights from Table 1 and Fig. 2 reveal that the sparsity and scale of the datasets significantly affect Fed-DAE's performance. On datasets with lower sparsity and moderate scale, such as ML-100K and ML-1M, FedDAE shows rapid convergence and better results. In contrast, on datasets with higher sparsity, like Video and QB-article, the model's convergence speed and final performance are noticeably slower. This indicates that additional strategies may be needed to improve the model's learning ability when dealing with highly sparse datasets. Furthermore, while the Adam optimizer helps quickly find optimal model parameters and enhances recommendation performance, it also causes more noticeable fluctuations in FedDAE's convergence curves. These fluctuations might be due to the heterogeneity and noise in the datasets. A theoretical convergence analysis of the convergence is provided in our appendix."}, {"title": "Ablation Study", "content": "To investigate the impact of each component on the model's performance, we propose a variant of the FedDAE method, named FedDAEw. It uses a fixed weight ww for each client to weigh the output of the global encoder. Since the weight of the local encoder is complementary to w (i.e., 1 \u2013 w), the local encoder's weight is also fixed. For more ablation studies, please refer to our appendix."}, {"title": "Abalation Study on Adaptive Weights.", "content": "To explore the impact of the gating network output weight on each client in FedDAE, we compared the performance of FedDAEw with different fixed weights (w = 0.25, 0.5, 0.75) and FedDAE on the ML-100K dataset. Table 3 shows that when"}, {"title": "Ablation Study on Item Representation.", "content": "To visually demonstrate the differentiated capability of FedDAE encoders in modeling item features, we selected three local encoders {$\\varphi_{u}\\}_{u=43,178,345}$ from different users and the global encoder $\\varphi$, all learned from the ML-100K dataset. The weights of all layers of each encoder were multiplied and then visualized. Since this study primarily focuses on implicit feedback recommendation, where each item is either interacted with by the user or not, We employe t-SNE (Van der Maaten and Hinton 2008) to map these item embeddings into a two-dimensional space, with the normalized results shown in Fig. 3. In these visualizations, blue represents items not interacted with by the user, while red represents items that have been interacted with. From the first column of Fig. 3, we can see that in the global encoder's modeled item representation, interacted and non-interacted items are mixed together, indicating that qe only models the shared features of items. The second column illustrates that each user's local encoder can divide the item features into"}, {"title": "Ablation Study on Privacy Protection.", "content": "To evaluate the impact of additional privacy protection measures on the recommendation performance of FedDAE and FedDAEw, we introduce noise into the gradients uploaded by users and test these methods on the ML-100K dataset across various noise levels ($\u03ba^2$ = 0,0.2, 0.5, 0.8, 1). Table 4 shows that while performance declines as noise increases, Fed-DAE consistently outperforms other variants at all noise levels, likely due to its adaptive balancing of global and local encoder outputs. FedDAEw=0.5 ranks second, demonstrating strong robustness across different noise levels. In contrast, FedDAEw=0.25 performs worse than both Fed-DAE and FedDAEw=0.5 but better than FedDAEw=0.75. As the weight parameter w increases, FedDAE becomes more reliant on the global encoder, which may explain why FedDAEw=0.75 performs the worst under all noise conditions. These results highlight the importance of balancing the weights of the dual encoders, especially when implementing privacy protections."}, {"title": "Conclusion", "content": "This paper first revisits FedCF from the perspective of VAEs and proposes a novel personalized FedCF method called FedDAE. FedDAE constructs a VAE model with dual encoders and a global decoder for each client, capturing user-specific representations of item features while preserving globally shared information. The outputs of the two encoders are dynamically weighted through a gating network based on user interaction data, achieving adaptive additive personalization. Experimental results on four widely used"}]}