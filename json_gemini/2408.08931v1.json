{"title": "Personalized Federated Collaborative Filtering: A Variational AutoEncoder Approach", "authors": ["Zhiwei Li", "Guodong Long", "Tianyi Zhou", "Jing Jiang", "Chengqi Zhang"], "abstract": "Federated Collaborative Filtering (FedCF) is an emerging field focused on developing a new recommendation framework with preserving privacy in a federated setting. Existing FedCF methods typically combine distributed Collaborative Filtering (CF) algorithms with privacy-preserving mechanisms, and then preserve personalized information into a user embedding vector. However, the user embedding is usually insufficient to preserve the rich information of the fine-grained personalization across heterogeneous clients. This paper proposes a novel personalized FedCF method by preserving users' personalized information into a latent variable and a neural model simultaneously. Specifically, we decompose the modeling of user knowledge into two encoders, each designed to capture shared knowledge and personalized knowledge separately. A personalized gating network is then applied to balance personalization and generalization between the global and local encoders. Moreover, to effectively train the proposed framework, we model the CF problem as a specialized Variational AutoEncoder (VAE) task by integrating user interaction vector reconstruction with missing value prediction. The decoder is trained to reconstruct the implicit feedback from items the user has interacted with, while also predicting items the user might be interested in but has not yet interacted with. Experimental results on benchmark datasets demonstrate that the proposed method outperforms other baseline methods, showcasing superior performance.", "sections": [{"title": "Introduction", "content": "In the digital age, Recommendation Systems have become essential tools for filtering online information and helping users discover products, content, and services that match their preferences (Ko et al. 2022). Collaborative Filtering (CF) is widely recognized for its ability to generate personalized recommendations by analyzing the relationships between users and items based on user interaction data (Shen, Zhou, and Chen 2020). However, with the enforcement of data privacy laws like GDPR (Voigt and Von dem Bussche 2017), safeguarding privacy has become increasingly critical.Traditional CF methods typically require centralizing user data on servers for processing, a practice that is no longer viable in today's privacy-conscious environment.\nTo address this challenge, Federated Collaborative Filtering (FedCF) has emerged, combining the principles of federated learning (FL) and CF (Yang et al. 2020). FedCF enables models to be trained on users' devices, eliminating the need to upload private data to central servers, thus ensuring data privacy while still providing recommendation services (Ammad-Ud-Din et al. 2019). Existing work on FedCF (Chai et al. 2020; Lin et al. 2020b,a) is mostly based on matrix factorization (MF) (Koren, Bell, and Volinsky 2009), which is favored for its computational efficiency and strong interpretability. MF effectively captures the latent relationships between users and items, making it widely used in many practical applications and delivering strong performance in recommendation systems (Mehta and Rana 2017).\nHowever, the fundamental assumption of MF is that the relationships between users and items are linear, which may limit the model's ability to handle complex, non-linear relationships. The success of Neural Collaborative Filtering (NCF) (He et al. 2017) demonstrates the necessarity of building non-linear relationships among users and items. In addition to FedNCF (Perifanis and Efraimidis 2022) which is a federated NCF algorithm, some recent works proposed to preserve the personalization on neural models in federated settings. PFedRec (Zhang et al. 2023a) proposed using personalized item embeddings to capture each user's unique perspective about the relationships among items. FedRAP (Li, Long, and Zhou 2024) decomposes the item embedding to two additive components that preserve shared and personalized knowledge respectively. However, these methods rely on personalized item embedding that may lead to poor performance on model generalization, and maintaining a personalized item embedding is computation consuming.\nMain Contribution. To effectively preserve fine-grained personalization and non-linear user-item relationships, we propose a novel gating dual-encoder structure that transforms the user-item interaction vector into two separate latent subspaces. Specifically, through collaborative training across clients, the global encoder maps the user profile into a universal latent subspace shared among clients. Meanwhile, the local encoder remains on the client side, where it is trained to map the user profile into a user-specific latent subspace. Consequently, the global encoder retains shared knowledge across clients, while the local encoder preserves personalized knowledge. It is worth noting our proposed framework preserves personalized information at three levels. First, the user profile includes user-specific interaction records, allowing the global encoder to transform it into a user-specific vector within a universal latent subspace. Second, the personalized local encoder further transforms the user profile into a unique latent subspace. Third, the gating network adaptively adjusts the importance weights of the two encoders, balancing the performance of our proposed model between personalization and generalization.\nTo effectively train the proposed gating dual encoders, we revisit the FedCF problem from the perspective of Variational Autoencoders (VAEs) (Kingma and Welling 2013) and propose a novel personalized FedCF method, which incorporates a gating dual-encoder VAE, named FedDAE. To enhance the model's generalization capability, FedDAE employs a gating network that generates weights based on user interaction data, dynamically combining the outputs of the two encoders to achieve additive personalization. To train our method FedDAE, a global decoder is attached to the dual encoders, and the objective function of FedDAE is optimized through an designed alternating update process.\nThe paper's main contributions are summarized as below:\n\u2022 This work tackles the challenge in FedCF of effectively preserving complex nonlinear relationships between users and items while also maintaining fine-grained personalization in recommendations.\n\u2022 A novel dual-encoder mechanism is proposed to capture fine-grained personalization in the FedCF tasks;\n\u2022 A gating network is designed to adaptively adjust the importance weights of the two encoders according to user interactions, enhancing the generalization capabilities;\n\u2022 A novel VAE-based learning framework is introduced to effectively train our proposed FedCF method;\n\u2022 Comprehensive experimental analyses are conducted to validate the effectiveness of the proposed method;\n\u2022 Source codes are provided to support the reproducibility."}, {"title": "Related Work", "content": "Personalized Federated Learning. Standard federated learning methods, such as FedAvg (McMahan et al. 2017), learn a global model on the server while considering data locality on each device. However, these methods are limited in their effectiveness when dealing with non-IID data. PFL aims to learn personalized models for each device to address this issue (Tan et al. 2022), often necessitating server-based parameter aggregation (Arivazhagan et al. 2019; T Dinh, Tran, and Nguyen 2020; Collins et al. 2021). Several studies (Ammad-Ud-Din et al. 2019; Huang et al. 2020; T Dinh, Tran, and Nguyen 2020) accomplish PFL by introducing various regularization terms between local and global models. Meanwhile, some work focus on personalized model learning by promoting the closeness of local models via variance metrics (Flanagan et al. 2020), or enhancing this by clustering users into groups and selecting representative users for training (Li et al. 2021; Luo, Xiao, and Song 2022; Tan et al. 2023; Yan and Long 2023), instead of random selection. APFL (Deng, Kamani, and Mahdavi 2020) proposes adaptive PFL and derives the generalization bound for the mixture of local and global models, thereby achieving a balance between global collaboration and local personalization.\nFederated Collaborative Filtering. As privacy protection becomes increasingly important, many studies (Heged\u0171s, Danner, and Jelasity 2019; Chai et al. 2020; Zhang and Jiang 2021; Zhang et al. 2024a,c,b) have focused on FedCF. In this paper, we mainly focus on model-based personalized FedCF, which leverages users' historical behavior data, such as ratings, clicks, and purchases, to learn latent factors that represent user and item characteristics (Aggarwal and Aggarwal 2016). To mitigate the impacts caused by client heterogeneity, personalized FedCF has received considerable attention due to its ability to take into account personalized information for each user. Early works (Lin et al. 2020a; Liang, Pan, and Ming 2021; Zhu et al. 2022; Luo et al. 2023; Yuan et al. 2023; Zhang et al. 2023b) primarily focused on modeling user preferences. In contrast, PFedRec (Zhang et al. 2023a) and FedRAP (Li, Long, and Zhou 2024) have implemented dual personalization, which means they personalized both user preferences and item information. However, the nature of most existing work on matrix factorization limits their ability to model complex nonlinear relationships in data.\nVariational Autoencoders. VAEs have become increasingly important in recommendation systems due to their ability to model complex data distributions and handle data sparsity issues (Liang et al. 2024). VAEs learn latent representations of user-item interaction data by mapping high-dimensional data to a low-dimensional latent space, which effectively captures complex patterns in user preferences and item characteristics (Kingma and Welling 2013). This latent space enables efficient encoding of both user and item information, facilitating accurate predictions of future interactions (Shenbin et al. 2020). Recent research has focused on enhancing VAE models to improve recommendation performance, including combining VAEs with Generative Adversarial Networks (Lee, Song, and Moon 2017; Yu et al. 2019; Gao et al. 2020), integrating VAEs with more effective priors (Tomczak and Welling 2018; Klushyn et al. 2019; Shenbin et al. 2020; Tran and Lauw 2024), and enhancing VAEs with contrastive learning techniques (Aneja et al. 2021; Xie et al. 2021; Wang et al. 2022). These strategies have significantly boosted the performance of VAEs in recommendation tasks. HI-VAE (Nazabal et al. 2020) has demonstrated the effectiveness of VAE in density estimation and missing data imputation through experiments on heterogeneous data completion tasks. Mult-VAE (Liang et al. 2018) proves that using multinomial distribution as the likelihood function is more suitable for implicit feedback. FedVAE (Polato 2021) represents the first attempt to extend VAE to a federated setting; however, it aggregates gradients for all parameters on the server, which can lead to the loss of personalized information from individual clients."}, {"title": "Problem Formulation", "content": "Given n users and m items, let U and I be the sets of users and items respectively. We assume of knowing the users' implicit feedback, i.e., $R = [r_1, r_2,\u2026, r_n]^T\\in {0,1}^{n\\times m}$,"}, {"title": "Federated Collaborative Filtering (FedCF)", "content": "In recommendation tasks with implicit feedback, Collaborative Filtering (CF) methods typically rely solely on users' interaction patterns R to capture the relationships between users U and items I, allowing for generating item recommendations without the need for exogenous information about the users or items (Koren, Rendle, and Bell 2021). However, in the context of FL, the task of FedCF is to model the latent relationships between each user u and items I at their client using the observed portions of their interaction data $r_u$ locally. The goal is to generate predicted ratings $\\hat{r}_u$ while simultaneously ensuring the protection of user privacy. Thus, we have the following for recommendation:\n$\\min \\sum_{u=1}^n L_{recon}(\\hat{r}_u, r_u)$  (1)\nwhere $L_{recon}$ measures the reconstruction loss between $r_u$ and $\\hat{r}_u$. By optimizing Eq. 1, we ensure that $\\hat{r}_u$ follows the same distribution as $r_u$, i.e., $\\hat{r}_u \\sim p(r_u)$ (u = 1, 2, . . ., n)."}, {"title": "A VAE perspective for FedCF.", "content": "From the perspective of VAEs, we first sample a latent variable $z_u \\sim N(0, I_k)$, which is assumed to model the u-th user's decision logic of the preference in recommendation system. It is worth noting that the work (Liang et al. 2018) suggested that multi-nominal distribution might be an appropriate option for recommendation with implicit feedback.\nGiven the sampled variable $z_u$, the user's preference probabilities for items can be generated by a nonlinear function $f_\\theta: \\mathbb{R}^k \\rightarrow \\mathbb{R}^m$, parameterized by $\\theta$, applied to $z_u$:\n$\\pi(z_u) = f_\\theta(z_u)$, (2)\ns.t. $r_u$ is drawn from $r_u \\sim Mult(m_u, \\pi(z_u))$. To measure the correctness of the predicted user preference, we use log-likelihood of $r_u$ conditioned on $z_u$ defined as follows:\n$\\log p_\\theta(r_u|z_u) = \\sum_{i=1}^m r_{ui} \\log \\pi_i(z_u)$. (3)\nWe apply variational inference (Jordan et al. 1999) to approximate the intractable posterior distribution $p(z_u|r_u)$ using variational distribution to estimate $\\theta$ for the function $f_\\theta$:\n$q_{\\Phi_u}(z_u|r_u) = N(\\mu_{\\Phi}(r_u), diag{\\{\\sigma^2_{\\Phi}(r_u)\\}})$. (4)\nGiven $k << \\{n,m\\}$, both of the vectors $\\mu_{\\Phi}(r_u)$ and $\\sigma_{\\Phi}(r_u)$ in Eq. (4) are k-dimensional outputs of the data-dependent function $g_{\\Phi_u}(r_u) = [\\mu_{\\Phi_u}(r_u), \\sigma_{\\Phi_u}(r_u)] \\in \\mathbb{R}^{2k}$, where the parameter $\\Phi_u$ is used to capture the representation of item features on the u-th client (u = 1, 2, . . ., \\eta)."}, {"title": "Methodology", "content": "Fig. 1 shows the overall framework of FedDAE. For each client u, FedDAE inputs the interaction vector $r_u$ into the global encoder $q_{\\phi}$, the local encoder $q_{\\phi_u}$, and the gating network $h_{\\psi_u}$, separately. By multiplying the outputs of the dual encoders by the weights generated by $h_{\\psi_u}$ based on the user's interaction data and then summing them, Fed-DAE achieves adaptive additive personalization. The resulting output is then passed to the global decoder $p_\\theta$ to reconstruct the user's interaction data $r_u$ (u = 1, 2, . . ., n). During the training phase, the global encoder and global decoder are collaboratively trained across clients, while the local encoder and gating network are trained locally.\nDual Encoders\nWe propose a dual-encoder mechanism to separately preserve shared knowledge across clients and client-specific personalized knowledge. Specifically, the encoder $q_{\\Phi}$ in Eq. 4 is implemented through a dual-encoder structure, consisting of a global encoder $q_{\\phi}$ and a local encoder $q_{\\phi_u}$ for each user u. The parameters $\\phi$ and $\\phi_u$ capture the globally shared representation of item features and the personalized representation specific to user u, respectively. This enables FedDAE to achieve personalized item feature representations while maintaining shared information.\nGating Network\nTo effectively combine the global and local representations, we use a gating network $h_{\\psi_u}(r_u) = [w_{u1},w_{u2}] \\in \\mathbb{R}^{2}$, parameterized by $\\psi_u$, which dynamically assigns weights $w_{u1}$ and $w_{u2}$ to the outputs of $g_{\\phi}$ and $g_{\\phi_u}$ based on the data $r_u$.\nLemma 1 (Additivity of Gaussian distributions) Given two independent Gaussian random variables X and Y, distributed as $\\mathcal{N}(\\mu_1, \\sigma^2_1)$ and $\\mathcal{N}(\\mu_2, \\sigma^2_2)$, respectively. Then $Z = w_1X + w_2Y$ follows a new Gaussian distribution $Z \\sim \\mathcal{N}(w_1\\mu_1 + w_2\\mu_2, w^2_1\\sigma^2_1 + w^2_2\\sigma^2_2)$.\nConsidering Lemma 1, since the latent variable $z_u$ is drawn from $q_{\\Phi}(z_u|r_u)$ according to Eq. (4), the combined $\\mu_{\\Phi}(r_u)$ and $\\sigma^2_{\\Phi}(r_u)$ are then defined as follows:\n$\\mu_{\\Phi_u}(r_u) = w_{u1} \\cdot \\mu_{\\phi}(r_u) + w_{u2} \\cdot \\mu_{\\phi_u}(r_u)$,\n$\\sigma_{\\Phi_u}(r_u) = w^2_{u1} \\cdot \\sigma^2_{\\phi}(r_u) + w^2_{u2} \\cdot \\sigma^2_{\\phi_u}(r_u),$ (5)"}, {"title": "Reconstruction and Prediction", "content": "We define the VAE loss for the FedCF problem as follows. By combining Eq. (3) and Eq. (4) to form a VAE, the evidence lower bound (ELBO) on the u-th client is given by:\n$\\mathcal{L}_p(r_u; \\Phi_u, \\theta) =\\mathbb{E}_{q_{\\Phi_u}(z_u|r_u)}[\\log p_\\theta(r_u|z_u)] - \\beta\\cdot KL(q_{\\Phi_u}(z_u|r_u)||p_\\theta(z_u))$,  (6)\nwhere KL() is the Kullback-Leibler divergence, and $\\beta\\in [0,1]$ is a hyperparameter controlling the strength of regularization, following the $\\beta$-VAE (Higgins et al. 2017). The first term in Eq. (6) is the reconstruction loss, which equals to $-L_{recon}(\\hat{r}_u, r_u)$, while the second term is for prior matching. By jointly maximizing Eq. (8) of all users, we obtain the reconstructed interactions $\\hat{r}_u$ for each user u (u = 1,2,...,n). However, relying solely on sharing $\\theta$ is insufficient for effective information sharing across clients, which may result in suboptimal recommendation performance in a federated setting. To address this issue, we propose a novel personalized FedCF method called FedDAE.\nAfter sampling $z_u$ from $q_{\\Phi_u}(z_u|r_u)$, it is decoded using the decoder $p_\\theta$ defined in Eq. (3) to obtain the reconstructed interactions $\\hat{r}_u$. Combining Eq. (6), the objective of FedDAE is to maximize the ELBO to approximateas $\\log p(r_u)$ across all clients (u = 1, 2, . . ., n), which is defined as follows:\n$\\max_{\\{\\phi, \\{\\phi_u\\}, \\{\\psi_u\\}, \\theta\\}} \\sum_{u=1}^n \\alpha_u \\mathcal{L}_p(r_u; \\Phi_u, \\theta)$, (7)\nHere, $\\alpha_u$ is the weight of the loss for the u-th client, used to balance the contribution of this client, satisfying $\\sum_{u=1}^n \\alpha_u = 1$. Once the objective function in Eq. (7) is optimized, we obtain the final prediction $\\hat{r}_u = \\mu_{\\Phi}(r_u)$, which will be used to personalize recommendations for the u-th user, suggesting items they might be interested in."}, {"title": "Algorithm", "content": "To optimize the objective function outlined in Eq. (7), we utilize an alternative optimization algorithm for training the FedDAE method. The overall workflow of this algorithm is summarized in several steps, as demonstrated in the Alg. 1. The process begin by initializing $\\phi$ and $\\theta$ on the server, and all $\\psi_u$ and $\\Psi_u$ on their respective clients. For each communication round t, the server randomly selects a subset of clients to participate in the training, denoted as $S_t$. It is important to note that, as discussed in (Chai et al. 2020; Li, Long, and Zhou 2024), for privacy protection, no client should participate in training for two consecutive rounds. According to the line 4 in the Global Procedure, the latent variable $z_u$ is sampled from the normal Gaussian distribution using the reparameterization trick. After receiving $\\phi$ and $\\theta$, the selected clients call the ClientUpdate function to update their local models with the learning rate $\\eta$.\nIn the ClientUpdate function, The local model updates are performed over E epochs (line 13). Once the update is completed, the accumulated gradients $\\nabla_{\\phi^{(u)}} \\mathcal{L}_p^{(u)}$ and $\\nabla_{\\theta^{(u)}} \\mathcal{L}_p^{(u)}$, and the predicted scores $\\hat{r}_u$ are uploaded to the server for global aggregation. The server then apply the updated $\\phi$ and $\\theta$ to the next training round. After the training phase is completed, FedDAE use the predicted scores R as the recommendation guide. On each client, FedDAE constructs a global encoder to model the global representation of item features and a local encoder to capture the personalized representation of features influenced by user preferences. The outputs of these encoders are weighted and combined through a gating network, achieving adaptive additive personalization in recommendations. Therefore, despite variations in users' interaction data due to their preferences, FedDAE ensures that data within a user adheres to the i.i.d. assumption. Additionally, the user-related local encoder is stored only locally on the client, protecting user privacy."}, {"title": "Discussions", "content": "Matrix Factorization\nIn recommendation systems, MF-based CF methods typically decompose the interaction data matrix into an item embedding that preserves shared knowledge and a user embedding that retains personalized knowledge. In our proposed method, we model the data distribution of the interaction matrix and use the weights of the dual encoders to capture both the globally shared knowledge across clients and the personalized knowledge for each user. This allows our method, FedDAE, to retain more complex information, rather than being limited to the form of user-item vectors."}, {"title": "Complexity Analysis", "content": "Given n users and m items, the input dimension of the encoder is m. Assuming each encoder on the client side consists of L fully connected layers with a hidden layer dimension of d >> k, the time complexity of the encoder is O(Lmd), and the space complexity is O(L(m + 1)d). Similarly, assuming the decoder has L' fully connected layers with the same hidden layer dimension d, the time and space complexities of the decoder are O(L'md) and O(L'(m + 1)d) respectively. The reparameterization involves generating the mean and variance of the latent subspace $z_u$ and sampling using a standard normal distribution, with a time complexity of O(k). Additionally, the gating network on each client requires O(2m) time and space complexity. Thus, the overall time complexity is O(n(2L + L')md). Due to each client in FedDAE having an independent gating network, an independent encoder, a globally shared encoder, and a globally shared decoder, the overall space complexity is $O(((n + 1)L + L')(m + 1)d)."}, {"title": "Privacy-preservation Enhancement", "content": "The proposed framework adopts a decentralized architecture from the FL scheme, which significantly reduces the risk of privacy leaks by maintaining data locality, which generally performs well in a trusted environment. Similar to most FedCF methods (Chai et al. 2020; Zhang et al. 2023a; Li, Long, and Zhou 2024), our approach only transmits the gradients of global model parameters, $\\nabla_{\\phi^{(u)}} \\mathcal{L}_p^{(u)}$ and $\\nabla_{\\theta^{(u)}} \\mathcal{L}_p^{(u)}$, without sharing any raw data with third parties. Additionally, if in an untrusted environment, the proposed method can be easily combined with other advanced privacy-preserving techniques, such as differential privacy (Abadi et al. 2016) or randomly cropping gradients, to further strengthen user privacy guarantees. In our experiment, we perform an ablation study to evaluate the effectiveness of privacy protection."}, {"title": "Experiments", "content": "We performed an extensive experimental analysis to assess the performance of FedDAE on four widely utilized datasets: MovieLens-100K (ML-100K), MovieLens-1M (ML-1M), Amazon-Instant-Video (Video), and QB-article. Table 1 provides the statistical details of the datasets employed in this study. Each dataset exhibits a high degree of sparsity, with the percentage of observed ratings compared to the total possible ratings (#Users \u00d7 #Items) exceeding 90%. The first three datasets contain explicit ratings ranging from 1 to 5. Since FedDAE focuses on generating recommendation predictions for data with implicit feedback, any rating above 0 in these datasets is considered a positive interaction by the user and is assigned 1. The QB-article dataset is an implicit feedback dataset that records user click behavior. In each dataset, we only include users who rated at least 10 items. All datasets used are publicly available.\nBaselines\nThe efficacy of FedDAE is comparatively evaluated against several cutting-edge approaches in both centralized and federated environments for validation:\nMult-VAE (Liang et al. 2018): A variational autoencoder model that uses a multinomial distribution as the likelihood function to generate latent representations of users, suitable for collaborative filtering tasks with implicit feedback data in recommendation systems.\nRecVAE (Shenbin et al. 2020): significantly improves the performance of Mult-VAE by introducing a composite prior distribution and an alternating training method,\nLightGCN (He et al. 2020): enhances recommendation performance to a new level while retaining the advantages of GCN models by simplifying the design of graph convolutional networks (GCNs) and removing nonlinear transformations and weight matrices,\nFedVAE (Polato 2021): extends Mult-VAE to the FL framework, achieving collaborative filtering by only transmitting model gradients between clients and the server without exposing the raw user data, ensuring user privacy and security.\nPFedRec (Zhang et al. 2023a): achieves the personalization of user information and item features by introducing a dual personalization in the FL framework, enabling the recommendation system to adapt to the needs of different users.\nFedRAP (Li, Long, and Zhou 2024): enhances the performance of recommendation systems by applying dual personalization of user and item information in federated learning while retaining the shared parts of item information, particularly excelling in handling heterogeneous data.\nThe implementations of these baselines are publicly available in their respective papers. Additionally, we implement a central version of FedDAE, named CentDAE, to explore the performance of a VAE with dual encoders.\nExperimental Setting\nWe refer to previous work (He et al. 2017; Zhang et al. 2023a; Li, Long, and Zhou 2024), randomly selecting 4 negative samples for each positive sample in the training set, and using the leave-one-out strategy to validate the methods. In this work, we set $h_{\\psi_u}(r_u) = softmax(r_u \\Psi)$ and thus $w_{u1} + w_{u2} = 1$. We perform hyper-parameter tuning for FedDAE and select the learning rate $\\eta$ from ${10^i | i = -8,..., -1\\}$. The Adam optimizer (Kingma and Ba 2014) is applied to update FedDAE's parameters. Referring to previous work (Liang et al. 2018; Polato 2021), we gradually anneal $\\beta = 1$. To ensure fairness, we fix the latent embedding dimension to 256 and the training batch size to 2048 for all methods. The number of layers for methods with hidden layers is fixed at 3. We set $\\alpha_u = 1/n$ for the average schemes of all userd federated methods. Following Mult-VAE, FedDAE also employs a dropout (Srivastava et al. 2014) layer before inputting $r_u$ into the encoders to reduce the risk of over-fitting and uses the reparametrization trick (Kingma and Welling 2013) to eliminate the stochasticity caused by sampling $z_u$, which allows the model to be optimized using gradient descent. In the main experiments, all clients participated in the training for FedVAE, PFedRec, FedRAP, and FedDAE, and no additional privacy protection measures are applied. In addition, the number of local epoch is set to 10 for these federated methods, and we do not use any pre-training strategies for any methods.\nEvaluation\nWe evaluate the prediction performance in the experiments using two widely used metrics: Hit Rate (HR@K) and Normalized Discounted Cumulative Gain (NDCG@K). These criteria were formally defined in the work (He et al. 2015). In this study, we set K = 20 and repeat all experiments five times to report the mean and standard deviations."}, {"title": "Comparison Analysis", "content": "Table 2 presents the experimental results of all baseline methods and FedDAE on four widely used recommendation datasets, demonstrating that FedDAE outperforms all federated methods and achieves performance close to centralized methods. This is attributed to FedDAE's ability to adaptively and individually model each user's data distribution, retaining shared information about item features while better integrating user-specific item representations. This indicates that FedDAE has learned fine-grained personalized features, effectively leveraging user preferences.\nAdditionally, to study FedDAE's convergence, we evaluated its performance over 100 iterations on all datasets and visualized the HR@20 and NDCG@20 results, as shown in Fig. 2. Insights from Table 1 and Fig. 2 reveal that the sparsity and scale of the datasets significantly affect FedDAE's performance. On datasets with lower sparsity and moderate scale, such as ML-100K and ML-1M, FedDAE shows rapid convergence and better results. In contrast, on datasets with higher sparsity, like Video and QB-article, the model's convergence speed and final performance are noticeably slower. This indicates that additional strategies may be needed to improve the model's learning ability when dealing with highly sparse datasets. Furthermore, while the Adam optimizer helps quickly find optimal model parameters and enhances recommendation performance, it also causes more noticeable fluctuations in FedDAE's convergence curves. These fluctuations might be due to the heterogeneity and noise in the datasets. A theoretical convergence analysis of the convergence is provided in our appendix."}, {"title": "Ablation Study", "content": "To investigate the impact of each component on the model's performance, we propose a variant of the FedDAE method, named FedDAEw. It uses a fixed weight ww for each client to weigh the output of the global encoder. Since the weight of the local encoder is complementary to w (i.e., 1 \u2013 w), the local encoder's weight is also fixed. For more ablation studies, please refer to our appendix."}, {"title": "Abalation Study on Adaptive Weights.", "content": "To explore the impact of the gating network output weight on each client in FedDAE, we compared the performance of FedDAEw with different fixed weights (w = 0.25, 0.5, 0.75) and FedDAE on the ML-100K dataset. Table 3 shows that when the weight w = 0.5, FedDAEw performs most similarly to FedDAE. However, as the weight increases to 0.75, performance significantly decreases, indicating that personalized information plays a crucial role in FedDAE. The lack of personalized information results in a greater decline in recommendation performance compared to the lack of shared information. Overall, this ablation study demonstrates the effectiveness of the gating network in FedDAE."}, {"title": "Ablation Study on Item Representation.", "content": "To visually demonstrate the differentiated capability of FedDAE encoders in modeling item features, we selected three local encoders $\\{q_{\\phi_u}\\}_{u=43,178,345}$ from different users and the global encoder $q_{\\phi}$, all learned from the ML-100K dataset. The weights of all layers of each encoder were multiplied and then visualized. Since this study primarily focuses on implicit feedback recommendation, where each item is either interacted with by the user or not, We employe t-SNE (Van der Maaten and Hinton 2008) to map these item embeddings into a two-dimensional space, with the normalized results shown in Fig. 3. In these visualizations, blue represents items not interacted with by the user, while red represents items that have been interacted with. From the first column of Fig. 3, we can see that in the global encoder's modeled item representation, interacted and non-interacted items are mixed together, indicating that $q_\\phi$ only models the shared features of items. The second column illustrates that each user's local encoder can divide the item features into two distinct clusters, proving that the local encoder $q_{\\phi_u}$ can learn user-specific personalized features. The third column shows the average combination of global and local representations, but its boundaries are not as distinct compared to the weighted combination of FedDAE shown in the fourth column. The weighted combination, based on the output of the gating network $h_{\\psi}$ from user interaction data $r_u$, more clearly differentiates the two clusters. This demonstrates that FedDAE's adaptive additive personalization can more effectively represent item features, supporting its superior performance in personalized recommendation."}, {"title": "Abalation Study on Privacy Protection.", "content": "To evaluate the impact of additional privacy protection measures on the recommendation performance of FedDAE and FedDAEw, we introduce noise into the gradients uploaded by users and test these methods on the ML-100K dataset across various noise levels ($\\kappa^2$ = 0,0.2, 0.5, 0.8, 1). Table 4 shows that while performance declines as noise increases, FedDAE consistently outperforms other variants at all noise levels, likely due to its adaptive balancing of global and local encoder outputs. FedDAEw=0.5 ranks second, demonstrating strong robustness across different noise levels. In contrast, FedDAEw=0.25 performs worse than both FedDAE and FedDAEw=0.5 but better than FedDAEw=0.75. As the weight parameter w increases, FedDAE becomes more reliant on the global encoder, which may explain why FedDAEw=0.75 performs the worst under all noise conditions. These results highlight the importance of balancing the weights of the dual encoders, especially when"}]}