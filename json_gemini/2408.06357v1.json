{"title": "Algorithm Research of ELMo Word Embedding and Deep Learning Multimodal Transformer in Image Description", "authors": ["Xiaohan Cheng", "Taiyuan Mei", "Yun Zi", "Qi Wang", "Zijun Gao", "Haowei Yang"], "abstract": "Abstract-Zero sample learning is an effective method for data deficiency. The existing embedded zero sample learning methods only use the known classes to construct the embedded space, so there is an overfitting of the known classes in the testing process. This project uses category semantic similarity measures to classify multiple tags. This enables it to incorporate unknown classes that have the same meaning as currently known classes into the vector space when it is built. At the same time, most of the existing zero sample learning algorithms directly use the depth features of medical images as input, and the feature extraction process does not consider semantic information. This project intends to take ELMo-MCT as the main task and obtain multiple visual features related to the original image through self-attention mechanism. In this paper, a large number of experiments are carried out on three zero-shot learning reference datasets, and the best harmonic average accuracy is obtained compared with the most advanced algorithms.\n\nKeywords-Sample learning; deep learning; medical image recognition; attention mechanism; ELMo-MCT", "sections": [{"title": "I. INTRODUCTION", "content": "Medical image recognition technology, based on deep learning, has achieved significant breakthroughs[1-3]. However, traditional medical image classification algorithms require a large number of labeled samples and struggle to identify novel categories [4]. As new categories emerge, distinguishing them necessitates collecting numerous labeled samples and developing a new model, which is time-consuming. Zero-shot learning, which can be categorized into conventional and generalized types, offers a solution [5]. In conventional zero-shot learning, only unseen categories are present during testing, while generalized zero-shot learning, a more practical and challenging approach, involves both known and unseen categories. Current research primarily explores prototype-based and embedding-based zero-shot learning methods.\n\nThe method maps the features and quasi-semantic information of medical images to a certain vector space to achieve alignment between the two modes [6]. During the experiment, the type of sample to be detected is determined by the nearest neighbor lookup between the data to be detected and the aligned space. However, under the generalized zero-shot condition, the mapping relationship is only established for the known categories, which is likely to lead to the overfitting of the unknown categories and the obvious deviation in the prediction of the unknown categories [7]. Therefore, the core problem of the embedded method is how to make the model can extract the most typical embedded expression from the sample, and have sufficient differentiation to distinguish other types of features.\n\nIn the past, the learning method based on convolutional neural network has occupied absolute advantages in many research directions. However, with its excellent performance in medical image recognition, the current mainstream research framework has shifted from the traditional framework based on convolutional neural network to the framework based on automatic attention network based on Transformer [8-10]. This paper first studies the semantic similarity-based multi-label"}, {"title": "II. MULTI-MODE IMAGE DESCRIPTION CONVERTER", "content": "The feature quantity is input to the encoder for self-attention learning, and the visual attention representation of the image is obtained. The next word is then predicted based on the previous work and the encoder's visual attention information [11]. The network architecture of the multimodal image description converter is shown in Figure 1"}, {"title": "A. Image feature encoder", "content": "Its core task is to extract image characteristics and use attention mechanism to establish attention matrix between image modes, so as to obtain the correlation between images [12-14]. The algorithm mainly includes two aspects: one is the image feature extraction algorithm, the other is the multi-channel transform encoder.\n\n1) Image feature extractor\nIn the part of image feature extraction, the characteristics of an image are expressed [15]. This project intends to adopt the image feature extraction method based on the bottom-up attention mechanism[16], use the existing visual gene sequencing Faster-RCNN to identify the object association region[17-18], and adopt the mean pooling method to obtain the features of each object, express the characteristics of each object as X\u2081, and express the overall image as U[19]. Finally, the obtained image feature U is fed into the neural network, and the image feature matrix U\u00ba is formed by adjusting the image feature dimension to make it consistent with the spatial scale of the encoder.\n\n2) Multi-Channel Converter\nThe converted image feature U\u00ba is then input to a multimode converter, which contains M attention modules {B\u2081, B\u2082,..., B\u2098}. The m attention module B\u2098 receives the output U\u1d50\u207b\u00b9 of the m\u22121 attention module and computes it to get the attention characteristic U\u1d50 in the image mode after the attention weight. Here's the formula:\n\nU\u1d50 = B\u2091(U\u1d50\u207b\u00b9)  (1)\n\nEach B\u2091(U\u1d50\u207b\u00b9) is divided into multiple concerns (MHA) and feedback forward (FFN). The multi-focus attention module consists of a single-focus attention module (Figure 2)."}, {"title": "Fig. 2. Multi-head attention of the attention mechanism", "content": "Input image characteristic information U\u1d50\u207b\u00b9 to a single head attention module with a different parameter as shown in FIG. 2, and obtain a weight matrix C\u1d50\u207b\u00b9 representing the attention matrix between each head's image modes at layer m-1.\n\nC\u1d50\u207b\u00b9 = Attend(W,T,U) = soft max(W\u1d40T U / \u221as)  (2)\n\nC\u1d50\u207b\u00b9 is the property matrix derived from the i height in level m-1, and s is determined by the number of columns in level W,T. Finally, the C\u1d50\u207b\u00b9 matrix generated by the concerns is concatenated in a column.\n\nC\u1d50\u207b\u00b9 = Concat(C\u1d50\u207b\u00b9, C\u1d50\u207b\u00b9,..., C\u1d50\u207b\u00b9)  (3)\n\nThe resulting image is then linearly transformed to obtain image C\u1d50\u207b\u00b9.\n\nC\u1d50\u207b\u00b9 = Norm(U\u1d50 + G\u1d50\u0108\u1d50\u207b\u00b9) (4)\n\nThe network model is a double-layer completely connected structure with Relu as the activation function. Here's the formula:\n\nFFN(C\u1d50) = max(0, G\u2081C\u1d50\u207b\u00b9 + \u03c3\u2081)G\u2082 + \u03c3\u2082 + \u03c3 (5)\n\nU\u1d50 = Norm(C\u1d50\u207b\u00b9 + FFN(C\u1d50\u207b\u00b9)) (6)\n\nG, is the parameter of the fully connected network to be trained. By calculating the attention weight of multiple modes, the attention weight matrix UM in the multi-mode is obtained."}, {"title": "B. Text decoder", "content": "The text is decoded according to the image characteristic matrix after encoder. The specific research contents include: (1) This paper extracts the attention matrix of the correlation between words from the word pattern based on the internal attention theory of pattern; (2) Establish the association between images and text based on inter-pattern attention, obtain the image-oriented attention weight matrix, and finally generate the corresponding feature description.\n\n1) Multi-mode lexical information hidden encoder\nThe primary task of a multimode word steganography encoder is to encode the input word and then construct the corresponding character [20]. The algorithm consists of two aspects (Figure 3 cited in Intelligent Systems with Applications, Volume 18, May 2023, 200221), namely, the standard word embedded encoder and the ELMO word embedded encoder."}, {"title": "Fig. 3. Multimodal word embedding encoder", "content": "The multimodal word embedding encoder first uses the standard word embedding encoder to generate the standard word embedding[21]. The word segmentation operation is carried out, the sentence is divided into a single word, and the unique token corresponding to each word is formed, and then the input sentence is converted into the form of a token, and the completion operation is carried out [22]. The word vector encoding algorithm is then used to encode each word A, and finally B is used to represent a standard feature matrix of a description statement, where C is the dimension of the word embedding operation.\n\nThe multimodal vector encoder generates a standard vector through the standard vector encoder, that is, through segmentation, the sentence is broken down into a single word, and the corresponding symbol is established for each word, and then the sentence is converted into a symbol to complete the filling operation [23]. Then u\u1d50 \u2208 D\u1d49\u1d50\u1d47 is encoded for each word-by-word vector coding, and finally UM is used to express the canonical feature matrix, where emb is the dimension of lexical embedding operation."}, {"title": "The second step is the multimodal word embedding encoder to generate a word embedding feature matrix based on context information according to the ELMo word embedding encoder [24]. In contrast to the standard vocabulary embedding method, the ELMo vocabulary embedding encoder encodes each word in a sentence by associating it with context.", "content": "The text code Uchar of each description statement is obtained through a text coding layer to avoid the occurrence of out-of-set V situation, and then the obtained text code is imported into the bidirectional LSTM to obtain the feature matrix\n\nu\u03c3\u03af \u2208D(a+1)\u00d7G\u00d7emb with text background, where a is the number of layers of LSTM and G is the number of words. In this way, the feature matrix containing context is obtained. Finally, the context-containing statements are input into the mixed layer a+1 and the eigenmatrix is formed by weighting each vector to obtain the ELMo word vector UE.\n\n2) Multimodal Transformer decoder\nThe masked multi-head attention module models the input word embedding matrix U. Generate the top triangle first, and then do the attention model. The remaining matrix of zeros has a mask. Prevents this pattern from using words that follow the current time series. Then, the attention characteristic matrix D\u2098 in the word pattern is obtained by the attention weight of the word vector matrix.\n\nD\u2098 = Norm(U + Mask _MHA(U,U,U)) (7)\n\nThe image-oriented visual attention weight matrix Df is obtained by calculating the attention weight of multimodal data set UM and text data respectively. The detailed calculation method is as follows:\n\nDf = Norm(D\u2098 +MHA(D\u2098,UM,UM)) (8)"}, {"title": "A. Data Set", "content": "This project is based on Microsoft COCO2014 open-source database, and the proposed algorithm is verified by simulation [25]. The method divides the samples into 5000 image validation samples, 5000 image samples and 113287 image samples to train and test the model respectively. BLEU (A1, A2, A3, A4), ROUGE-L (R), and CIDErD (C) are used as the evaluation index and the percentage is obtained. The dataset is administered utilizing the Linked Data methodology [26], which facilitates the integration of diverse data types-a critical aspect in scholarly research. This structured approach simplifies the referencing process, thereby enhancing dataset interoperability. This characteristic proves particularly beneficial in domains such as machine learning and artificial intelligence, where the caliber of data exerts a direct influence on model training and consequently on the precision of the outcomes."}, {"title": "B. Experimental Details", "content": "Each image contains five captions in English, and the text in each caption is converted to lower case and then broken down to form a standard vocabulary about 9, 957 long. For the super parameters in this model, the feature dimension of Bottom-Up is 2048, the feature dimension of the head is 1024, the quantity h is 8, the feature dimension of the head is 128, and the term vector is 1024. Thirty cycles were trained using the cross-loss method, 10 cycles were set here, and the Adam optimizer was used for them with a learning rate of 0.0005 and a momentum of 0.9."}, {"title": "C. Experimental results and analysis", "content": "First, this project intends to take image characteristics and short-term memory (LSTM) based on Bottom-UP attention mechanism as the baseline and compare them with MCT (MCT)[27]. Secondly, this paper will make use of ELMo lexically-embedded ELMO-MCT and compare it with the above two models. The test results are shown in Table 1. Compared with the CIDEr scores of MCT and ELMO-MCT modes, the CIDEr scores of MCT and ELMO-MCT modes increased by 2.8 and 4.9 percentage points respectively."}, {"title": "IV. CONCLUSION", "content": "Encoding it with languages such as Word2Vec and Glove resulted in the loss of a large amount of useful data. For this purpose, the image representation method of multi-mode converter is established, and the accuracy of the model is improved by joint modeling of the attention within and between modes. By introducing the method of combining ELMo and vector, the semantic expression level of the model is further improved. Overall, our findings demonstrate the potential of integrating semantic similarity measures with advanced feature extraction technologies to enhance the capabilities of zero-shot learning in medical image recognition. This strategy not only improves model performance but also reduces the dependency on extensive labeled datasets, offering a viable path forward in the development of more adaptable and efficient medical imaging technologies."}]}