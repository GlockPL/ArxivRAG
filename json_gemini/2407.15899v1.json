{"title": "Spatial-Temporal Cross-View Contrastive Pre-training for Check-in Sequence Representation Learning", "authors": ["Letian Gong", "Huaiyu Wan", "Shengnan Guo*", "Xiucheng Li", "Yan Lin", "Erwen Zheng", "Tianyi Wang", "Zeyu Zhou", "Youfang Lin"], "abstract": "The rapid growth of location-based services (LBS) has yielded massive amounts of data on human mobility. Effectively extracting meaningful representations for user-generated check-in sequences is pivotal for facilitating various downstream services. However, the user-generated check-in data are simultaneously influenced by the surrounding objective circumstances and the user's subjective intention. Specifically, the temporal uncertainty and spatial diversity exhibited in check-in data make it difficult to capture the macroscopic spatial-temporal patterns of users and to understand the semantics of user mobility activities. Furthermore, the distinct characteristics of the temporal and spatial information in check-in sequences call for an effective fusion method to incorporate these two types of information. In this paper, we propose a novel Spatial-Temporal Cross-view Contrastive Representation (STCCR) framework for check-in sequence representation learning. Specifically, STCCR addresses the above challenges by employing self-supervision from \"spatial topic\" and \"temporal intention\" views, facilitating effective fusion of spatial and temporal information at the semantic level. Besides, STCCR leverages contrastive clustering to uncover users' shared spatial topics from diverse mobility activities, while employing angular momentum contrast to mitigate the impact of temporal uncertainty and noise. We extensively evaluate STCCR on three real-world datasets and demonstrate its superior performance across three downstream tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "LOCATION-BASED services (LBS), such as Gowalla, Weeplace, and Yelp, have experienced significant develop- opment over the past decade. These platforms enable users to share and discover location information and surrounding services, resulting in the accumulation of extensive data on human mobility behavior, e.g. check-in sequences at points of interest (POIs). This offers prospects for analyzing and comprehending human mobility patterns for various practical applications, such as predicting the next check-in location or time for personalized recommendations, linking trajectories to users, and detecting abnormal trajectories for safety control purposes etc. Learning accurate and universal representations for check-in sequences is a crucial task in human mobility data mining. However, the existing excellent end-to-end models for check-in sequences modeling, such as those designed for location prediction [1], [2], [3], time prediction [4], and trajectory user link [5], [6], [7], often struggle to learn generalized representations for check-in sequences and fail to comprehensively describe the spatial-temporal patterns of human mobility, since the su- pervision signals of these models usually rely on limited single-type labels. Therefore, the learned representations are task-specific and poorly generalized. To facilitate the generalization ability for the check-in sequence's representa- tions, pre-training check-in sequence representation via self- supervised learning has been widely studied and proven to be an effective way to fully exploit massive unlabeled check- in data to boost the performance of the downstream tasks. Representation learning is always one of the hot re- search topics in deep learning. And recently, contrastive pre-training with self-supervised signals [8] has emerged as the most effective approach for sequence modeling. In particular, some representative works [9], [10], [11] in the spatial-temporal data mining (STDM) domain have proven their effectiveness in learning the representations of check- in sequences. However, the unique spatial and temporal characteristics of check-in sequences raise challenges for these contrastive pre-training based models, meanwhile weakening their ability to capture the macroscopic spatial- temporal mobility patterns and to understand the high-level semantics of user mobility activities. Specifically, we identify three key challenges: (1) Temporal uncertainty: Understanding the temporal intention of users' ability from the check-in sequence with uncertain temporal information is challenging. As shown in Fig. 1, based on the user's historical sequence and the user's historical spatial-temporal behavior patterns, the user is most likely to go for dinner next, with the strongest temporal intention being at 17:00. However, the exact time"}, {"title": "2 RELATED WORK", "content": "2.1 Mobility Data Mining\nLocation-based services have given rise to a new and promising research topic known as mobility data mining, which has led to the emergence of three significant tasks that contribute to enhancing the quality of services: next location prediction (LP), next time prediction (TP), and trajectory user link (TUL). Recent studies have confirmed that deep learning techniques, specifically recurrent neural networks (RNNs) and attention mechanisms, are highly effective in capturing sequential and periodic patterns of human mo- bility. By combining deep learning techniques, researchers have made significant advancements in capturing both the sequential and periodic patterns of human mobility. The core of these models is the modeling of check-in sequences, which leads to improved accuracy in location prediction and trajectory analysis.\nLP aims to anticipate a user's future location based on their historical movement. Several notable models have emerged as prominent approaches in LP. DeepMove [1] leverages RNNs and attention mechanisms to capture the spatial-temporal intentions in users' location data and pre- dict their next destination. STAN [14] introduces a spatial- temporal attention network that incorporates spatial and temporal contexts for accurate prediction. LSTPM [2] fo- cuses on long and short-term patterns in user trajectory us- ing an attention-based LSTM [15] model. SERM [3] utilizes an encoder-decoder architecture with a spatial-temporal residual network to capture user preferences and predict future locations. PLSPL [16] trains two LSTM models for location and category based sequence to capture the user's preference. LightMove [17] designs neural ordinary differ- ential equations to enhance robustness against sparse or incorrect inputs. HMT-GRN [18] alleviates the data spar- sity problem by learning different User-Region matrices of lower sparsities in a multitask setting. Graph-Flashback [19] constructs a spatial-temporal knowledge graph to enhance the representation of POIs. GETNext [20] introduces a user- agnostic global trajectory flow map as a means to leverage the abundant collaborative signals.\nTUL is a significant task that focuses on establishing connections between different trajectories, facilitating the analysis of user movement patterns, and uncovering valu- able insights about their behavior. Notable models have been specifically developed to address the challenge of predicting trajectory links. TULER [6] takes advantage of advanced algorithms to establish links between trajecto- ries, allowing for a comprehensive understanding of user movement patterns. DeepTUL [5] utilizes deep learning techniques to extract representations from trajectory data and facilitate the prediction of trajectory links. S2TUL [21] utilizes graph convolutional networks and sequential neu- ral networks to capture trajectory relationships and intra-trajectory information. GNNTUL [22] employs graph neural networks for human mobility and associates the traces with users on social networks.\nTP focuses on estimating the time at which a user is likely to visit their next location. To accomplish this, it is common practice to use intensity functions to represent the rate or density of event occurrences, various models have been developed to model the intensity function and make accurate time predictions effectively. Modeling the intensity function using RNNs or attention mechanisms is a common approach for predicting the occurrence of events. RMTPP [23] utilizes RNNs to model the intensity function. SAHP [24] combines the Hawkes process with self-attention mechanisms to capture the temporal depen- dencies and spatial influences in event sequences. THP [25] combines the Hawkes process with transformer-based ar- chitectures to capture temporal dependencies in event se- quences. NSTPP [26] utilizes neural ODEs to model discrete events in continuous time and space, enabling the learning of complex distributions in spatial and temporal domains. IMTPP [27] models the generative processes of observed and missing events and utilizes unsupervised modeling and inference methods for time prediction. DSTPP [28] purposes a novel parameterization framework that uses diffusion models to learn complex joint distributions.\nIt is important to note that these end-to-end supervised methods designed for specific tasks are not universal. These models do not have a good grasp of the macroscopic se- mantics of check-in sequences. Thus, learning the universal representation of check-in sequences to improve the model's ability and understand high-level semantics is critical.\n2.2 Pretraining and Contrastive Learning\nThe essence of mobility mining tasks lies in learning the representation of check-in sequences. Numerous studies have demonstrated the effectiveness of employing the pre- training paradigm to achieve check-in sequence representa- tion learning. For instance, TULVAE [7] and MoveSim [29] utilize Variational Auto-Encoder and Generative Adversar- ial Network, respectively, to capture the movement patterns of check-in sequences through pre-training. TALE [30] pro- poses a pre-training representation scheme for trajectory point location embedding that incorporates temporal se- mantics, which effectively improves the performance of next location prediction and location traffic prediction. CTLE [31] proposes a location pre-training representation model that incorporates domain features, which dynamically generates feature representations of the domain environment of the target location so that the model can better capture the macroscopic higher-order semantic information in the lat- itude and longitude.\nAs a kind of advanced SSL technology, contrastive learning-based pre-training techniques have demonstrated great potential in the field of Natural Language Process- ing (NLP). It utilizes self-supervised training by compar- ing positive and negative pairs generated through data"}, {"title": "3 PRELIMINARIES", "content": "3.1 Definitions\nDefinition 1. POI Visiting Record. In location-based ser- vices datasets, a user's visit to a certain place is repre- sented by a POI visiting record r = (u,l,t), where u represents the user, l indicates the visited location, and t denotes the timestamp of the visit. The location l is represented by (lid, lon, lat, c), comprising lid as a POI index or a grid index, and accurate longitude lon and latitude lat. c denotes the category of the visited location (e.g., hospital or restaurant).\nDefinition 2. Check-in Sequence. The movement of a user during a specific period can be represented by a list of sequential POI visiting records, which we refer to as a check-in sequence. We denote a check-in sequence as T =< r1,r2,\u2026\u2026,rs >, where the POI visit records are ordered by their visited time, and s is the length of the sequence.\n3.2 Problem Statement\nPre-training Representation for check-in Sequence. The goal of this paper is to pre-train a parameterized encoder G capable of generating a contextual representation for a given check- in sequence T, denoted as G(T). Specifically, the encoder G is first trained within a spatial-temporal cross-view frame- work using a contrastive manner, without task-specific ob- jectives. Then, it can be applied to various downstream"}, {"title": "4 SPATIAL-TEMPORAL CROSS-VIEW CONTRASTIVE FRAMEWORK", "content": "As illustrated in Fig. 3, we propose a Spatial Temporal Cross-view Contrastive Representation (STCCR) model that leverages self-supervision to capture high-level semantics, i.e., the spatial topic and temporal intention of check-in sequences separately and then fuse them at a macroscopic level. To extract the shared spatial topic of the check-in sequence, we introduce the Spatial Topic Module (STM). This module employs contrastive clustering to encode the spatial information of check-in sequences, forcing check- in sequences with similar spatial topics to have similar representations. Additionally, we combine time and user information in the Temporal Intention Module (TIM) dur- ing pre-training to extract the temporal intention of users. Specifically, we adopt a contrastive learning scheme with an angular margin to model noisy temporal information. Finally, the ST Cross-View Contrastive Module aligns the high-level spatial and temporal semantics into a unified semantic space using project heads, facilitating the inte- gration of spatial-temporal information at the macroscopic semantic level. Next, we provide a detailed explanation of our proposed model in the following sections.\n4.1 Spatial Topic Module\nThe Spatial Topic Module comprises a geohash layer, a transformer layer, and a spatial cluster contrastive block. The geohash layer and transformer layer work together to embed geographical location information into the embed- ding space. The contrastive spatial cluster block leverages contrastive clustering to capture spatial topics of users' mobility.\n4.1.1 Geographical Location Information Encoding\nThe key advantage of Geohash 1 encoding is its ability to convert geographic coordinates into a string of characters, enabling efficient storage, retrieval, and analysis of location- based data. Geohash represents latitude and longitude infor- mation in the following three steps. First, the latitude and longitude are converted into two binary sequences, elat and elon. These sequences are obtained by recursively dividing the latitude and longitude ranges. For the latitude value, the range (-90\u00b0, 90\u00b0) is divided into two sub-ranges: (-90\u00b0, 0) and (0,90\u00b0). If the latitude falls within the lower sub-range, a '0' is appended to elat, otherwise a '1' is appended. The same process is applied to elon using the initial range (-180\u00b0, 180\u00b0). Next, the even bits of egeo are set to elat and the odd bits are set to elon to create the concatenated binary sequence egeo, where i = {0, 1, 2, \u2026\u2026\u2026, 15}.\n\\(e_{(geo,2i)} = e_{(lat,i)}\\)\n\\(e_{(geo,2i+1)} = e_{(lon,i)}\\)\n(1)\nFinally, egeo is converted to Base32 encoding to produce the geohash representation.\n4.1.2 POI Category representation\nTo represent the category description of a POI, we treat the description as words and directly utilize a public pre- trained BERT model\u00b2 for sequence representation. We use a variant of BERT in which the final output of the [CLS] token is selected as the representation of the description. The representation of a POI description is denoted as ecat.\n4.1.3 Spatial Cluster Contrastive Block\nTo gain a more comprehensive understanding of user mobil- ity patterns, we propose a spatial cluster contrastive block to capture the underlying shared spatial topics of users' mobility. As discussed in Section 1, we can find a great deal of diversity among the POIs of check-in sequences. Thus, treating each individual check-in sequence separately without considering their common mobility patterns makes it hard to share statistical strength across sequences. We find that users tend to exhibit movement patterns centered on specific spatial topics during different periods. For example, users tend to move around work areas, dining areas, and residential areas during the working days, while focusing on leisure activities such as travel areas, shopping centers, and dining areas during the weekends. Therefore, extracting spatial topics from diverse check-in sequences is crucial to effectively learning the shared mobility patterns of users' movement.\nThat is, spatial topics refer to the check-in sequences that are generated over different locations but have similar, relative, and shared patterns in terms of spatial movement. To explore the shared spatial topics among sequences, we introduce the \"clustering consistency\" and \"reweighted con- trastive\" strategies into our model. To represent different shared spatial topics, we define a prototype C which is a set of k cluster centers C = {c1,\u2026,\u0441\u043a}. Meanwhile, we assume that check-in sequences with the same spatial topics fall into a similar semantic space. We use a Bi-GRU as the spatial encoder to encode the spatial information of check-in sequences combined by egeo and ecat. Given a representation of the check-in sequence through the spatial encoder z as the anchor, we use the dropout augmentation manner as SimCSE [32] to obtain its augmentation zm. We calculate each prototype assignment qi by assessing the similarity of the representation to the prototype as follows:\n\\(q_{i} = \\frac{exp(\\frac{z_{i}^T c_{k}}{\\tau})}{\\sum_{k' \\neq k} exp(\\frac{z_{i}^T c_{k'}}{\\tau})} = [q_{1}, q_{2}, ..., q_{k},..., q_{(K)}], i = {1, 2, ..., N}\\)\n(2)\nwhere each \\(q_{i} \\in R^N\\), N is the total number of check-in sequences, \\(\\tau\\) is a temperature parameter. To ensure the consistency of class attribution between the anchor and augmented sample, we define the clustering consistency loss function as:\n\\(L_{c} (z_{s}^{n}, z_{s}^{m}) = l(z_{s}^{n}, q_{n}) + l(z_{s}^{m}, q_{m}),\\)\n(3)\nwhere l(zs, q) measures the fit between representation zs and assignment q. We compare the representations zn and zm using their prototype assignments qn and qm. Each term in Eq. 3 represents the cross-entropy loss between q and the probability obtained by taking a softmax of the dot products of z, and all columns in C, i.e.,\n\\(l (z_{s}, q_{n}) = - \\sum_{k} q_{k}^{(n)} log q_{k}\\)\n(4)\nConsistency loss makes the anchor and its corresponding sample belong to a similar assignment as much as possible. But this may lead to a plain solution (i.e., the model assigns all samples inside one cluster). To avoid this, we propose a reweighting strategy that assigns larger weights to meaning- ful negative samples with a moderate prototype distance to the anchor, and smaller weights to negative samples that are easily distinguished. This assigns the samples in the batch and queue to the K classes according to q while satisfying the inter-cluster balance constraint. This makes samples that have similar prototype assignments grouped together as"}, {"title": "4.2 Temporal Intention Module", "content": "much as possible and avoids the plain-solution problem. We define the reweighting strategy denoted LR as:\n\\(LR = - \\sum_{n=1}^{N} log \\frac{\\phi(n,m)}{\\phi(n,m) + M_{n} \\sum_{j \\in S} w_{nj}\\phi(n,j)}\\)\n(5)\nwhere \\(\\phi(n, m) = exp(\\frac{z_{n}^T z_{m}}{\\tau})\\), wnj is the weight of neg- ative pairs (zn, zj). \\(M_{n} = \\frac{2\\beta}{(\\sum_{j \\in S} w_{nj})}\\) is the normal- ization factor, \u1e9e is the number of the set S. \\(S = {j | c_{n} \\neq c\"}\\), where cn and c\" are the most probable prototypes of the check-in sequence zn and zj, respectively.\nWe utilize the cosine distance to measure the distance between two assignments qn and qj as: D (qn, qj) = 1-(qn.qj)/(|qn|2||qj|2). Then, we define the weight based on the above assignment distance with the format of the Gaussian function as:\n\\(w_{nj} = exp \\{-\\frac{[D(q_{n}, q_{j}) - \\mu_{n}]^{2}}{2 \\sigma_{n}^{2}}\\}\n(6)\nwhere \u00b5\u03b7 and \u03c3n are the mean and standard deviation of D (qn, qj) for anchor zn, respectively. In this way, selected negative samples can enjoy desirable semantic differences from the anchor, and those similar ones are \"masked\" out in the objective.\nSince different clusters represent distinct underlying semantics, such a sampling strategy can ensure a distin- guishable semantic difference between the anchor and its negatives. The final training objective is the combination of LR and LC to jointly optimise the spatial topic, formulated as:\n\\(L_{Spatial} = \\eta L_{C} + L_{R}\\)\n(7)\nwhere the constant \u03b7 balances the clustering consistency loss LC and the reweighted contrastive loss LR. This loss function is jointly minimized concerning the prototype C and the parameters \u0398 of the spatial encoder used to produce the spatial representation zs.\nThe Temporal Intention Module aims at analyzing users' temporal intentions. It includes a timestamp angular con- trastive block and a social aware block. They leverage the angular margin to mitigate the effects of temporal uncer- tainty and noise."}, {"title": "4.2.1 Timestamp Embedding", "content": "4.2.1 Timestamp Embedding\nThe timestamp embedding layers convert the original tem- poral features into dense vectors. Specifically, we first dis- cretize each timestamp t into hourly intervals and then represent it as a T-dimensional one-hot vector (T = 48). To distinguish weekends from weekdays, we treat weekends as an additional 24 hours. Then we learn the embedding for each time interval, denoted by Et \u2208 RT\u00d7dt.\n4.2.2 Temporal Angular Contrastive Block\nThis block leverages the angular margin scheme to enable contrastive learning to mitigate the effects of temporal noise and extract users' temporal intention. To model the positive and negative pairwise relations between sequences, we first generate sequence representations and group them into positive and negative pairs. We then use these pairs as input to a training objective for optimization."}, {"title": "4.2.2 Temporal Angular Contrastive Block", "content": "To generate temporal representations from check-in se- quences in the temporal dimension, we employ two Bi- GRU encoders, denoted as M\u2081 and M. Similarly to the approach in ESimCSE [37], we use momentum contrast as the data augmentation method. In particular, we use the momentum-updated encoder to encode the enqueued sequence representation. Formally, denoting the parameters of the encoder Mt as \\(\\Theta_{t}\\) and those of the momentum- updated encoder M' as \\(\\Theta_{t}^{'}\\), we update \\(\\Theta_{t}^{'}\\) in the following way:\n\\(\\Theta_{t}^{'} \\leftarrow \\eta \\Theta_{t}^{'} + (1-\\eta)\\Theta_{t}\\),\n(8)\nwhere \u03b7 \u2208 [0,1) is a momentum coefficient parameter. Note that only the parameters \\(\\Theta_{t}\\) are updated by back- propagation. To generate temporal representations, we in- troduce a new set of parameters denoted as \u03b8, which are updated using momentum to ensure a smoother evolution than \\(\\Theta_{t}\\). We obtain two different temporal representations, denoted as the anchor z and the augmentation zm, by passing it through the models Mt and M\u2081, respectively. These representations share the same semantics and form a positive pair, while negative pairs are obtained by com- paring representations from different samples in the same batch. The most widely adopted training objective is the NT-Xent loss, which is formulated as follows:\n\\(L_{NT-Xent} = -log\\frac{e^{sim(z,z)/\\tau}}{\\sum_{N} e^{sim(z,z)/\\tau}}\\),\n(9)"}, {"title": "4.2.3 Social Aware Block", "content": "where sim(zn, zm) is the cosine similarity \\(\\frac{z_{n}z_{m}}{\\|z_{n}\\|*\\|z_{m}\\|}\\), \u03c4 is a temperature hyper-parameter and n is the number of sequences within a batch.\nAlthough the training objective tries to pull represen- tations with similar semantics closer and push dissimilar ones away from each other, these representations may still not be sufficiently discriminative and not be very robust to temporal noise. To demonstrate this, let us first denote angular \u03b8n,m as follows:\n\\(\\theta_{n,m} = arccos(\\frac{z_{n}^{T} z_{m}}{\\|z_{n}\\|*\\|z_{m}\\|})\\)\n(10)\nThe angular margin for zn in NT-Xent is \u03b8n,m = \u03b8n,j, as show in Fig. 5. Due to the lack of a decision margin, a tiny time perturbation around the angular margin may lead to an incorrect decision. To overcome this problem, we propose a new training objective for temporal representation learning by adding an additive angular margin o between positive pairs z and zm. This means that we want to keep some interval between the positive samples and do not force them exactly the same. We named it Time Angular Margin contrastive loss (TAM Loss), which can be formulated as follows:\n\\(L_{TAM} = -log\\frac{e^{cos(\\theta_{n,m}+\\sigma)/\\tau}}{e^{cos(\\theta_{n,m}+\\sigma)/\\tau} + \\sum_{j\\neq n} e^{cos(\\theta_{n,j})/\\tau}}\\)\n(11)\nThe TAM loss introduces a angular margin for z that is defined as \u03b8n,m + \u03c3 = \u03b8n,j, as shown in Fig. 5. Compared to the NT-Xent loss, the TAM loss further encourages z to move towards the region where \u03b8n,m is smaller and \u03b8n,j is larger. It increases the similarity of temporal representa- tions with similar semantics and enlarges the discrepancy between different semantic representations. This enhances the alignment and uniformity properties, which are the two key measurements of representation quality related to contrastive learning [32]. Moreover, the angular margin provides an extra margin o to \u03b8n,m = \u03b8n,j, which is often utilized during inference, making the loss more tolerant to temporal noise and better at capturing the underlying semantic intentions of the user. Overall, these properties make the TAM loss a more effective training objective than traditional alternatives for extracting users' temporal intentions."}, {"title": "4.2.3 Social Aware Block", "content": "To capture the intrinsic spatial-temporal movement patterns of different users more effectively, we propose the Social Aware Block, which generates a unique representation for each user. Specifically, we utilize Graph Attention Networks (GATs) to aggregate the neighbor features of each user and employ an adaptive adjacency matrix to aggregate higher- order neighbor features.\nGiven the social network G = {U,E}, where U and E denote the user and link sets respectively, the i-th user is denoted as uz. We denote the matrix Eu \u2208 RU|\u00d7Du as the lookup table of user embedding. Then we leverage GAT to aggregate neighbors' representations and update its embedding for each user, the new embedding is computed as:\n\\(h_{i}^{(l)} = \\sigma (\\sum_{j \\in N_{i}} a_{ij}W h_{j}^{(l-1)}),\\)\n(12)"}, {"title": "4.3 ST Cross-View Contrastive Module", "content": "where \\(h_{i}^{(l-1)} \\in R^{D_{u}}\\) denote the embedding of \\(u_{i}\\) in the (l - 1)-th layer. \\(N_{i}\\) is the neighbour set of user i. The initial embedding of user ui, denoted by \\((h_{i}^{(0)})\\), is simply the i-th row of Eu. \u03c3 is the sigmoid activation function. To compute the attention weight aij between users ui and uj, we use the following formula:\n\\(a_{ij} = \\frac{exp(\\phi(a^{T}[h_{i} || h_{j}]))}{\\sum_{c\\in N_{i}} exp(\\phi(a^{T}[h_{i} || h_{c}]))}\\)\n(13)\nwhere \\(\\phi\\) is the LeakyReLu activation function. Finally, we concat the final user embedding hu with zt to update the user's temporal representation zt.\n4.3 ST Cross-View Contrastive Module\nThe temporal intention and the spatial topic are strongly correlated with users and highly susceptible to each other's impacts. This gives rise to a wealth of high-quality self- supervised signals. For example, most users engage in activ- ities such as eating three meals, working, and exercising, but their schedules vary between weekdays and weekends and have different spatial topics. Different professions exhibit diverse spatial topics during the week due to their unique requirements. For instance, doctors, service workers, and students have distinct temporal intentions owing to their professional requirements.\nTo model check-in sequences based on these self- supervised cross-view signals, we propose a spatial- temporal cross-view contrastive learning framework. This approach enables the encoder to focus on learning optimal representations in both temporal and spatial views in the early stage and fuse at the semantic level. As learning pro- gresses, we align the temporal and spatial representations by unifying them into a shared semantic space based on spatial-temporal parallel pairs. By tightly fusing the most relevant semantic information from the temporal and spatial views, we fully utilize the diverse check-in sequence data to uncover the spatial-temporal patterns of users.\nThe Spatial-Temporal Cross-View contrastive module is designed to learn unified check-in sequence representa- tions prior to fusion. It learns a similarity function s gs (zs) gt (zt), which assigns higher similarity scores to"}, {"title": "4.4 Fine-tuning for Downstream Applications", "content": "parallel spatial-temporal pairs. Here, gs and gt are spatial and temporal projection heads that map the representation of sequences to a unified semantic space. For each spatial and temporal pair, we calculate the softmax-normalized spatial-to-temporal similarity score as follows:\n\\(P_{m}^{s2t}(z_{s}) = \\frac{exp(s(z_{s}, z_{m})/\\tau)}{\\sum_{n}exp(s(z_{s}, z_{x})/\\tau)},\\)\n(14)\nand the temporal-to-spatial similarity as:\n\\(P_{m}^{t2s}(z_{t}) = \\frac{exp(s(z_{t}, z_{m})/\\tau)}{\\sum_{n}exp(s(z_{t}, z_{x})/\\tau)},\\)\n(15)\nwhere 7 is a learnable temperature parameter. Let ys2t (zs) and yt2s (zt) denote the ground-truth one-hot similarity, where negative pairs have a probability of 0 and the positive pair has a probability of 1. The spatial-temporal contrastive loss is defined as the cross-entropy H between p and y :\n\\(L_{ST} = \\frac{1}{2} E_{(z_{s}, z_{t}) \\sim D}[H(y^{s2t}(z_{s}), p^{s2t}(z_{s}))+H(y^{t2s}(z_{t}), p^{t2s}(z_{t}))]\\)\n(16)\nUltimately, the pre-training loss is represented as:\n\\(L_{Pre} = L_{Spatial} + L_{TAM} + L_{ST}\\).\n(17)\n4.4 Fine-tuning for Downstream Applications\nWe employ the training set to pre-train the STCCR. We combine the spatial representation with the temporal rep- resentation as the global human behavior representation during the pre-training stage. Then we use a projection head to fine-tune among next location prediction (LP), next time prediction (TP), and trajectory user link (TUL) tasks, respec- tively. Firstly, we consider LP and TUL downstream tasks as multiclassification problems, as formulated below. Given a check-in sequence Tu from a specific user u \u2208 U, we feed it to the pre-trained encoder to obtain the check-in sequence representation G(Tu). Then we use a projection head fe to predict the classification y such as the next location where the user will soon arrive or the user who generated this check-in sequence, i.e., fo (G(Tu),0) \u2192 y. We maximize the conditional log-likelihood log fe(y | G(Tu)) for a given N observations {(G(Tu), y)}~\u2081 as follows:\n\\(L_{MLE}(\\theta) = \\sum_{i=1}^{N}log f_{\\theta}(y | G(T^{u})),\\)\n(18)\nfo (y | G(Tu)) = softmax (WG(Tu) + b) .\nSecondly, for downstream tasks of time prediction, we follow the method of IFLTPP [4] using an intensity-free method to model the interaction time as a mixture distri-bution. We first gain the mixture weights w, means \u03bc and standard deviations s from the check-in sequence represen- tation G(Tu). Then we build a mixed distribution function and sample to get the prediction time \u03c4 as follows:\n\\(p(\\tau|\\omega, \\mu, s) = \\sum_{k=1}^{K}\\frac{\\omega_{k}}{\\tau s_{k} \\sqrt{2\\pi}} exp(-\\frac{(log\\tau - \\mu_{k})^{2}}{2s_{k}^{2}}),\\)\n(19)\nwhere k represents the number of independent Gaussian distributions in the mixed distribution. Then, we can sample from the mixture model in the parsing solution.\n\\(\\tau = \\sum_{k=1}^{K} \\omega_{k} exp(a \\mu_{k} + b + \\frac{a^{2}s_{k}^{2}}{2})),\\tau \\sim N(\\mu, \\sigma)\\)\n(20)\nwhere a denotes the mean of the whole set and b denotes the standard deviation of the whole set."}, {"title": "5 EXPERIMENTS", "content": "To evaluate the performance of our proposed model, we carried out extensive experiments on three real-world check- in sequence datasets, targeting three different types of downstream: Location Prediction (LP), Trajectory User Link (TUL), and Time Prediction (TP). The code has been released at: https://github.com/LetianGong/STCCR.\n5.1 Datasets\nIn our experiments, we use three real-world datasets de- rived from raw WeePlace 3, Gowalla 4 of New York City (NYC), and Tokyo (TKY) check-in data. Our model under- goes a filtering process that selects high-quality trajectory sequences for training. To ensure data consistency, we set a maximum historical time limit of 120 days and filter out users with fewer than 10 records and places visited fewer than 10 times. The three datasets exhibit distinct spatial-temporal correlations. For example, on workdays, a user may visit a breakfast restaurant at a certain time, while on rest days, the user's eating schedule may shift, making their behavior more time-sensitive. Due to the large number of POIs, sparse data sets, irregular time intervals, and varying user intentions, it is challenging to forecast and extract geographic and temporal information. Through our experiments, we demonstrate the superior- ity of our STCCR model, which we evaluate on all three datasets."}, {"title": "5.2 Baselines", "content": "5.2 Baselines\nLocation Prediction Methods We cover one classic check-in prediction models and three state-of-the-art LP models to demonstrate the superiority of our model.\n\u2022 DeepMove [1] is a classical check-in sequence po- sition prediction model to capture the periodicity of trajectory motion."}, {"title": "5.3 Settings", "content": "We standardize all data using the Log Mean-Std approach and feed the normalized data into the model"}, {"title": "Spatial-Temporal Cross-View Contrastive Pre-training for Check-in Sequence Representation Learning", "authors": ["Letian Gong", "Huaiyu Wan", "Shengnan Guo*", "Xiucheng Li", "Yan Lin", "Erwen Zheng", "Tianyi Wang", "Zeyu Zhou", "Youfang Lin"], "abstract": "The rapid growth of location-based services (LBS) has yielded massive amounts of data on human mobility. Effectively extracting meaningful representations for user-generated check-in sequences is pivotal for facilitating various downstream services. However, the user-generated check-in data are simultaneously influenced by the surrounding objective circumstances and the user's subjective intention. Specifically, the temporal uncertainty and spatial diversity exhibited in check-in data make it difficult to capture the macroscopic spatial-temporal patterns of users and to understand the semantics of user mobility activities. Furthermore, the distinct characteristics of the temporal and spatial information in check-in sequences call for an effective fusion method to incorporate these two types of information. In this paper, we propose a novel Spatial-Temporal Cross-view Contrastive Representation (STCCR) framework for check-in sequence representation learning. Specifically, STCCR addresses the above challenges by employing self-supervision from \"spatial topic\" and \"temporal intention\" views, facilitating effective fusion of spatial and temporal information at the semantic level. Besides, STCCR leverages contrastive clustering to uncover users' shared spatial topics from diverse mobility activities, while employing angular momentum contrast to mitigate the impact of temporal uncertainty and noise. We extensively evaluate STCCR on three real-world datasets and demonstrate its superior performance across three downstream tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "LOCATION-BASED services (LBS), such as Gowalla, Weeplace, and Yelp, have experienced significant development over the past decade. These platforms enable users to share and discover location information and surrounding services, resulting in the accumulation of extensive data on human mobility behavior, e.g. check-in sequences at points of interest (POIs). This offers prospects for analyzing and comprehending human mobility patterns for various practical applications, such as predicting the next check-in location or time for personalized recommendations, linking trajectories to users, and detecting abnormal trajectories for safety control purposes etc. Learning accurate and universal representations for check-in sequences is a crucial task in human mobility data mining. However, the existing excellent end-to-end models for check-in sequences modeling, such as those designed for location prediction [1], [2], [3], time prediction [4], and trajectory user link [5], [6], [7], often struggle to learn generalized representations for check-in sequences and fail to comprehensively describe the spatial-temporal patterns of human mobility, since the supervision signals of these models usually rely on limited single-type labels. Therefore, the learned representations are task-specific and poorly generalized. To facilitate the generalization ability for the check-in sequence's representations, pre-training check-in sequence representation via self- supervised learning has been widely studied and proven to be an effective way to fully exploit massive unlabeled check- in data to boost the performance of the downstream tasks. Representation learning is always one of the hot research topics in deep learning. And recently, contrastive pre-training with self-supervised signals [8] has emerged as the most effective approach for sequence modeling. In particular, some representative works [9], [10], [11] in the spatial-temporal data mining (STDM) domain have proven their effectiveness in learning the representations of check- in sequences. However, the unique spatial and temporal characteristics of check-in sequences raise challenges for these contrastive pre-training based models, meanwhile weakening their ability to capture the macroscopic spatial- temporal mobility patterns and to understand the high-level semantics of user mobility activities. Specifically, we identify three key challenges: (1) Temporal uncertainty: Understanding the temporal intention of users' ability from the check-in sequence with uncertain temporal information is challenging. As shown in Fig. 1, based on the user's historical sequence and the user's historical spatial-temporal behavior patterns, the user is most likely to go for dinner next, with the strongest temporal intention being at 17:00. However, the exact time"}, {"title": "2 RELATED WORK", "content": "2.1 Mobility Data Mining\nLocation-based services have given rise to a new and promising research topic known as mobility data mining, which has led to the emergence of three significant tasks that contribute to enhancing the quality of services: next location prediction (LP), next time prediction (TP), and trajectory user link (TUL). Recent studies have confirmed that deep learning techniques, specifically recurrent neural networks (RNNs) and attention mechanisms, are highly effective in capturing sequential and periodic patterns of human mobility. By combining deep learning techniques, researchers have made significant advancements in capturing both the sequential and periodic patterns of human mobility. The core of these models is the modeling of check-in sequences, which leads to improved accuracy in location prediction and trajectory analysis.\nLP aims to anticipate a user's future location based on their historical movement. Several notable models have emerged as prominent approaches in LP. DeepMove [1] leverages RNNs and attention mechanisms to capture the spatial-temporal intentions in users' location data and predict their next destination. STAN [14] introduces a spatial- temporal attention network that incorporates spatial and temporal contexts for accurate prediction. LSTPM [2] focuses on long and short-term patterns in user trajectory using an attention-based LSTM [15] model. SERM [3] utilizes an encoder-decoder architecture with a spatial-temporal residual network to capture user preferences and predict future locations. PLSPL [16] trains two LSTM models for location and category based sequence to capture the user's preference. LightMove [17] designs neural ordinary differential equations to enhance robustness against sparse or incorrect inputs. HMT-GRN [18] alleviates the data sparsity problem by learning different User-Region matrices of lower sparsities in a multitask setting. Graph-Flashback [19] constructs a spatial-temporal knowledge graph to enhance the representation of POIs. GETNext [20] introduces a user- agnostic global trajectory flow map as a means to leverage the abundant collaborative signals.\nTUL is a significant task that focuses on establishing connections between different trajectories, facilitating the analysis of user movement patterns, and uncovering valuable insights about their behavior. Notable models have been specifically developed to address the challenge of predicting trajectory links. TULER [6] takes advantage of advanced algorithms to establish links between trajectories, allowing for a comprehensive understanding of user movement patterns. DeepTUL [5] utilizes deep learning techniques to extract representations from trajectory data and facilitate the prediction of trajectory links. S2TUL [21] utilizes graph convolutional networks and sequential neural networks to capture trajectory relationships and intra-trajectory information. GNNTUL [22] employs graph neural networks for human mobility and associates the traces with users on social networks.\nTP focuses on estimating the time at which a user is likely to visit their next location. To accomplish this, it is common practice to use intensity functions to represent the rate or density of event occurrences, various models have been developed to model the intensity function and make accurate time predictions effectively. Modeling the intensity function using RNNs or attention mechanisms is a common approach for predicting the occurrence of events. RMTPP [23] utilizes RNNs to model the intensity function. SAHP [24] combines the Hawkes process with self-attention mechanisms to capture the temporal dependencies and spatial influences in event sequences. THP [25] combines the Hawkes process with transformer-based architectures to capture temporal dependencies in event sequences. NSTPP [26] utilizes neural ODEs to model discrete events in continuous time and space, enabling the learning of complex distributions in spatial and temporal domains. IMTPP [27] models the generative processes of observed and missing events and utilizes unsupervised modeling and inference methods for time prediction. DSTPP [28] purposes a novel parameterization framework that uses diffusion models to learn complex joint distributions.\nIt is important to note that these end-to-end supervised methods designed for specific tasks are not universal. These models do not have a good grasp of the macroscopic semantics of check-in sequences. Thus, learning the universal representation of check-in sequences to improve the model's ability and understand high-level semantics is critical.\n2.2 Pretraining and Contrastive Learning\nThe essence of mobility mining tasks lies in learning the representation of check-in sequences. Numerous studies have demonstrated the effectiveness of employing the pre- training paradigm to achieve check-in sequence representation learning. For instance, TULVAE [7] and MoveSim [29] utilize Variational Auto-Encoder and Generative Adversarial Network, respectively, to capture the movement patterns of check-in sequences through pre-training. TALE [30] proposes a pre-training representation scheme for trajectory point location embedding that incorporates temporal semantics, which effectively improves the performance of next location prediction and location traffic prediction. CTLE [31] proposes a location pre-training representation model that incorporates domain features, which dynamically generates feature representations of the domain environment of the target location so that the model can better capture the macroscopic higher-order semantic information in the latitude and longitude.\nAs a kind of advanced SSL technology, contrastive learning-based pre-training techniques have demonstrated great potential in the field of Natural Language Processing (NLP). It utilizes self-supervised training by comparing positive and negative pairs generated through data"}, {"title": "3 PRELIMINARIES", "content": "3.1 Definitions\nDefinition 1. POI Visiting Record. In location-based services datasets, a user's visit to a certain place is represented by a POI visiting record r = (u,l,t), where u represents the user, l indicates the visited location, and t denotes the timestamp of the visit. The location l is represented by (lid, lon, lat, c), comprising lid as a POI index or a grid index, and accurate longitude lon and latitude lat. c denotes the category of the visited location (e.g., hospital or restaurant).\nDefinition 2. Check-in Sequence. The movement of a user during a specific period can be represented by a list of sequential POI visiting records, which we refer to as a check-in sequence. We denote a check-in sequence as T =< r1,r2,\u2026\u2026,rs >, where the POI visit records are ordered by their visited time, and s is the length of the sequence.\n3.2 Problem Statement\nPre-training Representation for check-in Sequence. The goal of this paper is to pre-train a parameterized encoder G capable of generating a contextual representation for a given check- in sequence T, denoted as G(T). Specifically, the encoder G is first trained within a spatial-temporal cross-view framework using a contrastive manner, without task-specific ob- jectives. Then, it can be applied to various downstream"}, {"title": "4 SPATIAL-TEMPORAL CROSS-VIEW CONTRASTIVE FRAMEWORK", "content": "As illustrated in Fig. 3, we propose a Spatial Temporal Cross-view Contrastive Representation (STCCR) model that leverages self-supervision to capture high-level semantics, i.e., the spatial topic and temporal intention of check-in sequences separately and then fuse them at a macroscopic level. To extract the shared spatial topic of the check-in sequence, we introduce the Spatial Topic Module (STM). This module employs contrastive clustering to encode the spatial information of check-in sequences, forcing check- in sequences with similar spatial topics to have similar representations. Additionally, we combine time and user information in the Temporal Intention Module (TIM) during pre-training to extract the temporal intention of users. Specifically, we adopt a contrastive learning scheme with an angular margin to model noisy temporal information. Finally, the ST Cross-View Contrastive Module aligns the high-level spatial and temporal semantics into a unified semantic space using project heads, facilitating the integration of spatial-temporal information at the macroscopic semantic level. Next, we provide a detailed explanation of our proposed model in the following sections.\n4.1 Spatial Topic Module\nThe Spatial Topic Module comprises a geohash layer, a transformer layer, and a spatial cluster contrastive block. The geohash layer and transformer layer work together to embed geographical location information into the embedding space. The contrastive spatial cluster block leverages contrastive clustering to capture spatial topics of users' mobility.\n4.1.1 Geographical Location Information Encoding\nThe key advantage of Geohash 1 encoding is its ability to convert geographic coordinates into a string of characters, enabling efficient storage, retrieval, and analysis of location- based data. Geohash represents latitude and longitude information in the following three steps. First, the latitude and longitude are converted into two binary sequences, elat and elon. These sequences are obtained by recursively dividing the latitude and longitude ranges. For the latitude value, the range (-90\u00b0, 90\u00b0) is divided into two sub-ranges: (-90\u00b0, 0) and (0,90\u00b0). If the latitude falls within the lower sub-range, a '0' is appended to elat, otherwise a '1' is appended. The same process is applied to elon using the initial range (-180\u00b0, 180\u00b0). Next, the even bits of egeo are set to elat and the odd bits are set to elon to create the concatenated binary sequence egeo, where i = {0, 1, 2, \u2026\u2026\u2026, 15}.\n\\(e_{(geo,2i)} = e_{(lat,i)}\\)\n\\(e_{(geo,2i+1)} = e_{(lon,i)}\\)\n(1)\nFinally, egeo is converted to Base32 encoding to produce the geohash representation.\n4.1.2 POI Category representation\nTo represent the category description of a POI, we treat the description as words and directly utilize a public pre- trained BERT model\u00b2 for sequence representation. We use a variant of BERT in which the final output of the [CLS] token is selected as the representation of the description. The representation of a POI description is denoted as ecat.\n4.1.3 Spatial Cluster Contrastive Block\nTo gain a more comprehensive understanding of user mobility patterns, we propose a spatial cluster contrastive block to capture the underlying shared spatial topics of users' mobility. As discussed in Section 1, we can find a great deal of diversity among the POIs of check-in sequences. Thus, treating each individual check-in sequence separately without considering their common mobility patterns makes it hard to share statistical strength across sequences. We find that users tend to exhibit movement patterns centered on specific spatial topics during different periods. For example, users tend to move around work areas, dining areas, and residential areas during the working days, while focusing on leisure activities such as travel areas, shopping centers, and dining areas during the weekends. Therefore, extracting spatial topics from diverse check-in sequences is crucial to effectively learning the shared mobility patterns of users' movement.\nThat is, spatial topics refer to the check-in sequences that are generated over different locations but have similar, relative, and shared patterns in terms of spatial movement. To explore the shared spatial topics among sequences, we introduce the \"clustering consistency\" and \"reweighted contrastive\" strategies into our model. To represent different shared spatial topics, we define a prototype C which is a set of k cluster centers C = {c1,\u2026,\u0441\u043a}. Meanwhile, we assume that check-in sequences with the same spatial topics fall into a similar semantic space. We use a Bi-GRU as the spatial encoder to encode the spatial information of check-in sequences combined by egeo and ecat. Given a representation of the check-in sequence through the spatial encoder z as the anchor, we use the dropout augmentation manner as SimCSE [32] to obtain its augmentation zm. We calculate each prototype assignment qi by assessing the similarity of the representation to the prototype as follows:\n\\(q_{i} = \\frac{exp(\\frac{z_{i}^T c_{k}}{\\tau})}{\\sum_{k' \\neq k} exp(\\frac{z_{i}^T c_{k'}}{\\tau})} = [q_{1}, q_{2}, ..., q_{k},..., q_{(K)}], i = {1, 2, ..., N}\\)\n(2)\nwhere each \\(q_{i} \\in R^N\\), N is the total number of check-in sequences, \\(\\tau\\) is a temperature parameter. To ensure the consistency of class attribution between the anchor and augmented sample, we define the clustering consistency loss function as:\n\\(L_{c} (z_{s}^{n}, z_{s}^{m}) = l(z_{s}^{n}, q_{n}) + l(z_{s}^{m}, q_{m}),\\)\n(3)\nwhere l(zs, q) measures the fit between representation zs and assignment q. We compare the representations zn and zm using their prototype assignments qn and qm. Each term in Eq. 3 represents the cross-entropy loss between q and the probability obtained by taking a softmax of the dot products of z, and all columns in C, i.e.,\n\\(l (z_{s}, q_{n}) = - \\sum_{k} q_{k}^{(n)} log q_{k}\\)\n(4)\nConsistency loss makes the anchor and its corresponding sample belong to a similar assignment as much as possible. But this may lead to a plain solution (i.e., the model assigns all samples inside one cluster). To avoid this, we propose a reweighting strategy that assigns larger weights to meaning- ful negative samples with a moderate prototype distance to the anchor, and smaller weights to negative samples that are easily distinguished. This assigns the samples in the batch and queue to the K classes according to q while satisfying the inter-cluster balance constraint. This makes samples that have similar prototype assignments grouped together as"}, {"title": "4.2 Temporal Intention Module", "content": "much as possible and avoids the plain-solution problem. We define the reweighting strategy denoted LR as:\n\\(LR = - \\sum_{n=1}^{N} log \\frac{\\phi(n,m)}{\\phi(n,m) + M_{n} \\sum_{j \\in S} w_{nj}\\phi(n,j)}\\)\n(5)\nwhere \\(\\phi(n, m) = exp(\\frac{z_{n}^T z_{m}}{\\tau})\\), wnj is the weight of negative pairs (zn, zj). \\(M_{n} = \\frac{2\\beta}{(\\sum_{j \\in S} w_{nj})}\\) is the normalization factor, \u03b2 is the number of the set S. \\(S = {j | c_{n} \\neq c\"}\\), where cn and c\" are the most probable prototypes of the check-in sequence zn and zj, respectively.\nWe utilize the cosine distance to measure the distance between two assignments qn and qj as: D (qn, qj) = 1-(qn.qj)/(|qn|2||qj|2). Then, we define the weight based on the above assignment distance with the format of the Gaussian function as:\n\\(w_{nj} = exp \\{-\\frac{[D(q_{n}, q_{j}) - \\mu_{n}]^{2}}{2 \\sigma_{n}^{2}}\\}\n(6)\nwhere \u00b5\u03b7 and \u03c3n are the mean and standard deviation of D (qn, qj) for anchor zn, respectively. In this way, selected negative samples can enjoy desirable semantic differences from the anchor, and those similar ones are \"masked\" out in the objective.\nSince different clusters represent distinct underlying semantics, such a sampling strategy can ensure a distinguishable semantic difference between the anchor and its negatives. The final training objective is the combination of LR and LC to jointly optimise the spatial topic, formulated as:\n\\(L_{Spatial} = \\eta L_{C} + L_{R}\\)\n(7)\nwhere the constant \u03b7 balances the clustering consistency loss LC and the reweighted contrastive loss LR. This loss function is jointly minimized concerning the prototype C and the parameters \u0398 of the spatial encoder used to produce the spatial representation zs.\nThe Temporal Intention Module aims at analyzing users' temporal intentions. It includes a timestamp angular con- trastive block and a social aware block. They leverage the angular margin to mitigate the effects of temporal uncertainty and noise."}, {"title": "4.2.1 Timestamp Embedding", "content": "4.2.1 Timestamp Embedding\nThe timestamp embedding layers convert the original temporal features into dense vectors. Specifically, we first discretize each timestamp t into hourly intervals and then represent it as a T-dimensional one-hot vector (T = 48). To distinguish weekends from weekdays, we treat weekends as an additional 24 hours. Then we learn the embedding for each time interval, denoted by Et \u2208 RT\u00d7dt.\n4.2.2 Temporal Angular Contrastive Block\nThis block leverages the angular margin scheme to enable contrastive learning to mitigate the effects of temporal noise and extract users' temporal intention. To model the positive and negative pairwise relations between sequences, we first generate sequence representations and group them into positive and negative pairs. We then use these pairs as input to a training objective for optimization."}, {"title": "4.2.2 Temporal Angular Contrastive Block", "content": "To generate temporal representations from check-in sequences in the temporal dimension, we employ two Bi- GRU encoders, denoted as M\u2081 and M. Similarly to the approach in ESimCSE [37], we use momentum contrast as the data augmentation method. In particular, we use the momentum-updated encoder to encode the enqueued sequence representation. Formally, denoting the parameters of the encoder Mt as \\(\\Theta_{t}\\) and those of the momentum- updated encoder M' as \\(\\Theta_{t}^{'}\\), we update \\(\\Theta_{t}^{'}\\) in the following way:\n\\(\\Theta_{t}^{'} \\leftarrow \\eta \\Theta_{t}^{'} + (1-\\eta)\\Theta_{t}\\),\n(8)\nwhere \u03b7 \u2208 [0,1) is a momentum coefficient parameter. Note that only the parameters \\(\\Theta_{t}\\) are updated by back- propagation. To generate temporal representations, we introduce a new set of parameters denoted as \u03b8, which are updated using momentum to ensure a smoother evolution than \\(\\Theta_{t}\\). We obtain two different temporal representations, denoted as the anchor z and the augmentation zm, by passing it through the models Mt and M\u2081, respectively. These representations share the same semantics and form a positive pair, while negative pairs are obtained by comparing representations from different samples in the same batch. The most widely adopted training objective is the NT-Xent loss, which is formulated as follows:\n\\(L_{NT-Xent} = -log\\frac{e^{sim(z,z)/\\tau}}{\\sum_{N} e^{sim(z,z)/\\tau}}\\),\n(9)"}, {"title": "4.2.3 Social Aware Block", "content": "where sim(zn, zm) is the cosine similarity \\(\\frac{z_{n}z_{m}}{\\|z_{n}\\|*\\|z_{m}\\|}\\), \u03c4 is a temperature hyper-parameter and n is the number of sequences within a batch.\nAlthough the training objective tries to pull representations with similar semantics closer and push dissimilar ones away from each other, these representations may still not be sufficiently discriminative and not be very robust to temporal noise. To demonstrate this, let us first denote angular \u03b8n,m as follows:\n\\(\\theta_{n,m} = arccos(\\frac{z_{n}^{T} z_{m}}{\\|z_{n}\\|*\\|z_{m}\\|})\\)\n(10)\nThe angular margin for zn in NT-Xent is \u03b8n,m = \u03b8n,j, as show in Fig. 5. Due to the lack of a decision margin, a tiny time perturbation around the angular margin may lead to an incorrect decision. To overcome this problem, we propose a new training objective for temporal representation learning by adding an additive angular margin o between positive pairs z and zm. This means that we want to keep some interval between the positive samples and do not force them exactly the same. We named it Time Angular Margin contrastive loss (TAM Loss), which can be formulated as follows:\n\\(L_{TAM} = -log\\frac{e^{cos(\\theta_{n,m}+\\sigma)/\\tau}}{e^{cos(\\theta_{n,m}+\\sigma)/\\tau} + \\sum_{j\\neq n} e^{cos(\\theta_{n,j})/\\tau}}\\)\n(11)\nThe TAM loss introduces a angular margin for z that is defined as \u03b8n,m + \u03c3 = \u03b8n,j, as shown in Fig. 5. Compared to the NT-Xent loss, the TAM loss further encourages z to move towards the region where \u03b8n,m is smaller and \u03b8n,j is larger. It increases the similarity of temporal representations with similar semantics and enlarges the discrepancy between different semantic representations. This enhances the alignment and uniformity properties, which are the two key measurements of representation quality related to contrastive learning [32]. Moreover, the angular margin provides an extra margin o to \u03b8n,m = \u03b8n,j, which is often utilized during inference, making the loss more tolerant to temporal noise and better at capturing the underlying semantic intentions of the user. Overall, these properties make the TAM loss a more effective training objective than traditional alternatives for extracting users' temporal intentions."}, {"title": "4.2.3 Social Aware Block", "content": "To capture the intrinsic spatial-temporal movement patterns of different users more effectively, we propose the Social Aware Block, which generates a unique representation for each user. Specifically, we utilize Graph Attention Networks (GATs) to aggregate the neighbor features of each user and employ an adaptive adjacency matrix to aggregate higher- order neighbor features.\nGiven the social network G = {U,E}, where U and E denote the user and link sets respectively, the i-th user is denoted as uz. We denote the matrix Eu \u2208 RU|\u00d7Du as the lookup table of user embedding. Then we leverage GAT to aggregate neighbors' representations and update its embedding for each user, the new embedding is computed as:\n\\(h_{i}^{(l)} = \\sigma (\\sum_{j \\in N_{i}} a_{ij}W h_{j}^{(l-1)}),\\)\n(12)"}, {"title": "4.3 ST Cross-View Contrastive Module", "content": "where \\(h_{i}^{(l-1)} \\in R^{D_{u}}\\) denote the embedding of \\(u_{i}\\) in the (l - 1)-th layer. \\(N_{i}\\) is the neighbour set of user i. The initial embedding of user ui, denoted by \\((h_{i}^{(0)})\\), is simply the i-th row of Eu. \u03c3 is the sigmoid activation function. To compute the attention weight aij between users ui and uj, we use the following formula:\n\\(a_{ij} = \\frac{exp(\\phi(a^{T}[h_{i} || h_{j}]))}{\\sum_{c\\in N_{i}} exp(\\phi(a^{T}[h_{i} || h_{c}]))}\\)\n(13)\nwhere \\(\\phi\\) is the LeakyReLu activation function. Finally, we concat the final user embedding hu with zt to update the user's temporal representation zt.\n4.3 ST Cross-View Contrastive Module\nThe temporal intention and the spatial topic are strongly correlated with users and highly susceptible to each other's impacts. This gives rise to a wealth of high-quality self- supervised signals. For example, most users engage in activities such as eating three meals, working, and exercising, but their schedules vary between weekdays and weekends and have different spatial topics. Different professions exhibit diverse spatial topics during the week due to their unique requirements. For instance, doctors, service workers, and students have distinct temporal intentions owing to their professional requirements.\nTo model check-in sequences based on these self- supervised cross-view signals, we propose a spatial- temporal cross-view contrastive learning framework. This approach enables the encoder to focus on learning optimal representations in both temporal and spatial views in the early stage and fuse at the semantic level. As learning progresses, we align the temporal and spatial representations by unifying them into a shared semantic space based on spatial-temporal parallel pairs. By tightly fusing the most relevant semantic information from the temporal and spatial views, we fully utilize the diverse check-in sequence data to uncover the spatial-temporal patterns of users.\nThe Spatial-Temporal Cross-View contrastive module is designed to learn unified check-in sequence representations prior to fusion. It learns a similarity function s gs (zs) gt (zt), which assigns higher similarity scores to"}, {"title": "4.4 Fine-tuning for Downstream Applications", "content": "parallel spatial-temporal pairs. Here, gs and gt are spatial and temporal projection heads that map the representation of sequences to a unified semantic space. For each spatial and temporal pair, we calculate the softmax-normalized spatial-to-temporal similarity score as follows:\n\\(P_{m}^{s2t}(z_{s}) = \\frac{exp(s(z_{s}, z_{m})/\\tau)}{\\sum_{n}exp(s(z_{s}, z_{x})/\\tau)},\\)\n(14)\nand the temporal-to-spatial similarity as:\n\\(P_{m}^{t2s}(z_{t}) = \\frac{exp(s(z_{t}, z_{m})/\\tau)}{\\sum_{n}exp(s(z_{t}, z_{x})/\\tau)},\\)\n(15)\nwhere 7 is a learnable temperature parameter. Let ys2t (zs) and yt2s (zt) denote the ground-truth one-hot similarity, where negative pairs have a probability of 0 and the positive pair has a probability of 1. The spatial-temporal contrastive loss is defined as the cross-entropy H between p and y :\n\\(L_{ST} = \\frac{1}{2} E_{(z_{s}, z_{t}) \\sim D}[H(y^{s2t}(z_{s}), p^{s2t}(z_{s}))+H(y^{t2s}(z_{t}), p^{t2s}(z_{t}))]\\)\n(16)\nUltimately, the pre-training loss is represented as:\n\\(L_{Pre} = L_{Spatial} + L_{TAM} + L_{ST}\\).\n(17)\n4.4 Fine-tuning for Downstream Applications\nWe employ the training set to pre-train the STCCR. We combine the spatial representation with the temporal representation as the global human behavior representation during the pre-training stage. Then we use a projection head to fine-tune among next location prediction (LP), next time prediction (TP), and trajectory user link (TUL) tasks, respectively. Firstly, we consider LP and TUL downstream tasks as multiclassification problems, as formulated below. Given a check-in sequence Tu from a specific user u \u2208 U, we feed it to the pre-trained encoder to obtain the check-in sequence representation G(Tu). Then we use a projection head fe to predict the classification y such as the next location where the user will soon arrive or the user who generated this check-in sequence, i.e., fo (G(Tu),0) \u2192 y. We maximize the conditional log-likelihood log fe(y | G(Tu)) for a given N observations {(G(Tu), y)}~\u2081 as follows:\n\\(L_{MLE}(\\theta) = \\sum_{i=1}^{N}log f_{\\theta}(y | G(T^{u})),\\)\n(18)\nfo (y | G(Tu)) = softmax (WG(Tu) + b) .\nSecondly, for downstream tasks of time prediction, we follow the method of IFLTPP [4] using an intensity-free method to model the interaction time as a mixture distribution. We first gain the mixture weights w, means \u03bc and standard deviations s from the check-in sequence representation G(Tu). Then we build a mixed distribution function and sample to get the prediction time \u03c4 as follows:\n\\(p(\\tau|\\omega, \\mu, s) = \\sum_{k=1}^{K}\\frac{\\omega_{k}}{\\tau s_{k} \\sqrt{2\\pi}} exp(-\\frac{(log\\tau - \\mu_{k})^{2}}{2s_{k}^{2}}),\\)\n(19)\nwhere k represents the number of independent Gaussian distributions in the mixed distribution. Then, we can sample from the mixture model in the parsing solution.\n\\(\\tau = \\sum_{k=1}^{K} \\omega_{k} exp(a \\mu_{k} + b + \\frac{a^{2}s_{k}^{2}}{2})),\\tau \\sim N(\\mu, \\sigma)\\)\n(20)\nwhere a denotes the mean of the whole set and b denotes the standard deviation of the whole set."}, {"title": "5 EXPERIMENTS", "content": "To evaluate the performance of our proposed model, we carried out extensive experiments on three real-world check- in sequence datasets, targeting three different types of downstream: Location Prediction (LP), Trajectory User Link (TUL), and Time Prediction (TP). The code has been released at: https://github.com/LetianGong/STCCR.\n5.1 Datasets\nIn our experiments, we use three real-world datasets derived from raw WeePlace 3, Gowalla 4 of New York City (NYC), and Tokyo (TKY) check-in data. Our model undergoes a filtering process that selects high-quality trajectory sequences for training. To ensure data consistency, we set a maximum historical time limit of 120 days and filter out users with fewer than 10 records and places visited fewer than 10 times. The three datasets exhibit distinct spatial-temporal correlations. For example, on workdays, a user may visit a breakfast restaurant at a certain time, while on rest days, the user's eating schedule may shift, making their behavior more time-sensitive. Due to the large number of POIs, sparse data sets, irregular time intervals, and varying user intentions, it is challenging to forecast and extract geographic and temporal information. Through our experiments, we demonstrate the superiority of our STCCR model, which we evaluate on all three datasets."}, {"title": "5.2 Baselines", "content": "5.2 Baselines\nLocation Prediction Methods We cover one classic check-in prediction models and three state-of-the-art LP models to demonstrate the superiority of our model.\n\u2022 DeepMove [1] is a classical check-in sequence position prediction model to capture the periodicity of trajectory motion."}, {"title": "5.3 Settings", "content": "We standardize all data using the Log Mean-Std approach and feed the normalized data into the model, which is optimized using reverse mode automatic differentiation and Adam [38]. To train the model, historical data of locations and users are first embedded, followed by location features fed into the spatial topic module and user IDs fed into the user embedding layer. We found that the time information worked better without the embedding layer, so we did not embed it. The representation vectors output from the spatial topic module and the temporal intention module are first mapped in the spatial-temporal joint space by the projection head. After that, we do contrastive learning between the outcomes of the two modules. Three separate downstream tasks are subsequently completed by combining the two representations from those two modules.\nTo assess the projected values, we retransform them back to the actual values and compare them to the ground truth. The evaluation measures of the location prediction and trajectory user link tasks include Acc@k and mean reciprocal rank (MRR). The evaluation measures of the time prediction task include mean absolute error (MAE), the root mean square error (RMSE), and mean absolute percentage error (MAPE). The STCCR model was constructed using the PyTorch. The loss function is a cross-entropy loss for the LP and TUL tasks and an MAE loss for the TP task. The performance on the validation sets determines the hyper-parameters and the best models. All experiments are performed five times, and the means and standard deviations are calculated. To make a fair comparison, for all methods, the embedding dimension d is 256, while the hidden state h has a size of 256. The learning rate is 0.001. Other detailed settings of the STCCR model for the three datasets are described in . The is pre-trained for 100 epochs on the training sets with the early-stopping mechanism of 10 patience. All trials have been conducted on Intel Xeon E5-2620 CPUs and NVIDIA RTX A5000 GPUs."}, {"title": "5.4 Comparison and Analysis of Results on the Down-Stream Tasks", "content": "Table 2 and Table 3 shows the comparison results of our model with other baseline models on three downstream tasks. The best is shown in bold, and the second-best is shown as underlined.\nOur representation learning pre-trained models can meet or even exceed the best-performing end-to-end models. Besides, the performance of STCCR far outperforms other sequential representation learning models on three tasks. In the LP task, STCCR improves the sota"}]}]}