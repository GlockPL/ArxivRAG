{"title": "CANDERE-COACH: Reinforcement Learning from Noisy Feedback", "authors": ["Yuxuan Li", "Srijita Das", "Matthew E. Taylor"], "abstract": "In recent times, Reinforcement learning (RL) has been widely applied to many challenging tasks. However, in order to perform well, it requires access to a good reward function which is often sparse or manually engineered with scope for error. Introducing human prior knowledge is often seen as a possible solution to the above-mentioned problem, such as imitation learning, learning from preference, and inverse reinforcement learning. Learning from feedback is another framework that enables an RL agent to learn from binary evaluative signals describing the teacher's (positive or negative) evaluation of the agent's action. However, these methods often make the assumption that evaluative teacher feedback is perfect, which is a restrictive assumption. In practice, such feedback can be noisy due to limited teacher expertise or other exacerbating factors like cognitive load, availability, distraction, etc. In this work, we propose the CANDERE-COACH algorithm, which is capable of learning from noisy feedback by a nonoptimal teacher. We propose a noise-filtering mechanism to de-noise online feedback data, thereby enabling the RL agent to successfully learn with up to 40% of the teacher feedback being incorrect. Experiments on three common domains demonstrate the effectiveness of the proposed approach.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has made rapid progress in part due to the advancements of deep learning, which has been successfully applied to the challenging game of Go Silver et al. [2017], solving Rubik's cube with a robot arm Akkaya et al. [2019], and for deciding the treatment regimen for cancer Tseng et al. [2017]. Although successful in solving these problems, its progress has still been significantly hindered by the well-known sample-inefficiency problem Ibarz et al. [2021] because the agent may need millions of interactions with an environment to learn a near-optimal policy.\nMoreover, deep RL's performance often deteriorates in domains with sparse reward because of slower propagation of the reward signal to the entire state-space Yu et al. [2020], Knox et al. [2023]. To combat this problem, reward functions may be manually specified by task experts or RL developers, often by trial-and-error. However, even when human subjects carefully designed reward functions,"}, {"title": "2 Related Work", "content": "RL agents learning from teacher advice: RL agents are often challenged by sparse reward domains. Receiving help from a knowledgeable teacher can ameliorate such problems, such as when a teacher agent (or human) provides demonstrations (e.g., in imitation learning Billard et al. [2003], Giusti et al. [2015], Ross et al. [2011] or inverse reinforcement learning Das et al. [2020], Ho and Ermon [2016]). There are also teacher-student frameworks Torrey and Taylor [2013], Ilhan et al. [2021] where the student agent asks for action advice from the teacher, and learning from preferences Wilson et al. [2012], Christiano et al. [2017], Lee et al. [2021], where preferences over pairs of trajectories are provided. Learning from feedback is another advising framework, where agents learn from binary feedback provided by the teacher. Knox et al. Knox and Stone [2009] proposed TAMER to exploit human feedback signals; which in turn is used to learn an expectation of human feedback that the agent can maximize. Macglashan et al. MacGlashan et al. [2017] proposed COACH, which uses feedback as the advantage function in the policy-gradient objective. These methods have also been extended in Deep RL settings like Deep TAMER Warnell et al. [2018] and Deep COACH Arumugam et al. [2019]. Loftin et al. Loftin et al. [2016] proposed methods that take teacher's strategy into account by inferring teacher's strategy, but most of the experiments are done in bandit domains. Compared to learning from demonstration methods, learning from feedback may require less knowledge, as judging good or bad actions is essentially easier than specifying the optimal action. However, to the best of our knowledge, all previous work on learning from feedback does not explicitly try to detect and correct errors. We propose a learning from feedback method that learns from noisy teacher feedback.\nNoise detection in supervised learning: Learning with noisy data has become increasingly chal-lenging with massive datasets and increasing the risk of noise from annotated data collection Song"}, {"title": "3 Background", "content": "Reinforcement Learning: RL is defined using a Markov Decision Process (MDP). An MDP is denoted by a quintuple as M = {S, A, T, r, y}, where S denotes the agent's state space, A is the agent's action space, T : S \u00d7 A \u00d7 S \u2192 [0, 1] is the environmental dynamics transition probability, r : S \u00d7 A \u00d7 S \u2192 R is the function that gives an immediate reward, y is a discount factor. In a typical MDP setting, the RL agent starts at state so and takes an action ao following its policy \u03c0, which leads the agent to its next state s\u2081 and receives reward ro. The interaction repeats for T steps until a terminal state is reached. RL agent optimizes the policy \u3160 by maximizing the expected cumulative reward \u0395\u03c4\u03c0[\u03a3=ort]\nDeep COACH: Feedback is often defined as a scalar value f, describing the teacher's judgment of the agent's current behavior, and the teacher can be a human teacher or a scripted teacher, Like COACH Warnell et al. [2018] and TAMER Knox and Stone [2009], we define feedback as f\u2208 {\u22121,1}, where \u20131 means the teacher discourages the agent's behavior, and 1 encourages the agent's behavior, which is a pair of state and action (st, at), at time step t. In COACH, the feedback ft is deemed as an estimate of the advantage value and is used to update the policy. In Deep-COACH Arumugam et al. [2019], the policy is updated as Equation 1:\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{a_t \\sim \\pi_\\theta(s_t)}[\\nabla_{\\theta} \\log(\\pi_\\theta(a_t|s_t)) \\cdot f_t]."}, {"title": "4 Classifier Augmented Noise Detecting and Relabelling COACH", "content": "4.1 Problem Statement\nGiven: An RL agent that has no access to the reward function r, a teacher T that can provide noisy binary feedback f \u2208 {\u22121,1} based on the learning agent's visited state and action; and the ground truth feedback f\u2208 {\u22121,1}.\nObjective: Train the agent policy \u03c0\u03bf by using the noisy feedback f given by the teacher.\nAssumptions: We make the following assumptions: (1) Static and symmetric noise is added to f to produce f (i.e., the noise distribution does not change over time and every feedback can be incorrectly flipped with equal probability. (2) The proportion of labels that are flipped (i.e., the noise proportion Pnoise) is known.\u00b9 (3) The ground truth feedback is deterministic for each state and action pair, i.e., only one correct feedback exists for each state-action pair. (4) The agent does not have access to the reward function during training (but may be used to evaluate the agent's policy).\n4.2 Methodology and Algorithm\nThe proposed framework Classifier Augmented Noise Detecting and RElabelling COACH (CANDERE-COACH) is illustrated in Figure 1. The agent follows the policy \u03c0\u03b8. The teacher provides noisy binary feedback f on the state-action pairs visited by the agent. This feedback, along with the corresponding state-action pair, is stored in a replay buffer. During training, a mini-batch is sampled and is input to a classifier, C (Section 4.2.1), which acts as a noise detector."}, {"title": "4.2.1 Noise-filtering classifier", "content": "In order to identify noisy feedback, we train a classifier C: S\u00d7A \u2192 [0, 1], 2 which maps the state and action pair to predicted feedback probability distribution. The feedback dataset D = {(si, ai, fi)}1 where (si, ai) refers to the ith state, action pair and fi refers to the corresponding observed feedback. This classifier is pretrained on a small noise-free feedback dataset D using the cross entropy loss. as shown in Eqn 2), where N refers to the size of the pretraining dataset, P(c = fi|si, ai, d) refers to the predicted probability of feedback by the classifier and c \u2208 {\u22121, +1} is the feedback class.\n1(\u03c6) = -\\sum_{i=1}^{N} \\sum_{c\\in{-1,1}} I[f_i = c] \\log P(f_i = C|S_i, a_i, \\phi)\nWe also use focal loss Lin et al. [2017] instead of cross entropy loss for pretraining datasets of imbalanced classes, as shown in Eqn 4.2. y is the focusing parameter and a : f \u2192 R is a map from a data class of input to a scalar weight, which is set based on the proportion of that class in the dataset.\n1(\\phi) = -\\sum_{i=1}^{N} \\sum_{c\\in{-1,1}} I[f_i = c] \\log P(f_i = c|S_i, a_i, \\phi)\\alpha(c)(1 \u2013 P(f_i = c|S_i, a_i, \\phi))^{\\gamma}"}, {"title": "4.2.2 Noise-detection", "content": "In this step, we detect potentially noisy feedback by using the trained classifier C. In every learning iteration, once a mini-batch is sampled from the replay buffer, Co predicts"}, {"title": "4.2.3 Active relabeling", "content": "Once suspected noisy batch Bn has been identified, we flip the feedback labels of the batch Bn resulting in Bar. The intuition is that the opposite feedback label is necessarily the correct label for binary valued feedback. We call this method active-relabelling. After data has been relabelled, Be and Bar are provided to the RL agent for training the policy \u03c0\u03bf as well as to the classifier C for online training as per Eqn 2. We use policy gradient to train the RL agent using the filtered feedback as per Eqn 1."}, {"title": "Algorithm", "content": "Algorithm 1 shows the algorithm for CANDERE-COACH. With a pretrained classifier C, the agent interacts with the environment and queries the teacher for feedback at a set frequency (line 5). The agent's observation, action, and feedback are stored as a tuple in the replay buffer R. After sampling a minibatch B from R, it is fed into the classifier to evaluate data point-wise cross entropy loss (line 8). These loss values are sorted in ascending order and the first R(B) B data-points in the batch are selected as Be (line 10). Similarly, R'(B)|B| data-points (denoted as Bn) are selected by sorting them per their cross entropy loss value (Equation 2) in descending order and the feedback label of this set is flipped (lines 11-12) as Bar. In the last step, lines 14\u201315, in addition to updating the policy \u03c0\u03b8, the classifier is also updated using online training with the filtered feedback batch (Be and Bar). The classifier gradually adapts to the new state distribution as in the training set and related ablation study can be found in Section 5.2."}, {"title": "5 Experimental evaluation and results", "content": "Research questions This section addresses the following three questions:\n1. In what kind of setup is the performance of Deep COACH sensitive to noisy feedback?\n2. Can CANDERE-COACH learn effectively with different proportions of noisy feedback?\n3. Does active relabelling help CANDERE-COACH in noise correction?\nDomains: We conduct our experiments in three Gymnasium Towers et al. [2023] domains: Cart Pole, Lunar Lander, and Minigrid Doorkey, as shown in Figure 2. In Cart Pole, the agent controls a moving cart to prevent the attached rod from falling. The Lunar Lander agent has to land a spacecraft on the moon, controlling three thrusters to avoid crashing. In Minigrid Door Key, the agent needs to explore to find a key, unlock the door, and reach the goal.\nEvaluation metrics: The agent is evaluated based on its accumulated reward on evaluation episodes by executing the current policy. During evaluation episodes, the agent's policy is frozen, hence no exploration. Recall that the agent does not have access to the reward signal \u2013 however, we use the default built-in reward function of our domains to evaluate the agent performance. We also use the % of correct feedback of the filtered data, namely the pure ratio to evaluate the algorithm's noise-filtering capability."}, {"title": "5.1 Results", "content": "When is Deep COACH sensitive to noisy feedback? To answer RQ1, we conduct experiments to evaluate Deep COACH with different proportions of noise. We evaluated Deep COACH with noise amounts of {0%, 10%, 20%, 30%, 40%} in the three domains. We consider both limited and unlimited feedback settings for this experiment. As shown in Figure 3(a) and Figure 3(b) for Cart Pole, we observe that higher noise leads to worse agent performance (less reward) and/or more time required to converge to optimal performance. Interestingly, with an unlimited budget, the agent is still able to learn with 40% feedback noise. Statistically, by the law of large numbers, as long as there are more correct labels than incorrect ones and unlimited feedback data, the agent eventually learns the correct policy given infinite amounts of feedback and learning time. However, for limited budget feedback, as shown in Figure 3(b), the agent performance significantly deteriorates and even unlearns over time. Similar results can also be found in the other two domains (Lunar Lander and Minigrid Doorkey), shown in Appendix Section A.\nTo summarize, we address RQ1 by showing that noisy feedback poses a significant negative impact on Deep COACH, especially in a limited feedback setting."}, {"title": "CANDERE-COACH evaluation", "content": "To answer RQ2 and RQ3, we evaluate the performance of the original CANDERE-COACH with a limited feedback budget based on the performance of Deep COACH from the previous experiments. We also evaluate CANDERE-COACH without active relabelling, denoted as CANDERE-COACH (w/o AR)."}, {"title": "5.2 Ablation studies", "content": "We conduct additional ablation studies to understand the components and effects of hyperparameters on the performance of CANDERE-COACH.\nEffect of online training on noise-filtering classifier: Online training of the classifier tries to balance two problems. First, the state action distribution can shift (relative to the data used to pretrain the classifier), suggesting online training will help. Second, trying to update the classifier with noisy labels could hurt performance, suggesting online training will not help. We denote the CANDERE-COACH without online training and active relabelling as CANDERE-COACH (w/o AR, w/o OT). Figure 6 shows that for CANDERE-COACH (w/o AR, w/o OT), the pure ratio reduces over time, suggesting that as the agent explores different areas of the state-action space, the distribution shift leads to worse classifier performance. In contrast, online training allows the pure ratio to gradually increase. Eventually, the pure ratio stabilizes around 95% and as a result, CANDERE-COACH (w/o AR) is able to learn robustly against 30% noise with pretraining dataset of size 25. To summarise,"}, {"title": "Effect of noisy pretraining Dataset", "content": "In prior sections, CANDRE-COACH was allowed access to a small noise-free feedback dataset for pretraining the classifier. In this section, we invalidate this assumption by testing CANDERE-COACH with a noisy pretraining dataset. As observed in Figure 5, CANDERE-COACH can still perform well under low amounts of noise, such as 10% and 20% (see Figures 5(a) and Figure 5(b)), while CANDERE-COACH (w/o AR) cannot outperform our baseline. However, if the noise level is too high (30%), as shown in Figure 5(c), we observe that the performance of CANDERE-COACH begins to degrade. Also, pretraining with a noisy dataset reduces the overall performance compared to the performance reported in previous sections, where a noise-free pretraining dataset is available. To summarise, CANDERE-COACH can work with noisy pretraining dataset but it only shows promising results under reasonably low noise levels like 10% or 20% noise."}, {"title": "5.3 Extending to CANDERE-TAMER", "content": "There is no fundamental challenge to expand our proposed de-noising mechanism to other learning from feedback algorithms. Here, we present CANDERE-TAMER, which is similar to CANDERE-COACH but built based on Deep TAMER Warnell et al. [2018]. The results are shown in Figure 7. We observe a similar pattern in which our algorithms outperform our baselines (Deep TAMER and Deep TAMER (Preload)) and learn successfully against 30% noise, while the performance of baselines degrades poorly with noise. In 40% noise, CANDERE-TAMER still shows a better average return and outperforms our baselines; however, its performance is significantly worse than that with 30% noise. This experiment shows the potential of our approach to be used as a plug and play tool inside any human-in-the-loop RL algorithm, which will be explored as future work."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, a new algorithm is proposed inside the learning from feedback RL framework. Through experiments in multiple settings and tasks, we show that the CANDERE-COACH is able to handle up to 40% noise, with a small noise-free feedback dataset, outperforming the baselines. We also show that if the noise is small enough, our CANDERE-COACH can also work with a noisy pretraining dataset. Besides Deep COACH, we further show that the proposed noise detection mechanism can be extended to Deep TAMER. For future work, we intend to expand this framework to learning from preferences algorithms and also test its performance with different types of noise, such as non-symmetric and feature-dependent noise, as well as conducting a human subject study."}, {"title": "A Deep COACH with noise", "content": "We also conducted experiments to study the impact of noise on our baseline (Deep COACH) in other domains. In Door Key, noticeably, the performance of COACH seems less impacted with a limited budget with a smaller average return drop, shown in Figure 8. This is mainly due to the unique settings in Door Key. In Door Key, the agent needs to grab the key, unlock the door, and reach the destination. An evaluative reward of positive +1 discounted by time steps is given when it reaches the destination. Even a randomly initialised agent can reach the destination and achieve a nonzero return at step 0, which is different from Cart Pole and Lunar Lander, where a bad policy results in low episodic returns. Therefore, the performance in Door Key seems to receive less negative impact with a limited budget. However, we still observe a drop in performance. For example, in 40% noise, with an unlimited feedback budget, the return reaches up to 0.4 after 50k steps, while with a limited budget it converges around 0.2.\nFor Lunar Lander, we observe the same trends as Cartpole, as shown in Figure 9; the agent perfor-mance for unlimited feedback setting decreases as noise % increases. However, the agent is still able to learn, though not as good as learning from clean feedback (0% noise). With limited feedback, the agent's learning performance deteriorates significantly and the agent eventually unlearns over time."}, {"title": "B CANDERE-COACH with unlimited budget", "content": "In this section, we evaluate the performance of the original CANDERE-COACH with an unlimited budget. We conducted experiments with different sizes of pretraining datasets for the classifier.\nAs can be seen from Figure 10, CANDERE-COACH is able to outperform when we have a pretraining data size of 30, against 40% of noise. Though CANDERE is able to learn faster than the baseline, with an unlimited budget of feedback, both the COACH and CANDERE is able to learn and reach almost perfect performance eventually. Furthermore, when the classifier does not have enough data for pretraining and cannot select correct labels for agent updates, the performance will be downgraded to a very low level. Here, to conclude, with a small amount of feedback dataset, CANDERE-COACH is able to outperform Deep COACH by learning faster. But with an unlimited budget of feedback, even our baseline is able to learn well against high noise."}, {"title": "C More ablation study on online training", "content": "Online training does not always show such a great improvement when the noise level increases. The results with 40% noise can be found in Figure 12. We also observe that the pure ratio under extremely high noise is reduced and unstable, which differs from the pattern seen in 30% noise, which shows that the online training mechanism tends to perform worse under very high noise. However, its performance remains better than CANDERE-COACH without online training both in average return (shown in Figure 12(b)) and pure ratio (shown in Figure 12(a))."}, {"title": "D Ablation study on pretraining sizes", "content": "As shown in Figure 13, our method can learn well against 30% noise, while Deep COACH shows highly unstable performance. In fact, even with different amounts of preloaded pure dataset, under the same settings, Deep COACH shows a significantly different learning curve due to the very unpredictability of the noise. Furthermore, it is revealed in Figure 14 that a classifier pretrained with datset of size 25 barely filters noise successfully as the pure ratio hovers around 70% under 30% noise, while the other two classifiers successfully improve the pure ratio up to 80% and 85%. We also tested CANDERE-COACH on 40% noise, as shown in Figure 15. With extremely high"}, {"title": "E CANDERE-COACH in other noise levels", "content": "We also conduct experiments in lower noise level like 20%, the results are shown in Figure 18. With lower noise, in Cart Pole and Door Key, the performance difference is less significant because our baseline Deep COACH (Preload) can also perform well. However, Lunar Lander is our hardest domain, and we still observe a similar pattern in 20% noise, where CANDERE-COACH shows best performance and reaches 200 in episodic average return, while CANDERE-COACH (w/o AR) and baselines fail to do so."}, {"title": "F Study on feedback budget", "content": "We conduct most of our experiments with a limited budget. As different budgets can lead to different results, the budget for experiments is chosen based on such criteria: the budget should at least allow the agent to solve the task with 0% noise, while the agent may fail with higher noise levels.\nA study on budget's influence on Deep COACH with a noise-free budget of 30 is shown in Figure 19. A budget of 1000 allows the agent to reach very close to maximum episodic return smoothly, while the performance drops as noise increases. With a smaller budget like 750, the agent fails to do so and hence we choose 1000 as the budget for Cart Pole in our experiments for a comparison between Deep COACH and CANDERE-COACH."}, {"title": "G Scripted teacher", "content": "Our scripted teacher is trained with PPO Schulman et al. [2017] following hyperparameters of RL Zoo Raffin [2020]. More details are described in the following subsections for each domain."}, {"title": "G.1 Cart Pole", "content": "In Cart Pole, the expert is trained with PPO with hyperparameters shown in Table 1."}, {"title": "G.2 Door Key", "content": "In Door Key, the expert is trained with PPO with hyperparameters shown in Table 2. Furthermore, Minigrid Doorkey is set to be fully observable and we use a CNN-based feature extractor to reduce the observation space to 5."}, {"title": "G.3 Lunar Lander", "content": "In Lunar Lander, the expert is trained with PPO with hyperparameters shown in Table 3."}, {"title": "H Collecting the pretraining dataset", "content": "The pretraining dataset is collected with our scripted teacher to label with state-action pairs positive or negative feedback. We first collect states following a certain distribution to ensure better coverage of the state space. Then we label the states and optimal actions to be positive and the collected dataset will be randomly shuffled. Lastly, we practice data augmentation by labelling all the nonoptimal actions to be negative. The details of the state sampling distribution of each domain are described in the following subsections."}, {"title": "H.1 Cart Pole", "content": "In Cart Pole, the state is sampled uniformly in a clipped observation space. The state space of Cart Pole consists of four dimensions and two of them are not bounded. Therefore, we properly choose a suitable clip range to sample the states. We set the sampling range of position and velocity based on the fact that an episode will be terminated if the cart leaves the (-2.4, 2.4) range. Details can be seen in Table 4."}, {"title": "H.2 Door Key & Lunar Lander", "content": "In Door Key and Lunar Lander, the dataset is sampled from trajectories following a sampling policy that takes expert action by 50% chance and random action by 50% chance. The reason for this is different for these domains. In Door Key, we cannot practice uniform sampling in the RGB image array space. In Lunar Lander, the observation space is significantly larger and if we practice uniform sampling, most sampled states will never be visited by the agent and therefore bring low performance of the classifier."}, {"title": "I COACH Hyperparameters", "content": "In this section, we show the hyperparameters used in our experiments for CANDERE-COACH and Deep COACH. Deep COACH shares the same hyperparameters with CANDERE-COACH if applicable."}, {"title": "I.1 Cart Pole", "content": "The hyperparameters in Cart Pole can be seen in Table 5."}, {"title": "1.2 Door Key", "content": "The hyperparameters in Door Key can be seen in Table 6."}, {"title": "1.3 Lunar Lander", "content": "The hyperparameters in Lunar Lander can be seen in Table 7."}, {"title": "1.4 Summary on variations of CANDERE-COACH", "content": "We summarise the aforementioned CANDERE-COACH and its variations' domain-wise performance and required amount of pretraining dataset size, as shown in Table 8."}]}