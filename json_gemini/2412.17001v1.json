{"title": "SOLVING NONLINEAR ENERGY SUPPLY AND DEMAND SYSTEM USING PHYSICS-INFORMED NEURAL NETWORKS", "authors": ["Van Truong Vo", "Samad Noeiaghdam", "Denis Sidorov", "Aliona Dreglea", "Liguo Wang"], "abstract": "Nonlinear differential equations and systems play a crucial role in modeling systems where time-dependent factors exhibit nonlinear characteristics. Due to their nonlinear nature, solving such systems often presents significant difficulties and challenges. In this study, we propose a method utilizing Physics-Informed Neural Networks (PINNs) to solve the nonlinear energy supply-demand (ESD) system. We design a neural network with four outputs, where each output approximates a function that corresponds to one of the unknown functions in the nonlinear system of differential equations describing the four-dimensional ESD problem. The neural network model is then trained and the parameters are identified, optimized to achieve a more accurate solution. The solutions obtained from the neural network for this problem are equivalent when we compare and evaluate them against the Runge-Kutta numerical method of order 4/5 (RK45). However, the method utilizing neural networks is considered a modern and promising approach, as it effectively exploits the superior computational power of advanced computer systems, especially in solving complex problems. Another advantage is that the neural network model, after being trained, can solve the nonlinear system of differential equations across a continuous domain. In other words, neural networks are not only trained to approximate the solution functions for the nonlinear ESD system but can also represent the complex dynamic relationships between the system's components. However, this approach requires significant time and computational power due to the need for model training.", "sections": [{"title": "1 Introduction", "content": "Differential equations and their systems are powerful mathematical methods used to model problems across various real-world fields such as physics, engineering, economics, healthcare, energy, and others [1, 2, 3]. In the energy sector, the development of mathematical models for managing efficient energy supply and demand plays a crucial role in the flexible energy community with energy storage systems and renewable energy generation [4]. One notable application of differential equation systems is modeling the ESD system, which describes the dynamic relationship between energy supply, demand, and the distribution of energy across different regions based on development indicators of each area. The behavior of these variables is modeled through a system of differential equations, as formulated by Mei Sun et al"}, {"title": "2 Problem description", "content": "The four-dimensional ESD system is a complex model that describes the time-varying relationship between energy supply and demand, as well as the distribution of energy between different regions, E and F. This dynamic relationship is represented by the following system of differential equations [5]:\n\n$x1'(t) = a1x1(t)(1-\\frac{x1(t)}{M}) - a2(x2(t) + x3(t)) \u2013 d3x4(t)$\n$x2'(t) = -z1x2(t) \u2212 z2x3(t) + z3x1(t)[N \u2212 (x1(t) \u2212 x3(t))]$\n$x3'(t) = 81x3(t)(s2x1(t) - 83)$\n$x4'(t) = d1x1(t) - d2x4(t)$\n\nwhere: $x1(t)$ is a function representing the time-varying energy resource demand of region F, $x2(t)$ is a function representing the time-varying energy resource supply from region E to region F, $x3(t)$ is a function representing the time-varying energy resource imports of region F, and $x4(t)$ is a function representing the time-varying renewable energy resources of region F. $ai, di, zi, Si, N, M > 0$ are positive constants and $N < M$. With the coefficients $a\u2081 = 0.09$, $a2 = 0.15, z1 = 0.06, z2 = 0.082, z3 = 0.07, S\u2081 = 0.2, S2 = 0.5, S3 = 0.4, M = 1.8, N = 1, d\u2081 = 0.1, d2 = 0.06, d3 = 0.08$ and the initial conditions $x\u2081(0) = 0.82, x2(0) = 0.29, x3(0) = 0.48, x4(0) = 0.1$. The system (1) is in a chaotic state [5, 7].\nOur objective is to determine the time series values described by the energy supply-demand problem by solving the system of differential equations (1) under chaotic conditions with the provided parameters."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Deep learning neural networks:", "content": "A deep neural network is characterized by an architecture consisting of multiple interconnected layers of neurons, where the connections between these layers are represented by a set of weights. A typical architecture of a deep neural network generally includes three main layers [26]: The input layer, which is the first layer that receives the data. In most problems, the input data can be features of the objects or the data to be computed. In this case, the input data consists of different time value points that need to be calculated. The hidden layer performs complex computations, including nonlinear operations through activation functions [27], to learn and extract features from the data. The output layer contains the results to be predicted or calculated; In this problem, these are the solutions that need to be found for the system of differential equations describing the four-dimensional energy supply-demand system. During the training phase, the neural network's weights are gradually adjusted through the training iterations to minimize the loss function [28], which quantifies the error between the network's output and the constraints from the system of differential equations or the desired output values. The neural network accomplishes this through two processes known as forward propagation [29], where data is transmitted from the input layer through the hidden layers and finally to the output layer, the second process called backward propagation [30], which occurs when the network has computed the errors of the loss function. The network will make adjustments and update the weights to minimize the error value by calculating the Gradient Descent and using optimization algorithms such as [26, 27, 28, 29, 30]: Stochastic Gradient Descent (SGD), RMSprop, Adam, or LBFGS..etc."}, {"title": "3.1.1 The generalized model of a neural network:", "content": "Consider a neural network where each hidden layer is denoted as L, the v-th (v \u2265 1; v \u2208 Z) hidden layer is denoted as $L(v)$. Let the number of nodes in the v-th hidden layer be denoted as $n(v)$.\nLet the weight matrix between layer $L(2-1)$ and layer $L(v)$ be denoted as $W(v)$ ( the matrix $W(v)$ will have dimensions \u03b7(v-1) \u00d7 \u03b7(v) ), where each element $W_{ij}$ of the weight matrix represents a connection from the i-th node (1 \u2264 i \u2264 n(v\u22121)) of layer $L(v\u22121)$ to the j-th node (1 \u2264 j \u2264 n(v)) of layer $L(v)$.\nLet $b(v)$ be a one-dimensional vector with n(v) elements, where the vector $b(v)$ includes a set of bias values for each node in layer $L(v)$\nEach node in the neural network is designed to perform two calculations as follows [23, 28, 30]:\n* The first operation:\n\n$Z_j^{(v)} = \\sum_{i=1}^{\u03b7(v-1)} a_i^{(v-1)} \\times W_{ij}^{(v)} + b_j^{(v)},$\n\nwhere: $Z_j^{(v)}$ is the linear sum of the product of the output values of the nodes in layer $L^{v\u22121}$ and their corresponding weights to the j-th node of layer $L(v)$, plus the bias term of the node being considered.\n$W_{ij}^{(v)}$: is the weight connecting the i-th node of layer $L(v\u22121)$ to the j-th node of layer $L(v)$.\n$b_j^{(v)}$: is the bias value of the j-th node in $L(v)$.\n* The second operation uses nonlinear activation functions:\n\n$a_j^{(v)} = \u03c3(Z_j^{(v)}),$\n\nwhere: $a_j^{(v)}$ is the output value of the j-th node in layer $L(v)$\n\u03c3: is an activation function\nIn neural networks, activation functions o play an important role as they help represent nonlinear relationships between the nodes of the neural network [26, 29]. The Sigmoid, Tanh, ReLU, and Softmax functions [27, 28, 29, 30] are a few examples of frequently used nonlinear activation functions. Each function is appropriate for the particular needs of various issues. In the experimental part of this study, we used the tanh (Hyperbolic Tangent) activation function. The following is the formula for the tanh function, which transforms the input variables into nonlinear values within the"}, {"title": "3.1.2 The process of optimizing the parameters of a neural network:", "content": "The updating of weights in the neural network is performed through the backpropagation process, which consists of two main steps. In the first step, neural networks compute the partial derivatives of the loss function with respect to each weight within the network. This operation is computed backward from the output layer to the input layer using the chain rule [26, 27]. Considering a specific layer:\n\n$\\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial a_{j}} \\cdot \\frac{\\partial a_{j}}{\\partial z_{j}} \\cdot \\frac{\\partial z_{j}}{\\partial w_{ij}},$\n\nwhere:\n$\\frac{\\partial L}{\\partial w_{ij}}$ is the partial derivative of the loss function with respect to the i-th weight of neuron j.\n$\\frac{\\partial L}{\\partial a_{j}}$ is the partial derivative of the loss function with respect to the output (according to the activation function) at the node with the weight being considered.\n$\\frac{\\partial a_{j}}{\\partial z_{j}}$ is the derivative of the output value at the j-th node with respect to the sum function $z_{j}$.\n$\\frac{\\partial z_{j}}{\\partial w_{ij}}$ is the derivative of the sum $z_{j}$ at the j-th node with respect to the weight being considered.\n\nThe second step in updating the weights of the neural network is based on the Gradient Descent optimization algorithm. The weights of the network will be updated according to the following formula [26, 29, 30]:\n\n$W_{update} = W_{old} - \u03b7 \\frac{\\partial L}{\\partial w},$\n\nwhere: w is the weight to be determined in the network,\n\u03b7 is learning rate,\n$\\frac{\\partial L}{\\partial w}$ is the partial derivative of the loss function concerning the weight.\n\nThe updating of bias values is performed in the same manner as for weights. The process of updating the weights and biases of the neural network is repeated multiple times across each epoch until the loss function decreases to a desired value or until convergence is achieved [26, 29]."}, {"title": "3.2 Physics-Informed Neural Networks (PINNS)", "content": "PINNs are a unique class of neural networks designed by combining traditional neural networks and physical or mathematical models. Its main idea is that physical constraints and conditions are directly integrated into the neural network through the loss function [22, 23, 24, 25]. To handle and compute complex derivative operations integrated into the loss function of PINNs, we use Automatic Differentiation [15, 18, 20], which is a powerful technique that allows for the computation of high-order and complex derivatives.\nThe process of building and training a PINNs to solve a system of differential equations is carried out through the following main steps:\nConstructing and designing the neural network: Construct a neural network with input and output layers adapted to each specific problem to be solved, in which each unknown function in the system of differential equations is approximated by the respective outputs of the neural network. The architecture, including the number of hidden layers, the nodes per layer, and their activation functions, is defined and optimized to obtain solutions that satisfy the system of equations.\nDetermining physical constraints: The differential equation system, initial conditions, and boundary conditions are used to integrate into the neural network's loss function as constraints.\nOptimizing the loss function: Select and use appropriate optimization algorithms to minimize the loss function.\nPredicting the solution: A trained neural network can be used to compute and predict the solution of the differential equation system."}, {"title": "4 Model Building", "content": "In this study, we propose a solution based on the concept of PINNs, designing a deep neural network to solve the nonlinear system of differential equations that describes the ESD system (1) through four main steps:\nStep 1: A neural network for this problem is designed with 4 outputs. In this study, we describe it as a mathematical function NN(t, Wb), this function, or in other words, this neural network, is dependent on two variables: the time variable t represents the input data of the neural network, and Wb represents the set of weights, biases, and parameters that need to be determined for the neural network. Each unknown solution function of the nonlinear differential equation system (1) describing the 4-dimensional ESD system is approximated by a corresponding output of the network through the training process. These neural network outputs constitute a vector function described as follows:\n\nOutPut(NN(t,Wb)) = [X\u2081(t, Wb), X2(t, Wb), X3(t, Wb), X4(t, Wb)]\n\nThe objective is to find the parameters Wb such that the values of the functions generated by the neural network satisfy the following condition:\n\nX1(t, Wb) \u2248 x1(t), X2(t, Wb) \u2248 x2(t), X3(t, Wb) \u2248 x3(t), X4(t, Wb) \u2248 x4(t).\n\nStep 2: Define the time domain for computation as t \u2208 [a, b], divide this domain into N consecutive points with different values of t (to < t1 < t2 < ... < tN-1 where to = a, tn\u22121 = b). These values are subsequently input into the neural network for training and calculation.\nStep 3: Determine the constraints and design the loss function\nThe constraints satisfying the mathematical conditions of the nonlinear differential equation system (1) and the initial conditions are incorporated into the loss function.\na) The constraints are integrated into the loss function to satisfy the differential equation system defined as follows:\n\n$LosSX1_eq = \\sum_{i=1}^{N} ||X1'(ti, Wb) \u2013 [a1X1(ti, Wb)(1 \u2013 \\frac{X1(Wb)}{M}) - a2(X2(ti, Wb) + X3(ti, Wb)) \u2013 d3X4(t_{i}, Wb)]||^2$\n$Loss X2_eq = \\sum_{i=1}^{N} ||X2' (ti, Wb) \u2013 (-21X2(ti, Wb) \u2013 22X3(ti, Wb) + 23X1(ti, Wb)[N - (X1(ti, Wb) \u2013 X3(ti, Wb))])||^2$\n$Loss X3_eq = \\sum_{i=1}^{N} ||X3' (ti, Wb) - [81X3(ti, Wb) (82X1(ti, Wb) \u2013 83]||^2$\n$Loss X4_eq = \\sum_{i=1}^{N} ||X4' (ti, Wb) \u2013 [d1X1(ti, Wb) \u2013 d2X4(ti, Wb)]||^2$\n\nwhere Loss X1_eq, LosSX2_eq, Lossx3_eq, and Loss X4_eq are constraint functions that measure the error for the four output values of the neural network, satisfying the mathematical conditions of the nonlinear differential equation system (1). The objective is to ensure that, when the solutions generated by the neural network are substituted into the system of equations (1), the Mean Squared Error (MSE) [26] between the left and right sides of the equations is minimized as much as possible.\nb) The constraints are integrated into the loss function to satisfy the initial conditions defined as follows:\n\n$Lossinitial = \\sum_{i=1}^{4} ||Xi(tinitial, Wb) \u2013 Xi(tinitial)||^2$\n\nwhere: Lossinitial is a constraint function measuring the error between the neural network's output values and initial conditions of the system. This function is computed based on the squared error between the solutions generated by the neural network when the variable t is at the initial time point and the given initial condition values of the functions to be determined in the system of equations. The objective is also to ensure that this error value is minimized.\ntinitial: is the value of the time variable at the initial time point.\nXi(tinitial, Wb): is the value of the i-th output of neural network when the variable t is at the initial time point."}, {"title": "c) Define the total loss function for the neural network:", "content": "Losstotal = a(LossX1_eq + LosSX2_eq + LosSX3_eq + LosSX4_eq) + \u1e9eLosSinitial\nwhere: a and \u03b2 are real-valued parameters, selecting and adjusting these parameters appropriately will enable the model to focus on higher-priority conditions, thereby improving convergence speed and accuracy.\nStep 4: Optimization algorithms are used to train the deep learning network to find the best parameters of the model in order to minimize the total loss function. In this study, we utilize the Adam optimization algorithm integrated within the TensorFlow library [27, 28]."}, {"title": "5 Results and Evaluation", "content": "In this study, we conducted an experiment to construct a neural network with an architecture consisting of an input layer that receives different time data points, 16 hidden layers with 100 neurons each, and an output layer with 4 neurons. The neurons in the output layer represent the time-dependent values of the four functions to be determined in the system of equations (1). We set a = 10, \u03b2 = 1 and used the Adam optimizer [28, 29] to minimize the loss function. The time interval t = [0, 100] was divided into N = 20,000 equally spaced time data points. For the numerical method, in this study, we use the SciPy library [31] to employ the RK45 method [9, 32], which combines the fourth-order and fifth-order Runge-Kutta formulas to achieve high accuracy and efficiency. This method allows for the adaptive adjustment of step size to meet the required accuracy while optimizing computational time. We solved system (1) using the RK45 numerical method with an absolute tolerance of 1 \u00d7 10-06 and a relative tolerance of 1 \u00d7 10-03, over the same time domain with N time points, as used by the neural network method. The solutions from both methods were then compared and evaluated. When using the neural network method, we applied a learning rate schedule, where the initial learning rate was set to 8 \u00d7 10-05, and gradually reduced over the training epochs, with the minimum learning rate being 1 \u00d7 10-06. After 175,000 epochs, Figure 2 shows the value of the loss function over the training epochs. We observed that the neural network method, with a simple network architecture in our experiment, began to provide more accurate solutions than the RK45 method for all four solutions sought, as presented in Table 1 and illustrated in Figure 3."}, {"title": "6 Conclusion", "content": "In this study, we proposed a method using PINNs to solve a system of nonlinear differential equations describing the ESD system. Experimental results indicate that the PINNs method is a novel and effective approach. PINNs, a type of neural networks, present several distinct advantages, such as providing solutions over a continuous domain and the ability to leverage the computational power of modern computers. The experimental results in this study show that PINNs achieve solutions comparable to the RK45 method. Furthermore, this approach demonstrates outstanding potential. According to general methods for improving and developing deep learning models, the performance of PINNs can be enhanced by adjusting the network architecture to be more complex (such as increasing the number of hidden layers and neurons in each layer), selecting or developing suitable optimization functions, as well as increasing the data and training time. These factors will help the model learn more complex representations, playing a crucial role in fully harnessing the power of PINNs. However, this comes at the cost of greater computational time and the need for a sufficiently powerful computing system. Moreover, ensuring the stability and convergence speed of the model is a significant challenge. Overall, although this is a promising approach with high applicability in solving the nonlinear ESD system, there remain significant challenges, particularly in enhancing model stability, optimizing convergence speed, and reducing computational power requirements. Therefore, this research topic warrants further investigation in the future."}]}