{"title": "Deep Frequency Derivative Learning for Non-stationary Time Series Forecasting", "authors": ["Wei Fan", "Kun Yi", "Hangting Ye", "Zhiyuan Ning", "Qi Zhang", "Ning An"], "abstract": "While most time series are non-stationary, it is inevitable for models to face the distribution shift issue in time series forecasting. Existing solutions manipulate statistical measures (usually mean and std.) to adjust time series distribution. However, these operations can be theoretically seen as the transformation towards zero frequency component of the spectrum which cannot reveal full distribution information and would further lead to information utilization bottleneck in normalization, thus hindering forecasting performance. To address this problem, we propose to utilize the whole frequency spectrum to transform time series to make full use of data distribution from the frequency perspective. We present a deep frequency derivative learning framework, DERITS, for non-stationary time series forecasting. Specifically, DERITS is built upon a novel reversible transformation, namely Frequency Derivative Transformation (FDT) that makes signals derived in the frequency domain to acquire more stationary frequency representations. Then, we propose the Order-adaptive Fourier Convolution Network to conduct adaptive frequency filtering and learning. Furthermore, we organize DERITS as a parallel-stacked architecture for the multi-order derivation and fusion for forecasting. Finally, we conduct extensive experiments on several datasets which show the consistent superiority in both time series forecasting and shift alleviation.", "sections": [{"title": "1 Introduction", "content": "Time series forecasting has been playing an important role in a variety of real-world industries, such as traffic analysis [Ben-Akiva et al., 1998], weather prediction [Lorenz, 1956], financial estimation [King, 1966; Ariyo et al., 2014], energy planning [Fan et al., 2024a], etc. Following by classic statistical methods (e.g., ARIMA [Whittle, 1963]), many deep machine learning-based time series forecasting methods [Salinas et al., 2020; Han et al., 2024; Zhang et al., 2023] have recently achieved superior performance in different scenarios. Despite the remarkable success, the non-stationarity widely existing in time series data has still been a critical but under-addressed challenge for accurate forecasting [Priestley and Rao, 1969; Huang et al., 1998; Brockwell and Davis, 2009].\nSince time series data are usually collected at a high frequency over a long duration, such non-stationary sequences with millions of timesteps inevitably let forecasting models face the distribution shifts over time. This would lead to performance degradation at test time [Kim et al., 2021] due to the covariate shift or the conditional shift [Woo et al., 2022a].\nFor this issue, pioneer works [Ogasawara et al., 2010] propose to normalize time series data with global statistics; one recent work [Kim et al., 2021] proposes to use instance statistics to normalize time series against distribution shifts. Then, some work brings statistical information into self-attention computation [Liu et al., 2022b]; another work [Fan et al., 2023] transform time series with learnable statistics and consider shifts input and output sequences, and [Liu et al., 2023] utilize predicted sliced statistics for adaptive normalization.\nMost of these existing works focus on transforming each timestep of time series with certain statistics (usually mean and std.) After our careful theoretical analysis, we have found that these operations can actually be regarded as the normalization towards the zero frequency component of the spectrum in the frequency domain, as shown in Figure 1(a). However, they cannot fully utilize distribution information of time series signals; moreover, this would lead to information utilization bottleneck in normalization and thus hinders the performance of time series forecasting. To address this problem, we propose to utilize the whole frequency spectrum for"}, {"title": "2 Related Work", "content": "Time series forecasting is a longstanding research topic. Traditionally, researchers have proposed statistical approaches, including exponentially weighted moving averages [Holt, 1957] and ARMA [Whittle, 1951]. Recently, with the advanced development of deep learning [Chen et al., 2023; Fan et al., 2020; Pu et al., 2023; Ning et al., 2021; Chen et al., 2024; Fan et al., 2021; Pu et al., 2022; Ning et al., 2022], many deep time series forecasting methods have been developed, including RNN-based methods (e.g., deepAR [Salinas et al., 2020], LSTNet [Lai et al., 2018]), CNN-based methods (e.g., SCINet [Liu et al., 2022a], TCN [Bai et al., 2018]), MLP-based Methods (e.g., DLinear [Zeng et al., 2022], N-BEATS [Oreshkin et al., 2020]) and Transformer-based methods (e.g., Autoformer [Wu et al., 2021], PatchTST [Nie et al., 2023]). While time series are non-stationary, existing works try normalize time series with global statistics [Ogasawara et al., 2010], instance statistics [Kim et al., 2021], learnable statistics [Fan et al., 2023] and sliced statistics [Liu et al., 2023] in order to relieve the influence of distribution shift on forecasting. Other works bring time-index information [Woo et al., 2022a] or statistical information into network architectures [Liu et al., 2022b; Fan et al., 2024b] to overcome the shifts."}, {"title": "2.2 Frequency Analysis in Time Series Modeling", "content": "The frequency analysis has been widely used to extract knowledge of the frequency domain in time series modeling and forecasting. Specifically, SFM [Zhang et al., 2017] adopts Discrete Fourier Transform to decomposes the hidden state of time series by LSTM into frequency components; StemGNN [Cao et al., 2020] adopts Graph Fourier Transform to perform graph convolutions and uses Discrete Fourier Transform to computes series-wise correlations. Autoformer [Wu et al., 2021] replaces self-attention in Transformer [Vaswani et al., 2017] and proposes the autocorrelation mechanism implemented by Fast Fourier Transform. FEDformer [Zhou et al., 2022] introduces Discrete Fourier Transform-based frequency enhanced attention by acquiring the attentive weights by frequency components and then computing the weighted sum in the frequency domain. In addition, [Woo et al., 2022b] transforms hidden features of time series into the frequency domain with Discrete Fourier Transform; [Fan et al., 2022] uses Discrete Cosine Transform"}, {"title": "3 Problem Formulation", "content": "Let $x = [x_1;x_2;\u00b7\u00b7\u00b7 ; x_T] \\in \\mathbb{R}^{T \\times D}$ be regularly sampled multi-variate time series with T timestamps and D variates, where $x_t \\in \\mathbb{R}^{D}$ denotes the multi-variate values at timestamp t. In the task of time series forecasting, we use $X_t \\in \\mathbb{R}^{L \\times D}$ to denote the lookback window, a length-L segment of x ending at timestamp t (exclusive), namely $X_t = X_{t-L:t} = [x_{t-L}; x_{t-L+1};\u00b7\u00b7\u00b7; x_{t-1}]$. Similarly, we represent the horizon window as a length-H segment of x starting from timestamp t (inclusive) as $Y_t$, so we have $Y_t = X_{t:t+H} = [x_{t}; x_{t+1}; \u00b7\u00b7\u00b7 ; x_{t+H-1}]$. The classic time series forecasting formulation is to project lookback values $X_t$ into horizon values $Y_t$. Specifically, a typical forecasting model $f_{\\Theta} : \\mathbb{R}^{L \\times D} \\rightarrow \\mathbb{R}^{H \\times D}$ produces forecasts by $Y_t = f_{\\Theta}(X_t)$ where $\\hat{Y}_t$ stands for the forecasting results and $\\Theta$ encapsulates the model parameters.\nIn this paper, we aim to study the problem of non-stationarity in deep time series forecasting. As aforementioned in Section 1, long time series with millions of timesteps let forecasting models face distribution shifts over time due to the non-stationarity. The distribution shifts in time series forecasting are usually the covariate shift [Wiles et al., 2021; Woo et al., 2022a]. Specifically, given a stochastic process, let $p (x_t, X_{t-1},..., x_{t-L+1})$ be the unconditional joint distribution of a length L segment where $x_t$ is the value of univariate time series at timestamp t. The stochastic process experiences covariate shift if any two segments are drawn from different distributions, i.e. $p (X_{t-L}, X_{t-L+1}...,X_{t-1}) \\neq p (X_{t'-L}, X_{t'-L+1},...,x_{t'-1}),\\forall t \\neq t'$. Subsequently, let $p (x_t | X_{t-1},..., x_{t-L})$ represents the conditional distribution of $x_t$, such a stochastic process experiences conditional shift if two segments have different conditional distributions, i.e. $p(x_t | X_{t-1},...,X_{t-L+1},X_{t-L}) \\neq p(x_{t'} | X_{t'-1},..., X_{t'-L+1}, X_{t'-L}),\\forall t \\neq t'$."}, {"title": "4 Methodology", "content": "In this section, we elaborate on our proposed deep frequency derivative learning framework, DERITS, designed for non-stationary time series forecasting. First, we introduce our novel reversible transformation, Frequency Derivative Transformation (FDT) in Section 4.1. Then, to fuse multi-order information, we present the parallel-stacked frequency derivative learning architecture in Section 4.2. Finally, we introduce our Order-adaptive Fourier Convolution Network (OFCN) for frequency learning in Section 4.3."}, {"title": "4.1 Frequency Derivative Transformation", "content": "As aforementioned in Section 1, to fully utilize the whole frequency spectrum for the transformation of time series with sufficient distribution information, we propose the novel Frequency Derivative Transformation (FDT) to achieve more stationary frequency representations of time series signals. For this aim, FDT mainly includes two distinct stages respectively for domain transformation and frequency derivation.\nIn the first stage, to be specific, we make use of fast Fourier transform [Nussbaumer and Nussbaumer, 1982] to enable the decomposition of time series signals from the time domain into their inherent frequency components. Formally, given the time domain input signals X(t), we convert it into the frequency domain by:\n$X(f) = \\mathcal{F}(X(t)) = \\int_{-\\infty}^{\\infty} X(t)e^{-j2\\pi ft} dt$\n$=\\int_{-\\infty}^{\\infty} X(t) cos(2\\pi ft)dt + j \\int_{-\\infty}^{\\infty} X(t) sin(2\\pi ft)dt,$\n(1)\nwhere $\\mathcal{F}$ is the fast Fourier transform, f is the frequency variable, t is the integral variable, and j is the imaginary unit, defined as the square root of -1; $\\int X(t) cos(2\\pi ft)dt$ is the real part of X and is abbreviated as Re(X); $\\int X(t) sin(2\\pi ft)dt$ is the imaginary part and is abbreviated as Im(X). After that we can rewrite X as $X = Re(X) + jIm(X)$.\nIn the second stage, with the transformed frequency components, we propose to utilize the whole frequency spectrum for the signal derivation, in order to represent time series in a more stationary space. The basic idea is to perform our proposed Fourier Derivative Operator in the frequency domain, which is defined as follows:\nGiven the time domain input signals X(t) and its corresponding frequency components X(f), we then define $\\mathcal{R}(X(f)) := (j2\\pi f)X(f)$ as the Fourier Derivative Operator (FDO), where f is the frequency variable and j is the imaginary unit.\nIn the derivation, different order usually represents different signal representations. We propose to incorporate multi-order information in DERITS to further enhance the forecasting. For this aim, we extend above definition and further define the k-order Fourier Derivative Operator $R_k$ as:\n$\\mathcal{R}^k(X(f)) = (j2\\pi f)^kX(f)$.\n(2)\nWith such two stages, we can finally write the k-order Frequency Derivation Transformation FDTk as:\n$FDT_k(X(t)) = (j2\\pi f)^k\\mathcal{F}(X(t))$\n(3)\nwhere X (t) is the time domain input signal; $\\mathcal{F}$ stands for fast Fourier transform and f is the frequency variable.\nGiven X(t) in the time domain and X(f) in the frequency domain correspondingly, the k-order Fourier Derivative Operator on X(f) is equivalent to k-order derivation on X(t) with respect to t in the time domain, written by:\n$(j2\\pi f)^kX(f) = \\mathcal{F}( \\frac{d^k}{dt^k}X (t)),$\n(4)\nwhere $\\mathcal{F}$ is Fourier transform, $\\frac{d^k}{dt^k}$ is k-order derivative with respect to t, and j is the imaginary unit."}, {"title": "4.2 Frequency Derivative Learning Architecture", "content": "The main architecture of DERITS is depicted in Figure 2, which is built upon the Frequency Derivative Transformation and its inverse for the frequency derivative learning.\nAs mentioned in Section 4.1, DERITS needs to conduct predictions in a more stationary frequency space achieved by frequency derivative transformation. We naturally need to recover the predictions back to the time domain for final forecasting and evaluation. To make FDT fully reversible, we let both stages of FDT reversible, including Fourier transform and Fourier Derivation. Specifically, following Equation (3), we can symmetrically write the inverse frequency derivative transformation (iFDT) of k order as:\n$iFDT(\\hat{X}(f)) = \\mathcal{R}_k^{-1}(\\hat{X}(f)) = \\mathcal{F}^{-1}(\\frac{1}{(j2\\pi f)^k}(\\hat{X}(f))),$\n(5)\nwhere $\\mathcal{R}_k^{-1}$ is the inverse process of Fourier Derivative Operator of k order; $\\mathcal{F}^{-1}$ is the inverse Fourier transform; $\\hat{X}(f)$ is the frequency components that need to be recovered to the time domain. Actually, the inverse process $\\mathcal{R}_k^{-1}$ is equivalent to an integration operator in the time domain.\nTo conduct the multi-order frequency derivation transformation and learning, we have organized our DERITS framework as a parallel-stacked architecture, where each branch represents an order of frequency derivation learning, as shown in Figure 2. Let DERITS have K branches in total. For each branch, we first take lookback values $X_t$ to frequency derivative transformation by:\n$X_t^k = FDT_k(X_t), k = 1,2,..., K$\n(6)\nwhere $FDT_k$ is the k-order FDT and $X_t^k$ is the frequency derivative representation for $X_t$ at timestamp t. Then, the learned representations for each branch are taken to the Fourier Convolution Network (FCN) for frequency dependency learning. Since our FCN is order-adaptive in each parallel branch, we also take k as input with the computation by:\n$\\hat{H}_t^k = \\text{Order-adaptiveFourierConvolution}(k, X_t^k)$\n(7)\nwhere $\\hat{H}_t^k$ are the predicted frequency components for $X_t^k$. Note that FourierConvolution is not parameter-sharing for different branches. After that, we recover the predictions to the time domain by:\n$\\hat{H}_t^k = iFDT(\\hat{H}_t^k), k = 1,2,..., K$\n(8)\nwhere $\\hat{H}_t^k$ is the recovered representation of k order in the time domain. After acquiring it, we finally fuse the multi-order representations from parallel branches for the forecasting with MLP layers, which is given by:\n$\\hat{Y}_t = \\text{MultilayerPerceptron}(\\hat{H}_t^1, \\hat{H}_t^2,......,\\hat{H}_t^K)$\n(9)\nwhere $\\hat{Y}_t$ are forecasting results by DERITS for evaluation."}, {"title": "4.3 Order-adaptive Fourier Convolution Network", "content": "Apart from the frequency derivative transformation, another aim of DERITS is to accomplish the dependency learning with the derived signals in the frequency domain. We thus introduce a novel network architecture, namely Order-adaptive Fourier Convolution Network (OFCN) to enable the frequency learning. Specifically, OFCN is composed of two important components, i.e., order-adaptive frequency filter and Fourier convolutions, which are illustrated as follows:\nWe aim to fuse multi-order derived signal information for forecasting, while it is notable that different order corresponds to different frequency comments. Since time series include not only meaningful patterns but also high-frequency noises, we develop an order-adaptive frequency filter to enhance the learning process.\nSupposing there are S frequencies in $X_t^k$, we sort $X_t^k$ on the frequencies in a descending order of amplitude for each frequency to get $X_t^k$ for k order. Then, we design an adaptive mask $m_k$ to concentrate on only $\\frac{S}{2^{(K-k)}}$ frequency components of $X_t^k$ for further learning. We write the adaptive frequency filtering process by:\n$\\hat{H}'_t^k = m_k \\odot v_k \\odot X_t^k$\n$m_k = \\underbrace{[1,\\cdots,1,\\overbrace{0,\\cdots, 0]}^S}_{S/2^{(K-k)}} \\odot v_k$\n(10)"}, {"title": "5 Experiments", "content": "In this section, in order to evaluate the performance of our model, we conducts extensive experiments on six real-world time series benchmarks to compare with the state-of-the-art time series forecasting methods."}, {"title": "5.1 Experimental Setup", "content": "We follow previous work [Wu et al., 2021; Zhou et al., 2022; Nie et al., 2023; Yi et al., 2023b] to evaluate our DERITS on different representative datasets from various application scenarios, including Electricity [Asuncion and Newman, 2007], Traffic [Wu et al., 2021], ETT [Zhou et al., 2021], Exchange [Lai et al., 2018], ILI [Wu et al., 2021], and Weather [Wu et al., 2021]. We preprocess all datasets following the recent frequency learning work [Yi et al., 2023b] to normalize the datasets and split the datasets into training, validation, and test sets by the ratio of 7:2:1.\nWe conduct a comprehensive comparison of the forecasting performance between our model DERITS and several representative and state-of-the-art (SOTA) models on the six datasets, including Transformer-based models: Informer [Zhou et al., 2021], Autoformer [Wu et al., 2021], FEDformer [Zhou et al., 2022], PatchTST [Nie et al., 2023]; MLP-based model: LSTF-Linear [Zeng et al., 2023]; Frequency domain-based model: FreTS [Yi et al., 2023b]. Besides, we also consider the existing normalization methods to-"}, {"title": "5.2 Overall Performance", "content": "To verify the effectiveness of DERITS, we conduct the performance comparison of multivariate time series forecasting in several benchmark datasets. In brief, the experimental results demonstrate that DERITS achieves the best performances in most cases as shown in Table 1. Quantitatively, compared with the best results of transformer-based models, DERITS has an average decrease of more than 20% in MAE and RMSE. Compared with more recent frequency learning model, FreTS [?] and the state-of-the-art transformer model, PathchTST [Nie et al., 2023], DERITS can still outperform them in general. This has shown the great potential of DERITS in the time series forecasting task."}, {"title": "5.3 Comparison with Normalization Techniques", "content": "In this section, we further compare our performance with the recent normalization technique, RevIN [Kim et al., 2022] and Dish-TS [Fan et al., 2023] that handle distribution shifts in time series forecasting. Table 2 has shown the performance comparison in time series forecasting taking the LTSF-Linear [Zeng et al., 2022]. Since FDT transforms signals to the frequency domain, we implement a simple single-layer Linear model in the frequency domain. From the results, we can observe that the existing RevIN and Dish-TS can only improve the backbone in some shifted datasets. In some situations, it might lead to worse performances. Nevertheless, our FDT can usually achieve the best performance. A potential explanation is that FDT transforms data with full frequency spectrum and thus achieves stable improvement while other normalization techniques cannot reveal full data distribution and thus cannot make use of them for transformation."}, {"title": "5.4 Model Analysis", "content": "It is notable that our proposed FDT plays an important role in DERITS, and we aim to analysis the impact of FDT on the model performance. Thus, we consider a special case of FDT, which is when we set k = 0, the derivation is removed and FDT is degraded to naive Fourier transform. In addition to this setting, we also vary different orders (k) of derivation to test the effectiveness. The superior efficiency metrics and the reduced parameter counts achieved by our model position it as a compelling choice for non-stationary time series forecasting."}, {"title": "5.5 Visualization Analysis", "content": "To study the Fourier derivative transformation in DERITS, we visualize the original signals and derived signals for comparison. Since the direct outputs of FDT are complex values that are difficult to visualize completely, we thus transform the derived frequency components back to the time domain via inverse FDT, which allows us to show the corresponding time values for visualization. Specifically, we choose two non-stationary time series from the Weather dataset and Exchange dataset, respectively. As shown in Figure 5, we can observe the original signals have included obvious non-stationary oscillations and trends. In contrast, the derived signals exhibit a larger degree of stationarity compared with raw data. This further reveals that learning in the derivative representation of signals is more stationary and thus can achieve better performance.\nTo further analysis the model performance, we carry out the case study for non-stationary time series forecasting. As shown in Figure 6, it becomes evident that DERITS can be capable of aligning with the ground truth when the time series distribution is largely shifted, while the baseline method deviates from the true values. The visualizations demonstrates the model's adaptability to shifts."}, {"title": "6 Conclusion Remarks", "content": "In this paper, we propose to address non-stationary time series forecasting from the frequency perspective. Specifically, we utilize the whole frequency spectrum for the transformation of time series in order to make full use of time"}]}