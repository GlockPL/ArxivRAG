{"title": "EmoPro: A Prompt Selection Strategy for Emotional Expression in LM-based Speech Synthesis", "authors": ["Haoyu Wang", "Chunyu Qiang", "Tianrui Wang", "Cheng Gong", "Qiuyu Liu", "Yu Jiang", "Xiaobao Wang", "Chenyang Wang", "Chen Zhang"], "abstract": "Recent advancements in speech synthesis models, trained on extensive datasets, have demonstrated remarkable zero-shot capabilities. These models can control content, timbre, and emotion in generated speech based on prompt inputs. Despite these advancements, the choice of prompts significantly impacts the output quality, yet most existing selection schemes do not adequately address the control of emotional intensity. To address this question, this paper proposes a two-stage prompt selection strategy EmoPro, which is specifically designed for emotionally controllable speech synthesis. This strategy focuses on selecting highly expressive and high-quality prompts by evaluating them from four perspectives: emotional expression strength, speech quality, text-emotion consistency, and model generation performance. Experimental results show that prompts selected using the proposed method result in more emotionally expressive and engaging synthesized speech compared to those obtained through baseline. Audio samples and codes will be available at https://whyrrrrun.github.io/EmoPro/.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, language models (LMs) like GPT [1] have achieved significant success in the field of Natural language processing. Inspired by this, LMs have also become a main- stream framework in the speech synthesis domain, exemplified by systems like VALL-E [2] and SPEARTTS [3]. The quality of synthesized speech has now reached a level comparable to human speech. LM-based TTS systems utilize neural audio codecs [4]\u2013[7] to convert speech into discrete tokens, which encapsulate extensive information about the speech. These systems then employ a language model architecture to autore- gressively generate subsequent speech tokens. Existing LM- based TTS models implement incontext learning capabilities. Current LM-based TTS methods [7]\u2013[9] employ autore- gressive generation to produce subsequent tokens from input prompts and text. These advanced TTS methods achieve zero- shot voice cloning with just a few seconds of prompt speech. However, the quality of the prompts significantly influences the generated speech output, impacting aspects such as timbre, perceptual quality, and emotional expression [10], [11]. Consequently, selecting an appropriate prompt is crucial [12]\u2013[14]. There are two mainstream methods for prompt selection: 1) Random: randomly choosing speech from a specific speaker with a certain emotional speech, or 2) Text- based Methods [15], [16]: selecting prompts based on the simi- larity between the synthesized text and prompt text. However, these methods are primarily designed for general scenarios and face limitations in emotional speech synthesis. Random selection often fails to provide rich emotional information and expressive capabilities, and focusing solely on the text can yield subpar emotional performances, as there's frequently a weak connection between the text and the desired emotion [17], [18]. Therefore, additional research is required to identify prompts that can enhance emotional expressiveness, speaker similarity, and stability across various LM-based methods in emotional speech synthesis scenarios [19], [20]. To tackle these challenges, we propose an innovative two- stage prompt selection strategy EmoPro. In the static se- lection stage, we evaluate both the inherent emotional quality of the prompt candidates and their specific expressive power within the model. In the dynamic selection stage, we choose the most semantically relevance and contextually appropriate prompts from the candidates after static selection stage, based on the synthesized text. This strategy aims to systematically screen and rank prompts based on various metrics, ultimately selecting prompts with strong emotional expressiveness, high speaker similarity, and high stability. The specific contributions of this paper are as follows:\n1) We propose a two-stage emotion prompt selection strat- egy EmoPro, which combines static-dynamic selec- tion for LM-based TTS.\n2) We conduct a multi-perspective analysis about the text and speech of the prompt, taking into account the ability of prompt in specific methods as well as the emotional quality of the prompt itself."}, {"title": "II. METHOD", "content": "The EmoPro we propose is illustrated in Fig. 1, and it consists of two stages: static and dynamic selection. In the static selection stage, we select prompt candidates based on emotional expressiveness, perceptual quality, and textual emotional coherence. The selected candidates are then used for inference with the LM-based TTS methods. The objective metrics are used to evaluate candidates and retain those with high quality, expression, and stability. In the dynamic selection stage, we identify the prompt with the highest semantic relevance to the target text input, choosing from the previously filtered candidates. Finally, this prompt is the one that best reflects the required emotional effect of the synthesized text under the current model."}, {"title": "B. Static Selection", "content": "For prompt static selection, we evaluate the quality of the prompt speech across three key dimensions: pitch, DNSMOS [21], and textual emotional coherence derived from a large language model [22]. Additionally, we assess the inference results of the prompt candidates, considering metrics such as character error rate (CER), emotion similarity, and speaker similarity. By integrating these factors, we identify the prompt candidates deemed most suitable for the emotion.\n1) Pitch: The pitch, or fundamental frequency, is a prever- bal feature that imparts tonal and rhythmic qualities to speech [23]. As a suprasegmental speech feature, pitch conveys infor- mation over a longer time scale than segmental features such as spectral envelopes. Features describing overall attributes of the pitch contour, such as mean and variance, are more emotionally resonant than those describing the pitch shape itself, such as slope, curvature, and inflection [24]. a) Mean: This refers to the average pitch level over a period of speech. It can indicate the general tone or mood of the speaker. b) Variance: This measures the variability in pitch over time. Greater variance might suggest more animated or emotional speech, while less variance could indicate a monotone delivery. Different emotional states are associated with distinct pitch patterns [25]. Both sadness and comfort exhibit relatively low mean and variance in pitch, indicating calmer and lower pitch characteristics, with sadness being slightly more subdued. On the other hand, emotions like happiness and surprise demonstrate higher mean and variance, reflecting more pro- nounced emotional intensity [26]. We select the prompt speech based on the distinct tonal features associated with each emotion category. Initially, we calculated the mean and variance for each emotion type. Subsequently, we apply the K-Means algorithm [27] to cluster 10 groups based on the mean and variance of the prompt candidates for different speakers and emotions. We select $m$ clusters with stronger or weaker means and variances based on the various states of different emotional classes.\n2) Perceptual and Textual Selecting: We comprehensively consider both perceptual quality and text consistency. DNSMOS: We regard the quality of the prompt speech as a critical factor and utilize DNSMOS [21] for this purpose. DNSMOS is a deep learning-based audio quality assessment tool designed to evaluate the quality of audio signals. It can assess the clarity, naturalness, and overall quality of audio. By leveraging neural network models to simulate human auditory perception, it provides objective scores that are highly correlated with subjective ratings. We measure DNSMOS on all results following pitch selection. Textual Emotional Coherence: When the text of speech aligns more closely with a particular emotional expression, the sentence can more effectively convey the desired emotion. To assess the relevance of the text to the corresponding emotion in the prompt speech, we use the ChatGPT [22] API. First, we input a text and its corresponding emotion to establish a benchmark for the model's judgment. This benchmark is subsequently used as a prompt for further assessments to ensure consistency in the model's evaluation standards. We compute the textual emotional coherence for all pitch-filtered prompt texts. We add the textual emotional coherence scores and DNS- MOS scores together to select out the Top $n%$ as the most emotionally expressive data.\n3) Selecting with Performance under LM-based TTS Method: The method above focuses on the selection method for evaluating the quality of the prompt speech itself. Addi- tionally, we recognize that even when identical prompt speech is input into different methods, the resulting outputs can vary significantly. This variability primarily depends on factors such as the selection of speech tokens. To address this question, we propose a strategy that considers the specific performance of different models when processing the same prompt speech. Specifically, we select 20 descriptive neutral texts for infer- ence based on the prompt candidates from our prompt speech quality selection process. We then evaluate the inference results for all prompt speeches by calculating the CER of the synthesized speech. Furthermore, we use Resemblyzer [28] and WavLM [29] to evaluate speaker similarity and assess the model's capability to generate the same speaker's voice from the given prompt. Finally, we employ the emotion2vec [30] model to assess the emotion similarity between the synthesized speech and the prompt speech, which serves as an indicator of the model's effect in capturing the emotional information of the prompt speech. The three metrics of CER, speaker similarity, and emotion similarity form the framework for assessing our model's effect in capturing various speech information. In our selection strategy, we prioritize the quality of the prompt candidates themselves. We start with an initial selection of their intrinsic emotional quality before applying the model-specific selection method."}, {"title": "C. Dynamic Selection", "content": "The consistency between the prompt text and the target text also affects the results, so we employ a dynamic selection strategy based on the text. The stsb-distilroberta-base\u00b9 ana- lyzes the currently synthesized text alongside the statically selected speeches from the prompt candidates. This allows us to identify the most relevant prompt for the current text, which is then chosen as the final prompt."}, {"title": "III. EXPERIMENTS", "content": "We use a private emotional prompt dataset employed in [31] comprising two men and two women to validate our prompt selection strategy. The dataset includes five distinct emotions: comfort, happy, sad, anger, and surprised. Each speaker exhibits four of these emotions, and 200 data samples for each emotion result in 800 data samples per speaker."}, {"title": "B. Compared Methods", "content": "To verify the effectiveness of our approach, we compare the following strategies for selecting prompt speech: 1) Random: We randomly select from all prompts as the prompt choice. 2) Text-based Methods: We achieve the selection by performing semantic similarity analysis between the synthetic text and the prompt text [16], using all-MiniLM-L6-v2\u00b2 (MiniLM) [32] to implement the prompt selection."}, {"title": "C. Test Metrics", "content": "For our subjective evaluation, we select 20 native judges. For main method comparison, we provid 10 different sentences for each emotion of data, and for other ablation experiment, we use 5 of the 20 descriptive neutral texts mentioned in II-B3. The test metrics used in the subjective evaluation are as follows:\n\u2022 Emotion MOS (MOS): This metric evaluates the quality and emotional expression of the synthesized speech.\n\u2022 Strength Perception (SP): A subjective strength percep- tion test. The judger is asked to rate the emotion strength on a scale from 0 to 1. The object evaluation metrics include speaker similarity, emotion similarity (ES), character error rate (CER). Resemb and WavLM are calculated via cosine similarity between speaker representations of the target and generated speech using Resemblyzer [28] and WavLM [29], while ES uses cosine similarity between emotion2vec [30] representations. CER compares the target text with Paraformer [33] output."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "In our experiments, we validate our method in several ways. Firstly, to demonstrate the necessity of the prompt model performance module, we use two different prompt-based TTS models for validation. The experimental results are shown in Table I. We use data from female speaker 1's happy emotions as input to complete the prompt selection on both the Cosy Voice and GPT-SOVITS models. The results show that different prompt speeches are obtained for the same data under different models, indicating that the same prompt can have varying effects depending on the model used. Additionally, we find that the synthesis effect of CosyVoice significantly outperforms GPT-SOVITS in terms of speaker similarity and emotion similarity. This is primarily because CosyVoice in- corporates an ASR-supervised tokenizer along with additional speaker x-vector inputs. Therefore, all subsequent experiments are primarily conducted using the Cosy Voice model."}, {"title": "B. Range of Prompt Selection", "content": "We conduct experiments on the range of data selected at each stage, including the selection of $m,n,k$, and other variables under different conditions. The specific results are shown in Table III. From the table, we can clearly observe that as the degree of selection of the pitch clusters increases ($m$ decreases), the emotional impact of the prompts also gradually enhances. The results indicating that emotion similarity and strength perception increase as $n$ decrease highlight the effec- tiveness of using DNSMOS and textual emotional coherence to prompt selection. Considering the limited number of prompt speeches, the prominence of the prompter's emotional effect, and the uncertainty of text content during inference, we ultimately choose the parameters $m = 3, n = 15$, and $k = 5$."}, {"title": "C. Importance of Quality Selecting", "content": "Table IV presents the results of our experiments on the prompt speech quality module. We employ an inverse selection strategy compared to EmoPro to finish these experiments. Specifically, we select the $m$ clusters that performed the worst after pitch clustering, along with the Bottom $n%$ of data based on DNSMOS scores and textual emotional coherence weighted results. Finally, we compare the performance of the Top $k$ prompts candidates through prompt model performance respectively. The results indicate that EmoPro successfully selects emotionally expressive prompts."}, {"title": "D. Importance of Model Performance Selecting", "content": "Table V shows the results of the experiments on the prompt model performance module, after the prompt speech quality selected using the positive selection of EmoPro, we compare the performance of the Top $k$ and Bottom $k$ prompts can- didates to validate the role of prompt model performance module. The experimental results indicate that the module significantly enhances the user's listening experience."}, {"title": "E. Comparison with Baseline Methods", "content": "We choose Cosy Voice and GPT-SOVITS as the main models to compare with other baseline methods, and the specific experimental results are shown in Table II, which show that we have achieved far better experimental results than baseline by taking into full consideration of the quality of the prompt itself, its performance under different models, and by analysing the relevance between synthesized text and the prompt text."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we propose EmoPro, a novel two-stage emo- tion prompt selection strategy that evaluates both the emotional quality of prompts and their generation performance. EmoPro also performs dynamic prompt selection based on the input text to select the most relevant prompt among the emotional prompt candidates. The experiments show that, compared to the baseline methods, the speech generated using the prompt selection strategy proposed in this paper demonstrates ad- vantages in emotional expressiveness, perceptual quality, and content accuracy. In the future, we will further explore prompt selection strategies across other dimensions and try to apply them to various tasks such as text-to-audio."}]}