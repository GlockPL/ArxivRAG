{"title": "Patchview: LLM-Powered Worldbuilding with Generative Dust and Magnet Visualization", "authors": ["John Joon Young Chung", "Max Kreminski"], "abstract": "Large language models (LLMs) can help writers build story worlds by generating world elements, such as factions, characters, and locations. However, making sense of many generated elements can be overwhelming. Moreover, if the user wants to precisely control aspects of generated elements that are difficult to specify verbally, prompting alone may be insufficient. We introduce Patchview, a customizable LLM-powered system that visually aids worldbuilding by allowing users to interact with story concepts and elements through the physical metaphor of magnets and dust. Elements in Patchview are visually dragged closer to concepts with high relevance, facilitating sensemaking. The user can also steer the generation with verbally elusive concepts by indicating the desired position of the element between concepts. When the user disagrees with the LLM's visualization and generation, they can correct those by repositioning the element. These corrections can be used to align the LLM's future behaviors to the user's perception. With a user study, we show that Patchview supports the sensemaking of world elements and steering of element generation, facilitating exploration during the worldbuilding process. Patchview provides insights on how customizable visual representation can help sensemake, steer, and align generative AI model behaviors with the user's intentions.", "sections": [{"title": "1 INTRODUCTION", "content": "Rapid progress in the development of generative large language models (LLMs) [8, 14] has recently led to the introduction of numerous LLM-based tools for storywriting [1, 17, 34, 61]. While many of these tools aim to generate text for direct inclusion in a finished story, opportunities also lie in using LLMs to support other aspects of the writing process, such as worldbuilding. Worldbuilding-the act of constructing a coherent fictional world [24]-establishes a setting from which a variety of stories could arise. It requires writers to envision myriad aspects of a world, from abstract values (e.g., religion, ideology) to more specific elements, such as factions, characters, places, or props. As worldbuilding involves creating many different world elements, writers often put a lot of time and effort into it. Generative LLMs could be used to support this process, for instance by producing additional world elements that fit into the established setting or even inspire writers to expand their conception of the world they are creating.\nHowever, when generating many world elements with LLMs, understanding their overall landscape can be challenging. That is, to unfold a story where different elements interact with each other, the writers would need to have a holistic view regarding what kind of attributes and values those elements hold. As LLMs can quickly add many elements to the world, it can be challenging for a writer to understand the rapidly growing world. Moreover, once the writer has understood existing world elements, they might want to generate a specific type of world elements. One way to guide LLMs for such a purpose would be to write natural language prompts. However, if the writer wants to express verbally elusive or ambiguous concepts, writing natural language prompts can either be cumbersome [49] or have limited expressivity [15].\nTo support sensemaking and steering of world element generation, we propose generative dust and magnet (GD&M) visualization, which adapts dust and magnet visual representation [11, 60] to the use of generative models. GD&M visualizes elements as \u201cparticles of iron dust\" which are attracted to different concepts, or \"magnets\u201d, based on their relevance to each concept (i.e., placed more closely if more relevant) (Figure 1a). This approach supports flexible visualization of semantic association between concepts and elements with an arbitrary number of concepts, by leveraging intermediate spaces between extreme \"anchoring\" concepts. Moreover, spaces between concepts can be used for guiding generation, even allowing expression of ambiguity between concepts (Figure 1b). When the user disagrees with steered generation and recognition results, the user can straightforwardly correct them by simply moving dust particles to other positions (Figure 1c). With repositioning, the user can indicate the generated element's correct placement (Figure 1c1) or command AI to revise the element to fit in the new position (Figure 1c2). These corrections can feed back into the AI as examples of the user's perspective for future steering and recognition."}, {"title": "2 RELATED WORK", "content": "2.1 Worldbuilding\nWorldbuilding is a process of architecting fictional worlds that can be cornerstones of narrative fiction [24]. It considers various aspects, such as places, characters, or even cultures, and well-constructed worlds add believability to the stemming narrative stories. A well-built story world also entertains readers, as readers build out the conception of a coherent world out of various stemming stories [21, 39]. With a story world, readers can also participate in active consumer experiences, such as creating fan fiction and even transforming the canon world into alternative worlds [21, 46]. While worldbuilding can be a complex process with many aspects to consider, there have been practical frameworks and structures that practitioners use. Practitioners would likely first focus on the frameworks of the world, which can include scope (geography of the world), sequence (temporal history of the world), and perspective (from whom the world is explained) [24]. Under such frameworks, practitioners would create structures of the world. Governance (e.g., government presence, rule of law), economics (e.g., economic strength, wealth distribution), social relations (e.g., class, race, and ethnic relations), cultural influences (e.g., religious influences, cultural influences), and character alignments (e.g., good-evil, lawful-chaotic) [24] are some examples of world structures. Then, around frameworks and structures, practitioners would create catalogs of fictional worlds, or elements of the world, such as characters, places, props, and events [24]. Worldbuilding can be done either solely (e.g., Tolkien's world of Lord of Rings) or collectively (e.g., Marvel Universe), and commercial projects often tend to be collaborative as doing creative work can be overloading to an individual. WorldSmith is one of the few AI-powered tools"}, {"title": "2.2 AI-Powered Story Writing", "content": "With advancing AI technologies, researchers and practitioners have developed many tools to support story writing. For example, TaleStream supports story ideation by showing potentially inspiring story tropes [13]. Loose Ends is a rule-based mixed-initiative Al system that allows users to explore plot threads with some constraints [31]. Portrayal leveraged NLP and visualizations to help writers analyze characters in their stories [25]. LLM's advanced generative capabilities introduced tools that suggest texts that users can incorporate into their writing [9, 19, 35]. Researchers investigated diverse interactions for such tools, from allowing distinct suggestion operations [61] to incorporating multimodality [23], hierarchical generation [40], and sketching inputs [17]. With these rapidly advancing capabilities, researchers also studied story writer's expectations for these technologies, such as what they would take as a benefit and what they want [7, 22, 29]. Lee et al. [34] reflected on the design space of writing tools through a literature survey. LLMs also enabled story applications where the story is generated with minimal writer interventions, directly facing the audience [44, 55]. While many LLM-powered story writing tools focused on supporting prose text writing, some focused on other types of support. For example, CALYPSO leverages LLMs to provide support to dungeon masters when playing Dungeons & Dragons [63]. In a similar vein, we design Patchview to provide LLM-powered support in worldbuilding, which is other than writing story texts themselves."}, {"title": "2.3 Visually Interacting with Generative AI", "content": "While natural language-based interfaces (e.g., prompts, chat) have been widely used for generative AI models, many previous systems used visual interactions to complement natural language interactions. Some tools leverage node-based input interactions to control generation, such as chaining subtasks [4, 6, 30, 58]. Among them, ChainForge [6] and Cells-Generators-Lenses framework [30] also allowed evaluation of generated results with visualization nodes. While these tools allowed flexible control, steering and evaluation happened in separated interfaces, leading to visual complexity. As another type, Scenescape [51] and Graphologue [28] leveraged graph and tree visualization to help understand complex information. While the user can steer further generations by clicking on the node which the user is willing to learn more details about, these focus more on presenting information than allowing flexible steering. Some tools allow steering or evaluation of multiple generation results on dimensional spaces of attributes, represented in either sliders [38], mixed color spaces [15], temporal line drawing [17], or scatter plots organized in grids [50]. Among them, TaleBrush [17] and Luminate [50] tied steering and evaluation interactions on a single visual representation, minimizing clutters. TaleBrush considers a continuous dimensional scale but on a fixed attribute. On the other hand, Luminate allows arbitrary dimensional attributes but only with categorical/ordinal attributes. Moreover, all aforementioned tools do not allow users to correct AI behaviors when Al's steered generation and recognition results do not align with the user's thoughts. Patchview extends previous work by allowing generation steering, evaluation, and user corrections on an integrated single visual representation with the flexibility of allowing continuous scales of any arbitrary concepts of interest."}, {"title": "3 GENERATIVE DUST AND MAGNET", "content": "Patchview's central design metaphor-generative dust and magnet (GD&M)-leverages a dust and magnet (D&M) visual representation [60] to facilitate interaction with generative AI models. In this section, we first describe settings where GD&M can be helpful (Section 3.1). Then, we describe the original D&M visualization and how we translate its components for use with generative models (Section 3.2). Finally, we describe specific GD&M interactions that close gaps in the interactive alignment of AI models: evaluation support, specification alignment, and process alignment [52] (Section 3.3)."}, {"title": "3.1 Need for Generative Dust and Magnet", "content": "Interaction with generative AI might benefit from a wide range of different interaction approaches in different settings. In general, we expect GD&M interaction to be most effective when the user must generate many distinct units of output (e.g., storyworld elements) that vary along diverse and expressive conceptual dimensions. Breaking this ideal setting down further, we arrive at a set of three conditions that typify good application domains for GD&M interaction.\nFirst, the user must make use of generative models to gather a collection of many generated outputs. This imposes a need for sensemaking (N1), as understanding how outputs distribute along the user's conceptual dimensions of interest is difficult due to the large scale of generation.\nSecond, the user must have desires to create artifacts within their unique characteristics and values, which often occurs in artistic creation [16]. This imposes a need for configurability (N2), where behaviors of Al functions (e.g., generation and evaluation of generated results) consider the user's unique styles and interests.\nThird, the user must need to express nuanced specifications that align generation with the user's specific intentions and facilitate exploration of subtly different options. This imposes a need for expressivity (N3) where the user can guide generation even with subtle intentions.\nGD&M interaction would be ideal for user tasks with the above characteristics. Worldbuilding meets all of these conditions: the writer must create many world elements to fill out a unique and idiosyncratic world, and created elements can have nuanced differences between each other [24]. In the following sections, we describe how GD&M can fulfill the aforementioned needs."}, {"title": "3.2 From D&M Visualization to GD&M", "content": "Yi et al.'s original dust and magnet visualization represents individual data elements as \u201cdust particles\" while representing each variable for which data elements can possess different values as a \"magnet\". Both dust particles and magnets are rendered as glyphs"}, {"title": "3.3 Specific GD&M Interactions", "content": "Several specific GD&M interactions are designed to meet user needs discussed in Section 3.1. We organize these interactions in terms of how they support interactive alignment of AI models [52]. Extending challenges of the gulf of evaluation and execution [42], Terry et al. [52] emphasized three facets of interactive alignment of AI models: 1) evaluation support (I1), or users making sense of AI outputs; 2) specification alignment (I2), or users efficiently and reliably communicating their objectives to AI; and 3) process alignment (13), or users verifying or controlling Al's execution process."}, {"title": "3.3.1 11: Evaluation Support - User Configurable Dust and Magnet Visualization (N1, N2)", "content": "To support users' sensemaking of many generated elements according to their concepts of interest, the user can add generated elements to the magnet space configured with concepts of the user's interest (Figure 3-I1). Then, an Al model measures the relevance of each element to different concepts and"}, {"title": "3.3.2 12: Specification Alignment - Generation Specifications within Magnet Space (N2, N3)", "content": "GD&M interaction also allows users to guide the generation of new elements by indicating the ideal placement of these elements within the visual-semantic magnet space defined by a set of user-configured concepts. That is, the user can place a marker on the magnet space to request the generation of elements"}, {"title": "3.3.3 13: Process Alignment - Correcting Al on Magnet Space (N2)", "content": "Al behaviors may not always align with user intent: for instance, the user might not agree with how the AI interprets concepts during generation and placement of generated elements. In such cases, the user can freely re-specify concepts to more accurately convey how they think about each concept (e.g., adding more details about what \"good\" means in a specific story world, Figure 4c). They can also leverage the magnet space itself to correct AI behavior, by simply moving a misplaced generated element to wherever the user thinks it should be in the magnet space (Figure 1c and 3-I3). Repositioning an element can convey two intentions: either 1) that the element's \"correct\u201d placement is in a new position (e.g., indicating that the character should sit in the middle of \"good\" and \"evil\" as in Figure 1c1) or 2) that the element should be revised to better fit the indicated position (e.g., request AI to rewrite the character description to sit in the middle of \"good\u201d and \u201cevil\u201d, as in Figure 1c2). These corrections can then be used as examples to better align future generation and placement with user perception of concepts."}, {"title": "4 PATCHVIEW: INTERFACE AND TECHNICAL DETAILS", "content": "With GD&M, we built Patchview, an LLM-powered tool for world element creation (Figure 5). Specifically, Patchview supports sensemaking and steering of world element generation. To demonstrate the effectiveness of GD&M for sensemaking and steering, Patchview focuses on creating initial \"seeds\" of story world elements in two to three sentences. Afterward, users can develop details of these seed elements either by themselves or with the help of AI; the final rendering of seed elements into a more complete form is left to future work.\nPatchview's user interface consists of the list module, which shows existing world elements as a list of notes (Figure 5b), and the view module, which organizes world elements via GD&M (Figure 5a). Note that Patchview leverages AI to generate specific world elements (e.g., characters, places) rather than generating frameworks or structures of the world (e.g., ideology, values). The user can manually specify frameworks and structures as open-ended text in notes.\nWe explain the envisioned usage pattern with a hypothetical user, Alex. Alex is a game scenario writer who is trying to design a story world for the new game her team is developing. To get help with the process, Alex decides to use Patchview."}, {"title": "4.1 List Module", "content": "As Alex loads Patchview, she first sees the list module on the right. With this module, Alex can generate and create an initial set of world elements as textual notes. To set an initial high-level concept for the world, Alex decides to manually create a note by clicking"}, {"title": "4.2 View Module", "content": "4.2.1 Creating and Configuring View (11). To make sense of this proliferation of world elements, Alex decides to use the view module to organize them. In Patchview, a view is a single GD&M visual-semantic space that organizes world elements in relation to a specific set of user-defined concepts. Alex can create a new view by first clicking the View button in the bottom left corner and then clicking the + button. The user can set the concept associated with each magnet in the view either by dropping existing notes into placeholder magnets (i.e., using elements as concepts, Figure 7a) or clicking the \"Type in a new magnet\" button that shows up when the user hovers their mouse close to the placeholder or existing concepts (Figure 7b). Once Alex configures the view with a set of concepts, she adds relevant elements as dust particles in the view by dragging and dropping elements from the list module to the target view. As Alex adds an element to the view, Patchview calculates its position within the view visualization space. Alex can also add multiple elements by first checking multiple of those in the list module. Note that Alex can add elements of different types in a single view, if they are relevant (e.g., putting a good character and a good faction under \"good\"-\"evil\" view). For concepts and elements in the view, Alex can read their full descriptions by hovering the cursor over them (Figure 8). When Alex selects added elements from the list module, to let her know where they are in the view, the tool highlights them on the visualization."}, {"title": "4.2.2 Correcting View Visualization (13)", "content": "For some elements added to the view, Alex does not agree with how Patchview positioned them. If Alex thinks the description of a particular concept is not detailed enough for the tool to grasp, she can modify it via the list module. Alternatively, she can edit the concept's definition text directly within the view module by hitting enter while hovering the cursor over the concept's magnet (similar to Figure 8, but with concepts). As Alex updates the concept, Patchview tries to reposition elements in relation to the concept. For elements still misplaced from Alex's perspective, Alex can manually adjust their positions by dragging them in the view (Figure 1c1). When positioning future"}, {"title": "4.2.3 Sensemaking Multiple Views (11)", "content": "Alex continues organizing world elements by creating multiple views. Alex organizes these views by dragging view names and concepts. At a certain point, Alex realizes that it is difficult to understand how characters are distributed along the conceptual dimensions of two views, good-evil and lawful-chaotic alignments. To have a better understanding, Alex anchors them together and Patchview connects the same elements in both views with a thin line (Figure 9). Note that only elements that exist in both views get connected. As Alex hovers her cursor over an element in one of the views, the identical element in another view and the thin connecting line between these elements are highlighted (Figure 9). After connecting these views, as each view is defined by only two concepts, Alex thinks that it would be easiest to make sense of these elements via a 2-dimensional visualization with two axes. For that, Alex can cross two views, and Patchview renders the view in 2D plane visualization instead of connecting elements with lines (Figure 10). Note that Patchview only visualizes elements that exist in both crossed views. As Alex adds more views, she continues to experiment with other visual arrangements, such as radar charts and parallel coordinate charts [18, 41] (Figure 11)."}, {"title": "4.2.4 Steering Generation in the View (12)", "content": "As Alex organizes world elements in the view, she finds herself wanting to add more characters to populate empty spaces within view visualizations. To steer the generation with this nuanced intention, Alex leverages a generative steering function on each view. Alex first clicks on the \"with Steering\" toggle switch at the top right to enter the generation mode. Then, Alex places the generation control right on the view space itself, indicating that Alex wants the newly generated element to be placed near the specified position. To steer generation along multiple aspects, Alex can also place multiple generation controls on multiple views. After placing controls, Alex clicks on one of the type buttons or prompts the LLM (thin and thick dashed lines in Figure 6, respectively) to generate an element with the steering constraints applied. After generating the element, Patchview calculate its position in the view to place it on the view. Similar to how Patchview visualizes elements in the view,"}, {"title": "4.2.5 Correcting Generation (13)", "content": "Sometimes, generated items do not perfectly align with Alex's specifications. To iterate on those, she can directly modify the text of the element or ask Patchview to rewrite it by dragging the element while holding the shift key (Figure 1c2). If Alex disagrees with how Patchview places a generated element, she can reposition it by dragging (Figure 1c1). For elements that Alex does not want to keep, Alex can either remove them from the view by hitting the minus key while hovering the cursor over the element or delete them from the view and the list by hitting the backspace key while hovering the cursor. Alex continues generating, editing, and organizing elements until she is done."}, {"title": "4.3 Technical Details and Implementation", "content": "We built Patchview as a web application with a React-based frontend and a Node-based backend server. We provide technical details on 1) mapping between the position of the element and the its relevance to the concepts and 2) LLM prompting."}, {"title": "4.3.1 Mapping Between Position and Weight", "content": "To enable visualization and visual steering on the view, Patchview needs to map the visualized position of an element to its relevance to considered concepts and vice versa. Here, we quantified the relevance of an element to the concept as weight values between 0 and 1. We first forced the placement of concepts to be convex, as the non-convex arrangement of concepts can bring in more complexities with those mappings. With the convex arrangement of concepts, we can compute the element position easily by weight-summing the concept positions with weights on those concepts. However, deciding weights from the element position is not trivial if there are more than three concepts, as a single position does not fall into one weight combination. That is, with more than three concepts, there can be more than three weights that need to be decided, but there would be only three equations with a 2D arrangement of concepts:\n\n$\\Sigma_{i=1}^{n}W_{i}x_{i} = X_{e}$  \n$\\Sigma_{i=1}^{n}W_{i}y_{i} = Y_{e}$  \n$\\Sigma_{i=1}^{n}Wi = 1$\n\nxi and yi stand for the position of each concept, while xe and ye indicate the position of the element. wi stands for the weight that needs to be inferred and n is the number of concepts.\nDue to the above reason, with more than three concepts, we used the following heuristic to compute one weight combination: With all combinations of three concepts from all concepts, we first calculated weights for each combination. Then, we filtered out combinations with negative weights. After that, we calculated a weight for each concept by summing all weights from all the left combinations. Then, we finalized the weights by dividing each weight by the sum of all weights for all concepts. Note that with this approach, steering element generation with more than three concepts does have a limitation as not all possible weights are expressible with one geometric positioning of concepts. When two axes are crossed to form a 2D plane visualization (as in Figure 10), for each view, we first calculated the crossing point of the following two lines: 1) the line that passes through two concepts of the view"}, {"title": "4.3.2 LLM Prompting", "content": "To generate elements with steering inputs and recognize the relevance of elements to concepts, we prompted claude-2.0 and claude-instant-1.2 from Anthropic [5], respectively. We chose these models because they have shown better performance in creative writing contexts than leading alternatives [10]. Prompts for both generation and recognition began by introducing a set of existing world elements for context, as in Figure 12a. By default, all existing world elements were supplied as part of this context; the user could also select a subset of existing elements to ensure that only those elements would be provided as context.\nWhen generating new world elements without visual steering input, introductory context was followed directly by an instruction describing what kind of element to generate. When generating with visual steering input, we first appended concepts of all views (Figure 12b-I-1) and examples of how existing elements have relevance to those concepts (Figure 12b-I-2). These examples came from elements that the user has already placed in the view, including those repositioned by the user. Note that in the prompt, all views and concepts are phrased as \"dimensions\u201d and \u201ccharacteristics\u201d, respectively. A chain-of-thought [57] style generation instruction prompt followed after (Figure 12b-2), which asked the LLM to first reason about how the element description should be written considering steering inputs and then to write the element description.\nRecognition of concept relevance values takes place on a per-view basis, so a prompt for the recognition task included concept descriptions for only a particular considered view, as in Figure 12c. In the recognition prompts, introductory context (Figure 12a) and concept descriptions (Figure 12c-I) were followed by instructions about how to interpret numbers (Figure 12c-2), and then by examples of the correct performance of a recognition task for this set of concepts (Figure 12c-2-I). Because these examples were taken from past placements of elements into this specific view, information about how the user repositioned elements in this view were taken into account at this step. Finally, the world element to be analyzed was attached (Figure 12c-2-2), with the chain-of-thought [57] instruction that the LLM should provide reasoning before the result. The LLM was asked to provide recognition results in a JSON format with concept identifiers as keys and concept relevance weights as values."}, {"title": "5 USER STUDY", "content": "We conducted a user study on Patchview to learn if it supports sensemaking and steering of world element generation under the user's unique story world context. Specifically, we tried to answer the following research questions to determine if Patchview effectively supports the interactions described in Section 3.3.\n\u2022 RQ1: Does Patchview help the user with sensemaking world elements? (11)\n\u2022 RQ2: Does Patchview help users express nuanced intentions with visual steering? (12)\n\u2022 RQ3: Does Patchview help users correct Al results and behaviors? (13)"}, {"title": "5.1 Participants", "content": "We recruited nine participants (four women, three men, one non-binary, and one who did not disclose gender, ages 24-51, M=33.4, SD=8.7) through Upwork\u00b9, a gigwork platform. We focused on recruiting hobbyists with extensive years of experience (at least five)"}, {"title": "5.2 Procedure", "content": "The study was conducted remotely via Google Meet\u00b2. After welcoming the participants, we asked if they were okay with recording the session. Then, we asked participants to watch two instruction videos, each on 1) the overview of Patchview and ways to generate or create notes on the list module and 2) reading view visualizations. After each video, participants were given an opportunity to experiment with the functions that had just been introduced.\nAfter two instruction videos, we asked participants to conduct the first task, answering sensemaking questions (RQ1). Specifically, we provided two types of questions: 1) landscape questions, characterizing the distribution of world elements in relation to specific concepts (e.g., To which faction most characters are associated with?), and 2) comparison questions, comparing different characters according to their relevance to concepts (e.g., Which character is most associated with faction A?). These were multiple choice questions with one correct option. We measured whether the participants were correct and the time taken to answer. We expected that if the visualization could help users with sensemaking, they would answer more accurately in less time.\nParticipants conducted the task in a within-subject fashion, in two conditions: only with the list interface of Figure 5b (baseline) and together with the view visualization (treatment). We prepared two collections of elements, both focusing on character descriptions. One collection considered three different factions to characterize"}, {"title": "5.3 Results", "content": "We analyzed survey responses (Figure 13), recognition and steering errors from log data (Figure 14 and 16), answer time and correctness of the sensemaking questions (Figure 15), video recordings, and interview data. We measured recognition errors by the difference between Patchview's automatic placements of elements and the user's final placements of the same elements in views. This error"}, {"title": "5.3.1 RQ1: Visualization helped users with sensemaking world elements", "content": "The participants seemed to largely agree with how Patchview placed world elements in the view. Figure 14 shows that the mean recognition error was measured to be 0.04 on a 0-to-1 scale for the user's arbitrary concepts. This result resonates with participants' interview responses (N = 6). For instance, P9 mentioned that Patchview accurately recognized concept relevance even in challenging cases: \"It actually grasped my intention even though I gave two words, basically.\"\nWith largely accurate automatic visualization, in the first survey question (Figure 13), participants responded that Patchview helped them understand the landscape of elements in the story world. The helpfulness of visualization also manifests in the sensemaking question results (Figure 15), specifically for landscape questions. When answering landscape questions, participants were significantly faster with visualization than without (Mann-Whitney U = 79, n\u2081 = n2 = 9, p < 0.0013) and more correctly answered questions. However, for comparison questions, there was no significant difference in time taken to answer questions between conditions (Mann-Whitney U = 60, n\u2081 = n2 = 9, p > 0.05). Moreover, participants were similarly accurate in answering comparison questions. Interview results resonated with these findings: participants mentioned that they could easily understand the landscape of world elements with the help of visualization (N = 9), allowing them to track generated elements while keeping the world under the rule and the structure. P1 mentioned: \"The different views and stuff, actually seeing that on there and keeping track of it, I think, would be helpful. ... Because I end up building up too many and then I forget what the differences in each one's personality are sometimes.\u201d P9 also appreciated the customizability of the visualization.\nPatchview's visualization also influenced how participants thought about each concept. That is, when participants do not agree with Patchview's placement of elements, some participants reflected on their own perception of concepts (N = 5), often concretizing how they think about concepts. For example, P4 mentioned: \"When I"}, {"title": "5.3.2 RQ2: Visual steering of Patchview allowed users to steer the generation with nuanced intentions", "content": "The results indicate that the steering function was fairly accurate when used for arbitrary concepts of the participants' interests. The mean steering error was measured to be 0.18 on a 0-to-1 scale (Figure 14). As the ordinal scale of five on a bi-directional dimension is often considered to be easily discernible by people [48], if we assume uniform intervals between levels (which is often used in ML [48, 62]), the error of 0.18 would be smaller than a single gap in a five-level ordinal scale (0.25). Hence, we conclude that Patchview allows users to steer the generation accurately in a granularity finer than easily discernible five-level scale on dimensions with two concepts. While this standard would need to be different for cases when a view has more than two concepts, in our study, only four steered generation results considered more than two concepts. As in the second question in Figure 13, participants also perceived that the tool helped them steer the world element generation with their nuanced intentions. Participants mentioned that visual steering for element generation and rewriting was intuitive (N = 7). For example, for visual rewriting interaction, P6 mentioned: \"All I had to do is to move where I wanted the story element to be reconnected to and that's like a no-brainer that just takes a couple of mouse clicks and you're good to go.\" Participants also noted that visual steering helped them express nuanced intentions, even allowing them to realize the semantic space that they could not think about (N = 6). For example, P2 mentioned that they could use visual steering to create a set of characters that would make more conflicts than randomly generating them. On the other hand, participants thought that natural language prompts often require more cognitive effort as they need to bring up specific instructions (N = 2). However, participants thought that natural language prompts are beneficial as the user can be more specific in the instruction (N = 4). With different strengths, some participants (N = 2) thought that visual steering and natural language prompting complement each other, as P7 mentioned, by \"choosing the point via steering and then giving it a little bit of (natural language) input.\u201d For example, one limitation of the current visual rewriting interaction is that it often changes aspects the user likes. Adding a natural"}, {"title": "5.3.3 RQ3: With more user examples, Patchview could only improve recognition, not the steering, but to a small extent", "content": "In the third question of Figure 13, most participants answered that they could convey their interpretations of the concepts through repositioning elements. Similarly, during the interview, participants mentioned that it was easy to revise AI results by simply moving elements on the view or by rewriting interactions (N = 6). For example, P7 mentioned: \u201cI really like that you have the ability to say, kind of like, 'No I'm telling you where this should go' versus I want you to actually adjust it to fit there.\"\nHowever, the user's correction of concepts through the addition of more examples did not turn into dramatic changes in AI behaviors. As in Figure 16a, when we conducted a linear regression on the relation between the number of examples and the recognition error, the addition of more examples significantly decreased errors (p < 0.05), but with a small magnitude (coef f = -0.006) and a small ability to explain variations (R2 = 0.036). The analysis on steering errors (Figure 16b) revealed no significance in the correlation between the number of added examples and errors (p > 0.05). These resonated with the interview responses: participants felt that the study session was not long enough to sense that the tool is learning from what they are doing in the tool (N = 3). P7 mentioned that rather than having such tool behavior changes implicit, making them more explicit to the user would be helpful: \"I would have had to play with it a lot more to know if it actually was learning ... It'd be interesting if I could have a feature to refresh ... So a refresh thing would help me see what it was learning from me.\""}, {"title": "5.3.4 RQ4: Participants could flexibly create their own story world and suggested ways to improve the tool for more comprehensive story writing", "content": "With Patchview, participants could structure the story world according to concepts of their interest. Table 2 shows the summary of views participants created. Many participants created views for alignments [24"}]}