{"title": "SinaTools: Open Source Toolkit for Arabic Natural Language Processing", "authors": ["Tymaa Hammouda", "Mustafa Jarrara", "Mohammed Khalilia"], "abstract": "We introduce SinaTools, an open-source Python package for Arabic natural language processing and understanding. SinaTools is a unified package allowing people to integrate it into their system workflow, offering solutions for various tasks such as flat and nested Named Entity Recognition (NER), fully-flagged Word Sense Disambiguation (WSD), Semantic Relatedness, Synonymy Extractions and Evaluation, Lemmatization, Part-of-speech Tagging, Root Tagging, and additional helper utilities such as corpus processing, text stripping methods, and diacritic-aware word matching. This paper presents SinaTools and its benchmarking results, demonstrating that SinaTools outperforms all similar tools on the aforementioned tasks, such as Flat NER (87.33%), Nested NER (89.42%), WSD (82.63%), Semantic Relatedness (0.49 Spearman rank), Lemmatization (90.5%), POS tagging (97.5%), among others. SinaTools can be downloaded from (https://sina.birzeit.edu/sinatools).", "sections": [{"title": "1. Introduction", "content": "Despite the progress in Arabic NLP [9], there remain a lack of tools and resources that offer solutions for Arabic NLP and NLU tasks. Developing machine learning tools is crucial in democratizing Arabic NLP, as they allow people to incorporate machine learning into their workflows with less technical knowledge. The availability of open-source and advanced AI tools remains limited. Low-code platforms and toolkits can bridge this gap by offering intuitive interfaces and trained models, making it easier for people in industry, research, and education to tap into the power of NLP to develop and deploy NLP applications."}, {"title": "2. Related Work", "content": "This section overviews different Arabic NLP toolkits, which we also summarize in Table 1. The Stanford CoreNLP [42] is a general-purpose toolkit, supporting nine languages including Arabic, but its Arabic support is limited to basic tasks. MADAMIRA [49] is designed for morphological analysis and disambiguation, including lemmatization, POS tagging, and segmentation. As will be demonstrated later, MADAMIRA is among the top-performing tools in terms of accuracy. However, it is not open-source, is notably slow, and its functionalities are limited to morphology tasks. Farasa [10] focuses also on Arabic morphology tasks, mainly lemmatization, POS tagging, and segmentation. Farasa is designed for speed, offering fast morphological analysis. However, its results are less sophisticated compared to MADAMIRA. For instance, Farasa's POS tagger is limited to 20 tags, whereas MADAMIRA supports 40 tags. Additionally, Farasa's lemmatizer returns ambiguous lemmas as it removes all diacritics.\nCAMELTools [46] is an open-source NLP toolkit providing utilities for pre-processing, morphological modeling, dialect identification, NER, and sentiment analysis. Although CAMeLTools covers a broad range of tasks, its support of NLU tasks is limited. For example, its NER module can only detect three flat entity types: PERS, LOC, and ORG."}, {"title": "3. Design and Implementation", "content": ""}, {"title": "3.1. Design", "content": "SinaTools is an open-source toolkit consisting of a collection of Python application programming interfaces (APIs) and their corresponding command-line tools, which encapsulate these APIs. It adheres to these core design principles:\n1. Modularity: Each function is encapsulated in its own module, allowing for independent development, testing, and maintenance. This structure facilitates the seamless addition of new features without impacting existing ones.\n2. Extensibility: The architecture is designed to be extensible, enabling users to easily integrate additional tasks or replace existing components with custom implementations.\n3. User-Friendly APIs: SinaTools provides intuitive and consistent APIs that abstract the complexity of underlying algorithms and data structures. This ensures that users, regardless of their expertise in NLP, can leverage the full capabilities of the tools with a minimal learning curve.\n4. Performance Optimization: The implementation emphasizes efficient processing of large datasets. Large models and datasets required for various tasks are loaded the first time they are used or through a specific implemented download command. This approach minimizes load times and ensures efficient memory usage during operations."}, {"title": "3.2. Implementation", "content": "SinaTools is implemented in Python 3.10.8 and can be installed via pip install. Python was selected due to its ease of use and its widespread adoption for NLP and Machine Learning. SinaTools is designed to be compatible with Python version 3.10 on Linux, macOS, and Windows. Currently, SinaTools provides both an API and a command-line interface for each component, in addition to demo pages. It is important to note that SinaTools is under continuous development, with ongoing additions of new features and updates to existing ones. This paper reports on the current components, but updates and new components will be continuously published."}, {"title": "4. SinaTools Modules", "content": ""}, {"title": "4.1. Morphology Module", "content": "This module is an implementation of the Alma )\u0623\u0644\u0649( morphology tagger presented in [22], which consists of three sub-modules: (1) lemmatizer (2) POS tagger, and (3) root tagger. These sub-modules utilize a pre-computed memory, consisting of a dictionary containing many wordforms and their morphological solutions. This dictionary is implemented as a Python hashmap, simplifying the lemmatization, POS, and root tagging tasks into straightforward lookup operations. Each entry in the dictionary is a key-value pair, where the key is a wordform and the value is its corresponding morphological solution. A morphological solution consists of a <lemma, POS, root>, and the frequency of this solution. The Alma dictionary is frequency-based. We have gathered a large collection of word forms and their morphological solutions from lexicographic resources developed at SinaLab. Using these, we calculated the frequency of each solution (see [22]). The primary resource for building Alma is the Qabas lexicographic data graph [30], which includes about 58k lemmas linked with lemmas in the Arabic Ontology [21, 19], 110 Arabic lexicons [20, 23, 24, 5], and other annotated corpora and resources [45, 35, 15, 28, 27, 26, 18]. The Alma dictionary retains the most frequent solution on the top (i.e., default) solution. For example, the wordform )\u0630\u0647\u0628 /dhb) can be a noun and a verb. However, as the verb form is more common, SinaTools consistently tags it as a verb, regardless of the context. Although this method is simple and out-of-context, our evaluations show that it is more accurate and significantly faster than others.\nOut-of-Vocabulary: As SinaTools relies on a dictionary, it cannot provide solutions for wordforms not included in its dictionary. However, our benchmarking showed that out-of-vocabulary (OOV) instances are not a significant issue. To address OOV cases, SinaTools integrates a fine-tuned BERT model for POS tagging, ensuring robust performance even when encountering words not present in the dictionary (See [22])."}, {"title": "4.2. Named Entity Recognition Module", "content": "This module\u00b3 is based on a BERT model that fine-tuned with our Wojood datasets [32]. The module supports flat, nested, and fine-grain entity types. Since Wojood has 21 entity types, our model includes 21 classification layers, one layer for each entity type. Each layer classifies the token into one of three classes, $C = \\{I, O, B\\}$. As illustrated in Figure 2, each classifier is an expert in one entity type, which will output one of the three labels in $C$ for each token."}, {"title": "4.3. Word Sense Disambiguation Module", "content": "This module is an implementation of a novel end-to-end semantic analyzer called Salma )\u0633\u0644\u0645\u06cc(. Figure 3 illustrates our system architecture, as a pipeline of components: Tokenizer, Lemmatizer, POS Tagger, NER, Target Sense Verification (TSV), and two sense inventories. Given an input sentence, the WSD module conducts semantic analysis, which includes disambiguating single-word and multi-word expressions, and tagging named entities.\nThe WSD system consists of a pipeline of the following sub-processes:\nPhase 1 (n-gram tokenization): we first generate n-grams from the input text for all $2 \\leq n \\leq 5$. The n-grams will be used in later phases to query the sense inventory.\nPhase 2 (Lemmatization): Each token in a given n-gram is lemmatized using the SinaTools Lemmatizer.\nPhase 3 (Multi-word WSD): At this phase we retrieve senses for multi-word expressions such as )\u0636\u0631\u064a\u0628\u0629 \u062f\u062e\u0644/income tax) and )\u0633\u0646\u0629 \u0643\u0628\u064a\u0633\u0629/leap year). SinaTools includes a dictionary of such expressions that we collected from our 150 lexicons [23], storing each multi-word expression with its glosses. This dictionary serves as our multi-word sense inventory. For each lemmatized multi-word expression (i.e., n-gram, where $2 \\leq n \\leq 5$), we perform a lookup in the sense inventory, starting with n = 5. If a sense is not found, we reduce n by 1 and attempt the retrieval again. For example)\u0636\u0631\u064a\u0628\u0629 \u062f\u062e\u0644/income tax) has two glosses (i.e., senses) in the sense inventory. The senses along with the original input sentence are sent to the TSV module for disambiguation (Phase 6).\nPhase 4 (NER): As no need to sense-disambiguate all n-grams (e.g., text spans such as )\u0645\u0635\u0631/Egypt) and )\u0648\u0632\u0627\u0631\u0629 \u0627\u0644\u0627\u0642\u062a\u0635\u0627\u062f/Ministry of Economy) are proper nouns), the NER module tags each span with entity types.\nPhase 5 (single-word WSD): All tokens that were not sense-disambiguated or NER-tagged in the previous steps will undergo an additional step. We disambiguate them as single words (uni-grams). For instance, in the example above, three uni-grams need to be single-word disambiguated: )\u062a\u0642\u0648\u0645/is()\u0628\u062a\u062e\u0641\u064a\u0636/reducing)and )\u0641\u064a/in). The glosses of these uni-grams are retrieved from the single-word sense inventory and sent to the TSV module for disambiguation (Phase 6).\nPhase 6 (Target Sense Verification (TSV)): We implemented the TSV approach proposed in [33], which is a core step in the WSD pipeline. Given a pair of sentences (context and gloss), TSV calculates the Positive and Negative probabilities to classify whether this pair is True or False, indicating whether the gloss is the correct sense used in the context based on the higher probability. Thus, for a word w in context C, and all w glosses $g_i \\in G$, we generate a set of pairs $\\{(C, g_1),(C, g_2),..., (C, g_m)\\}$. These pairs are sent to the TSV model for evaluation. The pair with the highest Positive probability is considered the target sense. We fine-tuned the TSV model using AraBERT V2 [7] and the ArabGlossBERT dataset (167k pairs) [3]. We experimented with an augmented dataset [40] but it did not enhance the results. We also addressed WSD using the WiC model [4] but the performance was bad.\nEvaluation: As SinaTools is the first tool for Arabic WSD, we computed its baseline using the Salma corpus [33], the only available Arabic sense-annotated corpus. Salma is only annotated with NER and single-word expressions, but we extend its annotations to include multi-word expressions. Table 4 shows that SinaTools achieves an overall WSD performance of 82.63% accuracy. It's important to note that calculating the accuracy of the single-word WSD is not straightforward because a word might have multiple correct senses in Salma, and whether the accuracy should include words with single senses. Thus, we refer the reader to [33] for more details. It is also worth noting that Salma was used in the ArabicNLU Shared Task [36], where the best-performing system on single-word WSD achieved 77.8% [50]."}, {"title": "4.4. Semantic Relatedness Module", "content": "This task is useful in NLP for many scenarios such as evaluating sentence representations [8], document summariza- tion, and question answering. Given two sentences, the semantic relatedness task aims to assess the degree of association between two sentences across various dimensions, including meaning, underlying concepts, domain-specificity, topic overlap, or viewpoint alignment [1]. SinaTools supports MSA semantic relatedness, which represents our participation [41] in the SemRel Shared Task [48], where we achieved the top rank. Unlike the lexical overlap (using dice coefficient) proposed in [47], we extracted the mean-pooling embeddings of the sentences from BERT-based model, then employed cosine similarity as an unsupervised technique to calculate the sentence-pair scores. We evaluated our method using the 595 sentence-pairs test set provided in the SemEval-2024 Task 1 on Semantic Textual Relatedness for African and Asian Languages [48]. Table 5 shows that we outperformed the baseline. We used Spearman rank correlation coefficient, the official evaluation metric used in the shared task, which captures the level to which the system predictions align with the human judgments of the test pairs."}, {"title": "4.5. Synonyms Module", "content": "Arabic is low-resourced in terms of synonymy resources and tools [44, 31, 17]. SinaTools includes an implementation of the synonymy extraction and evaluation algorithm introduced in [14], which was also tested in extracting Welsh synonyms [37]. The algorithm is designed to extract synonyms from mono and multilingual lexicons. It leverages synonymy and translation pairs from these resources to generate a synonymy graph, where nodes involved in cyclic paths are deemed synonyms. The extracted synonyms are assigned fuzzy values to indicate their degree of belonging to the synonym set. The algorithm is evaluated using the Arabic WordNet, achieving 98.7% accuracy at 3rd level and 92% at 4th level. We utilized about 100 mono and bilingual lexicons [23], including the Arabic Ontology [21, 19], Qabas [30], 40 ALECSO lexicons, 11 from the Arabic Academy, WordNet, and others. These resources were used to compute two synonymy graphs: a 2nd level graph (75MB) and a 3rd level graph (1.1GB). SinaTools features two main methods: (i) SynExtract for synonym extraction, and (ii) SynEval for synonym evaluation. The SynExtract method allows users to give one or more terms as input and retrieve their synonyms (See example 1). Each synonym is given a fuzzy value. The more terms are provided in the input the better the accuracy. The SynEval method enables users to input a set of terms and receive a fuzzy score for each term, reflecting its synonymy strength (See example 2). In both methods, users can choose between the faster 2nd level graph or the richer but slower 3rd level graph.\nExample 1: the function SynExtract)\u0645\u0631, \u0637\u0631\u064a\u0642(returns }15 \u0633\u0643\u0629,%20 \u0632\u0642\u0627\u0642 ,%23 \u0646\u0647\u062c ,%50 \u0633\u0628\u064a\u0644,%61 \u0645\u0633\u0644\u0643% ,...\\{\nExample 2: the function SynEval)\u0645\u0633\u0644\u0643,\u0645\u0645\u0631, \u0637\u0631\u064a\u0642(returns } 30 \u0637\u0631\u064a\u0642 ,%50 \u0645\u0645\u0631 ,%70 \u0645\u0633\u0644\u0643% \\{."}, {"title": "4.6. Other Modules and Tools", "content": "In addition to its core functionalities, SinaTools includes a variety of utility modules designed for text processing capabilities. The Sentence Splitter module offers the capability to segment text into sentences, with the flexibility to specify separators (e.g., periods, question marks, exclamation marks, and line breaks), thereby accommodating varied text structures. Notably, this feature selectively incorporates chosen separators while disregarding unselected ones, thereby enhancing the precision of text segmentation. The Diacritic-Based Matching of Arabic Words module compares two Arabic words to find out whether they are the same, taking into account their partial or full diacratizition ]34[. For example, the two words )\u0641\u064e\u0639\u0644 fala \u0641\u0639\u0651\u0644 Ifal ) are compatible and the )\u0641\u0639\u0644( fala \u0641\u0639\u0644 /fila ) are not. The Text Duplicate Removal module employs cosine similarity to eliminate redundant sentences from input text under a similarity threshold, which is useful for corpora pre-processing. The Arabic Jaccard module computes union, intersection, and similarity metrics between sets of Arabic words, taking into account partial diacritization. Moreover, the Arabic Diacritic Removal (arStrip) module designed to cleanse Arabic words by selectively removing diacritics, shaddah, digits, alif, and special characters, according to user-specified parameters. Lastly, the Transliteration Module facilitates seamless conversion between Arabic and Buckwalter transliteration schemas, thereby ensuring interoperability across different linguistic representations."}, {"title": "5. Conclusions and Future Work", "content": "We presented SinaTools, an open-source Python package for Arabic NLP, offering solutions for various tasks such as NER, WSD, semantic relatedness, synonymy extraction and evaluation, lemmatization, POS tagging, among others. Our benchmarking results highlight that SinaTools consistently outperforms similar tools across all tasks.\nLooking ahead, we plan to expand SinaTools by incorporating additional Arabic NLU modules. These enhancements will focus on areas currently underserved by existing tools, such as intent detection [25, 39], relationship extraction [6], detection of hate speech [16], bias and propaganda [11, 51], among others."}]}