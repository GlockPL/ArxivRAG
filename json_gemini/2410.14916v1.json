{"title": "Cooperation and Fairness in Multi-Agent Reinforcement Learning", "authors": ["JASMINE JERRY ALOOR", "SIDDHARTH NAYAK", "SYDNEY DOLAN", "HAMSA BALAKRISHNAN"], "abstract": "Multi-agent systems are trained to maximize shared cost objectives, which typically reflect system-level efficiency. However, in the resource-constrained environments of mobility and transportation systems, efficiency may be achieved at the expense of fairness certain agents may incur significantly greater costs or lower rewards compared to others. Tasks could be distributed inequitably, leading to some agents receiving an unfair advantage while others incur disproportionately high costs. It is, therefore, important to consider the tradeoffs between efficiency and fairness in such settings.\nWe consider the problem of fair multi-agent navigation for a group of decentralized agents using multi-agent reinforcement learning (MARL). We consider the reciprocal of the coefficient of variation of the distances traveled by different agents as a measure of fairness and investigate whether agents can learn to be fair without significantly sacrificing efficiency (i.e., increasing the total distance traveled). We find that by training agents using min-max fair distance goal assignments along with a reward term that incentivizes fairness as they move towards their goals, the agents (1) learn a fair assignment of goals and (2) achieve almost perfect goal coverage in navigation scenarios using only local observations. For goal coverage scenarios, we find that, on average, the proposed model yields a 14% improvement in efficiency and a 5% improvement in fairness over a baseline model that is trained using random assignments. Furthermore, an average of 21% improvement in fairness can be achieved by the proposed model as compared to a model trained on optimally efficient assignments; this increase in fairness comes at the expense of only a 7% decrease in efficiency. Finally, we extend our method to environments in which agents must complete coverage tasks in prescribed formations and show that it is possible to do so without tailoring the models to specific formation shapes. [Code]\u00b9", "sections": [{"title": "1 INTRODUCTION", "content": "Multi-agent vehicular systems, where large numbers of vehicles coordinate to execute complex missions, have the potential to transform the transportation and mobility domains. Such systems have wide-ranging applications [44], including disaster response [12], wildfire detection [3], sensing and monitoring [6, 7, 40], ridesharing [4], agriculture [50], and spacecraft operations [9, 17]. Despite the differences in application domains, these operations tend to occur in"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Fairness", "content": "Fairness has been extensively studied in many contexts, including in game theory [33], economics [20], and machine learning [10]. In optimization approaches, fairness is usually formulated as a strict mathematical objective or constraint. These approaches work better when the problem structure is fixed, such as a predefined network structure or creating specific formations [49]. In machine learning, work in fairness often refers to mitigating social biases and the social, legal, and ethical aspects of machine learning discrimination. One of the most common classes of approaches is the alpha fairness method [5]. Our work is concerned with fairness in the network engineering sense [28], where individual users receive a fair share of system resources [35].\nThere are relatively few works in MARL studying this definition of fairness. It is a growing area of interest due to its impact on shaping cooperative behavior in resource-constrained environments. De Jong et al. [16] incorporated priority awareness into their fairness modeling. As human notions of fairness often consider it fair if agents receive slightly different rewards in the presence of additional contextual information, this work assumes that each agent knows the true priority of all other agents in the simulation. This approach produces sub-optimal reward solutions. Unlike the priority-based model, Jiang and Lu [31] propose a Fair-Efficient Network (FEN) algorithm to maintain fairness and efficiency using a hierarchical controller that chooses between efficient and fair policies based on the average utility obtained at every episode. The authors state that learning efficiency and fairness is challenging with a single policy. We overcome this using our optimization-based min-max fair assignment, which finds a fair assignment using the distance metric indirectly considering efficiency as well.\nThe Fair-Efficiency Multi-Agent Deep Deterministic Policy Gradient (FE-MADDPG) algorithm [37] implements a fairness reward similar to FEN. However, it operates using global information, offering lower levels of privacy and decentralization. Another key limitation is scaling this method to different numbers of entities, which would require retraining the model every time. Zimmer et al. [56] propose a fairness framework that balances efficiency and equity using a welfare function adaptable to various fairness metrics (lexicographic maximin, Gini). Their approach includes sub-networks to account for fairness and efficiency, where the agents choose a self-oriented efficient policy or a fair team-oriented policy based on a probability parameter that decreases over time. Initially, the agents are trained to maximize efficiency, and later, they are trained to improve fairness. Our approach trains agents to navigate to goals based on a min-max fair assignment, aiming to equalize costs associated with navigating to goals without sacrificing efficiency.\nGrupen et al. [23] find that sophisticated coordination behavior only emerges when there is a shared reward, but this emergent behavior does not ensure fairness. They introduce a soft-constraint equivariant policy learning method to dynamically balance the fairness-utility trade-off. In contrast to De Jong et al. [16], Liu et al. [37] and Grupen et al. [23], our work considers fairness in navigation-based settings where global information is not available to each agent."}, {"title": "2.2 Communication Structures in Multi-Agent Reinforcement Learning", "content": "Our work focuses on the problem of multi-agent navigation and collision avoidance among a set of decentralized N agents that can only sense the presence of other obstacles and agents within a limited radius r. Communication between agents is vital for them to complete their respective tasks successfully. The study of the role of communication between agents is an active and extensive field within MARL, so we refer the reader to a survey on the topic [55] for a comprehensive description. We highlight several works in this area that address problems similar to ours.\nExisting MARL work on this problem often assumes that even if the behavior of the agents is decentralized, communication amongst them is centralized. This means that all agents have access to messages from all other agents in the environment. Unfortunately, this means that as the number of agents increases, the computational expense of communication increases superlinearly. Centralized communication also creates privacy concerns, where agents are unable to participate in the MARL framework without sharing data with everyone. To address this expense, ATOC [15, 30] relies on attention mechanisms to provide a compact representation of message priority. In EMP [2], the environment is translated into a shared agent-entity graph representation that allows agents to communicate along connected edges. This formulation provides a compact graph representation of the communication connections between all entities in the environment. However, all agents must know the positions of all entities in the graph at the beginning of an episode, and thus, a similar centralized communication constraint arises. In contrast to these centralized communication approaches, InforMARL [41] differs from these works in that it relies on only locally available information throughout training and during evaluation. Our algorithm relies on InforMARL as the underlying MARL algorithm for training and evaluation, with several adaptations to its reward and buffer structure."}, {"title": "2.3 Formation in MARL", "content": "Multi-agent reinforcement learning has proven effective for the formation control of multiple robots. Shen et al. [45] propose a modified deep deterministic policy gradient (DDPG) algorithm to achieve the formation control for multiple unmanned aerial vehicles with the help of digital twin technology. Graph convolutional networks are used for policy gradient methods in [34], which leverage the underlying graph formed by the agents. To maintain the formation shape while navigating obstacles in the environment, Gong et al. [22] use a hybrid approach where they switch between an RL controller and a PID controller based on the state. However, the above-mentioned works use some form of reward shaping where they only account for goal-reaching and collision avoidance as their main goals. These works assume the formation pattern to be fixed and task each agent to navigate to a fixed position in the environment. Our work contrasts these methods by introducing a shape-independent approach that allows agents to come into various formations in a scalable manner. We do not fix the assignment of agents to positions of the formations; instead, agents decide what position to occupy based on local observations and updated rewards at each time step."}, {"title": "3 METHODOLOGY", "content": "In this section, we outline the methodologies used to achieve policies with fairer outcomes for agents navigating to goals. We describe the environment and the training setup, detail the formulations employed for agent observations, and explain the modifications made to the reward functions that lead to fair behavior. We also discuss centralized efficiency and fairness optimization formulations for goal assignments.\nWe train an adapted version of InforMARL, an existing CTDE multi-agent reinforcement learning algorithm, on our modified reward functions. The details of this algorithm are beyond the scope of this paper and can be found in [41]."}, {"title": "3.1 Preliminaries", "content": "Following the InforMARL framework, our environment comprises entities categorized into agents, obstacles, and goals. For each agent i at each time-step t, we define an agent-entity graph as $g^{(i)}_{t} \\in G : (V, \\mathcal{E})$, where each node $v \\in V$ is an entity in the environment. Entities are connected to each other by edges if they are within a certain sensing distance. Agent-agent edges are bi-directional, which is equivalent to a communication channel between them, whereas agent-non-agent edges are unidirectional, with messages being passed from the non-agent entity to the agent.\nOur environment is based on the Multi-Particle Environments (MPE) [38] collection of tasks. We formulate our environment as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) defined by the tuple $(\\mathcal{N}, \\mathcal{S}, \\mathcal{O}, \\mathcal{A}, \\mathcal{G}, \\mathcal{P}, \\mathcal{R}, \\gamma)$, where:\n\u2022 \\mathcal{N} is the number of agents\n\u2022 $s\\in \\mathcal{S} = \\mathbb{R}^{\\mathcal{N}\\times D}$ is the state space of the environment, with D as the dimension of the state\n\u2022 $o^{(i)} = O(s^{(i)}) \\in \\mathbb{R}^d$ is the local observation for agent i\n\u2022 $a^{(i)} \\in \\mathcal{A}$ is the action space for agent i. The action space for each agent is discretized such that it can control unit acceleration and deceleration in the x- and y- directions\n\u2022 $g^{(i)} \\in \\mathcal{G}(s; i)$ is the graph network formed by the entities in the environment with respect to agent i\n\u2022 $P(s'|s, \\mathcal{A})$ is the transition probability from s to s' given the joint action \\mathcal{A}\n\u2022 $R(s, \\mathcal{A})$ is the joint reward function\n\u2022 $\\gamma\\in [0, 1)$ is the discount factor\nWe investigate a coverage navigation problem [14, 47] where a group of agents must navigate to different goals. We have N agents that navigate a 2D space using a double integrator dynamics model [42]. There are N goals in the environment, each denoted by $\\zeta^{(i)}$. Agents navigate in such a way that each agent reaches a unique goal while avoiding obstacles and walls. The task is to find a policy $\\Pi = (\\pi^{(1)}, ..., \\pi^{(N)})$, where $\\pi^{(i)}(a^{(i)} | o^{(i)}, g^{(i)})$ for agent i selects action $a^{(i)}$ based on its graph network $g^{(i)}$ and the local observation $o^{(i)}$."}, {"title": "3.2 Agent training framework", "content": "We group our training steps into multiple episodes. At the beginning of each episode, the environment is initialized with agents starting at random locations and goals randomly distributed in the environment. The policy provides an action for each agent i in the first step. The agent executes the action and observes the change in state. At each time step, a goal assignment mechanism assigns a goal to the agent, and a preliminary distance-based reward $R_{dist}(s_t, a^{(i)}_t)$ is computed by taking the negative of the Euclidean distance to this assigned goal. In Section 3.2.2, we discuss the goal assignment techniques used in this work. Additionally, we include a fairness metric to the reward function to create a \"fair\" reward. In Section 3.2.1, we discuss the structure of our fairness reward function.\nThe agent uses its sensing radius to detect the presence of other entities in its vicinity. A graph $g^{(i)}_t$ is created using these entities and passed into a graph neural network (GNN). The output from the GNN-$x_{agg}$-and the observation vector $o^{(i)}$ are used by the agent to select its next action. In Section 3.2.3, we elaborate on the observation functions and the input to the graph network. Agents are penalized when they collide with other agents or obstacles, with a collision penalty of -C added to the reward. Agents utilize the observations and rewards to learn which goal locations to navigate to. When an agent reaches its assigned goal, it receives a one-time goal reward $R_{goal}(s_t, a^{(i)}_t)$. These agents are flagged as 'done' and remain stationary at the goal until the end of the current episode. We also restrict the actions that can be taken by these 'done' agents. This prevents the agents from drifting away under the collision force influence"}, {"title": "3.2.1 Fairness metric", "content": "Fairness is context-dependent, and there are different definitions of fairness for different problems. In this work, we focus on fairness in the context of multi-agent navigation to goal assignments. Prior works have considered fairness in multi-agent networked systems [8, 28]. We extend the definition of fairness used in networked systems and choose the distance traveled and time each agent takes to reach its goal as the resources to be treated fairly. We want to minimize the standard deviation of distance traveled and time to reach goals among all agents. By reducing the spread of these metrics and incorporating these metrics in our reward function, we can add fairness to agent behavior in the system. Large imbalances in travel distances could lead to significant disparities in the effort required by different agents. Our metric aligns with this task by ensuring a balanced workload across all agents, minimizing the variance in their travel distances.\nThe distance traveled by each agent i at each time step t is represented by $d^{(i)}_t$, with an overall mean $\u03bc_t$ and standard deviation $\u03c3_t$ for all agents. We compute the coefficient of variation as $CV_t = \u03c3_t/\u03bc_t$. We choose our fairness metric $F = 1/CV$ to be the inverse of CV so that it is a non-dimensional quantity with higher numerical values indicating greater fairness. To avoid a zero denominator, we include a small positive number \u03f5 to the fairness metric F. Thus, the fairness metric is\n\n$F_t(s_t, a^{(i)}_t) = \\frac{\\mu_t}{\\sigma_t + \\epsilon}$   (1)"}, {"title": "Goal assignment", "content": "3.2.2\nWe assign agents to goals in training for the models to learn fair and efficient behaviors. Assigning agents to goals is a well-investigated problem [46]. When we trained models without goal assignments, the agents did not learn to cover unique goals. Instead, we utilize random assignment, which provides increased fairness, an optimal distance-based cost assignment, which increases efficiency, and a fair assignment that uses min-max fairness that improves fairness while considering the distance cost. In the optimal and fair assignment models, we calculate the assignment at every timestep. This ensures the agents maintain efficiency or fairness throughout the episode.\nThe assignment function takes into account each agent's current position and the location of all goals in the environment. Note that these assignments are only used to determine the rewards for the agents and are not used to determine the goal positions for the agent's local or graph observation vectors."}, {"title": "3.2.2.1 Random assignment", "content": "Randomization is a commonly used resource allocation method to improve fairness. We assign agents to goals randomly at the start of the episode. We denote this assignment by (RA). At every time step, the agents are provided a reward $R^{RA}_{dist}(s_t, a^{(i)}_t)$ based on the distance to these assigned goals. The random assignment does not consider any cost in the allocation and consequently may be very inefficient. In this paper, we use models trained with random goal assignments as the baseline. Randomness inherently offers a level of fairness by allowing the assignment of goals to agents without any bias and allows the exploration of diverse behaviors. It serves as a baseline to show how our proposed model outperforms a simple, fair-by-chance allocation strategy using the same setup."}, {"title": "3.2.2.2 Optimal distance cost assignment", "content": "For an efficient assignment, we use the linear sum assignment or minimum weight matching in the bipartite graphs algorithm. This matches each agent i to a goal j so that the total cost $c_{ij}$ for all agents is minimized, which here corresponds to minimizing the total distance, or $\\min_{x_{ij}} \\sum_{i=1}^n\\sum_{j=1}^n c_{ij}x_{ij}$, where $x_{ij}$ is 1 if agent i is assigned to goal j and 0 otherwise. With the optimal distance cost assignment, we attempt to minimize the total distance traveled D by each agent based on the locations of agents and goals. This distance-based cost approach does not consider fairness. We denote this assignment in our experiments by (OA). The distance-based reward for an optimally assigned goal is represented as $R^{OA}_{dist}(s_t, a^{(i)}_t)$. Comparing against this model allows us to demonstrate the tradeoffs in efficiency while attempting to improve fairness."}, {"title": "3.2.2.3 Min-max fair assignment", "content": "We compare the optimal cost assignment with the performance of agents when the system is provided with a fair assignment of agent-goal results. Min-max fairness is a popular concept that reduces"}, {"title": "3.2.3 Agent observations", "content": "Each agent's local observation vector consists of its position $p_i$ and velocity $v_i$ in a global frame of reference and the information about the locations of goals. Any goal position that is input to the agent's observation is the relative position of the goals with respect to the agent's position, i.e., $p^{goal}_{i}$.\nIn centralized MARL applications, agents are provided with all goal positions as well as information about the neighboring agents in the environments. This approach is not feasible for a scalable decentralized system, as it fixes the model's input and output matrices. This also leads to a lower level of privacy and assumes global knowledge of every agent's position. When agents are sparsely distributed in larger environments, they can only observe parts of the environment, preventing them from knowing the positions of all goals a priori. We model this in our observation function by only using the positions of the closest two goals from each agent.\nWe investigated the effects of not providing any goal information in the agent's observation, which led to agents wandering in the environment and not being able to reach goals. When only the closest goal's position was provided, agents would tend to loiter around that goal, even if it was occupied, without awareness of additional goals. However, increasing the amount of goal information too much increases the computational burden and decreases scalability. Moreover, it fixes the network input size, limiting flexibility when the number of goals changes dynamically. Balancing the amount of goal information provided allows agents to operate efficiently without compromising scalability or privacy.\nWe provide a goal occupancy flag in the observation vector that informs agents how close any agent is to that goal. It allows agents to know if a goal in their sensing range is occupied or will soon be occupied due to another agent's presence in its proximity. The goal occupancy flag \u03b7 is created for each goal in the environment. We populate the flags based on the distance of the closest agent to the goal. At the start of an episode, for each goal (j) with position $p^{goal(j)}$, we initialize all $\u03b7^{goal_j}$ to 0. As agents move closer to the goals, we calculate the minimum distance any agent is from a"}, {"title": "particular goal j", "content": "particular goal j,\n$d^{(j)}_{min} = \\min_{i\\in N}||x^{(i)}_t - p^{goal(j)}_{t}||_2$\n$\\eta^{goal_j} = 1 - \\frac{d^{(j)}_{min}}{P_0 + P^d}$\n    (3)\nWe restrict the value of \u03b7 to be within 0 and 1 for ease of computation. For a given goal, the flag value increases from 0 to 1 as an agent tries to reach it. In particular, $\u03b7^{goal_j} = 0$ means the goal (j) is available for any agent, and 1 indicates that the goal is fully occupied. The goal occupancy flag update scheme is key to ensuring agents share accurate information. It also demonstrates agent cooperation.\nWhen an agent is near all occupied goals and cannot sense the presence of other unoccupied goals in its vicinity, the agent slows down and explores the environment until it finds an available goal. We hasten this process by allowing the agent to request the position of the nearest unoccupied goal to prevent agents from slowing down near goals already occupied by other agents. Thus, the agent is explicitly made aware of the presence of goals outside its sensing range only when there are no nearby unoccupied goals. The final ego observation vector $o^{(i)}$ can be represented as $o^{(i)} = [P_i, v_i, p^{goal_1}, \\eta^{goal_1}, p^{goal_2}, \\eta^{goal_2}]$.\nWithin an agent's sensing range, there may be other agents, goals, and obstacles. The neighborhood information is collected into a graph observation vector xj that is then passed into the GNN. Graph message passing can provide an encoded representation of goals outside the sensing distance. For each agent i, $x^{(i)}_t$ = [$P_v, P^{goal_1}, P^{goal_2},  \\eta^{goal_1}, entity\\_type(j)$] where $P_v, P^{goal_1}, P^{goal_2}$ are the relative position, velocity, and position of the closest goal of the entity at node j with respect to agent i, respectively. The variable $entity\\_type(j)$ \u2208 {agent, obstacle, goal} determines the type of entity at node j. The ego observation is combined with the GNN-encoded observation $x^{agg}$ and input to the agent to produce an action. This communication mechanism is central to the cooperative nature of the task and further strengthens the system's ability to scale and adapt to various conditions."}, {"title": "Training reward", "content": "3.2.4\nAt every timestep, each agent gets the distance-based reward to an assigned goal, $R_{dist}(s_t, a^{(i)}_t)$. When an individual agent reaches their assigned goal (indicated by \u03c1), it receives a one-time goal-reaching reward $R_{goal}(s_t, a^{(i)}_t)$. \u03c1 is 1 if the agent reached the assigned goal and was previously not at the assigned goal; otherwise, 0. In the models that are tuned with fairness metric, we add the fairness reward $R_{fair}(s_t, a^{(i)}_t)$. We also penalize agents colliding with other agents or obstacles in the environment using a collision penalty -\u0421. \u03ba is a 0/1 variable that indicates if an agent collided with another agent or an obstacle. The total reward at every timestep then becomes\n\n$R_{total}(s_t, a^{(i)}_t) = R_{dist}(s_t, a^{(i)}_t) + R_{fair}(s_t, a^{(i)}_t) + \\rho R_{goal}(s_t, a^{(i)}_t) - \\kappa C$  (4)"}, {"title": "3.2.5 Model variants", "content": "We use all goal assignment schemes and add the fairness reward to create four models for the coverage navigation scenario, varying using the optimal or fair assignment and including and not including the fairness reward in Eqn. 2. We also train a model that has goals assigned randomly to the agent to investigate the effect of random goal assignments on fairness. The four models trained, along with their per-step reward structures, are,\n(1) Random goal assignment with no fairness reward (RA). This model serves as our baseline.\n\u2022 $R_{total} = R^{RA}_{dist} + \\rho R_{goal} \u2013 \u03baC$\n(2) Optimal distance cost goal assignment with no fairness reward (OA)\n\u2022 $R_{total} = R^{OA}_{dist} + \\rho R_{goal} \u2013 KC$\n(3) Fair goal assignment with no fairness reward (FA)\n\u2022 $R_{total} = R^{FA}_{dist} + \\rho R_{goal} \u2013 \u03baC$"}, {"title": "3.3 Decentralized execution framework", "content": "During the evaluation, at the start of each episode, we initialize N agents and N goals randomly in the environment. Each agent can go to any goal in the environment. Similar to the training setup, the agents have their local observation vector and the neighborhood graph network (as described in Sec. 3.2.3). The policy provides an action for each agent at every time step.\nHowever, unlike in training, we do not assign any goals; rather, the agents rely on their local observations and the goal assignments learned during training to navigate to the available goals. We also do not provide any distance reward, goal-reaching reward, or fairness reward, as agents do not rely on a centralized critic during execution. When an agent reaches a goal, we mark it as 'done' until the end of the episode. The episode ends when either all agents are done or the last time step for an episode is reached."}, {"title": "4 EXPERIMENTS", "content": "This section presents the main findings of the approach discussed in Section 3. We begin with a description of our specific navigation problem setting and introduce our evaluation metrics. We extend our experiments to formation scenarios and investigate how congestion and crowding affect these metrics. All codes and model weights used to evaluate the results are in the open-sourced code-base\u00b2 and the associated Readme."}, {"title": "4.1 Problem settings", "content": "We evaluate models trained with a fair assignment to the optimal distance cost assigned and randomly assigned models on coverage tasks in which agents can navigate to any goal. We want to assess the tradeoff between agents navigating in a fair manner and maintaining efficiency by minimizing the total travel distance. We vary the number of agents and total entities in the environment in our experiments.\nAn example evaluation scenario of these models for an episode is shown in Fig. 2. Three agents are initialized in the upper portion of the environment, and the three goals are located in the lower left corner. Agents navigate to the goals based on the different policies they are trained on, and the colored dots show their trajectories. An optimal distance assignment is one where the leftmost agent (Agent 1) selects its nearest goal, the middle agent (Agent 2) chooses the goal in the middle, and the rightmost agent (Agent 3) selects the lower goal. However, this assignment is inherently unfair, as it requires the rightmost agent to cover a significantly longer distance compared to the leftmost agent. A fair"}, {"title": "4.2 Evaluation metrics", "content": "We calculate the following metrics to determine the performance of our method:\n(1) Fairness, which is the total fairness value (Eqn. 1) obtained at the end of the episode, denoted by F (higher the better).\n(2) The fraction of an episode time all agents take to reach their goal, denoted T (lower is better). T is set to 1 if any agent does not reach its goal.\n(3) The total distance traveled by the group of agents per episode D (lower is better).\n(4) Success rate as the percentage of agents able to get to unique goals and become 'done' denoted by S% (higher is better)."}, {"title": "4.3 Effect of goal assignments and fairness in reward", "content": "To evaluate the impact of fairness reward and fair goal assignment, we compare the test performance of our four models trained on 3 agents. We run evaluations with 3, 5, 7, and 10 agents over 100 episodes. Our objective was to observe how the fairness reward and goal assignment strategies influence the agents' behavior and performance metrics during execution, where no goals are assigned to a varying number of agents, which is different from what the model was trained on."}, {"title": "4.3.1 Effect of goal assignments", "content": "The key takeaways regarding the effect of goal assignments are as follows:\n\u2022 The models that were trained without any goal assignments had very low success rates, i.e., the agents did not cover all the goals. We do not consider these models any further.\n\u2022 Our baseline, the random assignment model (RA), improves the fairness metric but achieves this at the expense of the total distance traveled. Additionally, the model has a lower success rate, indicating that the agents could not learn to cover all the goals effectively within the given episode time compared to the other models.\n\u2022 The model trained with the optimal distance cost goal assignments (OA) achieves the best efficiency, traveling shorter distances. However, this is accomplished by a low fairness metric.\n\u2022 Models trained with min-max fair goal assignment (i.e., (FA) and (FA+FR)) learn to effectively navigate the tradeoffs between efficiency and fairness. This can be attributed to the approach of the min-max fair assignment, which incorporates a distance cost metric, achieving a balance between fairness and efficiency. The (FA, *) models also have higher success rates S% and lower fractions of episode time T. These results highlight the efficacy of the min-max fair goal assignment method in optimizing both fairness and efficiency.\n\u2022 Models trained on fair assignments achieve greater fairness than the baseline random assignments models for moderate levels of congestion. We will see later in Section 4.4 that (RA) is fairer in congested settings."}, {"title": "4.3.2 Effect of fairness reward", "content": "We attempt to increase the fairness of the trained model by including a fairness reward in addition to the min-max fair goal assignment (FA+FR). While training this model, the fairness metric is checked at every step of the episode. The agents start navigating to the goals, and the fairness reward is applied to the agents in addition to the distance reward. Thus, agents are incentivized to maintain a high fairness metric throughout the episode. From the results, our method (FA+FR) is among the highest fairness metrics providing models, demonstrating the impact of combining a fair assignment with a fairness reward.\nWe compute the improvement in the median fairness metric F of our (FA+FR) model to the (OA) model for the 3, 5, 7, and 10 agent scenarios as detailed in Table 2. We also calculate the change in the total distance traveled D for the (FA+FR) model to the (OA) model. From all the experiments, we find that the fairness metric for the (FA+FR) model has an average of 20.65\u00b15% improvement when compared to the (OA) model, with only an average of 6.86\u00b16% increase in the total distance traveled by the agents. This shows that training with a fair goal assignment and fairness reward improves overall fairness in the navigation scenario without an extensive tradeoff in efficiency."}, {"title": "4.4 Impact of congestion on overall fairness and efficiency", "content": "In this subsection, we examine the effect of increased agent density on agent performance metrics. We also investigate how obstacles and walls in the environment impact overall fairness. A common challenge in transportation systems is to manage traffic flow in congested areas, such as during urban rush hours. In these scenarios, streets are often packed with vehicles, leading to increased travel times and higher chances of collisions. As the number of agents increases in a fixed environment size, the higher entity density leads to more agent interactions and a greater likelihood of collisions compared to cases with fewer agents. Obstacles and walls can create bottlenecks and restricted pathways, leading to unequal access to resources or destinations for different agents."}, {"title": "4.5 Formation scenario", "content": "Apart from navigation scenarios, our approach can be extended to agents coordinating and forming various shapes. We consider four formation shapes: a circle, a line, an arrow, and a lemniscate curve (or, the infinity shape, with parametric equations $x = \\frac{a sin t}{(1 + cos^2t)}$ and $y = \\frac{a sin t cos t}{(1 + cos^2 t)}$, where a is the half-width), as shown in Fig. 6. The shapes have either a single or pair of landmarks, which could be the center or the endpoints. The shapes are discretized into multiple points equal to the number of agents used for the experiment. These points are the \"expected positions\" the agents can occupy to form the shapes. Similar to the observation vector in the navigation scenario (Sec.3.2.3), the agents are provided the nearest expected position. During training, the reward function assigns positions based on the fair goal assignment scheme or the optimal distance cost goal assignment scheme. These assignments are utilized for agents to learn to reach unique positions and arrange themselves in an equidistant manner. The agents receive a distance-based reward at every time step, and once they get to their assigned position, they receive a goal reward. We also include the fairness reward and train three models of fairness-informed experiments similar to the navigation scenario, namely (FA+FR), (FA), and (OA). We do not consider the randomly assigned goals model (RA) because of its low success rates in the coverage navigation task.\nAs the \"expected positions\" are independent of the shape used in the training, this method can allow agents to come into any desired formation based on the location of the landmark(s). We can utilize a model trained on a circle formation to create any other formation using a different set of discretized expected positions. However, unlike the goal navigation"}, {"title": "4.5.1 Evaluation of formation", "content": "During deployment, we do not assign specific target positions to the agents. Instead, agents rely on the trained goal assignments to arrange themselves on the expected positions observed at each time step. To evaluate the performance of the formation task, we use the following success metric tailored for each formation.\n(1) Circle: Agents must reach within a threshold distance of a given radius to a central landmark.\n(2) Line: Agents must reach within a threshold distance along the line connecting the two landmarks representing the endpoints of the line.\n(3) Arrow: Agents arrange themselves on the two tails of the arrowhead, with the tip represented by the landmark.\n(4) Infinity: The landmark is placed at the midpoint of the formation. Agents arrange themselves on the two lobes, approximated as two circles for ease of calculation. We measure the agent's distance from either of the two centers, which must be within a threshold radius.\nAgents are initialized at random positions and tasked to arrange themselves in various formation shapes within the given episode time. We evaluate our four fairness-informed models described in Sec. 4.1 over 100 episodes. Similar to previous evaluations, we do not assign any target positions, and the agents rely on the assignments they have learned during training to create a formation. We also do not provide any distance or goal-reaching rewards as we do not rely on a centralized critic during execution. We note when agents reach their first position and mark them as 'done' till the end of an episode."}, {"title": "4.6 Limitations", "content": "We identify the following limitations of the proposed approach in its current form:\n(1) Increased fairness at the expense of efficiency: As the number of agents increases, the difficulty of navigating to easily reachable and observable goals also increases. In such cases, agents often prioritize improving"}]}