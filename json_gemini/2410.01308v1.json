{"title": "Rethinking the Expressiveness of GNNs: A Computational Model Perspective", "authors": ["Guanyu Cui", "Zhewei Wei", "Hsin-Hao Su"], "abstract": "Graph Neural Networks (GNNs) are extensively employed in graph machine learning, with considerable research focusing on their expressiveness. Current studies often assess GNN expressiveness by comparing them to the Weisfeiler-Lehman (WL) tests or classical graph algorithms. However, we identify three key issues in existing analyses: (1) some studies use preprocessing to enhance expressiveness but overlook its computational costs; (2) some claim the anonymous WL test's limited power while enhancing expressiveness using non-anonymous features, creating a mismatch; and (3) some characterize message-passing GNNs (MPGNNs) with the CONGEST model but make unrealistic assumptions about computational resources, allowing NP-Complete problems to be solved in O(m) depth. We contend that a well-defined computational model is urgently needed to serve as the foundation for discussions on GNN expressiveness. To address these issues, we introduce the Resource-Limited CONGEST (RL-CONGEST) model, incorporating optional preprocessing and postprocessing to form a framework for analyzing GNN expressiveness. Our framework sheds light on computational aspects, including the computational hardness of hash functions in the WL test and the role of virtual nodes in reducing network capacity. Additionally, we suggest that high-order GNNs correspond to first-order model-checking problems, offering new insights into their expressiveness.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs) have attracted widespread attention in the graph machine learning community due to their impressive performance in areas such as recommendation systems, drug discovery, and combinatorial optimization. One key area of research has focused on characterizing the expressive power of existing GNNs and developing new models with enhanced expressive power. Existing work in this area typically aligns GNNs with various algorithms. One line of research focuses on connecting GNNs to the Weisfeiler-Lehman (WL) graph isomorphism test and its variants. For instance, [50] pioneered the exploration of the relationship between message-passing GNNS (MPGNNs) and the WL test. Several studies [37, 36, 14] have proposed high-order GNNs inspired by the k-WL test and the k-Folklore WL (FWL) test, showing that these models exhibit stronger power compared to standard MPGNNs. Additionally, works such as [4, 13, 16, 54] introduced subgraph GNNs, where subgraphs are obtained through sampling or partitioning, followed by message-passing on these subgraphs. Furthermore, [55, 52] analyzed the counting capabilities of different GNN types. Other studies focus on aligning GNNs with traditional graph algorithms. For instance, [53] designed a GD-WL framework, which incorporates precomputed distance information as additional features in message-passing, enabling the detection of graph biconnectivity. Additionally, [34] attempted to align MPGNNs with the CONGEST model in distributed computing. They used existing lower bounds on the communication complexity of graph algorithms in the CONGEST model to derive lower bounds on the width and depth of MPGNNs when simulating these algorithms.\nWe carefully revisit these works and identify inconsistent or unreasonable results among them:\n\u2022 Underestimated Preprocessing Time Complexity. Some existing works employ preprocessing techniques, such as substructure recognition or distance computation, to enhance their models and show that the proposed models"}, {"title": "2 Preliminaries", "content": "In this section, we first define the notations used throughout the paper. We then provide an overview of the relevant background knowledge, including GNNs, various variants of the WL tests, distributed computing models, and basic concepts in logic."}, {"title": "2.1 Notations", "content": "We use curly braces {\u00b7} to denote a set and double curly braces {{\u00b7} for a multi-set where elements can appear multiple times. [n] is shorthand for the set {0,1,..., n \u2212 1}. Boldface lowercase letters, such as a and b, represent vectors,"}, {"title": "2.2 Graph Neural Networks and Weisfeiler-Lehman Tests", "content": "Graph Neural Networks (GNNs) are neural network models defined on graphs. The most prominent and widely used framework for implementing GNNs, as found in libraries like PyTorch-Geometric [15] and DGL [46], is the message-passing GNN (MPGNN) framework proposed by [19]. The MPGNNs can be formulated as:\n$h_u^{(l+1)} = UPD^{(l)} (h_u^{(l)}, \\{\\{MSG^{(l)} (h_u^{(l)}, h_v^{(l)}, e_{(v,u)}) : v \\in N(u) \\}\\}), \\forall u \\in V,$\nwhere $h_u^{(l)}$ is the feature of node u in the l-th layer, $e_{(v,u)}$ is the edge feature on (v, u), $UPD^{(l)}$ is the updating function in the l-th layer, and $MSG^{(l)}$ is the message function in the l-th layer, which maps the features of a pair of adjacent nodes and the edge feature to another vector called a message.\n[50] claim that the expressive power of MPGNNs is bounded by the Weisfeiler-Lehman (WL) test, which was proposed by Weisfeiler and Lehman in [47] as a graph isomorphism test. Initially, each node is assigned a natural number, called a color, from [n] (usually, all nodes are assigned 0). The iteration formula of the WL test is as follows:\n$C^{(l+1)} (u) = HASH^{(l)} (C^{(l)} (u), \\{\\{C^{(l)} (v) : v \\in N (u) \\}\\}), \\forall u \\in V,$\nwhere $C^{(l)} (u)$ is the color of node u in the l-th iteration, and $HASH^{(l)}$ is a perfect hashing function mapping a multi-set of colors to a new color. It can be observed that the iteration formula of the WL test can be regarded as a special case of MPGNNs, where the message function outputs only the features of the neighboring nodes, and the updating function is a hashing function.\nThere are several variants of the standard WL test, and we will introduce some of them that will appear in our discussions later. A generalization is the higher-order WL tests, such as k-WL or k-FWL, which are defined on k-tuples of nodes in G. The updating formula for k-WL is described in [24] as:\n$C^{(l+1)}(u) = HASH^{(l)} (C^{(l)} (u), \\{\\{C^{(l)} (v) : v \\in N_1 (u) \\}\\} , ..., \\{\\{C^{(l)} (v) : v \\in N_k (u) \\}\\}), \\forall u \\in V^k,$\nwhere u = (u1,...,uk) \u2208 Vk is a k-tuple of nodes, and the i-th neighborhood of u is defined as $N_i(u) = \\{(U_1, ..., U_{i-1}, v, U_{i+1},..., U_k) : v \\in V\\}$, consisting of all k-tuples in which the i-th coordinate is substituted with each node v. Meanwhile, the updating formula for k-FWL is described in [24] as:\n$C^{(l+1)}(u) = HASH^{(l)} (C^{(l)}(u), \\{\\{ (C^{(l)} (U[1] \\leftarrow w), ..., C'(l) (U[k]\\leftarrow w)) : w \\in V \\}\\}), \\forall u \\in V^k,$\nwhere u[i]\u2190w = (U1, \u00b7 \u00b7 \u00b7, U\u017c\u22121, w, Ui+1,\u00b7\u00b7\u00b7, uk) is the k-tuple of nodes where the i-th coordinate in u is substituted with node w.\nAnother variant is the GD-WL framework proposed by [53], which is defined as:\n$C^{(l+1)}(u) = HASH^{(l)} (\\{\\{(d_G(u, v), C^{(l)} (v)) : v \\in V \\}\\}), \\forall u \\in V,$\nwhere $d_G(u, v)$ is a distance, such as Shortest Path Distance (SPD) or Resistance Distance (RD).\nHigh-order GNNs relate to high-order WL tests in the same way that MPGNNs relate to the standard WL test. In other words, if we replace the HASH function in the updating formula of a variant of the WL test with another updating function UPD, we obtain a corresponding GNN model. Therefore, we sometimes use the terms WL tests and their corresponding GNN models interchangeably."}, {"title": "2.3 Distributed Computing Models", "content": "Distributed computing involves multiple processors collaborating to compute a common result. A distributed computing model is an abstract framework used to characterize this process. LOCAL and CONGEST proposed by [32, 33, 38] are two classic distributed computing models based on synchronous message-passing between processors. In this paper, we follow the model definitions from [18]. These models are based on an n-node graph G = (V = [n], E), where G is assumed to be simple and undirected unless stated otherwise. Each node in the network hosts a processor. Initially, each processor knows the total number of nodes n, its unique identifier in [n], and its initial features. In each round, a node computes based on its knowledge and sends messages to its neighbors, which may differ for each. By the end of the round, it receives all messages from its neighbors. In this model, each node must determine its own portion of the output. This process is described in [34] as:\n$s_u^{(l+1)} = UPD^{(l)} (s_u^{(l)}, \\{\\{MSG_{vu} (s_u^{(l)}, v, u) : v \\in N (u) \\}\\}), \\forall u \\in V,$\nwhere $s_u^{(l)}$ is the internal state (which may not be a vector) of the processor at node \u0e19. The primary difference between the LOCAL and CONGEST models is that, in each communication round, the LOCAL model permits nodes to exchange messages of unbounded length, while the CONGEST model restricts messages to a bounded length, typically O(log n)."}, {"title": "2.4 Basic Concepts in First-Order Logic", "content": "First-Order Logic (FOL) is a formal system widely used in mathematics and various fields of computer science. An formula in FOL is composed of variable symbols such as x, y, z, and so on; punctuation symbols like parentheses and commas; relation symbols or predicates such as P, Q, R, and so forth; logical connectives including V, A, \u00ac, \u2192, and \u2194 ; and logical quantifiers, specifically the universal quantifier \u2200 and the existential quantifier \u2203. A sentence is a special case of a formula where all variables are quantified; in other words, there are no free variables. We also introduce an extension to standard FOL called First-Order Logic with Counting (FOLC), which incorporates additional counting quantifiers. Specifically, for any natural number i \u2208 N, we define the counting quantifiers \u2203\u2265i, \u2203\u2264i, and \u2203=x. The expression $\u2203^{\u2265i}x\\phi(x)$ ($\u2203^{\u2264i}x\\phi(x)$, $\u2203^{=i}x\\phi(x)$) means that there exist at least (or at most, exactly, respectively) i elements that satisfy the property 6. We use Lk and Ck to denote the sets of FOL and FOLC sentences, respectively, that use no more than k variables."}, {"title": "3 Issues Due to Absence of Well-Defined Computational Model", "content": "Many studies have analyzed the expressiveness of GNNs, but they lack a well-defined computational model as a foundation for discussion, often relying on ad-hoc methods that lead to unreasonable outcomes."}, {"title": "3.1 Underestimated Preprocessing Time Complexity", "content": "Many GNNs fit into a \u201cpreprocessing-then-message-passing\u201d framework. Given an input graph G with features X, they first perform preprocessing to build a new graph G' with updated features X', followed by message passing on G'. For example, high-order GNNs based on k-WL and k-FWL tests construct graphs G' = (Vk, E') on k-tuples of nodes, where $E' = \\{(u, v) \\in V^k \\times V^k : d_H (u, v) = 1\\}$. Subgraph GNNs and GNNs with additional precomputed features naturally fit this framework. These models typically target algorithmic tasks beyond the capabilities of the standard WL test; however, in some cases, the time complexity of the preprocessing phase exceeds that of the algorithmic task used to show the model's superior expressiveness, which is unreasonable from a complexity alignment perspective.\nFinding Pattern Graphs is Computationally Expensive. One example comes from subgraph GNNs, which identify pattern subgraphs H in the input graph G. We note that without restrictions on H, it implies overly powerful preprocessing capabilities. For instance, [49] proposed the NF-WL, FR-WL, and HLG-WL variants, asserting that certain fragmentation schemes make these models strictly more powerful than k-WL for any k. However, this theorem places no limitations on H, thus implicitly assumes we are capable of counting isomorphic subgraphs. Such preprocessing requirements are overly strong from a theoretical perspective. The counting version of subgraph isomorphism is a #P-Complete problem, as its decision version is NP-Complete [27] and by the definition of #P-completeness."}, {"title": "3.2 Mismatch Between Anonymous WL Test and Non-Anonymous Features", "content": "We observe that many existing works [7, 53, 49] comparing GNNs' expressiveness to the WL test handle the anonymous setting either arbitrarily or in an ad-hoc manner. These works often claim that the standard WL test has limited expressiveness, as it fails to distinguish certain toy examples. Consequently, they propose to incorporate additional features to enhance the expressiveness of their models beyond the capabilities of the WL test. However, we show that their analysis \u2013 that the WL test has weak expressiveness \u2013 holds under the anonymous setting, while their solution of using additional features to enhance expressiveness actually breaks the anonymous setting, leading to inconsistency and mismatch.\nWL Test is Weak in the Anonymous Setting. As described by [24], the WL test operates under an anonymous setting, where all nodes are initially assigned the same color and the process relies solely on node colors, making it unable to distinguish between nodes with identical colors during its iterations. This limited expressive power is often exemplified by its inability to distinguish certain graph pairs, such as two disjoint triangle graphs (C3 U C3) and a six-node cycle (C6). As shown in Figure 1, the \"type\" of each node, which consists of its color and the multiset of its neighbors' colors, is the same (O, {{0, 0}} ) for all nodes in both graphs. After applying the HASH function, all node colors remain identical, so the WL test cannot distinguish between the graphs due to the identical multisets."}, {"title": "3.3 CONGEST Addresses Mismatch but Retains Unrealistic Assumptions", "content": "We observe that one existing work [34] attempts to align MPGNNs with the CONGEST model, which unintentionally resolves the inconsistency in the anonymity setting. However, directly using the CONGEST model as a computational framework introduces a problem: nodes are assumed to have unlimited computational resources, which is unrealistic and leads to impractical outcomes.\nBreaking Anonymity Empowers Models! In the previous section, we noted that many variants enhance the expressive power by adding precomputed features, which may implicitly rely on node IDs to distinguish nodes. This raises a natural question: can we directly improve the expressive power by explicitly incorporating node IDs into the framework of the WL test?\nWhen comparing the equation of MPGNNs with the WL test, it becomes clear that MPGNNs can be viewed as the WL test without the constraints of anonymous nodes and the HASH function. Meanwhile, MPGNNs are similar to the CONGEST model if we compare Equation 1 and 6. Thus, MPGNNs and CONGEST models are expected to have stronger expressive power than the WL tests once these limitations are removed. [34] provides evidence for this through the following theorem by aligning MPGNNs with LOCAL and CONGEST models:\nTheorem 3 ([34]). MPGNN can compute any computable function over connected graphs if the conditions are jointly met: (1) each node is uniquely identified; (2) the message and update functions are Turing-complete for every l; and (3) the depth and width are sufficiently large.\nHence, removing the limitations enhance the expressiveness.\nDirect Use of CONGEST Is Inappropriate. In the aforementioned paper [34], the author proposes using the CONGEST model from distributed computing to characterize MPGNNs, as it permits non-anonymous nodes and supports more complex update functions compared to hash functions, making it a closer representation of the real-world implementation of MPGNNs than the WL test. However, we argue that directly using the CONGEST model as a computational model for MPGNN is not entirely appropriate, as unlimited computational resources assumption can lead to unrealistic and surprising results, as stated in the following theorem:\nTheorem 4. If we allow a single node to have unbounded computational power to solve any computable problem, then every NP-Complete decision problems on undirected unweighted graphs can be solved by the CONGEST model in O(m) rounds.\nThis unreasonable outcome shows that directly using the CONGEST model as the computational model for MPGNNs is inappropriate due to the unlimited computational resources for nodes."}, {"title": "4 Proposed Computational Model and Our Results", "content": "In the previous section, we discussed the inconsistent or unreasonable results in existing studies on the expressiveness of GNNs. We argue that these problems primarily arise from the lack of a well-defined computational model for GNNs, leading researchers to propose various ad-hoc solutions, some of which are inconsistent or unreasonable. In"}, {"title": "Definition 1 (RL-CONGEST Model and Computation Process)", "content": "Given a model width w \u2208 N and a complexity class C (e.g., TC\u00ba2, P), the RL-CONGEST model with width w and computational resource C is defined as a CONGEST model where message sizes are limited to w [log|V(G)|] bits, and nodes can solve any problems in C.\nFor an attributed graph G = (V, E, X, E), where X and E represent node and edge features, the computation of a GNN using the RL-CONGEST model involves the following phases:\n1. (Optional) Preprocessing: Operations such as building a hypergraph (for higher-order GNNs), extracting sub-graphs (for subgraph GNNs), or computing additional features (for distance-based GNNs) occur in this phase, resulting in a new attributed graph G' = (V', E', X', E'). The time complexity of this step must be explicitly provided.\n2. Message-Passing with Limited Computational Resources: Each node u \u2208 V' starts with its node features $X_u$, and the edge features of its incident edges $\\{(v, E_{(u,v)}) : v \\in N (u)\\}$. The message-passing proceeds as in the standard CONGEST model, but with each node allowed to update its internal state using computations in C. The total number of communication rounds corresponds to the GNN model's depth d.\n3. (Optional) Postprocessing: Additional computations, such as a READOUT operation, can be performed after message-passing. The time complexity must be explicitly stated."}, {"title": "4.1 WL Test Requires Large Networks to Compute", "content": "In previous work aligning MPGNNs with the WL test to study their expressive power, researchers aligned the update function of MPGNNs directly with the HASH function in the WL test. [1] noted the challenges in constructing the HASH function for the WL test but did not establish a lower bound on the trade-off between network depth and width. Within the RL-CONGEST framework, we rigorously prove the relationship between the depth and width required for an MPGNN to simulate one iteration of color-refinement in the WL test. This enables us to prove that the HASH function in the WL test is computationally hard, as shown by the following theorem.\nTheorem 5. If an MPGNN can simulate one iteration of the WL test without preprocessing, either deterministically or randomly with zero error, regardless of the computational power available to each node, the model's width w and depth d must satisfy $d = \\Omega (\\frac{m}{D+wlog n})$ given that $w = O(\\frac{n}{log n})$.\nWe defer the formal definition of the problem concerning one iteration of the WL test, along with the proof of the above theorem, to Appendix H. Notably, in our proof, we employed techniques from communication complexity without making any assumptions about the complexity class required for computational resources in the RL-CONGEST model. Therefore, the result also holds for the general CONGEST model, indicating that our findings \u2013 showing that WL-like HASH functions are hard to compute are of independent interest to the field of distributed computing.\nFurthermore, we design a deterministic RL-CONGEST algorithm with a round complexity that nearly matches the lower bound, indicating that the algorithm is near-optimal.\nTheorem 6. There exists a deterministic RL-CONGEST algorithm that can simulate one iteration of the WL test without preprocessing, with width w and depth d satisfying $d = O (D + \\frac{m}{w})$ . Additionally, it is sufficient to set the nodes' computational resource class to C = DTIME($n^2$ log n)."}, {"title": "4.2 Virtual Nodes Reduce Network Size for WL Test", "content": "Several works have attempted to enhance the performance of GNNs by introducing a virtual node that connects to all or some nodes in the original graph [19, 25]. Subsequent studies have analyzed the impact of this node. For instance, [5] show that virtual nodes can bring GNNs closer to a C2 classifier, while [40] compare MPGNNs with virtual nodes and graph transformers. To the best of our knowledge, no prior work has explored how a virtual node helps reduce the network's capacity when simulating one iteration of the WL test. We address this in the following theorem:\nTheorem 7. There exists a deterministic RL-CONGEST algorithm that can simulate one iteration of the WL test by adding a virtual node, which connects to other nodes, as preprocessing. The algorithm operates with width w and depth d satisfying dw = O(\u0394), where \u25b3 is the maximum degree of the graph before the addition of the virtual node. Additionally, it is sufficient to set the nodes' computational resource class to C = DTIME($n^2$ log n).\nSome studies suggest that virtual nodes do not enhance expressive power [56], which contrasts with empirical evidence showing improvements in model performance. We find this is because they often equate \u201cexpressive power\u201d with the ability to compute specific functions, akin to computability. Through our analysis, we introduce a computational model that provides a more refined view of expressive power by examining problem complexity and focusing on resource usage. This approach shows that virtual nodes can reduce the network size required to simulate the WL test, deepening our understanding of their impact."}, {"title": "4.3 Aligning High-Order GNNs with Model Checking is Natural", "content": "In this section, we show from both fine-grained and descriptive complexity perspectives that it is more natural to align higher-order GNNs with the Ck model checking problem. We will begin by introducing the model checking problem and the related model equivalence problem.\nThe Model Checking (MC) problem asks whether, given a model A and a logic sentence \u03c6, the sentence \u03c6 holds in A (i.e., A |= \u03c6). In this paper, we focus on cases where the model is a graph G, and \u03c6 uses only the edge predicate E(x, y) and the equality predicate =(x, y)3. The Lk MC problem is highly expressive, capturing many key problems. For instance, deciding whether G |= \u03c6\u25b3, where $\u03c6\u25b3 := \u2203x\u2203y\u2203z(E(x,y) \u2227 E(y, z) \u2227 E(z, x))$, determines whether G contains a triangle subgraph. Due to its expressiveness, the Lk MC problem has been widely studied. It applies to database queries like SQL [17], formal verification [20], and is central to fine-grained complexity in P [39, 48, 17]. In Appendix K, we provide evidence from theoretical computer science to support the classification of the PNF Ck model checking problem 4 in the \u0398 (min{nk, mk-1}) complexity class.\nAnother related problem is the Model Equivalence (ME) problem. Given two models A and B, and a class of logic sentences, the task is to determine whether for any sentence \u03c6 in that class, A |= \u03c6 if and only if B |= \u03c6. In other words, the goal is to check whether that logic cannot distinguish between the two models. Two important results that connect descriptive complexity and WL tests were proven by [9] and [22], showing that, for any k \u2265 3, the expressiveness of (k \u2212 1)-FWL and k-WL is equivalent to Ck ME problem. Another result by [21] shows that the expressiveness of both the standard WL test and the 2-WL test is equivalent to C2 ME problem. This means that the output colors from WL tests provide only a \"type\" of the graph, and we cannot directly interpret it for specific tasks such as determining whether a graph contains a triangle or is biconnected. The most we can infer is that if two graphs produce the same color multisets, then either both contain a triangle (or are biconnected, respectively), or neither does.\nTherefore, we argue that from a computational model perspective, it is more meaningful to discuss the expressiveness of GNNs in terms of solving problems, such as model checking, rather than limiting the discussion to model equivalence, which only determines whether graph pairs are indistinguishable. A natural approach is to align higher-order GNNs, inspired by k-WL or (k \u2212 1)-WL tests, with the Ck MC problem. We support this claim by proving the following weaker theorem:\nTheorem 8 (Informal). If constructing the k-WL graph and additional features as preprocessing is allowed, the RL-CONGEST model can solve the PNF Ck model checking problem in O(k2) rounds. Additionally, the computational resources required by each node are C = DTIME(k2n)."}, {"title": "6 Conclusions", "content": "In this paper, we identify three common issues in existing analyses of GNNs' expressive power, stemming from the absence of a well-defined computational model. To address this, we introduce the RL-CONGEST model, which includes optional preprocessing and postprocessing phases, as a standard framework for analyzing GNNs. Our framework addresses these issues and produces several noteworthy results, including the hardness of the WL problem, which may be of independent interest to the field of distributed computing. Additionally, we outline some open problems for potential future research."}, {"title": "A A Brief Introduction to Communication Complexity", "content": "To prove lower bounds on the rounds of CONGEST algorithms, a key tool is the communication complexity which was first introduced by [51].\nTwo-party communication complexity involves two participants, Alice and Bob, who collaborate to compute a function f: X \u00d7 Y \u2192 Z, where X and Y are their input domains, respectively. They agree on a strategy beforehand but are separated before receiving their inputs (x, y) \u2208 X \u00d7 Y. They then exchange messages to compute f(x, y), with the goal of minimizing the total number of bits exchanged.\nIn deterministic communication, the strategy is fixed, and the minimum number of bits required to compute f in this setting is known as the deterministic communication complexity, denoted by D(f). Similarly, in randomized communication, where Alice and Bob can use random bits and a two-sided error of e is allowed, the minimum number of bits required is the randomized communication complexity. If the randomness is private, it is denoted by $R_{priv}^e (f)$, and if it is public, it is denoted by $R_{pub}^e (f)$.\nThe Equality (EQ) problem between two n-bit strings, denoted by $EQ_n$: {0,1}\u2033 \u00d7 {0,1}n \u2192 {0, 1}, is defined as\n$EQ_n (x, y) =\n\\begin{cases}\n1, & \\text{x = y,}\\\\\n0, & \\text{otherwise.}\n\\end{cases}$\nIt is arguably the most well-known problem in two-party communication complexity which has been extensively studied. We summarize its communication complexity under different settings in Table 1 below."}, {"title": "BA Brief Introduction to Boolean Circuits", "content": "Boolean circuits are computational models used to represent Boolean function computations. A Boolean circuit with input size n can be described as a Directed Acyclic Graph (DAG), with n source nodes as inputs and one sink node as the output. The nodes represent gates, including NOT (\u00ac, with one input), AND (\u0245, with two or more inputs), OR (V, with two or more inputs), and threshold gates ($Th_{a,\\theta}$), which output 1 if and only if ax > 0, where a and \u03b8 are independent of the input x.\nA circuit family (Cn)n>1 is a sequence of circuits with input size growing from 1 to infinity. A family is L-uniform if a Turing machine can construct Cn in O(log n) space, given n in unary. Circuit families are assumed uniform unless otherwise stated. There are three major circuit complexity classes:\nACk consists of problems solvable by circuits with \u00ac, ^, and V gates, polynomial size, depth O(logk n), and unbounded fan-in.\nNCk includes problems solvable by circuits with \u00ac, ^, and \u2228 gates, polynomial size, depth O(logk n), and fan-in 2.\nTCk comprises problems solvable by circuits with polynomial size, depth O(logk n), unbounded fan-in gates."}, {"title": "C Time Complexity of All-Pairs Shortest Paths in Unweighted Undirected Graphs", "content": "The shortest path problem is one of the fundamental problems in graph theory. The All-Pairs Shortest Path (APSP) problem seeks to determine the shortest path distance between all pairs of nodes in a given graph G. To the best of our knowledge, the fastest algorithm for APSP on unweighted and undirected graphs can be formally stated as follows:\nLemma 1 (Folklore; [41]). The computation of APSP for an unweighted, undirected graph with n nodes and m edges can be achieved with a time complexity of $\u00d5 (min (nm, n^\u03c9))$, where w < 2.372 is the matrix multiplication exponent."}, {"title": "DA Brief Introduction to Resistance Distance", "content": "In this section, we introduce the concept of Resistance Distance (RD), covering its definition and the time complexity of approximately computing All-Pairs Resistance Distances (APRD). We begin with the definition of resistance distance:\nDefinition 2 (Resistance Distance). Given an undirected graph G and a pair of nodes s and t, the resistance distance between s and t, denoted by R(s,t), is defined as:\n$R(s,t) = (e_s - e_t)^T L^{\\dagger} (e_s \u2013 e_t) = L_{ss}^+ - L_{st}^+ - L_{ts}^+ + L_{tt}^+ ,$\nwhere es is a one-hot vector with a 1 in the s-th position, and $L^{\\dagger}$ is the Moore-Penrose pseudo-inverse of the graph Laplacian matrix L := D \u2013 A, satisfying $LL^{\\dagger}$ = I and span($L^{\\dagger}$) = span(L) = {v\u2208 Rn : v\u00af1 = 0}. Here,\n$I = I_n - \\frac{1}{n} \\mathbb{1} \\mathbb{1}^T$\nis the projection matrix onto span(L).\nAs shown by [28], R(s, t) is a valid distance metric on graphs. Additionally, we present the following lemma, which connects resistance distance to spanning trees:\nLemma 2 ([35, 23]). Given an edge (s,t) in an unweighted undirected graph G, we have\n$R(s,t) = Pr_{T \\sim \\mu_G} (I[(s, t) \\in E(T)]),$\nwhere T is a spanning tree sampled from the uniform distribution of spanning trees of G, denoted by \u00b5G, and \u2161[\u00b7] is the indicator function.\nNext, we define the approximate computation of APRD:\nDefinition 3 (Approximate Computation of APRD). Given an undirected, unweighted graph G = (V, E), an error threshold \u03f5 > 0, and a failure probability 0 \u2264 pf < 1, compute a matrix $R \\in R^{n \\times n}$ such that for any node pair u, v,\n$Pr (|R_{uv} \u2013 R(u, v)| > \u03b5R(u, v)) \u2264 p_f$.\nTo the best of our knowledge, the fastest algorithm for approximating APRD can be formally stated as follows:\nLemma 3 ([11]). The approximate computation of APRD for a graph with n nodes and m edges can be achieved with a time complexity of\n$\u00d5 (min \\{ nm, n^\u03c9, k(D^{-1/2}LD^{-1/2}) + n^2 \\}),$\nwhere w < 2.372 is the matrix multiplication exponent, and k denotes the condition number of the matrix."}, {"title": "E Proof of Theorem 1", "content": "Theorem 1. For any integer n \u2265 3, there exist two graphs with n nodes, Pn and Cn, such that for any two adjacent nodes in Pn, the resistance distance is 1, while in Cn, it is 1 \u2212 1/n. Furthermore, Pn is neither vertex- nor edge-biconnected, whereas Cn is both vertex- and edge-biconnected.\nProof. Let Pn be the path graph with n nodes, and Cn be the cycle graph with n nodes."}, {"title": "F Proof of Theorem 2", "content": "Theorem 2. Given any edge (u, v) \u2208 E in an undirected, unweighted graph G, (u, v) is a cut edge if and only if the resistance distance R(u, v) = 1.\nProof. It is straightforward to show that an edge (u, v) is a cut edge if and only if it is included in every spanning tree of G. Therefore, by Lemma 2, we have\n$R(u, v) = Pr_{T \\sim \\mu_G} (I[(u, v) \\in E(T)]) = 1.$"}, {"title": "G Proof of Theorem 4", "content": "Before proving Theorem 4, we present some basic facts about the CONGEST model. First, we show that a spanning tree rooted at a node u can be constructed using the FLOOD algorithm.\nLemma 4 ([38], FLOOD Algorithm). There exists a CONGEST algorithm in which a designated node u \u2208 V can construct a spanning tree T rooted at u with depth depth(T) = max dg (u, v) in max dg (u,v) = O(D) rounds, where D is the diameter of the graph.\nThe idea behind the FLOOD algorithm is straightforward: Initially, the source node u sends a special token to all its neighbors. Each node, upon receiving the token for the first time, stores it and forwards it to its neighbors. If a node receives the token again, it discards it and does nothing.\nAdditionally, we include the following lemmas, which describe the ability to broadcast and collect messages to and from a designated node."}, {"title": "H The Weisfeiler-Lehman Problem and Its Hardness ("}]}