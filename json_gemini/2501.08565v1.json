{"title": "DualOpt: A Dual Divide-and-Optimize Algorithm for the Large-scale Traveling Salesman Problem", "authors": ["Shipei Zhou", "Yuandong Ding", "Chi Zhang", "Zhiguang Cao", "Yan Jin"], "abstract": "This paper proposes a dual divide-and-optimize algorithm (DualOpt) for solving the large-scale traveling salesman problem (TSP). DualOpt combines two complementary strategies to improve both solution quality and computational efficiency. The first strategy is a grid-based divide-and-conquer procedure that partitions the TSP into smaller sub-problems, solving them in parallel and iteratively refining the solution by merging nodes and partial routes. The process continues until only one grid remains, yielding a high-quality initial solution. The second strategy involves a path-based divide-and-optimize procedure that further optimizes the solution by dividing it into sub-paths, optimizing each using a neural solver, and merging them back to progressively improve the overall solution. Extensive experiments conducted on two groups of TSP benchmark instances, including randomly generated instances with up to 100,000 nodes and real-world datasets from TSPLIB, demonstrate the effectiveness of DualOpt. The proposed DualOpt achieves highly competitive results compared to 10 state-of-the-art algorithms in the literature. In particular, DualOpt achieves an improvement gap up to 1.40% for the largest instance TSP100K with a remarkable 104x speed-up over the leading heuristic solver LKH3. Additionally, DualOpt demonstrates strong generalization on TSPLIB benchmarks, confirming its capability to tackle diverse real-world TSP applications.", "sections": [{"title": "Introduction", "content": "The Traveling Salesman Problem (TSP) is an NP-hard combinatorial optimization problem, which has numerous real-world applications (Madani, Batta, and Karwan 2021; Hacizade and Kaya 2018; Matai, Singh, and Mittal 2010). Let G = (V, E) represent an undirected graph, where V = {vi | 1 \u2264 i \u2264 N} is the set of nodes and E = {lij | 1 \u2264 i,j < N} is the set of edges, with N being the total number of nodes. For each edge eij, the travel cost cost(i, j) is defined as the Euclidean distance between the nodes vi and vj. A special node va \u2208 V serves as the depot, from which the salesman starts and ends the trip. A feasible solution of TSP is defined as a Hamiltonian cycle that starts from the depot, visits each node exactly once, and ends at the depot. The objective is to minimize the total travel cost of the solution route T, denoted as\n\nL(T) = \\sum_{i=1}^{N-1} cost(T_i, T_{i+1}) + cost(T_N, T_1), where t_i represents the i th node in the route.\n\nDue to its theoretical and practical interest, various traditional exact/heuristic and machine learning-based algorithms have been proposed in the literature. While exact algorithms are generally computationally infeasible for large-scale instances, heuristics can provide near-optimal solutions for TSP with millions of cities, though they cannot guarantee optimality. They often involve time-consuming iterative searches, making them less suitable for time-sensitive tasks. Machine learning based algorithms, on the other hand, offer high computational efficiency and can achieve solution quality comparable to traditional methods for small-scale TSP instances. However, applying them to large-scale TSP, especially those with over 1,000 cities, remains a challenge. One approach is to leverage pre-trained models from small-scale instances for larger ones, but this often results in poor performance due to distribution shift (Joshi et al. 2022). Training models specifically for large-scale TSP is also impractical due to computational resource limitations.\n\nA basic approach to deal with the large-scale routing problem is to apply the general principle of \"divide-and-conquer\" (Fu, Qiu, and Zha 2021; Pan et al. 2023; Hou et al. 2023; Chen et al. 2023; Ye et al. 2024b; ?; Xia et al. 2024). It involves decomposing TSP into a subset of small sub-problems, which can be efficiently solved using traditional or machine learning algorithms to attain sub-solutions. The final solution is then obtained by combining these sub-solutions. This approach can significantly reduce the computational complexity of the large-scale TSP, and yield high-quality solutions in a relatively short time.\n\nIn this work, we introduce a novel dual divide-and-optimize algorithm DualOpt based on the divide-and-conquer framework. The first divide-and-optimize procedure partitions the nodes into M \u00d7 M equal-sized grids based on their coordinates. The nodes within each grid are initially solved using the well-known LKH3 solver (Helsgaun 2017). Next, an edge-breaking strategy is proposed to decompose the route into partial routes and nodes, significantly reducing the number of nodes. Subsequently, every four adjacent grids are merged into one larger grid. This pro-"}, {"title": "Related Work", "content": "Here we present representative traditional and machine learning-based algorithms to solve TSP, and then focus on the divide-and-conquer algorithms that are more effective for solving large-scale TSP over 1000 nodes.\n\nTraditional Algorithms Traditional algorithms can be roughly classified into two categories: exact and heuristic algorithms (Gutin and Punnen 2006; Accorsi and Vigo 2021). Concorde (Applegate et al. 2009) is one of the best exact solvers, which models TSP as a mixed-integer programming problem and solves it using a branch-and-cut (Toth and Vigo 2002). Exact algorithms can theoretically guarantee optimal solutions for instances of limited size, but are impractical for solving large instances due to their inherent exponential complexity. LKH3 (Helsgaun 2017) is one of the state-of-the-art heuristics that uses the k-opt operators to find neighboring solutions, which is guided by a a-nearness measure based on the minimum spanning tree. The heuristics are the most widely used algorithms in practice, yet they are still time consuming to obtain high-quality solutions when solving problems with tens of thousands of nodes.\n\nMachine Learning-based Algorithms In recent years, machine learning-based algorithms have attracted more interest in solving the TSP. Depending on how solutions are constructed, they can be broadly categorized into end-to-end approaches and search-based approaches.\n\nEnd-to-end approaches generate solutions from scratch. The Attention Model (Kool, van Hoof, and Welling 2019) utilizes the Transformer architecture (Vaswani et al. 2017) and is trained with REINFORCE (Wiering and Van Otterlo 2012) using a greedy rollout baseline. POMO (Kwon et al. 2020) improves on this by selecting multiple nodes as starting points, using symmetries in solution representation, and employing a shared baseline to enhance REINFORCE training. DIFUSCO (Sun and Yang 2024) uses graph-based denoising diffusion models to generate solutions, which are further optimized by local search with k-opt operators. Although DIFUSCO can handle problems with up to 10,000 nodes, it is less suitable for time-sensitive scenarios. Pointerformer (Jin et al. 2023) incorporates a reversible residual network in the encoder and a multi-pointer network in the decoder, allowing it to solve problems with up to 1000 nodes. Search-based approaches start with a feasible solution and iteratively apply predefined rules to improve it. NeuRewriter (Chen and Tian 2019) iteratively rewrites local components through a region-picking and rule-picking process, with the model trained using Advantage Actor-Critic, and the reduced cost per iteration serves as the reward. NeuroLKH (Xin et al. 2021) enhances the traditional LKH3 solver by employing a sparse graph network trained through supervised learning to simultaneously generate edge scores and node penalties, which guide the improvement process. DeepACO (Ye et al. 2024a) integrates neural enhancements into Ant Colony Optimization (ACO) algorithms, further improving its performance.\n\nMachine learning-based algorithms perform well on TSP instances with up to 1,000 nodes, but struggle with larger instances due to the exponential increase in memory requirements and computation time as the number of nodes grows. This leads to memory and time constraints during training, making it difficult to converge to near-optimal solutions. To address this challenge in solving large-scale problems with thousands or more nodes, the divide-and-conquer strategy is often employed. It is always combined with traditional or learning-based algorithms to generate high-quality solutions in a relatively short time. GCN-MCTS (Fu, Qiu, and Zha 2021) applies graph sampling to construct fixed-size sub-problems, solved by graph convolutional networks, with heatmaps guiding Monte-Carlo Tree Search (MCTS). H-TSP (Pan et al. 2023) hierarchically builds TSP solutions using a two-level policy: the upper-level selects sub-problems, while the lower-level generates and merges open-loop routes. ExtNCO (Chen et al. 2023) uses LocKMeans with o(n) complexity to divide nodes into sub-problems, solving them with neural combinatorial optimization and merging solutions via a minimum spanning tree. GLOP (Ye et al. 2024b) learns global partition heatmaps to decompose large-scale problems and introduces a scalable real-time solver for small Shortest Hamiltonian Path problems. UDC (Zheng et al. 2024) proposes a Divide-Conquer-Reunion framework using efficient Graph Neural Networks for division and fixed-length solvers for sub-problems. Soft-Dist (Xia et al. 2024) demonstrates that a simple baseline method outperforms complex machine learning approaches in heatmap generation, and the heatmap-guided MCTS paradigm is inferior to the LKH3 heuristic despite leveraging hand-crafted strategies."}, {"title": "The Proposed DualOpt Algorithm", "content": "The proposed DualOpt algorithm is based on and extends the basic divide-and-conquer method by incorporating a grid-based procedure and a path-based procedure to improve efficiency. Each procedure operates on two levels: the first level is responsible for generating sub-problems, while the second level focuses on solving these sub-problems."}, {"title": "A Grid-based Divide-and-Conquer Procedure", "content": "To decompose the large-scale TSP for efficient solving without significantly downgrading solution quality, we employ an iterative grid decomposition strategy, denoted as the Grid-based Divide-and-Conquer Procedure, as depicted in Algorithm 2. Initially, the node set \u05d0 contains all the nodes of the TSP instance, while the partial route set Y is initialized as empty. In each iteration, the 2D space is discretized into an evenly spaced grid of size K = 2^{Niter-iter} \u00d7 2^{Niter-iter} grids. The node set \u05d0 and partial route set Y are then divided into K subsets {(\u03a51,\u05d0)\u2081\u2081,..., (\u03a5\u03ba,\u05d0)\u03ba},{ based on their positions within the grid. Each of these subsets is solved in parallel using the well-known TSP solver LKH3 (Helsgaun 2017), resulting in K small routes {R1, ..., RK}. If the current iteration (iter) is not the last one, the algorithm proceeds by applying a proposed edge-breaking strategy in parallel, which breaks a subset of the edges of the routes and updates Y and \u05d0 with new sets of partial routes and nodes {(\u03a51,\u05d0)'\u2081\u2081,..., (\u03a5\u03ba,\u05d0)'\u03ba}. As the iterations progress, K decreases and the grid size increases correspondingly until only one grid remains. At this final stage, a reduced problem consisting of partial routes and nodes is formed, from which an initial high-quality TSP solution 7 is obtained.\n\nThe LKH3 Solver The LKH3 solver, a state-of-the-art heuristic for the TSP, is built upon the foundational Lin-Kernighan heuristic, incorporating several advanced compo-"}, {"title": "Edge-breaking Strategy", "content": "Recall that multiple routes are generated, one for each grid within the 2D space. When merging and optimizing these routes, it is essential to reschedule the surrounding nodes of each grid in conjunction with the nodes of adjacent grids, while excluding the internal nodes from this rescheduling process. To achieve this, we propose an edge-breaking strategy that effectively reduces the problem's scale. Specifically, for each grid, we define an internal grid that maintains a fixed spacing from the outer grid. This spacing is calculated as spacing = \\frac{max - min}{2^{Niter+2}}, where Xmax and Xmin denote the maximum and minimum x-coordinates of the grid, respectively. We then remove all edges that lie outside the internal grid, as well as those crossing into it. As a result, the nodes and edges within the internal grid form several partial routes that each may consist of more than two connected nodes, along with isolated nodes."}, {"title": "A Path-based Divide-and-Optimize Procedure", "content": "Based on the solution derived from the grid-based divide-and-conquer procedure, further optimization is achieved through the path-based divide-and-optimize procedure, as detailed in Algorithm 3. Following previous studies (Kim, Park et al. 2021; Ye et al. 2024b; Zheng et al. 2024), this iterative algorithm aims to refine an initial solution 7 by partitioning it into smaller sub-paths, optimizing these sub-paths individually, and then merging them to generate an improved solution T*.\n\nThe procedure begins by iterating over a specified set of sub-path lengths len1,...,lenm and iterations iter1,..., iterm. For each sub-path length len, the initial solution 7 is divided into sub-paths of the designated length. If the length of the last sub-path is shorter than the designated length, it remains unchanged. Each sub-path can be considered as an open-loop TSP with two fixed endpoints. These sub-paths undergo a distribution normalization of vertex coordinates to maintain consistency with the original TSP instance, resulting in normalized sub-paths denoted as SubPathSet'. Following normalization, a neural solver is employed in batch mode to optimize the normalized sub-paths, and sub-paths are updated by the policy when they are better than the current ones, producing a set of sub-paths SubPathSet*. These sub-paths are then merged to form an improved solution T* by connecting their fixed endpoints in their original order. To ensure continuous refinement and overlap during the iterations, the starting point \u03ba for sub-path division is incremented by max(1, len). This iterative refinement process is repeated for each combination of sub-path length and iteration count, progressively refining the manageable sub-paths of the initial solution 7 to yield a highly optimized solution T*."}, {"title": "Neural Solver", "content": "The underlying solver of the path-based divide-and-optimize procedure is an attention-based neural network, which can efficiently solve the obtained sub-paths through batch parallelism.\n\nThe neural network uses an encoder-decoder architecture. The encoder employs self-attention layers to embed the input node sequence, while the decoder generates the node sequence auto-regressively. Each sub-path \u03c0 can be regarded as an open-loop TSP with specified start and end nodes. We employ a modified context embedding inspired by (Kim, Park et al. 2021), defined as h(t) = [h_\u03bc^{(L)}, h_e, s^{(L)}, h_e]. Here, h denotes a high-dimensional embedding vector from the encoder, and L represents the number of multi-head attention layers. h_\u03bc^{(L)} is the mean of the final node embeddings, h_e is the embedding of the previously selected node, and h_e is embedding of the end node of the sub-path. Moreover, following insights from previous studies (Kim, Park et al. 2021; Cheng et al. 2023; Ye et al. 2024b), we leverage the symmetry of sub-paths. Specifically, reversing the traversal direction of a sub-path (moving backward versus forward) yields an equivalent sequence, which helps improve the network's robustness and efficiency.\n\nThe network employs a single-step constructive policy aimed at optimizing the solution by generating sequences in both forward and backward directions. The policy is defined as follows, where s represents a sub-path with a start node \u03c0\u2081 and an end node \u03c0\u03b7. The policy po(\u03c0\u0192, \u03c0\u03b9|s) is parameterized by @ and specifies a stochastic policy for constructing the forward solution f and the backward solution \u03c0\u03b9:\n\nPo(\u03c0f, \u03c0l|s) = po(\u03c0f|s)po(\u03c0l|s)\n\n= \\prod_{t=1}^{n-1} P_\u03b8(\u03c0_{1+t}|s, \u03c0_{1:\u03c4}, \u03c0_\u03b7) \u00d7 \\prod_{t=1}^{n-2} P_\u03b8(\u03c0_{n-t}|s, \u03c0_{n:n-t+1}, \u03c0_1).\n\nThe model is trained using REINFORCE with a shared baseline. The objective is to minimize the expected length of the solutions to the sub-problems. To achieve this, the loss function is defined as the expected length of these solutions. The policy gradient is then computed as follows:\n\n\\nabla L(\u03b8|s) = E_{p_\u03b8(\u03c0f|s)}[R(\u03c0_f) \u2013 b(s)]\\nabla log p_\u03b8(\u03c0_f|s) + E_{p_\u03b8(\u03c0l|s)}[R(\u03c0_l) \u2013 b(s)]\\nabla log p_\u03b8(\u03c0_l|s).\n\nThe shared baseline b(s) is employed to reduce variance and enhance training stability. This baseline is computed by averaging the path lengths obtained from two greedy roll-outs. Note that during model training, both forward and backward solutions are utilized. However, during inference, only the superior trajectory between the two solutions is selected to ensure the best possible outcome.\n\nNode Coordinate Normalization To improve the robustness of the model with respect to the sub-paths, we normalize the node coordinates to be uniformly distributed within the range [0,1] (Fu, Qiu, and Zha 2021). We define Xmax = maxi\u2208G Xi, Xmin = mini\u2208G Xi, \u0423\u0442\u0430\u0445 = maxi\u2208G Yi, Ymin = mini\u2208G Yi as the maximum and minimum values of the horizontal and vertical coordinates of all N nodes in the TSP instance. For each node in the sub-path, we convert its coordinate (xi, yi) to (xi, y) as shown in Eq. (3), ensuring that all coordinates fall within the range [0, 1].\n\nX' = \\frac{X_i - X_{min}}{max(X_{max} - X_{min}, Y_{max} - Y_{min})}\nY' = \\frac{Y_i - Y_{min}}{max(X_{max} - X_{min}, Y_{max} - Y_{min})}"}, {"title": "Experiments", "content": "To evaluate the performance of our proposed DualOpt, we conducted extensive experiments on the large-scale TSP instances, with up to 100,000 nodes. We compared the performance of DualOpt with that of state-of-the-art algorithms from the literature. All experiments, including rerunning the baseline algorithms, were executed on a machine with an RTX 3080(10GB) GPU and a 12-core Intel(R) Xeon(R) Platinum 8255C CPU.\n\nOur evaluation focused on two groups of the large-scale instances: randomly generated instances TSP_random and the well-known real-world benchmark TSPLIB. The TSP_random represents random Euclidean instances, with node coordinates uniformly sampled from a unit square [0, 1]2. This group includes seven datasets, each corresponding to a different number of nodes, denoted as TSPN, i.e., TSP1K (1,000 nodes), TSP2K, TSP5K, TSP10K, TSP20K, TSP50K, and TSP100K (100,000 nodes). For a fair comparison, we used the same test instances provided by Fu et al. (Fu, Qiu, and Zha 2021) and Pan et al. (Pan et al. 2023). Each dataset contains 16 instances, except TSP1K, which includes 128 instances, and TSP100K, which contains one instance. The TSPLIB is a well-known real-world benchmark (Reinelt 1991) that consists of 100 instances with diverse node distributions derived from practical applications, with sizes ranging from 14 to 85,900 nodes. For our experiments, we select the ten largest TSPLIB instances, each containing more than 5,000 nodes.\n\nBaselines We compare DualOpt against ten leading TSP algorithms in the literature. Traditional Algorithms: 1) Concorde (Cook et al. 2011): One of the best exact solvers. 2) LKH3 (Helsgaun 2017): One of the highly optimized heuristic solvers. Machine Learning Based Algorithms: 1) POMO (Kwon et al. 2020): Feature an end-to-end model based on Attention Model, comparable to LKH3 for small-scale TSP instances. 2) DIMES (Qiu, Sun, and Yang 2022): Introduce a compact continuous space for parameterizing the underlying distribution of candidate solutions for solving large-scale TSP with up to 10K nodes. 3) DIFUSO (Sun and Yang 2024): Leverage a graph-based denoising diffusion model to solve large-scale TSP with up to 10K nodes. 4) SIL (Luo et al. 2024): Develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without labeled data, handling TSP instances with up to 100K nodes. Divide-and-Conquer Algorithms: 1) GCN+MCTS (Fu, Qiu, and Zha 2021): Combine a GCN model trained with supervised learning and MCTS to solve large-scale TSP involving up to 10K nodes with a long searching time. 2) SoftDist (Xia et al. 2024): Critically evaluate machine learning guided heatmap generation, the heatmap-guided MCTS paradigm for large-scale TSP. 3) H-TSP (Pan et al. 2023): Hierarchically construct a solution of a TSP instance with up to 10K nodes based on two-level policies. 4) GLOP (Ye et al. 2024b): Decompose large routing problems into Shortest Hamiltonian Path Problems, further improved by local policy. It is the first neural solver to effectively scale to TSP with up to 100K nodes."}, {"title": "Parameter Settings", "content": "For the grid-based divide-and-conquer procedure, we set the value of Niter for different TSP problem scales, as shown in Table 1. The LKH3 solver is used with its default parameters as specified in the literature (Helsgaun 2017) both in DualOpt and baseline. LKH3 baseline runs instance-by-instance. In the grid-based divide-and-conquer stage, LKH3 sub-solver just run parallel for grids in a single instance. For the path-based divide-and-optimize procedure, we train different neural solvers with graph size len = {50, 20, 10}, with node coordinates randomly generated from a uniform distribution within a unit square. During the training process, we use the same hyper-parameter settings as those used by Kool et al (Kool, van Hoof, and Welling 2019). And during test time, we set iter = {25, 10, 5}. Our code is publicly available.\u00b9"}, {"title": "Comparative Studies", "content": "Table 2 presents a comparison of 10 leading algorithms and our DualOpt algorithm on randomly distributed datasets TSP_random. The \"Obj.\" column shows the average objective lengths of the routes obtained by each algorithm for each instance, while the \u201cGap\" column measures the percentage difference between the average route lengths attained by each algorithm and the LKH3, considered as the ground truth, i.e., Gap = \\frac{Obj. of Algo. - Obj of LKH3}{obj. of LKH3} \u00d7 100%. The \"Time\" column indicates the average time required to solve each instance. The \u2020 following the algorithm indicates that the results of this algorithm are drawn from the literature directly and \"-\" means the results are not provided in the literature. The \"OOM\" stands for out of CUDA memory with our platform.\n\nFrom Table 2, we can make the following comments about the TSP_random instances. First, the heuristic solver LKH3 provides the highest quality solutions for all instances except TSP1K and TSP2K, though it requires long time searching. DualOpt matches or improves upon LKH3's results for all instances except TSP5K, and achieves an improved result with an improvement gap up to 1.40% for the largest instance TSP100K, with a remarkable 104x speed-up. Second, DualOpt clearly outperforms machine learning based algorithms in both solution quality and running time. POMO underperforms across all instances, primarily because it cannot be trained directly on large-scale instances, and models trained on small-scale instances fail to generalize effectively to large-scale instances. Compared to the current state-of-the-art SIL, our DualOpt surpasses SIL in all instances except for a tie on TSP1K. Third, DualOpt consistently outperforms all four divide-and-conquer algorithms in terms of the best objective result, achieving better results"}, {"title": "An Ablation Study", "content": "Our DualOpt integrates two important divide-and-conquer procedures to achieve superior performance. To evaluate the impact of each procedure, we conducted an ablation study with two distinct variants of DualOpt: DualOptw/oPath and DualOptw/oGrid\u00b7 DualOptw/oPath employs only the grid-based divide-and-conquer procedure, while DualOptw/oGrid generates the initial solution using a simple yet effective approach called random insertion, which is then improved using the path-based divide-and-optimize procedure. The results of this ablation study are summarized in Table 4. The comparative results clearly demonstrate the significance of both procedures within DualOpt. Specifically, the original DualOpt consistently outperforms the variants, underscoring the importance of combining both the grid-based and path-based procedures for achieving superior performance."}, {"title": "Conclusion", "content": "In this paper, we introduced a novel dual divide-and-optimize algorithm DualOpt to solve large-scale traveling salesman problem, which integrates two complementary strategies: a grid-based divide-and-conquer procedure and a path-based divide-and-optimize procedure. Through extensive experiments on randomly generated instances and real-world datasets, DualOpt consistently achieves highly competitive results with state-of-the-art TSP algorithms in terms of both solution quality and computational efficiency. Moreover, DualOpt is able to generalize across different TSP distributions, making it a versatile solver for tackling diverse real-world TSP applications. The success of DualOpt paves the way for future research into more sophisticated divide-and-conquer strategies and their application to different types of other combinatorial optimization problems."}]}