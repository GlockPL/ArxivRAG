{"title": "Controlling Face's Frame generation in StyleGAN's latent space operations", "authors": ["Agust\u00edn Roca", "Nicol\u00e1s Britos"], "abstract": "Innocence Project is a non-profitable organization that works in reducing wrongful convictions. In collaboration with Laboratorio de Sue\u00f1o y Memoria from Instituto Tecnol\u00f3gico de Buenos Aires (ITBA), they are studying human memory in the context of face identification. They have a strong hypothesis stating that human memory heavily relies in face's frame to recognize faces. If this is proved, it could mean that face recognition in police lineups couldn't be trusted, as they may lead to wrongful convictions. This study uses experiments in order to try to prove this using faces with different properties, such as eyes size, but maintaining its frame as much as possible.\nIn this project, we continue the work from a previous project [1] that provided the basic tool to generate realistic faces using StyleGAN2. We take a deep dive into the internals of this tool to make full use of StyleGAN2 functionalities, while also adding more features needed by the Lab, such as projecting a face from a target face image or modifying certain of its attributes, including mouth-opening or eye-opening.\nAs the usage of this tool heavily relies on maintaining the face-frame, we develop a way to identify the face-frame of each image and a function to compare it to the output of the neural network after applying some operations, such as the modification of the eye-opening. The objective is to have a numeric value that measures how much does a certain operation change the face-frame and to know how much of it is maintained, having a clearer perspective of which face image may be a better candidate for generating false memories.\nWe conclude that the face-frame is maintained when modifying eye-opening or mouth opening. When modifying vertical face orientation, gender, age and smile, have a considerable impact on its frame variation. And finally, the horizontal face orientation shows a major impact on the face-frame. This way, the Lab may apply some operations being confident that the face-frame won't significantly change, making them viable to be used to deceive subjects' memories.", "sections": [{"title": "1 Introduction", "content": "Understanding how our brains store and recover memories can be really powerful. Marketing, psychology, design, biology, medicine or even computer science can have great breakthroughs by knowing how our brains remember things. Better advertisements [2], having a more solid base of knowledge to fight Alzheimer [3], dealing with traumas in an efficient way [4] or creating new models of information based on it [5]; all these can be potential consequences of understanding memories in our brains. However, we also know that our memory is malleable [6]. It can be manipulated and affected by external factors and deceive ourselves [7].\nHaving this information, it is natural to doubt the accuracy of human memory. This is the main reason why Innocence Project is doing its research. They are investigating cases where the eyewitnesses were the main evidence for a conviction. They theorize that many of these cases may be result of corruption, manipulation, or honest mistakes when identifying the culprit of the crime. In 1983, around 52% of wrongful convictions were results of eyewitnesses mistakes [8].\nInnocence Project Argentina, partnered with Laboratorio de Sue\u00f1o y Memoria from Instituto Tecnol\u00f3gico de Buenos Aires (ITBA), to research how humans remember faces, which are the key factors for our brains to identify faces and how accurate is human memory for face identification. The experiments the lab is conducting consists on simulating a crime that a subject witnesses. Later, the subject is given the task to try to recognize the perpetrator of the crime in a lineup where the criminal may be present or not. The objective of the experiment is to know which are the attributes that are common between the real criminal and the identified person.\nDuring this research, they theorized that the contour of the face, the hair and the ears, are the things that our memory first notice in a face. This set of properties is called the \"face-frame\".\nCurrently, the Lab is using a tool to generate faces similar to the criminal in the experiment [1]. This tool is based on NVidia's StyleGAN2 [9], a Generative Adversarial Network (GAN). A GAN is a deep-learning-based model that allows to create synthetic data which is indistinguishable from real data. Nvidia used this model and changed its architecture to create StyleGAN2, capable of generating realistic images of faces, which is why is used by the Lab.\nAlbeit the tool being used by the Lab is capable of generating images of faces, it's still missing some key functionalities to fully take advantage of StyleGAN2, like being able to change one face's attributes using the very same neural network, or mapping an existing face to a one generated by the network.\nThis way, the project can be divided in two main parts. First, we implement the missing functionalities that StyleGAN2 offers: style mixing and projecting an image into the latent space. These functionalities provide additional tools for the Lab to generate faces, especially when they have specific requirements about how the face they want to generate should look like. In addition, we use some latent directions known by the community [10] to be able to modify some attributes of the face. This attributes include eye-opening, mouth-opening, smile, face orientation, presumable age and gender, among others.\nThe second part consists in measuring how much the face differs from the original one after modifying it with these functionalities. These also allows us to find a way to project an image maintaining its frame as much as possible. Style mixing is not part of the experiments as it only changes color scheme and microstructure of the image [11].\nIn this work we will first present the main stakeholders of this investigation, a brief introduction to human's memory, Generative Adversarial Networks and how faces generated from them can be modified within the same network (specifically in StyleGAN2) and this project's initial status in Section 2. In Section 3 and 4 readers will find all the definitions and functions we've used to make our measurements, the faces we've used and the setup we have required. Furthermore, in Section 5 we explain the experiments we have done and in Section 6 we present the results we've obtained making a focus on face-frame deviation when modifying faces when moving across some of the neural network directions and in Section 7 we discuss the importance and significance of these results, and how they may be used. Finally, in Section 8 and Section 9 we conclude this work with future works and acknowledgements."}, {"title": "2 Review of similar works", "content": "Innocence Project is one of the 69 organizations that are part of the Innocence Network, that work to free innocent people from jail and to prevent wrongful convictions. This work can be with legal services or investigation of the case presented. [12]\nIn the United States, the Innocence Project reported over 375 DNA exonerations [13]. The causes of wrongful conviction that the organization are aware of are: eyewitness identification, false confessions, forensic science errors, poor defense, among others. [12]\nInnocence Project Argentina is part of the Innocence Network, and it works in Argentinian cases. This organization is the one that got in touch with Laboratorio de Sue\u00f1o y Memoria from Instituto Tecnol\u00f3gico de Buenos Aires (ITBA).\nThe Lab does research in neuroscience. Some of the areas they are working on are: activation of memories while sleeping, sleep paralysis, wave processing for the brain and false memories formation [14]."}, {"title": "2.2 Memories", "content": "In 2019, Zlotnik and Vansintjan defined memory as \"the capacity to store and retrieve information\" [15]. The process of formation of memories have different phases.\nThe first of these phases is the acquisition. In this phase, the sensory stimuli are encoded into neurochemical representations. The second phase is the consolidation, that is a period in which the memory is stabilized in order to subsist through time. The retrieval phase is where the information in a memory can be recovered. A consolidated memory can go through a re-consolidation phase, in which its information can be modified and it is re-stabilized. [16]\nThis last phase is the one that has a key role for the experiments of the Lab. There are some theories about how false memories are created. Between them, there is the activation-monitoring theory, which explains the creation of false memories using two principles. The first came as a conclusion of an experiment that consists in giving the subject a list of words and later ask if a certain word was in the list or not. It was discovered that if the words in the list were related somehow, and the word asked later followed the same topic, the subject would think the word was in the list, even if it that was not the case. The second principle states that if the person cannot remember the source of the information of interest, that person may create a false memory. [17]"}, {"title": "2.3 Generative Adversarial Networks", "content": "A Generative Adversarial Network (GAN) is a deep-learning-based model introduced by Goodfellow [18], that allows to create synthetic data that is indistinguishable from real data. The model has two neural networks (a generator and a discriminator), and a training data set.\nThe generator's objective is to create data as similar as possible of the training data set. The discriminator's objective is to say which data comes from the generator and which data comes from the training data set just having the data as input. This way, the generator and discriminator competes against each other to obtain better results.\nIf the discriminator fails to tell the source of the data, it means the generator was able to fool the discriminator. In this case, the discriminator will be the one learning from its mistake. If the discriminator is able to tell correctly the source of the data, the generator is the one that will be learning. When the training is complete, the generator network is the one used to keep generating data."}, {"title": "2.3.1 Loss function", "content": "The generator (G) and the discriminator (D) are competing against each other, playing a two-player minimax game. G receives some input (z) and returns a synthetic image. D receives an image, and tries to tell if it is a real image or a synthetic one. This competition can be described with the following function [18].\n$\\min _{G} \\max _{D} E_{x \\sim P_{\text {data }}(x)}[\\log D(x)]+E_{z \\sim p_{z}(z)}[1-\\log D(G(z))]$"}, {"title": "2.3.2 Wasserstein GAN (WGAN)", "content": "WGAN [19] is an alternative algorithm to the traditional GAN algorithm. It proposes a new loss function using Wasserstein distance, the original GAN loss function uses the Jensen-Shannon divergence instead. This change improves the stability of learning and solve problems like mode collapse that the traditional GAN has."}, {"title": "2.3.3 Deep Convolutional GAN (DCGAN)", "content": "DCGANs [20], in comparison with the traditional GANs, changes the architecture of the Generator and Discriminator. This model uses convolutional networks for both of them. The most important result this model gives is the stability it has."}, {"title": "2.3.4 Conditional GAN (cGAN)", "content": "In the traditional GAN, there is no control in the outputs of the generator. This is why cGANs [21] are introduced. A cGAN is an extension of GAN that introduces the possibility of adding labels to the input in order to influence the generator and discriminator networks. This gives some control to the synthetic images the generator creates. It also presents the possibility of generating new tags to an image."}, {"title": "2.3.5 Auxiliary Classifier GAN (AC-GAN)", "content": "AC-GAN [22] is an extension of cGANs that modifies the discriminator to also predict the label (also known as class or auxiliary classifier) of the input image."}, {"title": "2.3.6 Information Maximizing GAN (InfoGAN)", "content": "An InfoGAN [23] is an extension to GANs that is based in information theory. InfoGANs are able to learn disentangled representations in a completely unsupervised manner. This representations are competitive to the ones learned with surpervised methods."}, {"title": "2.3.7 Pix2Pix", "content": "Pix2Pix [24] is a software that uses a cGAN for image-to-image translation. It can be used for different purposes. Some of the known applications are labels-to-street-scene, aerial-to-map, labels-to-facade, day-to-night and edges-to-photo."}, {"title": "2.3.8 Stacked GAN (StackGAN)", "content": "StackGAN [25] proposes an architecture that chains multiple GANs together to create photo-realistic images. The output of one of the GANs is the input of the next one. The idea is to give more resolution to the output image throughout each GAN it passes through. When introduced, it was tested with a text-to-image translation, giving some interesting results, as seen in figure 2.3."}, {"title": "2.3.9 Cycle-Consistent GAN (CycleGAN)", "content": "CycleGAN [26] is an extension to GAN that connects two GANs in a cycle. Its objective is to obtain two mapping functions that translates from one set of images to another. For example, some of the translations can be horses-to-zebras, artist-to-photo and winter-to-summer, and their respective inverses."}, {"title": "2.3.10 Progressive Growing GAN (PGGAN)", "content": "PGGAN [27] introduces a new training methodology for GANs. First it trains a GAN to generate 4x4 images, then it adds another layer to both the generator and discriminator to generate 8x8 images, and so on until the desired resolution (the paper goes up to 1024x1024). The idea behind the algorithm is to first learn greater-scale structure and later on focus in the fine details. This methodology reduces training time and obtains more realistic results."}, {"title": "2.3.11 Style-Based GAN (StyleGAN)", "content": "StyleGAN [9] is an extension of PGGAN that modifies the generator architecture. GANs used to use the vector z directly, but StyleGAN maps it to a new vector w. This new vector w is divided into different layers which are used together with Gaussian noise vectors as inputs of the different layers of the StyleGAN's generation algorithm."}, {"title": "2.4 Facial properties modification within GAN", "content": "In 2020, Erik H\u00e4rk\u00f6nen, Aaron Hertzmann, et al. released a paper titled GANSpace: Discovering Interpretable GAN Controls [28] showing how to find latent directions allowing to modify the properties of the objects in the generated image by the neural network. This was mainly based on Principal Component Analysis (PCA), which provides a much faster way to extract meaningful latent directions in comparison to manual supervision or expensive optimization. The results of this paper can be applied to StyleGAN's 2 architecture, paving the way to modify facial attributes of a generated image within the neural network itself."}, {"title": "2.5 Face editing with GAN", "content": "Another paper was released in 2022 focusing in face editing with GANs moving across a direction in the latent space. Face editing with GAN's - A Review [29], written by Parthak Mehta, Sarthak Mishra, et al, shows how can other classification models can be used, such as logistic regression or SVM, to find the directions that represent a feature in the trained StyleGAN model. Not only that, but they also found a way to find a latent vector which generated image is similar to a target one. This way, a real portrait can be used as an input to find a similar face generated by the neural network, which attributes could then be modified using the directions that represent a feature."}, {"title": "2.6 Initial project status", "content": "This project aims to continue the work of Jimena Lozano and Maite Herr\u00e1n Oyhanarte on the subject of GANs to create and manipulate facial images [1]. This tool was intended as a software to generate realistic facial images with the ability to manipulate their main characteristics, such as eye size and separation, nose size, etc. The resulting software is able to generate new random, artificial facial images from scratch and generate transitions between two generated pictures. StyleGAN 2 neural network was the core of this software to provide those features, paired with an API and a Web application built in Python and React respectively. All of these made possible an initial version of a software that enabled the Lab to improve their preparation workflow for their experiments."}, {"title": "3 Face-frame stabilization in image projection to latent space", "content": "There is no standardized way of defining a face-frame. We define it as the set of characteristics of a face that gives context for the internal characteristics of such face. A face-frame includes the contour of the face, hair, ears. A face-frame does not include eyes, mouth, nose or eyebrows. For example, Figure 3.1 shows two faces with the same frame.\nThe hypothesis of the Lab is that this two faces may be mistaken with each other because they share the same frame.\nHaving this in mind, we measure the variation of the face-frame between two images based on the position of the face and hair in the image."}, {"title": "3.1.1 Image segmentation", "content": "The first thing we need to do is to identify the face and hair inside an arbitrary image. This consists of a simple segmentation problem, separate the pixels of an image in three groups: face, hair and background. For this, an open-source pretrained neural network is used [30]. This network proved to give satisfying results for different faces and images with different conditions. This is essential for the measurement because there is no need to modify any parameters for different images. It is worth mentioning that although the neck is not part of the face-frame, the Lab thinks is better if it is preserved. Therefore, for the objective of this work, it is acceptable that the neural network classifies the neck as part of the face. In Figure 3.2, we can see some examples of segmentation made by this network. In yellow the pixels classified as hair; in blue the pixels classified as face; and in purple the pixels classified as background."}, {"title": "3.1.2 Face-frame variation formula", "content": "Once each pixel of each of the two images is classified, we compare the classifications between the pixels in the same position of the two images. Let f be a function that compares the classifications of two pixels,\n$f(c_1, c_2)=\\begin{cases}0&c_1 = c_2\\\\0.2 & c_1 = \\\"Face\\\" \\land c_2 = \\\"Hair\\\"\\\\0.2 & c_1 = \\\"Hair\\\" \\land c_2 = \\\"Face\\\\1 & otherwise\\end{cases}$"}, {"title": "3.2 Correction for image projection to latent space", "content": "important than variations involving the background of the image.\nLet F be the function that measures the variation of face-frame between two images (I1 and I2) of same size, with heighth and width w. The classifications of the pixels of I\u2081 are Pi,j, and the classifications of the pixels of I2 are qi,j.\n$F(I_1, I_2) = \\frac{\\sum_{i=1}^{h} \\sum_{j=1}^{w} f(p_{i,j}, q_{i,j})}{h * w}$\nThis function yields a percentage of the images that is classified differently, giving more importance to background variations. If the face-frame does not vary between two images I\u2081 and I2, then F(I1, I2) = 0. If there is a change in at least one pixel, then 0 < F(I1, I2) \u2264 1.\nAs mentioned in section 4.1, StyleGAN2 uses a metric [31] to estimate perceptual similarity to the target image. Albeit this method works pretty well, it does not take into account the face-frame variation. To try to obtain results with the least variation of face-frame as possible, a post-processing algorithm (henceforth referred to as face-frame correction) has been designed to pick the best image, which has the lower variation of them all, using the function defined in 3.1.2 to measure this metric.\nThe correction takes the target image and the latent code of the projection as an input. First, it calculates the standard deviation of 10000 random latent codes that create realistic images, which allows us to apply noise to the original latent code in a way that we can be certain it will yield a realistic image. The f(targetimage, G(initiallatentcode)) is also calculated and stored in this first step. The noise strength in each iteration is the result of applying the following formula:\n$strength(i) = I_{std} * noise_0 * (\\frac{1 - i}{nrl} * \\frac{i}{10000})^2$\nBeing i the current iteration number, Istd the standard deviation of the latent codes previously mentioned, noise0 a constant representing the initial noise factor, and nrl a constant representing the noise ramp length. In these experiments, the values are noise0 = 0.005 and nrl = 0.75, which follow what StyleGAN2 uses [32]\nThroughout every of the n iterations, the algorithm introduces a Gaussian noise multiplied by strength(i) to the latent code. Depending on the constants noise0 and nrl, it can be expected that the output image differs too little from the original one so that, if this new latent code generates an image with less face-frame variation than the previous latent code, then this new one is stored and used as the current latent code. If that is not the case, the original is preserved. In Figure 3.3, we can see an example of a target face being projected into the latent space as it outputs the neural network and another one after applying the correction algorithm.\nHaving set fixed values of noise0 = 0.005 and nrl = 0.75 taken from StyleGAN2 [32], the optimal value of n is going to be studied having in mind the reduction of the face-frame variation of the target image and the projected one as much as possible."}, {"title": "4 Materials And Methods", "content": "The image to latent space projection operation StyleGAN2 provides, takes advantage of the fact that the latent space is semantically smooth. This means that small changes in the input vector, will result to small changes in the resulting image. A random input vector is used to start with. This vector is slightly modified and the resulting image is compared to the target image. It uses a standard LPIPS metric to estimate perceptual similarity between an image and the target image [31]. If the modification resulted in a better similarity to the target image, it is maintained. This process of slight modifications and measurements is done a thousand times.\nThe target image's frame and the resulting image's frame are compared.\nWe can take advantage of the fact that there are multiple latent directions we can move through, modifying certain attributes of an image. Some of this latent directions are of interest as they can be used by the Lab to edit an image right in the neural network itself. We define an operation done on an image which originates by a latent code as an addition or subtraction of a vector in such a way that a property of the face is modified.\nHaving this in mind, we will study the face-frame variation when applying some of these operations. Let O be the operation to be studied. The face-frames to compare are the ones from I\u2081 and O(I\u2081) = I2. As the results may vary from image to image, the measurement is taken as the average measurement of a hundred different images. The same hundred images are used in each operation."}, {"title": "4.2.1 Moving through latent directions", "content": "The age direction transforms the face in a way that it looks like older or younger version of the original face."}, {"title": "4.2.1.2 Gender", "content": "The gender direction transforms the face in a way that it looks like a more masculine or feminine version of the original face."}, {"title": "4.2.1.3 Horizontal orientation", "content": "The horizontal orientation direction transforms the face in a way that it looks like rotating horizontally."}, {"title": "4.2.1.4 Vertical orientation", "content": "The vertical orientation direction transforms the face in a way that it looks like rotating vertically."}, {"title": "4.2.1.5 Eyes open", "content": "The eyes open direction transforms the face opening or closing its eyes."}, {"title": "4.2.1.6 \u039couth open", "content": "The mouth open direction transforms the face opening or closing its mouth."}, {"title": "4.2.1.7 Smile", "content": "The smile direction transforms the face adding or removing a smile."}, {"title": "4.2.2 Style mixing", "content": "Style mixing is an operation that takes two images, and generate other two images exchanging their styles. [9] This means that their layout of the face is maintained but the style of the image will be the one of the other input image."}, {"title": "5 Experiments", "content": "In order to be examine the effectiveness of the face-frame correction algorithm proposed in Section 4.1, we first need to take a look at how well StyleGAN2 does when projecting a target image to the latent space in terms of face-frame variation. To measure this, we project 10 images of different faces to the latent space without using the correction algorithm, and we measure the variation using the function defined in Section 3.1.2. These results are then processed into a table allowing us to have a baseline of how much the variation is using the default StyleGAN2's function."}, {"title": "5.2 Correction for projection", "content": "After examining the variation that StyleGAN2 introduces, we can then study how well our proposed algorithm corrects this variation. In order to test this, we set n = 2000 and we run our algorithm for each of the 10 output images of the previous Section 5.1. We then compare the variation in each iteration, allowing us to study the optimal n in terms of computational time and reduced variation."}, {"title": "5.3 Latent directions", "content": "We also want to take a look at the face-frame variation for the 10 images when transforming those moving across the aforementioned latent directions. This allows us to rank each operation in respect of the variation they introduce on the generated image, making them more or less elegible for image modification if face-frame stabilization is of the essence."}, {"title": "6 Results", "content": "Table 6.1 shows the face-frame variation that the image to latent space projection achieved with each image. The maximum variation is at 3.192% while the minimum at 0.747%. The mean variation between these images is of 1.812%. This results suggest that images are a decent starting point when trying to generate similar faces with minimum face-frame variation."}, {"title": "6.2 Correction for projection", "content": "Figures 6.2 and 6.3 illustrate the improvement in the face-frame variation through the iterations of the correction. The curves of variation significantly decrease in the first 750 iterations. The last 1250 iterations takes account for the last 10% of the improvement. It is notable that the improvement in the variation is always superior to the 20% of the initial value, and in some of the cases, it goes over the 50%."}, {"title": "6.3 Latent directions", "content": "We also check the face-frame variation for the 10 images when transforming those moving across the aforementioned latent directions. In Figure 6.5, we can see the chosen images. They are labeled as 1 to 10 from left to right.\nWhen an image moves a great magnitude through a direction, they tend to give unexpected results, deforming the base structure of the face and giving unrealistic images. This is the reason the range measured in each direction changes in each case. For the set of chosen images, we decided the following ranges:"}, {"title": "7 Conclusion", "content": "In this work, we have explored how different operations in StyleGAN2's latent space affect the face-frame of the resulting facial image. We have also studied how StyleGAN2's image projection affects the face-frame of the original image and we proposed an algorithm to reduce this variation when projecting an image as much as possible.\nThis algorithm can be useful when trying to project a target image generating a new one which is not only similar to the original one in terms of facial characteristics, but it also keeps most the facial frame as much as possible.\nBesides that, the exploration of the latent space can be useful to understand some underlying aspects of this generative model and the interpretable latent directions. The face-frame variation seems to increase with the magnitude of the movement through any of the studied directions in a linear way. However, the rate of such variation is different for each direction. For example, mouth-open and eyes-open directions presented the least variation while the horizontal and vertical face orientation, age and gender presented the greater variation, which is somewhat expected as the face-frame is different depending on the face's orientation and as the face have different characteristics depending on the age or gender, which may include parts that define its frame.\nWhen taking into consideration that the main goal of this paper is to help Laboratorio of Sue\u00f1o y Memoria prove that face-frame plays a key role in the creation of false memories to convict innocent people in identification parades in a crime and that most of the known facts in these crimes are age and gender, these results can be useful for them to project target images faces and modify their relevant features changing as less as possible their frames."}, {"title": "8 Future Work", "content": "The face-frame correction algorithm and the variation function proposed in this paper provide an insight on how the face-frame may be maintained when a facial image is projected onto the latent space. However, we provide an initial approach which can be further enhanced in order to continue to minimize the face-frame variation while maintaining or even improving the output image fidelity. Here are some ideas we propose:"}, {"title": "8.1 New directions", "content": "Latent directions are the key to modify faces inside the latent space. However, only some of the known latent directions have been studied. New directions can be studied to understand how the latent space is arranged. To find new directions, an unsupervised discovery method can be used, for example the one Voynov and Babenko propose [33].\nIf one of the found directions is interpretable in a similar way than other known, it is possible to compare them by their face-frame variation to choose between them for a better option.\nThis will allow further face's characteristics modification within the net itself in such a way that it could be possible to help the Lab disprove the belief that police lineups are helpful when searching for a crime perpetrator."}, {"title": "8.2 Different face-frame variation functions", "content": "The face-frame variation function used for this project described in section 3.1.2, is a first version that was created with this project in mind and isn't flawless: sometimes it confuses facial hair, such as beard, as being part of the face and sometimes it labels it as part of the background. The neck is also sometimes considered part of the face, which in reality isn't. There may be a more precise way to measure the face-frame variation between two images that we are not aware of and which can solve this issues.\nOne approach could be taking advantage of facial landmark detection algorithms [34] to also avoid losing facial characteristics, as our algorithm only considers the face-frame, but it doesn't take into consideration all of the facial characteristics."}, {"title": "8.3 Improvement of face-frame variation when modifying faces", "content": "The face-frame correction algorithm used is only applied and tested when projecting images to the latent space. However, it may also be used to reduce face-frame variation when performing some operations to a facial image, such as modifying its eye-opening."}, {"title": "8.4 Different generative networks", "content": "This project was done using StyleGAN2 [9] as it is the state of art of face images generation at the moment. However, the same experiment can be done using different networks.\nText-to-image networks are starting to be popular, it may exist a way that tools such us DALL-E [35] or Stable diffusion [36], can be used to modify faces with similar results as the ones obtained with StyleGAN2. In fact, DALL-E 2[37] allows for replacing parts of an image using a text description entered in a prompt, in a process called inpainting. This allows for photo realistic modification of images, which yields a credible image in a fast and accurate way."}]}