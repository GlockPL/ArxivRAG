{"title": "Label-template based Few-Shot Text Classification with Contrastive Learning", "authors": ["Guanghua Hou", "Shuhui Cao", "Deqiang Ouyang", "Ning Wang"], "abstract": "As an algorithmic framework for learning to learn, meta-learning provides a promising solution for few-shot text classification. However, most existing research fail to give enough attention to class labels. Traditional basic framework building meta-learner based on prototype networks heavily relies on inter-class variance, and it is easily influenced by noise. To address these limitations, we proposes a simple and effective few-shot text classification framework. In particular, the corresponding label templates are embed into input sentences to fully utilize the potential value of class labels, guiding the pre-trained model to generate more discriminative text representations through the semantic information conveyed by labels. With the continuous influence of label semantics, supervised contrastive learning is utilized to model the interaction information between support samples and query samples. Furthermore, the averaging mechanism is replaced with an attention mechanism to highlight vital semantic information. To verify the proposed scheme, four typical datasets are employed to assess the performance of different methods. Experimental results demonstrate that our method achieves substantial performance enhancements and outperforms existing state-of-the-art models on few-shot text classification tasks.", "sections": [{"title": "1 Introduction", "content": "Text classification is a cornerstone in Natural Language Processing (NLP) [14, 27]. Traditional deep learning approaches for text classification heavily rely on large-scale labeled data to achieve satisfactory generalization performance on unseen datasets [3,11]. However, in many real-world scenarios, such as medicine, finance, and biology, obtaining sufficient labeled data is challenging due to privacy concerns and costs. Unlike conventional machine learning systems, humans possess a remarkable ability to rapidly acquire new knowledge and establish cognition through limited examples. For instance, a child can master the names and traits of various animals and accurately differentiate them after viewing only a small number of animal images. The rapid learning capability has given rise to"}, {"title": "2 Related Work", "content": "2.1 Fine-tuning Based approaches\nThe Fine-tuning method utilizes a pre-trained model on source classes as a starting point and adapts to new tasks by making minor parameter adjustments on target classes. This method fully leverages the rich knowledge from source classes while avoiding the dilemma of training the model from scratch on target classes. The key to the fine-tuning method lies in how to effectively utilize the knowledge from the pre-trained model and find the most suitable parameter configuration for the characteristics of the target task. Howard et al. [10] proposed ULMFIT, a universal language fine-tuning model aimed at being applicable to all NLP tasks. Nakamura and Harada [21] demonstrated that employing an adaptive gradient optimizer during fine-tuning enhances test accuracy. Suchin et al. [7] performed a second pre-training step on the pre-trained model before fine-tuning, including domain-adaptive and task-adaptive pre-training.\nRecently, innovative prompt-based methods have been presented [29, 31]. These methods introduce additional prompt information into the model's input to guide the learning process and have shown promising performance in FSTC tasks. Prompts typically consist of selected words, phrases, or sentences that provide context and semantic information for the target task [18]. However, when the source and target datasets exhibit significant distribution shifts, fine-tuning based methods face the challenge of over-fitting in few-shot scenarios with scarce data.\n2.2 Meta-learning Based approaches\nMeta-learning intends to build models that can rapidly acclimate to target tasks of unseen classes with some small tasks constructed by a few samples"}, {"title": "3 Problem Formulation", "content": "In this chapter, we will present a summary of the standard meta-learning architecture and describe the the problem formulation of FSTC tasks. Formally, let Ytrain, Yvalid, and Ytest represent the Non-overlapping sets of training, validation, test classes and use S, Q as support set and query set.\nTraining Phase For a n-way k-shot text classification problem, at each episode, we first sample n classes from Ytrain. For each class, k instances are sampled to form S, and another disjoint m instances are sampled to form Q. We use (xic,i, yc,i) to denote the i-th instance in S, where xic,i is the input text and yc,i is the corresponding label. Respectively, (xq,jyq,j) is used to denote the j-th instance in Q. The model's goal during training is to learn how to classify Q based on S.\nTesting Phase After training, the model is evaluated on Yvalid and Ytest, where the classes are unseen to those in Ytrain. The evaluation process is similar to the training phase: for each episode, n classes are sampled from Yvalid or Ytest, and k instances per class are sampled to form S, with another disjoint m instances forming Q. The model's performance is assessed based on its ability to classify Q using S."}, {"title": "4 Method", "content": "4.1 Feature Extraction with Label Information\nIn this work, We transform labels by utilizing artificially constructed templates. First, each label is expressed as a structurally complete sentence, and then it is concatenated with the original text to form a coherent text that simultaneously contains sentence semantics and label semantics. We design a series of label templates by adding prompt words. After repeated experiments, \"Overall, the topic of the text is\" performs the best in various datasets. Therefore, in"}, {"title": "4.2 Label-semantic Augmented Supervised Contrastive Learning", "content": "To further improve the pre-trained model's capacity to utilize and integrate label semantics, we incorporate the idea of using label semantics to enhance feature extraction into the contrastive learning framework. Supervised contrastive learning is employed to bring text representations of the same class closer and push away those of different categories. Guided by enriched semantic information for class labels, contrastive learning can identify more similar features within the same class and more distinct features across different categories.\nUnder N-way K-shot setting, given S and Q in an episode, we combine the N * K support instances {(xic,i,lc),yi} and the N * M query instances {(xq,jyq,j)} into a single set D = {xp, yp}p\u2208I(K+M), where the index of the instances is I = {1, 2, ...NK, ..., N(K + M)}. It is a critical step to construct positive pairs and negative pairs for contrastive learning. Following Khosla et al.'s supervised contrastive learning method [12], we construct positive pairs (x, x+) and negative pairs (x,x\u2212) based on the instance label in one episode. Specifically, for each instance xe in the set D, (K+M-1) instances from the same class c are selected as positive instances, and (N \u2212 1)(K + M) instances from different classes are selected as negative instances. Considering instance xp, the contrastive loss Lcon can be formulated as follows:\nsim(v1, v2) = v1 \u00b7 v2/||v1||||v2||\nLconp = \u2212 1/|H(p)| \u03a3h\u2208H(p)log exp(sim(vp, vh))/\u03c4 /\u03a3\u03c4\u2208A(p) exp(sim(vp, v\u03c4 ))/\u03c4,\nH(p) = {h \u2208 A(p)|yh = yp},\nA(p) = I \\ p,\nwhere sim(v1, v2) is cosine similarity between two text representations v1 and v2, Lconp represents the contrastive learning loss for sample p. Let A(p) denote the set of all samples excluding sample p, and let H(p) represent the set of samples that share the same label as sample p.\u03c4 \u2208 R+ represents the temperature factor used to adjust the scale of the similarity score."}, {"title": "4.3 Update Prototype network through Attention Mechanism and Label-semantic", "content": "The attention mechanism employed in this work is inspired by the HATT [5] model, we only use the instance-level attention and incorporate it into the prototype network. By considering the relevance between support samples and query samples, the attention mechanism assigns different weight parameters to each support sample. This allows the prototype network to focus more on support samples that are closely related to the query samples in specific aspects. As a result, when calculating class prototypes, these relevant support samples carry greater weight compared to others. Specifically, for a meta-learning task consisting of n classes sampled using N-way K-shot strategy, we first calculate attention weights \u03b3i of each support sample (xic,i, lc) from class c with respect to all query samples xq in the query set using the following formula:\n\u03b3i = exp(ei) /\u03a3kK=1 exp(ek),\nei = sum{o(g(vci) \u2299 g(vq))},\nvci = fo(xci, lc), vq = fo(xq),\nwhere vci are the text representations of the i-th support sample in the c-th class. Respectively, vq \u2208 Q is from the same class, but the model is unaware of its class. g(\u00b7) constitutes a fully connected layer that maintains identical dimensions for both its input and output. \u2299 is element-wise multiplication, and o is tanh activation function.\nThen, the class prototype wc of any given class c will be expressed as the weighted average sum of the support samples. Given a query sample xq to be classified, we can get its probability of belonging to each class by calculating the cosine similarity with the class prototypes wc. The process can be described as:\nwc = \u03a3(xci,lc)\u2208Sc \u03b3i fo(xic, lc),"}, {"title": "4.4 Objective Function", "content": "Our work uses multi-task learning to optimize the model, addressing the overfitting issue common in FSL. To mitigate this, we introduce two tasks during training: contrastive learning and prototype learning. The contrastive learning task leverages supervised contrastive learning to obtain more discriminative feature representations. The goal is to cluster text representations of the same class while separating those of different classes. Meanwhile, the prototype classification task uses an attention-based prototype network to create more representative prototype representations for each class. The model is guided to optimize its learning towards the correct class for the query text. The objective function L is defined as:\nL = (1 \u2212 \u03c1)Lpn + \u03c1Lcon,\nwhere \u03c1 serves as the weight parameter that balances the two loss. In our work, it is fundamental to find a proper \u03c1 and the changing strategy during the training process to balance the two tasks. Our optimized settings are presented in the experiment section."}, {"title": "5 Experiment", "content": "5.1 Datasets\nFollowing ContrastNet [2], our experiments are conducted on 4 news and review datasets that are widely used in FSTC tasks. Details are shown in Table 1.\n20News [13] comprises 18,820 documents that originate from 20 different newsgroups. The average length of sentences is the longest among our 4 datasets.\nAmazon [9] is composed of 142.8 million reviews across 24 categories. We adhere to the setting in [8] which only uses a subset containing 1000 reviews per class.\nHuffPost [1] is a dataset of news articles from the HuffPost website. It contains 900 samples per class and 41 classes.\nReuters [1] is a news dataset sourced from Reuters newswire in 1987. We follow [1] and only use 31 classes with 20 samples per class."}, {"title": "5.2 Baselines", "content": "We evaluate our method against the following baselines:\nPN [24](Prototype Network) is a straightforward and effective metric-based FSL method that aligns query instances with class prototypes.\nMAML [4] is an optimization-based approach that enables rapid acclimatization to new tasks through just several gradient updates.\nInd [6](Induction Network) introduces a dynamic routing algorithm to learn class-level representaions, which shows strong performance in FSTC tasks.\nDS-FSL [1] uses a MLP (Multilayer perceptron) in Prototypical Network and extends the model. It also builds a meta-learner features generalization ability.\nMLADA [8] introduces an adversarial network to heighten the cross-domain transferability of meta-learning and create superior sentence embeddings.\nLaSAML [19] is a method that incorporates the label information in FSL to produce discriminative feature of input texts by adding label token after the text token.\nContrastNet [2] is a supervised contrastive learning model that involves both the support and query instances."}, {"title": "5.3 Implement Details", "content": "The model undergoes training and evaluation on standard 5-way 1-shot and 5-way 5-shot FSTC tasks across multiple datasets. Consistent hyperparameters and training methodologies are applied to all datasets, following the approach outlined by [2].\nWe use Adam optimizer at the learning rate of le-6 and an early stopping strategy is implemented with a patience of 3 epochs, activated if validation accuracy fails to improve. Training is capped at 10,000 iterations, with evaluations every 100 iterations. Validation and test results are averaged over 1,000 episodes. 5-fold cross-validation is employed to enhance evaluation. The max sequence length is 256. The temperature factor \u03c4 for contrastive learning is set to 5, and the loss weight \u03c1 increases linearly from 0 to 1 during training."}, {"title": "5.4 Experiment Results", "content": "Table 2 compares the performance of our model against the baselines across four datasets. Our model surpasses all baselines in both settings for four datasets, except in the Reuters 5-shot setting. The HuffPost appears to be the most challenging due to the short sentence length.\n1-Shot Performance Our model surpasses all baselines in the 1-shot setting. It achieves an average accuracy improvement of 4.7% and an average F1 improvement of 5.4% comparing to the best baseline. On the most challenging dataset, our model shows the largest improvement, with a 7.5% improvement in accuracy and an 8.4% improvement in F1 measure. The result reveals that our model effectively grasps the semantic context of the label and the text, especially when the length of the text is short.\n5-Shot Performance Comparing to the best baseline, the model demonstrates an average accuracy improvement of 2.6% and an average F1 improvement of 2.9% in the 5-shot setting. In particular, our model also shows up to a 3.9% improvement in accuracy and a 4.4% improvement in F1 on the HuffPost. In the Reuters 5-shot setting, our model did not achieve optimal performance. This may be attributed to the characteristics of the Reuters dataset, which features"}, {"title": "5.5 Ablation Study", "content": "We will investigate how components contribute to the overall method and demonstrate the effectiveness of each part individually and in combination in this section. As shown in Table 4, we achieve 8.12% and 3.48% improvement in 1-shot and 5-shot settings by means, compared to the Prototypical Network with BERT."}, {"title": "Effect of Attention Mechanism in Prototypical Network", "content": "We denote the Prototypical Network with attention mechanism as PN-ATT. In the original method, prototypes are the mean of support instances, making the prototype a single instance in the 1-shot scenario. Consequently, in the ablation study shown in Table 4, this improves the model's 5-shot performance but not the 1-shot setting. The attention mechanism helps produce more representative class prototypes, as shown in the Table 3 and 4."}, {"title": "Effect of Contrastive Learning", "content": "Contrastive learning helps the model to extract more robust and distinctive features from input texts by using all the instances in the episodes, which can make the prototype more representative. The result in Table 4 shows that it can improve the model's performance by 1.08% and 1.66% in 1-shot and 5-shot settings, respectively."}, {"title": "Effect of Label Template", "content": "Label Template shows the greatest improvement in both settings among three methods. By simply adding the label token after the text token, the model performance is dramatically improved, especially in the 1-shot scenario. The efficacy of the Label Template technique underscores its promise as a strong strategy for increasing the accuracy and reliability of few-shot learning.\nIn addition, we note that in the 5-shot setting, the model that combines attention mechanisms, contrastive learning, and label templates does not surpass the model solely utilizing label templates in handling tasks from HuffPost and Reuters. This could be attributed to the former's more intricate structure and higher number of parameters, potentially increasing the likelihood of overfitting."}, {"title": "5.6 Convergence Speed and Visualization", "content": "Convergence Speed We also compare the convergence rate of our model with two strong baselines on the HuffPost dataset. The figure 2 shows that our method converges 2.62 times faster than ContrastNet under 5-shot setting with 8.49% accuracy improvement.\nVisualization We use t-SNE [20] to visualize our learned embeddings, as done by [19] and [2]. The model trained on 1-shot tasks is chosen instead of the 5-shot tasks to better demonstrate its learning and generalization abilities. ContrastNet and LaSAML, two strong methods, are chosen as our baselines for comparison. The results shown in Figure 3 demonstrate that our embeddings are more discriminative between different classes and more compact within the same class compared to ContrastNet. LaSAML shares a similar trend with our label template method, but our embeddings display more evenly distributed class representations and clearer boundaries between classes. This indicates our model better captures intra-class and inter-class semantic differences, as reflected in Table 2."}, {"title": "6 Conclusion", "content": "In this paper, we put forward a novel meta-learning framework for FSTC task. With the guidance of the label template, our method employs the supervised contrastive learning and attention mechanism to enhance the capability of the Prototypical Network. The framework can acquire more typical and robust feature vector for FSTC tasks with just few iterations. Extensive experiemntal results confirm the superiority of our method over traditional prototype networks and most of strong baselines, notably in the 1-shot scenario. In upcoming research, we aim to expand the proposed framework to include a wider range of few-shot tasks."}]}