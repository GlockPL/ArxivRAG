{"title": "Collaborative Learning for 3D Hand-Object Reconstruction and Compositional Action Recognition from Egocentric RGB Videos Using Superquadrics", "authors": ["Tze Ho Elden Tse", "Runyang Feng", "Linfang Zheng", "Jiho Park", "Yixing Gao", "Jihie Kim", "Ale\u0161 Leonardis", "Hyung Jin Chang"], "abstract": "With the availability of egocentric 3D hand-object interaction datasets, there is increasing interest in developing unified models for hand-object pose estimation and action recognition. However, existing methods still struggle to recognise seen actions on unseen objects due to the limitations in representing object shape and movement using 3D bounding boxes. Additionally, the reliance on object templates at test time limits their generalisability to unseen objects. To address these challenges, we propose to leverage superquadrics as an alternative 3D object representation to bounding boxes and demonstrate their effectiveness on both template-free object reconstruction and action recognition tasks. Moreover, as we find that pure appearance-based methods can outperform the unified methods, the potential benefits from 3D geometric information remain unclear. Therefore, we study the compositionality of actions by considering a more challenging task where the training combinations of verbs and nouns do not overlap with the testing split. We extend H2O and FPHA datasets with compositional splits and design a novel collaborative learning framework that can explicitly reason about the geometric relations between hands and the manipulated object. Through extensive quantitative and qualitative evaluations, we demonstrate significant improvements over the state-of-the-arts in (compositional) action recognition.", "sections": [{"title": "Introduction", "content": "Understanding hand and object interaction is a fundamental problem in various downstream applications including augmented and virtual reality (AR/VR) (Han et al. 2020; Mueller et al. 2019; Han et al. 2022; Tse et al. 2023). While a significant amount of research has focused on estimating the poses of hand and objects (Tse et al. 2022a,b; Hampali et al. 2022; Yang et al. 2022; Ye, Gupta, and Tulsiani 2022; Chen et al. 2022; Fan et al. 2023; Chen et al. 2023; Ye et al. 2023b; Feng et al. 2023b,a), jointly recognising hand-object interactions has received far less attention. In this paper, as illustrated in Fig. 1, we study jointly the problem of hand-object reconstruction and interaction recognition from egocentric view.\nRecent egocentric hand-object interaction datasets (Kwon et al. 2021; Garcia-Hernando et al. 2018) with 3D annotations enable the development of a unified framework for estimating hand-object poses and interaction classes (Tekin, Bogo, and Pollefeys 2019; Yang et al. 2020; Kwon et al. 2021; Wen et al. 2023; Cho et al. 2023). These methods couple 3D geometric cues (i.e. hand-object poses and/or contact maps) and appearance features to predict interaction class. Despite a unified understanding of the hand and manipulated object dynamics being crucial for recognising egocentric interactions, we find that pure appearance-based methods (i.e. MViT (Fan et al. 2021; Li et al. 2022)) can achieve comparable performance to the state-of-the-arts (as shown in Table 1). This raises immediate questions on when or how 3D geometric features can benefit interaction recognition.\nIn contrast, while deep architectures trained on large-scale datasets (Sigurdsson et al. 2016; Kay et al. 2017; Karpathy et al. 2014) exhibit strong distribution learning capabilities, mainstream action recognition models (Simonyan and Zisserman 2014; Carreira and Zisserman 2017; Feichtenhofer et al. 2019; Wang et al. 2016) primarily focus on frame appearance rather than temporal reasoning."}, {"title": "", "content": "Consequently, reversing the order of the video frame at test time will often produce the same classification result (Materzynska et al. 2020; Zhou et al. 2018). In particular, classical activity recognition methods like the two-stream Convolutional Neural Network (Simonyan and Zisserman 2014) and I3D (Carreira and Zisserman 2017) have demonstrated strong performance on various video datasets, including UCF101 (Soomro, Zamir, and Shah 2012) and Sport1M (Karpathy et al. 2014), with only still frames and optical flow. While appearance features can be highly predictive of the action class (Santoro et al. 2017; Battaglia et al. 2018), it remains challenging for appearance-based deep networks to capture the compositionality of action and objects without temporal transformations or geometric relations (Materzynska et al. 2020).\nTo address the aforementioned problems, Materzynska et al. (2020) extends the Something-Something dataset (Goyal et al. 2017) and introduces the Something-Else task with a new compositional split. This presents a novel task known as compositional action recognition, in which methods are required to recognise an action with unseen objects. Under this problem setting, the combinations of actions and object instances do not overlap in the training and testing split. Therefore, models are encouraged to learn the compositionality of action verb and noun, and not over-fit to the correlation between appearance features and action classes. Nonetheless, the current research in this task is primarily generic approaches using 2D geometric cues such as 2D instance bounding boxes. The potential benefits offered by 3D geometric information remain an open problem.\nTherefore, we take an alternative approach which exploits the compositionality of actions using 3D geometric information. To achieve that, we first extend the two existing 3D annotated egocentric hand-object datasets, H2O (Kwon et al. 2021) and FPHA (Garcia-Hernando et al. 2018), by introducing new compositional splits. Our experiments show that the existing state-of-the-art approaches (Yang et al. 2020; Wen et al. 2023; Fan et al. 2021; Li et al. 2022) still face significant challenges in recognising a seen action when facing new objects. This is because the current methods (either single or dual branches) are unable to tackle the problem of appearance bias in objects, as they have to take the combination of appearance and geometric information as a whole. In addition, these approaches focus on extracting features for the whole scene and do not explicitly recognise objects as individual entities. Hence, they cannot fully capture the compositionality of the action.\nIn this paper, we propose a collaborative learning framework that allows an action verb and object to interact and complement each other. The key motivation for this strategy is that the tasks of estimating hand-object poses and recognising interactions are naturally closely-correlated. Existing collaborative learning methods (Yang et al. 2020; Tse et al. 2022a) in understanding hand-object interactions typically follow an iterative approach where the multiple target learning tasks (i.e. hand-object pose estimation and action recognition) boost each other mutually and progressively. However, connecting branches iteratively can lead to highly unstable training (Tse et al. 2022a). This is because gradients from one branch can propagate through the connections to affect the other branches which causes unstable gradients. We explicitly address this by a new transformer-based design to exploit the compositionality of actions and avoid branch stacking. In addition, we propose to use superquadrics (Barr 1981) as the intermediate 3D object representation. This is motivated by the fact that existing action recognition methods have limitations in accurately representing objects' shape and movement with only 2D/3D bounding boxes. But at the same time, accurately reconstructing 3D objects without object templates remains highly challenging, especially in scenarios involving unseen objects. Therefore, superquadrics offer a compact representation with their ability to represent a wide range of shapes with few parameters. In addition, it allows models to interpret objects with basic geometric primitives.\nOur contributions are the following:\n1. We propose an end-to-end collaborative learning framework to leverage 3D geometric information for compositional action recognition from egocentric RGB videos.\n2. We show that using superquadrics as the intermediate 3D object representation is beneficial for 3D hand pose estimation and interaction recognition. To the best of our knowledge, we are the first work to exploit superquadrics for both template-free object reconstruction and interaction recognition.\n3. We extend two egocentric hand-object datasets by introducing new compositional splits and investigate compositional action recognition where a subset of action verb and noun combinations do not exist during training.\n4. We achieve state-of-the-art performance on two public datasets, H2O (Kwon et al. 2021) and FPHA (Garcia-Hernando et al. 2018), in both official and compositional settings."}, {"title": "Related Work", "content": "Our work tackles the joint problem of 3D hand-object reconstruction and action recognition from egocentric RGB videos. We first review the literature on reconstructing and recognising Hand-object interactions from RGB inputs. Then, we provide a brief review on Compositional action recognition and Superquadrics.\nReconstructing hand-object interactions. While most existing research has primarily focused on single-hand (Ge et al. 2019; Boukhayma, Bem, and Torr 2019; Baek, Kim, and Kim 2019; Simon et al. 2017; Zimmermann and Brox 2017) or objects (Li et al. 2018; Zheng et al. 2022; Wang et al. 2019; Lepetit, Pilet, and Fua 2004; Zheng et al. 2022, 2024) in isolation, recently there has been a surge in interest of joint understanding of hand-object pose estimation. As the problem of reconstructing both hand and object is extremely ill-posed due to heavy mutual occlusions, many works (Cao et al. 2021; Tse et al. 2022b; Liu et al. 2021; Yang et al. 2021; Hampali et al. 2022; Yang et al. 2022) reduce this problem to 6D pose estimation with instance-specific templates. Meanwhile, some previous efforts (Hasson et al. 2019; Tse et al. 2022a; Ye, Gupta, and Tulsiani"}, {"title": "", "content": "2022; Chen et al. 2022; Ye et al. 2023a; Chen et al. 2023) do not assume to have access to ground-truth object models at test time and follow a template-free paradigm. However, these approaches would fail on unseen object instances as they either lack geometrical prior or overfit to a limited number of training objects. Our work is a template-free approach where we utilise primitive shape information from estimated superquadrics, and therefore it is more generalisable to unseen object instances for understanding hand-object interaction.\nRecognising hand-object interactions. Action recognition is one of the most actively researched areas in computer vision (Jhuang et al. 2013; Varol, Laptev, and Schmid 2017; Kantorov and Laptev 2014) and significant progress has been made with the availability of large-scale datasets (Sigurdsson et al. 2016; Kay et al. 2017; Karpathy et al. 2014; Grauman et al. 2022). Here, we focus on methods (Tekin, Bogo, and Pollefeys 2019; Yang et al. 2020) that simultaneously estimate hand(-object) poses and interactions from egocentric videos. Garcia-Hernando et al. (2018) presents the first egocentric dataset and shows that 3D hand poses are beneficial for recognising actions. As the dataset contains visible magnetic sensors and does not include two-hand poses, a markerless dataset named H2O is developed to provide rich 3D annotations for egocentric 3D interaction recognition (Kwon et al. 2021). This enables recent research to develop unified models for understanding hand-object interactions. HTT (Wen et al. 2023) and H2OTR (Cho et al. 2023) are two closely related works that are based on a transformer architecture. HTT focuses on leveraging different temporal granularity information, while H2OTR exploits contact map for robust estimation. All of the above methods share a common approach by following the semantic relationship between hand poses and object labels to infer action. In contrast, we propose a collaborative learning framework which is designed explicitly to recognise interactions in a compositional fashion.\nCompositional action recognition. This task is designed to alleviate the problem of appearance bias by disjointing the combination of actions and objects between training and testing. STIN (Materzynska et al. 2020) models actions as transformation of geometric relations in both spatial and temporal domains. This approach generalises well to most actions but fails when there are intrinsic state changes of objects. Kim and Hager (2020) proposes to fuse RGB information with instance bounding boxes to capture more complex actions. Sun et al. (2021) removes the appearance effect by counterfactual debiasing inference. While these methods have proven effective, they are primarily designed to leverage 2D geometric information and do not fully explore the potential of 3D geometric cues. As a result, their performance remains comparable to I3D (Carreira and Zisserman 2017). In contrast, we focus on leveraging 3D geometric cues for compositional action recognition.\nSuperquadrics recovery. Superquadric is a well-studied computational primitive shape abstraction, offering a diverse range of shape representations including cuboids, ellipsoids, cylinders, octohedra, and other variations. It was first proposed to model complex objects in computer graphics (Barr"}, {"title": "", "content": "1981). Solina and Bajcsy (1990) presents a method for abstracting simple objects from range images using a single superquadric. Subsequently, Leonardis, Jaklic, and Solina (1997); Chevalier, Jaillet, and Baskurt (2003) extend to recover more complex objects with multiple superquadrics. Recently, Liu et al. (2022) proposes a probabilistic approach to improve robustness to outlier and fitting accuracy. To the best of our knowledge, we are the first to leverage superquadrics for action recognition and demonstrate its effectiveness in in-the-wild scenarios."}, {"title": "Methodology", "content": "Our training pipeline, as shown in Fig. 2, takes a sequence of T RGB frames $I \\in R^{T\\times256\\times256\\times3}$ of dynamic hands manipulating objects as input. We first obtain spatial features $x \\in R^{T\\times d}$ by passing each frame into a ResNet-18 (He et al. 2016) encoder where d refers to feature dimensions. To enhance the interaction between visual and geometric cues, we propose a simple collaborative learning framework by leveraging the Transformer encoder and decoder (Vaswani et al. 2017) as basic building blocks. Specifically, we design a two-branch network where the appearance branch extracts video features and the geometric branch aims to recover 3D hand-object geometric information. With such design, we can model appearance and geometric information at different temporal granularity. In addition, we explicitly model the compositionality of the interaction by decomposing the action class into a verb-and-noun pair. Finally, we combine video appearance and geometric representations for recognising egocentric hand-object interactions.\nAppearance Branch\nGiven image features x, we concatenate with a learnable token $T_{img} \\in R^{d}$ and apply positional encoding before feeding them into a Transformer encoder. The encoder models the relationships of different spatial regions and $T_{img}$ through self-attentions. The learnt token $T_{img}$ captures essential global contexts from backbone representation which are used for compositional reasoning. In addition, the encoder outputs aggregated spatial representation $X_{appearance} \\in R^{T\\times d}$ which is later used for interaction recognition.\nGeometric Branch\nAs estimating the poses of hand and object requires more local or nearby frames, we divide the video sequence into N consecutive segments. Specifically, we follow Wen et al. (2023) and use a shifting window strategy with window size t, i.e. N = T/t. The frames beyond sequence length T are padded but masked out from attention computation. Readers are referred to Wen et al. (2023) for more details.\nInstead of aiming to reconstruct the hand and the manipulated object simultaneously, we first estimate the shape and poses of the manipulated object and leverage this geometric information to predict hand poses. The reason behind this approach is that joint estimation poses a significantly harder problem: First, self-occlusion and self-similarity between the joints of two hands are unique problems in interacting hands. Second, when interacting with objects, hands"}, {"title": "Preliminaries.", "content": "As shown in Fig. 3, superquadrics are a family of geometric primitives, i.e. cuboids, cylinders, ellipsoids, octahedra and their intermediates, which can be defined by an implicit function $f(\u00b7)$ (Barr 1981):\n$f(p) = (\\frac{x}{a_x})^{\\frac{2}{\\epsilon_1}} + (\\frac{y}{a_y})^{\\frac{2}{\\epsilon_1}} + (\\frac{z}{a_z})^{\\frac{2}{\\epsilon_2}} = 1,$\nwhere points p = [x,y,z] \u2208 R\u00b3 satisfying Eq. (1) form the surface of a superquadric. It can be encoded using 5 parameters: shape parameters $ \\epsilon_1,\\epsilon_2 \\in [0,2] \\subset R$ and scale parameters $a_x, a_y, a_z \\in R>0$. While the shape parameters can exceed 2 and result in non-convex shapes, we limit them within the convex region in this paper. We can now fully parameterise a superquadric by including the Euclidean transformation g \u2208 SE(3), i.e. g = [R \u2208 SO(3), t \u2208 R3].\nSuperquadrics decoder. We use superquadrics \u03b8 to be the intermediate 3D object representation, as it offers a com-"}, {"title": "", "content": "pact way to represent a wide range of geometric primitives, i.e. $\u03b8 = {\\epsilon_1, \\epsilon_2, a_x, a_y, a_z, g} \u2208 R^{11}$. As shown in Fig. 4, superquadrics can provide sufficient expressiveness to reasonably model a diverse range of everyday objects. To this end, we extract geometric features $T_{geo} \\in R^{N\\times d}$ from the segmented spatial features using a Transformer decoder. We train a fully-connected layer to predict \u03b8 from the learnt flatten $T_{geo}$ by minimising the L1 loss $L_{superquadrics}$. The main motivation to use Transformer decoder here instead of encoder is to leverage the autoregressive self-attention mechanism for modelling objects with multiple superquadrics. In addition, having separate encoder and decoder for this branch enables pre-training on large-scale object datasets that lack hand pose annotations.\nHand pose estimator. Given the learnt geometric features $T_{geo}$, we concatenate with a learnable verb token $T_{verb}$ and use self-attention in a Transformer encoder to create global context-aware features between object shape and hand poses. This encoder outputs aggregated geometric features $x_{geometric} \u2208 IR^{N\\times d}$ and verb token features $T_{verb} \u2208 R^{d}$.\nWe map the aggregated geometric features $x_{geometric}$ to hand pose space by a 3-layer MLP and use MANO joint angles (Romero, Tzionas, and Black 2017) for hand pose representation. Specifically, we estimate 16 3D joint angles under the hand kinematic tree and MANO hand shape parameter per hand. Then, we can compute the 21 root-relative 3D joint locations of each hand by using the predicted joint angles and hand shape parameters. They are learned by minimising L1 loss $L_{hand}$.\nCompositional Reasoning\nIn the following, we describe how we leverage the compositional nature of actions by exploiting the action class as verb-noun pair with 3D geometric cues, i.e. superquadrics \u03b8 and geometric-aware verb token $T_{verb}$.\nObject category predictor. We first predict the category of the manipulated object as it corresponds to the noun of an action. To achieve that, we leverage the basic primitive geometric information from superquadrics as object shape provides strong signals to estimating object category. More specifically, we predict the classification probability vector for object category by linearly projecting the concatenation of superquadrics \u03b8 and $T_{img}$. We supervise this linear layer by minimising the cross-entropy loss $L_{noun}$.\nVerb predictor. Similarly, we predict action verb by feeding the concatenation of $T_{verb}$ and $T_{img}$ to a linear layer. It is also trained by minimising the cross-entropy loss $L_{verb}$.\nDiscussion. The key idea for concatenating $ with $T_{img}$ is to allow action verb and noun to interact with the appearance branch. Also, it generates loss gradients for both branches to develop a collaborative learning relationship. In addition, the motivation for estimating superquadrics first in the geometric branch is based on the fact that the human visual system flavours abstracting scenes into canonical parts for better perceptual understanding (Liu et al. 2022). This enables robust action recognition using basic geometric primitives instead of relying on accurate point-wise estimation."}, {"title": "", "content": "In summary, our design targets the problem of recognising a seen action when facing new objects by enabling the network to capture the compositionality of an action explicitly.\nInteraction Recognition\nBesides explicitly modelling the compositionality of actions, our proposed framework can easily combine with any video-level appearance representation. The impact of appearance features can be two-fold: 1) The presence of appearance features can be particularly beneficial for action classes that lack prominent inter-object dynamics (Materzynska et al. 2020). 2) Conversely, appearance bias can inhibit the model learning ability by making strong correlations on spatial appearance rather than temporal or geometric transformations (Sun et al. 2021). To overcome the limitations of existing methods that can only accept or reject appearance information as a whole, we propose to use a Transformer decoder for recognising interaction. There are two key motivations for this design: 1) Transformer decoder has skip connections by design so it can attend segments of appearance features. 2) It can take different temporal granularity from both branches as inputs to make action predictions. Specifically, this decoder takes the aggregated geometric and spatial features, i.e. $X_{geometric}$ and $X_{appearance}$ as input and extracts relevant image features through cross-attention between geometric features. The vector output of this decoder is fed to a 3-layer MLP classifier of width d and is supervised with cross-entropy loss $L_{action}$. We investigate and analyse our design choices and provide details for the architecture in the supplementary materials."}, {"title": "Experiments", "content": "Implementation details. We train our model with the Adam optimiser (Kingma and Ba 2015) using an initial learning rate of 3 \u00d7 10\u22125 which halves in every 15 epochs. We keep the relative weights between different losses and normalise them for all experiments. We use ResNet (He et al. 2016) pre-trained on ImageNet (Russakovsky et al. 2015) for our backbone. For all Transformer encoders and decoders, we use 2 encoding/decoding layers where each layer has 8 attention heads. We use the fixed sine/cosine functions for positional encoding and add layer normalisation before the attention and feed-forward computations (Vaswani et al. 2017). We follow Wen et al. (2023) by setting T = 128, t = 16, d = 512 and training for 45 epochs with batch size of 2. Our final loss $L_{final}$ is defined as:\n$L_{final} = L_{superquadrics} + L_{hand} + L_{noun} + L_{verb} + L_{action}$.\nDatasets. We conduct experiments on 3 interacting hand-object datasets and detail below.\nObMan (Hasson et al. 2019) is a synthetic dataset which was produced by rendering hand meshes with selected objects from ShapeNet (Chang et al. 2015b) dataset. It captures 8 object categories of everyday objects (e.g. bottles, cans and jars) and results in a total of 2,772 meshes. We precomputed superquadrics for all object meshes using the EMS algorithm (Liu et al. 2022) and pretrained the geometric branch on ObMan before training on other real datasets."}, {"title": "", "content": "We observed consistent improvements over training directly on real data as the number of objects in hand-object interaction dataset is very limited.\nFirst-person hand benchmark (FPHA) (Garcia-Hernando et al. 2018) is a real dataset which records egocentric videos on diverse hand-object interactions. It captures 6 subjects performing 45 actions by interacting with 26 objects, i.e. juice bottle, liquid soap, milk and salt. We evaluate on the action split where all subjects and actions are present in both training and testing. This split consists of 600 and 575 videos for training and testing, respectively.\nH2O (Kwon et al. 2021) is a recent real dataset which provides markerless 3D annotations for two hands and the 6D pose of manipulated objects. It is the first unified dataset for egocentric interaction recognition with rich 3D annotations of 4 subjects performing 36 actions on 8 objects. We follow Wen et al. (2023); Kwon et al. (2021) and use the sequences of egocentric view for training and testing. Specifically, the training split consists of 569 videos from the first 3 subjects, while the testing split includes 242 videos from the remaining unseen subjects.\nBaselines. We compare our method against MViTv2 (Li et al. 2022), HTT (Wen et al. 2023) and H2OTR (Cho et al. 2023). MViTv2 is widely adopted for video recognition tasks and can serve as a strong appearance-based baseline. We train the base variant of MViTv2 with weights pretrained on Kinetics-400 dataset (Kay et al. 2017) using the PySlowFast library (Fan et al. 2020). For the pose-based baselines, we consider HTT and H2OTR as they are recent methods that achieves state-of-the-art performance on both FPHA and H2O datasets. We also consider two Transformer-based baselines which do not contain compositional reasoning and superquadrics, respectively. We provide more details about the baselines in the supplementary materials. They are useful for understanding the importance of superquadrics and compositional reasoning for recognising interactions with unseen objects."}, {"title": "Evaluation metrics.", "content": "We report the Mean End-Point Error (MEPE) in mm to evaluate pose estimation. MEPE measures the mean Euclidean distances between predictions and ground-truths. We also report the top-1 classification accuracy for action recognition.\nQuantitative Comparison\nComparison with state-of-the-art. We report quantitative comparisons with the state-of-the-art methods on H2O and FPHA datasets in Table 1. We split the table into two sections: appearance-based (Wang et al. 2018; Carreira and Zisserman 2017; Feichtenhofer et al. 2019; Li et al. 2022) and geometric (Tekin, Bogo, and Pollefeys 2019; Yang et al. 2020; Kwon et al. 2021; Wen et al. 2023; Cho et al. 2023) methods for clear comparison. By considering the performances of action recognition, the appearance-based baseline MViTv2 performs competitively with the state-of-the-art geometric method H2OTR (Cho et al. 2023). It raises an immediate question as to when 3D geometric cues be beneficial to recognising interaction as collecting ground-truth contact maps or other 3D annotations are non-trivial (Brahmbhatt et al. 2019; Tse et al. 2022b; Kwon et al. 2021). We will address this question in the following paragraph. Nonetheless, we demonstrate the effectiveness of explicit compositional reasoning with superquadrics by outperforming all methods. On the other hand, for pose estimations, H2OTR is a Transformer-based framework which achieves state-of-the-art accuracy for hand pose estimation. However, all of the compared methods cannot recover dense object geometries without manual selection of object models at test time. In contrast, our method performs competitively without known object templates and outperforms all methods on object pose estimation. We attribute this to the fact that superquadrics can provide dense 3D geometric information about the manipulated object, whereas 2D or 3D bounding boxes have limitations on representing object shape and movement. We show qualitative results on H2O dataset in Fig. 5.\nCompositional action recognition. We further evaluate on the compositional recognition task in Table 2. Following Materzynska et al. (2020), we first create new splits for the task of compositional action recognition by extending existing egocentric hand-object datasets. Specifically, we remove the sequences that contain the predefined object category from the train split, such that the combinations of a verb (action) and nouns do not overlap in the testing set. To gain a deeper understanding of the model generalisation ability on unseen objects, we evaluate the model using Nobj-fold cross validation where Nobj refers to the number of total objects presented in the dataset. We keep the original testing splits (named So) to illustrate the difficulty of this compositional task. We further experiment on a more challenging split where two object categories are randomly removed in the H2O dataset. We name the base splits by S\u2081 and the more difficult splits where additional verb-nouns combinations are removed from training by S2. We report the mean and the standard deviation of top-1 classification accuracy for all experiments. We present the main results in Table 2, while full results are presented in the supplementary."}, {"title": "Ablations", "content": "To motivate our design choices, we perform additional qualitative evaluation of our method with various key components disabled in Table 3. We evaluate the effectiveness of our collaborative learning framework by experimenting on a single appearance branch (row 1) and a two-branch network without gradient flow (row 2), which yields the lowest performance in H2O. Then, we observe significant performance drops by removing either one of the inter-branch classifiers (row 3, 4). These results demonstrate the effectiveness of our collaborative learning framework which encourages information sharing between two-branches. Further, we are interested in finding out whether incorporating the interaction decoder is important as action class can be obtained by combining verb and noun predictions. We show that the interaction decoder can bring performance gain for verb and noun classifiers by additional supervision in row 5. Finally, we show that superquadric predictions (i.e. by removing the purple block in Fig. 2) can push the limit to the new state-of-the-art in all metrics."}, {"title": "Conclusion", "content": "We showed that, by explicitly leveraging 3D geometric information, we could recognise actions performed on unseen objects much more accurately than existing state-of-the-arts. We also demonstrated that superquadrics as a new object representation for action recognition to be effective. We validated our approach by extending existing datasets with compositional splits and achieved state-of-the-art performance. We plan to investigate more complex interactions with articulated objects in the future.\nLimitations. Our approach relies on the expressiveness of superquadrics. First, we found that multi-superquadrics recovery is necessary to model more complex shapes, where preliminary point cloud segmentation can be helpful. Sec-"}, {"title": "Implementation Details", "content": "Transformer encoder. The encoder consists of 2 identical layers. Each of the layers is composed of a multi-head self attention mechanism and a positional-wise fully connected feed-forward network. Following Vaswani et al. (2017), we employ residual connections around both sub-layers, followed by layer normalisation. The encoder is used to aggregate input features by performing self-attention.\nTransformer decoder. Similarly, the decoder consists of 2 identical layers. In addition to the two sub-layers in encoder layer, the decoder layer has another multi-head attention mechanism for the secondary input. It also consists of residual connections around each of the sub-layers, followed by layer normalisation. The decoder is used to extract features by performing self-attentions on primary inputs and cross-attentions together with secondary inputs. We refer readers to (Vaswani et al. 2017) for additional implementation details.\nMVIT baseline. We use the PySlowFast library (Fan et al. 2020) for performing action recognition comparisons with the state-of-the-art appearance-based methods, i.e. C2D (Wang et al. 2018), I3D (Carreira and Zisserman 2017), SlowFast (Feichtenhofer et al. 2019) and MViT (Fan et al. 2021). As we found that MViT performs significantly better than the rest of appearance-based methods in our preliminary experiments, we chose MViT to serve as a strong image-based learning baseline. For all experiments, we use MViTv2-B model (Fan et al. 2021) pre-trained on Kinetics 400 (Kay et al. 2017). We found that utilising a pre-trained model is essential as we observed a significant performance drop to less than 25% when training from scratch without any pre-training. MViT-B consists of 4 scale stages where each of them have several transformer blocks of consistent channel dimension. The model initially projects the dimension of the input channel to 96 with patch kernel shape of (3, 7, 7). We set patch stride and padding to be (2, 4, 4) and (1, 3, 3), respectively. The resulting sequence is reduced by a factor of 4 for each additional stage. The number of head for multi-head pooling attention (MHPA) is set to 1 at initial stage and increases with channel dimension. The output dimension of MLP at each transition is increased by 2 and MHPA pools Q tensors with stride (1, 2, 2) as the input for the next stage. The K, V pooling in MPHA employs stride at (1,8,8) and adaptively adjusts with respect to the scale across stages to ensure consistent scaling of the K, V tensors in all blocks. We refer readers to Fan et al. (2021) for more details."}, {"title": "Additional Analysis", "content": "Compositional action recognition FPHA. We present full results for compositional action recognition on FPHA dataset in Table 4.\nH2O. We present full results for compositional action recognition on H2O dataset for splits S\u2081 and S2 in Table 5 and Table 6, respectively.\nSuperquadrics Data pre-processing. We extract superquadrics from object models using the Expectation, Maximization and Switching (EMS) algorithm developed by Liu et al. (2022). This is the first probabilistic method to recover superquadrics from noisy point clouds. We treat mesh"}, {}]}