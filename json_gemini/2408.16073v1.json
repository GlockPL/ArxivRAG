{"title": "Using Large Language Models to Create AI Personas for Replication and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings", "authors": ["Leo Yeykelis", "Kaavya Pichai", "James J. Cummings", "Byron Reeves"], "abstract": "This report analyzes the potential for large language models (LLMs) to expedite accurate replication of published message effects studies. We tested LLM-powered participants (personas) by replicating 133 experimental findings from 14 papers containing 45 recent studies in the Journal of Marketing (January 2023-May 2024). We used a new software tool, Viewpoints AI (https://viewpoints.ai/), that takes study designs, stimuli, and measures as input, automatically generates prompts for LLMs to act as a specified sample of unique personas, and collects their responses to produce a final output in the form of a complete dataset and statistical analysis. The underlying LLM used was Anthropic's Claude Sonnet 3.5. We generated 19,447 AI personas to replicate these studies with the exact same sample attributes, study designs, stimuli, and measures reported in the original human research. Our LLM replications successfully reproduced 76% of the original main effects (84 out of 111), demonstrating strong potential for AI-assisted replication of studies in which people respond to media stimuli. When including interaction effects, the overall replication rate was 68% (90 out of 133). The use of LLMs to replicate and accelerate marketing research on media effects is discussed with respect to the replication crisis in social science, potential solutions to generalizability problems in sampling subjects and experimental conditions, and the ability to rapidly test consumer responses to various media stimuli. We also address the limitations of this approach, particularly in replicating complex interaction effects in media response studies, and suggest areas for future research and improvement in AI-assisted experimental replication of media effects.", "sections": [{"title": "STUDY OVERVIEW AND RELATED WORK", "content": "Research about the effectiveness of media messages is increasingly difficult, attributable to both administrative challenges (e.g., stimulus acquisition and creation, data management demands of digital trace data, acquisition of participants and especially those in special groups like children, minorities and international groups), as well as requirements to deal with new and critical challenges to the very nature of social research, as exemplified by existential issues of replication and reproducibility, and the ability to generalize findings across people, media stimuli and experimental contexts. We briefly review these issues with an eye toward our current test of whether new LLM tools may help solve the problems mentioned, and with significant advantages in cost, time, and research personnel. The fundamental question for this project is whether an AI can accurately answer research questions, with accuracy measured in this case as the ability of AI to virtually replicate identical studies (i.e., the same media stimuli, measures, and participate sample specifications) that were conducted with human subjects.\nThe Replication Crisis in Social Sciences\nIoannidis (2005) suggested that most published research findings are false, citing small sample sizes, small effect sizes, and researcher degrees of freedom as contributing factors. The Open Science Collaboration (2015) corroborated this concern, successfully replicating only 39% of psychology studies. Hubbard and Armstrong (1994) reported low replication rates in marketing journals. Munaf\u00f2 et al. (2017) identified publication bias, p-hacking, and lack of replication incentives as key factors in this crisis.\nThe Generalizability Crisis\nYarkoni (2022) extended the critique beyond replicability to a \"generalizability crisis,\" arguing that researchers often draw overly broad conclusions from narrow empirical findings. This issue is particularly pertinent in marketing and media effects research, where findings are frequently presumed to generalize across diverse consumer contexts.\nTo address such concerns Yarkoni (2022) advocated more rigorous consideration of research boundary conditions, increased use of large-scale naturalistic datasets, and development of computational tools for comprehensive analysis of research claims. More recently, there has also been growing interest in the prospect of leveraging AI to address historical and ongoing crises and less severe threats to validity in the social sciences (Bail, 2024), and initial attempts to use AI in research replication have shown promise. Recent advancements in LLMs, exemplified by GPT-3 (Brown et al., 2020) and BERT (Devlin et al., 2018), have expanded applications to research tasks. These include literature review automation, hypothesis generation, and scientific discovery assistance (Wang et al., 2023). In marketing research, LLMs have been applied to sentiment analysis of consumer reviews (Zhang et al., 2022), and marketing content generation (Kshetri et al., 2024). However, the potential of AI to replicate experimental studies in media effects research remains largely unexplored. More broadly in psychology, Binz et al. (2023) showed the capability of GPT-3 to replicate psychological experiments, finding success particularly in language-based tasks.\nWhile interest grows in addressing the replication crisis and leveraging AI in research, empirical efforts have not extensively explored Al's capacity to replicate more complex research scenarios in which users respond to specific mediated stimuli. Our study aims to address this gap by systematically applying"}, {"title": "METHOD", "content": "LLM-powered participants to replicate recent marketing experiments that investigated human participants' responses to media stimuli, assessing the feasibility of this approach, and potentially enhancing the reliability and generalizability of research findings in marketing.\nSampling Stimulus-Response Studies for Replication with AI Participants\nTo assess the replicability of marketing experiments using LLMs, we collected a sample of recent research that experimentally examined responses to different types of media messages. Specifically, we systematically reviewed all articles in the Journal of Marketing published from January 2023 through May 2024. This resulted in an initial corpus of 69 papers containing 210 unique studies. The Journal of Marketing was chosen for this initial test of AI replication accuracy because it frequently publishes tests of media message effectiveness (consistent with our interest in commercial and theoretical work about media psychology), and because journal policies encourage detailed reporting about measures, sampling and inclusion of actual visual and textual material used in studies.\nWe then reviewed this sample of candidate studies to evaluate suitability for AI-assisted replication. We applied the following inclusion criteria: (1) the study had to be a true experiment incorporating manipulated study conditions (not simply a survey that scored participant attitudes or beliefs or a study that compared correlations between individual difference variables and outcomes); (2) it had to be compatible with the features of our Al software (e.g., manipulating stimuli between conditions and presenting all questions at the end); (3) all original study materials (i.e., stimuli and measures) needed to be provided by the authors or otherwise publicly available; and (4) the study procedures or outcomes could not require physical actions or behavioral measures (e.g., eye-tracking, monitoring of subsequent purchasing behavior). In essence, for this initial test, we selected experiments that could typically be conducted through online recruitment platforms like Mechanical Turk or Prolific. This selection process resulted in a final total of 45 studies sourced from 14 distinct research articles (see Table 1 in Supplementary Materials for full list of research articles).\nEach study's experimental procedure, data collection, and analysis was then replicated using new software, Viewpoints AI. Viewpoints AI is software designed to test AI responses to different versions of multimodal media. The software allows researchers to input various media stimuli (images, videos, or text), organize them into experimental conditions, specify participant characteristics, and define survey questions, and scales. The system then generates responses from participants based on these parameters. For each study, a series of unique LLM instantiations\u2014 one for each virtual persona\u2014is created on the fly (i.e. in real time as the study was run) to exactly match the sample distributions, characteristics, and context as given in the actual study. Each persona was then given the exact text, image, and/or video stimulus used in the original study to view, along with all other original study instructions. The creation of a unique AI instance for each virtual persona differentiates Viewpoints AI from other attempts to use AI to answer questions in social science research.\nSeparate AI instances for each virtual participant also allows instances to be statistically aggregated, and differences tested, in exactly the same way that human subject statistical results are computed in the original research. For each study, we replicated the experimental conditions and measures as closely as"}, {"title": "Strategy for Creating LLM-Powered Participants", "content": "possible within the constraints of the Viewpoints AI platform. This replication effort entailed generating a grand total of 19,447 virtual participants across the 45 studies.\nFor each study, we generated an equivalent number of LLM-powered participants to match the sample size (N) of the original experiment. Each participant was generated as an individual instance of Anthropic's Claude Sonnet 3.5 (claude-3-5-sonnet-20240620), one of the most advanced LLMs available at the time of this research. We used a model temperature of 0.7, as this value is currently the industry standard (Argyle et al., 2023). The sample of participants was then imbued with specific persona characteristics mirroring the types, frequencies, and distributions of those reported in the original study. For example, a generated participant might be characterized as a \"45-year-old woman with 20 years of managerial experience in the manufacturing industry.\"\nOur software then constructed a prompt that instructed the LLM to i) embody the assigned persona, ii) examine the presented stimuli (which could include text, images, videos, or any combination thereof), and iii) respond to the subsequent questions. The question wordings and response scales provided to the generated participants were directly transplanted from the original experiments, maintaining fidelity to the source material. This approach allowed for flexibility in accommodating various question types and scale formats, ranging from open-ended queries (e.g., \u201cWhat is the highest price you would be willing to pay for this product?\u201d) to Likert-style scales of varying points (e.g., 1 = very unlikely, 7 = very likely).\nA crucial aspect of our methodology was ensuring that the LLM-powered participants remained unaware of the study goals or of the original study being replicated, thereby precluding the possibility of them using study-specific training data in providing their responses. This design choice was implemented to minimize any potential reference to training data that might pertain to the experiments, as our primary interest lay in assessing whether LLMs could generate responses from simulated human personas that in aggregate-closely resemble those from real human samples when exposed to media messages. To validate this approach, we conducted a pretest using Claude 3.5 Sonnet, querying its awareness of any of the 14 published papers whose experiments we aimed to replicate. The model reported no prior knowledge of these studies, suggesting no direct threat to the integrity of our replication efforts. Further, we deliberately avoided mentioning any paper title, authors, or journal information when presenting the stimuli to the Al personas, ensuring the responses were based on the given prompts rather than any prior knowledge.\nFor a given study's data collection process, each generated LLM participant corresponded to a unique API call. After responding to the measures, their answers were logged in a database. We then extracted these responses and employed R for statistical analysis, replicating the exact statistical procedures used by the original authors. This included various techniques such as ANOVA, linear models, and chi-squared tests."}, {"title": "Assessing Replication of Original Human Participant Findings", "content": "Our unit of analysis for replication was each predictive \"finding\" within a study (i.e., each main effect of an independent variable on a dependent variable, or interaction of multiple independent variables on a dependent variable). For each of the 45 studies, we compared our results to those reported in the original experiment, resulting in 133 replication observations."}, {"title": "RESULTS", "content": "Anderson and Maxwell (2016) note that there are many different goals researchers may seek to achieve through replication work. The aims of our study aligned with replication \u201cGoal 1\u201d - simply, \"To infer the existence of a replication effect (p.3).\" In turn, following their recommendation, we considered a previously significant finding successfully replicated if it matched the original study's result in direction and statistical significance (p < .05). This criterion focused on reproducing the existence and direction of an effect, which Anderson and Maxwell argue is often sufficient, especially for unexpected or counterintuitive findings in psychological research. Additionally, we also aimed to replicate the null effects from studies reporting non-significant findings. We considered a non-significant finding successfully replicated if the LLM-based study also yielded a non-significant result (p >= .05). This approach acknowledges the importance of reproducing null effects, which can be just as theoretically meaningful as significant results in certain contexts, aiding in theory validation and falsification (Meehl, 1978; Popper, 2005), improving reliability (Ioannidis, 2005), and helping demonstrate the relative robustness of past findings (c.f., Open Science Collaboration, 2015). However, it is important to note that this method does not definitively prove the absence of an effect, but rather demonstrates a failure to reject the null hypothesis in both the original and replication studies."}, {"title": "Overall Replication Success Rate with Virtual Participants", "content": "Overall, our LLM replications successfully reproduced 76% of the original main effect findings (84 out of 111). When including interaction effects, the replication rate was 68% (90 out of 133). Notably, the replication rate for interaction effects alone was substantially lower at 27%. This difference in replication success for main and interaction effects parallels the differences for replication of human effects studies. Crede and Sotola (2024) suggest the poorer replicability of interaction effects compared to main effects can be attributed due the relatively higher statistical power needed for detection and the greater potential influence of context-specific factors. Additionally, as these statistical attributes of interaction effects lead to a higher proportion of non-significant findings compared to main effects, they may also lead to greater susceptibility to publication bias and questionable research practices (QRPs) than in the case for the testing and reporting of main effects including biases associated with selective reporting (Ioannidis, 2008) \u2014 particularly in the social sciences (Fanelli, 2010) \u2014 and HARKing (Hypothesizing After the Results are Known; Kerr, 1998)."}, {"title": "Replication Success Rate by p value Range", "content": "Previous literature suggests several reasons why replication success may vary as a function of the p value of the originally observed effect. For instance, Ioannidis (2005) described how the probability of a finding being indeed true depends on, amongst other things, the level of statistical significance; and further, studies with p values nearer to .05 may be the result of biases in reporting, meaning they would in turn be less likely to replicate. Additionally, even without any such biases, p values nearer to .05 may indicate greater likelihood that the original effect is marginal (Wasserstein & Lazar, 2016). To this end, we examined the relationship between the p value of the original findings and the rate of successful replication. "}, {"title": "Replication Success Rate by Effect Size", "content": "Beyond statistical significance, it is generally the case that larger effects are more robust and easier to detect (Cohen, 2013; Ioannidis, 2005). As such, larger effects may permit greater replicability (Van Bavel et al., 2016). To this end, we also investigated the relationship between effect size of original findings and rate of successful replication."}, {"title": "DISCUSSION", "content": "For the findings that reported effect sizes (98 out of 103), we categorized the effect sizes into four ranges based upon the benchmarks set by Cohen (1988), and examined the replication rates for each category. The results are shown in Figure 3. There is a positive correlation between the magnitude of the original effect size and the likelihood of successful replication using LLM-powered participants. As the effect size increases, so does the rate of replication.\nIntroduction\nOur study suggests that empirical studies about media message processing can use AI virtual personas to replicate existing research, and especially the main effects proposed in studies. The level of replication, while not perfectly matched with identical human studies, is at a level of success (76%) that offers promise to help solve an important challenge to current social research.\nNot all original study results replicated similarly, however, with implications for capabilities and limitations of LLMs for consumer behavior research. First, the high reliability of LLMs in replicating strongly significant findings suggests their value for confirming robust effects. Second, the observed decline in replication success as p values increase emphasizes the critical role of the original evidence's strength in the interpretation of LLM-based replications. Stronger original evidence is more likely to be successfully replicated by LLMs, which is an important consideration for researchers relying on these models. Third, the mixed performance of LLMs on findings with marginal or non-significant p values raises concerns about AI sensitivity to subtle effects. This variability suggests that LLMs may be prone to both false positives and false negatives, indicating a potential risk when using them to detect or confirm less pronounced effects. Last, the balanced replication outcomes for p values above 0.5 reveal that while LLMs may sometimes accurately identify the absence of an effect, they also risk introducing spurious findings. This dual possibility underscores the need for caution when interpreting LLM-based replications, particularly in cases where the original findings suggest a null effect.\nThe value of AI replication could come in several forms. Most obviously, the replications, when they agree with identical human subject studies, can provide evidence that existing results may be used with confidence. And when the AI replications do not work, this might identify results that should be a priority for replications with human subjects so that differences can be adjudicated. The general benefit of AI replications, even with uncertainties in these early years of AI technology, may be particularly appealing given the immense task, and consequent neglect, of replicating research. Not only do the demands of time and money for individual researchers often preclude replication, but replication is still considered questionable with respect to acceptable professional contributions for academic advancement. Beyond individual scholars, replication projects organized by groups of researchers (e.g., Baumeister, et al., 2023), might similarly use the AI results to prioritize field-wise research agendas and to review progress in the field more generally.\nReplication possibilities represent a strong promise for media research, similar to other uses that may allow AI to impersonate humans in simulated media environments, create experimental media stimuli or generate plausible human behavior that can be placed within media presentations. All of these applications depend to varying degrees on AI being able to offer insight into how human intelligence works, which is the promise that garners the most discussion for both scientific advancement as well as"}, {"title": "Accuracy of AI Replications", "content": "apocalyptic worries. Perhaps equally as influential in the near term, however, is the ability of AI to just make research faster and cheaper \u2013 possibly by several orders of magnitude. A revolution in research efficiency could depend primarily on predictive rather than explanatory advancements. But regardless of whether AI can explain how and why humans reason about media messages, if AI can make an accurate prediction about results, using any path available in the LLM, the nature of research could be dramatically changed. We mention possible changes in two different research contexts, at opposite ends of an applied-theory continuum.\nMuch of applied research is interested in specific tests of alternatives for persuasive messages, often in the context of message design exercises that proceed quickly and without significant resources. For example, researchers might want to pretest versions of a health PSA designed to change behavior, TV advertisements designed to solidify brands, or social media posts seeking to promote clicks. In any of these applied settings, new studies using AI personas could be conducted in minutes, maybe even during the one-hour meeting where designers propose, test, and select a finalist message. In the same design meeting, multiple versions of similar tests could be conducted that explore subtle differences in language use, differences among children, adolescents and adults or differences in the US, EU, and Asia. Designers could also quickly test the effects of placing messages in different contexts (e.g., different social media platforms). Responses could be assessed within minutes, and the results plugged into design discussions in close to real time.\nScholarly programs building theory in media psychology could be similarly advantaged by speed and cost. Theory development that requires the sequencing of questions about media could proceed more quickly (e.g., depending on the results of a first study, we could move to one of two second questions). A significant advantage in theory building with an AI persona method is the ability to quickly and purposefully go between induction and deduction. That is, sequencing questions about the description of what people are doing as separate for the explanation of why and how those actions might happen. Currently, theory in media psychology privileges deductions from theory, possibly prematurely because we do not yet have adequate descriptions of the media behaviors that new theories should be about. AI studies could accelerate transitions in the inductive-deductive cycle.\nThe ability to move quickly in research also connects well with the extremely fast pace at which media technologies and their uses are changing. As media psychologists, we volunteer to link theories with characteristics of media; for example, the pacing of presentations, visual vs. textual emphases, interactive potential. But those features constantly change. Currently, much of media psychology theory is based on media of decades past, and especially television. New technologies dramatically shift what theories should be developed to make certain we are studying the most essential features of the stimuli that ground our interests.\nAny excitement about AI personas depends crucially on accuracy. There is not perfect agreement between AI and human studies, and while the absolute level of agreement is high (76%), there is still uncertainty about errors, both within the matching and divergent results. Here, we review three study features that determine the level of AI-Human matching, and other general issues that will be critical to understand as Al persona research develops."}, {"title": "Interaction vs. main effects", "content": "Main effects were substantially easier to replicate with AI personas (76%) than were interaction effects (27%), meaning that interaction effects may be particularly prone to false positives or inflated effect sizes. This is a pattern that has been identified in human subject replications of social research, and the differences between main and interaction effect results are similar to our finding. Crede and Sotola (2024) examined 244 tests of interaction effects from leading organizational science journals and estimated an overall replicability rate of only 37% using z-curve analysis. They found that over half of reported p values for interactions fell between .01 and .05, far higher than expected for well-powered studies of true effects. Similarly, Altmejd, et al. (2019) suggest that the study attribute most predictive of poor replicability is whether central tests describe interactions between variables or (single-variable) main effects. In their review of organizational behavior studies, only eight of 41 interaction effect studies replicated, while 48 of the 90 other studies did. One explanation for the lower replicability of interaction effects may be that holding sample size constant, interactions will have lower statistical power. In addition, they also represent more complex conceptual expectations. Additionally, many reported interaction effects may simply not reflect reality: Sherman and Pashler (2019) analyzed five large-scale datasets including thousands of participants and hundreds of demographic and psychological predictors and found that interaction effects are generally small, infrequent, and likely due to sampling error. Thus, while virtual participants may not permit substantive replication of interaction effects reported in the existing literature, such an approach appears no worse than what can be gleaned through replication with human subjects and may actually permit a faster, less expensive route towards assessing the relative validity of previously reported moderation findings."}, {"title": "Ground Truth", "content": "There are several possible explanations for mismatches between human subject studies and AI persona replications. In our analysis, about 1 in 4 of the statistically significant main effects reviewed resulted in no significant differences when using AI personas. Notably, there were no cases where the two methods disagreed in the direction of effects; differences were only about whether they could be viewed as statistically significant.\nReconciling the differences is not (yet) straightforward. There are two different arguments for determining which results, human subjects or AI personas, represent the most accurate characterization of an effect when there are differences. So far in the literature, ground truth is mostly the province of results based on human data. We know, however, that there are multiple critiques of human subjects studies, not necessarily forwarded in the context of AI alternatives, that limit conclusions from those studies. These critiques prominently include biases associated with gender, race, age, and cultural context. Since the LLM models are trained on information that also includes those biases, it is possible (and even likely) that biases get transferred into the AI models. These biases might be made less influential in the Al models; for example, using our replication tool, studies could be changed to include samples of different (and hard to acquire) demographic backgrounds. More inclusive AI samples might even flip the ground truth assumptions, making diversity made possible with AI the gold standard."}, {"title": "Stimulus Sampling", "content": "The generalizability crisis underscores the danger of making broad claims based on narrow samples. Reeves et al. (2016) highlight that while there is substantial investment in sampling human subjects in"}, {"title": "General Challenges with AI and Social Research", "content": "research about responses to media stimuli, there is a significant underinvestment in the sampling of media stimuli themselves. This imbalance poses a severe threat to the external validity and to the overall usefulness of research findings (Cummings & Reeves, 2022). In media psychology, this crisis is exacerbated by the reliance on limited, often unrepresentative, media stimuli. Common practice often overlooks extensive variability inherent in media content, which can lead to erroneous conclusions and undermine the reliability of research findings. Even when participant samples and procedures are replicated, different stimuli can lead to divergent outcomes, suggesting that the original findings may not have been robust or generalizable.\nWestfall et al. (2015) emphasize that research must resample stimuli in replication attempts to increase confidence in the findings, as relying solely on participant replication without considering stimulus variation can introduce unintended variables that alter results. By not adequately sampling media stimuli, researchers risk committing Type I errors, where they falsely identify effects that do not generalize beyond the specific stimuli used. This contributes to the replication crisis, as subsequent studies using different stimuli may fail to reproduce the original findings, and primarily because they use different stimuli rather than different participants. Therefore, to address the generalizability crisis, it is essential that researchers invest in sampling a broader and more representative range of stimuli, not just subjects. This approach would ensure that research findings are more robust, replicable, and applicable to real-world contexts. AI tools can help substantially with this problem because they can easily produce new media to detailed specifications, a task that would require significant resources and time if done by human designers and producers.\nOne of the greatest challenges to the evaluation of AI applications is the inability of researchers to examine training data information from proprietary LLMs. Consequently, it is difficult to understand how biases of the internet and other training data might affect the accuracy of models as applied in certain contexts. Al represents one of the first major technology developments that has flipped the progression of research from university labs to technology companies. For AI, it is the technology companies who now own access to the models and other labs who are trying to understand how the models work. There are important new calls for development of open LLMs that will allow appropriate experimentation and knowledge about training data and those will be critical for developing social research applications of AI (Li, 2023).\nEven working within the constraints of current commercial models, there are parameters of the models that can be manipulated by researchers to increase the value of the technology. Two important features are the specification of prompts that translate variables and measures into information on which LLMs can operate, and the temperature settings that models provide that control the variance (or determinism) of models. In this research, we used a temperature of 0.7. This resulted in some instances where the LLM's responses showed less variance, as measured by standard deviations, compared to human responses for the same findings. Higher temperatures may lead to more variance in LLM responses, which could mitigate the tendency for LLMs to produce homogenous results."}, {"title": "Conclusion", "content": "research about responses to media stimuli, there is a significant underinvestment in the sampling of media stimuli themselves. This imbalance poses a severe threat to the external validity and to the overall usefulness of research findings (Cummings & Reeves, 2022). In media psychology, this crisis is exacerbated by the reliance on limited, often unrepresentative, media stimuli. Common practice often overlooks extensive variability inherent in media content, which can lead to erroneous conclusions and undermine the reliability of research findings. Even when participant samples and procedures are replicated, different stimuli can lead to divergent outcomes, suggesting that the original findings may not have been robust or generalizable.\nWe believe the efficiency rationale for pursuit of AI personas is compelling. The exercise in this project simulated 133 empirical tests that were originally conducted with 19,447 human subjects by 47 different researchers and published in 14 articles with publication timelines that take months and more often years to complete. The money spent is likely in the tens or hundreds of thousands of dollars (and millions counting salaries), and the time spent by 47 researchers would be in the months and years. Importantly, adequate replication designs may be even more costly: recent work suggests replication studies require samples 16 times that of the original study to achieve the 80% of the power required for detecting the original effect with ap value of .05 (van Zwet & Goodman, 2022). Our replication studies, consisting of nearly 20,000 AI personas, were conducted with tens of dollars in only a few hours. That is a huge resource and time advantage, certainly enough to warrant continued evaluation of the accuracy of AI subject studies."}]}