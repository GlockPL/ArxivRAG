{"title": "Using Large Language Models to Create AI Personas for\nReplication and Prediction of Media Effects:\nAn Empirical Test of 133 Published Experimental\nResearch Findings", "authors": ["Leo Yeykelis", "Kaavya Pichai", "James J. Cummings", "Byron Reeves"], "abstract": "This report analyzes the potential for large language models (LLMs) to expedite accurate replication of\npublished message effects studies. We tested LLM-powered participants (personas) by replicating 133\nexperimental findings from 14 papers containing 45 recent studies in the Journal of Marketing (January\n2023-May 2024). We used a new software tool, Viewpoints AI (https://viewpoints.ai/), that takes study\ndesigns, stimuli, and measures as input, automatically generates prompts for LLMs to act as a specified\nsample of unique personas, and collects their responses to produce a final output in the form of a complete\ndataset and statistical analysis. The underlying LLM used was Anthropic's Claude Sonnet 3.5. We\ngenerated 19,447 AI personas to replicate these studies with the exact same sample attributes, study\ndesigns, stimuli, and measures reported in the original human research. Our LLM replications\nsuccessfully reproduced 76% of the original main effects (84 out of 111), demonstrating strong potential\nfor AI-assisted replication of studies in which people respond to media stimuli. When including\ninteraction effects, the overall replication rate was 68% (90 out of 133). The use of LLMs to replicate and\naccelerate marketing research on media effects is discussed with respect to the replication crisis in social\nscience, potential solutions to generalizability problems in sampling subjects and experimental conditions,\nand the ability to rapidly test consumer responses to various media stimuli. We also address the\nlimitations of this approach, particularly in replicating complex interaction effects in media response\nstudies, and suggest areas for future research and improvement in AI-assisted experimental replication of\nmedia effects.", "sections": [{"title": "STUDY OVERVIEW AND RELATED WORK", "content": "Research about the effectiveness of media messages is increasingly difficult, attributable to both\nadministrative challenges (e.g., stimulus acquisition and creation, data management demands of digital\ntrace data, acquisition of participants and especially those in special groups like children, minorities and\ninternational groups), as well as requirements to deal with new and critical challenges to the very nature\nof social research, as exemplified by existential issues of replication and reproducibility, and the ability to\ngeneralize findings across people, media stimuli and experimental contexts. We briefly review these\nissues with an eye toward our current test of whether new LLM tools may help solve the problems\nmentioned, and with significant advantages in cost, time, and research personnel. The fundamental\nquestion for this project is whether an AI can accurately answer research questions, with accuracy\nmeasured in this case as the ability of AI to virtually replicate identical studies (i.e., the same media\nstimuli, measures, and participate sample specifications) that were conducted with human subjects.\n\nThe Replication Crisis in Social Sciences\nIoannidis (2005) suggested that most published research findings are false, citing small sample sizes,\nsmall effect sizes, and researcher degrees of freedom as contributing factors. The Open Science\nCollaboration (2015) corroborated this concern, successfully replicating only 39% of psychology studies.\nHubbard and Armstrong (1994) reported low replication rates in marketing journals. Munaf\u00f2 et al. (2017)\nidentified publication bias, p-hacking, and lack of replication incentives as key factors in this crisis.\n\nThe Generalizability Crisis\nYarkoni (2022) extended the critique beyond replicability to a \"generalizability crisis,\" arguing that\nresearchers often draw overly broad conclusions from narrow empirical findings. This issue is particularly\npertinent in marketing and media effects research, where findings are frequently presumed to generalize\nacross diverse consumer contexts.\n\nTo address such concerns Yarkoni (2022) advocated more rigorous consideration of research boundary\nconditions, increased use of large-scale naturalistic datasets, and development of computational tools for\ncomprehensive analysis of research claims. More recently, there has also been growing interest in the\nprospect of leveraging AI to address historical and ongoing crises and less severe threats to validity in the\nsocial sciences (Bail, 2024), and initial attempts to use AI in research replication have shown promise.\nRecent advancements in LLMs, exemplified by GPT-3 (Brown et al., 2020) and BERT (Devlin et al.,\n2018), have expanded applications to research tasks. These include literature review automation,\nhypothesis generation, and scientific discovery assistance (Wang et al., 2023). In marketing research,\nLLMs have been applied to sentiment analysis of consumer reviews (Zhang et al., 2022), and marketing\ncontent generation (Kshetri et al., 2024). However, the potential of AI to replicate experimental studies in\nmedia effects research remains largely unexplored. More broadly in psychology, Binz et al. (2023)\nshowed the capability of GPT-3 to replicate psychological experiments, finding success particularly in\nlanguage-based tasks.\n\nWhile interest grows in addressing the replication crisis and leveraging AI in research, empirical efforts\nhave not extensively explored Al's capacity to replicate more complex research scenarios in which users\nrespond to specific mediated stimuli. Our study aims to address this gap by systematically applying"}, {"title": "METHOD", "content": "LLM-powered participants to replicate recent marketing experiments that investigated human\nparticipants' responses to media stimuli, assessing the feasibility of this approach, and potentially\nenhancing the reliability and generalizability of research findings in marketing.\n\nSampling Stimulus-Response Studies for Replication with AI Participants\nTo assess the replicability of marketing experiments using LLMs, we collected a sample of recent\nresearch that experimentally examined responses to different types of media messages. Specifically, we\nsystematically reviewed all articles in the Journal of Marketing published from January 2023 through\nMay 2024. This resulted in an initial corpus of 69 papers containing 210 unique studies. The Journal of\nMarketing was chosen for this initial test of AI replication accuracy because it frequently publishes tests\nof media message effectiveness (consistent with our interest in commercial and theoretical work about\nmedia psychology), and because journal policies encourage detailed reporting about measures, sampling\nand inclusion of actual visual and textual material used in studies.\n\nWe then reviewed this sample of candidate studies to evaluate suitability for AI-assisted replication. We\napplied the following inclusion criteria: (1) the study had to be a true experiment incorporating\nmanipulated study conditions (not simply a survey that scored participant attitudes or beliefs or a study\nthat compared correlations between individual difference variables and outcomes); (2) it had to be\ncompatible with the features of our Al software (e.g., manipulating stimuli between conditions and\npresenting all questions at the end); (3) all original study materials (i.e., stimuli and measures) needed to\nbe provided by the authors or otherwise publicly available; and (4) the study procedures or outcomes\ncould not require physical actions or behavioral measures (e.g., eye-tracking, monitoring of subsequent\npurchasing behavior). In essence, for this initial test, we selected experiments that could typically be\nconducted through online recruitment platforms like Mechanical Turk or Prolific. This selection process\nresulted in a final total of 45 studies sourced from 14 distinct research articles\n\nEach study's experimental procedure, data collection, and analysis was then replicated using new\nsoftware, Viewpoints AI. Viewpoints AI is software designed to test AI responses to different versions of\nmultimodal media. The software allows researchers to input various media stimuli (images, videos, or\ntext), organize them into experimental conditions, specify participant characteristics, and define survey\nquestions, and scales. The system then generates responses from participants based on these parameters.\nFor each study, a series of unique LLM instantiations\u2014 one for each virtual persona\u2014is created on the\nfly (i.e. in real time as the study was run) to exactly match the sample distributions, characteristics, and\ncontext as given in the actual study. Each persona was then given the exact text, image, and/or video\nstimulus used in the original study to view, along with all other original study instructions. The creation of\na unique AI instance for each virtual persona differentiates Viewpoints AI from other attempts to use AI\nto answer questions in social science research.\n\nSeparate AI instances for each virtual participant also allows instances to be statistically aggregated, and\ndifferences tested, in exactly the same way that human subject statistical results are computed in the\noriginal research. For each study, we replicated the experimental conditions and measures as closely as"}, {"title": "Strategy for Creating LLM-Powered Participants", "content": "possible within the constraints of the Viewpoints AI platform. This replication effort entailed generating a\ngrand total of 19,447 virtual participants across the 45 studies.\n\nFor each study, we generated an equivalent number of LLM-powered participants to match the sample\nsize (N) of the original experiment. Each participant was generated as an individual instance of\nAnthropic's Claude Sonnet 3.5, one of the most advanced LLMs\navailable at the time of this research. We used a model temperature of 0.7, as this value is currently the\nindustry standard . The sample of participants was then imbued with specific persona\ncharacteristics mirroring the types, frequencies, and distributions of those reported in the original study.\nFor example, a generated participant might be characterized as a \"45-year-old woman with 20 years of\nmanagerial experience in the manufacturing industry.\"\n\nOur software then constructed a prompt that instructed the LLM to i) embody the assigned persona, ii)\nexamine the presented stimuli (which could include text, images, videos, or any combination thereof), and\niii) respond to the subsequent questions. The question wordings and response scales provided to the\ngenerated participants were directly transplanted from the original experiments, maintaining fidelity to the\nsource material. This approach allowed for flexibility in accommodating various question types and scale\nformats, ranging from open-ended queries (e.g., \"What is the highest price you would be willing to pay\nfor this product?\u201d) to Likert-style scales of varying points (e.g., 1 = very unlikely, 7 = very likely).\n\nA crucial aspect of our methodology was ensuring that the LLM-powered participants remained unaware\nof the study goals or of the original study being replicated, thereby precluding the possibility of them\nusing study-specific training data in providing their responses. This design choice was implemented to\nminimize any potential reference to training data that might pertain to the experiments, as our primary\ninterest lay in assessing whether LLMs could generate responses from simulated human personas that in\naggregate-closely resemble those from real human samples when exposed to media messages. To\nvalidate this approach, we conducted a pretest using Claude 3.5 Sonnet, querying its awareness of any of\nthe 14 published papers whose experiments we aimed to replicate. The model reported no prior\nknowledge of these studies, suggesting no direct threat to the integrity of our replication efforts. Further,\nwe deliberately avoided mentioning any paper title, authors, or journal information when presenting the\nstimuli to the Al personas, ensuring the responses were based on the given prompts rather than any prior\nknowledge.\n\nFor a given study's data collection process, each generated LLM participant corresponded to a unique API\ncall. After responding to the measures, their answers were logged in a database. We then extracted these\nresponses and employed R for statistical analysis, replicating the exact statistical procedures used by the\noriginal authors. This included various techniques such as ANOVA, linear models, and chi-squared tests.\n\nAssessing Replication of Original Human Participant Findings\nOur unit of analysis for replication was each predictive \"finding\" within a study (i.e., each main effect of\nan independent variable on a dependent variable, or interaction of multiple independent variables on a\ndependent variable). For each of the 45 studies, we compared our results to those reported in the original\nexperiment, resulting in 133 replication observations."}, {"title": "RESULTS", "content": "possible within the constraints of the Viewpoints AI platform. This replication effort entailed generating a\ngrand total of 19,447 virtual participants across the 45 studies.\n\nStrategy for Creating LLM-Powered Participants\nFor each study, we generated an equivalent number of LLM-powered participants to match the sample\nsize (N) of the original experiment. Each participant was generated as an individual instance of\nAnthropic's Claude Sonnet 3.5, one of the most advanced LLMs\navailable at the time of this research. We used a model temperature of 0.7, as this value is currently the\nindustry standard . The sample of participants was then imbued with specific persona\ncharacteristics mirroring the types, frequencies, and distributions of those reported in the original study.\nFor example, a generated participant might be characterized as a \"45-year-old woman with 20 years of\nmanagerial experience in the manufacturing industry.\"\n\nOur software then constructed a prompt that instructed the LLM to i) embody the assigned persona, ii)\nexamine the presented stimuli (which could include text, images, videos, or any combination thereof), and\niii) respond to the subsequent questions. The question wordings and response scales provided to the\ngenerated participants were directly transplanted from the original experiments, maintaining fidelity to the\nsource material. This approach allowed for flexibility in accommodating various question types and scale\nformats, ranging from open-ended queries (e.g., \"What is the highest price you would be willing to pay\nfor this product?\u201d) to Likert-style scales of varying points (e.g., 1 = very unlikely, 7 = very likely).\n\nA crucial aspect of our methodology was ensuring that the LLM-powered participants remained unaware\nof the study goals or of the original study being replicated, thereby precluding the possibility of them\nusing study-specific training data in providing their responses. This design choice was implemented to\nminimize any potential reference to training data that might pertain to the experiments, as our primary\ninterest lay in assessing whether LLMs could generate responses from simulated human personas that in\naggregate-closely resemble those from real human samples when exposed to media messages. To\nvalidate this approach, we conducted a pretest using Claude 3.5 Sonnet, querying its awareness of any of\nthe 14 published papers whose experiments we aimed to replicate. The model reported no prior\nknowledge of these studies, suggesting no direct threat to the integrity of our replication efforts. Further,\nwe deliberately avoided mentioning any paper title, authors, or journal information when presenting the\nstimuli to the Al personas, ensuring the responses were based on the given prompts rather than any prior\nknowledge.\n\nFor a given study's data collection process, each generated LLM participant corresponded to a unique API\ncall. After responding to the measures, their answers were logged in a database. We then extracted these\nresponses and employed R for statistical analysis, replicating the exact statistical procedures used by the\noriginal authors. This included various techniques such as ANOVA, linear models, and chi-squared tests.\n\nAssessing Replication of Original Human Participant Findings\nOur unit of analysis for replication was each predictive \"finding\" within a study (i.e., each main effect of\nan independent variable on a dependent variable, or interaction of multiple independent variables on a\ndependent variable). For each of the 45 studies, we compared our results to those reported in the original\nexperiment, resulting in 133 replication observations.\n\nOverall Replication Success Rate with Virtual Participants\nOverall, our LLM replications successfully reproduced 76% of the original main effect findings (84 out of\n111). When including interaction effects, the replication rate was 68% (90 out of 133). Notably, the\nreplication rate for interaction effects alone was substantially lower at 27%. This difference in replication\nsuccess for main and interaction effects parallels the differences for replication of human effects studies.\nCrede and Sotola (2024) suggest the poorer replicability of interaction effects compared to main effects\ncan be attributed due the relatively higher statistical power needed for detection and the greater potential\ninfluence of context-specific factors. Additionally, as these statistical attributes of interaction effects lead\nto a higher proportion of non-significant findings compared to main effects, they may also lead to greater\nsusceptibility to publication bias and questionable research practices (QRPs) than in the case for the\ntesting and reporting of main effects including biases associated with selective reporting (Ioannidis, 2008)\nparticularly in the social sciences (Fanelli, 2010) and HARKing (Hypothesizing After the Results\nare Known; Kerr, 1998).\n\nReplication Success Rate by p value Range\nPrevious literature suggests several reasons why replication success may vary as a function of the p value\nof the originally observed effect. For instance, Ioannidis (2005) described how the probability of a finding\nbeing indeed true depends on, amongst other things, the level of statistical significance; and further,\nstudies with p values nearer to .05 may be the result of biases in reporting, meaning they would in turn be\nless likely to replicate. Additionally, even without any such biases, p values nearer to .05 may indicate\ngreater likelihood that the original effect is marginal (Wasserstein & Lazar, 2016). To this end, we\nexamined the relationship between the p value of the original findings and the rate of successful\nreplication."}, {"title": "DISCUSSION", "content": "Overall Replication Success Rate with Virtual Participants\nOverall, our LLM replications successfully reproduced 76% of the original main effect findings (84 out of\n111). When including interaction effects, the replication rate was 68% (90 out of 133). Notably, the\nreplication rate for interaction effects alone was substantially lower at 27%. This difference in replication\nsuccess for main and interaction effects parallels the differences for replication of human effects studies.\nCrede and Sotola (2024) suggest the poorer replicability of interaction effects compared to main effects\ncan be attributed due the relatively higher statistical power needed for detection and the greater potential\ninfluence of context-specific factors. Additionally, as these statistical attributes of interaction effects lead\nto a higher proportion of non-significant findings compared to main effects, they may also lead to greater\nsusceptibility to publication bias and questionable research practices (QRPs) than in the case for the\ntesting and reporting of main effects including biases associated with selective reporting (Ioannidis, 2008)\nparticularly in the social sciences (Fanelli, 2010) and HARKing (Hypothesizing After the Results\nare Known; Kerr, 1998).\n\nReplication Success Rate by p value Range\nPrevious literature suggests several reasons why replication success may vary as a function of the p value\nof the originally observed effect. For instance, Ioannidis (2005) described how the probability of a finding\nbeing indeed true depends on, amongst other things, the level of statistical significance; and further,\nstudies with p values nearer to .05 may be the result of biases in reporting, meaning they would in turn be\nless likely to replicate. Additionally, even without any such biases, p values nearer to .05 may indicate\ngreater likelihood that the original effect is marginal (Wasserstein & Lazar, 2016). To this end, we\nexamined the relationship between the p value of the original findings and the rate of successful\nreplication. \n\nIntroduction\nOur study suggests that empirical studies about media message processing can use AI virtual personas to\nreplicate existing research, and especially the main effects proposed in studies. The level of replication,\nwhile not perfectly matched with identical human studies, is at a level of success (76%) that offers\npromise to help solve an important challenge to current social research.\n\nNot all original study results replicated similarly, however, with implications for capabilities and\nlimitations of LLMs for consumer behavior research. First, the high reliability of LLMs in replicating\nstrongly significant findings suggests their value for confirming robust effects. Second, the observed\ndecline in replication success as p values increase emphasizes the critical role of the original evidence's\nstrength in the interpretation of LLM-based replications. Stronger original evidence is more likely to be\nsuccessfully replicated by LLMs, which is an important consideration for researchers relying on these\nmodels. Third, the mixed performance of LLMs on findings with marginal or non-significant p values\nraises concerns about AI sensitivity to subtle effects. This variability suggests that LLMs may be prone to\nboth false positives and false negatives, indicating a potential risk when using them to detect or confirm\nless pronounced effects. Last, the balanced replication outcomes for p values above 0.5 reveal that while\nLLMs may sometimes accurately identify the absence of an effect, they also risk introducing spurious\nfindings. This dual possibility underscores the need for caution when interpreting LLM-based\nreplications, particularly in cases where the original findings suggest a null effect.\n\nThe value of AI replication could come in several forms. Most obviously, the replications, when they\nagree with identical human subject studies, can provide evidence that existing results may be used with\nconfidence. And when the AI replications do not work, this might identify results that should be a priority\nfor replications with human subjects so that differences can be adjudicated. The general benefit of AI\nreplications, even with uncertainties in these early years of AI technology, may be particularly appealing\ngiven the immense task, and consequent neglect, of replicating research. Not only do the demands of time\nand money for individual researchers often preclude replication, but replication is still considered\nquestionable with respect to acceptable professional contributions for academic advancement. Beyond\nindividual scholars, replication projects organized by groups of researchers (e.g., Baumeister, et al., 2023),\nmight similarly use the AI results to prioritize field-wise research agendas and to review progress in the\nfield more generally.\n\nReplication possibilities represent a strong promise for media research, similar to other uses that may\nallow AI to impersonate humans in simulated media environments, create experimental media stimuli or\ngenerate plausible human behavior that can be placed within media presentations. All of these\napplications depend to varying degrees on AI being able to offer insight into how human intelligence\nworks, which is the promise that garners the most discussion for both scientific advancement as well as"}, {"title": "Accuracy of AI Replications", "content": "apocalyptic worries. Perhaps equally as influential in the near term, however, is the ability of AI to just\nmake research faster and cheaper \u2013 possibly by several orders of magnitude. A revolution in research\nefficiency could depend primarily on predictive rather than explanatory advancements. But regardless of\nwhether AI can explain how and why humans reason about media messages, if AI can make an accurate\nprediction about results, using any path available in the LLM, the nature of research could be dramatically\nchanged. We mention possible changes in two different research contexts, at opposite ends of an\napplied-theory continuum.\n\nMuch of applied research is interested in specific tests of alternatives for persuasive messages, often in the\ncontext of message design exercises that proceed quickly and without significant resources. For example,\nresearchers might want to pretest versions of a health PSA designed to change behavior, TV\nadvertisements designed to solidify brands, or social media posts seeking to promote clicks. In any of\nthese applied settings, new studies using AI personas could be conducted in minutes, maybe even during\nthe one-hour meeting where designers propose, test, and select a finalist message. In the same design\nmeeting, multiple versions of similar tests could be conducted that explore subtle differences in language\nuse, differences among children, adolescents and adults or differences in the US, EU, and Asia. Designers\ncould also quickly test the effects of placing messages in different contexts (e.g., different social media\nplatforms). Responses could be assessed within minutes, and the results plugged into design discussions\nin close to real time.\n\nScholarly programs building theory in media psychology could be similarly advantaged by speed and\ncost. Theory development that requires the sequencing of questions about media could proceed more\nquickly (e.g., depending on the results of a first study, we could move to one of two second questions). A\nsignificant advantage in theory building with an AI persona method is the ability to quickly and\npurposefully go between induction and deduction. That is, sequencing questions about the description of\nwhat people are doing as separate for the explanation of why and how those actions might happen.\nCurrently, theory in media psychology privileges deductions from theory, possibly prematurely because\nwe do not yet have adequate descriptions of the media behaviors that new theories should be about. AI\nstudies could accelerate transitions in the inductive-deductive cycle.\n\nThe ability to move quickly in research also connects well with the extremely fast pace at which media\ntechnologies and their uses are changing. As media psychologists, we volunteer to link theories with\ncharacteristics of media; for example, the pacing of presentations, visual vs. textual emphases, interactive\npotential. But those features constantly change. Currently, much of media psychology theory is based on\nmedia of decades past, and especially television. New technologies dramatically shift what theories\nshould be developed to make certain we are studying the most essential features of the stimuli that ground\nour interests.\n\nAny excitement about AI personas depends crucially on accuracy. There is not perfect agreement between\nAI and human studies, and while the absolute level of agreement is high (76%), there is still uncertainty\nabout errors, both within the matching and divergent results. Here, we review three study features that\ndetermine the level of AI-Human matching, and other general issues that will be critical to understand as\nAl persona research develops."}, {"title": "Interaction vs. main effects", "content": "Any excitement about AI personas depends crucially on accuracy. There is not perfect agreement between\nAI and human studies, and while the absolute level of agreement is high (76%), there is still uncertainty\nabout errors, both within the matching and divergent results. Here, we review three study features that\ndetermine the level of AI-Human matching, and other general issues that will be critical to understand as\nAl persona research develops.\n\nMain effects were substantially easier to replicate with AI personas (76%) than were interaction effects\n(27%), meaning that interaction effects may be particularly prone to false positives or inflated effect sizes.\nThis is a pattern that has been identified in human subject replications of social research, and the\ndifferences between main and interaction effect results are similar to our finding. Crede and Sotola (2024)\nexamined 244 tests of interaction effects from leading organizational science journals and estimated an\noverall replicability rate of only 37% using z-curve analysis. They found that over half of reported p\nvalues for interactions fell between .01 and .05, far higher than expected for well-powered studies of true\neffects. Similarly, Altmejd, et al. (2019) suggest that the study attribute most predictive of poor\nreplicability is whether central tests describe interactions between variables or (single-variable) main\neffects. In their review of organizational behavior studies, only eight of 41 interaction effect studies\nreplicated, while 48 of the 90 other studies did. One explanation for the lower replicability of interaction\neffects may be that holding sample size constant, interactions will have lower statistical power. In\naddition, they also represent more complex conceptual expectations. Additionally, many reported\ninteraction effects may simply not reflect reality: Sherman and Pashler (2019) analyzed five large-scale\ndatasets including thousands of participants and hundreds of demographic and psychological\npredictors and found that interaction effects are generally small, infrequent, and likely due to sampling\nerror. Thus, while virtual participants may not permit substantive replication of interaction effects\nreported in the existing literature, such an approach appears no worse than what can be gleaned through\nreplication with human subjects and may actually permit a faster, less expensive route towards assessing\nthe relative validity of previously reported moderation findings.\n\nGround Truth\nThere are several possible explanations for mismatches between human subject studies and AI persona\nreplications. In our analysis, about 1 in 4 of the statistically significant main effects reviewed resulted in\nno significant differences when using AI personas. Notably, there were no cases where the two methods\ndisagreed in the direction of effects; differences were only about whether they could be viewed as\nstatistically significant.\n\nReconciling the differences is not (yet) straightforward. There are two different arguments for\ndetermining which results, human subjects or AI personas, represent the most accurate characterization of\nan effect when there are differences. So far in the literature, ground truth is mostly the province of results\nbased on human data. We know, however, that there are multiple critiques of human subjects studies, not\nnecessarily forwarded in the context of AI alternatives, that limit conclusions from those studies. These\ncritiques prominently include biases associated with gender, race, age, and cultural context. Since the\nLLM models are trained on information that also includes those biases, it is possible (and even likely) that\nbiases get transferred into the AI models. These biases might be made less influential in the Al models;\nfor example, using our replication tool, studies could be changed to include samples of different (and hard\nto acquire) demographic backgrounds. More inclusive AI samples might even flip the ground truth\nassumptions, making diversity made possible with AI the gold standard.\n\nStimulus Sampling\nThe generalizability crisis underscores the danger of making broad claims based on narrow samples.\nReeves et al. (2016) highlight that while there is substantial investment in sampling human subjects in"}, {"title": "General Challenges with AI and Social Research", "content": "research about responses to media stimuli, there is a significant underinvestment in the sampling of media\nstimuli themselves. This imbalance poses a severe threat to the external validity and to the overall\nusefulness of research findings (Cummings & Reeves, 2022). In media psychology, this crisis is\nexacerbated by the reliance on limited, often unrepresentative, media stimuli. Common practice often\noverlooks extensive variability inherent in media content, which can lead to erroneous conclusions and\nundermine the reliability of research findings. Even when participant samples and procedures are\nreplicated, different stimuli can lead to divergent outcomes, suggesting that the original findings may not\nhave been robust or generalizable.\n\nWestfall et al. (2015) emphasize that research must resample stimuli in replication attempts to increase\nconfidence in the findings, as relying solely on participant replication without considering stimulus\nvariation can introduce unintended variables that alter results. By not adequately sampling media stimuli,\nresearchers risk committing Type I errors, where they falsely identify effects that do not generalize\nbeyond the specific stimuli used. This contributes to the replication crisis, as subsequent studies using\ndifferent stimuli may fail to reproduce the original findings, and primarily because they use different\nstimuli rather than different participants. Therefore, to address the generalizability crisis, it is essential\nthat researchers invest in sampling a broader and more representative range of stimuli, not just subjects.\nThis approach would ensure that research findings are more robust, replicable, and applicable to\nreal-world contexts. AI tools can help substantially with this problem because they can easily produce\nnew media to detailed specifications, a task that would require significant resources and time if done by\nhuman designers and producers.\n\nOne of the greatest challenges to the evaluation of AI applications is the inability of researchers to\nexamine training data information from proprietary LLMs. Consequently, it is difficult to understand how\nbiases of the internet and other training data might affect the accuracy of models as applied in certain\ncontexts. Al represents one of the first major technology developments that has flipped the progression of\nresearch from university labs to technology companies. For AI, it is the technology companies who now\nown access to the models and other labs who are trying to understand how the models work. There are\nimportant new calls for development of open LLMs that will allow appropriate experimentation and\nknowledge about training data and those will be critical for developing social research applications of AI\n(Li, 2023).\n\nEven working within the constraints of current commercial models, there are parameters of the models\nthat can be manipulated by researchers to increase the value of the technology. Two important features are\nthe specification of prompts that translate variables and measures into information on which LLMs can\noperate, and the temperature settings that models provide that control the variance (or determinism) of\nmodels. In this research, we used a temperature of 0.7. This resulted in some instances where the LLM's\nresponses showed less variance, as measured by standard deviations, compared to human responses for\nthe same findings. Higher temperatures may lead to more variance in LLM responses, which could\nmitigate the tendency for LLMs to produce homogenous results."}, {"title": null, "content": "We believe the efficiency rationale for pursuit of AI personas is compelling. The exercise in this project\nsimulated 133 empirical tests that were originally conducted with 19,447 human subjects by 47 different\nresearchers and published in 14 articles with publication timelines that take months and more often years\nto complete. The money spent is likely in the tens or hundreds of thousands of dollars (and millions\ncounting salaries), and the time spent by 47 researchers would be in the months and years. Importantly,\nadequate replication designs may be even more costly: recent work suggests replication studies require\nsamples 16 times that of the original study to achieve the 80% of the power required for detecting the\noriginal effect with ap value of .05. Our replication studies, consisting of\nnearly 20,000 AI personas, were conducted with tens of dollars in only a few hours. That is a huge\nresource and time advantage, certainly enough to warrant continued evaluation of the accuracy of AI\nsubject studies."}]}