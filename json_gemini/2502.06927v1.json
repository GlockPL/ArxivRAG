{"title": "Neighborhood-Order Learning Graph Attention Network for Fake News Detection", "authors": ["Batool Lakzaei", "Mostafa Haghir Chehreghani", "Alireza Bagheri"], "abstract": "Fake news detection is a significant challenge in the digital age, which has become increasingly important with the proliferation of social media and on-line communication networks. Graph Neural Networks (GNN)-based meth-ods have shown high potential in analyzing graph-structured data for this problem. However, a major limitation in conventional GNN architectures is their inability to effectively utilize information from neighbors beyond the network's layer depth, which can reduce the model's accuracy and effectiveness. In this paper, we propose a novel model called Neighborhood-Order Learning Graph Attention Network (NOL-GAT) for fake news detection. This model allows each node in each layer to independently learn its optimal neighborhood order. By doing so, the model can purposefully and efficiently extract critical information from distant neighbors. The NOL-GAT architec-ture consists of two main components: a Hop Network that determines the optimal neighborhood order and an Embedding Network that updates node embeddings using these optimal neighborhoods. To evaluate the model's performance, experiments are conducted on various fake news datasets. Results demonstrate that NOL-GAT significantly outperforms baseline models in metrics such as accuracy and F1-score, particularly in scenarios with lim-ited labeled data. Features such as mitigating the over-squashing problem, improving information flow, and reducing computational complexity further highlight the advantages of the proposed model.", "sections": [{"title": "1. Introduction", "content": "In today's world, the Internet and social media platforms play a central role in communication and information sharing, with social networks becoming the primary channels for interaction [1], such that many individuals prefer online resources and social networks over traditional media for communication, entertainment, and news [2]. The unreliability of informa-tion disseminated on these platforms has drawn attention to the spread of misinformation on social networks as a significant social issue. A series of in-cidents in recent years have demonstrated that substantial harm, in the form of social disruption, can arise due to fake news [3], as fake news contains false and misleading information presented in the format of a news story, thereby possessing the potential to cause widespread turmoil across the globe [4].\nIn recent years, researchers have proposed various methods for detect-ing fake news using machine learning techniques, particularly deep learn-ing approaches, which are primarily based on supervised learning methods. These approaches often face challenges such as the lack of large, labeled datasets, making them costly and time-consuming [5]. Semi-supervised meth-ods, which rely on a small amount of labeled data, can provide an effective solution to overcome these challenges.\nThe representation of data plays a critical role in the success of ma-chine learning algorithms, particularly in scenarios with a scarcity of la-beled data [6]. Graph-based models efficiently analyze complex and non-Euclidean data [7] and excel in semi-supervised learning by capturing struc-tural dependencies [8]. These models have demonstrated high effectiveness for semi-supervised learning by leveraging structural and contextual rela-tionships between data points [9]. However, traditional machine learning methods and neural networks are not well-suited for processing graph data due to the non-Euclidean nature of such data. Graph Neural Networks (GNNs) [10, 11, 12, 13, 14, 15], introduced in recent years, have addressed this challenge with their unique capability to process graph data. These models enable effective learning from non-Euclidean data by employing specialized architectures designed to capture the complex structures and nonlinear rela-tionships inherent in graphs. In Graph Neural Networks, the goal is to learn a"}, {"title": "2. Related Works", "content": "Researchers have explored various techniques like natural language pro-cessing, machine learning, and social media analysis for fake news detection. These methods are generally divided into three categories [27]:\n\u2022 Content-based methods: which focus on analyzing the text or vi-suals to identify fake news [28].\n\u2022 Context-based methods: which use contextual data such as user profiles and propagation patterns [29].\n\u2022 Hybrid methods: which combine both content and context to detect fake news [30].\nOur approach is content-based, relying solely on textual content, which is the key feature in all existing datasets. As a result, our method can be applied to any dataset. We note that context information may not always be available.\nA significant portion of existing methods for fake news detection relies on supervised approaches [31, 32, 33]. The primary challenge and limitation of this approach is its dependency on large, labeled datasets, which makes it time-consuming and costly to generate. Additionally, obtaining sufficient labeled data can be challenging, especially when it comes to handling the vast amounts of news data available. Therefore, semi-supervised approaches, which can work with a small subset of labeled data and classify the larger"}, {"title": "3. Problem definition", "content": "Let $D = \\{d_1,d_2, ..., d_n\\}$ be the news set with n news items, where each news item $d_i$ contains text content $t_i$. We formulate the problem of fake news detection as a binary classification task utilizing a semi-supervised approach. The class labels are represented by $y \\in \\{0,1\\}$, where $y = 1$ indicates fake news, and $y = 0$ indicates real news. Following the semi-supervised approach, labels for a small subset of news, denoted as $D_L \\subset D$, are observed. Our goal is to learn the model $F$ for predicting labels for all unlabeled data $D_U = D \u2013 D_L$, utilizing the labeled news:\n$\\hat{y} = F(d_i)$   (1)\nwhere $\\hat{y} \\in \\{0,1\\}$ is the predicted label, and $d_i \\in D_U$ is an unlabeled example.\nWe summarize frequently used notations in Table 1."}, {"title": "4. Proposed model", "content": "In this paper, we propose our Neighborhood-Order Learning Graph At-tention Network (NOL-GAT) for detecting fake news using a semi-supervised approach. The NOL-GAT model, illustrated in Figure 1, consists of four main modules: feature extraction, graph construction, Neighborhood-Order Learn-ing Graph Attention Network (NOL-GAT), and classification. Initially, the feature extraction module generates textual embeddings for the news using the Doc2Vec model. Then, the graph construction module builds a similarity graph $G_{KNN}$ among the news items. In this graph, each news item is viewed as a node, and each node is connected to its K nearest neighbors through undirected edges. Then this graph is fed into the NOL-GAT model, where each node in each layer independently learns the most effective k-hop neigh-borhood to update its embedding vector. The final embedding vectors are fed into a Multi-Layer Perceptron (MLP) classifier, to be classified as fake or real."}, {"title": "4.1. Feature extraction", "content": "We utilize the Doc2Vec [47] approach, to analysis the textual content of news items and generate initial feature vectors for news items. It is an embedding algorithm designed to represent entire documents or paragraphs as fixed-length numerical vectors. It extends Word2Vec [48] by adding a unique identifier for each document, enabling it to capture the semantic meaning of the entire text:\n$Doc2Vec(t_i) = x_i$  (2)\nwhere $t_i$ denotes textual content for news item $d_i \\in D$, $x_i \\in R^{l_t}$ denotes the text embedding vector for $d_i$, and $l_t$ denotes the length of text's embedding vectors."}, {"title": "4.2. Graph construction", "content": "We model D (news items dataset) as a graph $G_{KNN} = (V_{KNN}, E_{KNN})$ employing K-nearest neighbors algorithm. We view each news items $d_i \\in$"}, {"title": "4.3. Neighborhood-Order Learning Graph Attention Network (NOL-GAT)", "content": "Graph neural networks update the target node's features through a mes-saging mechanism and information aggregation from its neighbors. However, the standard GNN architecture faces challenges that can limit its perfor-mance. One of the main challenges is the restricted access to information from more distant neighbors in the graph. In the standard setup, a GNN with L layers can at most obtain information from neighbors at a distance of L. This means that if a high-importance node (in terms of, for example, topology or features) is located farther than L hops from the target node, its information cannot influence the target node. For example, consider Fig-ure 2. In this graph, the nodes are arranged in a simple path with 7 nodes (a to g). Each node is connected to its adjacent nodes, i.e., its previous and next nodes if they exist. Now, assume we have a 2-layer GNN:\n\u2022 In this network, each node can collect information from its 1-hop neigh-bors at each layer.\n\u2022 After the first layer, each node will have information from itself and its 1-hop neighbors.\n\u2022 After the second layer, each node will have information from its 1-hop and 2-hop neighbors.\nIf the target node is a, after two layers of the GNN, this node will only be able to access information from nodes b and c. The information from other nodes will remain inaccessible to node a, even if these nodes hold important information. Increasing the number of layers in a GNN is proposed as a naive solution to this issue, but this approach comes with serious problems:\n\u2022 Over-squashing: As the number of layers increases, information col-lected from a large number of nodes is compressed into embeddings of"}, {"title": "5. Model properties", "content": "In this section, we present the key features of the proposed model. The NOL-GAT model offers unique characteristics that differentiate it from other GNN-based models. These features are as follows:"}, {"title": "1. Independence of decision-making at node and layer level", "content": "One of the key features of the NOL-GAT model is that the selection of the neighborhood order for each node is made independently of other nodes in the same layer. This means that nodes can select their neigh-"}, {"title": "2. Flexibility in neighbor selection", "content": "NOL-GAT uses an adaptive approach that allows each node to select the optimal neighbors based on its position in the graph. This adaptive approach is particularly beneficial in unbalanced graphs or graphs with nodes of varying degrees. As a result, the model can gather more precise information from its neighbors and prevent the transfer of irrelevant information."}, {"title": "3. Reduction of over-squashing and improved information flow", "content": "One of the main challenges in GNNs is the issue of over-squashing, which occurs when important information from distant nodes is exces-sively compressed and fails to be transferred correctly to the target node. In NOL-GAT, the targeted and limited selection of neighbors helps mitigate this issue, as information from distant nodes is trans-ferred to the target node without excessive squashing. This approach ensures that information from distant nodes reaches the target nodes more easily, without the need for complex routing or communication in deep graphs."}, {"title": "4. Reduction of over-smoothing", "content": "In GNNs, over-smoothing occurs when the features of nodes become overly similar as they propagate through multiple layers, causing se-mantic distinctions between nodes to vanish. This phenomenon typ-ically arises when the number of layers increases, and messages from nodes are spread horizontally and vertically across the graph. In NOL-GAT, the optimal neighborhood order selection for each node in every layer helps mitigate over-smoothing. This is because the model allows each node to select neighbors based on its own needs and position, and this targeted selection prevents the features of nodes from becoming too homogeneous across layers. This approach helps maintain feature di-versity and prevents over-similarity between features as they propagate through layers, allowing nodes to retain their distinctive characteristics during the update process."}, {"title": "5. Reduction of computational complexity", "content": "Unlike traditional models that require increasing the number of lay-ers to achieve high accuracy or increase computational complexity by transferring information through long paths, NOL-GAT keeps com-putational complexity under control by using a targeted selection of neighbors at each layer. This reduction in complexity allows the model to perform better without the need for excessive layers or network com-plexity."}, {"title": "6. Experiments", "content": "In this section, we first present an overview of the datasets, baseline models, and evaluation metrics employed in our empirical analysis. Next, we provide a detailed report on the results of our extensive experiments, conducted under various settings and conditions to thoroughly evaluate the effectiveness of our approach 1."}, {"title": "6.1. Datasets", "content": "We employ five fully labeled datasets to validate our proposed approach: two in Portuguese and three in English:\n\u2022 Fake.Br2 is a Portuguese dataset created for fake news detection, consisting of 7,200 articles evenly divided into 3,600 fake and 3,600 real news. These articles span six categories: politics (58%), TV and celebrities (21.4%), society and daily life (17.7%), science and technol-ogy (1.5%), economy (0.7%), and religion (0.7%) [54]. The dataset is available in both complete and truncated formats, and we utilize the truncated version to ensure consistent text length and avoid potential biases during learning.\n\u2022 Fact-checked News\u00b3 is a collection of 2,168 Brazilian political news articles curated between August 2018 and May 2019, a period marked by heightened disinformation around Brazil's presidential elections. This dataset, sourced from platforms such as AosFatos, Ag\u00eancia Lupa, and UOL Confere, includes 1,124 real and 1,044 fake news items."}, {"title": "6.2. Baseline models", "content": "We assess the performance of our proposed model by comparing it against six semi-supervised fake news detection methods:\n\u2022 CO-GNN [45] is a graph neural network framework where each node dynamically chooses one of four actions (Listen, Broadcast, Standard, or Isolate) at each layer to control the flow of information. The nodes update their states based on their own actions and the actions of neigh-boring nodes, enabling more flexible and adaptive message-passing compared to traditional methods.\n\u2022 L2Q [43] models the probability of quitting at each propagation step using a stick-breaking process, assigning a non-parametric probability to each step. This approach helps prevent the use of deeper layers and allows the model to automatically select the optimal propagation strategy for each node.\n\u2022 TGNcl [56] is a text classification model that transforms each text into a heterogeneous graph of words and labels, called a WL-graph. It learns text representations by encoding the graph using a message passing process, and then applies contrastive learning to enhance the feature differentiation. The model also employs graph augmentation techniques such as node drop, edge drop, and shuffle to improve ro-bustness and accuracy.\n\u2022 LSTM-CP-GCN [57] investigates intra-article interactions by mod-eling each article as a weighted graph, where sentences serve as ver-tices. An LSTM generates feature vectors for the vertices, while a CP decomposition-based approach computes the weight matrix using lo-cal word co-occurrence information between sentences. The resulting graph is then input into a GCN for classification.\n\u2022 TextGCN [58] models the entire corpus as a heterogeneous graph that includes both word and news nodes. It employs a GCN to learn embeddings for both node types, effectively capturing the relationships between words and news items to perform news classification.\n\u2022 GATv2 [53] utilizes the standard message passing mechanism in GATv2 to learn embeddings, ultimately applying them for news classification."}, {"title": "6.3. Experimental setup", "content": "For feature extraction from news text content, we utilize the Doc2Vec model, generating 500-dimensional vectors. The graph is constructed using the KNN method with seven different values for K (3, 4, 5, 6, 7,8). A semi-supervised learning strategy is adopted, where three scenarios with different proportions of labeled data (10%, 20%, and 30%) are considered. As men-tioned in subsection 4.3 The proposed model consists of two networks, hop network (\u03a6) and embedding network (\u03a8), both implemented as a two-layer GATv2 with 128 and 64 hidden units, respectively. The model is trained using the Adam optimizer with a weight decay strategy over 200 epochs.\nTo assess model performance, macro-F1 and Accuracy metrics are used. Each model (NOL-GAT and all Baselines) is executed 10 times, and the average results are reported. The accuracy metric is defined as follows:\n$Acc = \\frac{(TP + TN)}{(TP + FP + FN +TN)}$\n(11)\nHere, TP, TN, FP and FN indicate True Positive, True Negative, False Pos-itive, and False Negative, respectively. In scenarios where data distribution across different classes is imbalanced, relying solely on accuracy for evalua-tion becomes unwise. This is because methods that predominantly classify new samples into the majority class may achieve high accuracy while failing to properly distinguish the minority class [59]. Since fake news detection often involves imbalanced datasets, including the FNN dataset used in our study, we adopt macro-F1 as another evaluation metric. The macro-F1 score for a dataset with k classes is computed through the following equations:\n$P_i = \\frac{TP_i}{TP_i + FP_i},$    $R_i = \\frac{TP_i}{TP_i + FN_i}$     (12)\n$P_{Macro} = \\frac{\\sum_{i=1}^k P_i}{k}$,    $R_{Macro} = \\frac{\\sum_{i=1}^k R_i}{k}$   (13)\n$Macro-F1 = \\frac{2P_{Macro} \\times R_{Macro}}{P_{Macro} + R_{Macro}}$  (14)\nThe intereset-F1 is defined as follows:\n$P_{Interest} = \\frac{TP_{Interest}}{TP_{Interest} + FP_{Interest}}$ (15)"}, {"title": "6.4. Results", "content": "The performance of the NOL-GAT model is evaluated against six baseline models across five benchmark datasets: Fake.Br, Fact-checked News, FNN, FakeNewsDetection and FakeNewsData. The evaluation metrics considered are accuracy and macro-F1 score, with labeled data proportions of 10%, 20%, and 30%.\nOn the FakeBr dataset, as can be seen in Table 3, NOL-GAT significantly outperforms all the competing models across all the label proportions. With 10% labeled data, NOL-GAT achieves an accuracy of 0.8126 and a macro-F1 score of 0.8139, whereas the closest competitor, TextGCN, obtains an accuracy of 0.6235 and a macro-F1 score of 0.6112. As the label proportion increases to 30%, NOL-GAT further improves its performance, achieving an accuracy of 0.8287 and a macro-F1 score of 0.8327. This demonstrates the robustness of NOL-GAT in handling fake news detection in low-resource settings, maintaining a substantial margin over other methods, including Co-GNN and GATv2.\nOn the Fact-checked News dataset, as seen in Table 4, NOL-GAT again exhibits superior performance, with its accuracy increasing from 0.9358 (10% labeled data) to 0.9501 (30% labeled data). The macro-F1 scores follow a"}, {"title": "6.4.1. Analysis of the impact of adaptive message passing in NOL-GAT: a comparative study with standard GATv2", "content": "Figure 3 presents the comparison results between the proposed NOL-GAT model and the baseline GATv2 model in terms of accuracy and macro-F1. This comparison, conducted on five different datasets, demonstrates that NOL-GAT consistently outperforms GATv2 across all the cases. To ensure a fair comparison, both models share the same overall architecture, meaning that the number of layers, neurons, and other settings are identical, with the"}, {"title": "6.4.2. Analysis of the impact of the quantity of labeled data", "content": "As observed in Figure 4, the NOL-GAT model demonstrates improved performance across all datasets as the amount of labeled data increases. How-ever, the extent of this improvement varies depending on the characteristics of each dataset. In smaller datasets, increasing the amount of labeled data has had a more significant impact on enhancing the model's accuracy and its ability to distinguish between fake and real news. In contrast, in larger datasets, the model has already achieved a reasonable performance even with a small amount of labeled data, and further increases in labeled data have had a more limited effect on the results. The distribution of fake and real news also plays a crucial role in the model's performance. In datasets with a"}, {"title": "6.4.3. Analysis of the impact of parameter k", "content": "In this section, the effect of the k parameter in the KNN-based graph con-struction process on the performance of the NOL-GAT model is evaluated across five datasets. The macro-F1 score, which reflects the model's ability to distinguish between different classes, is analyzed for various values of k. The results indicate that the optimal value of k is generally within the range of 6 to 7. At lower values of k (particularly k=3 and k=4), the macro-F1 score is lower, likely due to insufficient connectivity between nodes in the graph, resulting in a loss of important structural information. This limitation re-duces the model's ability to learn meaningful relationships within the data and ultimately impaired overall performance. As k increased to 6 and 7, the macro-F1 score reaches its peak, indicating that the graph connections are optimized in this range, allowing the model to achieve its highest level of class distinction. However, in certain cases, a value of 5 also provides relatively good results. On the other hand, setting k=8 leads to a performance drop across all datasets. This decline may be attributed to an excessive number of graph edges, which blurred the distinction between different data points and introduced noise into the node relationships. As a result, the model may have merged key features with irrelevant data, ultimately reducing its ability to accurately classify instances. These observations underscore the impor-tance of carefully selecting k during graph construction, as it can significantly enhance the model's performance."}, {"title": "7. Conclusion", "content": "With the increasing spread of misinformation and fake news in the digital age, developing efficient and accurate methods for detecting such content has become more critical than ever. This paper introduced a novel model called NOL-GAT to address this challenge. By employing an innovative approach"}]}