{"title": "Multi-Peptide: Multimodality Leveraged\nLanguage-Graph Learning of Peptide Properties", "authors": ["Srivathsan Badrinarayanan", "Chakradhar Guntuboina", "Parisa Mollaei", "Amir Barati Farimani"], "abstract": "Peptides are essential in biological processes and therapeutics. In this study, we in-\ntroduce Multi-Peptide, an innovative approach that combines transformer-based lan-\nguage models with Graph Neural Networks (GNNs) to predict peptide properties. We\ncombine PeptideBERT, a transformer model tailored for peptide property prediction,\nwith a GNN encoder to capture both sequence-based and structural features. By em-\nploying Contrastive Language-Image Pre-training (CLIP), Multi-Peptide aligns embed-\ndings from both modalities into a shared latent space, thereby enhancing the model's\npredictive accuracy. Evaluations on hemolysis and nonfouling datasets demonstrate\nMulti-Peptide's robustness, achieving state-of-the-art 86.185% accuracy in hemolysis\nprediction. This study highlights the potential of multimodal learning in bioinformat-", "sections": [{"title": "Introduction", "content": "Peptides, composed of distinct sequences of amino acid residues, serve as essential com-\nponents in numerous biological processes and applications. 1-3 The unique properties of\npeptides, such as hemolysis and fouling behavior are critical considerations in the devel-\nopment of effective peptide-based therapeutics and biomaterials.4 Hemolysis, characterized\nby the disruption of red blood cells, is particularly significant in the design of peptide-\nbased drugs.5 Peptides with nonfouling attributes demonstrate reduced interactions with\nsurrounding molecules, making them highly desirable for diverse biomedical applications. 6\nThe structural and functional attributes of peptides, shaped by their specific arrangement\nof amino acids and overall length, govern how they interact with biological targets and\ntheir surroundings. 7,8 Understanding this complex interplay is essential for designing cus-\ntomized biomaterials and developing therapeutic strategies. 9\u201311 Traditionally, computational\nmethodologies including quantitative structure-activity relationship (QSAR) models, 12 have\nestablished links between peptide sequences and specific structural properties. However,\nsuch methods face challenges in scalability and computational efficiency, particularly when\ndealing with exponentially expanding sequence repositories. 13 This underscores the urgent\nneed for innovative computational approaches to decipher the intricate properties associ-\nated with protein sequences. In recent years, machine learning techniques have significantly\ntransformed these methodologies and provide advanced capabilities for data analysis and\nprediction. 14\u201318 Machine learning models are uniquely suited to leverage the vast amounts of\nbiological data. The exponential growth in biological data, including protein sequence data\nthrough continuous additions to the Protein Data Bank,19,20 propelled by advancements in\nhigh-throughput sequencing technologies, presents lots of opportunities for predictive mod-"}, {"title": "Methods", "content": "The datasets for hemolysis and nonfouling behavior consist of letter sequences paired with\nlabels indicating whether each sequence corresponds positively or negatively to the respec-\ntive property. 28 The sequences are of various lengths as seen in figure 2. For this study,\nthe sequence data from the referenced dataset is fed into the AlphaFold system to gather\nmore information about the peptide sequence. The output of AlphaFold is a Protein Data\nBank (PDB) containing detailed atomic coordinates, which provides crucial insights into the\nstructural arrangement of proteins.\nComputational techniques are employed to predict hemolytic properties, utilizing the\nDatabase of Antimicrobial Activity and Structure of Peptides (DBAASPv3). 30 Due to exper-\nimental variability, the sequences appear multiple times with different labels in the dataset.\nAfter removing duplicates with both positive and negative labels, we are left with 845 posi-\ntively marked sequences (15.065%) and 4764 negatively marked sequences (84.935%).\nInformation for forecasting resistance against nonspecific interactions (nonfouling) is\ngathered from a study, 31 and employed on a dataset of 3,600 positively marked sequences\nand a dataset of 13,585 negatively marked sequences. Here, positively marked indicates a\nnonfouling protein sequence. Removing 7 sequences which were duplicates having the same\nlabel, and 3 sequences for which AlphaFold failed to generate the corresponding structural\nPDB files, we are left with 3596 positively marked sequences (20.937%) and 13579 negatively\nmarked sequences (79.063%). Negative examples are derived from insoluble and hemolytic\npeptides, along with scrambled negatives (with a length similar to the positive sequences),\nfollowing an approach outlined in a referenced work. 32\nThe datasets were preprocessed via a custom encoding method, where each of the 20\namino acids was represented by its corresponding index in a predefined array. To ensure\ncompatibility with our ensemble, we needed to ensure that the data could be used by both"}, {"title": "Model Architecture", "content": "In this study, we propose an innovative approach that aims to improve the prediction accu-\nracy of a transformer-based language model for the specific peptide properties by introducing\ntraining in an additional modality. Building upon previous work, PeptideBERT, 26 which was\nfine-tuned over the pre-trained ProtBERT33 transformer model to predict the peptide prop-\nerties by taking just the protein sequences as inputs, we enhanced the predictive capabilities\nof the existing framework by introducing another modality of data (in this case, graphical\ndata), to capture additional dependencies. The transformer-based BERT model, 34 which has\nthe attention mechanism at its heart, 35 captures long-range dependencies and global con-\ntext. The additional modality, in this case the graph-based learning through Graph Neural\nNetworks over the protein structure data, augments the transformer model's understanding\nby focusing on local features and structural relationships.\nThe model architecture comprises three key components: a Graph Neural Network\n(GNN), the pretrained language model (PeptideBERT), and the projection heads for project-\ning embeddings into the shared latent space for the CLIP loss matrix computation across the\ntwo modalities. The 11 features corresponding to each atom (extracted from the PDB files)\nare the nodes of the GNN, and the edges represent the relationship between the nodes. The\nGNN module, leveraging PyTorch Geometric's36 SAGEConv layer, conducts graph convolu-\ntion on protein sequence graphs, aggregating information from neighboring nodes iteratively.\nIt is followed by a fully connected neural network incorporating ReLU activation functions"}, {"title": "", "content": "and a sigmoid layer, for the extraction of graph embeddings. On the other hand, Pep-\ntideBERT processes protein sequences and their corresponding attention masks to generate\ncontextual text embeddings through ProtBERT. Additionally, projection heads are used to\nmap the graph and text embeddings into a unified latent space, to implement contrastive\nlearning through CLIP loss. These projection heads consist of linear projection layers, GELU\nactivation, fully connected layers, dropout, and layer normalization. The integration of the\nGNN, PeptideBERT, and projection heads to project embeddings onto a shared latent space\nwithin a single module facilitates the joint learning of structural and textual representations\nfrom protein sequences.\nOur approach leverages pre-training3738 individually for both the PeptideBERT model\nand the GNN model, which will later be used in the ensemble as above. By leveraging pre-\ntrained weights for each model, acquired from training over each of the datasets, our models\ngain the ability to generalize their knowledge to specific tasks. This process of knowledge\ntransfer enables each model to summarize high-level features and correlations relevant to\npeptide and protein properties, leading to better performance and faster convergence during\ntraining. The pre-trained individual models are then combined together in the same latent\nspace, using a variant of the Contrastive Language-Image Pretraining (CLIP) loss 29 to then\nundergo fine-tuning on a targeted task of peptide protein property prediction.\nCLIP, pioneered by OpenAI, extends the capabilities of pre-trained individual models\nby enabling them to understand and reason across different modalities. CLIP operates on\nthe principle of contrastive learning, where the model is trained to associate similar pairs of\ninputs while distinguishing dissimilar pairs. At its core, the CLIP loss function employs a\nsoftmax operation to normalize similarity scores between GNN and PeptideBERT embed-\ndings, modulated by a temperature parameter. These normalized similarity scores serve as\ntargets for the model's predictions, ensuring that the model learns to align embeddings from\nboth modalities in a manner that reflects their semantic and structural correspondence.\nThe loss computation involves calculating cross-entropy loss between predicted logits,"}, {"title": "", "content": "derived from the dot product of PeptideBERT embeddings and transposed GNN embeddings,\nand the target similarity scores. It is important to note that CLIP is performed on the\nembeddings from the baseline ProtBERT model of PeptideBERT, since PeptideBERT is\na fine-tuned model built over ProtBERT. By optimizing over the loss function, the model\nlearns to associate relevant textual descriptions with corresponding graphical information\nand vice versa, enabling accurate classification and retrieval tasks across diverse inputs.\nIn the context of our study, CLIP loss encourages the model to learn meaningful repre-\nsentations by contrasting peptide sequences with their associated binary protein properties.\nFor a particular protein, say p, the graph (g) and text (t) embeddings generated by the GNN\nand PeptideBERT encoders are represented by\n$e_g = \\text{GNN}(\\text{structure}(p))$ and $e_t = \\text{PeptideBERT}(\\text{sequence}(p))$,\nrespectively. Let the similarity between two vectors (in this case the two modalities' em-\nbeddings) x and y be measured and represented through the function sim(x, y), where the\nfunction sim(x, y) is the dot product between the normalized embeddings, reflecting the co-\nsine similarity. The temperature parameter of the analysis is denoted by T, and the overall\nsymmetric loss between the graph and text modalities (denoted by g and t respectively),\nL(g, t) is given by\n$L(g, t) = \\frac{1}{2} [l(g, t) + l(t, g)],$\nwhere\n$l(g, t) = - \\sum_{i=1}^N \\log{\\frac{e^{\\text{sim}(e_{g_i}, e_{t_i})/T}}{\\sum_{j=1}^N e^{\\text{sim}(e_{g_i}, e_{t_j})/T}}}.$\nIn the CLIP algorithm, the embeddings $e_g$ and $e_t$ are normalized, and the similarities are\ncomputed as dot products. The targets are computed as a softmax over the average of the\nsimilarities from both modalities. This results in the above loss, and similarly for l(t,g).\nThe objective of the CLIP loss in this binary classification task is to train a model to"}, {"title": "", "content": "accurately discriminate between two classes (say, positive and negative) by learning repre-\nsentations that effectively capture the semantic and graphical content of the data. Through a\ncontrastive learning framework, CLIP aims to map textual descriptions (in this case, protein\nsequences) and corresponding graphs (protein structure information) to a shared embed-\nding space where similar instances are positioned close together while dissimilar instances\nare pushed farther apart. The CLIP loss encourages the model to assign distinct and dis-\ncriminative representations to positive and negative examples, facilitating better separation\nbetween the two classes.\nIn our study, we backpropagate through the CLIP loss matrix after pre-training each\nmodel (PeptideBERT and GNN) individually over each protein property dataset. We freeze\nthe pre-trained GNN model's weights during backpropagation, allowing the algorithm to\nupdate the PeptideBERT model's weights. After the weights are updated and tuned, the\nPeptideBERT model's weights are used for inference on unseen protein sequences, predicting\na latent space embedding vector for each sequence. By comparing these latent vectors with\nthose of the training samples, the most similar output is chosen as the output for the test\nsequence. Therefore, by leveraging pre-trained individual models to incorporate multimodal-\nity in training using CLIP loss, the PeptideBERT model gains a deeper understanding of\nthe relationships between peptide sequences and protein properties."}, {"title": "Results and Discussion", "content": "The PeptideBERT model and the GNN model were each trained individually on both of\nthe datasets. The specific parameters in each of the model architectures, and the relevant\nhyperparameters used during pre-training are shown in Supporting Information under the\nsection called Model training.\nThe individually pre-trained models were then projected onto the same latent space with\nthe CLIP loss implemented. The CLIP ensemble was trained for 100 epochs with a batch"}, {"title": "", "content": "size of 20. For CLIP training, a learning rate of 6.0e-5 was employed. The learning rate\nscheduler utilized was the Learning Rate on Plateau, used to reduce the learning rate by a\nfactor of 0.4 for a patience of 5 epochs. These parameter settings were meticulously chosen\nand fine-tuned through iterative experimentation to maximize the model's effectiveness in\nthe targeted research task. For each task, a separate model was trained on the corresponding\ndataset. The models were trained using the AdamW optimizer, and the binary cross-entropy\nloss function was employed within the larger CLIP loss. Training was performed on a single\nNVIDIA GeForce RTX 2080Ti GPU with 4 cores and 11GiB of memory in each core.\nOnce the ensemble is trained, the weights from the CLIP matrix are extracted. The\nweights corresponding to the BERT transformer model are then used for inference on each\nof the test datasets. We expect the transformer model to have learned synergistically with\nthe GNN model, capturing dependencies based on the features extracted and correlations\nlearned in coherence. As seen in table 1, the accuracy of the multimodality-leveraged trans-\nformer being on par with a fine-tuned PeptideBERT and other models26,39 demonstrated\nthe advantages of the introducing and leveraging an additional mode of data. It is important\nto note that we have benchmarked the Multi-Peptide model against the individual Peptide-\nBERT and GNN components as well. Multi-Peptide's BERT gives state-of-the-art (SOTA)\nresults for the Hemolysis dataset in particular. The improvement in Multi-Peptide's accu-\nracy for the Hemolysis dataset even though the individual components have lower accuracy\ndemonstrates the capability of the CLIP-implemented ensemble to learn from the presence\nof multiple modalities.\nIn this particular analysis, however, the accuracy of Multi-Peptide's ensemble does not\nsurpass that of the fine-tuned PeptideBERT for the non-fouling dataset. This may be due to\nseveral factors. Introducing a GNN model into the ensemble significantly increases overall\ncomplexity, necessitating more extensive and precise data for effective training compared\nto the sequence-based transformer model. The effectiveness of the GNN relies heavily on\nthe AlphaFold model's capability to generate highly accurate protein structure data. How-"}, {"title": "", "content": "ever, protein structure data can be noisy 40 or less directly predictive of the binary prop-\nerty compared to sequence data. Incorporating noisy data can also degrade overall model\nperformance. Additionally, combining sequence-based features from the transformer with\nstructure-based features from the GNN presents challenges in effectively aligning these dif-\nferent feature types. If the features are not well-aligned or complementary, the ensemble\nstruggles to learn useful representations.\nMoreover, it is crucial to consider that PeptideBERT26 was trained on a heavily imbal-\nanced and augmented dataset. In contrast, Multi-Peptide was trained on a balanced dataset\nachieved through oversampling and relied solely on data from primary sources without ad-\nditional augmentations. While PeptideBERT excels in the specific property prediction task\ndue to its fine-tuning, our study highlights the robustness and capability of Multi-Peptide to\npredict protein properties by integrating both sequence and structural data. Here, it is also\nimportant to note that hyperparameter tuning was not conducted on each dataset, but was\nrather done to improve the overall performance of the Multi-Peptide model on the whole.\nThis demonstrates the potential of Multi-Peptide to provide a more holistic understanding of\nprotein properties, even though its current performance accuracy on the nonfouling dataset\nis slightly lower compared to the specialized fine-tuned PeptideBERT."}, {"title": "Visualization of Representations", "content": "To further explain the performance and capabilities of our models, we employ t-distributed\nStochastic Neighbor Embedding (t-SNE) to visualize the embedding spaces derived from var-\nious methodologies. t-SNE41 is a powerful tool for dimensionality reduction that helps in un-\nderstanding the structure of high-dimensional data by projecting it into a lower-dimensional\nspace. By visualizing these embeddings as seen in figure 3, we aim to provide a qualitative\nanalysis of how well each model captures the underlying patterns and relationships within\nthe peptide sequences. This visualization will help us understand the clustering behavior\nand the degree of separation between different classes, which are crucial for the accuracy and\neffectiveness of the models. In this study, we utilized datasets with a significant class imbal-\nance, with a notably larger number of negatively marked sequences compared to positively\nmarked ones. The t-SNE visualizations for PeptideBERT, GNN, and post-CLIP embeddings\noffer critical insights into how each model handles this imbalance and achieves class separa-\ntion. In particular, this section aims to qualitatively explain the CLIP embedding process\nfor the non-fouling dataset to understand if CLIP enhances the separation of classes, even\nthough the test accuracy seems to be slightly lower than that of the fine-tuned PeptideBERT\nmodel.\nThe t-SNE plot of PeptideBERT embeddings revealed a central cluster of negatively\nmarked sequences, surrounded by smaller, dispersed groups containing both negatively and\npositively marked sequences. This indicated that the PeptideBERT model captures seman-\ntic similarities within each class but also shows overlap between classes, highlighting the\nchallenge of clear class separation with the BERT architecture alone.\nContrarily, the t-SNE plot of GNN embeddings demonstrated a more segregated distribu-\ntion. A significant pattern of negatively marked sequences occupied the majority of the plot,\nwith positives mirroring a similar pattern, though with some segregation. This segregation\nhighlighted the GNN's potential to encode structural relationships within peptide sequences,\nleading to potential class-specific clusters after CLIP."}, {"title": "", "content": "Following Contrastive Language-Image Pre-training (CLIP), the t-SNE plots showed a\nprominent central cluster of negatively marked sequences, surrounded by smaller, more con-\ncentrated patches of mixed classes. These mixed patches appeared closer smaller and more\ntight knit compared to the PeptideBERT embeddings, suggesting that CLIP enhanced the\ndiscrimination between classes in the reduced-dimensional space. This is more prominent in\nthe 3D t-SNE plot following the CLIP implementation, where we can observe some separa-\ntion within the smaller mixed clusters in the 3D space as well. This improvement in class\nseparation highlights the efficacy of CLIP in capturing distinct class patterns and mitigating\nthe impact of class imbalance.\nThe t-SNE visualizations provide valuable insights into the strengths and limitations of\neach embedding technique for peptide sequence classification. Through the embeddings, we"}, {"title": "", "content": "observe that PeptideBERT demonstrated semantic understanding, GNN captured structural\nrelationships, and the CLIP process enabled improved class discrimination. These visual-\nizations underscore the potential of multimodal pre-training approaches such as the one\nproposed in this paper."}, {"title": "Conclusion", "content": "In this study, we introduced Multi-Peptide, a novel approach that leverages multimodality in\nmachine learning to enhance the prediction accuracy of peptide properties. By integrating a\nGraph Neural Network (GNN) with the transformer-based PeptideBERT model, we aimed\nto capture both the sequence-based and structural features of peptides. Our approach uti-\nlizes a variant of Contrastive Language-Image Pre-training (CLIP) to align the embeddings\nfrom these two modalities into a shared latent space, thereby facilitating more robust and\ncomprehensive representations.\nThe results from our experiments demonstrate the potential of Multi-Peptide in advanc-\ning peptide property prediction. Integration of GNN and transformer-based models allows\nus to capture a broader range of features. Although the accuracy of Multi-Peptide was\ncomparable to that of the fine-tuned PeptideBERT, this highlights the robustness of our\napproach in handling complex data structures and extracting meaningful features from both\nsequence and structural information. Our t-SNE visualizations, especially the post-CLIP\nembeddings provided valuable insights, showing enhanced class discrimination, highlighting\nthe efficacy of multimodal pre-training in mitigating class imbalance and improving classi-\nfication performance, and underscoring the strength of our approach in providing a more\nholistic understanding of protein properties.\nIn conclusion, Multi-Peptide represents a significant step forward in leveraging multi-\nmodality for peptide property prediction. Our approach holds promise for enhancing the\naccuracy and robustness of predictive models, offering a deeper understanding of peptide"}, {"title": "", "content": "characteristics. Future work will focus on refining the integration of modalities, and further\noptimizing the model architecture to fully harness the complementary strengths of sequence-\nbased and structural features. This study underscores the importance and potential of\nmultimodal learning in advancing the field of bioinformatics, through Multi-Peptide's ability\nto integrate sequence and structural data."}, {"title": "Data and Software Availability", "content": "The necessary code and data used in this study can be accessed here: https://github.\ncom/srivathsanb14/MultiPeptide"}, {"title": "Supporting Information", "content": "Table 2 shows the parameters associated with the individual PeptideBERT and GNN model\narchitectures. These models were used for individual pre-training, and the respective hyper-\nparameters used as also listed in the same table. These hyperparameters were meticulously\nchosen after thorough analysis. This step of pre-training was done for 50 epochs each, with\na batch size of 20. The projection dimension of the GNN was made to be 2048 to enhance\nmodel projection and thereby capture more information. The GNN had 11 input features,\nas listed before. The PeptideBERT configuration included a hidden size of 256, 8 hidden\nlayers, 8 attention heads, and a dropout rate of 0.10, with a vocabulary size of 25.\nThe hyperparameters associated with the CLIP process have been listed earlier. Table 3\nshows the CLIP training time for each dataset. It is important to note that this process is\ncarried out after the individual pre-training step. The table showcases the time complexity\nof the CLIP process by showing the training time for each dataset."}, {"title": "Reproducibility and other comments", "content": "In order to confirm the reproducibility of our results, the CLIP training process was con-\nducted multiple times. This training process was repeated, keeping the same set of hyperpa-\nrameters as earlier. Table 4 showcases the accuracy recorded at the inference stage after each\ntraining run, demonstrating that the model is robust and only susceptible to the stochastic\nnature of the training and inference processes.\nOther tests like ablation studies are not conducted as the primary objective of this study\nis to demonstrate the efficacy of the Multi-Peptide approach rather than to analyze the\ncontribution of each individual component. The individual components of each model are\nwell-studied, with the accuracy of the PeptideBERT model documented in existing literature\ntoo. Since our study entirely focuses on the synergistic understanding between the Peptide-\nBERT and GNN models, it would be futile to mask individual components for ablation"}]}