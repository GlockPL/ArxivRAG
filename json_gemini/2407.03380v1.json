{"title": "Multi-Peptide: Multimodality Leveraged\nLanguage-Graph Learning of Peptide Properties", "authors": ["Srivathsan Badrinarayanan", "Chakradhar Guntuboina", "Parisa Mollaei", "Amir Barati Farimani"], "abstract": "Peptides are essential in biological processes and therapeutics. In this study, we in-\ntroduce Multi-Peptide, an innovative approach that combines transformer-based lan-\nguage models with Graph Neural Networks (GNNs) to predict peptide properties. We\ncombine PeptideBERT, a transformer model tailored for peptide property prediction,\nwith a GNN encoder to capture both sequence-based and structural features. By em-\nploying Contrastive Language-Image Pre-training (CLIP), Multi-Peptide aligns embed-\ndings from both modalities into a shared latent space, thereby enhancing the model's\npredictive accuracy. Evaluations on hemolysis and nonfouling datasets demonstrate\nMulti-Peptide's robustness, achieving state-of-the-art 86.185% accuracy in hemolysis\nprediction. This study highlights the potential of multimodal learning in bioinformat-", "sections": [{"title": "Introduction", "content": "Peptides, composed of distinct sequences of amino acid residues, serve as essential com-\nponents in numerous biological processes and applications. 1-3 The unique properties of\npeptides, such as hemolysis and fouling behavior are critical considerations in the devel-\nopment of effective peptide-based therapeutics and biomaterials.4 Hemolysis, characterized\nby the disruption of red blood cells, is particularly significant in the design of peptide-\nbased drugs.5 Peptides with nonfouling attributes demonstrate reduced interactions with\nsurrounding molecules, making them highly desirable for diverse biomedical applications. 6\nThe structural and functional attributes of peptides, shaped by their specific arrangement\nof amino acids and overall length, govern how they interact with biological targets and\ntheir surroundings. 7,8 Understanding this complex interplay is essential for designing cus-\ntomized biomaterials and developing therapeutic strategies. 9\u201311 Traditionally, computational\nmethodologies including quantitative structure-activity relationship (QSAR) models, 12 have\nestablished links between peptide sequences and specific structural properties. However,\nsuch methods face challenges in scalability and computational efficiency, particularly when\ndealing with exponentially expanding sequence repositories. 13 This underscores the urgent\nneed for innovative computational approaches to decipher the intricate properties associ-\nated with protein sequences. In recent years, machine learning techniques have significantly\ntransformed these methodologies and provide advanced capabilities for data analysis and\nprediction. 14\u201318 Machine learning models are uniquely suited to leverage the vast amounts of\nbiological data. The exponential growth in biological data, including protein sequence data\nthrough continuous additions to the Protein Data Bank,19,20 propelled by advancements in\nhigh-throughput sequencing technologies, presents lots of opportunities for predictive mod-"}, {"title": "Methods", "content": "The datasets for hemolysis and nonfouling behavior consist of letter sequences paired with\nlabels indicating whether each sequence corresponds positively or negatively to the respec-\ntive property. 28 The sequences are of various lengths as seen in figure 2. For this study,\nthe sequence data from the referenced dataset is fed into the AlphaFold system to gather\nmore information about the peptide sequence. The output of AlphaFold is a Protein Data\nBank (PDB) containing detailed atomic coordinates, which provides crucial insights into the\nstructural arrangement of proteins.\nComputational techniques are employed to predict hemolytic properties, utilizing the\nDatabase of Antimicrobial Activity and Structure of Peptides (DBAASPv3). 30 Due to exper-\nimental variability, the sequences appear multiple times with different labels in the dataset.\nAfter removing duplicates with both positive and negative labels, we are left with 845 posi-\ntively marked sequences (15.065%) and 4764 negatively marked sequences (84.935%).\nInformation for forecasting resistance against nonspecific interactions (nonfouling) is\ngathered from a study, 31 and employed on a dataset of 3,600 positively marked sequences\nand a dataset of 13,585 negatively marked sequences. Here, positively marked indicates a\nnonfouling protein sequence. Removing 7 sequences which were duplicates having the same\nlabel, and 3 sequences for which AlphaFold failed to generate the corresponding structural\nPDB files, we are left with 3596 positively marked sequences (20.937%) and 13579 negatively\nmarked sequences (79.063%). Negative examples are derived from insoluble and hemolytic\npeptides, along with scrambled negatives (with a length similar to the positive sequences),\nfollowing an approach outlined in a referenced work. 32\nThe datasets were preprocessed via a custom encoding method, where each of the 20\namino acids was represented by its corresponding index in a predefined array. To ensure\ncompatibility with our ensemble, we needed to ensure that the data could be used by both"}, {"title": "Model Architecture", "content": "In this study, we propose an innovative approach that aims to improve the prediction accu-\nracy of a transformer-based language model for the specific peptide properties by introducing\ntraining in an additional modality. Building upon previous work, PeptideBERT, 26 which was\nfine-tuned over the pre-trained ProtBERT33 transformer model to predict the peptide prop-\\perties by taking just the protein sequences as inputs, we enhanced the predictive capabilities\nof the existing framework by introducing another modality of data (in this case, graphical\ndata), to capture additional dependencies. The transformer-based BERT model, 34 which has\nthe attention mechanism at its heart, 35 captures long-range dependencies and global con-\ntext. The additional modality, in this case the graph-based learning through Graph Neural\nNetworks over the protein structure data, augments the transformer model's understanding\nby focusing on local features and structural relationships.\nThe model architecture comprises three key components: a Graph Neural Network\n(GNN), the pretrained language model (PeptideBERT), and the projection heads for project-\ning embeddings into the shared latent space for the CLIP loss matrix computation across the\ntwo modalities. The 11 features corresponding to each atom (extracted from the PDB files)\nare the nodes of the GNN, and the edges represent the relationship between the nodes. The\nGNN module, leveraging PyTorch Geometric's36 SAGEConv layer, conducts graph convolu-\ntion on protein sequence graphs, aggregating information from neighboring nodes iteratively.\nIt is followed by a fully connected neural network incorporating ReLU activation functions"}, {"title": "Results and Discussion", "content": "The PeptideBERT model and the GNN model were each trained individually on both of\nthe datasets. The specific parameters in each of the model architectures, and the relevant\nhyperparameters used during pre-training are shown in Supporting Information under the\nsection called Model training.\nThe individually pre-trained models were then projected onto the same latent space with\nthe CLIP loss implemented. The CLIP ensemble was trained for 100 epochs with a batch"}, {"title": "Visualization of Representations", "content": "To further explain the performance and capabilities of our models, we employ t-distributed\nStochastic Neighbor Embedding (t-SNE) to visualize the embedding spaces derived from var-\nious methodologies. t-SNE41 is a powerful tool for dimensionality reduction that helps in un-\nderstanding the structure of high-dimensional data by projecting it into a lower-dimensional\nspace. By visualizing these embeddings as seen in figure 3, we aim to provide a qualitative\nanalysis of how well each model captures the underlying patterns and relationships within\nthe peptide sequences. This visualization will help us understand the clustering behavior\nand the degree of separation between different classes, which are crucial for the accuracy and\neffectiveness of the models. In this study, we utilized datasets with a significant class imbal-\nance, with a notably larger number of negatively marked sequences compared to positively\nmarked ones. The t-SNE visualizations for PeptideBERT, GNN, and post-CLIP embeddings\noffer critical insights into how each model handles this imbalance and achieves class separa-\ntion. In particular, this section aims to qualitatively explain the CLIP embedding process\nfor the non-fouling dataset to understand if CLIP enhances the separation of classes, even\nthough the test accuracy seems to be slightly lower than that of the fine-tuned PeptideBERT\nmodel.\nThe t-SNE plot of PeptideBERT embeddings revealed a central cluster of negatively\nmarked sequences, surrounded by smaller, dispersed groups containing both negatively and\npositively marked sequences. This indicated that the PeptideBERT model captures seman-\ntic similarities within each class but also shows overlap between classes, highlighting the\nchallenge of clear class separation with the BERT architecture alone.\nContrarily, the t-SNE plot of GNN embeddings demonstrated a more segregated distribu-\ntion. A significant pattern of negatively marked sequences occupied the majority of the plot,\nwith positives mirroring a similar pattern, though with some segregation. This segregation\nhighlighted the GNN's potential to encode structural relationships within peptide sequences,\nleading to potential class-specific clusters after CLIP."}, {"title": "Conclusion", "content": "In this study, we introduced Multi-Peptide, a novel approach that leverages multimodality in\nmachine learning to enhance the prediction accuracy of peptide properties. By integrating a\nGraph Neural Network (GNN) with the transformer-based PeptideBERT model, we aimed\nto capture both the sequence-based and structural features of peptides. Our approach uti-\nlizes a variant of Contrastive Language-Image Pre-training (CLIP) to align the embeddings\nfrom these two modalities into a shared latent space, thereby facilitating more robust and\ncomprehensive representations.\nThe results from our experiments demonstrate the potential of Multi-Peptide in advanc-\ning peptide property prediction. Integration of GNN and transformer-based models allows\nus to capture a broader range of features. Although the accuracy of Multi-Peptide was\ncomparable to that of the fine-tuned PeptideBERT, this highlights the robustness of our\napproach in handling complex data structures and extracting meaningful features from both\nsequence and structural information. Our t-SNE visualizations, especially the post-CLIP\nembeddings provided valuable insights, showing enhanced class discrimination, highlighting\nthe efficacy of multimodal pre-training in mitigating class imbalance and improving classi-\nfication performance, and underscoring the strength of our approach in providing a more\nholistic understanding of protein properties.\nIn conclusion, Multi-Peptide represents a significant step forward in leveraging multi-\nmodality for peptide property prediction. Our approach holds promise for enhancing the\naccuracy and robustness of predictive models, offering a deeper understanding of peptide"}, {"title": "Data and Software Availability", "content": "The necessary code and data used in this study can be accessed here: https://github.\ncom/srivathsanb14/MultiPeptide"}, {"title": "Supporting Information", "content": null}, {"title": "Model training", "content": "Table 2 shows the parameters associated with the individual PeptideBERT and GNN model\narchitectures. These models were used for individual pre-training, and the respective hyper-\nparameters used as also listed in the same table. These hyperparameters were meticulously\nchosen after thorough analysis. This step of pre-training was done for 50 epochs each, with\na batch size of 20. The projection dimension of the GNN was made to be 2048 to enhance\nmodel projection and thereby capture more information. The GNN had 11 input features,\nas listed before. The PeptideBERT configuration included a hidden size of 256, 8 hidden\nlayers, 8 attention heads, and a dropout rate of 0.10, with a vocabulary size of 25.\nThe hyperparameters associated with the CLIP process have been listed earlier. Table 3\nshows the CLIP training time for each dataset. It is important to note that this process is\ncarried out after the individual pre-training step. The table showcases the time complexity\nof the CLIP process by showing the training time for each dataset."}]}