{"title": "IMAGINE: An 8-to-1b 22nm FD-SOI Compute-In-Memory CNN Accelerator With an End-to-End Analog Charge-Based 0.15-8POPS/W Macro Featuring Distribution-Aware Data Reshaping", "authors": ["Adrian Kneip", "Martin Lefebvre", "Pol Maistriaux", "David Bol"], "abstract": "Charge-domain compute-in-memory (CIM) SRAMS have recently become an enticing compromise between computing efficiency and accuracy to process sub-8b convolutional neural networks (CNNs) at the edge. Yet, they commonly make use of a fixed dot-product (DP) voltage swing, which leads to a loss in effective ADC bits due to data-dependent clipping or truncation effects that waste precious conversion energy and computing accuracy. To overcome this, we present IMAGINE, a workload-adaptive 1-to-8b CIM-CNN accelerator in 22nm FD-SOI. It introduces a 1152\u00d7256 end-to-end charge-based macro with a multi-bit DP based on an input-serial, weight-parallel accumulation that avoids power-hungry DACs. An adaptive swing is achieved by combining a channel-wise DP array split with a linear in-ADC implementation of analog batch-normalization (ABN), obtaining a distribution-aware data reshaping. Critical design constraints are relaxed by including the post-silicon equivalent noise within a CIM-aware CNN training framework. Measurement results showcase an 8b system-level energy efficiency of 40TOPS/W at 0.3/0.6V, with competitive accuracies on MNIST and CIFAR-10. Moreover, the peak energy and area efficiencies of the 187kB/mm\u00b2 macro respectively reach up to 0.15-8POPS/W and 2.6-154TOPS/mm\u00b2, scaling with the 8-to-1b computing precision. These results exceed previous charge-based designs by 3-to-5x while being the first work to provide linear in-memory rescaling.", "sections": [{"title": "I. INTRODUCTION", "content": "As of today, the fast-growing deployment of ever-more complex AI tasks in embedded systems has pushed conventional edge devices to their limit. Targeting a variety of biomedical, industrial or environmental applications, powerful AI algorithms such as convolutional neural networks (CNNs) require dedicated edge nodes that provide extreme levels of energy efficiency in order to save battery lifetime and avoid frequent replacement [1], [2]. In that regard, compute-in-memory (CIM) architectures [3], [4] have rapidly gained attention for their ability to efficiently perform massively-parallel dot-product (DP) operations while bypassing the von-Neumann bottleneck, making them suitable candidates to accelerate CNNs at the edge, as depicted in Fig. 1(a). Nowadays, the current landscape of CIM implementations undergoes different trade-offs between computing efficiency and accuracy. On the one hand, analog CIM-SRAMs based on current-domain DP operators yield outstanding computing efficiency [6], [7], but are also highly sensitive to analog impairments such as nonlinearity, process variations and transistors mismatch, which hinder their computing accuracy [3], [8]. While part of these non-idealities can be compensated during the off-line training of the CNN [7], their actual computing precision usually remains below 4b, restricting their applicative scope. On the other hand, digital CIM macros can support high-precision targets with near-golden accuracy, but waste efficiency due to the overhead of distributed adder trees [9], [10] compared to their analog counterpart. In between, analog charge-based CIM-SRAMS offer an interesting compromise, relying on variation-insensitive metal-oxyde-metal (MoM) capacitances to perform analog DPs with a moderate area overhead [11], [12]. With recent progress in quantization-aware training relaxing the precision requirements below 8b for many edge applications [13], [14], charge-based architectures emerge as appealing candidates for the deployment of versatile CNN workloads at the edge.\nNonetheless, charge-based architectures come with their own challenges, qualitatively illustrated in Fig. 1(b). First, previous works that rely on charge-injection [15], [16] utilize a fixed analog DP voltage swing, bound to the maximum array size. Yet, this approach reduces the effective number of available ADC bits when mapping small- or medium-sized CNN layers that do not require full macro utilization. Furthermore, combining a conventional full-scale ADC with such a fixed voltage swing introduces either clipping or truncation of the ADC output depending on the inbound DP data distribution. Altogether, these issues lead to wasted ADC area and energy, as well as to a loss of information that cannot be recovered by post-ADC rescaling. Finally, supporting a scalable 1-to-8b computing precision puts harsh constraints on DAC and ADC designs, consuming a significant fraction of the total energy and area of the macro and jeopardizing its flexibility towards small workloads. Although fully-serial implementations have been proposed to ease on that side [17], they suffer from a costly 8b ADC conversion per input precision bit.\nIn this work, we present IMAGINE, a massively-parallel CIM-CNN accelerator embedding a 1-to-8b In-Memory-computing SRAM with end-to-end Analog charGe-domain computIng and distributioN-aware data rEshaping. The featured macro combines a channel-wise swing-adaptive DP operator with an in-ADC multi-bit analog batch-normalization (ABN) function. Flexible bit-precision scaling is enabled by a novel input-serial, weight-parallel post-DP accumulation scheme. The CIM-SRAM macro is co-designed with hardware-aware CNN training to provide resilience against residual nonlinearity and variability, enabling approximate LSB computing at the 8b precision to relax the ADC design constraints. IMAGINE has been implemented in 22nm FD-SOI within the CERBERUS chip, with measurement results showcasing peak 8b energy efficiencies of 150 and 40TOPS/W for the standalone macro and the entire accelerator, respectively. The CIM-SRAM also reaches competitive throughput with a high 187kB/mm\u00b2 density, improving overall performance metrics over the state of the art, while being the first to propose a linear in-memory gain rescaling.\nThe rest of this paper is outlined as follows. Section II zooms in on the working principle and related challenges of configurable charge-based CIM designs. Then, Section III presents the CIM-SRAM macro architecture, while Section IV covers the accelerator's digital dataflow. Finally, Section V discusses measurement results."}, {"title": "II. BASICS AND CHALLENGES OF CHARGE-BASED CIM", "content": "In order to deal with the high inherent nonlinearity and variability of current-domain architectures, charge-based designs leverage variation-insensitive MoM capacitors to complete the analog DP operation with high accuracy. A typical architecture of charge-based CIM-SRAM relying on capacitive coupling is depicted in Fig. 2(a), focusing on the array of DP operators. After the DAC conversion, non-zero inputs X are propagated horizontally on the differential input lines DP-IN and DP-INb. Then, an analog XNOR operation takes place within each 10T1C bitcell, as represented in Fig. 2(b): depending on the value of the stored binary weight W, acting as a +1/-1 factor, the analog XNOR outputs of each cell are accumulated on their shared dot-product line (DPL) by charge injection through the coupling MoM capacitance Cc, such that\n$V_{DPL,j} = V_{DD/2}+ \\alpha \\sum_{i=0}^{N_{on-1}} V_{in,i} W_{i,j}$\nwhere\n$\\Delta V_{in,i} = \\frac{1}{2r_{in}} \\sum_{k=0}^{r_{in}-1} \\sum_{l=0}^{L} (-1)^{1-X[k]} 2^{k} V_{DD}.$\n$r_{in}$ is the bitwise input precision, $N_{on}$ the number of activated rows and \u03b1 the charge-injection attenuation factor given by \u03b1 = 1/Nrows, ignoring parasitics and other DPL loads. While Eq. (1) demonstrates an excellent linearity, it also points out that covering the full $V_{DD}$ voltage swing of the DPL is only possible at maximum array utilization, and decreases linearly with $N_{on}$.\nThe inability to rescale the DPL swing penalizes the mapping of layers with narrow DP distributions, as well as that of layers not utilizing the entire input span (e.g., early layers in CNNs), as they fail to fully utilize the available ADC dynamic range. Taking an example in Fig. 3(a), a CNN layer with a zero-centred DP distribution that uses all (resp. 1/4th) of the CIM-SRAM inputs sees an effective ADC precision reduced by 2b (resp. 3b). This can result in a significant accuracy loss due to the network's inability to learn proper scaling factors, as observed in [7] for ADCs below 6b. Moreover, this dynamic"}, {"title": "III. PROPOSED CIM-SRAM ARCHITECTURE", "content": "The proposed IMAGINE accelerator supports the 1-to-8b mapping of CNN layers with various dimensions, thereby offering flexibility on top of high efficiency. This CIM-CNN accelerator is embedded within the 22nm FD-SOI CERBERUS micro-controller unit (MCU), depicted in Fig. 4(a). Detailed in Fig. 4(b), the accelerator features a highly-parallel datapath inspired from [7] with 2\u00d732kB local memory (LMEM) and im2col acceleration. IMAGINE's configurable datapath enables 128b end-to-end data transfers in a pipelined manner, providing precision- and size-dependent routing between the data LMEMs and the charge-domain CIM-SRAM macro, which contains the model's weights. The dataflow of the entire accelerator is further discussed in Section IV. Beforehand, let us first focus below on the CIM-SRAM architecture.\nA. Overall Macro Architecture\nFig. 5(a) showcases the top-level block diagram of the proposed CIM-SRAM macro, supporting 1-to-8b I/O CIM data and a conventional SRAM read/write (R/W) interface to store weights and biases (not shown here for convenience). The macro uses low and high voltage levels VDDL and VDDH, respectively with nominal values of 0.4V and 0.8V. The analog core of the macro spans from the 1152\u00d7256 DP array to the distribution-shaping charge-injection (DSCI) ADCs that implement the ABN function. Interestingly, both units involve the same 10T1C bitcell structure shown in Fig. 2(b). Between both ends, the multi-bit input-and-weight (MBIW) units carry out a bitwise sequential input accumulation before applying the weight bits by means of a summation across adjacent columns. Therefore, the analog core is split into 64 blocks of four columns each, mapping 1-to-4b weight bits as needed and plainly expendable to 8b. Importantly, Fig. 2(b) underlines that each colunm-wise analog operation occurs on the same DPL without discontinuity, from the DP computation down to the ADC conversion, relying on process-robust charge-domain operations along the whole way. Transmission gates progressively disconnect parts of the DPL as they become irrelevant, successively reducing the capacitive load seen during the DP, MBIW and ADC stages.\nThe macro's overall flow of operations can be divided into four main steps, qualitatively represented in Fig. 5(c), linked to the analog core structure. Assuming an 8b input precision rin, unsigned input data are first broadcast along the enabled DP-IN(b) lines in parallel, performing charge-based DP operations as per Eq. (1) one bit at a time. This process starts with LSB input bits and is repeated rin times, with each subsequent DP operation separated by an accumulation of the"}, {"title": "B. Swing-Adaptive Charge-based DP Operator", "content": "Within each column of the DP array, the total 1152 bitcell rows can be divided into 32 DP units containing 36 cells each. In this way, each unit can map a filter of 2D-convolutional layers with a kernel size of 3\u00d73 and a minimum input channel depth Cin of size 4. However, compared to the ideal operator in Eq. (1), the presence of significant load capacitances on the DPL compresses the maximum DPL swing, replacing \u03b1 by an actual attenuation factor \u03b1eff, given by\n$\\alpha_{eff} = C_c/(N_{dp} C_c + C_p + C_L)$,\nwhere $N_{dp}$ = Nrows in baseline designs, Cp models the parasitics due to metal routing, and CL is the total load capacitance related to non-DP blocks connected to the DPL. While CL is dominated by the ADC input capacitance, its total value can be brought down to 40fF by adapting the ADC architecture, as later described in Section III.D. To further improve \u03b1eff, one should thus try to make Ndp and Cp functions of the input channel depth Cin so as to match the CNN layer size. To that end, we present two ways to split the DPL in Fig. 6(a): a parallel-split DPL with a Cin-dependent amount of local DPLs connected to a shared global DPL through switches, and a serial-split solution directly separating the sub-units with switches on the main DPL. Fig. 6(b) demonstrates the DPL swing improvement using both techniques compared to the baseline case, vastly restoring the effective number of ADC bits at low Cin. Still, the maximum DPL swing remains limited by the significant DPL load CL. This point is considered during CNN training to compensate the swing reduction by increasing the ABN gain during the DSCI-ADC stage. Furthermore, parallel-DPL suffers from additional parasitics $C_{p,glob}$ associated with the global DPL routing, which results in a lower swing improvement compared to a serial-split DPL. Finally, Fig. 6(c) highlights the DP energy reduction with such a DPL division as a function of the number of activated 3\u00d73 channel rows, for various $C_L$ loads and Cin configurations (i.e., number of connected DP units). The savings reach up to 72% with 64 channels and a 40fF load, but rapidly diminish with a higher load as the total capacitance seen by the DP-IN driver rises. This further underlines the interest of optimizing the total DPL load.\nThe 10T1C bitcell's layout is drawn in Fig. 7 and achieves an area of 0.44\u00b5m\u00b2, corresponding to four times that of a 6T-pushed rule bitcell in the same technology. With bottom metal layers M1-3 used for the bitcell data, control and power routing, a custom MoM capacitor is layouted atop the cell in M5-6-7 layers, with the M6 one propagated vertically. The M4 layer remains mostly unused to avoid any significant coupling between the MoM and the internal bitcell nodes. The resulting Cc capacitance reaches 0.7fF, generating a 2.4mV kBT/Cc noise from the active bitcell transistors. This noise"}, {"title": "C. Multi-Bit Input-and-Weight Accumulation", "content": "The overall architecture of one of the 64 MBIW units is depicted in Fig. 9(a) across a block of four adjacent columns. Each column possesses its own accumulation capacitance Cacc, layouted within the vertical 10T1C pitch and sized to equal the remaining DPL load. Hence, $C_{acc} = C_{mb} + C_{adc}$, with Cmb and Cade respectively the total DPL loads associated with the MBIW and ADC blocks. The MBIW operates along four phases described in Figs. 9(b) and (c), all relying on capacitive charge-sharing. Phases 1 and 2 correspond to the accumulation of input bit precisions by repeating the DP operation rin times. During the DP phase, the DPL voltage is sampled on the total MBIW and ADC load capacitance, while the accumulation capacitance is disconnected from it, storing the previous accumulation voltage Vacc,k-1. Then, the DP array is disconnected, while the accumulation capacitance is shared with the DPL load. Before performing the next DP computation, the MBIW accumulation node is disconnected from that load and the DPL is reset to VDDL. Critically, the signals that respectively control connections to the DP array (CSDP) and accumulation node (ACCin) must not overlap to avoid any corruption of the stored Vacc. The DPL voltage resulting from these rin cycles is given by\n$V_{DPL} = V_{DDL} + \\sum_{k=0}^{r_{in}-1} (\\alpha_{mb}^{r_{in}-1+k}) + \\alpha_{eff} \\sum_{i=0}^{N_{on-1}} X_i [k] W_{i,j}),$\nwhere \u03b1mb = (Cmb + Cadc)/(Cacc + Cmb + Cadc) ~ 1/2 is the multi-bit attenuation factor. In case of binary inputs, the accumulation phase is bypassed altogether, preserving the voltage swing seen at DP time. Once the input accumulation is finished, weight accumulation takes place during phases 3 and 4. The LSB weight is first self-weighted by sharing its DPL with the VDDL-precharged accumulation node. Then, inter-column charge sharing takes place, successively sharing DPLs"}, {"title": "D. Distribution-Shaping Charge-Injection ADC", "content": "At the end of the bitwise accumulation, the MBIW unit is disconnected from the DPL and column-wise ADCs with a 1-to-8b precision rout transform the analog DP result stored on the DPL into digital outputs Dout. As seen in Fig. 11(a), these outputs are stored within custom 8b asynchronous DFF registers. Relying on separated control signals for their master and slave latches allows to simultaneously write new ADC outputs to the first latch while preserving previous values in the second latch. Consequently, data available at the CIM output are only updated at the next positive clock edge by a CSout pulse, which enables system-level pipelining as later discussed in Section IV. Finally, the necessary ADC inputs are generated by a gain-adaptive unit based on process- and variation-insensitive reference voltages VBN,P, generated by an on-chip calibrated reference [19].\nDetailed in Fig. 11(c), the DSCI ADCs perform data conversion in the charge domain, similar to the rest of the analog core. On top of providing in-situ storage ability, 10T1C bitcells form a binary-weighted capacitive array that mimics a charge-injection DAC, where the DPL holds the residual voltage in a SAR-like conversion. Moreover, the ADC performs distribution shaping by implementing ABN offset and gain stages. Hence, the overall ADC is divided in three sub-blocks: (i) a 5b ABN offset unit achieving a +/-30mV offset range on the DPL, (ii) a 7b calibration unit to counteract the sense amplifier (SA) offset, and (iii) an 8b SAR array embedding the ABN gain function. As such, the ADC's 10T1C bitcells not only hold the pre-computed offset and calibration data, but also duplicate the SAR output code storage in-situ. This redundancy avoids to face extreme routing congestion between the 8b ADC registers and the SAR capacitive bank within the tight column pitch.\nOn top of exploiting bitcells, the ADC introduces a voltage-split charge-injection DAC topology, seen in 11(c), that concurrently reduces the total ADC load and enables ABN scaling. Here, relying on a conventional capacitance-split DAC [20] to shrink the large 8b load $C_{adc} = 128 \u00d7 Cc$ is prohibited by the floating state of the DPL, which is sensitive to kickback coupling. Instead, the SAR array is split into an MSB part with binary-weighted capacitance values, similar to a conventional DAC, and an LSB part with unit capacitance but linearly-downscaled input swing on the S-IN(b) lines. In this design, the LSB array uses two bitcells to reduce the ADC load by more than 70%, while only requiring two additional input levels and bringing a negligible amount of additional nonlinearity. This technique is also used in the offset and calibration blocks, improving their maximum swing on the DPL by minimizing the load overhead. As a result, ADCS account for less than 5% of the total CIM-SRAM area, with a total load of 40fF per column, including parasitics, enabling the high energy savings predicted in Fig. 6(c). Furthermore, changing the maximum voltage swing of the SAR inputs also offers the opportunity to perform global ABN scaling without the need for an explicit gain stage. Indeed, applying the invert gain factor 1/\u03b3 to all S-IN(b) lines compresses the dynamic range of the ADC, working as a zoom effect equivalent to an explicit scaling of the DPL voltage distribution, as depicted in Fig. 11(c). To that end, the reference unit detailed in Fig. 11(b) adapts the S-IN(b) levels feeding the MSB and LSB split DAC arrays based on the \u03b3 configuration. These levels are directly selected from a double-sided resistive ladder, activated during"}, {"title": "E. Mismatch and Low-Frequency Noise Calibration", "content": "The ADC calibration occurs on a rare basis to refresh the combined compensation of the SA mismatch and the low-frequency noise affecting the DPL. During calibration, $V_{DPL,0}$ is precharged to VDDL while decision and update phases happen similarly to the conversion phase, but applied to the calibration unit instead of the SAR. In this way, the calibration code converges towards matching the voltage offset seen at the input of each SA, compensating for it during CIM computations as seen in Fig. 13. This offset is namely dominated by transistor mismatch within the SA, which implements the low-kickback StrongArm topology with minimum-length input differential pair shown in Fig. 14(a). These techniques minimize the kickback level on the floating DPL undergone during each SA decision, bringing it below 0.03mV. However, the minimum-length devices further degrade the robustness to mismatch. Therefore, the 7b calibration unit adopts a 4 \u00d7 Cc MSB capacitance, covering the 3-0 60mV pre-layout SA offset observed in Fig. 14(b). As such, the calibration process offers a resolution of 0.47mV, making the SAR resilient to any low-frequency SA offset for \u03b3 < 8. Yet, post-layout effects and resizing constraints to fit the column pitch lead to a steep 75% increase of the pre-layout deviation, such that only a 2-\u03c3 offset range remains fully handled by the calibration block. With no design-time compensation, out-of-bond offsets might lead to a few dysfunctional columns per macro, as seen in Fig. 14(c). If identified, extreme offsets can be partly dealt with by the ABN offset block, reducing the tunable offset range for that column."}, {"title": "IV. CIM-CNN ACCELERATOR DATAFLOW", "content": "IMAGINE embeds the CIM-SRAM macro within a 1-to-8b highly parallel datapath to provide layer-by-layer execution of CNNs. The digital datapath surrounding the macro is based on [7] and represented in Fig. 15(a): it operates with 128b I/O transfers between the CIM-SRAM and LMEMs regardless of the configured bit precision and number of channels. The accelerator operates along four pipelined stages: (i) data fetching from the input LMEM, where data are encoded in a precision-first, channel-second and kernel-last format (ii) optional im2col transform for convolutional layers, rearranging the input data in a channel-last format suiting the CIM-SRAM's input shift-register, and applying zero-padding as requested, (iii) CIM computation as described in Section III, and finally (iv) CIM output storage to the output LMEM, in the same kernel-last format. Such format not only minimizes the number of LMEM accesses, but also enables direct data reuse for the next layer, after switching input and output LMEMS in a ping-pong manner. In that sense, LMEM data mapping is similar to [7], extending here the support to an 8b I/O precision. Besides, stage (ii) supports an optional signed-to-unsigned type conversion, and stage (iv) its reverse. While being optimized for 3\u00d73 kernels, larger ones (5\u00d75, etc.) can be executed on IMAGINE as a succession of 3\u00d73 kernels [?].\nCompared to [7], the im2col conversion is performed sequentially on batches of 128b data fetch from the input LMEM, rather than in a one-shot fashion. This change significantly reduces the required size of the pre-im2col buffer, down to 128b instead of the CIM-SRAM's 1152\u00d78b full input bandwidth, reducing the bit-normalized area overhead of the digital datapath by more than 60%. Nonetheless, this solution requires a more complex input shift-register architecture to support the conditional enabling of different input register subsets on the CIM-SRAM side, depending on the target layer configuration. To that end, the entire shift-register is split into 32 sub-block as detailed in Fig. 15(d), matching the DP array division described in Section III.A. Within each sub-block, local clock gating (CG) latches control the update of the block's registers, with 32 CH\u2081 signals dictating the access to each block, while three CSK,j signals decide which kernels within the blocks are to be updated. Therefore, the minimum configuration of the CIM-SRAM macro is four input channels in convolutional mode, and one full sub-block in fully-connected mode.\nTo illustrate the digital-to-macro interface functionality, two transfer situations between the input LMEM and the CIM-SRAM's shift-register are covered in Fig. 15(b), highlighting the im2col data reshaping in convolutional mode. On the one hand, CNN layers with few channels and low precision can transfer multiple kernels in a same image row within a single transfer. On the other hand, large CNN layers need to split the transfer of a single kernel over multiple cycles. At fixed Cin, the number of cycles is directly proportional to the input precision rin, re-routing the shift-register inputs in the im2col as depicted.\nEventually, Fig. 15(d) presents the qualitative flow of IMAGINE's operation phases, considering various situations. Assuming a serial processing first, input transfers remain stalled until all CIM output data have been stored to the output LMEM, leading to a cycle penalty of\n$N_{stall} = 1 + N_{cim} + ceil (\\frac{r_{out} \u00d7 C_{out}}{BW}),$\nwhere BW = 128b is the LMEM I/O bandwidth and Ncim is the number of clock cycles allowed for CIM-SRAM operations, usually set to one. This penalty severely hinders the overall CIM-CNN throughput, calling for pipelining IMAGINE's operating phases. Ideally, all four phases might then take place simultaneously: while the CIM-SRAM performs the i-th DP computation, previous outputs i-1 are stored to the output LMEM whereas new inputs i + 1 are fetched from the input LMEM. However, this pipeline is subject to data-movement constraints between the different phases, depending on the CNN layer configuration, similar to [7]. On the one hand, input-dominated layers wait for input transfers to complete before issuing the next CIM-SRAM and store operations. For a convolutional layer, the number of cycles needed to process a single output-map value is then given by\n$N_{cycles} = N_{in} = (\\frac{K \u00d7 \\frac{r_{in} \u00d7 C_{in}}{BW}. (N_{cim} - 1)) + ceil $,\nEq. (9) showcases that multi-cycle CIM-SRAM computations increase the number of cycles as the data stored within the input shift-register have to remain constant during the entire macro operation. Relying on split control signals for the master and slave latches, as for the macro's output registers, could however circumvent this bottleneck. Moreover, Eq. (9) only holds for data transfers within a same image row, dividing the"}, {"title": "V. MEASUREMENT RESULTS", "content": "The IMAGINE accelerator was embedded in the CER-BERUS MCU, fabricated in 22nm FD-SOI. The chip microphotograph and the macro's layout are shown in Fig. 16(a). The CIM-SRAM area is dominated at 74% by the DP array, due to the use of large 10T1C bitcells while minimizing the ADC area as described in Section III.D. Overall, the macro occupies 53% of the 0.373mm\u00b2 total accelerator's area. Both the individual macro and entire accelerator have been characterized using the setup shown in Fig. 16(b), allowing to decouple macro- and system-level performance.\nA. CIM-SRAM Characterization\nFirstly, standalone characterization of the CIM-SRAM macro is carried out using its fully-connected (FC) configuration to enable easy one-to-one mapping between input LMEM and CIM data. A dedicated test mode allows accurate power estimates by ensuring a 100% duty-cycled utilization of the macro, changing a single 128b patch of inputs per CIM cycle. Fig. 17 reports the macro's transfer function and INL at 8b and 0.6V measured across 256 columns, considering 16 channels (i.e., 128 rows in FC mode) and varying its gain \u03b3. Here, inputs are kept at zero while weights are progressively changed from all-0's to all-1's, from the bottom to the top of the DP array. In this way, we can detect a peak of mean INL around zero-valued DPs, resulting from a slightly short DP timing pulse in the measured slow chip corner, as predicted in Fig. 8(b). Moreover, the aggregated output variability resulting from temporal noise (100 iterations) and residual column-to-column mismatch, after calibration, amounts to a maximum deviation of 3.5 LSB on the INL under unity gain. This yields a maximum RMS error of 0.52 LSB, which scales up with \u03b3 in Fig. 18(a) given the growing sensitivity to the residual noise floor. This error level is obtained after performing the calibration described in Section III.D, which brings the spatial deviation from 17 LSB down to just 2 LSB at the 8b precision, as seen in Fig. 19. Residual errors come from of mix of out-of-range SA offsets, insufficient voltage resolution during the calibration process, and noise induced by MoM caps. Reducing the error further down requires to use larger Ce capacitances and to upsize the SA to contain its mismatch. Nonetheless, both techniques would negatively impact the power and area of the macro. Instead, these low-noise statistics are included during the off-line CNN training to be partly compensated. Furthermore, with regards to \u03b3 scaling, linearity slowly degrades as the VDDL supply voltage is reduced from 0.4 down to 0.28V in Fig. 18(b), keeping VDDL = VDDH/2. Functionality is lost below 0.28V due to the insufficient configuration range of internal timings. Finally, the 8b peak energy efficiency of the CIM-SRAM macro indirectly depends on \u03b3 in Fig. 18(c), as the maximum operating frequency of the macro slightly improves between 2 and 16 thanks to the shorter transition time of the compressed Vsar levels. However, the efficiency remains better for unity gain as the SAR MSBs are directly connected to ground and supply levels, alleviating the resistive ladder's total load.\nFig. 20(a) showcases the impact of changing the number of input channels used. Overall, the output dynamic range improves at constant gain and supply when scaling Cin up, closely following post-layout estimates up to 32 channels. However, the maximum swing drops above this value as a result of distortions resulting from an unsettled DP result in the measured slow chip corner. Such distortion is estimated in Fig. 20(b) by measuring the output obtained when expecting a zero-valued DP, realized through different combinations of weight and input values. In particular, with inputs fixed at zero"}, {"title": "CONCL", "content": "In this work, we presented IMAGINE, a 1-to-8b compute-in-memory (CIM) CNN accelerator embedding a charge-based CIM-SRAM with a multi-bit input-serial, weight-parallel (MBIW) end-to-end dot-product (DP) operation. Namely, a split DP line structure allows up to 20\u00d7 higher voltage swing utilization, depending on the input channel configuration, while in-ADC 5b offset and rescaling effects enable linear data shaping, making full use of the selected ADC precision. Standalone measurement results of the dense 187kB/mm\u00b2 macro showcase peak energy and area efficiencies of 0.15-to-SION In this work, we presented IMAGINE, a 1-to-8b compute-in-memory (CIM) CNN accelerator embedding a charge-based CIM-SRAM with a multi-bit input-serial, weight-parallel (MBIW) end-to-end dot-product (DP) operation. Namely, a split DP line structure allows up to 20\u00d7 higher voltage swing utilization, depending on the input channel configuration, while in-ADC 5b offset and rescaling effects enable linear data shaping, making full use of the selected ADC precision. Standalone measurement results of the dense 187kB/mm\u00b2"}]}