{"title": "Unsupervised Learning in Echo State Networks for Input Reconstruction", "authors": ["Taiki Yamada", "Yuichi Katori", "Kantaro Fujiwara"], "abstract": "Conventional echo state networks (ESNs) require supervised learning to train the readout layer, using the desired outputs as training data. In this study, we focus on input reconstruction (IR), which refers to training the readout layer to reproduce the input time series in its output. We reformulate the learn-ing algorithm of the ESN readout layer to perform IR using unsupervised learning (UL). By conducting theoretical analysis and numerical experiments, we demonstrate that IR in ESNs can be effectively im-plemented under realistic conditions without explicitly using the desired outputs as training data; in this way, UL is enabled. Furthermore, we demonstrate that applications relying on IR, such as dynamical system replication and noise filtering, can be reformulated within the UL framework. Our findings estab-lish a theoretically sound and universally applicable IR formulation, along with its related tasks in ESNs. This work paves the way for novel predictions and highlights unresolved theoretical challenges in ESNs, particularly in the context of time-series processing methods and computational models of the brain.", "sections": [{"title": "1 Introduction", "content": "Reservoir computing (RC) is an efficient computational framework for time-series processing. Although RC implementations are diverse, models, such as the echo state network (ESN) [1] and the liquid state machine [2], share the same structure: fixed input and reservoir layers and a learnable readout layer [3]. The learnable readout layer in RC is trained to match the desired output time series. To define such a time series for RC, we need to define a time-series task. In some cases, the input time series itself can also be used as the desired output. In this paper, we refer to this task as input reconstruction.\nInput reconstruction (IR) is the simplest yet fundamental task. The RC performance is based on two time-series processing characteristics: preservability and transformability of the input time series within RC [2,4]. Considering this, IR requires the preservation of the input without transformation; consequently, it is the most basic time-series processing task. Despite its simplicity, IR in RC is sufficient for the learning of a dynamical system that generates an input time series [5-8]. Here, we focus on IR to investigate the fundamental properties of RC.\nIR has been studied not only within the context of RC but also through two major unsupervised learning (UL) approaches: noise filtering [9] and blind source separation [10]. Noise filtering methods, such as the Kalman filter [11] and its nonlinear extensions (see [12] for a review), rely on the prior knowledge of a system model governing the input time series. In noise filtering, this prior knowledge is used to reconstruct the original input time series or system state using noisy observations (Figure 1(b)). In contrast, blind source separation relies solely on statistical properties without assuming a specific system model: this approach includes principal component analysis [13], independent component analysis [14], and slow feature analysis [15]. Compared with noise filtering, blind source separation typically provides approximate IR by identifying the permuted and scaled versions of the input signals. This method relies on minimal assumptions, such as invertibility, to reconstruct unobserved source signals obtained from mixed observations (Figure 1(c)).\nHowever, the current standard approach in RC for IR is supervised learning (SL) of the readout layer (Figure 1(a)). Studies on combining RC with noise filtering (e.g., [16-19]) or blind source separation (e.g., [20-22]) have been conducted; however, in those, the unobserved original input time series was used as supervised data for training the readout layer. Although the listed studies are not exhaustive, this reliance on supervised data appears to be common. We believe that this problem formulation is unnatural compared with noise filtering and blind source separation for IR.\nThe objective of this study is to refine and advance the IR problem formulation in RC. We propose an IR formulation based on UL. Specifically, we propose a method to replace the availability assumption of an unobserved original input time series with prior knowledge of RC parameters that satisfy certain conditions (Figure 1(d)). The proposed method enables us to formulate IR independently of a specific input time series, allowing for a canonical representation of IR in RC. To illustrate the utility of this novel formulation, we develop algorithms for dynamical system replication and noise filtering that do not explicitly rely on the unobserved original input time series. In addition, we conduct numerical experiments to demonstrate the feasibility of the proposed formulation.\nThis paper is organized as follows. In Section 2, we define IR in ESNs and propose its reformulation based on UL. In Section 3, we demonstrate that dynamical system replication and noise filtering in ESNs can be theoretically achieved via IR, thus reformulated as UL tasks. Additionally, we provide numerical examples illustrating both tasks solved using the proposed method. In Section 4, we discuss the implications of our findings, highlighting how IR, dynamical system replication, and noise filtering in ESNs are fundamentally redefined under the UL framework. Finally, we present our conclusions in Section 5."}, {"title": "2 Unsupervised Input Reconstruction", "content": "In this section, we formalize unsupervised learning (UL) for input reconstruction (IR) in RC. Initially, we in-troduce ESN [1] as a numerically implementable and mathematically tractable RC model. Next, we formalize IR in ESNs. Finally, we describe the mathematical conditions that enable UL for IR in ESNs.\nHere, we describe the notation used throughout this paper. We denote the (vector) value of variable x at time $t\\in Z$ as $x_t$; we denote the horizontal concatenation of $T$ column vectors $x_t,...,x_{t+T-1} \\in R^n$ as $\\mathcal{X}_{t,t+T-1} \\in R^{n\\times T}$, using the capital calligraphic letter corresponding to $x$. Let $f$ be a map. We use the notation $f(\\mathcal{X}_{t,t+T-1})$ to represent the horizontal concatenation of vectors $f(x_t),..., f(x_{t+T-1})$.", "children": [{"title": "2.1 Echo State Network", "content": "ESN is a recurrent neural network sequentially driven by an input time series. The ESN input at time $t$ is a vector denoted as $d_t \\in R^{n_{in}}$. The state of the ESN (reservoir state) at time $t$ is also a vector denoted as $r_t \\in R^{n_r}$. The reservoir states are sequentially determined using the following equation:\n$r_{t+1} = g(d_t, r_t) := \\sigma [Ad_t + Br_t]$,\nwhere $\\sigma: R^{n_r} \\rightarrow R^{n_r}$, $A \\in R^{n_r \\times n_{in}}$, and $B\\in R^{n_r \\times n_r}$. Function $g$ and matrices $A$ and $B$ are fixed for each ESN. Function $\\sigma$ is referred to as the activation function, an matrices $A$ and $B$ are referred to as the input layer and the reservoir layer, respectively.\nThe ESN output at time $t$ is a vector defined as $Wr_t \\in R^{n_{out}}$, where $W\\in R^{n_{out} \\times n_r}$. Matrix $W$ is learnable, which is referred to as the readout layer. Let $o_t \\in R^{n_{out}}$ denote the desired ESN output at time $t$. $W$ is learned so that the ESN output approximates the desired output: $||Wr_t - o_t||_2 \\approx 0$, where $|| \\cdot ||$ is the Euclidean norm of a vector.\nThe standard approach to learning $W$ is the least squares method which requires the desired output data $\\mathcal{O}_t$. The least squares solution for the readout layer parameter that minimizes $||Wr_t - o_t||_2$ is\n$W_o := \\mathcal{O}_{1,T}\\mathcal{R}^+_{1,T}\\in R^{n_{out}\\times n_r}$,\nwhere $\\mathcal{R}^+_{1,T}$ is the Moore-Penrose inverse of $\\mathcal{R}_{1,T}$. Notation $W_o$ emphasizes that it uses samples of desired outputs $\\mathcal{O}_{1,T}$."}, {"title": "2.2 Input Reconstruction", "content": "In IR, we consider the case where the desired ESN output is equal to its input, i.e., $o_t = d_t (n_{out} = n_{in})$ for the entire time $t$. Then, using the standard approach introduced in (2), the solution for IR is\n$W_D := \\mathcal{D}_{1,T}\\mathcal{R}^+_{1,T}\\in R^{n_{out} x n_r}$.\nNotation $W_D$ emphasizes that its calculation requires samples obtained from the original input time series $\\mathcal{D}_{1,T}$. Thus, the standard approach for IR in ESN assumes that samples obtained from the original input time series $\\mathcal{D}_{1,T}$ are available during the training of the readout layer. Nevertheless, its objective is to reconstruct the original input during testing. We address the distinction in the problem settings between training and testing by introducing a training method for the readout layer that does not explicitly rely on the original input."}, {"title": "2.3 Unsupervised Learning for Input Reconstruction", "content": null, "children": [{"title": "2.3.1 Main Statements", "content": "The most vital key point of our method is that the original input $d_1$ can be expressed using reservoir state $r_t$ under certain invertibility assumptions. Specifically, we have the following lemma:\nLemma 1. $d_t = A^+ [\\sigma^{-1}(r_{t+1}) \u2013 Br_t]$ for all $t \\in Z$ if $\\sigma$ is invertible and $rank(A) = n_{in}$.\nProof. Solve (1) for $d_t$ using $A^+ A = I$.\nLemma 1 implies the following corollary using concatenated notations:\nCorollary 1. $\\mathcal{D}_{1,T} = A^+ [\\sigma^{-1}(\\mathcal{R}_{2,T+1}) \u2013 B\\mathcal{R}_{1,T}]$ for all $t \\in Z$ if $\\sigma$ is invertible and $rank(A) = n_{in}$"}, {"title": "2.3.2 Remarks", "content": "Using Theorem 2, the least squares solution of IR in ESNs is expressed as $W_R = A^+ (\\mathcal{B}_T \u2013 B)$, where $\\mathcal{B}_T$ minimizes the following square loss function:\n$L_{\\sigma,T}(\\mathcal{B}) := \\sum_{t=1}^{T-1} ||\\sigma^{-1}(r_{t+1}) \u2013 \\mathcal{B}r_t||^2$.\nNote that $\\mathcal{B}_T$ can be calculated without using original inputs $d_t$, $A$, and $B$. Apart from using the Moore-Penrose inverse of a matrix, numerous optimizations are available to obtain the minimizer of the loss function $L_{\\sigma,T}(\\mathcal{B})$. For example, the recursive least square (RLS) algorithm [23] sequentially updates $(\\mathcal{B}_t, P_t)$ to $(\\mathcal{B}_{t+1}, P_{t+1})$ based on newly obtained data $(r_t, \\sigma^{-1}(r_{t+1}))$, where $P_t$ is the recursive estimation of the precision matrix of the reservoir state. Hence, a sequential algorithm for IR in ESNs based on UL is presented (Algorithm 1)."}, {"title": "2.3.3 Numerical Experiments", "content": "We conducted numerical experiments using Algorithm 1. For ESN, we considered a 1-dimensional input $d_t$, which is defined as follows:\n$d_t \\begin{cases}\ncos(\\pi t/50) & 1 < t < 400, \\\\\ncos(\\pi t/100) + sin(\\pi t/25) & 400 \\leq t < 800, \\\\\ncos^3(\\pi t/50) & 800 < t \\leq 1200.\n\\end{cases}$\nWe used a 50-dimensional ESN with the element-wise hyperbolic tangent function ($tanh$) as the activation function $\\sigma$. We initialized the elements of the input layer $A$ using i.i.d. samples obtained from $\\mathcal{N}(0,0.02)$. Let $B_0 \\in R^{500\\times500}$ be a matrix with elements being i.i.d. samples obtained from $\\mathcal{N}(0,1)$. We set $B = 0.9 \\cdot \\lambda(B_0) \\cdot B_0$, where $\\lambda(B_0)$ denotes the spectral radius of $B_0$. We conducted numerical experiments for 10 different implementations of parameters $A$ and $B$. We applied the RLS Algorithm 1 to obtain the readout layer $(W_R)_t$ without explicitly using the original input $d_t$. Thus, $(W_R)_t r_t$ represents the ESN output for IR at time $t$.\nFigure 2 shows the results obtained from Algorithm 1 using IR based on UL. We used an input varying every 400 steps (Figure 2(a)). Driven by the input, the reservoir state evolved (Figure 2(b)). As learning progressed, the ESN output approached the original input on average (Figure 2(c)). Despite the varying input, the readout weights exhibited minimal adjustment after 400 steps on average (Figure 2(d)). By conducting numerical experiments, we verified that IR in ESNs can be achieved without explicitly using the original input, thus validating UL."}]}]}, {"title": "3 Application of Unsupervised Input Reconstruction", "content": "In this section, we examine the applications of IR in ESNs. In Section 2, we formulated IR in ESNs as UL. Thus, problems depending on IR are reformulated as UL problems, which can be treated independently of the original ESN input.\nSpecifically, we initially examine the replication problem of deterministic dynamical systems governing the generation of the original input. We will demonstrate that dynamical system replication can be formulated as a UL problem.\nNext, we examine the noise filtering problem to enhance the performance of IR in ESNs in the presence of noise in the input. We demonstrate that a dynamical system replicated using UL can be employed for effective noise filtering.", "children": [{"title": "3.1 Dynamical System Replication", "content": null, "children": [{"title": "3.1.1 Problem Formulations", "content": "Let $(\\mathcal{X}, f^\\circ)$ be a dynamical system, where $\\mathcal{X} \\subset R^k$ and $f^\\circ$ is a function from $\\mathcal{X}$ to itself. We refer to $f^\\circ$ as the true dynamical system. Let $\\{x_t\\}_{t=1}^T$ be a finite orbit of the dynamical system $(\\mathcal{X}, f^\\circ)$, such that $x_1 \\in \\mathcal{X}$ and $x_{t+1} = f^\\circ (x_t)$ for all $t = 1,...,T \u2013 1$. The purpose of dynamical system replication is to construct a dynamical system $(\\mathcal{Y}, f)$ from a finite orbit $\\{x_t\\}_{t=1}^T$ (Figure 3). We informally define that dynamical system replication is successful if $f^\\circ$ and $f$ are equivalent."}, {"title": "3.1.2 Remarks", "content": "In [24], an algorithm for dynamical system replication was proposed. The algorithm is based on IR in ESN with a finite orbit $\\{x_t\\}_{t=1}^T$, which is used as the ESN input. Consider an ESN $r_{t+1} = g(d_t, r_t)$, where $d_t = x_t$. We recall that $W_D$ is a readout layer obtained using SL so that it reconstructs the input: $W_D r_t \\approx d_t = x_t$.\nThe output of the algorithm is the dynamical system on $\\mathcal{Y} = R^{n_r}$, which is defined as follows:\n$f_D : r \\rightarrow g(W_D r, r) = \\sigma[(AW_D + B)r]$.\nBy applying some conditions on the ESN, $f^\\circ$ and $f_D$ are equivalent in the sense of the topological conjugacy of dynamical systems, supporting the correctness of the dynamical system replication algorithm (See [6,8,25-31] for more details).\nIn Section 2, we showed that $W_D$ is reformulated as $W_R$, which was defined in (5). Thus, the output of the dynamical system replication algorithm is reformulated as follows:\n$f_R : r \\rightarrow g(W_R r, r) = \\sigma((AW_R + B)r) = \\sigma(Br)$.\nThe calculation of $W_R$ and $B$ does not explicitly use the finite orbit $\\{x_t\\}_{t=1}^T$ of the true dynamical system. This indicates that, similar to IR, dynamical system replication is also possible within the agent (Figure 3)."}, {"title": "3.1.3 Numerical Experiments", "content": "We conducted numerical experiments to demonstrate dynamical system replication based on UL. Consider the Lorenz-63 system defined as\n$\\begin{cases}\n\\xi = 10(\\eta \u2013 \\xi), \\\\\n\\dot{\\eta} = \\xi(28 \u2013 \\zeta) \u2013 \\eta, \\\\\n\\dot{\\zeta} = \\xi \\eta \u2013 \\zeta.\n\\end{cases}$\nLet $\\mathcal{F}_{0.02} : [\\xi(t), \\eta(t), \\zeta(t)] \\rightarrow [\\xi(t+\\tau), \\eta(t+\\tau), \\zeta(t+\\tau)]$ be the flow map of the Lorenz system (10). We used $\\tau = 0.02$ as the true dynamical system $f^\\circ$. We numerically calculated a 7000-step orbit $\\{x_t := \\mathcal{F}_{0.02}(x_1)\\}_{t=1}^{7000}$ of the true dynamical system $\\mathcal{F}_{0.02}$ using the 4th-order Runge-Kutta method with the initial state $x_1 = [1, 1, 1]$ and a time step size of 0.02. We used $\\{x_t\\}_{t=1}^{5000}$ for training and $\\{x_t\\}_{t=5001}^{7000}$ for testing. Additionally, we used a 500-dimensional ESN with the element-wise tanh function as the activation function $\\sigma$. We set the elements of the input layer $A$ using i.i.d. samples obtaind from $\\mathcal{N}(0,0.02)$. Let $B_0 \\in R^{500\\times500}$ be a matrix with elements being i.i.d. samples from $\\mathcal{N}(0,1)$. We set $B = 1.2 \\cdot \\lambda(B_0) \\cdot B_0$, where $\\lambda(B_0)$ is the spectral radius of $B_0$. We conducted numerical experiments for 10 different implementations of parameters $A$ and $B$, and numerically calculated $W_D$ and $W_R$ using (3) and (5), respectively. Then, we numerically calculated a 2000-step orbit $\\{\\hat{x}_t\\}_{t=1}^{2000}$ of $f_D$ with the initial condition $\\hat{x}_1 = r_{5001}$. Each orbit of the 500-dimensional dynamical system $\\{\\hat{x}_t\\}_{t=1}^{2000}$ of $f_D$ was projected onto a 3-dimensional space by calculating $\\{W_D r_t\\}_{t=1}^{2000}$ for visualization.\nWe performed the same procedure for $f_R$. We used the inverse map of $\\sigma = tanh$ and the Moore-Penrose inverse of matrix $A$ to calculate $W_R$ using (5). It should be noted that, in the numerical experiments, we used numpy.arctanh and numpy.linalg.pinv employed in the Python package NumPy. All numerical calculations were performed with float64 precision as per the default setting of NumPy.\nFigure 4 shows a typical numerical example of the orbits of the dynamical systems. Figures 4(a) and (b) show the orbits of the true dynamical system used for training and testing, respectively. We observe that the projected orbits of both the replicated dynamical system $f_D$ (Figure 4(c)) and $f_R$ (Figure 4(d)) were similar to the orbit of the true dynamical system (Figure 4(b)) in terms of the Lorenz attractor shape. However, the terminal states were different among the replicated dynamical systems although the initial states were the same (Figure 4(c) and (d))."}]}]}, {"title": "3.2 Noise Filtering", "content": null, "children": [{"title": "3.2.1 Problem Formulations", "content": "Let $d_t \\in R^{n_{in}}$ be the original ESN input. Suppose that the readout layer $W_R^{(1)}$ is trained for IR using (5) with the reservoir states $r_t^{(1)}$ driven by input $d_t^{(1)}$ being contaminated by white Gaussian noise with zero mean and covariance matrix $\\Sigma^{(1)} \\in R^{n_{in}\\times n_{in}}$. Consider the case where the readout layer $W_R^{(1)}$ is kept fixed as prior knowledge (Figure 5(i)). The purpose of noise filtering is to reconstruct the original input $d_t$ from a history of reservoir states $r_t^{(2)}$ driven by input $d_t^{(2)}$ that are contaminated by white Gaussian noise with zero mean and covariance matrix $\\Sigma^{(2)}$ (Figure 5(ii)). We focus on the $tr(\\Sigma^{(1)}) < tr(\\Sigma^{(2)})$ case."}, {"title": "3.2.2 Remarks", "content": "A naive solution for noise filtering is the direct use of the readout layer $W_R^{(1)}$ to obtain $W_R^{(1)} r_t^{(2)}$ as a reconstruction of the input (Figure 5(ii), upper row). This results in a good reconstruction with small $tr(\\Sigma^{(2)})$; however, it is not effective when $tr(\\Sigma^{(2)})$ is large because $W_R^{(1)}$ has not been trained to handle a significant amount of noise.\nAs shown in Section 3.1, the trained readout layer $W_R^{(1)}$ induces the autonomous dynamical system $f_R^{(1)}$ defined in (9). By definition, the dynamical system $f_R^{(1)}$ is regarded as the evolution law of the reservoir state virtually driven by input $d_t^{(1)}$ with a noise intensity lower than that of input $d_t^{(2)}$. Thus, intuitively, we can use $f_R^{(1)}$ as prior knowledge in IR. We will demonstrate that, using Kalman filter theory and algorithms, we can utilize this prior knowledge to filter the noise and refine the IR performance. Readers may refer to [11,32] for a comprehensive understanding of the Kalman filter theory and algorithms.\nWe used the extended Kalman filter to obtain the filtered reservoir state $r_t^{(2\\rightarrow 1)}$ from the noisy reservoir state $r_t^{(2)}$. Then, the output $W_R^{(1)} r_t^{(2\\rightarrow 1)}$ is close to $W_R^{(1)} r_t^{(1)}$, resulting in an improved reconstruction perfor-mance of the original input $d_t$ than that of $W_R^{(1)} r_t^{(2)}$. To apply the extended Kalman filter, we define the following state-space model:\n$\\begin{aligned}\nr_{t+1}^{(1)} &:= f_R^{(1)} (r_t^{(1)}) + w_t, \\\\\nr_t^{(2)} &= r_t^{(1)} + v_t,\n\\end{aligned}$\nwhere $w_t$ and $v_t$ are assumed to be white Gaussian noise with zero mean and covariance matrices $Q \\in R^{n_r\\times n_r}$ and $R \\in R^{n_r\\times n_r}$, respectively. The Kalman filter algorithm requires the values of $Q$ and $R$. The covariance matrix $Q$ of $w_t$ is empirically calculated using $Q = \\frac{1}{T} \\sum_{t=1}^{T} w_t w_t^\\top$. Note that, as $f_R^{(1)}$ can be obtained using UL (see Section 3.1), the same applies to $Q$ using the reservoir state $r_t^{(1)}$ provided in prior learning (Algorithm 2)."}, {"title": "3.2.3 Numerical Experiments", "content": "We conducted numerical experiments to demonstrate IR with noise filtering using prior knowledge. We considered the original input $d_t$, which is defined as follows:\n$d_t = cos(2\\pi t/100) \\quad (1 \\leq t \\leq 2000)$.\nWe generated inputs $d_t^{(1)} = d_t + \\epsilon_t^{(1)}$ and $d_t^{(2)} = d_t + \\epsilon_t^{(2)}$, where $\\{\\epsilon_t^{(1)}\\}$ and $\\{\\epsilon_t^{(2)}\\}$ are zero-mean white Gaussian noise with variance $tr(\\Sigma^{(1)}) = 0.01$ and $tr(\\Sigma^{(2)}) \\in numpy.logspace(-2, 1, 10)$:10 numbers spaced evenly on a log scale between $10^{-2}$ and $10^1$. We used a 10-dimensional linear ESN with the identity function $id : r \\rightarrow r$ as the activation function $\\sigma$. We set the elements of the input layer parameter $A \\in R^{10\\times 1}$ using i.i.d samples obtained from $\\mathcal{N}(0,0.02)$. Let $B_0 \\in R^{10\\times10}$ be a matrix with elements being i.i.d samples from $\\mathcal{N}(0,1)$. We set $B = 0.9 \\cdot \\lambda(B_0) \\cdot B_0$, where $\\lambda(B_0)$ is the spectral radius of $B_0$. For each ESN implementation and noisy inputs $\\mathcal{D}_{1,2000}^{(1)}$ and $\\mathcal{D}_{1,2000}^{(2)}$, we calculated reservoir states $\\mathcal{R}_{1,2001}^{(1)}$ and $\\mathcal{R}_{1,2001}^{(2)}$ with the zero vector as the initial reservoir state. We applied Algorithm 2 to obtain $W_R^{(1)}$, $f_R^{(1)}$ and $Q$ from $\\mathcal{R}_{1,2000}^{(1)}$. Then, we applied the adaptive Kalman filter Algorithm 3 with $\\alpha_R = 0.1$ to obtain filtered reservoir states $r_t^{(2\\rightarrow 1)}$ from the sequence of contaminated reservoir states $r_t^{(2)}$. We conducted simulations for 50 different implementations of the ESN parameters $A$ and $B$ and noisy inputs $\\mathcal{D}_{1,2000}^{(1)}$ and $\\mathcal{D}_{1,2000}^{(2)}$. We calculated the relative root mean squared error (RRMSE) to evaluate the IR performance.\nFigure 6 shows a typical numerical example of the ESN inputs and outputs using IR with noise filtering. We used low-intensity (Figure 6(a)) and high-intensity (Figure 6(b)) noise inputs for training and testing, respectively. In this trial, the RRMSE of IR was reduced from 1.41 to 0.57 by applying noise filtering (Figure 6(c) and (d))."}]}, {"title": "4 Discussion", "content": null, "children": [{"title": "4.1 Unsupervised Input Reconstruction", "content": "In Section 2, we demonstrated that input reconstruction (IR) in ESN can be formalized as unsupervised learning (UL), which does not require data from the original inputs.", "children": [{"title": "4.1.1 ESN information processing ability", "content": "This UL-based formalization enhances our understanding of the ESN information processing ability. The ESN readout layer is trained to match the desired ESN output ((2) in Section 2). As the desired output is generally a nonlinear and time-delayed transformation of the input, the input itself can be regarded as the simplest choice for the desired output. In this sense, the IR performance could correspond to the upper limit of the ESN information processing ability. Our finding that IR in ESNs is UL indicates that the upper limit of the ESN information processing ability is independent of the type of input. This natural statement facilitates the independent investigation of the following questions:\n\u2022 What kind of ESN properties, regarding the activation function $\\sigma$ and parameters $A$ and $B$, are suitable for IR?\n\u2022 Which properties of the ESN input degrade its raw information processing ability?\nThus, the UL-based IR formalization provides guidelines for understanding both the universal and task-dependent ESN properties."}, {"title": "4.1.2 ESN behavior toward the input", "content": "We developed an algorithm to compute the readout layer parameter $W_R \\in R^{n_{in}\\times n_r}$, which maps the reservoir state $r_t$ to the current input $d_t$. Using this, we can numerically obtain the right inverse map of $W_R$ as follows:\n$V_{R,\\Xi} := W_R^+ + (I \u2013 W_R^+ W_R) \\Xi$,\nwhere $\\Xi \\in R^{n_r\\times n_r}$ is an arbitrary matrix. Suppose that $rank(W_R) = n_{in}$. Then, by definition, the following equation holds:\n$W_R V_{R,\\Xi} d_t = d_t \\approx W_R r_t$."}]}, {"title": "4.2 Unsupervised Dynamical system Replication", "content": "In Section 3.1, we demonstrated that the replication of a dynamical system generating the ESN input can be achieved using UL.", "children": [{"title": "4.2.1 Standard approach to dynamical system replication", "content": "The UL-based formalization encourages us to reconsider the standard approach to dynamical system repli-cation. We observed that the replicated dynamical systems using SL and UL differ because of the numerical errors. These errors are practically unavoidable. For example, numerical errors in the computation of the inverse tanh function are unavoidable because they involve Napier's constant, which is approximated as a rational number in computers. Similarly, numerical errors in the computation of the Moore-Penrose inverse of matrices are unavoidable because of the singular value decomposition, where singular values smaller than the machine epsilon are typically truncated by default. Thus, we propose treating the replicated dynamical systems obtained from SL and UL as distinct, even though they are theoretically equivalent, based on assump-tions made for ESNs, as explained in Section 2. Moreover, we propose considering the replicated dynamical systems obtained from UL as a standard because this approach does not require data for the original ESN input. In contrast, the replicated dynamical systems obtained from SL are treated as special cases that can be implemented only when the original ESN input is exceptionally available. It is worth investigating how the assumption of the original input availability affects characteristics such as dynamical system replication robustness."}, {"title": "4.2.2 Contributions to theoretical neuroscience", "content": "A formalization independent of the original ESN input enables the investigation of the computational prin-ciples of self-organizing systems, such as the brain. Our perception, sustained by the brain, allows us to understand the external environment through sensory information. All computations required for perception occur within the brain and do not explicitly depend on the true state of the external environment. In this context, our UL-based formalization contributes to further exploration of the computational principles of the brain. In particular, if the activation function $\\sigma$ is element-wise and injective, such as the tanh or the identity function, the loss function defined in (6) is equivalent to the following loss function of $\\mathcal{B}$:\n$\\sum_{t=1}^{T-1} ||\\underbrace{r_{t+1}}_{stimulus-evoked activity} - \\underbrace{\\sigma(\\mathcal{B}r_t)}_{internally-predicted activity}||^2$.\nThis reformulation interprets the loss function $L_{\\sigma,T}(\\mathcal{B})$ as the discrepancy between the stimulus-evoked activity $r_{t+1}$ and the internally-predicted activity $\\sigma(\\mathcal{B}r_t)$. This perspective parallels prior studies [39-42], which propose that minimizing the discrepancy between these activities reflects a core computational principle underlying perception. We highlight a shared mathematical foundation between the prior studies addressing brain models for perception and the ESNs computational framework. Notably, we deductively derived this loss function from the mathematical observation on IR, hence avoiding heuristic implementation of the loss function (16). Providing that ESN is one of the simplest models capturing the essence of perception based on the loss function (16), we believe the ESN potential as a theoretical model of the brain. To the best of our knowledge, it is remain unclear whether the introduction of additional complexities in the model guarantee qualitative benefits in the tasks treated in this study. We believe that our formulation using ESN provides a foundational basis for future investigations into the detailed properties of both the brain and ESNs."}]}, {"title": "4.3 Unsupervised Noise Filtering", "content": "In Section 3.2, we demonstrated that a replicated dynamical system can be used for noise filtering, and the algorithm to achieve this does not require explicit use of the ESN input.", "children": [{"title": "4.3.1 Effective design of ESN with error feedback", "content": "We proposed a method that fully applies the Kalman filter algorithm to ESNs. This approach induces the design of an ESN with effectiveness being supported by the Kalman filter theory. We consider a linear ESN, where the activation function is the identity function. Under this assumption, we obtain the asymptotic form of the dynamics of the filtered reservoir state $r_{t+1}^{(2\\rightarrow 1)}$\n$r_{t+1}^{(2\\rightarrow 1)} = f_R^{(1)} (r_t^{(2\\rightarrow 1)}) + K(r_t^{(2)} \u2013 f_R^{(1)} (r_t^{(2\\rightarrow 1)}))$,\nwhere $K = P(P+R)^{-1} \\in"}]}]}]}