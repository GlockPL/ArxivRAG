{"title": "MMT-BERT: CHORD-AWARE SYMBOLIC MUSIC GENERATION BASED\nON MULTITRACK MUSIC TRANSFORMER AND MUSICBERT", "authors": ["Jinlong Zhu", "Keigo Sakurai", "Ren Togo", "Takahiro Ogawa", "Miki Haseyama"], "abstract": "We propose a novel symbolic music representation and\nGenerative Adversarial Network (GAN) framework spe-\ncially designed for symbolic multitrack music generation.\nThe main theme of symbolic music generation primarily\nencompasses the preprocessing of music data and the im-\nplementation of a deep learning framework. Current tech-\nniques dedicated to symbolic music generation generally\nencounter two significant challenges: training data's lack\nof information about chords and scales and the require-\nment of specially designed model architecture adapted to\nthe unique format of symbolic music representation. In\nthis paper, we solve the above problems by introducing\nnew symbolic music representation with MusicLang chord\nanalysis model. We propose our MMT-BERT architecture\nadapting to the representation. To build a robust multitrack\nmusic generator, we fine-tune a pre-trained MusicBERT\nmodel to serve as the discriminator, and incorporate rel-\nativistic standard loss. This approach, supported by the\nin-depth understanding of symbolic music encoded within\nMusicBERT, fortifies the consonance and humanity of mu-\nsic generated by our method. Experimental results demon-\nstrate the effectiveness of our approach which strictly fol-\nlows the state-of-the-art methods.", "sections": [{"title": "1. INTRODUCTION", "content": "Music plays an indispensable role in our daily lives, and\nthere is a significant demand for creating new musical con-\ntents. Automatic music generation is one of the most in-\ntriguing tasks in bringing new music experiences to con-\nsumers [1]. The earliest studies in the 1950s focused on\na combination of music theory and Markov-chains-based\nprobabilistic models, and realized randomly creating mu-\nsic parts and combining them into a synthesis [2]. Con-\ntemporary studies have achieved higher quality and faster\nmusic generation by utilizing advanced neural networks\nsuch as Generative Adversarial Networks (GANs), Trans-\nformer, and diffusion models [3-5]. Despite significant\nadvancements, previous methods continue to suffer from\nchallenges such as insufficient data extraction and unsta-\nble training trajectories. Consequently, there is room for\nnew approaches for more effective music representations\nand more robust deep learning architectures.\nIn particular, chords are crucial for conveying emo-\ntional and humanistic expressions in music, yet few meth-\nods take chords into account in symbolic music represen-\ntation. Consequently, previous methods are deprived of\nindispensable information about chords and scales. This\nlack results in the generation of music that exhibits a di-\nminished degree of humanity. Therefore, previous meth-\nods for music generation face limitations in their ability\nto produce human-like and high quality expressions [6].\nA feasible solution to overcome this difficulty is the in-\ntegration of a chord analysis model [7]. Chord analysis\nmodel aids in the extraction of chord data from raw audio,\nfostering a novel representation method that encompasses\nchord information [8-11]. With the aid of state-of-the-art\nchord analysis models, we can generate more harmonious\nand structured music with more regular chord progressions\nby automatically extracting and encoding chords from raw\naudio files. Therefore, it is expected that adopting chord\nanalysis models in creating new symbolic representations\nof music will enable the generation of music that is closer\nto human composition.\nAnother problem arises from the ever-changing format\nof symbolic music representation, which makes design-\ning the model's architecture that fits symbolic music gen-\neration to be another challenge. GANs are widely ap-\nplied in the symbolic music generation field because the\naddition of a discriminator obviously strengthens the fi-\ndelity of the overall generative model [12-17]. The per-\nformance of GANs is deeply influenced by the architec-\nture of the generator and discriminator. Previous stud-\nies have demonstrated the effectiveness of transformer-\nbased generators [6, 7, 18-23]. Whereas, the architecture\nof the discriminator has been extensively discussed in re-\ncent years. Some methods [12, 14, 16, 17] involve con-\nstructing a discriminator based on CNN or Transformer,\nwhile others [15] utilize pre-trained models adapted to\ntheir tasks. Compared to hand-crafted discriminators, us-\ning pre-trained models often achieves a fairly good result"}, {"title": "2. RELATED WORKS", "content": "To enable computers to properly understand music, re-\nsearch on symbolic music representation has been con-\nducted for many years [32]. Musical Instrument Digital In-\nterface (MIDI) is the most commonly used format for sym-\nbolic music representation, containing performance data\nand control information for musical notes. In the music\nprocessing community, many researchers symbolize music\nwith MIDI-like events [33].\nHuang et al. have proposed REvamped MIDI-derived\nevents (REMI), which adds note duration and bar events,\nenabling models to generate music with subtle rhythmic\nrepetition [7]. However, the REMI representation often en-\ncounters a challenge that the sequence is too long. Build-\ning upon the REMI framework, Hsiao et al. have pro-\nposed Compound Word Transformer (CP) [18]. CP mod-\nifies REMI's approach by transforming one-dimensional\nsequence tokens into compound words sequence using spe-\ncific rules. Although this modification significantly short-\nens the average token sequence length and simplifies the\nmodel's ability to capture musical nuances, CP is hard\nto generate multitrack music [6]. Dong et al. have pro-\nposed their multitrack music representation, which rep-\nresents music with a sequence of sextuple tokens, along\nwith a Transformer-XL-based generation method Multi-\ntrack Music Transformer (MMT). This approach utilizes\na decoder-only Transformer architecture, adept at process-\ning multi-dimensional inputs and outputs. MMT leverages\nthe advantages of the Transformer to enable the generation\nof longer multitrack music compositions than previous mu-\nsic generation methods. However, MMT's representation\nscheme lacks chord event inclusion, an essential element\nin musical compositions. In contrast, our symbolic mu-\nsic representation technique builds on the foundation laid\nby MMT by integrating chord information, enabling our\nmodel to produce more harmonically rich compositions."}, {"title": "2.2 Generative Adversarial Network-based Music\nGeneration", "content": "Previous studies have employed various GANs to realize\nsymbolic music generation [12, 13, 17]. In early states,\nDong et al. have proposed MuseGAN, a CNN-based\nGAN architecture, managing to generate multitrack music\npieces [16]. However, CNN-based GANs often suffer from\nproblems such as limited local perception, fixed-size in-\nputs, etc. Muhamed et al. solved this problem by introduc-\ning their Transformer-GANs model, using a Transformer-"}, {"title": "3. METHODOLOGY", "content": "In our approach, we introduce a novel symbolic music rep-\nresentation that incorporates chords. While most conven-\ntional representations omit details about chords, we focus\non chord information-aware representation to facilitate the\nprocess of generating music that more closely resembles\nhumans. First, we extract music data including chords and\nnotes from MIDI files based on MuspyToolkit [35] and\nMusicLang. During the extraction process, we recognize a\nchord once per bar, i.e., every four beats. We exclude songs\nwith a time signature other than 4/4, limit the number of\nchords in a bar to one, and ignore chord changes within a\nbar since MusicLang only detects chord changes once per\nbar. Each time MusicLang detects a chord change, it ex-\ntracts the scale degree, tonality root, tonality mode, chord\noctave and extension note of the chord. After extracting\nchord and note information, we encode a piece of music\ninto a sequence of quintuple tokens $X = (x_0, ..., x_{N-1})$,\nwhere $x_i$ and $N$ denote the i-th quintuple token and the to-\ntal number of quintuple tokens, respectively. Here, t repre-\nsents the following event type: {start-of-song, instrument,\nstart-of-score, note, chord, end-of-song}. The meanings of\neach event type are shown as follows:\n\u2022 Start-of-song: The beginning of the music piece\n\u2022 Instrument: An instrument used in the music piece"}, {"title": "3.2 MMT-BERT Architecture", "content": "The fundamental structure of our MMT-BERT architec-\nture is based on a GAN architecture, employing MMT as\nthe generator and MusicBERT as the discriminator. The\nprimary concept of GAN is minimizing the loss to en-\nhance the generator's ability to deceive the discriminator\nby producing fake music indistinguishable from real mu-\nsic, while simultaneously maximizing the discriminator's\naccuracy in distinguishing between real and fake music.\nDetails of the generator and discriminator will be discussed\nlater."}, {"title": "3.2.1 Generator", "content": "As the generator, we employ MMT [6], a Transformer-XL-\nbased model that consists solely of decoders. In MMT, ele-\nments in the quintuple token $x_i$ are individually embedded\nfirst, and then concatenated, followed by the addition of"}, {"title": "3.2.2 Discriminator", "content": "As the discriminator, we adopted MusicBERT [29], a\nlarge-scale Transformer model developed for symbolic\nmusic understanding. MusicBERT consists of a Trans-\nformer encoder and utilizes a masked language modeling\napproach where certain tokens in the input music sequence\nare masked and then predicted by the model output. The\noriginal proposed encoding method, called OctupleMIDI\nprocess transforms a symbolic music piece into a sequence\nof octuple tokens, each containing eight basic elements re-\nlated to a musical note. In order to make MusicBERT act\nas a discriminator adapted to the proposed representation\nmentioned in Sec. 3.1, we refine the input and output for-\nmat of MusicBERT. Quintuple tokens are converted into a\nsingle vector through the concatenation of embeddings and"}, {"title": "3.2.3 Relativistic Standard Loss", "content": "Inspired by RS-GAN [31], one of the state-of-the-art meth-\nods in GANs, we adopt the relativistic standard loss as our\nobjective function. Applying relativistic standard loss pre-\nvents the network from becoming overconfident, leading to\nslower and more careful decisions, allowing the generator\nmore room to adjust its weights and improve the training\nprocess [30]. The probability that the given fake data is\nmore realistic than a randomly sampled real data is defined\nas follows:\n$D(x) = sigmoid(C(f) \u2013 C(r))$,\nwhere $C(\u00b7)$ denotes a non-transformed layer, and 7 denotes\nreal/fake data pairs $x = (r, f)$. Hence, the loss function\nof the generator G and the discriminator D are defined as\nfollows:\n$L_G =E_{(r,f)~p(r, f)} [log(sigmoid(C(r) \u2013 C(f))]$\n$L_D =E_{(r,f)~p(r,f)} [log(sigmoid(C(f) \u2013 C(r))]$,\nwhere $r_i$ and $f_i$ denote ground truth logits and generated\nmusic logits, respectively. It is noted that we add cross en-\ntropy to the loss function of generator in order to accelerate"}, {"title": "4. EXPERIMENT", "content": "In the experiment, we utilize the Symbolic Orchestral\nDatabase (SOD) [36], which comprises 5,864 music pieces\nencoded as MIDI files along with associated metadata.\nThe dataset is partitioned into training, testing, and val-\nidation sets, receiving 80%, 10%, and 10% of the data,\nrespectively. We set a temporal resolution of 12 time\nsteps per quarter note for detailed timing accuracy. The\nTransformer-XL generator is composed of six decoder lay-\ners with 512 dimensions and eight self-attention heads,\nand the MusicBERT discriminator consists of one encoder\nlayer with two self-attention heads. The maximum length\nfor symbolic music sequences is set at 1024, with a maxi-\nmum of 256 beats. To optimize the models, we employ the\nAdagrad optimizer to mitigate issues of gradient explosion\nand vanishing [37]. Additionally, to enhance the robust-\nness of the data, we augment it by randomly transposing\nall pitches by s ~ U(\u22125,6)(s \u2208 Z) semitones and assign\na starting beat. Here, U denotes a uniform distribution.\nAs comparative methods, we employ three state-of-the-\nart music generation models: MMM [21], FIGARO [23],\nand MMT [6]. We validate the performance of our MMT-\nBERT model by conducting quantitative evaluations using\nexisting metrics and subjective experiments to assess the\nhuman-like qualities of the generated music pieces."}, {"title": "4.2 Quantitative Evaluation", "content": "Following [6], we evaluate the generated music pieces us-\ning four metrics: pitch class entropy similarity (PCES),\nscale consistency similarity (SCS), groove consistency\nsimilarity (GCS), and average length (AL). We consider\nhigher values of PCES, SCS, and GCS as indicators of su-\nperior quality, while a higher AL denotes a greater capa-\nbility to produce long-duration music pieces.\nIn preparation for calculating PCES, the pitch class en-\ntropy (PCE) is defined as follows:\n$PCE = - \\sum_{i=0}^{11}h_i log_2(h_i)$,\nwhere $h_i$ denotes the number of occurrences of each note\nname in the 12-dimensional pitch class histogram. As the\nPCE values increase, the tonality of the generated music\npieces exhibits greater instability. However, it is important\nto recognize that more stable tonality does not necessarily\nimply higher quality. Subsequently, we calculate PCES be-\ntween generated music samples and human compositions\nas follows:\n$PCES = 1 - \\frac{PCE_{gen} - PCE_{tr}}{PCE_{tr}}$,"}, {"title": "4.3 Impacts of Chord Event and Discriminator", "content": "MMT-BERT aims to generate more harmonious, more\nhuman-like music pieces through the addition of chord\nevents and adversarial generative learning by employing\nMusicBERT as its discriminator. To evaluate aspects re-\nlated to richness and humanness, we have conducted sub-\njective experiment and ablation study.\nIn the subjective experiment, we asked 18 music am-\nateurs as the following five questions and requested that\nthey rated each on a five-point scale.\n\u2022 Richness (R): Does the music piece have diversity\nand interestingness?\n\u2022 Humanness (H): Does the music piece sound like it\nwas composed by an expressive human musician?\n\u2022 Correctness (C): Does the music piece contain per-\nceived mistakes in composition or performance?\n\u2022 Structureness (S): Does the music piece exhibit\nstructural patterns such as repeating themes or the\ndevelopment of musical ideas?\n\u2022 Overall (O): What is the general score of the music\npiece?\nAs mentioned in Sec. 4.2, FIGARO and MMM employ a\nmusic representation that considers percussive sounds and\ntypically generates much shorter pieces. Therefore, the\nnature of the music pieces generated by these models, FI-\nGARO and MMM, differs significantly from that of MMT-\nBERT and MMT due to their use of percussive sounds and\nshorter compositions. To fairly evaluate the human-like\nquality of the generated music pieces, we compared MMT-\nBERT with MMT, a state-of-the-art approach whose gener-\nated compositions have lengths and musical styles that are"}, {"title": "5. CONCLUSION", "content": "In this paper, we have proposed the chord-aware symbolic\nmusic generation approach, named MMT-BERT. By ex-\ntracting chord information from raw audio files, we have\ndevised a chord-aware symbolic music representation. We\nalso developed a novel RS-GAN architecture based on\nMMT and MusicBERT. Both experimental evaluations val-\nidate the efficacy of our method in producing music pieces\nof superior quality, enhanced human likeness, and consid-\nerable length. In future works, we plan to explore methods\nthat refine musical structure and incorporate information\nfrom various musical modalities."}]}