{"title": "Unified-EGformer: Exposure Guided Lightweight Transformer for Mixed-Exposure Image Enhancement", "authors": ["Eashan Adhikarla", "Kai Zhang", "Rosaura G. VidalMata", "Manjushree Aithal", "Nikhil Ambha Madhusudhana", "John Nicholson", "Lichao Sun", "Brian D. Davison"], "abstract": "Despite recent strides made by AI in image processing, the issue of mixed exposure, pivotal in many real-world scenarios like surveillance and pho-tography, remains inadequately addressed. Traditional image enhancement tech-niques and current transformer models are limited with primary focus on either overexposure or underexposure. To bridge this gap, we introduce the Unified-Exposure Guided Transformer (Unified-EGformer). Our proposed solution is built upon advanced transformer architectures, equipped with local pixel-level refinement and global refinement blocks for color correction and image-wide ad-justments. We employ a guided attention mechanism to precisely identify exposure-compromised regions, ensuring its adaptability across various real-world condi-tions. U-EGformer, with a lightweight design featuring a memory footprint (peak memory) of only ~1134 MB (0.1 Million parameters) and an inference time of 95 ms (9.61x faster than the average\u00b3), is a viable choice for real-time applica-tions such as surveillance and autonomous navigation. Additionally, our model is highly generalizable, requiring minimal fine-tuning to handle multiple tasks and datasets with a single architecture.", "sections": [{"title": "1 Introduction", "content": "AI-driven image processing have significantly broadened the scope for enhancing vi-sual media quality. A critical challenge within the low-light image enhancement (LLIE) domain is addressing mixed exposure in images [Fig. 1 g.], where a single frame con-tains both underlit (below 5 lux, including underexposed)\u2074 and overlit (overexposed)5 regions. This issue extends beyond academic interest and has significant real-world implications. For instance, in video calls (e.g., in cafeterias; [Fig. 1 e.]) and live stream-ing, low-light enhancement is pivotal for clear visual communication. Other areas of application include autonomous driving, surveillance and security, photography, etc."}, {"title": "2 Related Work", "content": "Traditional to Advanced Deep Learning Techniques. In the realm of image en-hancement and exposure corrections, significant strides have been made to address the challenges posed by exposure scenarios. Early techniques [5,23,33] leveraged contrast-based histogram equalization (HE), laying the groundwork for more advanced methods. These initial approaches were followed by studies in Retinex theory, which focused on decomposing images into reflection and illumination maps [32,31]. The advent of deep learning transformed exposure correction, with a shift from enhancing low-light images to addressing both underexposure and overexposure [1,2,9,15,55,63,69,73]. The notable work of Afifi et al. [1] stands out, employing deep learning to simultaneously address underexposure and overexposure, a task not adequately tackled by previous methodolo-gies. There was a momentous shift towards convolutional neural network (CNN)-based methods, achieving state-of-the-art results and improving the accuracy and efficiency of exposure correction algorithms [15,29,47,51,55,64].\nAddressing the Challenges of Mixed Exposure. Despite these advancements, the challenges of mixed exposure have remained relatively unaddressed in high-contrast scenarios. Benchmark datasets such as LOL [60], LOL-[4K;8K] [57], SID [7], SICE [4], and ELD [61] offer limited mixed exposure instances, highlighting a gap in both"}, {"title": "3 Methodology: Unified-EGformer", "content": "Unifed-EGformer achieves image enhancement through an Attention Map Generation mechanism that identifies exposure adjustment regions, a Local Enhancement Block for pixel-wise refinement, a Global Enhancement Block for color and contrast adjustment,"}, {"title": "3.1 Guided Map Generation", "content": "Unified-EGformer introduces significant advancements in the attention mechanism and feed-forward network within its architecture to adeptly handle mixed exposure scenar-ios. These enhancements are encapsulated as follows:\nThresholding. To highlight the sub-regions of the images with impacted exposure problems, we need a way to point out those impacted set of pixels within the input. We use Otsu thresholding, a traditional yet effective technique. It is a global thresholding technique for automatic thresholding that works by selecting the threshold to minimize intra-class variance (variance within a class) or maximize inter-class variance (vari-ance between classes). However, this method induces granular noise in the image [49]"}, {"title": "Attention Map Generator.", "content": "The Unified-EGformer begins with a guided attention map generator, designed to iden-tify regions within an image affected by mixed exposure. This process involves gener-ating a map $M_g \\in R^{B\\times H\\times W\\times C}$, where H, W, and C represent the height, width, and channel dimensions of the input image $x \\in R^{H\\times W\\times C}$. This map, $m \\in M_g$, is used in an element-wise dot product with the image, resulting in a guided input image that undergoes underexposed, overexposed, or mixed exposure enhancement, as depicted in Fig. 4, demonstrating how we apply Otsu thresholding to get attention masks labels."}, {"title": "3.2 Unified-Enhancement", "content": "Local Enhancement Block (LEB)."}, {"title": "Global Enhancement Block (GEB).", "content": "Complementary to the LEB, the global enhancement block adjusts the image's over-all exposure through adaptive gamma correction and color balance. Unlike static bias adjustments [11], this block dynamically calculates the gamma correction factor and color transformation parameters based on the image's content. We implement a con-volutional subnetwork with GELU activations and adaptive average pooling, followed by a sigmoid to automatically compute global parameters, denoted by \u03b8. This adaptive approach to global enhancement allows for a more nuanced and content-aware balance of contrast and color. The global correction function is described as:\n$G(I) = f_\\theta(I; \\theta), I\\in R^{H\\times W\\times C}$    (2)\nwhere $G(I)$ is the globally enhanced image, and $f_\\theta$ represents the function for global adjustments, influenced by the calculated parameters \u03b8."}, {"title": "3.3 Exposure-Aware Fusion (EAF) Block", "content": "Our novel exposure aware fusion block is architecturally designed to integrate both lo-cal and global enhancement features, enabling comprehensive and cohesive image en-hancement. The fusion process begins with two convolutional layers that apply spatial filtering to extract the salient features necessary for exposure correction. We also use global average pooling, mapping the feature maps to the global context vector. These fusion weights serve as a gating mechanism to regulate the contribution of local and global features. They are adaptively learned, encapsulating both detailed texture infor-mation and broad illumination context."}, {"title": "3.4 Loss Functions", "content": "To enhance multiple aspects of image quality, our training uses a detailed loss function setup in RGB color space. It includes $L_1$ and $L_2$ losses for handling outliers and detail, SSIM for structural integrity, and VGG for semantic consistency. We also incorporate a novel MUL-ADD (MA) loss to adjust the image's contrast and brightness accurately, ensuring that the dynamic range is well represented without blurring details. The VGG loss helps match the output to high-level visual quality standards. Our combined loss function $L_{total}$, considering both local and global outputs, is detailed below:\n$C_{total}(y, \\hat{y}, I_{low}, I_{high}) = \\alpha L_1(y, \\hat{y})_{(1,9)} + \\beta L_2(y, \\hat{y}) + \\gamma L_{SSIM}(y,\\hat{y})$   (3)\n$+ \\delta L_{VGG}(y, \\hat{y}) + \\eta L_{MA}(M_L, A_L, I_{low}, I_{high} + L_{attn}(M_g(b), M)$   (4)\nwhere \u03b1, \u03b2, \u03b3, \u03b4, and \u03b7 are hyperparameters balancing the influence of each loss term, y is the ground truth, \u0177 is the predicted image, $M_L$ and $A_L$ are the multiplicative and additive components of the local block, $I_{low}$ is the low-light input, and $I_{high}$ is the target high-quality image. Our fine-tuning stage's loss equation can be presented with the physics based KL-divergence loss:\n$L_{finetune}(y, \\hat{y}, P, Q) = \\alpha L_1(y, \\hat{y}) + \\mu L_{SSIM}(y, \\hat{y}) + \\nu L_{KL}(P,Q)_{(1,g)}$  (5)"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Framework Setting", "content": "Datasets. In our study, we employ eight diverse datasets to rigorously train and eval-uate our proposed model: LOL-v1 and LOL-v2 for foundational training and testing"}, {"title": "4.2 Qualitative Results", "content": "The LOL dataset is still one of the challenging datasets even for state-of-the-art models due to its extremely low-light scenario. In Fig. 8. top, we compare recent top models, where most of the models fail to match the color of the wood pane in this case with a lower PSNR score. In Fig. 8. bottom, we show visual results for the SICE Grad dataset for the mixed-exposure task. In Fig. 12, we show a few most challenging examples where LEB and GEB alone could not manage certain cases with extreme low and bright pixels, where the EAF block helps in re-highlighting the attended features from GAMG."}, {"title": "Quantitative Comparison.", "content": "Tab. 1 reports the PSNR and SSIM scores for the U-EGFormer and U-EGFormereaf. U-EGformer demonstrates superior performance in handling both underexposure and overexposure scenarios across ME-v2 and SICE-v2 datasets, outperforming majority existing methods with significantly fewer parameters. Despite SID-L [17] with > 10M params we compare very close (with difference of 0.0398/0.05 for SSIM/PSNR), where U-EGformer is 115 times smaller network than SID-L. In Tab. 2a, we show the remarkable generalization across LOL-v1, LOL-v2, and MIT-FiveK datasets, outperforming many baselines and illustrating its robustness in exposure correction. Moreover, Tab. 2b sets new benchmarks on the challenging SICE Grad and SICE Mix datasets, underscoring its superior performance in correcting mixed exposure images. In [Fig. 7], we show a direct comparison of ssim map over the enhanced outputs between IAT and U-EGformer framework. Darker pixels in SSIM maps as seen more in IAT than in U-EGformer, indicate areas where the enhanced outputs from the two frameworks significantly differ with ground-truth. Moreover, in [Fig. 10], we emphasize the noise and the color consistency that visually seems better in U-EGformer's output."}, {"title": "Adaptable Learning Across Diverse Exposures.", "content": "Tackling the challenge of dataset di-versity, our methodology enhances transferability and adaptability of learned models. Leveraging mechanisms such as attention masks allows us to consider simultaneously varying exposure conditions. Our unified framework demonstrates enhanced general-ization capabilities, enabling effective fine-tuning across different datasets. Evidence of this robust adaptability is showcased in Tab. 2b, illustrating our model's consistent performance on varied datasets with minimal fine-tuning adjustments."}, {"title": "Ablation Study.", "content": "Our framework utilizes a data-centric approach with a smaller mem-ory footprint (~12.5 Mb) and computation alongside other strategies as we have shown through Tab. 1's '#params' column. Through Tab. 3, we show the effectiveness of each module in our framework over LOL-v2 dataset. We demonstrate that the inverse illumi-nance map, combined with the attention map and exposure-aware fusion block, achieves the best results when configured with the appropriate combination of loss functions. The first column achieves better performance on LOLv2. Additionally, Tab. 3 highlights a notable improvement in LPIPS score compared to the closest baseline [72]."}, {"title": "5 Conclusion", "content": "Our work introduces the Unified-EGformer, addressing mixed exposure challenges in images with a novel transformer model. Through specialized local and global refine-ment alongside guided attention, it demonstrates superior performance across various scenarios. Its lightweight architecture makes it suitable for real-time applications, ad-vancing the field of image enhancement and restoration. Enhancing Unified-EGFormer could involve refining the attention mechanism to become color independent to dimin-ish the influence of color artifacts in the enhanced output. Additionally, exploring the integration of lightweight state space models [14], with bi-exposure guidance offers promising avenues for further optimizing the network for efficiency and performance in image enhancement tasks."}, {"title": "6 Future Work & Discussion", "content": "The challenges presented by mixed exposure in images are not just specific to any sin-gle methodology but are a broader issue within the field of image enhancement. As illustrated in [Fig. 4], both overexposure and underexposure present unique challenges. Overexposure often results in the loss of critical details such as edges and contours, while underexposure can hide essential information in darkness. These scenarios make effective image reconstruction a complex task.\nA further complication arises when considering the standard for evaluation-what exactly constitutes an ideal ground truth in the context of mixed exposure? Often, ground truth images themselves may lack details in over-exposed areas, complicating the assessment of enhancement algorithms as can be seen in [Fig. 4]. This highlights a significant gap in our current understanding and capabilities, emphasizing the need for advancements that can precisely discern and correct varying degrees of exposure while preserving the integrity of the image details.\nThe field of mixed exposed is not new, however has limited exploration and yet far away to successfully solve the problem. Potential solutions such as Integrating Kolmogorov-Arnold Networks (KANs) [40] in place of MLP blocks in transformers can yield more efficient and explainable models. KANs' adaptive activation functions can better distinguish and process overexposed and underexposed regions, enhancing image quality.\nWhile current methods address many aspects of image quality, there is still room for improvement in creating color-independent ground truths for guided attention map gen-erators. Color-independent techniques prevent issues such as greens appearing yellow due to color influence, ensuring accurate hue representation and processing. Under-standing and correcting these color dynamics involve using attention map generators to identify inconsistencies.\nSuch methods must not only navigate the complexities introduced by mixed expo-sure but also contribute to a deeper understanding of what ideal image enhancement should entail in diverse real-world conditions."}, {"title": "7 Model Design Components", "content": "This extended material provides with more details in our model design components."}, {"title": "7.1 Algorithm", "content": "The details presented in the training are shown in the algorithm below:"}, {"title": "7.2 A-MSA", "content": "The A-MSA component can be mathematically represented as:"}, {"title": "7.3 Dual Gating Feedforward Network (DGFN).", "content": "[Eq. 7.3] The DGFN mechanism involves two parallel pathways that process the in-put features m \u2208 M\u1ef9 through distinct gating mechanisms, utilizing GELU activations ($) and element-wise products. Each path applies a sequence of convolutional transfor-mations to enrich local context, comprising a 1 \u00d7 1 convolution followed by a 3 \u00d7 3 depth-wise convolution. Moreover, depth-wise convolutions facilitate spatial informa-tion processing while significantly reducing computational costs, making them particu-larly suitable for developing a light-weight environment without compromising perfor-mance. Following Wang et al.'s, [57] approach, the outputs of these parallel pathways are then merged using element-wise summation, allowing for a comprehensive feature refinement that incorporates both detailed and global information.\nMathematically, the operation of DGFN on input features Y can be formulated as follows:\n$M = \\varphi (W^{3\\times3}_{M}) \\bigodot \\varphi (W^{1x1} W^{1x1}_{3\\times3}M) + \\Phi (W^{3\\times3}_{M}) \\bigodot \\varphi (W^{1x1} W^{1x1}_{3\\times3}M)$\nwhere $W^{1\\times1}_{3\\times3}$, $W_{1x1}$ and $W_{123}$ denote the convolutional filters in the two path-ways, and \\bigodot represents element-wise multiplication. The final output M thus combines the processed features from both pathways, ensuring that the network selectively em-phasizes informative regions for enhanced exposure correction in the attention map generation process.\nThis design allows the network to make more nuanced adjustments to the attention maps, focusing on areas of the image that require exposure correction while maintaining the integrity of well-exposed regions."}, {"title": "7.4 Model Training", "content": "For LOL-v1 [60], we adopt the standard split provided, comprising images of dimen-sions 400 \u00d7 600, with 500 images allocated for both training and evaluation. Similarly, for LOL-v2 [60], we utilize the real-world dataset for both training and evaluation, con-sisting of images of size 400 \u00d7 600, with 689 and 100 images in the respective splits. We adopt patch sizes of 256 \u00d7 256 and 324 \u00d7 324 for training across both LOL-v1 and LOL-v2 datasets. Additionally, we incorporate the MIT-Adobe FiveK [3] (FiveK)"}, {"title": "8 Extended Experimental Results", "content": ""}, {"title": "8.1 Generalization in the Wild", "content": "Our model demonstrates impressive visual qualitative results on the BAID dataset. Ini-tially trained on the LOLv2 dataset, the model was finetuned using only 300 randomly selected images from the BAID train dataset, out of the 3000 available backlit images. These BAID images were originally downsampled images and were directly used from Liang et al., [36]. The backlit images in BAID present a challenging scenario due to uneven exposure, typically featuring underexposure in the foreground and overexpo-sure in the background. Traditional approaches often struggle with these conditions, but Unified-EGformer, particularly with our Global Enhancement Block (GEB) com-bined with the Exposure Aware Fusion Block (EAF), excel by exploiting contextual features. Despite the limited finetuning data, our model significantly enhances backlit images, effectively generalizing to both indoor and outdoor scenes under diverse backlit conditions."}]}