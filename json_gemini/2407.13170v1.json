{"title": "Unified-EGformer: Exposure Guided Lightweight Transformer for Mixed-Exposure Image Enhancement", "authors": ["Eashan Adhikarla", "Kai Zhang", "Rosaura G. VidalMata", "Manjushree Aithal", "Nikhil Ambha Madhusudhana", "John Nicholson", "Lichao Sun", "Brian D. Davison"], "abstract": "Despite recent strides made by AI in image processing, the issue of mixed exposure, pivotal in many real-world scenarios like surveillance and photography, remains inadequately addressed. Traditional image enhancement techniques and current transformer models are limited with primary focus on either overexposure or underexposure. To bridge this gap, we introduce the Unified-Exposure Guided Transformer (Unified-EGformer). Our proposed solution is built upon advanced transformer architectures, equipped with local pixel-level refinement and global refinement blocks for color correction and image-wide adjustments. We employ a guided attention mechanism to precisely identify exposure-compromised regions, ensuring its adaptability across various real-world conditions. U-EGformer, with a lightweight design featuring a memory footprint (peak memory) of only ~1134 MB (0.1 Million parameters) and an inference time of 95 ms (9.61x faster than the average\u00b3), is a viable choice for real-time applications such as surveillance and autonomous navigation. Additionally, our model is highly generalizable, requiring minimal fine-tuning to handle multiple tasks and datasets with a single architecture.", "sections": [{"title": "1 Introduction", "content": "AI-driven image processing have significantly broadened the scope for enhancing visual media quality. A critical challenge within the low-light image enhancement (LLIE) domain is addressing mixed exposure in images [Fig. 1 g.], where a single frame contains both underlit (below 5 lux, including underexposed)\u2074 and overlit (overexposed)5 regions. This issue extends beyond academic interest and has significant real-world implications. For instance, in video calls (e.g., in cafeterias; [Fig. 1 e.]) and live streaming, low-light enhancement is pivotal for clear visual communication. Other areas of application include autonomous driving, surveillance and security, photography, etc."}, {"title": "2 Related Work", "content": "Traditional to Advanced Deep Learning Techniques. In the realm of image enhancement and exposure corrections, significant strides have been made to address the challenges posed by exposure scenarios. Early techniques [5,23,33] leveraged contrast-based histogram equalization (HE), laying the groundwork for more advanced methods. These initial approaches were followed by studies in Retinex theory, which focused on decomposing images into reflection and illumination maps [32,31]. The advent of deep learning transformed exposure correction, with a shift from enhancing low-light images to addressing both underexposure and overexposure [1,2,9,15,55,63,69,73]. The notable work of Afifi et al. [1] stands out, employing deep learning to simultaneously address underexposure and overexposure, a task not adequately tackled by previous methodologies. There was a momentous shift towards convolutional neural network (CNN)-based methods, achieving state-of-the-art results and improving the accuracy and efficiency of exposure correction algorithms [15,29,47,51,55,64].\nAddressing the Challenges of Mixed Exposure. Despite these advancements, the challenges of mixed exposure have remained relatively unaddressed in high-contrast scenarios. Benchmark datasets such as LOL [60], LOL-[4K;8K] [57], SID [7], SICE [4], and ELD [61] offer limited mixed exposure instances, highlighting a gap in both"}, {"title": "3 Methodology: Unified-EGformer", "content": "Unifed-EGformer achieves image enhancement through an Attention Map Generation mechanism that identifies exposure adjustment regions, a Local Enhancement Block for pixel-wise refinement, a Global Enhancement Block for color and contrast adjustment, and an Exposure Aware Fusion (EAF) block that fuses features from both enhancements for balanced exposure correction Fig. 2."}, {"title": "3.1 Guided Map Generation", "content": "Unified-EGformer introduces significant advancements in the attention mechanism and feed-forward network within its architecture to adeptly handle mixed exposure scenarios. These enhancements are encapsulated as follows:\nThresholding. To highlight the sub-regions of the images with impacted exposure problems, we need a way to point out those impacted set of pixels within the input. We use Otsu thresholding, a traditional yet effective technique. It is a global thresholding technique for automatic thresholding that works by selecting the threshold to minimize intra-class variance (variance within a class) or maximize inter-class variance (variance between classes). However, this method induces granular noise in the image [49]\ndue to the non-uniform pixels in low lux regions. The noise is highlighted by the resultant masks as shown in Fig. 3, and will influence the subsequent exposure correction.\nTo mitigate noise, we implemented adaptive thresholding using pixel blocks and downsampled images. We further reduced noise creep with nearest neighbor down-sampling and Gaussian blur. Integrating Charbonnier loss [6] into our attention map mechanism encouraged smoother transitions in areas of high gradient variance, specifically targeting denoising. This component, combined with the SSIM loss that is applied directly on the input, synergistically contributes to noise reduction.\nOur implementation of threshold selection is as follows. First, we calculate the Otsu average across the training set to establish a baseline for automatic thresholding. We then apply this average threshold to each image in the dataset. Using a data set-specific threshold, this method ensures a more uniform application of the Otsu method.\nAttention Map Generator.\nThe Unified-EGformer begins with a guided attention map generator, designed to identify regions within an image affected by mixed exposure. This process involves generating a map $M_g \\in R^{B \\times H \\times W \\times C}$, where H, W, and C represent the height, width, and channel dimensions of the input image $x \\in R^{H \\times W \\times C}$. This map, $m \\in M_g$, is used in an element-wise dot product with the image, resulting in a guided input image that undergoes underexposed, overexposed, or mixed exposure enhancement, as depicted in Fig. 4, demonstrating how we apply Otsu thresholding to get attention masks labels."}, {"title": "3.2 Unified-Enhancement", "content": "Local Enhancement Block (LEB)."}, {"title": "3.3 Exposure-Aware Fusion (EAF) Block", "content": "Our novel exposure aware fusion block is architecturally designed to integrate both local and global enhancement features, enabling comprehensive and cohesive image enhancement. The fusion process begins with two convolutional layers that apply spatial filtering to extract the salient features necessary for exposure correction. We also use global average pooling, mapping the feature maps to the global context vector. These fusion weights serve as a gating mechanism to regulate the contribution of local and global features. They are adaptively learned, encapsulating both detailed texture information and broad illumination context."}, {"title": "3.4 Loss Functions", "content": "To enhance multiple aspects of image quality, our training uses a detailed loss function setup in RGB color space. It includes $L_1$ and $L_2$ losses for handling outliers and detail, SSIM for structural integrity, and VGG for semantic consistency. We also incorporate a novel MUL-ADD (MA) loss to adjust the image's contrast and brightness accurately, ensuring that the dynamic range is well represented without blurring details. The VGG loss helps match the output to high-level visual quality standards. Our combined loss function $L_{total}$, considering both local and global outputs, is detailed below:\n\\begin{equation}\nC_{total}(y, \\hat{y}, I_{low}, I_{high}) = \\alpha L_{1}(y, \\hat{y})_{(1,9)} + \\beta L_{2}(y, \\hat{y}) + \\gamma L_{SSIM}(y,\\hat{y})\n\\end{equation}\n\\begin{equation}\n+ \\delta L_{VGG}(y, \\hat{y}) + \\eta L_{MA}(M_L, A_L, I_{low}, I_{high} + L_{attn}(M_g(b), M)\n\\end{equation}\nwhere $\\alpha$, $\\beta$, $\\gamma$, $\\delta$, and $\\eta$ are hyperparameters balancing the influence of each loss term, y is the ground truth, $\\hat{y}$ is the predicted image, $M_L$ and $A_L$ are the multiplicative and additive components of the local block, $I_{low}$ is the low-light input, and $I_{high}$ is the target high-quality image. Our fine-tuning stage's loss equation can be presented with the physics based KL-divergence loss:\n\\begin{equation}\nL_{finetune}(y, \\hat{y}, P, Q) = \\alpha L_{1}(y, \\hat{y}) + \\mu L_{ssIM}(y, \\hat{y}) + \\nu L_{KL}(P,Q)_{(1,g)}\n\\end{equation}"}, {"title": "4 Experiments", "content": "4.1 Framework Setting\nDatasets. In our study, we employ eight diverse datasets to rigorously train and evaluate our proposed model: LOL-v1 and LOL-v2 for foundational training and testing with real-world and synthetic images; Multiple-Exposure ME-v2 tailored for diverse exposure scenarios; SICE, including the SICE-Grad and SICE-Mix subsets for gradient and mixed-exposure challenges, respectively; and MIT-FiveK for benchmarking against professionally retouched images. LOL-v1 [60] contains 500 image pairs with 485 and 15 for training and testing datasets, where each image has a resolution of $(3 \\times 600 \\times 400)$. LOL-v2 is divided into real and synthetic subsets with detailed configurations for training/testing (with 689 and 100 images for real-world); in the BAcklit Image Dataset (BAID) dataset [43], we only use 380 randomly selected training images from Liang et al. [36] and utilize the complete 368 2K resolution images from the test set.\nTraining Strategy. In tackling the mixed exposure challenge in image processing, our approach adopts a pre-training stage and a finetuning stage as shown in Fig. 2. We engage in pre-training using our custom loss function, $C_{total}$ (Eq. 3.4), which combines several loss components with individually set hyperparameters for input-output pairs. In the finetuning phase, we refine the model with a physics-based pixel-wise reconstruction loss function tailored to camera sensors obeying Poisson distribution P [59] (more details are in the supplementary material).\nBoth stages of training leverage a combination of loss functions, which are detailed in the following subsection. This systematic progression from foundational learning to focused refinement helps to address the complexities inherent in mixed exposure challenges. (More details can be found in the supplementary material.)"}, {"title": "4.2 Qualitative Results", "content": "The LOL dataset is still one of the challenging datasets even for state-of-the-art models due to its extremely low-light scenario. In Fig. 8. top, we compare recent top models, where most of the models fail to match the color of the wood pane in this case with a lower PSNR score. In Fig. 8. bottom, we show visual results for the SICE Grad dataset for the mixed-exposure task. In Fig. 12, we show a few most challenging examples where LEB and GEB alone could not manage certain cases with extreme low and bright pixels, where the EAF block helps in re-highlighting the attended features from GAMG."}, {"title": "5 Conclusion", "content": "Our work introduces the Unified-EGformer, addressing mixed exposure challenges in images with a novel transformer model. Through specialized local and global refinement alongside guided attention, it demonstrates superior performance across various scenarios. Its lightweight architecture makes it suitable for real-time applications, advancing the field of image enhancement and restoration. Enhancing Unified-EGformer could involve refining the attention mechanism to become color independent to diminish the influence of color artifacts in the enhanced output. Additionally, exploring the integration of lightweight state space models [14], with bi-exposure guidance offers promising avenues for further optimizing the network for efficiency and performance in image enhancement tasks."}, {"title": "Adaptable Learning Across Diverse Exposures", "content": "Tackling the challenge of dataset diversity, our methodology enhances transferability and adaptability of learned models. Leveraging mechanisms such as attention masks allows us to consider simultaneously varying exposure conditions. Our unified framework demonstrates enhanced generalization capabilities, enabling effective fine-tuning across different datasets. Evidence of this robust adaptability is showcased in Tab. 2b, illustrating our model's consistent performance on varied datasets with minimal fine-tuning adjustments."}, {"title": "Ablation Study", "content": "Our framework utilizes a data-centric approach with a smaller memory footprint (~12.5 Mb) and computation alongside other strategies as we have shown through Tab. 1's '#params' column. Through Tab. 3, we show the effectiveness of each module in our framework over LOL-v2 dataset. We demonstrate that the inverse illuminance map, combined with the attention map and exposure-aware fusion block, achieves the best results when configured with the appropriate combination of loss functions. The first column achieves better performance on LOLv2. Additionally, Tab. 3 highlights a notable improvement in LPIPS score compared to the closest baseline [72]."}, {"title": "6 Future Work & Discussion", "content": "The challenges presented by mixed exposure in images are not just specific to any single methodology but are a broader issue within the field of image enhancement. As illustrated in [Fig. 4], both overexposure and underexposure present unique challenges. Overexposure often results in the loss of critical details such as edges and contours, while underexposure can hide essential information in darkness. These scenarios make effective image reconstruction a complex task.\nA further complication arises when considering the standard for evaluation-what exactly constitutes an ideal ground truth in the context of mixed exposure? Often, ground truth images themselves may lack details in over-exposed areas, complicating the assessment of enhancement algorithms as can be seen in [Fig. 4]. This highlights a significant gap in our current understanding and capabilities, emphasizing the need for advancements that can precisely discern and correct varying degrees of exposure while preserving the integrity of the image details.\nThe field of mixed exposed is not new, however has limited exploration and yet far away to successfully solve the problem. Potential solutions such as Integrating Kolmogorov-Arnold Networks (KANs) [40] in place of MLP blocks in transformers can yield more efficient and explainable models. KANs' adaptive activation functions can better distinguish and process overexposed and underexposed regions, enhancing image quality.\nWhile current methods address many aspects of image quality, there is still room for improvement in creating color-independent ground truths for guided attention map generators. Color-independent techniques prevent issues such as greens appearing yellow due to color influence, ensuring accurate hue representation and processing. Understanding and correcting these color dynamics involve using attention map generators to identify inconsistencies.\nSuch methods must not only navigate the complexities introduced by mixed exposure but also contribute to a deeper understanding of what ideal image enhancement should entail in diverse real-world conditions."}, {"title": "7 Model Design Components", "content": "This extended material provides with more details in our model design components."}, {"title": "7.1 Algorithm", "content": "The details presented in the training are shown in the algorithm below:"}, {"title": "7.2 A-MSA", "content": "The A-MSA component can be mathematically represented as:"}, {"title": "7.3 Dual Gating Feedforward Network (DGFN).", "content": "[Eq. 7.3] The DGFN mechanism involves two parallel pathways that process the input features $m \\in M$ through distinct gating mechanisms, utilizing GELU activations ($\\phi$) and element-wise products. Each path applies a sequence of convolutional transformations to enrich local context, comprising a $1 \\times 1$ convolution followed by a $3 \\times 3$ depth-wise convolution. Moreover, depth-wise convolutions facilitate spatial information processing while significantly reducing computational costs, making them particularly suitable for developing a light-weight environment without compromising performance. Following Wang et al.'s, [57] approach, the outputs of these parallel pathways are then merged using element-wise summation, allowing for a comprehensive feature refinement that incorporates both detailed and global information.\nMathematically, the operation of DGFN on input features $Y$ can be formulated as follows:\n\\begin{equation}\nM = \\phi\\left(\\frac{W^{(1)}_{M}}{3 \\times 3}\\right)  \\frac{W^{(1)}_{1x1} M} + \\Phi\\left(\\frac{W^{(2)}_{M}}{3 \\times 3}\\right)  \\frac{W^{(2)}_{1x1} M\\}\n\\end{equation}\nwhere $\\frac{W^{(1)}}{1 \\times 1}, \\frac{W^{(1)}}{3 \\times 3}, \\frac{W^{(2)}}{3 \\times 3}$, and $\\frac{W^{(2)}}{1 \\times 1}$ denote the convolutional filters in the two pathways, and $\\square$ represents element-wise multiplication. The final output M thus combines the processed features from both pathways, ensuring that the network selectively emphasizes informative regions for enhanced exposure correction in the attention map generation process.\nThis design allows the network to make more nuanced adjustments to the attention maps, focusing on areas of the image that require exposure correction while maintaining the integrity of well-exposed regions."}, {"title": "7.4 Model Training", "content": "For LOL-v1 [60], we adopt the standard split provided, comprising images of dimensions 400 \u00d7 600, with 500 images allocated for both training and evaluation. Similarly, for LOL-v2 [60], we utilize the real-world dataset for both training and evaluation, consisting of images of size 400 \u00d7 600, with 689 and 100 images in the respective splits. We adopt patch sizes of 256 \u00d7 256 and 324 \u00d7 324 for training across both LOL-v1 and LOL-v2 datasets. Additionally, we incorporate the MIT-Adobe FiveK [3] (FiveK)"}, {"title": "8 Extended Experimental Results", "content": "8.1 Generalization in the Wild\nOur model demonstrates impressive visual qualitative results on the BAID dataset. Initially trained on the LOLv2 dataset, the model was finetuned using only 300 randomly selected images from the BAID train dataset, out of the 3000 available backlit images. These BAID images were originally downsampled images and were directly used from Liang et al., [36]. The backlit images in BAID present a challenging scenario due to uneven exposure, typically featuring underexposure in the foreground and overexposure in the background. Traditional approaches often struggle with these conditions, but Unified-EGformer, particularly with our Global Enhancement Block (GEB) combined with the Exposure Aware Fusion Block (EAF), excel by exploiting contextual features. Despite the limited finetuning data, our model significantly enhances backlit images, effectively generalizing to both indoor and outdoor scenes under diverse backlit conditions."}]}