{"title": "Personalized Top-k Set Queries Over Predicted Scores", "authors": ["Sohrab Namazi Nia", "Subhodeep Ghosh", "Senjuti Basu Roy", "Sihem Amer-Yahia"], "abstract": "This work studies the applicability of expensive external oracles such as large language models in answering top-k queries over predicted scores. Such scores are incurred by user-defined functions to answer personalized queries over multi-modal data. We propose a generic computational framework that handles arbitrary set-based scoring functions, as long as the functions could be decomposed into constructs, each of which sent to an oracle (in our case an LLM) to predict partial scores. At a given point in time, the framework assumes a set of responses and their partial predicted scores, and it maintains a collection of possible sets that are likely to be the true top-k. Since calling oracles is costly, our framework judiciously identifies the next construct, i.e., the next best question to ask the oracle so as to maximize the likelihood of identifying the true top-k. We present a principled probabilistic model that quantifies that likelihood. We study efficiency opportunities in designing algorithms. We run an evaluation with three large scale datasets, scoring functions, and baselines. Experiments indicate the efficacy of our framework, as it achieves an order of magnitude improvement over baselines in requiring LLM calls while ensuring result accuracy. Scalability experiments further indicate that our framework could be used in large-scale applications.", "sections": [{"title": "1 INTRODUCTION", "content": "In several emerging applications that lie at the intersection of databases and machine learning, there is a need for efficient top-k queries, where the scoring function is user-defined. For example, consider a web search query asking for \"Hotels with Unique Decor in Manhattan.\" The task is to select the top-k hotels from a database of user-reviewed entities, based on a scoring criterion related to \"unique decor.\" Since the database may not directly include attributes such as \"unique decor,\" determining such a score becomes challenging. By analyzing user data, this information can be inferred. External oracles/experts, like Large Language Models (LLMs), can provide these predictions, but querying LLMs is costly. In this paper, we propose a framework that intelligently selects the next best question to ask the oracle, such that the answer maximizes the likelihood of accurately identifying the true top-k set.\nThe main focus of query processing over ML models has been to ensure efficiency without compromising accuracy [7, 14, 28]. Most existing work relies on sampling to reduce the cost of oracle calls in query processing [7, 9, 15, 18]. For instance, the work in [7] proposes to solve the problem of minimizing oracle usage for finding answers that meet a precision or recall target with provable statistical guarantees while achieving a precision or recall target. To the best of our knowledge, no existing work has tackled the problem of answering personalized top-k set based queries with arbitrary user-defined scoring functions over multi-modal data. Specifically, there is no work that explores how to minimize the number of LLM calls required to score potential top-k sets, where the scoring function is decomposed into a set of constructs, and the score for each construct can be predicted by LLMs.\nRunning Example. We assume a hotel database consisting of 5 unique entities, i.e., hotels as shown in Table 2. Each entity is represented using multiple items of different modalities (image/audio/text based reviews). Given a query (e.g., unique decor hotels in Manhattan), the goal is to identify a small set of 3 (k = 3) hotels that are most suitable.\nThe process leverages a user-defined set-based scoring function to identify the top-3 hotels. We consider scoring functions that are decomposable into constructs, such as, relevance, diversity, serendipity [4, 10, 13, 17], etc. Wlog, an example scoring function with l = 2 constructs, would be used to compute the top-k set that maximizes the sum of relevance and diversity $F(s, q) = \\sum_{e_i \\in s}Rel(q, e) + \\sum_{e_i,e_j \\in s}Div(e_i, e_j)$. The constructs of the scoring function in this example are unary (e.g., relevance) and binary (e.g., diversity). However, we must support constructs of arbitrary arity. Since the data is multimodal and the query is personalized, our framework aims to study the applicability of LLMs to retrieve scores of some entities for some scoring construct.\nChallenges. The overarching challenge is to support any set-based queries with user-defined scoring functions while minimizing the number of calls made to LLMs. The goal is to return the exact answer. Imagine a snapshot of the process - where at a given point in time, partial scores of the set of possible top-k sets (each such set is called a candidate) are computed. Based on those partial scores, the key challenge is to develop a principled \"model\" that quantifies the likelihood of each candidate being the actual top-k answer. The actual query answer therefore is a probability distribution function (pdf), where each candidate is a possible outcome and has a probability associated. Designing this model is non-trivial as computing the probability of a candidate being the actual top-k is not independent from other sets that share common entities with it. Given the probabilistic model, the next question to the LLM should be the one that minimizes uncertainty in the current pdf. Therefore, a challenge is to be able to model \"uncertainty\" in the current pdf and identify the next question that minimizes it.\nContributions. We propose a computational framework that works for arbitrary user-defined scoring functions, as long as the scoring function can be decomposed into a set of constructs for which LLM calls could be invoked. At a given point in time, the framework has access to a set of M candidates (each containing k entities), one of which will eventually become the final query answer. The framework can be summarized into 4 well-defined tasks:\nTask 1: Computing score bounds of candidate sets. At a given time, the framework knows either the partial or the full score of a candidate. When only partial score of a candidate is known, it computes the lower and upper bounds of a candidate's score.\nTask 2: Probabilistic model for finding the answer. This task makes a significant contribution by introducing a principled probabilistic model that estimates the likelihood of each candidate being the true top-k. When the score of a candidate is only partially known, it is treated as a discrete pdf with values uniformly distributed between the lower and upper bounds. The likelihood that one candidate having a higher score than the other is computed using max-convolution of pdfs [24]. This task considers two cases: computing max-convolution assuming independence between candidates, and accounting for dependence between candidates when they share common entities. Ultimately, this task formalizes the probability of each candidate being the winner or the actual answer. The algorithms developed for this task address both memory and computational efficiency, carefully considering potential bottlenecks in their design.\nTask 3: Determining the next question. The query answer is modeled as a random variable with M possible outcomes (one per candidate), and Task 2 formalizes how to compute the pdf of each outcome. The most widely used mathematical framework for quantifying uncertainty is entropy [25], with Shannon entropy in information theory being the most common, and the one we adopt in this work. Therefore, the next question to ask the oracle is the one that maximally reduces the uncertainty of this random variable (when entropy is 0, no further question needs to be asked). We make no further assumption about the possible responses to a question asked to the LLM and design the algorithm for this task with an emphasis on computational efficiency.\nTask 4: Response processing. This task involves aggregating responses from the oracle. This depends on whether the oracle provides a discrete answer or a range.\nFor each of these tasks, we study computational challenges and design efficient solutions.\nEmpirical Evaluations. We present a comprehensive experimental evaluation of our framework using three large datasets involving images, reviews, and structured data and three user-defined scoring functions. We implemented multiple baseline solutions as appropriate. Our experimental results indicate the efficacy of the proposed probabilistic models, as it achieves an order of magnitude improvement over baselines in requiring LLM calls while ensuring the achieved results are fully accurate. We measure the number of LLM calls and find that our method for determining the next question significantly reduces the cost compared to baselines. We also find that while the cost increases with an increasing k or number of candidates, our scalability experiments indicate that the proposed models and algorithms could be used inside large-scale applications.\nThe rest of the paper is organized as follows: Section 2 presents our data model and problem definitions. Sections 3 and 4 contains our framework and its algorithms. We provide experimental evaluations in Section 5. Related work is summarized in Section 6. Several extensions of the proposed work are discussed in Section 7 and we conclude in Section 8."}, {"title": "2 DATA MODEL & PROBLEM DEFINITION", "content": "2.1 Data Model\nInput Query: A user writes a query q with an input parameter k, with the goal of obtaining the top-k set of entities to q.\nIn our running example, q = \"Hotels with Unique Decor in Manhattan\" and we need to find the top-3 hotels with respect to q.\nDatabase. The query comes to a database D containing data of different modalities, such as, pictures, text, audio, etc. Item i represents a physical entity e (such as a hotel). There may be multiple items associated with an entity. Let E represent the set of unique entities, such that, |E| = n.\nFigure 1 contains a snapshot of our example with 5 entities or hotels. Each entity has two associated items. For instance, the hotel \"HNY\", has { i1, i3 }, an audio and an image respectively.\nScoring function and characteristics. A user provides a set-based scoring function F that admits a query q and an integer k, and scores any subset s \u2208 entities, such that |s| = k, i.e., F(s, q) \u2192 R. A scoring function needs to satisfy two conditions: (a) it is set-based, meaning it computes the score of a set of size k and need not provide the individual order of entities in the set, (b) it is decomposable into l constructs each of which could be sent to an oracle (in our case LLM) to obtain its predicted value."}, {"title": "2.2 Formalism", "content": "Given a scoring function F, we define the set of all possible questions, denoted as Qu, as the union of f decomposable constructs instantiated with input data, as appropriate."}, {"title": "3 PROPOSED FRAMEWORK", "content": "Depending on the scoring function F the framework identifies one, several, or all candidate top-k sets as answers to a query q. Wlog, we assume a set C of m such candidate sets and present 4 essential tasks to solve our problem.\nTask 1. Computing Score Bounds of Candidate Sets. At a given point in time, only the partial score of a candidate c \u2208 C is known. Technical Problem: Lower and upper bounds of score of c. Given F, and known information QK, compute the lower (resp. upper) bound of score of c as the smallest (resp. largest) score of c. Considering our example, to find the lowest and highest possible scores of $c_1$, we should replace known values from Table 2 and Table 3 in $F_{min}(C_1, q)$ and $F_{max}(C_1, q)$ and substitute unknown values in the formula with minimum and maximum possible response values, which are assumed to be 0 and 1 respectively. Therefore, we can compute the minimum and maximum overall scores of $c_1$ as follows:\n$F_{min}(c_1, q) = Rel(HNY, q)+Rel(MLN, q)+0+Div(HNY, MLN)+0+0$\n$F_{min}(c_1, q) = 0.5 + 1.0 + 0 + 0.5 + 0 + 0 = 2.0$\n$F_{max}(c_1, q) = Rel(HNY, q)+Rel(MLN, q)+1+Div(HNY, MLN)+1+1$\n$F_{max}(c_1, q) = 0.5 + 1.0 +1 +0.5 + 1 + 1 = 5.0$\nHence, the lower bound (LB) and upper bound (UB) of $c_1$ are:\n$(LB, UB)_{c_1} = (2.0, 5.0)$\nTask 2: Probabilistic Model for Finding the Answer. Given a set C of m candidate top-k sets, the probability P(c) represents that candidate c is the answer ($c^*$) of the query. Technical Problem: The probability of a candidate c being the query answer $c^*$ is:\n$P(c = c^*) = P(\\bigcap_{c_j \\in C} F(c, q) > F(c_j, q))$\nIndependence among candidates. In the simplest case, each candidate has unique entities with no entity in common across any two candidates. The joint probability could be calculated as:\n$P(c = c^*) = \\prod_{i=1}^M P (F(c, q) \\geq F (c_i, q))$\nDependence among candidates. When there exist entities that are common across multiple candidates, the probabilistic model capturing a candidate being the winner (or query answer) needs to account for conditional probabilities, as follows:\n$P(c = c^*) = \\prod_{i=1}^M P (F(c, q) \\geq F (c_i, q) | \\bigcap_{j=1}^{i-1} F(c, q) \\geq F (c_j, q))$\nTask 3: Determining the Next Question. For a candidate c, P(c) represents the probability of c being the answer. Given the set of candidates C, the actual answer $c^*$ is thus a random variable with probability distribution A, representing the probability of each candidate being the answer. Definition 3.1. Uncertainty in A. The uncertainty in $c^*$ is modeled as the entropy [25] in A, as follows: $H(A) = - \\sum_{c \\in C}P(c) log(P(c))$\nwhere P(c) is the probability of candidate c to be the query answer. Technical Problem: Selecting the next best question. Let H(A) be the entropy associated with the query answer $c^*$. When Q \u2208 Qu is provided by the oracle, let $H(A')$ be the reduced entropy, select Q\u2208 Qu such that H(A) \u2013 H(A') is maximized. In other words, maximizing the difference between H(A) \u2013 H(A') is equivalent to minimizing H(A'). Entropy measures the uncertainty associated with the query answer - thus, minimizing this enhances predictability. In fact, when the entropy is 0, the query answer could be decided with complete certainty. Using our running example with the three candidates, C = {$C_1, C_2, C_3$}, the associated entropy is 0.604. In Section 4.3, we will propose algorithms that select the next question to ask s.t. entropy is minimized. Minimizing entropy will minimize uncertainty in finding the query answer $c^*$. Given the running example, we shall show that our solution will choose Q = Div(HNY, MLN) as the next question. Given the LLM returns Div (HNY, MLN) = 1, H(c*) = 0 and $c_1$ becomes the query answer.\nTask 4: Response Processing. The final task is to process the obtained response from the oracle. There are two obvious possibilities: a. the oracle returns a discrete value, b. the oracle returns a range. There are also other types of responses such as aggregating multiple discrete oracle responses, or aggregating multiple oracle responses each providing a range. We study the first case closely in this work. In Section 4.4, we discuss how a discrete response (e.g., 0.7) from a single oracle is processed. In Section 7, we discuss other alternatives and how they could be adapted."}, {"title": "4 ALGORITHMS", "content": "We are now ready to provide algorithms that solve the tasks described in Section 3."}, {"title": "4.1 Computing Score Bounds of Candidate Sets", "content": "Given the set of candidates, we discuss algorithms to compute score bounds of each of them. Since only partial scores of the candidates are known, this process computes the score bounds $(LB_c, UB_c)$ of each candidate c. These bounds are calculated based on F (c, q) and the known information Qk and are updated when Qk is updated. Given c, the process looks at the constructs in F(c, q), and uses the actual scores for the parts that could be obtained from Qk. For the rest, it uses the minimum possible score to produce $LB_c$ and the maximum possible score to produce $UB_c$. Without any further assumption in place, this gives tight bounds. To simplify exposition, let us assume that the minimum and the maximum relevance and diversity scores of each construct is within (0, 1). On the other hand, as shown above, there exists a total of 6 constructs contributing to the overall score of a candidate. Therefore, initially, the LB and UB of any candidate is 0 and 6, respectively. Now, let us assume Rel(WLD, q) is obtained and Rel(WLD, q) = 0.5. Hence, we need to update LB and UB of those candidates containing WLD. As an example, $c_2$ now becomes $F(c_2, q) = Rel(HNY, q) + Rel(MLN, q) + Rel(WLD, q) + Div(HNY, MLN) + Div(HNY, WLD) + Div(MLN, WLD) = Rel(HNY, q) + Rel(MLN, q) + 0.5 + Div (HNY, MLN) + Div(HNY, WLD) + Div(MLN, WLD)\u21d2 0.5 \u2264 F (c2, q) \u2264 5.5$"}, {"title": "4.2 Probabilistic Model for Finding the Answer", "content": "The algorithm designed for this task, namely finding $c^*$, needs to calculate the probability that the score of $c^*$ is larger than the scores of all other candidates, as defined in Equation 1. We present algorithms for two variants - ProbInd assumes independence among candidates and ProbDep accounts for potential dependencies among candidates. Since the score of each candidate is a uniform probability distributions within (LB,UB), our solution requires to compute max convolution of probability distributions [24], as defined below. Definition 4.1. (Max-Convolution of Distributions). Assume that f(c) and g($c_1$) are the pdfs of the two independent random variables c and $c_1$ respectively. The pdf of the random variable Max(c, $c_1$) is the max convolution of the two pdfs and is calculated as follows: P (F($c_1$, q) \u2265 F ($c_2$, q)) = $2_{x,x_1}P(c = x) \u00d7 P(c_1 = x_1), x \u2208 [LB_c,UB_c], x_1 \u2208 [LB_{c_1},UB_{c_1}] : x \u2265 x_1$\nCase A - Independence among candidates Given the set C of M candidates, if every candidate contains entities that are only present in that candidate, the probability of a candidate c being the winner, i.e., P(c = $c^*$) is the joint probability of c being larger than every other candidate, and could be calculated using Equation 2. Algorithm ProbInd does that. Consider an imaginary example of the following kind C = {$C_1, C_2, C_3$}, such that $c_1$ = {1, 2, 3}, $c_2$ = {$e_4$, 5, 6}, $c_3$ = {$e_7$, $e_8$, $e_9$}. The probability of $c_1$ being the answer could be expressed as follows:\nP($c_1$ = $c^*$) = P (F($c_1$, q) \u2265 F ($c_2$, q)) \u00d7 P (F($c_1$, q) \u2265 F ($c_3$, q))\nSpecifically, the score of each candidate c is within the range (LB_c, UB_c). Algorithm ProbInd treats the score of each candidate c as a uniform distribution within (LB_c, UB_c) with m discrete values, therefore, the probability of P (F($c_1$, q) \u2265 F ($c_i$, q)) could be calculated using the maximum convolution of two probability distributions [24]. LEMMA 4.2. ProbInd takes (M\u00b2m) running time, where M is the number of candidates, and m is the number of discrete values of the pdf designating the score of candidate c. PROOF. (sketch.) Computing Max-convolution of any two pdfs takes (m) times. Therefore, computing the probability of a candidate c being the winner takes (Mm) time. ProbInd repeats this process on each candidate and therefore takes \u0398(M\u00b2m) times. \u03a0 Case B - Dependence among candidates Algorithm ProbDep is designed to identify the likely query answer from the candidate set, when the candidates have entities in common. It uses Equation 3 for that. Consider the running example C = {$C_1, C_2, C_3$}. Clearly, HNY exists in all 3 candidates, MLN exists in both $c_1$ and $c_2$, as well as HYN exists in both $c_1$ and $c_3$. Two challenges immediately emerge: a. computational and storage bottleneck to compute the winning probability of a candidate using Equation 3. b. disregarding the effect of common entities in the probability computation. Using Equation 3, if there are M candidates, the probability of a candidate being the winner is conditioned on as many as M - 1 terms. Generally, given M candidates, where the score of each candidate is a pdf with m discrete values, there are mM possible combinations of scores. A naive implementation of ProbDep needs to first identify which combinations satisfy the conditions expressed and computes the probability accordingly. This approach has a space complexity of O(mM), which becomes prohibitively expensive as M or m increases. 4.2.1 Avoiding memory and computational bottleneck. We therefore study storage and computational efficiency in designing ProbDep. Algorithm 1 contains the pseudocode. Recall Equation 4 and note that P(c = $c^*$) could be calculated as a sequence of pairwise probabilities. Instead of storing all O(mM) combinations, ProbDep decomposes the overall computation and performs it in pairwise steps. Given the overall formula for P($c_1$ = $c^*$): The algorithm runs iteratively and at step i, it computes:\nP(F($c_1$, q) \u2265 F ($c_2$, q), F ($c_1$, q) \u2265 F ($c_3$, q), ..., F ($c_1$, q) \u2265 F ($c_{i+1}$, q))\nby using probabilities calculated from step (i - 1), which is of the following form.\nP(F($c_1$, q) \u2265 F ($c_2$, q), F ($c_1$, q) \u2265 F ($c_3$, q), ..., F ($c_1$, q) \u2265 F ($c_i$, q))\nOnce step i is complete, the algorithm does not keep track of any past information from step i - 1 anymore, and only maintains the results obtained from the latest step i. The process continues until all M steps are complete. Using our running example, P($c_2$ = $c^*$) is calculated in two steps: the algorithms first performs max-convolution to compute P (F($c_2$, q) \u2265 F($c_1$, q)). In the next step, this computed information is then used to calculate P (F($c_2$, q) \u2265 F($c_3$, q) | (F($c_2$, q) \u2265 F($c_1$, q)). These two aforementioned probabilities are multiplied to produce P($c_2$ = $c^*$).\nDisregarding the effect of unknown common entities. Given two candidates $c_i$ and $c_j$, ProbDep needs to eliminate the effect of common unknown questions between $c_i$ and $c_j$ to compute the probability P(F($c_i$, q) \u2265 F($c_j$, q)). As an example, $c_1$ and $c_2$ have one unknown question in common, which is R(HNY, q). ProbDep assigns R(HNY, q) = 0 which means it has no effect on the score bounds of $c_1$ and $c_2$ particularly for computing P(F($c_2$, q) \u2265 F ($c_1$, q)). Hence, assuming R(HNY, q) = 0, the new bounds are:\n$c_1$ : (3.5, 4.5),\n$c_2$ : (2.5, 3.5)\nP(F($c_2$, q) \u2265 F ($c_1$, q)) =\nHaving P(F($c_2$, q) \u2265 F ($c_1$, q)) computed, the algorithm needs to next compute P(F($c_2$, q) \u2265 F ($c_3$, q) | F($c_2$, q) \u2265 F ($c_1$, q)) to get the final probability of P($c_2$ = $c^*$). However, ProbDep now eliminates the effect of common unknown questions between $c_2$ and $c_3$, which is R(HNY, q). Hence, assuming it is zero, the new bounds are: $c_2$ : (2.5, 3.5), $c_3$ : (3.0, 4.0)\nThis gives P(F($c_2$, q) \u2265 F ($c_3$, q) | F($c_2$, q) \u2265 F ($c_1$, q)) =\nFinally, these are multiplied to obtain P($c_2$ = $c^*$). Similarly, the probabilities P($c_1$ = $c^*$) and P($c_3$ = $c^*$) can be computed.\nIt could be shown that Algorithm ProbDep will produce\n$\\{$\\begin{matrix} 0.75 \\\\ 0.24 \\\\ 0.01 \\end{matrix}$$\\}$$\\begin{matrix} c = c_1, \\\\ c = c_2, \\\\ c = c_3. \\end{matrix}$\nLEMMA 4.3. ProbDep takes (M\u00b2m\u00b2) running time and O(m\u00b2) space. PROOF. (sketch.) Computing Max-convolution of two pdfs takes \u0398(m\u00b2) times. Therefore, computing the probability of a candidate c being the winner takes (Mm\u00b2) time. ProbDep repeats this process on each candidate and therefore takes (M\u00b2m\u00b2) times. \u03a0"}, {"title": "4.3 Determining the Next Question", "content": "In subsection 4.2, we discussed algorithms for the probabilistic model for finding the answer of the query. Formally speaking, the query answer is a random variable with M possible outcomes (each per candidate), and their probability could be computed using ProbDep or ProbInd. We use entropy as a measure of the uncertainty associated with this random variable as follows:\n$H(c^*) = - \\sum_{i=1}^n p(c_i = c^*) log p(c_i = c^*)$\nThe entropy of $C_1, C_2, C_3$ in the running example is H(c*) = - (0.75 log(0.75) + 0.01 log(0.01) +0.24 log(0.24)) = 0.604\nThe next best question should therefore be the one that minimizes entropy (ideally makes it 0). However, the challenge is to select this question from Qu without any further assumption about the response received from LLM. Algorithm EntrRed is designed for this task (Algorithm 2 has the pseudocode). This algorithm leverages ProbDep or ProbInd and first identifies the candidate (let c+ be that candidate) that has the highest probability to be the winner. Given Qu, it first narrows down to a smaller subset Qu, that involve c\u207a. It leverages a subroutine QEF (Subroutine 3) to quantify the effect of every question Q\u2208 Qu, and then selects Q\u2208 Qu, that has the maximum score associated. Subroutine QEF works as follows: For every Q \u2208 Qu', let CQ represent the subset of all candidates whose scores are influenced by Q, and Co be the other candidates. The score assigned to the question Q is the sum of absolute difference between winning probability of candidates in CQ and winning probability of candidates in Co. Intuitively, consider a pair of candidates co \u2208 Co and co e Co. if |P($c_Q$ = c*) \u2013 P($c\u00b4_o$ = c*)| is high, it means that among $c_o$ and $c\u00b4_o$, one has a much higher winning probability compared to the other which means the score bounds of $c_o$ and $c\u00b4_o$ have a small overlap, and asking Q as a question that influences the score of only one of the two candidates, can potentially diverge their bounds, and make one of the two candidates certainly better than the other, which will prune out one of the candidates. On the other hand, if Q influences both candidates, asking it will affect score bounds of both candidates in the same way, and will not result in pruning one of them. This is why for a given Q, we divide candidates by two groups based on if they are influenced by Q or not, and then consider the winning probability difference between candidates from the two groups. The higher each pairwise difference in winning probability is, the more valuable the Q is since it becomes more likely to prune out candidates from the two groups by asking Q."}, {"title": "4.4 Response Processing", "content": "For a given question Q, a discrete responser \u2208 [MIN, MAX], within the specified range [MIN, MAX], from a single oracle could be a normalized floating point number representing a score value (e.g. 0.7). Upon receiving r, the score bounds of each candidate c that contains Q are updated as follows: for the lower bound, substitute MIN by r, and for the upper bound substitute MAX by r."}, {"title": "5 EXPERIMENTAL EVALUATION", "content": "We present the effectiveness of our framework using two key metrics: M1 Quality of the solution, which assesses the framework's ability to reduce costs (i.e., number of LLM calls) compared to alternative approaches; and M2 Scalability of each component, which analyzes the time trends as the candidate space grows for each component. 5.1 Experimental Setup Experiment Settings. Algorithms are implemented in Python 3.11.1, utilizing GPT-40 mini as the oracle/expert in our framework. Experiments are conducted on Wulver (NJIT's HPC cluster using 6 nodes) with a 2.45 GHz AMD EPYC 7753 processor and 512 GB RAM (per node). The code and data are publicly available. Results are averaged over 10 runs. 5.1.1 Applications. We have three use cases: hotels, movies, and Yelp businesses. In all cases, the input comprises a user's query, k, and personalized definitions for each scoring function construct (relevance and diversity). Each LLM prompt requests either the relevance (or diversity) score of an entity (or pair of entities). We use six scoring functions (two per use case) with distinct relevance and diversity definitions as shown in Table 4. Prompts are carefully designed to include all necessary unstructured data, i.e., the user's query, relevance and diversity definitions, and unstructured data associated with the related entity (or entities). We use the LangChain framework to ensure that the LLM returns scores in our desired normalized floating-point format. 1) Top-k Hotels. This is a large dataset [5] of hotels, containing detailed descriptions of 719,218 hotels sourced from websites, travel agencies, and review platforms. These descriptions serve as the unstructured data associated with hotel entities that we use in our prompts. Two scoring functions with distinct relevance and diversity definitions are used, F\u2081 and F2 defined in Table 4. 2) Top-k Movies. This is the Wikipedia Movie Plots\u00b3, which includes 33,869 movies with plot descriptions from Wikipedia pages. These plots form the unstructured data associated with movie entities in our prompts. Two scoring functions with distinct relevance and diversity definitions are used, F3 and F4 defined in Table 4. 3) Top-k businesses in Yelp dataset. This is the Yelp dataset, which contains various attributes for 150,346 businesses (e.g., restaurants), including reviews and images. We used a bundle of reviews and images as the associated unstructured data for each business entity in our prompts. LangChain is used for image processing with LLMs. Two scoring functions with distinct relevance and diversity definitions are used, F5 and Fo defined in Table 4. 5.1.2 Implemented Algorithms. There does not exist any related work that studies the problem that we do. However, we still design baselines that are appropriate. *** Baseline. This approach makes all LLM calls to calculate the exact score for all candidates without maintaining bounds. It then selects the highest scoring candidate. *** EntrRed using ProbDep. The original version of our framework, which does not assume independence among candidates and accounts for their dependencies in the computation. *** EntrRed using ProbInd. A time-efficient variation of our framework that assumes independence among candidates, enabling more efficient computation of the probability distribution function (pdf). *** Random. This approach maintains score bounds. However, instead of computing a pdf and determining the next question based on that, it selects the next best question randomly. 5.1.3 Measures. Number of LLM Calls. The primary objective of our framework is to identify the top-k set with the highest score while minimizing the cost of using LLMs as oracles/experts. The number of LLM calls serves as a key indicator of cost. We hence compared the total number of LLM calls for Random vs. EntrRed using ProbInd. We also compared EntrRed using ProbDep vs. EntrRed using ProbInd to investigate if computing probabilities considering dependence using EntrRed using ProbDep can result in reducing cost more effectively compared to EntrRed using ProbInd. Recall. We evaluate recall of the top-k produced by our designed solutions wrt baseline. Time Taken for Each Component. We measured the total time taken by the different algorithms and the time taken for each of the four components of our framework."}, {"title": "5.2 Results", "content": "The top-k set produced by our proposed algorithms achieve 100% recall always, as expected."}, {"title": "5.2.1 Cost Experiments", "content": "Given a fixed number of candidates n", "Random": "Figure 3 compares the cost between our framework with Random across the 6 scoring functions defined over the 3 datasets", "observations": "Our method for determining the next question significantly reduces the cost (by an order of magnitude) compared to the Random algorithm. As expected", "ProbInd": "Here, we focus on comparing the two variants of our proposed probabilistic models discussed in Section 4.2. Figure 4 compares the costs of EntrRed using ProbInd versus EntrRed using ProbDep. For brevity, we only present a subset of results that are representative (additional results could be found in our technical report [2"}, {}]}