{"title": "A Multi-Agent Framework for Extensible Structured Text Generation in PLCs", "authors": ["DONGHAO YANG", "AOLANG WU", "TIANYI ZHANG", "LI ZHANG", "FANG LIU", "XIAOLI LIAN", "YUMING REN", "JIAJI TIAN"], "abstract": "Programmable Logic Controllers (PLCs) are microcomputers essential for automating factory operations. Structured Text (ST), a high-level language adhering to the IEC 61131-3 standard, is pivotal for PLCs due to its ability to express logic succinctly and to seamlessly integrate with other languages within the same standard. However, the inherent flexibility of the language and the varying requirements of different industries have led vendors to develop their own customized versions of ST. Additionally, the lack of comprehensive and standardized documentation for the full semantics of ST has contributed to inconsistencies in how the language is implemented. Consequently, the steep learning curve associated with ST, combined with ever-evolving industrial requirements, presents significant challenges for developers. In response to these issues, we present AutoPLC, an LLM-based approach designed to automate the generation of vendor-specific ST code. To facilitate effective code generation, we first built a comprehensive knowledge base, including Rq2ST and Instruction libraries, including equirements and corresponding code implementations, and fundamental instructions available in vendor technical documents. Then we developed a retrieval module to incorporate the domain-specific knowledge by identifying pertinent cases and instructions, guiding the LLM to generate code that meets the requirements. In order to verify and improve the quality of the generated code, we designed an adaptable code checker capable of identifying syntax and semantic errors in ST variants. If errors are detected, we initiate an iterative self-improvement process to instruct the LLM to revise the generated code. We evaluate AutoPLC's performance against seven state-of-the-art baselines using three benchmarks, one for open-source basic ST and two for commercial Structured Control Language (SCL) variants from Siemens. The results show that our approach consistently achieves superior performance across all benchmarks. An ablation study further emphasizes the significance of the combination of our knowledge base and RAG and the extensible checker in producing high-quality ST code. Further manual analysis also confirms the practical utility of the ST code generated by AutoPLC.", "sections": [{"title": "1 Introduction", "content": "Programmable Logic Controllers (PLCs) are the cornerstone of Industrial Control Systems (ICSs) within the broader field of Industrial Automation. They play a pivotal role in the operation and management of critical infrastructure across diverse sectors such as energy, manufacturing, and transportation. The significance of PLCs is further evidenced by their growing market projection, with forecasts anticipating a rise to USD 12.20 billion by 2024 and USD 15.12 billion by 2029, marking a robust Compound Annual Growth Rate (CAGR) of 4.37% from 2024 to 2029 [29, 61].\nWithin this thriving market, the importance of Structured Text (ST)\u2014a programming language governed by the IEC 61131-3 standard [19]-cannot be overstated. The syntax and structure of ST are similar to high-level languages such as C or Fortran, which eases the learning curve for developers who may be new to industrial automation but have experience with these programming languages. Furthermore, ST enables the incorporation of more complex algorithms and data processing tasks within PLCs [65].\nDespite the benefits ST offers, developers may face difficulties due to its complex syntax and the strict timing constraints that are characteristic of this language. Compounding these challenges is the fact that existing documentation fails to provide a comprehensive explanation or definition of ST's complete features [4]. Official manuals tend to introduce language capabilities with only a handful of examples, which are often insufficient for readers to gain a thorough understanding of the language [63]. Moreover, variability in the language's implementation by different vendors\u2014such as Allen-Bradley [54] and Siemens\u2014to suit their specific devices exacerbates the steepness of the learning curve and increases the likelihood of errors in vital industrial automation projects [63].\nManual programming in ST requires not only an in-depth understanding of the language as it pertains to the target devices from a specific manufacturer but also a comprehensive grasp of the complex logic that governs industrial processes. This dual requirement poses a significant challenge for developers striving to maintain precision and efficiency throughout large-scale automation endeavors. In light of these issues, the need for automatic ST code generation becomes increasingly critical.\nThe evolution of code generation methodologies holds considerable importance, particularly in the era of pre-trained Large Language Models (LLMs). These innovative techniques have proven effective in addressing simple coding tasks, as demonstrated by benchmarks such as CoNaLa [68] and MBPP [5]. Current research efforts increasingly aim to tackle more complex and practical coding challenges, as evidenced by benchmarks like SWE-bench [33] and VersiCode [67]. Notably, the majority of these advancements have been concentrated on mainstream programming languages such as Java and Python. This focus is largely due to the availability of vast training datasets on platforms like GitHub. The effectiveness of LLMs is fundamentally dependent on the availability of extensive training data. Regrettably, there is a pronounced scarcity of comprehensive datasets specifically for ST, posing a significant challenge to ST code generation and thus constraining the performance of existing methods. Despite this limitation, strides have been made in generating ST code: LLM4PLC has fine-tuned LLMs with public datasets to produce ST code [18], while Koziolek and Koziolek [38] have utilized retrieval-augmented generation techniques to incorporate proprietary function block libraries into their outputs. However, current research on ST code generation remains impractical due to the following two gaps:\n\u2022 Customization of Standard ST: Various groups have adapted their versions of ST to ensure better compatibility with their devices, either by modifying the syntax or by enhancing the libraries. For example, Siemens AG's Structured Control Language (SCL) exhibits several distinct characteristics compared to CODESYS's ST [11]. Two notable examples illustrate the differences of such customizations: In Siemens's SCL, the END_FOR and END_IF statements must be followed"}, {"title": "2 Background: ST Programming for PLC", "content": "PLCs are indispensable devices in industrial automation, widely used in mechanical control and process management across industrial environments such as refineries, power plants, and paper mills. PLCs execute predetermined control logic to process real-time input signals from sensors (temperature, pressure, flow, etc.) and correspondingly control the actions of actuators (pumps, motors, valves, etc.). The complexity of this logic ranges from simple Boolean operations to sophisticated optimization algorithms [7].\nST (Structured Text), as a mainstream PLC programming language, shares syntactic similarities with Pascal and C. It supports advanced programming structures including IF-THEN-ELSE, FOR loops, and can be extended to object-oriented programming paradigms, supporting features like classes and inheritance [62]. ST programs execute in PLCs through a fixed periodic cycle, which consists of three key stages:\n(1) Input Scanning: PLCs read the status of all physical input terminals and store the data in input image registers.\n(2) Program Scanning: Execute ST code, processing logical operations and control algorithms.\n(3) Output Updating: Write processing results to output image registers and update physical output terminal states.\nST holds a central position in PLC programming. It supports a rich set of data types and modularity mechanisms, enabling engineers to write structured and maintainable control programs [19]. Its ease of distribution and modification makes it particularly well-suited for the complex and dynamic requirements of modern PLC systems. Furthermore, even using the graphical languages for PLCs, such as LD, FBD and SFC, some parts of the program still need to be written in ST [63].\nCurrently, the industrial control market is predominantly led by major PLC manufacturers such as Siemens, Mitsubishi, and Rockwell [61]. While these vendors all implement ST language following the IEC 61131-3 standard, each has developed its unique ecosystem. Differences are evident not only in the range of supported basic data types (e.g., GX Works2 by Mitsubishi Electric supports 10 data types while CODESYS supports 17 [63]) but also extend to instruction sets, library functions, and development toolchains.\nThis market segmentation significantly impacts ST program development processes. Engineers must deeply adapt to vendor-specific platform characteristics during programming, as ignoring these features can not only lead to program inefficiency but also potentially cause serious safety hazards. Each platform has developed unique advantageous features, such as proprietary hardware-based simulation environments, targeted compilation optimization, and safety checking mechanisms. This requires engineers to thoroughly study platform-specific documentation and strictly follow"}, {"title": "3 Approach: AutoPLC", "content": "To address the adaptability and reliability challenges in the domain of automatic PLC programming, we propose AutoPLC, an autonomous ST programming framework. As seen in Figure 1, AutoPLC consists of a language infrastructure InfraPLC, and a generation framework GenPLC. InfraPLC includes three core components: Rq2ST case library, Instruction Library, and code checker tool, all of which are semi-automatically constructed from diverse data sources. GenPLC leverages the language resources provided by InfraPLC and generation capabilities of LLMs to achieve the end-to-end PLC programming."}, {"title": "3.1 Overview", "content": "To address the adaptability and reliability challenges in the domain of automatic PLC programming, we propose AutoPLC, an autonomous ST programming framework. As seen in Figure 1, AutoPLC consists of a language infrastructure InfraPLC, and a generation framework GenPLC. InfraPLC includes three core components: Rq2ST case library, Instruction Library, and code checker tool, all of which are semi-automatically constructed from diverse data sources. GenPLC leverages the language resources provided by InfraPLC and generation capabilities of LLMs to achieve the end-to-end PLC programming."}, {"title": "3.2 InfraPLC: Language Infrastructure", "content": ""}, {"title": "3.2.1 Rq2ST Library", "content": "To facilitate efficient and effective code generation, we build the Rq2ST library, which comprises requirements and corresponding code implementations. This serves as a comprehensive knowledge base for facilitating knowledge transfer and ensuring code quality assurance.\nThe library comprises 914 code snippets collected from the following three rigorously reviewed sources that accurately reflect genuine domain requirements: vendor-specific libraries, open-source function libraries, and competition datasets.\n(1) OSCAT Library [51] (718 cases): An open-source library widely used in industrial automation, offering modules for mathematical operations, logic control, and data processing. Its reliability is ensured through an active developer community and long-term maintenance. We choose the CODESYS version since since it is well-maintained."}, {"title": "3.2.2 Instruction Library", "content": "Correctly using specific instructions is crucial in the process of generating accurate ST code. In actual applications, PLC experts often refer to official documentation and learn relevant code to ensure proper use of instructions. To achieve automated integration of instructions in the code pipeline, we collect and store these instructions in a machine-readable formatted information. Specifically, we extract instruction information from vendor technical documents, including system manual, online help website and instruction case code library. To enhance the readability of the description, we further incorporate the GPT-40 to summarize the function of the instruction as well as provide application scenarios. Each instruction of our instruction library contains the following fields:\n(1) Name: The name of instruction.\n(2) Description: Functional description of each instruction and application scenarios summarized by GPT-40.\n(3) Parameters: The input and output parameters for each function, including parameter name, type and explanation, etc."}, {"title": "3.2.3 Code Checker", "content": "In software development, code verification is a crucial step in ensuring software quality. While open-source ST verification tools like MATIEC [15] and IECChecker [35] exist, they struggle to meet the complex requirements of various variants in modern industrial environments. On the other hand, although commercial programming platforms provide checking functionality for specific ST implementations, their closed-source nature makes it difficult to integrate them into automated code pipelines [59]. Therefore, we developed a new flexible and cost-effective code checker for ST variants used in modern industrial environments. Specifically, we designed a parsing solution based on EBNF [30] grammar definitionand used ANTLR [52] as the parsing engine to build the code checker. As shown in Algorithm 1, our checker takes ST code S as input and sequentially performs multiple checking procedures and returns checking feedback message M, divided into two parts - syntax analysis and semantic analysis:\nSyntax Analysis: We customized the ANTLR parser based on open-source ST grammar to identify syntax errors in code and provide comprehensive error reports. As shown in Procedure 1, given ST code, our customized parser will analyze the code and construct an abstract syntax tree (AST). When errors are discovered during analysis, the parser provides the detailed syntax error message (as shown in Table 2), including error ID, the token where the error occurred (check program termination), a context window of the interrupted token, and the specific grammar rules that were not satisfied. Then, our parser bypasses the error token and continues the parsing process. Once the analysis is completed, a detailed syntax error message $M_1$ along with an AST will be returned.\nSemantic Analysis: For semantic analysis, we customized an ANTLR Walker to perform a depth-first traversal of the AST produced by syntax analysis process (Procedure 1) to check semantic errors. As shown in Procedure 2, we implemented a semantic error checking for three primary data types, including functions, variables, and statements. For functions, we focus on checking the function calls by confirming that functions being invoked are already defined or provided the instruction library and that the parameters provided match the function's declared parameter specifications. For variables, we mainly check whether they have been properly initialized before their usage. For statements, we perform basic validation to ensure that the expressions used within them are legally formed. For example, in SCL, applying Typeof to non-IF and CASE statements is illegal. Once the semantic analysis is completed, a detailed semantic error message $M_2$ will be returned.\nGiven that ST variants typically follow the IEC-61131 standard and share similar AST structures, our code checking tool can be easily adapted to different ST variants. Through updating the EBNF grammar and making moderate modifications to our customized AST Walker, our tool could achieve promising verification capabilities tailored to the specific ST variant."}, {"title": "3.3 GenPLC: ST Generation", "content": "As shown in Figure 1, GenPLC follows a multi-agent collaborative paradigm[31] and includes three stages: \u25cf Retrieving similar ST cases and corresponding instructions from the Rq2ST and"}, {"title": "3.3.1 Language Knowledge Retrieval", "content": "Given that LLMs have show strong in-context learning capabilities, and can transfer knowledge from given examples to the current task [8, 12, 16, 21, 49], we design a RAG-style retriever to locate valuable information from our established knowledge base, i.e., Rq2ST and Instruction libraries, and use them to enhance the prompt for LLMs [26, 53]. Specifically, we first retrieve similar cases from Rq2ST, then retrieve the detailed information of the instructions Di used in the retrieved cases from the instruction library. To achieve efficient retrieval, we first use vector retrieval to obtain top-k candidate cases C from Rq2ST library. The query Q is formulated using the problem requirement, including the task description and the parameter information, and then return top-k similar cases based on the similarity score, resulting in C:\n$C = Top-K(cos(Embedding(Rq2ST), Embedding(Q)))$ (1)\nTo further select the relevant cases from the candidate set C, we instruct LLM with Preorder to further reorder the candidate case list based on the functionality of each code case and its contribution to the current coding task. Preorder uses a chain reasoning design: First instructs the LLM to summarize the functionality of each case and analyze its contribution to the current task according to the task requirements, then sorts the cases ensuring that the most relevant and beneficial cases are prioritized.\n$SortedCases = LLM(C, Preorder)$ (2)\nFor each case in SortedCases, we further retrieve its corresponding instruction detail Di from our instruction library, and produce the final output $C_{final}$, serving as the domain-specific knowledge to support generation in the next stage.\n$C_{final} = \\{(case^{req}_1, case^{code}_1, D_1), (case^{req}_2, case^{code}_2, D_2), ..., (case^{req}_n, case^{code}_n, D_n)\\}$ (3)"}, {"title": "3.3.2 Code Generation", "content": "In the code generation stage, we constructed an information-rich contextual prompt P to guide the LLM to generate code that meets the requirements, as illustrated in Figure 1. P consists of two parts: $P_{knowledge}$ and $P_{task}$. $P_{knowledge}$ provides domain-specific coding knowledge, including programming guidance and instruction details. The programming guidance PG is customized for the specific ST language, including coding standards and formats that need to be followed. The instruction details are the union of Di retrieved from section 3.3.1. $P_{task}$ consists of retrieved $(case_{req}, case_{code})$ tuples as mentioned in section 3.3.1 and the current problem requirement Q.\nIncluding similar cases' implementation details in the prompt could activate relevant knowledge in the LLM, enabling it to better handle code details in the specific ST language context. For example, due to PLC's cyclic execution characteristics, a typical challenge is using local variables to maintain states and process them correctly in new cycles. Such coding practices and unique PLC programming patterns are usually not directly provided in the requirements, making the model struggle to follow during code generation, especially when the model has not been explicitly trained to recognize and learn various PLC patterns. Thus, providing relevant cases can guide the model to follow specific coding styles and patterns that conform to project practices, thus generating code that is easy to maintain and understand. After P is constructed, we feed it into the LLM and extract the generated ST draft code S for further refinement and improvement of next stage:"}, {"title": "3.3.3 Self Improvement", "content": "In this stage, we use our customized code checker (presented in section 3.2.3) to analyze the previous generated draft ST code S. If the errors are detected, our checker will provide detailed error information of the ST code M, and an iterative improvement process is initiated.\nDuring the process, AutoPLC will instruct the LLM to analyze the error causes and correlations between various errors (if multiple errors are present), and then generate the corresponding patch given the error information and the ST code. Specifically, we instruct the LLM with $P_{fix}$ to combine its own knowledge to fully understand the code context to determine the cause of the error, rather than simply relying on the error feedback at the interruption point. To improve the repair efficiency and reduce the risk of dead loops, we provide all error information at once, rather than performing independent repairs for each error point. We use an enhanced chain reasoning prompt to improve the correctness of the patches, first listing specific repair recommendations, and then generating the corresponding code patches.\n$M = CodeChecker(S)$\n$H = LLM(S, M, P_{fix})$\n$S' = ApplyPatch(S, H)$ (5)\nWe further use regular expressions to extract the relevant code segments and patches from LLM's response H, and perform replacement operations on the code in turn to produce the revised code S'. Then the revised code will be checked in the subsequent self-improvement iteration, and the iteration process will terminate when no errors are detected or the maximum number of iterations i is reached."}, {"title": "4 Evaluation", "content": ""}, {"title": "4.1 Research Questions", "content": "RQ1 (Effectiveness Evaluation): What is the effectiveness of AutoPLC in generating PLC code across different ST variants when compared to baselines?\nRQ2 (Ablation Study): How significantly do our retrieval module and self-improvement component contribute to the effectiveness of code generation?\nRQ3 (Manual Evaluation): How do developers perceive the quality of code generated by AutoPLC?"}, {"title": "4.2 Datasets and Metrics", "content": ""}, {"title": "4.2.1 Datasets", "content": "Given that ST code is commonly tied to real-world devices, industrial enterprises often adhere to strict confidentiality to safeguard their codebases. Consequently, publicly available datasets for ST code generation are virtually nonexistent. Nonetheless, we observed that a recent study by Fakih et al. (2024) construct their benchmark from public ST libraries [18], but did not make their benchmark data accessible to the public. This lack of transparency has been a catalyst for our work.\nIn response, we build our benchmark datasets based on the Rq2ST library. We instruct the model to generate code from Rq2ST requirements. This choice is well-justified for several reasons: (1) the"}, {"title": "5 Results and Analysis", "content": ""}, {"title": "5.1 Addressing RQ1: Effectiveness Evaluation", "content": "The comparative results for all baselines, along with our AutoPLC, across the three benchmarks, are presented in Table 3. The data clearly shows that AutoPLC consistently outperforms other models in both metrics across all benchmarks. The second-best approach is Claude-3.5, which also serves as our foundational model. Notably, our method achieves remarking increases in both metrics across the three benchmarks than Claude-3.5. In particular, the Passed Ratio increases by 14.54% for OSCAT, 42.39% for LGF, and 33.33% for the Competition benchmark. Regarding the Average Errors, we observe reduction percentages of 37.15%, 62.50%, and 59.57% respectively. DeepSeek-V2.5 claims the third rank, surpassing GPT-40 at all three benchmarks. Though in LGF, DeepSeek-V2.5's Passed Ratio is lower, it still records a lower Average Errors.\nIt should be noted that our implementation of LLM4PLC utilizes GPT-40 as its foundational LLM. However, different from the findings obtained in the LLM4PLC paper, we observe that GPT-40 outperforms LLM4PLC in generating both ST and SCL code. As mentioned in Section 4.4, instead of introducing manual intervention of the verification, our replication of LLM4PLC employs an automatic modification loop by integrating MATIEC as syntax checker and SMV as the formal verifier. The integration of these checking tools constitutes the primary distinction between LLM4PLC and GPT-40.\nAfter analyzing the results, we have identified certain limitations with MATIEC; it occasionally flags syntactically correct code as erroneous. This results in misleading feedback to GPT-40, which in turn reduces overall efficiency. Besides, the backbone LLM is tasked with generating a formal model for verification purposes; however, this proves to be a considerable challenge. Based on our observations, GPT-40 has not been successful in producing formal models that meet the required"}, {"title": "5.2 Addressing RQ2: Ablation Study", "content": "In this section, our objective is to assess the influence of two key features on the generation of ST and SCL code by our AutoPLC. To achieve this, we constructed two modified versions of AutoPLC, each with either the Retrieval module or the Self-improvement module omitted. It is important to note that the Retrieval module operates in conjunction with our two libraries: Rq2ST library and Instruction library. Therefore, the exclusion of the Retrieval module effectively means the simultaneous removal of these two libraries from the system.\nThe outcomes are presented in Table 4. To quantify the extent of these variations, we include the change ratios accompanied by directional arrows and red figures to signify the degree of reduction or increase, respectively. We can make the following observations from Table 4.\n\u2022 Both critical modules remarkably enhance ST and SCL code generation. The importance of these modules is underscored by the observed decrease in the Passed Ratio and an increase in the Average Errors upon their exclusion."}, {"title": "5.3 Addressing RQ3: Manual Evaluation", "content": "Till now, we have assessed quantitative measures by evaluating the compilation pass ratio and the count of errors for each snippet of generated code. However, we aim to delve deeper into understanding to what extent can the generated code be practically useful in real engineering scenarios? To explore this question, we have carried out a user study where participants are invited to manually"}, {"title": "6 Discussion", "content": ""}, {"title": "6.1 Error Types Analysis", "content": "Besides our approach, we also analyzed the error types of Claude, our backbone model, and LLM4PLC, the compound pipeline approach, just like ours. To thoroughly assess the capabilities and limitations of baseline methods and our AutoPLC approach in generating ST and SCL code, we scrutinized the error profiles of each method across various benchmarks. We categorized the errors into four common types: undefined errors, mismatch errors, call errors, and type transition errors. Additionally, we also accounted for other types of errors, such as invalid code, redefinitions, invalid variables, and more.\nWe calculated the average frequency of each error type per approach for every benchmark, which is detailed in Table 5. To facilitate visualization, we employed a color-coding scheme: a light green background to highlight the most frequent error type, light red for the second-most prevalent, and light gray for the third.\nIn comparing the top three error types across all benchmarks, we noted significant overlaps. Notably, undefined error and mismatch error are predominant, often emphasized with a colored background. In particular, undefined error emerged as the leading error type in all three benchmarks,"}, {"title": "6.2 Threats to Validity", "content": "Internal Validity. The primary threat to internal validity stems from the construction of the Rq2ST Archive. Due to the inherently limited number of cases in the PLC domain and their dynamic nature, we crafted the archive by extracting three benchmarks, excluding cases that are either identical or suggestively similar to the test requirements. Consequently, there is a slight possibility that semantically equivalent or analogous cases to the testing requirements may remain. Given"}, {"title": "6.3 Limitations", "content": "Relatively Shallow Semantic Checking. In our current work, customized checker is designed to identify three type of semantic errors frequently made by LLMs. These include: 1) Incorrect Function Call; 2) Undefined Variable; and 3) Illegal Statements. Further code improvement could benefit from advanced time-sensitive and safety-critical executable semantics checks, such as verifying deterministic execution, resource usage constraints, and hardware configuration along with I/O mapping.\nLimited Assessment of Functional Correctness, Safety, and Reliability. Throughout our evaluation, we concentrate on metrics such as the Passed Compilation Ratio and the Average Error Number during quantitative analysis (pertinent to RQ1 and RQ2). In addressing RQ3, annotators evaluate the generated code's usefulness, normalization, modifiability, and safety. However, our evaluation does not include a thorough assessment of functional correctness, safety, and reliability using more formal and rigorous methodologies.\nComplicated Code Logic, Including complex Device Control in Larger Project Contexts, Is Not Addressed. Relatively, the tasks in our three benchmarks are easy, although multiple API calls are involved in a few cases. More intricate control logic, such as interlocking mechanisms to prevent simultaneous operations and recipe management for batch processing, is not extensively investigated in our current work."}, {"title": "7 Related Work", "content": ""}, {"title": "7.1 Code Generation for PLC", "content": "Automating the generation of programs given a specification has been an active topic in both academia and industry for years. In the context of PLC program generation, early efforts primarily relied on formal specification methods, pre-defined rules, and human-defined features. For example, Steinegger and Zoitl [60] presented a rule-based PLC code generator based on knowledge extraction from engineering artifacts and their corresponding tool database. Darvas et al. [13] proposed a PLC code generation approach based on formal specification. Specifically, they first introduced PLCspecif, a formal specification method tailored for PLC programs, and subsequently present a PLC code generation approach that leverages a systematic mapping between the semantics of the specification and the PLC code. These methods, while effective in certain scenarios, require extensive human intervention, making them costly, and their generation capabilities are limited, lacking adaptability and flexibility.\nRecent advancements in Large Language Models (LLMs) have significantly improved code generation [9, 50]. Codex [9], a pioneering generative model with 12 billion parameters, powers GitHub Copilot and has transformed coding experiences. Inspired by its success, numerous models emerged, including AlphaCode [44], CodeWhisperer [2], InCoder [20], Code Llama [55], CodeRL [39], Code-Gen [47, 48], StarCoder [43], DeepSeek-Coder [23], and OpenAI's GPT series. These models achieve substantial advancements in code generation. Furthermore, Haag et al. [24] introduced a method for refining LLM training through iterative compiler feedback, improving compilation success rates and semantic precision.\nDespite the success of LLMs, existing evaluations of their code generation capabilities have primarily concentrated on widely utilized programming languages like Python and Java. Motivated by the success of LLMs in general programming language generation, researchers are now investigating LLMs for PLC programming. Koziolek et al. [36] assessed ChatGPT's ability to generate IEC 61131-3 Structured Text (ST) code across 10 prompt categories, demonstrating syntactic correctness and reasoning potential. Also, they propose an innovative LLM-based method for generating IEC 61131-3 ST code from Piping and Instrumentation Diagrams (P&IDs) through image recognition in 2024 [38]. The feasibility is validated through three cases. These approaches facilitate progress in ST code generation. Nevertheless, the effectiveness of these techniques remains less than ideal. Koziolek et al. [36] mention that ChatGPT and other LLMs do not produce outputs deterministically. In our experiments, the compilation success rate generated by GPT-40 was also low, at 35.47%.\nTo improve the quality of generated PLC code, Fakih et al. [18] proposed LLM4PLC, a user-guided iterative pipeline that incorporates user feedback and external verification tools to direct the LLM to generate PLC code. However, given that the efficacy of this method heavily relies on valid human input, and individual capabilities differ, it leads to variability in the approach's stability. Some researchers explored the integration of RAG modules. Koziolek et al. [37] leverages proprietary and well-tested functions blocks to support code generation, significantly speeding up typical programming tasks. In addition, Liu et al. [45] presented Agents4PLC, a multi-agent framework based on LLMs for PLC code generation, combining RAG and the feedback from third-party checkers.\nHowever, the effectiveness of RAG largely depends on high-quality retrievers and knowledge bases. Currently, RAG-based PLC code generation efforts lack publicly available high-quality knowledge bases, making it challenging to generalize these methods. Additionally, these approaches are typically focused on a single language (such as CODESYS ST), limiting their applicability across diverse PLC programming environments. Moreover, the absence of a unified and reasonable benchmark makes it difficult to objectively compare and evaluate the performance of different"}, {"title": "7.2 Enhancing LLM for Improved Code Generation", "content": "Despite remarkable capabilities, LLMs often encounter challenges when generating correct code, particularly for complex problem-solving tasks and domain specific code generation scenarios. To tackle this issue, researchers have proposed various techniques to enhance LLM's code generation capability. One widely adopted approach for performance enhancement is the use of prompting techniques, where the prompts provided to LLMs are improved to effectively guide the models in generating the desired outputs. Chain of thought (CoT) [64] prompt, which directs LLMs to initially generate reasoning chains prior to producing the final output, has demonstrated its effectiveness in enhancing the quality of the generated code. Li et al. [42] proposed Structured Chain-of-Thought Prompting (SCoT), which incorporates program structures to constraint LLMs to think about how to solve requirements from the view of source code, and further improve the correctness of generated code. Jiang et al. [32] designed a self-planning prompt for code generation, which first generated plans for the programming task by providing a few demonstrations, and then guided LLM to generate code that adheres to the intended steps, step by step, based on the created plans. Retrieval augmented generation (RAG) techniques, which retrieve and incorporates relevant context into the prompt, are also widely used for improving LLMs' performance on knowledge-intensive tasks, including code generation. Zhang et al. [69] proposed RepoCoder, which first incorporated a retriever and and a language model to search for relevant code snippets from the repository, and then utilized the retrieved information to generate code through an iterative process of retrieval and generation. Cheng et al. [10] introduced a dataflow-guided retrieval augmentation approach for repository-level code completion, aiming to retrieve background knowledge that is more relevant to the unfinished code. Wu et al. [66] focused on improving the retrieval efficiency by proposing a selective RAG framework to avoid retrieval when unnecessary.\nMulti-agent collaboration approaches are also demonstrated to be effective in enhancing the performance of code generation tasks. Islam et al. [31] introduced a multi-agent Code generation framework MapCoder, which consists of four LLM agents tasked with recalling relevant examples, planning, code generation, and debugging. To enhance the generation process, they designed a structured pipeline schema that allows these agents to work collaboratively. Dong et al. [17] also presented a self-collaboration code generation framework, enabling efficient handling of complex requirements and enhancing code generation performance."}, {"title": "8 Conclusion", "content": "In the implementation of ST code within industrial automation settings, human engineers encounter significant learning hurdles. These challenges stem from inadequate manual documentation [4], inconsistencies in ST implementations across various vendors [63], and the dynamic nature of industry requirements [14, 40]. Moreover, the scarcity of publicly available resources further complicates the research into automated approaches [18, 36, 37, 45]. In our current research, we introduce AutoPLC, an innovative approach designed to automatically generate vendor-specific ST code based on NL requirements. To address the limitation of available public resources, we have developed two libraries: one comprising successful cases (including requirements and code), and another containing a public instruction library for the specific ST language variant. We have engineered a RAG mechanism to efficiently pinpoint pertinent cases and beneficial constructs within these libraries to facilitate effective code production. Additionally, we have implemented a self-improvement cycle featuring an integrated syntax and semantic checker tailored to our ST variant. This checker delivers feedback on the generated code, which is then analyzed by the underlying LLM to refine the output further.\nWe established three benchmarks encompassing CODESYS' ST and Siemens' SCL. Our experimental results underscore the superiority of AutoPLC over seven leading-edge approaches, particularly regarding the passed compilation ratio and the reduced number of errors. The importance of the retriever (alongside the libraries) and the self-improvement process (i.e., the checker) has been confirmed through ablation studies. Notably, the combined design of libraries and retriever proves indispensable for ST, a relatively niche language in this specialized domain. Five ST experienced experts affirmed the usefulness of our generated code. In the future, we aim to expand our research to encompass the generation of code that incorporates complex logic and interfaces with a variety of distinct devices. Additionally, we plan to conduct more robust experiments, drawing on an expanded set of benchmarks derived from authentic engineering practices."}]}