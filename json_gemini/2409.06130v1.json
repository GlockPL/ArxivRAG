{"title": "On the Weaknesses of Backdoor-based Model Watermarks: An Information-theoretic Perspective", "authors": ["Aoting Hu", "Yanzhi Chen", "Renjie Xie", "Adrian Weller"], "abstract": "Safeguarding the intellectual property of machine learning models has emerged as a pressing concern in AI security. Model watermarking is a powerful technique for protecting ownership of machine learning models, yet its reliability has been recently challenged by recent watermark removal attacks. In this work, we investigate why existing watermark embedding techniques particularly those based on backdooring are vulnerable. Through an information-theoretic analysis, we show that the resilience of watermarking against erasure attacks hinges on the choice of trigger-set samples, where current uses of out-distribution trigger-set are inherently vulnerable to white-box adversaries. Based on this discovery, we propose a novel model watermarking scheme, In-distribution Watermark Embedding (IWE), to overcome the limitations of existing method. To further minimise the gap to clean models, we analyze the role of logits as watermark information carriers and propose a new approach to better conceal watermark information within the logits. Experiments on real-world datasets including CIFAR-100 and Caltech-101 demonstrate that our method robustly defends against various adversaries with negligible accuracy loss (\u2264 0.1%).", "sections": [{"title": "I. INTRODUCTION", "content": "TRAINING modern machine learning models often requires significant computational resources and access to a large, privately annotated dataset [1]. For example, the performance of the GPT-3 language model exhibits a power-law relationship with factors such as model size, dataset size, and computational capacity [2], [3]. Given the considerable expenses associated with this endeavor, owners of such models are usually highly motivated to safeguard their intellectual property (IP) [4]\u2013[6].\nThe escalating threat of model theft intensifies the urgency of model copyright protection. The model stealing adversaries who possess black-box access to the victim model and a small part of unlabeled data can train a substitute model using the victim model's predictions as supervision. The substitute model is designed to functionally mimic the behavior of the victim model. Representative works in this field include [7]\u2013[9]. With the endorsement of model theft attacks, it enables the white- boxing of many other attacks, posing a significant threat to model security and privacy protection.\nWatermark embedding (WE) is a powerful technique for protecting the copyright of machine learning models [10]\u2013[12]. Originally used to protect the copyright of digital multimedia, WE works by hiding some secret information in the trained model [13], [14]. This secret information can be used later to verify the ownership of a model when copyright violation is suspected. Popular methods for implementing WE include either directly embedding the secret information into the parameters of the model [15]-[17] or by backdooring the model [16], [18]-[24]. The former method works by modifying the weights/biases of a DNN whereas the latter methods works by making the model highly predictive on some secret dataset (i.e., the trigger set) [18], [25], [26]. The latter methods can also be seen a as utilizing the overfitting ability of modern machine learning models (e.g., a deep neural network) to memorize concealed patterns that are unlikely to be triggered by normal samples. Compared to methods that directly embed watermark information into the model parameters, backdoor- based methods are more light-weighted as they only need to examine the model output rather than the whole model during ownership verification, thereby have increasingly attracted much attention among the community.\nOne important question in model WE is how to establish a strong defense against potential adversaries. These adversaries are often aware of the existence of embedded watermark and may possess full knowledge about the technical details of the underlying watermark embedding schemes. They may even have partial or full access of the model parameters and are dedicated to remove the watermark embedded in the model by all means. In fact, most of the backdoor-based WE schemes have been proven ineffective in the presence of subsequent carefully-designed attacks [22], [27]\u2013[34], especially when the model parameters are revealed [28]. One intuitive understanding of the vulnerability of backdoor-based methods is that these methods often rely on overfitting a specific dataset (i.e., the trigger set), which is fragile and can be easily overridden (by e.g., fine-tuning the model or knowledge distillation).\nIn this work, we aim to address the above threats of watermark erasure attacks by exploring two research questions: (a) why existing model watermarking schemes are susceptible to watermark erasure attacks; and (b) how to develop new watermark embedding schemes to overcome the limitation of existing schemes while maintaining utility. Our key insight is the vulnerability of existing watermark embedding schemes mainly comes from overfitting out-distribution trigger set samples, that if there is no overlapping between normal samples and trigger set samples, the adversary can easily remove watermark without affecting model utility. Only an appropriate"}, {"title": "II. PROBLEM FORMULATION", "content": "Throughout this work, we focus on classification problem, where we denote the input data as X, the label as Y and the model as f. In this setting, the model f : RN \u2192 RK maps the input X \u2208 RN to a vector of prediction logits f(X) = [f1(X), f2(X), ...fk(X)] where the subscript is used to denote the position of the element, and the prediction is done by taking Y = arg maxi fi (X). We use upper case characters (e.g., X) for random variables and lower case ones (e.g., x) for instances. Superscripts on instances indicate the instance number. For instance, x(i) denotes i-the input sample.\nIn this work, we consider a threat model with three parties: model owner, authority and adversary.\nModel owner. This party develops the deep neural networks and would like to protect the intellectual property of the model using some watermarking mechanism. The model owner holds dataset D and a secret credit T associated with the watermark embedding process E(D,T). Following Kerckhoffs' Principle, the credit T should be kept secret from potential adversaries, akin to the secret key in a cryptographic system.\nAuthority. This party is a trusted third party (e.g., the government) responsible for verifying the ownership of a suspected model. In particular, the authority runs an verification algorithm V(f,T) to determine whether a suspect model f matches with the credit provided by a party. In this work, we assume that V works in grey-box fashion where the authority verify model ownership by only inspecting the exposed model outputs, but reserve the rights to check the full details of the computational graph of the model.\nAdversary. This party is a malicious user aiming to steal a model f or evade tracking using an attacking algorithm A(f). Depending on their knowledge, an adversary may have either (1) black-box access, knowing only the model's output (predictions), or (2) white-box access, knowing both the model parameters and architecture. The adversary may also possess a small fraction of auxiliary data Daux drawn from the same distribution as the training data D. Additionally, we assume adversaries are aware of the existence of potential watermarks"}, {"title": "C. Backdoor-based Model Watermarking", "content": "The goal of model watermarking is to let model owner embed secret information into the DNN models, which can be verified by a trusted third party later to help claiming model ownership. An effective model watermarking method should satisfy the following two properties simultaneously:\nFidelity: the watermark embedding scheme does not impair the utility (i.e. prediction accuracy) of the protected model.\nRobustness: the embedded watermark is robust against watermark erasure attacks such as knowledge distillation, fine-tuning and model pruning.\nTypically, a model watermarking system involves two processes:\nWatermark embedding. In this stage, the model owner train the model with private database D = {x(i),y(i)}=1 and incorporates credit T into the model f, represented as\nE(D,T) \u2192 f. \nVarious approaches have been developed to realize this goal. In this work, we focus on backdoor-based method, which constructs the credit T as a trigger dataset T = (Xw,Yw) that is only known by the model owner, and train the model such that it can well predict Yw from Xw [18], [19], [21]:\nmin LCE (f(X), Y) + \u03b4LCE(f(Xw), Yw), \nf\nwhere LCE is the cross-entropy loss. The term Lce(f(X), Y) is called the main task objective and the term LCE(f(XW), Yw) is called the watermark task objective. \u03b4 > 0 controls the trade-off between the two objectives. Since Yw is secretly known by the model owner, a high classification accuracy on T will justify the ownership of the model.\nIn addition to the above methods that rely on trigger set, there also exist works choose n-bit string as credit T and directly embed it into the parameters or gradients of machine learning models [15]\u2013[17], [36]. After model training, T will be uploaded to the authority for future ownership verification. Because T is only known to the model owner and the trusted authority, containing the information of T will justify the ownership of the model. The watermark embedding algorithm E(D,T) should be non-invasive and robust to adversaries.\nWatermark verification. Once the ownership of a model is suspected, the model owner may initiate an ownership verification process to justify the true owner of the model. In this process, a trusted third party, such as an government"}, {"title": "D. Watermark Removal Attacks", "content": "We consider three representative attacks designed to remove watermarks from the model. Each of these attacks, denoted as A(f, Daux), takes a victim model f and some auxiliary data Daux as inputs and outputs a new model f*. A summary of these attacks is provided in Table II.\nKnowledge Distillation (KD). This black-box attack [35] steals a model f by training another model f* to mimic f's behavior. During the attack, only the output f(X) of the victim model is required. KD works by pairing the logits of f* and f:\nmin ALCE (f*(X),Y) +\nf*\n(1 \u2212 1)LCE (\u03c3 (f*(X)/\u03c4), \u03c3 (f(X)/\u0442)),\nwhere the first term is classification loss and the second term is pairing loss, A controls the balance between two losses. The pairing loss minimize the distance between softened logits of the new model \u03c3(f*(X)/\u315c) and the victim model o(f(X)/\u03c4). Here, o is the softmax function and \u03c4 \u2265 1 is distillation temperature whose choice may affect f*'s performance. Many well-known model stealing attacks are special cases of Formula 5. For example, by setting T = 1 and"}, {"title": "III. AN INFORMATION-THEORETIC PERSPECTIVE FOR MODEL WATERMARK EMBEDDING", "content": "In this section, we analyse existing watermark embedding scheme and corresponding attacks from an information- theoretic perspective, aiming to understand why existing methods are susceptible to watermark erasure attacks. This perspective also serves as a guidance of our own design later.\nWatermark embedding as infomax learning. Let f be the DNN model that need to be protected, (X,Y) be the input samples, T = (Xw, Yw) be a trigger set picked by model owner, S be some internal representation in f computed for X, i.e., S = s(X), and the decision making process of the DNN satisfies the following Markov chain:\nX\u2192S \u2192 \u0176,\nwhere \u0176 be the predicted label. The same Markov chain holds for samples Xw, Yw in the trigger set T. A well- trained network will yield S being a near-sufficient statistics for Y, i.e., I(S;Y) \u2248 I(X; Y), where I(\u00b7;\u00b7) denote the mutual information between two random variables.\nFor black-box watermarking schemes, which embed watermarks as Equation (2), its watermark embedding process can be shown to be equivalent to maximizing the mutual information between the learned representation s(Xw) and Yw:\nmax I(s(X); Y) + \u03b4I(s(Xw); Yw).\nwhich is a direct consequence of Proposition 2 in [38]. In this regard, the watermark verification process (4) can be seen as measuring whether I(s(Xw); Yw) is high enough for a suspicious model, where only high I(s(Xw); Yw) will indicate the ownership:\nV(f,T) = {1, if I(s(Xw); Yw) > t'\n0, if I(s(Xw); Yw) \u2264 \u0165',\nwhere t' is some threshold.\nWatermark removal as information-theoretic game. Therefore to erase the watermark embedded in a model, an adversary needs to reduce I(s(Xw); Yw) as much as possible while maintaining a reasonably high I(s(X); Y). This can be mathematically formulated by the following objective:\nmin I(s(Xw); Yw) s.t. I(s(X); Y) \u2265 (1 \u2212 \u0454)I(X; Y)\nwhere \u20ac \u2248 0 is a small positive value. A strong attack will find a good function s(\u00b7) whose corresponding e is small. Below, we show that an adversary can easily find such s if there is little overlap between p(X) and p(Xw), where p(X) denotes the distribution of X."}, {"title": "A. Pitfalls of Out-of-distribution Trigger-set", "content": "Based on the information-theoretic viewpoint above, we now analyse why many existing methods for model watermark- ing must fail in the presence of watermark erasure attacks. Specifically, these existing schemes largely make use of out-of- distribution trigger set T = {Xw,Yw} where p(Xw) hardly overlap with p(X); see Fig. 1. We argue that these outlier- based methods are by construction problematic, as an adversary can easily reduce the value of I(Sw; Yw) without affecting"}, {"title": "B. On Logits as Watermark Information Carrier", "content": "The above analysis also implies that the predicted label alone may not be sufficient to carry the information about watermark. To see this, consider the case where there is a significant overlap between p(X) and p(Xw) a necessary condition for defending against watermark removal attack. In such case, there exist lots of samples in x ~ p(X) and xw ~ p(Xw) whose semantic labels are the same. For these samples, the model prediction would be exactly the same, leaving there no room to embed any further information. In other words, we need richer information carrier to carry the watermark information.\nOur proposal here is to make use of the full logits f1(Xw),...fk(XW) rather than solely using the model pre- diction (i.e., the index of the maximal logit) in watermark verification, which can in essence carry more information about the watermark. This concept is closely related to Membership Inference Attacks (MIA) [39], which reveal that logits contain rich information about whether a sample belongs to a specific dataset. Here, we also utilize this phenomenon, but in a way that we use it to show the affinity of the model to a secret dataset.\nThe question remained is how to embed watermark infor- mation in the logits and how to extract watermark information from them. To answer this question, we first introduce the concept of redundant logits, which is defined as below:\nAn interesting property of redundant logits is that most of the information about the label Y is not in the redundant logits, namely:\nI(fItop-k (X); Y) \u2248 I(f(X);Y),\nwhere ftop-k (X) is a function extract the top-k logits from the original logits f(X). The rationale behind (10) is the ground truth label is often among the top-k prediction of the model, so that the values of non-top-k logits are indeed irrelevant for model prediction. In fact, if we slightly manipulate the values"}, {"title": "IV. IN-DISTRIBUTION WATERMARK EMBEDDING", "content": "In this section, we propose a new model watermarking scheme, named In-distribution Watermark Embedding (IWE), aiming to overcome the aforementioned fundamental limitation of existing methods. Our method is designed to achieve the following two desiderata in one framework:\nResist white-box attacks. Our method is designed to be effective even when the model parameters are completely revealed to the adversaries;\nSupport gray-box verification. Our method only needs to inspect the logits during ownership verification, though we reserve the right to open the box in case of logit liar attack.\nBelow, we show how to achieve these goals by a new trigger set construction scheme and a novel redundant logits reusing scheme. An overview of our method is in Fig. 3."}, {"title": "A. Design of In-distribution Trigger-set", "content": "From the above analysis, it is clear that there must be a certain overlap between the trigger set and normal samples. On the other hand, there must still be some difference between trigger set and normal samples, otherwise any model that attains high main task performance will automatically pass ownership verification. Therefore the key is a proper level of overlapping.\nTo this end, we design the trigger set as a dataset with a moderate overlap level using in-distribution samples. More specifically, we construct the trigger set T as the union of normal samples and their augmented versions:\nT = (Xw,Yw) = {(x,0)} \u222a {(Xaug, 1)}x\u2208D',\nwhere Xaug is some augmented version of sample x, and D' is a subset of training dataset D, D' C D. Therefore the trigger set is comprised of a subset of normal samples (with assigned labels yw = 0) and the augmented samples (with assigned labels yw = 1). While any data augmentation techniques can be used to generate Xaug in principle, we focus on the following two operations due to their simplicity:\nRotation. This corresponds to applying a rotation matrix R to the input x:\nXaug Rx, R =\ncos \u03b8\nsin\nsin\ncos \u03b8,\nwhere @ is the rotation angle.\nColor adjustment. This corresponds to changing the brightness, contrast and saturation level of the image [40]:\nXaug = C(x; Y1, Y2, 3),\nwhere C is a function to modify the brightness, the contrast and the saturation level of an image and 1, 2, 3 are the corresponding controlling hyperparameters.\nUnder this setup, the watermark embedding task is now formulated as a binary classification task where we predict whether a sample Xw is augmented or not\nLCE(f(Xw), Yw) = E [- log pf (Yw | Xw)],\nwhere pf (Yw | Xw) is the class probability parameterized by the model f. The only question remains is how to compute the binary class probability pf (Yw | Xw) from the network f, which is originally designed to compute the class probability pf(Y | Xw) for K-classes rather than two classes. We explain below how to compute pf(Yw | Xw) without introducing additional network modules."}, {"title": "B. Redundant Logit Reusing", "content": "We explain here how to compute pf(Yw | Xw) from the original class probabilities pf(Y = k|Xw) x efk(Xw) without introducing any additional modules. In this work, we compute Pf(Yw | Xw) by reusing the logits fk(Xw) in the original class probabilities:\npf(Yw = 1 | Xw) xe,\nPf(Yw = 0 | Xw) xe,\nwhere Itop-k is the indices of the largest k elements in the array {f1(Xw), ...fk(Xw)} (i.e., the indices of non-redundant logits) and K is the so-called partition key defined as below. K is the complement of K w.r.t the set I = {1, ..., K}.\nIn other words, we calculate the new class probabilities Pf(Yw | Xw) by first excluding the top-k logits, then computing the probabilities for class Yw = 0 and class Yw = 1 from the remaining logits according to their assigned partitions. This way, we turn the logits in a K-way classification problem into the logits in a binary classification problem, without introducing any additional component to the model.\nThe rationale behind the above method for computing pf(Yw | Xw) is the information-theoretic analysis in III-B, which reveals that non-top-k logits are indeed irreverent for predicting Y and can therefore be used to carry watermark information. Importantly, due to the redundancy of these non- top-k logits, the information they carry will not obscure the primary information in the top-k logits, thereby will not impact the main task performance.\nOnce the training of the model is done, the partition key K is no more used in the network and will be kept secret by the model owner. During verification, the model ownership needs to present both T and K to the authority to pass ownership verification. In this sense, the partition key K can be seen as part of the trigger set T."}, {"title": "V. WATERMARK VERIFICATION", "content": "A crucial question remained is how to verify the watermark for a suspicious model f. Statistical hypothesis testing offers a principled way for verifying the existence of the watermark in a model. Formally, let Fclean be the population of clean models (i.e., models on the same task but without watermark T), we test the null hypothesis Ho against the alternative H\u2081 below:\n{Ho : f\u2208 Fclean\nH1:fFclean,\nwhere the test statistics is defined as the accuracy on the trigger set, i.e.,\nACCw(f) = Exw,Yw~T [1[f(Xw) = Yw]],\nwhere 1 denotes the indicator function and K is the afore- mentioned partition key. Here f(Xw) is the network's pre- diction: f(Xw) = arg maxyw Pf(Yw|Xw) with pf(YwXw) computed as (15).\nWe reject Ho if the corresponding P-value of the test statistics ACCw(f) is smaller than a pre-defined significance level a (e.g., 5%). In other words, if we find that ACCw(f) is significantly high compared to the majority (e.g., 95%) of the clean models, then f is likely to be a watermarked model (and hence indicates model ownership). The whole process of this hypothesis testing-based watermark verification procedure is summarized in Algorithm 2.\nWe highlight that the above hypothesis testing-based water- mark verification method is very generic and can be applied to any backdoor-based model watermarking schemes, e.g., [17], [18], [20], [21]. Such universal applicability also provides us with an unified way to systematically compare among different watermark embedding schemes with different setups. For example, by comparing the true positive rate (TPR) i.e.,"}, {"title": "VI. SECURITY ANALYSIS", "content": "In this section, we provide security analysis of our IWE method in the presence of two types of attacks. The first type of attack aims at attacking the trigger set, where the adversary tries to pass verification by either guessing the trigger set directly or forging a fake trigger set. The second type of attack focuses on manipulating the values of logits, which can be seen as adaptive attacks that make use of the logits exposed in our method to pass ownership verification."}, {"title": "A. Attacking the Trigger Set", "content": "Guessing the trigger-set. The first threat we consider is directly guessing the trigger set T to pass ownership verification. Since T serves as the credit of model ownership, a direct breach of T will pose serious security threat. We note that directly guessing T is highly difficult in our IWE method. Specifically,\nDifficulties in forging trigger images. The construction of a trigger set involves the choice of base images and applying the augmentation operations. For the choice of base images, we first choose K' classes out of the K classes (K' \u226a K), then randomly draw samples from these K' classes. Therefore the probability of successfully guessing the trigger set is at most P\u2081 = \u00d7 \u221a, where Naug is the number of possible augmentation choices (there are numerous augmentation methods available in image processing. This includes adjusting the hyperparameters used in augmentation operations, e.g., rotation angles, brightness and saturation);\nDifficulties in forging partition key. There are multitude choice for the partition key K, rendering it difficult to forge. According to Definition 2, the total number of possible choice of K is (K2). The probability of successfully guessing a particular K is therefore P2 = 1/(K/2), which is 1/252 for K = 10 and is no more than 1/1028 for K = 100.\nJointly considering these two difficulties, the probability Psuccess of successfully forging both factors: Psuccess = P\u2081 \u00d7 P2 is indeed small. Therefore an adversary is unlikely to forge the ownership proof within a few attempts."}, {"title": "B. Logit Lair Attack", "content": "Adding noises to logits. In this section", "31]": "nfi(x) fi(x) + \u03be\u03af"}, {}, {"title": "VII. EXPERIMENT", "content": "In this section, we conduct extensive experiment to verify the effectiveness of proposed method. We first introduce the setup of the experiments, including datasets, neural networks, attacks, and evaluation metrics. We then assess the proposed IWE to show fidelity and robustness. In particular, we take both white-box and black-box attacks in to considerations."}, {"title": "A. Setup", "content": "Datasets. We use three real-world datasets in experiments: CIFAR-10, CIFAR-100 and Caltech-101. The first two datasets are chosen so as to follow existing works on watermark embedding [18], [28], [35]. We additionally incorporated the Caltech101 dataset [42] as the resolution of the image in this dataset resembles that of ImageNet, making it a choice highly closer to realistic settings.\nCIFAR-10. It consists of 32 \u00d7 32 color images of real-world objects. These objects are divided into 10 classes, with 5000 instances per each class in the training set and approximately 1000 instances per each class in the test set.\nCIFAR-100. It contains similar images as in CIFAR-10, but the classes are further divided into 100 more fine-grained categories, being more challenging than CIFAR-10.\nCaltech101. It consists of natural images of 101 categories, with 40 to 800 color images per each category. Each image is resized to 224 \u00d7 224 pixels.\nNeural networks architecture and training. In this work, we consider two commonly-used deep neural network architectures in computer vision:\nResNet-18. This architecture is used for CIFAR-10 and CIFAR-100. We use stochastic gradient descent (SGD) with momentum to optimize the network, where the momentum value is set to be 0.9. A small weight decay (5 \u00d7 10\u22124) is further used to avoid overfitting. We train the model for 100"}, {"title": "Robustness of IWE.", "content": "We next study the robustness of IWE against various adversaries. To demonstrate the robustness of IWE, we visualize the distributions of ACCw (f) for both clean models and IWE watermarked models when being attacked in Fig. 4. A significant separation between the distributions"}, {"title": "KD attack with other hyper-parameters.", "content": "Apart from the default hyper-parameter settings [35] (\u03c4 = 10, \u03bb = 1) shown in Table V, we further explore other settings of hyper-parameter to thoroughly study IWE's robustness against this attack. The results are shown in Table VI. Overall, we see that no KD"}, {"title": "VIII. CONCLUSION AND DISCUSSION", "content": "In this work, we introduce the In-distribution Watermark Embedding (IWE) as a novel approach to safeguarding the intellectual property of machine learning models. Our key finding is that existing backdoor-based watermark embedding schemes are inherently vulnerable to watermark erasure at- tacks due to their reliance on outlier trigger set samples-a fundamental issue not readily resolved by algorithmic tweaks. To address this issue, we developed a new scheme using in- distribution samples, where we carefully select triggers from a modified pool of normal samples. This approach encourages the strong entanglement between the main prediction task and the embedded watermarks, rendering any attempt to remove the watermark detrimental to performance. Additionally, our analysis of the role of logits as watermark information carrier has led to the innovative technique of redundant logits reusing, a technique designed to minimize the conflict between the watermark task and the main task. Experiments on three real-world datasets against diverse attacks including model fine-tuning, knowledge distillation, and network pruning fully demonstrate the effectiveness of the proposed method. To the best of our knowledge, we are the first to offer a robust defense against both white-box and black-box attacks with negligible performance impact, while without inspection of model parameters.\nOur work also reveals a crucial connection between model watermarking and membership inference attacks (MIA). MIA [39] reveal that the logits of a model can contain rich information that is well beyond just class labels, such as the membership of a specific sample in dataset. While MIA use logits information to attack data privacy, we use logits information to carry watermarks and verify the ownership of a model, where a strong membership in a secret dataset proves"}]}