{"title": "LLMcap: Large Language Model for Unsupervised PCAP Failure Detection", "authors": ["\u0141ukasz Tulczyjew", "Kinan Jarrah", "Charles Abondo", "Dina Bennett", "Nathanael Weill"], "abstract": "The integration of advanced technologies into telecommunication networks complicates troubleshooting, posing challenges for manual error identification in Packet Capture (PCAP) data. This manual approach, requiring substantial resources, becomes impractical at larger scales. Machine learning (ML) methods offer alternatives, but the scarcity of labeled data limits accuracy. In this study, we propose a self-supervised, large language model-based (LLMcap) method for PCAP failure detection. LLMcap, leveraging language-learning abilities, employs masked language modeling to learn grammar, context, and structure. Tested rigorously on various PCAPs, it demonstrates high accuracy despite the absence of labeled data during training, presenting a promising solution for efficient network analysis.", "sections": [{"title": "I. INTRODUCTION", "content": "In the dynamic landscape of today's telecommunications industry, the proliferation of sophisticated technologies and the intricate web of network components have introduced unprecedented challenges to the task of network troubleshooting. The sheer volume of data and messages exchanged between these components, coupled with the need for swift and accurate fault detection, demands innovative solutions that transcend the limitations of traditional methodologies.\nLLMcap is a self-supervised large language model (LLM)-based method designed to revolutionize fault detection in telecommunication networks. In an era where Mean Time to Detect, Mean Time to Repair, customer experience, and the detection of hidden fault types are paramount, LLMcap is an adaptable solution, providing a change in basic assumptions in how we approach network fault detection.\nLLMcap examines the extensive data and messages exchanged between network components and identifies abnormalities and failures. It achieves this without requiring extensive domain knowledge or a deep understanding of diverse protocols. Its implementation promises to significantly improve both the Mean Time to Detect and Mean Time to Repair for telecom operators, ultimately enhancing the overall customer experience. Furthermore, its capability to detect hidden failure types and measure abnormalities in message sequences and timing adds crucial dimensions to ensuring the reliability and efficiency of the telecommunication network.\nThis paper introduces an innovative LLM-based system designed for the efficient detection of failed PCAP files. The key contributions of our research include:\n\u2022 Utilization of state-of-the-art LLMs to acquire a deep understanding of the representation and grammar of successful PCAP files.\n\u2022 Eliminating the need for labeled data by using self-supervised LLMcap, making the solution adaptable to various use cases.\n\u2022 High accuracy in failure detection within PCAP data within our experimental results.\n\u2022 Accurate localization of failures in PCAP providing valuable insights for analysis.\n\u2022 Adaptive and flexible methodology that allows each building block to be replaced with an equivalent approach, enhancing versatility and customization.\n\u2022 A hypothesis that aggressive reduction of PCAP data into a dictionary format, with specific fields/entities, prior to training, does not result in a statistically significant loss in the quality of failure detection.\n\u2022 A new PCAP representation in the form of a key-value dictionary to train the LLM to reduce the problem's dimensionality. This representation contributes to the overall efficiency of our approach."}, {"title": "II. RELATED WORK", "content": "Packet capture (PCAP) data plays a crucial role in network troubleshooting and analysis. Prior work in the field has explored various strategies for detecting failures within PCAP-captured network traffic. Studies have focused on identifying abnormal traffic patterns, indicative of network issues or security threats. Traditional approaches have been heavily reliant on manual inspection and rule-based systems, which are time-consuming and may not scale with the increasing volume of network traffic [1]. Researchers have explored automated methods, including statistical analysis and simple heuristics, to improve the efficiency of failure detection in PCAP files [2].\nThe application of machine learning (ML) techniques to PCAP data is a relatively new field that has gained traction due to its potential to automate and enhance the accuracy of network analysis. Several studies have employed supervised learning methods, utilizing labeled datasets to train models for classifying network traffic and identifying malware traffic [3]. However, the challenge of obtaining a sufficiently large and diverse labeled dataset has been a significant barrier, often limiting the effectiveness of these ML models [4]. Un-supervised learning techniques, such as clustering and outlier detection, have been investigated as alternatives to address the scarcity of labeled data [5]. The main drawback of machine learning approaches is that they rely on handcrafted features and labeled data.\nLarge language models (LLMs) have recently emerged as powerful tools for anomaly detection across various domains. Their capacity to understand context and infer patterns within data makes them well-suited for identifying irregularities [6], [7]. In the realm of network security, LLMs have been leveraged to parse and analyze network logs and textual data, demonstrating an ability to detect sophisticated threats that conventional methods might miss. The concept of self-supervision in LLMs, where models learn to identify anomalies by inferring the underlying data structure, has shown promise in enhancing detection capabilities without the need for labeled examples [8].\nIn conclusion, while traditional and ML-based methods for PCAP failure detection have laid the groundwork, the integration of LLMs in this domain, as proposed by our method LLMcap, has the potential to redefine the benchmarks of accuracy and efficiency in the field."}, {"title": "III. PROBLEM FORMULATION", "content": "In this study, we address the challenge of PCAP failure detection by employing the Mask Language Model (MLM) strategy. Our definition of failure is a non-successful call. We make a clear distinction between the broader concept of error which can be any form of feedback from the network elements, and failure, where the intended initiated call didn't succeed. We hypothesize that an MLM approach is well-suited to identify the occurrence of failures in each PCAP file. In the latter, the model can effectively pinpoint the starting point for the operator's investigation by using the reconstruction error (tagging), as illustrated in Figure 1.\nIn the context of PCAP data, a single instance can be viewed as a sequence of consecutive character strings, (called tokens). The MLM approach utilizes unannotated text and randomly replaces a portion of words with a [MASK] token [8], [9]. The training objective for the model is to accurately reconstruct the intentionally hidden words based on the context and word semantics in the input sequence (Figure 1, training). The training phase comprises two steps. The first step is to preprocess the PCAP file to obtain a set of chunks, and the second step is to randomly mask tokens for the model to predict. Once the model is trained, the inference procedure follows the same path encompassing the preprocessing and masking steps (Figure 1, inference).\nGiven a set of N tokens in an input sequence $T = t_{i=1}^{N}$, a random subset of M tokens undergoes the masking procedure. For each reconstructed word, the Negative Log-Likelihood (NLL, Equation 1) loss is computed:\n$NLL = \\frac{1}{MC} \\sum_{i=1}^{M}\\sum_{c=1}^{C}y_i log(a_i)$ (1)\nwhere, y is the one-hot encoded label, which can be either equal to 0 or 1, for the correct token. Furthermore, a denotes the predicted probability for the corresponding true token when y is set to 1.\nIt is worth noting that $a_i$ vector undergoes the Softmax activation function before, thus its sum corresponds to the value of 1, where each entry represents the probability of selecting the c-th token.\nWe define the Softmax activation function as:\n$\\sigma = \\frac{e^{x_i}}{\\sum_{c=1}^{C} e^{x_c}}$ (2)\nwhere the $\\sigma$ symbol defines the Softmax nonlinearity function. The formula in Equation 1 denotes the cost function, i.e., loss, which is incorporated in the training phase of the LLM."}, {"title": "IV. PROPOSED SYSTEM/MODEL", "content": "This segment outlines the proposed approach for detecting failures in PCAPs. The process is broken down into distinct parts. Data collection is covered in Subsection IV-A. Subsection IV-B focuses on the PCAP parser and preprocessing steps. Subsection IV-C delves into the training strategy for LLM and MLM. The concluding Subsection IV-D introduces the Failure Detection algorithm (FDA)."}, {"title": "B. PCAP Parser and Preprocessor", "content": "In this section, we outline the four steps used to process PCAP files for MLM training and inference. The four steps: Parsing, Data Sanitization, Chunking, and Masking, are described in greater detail below.\n1) Parsing: Our method explored two distinct approaches for extracting the PCAP representation. The first involved extracting all available data as text from the PCAP file, offering a comprehensive view of the sample but increasing the computational burden during training. This enables treating a set of PCAPs as an extensive corpus, adopted during MLM training.\nConversely, the second method extracted a set of packet-separated fields from the PCAP as a dictionary. This facilitates substantial data reduction by incorporating only selected fields based on a priori or SME knowledge.\nThe extraction was performed using tshark v4.0.7 [11], employing the \"text\" flag for text extraction and the \u201cpdml\" flag for PDML extraction. Protocol filters (S1AP, NGAP, SIP, GTPV2, DIAMETER, HTTP2, PFCP, LCSAP, ULP, DNS, ICMP, TCP, RTP, F1AP, UDP, and HTTP) are applied.\nIn the PDML scenario, before obtaining dictionary repre-sentation, we converted the PCAP into a tabular format. This conversion allows structured data operations, indexing, and sorting, enhances readability and provides flexibility in the output.\n2) Data Sanitization: PCAP files inherently display variability, posing challenges for accurate prediction due to differing data elements. Additionally, user-related information in PCAP files, such as IP and MAC addresses, is sensitive, necessitating precautions to prevent the model from learning and potentially exposing this data. To address these concerns, we systematically removed user-specific details, by employing regular expressions. Sensitive information, like IP and MAC addresses, was replaced with generic tokens, the [REDACTED] symbol\u2014to maintain the integrity of the data while safeguarding user privacy within PCAP files.\n3) Chunking: At this stage, the obtained file is very variable in size. In this study we standardized the context length to a relatively small size (set to 64), to expedite training and streamline model iteration. This was achieved by tokenizing the text and dividing it into equal-sized chunks with the designed number of tokens. For the last chunk, we applied a padding operation, to preserve the constant dimensionality to ensure a standardized context length."}, {"title": "C. Large Language Model Training", "content": "In this section, we provide details of our large language model (LLM) training process, highlighting our choice of the Transformer-based DistilBERT model for masked language modeling [12]. Our approach follows the MLM technical solution defined in [8], with two modifications. Firstly, we only use the [MASK] token word for replacement in contrast to additional random word replacement in [8]), and secondly, we set the masking probability to 20%. We hypothesize that the 5% increase from [8], improves the LLM generalization capabilities, enhances feature extraction by emphasizing more salient keywords within the highly specialized PCAP representation, and improves robustness of the model during the inference. Furthermore, we omitted the next sentence prediction method defined in [8], reserving it for future exploration, and focus solely on MLM in this study. For clarity, we include here relevant implementation details.\n1) Model Selection: DistilBERT and Knowledge Distilla-tion: To select the best architecture for the Failure Detection Algorithm, we thoroughly reviewed various LLM and Machine Learning approaches (see Table II). This table compares the characteristics of the original implementations for each class of Large Language Models (LLMs) and related machine learning approaches. Note that newer implementations may diverge from these characteristics, reflecting ongoing advancements and optimizations in the field. Based on our comparison, Dis-tilBERT, with its encoder-only architecture, offers a balance of computational efficiency and high accuracy in contexts that require a deep understanding of data relationships, such as PCAP failure detection. Its efficiency in processing unlabelled data minimizes the need for extensive datasets and feature en-gineering, facilitating easier integration and scalability within network environments. Moreover, its potential for continuous learning with periodic updates enables adaptable anomaly detection in dynamic telecommunications networks.\nDistilBERT, a distilled version of the BERT model, employs knowledge distillation to reduce the model size while retaining 97% of the language understanding capabilities of the original BERT model with a 40% reduction in size. Additionally, DistilBERT operates at a 60% faster speed, making it optimal for our training pipeline.\nIn our DistilBERT architecture, the transformer encoder has 6 layers and 12 attention heads, and the intermediate layer size in the transformer encoder is set to 3072 units. For regularization and to prevent overfitting, the dropout rate in the fully connected layers was set to 0.1. The dimensionality of the encoder layers defaulted to 768. The maximum chunk size was set to 64. We applied the Gaussian Error Linear Unit (GELU) as the non-linearity. The vocabulary remained unchanged, with 30522 entries.\n2) Loss Function and Training Configuration: To train the LLM, we employed Negative Log Likelihood (NLL) loss as the optimization criterion. The training process extends to a maximum of 200 epochs, ensuring the model converges to an optimal state. To prevent overfitting, we incorporated an early stopping mechanism by setting the patience hyperparameter to 12 epochs to halt training if there was no improvement in the cost function.\n3) Input Token Masking and Shuffling: During training, we introduced a masking probability of 20% to the input token sequence, enabling the model to learn contextual relationships by predicting masked tokens. To prevent the LLM from learning the sample order, we shuffled the dataset in each epoch, providing variability in the training data.\n4) Experimental Setup: The experiments and LLM training were conducted on an AWS G5.xlarge instance, equipped with 4 CPUs, 16 GB of RAM, and 1 GPU with 24 GB of memory. The consistent environment across all experiments ensures reproducibility and reliability of results.\nThe AdamW optimizer facilitated the optimization of our LLM. AdamW extends the Adam optimizer, by directly in-corporating weight decay into the update step. Weight decay is a regularization technique that penalizes large parameter values to prevent overfitting. The \"W\" in AdamW specifically stands for \"Weight Decay,\" highlighting its integration of this regularization term. We set the learning rate to 2e-5., with a weight decay hyperparameter of 0.01. The beta coefficients are 0.9 and 0.999 for the gradient and its square respectively."}, {"title": "D. Failure Detection Algorithm (FDA)", "content": "Our Masked Language Model (MLM) provided predictions at the chunk level, which we aggregated to the PCAP level for final failure detection. We computed two metrics per chunk: the Number of Misclassifications (NOM) and the Mean Negative Log-Likelihood (MNLL) loss. For each PCAP, we selected the top k chunks based on these metrics and calculated their average. This process aims to identify whether any chunks within a PCAP file exhibit signs of failure. In our experiments, we set the k hyperparameter to 3, using these metrics to determine the presence of failures in PCAP files.\nFor the threshold-based FDA, we calculated the mean and standard deviation of NOM-k across the training set. PCAPs with a mean NOM-k exceeding three standard deviations from the mean were classified as containing failures, encompassing 99.7% of PCAPs from the training set deemed successful.\nIn the unsupervised ML-based approach to FDA, we em-ployed the Elliptic Envelope (EE) algorithm. This method computes the covariance matrix and defines an ellipsoid around the central point of the data, presumed to follow a Gaussian distribution. It applies the Mahalanobis distance to identify outliers, which, in our context, are failed PCAPs. We leveraged both NOM-k and MNLL-k metrics as features for representing PCAP files. After training the MLM, we collected these metrics from the training data to inform the EE algorithm, which was then integrated as the concluding step in our pipeline.\nUpon receiving a new PCAP, it was first processed through the MLM to calculate the relevant metrics. Subsequently, the FDA was applied, using either the threshold-based or the EE algorithm. It is noteworthy that various other aggregation strategies, such as mean NOM or mean MNLL, were explored but did not yield satisfactory results."}, {"title": "V. RESULTS AND DISCUSSION", "content": "In this section, we evaluate the impact of the PCAP file representation and the failure detection algorithms (V-A) on the performance of failure detection. We also assess the failure detection capability of LLMcap on two external datasets (V-B) and demonstrate an example of tagging relevant chunks when a failure is detected (V-C). Finally, we investigate the com-putational time for training and inference (V-D) of different approaches."}, {"title": "A. Best Representation and Best Failure Detection Algorithm", "content": "A series of experiments were conducted to determine the best PCAP file representation and the most effective Failure Detection algorithm. Our training dataset for the VoLTE service comprised 1311 successful PCAPs with subsample validation and test splits of 100 each. Additionally, a control set with 364 failed VoLTE PCAPs, not used during training was included. This resulted in 1111 successful PCAP files in the training set, 100 successful in the validation set, 100 successful in the test set, 100 with failure in the validation set, and 100 PCAP files with failure in the test set. In the validation and test sets, we kept the number of examples balanced between the two classes. Batch size and chunk size remained constant across the experiments with a batch size of 2 and each chunk containing 64 tokens Figure 3 illustrates the effectiveness of employed metrics, in filtering out failed PCAPS (orange triangles). Notably, successful PCAPs (blue circles), cluster together exhibiting lower misclassification and NLL loss values (Figure 3).\nThe results of our experiments are summarized in Table III, presenting precision, recall, F1-score, and F2-score inde-pendently for each class. While reporting these metrics, the primary focus of LLMcap is to detect failure emphasizing recall over precision. For that reason, our objective criterion for evaluating is the F2-score on the failure class. In the first experiment examining the impact on PCAP representation, the optimal approach is to use key-value pairs Subsequently, we tested a change in the failure detection method, employing the EE algorithm The results indicate that the EE algorithm is the superior failure detection method, improving the F2 score, although it does impact the precision of the failure class.\nThe motivation behind employing the EE algorithm is to cover non-square differences between both classes for the utilized metrics. Furthermore, it allows us to combine NOM and MNLL loss, to fine-tune the model's accuracy (see Figure 3).\nIn the previous experiments, the DICT representation lacked separation across the packet level. To evaluate the impact of introducing such separation, we modified the method to create a vocabulary for each packet, later concatenating them into one. This approach differs from the previous one in the ordering of the tokens in the input sequence, thus changing the context for the LLM during MLM. We hypothesized that this alteration may improve the generalization capabilities by providing more consistent predictions and reducing the number of false positives and negatives. This approach, labeled as PCT-DICT with a threshold failure detection model and the EE model (PCT-DICT), resulted in an increased F2 score. However, this representation in addition to the EE algorithm led to a lower precision for the Failure class. Consequently, we determined that the best combination is to use a reduced set of ordered key-value representations for the PCAP file alongside a threshold-based approach. In subsequent experiments, we exclusively retained the PCT-DICT-T model. In this scenario, the context window encapsulates packet-level information, enhancing reconstruction abilities and providing more consistent results due to the high correlation within the packet-level context."}, {"title": "B. Performances on External Services", "content": "The subsequent experiments aimed to assess if a model trained on PCAP files from one service could effectively detect failures in other services. While the training phase utilized the Voice over LTE (VoLTE) service dataset, evaluations were conducted on other services, such as 4G-5G NSA Connectivity and Voice over New Radio (VoNR) datasets The latter two services were not part of the training set and solely underwent the inference process. For the 4G-5G NSA Connectivity service, we collected 982 successful and 264 failed PCAPs. For the VoNR service, the dataset comprised 66 successful and 331 failed PCAPs.\nThe VoNR service dataset yielded comparable results to those obtained with the VoLTE dataset. The model achieved a higher precision for the failure class but at the cost of a lower recall. The nearly identical F1 scores for both classes suggest that the model performs well on this unseen dataset and adjustments to the threshold could enhance the F2 score. However, the model did not perform well on the 4G-5G-NSA Connectivity dataset, with the F2 score for the failure class dropping to 0.342. The difference in performances can be attributed to the similarity between VoLTE and VoNR services, sharing identical message flows and sequences, particularly in SIP, utilizing the same IMS components. Conversely, 4G-5G-NSA Connectivity scenarios have distinct message flows and sequences with a completely different structure, posing a challenge for the trained model that may not have learned this new syntax. While LLMcap demonstrated the ability to generalize across closely-related services without retraining, this generalization is limited when the services differ drasti-cally. This limitation might be overcome through retraining or fine-tuning on external services."}, {"title": "C. Tagging PCAP Content", "content": "LLMcap offers the advantage of providing additional in-sights when a failure is detected in a PCAP file. By selecting chunks with high NOM, we can identify the reason for the classification. Figure 4 illustrates an example of LLM\u0441\u0430\u0440 output, where the model made 8 mispredictions in a chunk corresponding to frame 223. The highlighted bold section indicates where the errors occurred. In this scenario, users can examine the identified error, such as \"500 Server Internal Error\" to understand the cause of the failure. This feature enhances the interpretability of LLMcap's findings, aiding users in diagnosing and addressing network issues. Reference for the error: RFC 3261, section 21.5.1. [13]."}, {"title": "D. Execution Time", "content": "In each experiment, we evaluated both the training and infer-ence times (Table V). The choice of PCAP file representation significantly influences the training and inference times. Using a full-text representation increases the number of chunks per file compared with a key-value representation. Notably, the ordering and the FDA have minimal impact on computational time.\nThe parallelizability of LLMcap extends beyond a mere technical advantage; it underpins a scalable and efficient approach to network failure detection. LLMcap is designed to promptly predict failures in individual data chunks, allowing for parallel processing across distributed computing resources. This asynchronous aggregation of results prevents network analysis from becoming a bottleneck, promoting timely failure detection even in telecommunication infrastructures.\nFurthermore, LLMcap has a minimal computational foot-print, making it ideal for deployment in edge computing scenarios. Its efficiency and compact size are crucial for telecommunication networks where data processing closer to the source (at the edge) reduces latency, enhances data privacy, and lowers bandwidth usage by avoiding the need to transmit large volumes of PCAP data to centralized cloud-based systems. LLMcap's design not only meets the immediate need for accurate PCAP failure detection but also aligns with the broader objectives in modern network management, which increasingly favors decentralized and edge-focused computing solutions."}, {"title": "VI. CONCLUSIONS", "content": "We introduced LLMcap as an innovative approach to uti-lizing LLMs for PCAP file analysis. Operating without la-beled data demonstrated high accuracy in failure detection and localization within PCAP data. Notably, LLMcap offers adaptability through interchangeable components without sac-rificing performance. We proposed a PCAP data reduction into a key-value dictionary format pre-training, showcasing efficiency without significant quality loss in failure detection. Our findings highlighted the balance between context length and processing speeds in LLMs, with potential improvements in model understanding by extending context. Data filter-ing emerged as a beneficial strategy to enhance the model. LLMcap serves as a foundational framework, enabling failure detection and root cause analysis in production, while lever-aging source PCAPs directly instead of traditional methods such as Performance and Configuration Management, or Key Performance Indicators. We also demonstrated that a key-value pair representation is optimal, indicating efficiency and simplicity in understanding PCAP language. We aim to extend our model's training set to incorporate a wider range of services to increase our approach's applicability. In conclusion, this research represents an advancement in PCAP analysis, unlocking the potential for real-time failure detection in live telecom networks. Our approach alleviates the challenges of traditional troubleshooting, paving the way for future devel-opments in network quality of service, fault detection, and performance optimization using advanced language models."}]}