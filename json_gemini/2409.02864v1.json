{"title": "Bioinformatics Retrieval Augmentation Data (BRAD)\nDigital Assistant", "authors": ["Joshua Pickard", "Marc Andrew Choi", "Natalie Oliven", "Cooper Stansbury", "Jillian Cwycyshyn", "Nicholas Galioto", "Alex Gorodetsky", "Alvaro Velasquez", "Indika Rajapakse"], "abstract": "We present a prototype for a Bioinformatics Retrieval Augmentation Data (BRAD) digital assistant.\nBRAD integrates a suite of tools to handle a wide range of bioinformatics tasks, from code execution to\nonline search. We demonstrate BRAD's capabilities through (1) improved question-and-answering with\nretrieval augmented generation (RAG), (2) BRAD's ability to run and write complex software pipelines,\nand (3) BRAD's ability to organize and distribute tasks across individual and teams of agents. We use\nBRAD for automation of bioinformatics workflows, performing tasks ranging from gene enrichment and\nsearching the archive to automatic code generation and running biomarker identification pipelines.\nBRAD is a step toward the ultimate goal to develop a digital twin of laboratories driven by self-contained\nloops for hypothesis generation and testing of digital biology experiments.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) and scientific foundation models (FMs), have expanded the potential\napplications of artificial intelligence [35, 12, 7]. Originally designed for machine translation, LLMs have now\nfound applications in areas such as question-and-answering, code generation, and data analysis. These\nmodels are driving innovation across disciplines, advancing research in aerospace [31], chemical engineering\n[6], and bioinformatics [39, 33, 34, 11, 21]. Allowing models to function as independent agents capable of\nanalyzing data and making decisions, or as collaborative teams where agents share instructions and results,\noffers a promising avenue for enhancing human-in-the-loop supervision in AI-driven discovery [36, 17]. By\nenhancing scientific understanding and accelerating discovery, these technologies offer unprecedented\nopportunities for research.\nAlthough a single LLM can accomplish a myriad of tasks, Multi-Agent LLM systems are increasing in\npopularity [17]. By delegating tasks to an array of individual LLM-powered agents, these agents can\ncollaborate to solve various tasks, such as software development [13] and biomedical discovery [16]. In\nbioinformatics, Multi-Agents can power autonomous labs equipped with data analysis, enhancing scientific\nknowledge and accelerate discovery [23].\nOur ultimate goal is the design of a self-driving, digital biology laboratory. Self-driving labs integrate\nboth experimental and analytical processes for a lab and vastly accelerate scientific progress. Although they\nare still in their infancy [29], closed loop systems have already been implemented in other domains such as\nchemistry [5] and synthetic biology [26]. Bioinformatics Retrieval Augmentation Data (BRAD) digital"}, {"title": "Anatomy of BRAD", "content": "At the heart of this architecture is an LLM(s) that orchestrates the usage of BRAD's different modules.\nThese modules enable BRAD to interface directly with documentation, published research, the internet,\nsoftware, and user data. This modular architecture grants BRAD the flexibility to integrate diverse tools\nwhile offering users a framework to customize and expand BRAD's capabilities.\nWhen responding to a user query, BRAD's modules adheres to a standard template (see fig. 1, left): (1) a\nsemantic router [25] selects the most appropriate module to handle the query; (2) within the selected module,\na series of LLM prompts are used to retrieve literature, search online, execute code, or other module specific\ntasks; (3) an LLM summarizes BRAD's findings and continues the conversation.\nFor queries demanding multiple modules, BRAD's PLANNER acts similar to a ReAct agent [40] by\nbreaking down the query into a series of tasks to be handled by individual modules. These tasks are executed\nsequentially, with BRAD compiling the results from each module to form a cohesive response. The PLANNER\ncan create linear workflows or more advanced pipelines that include feedback loops, branching, and other\ncontrol flow mechanisms.\nThe backbone of BRAD's design is code that supports its integration with other software and maintain\nits ease of use. Each BRAD chatbot or chat session outputs all figures, downloaded data, and other relevant\ninformation to a centralized output directory. This directory also maintains a comprehensive log that tracks\nBRAD's interactions with the user, databases, and other tools. Built on the LangChain framework, BRAD\ncan utilize LLMs and APIs either locally or from providers like NVIDIA and OpenAI. It can also be\nintegrated into any LangChain or LangGraph architecture, ensuring broad compatibility with standard\nframeworks [2]. In the remainder of this section, we outline the capabilities of BRAD's main modules, with\nfurther implementation details available in \u00a7SI.1."}, {"title": "Lab Notebook Module", "content": "The Lab Notebook Module connects BRAD to internally stored documents, including literature, protocols,\ntextbooks, and any other pdf files. It employs RAG to tailor the LLM's responses based on curated\nuser-provided information. BRAD can choose between a previously constructed document database or create\none dynamically. For further detail on database creation, see \u00a7SI 1.2.\nRAG functions in two phases: retrieval and generation [20]. During retrieval, the aforementioned\ndatabase is searched to find chunks of text relevant to the user's query. In the generation phase, the retrieved\ndocuments are combined with the user query to create a prompt for the LLM. This approach enhances\nresponse quality and reduces hallucinations [32].\nThe Lab Notebook module allows users to control database construction on their own literature and\ncustomize the retrieval and generation processes (see fig. 2). BRAD supports various techniques, including\nsimilarity-based search, maximum marginal relevance (MMR) [8], and multi-query retrieval during the\nretrieval phase, as well as compression [1] and reranking [31] in the generation phase."}, {"title": "Quantitative Assessment", "content": "Beyond the qualitative assesment of the Lab Notebook module, we employed\nthe RAG Assessment (RAGAs) framework to quantify the advantage of BRAD's RAG system [15]. We\ngenerated a set of research questions and evaluated BRAD's ability to answer them using various features of\nthe Lab Notebook. These questions were manually classified into three different topics: Mathematics,\nBiology, and Computer Science and Data. This framework employs an evolution-based question generation\napproach to produce a diverse range of questions [38]. This process begins by invoking an LLM to generate a\nbasic set of questions from the document database. Then, it takes those same questions as an input and asks\nthe LLM to \"evolve\" the questions by making them more complex or simple. This prompt engineering is\npreformed iteratively until the final set of questions are generated. BRAD then answers the generated\nquestions, and its performance is scored based on the expected answer for each question. A detailed\nexplanation of how each metric is calculated is included in SI \u00a72.1.2."}, {"title": "Software Module", "content": "BRAD's Software Module features a multistage pipeline that utilizes LLMs to write and execute code in\nresponse to user queries. It supports running Python, MATLAB, and Bash scripts. Although BRAD is\nprimarily designed for workflows that involve saving data to files at each stage, as is common in\nbioinformatics, this module also supports LLM generated code that avoids writing files between stages.\nBRAD performs a series of steps to safely execute a piece of software (see fig. 5 and SI.fig. 7). The\nprocess starts with BRAD reviewing the documentation of all available scripts and leveraging the LLM to\nselect the code that best addresses the task. Based on user input and selected code, BRAD uses the software\ndocumentation, few shot learning examples, and the LLM to to generate the appropriate code. Refer to\n\u00a7SI.1.3 for details on the implementation.\nTo ensure safe execution of LLM generated code, BRAD is restricted to running whitelisted software.\nThis allows BRAD to verify that the correct paths, inputs, outputs, and other arguments are provided for\neach script. Additional checks ensure the closure of all delimiters and other issues identified while testing\nLLM-generated code.\nErrors identified during code validation are sent back to the LLM along with the original query for\nrefinement. This iterative process of code generation and validation may continue until the code executes\nwithout error. Once all issues are resolved, the code is run.\nWe designed scripts to execute LLM generated code without running whitelisted scripts. While this\napproach allows BRAD to fully utilize LLM generated code, this process introduces several challenges: (1)\nensuring the safety of LLM-generated code, and (2) generating bug-free, free-form code with LLMs. To\ndemonstrate this feature, we designed a script that enables the safe execution of the full Scanpy library with\nBRAD. However, ensuring LLM generated code is error free remains a challenge."}, {"title": "Digital Library Module", "content": "The Digital Library module provides BRAD access to an array of literature, archives, and online\ndatabases (see fig. 5). Building on a standard RAG, BRAD can retrieve and share information from these\nsources, either directly with the user or through other modules. Below, we outline the integration of several\ndatabases, and we'd like to note the relative ease of integrating new databases.\nEnrichr. Enrichr is a comprehensive gene set enrichment tool commonly used in many bioinformatics\npipelines [9]. We integrated Enrichr into BRAD to leverage gene set annotations, curated pathways, and\nontology knowledge to enhance BRAD's responses to user queries.\nGene Ontology. The Gene Ontology (GO) database is an initiative to provide a uniform approach and\nvocabulary to studying genes and gene products across different species [4, 3]. When gene names, GO terms,\nor other keywords are identified, BRAD has the ability to define these GO terms and retrieve charts that\ndiagram their relationships with other genes. Moreover, the GO terms are associated with papers that may\nbe integrated via other databases and modules of BRAD."}, {"title": "Additional BRAD Modules", "content": "BRAD also contains modules responsible for documentation generation, planning and executing multistage\nqueries, documentation and code generation, and more. Refer to \u00a7SI.1.1-1.6 for additional implementation\ndetails."}, {"title": "Digital Laboratory", "content": "This section presents various examples and results achieved by BRADs. We demonstrate the ability to (1)\nperform a comprehensive search about a gene, using network analysis, multiple databases, and RAG, (2)\nautomatically build a RAG database targeting a specific question from scratch, (3) run biomarker\nidentification pipelines, and (4) perform automatic exploratory data analysis. The first three examples are\nperformed using single BRAD chatbot, and performing exploratory search of data is done with multiple\nBRAD agents that pass information between one another. Each use case demonstrates how a single BRAD\nor a team of BRADs tackles or automates complex research tasks under human supervision."}, {"title": "Enhanced Search", "content": "Integrating the SOFTWARE and Digital Library modules improves quality of the RAG responses generated\nby the Lab Notebook. Consider, for instance, Q2: What role does PCNA play in the cell cycle and imaging?\nBRAD orchestrates the following process: (1) identifies which genes interact with PCNA; (2) determines the\npathways related to PCNA; and (3) uses the related genes and pathways to search the literature and\nformulate a comprehensive answer. In addition to providing a detailed text response, this process generates\nsupplementary materials such as images, spreadsheets, and other documentation to offer further context to\nthe user (see fig. 6 and \u00a7SI.2.3)."}, {"title": "1.5 Planner Module", "content": "Unlike other modules of BRAD that respond to individual user queries, the Planner module can handle\ncomplex queries that require multiple tools. Given a user input, the scheduler determines a series of smaller\ntasks that can be executed using one module at a time whose collective output will address the user's query.\nThese tasks are shown to and edited by a human, after which point the set of tasks are automatically\nexecuted; see fig. 9 for a high level schematic of a workflow involving the Planner."}, {"title": "1.6\nModule Selection and Control Flow", "content": "A series of mechanisms are utilized by Brad to determine the appropriate module to execute specific tasks or\norchestrate complex processes involving multiple modules.\nModule Selection with Semantic Routing. BRAD's direction of a query to the RAG or any other\nmodule is a standard control flow problem. To route a users query to the appropriate module or function,\nBRAD uses a series of semantic routers. To manage the control flow and determine which databases or\nmodules to use in response to a user query, we employ semantic routers to make fast, real time decisions.\nSemantic routers are prediction or classification models that process user queries to determine the\nappropriate path for the prompt to flow through the chatbot. At a high level, BRAD uses semantic routing\nto determine if a query requires fetching data online, access to private data or custom software, or a standard\nRAG and LLM response. At a lower level, these routers can be used to identify specific function calls and\nparameters from queries to BRAD.\nAdaptive Routing through Reinforcement Learning. Routers are initialized with a series of prompts\nto guide BRAD's control flow, and these prompts are refined over time via a simple reinforcement learning\nprocedure. When interacting with BRAD, semantic routers direct user prompts to use different modules,\nperform online retrieval, or query the text database. Users can force a query down a path or indicate if a\nprior query was misrouted. As users interact with BRAD, the set of prompts to direct down different paths\nis recorded. This feedback is incorporated into the router, enabling similar queries to follow the appropriate\nroute in the future.\nControl Flow. The router can manage more intricate tasks schedules beyond simple linear execution. For\nexample, the scheduler might first execute RAG to gather initial information and then, based on the quality\nof the answer, proceed to search the web for additional literature. The scheduler's combination of an\ninstruction pointer and an instruction list allows for sophisticated control flow patterns, including loops,\nif/else statements, and other logic.\nFor instance, consider the scenario where BRAD receives a query addressed at the RAG but has no text\ndatabase for retrieval. In this case, the scheduler might construct a three stage loop consisting of: (1)\ndownload relevant literature to the user's query and include it in the database; (2) propose an answer based\non RAG to the user's query; and (3) determine if the answer in stage (2) is satisfactory, in which case the\nchain ends, or if the answer is unsatisfactory, the instruction pointer will return to step (1). In Step (3), Brad\nuses the LLM at the core of BRAD to evaluate complex control flow decisions and determine if previous\noutputs in the chain satisfy particular conditions set by the scheduler."}, {"title": "1.7 Prompt Templates", "content": "A core component of BRAD is a prompt template library designed to interface with various modules and\ngenerate responses for the users. This library of templates contains two distinct classes and is based on\nrecent LLM prompting techniques.\nInput v. Output Prompts. We developed two classes of prompts corresponding to the primary uses of\nan LLM within BRAD. The first class of prompts is designed to transform a user's query into actions,\nleveraging the LLM to generate or execute code. This includes tasks such as data or software selection,\ngenerating or editing code to execute, or selecting databases and online search terms, among other tasks. In\nthese prompts, a series of techniques for improving the responses via structured output, few shot prompting,\nand other methods are used.\nThe second class of prompts is used to generate output responses for the user. These include the\ngeneration stage of RAG, where pieces of text are formatted into a prompt template to improve user\nresponses, prompts used to summarize data resources and code utilized, and prompts for generating output\nPDF documentation. These prompts also convey specific instructions and often are templated to insert\ninformation and data retrieved while executing various modules."}, {"title": "Few Show Prompting.", "content": "Few-shot prompting is a technique to provide LLM responses by including a set\nexample outputs as input to the model. For instance, to execute code with BRAD a set of prompts have\nbeen constructed that contain examples on to construct an appropriate piece of code in either Matlabor\npython. These few examples are both built into BRAD's prompt library and can additionally be provided in\nthe documentation of software being run. By providing examples, the variability in response structure as well\nas the quality of the LLM's output are improved."}, {"title": "Chain of Thought Prompting.", "content": "Chain of thought prompting is a technique that uses reasoned through\nexamples as input to an LLM to demonstrate to how to reason through complex or multistep queries. When\nasking an LLM to solve a problem, the user could specify in the prompt to solve the problem using a series of\nsteps. In BRAD's code execution program, a series of such prompts are provided. When formulating a\nfunction call to execute a script, the LLM is asked to generate an appropriate piece of code based on the\nusers input, software documentation, and few shot examples of running the code. The output of the prompt\nis structured to require the LLM to explain is chain of thought and reasoning behind the constructed code."}, {"title": "2 Results and Examples", "content": "This section presents extended results and implementation details related to the use cases of BRAD discussed\nin the paper."}, {"title": "2.1 Lab Notebook Evaluation", "content": ""}, {"title": "2.1.1\nQualitative Example", "content": "Since all the BRAD responses to Q1 were too long, the complete output is provided here."}, {"title": "2.1.2 RAG Assessment (RAGAs) evaluation metrics", "content": "Five metrics were used to evaluate the utility of BRAD's RAG system:\n\u2022 Faithfulness: This quantifies the consistency between BRAD's answer and the literature.\n\u2022 Answer Relevance: This evaluates if the response is applicable to the particular question.\n\u2022 Context Precision: This quantifies the ranking of the chunks BRAD uses to formulate its response\nrelative to the ground truth.\n\u2022 Context Recall: This measures the use of context in BRAD's response.\n\u2022 Answer Correctness: This is the F1 score of claims that can be verified from the ground truth.\nAll metrics are from 0 to 1 with higher scores indicating better performance.\nIn order to determine these five metrics, a combination of the following four components were used:\nquestion, response, context, and ground truth. Since the questions are based on cutting edge research topics,\ntheir ground truth is based in research papers in our aforementioned database. The context is the chunks of\ntext RAG uses to generate a response."}, {"title": "2.2 Building a Database", "content": "Typically, we use our prior knowledge and publications of interest to construct a literature database for\nBRAD. However, the web scraping module makes it easy for BRAD to construct a new database aimed at a\nparticular topic area or question.\nTo demonstrate the utility of the RAG and Web Scraping features integrated into BRAD, we tested\nBRAD's ability to respond to questions without a pre-existing database. BRAD was initially provided an\nempty database. Through alternating between RAG and web scraping, following fig. 10, BRAD was able to\nadequately answer complex research questions by retrieving relevant information online.\nFor instance, consider the question: \"What are single cell foundation models?\" Initially, BRAD has no\ninformation on this topic. Responding with only the RAG module, BRAD will correctly identify a lack of\ninformation on this topic and can trigger a web search of this topic. Realizing the insufficiency of\ninformation, BRAD then proceeded to perform a web search to gather recent research papers and articles\nrelevant to single cell analysis and foundation models. Equipped with the new information, BRAD's answer\nto the initial question is improved and is able to discuss this research area, providing the names of several\nnotorious scFMs and a discussion of their use cases.\nThis process outlined in fig. 10 is designed by the planner module with clear instructions from the user.\nThe LLM engine of BRAD is used at multiple stages throughout this process. Besides its use in the RAG\nand Web Search modules, the LLM also acts as an evaluator to determine the quality of the RAG's responses.\nOnce the response meets sufficient quality, the LLM signals to exit the loop. Additionally, we have\nimplemented safeguards to prevent the system from executing an infinite loop."}, {"title": "2.3 Advanced Search", "content": "Below, we show the full output produced by BRAD in the process of answering Q2: What role does PCNA\nplay in the cell cycle and imaging? To answer this, BRAD executed a pipeline consisting of (1) running the\nSOFTWARE module to find first order interactions of PCNA in the STRING and HuRI databases [27, 24], (2)\nperforming gene enrichment with the Digital Library module, and (3) using the RAG system in the\nDigital Library to find other relevant information. After completing these tasks, the WRITER module was\nused to produce a report summarizing the entire process.\nThe WRITER module, although not a focal point of the paper, is a module designed to process the chat log\nand output-directory to produce a single, PDF document that describes BRAD's analysis. The goal is to\nensure BRAD's work is transparent, well-documented, and clear, even for users unfamiliar with AI, so they\ncan easily understand what occurred. The WRITER is implemented by using LLMs to generate latex code,\naccording to a user specified LATEX template, configured similarly to whitelisted software, to explain the\nresults of each module."}]}