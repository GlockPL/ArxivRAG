{"title": "OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering", "authors": ["Jiahao Nick Li", "Zhuohao (Jerry) Zhang", "Jiaju Ma"], "abstract": "People often capture memories through photos, screenshots, and videos. While existing AI-based tools enable querying this data using natural language, they mostly only support retrieving individual pieces of information like certain objects in photos, and struggle with answering more complex queries that involve interpreting interconnected memories like event sequences. We conducted a one-month diary study to collect realistic user queries and generated a taxonomy of necessary contextual information for integrating with captured memories. We then introduce OmniQuery, a novel system that is able to answer complex personal memory-related questions that require extracting and inferring contextual information. OmniQuery augments single captured memories through integrating scattered contextual information from multiple interconnected memories, retrieves relevant memories, and uses a large language model (LLM) to comprehensive answers. In human evaluations, we show the effectiveness of OmniQuery with an accuracy of 71.5% and it outperformed a conventional RAG system, winning or tying in 74.5% of the time.", "sections": [{"title": "1 INTRODUCTION", "content": "People often record their everyday life by taking photos, screenshots, and videos, whether for saving important information, documenting special occasions, or simply capturing a funny moment [36]. These captured instances, referred to as captured memories, collectively represent subsets of an individual's episodic memories [56], a type of long-term memory that contains both specific past experiences and associated contextual details. These episodic memories are essential for answering memory-related personal questions like, \"What social events did I attend during CHI 2024?\" (Figure 1a), which can help users reflect on past experiences and make informed decisions in daily tasks.\nHowever, these raw captured memories by themselves are insufficient to answer personal questions, as they lack contextual details that are typically implicit and scattered across multiple pieces of data. For examples, as shown in Figure 1b, memories of attending parties during CHI 2024 are not explicitly annotated as occurring during the event. Answering such personal questions requires extracting and integrating contextual information not typically contained within a single captured instance. For example, by integrating multiple memories that mention \u201cCHI 2024\u201d in their content and extracting their metadata, it is possible to determine when the users attended the conference and connect related social events memories from that period to CHI 2024 (Figure 1c), enabling the answer of the query (Figure 1d).\nAdvancements in AI have enable question answering (QA) on long documents [4, 58], knowledge graph [28, 62], multimodal databases [13, 54], egocentric videos [27, 43]. These methods typically rely on data-driven approaches to train powerful models for the target task. However, the private nature of captured memories makes it difficult to curate large datasets, posing challenges for training models specifically for QA on personal captured memories. Recent LLM-based work has adopted retrieval augmented generation (RAG) workflow to handle external databases without specific training [34]. However, such methods depend on explicit connections between queries and relevant external data [16]. In contrast, captured memories are often unstructured and lack contextual annotations, making it difficult to establish explicit links between queries and interconnected memories.\nTo achieve this, we propose OmniQuery, a novel approach designed to robustly and comprehensively answer users' queries on their captured memories. OmniQuery has two key components: (i) a question-agnostic pipeline to augment captured memories with contextual information extracted from other related memories to produce context-augmented memories, and (ii) a natural language question answering system that retrieves these processed memories and generates comprehensive answers with referenced captured memories as evidence. The processing pipeline in (i) is informed by the taxonomy of contextual information that we generated from a one-month diary study with 29 participants. Specifically, we collected and analyzed 299 user queries, identifying the types of"}, {"title": "2 Related Work", "content": "OmniQuery is inspired by and related to prior work in the areas on personal memory augmentation, multimodal question answering and applications that utilize contextual information."}, {"title": "2.1 Personal Memory Augmentation", "content": "A large body of work in human-computer interaction (HCI) has been focused on augmenting users' memory. This includes developing reminder tools for elderlies or people with memory impairments [8, 31, 32, 52], providing proactive support in daily tasks [11, 66], or manipulating users' memory focus in extended reality [5]. These work typically focus on the \"capturing\" stage of the memory augmentation, where researchers develop wearable devices that continuously capture data using designated sensors, which record various modalities such as videos [15, 24, 44], audios [23, 57], or bio-signals [10], to augment the memory database. For example, recent work such as Memoro developed a wearable, audio-based device that continuously records users' conversations and enables"}, {"title": "2.2 Multimodal Question Answering", "content": "Over time, natural language QA research has shifted to more complex settings, including QA across different modalities (e.g., images [2, 20], videos [41, 60, 64], tables [65] or knowledge graph [29, 63]), QA on large datasets [12, 34] and tasks that require multi-hop reasoning [45, 61]. Recent advancements in large language models (LLMs) and multimodal foundation models (e.g., [37-39]) have have enabled improved reasoning and answer generation over large, multimodal datasets. This is similar to OmniQuery's use case as answering personal questions requires handling large amounts of captured memories and performing complex reasoning. Prior work has used retrieval-augmented generation (RAG) workflow [34], which retrieve relevant information from external datasets based on a query and then generate output using the retrieved results. For example, MuRAG leverages RAG to answer open questions via retrieving related information from databases of images and text [13]. VideoAgent leverages structured memories processed from long videos to accomplish video understanding tasks [18]. However, these methods rely on datasets already rich in context (e.g., Wikipedia\u00b9) and improvements are often achieved by designing new retrieval workflows (e.g., Self-RAG [3] and tree-based retrieval [50]) or query augmentation [9]. More recently, GraphRAG introduced a data augmentation approach that generates a knowledge graph from extensive raw data to tackle tasks requiring higher-level understanding, such as query-focused summarization [16]. While GraphRAG leverages data-driven methods to identify themes and communities within graph nodes, OmniQuery extends this concept by adopting a taxonomy-based augmentation approach, informed by insights from a diary study, to enhance retrieval on personal captured memories."}, {"title": "2.3 Applications Utilizing Contextual Information", "content": "Contextual information has long been important in HCI research from early mixed-initiative systems [26] to recent agentic workflow [30]. Over the past few years, there has been a surge in the usage of AI and LLMs in the HCI community, which enables extracting contextual information from processing raw multimodal information. For example, Li et al. studied how visually impaired people cook and emphasized the importance of conveying contextual information to users through multimodal models [35]. Additionally, Human I/O leverages egocentric perceptions of users and detect situational impairments through reasoning on the multimodal sensing data [40]. GazePointAR develops a context-aware voice assistant to disambiguate users' intent when interacting with real-world information [33]. OmniActions categorizes digital follow-up actions on real-world information and provides proactive action prediction based on perceived context [36]. Specifically, these system utilized off-the-shelf multimodal models to process raw sensory data, and leverages the reasoning capabilities of LLMs to infer the semantic context. OmniQuery builds on this approach by applying these AI techniques to extract and integrate semantic context scattered across various unstructured, raw captured memories. This augmentation enhances users' memory databases, enabling them to answer personal questions about their memories through natural language queries."}, {"title": "3 DIARY STUDY", "content": "While single captured memory often lacks essential contextual information, OmniQuery proposes to augment such memories by extracting and inferring semantic context from other explicitly or implicitly related memories. To understand how to effectively augment captured memories, we need to answer the following research question:\nRQ: What contextual information is essential to integrate with captured memory instances to ensure accurate retrieval in response to user queries?\nThis question is important as \"context\" is a broad term, and thus the focus should be on categorizing and identifying the most effective contextual information that enables accurate and meaningful responses to the types of queries users generate when reflecting on past experiences."}, {"title": "3.1 Method", "content": "To answer the research question above, we conducted a diary study, a methodology that enables participants to log data whenever need arose [53]. Specifically, we adopted the snippet-based technique proposed by Brandt et al. [6]. We asked participants to log queries on their past memories only when they had real intent under a genuine context, rather than brainstorming potential questions they might ask to retrieve specific past memories. This approach enabled us to collect authentic and spontaneous queries that users have in real-world scenarios.\nWe collect the data including: (i) the queries participants would use to retrieve or ask about their past memories, (ii) the reasons and contexts prompting these queries (e.g., wanting to show a past"}, {"title": "3.2 Participants", "content": "Thirty-two participants (i.e., 14 male, 17 female, one binary) were initially recruited through an online RSVP form distributed via the X platform\u00b2. Participants come from North America and Asia. Eleven participants reported using Android devices, while the remainder used iOS devices in their daily life. Additionally, 16 participants reported actively logging their daily lives, 13 regularly logged important events and memorable experiences, 2 logged only essential information, and one seldom logged their lives. While participants were compensated based on their participation ($50 for full participation), they were not required to log a specific number queries each day or over the entire study period. This approach was intentional, as we did not want to pressure them into generating queries artificially."}, {"title": "3.3 Data Summary and analysis", "content": "During the diary study, one participant opted out during the first week, and two participants did not log any queries throughout the entire study. Of the remaining participants, seven stopped logging after the first week. The rest remained active until the end of the study. As a result, we collected a total of 299 queries. On average, each participant contributed 10.27 queries, with a standard deviation of 6.09, and the highest number of queries contributed by a single participant was 25.\nFrom the collected queries, we identified three major types of query: (1) direct content queries (75 queries), (2) context-based filters (28 queries), and (3) hybrid queries (191 queries). The rest five queries fell outside these categories as participants were attempting non memory-related tasks, such as \u201cMark yesterday pictures as favorites\".\nDirect content queries: These queries aim to get direct answers that can be retrieved by searching for memories via description (e.g., \"skateboarding in a tie-dye shirt\") or rely on information explicitly contained within a single captured instance (e.g., \u201cWhat is my driver's license number?\"). Typically, this type of query does not require additional context not contained in a single captured memory.\nContextual filters: These queries focus on retrieving memories based on specific contexts, such as time, location, or event. For example, a query like \"All the photos in Hawaii\" might only require filtering based on metadata like location. However, for more complex queries such as \u201cAll the photos from my graduation ceremony\u201d, it does require a deeper synthesis of multiple interconnected memories to reconstruct the context surrounding the event.\nHybrid queries: These queries are more complex, combining both direct content queries and contextual filters. For example, a participant asked \"Which meat did I order the last time I came to this Japanese BBQ restaurant?\" Answering such a query typically"}, {"title": "3.4 Analysis", "content": "Inspired by the psychological memory theory [56], our data summary indicates that 74.4% of the queries (contextual filters + hybrid queries) require more than just querying the direct content. The complexity in these queries require integration of contextual information in captured memories for accurate processing and filtering. Therefore, we take a step further to build a taxonomy of contextual information in user queries to inform the design of OmniQuery.\nTo identify this essential contextual information, two researchers on the team independently analyzed the logged queries, and coded, filtered, and categorized the types of context required to filter captured memories and better answer the queries. Their results were compared, and discrepancies in categorizations, hierarchy, naming, and granularity were discussed and resolved."}, {"title": "4 \u03a4\u0391\u03a7\u039f\u039d\u039fMY OF CONTEXTUAL INFORMATION", "content": "In this section, we present the taxonomy built from analyzing user queries. We identified three key types of contextual information that can be integrated with captured memories. These include (1) atomic context, (2) composite context and (3) semantic knowledge."}, {"title": "4.1 Atomic Context", "content": "Atomic context refers to contextual information typically obtainable from a single captured memory. This includes data directly from metadata, sensed from visual and auditory content, or inferred from the content itself. Table 1 shows the seven types of atomic contexts categorized from the queries. Among them, temporal information and geographical info can be directly obtained from the memory media's metadata. People and visual elements typically require machine learning models or facial recognition for detection."}, {"title": "4.2 Composite Context", "content": "Composite context is how people often remember and refer to past experiences with a single phrase, such as \"Who did I ski with in the lab retreat last year?\" These contexts can range from significant events like a wedding or a conference trip to smaller incidents like hanging out with a friend or a day trip to Seattle. Specifically, composite context is defined as a combination of multiple atomic contexts. For example, the composite context \"lab retreat\" encompasses atomic contexts including \"Feburary, 2024\" (temporal), \"Lake Tahoe, California\" (geographical) and \u201changing out with labmates\" (activity).\nWhile atomic context is typically available within a single captured memory, composite context requires integrating multiple memories to understand the connection between them. Since an individual's captured memories are linear on the timeline, memories related to a specific event tend to cluster closely together. We leveraged this temporal proximity to identify and extract various composite contexts from the raw captured memories. For a detailed discussion of this approach, please refer to Section 5.2."}, {"title": "4.3 Semantic Knowledge", "content": "In psychology theories, semantic knowledge refers to the general world knowledge that humans accumulate over time [46, 56], distinct from episodic memories, which are tied to specific experiences and events. Similarly, we can generate semantic knowledge from a user's captured memories, providing broader insights of the user's past experiences. For example, patterns like \"Jason has a habit of going to the gym 3-4 times a week\" can be inferred from multiple captured memories. Such patterns are helpful in answering queries"}, {"title": "5 OMNIQUERY: AUGMENTING CAPTURED MEMORIES", "content": "Informed by the generated taxonomy, OmniQuery employs a query-agnostic preprocessing pipeline to augment existing captured memories. The pipeline extracts scattered contextual information from interconnected captured memories, synthesizes it, and augment each memory with the enhanced context. Specifically, the augmentation pipeline involves three steps (as shown in Figure 3): (1) structuring individual captured memories via processing their content and annotating with atomic contexts, (2) identifying and synthesizing composite contexts from multiple captured memories using sliding windows, and (3) inferring semantic knowledge from multiple captured memories and the identified composite contexts."}, {"title": "5.1 Step 1: Structuring Individual Captured Memories", "content": "Raw captured memories are often unstructured and lack contextual annotation [51]. In this step, OmniQuery structures each captured memory, making it easier to analyze and extract information. Figure 4 shows an example of structuring an single captured memory, which involves two key parts: (1) processing and understanding the content of the memory and (2) annotating the memory with atomic contexts.\nProcessing content. Content of a captured memory includes the overall description of the memory as caption, visible text, and transcribed speech transcribed speech (for videos, not shown in Figure 4). Specifically, OmniQuery leverages multimodal models to process and describe the captured memory as the caption, performs optical character recognition (OCR) to recognize visible text, and uses audio-to-text models to transcribe speech."}, {"title": "5.2 Step 2: Identifying Composite Context", "content": "As captured memories are recorded in a linear manner along a personal timeline, those interconnected through semantic contexts often cluster closely together. For example, memories related to CHI 2024 are likely to occur during the event itself. Taking advantage of this temporal proximity, OmniQuery adopts a sliding window approach to analyze potentially interconnected captured in segments, identifying composite contexts.\nAs shown in Figure 5a, a static window size of seven days is used in current implementation. The inference is performed via an LLM, in which the input is the structured annotations of these memories and the output is the identified composite contexts along with their start and end dates and the associated captured memories"}, {"title": "5.3 Step 3: Inferring Semantic Knowledge", "content": "Different from composite contexts, semantic knowledge focuses on high-level general knowledge rather than specific memory details. For example, if a chat message screenshot mentions celebrating Jason's birthday, the inferred semantic knowledge might be \"Jason's birthday is on [SPECIFIC DATE].\" Similarly, analyzing multiple"}, {"title": "5.4 Implementation Details", "content": "To deduplicate images as people tend to capture similar content multiple times, we use CLIP [49] to encode images to embeddings"}, {"title": "6 OMNIQUERY: QUESTION-ANSWERING SYSTEM", "content": "With Captured memories augmented with contextual information, OmniQuery adopts a RAG architecture for the question answering system. RAG-based systems are effective in handling large datasets and mitigating hallucination issues by retrieving relevant content and grounding the generated results in this retrieved information. This approach ensures that the output is both relevant and accurate, leveraging specific data rather than relying solely on the model's internal knowledge. This approach is chosen because, on average, personal captured memories often exceed 30,000 photos and videos (as reported by participants in our diary study), which exceeds the limit of most foundation models nowadays."}, {"title": "6.1 Query Augmentation", "content": "As mentioned in Section 3.3, most user queries tend to be hybrid in nature or require contextual information. This means that directly searching based solely on the content of captured memories often results in an incomplete or insufficient retrieval of relevant memories. To enhance the retrieval process, OmniQuery adopts the query refinement concept from NLP [9] to augment the queries. The query augmentation process involves\n(1) Rewriting the query to declarative format to improve search accuracy of vector-based similarity matching;\n(2) Decomposing the query to extract necessary contextual filters, such as time, location, or events, which are grounded in the taxonomy. Note that only explicitly mentioned temporal contexts like \"... last week\u201d will be recognized temporal filters. Phrase like \u201c... during CHI 2024\" are part of a composite context, thus not counted as a temporal filter;\n(3) Inferring potential related contexts that may not be explicitly mentioned in the query but can enhance the filtering process also grounded in the taxonomy.\nFor example, as shown in Figure 6d-g, the query \"What social events did I attend during CHI 2024?\" is rewritten into a declarative format of \"The social events I attended during CHI 2024\". Since \"CHI 2024\" is explicitly mentioned and identified as a composite context, it is extracted and labeled with the appropriate composite context tag. \"Social events\" is also extracted and identified as an atomic context (activities). Additionally, because \"social events\" might include various activities like parties, dancing, or casual conversations, and likely involve multiple people present, OmniQuery infers the relevant atomic contexts (people and activities) and annotates them in the corresponding context category."}, {"title": "6.2 Retrieving Relevant Augmented Memories", "content": "The decomposed augmented query is used to comprehensively retrieve from the augmented captured memories. Specifically, the augmented captured memories consist of the structured captured memories (with processed content and annotated atomic contexts), the list of identified composite contexts and the list of semantic knowledge. OmniQuery uses the decomposed components from the augmented query to perform a multi-source retrieval, pulling related memories from each of these sources. The results are then consolidated into a comprehensive list of relevant memories, which are used to generate an accurate and detailed answer for the user's query.\nDeclarative query \u2192 Semantic knowledge & Processed content. The declarative query is first encoded into text embeddings and used"}, {"title": "6.3 Answer Generation", "content": "The retrieved results is then sent to an LLM to generate the final answer. Specifically, the input for the LLM consists of: (1) the augmented query, (2) the retrieved semantic knowledge from the list, (3) all the retrieved captured memory entries from the annotated database, including both the memory content and its associated contextual annotations.\nThe model analyzes and reasons which captured memories serve as references for the generated answer. These reference memories are also included in the final answer presented to the user. To enhance the reasoning process, OmniQuery leverages chain-of-thought prompting [59], ensuring the generation is more accurate and contextually rich. For specific prompts, please refer to Appendix A.3"}, {"title": "7 RESULTS", "content": "To demonstrate OmniQuery's capabilities in answering users' personal memory-related questions, we showcase four challenging examples of answering hybrid queries as shown in Figure 7."}, {"title": "7.1 Experiment Setting", "content": "OmniQuery was tested on the personal data of one of the researchers from the team, consisting of 2,590 images and videos downloaded from the researcher's smartphone. The data spans from mid-March to mid-August 2024, covering various activities, trips, and events, including a trip to CHI 2024, a summer trip to Hawaii, as well as captured memories of restaurants, fitness logs, and more."}, {"title": "7.2 Example Walkthrough", "content": "In example one, the user asked about the number of cardio sessions completed over the previous two months. OmniQuery successfully retrieved all relevant memories (photos of the stair master machine taken after each session in April and May) and accurately generated the answer (18 sessions), providing every instance as supporting evidence.\nIn example two, the user sought the name of a Poke restaurant they visited during CHI 2024 in Honolulu. They remembered taking a photo of the meal but forgot the restaurant's name. OmniQuery successfully retrieved the relevant memories based on the query and provided the correct answer.\nIn example three, the user asked about the hotel they stayed at during CHI. Although no explicit memories indicate the stay at the Prince Hotel, OmniQuery inferred the hotel from metadata associated with the photos taken at the venue, such as a nighttime photo of a broken takeout bag and a morning view of the marina.\nIn example four, the user asked about the length of stay during their \"second\u201d visit to Hawaii. OmniQuery was able to identify the second trip (the first was the CHI trip) and accurately count the number of stay during the visit."}, {"title": "8 USER EVALUATION", "content": "We conducted a self-guided user evaluation to let participants install and use OmniQuery on their local machine with their local album data. This is to protect participants' personal data including sensitive and personal identifiable information. In this section, we discuss the detailed process and the evaluation result."}, {"title": "8.1 Participants", "content": "We recruited 10 participants, including seven from our diary study and three additional participants via word-of-mouth. We recruited participants who have basic programming skills so that they were able to install OmniQuery from source on their local machine. They also consented to the whole process, including that their filtered personal data will be processed via an API service. All the 10 participants (4 male, 6 female, age range = 22 to 29, agemean = 25.3, agesD = 2.63) were fluent or native English speaker. Participants rated their frequency of logging their daily lives as 'Only record essential"}, {"title": "8.2 Apparatus", "content": "Two different systems were implemented in the user evaluation: the OmniQuery pipeline and a baseline system for comparison. The baseline system leverages a conventional RAG architecture, retrieving related memories based solely on vector similarities between the query and the description of the memory. The same base language model and prompt structure as OmniQuery are then used to generate the answer. For detailed implementation of the baseline, please refer to Appendix B."}, {"title": "8.3 Procedure", "content": "The user evaluation involved three stages: (1) system setup, (2) data preparation, and (3) the main testing session using user queries.\nSystem setup. Participants were given the source code for OmniQuery to install the back-end and a web application on their"}, {"title": "8.4 Comparison Metrics", "content": "After two answers were presented for a question, participants were asked to rate the two answers. We used the Chatbot Arena evaluation method [14], where participants compared answers from two systems and selected the better one or marked it as a tie. More specifically, for each question, participants rated the user perceived accuracy (UPA) and user perceived completeness (UPC) of the answers from both systems.\nThe UPA score was rated on a scale from 1 to 5: 1: Completely wrong or invalid result; 2: Incorrect, but provides at least some insight that helps answer or further refine the question; 3: Partially correct, or contains a subset of correct answers (e.g., only listing one meal when asked about all meals eaten last week); 4: Mostly correct, but missing some minor details (e.g., missing one subway trip when asked how many times I rode the subway); and 5: Completely correct. The UPC score, on the other hand, focused on the completeness and credibility of the given answers. In most cases, the questions asked by participants were challenging and needed to be explained and powered by evidence from the captured memory data. For example, when asked questions which have numeric answers like \"how many meals did I have during my last New York trip,\" the systems could be right on the numeric answer but wrong at which meals were counted. In those cases, participants examined the answers by looking back at the filtered data, and gave ratings on how they feel about the completeness and credibility of each answer.\nWe also directly compare the ratings of OmniQuery and the baseline to determine if one can outperform the other. In the comparison, if both systems have a UPA of 2 or lower (incorrect), the result is labeled \"both are bad.\" If at least one system provides a better-than-\"partially correct\" answer (\u22653), the system with the higher UPA is considered the winner. In cases where both systems"}, {"title": "8.5 Quantitative Result", "content": "Participants tested 137 queries in total during the main session. Among them, 28 were previously logged during the diary study. We manually labeled each tested query using the categorization and definition mentioned in section 3.3. As a result, 24 were categorized as direct content query while 17 were contextual filters and 96 were hybrid queries. We analyzed the performance metrics of both systems (OmniQuery and baseline) using the scores rated by the participants. Table 2 and 3 summarize our results. In addition to presenting the average UPA and UPC scores, we calculated binary accuracy to evaluate whether the systems provided mostly correct answers. An answer was considered accurate if its UPA score was equal to or greater than 4 (mostly correct) (Table 2). We also present the \"comparison result\" in Table 3, which compares the two systems head-to-head on answering personal questions.\nThe result shows that overall, OmniQuery outperforms the baseline system in both the accuracy and the completeness. Specifically, OmniQuery achieves an accuracy of 71.5%, outperforming the baseline by 28.4%, winning the comparison 52.6% of the time, and tying 21.9% of the time, while in 14.6% of the time, both results are bad. We also present the results for different categories of queries. The results indicate that simpler techniques like the baseline handle direct content queries reasonably well (62.5% accuracy, and winning or tying 41.6% of the time). While the baseline struggles with more complex queries such as contextual filters or hybrid queries (38.9% accuracy, winning or tying 31.0% of the time), OmniQuery demonstrates it capabilities in effectively handling such queries (69.0% accuracy, winning or tying 72.6% of the time)."}, {"title": "8.6 Qualitative Feedback and Findings", "content": "Apart from quantitative analysis, we also present qualitative feedback and findings from the think-aloud protocol and the exit interview on the usage of OmniQuery and suggestions of such intelligent question-answering system."}, {"title": "8.6.1 Comparing with Existing Tools", "content": "All participants have tried searching objects in their smartphone albums in their daily lives. They reported that they used the search features mostly to find a specific piece of information, like driver license, SSN number on the card, etc., matching the first type of questions (Direct Content Query) listed in section 3.3. They would also search for a specific event, like a trip to a specific location, matching the second type of questions (Contextual Filter). However, these searches are usually limited to retrieving a clear and specific object that users were looking for. They could not handle more complex questions like were collected in our diary studies. Plus, some of our participants also anticipated for this to happen because they \"know what can be searched and what cannot be searched\" from these existing album search tools (P2).\nIn the studies, a lot more challenging questions were asked. For Direct Content Queries, it would be challenging to answer when the object is ambiguous or when the users can only describe the object and do not know the exact name of it. For Contextual"}, {"title": "8.6.2 Reaction to Answers", "content": "Participants reacted differently to different types of answers. They provided their observations as well as perceived feelings on the answers. Note that since participants weren't aware of which answer was provided by OmniQuery or the baseline RAG system, we present their feedback on the overall answer structure, given that both OmniQuery and the baseline system provided a similar structure of answer and supporting materials.\nDynamics of Detail and Concise Answer. P7 mentioned that they did not always prefer to have a detailed answer composing all related media materials. More reference media would reduce the credibility of the answer to them. P8 also pointed out that if the answer just included all media it could find without a clear and concise connection to the answer, he \"might just use iOS album search to get all photos containing [a specific object] and look by myself.\""}, {"title": "8.6.3 Iterative Editing of Questions", "content": "Another recurring theme in our study is that six participants mentioned the possibility of iterating on the questions based on the answers. It occurred when participants were uncertain of what answers they were gonna get out of open-ended or subjective answers. They wanted to iterate on the questions based on the given answers, which gave them more understanding on the questions they wanted to ask. For example, P3 asked about how many places they visited during summer, and, based on the answer, realized they were more interested in the number of cities rather than the countries. It highlighted the potential need of a chat interface of personal captured memory data, which posit more challenges in this domain considering the query histories."}, {"title": "9 DISCUSSION", "content": "In this section, we draw on implications from our studies to discuss limitations and propose future work based on what we found."}, {"title": "9.1 From Chat Interface to Multimodal Interactions", "content": "As a system designed to answer users' questions on their personal captured memory, OmniQuery is currently designed in an ask-and-react manner for the purpose of evaluating its efficacy in a lab-study settings. In our studies, participants were excited about what OmniQuery was capable of, and gave feedback on having more multimodal interactions rather than a \"ChatGPT-like\" interface. We recognize the potential of a more interactive OmniQuery in the following ways:\nMultimodal Input and Output: Just like how ChatGPT iterated, OmniQuery could use more multimodal input including but not limited to audio, image, and even videos in the future. Recalling the summarized task cases which can be hard for existing album searching tools to accomplish, a number of these cases can inherently benefit from multimodal inputs. For example, users might want to look up and locate an oddly-shaped cup that they cannot refer to in plain text, or compare all existing dresses in album with a reference color on an image that is hard to describe. Natural language description's limitation needs to be addressed by introducing"}, {"title": "9.2 Enriching Memory Data and Visual Intelligence", "content": "At present, OmniQuery primarily processes media from a smartphone's photo album as its main source for captured memories. However, these media alone provide a limited view of a user's broader personal knowledge. For example, in one of the study's failure cases, OmniQuery struggled to infer personal relationships from social interactions captured in group photos. To enhance memory augmentation and improve retrieval accuracy, expanding OmniQuery's data sources and visual intelligence is essential.\nIntegrating additional data sources: Personal knowledge extends beyond photo albums and exists across various applications. While our participants' photo albums included screenshots of emails, calendar events, and chat histories, these represent only a fraction of the broader personal information available in other communication and social interaction apps. Incorporating data from such sources could significantly enhance OmniQuery's contextual understanding, allowing for more complex queries and richer memory retrieval. However, integrating these additional data sources presents substantial privacy and ethical challenges. While our evaluations were conducted entirely on users' local machines and did not explore privacy-preserving implementations in detail, existing research efforts, such as those focused on differential privacy and on-device machine learning, offer promising directions for secure and privacy-aware deployment. Additionally, commercial tools like Apple Intelligence's private cloud computing serve as examples of ongoing progress in protecting user data while enabling advanced memory retrieval capabilities."}, {"title": "9.3 Preserving Privacy", "content": "As was discussed in above subsections, protecting users' privacy is crucial in developing future personal AI assistants, including but not limited to handling personal data such as media in albums, chat histories or browsing history. Users have limited control over how their data is handled and must rely on service providers' adherence to privacy protocols. In this subsection, we take a step further to discuss more robust and rigorous measures should be adopted in real-world settings, where the amount of personal data is huge, making approaches like manual filtering in OmniQuery's evaluation infeasible.\nOne way is to incorporate more advanced data protection techniques on cloud servers including data anonymization [42] and encryption [47], while preserving the computational capabilities of large models via online computing. The other approach is leveraging on-device computing, where all data processing occurs locally on the user's device, ensuring full control over users' own data. Recent advances in model compression [25] have made it possible to run large model on smaller devices like smartphones. As OmniQuery is designed to be model-agnostic, it is able to work with different model sizes. While smaller, compressed on-device model may result in reduced performance, future work should focus on developing curated datasets and benchmarks to rigorously evaluate OmniQuery 's performance across different model sizes"}]}