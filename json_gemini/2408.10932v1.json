{"title": "The Evolution of Reinforcement Learning in Quantitative Finance", "authors": ["Nikolaos Pippas", "Cagatay Turkay", "Elliot A. Ludvig"], "abstract": "Reinforcement Learning (RL) has experienced significant advancement over the past decade, prompting a growing interest in applications within finance. This survey critically evaluates 167 publications, exploring diverse RL applications and frameworks in finance. Financial markets, marked by their complexity, multi-agent nature, information asymmetry, and inherent randomness, serve as an intriguing test-bed for RL. Traditional finance offers certain solutions, and RL advances these with a more dynamic approach, incorporating machine learning methods, including transfer learning, meta-learning, and multi-agent solutions. This survey dissects key RL components through the lens of Quantitative Finance. We uncover emerging themes, propose areas for future research, and critique the strengths and weaknesses of existing methods.", "sections": [{"title": "1 Introduction", "content": "Over the past decade, interest and development in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), have grown significantly. Progress in RL is notable in gaming, with recent algorithms achieving and surpassing human-level proficiency in Go, Chess, and StarCraft [288, 289, 316]. These advancements have spurred the exploration of RL in finance, particularly Quantitative Finance (QF). Although RL shows promise, it also faces steep challenges in the complex and dynamic QF domain. These challenges include the unpredictability of financial markets, high computational demands, and the need for robust model interpretability [89]. In addition, RL applications in finance must address critical challenges such as the transition from simulation to real-world application, sample efficiency, and the balance between online and offline RL settings. This paper reviews RL applications in QF, encompassing key aspects such as Portfolio Management (PM), option pricing, and asset allocation. However, these potential advantages come with the caveat that RL models must be carefully designed and validated to avoid overfitting and ensure robustness in real-world financial markets.\nPrior to RL's introduction to specific domains in finance, traditional Machine Learning (ML) methods were employed in attempts to establish successful trading systems. Atsalakis and Valavanis [17] provide a comprehensive survey of these practices, including advanced machine learning techniques like Neural Networks (NN) and neuro-fuzzy systems for financial market forecasting. Furthermore, Ozbayoglu et al. [252] review recent developments and attempts focused on Deep Learning (DL)\u00b9. The general pattern in these methods consists of a two-step process:\n(1) The training of an ML model such as a Support Vector Machine (SVM), NN, or Decision Tree with a specific dataset (features), followed by the generation of a forecast or signal over n periods ahead;\n(2) The integration of this forecast or signal into a trading system to determine actual trading action or holdings (e.g., buy, sell, or hold in three discrete representations) at a single stock or portfolio level.\nDespite the interest in the academic literature, this framework has several limitations [236], which can potentially be better addressed by RL:"}, {"title": "1.1 Reinforcement Learning and Finance", "content": "A good definition of RL is provided by Sutton in [302]: \"Reinforcement learning is learning what to do-how to map situations to actions-so as to maximise a numerical reward signal.\" This simple sentence conveys all the crucial notions of RL. To unpack this point further, the general flow of an RL system is laid out in Figure 1. The agent receives information about the state $S_t \\in S$ and a reward at time t and reacts upon this information with actions $A_t \\in A$. The resulting action feeds back to the environment, and the system generates a new state $S_{t+1}$ and a new numerical reward $R_{t+1} \\in R \\subset R$ at time $t + 1$. The goal is to learn a policy to maximise the expected total reward, which effectively creates a trajectory that can be represented as follows:\n$S_0, A_0, R_1, S_1, A_1, R_2, ...$\nRL is a framework for solving sequential decision-making problems. Naturally, many real-world applications fit this framework. For example, video games, robotic control, and driving can be seen as sequential decision-making problems [112, 231, 288]. The framework above also fits in finance, particularly QF. The agent could be a trader or a portfolio manager who observes the current state of financial markets (the environment) and acts upon this information to maximise a reward function (e.g., SR). Figure 2 illustrates a general RL-based framework in the context of QF, mapping key concepts such as the agent, state, action, reward, and environment. The figure shows several options for these components, depending on the specific QF application context."}, {"title": "1.2 Temporal Dynamics in Financial Applications", "content": "The time unit in the context of QF can vary depending on the specific application. It could be seconds, minutes, hours, days, months, or even years. The choice of time unit depends on the frequency of trading strategies employed. For example, high-frequency trading strategies might use milliseconds or microseconds, while long-term investment strategies might use days, weeks, or, in rare cases, even years. Moreover, in some QF applications, the rewards might not be immediate but delayed. For example, a reward might be given only at t + 12, representing a delayed return on investment, such as dividends. The RL framework must account for such delayed rewards, ensuring that the agent can still optimise its policy to maximise long-term gains despite the delay. This requires careful consideration of the timing and valuation of future rewards to ensure robust performance."}, {"title": "1.3 Multi-Agent systems in Finance", "content": "In RL, Multi-Agent Systems (MAS) extend the single-agent paradigm to environments where multiple agents interact, either cooperatively or competitively. The multi-agent model formalism in RL is crucial for addressing complex, dynamic and decentralised decision-making processes found in financial markets [52, 263]. Generalising from the single agent, we have:\n\u2022 Agents: Each agent i has its own set of states $S_i$, actions $A_i$, and policies $\\pi_i$. Agents can have distinct state spaces $S_i$ depending on their roles and the information they can access. Similarly, agents can have distinct action spaces $A_i$, reflecting their different capabilities or roles in the environment.\n\u2022 State Space: The joint state space S is a combination of all individual states $S = S_1 \u00d7 S_2 \u00d7 ... \u00d7 S_n$.\n\u2022 Action Space: The joint action space A is the Cartesian product of all individual action spaces $A = A_1 \u00d7 A_2 \u00d7 ... \u00d7 A_n$.\n\u2022 Reward Function: The reward function $R : S \u00d7 A \u2192 R^n$ provides a vector of rewards for all agents, where each component $R_i$ corresponds to the reward received by agent i."}, {"title": "1.4 Contribution and Paper Organisation", "content": "This survey assesses 167 publications to provide the first comprehensive review of the critical components of an RL agent within QF. Furthermore, our review demonstrates the diverse applicability of RL in different QF contexts. Although the recent surge in interest in RL within finance has produced several survey publications [66, 105, 139, 239], our study is distinctively focused on QF, diverging from other studies that encompass broader economic applications [66, 239] and rooted more closely within the literature"}, {"title": "2 Critical Considerations for RL in QF", "content": "Before delving into the research and implementation of RL in QF, we introduce some critical dimensions and considerations of RL in QF. Figure 2, as we covered in subsection 1.1, expands the generic RL framework and illustrates several options for potential mappings between key RL components and relevant concepts, techniques and practices from QF. In addition to these, there are some critical considerations for incorporating RL in QF including the transition from simulation to real-world applications, sample efficiency, online versus offline RL settings, on-policy versus off-policy frameworks, and how RL interacts with the unique challenges of financial markets."}, {"title": "2.1 Transition from Simulation to Real-World Application", "content": "Deploying RL models in real financial markets presents unique challenges. Unlike controlled simulation environments, real-world markets are unpredictable and influenced by myriad factors such as information asymmetry, variable transaction costs, taxes, and noise traders, which can significantly affect model performance [236, 297]. Therefore, a robust validation and simulation process is essential to ensure that RL models accurately capture the complexities of chaotic financial markets. Incorporating domain knowledge improves model resilience and adaptability, providing a more accurate reflection of market conditions, and improving"}, {"title": "2.2 Sample Efficiency", "content": "Improving sample efficiency is crucial in RL, especially in financial applications where data can be sparse, have a short history, or be expensive to obtain [139]. Many samples drawn from the environment are often necessary for stable convergence and low variance. Techniques such as model-based RL, experience replay, and transfer learning are valuable in this context.\nModel-free methods are typically slower to learn, whereas model-based methods build a transition model from the environment's feedback, allowing the agent to use this model to understand the effects of actions on states and rewards [263]. Model-based RL can generate additional training data, enhancing learning efficiency [288]. For instance, model-based RL can simulate various market conditions to create synthetic data in a stock-trading scenario. This data helps the model learn how to handle different market situations. However, for this approach to be effective, the model must accurately represent the environment. Experience replay allows RL agents to reuse past experiences, thereby improving learning from limited data [232]. In the context of financial applications, agents can reuse previous experiences to improve learning efficiency. Transfer learning enables leveraging knowledge from related tasks to improve performance in new but similar tasks, thus addressing data scarcity [187]. For a discussion of transfer learning and model-based RL, see subsection 6.3 and Section 9. When the transition model is accurate, model-based RL can also serve as a transfer learning mechanism, further enhancing learning efficiency [263]."}, {"title": "2.3 Online vs. Offline RL Settings", "content": "Online learning in QF refers to agents learning and adapting continuously as new data arrive. This approach benefits from the ability to dynamically update strategies in response to market changes. However, this can be highly impractical in HFT, where decisions occur in microseconds to milliseconds, because online learning requires real-time updating of algorithm parameters and significant computational resources to process new data. As pointed out by Hambly et al. [139], a potential solution in this setup might be to collect data with a pre-specified exploration scheme during trading hours and update the algorithm with newly collected data after trading closes. Transfer learning can also be leveraged in this context to bridge the gap between offline and online learning. Initially, the RL agent can be pre-trained using offline RL on historical data, learning general strategies and patterns. Once deployed, the agent can then use online learning to fine-tune these strategies in response to real-time market conditions. This hybrid approach can mitigate some of the computational burdens of online learning by reducing the frequency and extent required for real-time updates. Also, certain components of the RL framework can be constructed to support online learning, such as the introduction of Differential SR [235-238], which is an online version of SR (see subsection 5.3).\nIn contrast, offline RL uses historical data to develop strategies without continuous interaction with the environment. This method is particularly useful for backtesting and strategy development, where real-time data streams are not necessary. Offline RL can leverage vast amounts of historical market data to train models and then be deployed in live environments after thorough validation. Furthermore, offline learning does not face the computational constraints that online learning faces, allowing for extensive preprocessing and model training using conventional computing resources. Though training time can be extensive due to large datasets, it avoids the real-time computational demands of online RL."}, {"title": "2.4 On-Policy vs. Off-Policy Frameworks", "content": "The distinction between on-policy and off-policy methods significantly affects their application in various financial contexts, each presenting unique challenges and opportunities.\nOn-policy algorithms, such as Proximal Policy Optimisation (PPO) [280], use data generated only from the current policy. This means that only actions taken by the current policy generate data at each training iteration. Consequently, the data become unusable after policy updates, making on-policy methods inefficient. However, on-policy algorithms are typically more stable and exhibit lower variance because they learn directly from the policy they are improving [263]. For example, SARSA [273], an on-policy algorithm, is used in stock trading models due to its ability to adapt to market changes efficiently, resulting in lower computational resources requirements and robust performance under varying market conditions [77, 86]. Furthermore, on-policy methods could be practical in HFT due to the vast amount of data available despite being sample inefficient. In PM, trading at lower frequencies (e.g. monthly) could be less practical due to the scarcity of available data."}, {"title": "3 Main Reinforcement Learning Methods", "content": "Contemporary literature distinguishes four primary RL methods: Value-based (Critic-only), Policy-based (Actor-Only), Actor-Critic, and model-based. This section delves into a thorough exploration of the principal RL techniques encapsulated within these categories. We provide an overview of the general framework, the advantages and disadvantages, and the applicability of each method within the field of QF. Additionally, within these categories, we incorporate other forms of RL approaches such as multi-agent methods [157, 191]."}, {"title": "3.1 Value-Based Methods", "content": "3.1.1 Core Framework for Value-based RL in QF. Value-based methods are foundational in RL. The agent's objective is to learn the value of different actions in various states to maximise cumulative rewards. Neuneier's work [244] is one of the earliest contributions in this field, which set a methodological precedent that has informed many subsequent studies. This section outlines a generic framework derived from these methodologies:\n(1) Define a Finite Set of States $S_t$: States represent environment-derived information at each time point $t \\in \\{1, 2, ..., T\\}$. This information includes financial accounting data, prices, sentiment, and technical indicators.\n(2) Define a Set of Actions $A_t \\in \\{Buy, Sell, Hold\\}$: These are the possible actions of the agent at each time $t \\in \\{1, 2, ..., T\\}$.\n(3) Establish Transition Probabilities: These probabilities, typically unmodelled, define state transitions based on actions.\n(4) Formulate a Reward Function $R_t$: Provides numerical feedback to the agent in response to its preceding action.\n(5) Create a Policy $\\pi$: Maps states to actions for the agent to follow.\n(6) Construct a Value Function $V$: Maps states to the agent's expected total discounted reward from a given state until the episode's end under policy $\\pi$."}, {"title": "3.1.2 General Observations, Comments and Definitions for Value-based Methods.", "content": "Value-based methods are a well-established branch of research in RL applied to trading systems, despite their noted shortcomings. A significant challenge is the discrete nature of the action space, complicating practical trading applications 5.2.\nThe agent creates a value function to estimate the results of actions such as buying, holding, or selling, helping to choose the best action. This approach often uses model-free RL algorithms like Q-learning [330] and SARSA [273] to optimise the expected total reward. Q-learning, an off-policy RL algorithm, is proven to converge to the optimal solution in the tabular setting under specific conditions [331]. In Q-learning, the Q-values, $Q (S_t, A_t)$, represent the expected rewards for taking an action $A_t$ in state $S_t$. These Q-values are updated according to the rule:\n$Q (S_t, A_t) \\leftarrow Q (S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q (S_{t+1}, a) \u2013 Q (S_t, A_t)]$.\nThis update rule adjusts the current Q-value by incorporating the observed reward $R_{t+1}$ and the maximum estimated future reward, weighted by the learning rate $\\alpha \\in (0, 1]$. Over time, this process helps refine the Q-values, guiding the agent toward the optimal action values under specific conditions. Specifically, Q-learning is guaranteed to converge to the optimal solution with probability 1, provided that all state-action pairs are visited infinitely many times and the learning rate satisfies certain decay conditions. These convergence conditions are crucial for ensuring that the algorithm performs well in practice.\nQ-learning and SARSA are traditionally used in tabular settings, which are suitable for small, discrete state spaces. However, these algorithms can also be adapted to more complex state spaces through function approximation techniques. While tabular methods are inherently limited in scalability, FA allows these algorithms to generalize from similar states, making them applicable"}, {"title": "3.2 Policy-Based Method", "content": "3.2.1 Core Framework for Policy-Based RL in QF. Policy-based methods in RL focus on directly optimising the policy that dictates the agent's actions. Unlike Value-based methods, which estimate the value of actions to derive the best policy, Policy-based methods directly search for the optimal policy that maximises the cumulative reward. This approach can be particularly advantageous in environments with continuous action spaces and complex dynamics.\nAmong these methods, the RRL framework stands out due to its extensive use in the reviewed literature and its clear relevance to finance [235-238]. The RRL framework is particularly suited for financial applications because it can capture the temporal dependencies and sequential nature of trading decisions. Therefore, this subsection will focus on the RRL framework. Following Moody and Wu [235], an agent's actions are represented by:\n$A_t = A (\\theta_t; A_{t-1}, I_t) \\in \\{-1,0, 1\\}$ with $I_t = \\{z_t, z_{t-1}, z_{t-2}, ..., y_t, y_{t-1}, y_{t-2}, ...\\}$.\nHere, $\\theta_t$ refers to the learned parameters of the agent, $A_t$ corresponds to a trading position that can assume one of three states - sell, neutral, or buy, while $A_{t-1}$ represents the preceding action at time t \u2013 1, $I_t$, which denotes the information set at time t, is used as a state representation, composed of lagged asset prices $z_t$ and other external variables $y_t$. $A_t$ can be defined [238] as:\n$A_t = sign (\\upsilon A_{t-1} + v_0 r_t + v_1 r_{t-1} + ... + v_m r_{t-m} + w)$,where, $r_t$ denotes the price return, and $\\theta_t = \\{u, v_i, w\\}$ with $i \\in \\{0, 1, ..., m\\}$. The RRL framework further optimises the trading system by maximising a performance function $U_t$. This function can represent different goals, such as maximising profit, improving a utility function of wealth, or enhancing performance ratios like the Sharpe Ratio (SR). Essentially, it helps the trader aim for better financial performance based on their specific goals. One of those choices is the additive profits utility reward function [238]:\n$U_t(\\theta) = P_T = \\sum_{t=1}^T R_t = \\mu \\sum_{t=1}^T ( A_{t-1} ( r_t \u2013 \\delta_t A_{t} ) - \\delta_t A_{t-1} )$, where, $P_T$ denotes the cumulative profit at the end of trading period T, $R_t$ the profit or loss at time t, and T the total time steps.$\\mu > 0$ represents a fixed position size when buying/selling a stock, $r_t$ and $r_f$ are the absolute prices changes for the period from t \u2013 1 to t for the risky asset (stock) and the risk-free asset (e.g., T-Bills), respectively, and $\\delta_t$ denotes transaction costs from buying/selling the risky asset."}, {"title": "3.2.2 General Observations, Comments and Definitions for Policy-Based Methods.", "content": "Policy-based methods represent the second most explored area of RL in the reviewed literature. These methods provide a direct mapping from states to actions, eliminating the need to compute the expected outcome of different actions as in the Value-based approach, resulting in faster learning processes.\nA significant advantage of Policy-based methods is the continuous action space for the agent. Consider a portfolio of stocks: with the Value-based approach, portfolio weights can only take discrete values like buy, sell, or hold. In contrast, the Policy-based approach allows portfolio weights to assume any value in [0, 1] in the long-only case.\nPolicy-based methods exhibit robust performance even when dealing with noisy datasets, common in stock-related data [90, 238]. Furthermore, Policy-based methods typically converge more swiftly than Value-based methods [133], although the high variance of the gradient leads to a slower learning rate [178]. These methods inherently perform exploration [263], as the stochastic policy yields a distribution over the action space. However, Policy-based methods require a differentiable reward function. Policy-based approaches in finance were pioneered by Moody and Wu [235], with many variants since then proposed. RRL typically uses a recurrent NN structure, creating dependencies over previous steps and facilitating multi-period optimisation."}, {"title": "3.3 Actor-Critic Method", "content": "3.3.1 Core Framework for Actor-Critic RL in QF. The Actor-Critic method in RL is a hybrid approach that combines the strengths of Policy-based and Value-based methods to create a robust learning framework. It consists of two main components: the Actor and Critic modules. The Actor module takes the state St as input and determines the action At at time t. The Critic module subsequently receives the state St and the action At determined by the Actor module, evaluates the state-action pair, and computes the reward, adhering to the general framework outlined in subsection 3.1. Generic Actor-Critic frameworks are discussed in [263, 302]. The core components of an Actor-Critic framework can be described as follows:\n\u2022 Actor: The Actor is responsible for selecting actions based on the current state. It learns a policy, which is a mapping from states to actions. This policy can be:\nDeterministic: The Actor always chooses the same action for a given state.\nStochastic: The Actor chooses actions according to a probability distribution.\n\u2022 Critic: The Critic evaluates the action taken by the Actor by computing a value function. This value function estimates the expected cumulative reward (discounted over time) of being in a given state and taking a particular action.\n\u2022 Advantage Function: The advantage function helps to determine how much better or worse a particular action is compared to the average action taken from that state. It is defined as:\n$A(S_t, A_t) = Q(S_t, A_t) \u2013 V(S_t)$,\nwhere Q is the action-value function and V is the state-value function.\n\u2022 Gradient Ascent: Both the Actor and the Critic are trained using gradient ascent. The Actor updates its policy parameters to maximise the expected cumulative reward, while the Critic updates its parameters to provide more accurate evaluations of the actions.\n\u2022 TD Error: Temporal Difference (TD) error is used to update both the Actor and the Critic. It is the difference between the expected reward and the actual reward received, given by:\n$\\delta_t = R_{t+1} + \\gamma V (S_{t+1}) \u2013 V(S_t)$."}, {"title": "3.3.2 General Observations, Comments and Definitions for Actor-Critic Method.", "content": "Among the areas covered in this survey, the Actor-Critic method is the least represented. Yet, it remains among the most compelling of the four primary approaches, as it combines the advantages of both Policy-based and Value-based RL methods. As indicated in subsection 3.2, a notable challenge with Policy-based methods is their high variance, which may result in slower convergence or the propensity to become stuck in local optima. The Critic component in Actor-Critic methods mitigates this by providing a value function that stabilises policy-gradient updates, reducing variance. Moreover, Value-based RL methods are prone to high bias due to approximation errors in the value function [263]. The Actor component in Actor-Critic methods helps to directly optimise the policy, reducing bias by ensuring continuous adjustment based on accurate evaluations of actions. Several Actor-Critic algorithms featured in this literature are specifically engineered to surmount these obstacles. The algorithmic frameworks predominantly observed within the literature encompassed by this survey include the Deterministic Policy Gradient (DPG) [287], Deep Deterministic Policy Gradient (DDPG) [208], Asynchronous Advantage Actor-Critic (A3C) and the non-parallel version (A2C) [233], Trust Region Policy Optimisation (TRPO) [279], Proximal Policy Optimisation (PPO) [280], Soft Actor-Critic (SAC) [137].\nThough comparatively under-researched in QF, the Actor-Critic category shows significant promise in contemporary applications. Various methods within this category, as documented in works by Mnih et al. [208, 233] and Lillicrap et al., [208], have demonstrated state-of-the-art performance, marking this as a potential area for future QF breakthroughs."}, {"title": "3.4 Model-Based RL", "content": "3.4.1 Core Framework for Model-Based RL in QF. In model-free RL methods, the agent updates a policy directly from the environment's feedback on its actions. On the contrary, in model-based methods involve constructing a model of the environment, which is then used to simulate and plan actions. A foundational framework can be described as follows:\n(1) Define a Finite Set of States St: States represent environment-derived information at each time point t. This information includes financial accounting data, prices, sentiment, and technical indicators.\n(2) Define a Set of Actions At: These are the possible actions of the agent at each time t, such as Buy, Sell, and Hold.\n(3) Learn Transition Probabilities: Construct a transition model that predicts the next state St+1 and reward Rt+1 given the current state St and action At. This model can be a neural network or any other function approximator.\n(4) Formulate a Reward Function Rt: Provides numerical feedback to the agent in response to its preceding action, incorporating factors such as profit, risk, and transaction costs.\n(5) Planning and Policy Optimisation: The learnt model can be used to simulate future states and rewards, allowing the agent to plan and optimise actions. Techniques such as Monte Carlo Tree Search (MCTS) or Dynamic Programming can be used for this purpose [263, 302]."}, {"title": "3.4.2 General Observations, Comments and Definitions for Model-Based RL.", "content": "Model-based methods in RL offer several advantages and present unique challenges, particularly when applied to financial trading systems 6:\n(1) Learning Speed: Model-based RL methods typically learn faster than model-free approaches by using the learnt model for planning and action optimisation, crucial for timely financial market decisions.\n(2) Computational Complexity: Simulating and planning with complex financial models is computationally intensive, requiring efficient algorithms and high-performance computing, especially for HFT applications.\n(3) Risk Management: Robust risk management is vital. Model-based RL can simulate extreme market scenarios to assess potential risks, helping to develop strategies that maximise returns and manage risks effectively."}, {"title": "4 Environment", "content": "4.1 Introduction\nIn the RL framework, the environment characterises the current state of the system. The agent, the learner, and decision maker interact with this environment, selecting actions based on state information. This setup requires that the agent and the environment"}, {"title": "4.2 Features", "content": "Modelling financial markets with their inherent randomness is a considerable challenge. Consequently, input data, feature selection, and extraction become critical to the success of an RL system. The literature presents a diverse range of data sources, features, and feature selection mechanisms, reflecting the current state of research and highlighting the domain-specific relevance of different features."}, {"title": "4.2.1 Price History.", "content": "Price history and Open-High-Low-Close (OHLC) or OHLCV when incorporating volume [156, 311] values are prevalent features in the literature for assets such as stocks or bonds, along with their derivatives, like 20-day lagged returns [68] or co-integration [94] based features [197]. However, reliance on price-based features should not hinder the consideration of other features. For example, Sherstov and Stone [284] found that using a single price feature underperformed compared to benchmark strategies, hinting at the need for a richer state representation to capture the complexities of the financial market. Moreover, Benhamou et al. [35] noted the high correlation between OHLC features, which could introduce input noise. Thus, feature selections on raw features might be a more effective strategy in certain contexts, as discussed in subsection 4.3.\nVolatility, a critical feature derived from historical prices, is surprisingly overlooked in the literature, despite its crucial role in PM [226] and in identifying shifts in financial markets (regime changes) [188]. Its absence in RL-based PM applications [213, 344, 345] is noteworthy given its importance [35]. Volatility has only recently been used in PM applications [35-38]. In other RL-based finance applications, different volatility measures have been successfully used as a tool to discover regime changes [223-225]. In effect, the authors extended the original work of Moody and Wu [235] by adding a regime-switching extension to the RRL framework. In Bekiros [31], a comparison is made between changes in 20-day volatility and the previous day, while Tan et al. [307] used the standard deviation of a stock price and its correlation with the Dow Jones index to identify stock-specific cycles. Zhang and Maringer [351] included Garch [46] volatility in state representation, marking its first use in an Policy-based approach. Finally, certain authors incorporate the full covariance of closing prices in state modelling [171]."}, {"title": "4.2.2 Technical Analysis.", "content": "Technical analysis consists of indicators or rules designed to predict the direction of the price using past prices and volume variables. However, the effectiveness of these indicators is often challenged by the Efficient Market Hypothesis (EMH), which posits the unpredictability of stock market prices [97]. As such, research on these indicators has produced mixed results, and some studies have methodological issues in their testing procedures [254].\nDespite such criticism, numerous academics have questioned the EMH, arguing that future prices can be predicted based on past prices, thus challenging the random walk hypothesis [214]. Consequently, several researchers have used technical indicators to represent the agent's environment within this domain, including [81-83, 255, 327, 338, 352]. However, there are also publications that question the efficacy of technical analysis in the realm of RL in QF [83].\nKey technical indicators often used include the Moving Average (MA), Exponential Moving Average (EMA), Moving Average Convergence/Divergence (MACD), Japanese candlestick, and the Relative Strength Index (RSI) [241]. Moreover, some researchers have created features based on the original definitions of technical indicators, demonstrating the adaptability of these tools [116]."}, {"title": "4.2.3 Fundamental Data and Factor Investing.", "content": "Accounting data from financial statements underpin traditional factor investing, with numerous strategies developed over half a century based on such data aimed at explaining expected asset returns [72]. As identified by Harvey and Liu [142], more than 400 factors have been proposed, covering a wide spectrum from macroeconomic to statistical commonalities. With advances in computational power, the discovery of these factors and their risk premiums has been streamlined. Various research studies, for example, [98], have revealed factors that explain equity returns and contribute to long-term active outperformance. Prominent factors include Price momentum [162], Value [98], Size10 [24], Quality11 [132], Low beta12 [113]. These factors can be employed as features to describe the RL agent's environment.\nThese features are not often used in the RL-based financial literature, likely due to the infrequent updates of accounting data, which occur quarterly. This temporal mismatch, as in the case of the price-to-earnings ratio, can limit the effectiveness of these factors as daily signals. However, some researchers have begun to integrate accounting data or investment factors derived from RL. Zhang and Maringer [349, 351] pioneered the use of accounting ratios in an evolutionary feature selection scheme, while Wang et al. [323] included ratios like price to earnings in state representations. More recently, Coqueret and Andr\u00e9 [78] demonstrated the effective incorporation of various investment factors, including those rooted in accounting data, into RL-based trading systems."}, {"title": "4.2.4 Order Book.", "content": "Among the literature surveyed, we find a substantial focus on the areas of trade execution 13 and market making in financial markets. These specific applications tend to employ a unique set of features to model the environment, distinguishing them from other domains. While we explore these applications more comprehensively in subsections 7.4 and 7.5, it is beneficial to highlight some of the most commonly observed features in the associated literature. For example, Chan and Shelton [64] used order imbalance-based features14, depth of the market15, and time to fill a limit order, among others. Other authors used the Bid-Ask spread16, and remaining inventory, among others, as part of their environment [248]. Numerous other features are also deployed in this context, but a detailed discussion is omitted here for brevity."}, {"title": "4.2.5 Sentiment Data.", "content": "The impact of sentiment data on stock prices is well documented in the financial literature [11, 309]. Its application in RL strategies has also been demonstrated [104, 157, 172, 180, 243, 276, 342, 344]. Feuerriegel and Prendinger [104] pioneered the use of sentiment data, employing DGAP for an RL-based trading strategy. Kaur [172] followed, enriching the environment of the Q-learning agent with sentiment data. In particular, diverse sources such as Reuters News Corpus, Twitter, Google News, and Thomson Reuters News analytics have been used to create sentiment signals for RL strategies, affirming that diverse information mitigates uncertainty in the environment [180, 243, 276, 342, 344]. However, the use of sentiment/news data presents challenges. It primarily covers prominent companies, limiting its utility to lesser-known stocks. Despite sentiment's potential value, its integration into state representation remains scarce in published work."}, {"title": "4.2.6 Macroeconomic.", "content": "Macroeconomic data plays a pivotal role as it encapsulates the overarching economic landscape"}]}