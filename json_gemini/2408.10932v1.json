{"title": "The Evolution of Reinforcement Learning in Quantitative Finance", "authors": ["NIKOLAOS PIPPAS", "CAGATAY TURKAY", "ELLIOT A. LUDVIG"], "abstract": "Reinforcement Learning (RL) has experienced significant advancement over the past decade, prompting a growing interest in applications within finance. This survey critically evaluates 167 publications, exploring diverse RL applications and frameworks in finance. Financial markets, marked by their complexity, multi-agent nature, information asymmetry, and inherent randomness, serve as an intriguing test-bed for RL. Traditional finance offers certain solutions, and RL advances these with a more dynamic approach, incorporating machine learning methods, including transfer learning, meta-learning, and multi-agent solutions. This survey dissects key RL components through the lens of Quantitative Finance. We uncover emerging themes, propose areas for future research, and critique the strengths and weaknesses of existing methods.", "sections": [{"title": "1 Introduction", "content": "Over the past decade, interest and development in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), have grown significantly. Progress in RL is notable in gaming, with recent algorithms achieving and surpassing human-level proficiency in Go, Chess, and StarCraft [288, 289, 316]. These advancements have spurred the exploration of RL in finance, particularly Quantitative Finance (QF). Although RL shows promise, it also faces steep challenges in the complex and dynamic QF domain. These challenges include the unpredictability of financial markets, high computational demands, and the need for robust model interpretability [89]. In addition, RL applications in finance must address critical challenges such as the transition from simulation to real-world application, sample efficiency, and the balance between online and offline RL settings. This paper reviews RL applications in QF, encompassing key aspects such as Portfolio Management (PM), option pricing, and asset allocation. However, these potential advantages come with the caveat that RL models must be carefully designed and validated to avoid overfitting and ensure robustness in real-world financial markets.\nPrior to RL's introduction to specific domains in finance, traditional Machine Learning (ML) methods were employed in attempts to establish successful trading systems. Atsalakis and Valavanis [17] provide a comprehensive survey of these practices, including advanced machine learning techniques like Neural Networks (NN) and neuro-fuzzy systems for financial market forecasting. Furthermore, Ozbayoglu et al. [252] review recent developments and attempts focused on Deep Learning (DL)\u00b9. The general pattern in these methods consists of a two-step process:\n(1) The training of an ML model such as a Support Vector Machine (SVM), NN, or Decision Tree with a specific dataset (features), followed by the generation of a forecast or signal over n periods ahead;\n(2) The integration of this forecast or signal into a trading system to determine actual trading action or holdings (e.g., buy, sell, or hold in three discrete representations) at a single stock or portfolio level.\nDespite the interest in the academic literature, this framework has several limitations [236], which can potentially be better addressed by RL:"}, {"title": "1.1 Reinforcement Learning and Finance", "content": "A good definition of RL is provided by Sutton in [302]: \"Reinforcement learning is learning what to do-how to map situations to actions-so as to maximise a numerical reward signal.\" This simple sentence conveys all the crucial notions of RL. To unpack this point further, the general flow of an RL system is laid out in Figure 1. The agent receives information about the state \\(S_t \\in S\\) and a reward at time t and reacts upon this information with actions \\(A_t \\in A\\). The resulting action feeds back to the environment, and the system generates a new state \\(S_{t+1}\\) and a new numerical reward \\(R_{t+1} \\in R \\subset R\\) at time t + 1.\u2075 The goal is to learn a policy to maximise the expected total reward, which effectively creates a trajectory that can be represented as follows:\n\\(S_0, A_0, R_1, S_1, A_1, R_2, ...\\)\nRL is a framework for solving sequential decision-making problems. Naturally, many real-world applications fit this framework. For example, video games, robotic control, and driving can be seen as sequential decision-making problems [112, 231, 288]. The framework above also fits in finance, particularly QF. The agent could be a trader or a portfolio manager who observes the current state of financial markets (the environment) and acts upon this information to maximise a reward function (e.g., SR). Figure 2 illustrates a general RL-based framework in the context of QF, mapping key concepts such as the agent, state, action, reward, and environment. The figure shows several options for these components, depending on the specific QF application context."}, {"title": "1.2 Temporal Dynamics in Financial Applications", "content": "The time unit in the context of QF can vary depending on the specific application. It could be seconds, minutes, hours, days, months, or even years. The choice of time unit depends on the frequency of trading strategies employed. For example, high-frequency trading strategies might use milliseconds or microseconds, while long-term investment strategies might use days, weeks, or, in rare cases, even years. Moreover, in some QF applications, the rewards might not be immediate but delayed. For example, a reward might be given only at t + 12, representing a delayed return on investment, such as dividends. The RL framework must account for such delayed rewards, ensuring that the agent can still optimise its policy to maximise long-term gains despite the delay. This requires careful consideration of the timing and valuation of future rewards to ensure robust performance."}, {"title": "1.3 Multi-Agent systems in Finance", "content": "In RL, Multi-Agent Systems (MAS) extend the single-agent paradigm to environments where multiple agents interact, either cooperatively or competitively. The multi-agent model formalism in RL is crucial for addressing complex, dynamic and decentralised decision-making processes found in financial markets [52, 263]. Generalising from the single agent, we have:\n\u2022 Agents: Each agent i has its own set of states \\(S_i\\), actions \\(A_i\\), and policies \\(\\pi_i\\). Agents can have distinct state spaces \\(S_i\\) depending on their roles and the information they can access. Similarly, agents can have distinct action spaces \\(A_i\\), reflecting their different capabilities or roles in the environment.\n\u2022 State Space: The joint state space S is a combination of all individual states \\(S = S_1 \\times S_2 \\times ... \\times S_n\\).\n\u2022 Action Space: The joint action space A is the Cartesian product of all individual action spaces \\(A = A_1 \\times A_2 \\times ... \\times A_n\\).\n\u2022 Reward Function: The reward function \\(R : S \\times A \\rightarrow R^n\\) provides a vector of rewards for all agents, where each component \\(R_i\\) corresponds to the reward received by agent i.\nIn the context of finance, a multi-agent RL setup might involve various trading agents operating simultaneously, each with its distinct strategy and goal. For example, one agent might focus on long-term investments, while another specialises in high-frequency trading. These agents interact with each other and the market, influencing and responding to market dynamics. By incorporating multi-agent models, we can better simulate financial markets' complex and competitive nature. Finally, we discuss applications of MAS solutions in QF in subsection 6.6."}, {"title": "1.4 Contribution and Paper Organisation", "content": "This survey assesses 167 publications to provide the first comprehensive review of the critical components of an RL agent within QF. Furthermore, our review demonstrates the diverse applicability of RL in different QF contexts. Although the recent surge in interest in RL within finance has produced several survey publications [66, 105, 139, 239], our study is distinctively focused on QF, diverging from other studies that encompass broader economic applications [66, 239] and rooted more closely within the literature"}, {"title": "2 Critical Considerations for RL in QF", "content": "Before delving into the research and implementation of RL in QF, we introduce some critical dimensions and considerations of RL in QF. Figure 2, as we covered in subsection 1.1, expands the generic RL framework and illustrates several options for potential mappings between key RL components and relevant concepts, techniques and practices from QF. In addition to these, there are some critical considerations for incorporating RL in QF including the transition from simulation to real-world applications, sample efficiency, online versus offline RL settings, on-policy versus off-policy frameworks, and how RL interacts with the unique challenges of financial markets."}, {"title": "2.1 Transition from Simulation to Real-World Application", "content": "Deploying RL models in real financial markets presents unique challenges. Unlike controlled simulation environments, real-world markets are unpredictable and influenced by myriad factors such as information asymmetry, variable transaction costs, taxes, and noise traders, which can significantly affect model performance [236, 297]. Therefore, a robust validation and simulation process is essential to ensure that RL models accurately capture the complexities of chaotic financial markets. Incorporating domain knowledge improves model resilience and adaptability, providing a more accurate reflection of market conditions, and improving"}, {"title": "2.2 Sample Efficiency", "content": "Improving sample efficiency is crucial in RL, especially in financial applications where data can be sparse, have a short history, or be expensive to obtain [139]. Many samples drawn from the environment are often necessary for stable convergence and low variance. Techniques such as model-based RL, experience replay, and transfer learning are valuable in this context.\nModel-free methods are typically slower to learn, whereas model-based methods build a transition model from the environment's feedback, allowing the agent to use this model to understand the effects of actions on states and rewards [263]. Model-based RL can generate additional training data, enhancing learning efficiency [288]. For instance, model-based RL can simulate various market conditions to create synthetic data in a stock-trading scenario. This data helps the model learn how to handle different market situations. However, for this approach to be effective, the model must accurately represent the environment. Experience replay allows RL agents to reuse past experiences, thereby improving learning from limited data [232]. In the context of financial applications, agents can reuse previous experiences to improve learning efficiency. Transfer learning enables leveraging knowledge from related tasks to improve performance in new but similar tasks, thus addressing data scarcity [187]. For a discussion of transfer learning and model-based RL, see subsection 6.3 and Section 9. When the transition model is accurate, model-based RL can also serve as a transfer learning mechanism, further enhancing learning efficiency [263]."}, {"title": "2.3 Online vs. Offline RL Settings", "content": "Online learning in QF refers to agents learning and adapting continuously as new data arrive. This approach benefits from the ability to dynamically update strategies in response to market changes. However, this can be highly impractical in HFT, where decisions occur in microseconds to milliseconds, because online learning requires real-time updating of algorithm parameters and significant computational resources to process new data. As pointed out by Hambly et al. [139], a potential solution in this setup might be to collect data with a pre-specified exploration scheme during trading hours and update the algorithm with newly collected data after trading closes. Transfer learning can also be leveraged in this context to bridge the gap between offline and online learning. Initially, the RL agent can be pre-trained using offline RL on historical data, learning general strategies and patterns. Once deployed, the agent can then use online learning to fine-tune these strategies in response to real-time market conditions. This hybrid approach can mitigate some of the computational burdens of online learning by reducing the frequency and extent required for real-time updates. Also, certain components of the RL framework can be constructed to support online learning, such as the introduction of Differential SR [235-238], which is an online version of SR (see subsection 5.3).\nIn contrast, offline RL uses historical data to develop strategies without continuous interaction with the environment. This method is particularly useful for backtesting and strategy development, where real-time data streams are not necessary. Offline RL can leverage vast amounts of historical market data to train models and then be deployed in live environments after thorough validation. Furthermore, offline learning does not face the computational constraints that online learning faces, allowing for extensive preprocessing and model training using conventional computing resources. Though training time can be extensive due to large datasets, it avoids the real-time computational demands of online RL."}, {"title": "2.4 On-Policy vs. Off-Policy Frameworks", "content": "The distinction between on-policy and off-policy methods significantly affects their application in various financial contexts, each presenting unique challenges and opportunities.\nOn-policy algorithms, such as Proximal Policy Optimisation (PPO) [280], use data generated only from the current policy. This means that only actions taken by the current policy generate data at each training iteration. Consequently, the data become unusable after policy updates, making on-policy methods inefficient. However, on-policy algorithms are typically more stable and exhibit lower variance because they learn directly from the policy they are improving [263]. For example, SARSA [273], an on-policy algorithm, is used in stock trading models due to its ability to adapt to market changes efficiently, resulting in lower computational resources requirements and robust performance under varying market conditions [77, 86]. Furthermore, on-policy methods could be practical in HFT due to the vast amount of data available despite being sample inefficient. In PM, trading at lower frequencies (e.g. monthly) could be less practical due to the scarcity of available data.\nIn contrast, off-policy methods, such as DQN [232], do not have this limitation, allowing them to use data generated from different policies. This makes off-policy methods more sample-efficient, as they can reuse past experiences stored in memory. However, this requires significant memory storage to preserve past experiences. Additionally, off-policy methods can face challenges with stability and variance due to the complexity of updating policies from a mixture of different data sources and policies, which can amplify approximation errors and lead to instability [263, 302]. Off-policy algorithms often leverage large datasets for training and can learn from historical data, making them particularly suitable for environments where collecting real-time data is impractical. For example, the application of Q-learning [330] in financial trading demonstrates its effectiveness in using past market data to inform future trading decisions, which could lead to better adaptation to non-stationary market dynamics [77]. Furthermore, off-policy methods could be used in HFT due to their ability to process large amounts of data, although they may require longer training times, which is crucial in the HFT context. In PM, trading at lower frequencies (e.g. monthly) could benefit more from off-policy methods due to their better use of available data.\nA hybrid approach can be beneficial. For example, an RL agent could be pre-trained using off-policy methods on historical data to develop a baseline strategy. This agent can then be fine-tuned using on-policy methods in a live environment for stability and adaptability. In practice, such as in equity trading or broader quantitative finance setups, an agent initially trained with DQN can switch to PPO in live trading to balance stability and responsiveness, combining the benefits of both methods [267, 355]. General care should be taken when using historical data, as it could lead to biases and unintended trading behaviours. In portfolio management, reliance on past data can result in strategies that perform poorly under new market conditions due to non-stationary dynamics [207]. Thorough testing and validation are necessary to mitigate these risks."}, {"title": "3 Main Reinforcement Learning Methods", "content": "Contemporary literature distinguishes four primary RL methods: Value-based (Critic-only), Policy-based (Actor-Only), Actor-Critic, and model-based. This section delves into a thorough exploration of the principal RL techniques encapsulated within these categories. We provide an overview of the general framework, the advantages and disadvantages, and the applicability of each method within the field of QF. Additionally, within these categories, we incorporate other forms of RL approaches such as multi-agent methods [157, 191]."}, {"title": "3.1 Value-Based Methods", "content": "Value-based methods are foundational in RL. The agent's objective is to learn the value of different actions in various states to maximise cumulative rewards. Neuneier's work [244] is one of the earliest contributions in this field, which set a methodological precedent that has informed many subsequent studies. This section outlines a generic framework derived from these methodologies:\n(1) Define a Finite Set of States \\(S_t\\): States represent environment-derived information at each time point \\(t \\in \\{1, 2, ..., T\\}\\). This information includes financial accounting data, prices, sentiment, and technical indicators.\n(2) Define a Set of Actions \\(A_t \\in \\{Buy, Sell, Hold\\}\\): These are the possible actions of the agent at each time \\(t \\in \\{1, 2, ..., T\\}\\).\n(3) Establish Transition Probabilities: These probabilities, typically unmodelled, define state transitions based on actions.\n(4) Formulate a Reward Function \\(R_t\\): Provides numerical feedback to the agent in response to its preceding action.\n(5) Create a Policy \\(\\pi\\): Maps states to actions for the agent to follow.\n(6) Construct a Value Function V: Maps states to the agent's expected total discounted reward from a given state until the episode's end under policy \\(\\pi\\)."}, {"title": "3.1.2 General Observations, Comments and Definitions for Value-based Methods", "content": "Value-based methods are a well- established branch of research in RL applied to trading systems, despite their noted shortcomings. A significant challenge is the discrete nature of the action space, complicating practical trading applications 5.2.\nThe agent creates a value function to estimate the results of actions such as buying, holding, or selling, helping to choose the best action. This approach often uses model-free RL algorithms like Q-learning [330] and SARSA [273] to optimise the expected total reward. Q-learning, an off-policy RL algorithm, is proven to converge to the optimal solution in the tabular setting under specific conditions [331]. In Q-learning, the Q-values, \\(Q (S_t, A_t)\\), represent the expected rewards for taking an action \\(A_t\\) in state \\(S_t\\). These Q-values are updated according to the rule:\n\\(Q (S_t, A_t) \\leftarrow Q (S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q (S_{t+1}, a) - Q (S_t, A_t)]\\).\nThis update rule adjusts the current Q-value by incorporating the observed reward \\(R_{t+1}\\) and the maximum estimated future reward, weighted by the learning rate \\(\\alpha \\in (0, 1]\\). Over time, this process helps refine the Q-values, guiding the agent toward the optimal action values under specific conditions. Specifically, Q-learning is guaranteed to converge to the optimal solution with probability 1, provided that all state-action pairs are visited infinitely many times and the learning rate satisfies certain decay conditions. These convergence conditions are crucial for ensuring that the algorithm performs well in practice.\nQ-learning and SARSA are traditionally used in tabular settings, which are suitable for small, discrete state spaces. However, these algorithms can also be adapted to more complex state spaces through function approximation techniques. While tabular methods are inherently limited in scalability, FA allows these algorithms to generalize from similar states, making them applicable"}, {"title": "3.2 Policy-Based Method", "content": "Policy-based methods in RL focus on directly optimising the policy that dictates the agent's actions. Unlike Value-based methods, which estimate the value of actions to derive the best policy, Policy- based methods directly search for the optimal policy that maximises the cumulative reward. This approach can be particularly advantageous in environments with continuous action spaces and complex dynamics.\nAmong these methods, the RRL framework stands out due to its extensive use in the reviewed literature and its clear relevance to finance [235-238]. The RRL framework is particularly suited for financial applications because it can capture the temporal dependencies and sequential nature of trading decisions. Therefore, this subsection will focus on the RRL framework. Following Moody and Wu [235], an agent's actions are represented by:\n\\(A_t = A (\\theta_t: A_{t-1}, I_t) \\in \\{-1,0, 1\\}\\) with \\(I_t = \\{z_t, z_{t-1}, z_{t-2}, ..., y_t, y_{t-1}, y_{t-2}, ...\\}\\).\nHere, \\(\\theta_t\\) refers to the learned parameters of the agent, \\(A_t\\) corresponds to a trading position that can assume one of three states - sell, neutral, or buy, while \\(A_{t-1}\\) represents the preceding action at time t - 1, \\(I_t\\), which denotes the information set at time t, is used as a state representation, composed of lagged asset prices \\(z_t\\) and other external variables \\(y_t\\). \\(A_t\\) can be defined [238] as:\n\\(A_t = sign (uA_{t-1} + v_0r_t + v_1r_{t-1} + ... + v_mr_{t-m} + w),\\)\nwhere, \\(r_t\\) denotes the price return, and \\(\\theta_t = \\{u, v_i, w\\}\\) with \\(i \\in \\{0, 1, ..., m\\}\\). The RRL framework further optimises the trading system by maximising a performance function \\(U_t\\). This function can represent different goals, such as maximising profit, improving a utility function of wealth, or enhancing performance ratios like the Sharpe Ratio (SR). Essentially, it helps the trader aim for better financial performance based on their specific goals. One of those choices is the additive profits utility reward function [238]:\n\\(U_t(\\theta) = P_T = \\sum_{t=1}^{T} R_t = \\mu\\sum_{t=1}^{T} \\frac{r_t}{r_f} + A_{t-1} (\\frac{r_t}{r_f} - \\delta_t) A_t\\,\nwhere, \\(P_T\\) denotes the cumulative profit at the end of trading period T, \\(R_t\\) the profit or loss at time t, and T the total time steps. \\(\\mu > 0\\) represents a fixed position size when buying/selling a stock, \\(r_t\\) and \\(r_f\\) are the absolute prices changes for the period from t - 1 to t for the risky asset (stock) and the risk-free asset (e.g., T-Bills), respectively, and \\(\\delta_t\\) denotes transaction costs from buying/selling the risky asset.\nAn important aspect of the RRL framework is the online optimisation approach using stochastic gradient ascent to adjust the parameters. This process involves calculating how changes in the agent's strategy (represented by the parameters \\(\\theta\\)) affect its performance (measured by the utility function \\(U_t\\)). Essentially, it looks at how both the current and previous actions influence the agent's success, allowing for continuous improvement. Imagine a trader adjusting their strategy based on both their recent trades and past experiences to maximise profits. The gradient calculation is described by the following equation:\n\\(\\frac{dU_t(\\theta)}{d\\theta} = \\frac{dU_t(\\theta)}{dR_t} \\frac{dR_t}{dA_t} \\frac{dA_t}{d\\theta} + \\frac{dU_t(\\theta)}{dR_t} \\frac{dR_t}{dA_{t-1}} \\frac{dA_{t-1}}{d\\theta}.\\)\nThis equation shows that we need to compute the gradient of the utility function with respect to the actions and then update the parameters based on this gradient. The update is done using the learning rate \\(\\rho\\), as in \\(\\Delta \\theta_t = \\rho \\frac{dU_t(\\theta)}{d\\theta}\\). In simpler terms, we"}, {"title": "3.2.2 General Observations, Comments and Definitions for Policy-Based Methods", "content": "Policy-based methods represent the second most explored area of RL in the reviewed literature. These methods provide a direct mapping from states to actions, eliminating the need to compute the expected outcome of different actions as in the Value-based approach, resulting in faster learning processes.\nA significant advantage of Policy-based methods is the continuous action space for the agent. Consider a portfolio of stocks: with the Value-based approach, portfolio weights can only take discrete values like buy, sell, or hold. In contrast, the Policy-based approach allows portfolio weights to assume any value in [0, 1] in the long-only case.\nPolicy-based methods exhibit robust performance even when dealing with noisy datasets, common in stock-related data [90, 238]. Furthermore, Policy-based methods typically converge more swiftly than Value-based methods [133], although the high variance of the gradient leads to a slower learning rate [178]. These methods inherently perform exploration [263], as the stochastic policy yields a distribution over the action space. However, Policy-based methods require a differentiable reward function. Policy-based approaches in finance were pioneered by Moody and Wu [235], with many variants since then proposed. RRL typically uses a recurrent NN structure, creating dependencies over previous steps and facilitating multi-period optimisation."}, {"title": "3.3 Actor-Critic Method", "content": "The Actor-Critic method in RL is a hybrid approach that combines the strengths of Policy-based and Value-based methods to create a robust learning framework. It consists of two main components: the Actor and Critic modules. The Actor module takes the state \\(S_t\\) as input and determines the action \\(A_t\\) at time t. The Critic module subsequently receives the state \\(S_t\\) and the action \\(A_t\\) determined by the Actor module, evaluates the state-action pair, and computes the reward, adhering to the general framework outlined in subsection 3.1. Generic Actor-Critic frameworks are discussed in [263, 302]. The core components of an Actor-Critic framework can be described as follows:\n\u2022 Actor: The Actor is responsible for selecting actions based on the current state. It learns a policy, which is a mapping from states to actions. This policy can be:\nDeterministic: The Actor always chooses the same action for a given state.\nStochastic: The Actor chooses actions according to a probability distribution.\n\u2022 Critic: The Critic evaluates the action taken by the Actor by computing a value function. This value function estimates the expected cumulative reward (discounted over time) of being in a given state and taking a particular action.\n\u2022 Advantage Function: The advantage function helps to determine how much better or worse a particular action is compared to the average action taken from that state. It is defined as:\n\\(A(S_t, A_t) = Q(S_t, A_t) - V(S_t),\\)\nwhere Q is the action-value function and V is the state value function.\n\u2022 Gradient Ascent: Both the Actor and the Critic are trained using gradient ascent. The Actor updates its policy parameters to maximise the expected cumulative reward, while the Critic updates its parameters to provide more accurate evaluations of the actions.\n\u2022 TD Error: Temporal Difference (TD) error is used to update both the Actor and the Critic. It is the difference between the expected reward and the actual reward received, given by:\n\\(\\delta_t = R_{t+1} + \\gamma V (S_{t+1}) - V(S_t).\\)\nThe integration of deterministic and stochastic policies within the Actor-Critic framework has shown potential for enhancing decision processes across various complex environments. The first application of the Actor-Critic framework in our survey is found in [64], where the Actor-Critic method is employed to solve the market-making problem (more details, in subsection 7.5)."}, {"title": "3.3.2 General Observations, Comments and Definitions for Actor-Critic Method", "content": "Among the areas covered in this survey, the Actor-Critic method is the least represented (see Table 1). Yet, it remains among the most compelling of the four primary approaches, as it combines the advantages of both Policy-based and Value-based RL methods. As indicated in subsection 3.2, a notable challenge with Policy-based methods is their high variance, which may result in slower convergence or the propensity to become stuck in local optima. The Critic component in Actor-Critic methods mitigates this by providing a value function that stabilises policy-gradient updates, reducing variance. Moreover, Value-based RL methods are prone to high bias due to approximation errors in the value function [263]. The Actor component in Actor-Critic methods helps to directly optimise the policy, reducing bias by ensuring continuous adjustment based on accurate evaluations of actions. Several Actor-Critic algorithms featured in this literature are specifically engineered to surmount these obstacles. The algorithmic frameworks predominantly observed within the literature encompassed by this survey include the Deterministic Policy Gradient (DPG) [287], Deep Deterministic Policy Gradient (DDPG) [208], Asynchronous Advantage Actor-Critic (A3C) and the non-parallel version (A2C) [233], Trust Region Policy Optimisation (TRPO) [279], Proximal Policy Optimisation (PPO) [280], Soft Actor-Critic (SAC) [137].\nThough comparatively under-researched in QF, the Actor-Critic category shows significant promise in contemporary applications. Various methods within this category, as documented in works by Mnih et al. [208, 233] and Lillicrap et al., [208], have demonstrated state-of-the-art performance, marking this as a potential area for future QF breakthroughs."}, {"title": "3.4 Model-Based RL", "content": "In model-free RL methods, the agent updates a policy directly from the environment's feedback on its actions. On the contrary, in model-based methods involve constructing a model of the environment, which is then used to simulate and plan actions. A foundational framework can be described as follows:\n(1) Define a Finite Set of States \\(S_t\\): States represent environment-derived information at each time point t. This information includes financial accounting data, prices, sentiment, and technical indicators.\n(2) Define a Set of Actions \\(A_t\\): These are the possible actions of the agent at each time t, such as Buy, Sell, and Hold.\n(3) Learn Transition Probabilities: Construct a transition model that predicts the next state \\(S_{t+1}\\) and reward \\(R_{t+1}\\) given the current state \\(S_t\\) and action \\(A_t\\). This model can be a neural network or any other function approximator.\n(4) Formulate a Reward Function \\(R_t\\): Provides numerical feedback to the agent in response to its preceding action, incorporating factors such as profit, risk, and transaction costs.\n(5) Planning and Policy Optimisation: The learnt model can be used to simulate future states and rewards, allowing the agent to plan and optimise actions. Techniques such as Monte Carlo Tree Search (MCTS) or Dynamic Programming can be used for this purpose [263, 302].\nThis framework highlights the core components required for model-based RL in finance, facilitating the creation of a system where an agent learns to make optimal trading decisions through simulation and planning. Model-based RL is the least represented method in the current literature [135, 332, 345]."}, {"title": "3.4.2 General Observations, Comments and Definitions for Model-Based RL", "content": "Model-based methods in RL offer several advantages and present unique challenges, particularly when applied to financial trading systems 6:\n(1) Learning Speed: Model-based RL methods typically learn faster than model-free approaches by using the learnt model for planning and action optimisation, crucial for timely financial market decisions.\n(2) Computational Complexity: Simulating and planning with complex financial models is computationally intensive, requiring efficient algorithms and high-performance computing, especially for HFT applications.\n(3) Risk Management: Robust risk management is vital. Model-based RL can simulate extreme market scenarios to assess potential risks, helping to develop strategies that maximise returns and manage risks effectively."}, {"title": "4 Environment", "content": "In the RL framework, the environment characterises the current state of the system. The agent, the learner, and decision maker interact with this environment, selecting actions based on state information. This setup requires that the agent and the environment"}, {"title": "4.1 Introduction", "content": "In the RL framework, the environment characterises the current state of the system. The agent, the learner, and decision maker interact with this environment, selecting actions based on state information. This setup requires that the agent and the environment are mutually exclusive [301], providing distinct boundaries for rewards, actions, and states7. In financial contexts, the agent, an asset owner, uses the state of financial markets (the environment) and external factors such as stock indices, interest rates, commodity prices, and macroeconomic, political, and natural risks to inform actions (see, Figure 2).\nAssuming that the agent can access all relevant information, the RL problem can be addressed under the MDP framework [28]. However, this assumption implies that the environment is fully observable and that future states depend only on the current state and action, a property known as the Markov property. In financial markets, this assumption may be overly simplistic. Empirical evidence suggests that financial markets exhibit longer memory, influenced by factors such as investor behaviour, economic cycles, and external events. These factors introduce dependencies that extend beyond immediate state transitions [75, 215].\nGiven the complexity and partial observability of financial markets, a Partially Observable Markov Decision Process (POMDP) framework is more appropriate [16]. In Partially Observable (PO) environments, the transition probabilities between states in financial markets are typically not explicitly modelled. Instead, historical data and statistical methods, such as LSTM or RNN [146], are used to approximate these transitions. This approach acknowledges the inherent randomness and partial observability of financial markets, making exact environment representation virtually impossible.\nFinancial markets epitomise a PO environment, lacking deterministic representation; a single day's pricing cannot encapsulate the state of the environment, unlike full observability in Atari video games [231, 232]. The use of LSTM and RNN models is particularly relevant in this context, as they are capable of capturing long-term dependencies and patterns in sequential data. These models help address the limitations of the Markov assumption by incorporating memory effects and providing a more robust representation of the financial market dynamics.\nIn early adoptions of RL in QF, the environment was assumed to be fully observable [244], mainly because the inherent randomness of financial markets makes an exact environment representation virtually impossible. Furthermore, adding more features does not necessarily improve performance [227], necessitating strategic feature selection to manage randomness, avoid the curse of dimensionality [33], and address interpretability issues. Finally, we note that empirical studies have shown that models that incorporate memory effects, such as LSTM and RNN, significantly outperform those based on the Markov assumption, particularly in predicting long-term trends and capturing market anomalies [106]."}, {"title": "4.2 Features", "content": "Modelling financial markets with their inherent randomness is a considerable challenge. Consequently, input data, feature selection, and extraction become critical to the success of an RL system. The literature presents a diverse range of data sources, features, and feature selection mechanisms, reflecting the current state of research and highlighting the"}]}