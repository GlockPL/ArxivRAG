{"title": "CP-Guard: Malicious Agent Detection and Defense in Collaborative Bird's Eye View Perception", "authors": ["Senkang Hu", "Yihang Tao", "Guowen Xu", "Yiqin Deng", "Xianhao Chen", "Yuguang Fang", "Sam Kwong"], "abstract": "Collaborative Perception (CP) has shown a promising technique for autonomous driving, where multiple connected and autonomous vehicles (CAVs) share their perception information to enhance the overall perception performance and expand the perception range. However, in CP, ego CAV needs to receive messages from its collaborators, which makes it easy to be attacked by malicious agents. For example, a malicious agent can send harmful information to the ego CAV to mislead it. To address this critical issue, we propose a novel method, CP-Guard, a tailored defense mechanism for CP that can be deployed by each agent to accurately detect and eliminate malicious agents in its collaboration network. Our key idea is to enable CP to reach a consensus rather than a conflict against the ego CAV's perception results. Based on this idea, we first develop a probability-agnostic sample consensus (PASAC) method to effectively sample a subset of the collaborators and verify the consensus without prior probabilities of malicious agents. Furthermore, we define a collaborative consistency loss (CCLoss) to capture the discrepancy between the ego CAV and its collaborators, which is used as a verification criterion for consensus. Finally, we conduct extensive experiments in collaborative bird's eye view (BEV) tasks and our results demonstrate the effectiveness of our CP-Guard.", "sections": [{"title": "Introduction", "content": "Recently, multi-agent collaborative perception has attracted great attention from both academia and industries since it can overcome the limitation of single-agent perception such as occlusion and limitation of sensing range (Han et al. 2023; Hu et al. 2024a). Because the collaborative CAVs send complementary information (e.g., raw sensor data, intermediate features, and perception results) to ego CAV, the ego CAV can leverage this complementary information to extend its perception range and tackle the blind spot problem in its view, which is crucial for the safety of the autonomous driving systems. The operational flow of CP is as follows. Each CAV independently encodes local sensor inputs into intermediate feature maps. Then, these CAVs share their feature maps with their ego CAV by vehicle-to-vehicle (V2V) communication. Finally, the ego CAV fuses the received feature maps with its own feature maps and decodes them to acquire the final perception results.\nHowever, compared with single-agent perception, multi-agent CP is more vulnerable to security threats and easy to attack, since it incorporates the information from multiple agents which makes the attack surface larger. An attack could be executed by a man-in-the-middle who alters the feature maps sent to the victim agent, or by a malicious agent that directly transmits manipulated feature maps to the victim agent. For example, Tu et al. (Tu et al. 2021) generated adversarial perturbations on the feature maps and attacked the ego CAV, resulting in the wrong perception results. Additionally, as the encoded feature maps are not visually interpretable by humans, moderate modifications to these maps will go unnoticed, rendering the attack quite stealthy.\nThis issue raises significant risks to CP if the ego CAV cannot accurately detect and eliminate the malicious agents in its collaboration network and the perception results are corrupted, which may result to catastrophic consequences. For example, the ego CAV may misclassify the traffic light status or fail to detect the front objects, leading to serious traffic accidents or even loss of life. Therefore, it is essential to develop a defense mechanism for CP that is robust to attack from malicious agents and can remove the malicious agents in its collaboration network.\nIn order to address this issue, several works have explored this problem. For example, Li et al. (Li et al. 2023) proposed robust collaborative sampling consensus (ROBOSAC) to randomly sample a subset of the collaborators and verify the consensus, but it requires the prior probabilities of malicious agents, which are usually unknown in practice. In addition, Zhao et al. (Zhao et al. 2023) and Zhang et al. (Zhang et al. 2023) also developed defense methods against malicious agents, while these methods need to check the collaborators one by one, which is inefficient and computation-consuming. Moreover, other works (Tu et al. 2021; Raghunathan et al. 2020; Zhang and Li 2020) use adversarial training to enhance the robustness of the model. However, adversarial training introduces additional overhead during training and lacks generalization for unseen attacks. Additionally, it may result in a reduction in accuracy and is non-trivial to achieve computationally efficient and generalizable adversarial defense in CP.\nIn order to fill in the research gap and overcome the aforementioned limitations, we design a novel defense mechanism, CP-Guard. It can be deployed by each agent to accurately detect and eliminate malicious agents in the local collaboration network. The key idea is to enable CP to achieve a consensus rather than a conflict against the ego CAV's perception results. Following this idea, we first design a probability-agnostic sample consensus (PASAC) method to effectively sample a subset of the collaborators and verify the consensus without prior probabilities of malicious agents. In addition, the consensus is verified by our carefully designed collaborative consistency loss (CCLoss), which is used to calculate the discrepancy between the ego CAV and its collaborators. If a collaborator's collaborative consistency loss exceeds a predefined threshold, the collaborator is considered a benign agent, otherwise, it is considered a malicious agent. The main contributions of this paper are summarized as follows."}, {"title": "Background and Related Work", "content": "Collaborative perception has been investigated as a means to mitigate the limitations inherent to the field-of-view (FoV) in single-agent perception systems, enhancing the accuracy, robustness, and resilience of these systems (Fang et al. Aug. 2024, 2024a,d; Qu et al. 2024a,b,c; Lin et al. 2024d,b,c,a, 2023b,a). In this collaborative context, agents may opt for one of three predominant data fusion strategies: (1) early-stage raw data fusion, (2) intermediate-stage feature fusion, and (3) late-stage output fusion. Early-stage fusion, while increasing the data communication load, typically yields more precise collaboration outcomes. In contrast, late-stage fusion consumes less bandwidth but introduces greater uncertainty to the results. Intermediate-stage fusion, favored in much of the current literature, strikes an optimal balance between communication overhead and perceptual accuracy. Research aimed at enhancing collaborative perception performance is multifaceted, addressing aspects such as communication overhead (Su et al. 2024; Fang et al. 2022, 2021), robustness (Lu et al. 2023; Hu et al. 2024c,d; Tao et al. 2024; Hu et al. 2023; Fang et al. 2024b,c), system heterogeneity (Lu et al. 2024), and domain generalization (Hu et al. 2024b). Among these, robustness has emerged as a particularly critical focus within the field of collaborative perception. Despite extensive studies on system intrinsic robustness, addressing challenges such as communication disruptions (Ren et al. 2024), pose noise correction (Lu et al. 2023), and communication latency (Lei et al. 2022), most existing research works have not accounted for the presence of malicious attackers within the collaborative framework. Only a selected few studies examine the implications of robustness in scenarios compromised by malicious nodes, highlighting a significant gap in current research methodologies.\nAdversarial attacks targeting at single-vehicle perception systems predominantly employ techniques such as GPS spoofing (Li et al. 2021), LiDAR spoofing (Hallyburton et al. 2022), and the deployment of physically realizable adversarial objects (Tu et al. 2020; Ni et al. 2023d,b,c). In the context of multi-vehicle collaborative perception, the nature of adversarial strategies can vary significantly depending on the stage of collaboration. For early-stage collaborative perception, Zhang et al. (Zhang et al. 2023) have developed sophisticated attacks involving object spoofing and removal. These attacks exploit vulnerabilities by simulating the presence or absence of objects and reconstructing LiDAR point clouds using advanced ray-casting techniques. In contrast, late-stage collaboration typically involves the sharing of object locations (Schiegg et al. 2020), which provides adversaries with opportunities to manipulate these shared locations easily. Intermediate-stage attacks are particularly nuanced, often requiring that an attacker possesses white-box access to the perception models. This knowledge enables more precise manipulations of the system, though such systems are generally resistant to simplistic black-box strategies, such as ray-casting attacks, due to the protective effect of benign feature maps which significantly reduce the efficacy of such attacks. Tu et al. (Tu et al. 2021) were among the pioneers in articulating an untargeted adversarial attack aimed at maximizing the generation of inaccurate detection bounding boxes by manipulating feature maps in intermediate-fusion systems. Building on this foundational work, Zhang et al. (Zhang et al. 2023) have advanced the methodology by integrating perturbation initialization and feature map masking techniques to facilitate realistic, real-time targeted attacks. Our work is dedicated to exploring and mitigating adversarial threats specifically under intermediate-level collaborative perception framework, aiming to enhance system resilience against sophisticated attacks.\nTo fortify intermediate-level collaborative perception systems against adversarial attacks, Li et al. (Li et al. 2023) proposed the Robust Collaborative Sampling Consensus (ROBOSAC) method. This approach entails a random selection of a subset of collaborators for consensus verification. Despite its potential, the efficacy of ROBOSAC hinges on the availability of prior probabilities of malicious intent among agents, which are often unknown in real-world scenarios. Moreover, Zhao et al. (Zhao et al. 2023) and Zhang et al. (Zhang et al. 2023) have formulated defensive strategies targeting at identifying malicious agents. These techniques, however, involve scrutinizing each collaborator individually, rendering them both computationally intensive and inefficient. Adversarial training has also been explored as a mechanism to bolster system robustness, as demonstrated in the studies by Tu et al. (Tu et al. 2021), Raghunathan et al. (Raghunathan et al. 2020), and Zhang et al. (Zhang and Li 2020). While this approach enhances system security, it substantially increases the computational load during training and may not effectively generalize to novel, unseen attacks (Ni, Zhang, and Zhao 2023; Ni et al. 2023a; Yuan et al. 2024). Additionally, adversarial training often results in diminished model accuracy and poses significant challenges in developing a computationally efficient and scalable adversarial defense that can be broadly applied across collaborative perception platforms. In contrast, our research introduces a methodology that can be autonomously implemented by each agent to accurately detect and neutralize malicious entities within the local collaborative network, aiming to enhance both the efficiency and effectiveness of defense mechanisms in collaborative perception systems."}, {"title": "Problem Setup", "content": "BEV segmentation is essential for autonomous driving since it enables multi-modal sensor data (e.g. LiDAR point cloud, multi-view camera images) to be transformed into a unified BEV space for information aggregation or fusion. This approach offers significant advantages in accurately maintaining the spatial and temporal locations and scales of road elements. In this paper, we focus on the LiDAR-based collaborative BEV segmentation task with an intermediate feature fusion paradigm.\nSpecifically, consider a set of N + 1 CAVs, including the ego CAV, each CAV is installed with a feature encoder $f_e$ and a feature aggregator $f_a$ as well as a BEV segmentation decoder $f_D$. For the i-th CAV, the input is a set of voxelized LiDAR point cloud $X_i \\in \\mathbb{R}^{W \\times H \\times n}$, where W and H are the width and height of the voxelized point cloud, respectively, and n is the dimension of the voxelized point cloud. The collaborative BEV segmentation pipeline can be described as follows.\n1. Firstly, the feature encoder $f_e$ is used to extract the intermediate feature maps $F_i = f_e(X_i) \\in \\mathbb{R}^{\\frac{W}{K}\\times \\frac{H}{K}\\times C}$, where K indicates the down-sample rate and C is the number of channels.\n2. Then, the other CAVs transmit their intermediate feature maps $F_i$ to the ego CAV. The ego CAV leverages the aggregator $f_a$ to fuse the feature maps from all CAVs including its own feature map, which can be formulated as $F = f_A(F_0, F_1, F_2,\\dots,F_N)$, where $F_0$ is the ego CAV's feature map.\n3. Finally, the ego CAV decodes the aggregated feature map F into the final BEV segmentation map $Y = f_D(F)$.\nDuring training, given the ground truth BEV segmentation map $Y^*$, the loss function is defined as $\\mathcal{L}_{seg}(Y, Y^*)$. The goal is to minimize the loss function $\\mathcal{L}_{seg}$ by optimizing the parameters of the feature encoder $f_e$, feature aggregator $f_A$, and BEV segmentation decoder $f_D$.\nIn order to defend against malicious agents in CP, we first need to figure out the attack scenarios and the attacker's abilities. Specifically, we consider an attacker to have full access to malicious CAVs. In addition, since the BEV segmentation model is deployed on each CAV, the attacker has full access to the model architecture, parameters, and intermediate feature maps, enabling the attacker to launch a white-box attack. Based on this, the attacker aims to manipulate the intermediate feature maps by adding adversarial perturbations to maximize the ego CAV's BEV segmentation loss. Then, these adversarial messages are transmitted to the ego CAV to fool its perception fusion. The attacker's goal can be formulated as follows.\n$\\max_{\\delta} \\mathcal{L}_{seg}(Y, Y^*)$,\n$\\left \\|\\delta \\right \\|<\\Delta$\n(1)\ns.t. $Y = f_D(f_A(F_0, F_1, F_m + \\delta,\\dots,F_N))$,\nwhere Y is the adversarial collaborative BEV segmentation map, $F_m$ is the malicious agent's feature map, and $\\delta$ is the optimization perturbation which is constrained by $\\left \\|\\delta \\right \\| \\leq \\Delta$ to ensure its stealth to avoid being detected. Its size is the same as the size of intermediate feature map $F_m$, which is $\\mathbb{R}^{\\frac{W}{K}\\times \\frac{H}{K}\\times C}$"}, {"title": "Algorithm 1: PASAC", "content": ""}, {"title": "Method", "content": "In this section, we present our CP-Guard in detail. It consists of two main components: (1) Probability-Agnostic Sample Consensus (PASAC) and (2) Collaborative Consistency Loss Verification (CCLoss). PASAC is designed to effectively sample a subset of collaborators for consensus verification without relying on prior probabilities of malicious intent. CCLoss is proposed to verify the consensus between the ego CAV and the collaborative CAVs. These two components work collaboratively to detect and neutralize malicious agents in the collaborative perception network. We elaborate on these two components in the following subsections.\nTo achieve the consensus of collaborators, the most straightforward method is to check the collaborators one by one. However, this method is time-consuming and computation-intensive, especially when the number of collaborators is large. A better method is to randomly sample a subset of collaborators for consensus verification at each time, such as ROBOSAC (Li et al. 2023). However, ROBOSAC requires the prior probabilities of malicious intent among agents, which are often unknown in real-world scenarios. To fill in this research gap, we propose PASAC.\nMore specifically, given a set of N collaborators, the ego CAV will generate the collaborative BEV segmentation map $Y$ based on its observation and the received messages for feature fusion from the N collaborators. Firstly, the ego CAV generates its BEV segmentation map $Y_0$ based on its own observation. Then, it randomly split the collaborators into two groups of equal size. After receiving all the messages, the ego CAV fuses the features and generates the BEV segmentation map $Y_{G1}$ based on the messages $\\{F_i\\}_{i=1,...,\\frac{N}{2}}$ from the first group. Similarly, it generates the BEV segmentation map $Y_{G2}$ based on the messages $\\{F_i\\}_{i=\\frac{N}{2},...,N}$ from the second group.\nThen, the ego CAV verifies the consensus and checks if there is any malicious CAV in the two groups. The consensus is verified by CCLoss to be introduced in the next subsection. Specifically, the CCLoss is calculated between the ego CAV and each group, that is, $\\mathcal{L}_{CCLoss}(Y_0, Y_{G1})$ and $\\mathcal{L}_{CCLoss}(Y_0, Y_{G2})$. If the CCLoss exceeds a predefined threshold, the group is considered benign, otherwise, it is considered to contain malicious CAVS.\nSuppose the first group is benign and the second group is verified to have malicious CAVs, all CAVs in the first group are marked as benign and incorporated in the following collaboration. For the second group, the ego CAV continues to split the second group into two subgroups and repeats the consensus verification process. This process will continue until finding all the malicious CAVs or obtain enough benign CAVs. The detailed procedures of PASAC can be summarized as follows."}, {"title": "Collaborative Consistency Loss Verification", "content": "To verify the consensus between the ego CAV and the collaborators, we design a novel loss function, Collaborative Consistency Loss (CCLoss). CCLoss is used to calculate the discrepancy between the ego CAV and the collaborators. Given the intermediate feature maps $F_0$ of the ego CAV and a set of intermediate feature maps $\\{F_1,\\dots, F_i\\}$ from the collaborators. The ego CAV will generate two BEV segmentation maps:\n$Y_0 = f_D(F_0)$,\n(2)\n$Y_{fuse} = f_D(f_A(F_0, F_1,..., F_i))$,\n(3)\nwhere $Y_0$ and $Y_{fuse}$ are 3D matrices and their sizes are in $\\mathbb{R}^{W_D \\times H_D \\times C}$ with $W_D$, $H_D$, and C being the width, height, and the number of classes of the BEV segmentation map, respectively.\nAs stated before, our key idea is to enable CP to achieve consensus rather than conflict against the ego CAV's perception result. Following this idea, we carefully design the CCLoss to measure the discrepancy between the ego CAV and the collaborators, which is formulated as:\n$\\mathcal{L}_{CCLoss}(Y_0, Y_{fuse}) =$\n$\\frac{\\sum_{c=1}^{C}\\sum_{i=1}^{W_D}\\sum_{j=1}^{H_D} w_c \\cdot p_{i,j}^{0} \\cdot p_{i,j}^{fuse}}{\\sqrt{\\sum_{c=1}^{C}w_c(\\sum_{i=1}^{W_D}\\sum_{j=1}^{H_D} p_{i,j}^{0})^2 + \\sum_{c=1}^{C}w_c(\\sum_{i=1}^{W_D}\\sum_{j=1}^{H_D} p_{i,j}^{fuse})^2}}$\n(4)\nwhere C is the number of classes, $p_{i,j}^{0}$ and $p_{i,j}^{fuse}$ are the probabilities of the j-th class at the i-th pixel in the BEV segmentation map $Y_0$ and $Y_{fuse}$, respectively, and $w_j$ is the weight of the j-th class, defined as the inverse frequency of the class $w_i = \\frac{1}{\\sqrt{1 + e^{(\\sum_{i=1}^{W_D}\\sum_{j=1}^{H_D} p_{i,j}^{fuse})^2}}}$. For the numerator of $\\mathcal{L}_{CCLoss}$, it calculates the weighted sum of the product of the probabilities for each pixel and each class, which essentially measures the overlap between the two distributions. The weight $w_j$ ensures that the contribution of each class is adjusted according to its importance or frequency. The denominator sums up the weighted sums of the probabilities from both the ego CAV's prediction map and the fused segmentation maps for each class. It represents the total probability mass for each class, adjusted by the weights. Finally, the fraction measures the similarity between the two distributions. If these two distributions are similar, the value of $\\mathcal{L}_{CCLoss}$ will be close to 1. On the contrary, if these two distributions are different, the value $\\mathcal{L}_{CCLoss}$ will be close to 0.\nIn addition, we need to set a threshold $\\epsilon$ to determine whether the set contains a malicious agent. Thus, we have the following verification rule if there is a malicious agent in the set:\n$\\mathcal{L}_{CCLoss}(Y_0, Y_{fuse}) \\leq \\epsilon$.\n(5)\nThe choice of the threshold $\\epsilon$ is crucial. A large $\\epsilon$ will lead to a high false-positive rate, while a small $\\epsilon$ will lead to a high false-negative rate."}, {"title": "Experimental Setup", "content": "Datasets and Evaluation Metrics. In our experiments, we leverage V2X-Sim (Li et al. 2022) as our dataset. It is the first synthetic dataset for CP generated by CARLA-SUMO co-simulator. In addition, to evaluate the performance of the segmentation, we adopt mean Intersection over Union (mIoU) as the evaluation metric. We also use Verification Count to evaluate the performance of PASAC, which is the total number of times that malicious agents are checked.\nImplementation Details. We build the collaborative BEV segmentation model by PyTorch, using U-Net (Ronneberger, Fischer, and Brox 2015) as the backbone, V2VNet (Wang et al. 2020) as the fusion method. Our experiment is deployed on a computer consisting of 2 Intel(R) Xeon(R) Silver 4410Y CPUs (2.0GHz), four NVIDIA RTX A5000 GPUs, and 512GB DDR4 RAM. As for the implementation of adversarial attacks, we employ three kinds of attacks: fast gradient sign method (FGSM) (Goodfellow, Shlens, and Szegedy 2015), Carlini & Wagner (C&W) (Carlini and Wagner 2017), and the projected gradient descent (PGD) (Madry et al. 2018). For each attack, we set the maximum perturbation $\\delta_{max} = 0.1$, iterations steps T = 15, and the step size $\\alpha = 0.01$.\nWe evaluate the efficacy of our CP-Guard scheme against a variety of adversarial attacks. The outcomes of these evaluations are detailed in Table 1. In scenarios where the collaborative perception system lacks defensive mechanism, the mIoU across all three attack modalities significantly falls below the established lower bound, registering at 37.09. This substantial degradation in performance underscores the effectiveness of the adversarial attacks implemented. Conversely, our CP-Guard framework demonstrates robust defensive capabilities, effectively countering all evaluated attacks and achieving an mIoU that closely approaches the upper bound of 40.45. Specifically, for the FGSM and PGD attacks, setting the CCLoss threshold to $\\epsilon = 0.08$ proves optimal experimentally. This configuration allows CP-Guard to maximally leverage its defensive mechanisms, yielding mIoU scores of 39.30 and 39.34, respectively. The methodology for determining the optimal CCLoss threshold is further explored in the Ablation Studies section of this paper. In addition, it is noteworthy that while CP-Guard maintains performance above the lower bound when defending against the C&W attack, the mIoU observed is relatively lower at 37.95. This reduced efficacy can be attributed to the sophisticated fine-grained optimization process inherent to the C&W attack, which complicates the detection and mitigation efforts.\nTo investigate the performance of PASAC, we conduct extensive experiments to study the relationship between the verification count and the number of benign agents and malicious agents. As shown in Fig. 2, the x-axis represents the number of benign agents and the y-axis represents the verification count. There are three lines in each subfigure, which represent the minimum, average, and maximum verification count, respectively. We can observe that the verification count increases with the number of collaborative agents and the growth trend is fast at the beginning and then becomes slow. In addition, the verification count is far less than the total number of agents, which indicates that PASAC is efficient in sampling collaborators. For example, when the number of collaborative agents is 100, the number of malicious agents is m, the average verification count is usually around $100 \\times m \\times 0.1$, and the mean verification count is below $100 \\times m \\times 0.05$.\nWe conduct a comparative experiment with the previous state-of-the-art method, ROBOSAC (Li et al. 2023). Following the experiment setup from ROBOSAC, we evaluate the performance of ROBOSAC and PASAC under different attack ratios. The results are shown in Table 3. We observe that PASAC outperforms ROBOSAC in terms of the verification count. Specifically, PASAC achieves a lower verification count than ROBOSAC under different attack ratios. For example, when the attack ratio is 0.6, the average verification count of PASAC is 7.59, which is lower than the average verification count of ROBOSAC (8.29), and when the attack ratio is 0.4, the average verification count of PASAC is 6.60, which is much lower than the average verification count of ROBOSAC (10.36). In addition, the results of PASAC are more stable than ROBOSAC, for example, the maximum verification count of PASAC is 8, while the maximum verification count of ROBOSAC is 46, which is much higher than the average verification count. These results indicate that PASAC can effectively sample collaborators and outperforms the state-of-the-art method, ROBOSAC. Furthermore, ROBOSAC needs to know the prior probabilities of malicious agents, while PASAC is a probability-agnostic sample method, so it does not require this information, which makes PASAC more practical in real-world scenarios."}, {"title": "Qualitative Evaluation", "content": "As depicted in Fig. 3, we present the visualization results on the V2X-Sim dataset. Without CP-Guard, attackers can significantly disrupt collaborative perception, leading to a marked degradation in the performance of BEV segmentation tasks. However, our introduced CP-Guard framework can intelligently identify benign collaborators and eliminate malicious collaborators, thereby facilitating robust CP."}, {"title": "Ablation Studies", "content": "We have further undertaken ablation studies to ascertain the optimal CCLoss threshold for our CP-Guard framework in defending against PGD attacks. The results of these studies are summarized in Table 2. On the one hand, when the CCLoss threshold $\\epsilon$ is set below 0.08, the ability of the ego CAV to distinguish malicious agents is progressively impaired, resulting in a decline in the mIoU. Notably, at a CCLoss threshold of $\\epsilon = 0.02$, the ego-agent fails to identify the malicious agent, culminating in an mIoU of 25.97, which is substantially below the established lower-bound. On the other hand, as the CCLoss threshold $\\epsilon$ is increased beyond 0.08, there is a noticeable deterioration in the mIoU, approaching the lower-bound. This suggests that the ego CAV begins to mistrust benign collaborators, increasingly relying on its own inputs. Based on these ablation results, we determine that the optimal CCLoss threshold for CP-Guard against PGD attacks is 0.08. This setting optimally balances the trade-off between excluding malicious inputs and maintaining trust in benign collaborative data, thereby enhancing the overall robustness of the system."}, {"title": "Conclusion", "content": "In this paper, we have designed a novel defense framework for CP named CP-Guard. It consists of two parts, the first is PASAC which can effectively sample the collaborators without the prior probabilities of malicious agents. The second is collaborative consistency loss verification which calculates the discrepancy between the ego CAV and the collaborators, which is used as a verification criterion for consensus. The extensive experiments show that our CP-Guard can defend against different types of attacks and can adaptively adjust the trade-off between the performance and computational overhead."}]}