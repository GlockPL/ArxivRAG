{"title": "FACET: Fast and Accurate Event-Based Eye Tracking\nUsing Ellipse Modeling for Extended Reality", "authors": ["Junyuan Ding", "Ziteng Wang", "Chang Gao", "Min Liu", "Qinyu Chen"], "abstract": "Eye tracking is a key technology for gaze-based\ninteractions in Extended Reality (XR), but traditional frame-\nbased systems struggle to meet XR's demands for high accuracy,\nlow latency, and power efficiency. Event cameras offer a\npromising alternative due to their high temporal resolution\nand low power consumption. In this paper, we present FACET\n(Fast and Accurate Event-based Eye Tracking), an end-to-end\nneural network that directly outputs pupil ellipse parameters\nfrom event data, optimized for real-time XR applications.\nThe ellipse output can be directly used in subsequent ellipse-\nbased pupil trackers. We enhance the EV-Eye dataset by\nexpanding annotated data and converting original mask labels\nto ellipse-based annotations to train the model. Besides, a novel\ntrigonometric loss is adopted to address angle discontinuities\nand a fast causal event volume event representation method is\nput forward. On the enhanced EV-Eye test set, FACET achieves\nan average pupil center error of 0.20 pixels and an inference\ntime of 0.53 ms, reducing pixel error and inference time by\n1.6\u00d7 and 1.8\u00d7 compared to the prior art, EV-Eye, with 4.4\u00d7\nand 11.7\u00d7 less parameters and arithmetic operations. The code\nis available at https://github.com/DeanJY/FACET.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, Extended Reality (XR) is rapidly transforming\nthe way people perceive and interact with the digital world.\nEye tracking, a technology that measures and records eye\nmovements, has become indispensable for immersive XR\nexperiences [1], [2], especially after the introduction of the\nApple Vision Pro in June 2023 [3]. Eye tracking enables\ngaze-based interactions in XR environments [4]-[6], allow-\ning users to control and navigate virtual spaces simply by\ndirecting their gaze. While efforts continue to integrate this\ntechnology into wearable devices [7]-[9], challenges like\nlatency and power consumption must be addressed to ensure\nsmooth and effective experience.\nThe human eye is the fastest-moving organ, capable of\nmovements exceeding 300\u00b0/s [10]. Capturing these rapid\nmovements accurately requires a frame rate of kilo-hertz\nto ensure smooth tracking and reduce motion sickness in\nvirtual environments [11]. However, achieving such a high\nframe rate is challenging for wearable devices, which must\noperate at low power levels, typically in the milliwatt range.\nMost head-mounted devices (HMDs) rely on frame-based\neye-tracking systems. A recent study reports tracking delays\nbetween 45 and 81 ms in various HMD eye trackers [12],\nwhich falls short of the kilo-hertz frame rate needed for\naccurate eye movement capture. Additionally, frame-based\nsensors capable of reaching kilo-hertz consume substantial\npower. The large data volumes also require high bandwidth\nand significant energy for transfer and processing, posing\nchallenges for real-time applications on wearable devices.\nEvent cameras [13], also known as Dynamic Vision Sen-\nsors (DVS), offer an effective and efficient alternative for\nsolving eye-tracking challenges. By capturing only bright-\nness changes, they generate sparse asynchronous events, pro-\nviding high temporal resolution and low power consumption.\nThese unique characteristics make event cameras highly suit-\nable for high-speed, low-power eye tracking: they produce\nless data and reduce processing needs during fixation while\nstill capturing fast and subtle eye movements during sac-\ncades. Previous event-based eye-tracking studies have shown\npromising results [14]-[24]. However, most of them use\ndetection neural networks to detect the pupil in every step.\nThe high computational cost of neural networks prevents\nthese models from achieving higher frequency. [20], [21] use\nsimple ellipse-based trackers to track the pupil for most steps\nand employ neural network inference only when the pupil\ntracking is lost, for example after a blink. This detection-\ntracking schema significantly reduces the computational bur-\nden. However, these two methods use a segmentation model\nto acquire the mask of the pupil and then fit its ellipse\nboundary. Compared to lightweight detection models like\nMobileNet series [25]-[27], segmentation models (e.g. U-\nNet [28]) have larger computational cost. It also does not\ntake advantage of the fact that event cameras emphasize the\nboundaries of pupils.\nTo fully take advantage of the event data, we propose\nFACET, Fast and Accurate Event-based Eye Tracking, a\nlightweight pupil detector that takes in events and outputs\nellipse prediction of pupils, which is not only lighter and\nfaster, but also can be trained end-to-end. This detector can\nbe directly fitted into the existing detection-tracking eye-\ntracking schema. The main contributions of this work are\nas follows:\n\u2022\nWe introduced a fast and accurate end-to-end event-\nbased pupil detector using ellipse modeling.\n\u2022\nWe proposed a dataset enhancement method that uses a\nsemi-supervised approach to expand the annotated data\nand convert the mask labels to ellipse-based annotations.\nWe used this method to label all 1.5 million samples in"}, {"title": "II. RELATED WORKS", "content": "A. Frame-based Eye Tracking Method\nTraditional frame-based eye tracking typically utilizes\nframe-based cameras to capture eye movements, with two\ncommon approaches: model-based and appearance-based eye\ntracking. Model-based eye tracking [29]-[31] locates key\npoints corresponding to the eye's geometrical features and\nfits them to an eye model using optimization techniques.\nThese methods have limitations in headsets, which often\nrequire manual calibration and struggle with variations in\neye shape and lighting conditions. The appearance-based\nmethod [32]-[36] focuses on the visual appearance of the\neye, with a trend of using deep learning techniques to\ntrack the eye within the raw image. It requires substantial\ntraining data and the model can be computationally intensive\nand leads to large processing latency. Additionally, these\nframe-based methods often require high-resolution cameras,\nwhich can be both expensive and cumbersome for mobile\ndevices. Moreover, the frame rate of the standard frame-\nbased camera generally peaks at 200 Hz. Cameras with\nhigher frame rates consume significant power, often at watt\nlevels, which exceeds the milli-watt power budget for a\nmobile eye tracking system.\nB. Event-based Eye Tracking Method\nEvent-based eye tracking utilizes the sparse data stream\nfrom DVS for high frame rates with much less bandwidth, of-\nfering greater energy efficiency than traditional frame-based\nsystems. 3ET [14] introduces a sparse change-based convo-\nlutional Long-Short-Term-Memory (LSTM) model for event-\nbased eye tracking, which reduces arithmetic operations by\napproximately 4.7\u00d7, compared to a standard convolutional\nLSTM, without losing accuracy, however, uses synthetic\nevent data and fixed time windows, limiting its ability to meet\nkilo-hertz frame rate demands. Retina [15] introduces a neu-\nromorphic approach that integrates a directly trained Spiking\nNeuron Network (SNN) regression model and leverages\na state-of-the-art low power edge neuromorphic processor,\nachieving 5 mW power consumption and around 3-pixel error\non the INI-30 dataset with 64\u00d764 resolution. Lightweight\nmodels [19], [22] propose spatio-temporal convolution and\nbidirectional selective recurrent models, respectively, both\nwith approximately 3-pixel error with 60\u00d780 resolution of\nthe 3ET+ dataset [18]. [23] tracks the eye movements by\ndetecting and tracking the corneal glint; however, it requires\nillumination from a flashing light source.\nOn the other hand, frameworks [17], [20] have com-\nbined both frame and event data for eye tracking, utiliz-\ning geometric fitting techniques and segmentation networks,"}, {"title": "III. DATASET", "content": "EV-Eye [20], the largest existing event-based eye-tracking\ndataset, contains data from 48 individuals with a diverse\nrange of genders and ages. The dataset includes over 1.5\nmillion near-eye grayscale images and 2.7 billion event sam-\nples captured using two DAVIS346 event cameras. Frames\nare timestamped at 40 ms intervals, synchronized with the\ncorresponding event data. Although the EV-Eye dataset [20]\nprovides a valuable foundation, it has limitations that hinder\nits direct application to our project. It contains only around\n9,000 frames with annotated pupil segmentation, which is\ninsufficient for training robust models. The labels are full-\nsize segmentation masks, while subsequent tracking modules\nrequire ellipse predictions as input.\nTo address these issues, we improved the dataset in two\nkey ways: (1) we expanded it by labeling additional frames,\nand (2) we converted the full-image pupil segmentation\nmasks into ellipse parameter labels. Fig. 1 illustrates the\nprocess of generating the updated dataset. A semi-supervised\nlearning approach was employed to utilize the large vol-\nume of unlabeled data effectively. Pupil segmentations on\nunlabeled grayscale images were obtained using a U-Net\nmodel [28] trained on the labeled frames in this dataset. All"}, {"title": "IV. METHOD", "content": "This section covers the processing of events in Section IV-\nA, the network architecture in Section IV-B, and the loss\nfunction in Section IV-C. An overview of the entire frame-\nwork is shown in Fig. 2.\nPupil segmentation labels are fitted to ellipses expressed in\n(x, y, a, b, 0) format, where (x,y) represent the ellipse's center\ncoordinates, a and b are the lengths of the major and minor\naxes (a > b), and \u03b8\u2208 [0\u00b0, 180\u00b0) denotes the rotation angle.\nInaccurate labels are manually removed. From the updated\nEV-Eye dataset, 20,000 samples are randomly selected for\nthe training set, 5,000 for the validation set, and 5,000 for\nthe test set.\nA. Event Processing\n1) Event Binning Method: To prepare event data for\nneural network input, events are divided into bins and\naccumulated into representations. Choosing the appropriate\nbinning duration is important: Short bins may lack sufficient\ndata, while long bins can reduce frame rate and introduce\nexcessive noise. Previous works [14], [22] use fixed time\ninterval binning to maintain consistent frame rates. However,\nthis approach has limitations: when there is no eye move-\nment, no events are generated, yet the model still consumes\nresources on unnecessary inference; during eye movements, a\nlarge volume of events is produced in a short time, increasing\ncomputational load. In our FACET framework, we utilize a\nfixed-count binning method. This allows the model to avoid\nwasting resources on unnecessary inference when no events\nare generated due to the lack of eye movement.\n2) Event Accumulation Method: One method to accu-\nmulate events in bins into representations is the event vol-\nume [38]. However, the event volume at time t takes events\nboth before and after t, which is impossible during real\ntime processing. Due to the temporal causality of event\nsequences, causal event volume [19] using only events before\nt is more suitable for real-time processing. Building upon this\napproach, FACET proposes a fast causal event volume that\nfurther reduces the time required for event accumulation.\nFor an event bin B containing n events $E = \\{e_i|i=1\u2026n\\}$\nin a period of \u0394t, where $E_i = (x_i,y_i, p_i,t_i)$ represents the\ncoordinates, polarity, and timestamp of the number i event\nin the bin, causal event volume accumulates all the events in\nthe bin to a 2D representation. $p_i = 0$ means a negative event\nshowing the pixel gets dimmer at that time, while $p_i = 1$\nmeans a positive event showing the pixel gets brighter. For\nthe pixel at (x,y), the value of the causal event volume at\nthe end of the event bin t is calculated as follows:\n$V_{pos}(x,y) = \\sum_{\\{E_i|p_i=1\\}} \\delta_{xi,x} \\delta_{yi,y} k(\\frac{t-t_i}{\\Delta t})$\n$V_{neg} (x,y) = \\sum_{\\{E_i|p_i=0\\}} \\delta_{xi,x} \\delta_{yi,y} k(\\frac{t-t_i}{\\Delta t})$\n\u03b4 represents the Kronecker delta function, in which $\u03b4_{ij} = 1$\nonly if i = j, otherwise $\u03b4_{ij} = 0$. The kernel function k(t) and", "equations": ["V_{pos}(x,y) = \\sum_{\\{E_i|p_i=1\\}} \\delta_{xi,x} \\delta_{yi,y} k(\\frac{t-t_i}{\\Delta t})", "V_{neg} (x,y) = \\sum_{\\{E_i|p_i=0\\}} \\delta_{xi,x} \\delta_{yi,y} k(\\frac{t-t_i}{\\Delta t})"]}, {"title": "Algorithm 1 Fast Causal Event Volume", "content": "Input:\nParam:\nOutput:\n$E = \\{e_i | i = 1\u2026\u2026n\\}$ ($e_i = (X_i, Y_i, P_i, t_i)$)\nl: limit\nc: contribution\n$V_{pos}, V_{neg}$\nfor $e_i$ in E do\nif $p_i$ is positive then\nif $V_{pos}[X_i, Y_i] + C_i <l$ then\n$V_{pos}[X_i, Y_i] \u2190 V_{pos}[X_i, Y_i] + C_i$\nend if\nelse\nif $V_{neg}[X_i, Y_i] + C_i <l$ then\n$V_{neg}[X_i, Y_i] \u2190 V_{neg}[X_i, Y_i] + C_i$\nend if\nend if\nend for\nreturn $V_{pos}, V_{neg}$\nHeaviside step function H(x) are defined as:\n$k(\u03c4) = H(\u03c4) max(1 - |\u03c4|,0)$\n$H(x) = \\begin{cases}\n0, if x < 0\\\\\n1, if x \u2265 0\n\\end{cases}$\nhere, $V_{pos}$ and $V_{neg}$ represent the accumulated contribution\nof positive and negative polarity events, respectively, to the\noccupancy of the bin.\nAccumulating events with setting limits will reduce pro-\ncessing time and benefit for using min-max normalization,\ncontributing to more stable training. As shown in Algorithm\n1, fast causal event volume offers the advantage that once\nevents at the same coordinate reach the defined limit, they\nare considered to have sufficient information, and no further\naccumulation is performed, reducing the time to accumulate\nevents. Fig. 3 gives examples of different event accumulation\nmethods: event volume, causal event volume, and fast causal\nevent volume.\nB. Network\nThe network is designed with a focus on lightweight archi-\ntecture. We use MobileNetV3 [27] as the backbone for fea-\nture extraction, taking advantage of depthwise separable con-\nvolution (DSC) blocks to reduce complexity. Furthermore,\nwe accelerate the Feature Pyramid Network (FPN) [39] by\nreplacing traditional convolution blocks with DSC blocks,\nenhancing overall performance.\n1) MobileNetV3 Backbone: MobileNetV3 reduces model\nsize using DSC and enhances model expressiveness with\nsqueeze-and-excitation (SE) blocks, achieving high inference\nefficiency and high. MobileNetV3 has been proven to be an\nexcellent backbone for models working on edge devices like\nmobile phones and XR devices.\n2) FPN with DSC: We replace all normal convolution\nblocks in FPN with DSC, reducing the parameters of FPN\nand improving the speed of feature fusion. The FPN of", "equations": ["k(\u03c4) = H(\u03c4) max(1 - |\u03c4|,0)", "H(x) = \\begin{cases}\n0, if x < 0\\\\\n1, if x \u2265 0\n\\end{cases}"]}, {"title": "FACET can be represented as (5):", "content": "$P_i = DSC(C_i) + Upsample(P_{i+1}), i\u2208 \\{5,4,3,2\\}$\nwhere $C_i$ represents the feature maps from different stages\nof MobileNetV3, and $P_i$ represents the corresponding feature\nmaps in the FPN.\nDSC blocks consist of depthwise convolution (DWC)", "equations": ["P_i = DSC(C_i) + Upsample(P_{i+1}), i\u2208 \\{5,4,3,2\\}"]}, {"title": "C. Loss", "content": "In FACET, we designed a comprehensive loss function to\nenhance the model performance, defined as follows:\n$L = \u03bb_H L_H + \u03bb_O L_O + \u03bb_S L_S + \u03bb_G L_G + \u03bb_T L_T$\nwhere $L_H$ is the Heatmap Loss, $L_O$ is the Offset Loss, $L_S$\nis the Size Loss, and $L_G$ is the Gaussian IoU Loss, and\n$L_T$ is the Trigonometric Loss. The innovative aspect of our\napproach is the introduction of Trigonometric Loss ($L_T$),\nwhich significantly improves angle prediction by addressing\nthe discontinuity in traditional angle loss computation.", "equations": ["L = \u03bb_H L_H + \u03bb_O L_O + \u03bb_S L_S + \u03bb_G L_G + \u03bb_T L_T"]}, {"title": "1) Trigonometric Loss", "content": "We define the rotation of an\nellipse as placing the major axis of the ellipse horizon-\ntally and rotating the ellipse around its center at an angle\n\u03b8\u2208 [0\u00b0, 180\u00b0). Due to the symmetry of ellipses and the\nperiodicity of rotations, 0\u00b0 and_180\u00b0 represent the same\nellipse, which means the two ends of the range [0,180)\nshould be continuous. Regular loss functions usually measure\nthe norm of the difference between the prediction and the\nground truth, leading to a huge discontinuity at the two ends.\nThis discontinuity results in a large gradient event if the real\ndifference between the prediction and the ground truth is\nsmall. The mismatch harms the training of the model.\nTo deal with this discontinuity, we propose Trigonomet-\nric Loss. The model predicts p = (sin(2\u03b8), cos (2\u03b8)). The\ntrigonometric loss $L_T$ calculates the L2 loss between $\u03b3_p$ and\nthe ground truth $\u03b3_g$:\n$L_T = L_2(\u03b3_p, \u03b3_g)$\nThis mapping from \u03b8 to (sin (2\u03b8), cos (2\u03b8)) transfers the\ndiscontinuous domain [0,180) to a continuous 2D domain.\nFor example, in Fig. 4, $\u03b8_a$ = 179\u00b0 should be similar to $\u03b8_b$ =\n1\u00b0. Therefore, the loss should be small. But $\u03b8_c$ = 90\u00b0 should\nbe very different from $\u03b8_a$ and $\u03b8_b$, thus should have a big\nloss. As is shown in Table I, if we compare $L_T$ with regular\nL1 angle loss $L_A = L_1(\u03b8_p, \u03b8_g)$, we can see that $L_A(\u03b8_a, \u03b8_b)$\nis even bigger than $L_A(\u03b8_a, \u03b8_c)$ and $L_A (\u03b8_b, \u03b8_c)$. In contrast,\n$L_T$ offers a more reasonable loss, where $L_T(\u03b8_a, \u03b8_b)$ ~ 0 and\n$L_T(\u03b8_a, \u03b8_c) = L_T (\u03b8_b, \u03b8_c) \u226b L_T(\u03b8_a, \u03b8_b)$.\n2) Other Loss Components: Beyond the proposed\nTrigonometric Loss, the total loss function incorporates the\nHeatmap Loss $L_H$, Offset Loss $L_O$, and Size Loss $L_S$ from\nCenterNet [40] and Gaussian IoU Loss $L_G$ from ElDet [41].\n$L_H$ is the focal loss of the heatmap. $L_O$ and $L_S$ are smooth\nL1 losses between the predicted offset, scale and their corre-\nsponding ground truth. Gaussian IOU loss $L_G$ is proposed\nin ElDet. An ellipse bounding box $B(x, y, a, b, \u03b8)$ can be", "equations": ["L_T = L_2(\u03b3_p, \u03b3_g)"]}, {"title": "V. EXPERIMENTAL RESULTS", "content": "We evaluate our model and other comparative models: E-\ntrack [21], EV-Eye [20], ElDet [41], and TennSt [19], on\nthe enhanced EV-Eye dataset described in Section III. All\nmetrics are obtained at a resolution of 64 x 64.\nA. Training Details\nWe train our models implemented with PyTorch [43] on\na single NVIDIA RTX 3090 GPU. We used a batch size of\n32, with 70 training epochs. The optimizer is Adam, with\nan initial learning rate of 1\u00d710-3 and a weight decay of\n1 \u00d7 10\u20135. For the first five epochs, the warm-up learning\nrate is 1 \u00d7 10-5, and the learning rate will decay by a\nfactor of 0.7 every 10 epochs thereafter. For the proposed\nfast causal event volume, we set the limit l to be 25. We\napply data augmentation techniques such as rotation, scaling,\ntranslation, and horizontal flip to simulate varying distances,\nangles, positions, and orientations of the eye relative to the\nevent camera, improving model generalization."}, {"title": "D. Ablation Studies", "content": "Table III presents the results of the ablation study. EPT\ndenotes the event processing time in milliseconds. FACET\nperforms well across all metrics, achieving a $P_1$ of 99.59%,\na PE of 0.2030 pixels, and an EPT of 1.6493 ms. For\nevent accumulation, we found that traditional event volume\nand causal event volume methods increased the EPT by\n0.20 ms and 0.13 ms, respectively. In contrast, using fast\ncausal event volume leads to a decrease in all metrics. For\nthe event binning method, fixing the event count to 5000\nprovides the best-balanced performance. Reducing the count\nto 500 reduces $P_1$ to 93.52% and increases PE to 0.4326\npixels, indicating that fewer events do not capture enough\ninformation. Increasing the count to 10,000 raises $P_1$ to\n99.89%, but also increases the EPT to 2.8983 ms, nearly\n1.8x longer than using 5000 events. Using a fixed time\ninterval reduces the EPT to 0.8480 ms, but with variable\nevent counts, the accuracy drops to $P_1$ of 97.29%. Using\nAngle Loss instead of Trigonometric Loss lowers $P_1$ to\n98.90% and increases PE to 0.2878 pixels, confirming that\nour loss design improves accuracy."}, {"title": "VI. CONCLUSION", "content": "This work enhances the existing event-based eye-tracking\ndataset EV-Eye and proposes a fast and accurate eye-tracking\nsolution: FACET, using pure event data. FACET directly\noutputs ellipses accurately and quickly for subsequent track-\ning. It uses fast causal event volume to reduce event pro-\ncessing time and a novel trigonometric loss to address the\ndiscontinuity in traditional angle prediction. Our experiments\ndemonstrate that FACET is competitive in efficiency while\nachieving superior accuracy among the state-of-the-art meth-\nods, which highlights FACET's significant potential for eye\ntracking in XR environments. In future work, we aim to\nintegrate FACET into an optimized XR system using neural\nprocessing units and event-based sensors, enabling seamless\nreal-time eye tracking on headsets."}]}