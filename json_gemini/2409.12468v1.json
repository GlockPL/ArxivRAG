{"title": "FAMILIARITY-AWARE Evidence COMPRESSION FOR RETRIEVAL AUGMENTED GENERATION", "authors": ["Dongwon Jung", "Qin Liu", "Tenghao Huang", "Ben Zhou", "Muhao Chen"], "abstract": "Retrieval Augmented Generation (RAG) improves large language models (LMs) by incorporating non-parametric knowledge through evidence retrieval from external sources. However, it often struggles to filter out inconsistent and irrelevant information that can distract the LM from its tasks. While compressing the retrieved evidence with a compression model aims to address this issue, the compressed evidence may still be unfamiliar to the target model used for downstream task, potentially failing to utilize the evidence effectively. We propose FAVICOMP (Familiarity-aware Evidence Compression), a novel training-free evidence compression technique that makes retrieved evidence more familiar to the target model, while seamlessly integrating parametric knowledge from the model. Specifically, FAVICOMP proactively lowers the perplexity of the compressed evidence with regard to the target model by combining token probabilities from both the compression model and the target model to generate context that is more familiar to the target model. This approach balances the integration of parametric and non-parametric knowledge, which is especially helpful in complex tasks where the retrieved evidence set may not contain all the necessary information. Experimental results demonstrate that FAVICOMP consistently outperforms existing baselines in multiple open-domain QA datasets, achieving high compression rates and showcasing the effective integration of both parametric and non-parametric knowledge.", "sections": [{"title": "INTRODUCTION", "content": "Retrieval Augmented Generation (RAG) has become a common practice to leverage external knowledge beyond its inherent knowledge boundaries to perform better in knowledge-intensive tasks such as open-domain question answering (QA) (Lewis et al., 2020; Izacard & Grave, 2021; Guu et al., 2020) and fact-checking (Pan et al., 2023; Li et al., 2024b). In particular, incorporating multiple evidence pieces is crucial in solving complicated tasks such as multi-hop and complex reasoning (Trivedi et al., 2023; Jiang et al., 2023b; Li et al., 2024a), which require various sources of information to solve the questions.\nNevertheless, RAG often struggles to filter out inconsistent and irrelevant information from the multiple evidence set, which can interfere with downstream tasks (Shi et al., 2023). This highlights the need for compression-based RAG (Jiang et al., 2023a; Xu et al., 2024; Yoon et al., 2024) to identify and retain only the essential information for the LMs to utilize effectively. Traditionally, compression-based RAG has focused on reranking documents or sentences by relevance and then incorporating a top-ranked subset (Nogueira et al., 2020; Zhuang et al., 2023; Wang et al., 2023c) or compressing the documents into an abstractive summary that retains only essential context (Jiang et al., 2023a; Xu et al., 2024; Yoon et al., 2024). However, the reranked or compressed evidence might be unfamiliar to the LM employed for the downstream task (referred to as target model) due to discrepancies in internal knowledge and prompt preferences between the compression model and the target model, which are learned during the pretraining stage (Gonen et al., 2023; Lee et al., 2024; Mallen et al., 2023). When LMs encounter unfamiliar contextual information, they often fail in balancing parametric and non-parametric knowledge, either by overly relying on their parametric knowledge (Longpre et al., 2021; Tan et al., 2024) or by utilizing retrieved evidence without considering its relevance to the input (Wu et al., 2024).\nTo address these challenges, we propose FAVICOMP (Familiarity-aware Evidence Compression), a training-free evidence compression method that compresses evidence that is more familiar to the target model while integrating parametric knowledge into the compressed evidence. Inspired by the prior findings that an LM's familiarity with a prompt is generally reflected by low perplexity (Liu et al., 2024; Gonen et al., 2023; Wang et al., 2023b), FAVICOMP proactively lowers the perplexity of the compressed evidence with regard to the target model. Specifically, FAVICOMP leverages the token probabilities of two LMs, a compression model and the target model. The compression model is instructed to summarize the raw evidential documents into a relevant context to the input, while the target model is instructed to generate relevant context without referencing the documents. Instead of directly selecting the highest probability token from the compression model at each decoding step, we ensemble the token logits from both the compression and target models and then select the token with the highest probability from this combined set. This ensemble decoding constrains the token search space of the compression model to those with lower perplexity, making the context more familiar to the target model (Liu et al., 2024).\nFurthermore, FAVICOMP potentially synergizes the retrieved knowledge with the target model's parametric knowledge introduced during ensemble decoding. FAVICOMP can effectively discern when to leverage internal or external knowledge, which is particularly beneficial in the presence of noisy contextual evidence in complex tasks such as multi-document QA (Wang et al., 2024).\nFAVICOMP brings along key advantages of RAG for complex tasks from two perspectives. On the one hand, it is capable of compressing multiple augmented documents to a more favorable form to the target model. This mechanism not only helps the model better comprehend the essential evidence in the retrieval augmentation but also better balances knowledge utility in both the evidential context and the model's parametric memory. On the other hand, it is a training-free and model-agnostic approach that can be easily plugged into the RAG processes of any auto-regressive LMs.\nOur experiments show that FAVICOMP outperforms all existing baselines in five open-domain QA datasets with two downstream LMs while achieving high compression rates. Additionally, we conduct ablation studies by varying the degree of token ensembling and analyze its impact on performance and context perplexity. Moreover, we investigate how FAVICOMP effectively integrates parametric and non-parametric knowledge."}, {"title": "METHOD", "content": "We present FAVICOMP, a decoding-time evidence compression method that familiarizes the retrieved evidence with the target model while synergizing them with the model's parametric knowledge. We first illustrate the motivation for FAVICOMP in \u00a72.1 and provide the preliminaries of compression-based RAG in \u00a72.2, followed by a detailed definition of our proposed framework in \u00a72.3."}, {"title": "MOTIVATION AND METHOD OVERVIEW", "content": "Standard RAG faces the challenge of LMs struggling to filter out inconsistent and irrelevant information from multiple evidence pieces, which can interfere with downstream tasks (Shi et al., 2023). Previous research has primarily concentrated on question-focused compression (Jiang et al., 2023a; Xu et al., 2024; Yoon et al., 2024); however, this approach may lead to suboptimal performance in downstream tasks due to the compressed evidence's potential unfamiliarity with the target model employed. This unfamiliarity arises from discrepancies in internal knowledge and prompt preferences between the compression model and the target model, which are learned during pretraining (Gonen et al., 2023; Lee et al., 2024; Mallen et al., 2023). Furthermore, the unfamiliarity often leads to failure in balancing parametric and non-parametric knowledge, either by overly relying on their parametric knowledge (Longpre et al., 2021; Tan et al., 2024) or by using retrieved evidence without considering its relevance to the input (Wu et al., 2024). To address this issue, FAVICOMP introduces a novel approach that compresses evidence that better aligns with the target model's preferences while seamlessly integrating parametric knowledge into the compressed evidence using a novel ensemble decoding technique, thereby improving its performance on downstream tasks.\nFig. 1 illustrates the overview of FAVICOMP. In this example, FAVICOMP makes the compressed evidence more favorable to the target model and leverages its parametric knowledge to supplement the missing evidence (\u201cLionel Messi made his league debut in Barcelona\"), effectively combining evidential and parametric knowledge.\""}, {"title": "COMPRESSION-BASED RETRIEVAL AUGMENTED GENERATION", "content": "Given a set of k retrieved evidence snippets $D = \\{d_1,d_2,...,d_k\\}$ and a textual input sequence x, standard RAG aims to generate an output sequence y, conditioned on both D and x. However, standard RAG directly utilizes D which often contains irrelevant information to x, potentially confusing the target model in downstream tasks (Shi et al., 2023). Thus, the compression-based RAG uses an additional compression model to condense D into a concise and input-relevant context c, which is then used in place of D during the downstream generation process. Thus, the compression-based RAG is formalized as:\n$y^* = \\arg \\max_y P_t(y|x, \\hat{c}),$\n$\\hat{c} = P_c(c | x, [d_1,d_2,..., d_k]),$\nwhere $y^*$ is the final output sequence, $[,]$ denotes concatenation, and $P_t$ and $P_c$ represent the probability distributions of the target and compression models, respectively. In this work, we consider any natural language prompting tasks, such as open-domain QA tasks, where x represents the input prompt (also known as the query in QA tasks) and $y^*$ denotes the output sequence.\nThe compression model's objective is to produce a concise yet informative summary c of the evidential documents D that captures the essential information relevant to the input query x. We use an unsupervised approach, where the model is instructed to generate a query-relevant summary of D in a zero-shot manner using an evidence compression instruction prompt, denoted as $I_{comp}$, such as the one below:\nSpecifically, the evidence compression is done in an auto-regressive way formalized as,\n$P_c(c | I_{comp}, x, D) = \\prod_{i=1}^{|c|} P_c(c_i | I_{comp}, x, D, c_{<i}),$\nwhere $|c|$ is the length of the summary c."}, {"title": "ENSEMBLE DECODING FOR FAVICOMP", "content": "Simple compression techniques might lead to subpar performance in downstream tasks because the compressed evidence may not be familiar to the target model. To better align the context to the target model, FAVICOMP proactively lowers the perplexity of the context by introducing a constraint in decoding space from the target model during the evidence compression. FAVICOMP achieves this goal through ensemble decoding, which involves a multiplicative ensemble of two LMs-compression model and target model-at each decoding step.\nSpecifically, the target model is directed to generate a context c that would be helpful in answering the question x without referencing the evidence set. This is also done in zero-shot using a context generation instruction prompt $I_{gen}$ such as:\nThe context generation is also performed in an auto-regressive fashion, represented as:\n$P_t(c | I_{gen}, x) = \\prod_{i=1}^{|c|} P_t(c_i | I_{gen}, x, c_{<i}),$\nwhere $|c|$ denotes the length of the generated context c.\nOnce the compression model and the target model generate their respective probability distributions for the next token, the subsequent token is chosen by maximizing the weighted sum of the log probabilities from both models. The selected token is the continuation of the previously generated text aligned with their objectives. This process is formalized as follows:\n$c_i = \\arg \\max_{c_i \\in V} ((1-\\alpha) \\log P_c(c_i | I_{comp}, x, D, c_{<i}) + \\alpha \\cdot \\log P_t(c_i | I_{gen}, x, c_{<i})),$ where $c_i$ is the subsequent token, and $\\alpha$ is the ensemble coefficient that weighs between the two probability distributions. We demonstrate how the coefficient $\\alpha$ impacts both the perplexity and the downstream performance in \u00a74.2.\nEnsemble decoding proactively shifts the token search space in evidence compression by upweight-ing those tokens with lower perplexity from the target model's perspective (Liu et al., 2024), resulting in a compressed evidence that is more familiar to the target model. Note that since both objectives ultimately share the goal of generating context relevant to the question, combining the logits ensures alignment with this ultimate goal.\nIn addition, ensemble decoding enables FAVICOMP to seamlessly integrate both retrieval knowledge from the external evidence set and the target model's parametric knowledge. Specifically, FAVICOMP selects the arg max token from the target model only when the token's probability is higher than that of the compression model, demonstrating that FAVICOMP draws on parametric knowledge only when necessary-potentially when the compression model is uncertain about the next token. This is particularly beneficial for complex tasks like multi-document QA, where the evidence set may not include all the necessary information (Mallen et al., 2023). In such cases, the missing information in compressed evidence can be supplemented by tokens generated from context generation by the target model, which is entirely based on parametric knowledge. We demonstrate in \u00a74.3 and \u00a75 that FAVICOMP can incorporate knowledge from both sources effectively, leading to a performance boost compared to compression methods that solely focus on distilling knowledge from the evidence set."}, {"title": "EXPERIMENTAL SETTINGS", "content": "We assess the effectiveness of FAVICOMP on knowledge-intensive QA tasks. In this section, we delve into the details of the experimental settings."}, {"title": "DATASETS", "content": "We evaluate FAVICOMP on five open-domain QA datasets, including two single-document QA datasets, Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (TQA; Joshi et al. 2017), and three multi-document QA datasets, HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (2WikiMQA; Ho et al. 2020), and MuSiQue (Trivedi et al., 2022). Following prior studies (Asai et al., 2023; Xu et al., 2024), we evaluate the performance on the development set of each dataset and use two evaluation metrics, i.e. Accuracy (Acc) and token-level F1."}, {"title": "IMPLEMENTATION DETAILS", "content": "For all the comparison methods, we utilize two LMs as the target model to tackle downstream QA tasks with RAG, i.e. Llama3-8B-Instruct\u00b2 and Mistral-7B-Instruct\u00b3. For each question, we retrieve five documents from 2018 Wikipedia corpus (Karpukhin et al., 2020) using Contriever-MSMARCO (Izacard et al., 2021), so as to be consistent with previous studies (Xu et al., 2024; Yoon et al., 2024)\nFor FAVICOMP, we use the same LM for both the compression model and the target model. Note that the compression model and the target model can be different, as long as they share the same tokenizer to ensure compatible token logits. Also, we set \u03b1 to 0.5 by default, for which more analyses are given in \u00a74.2. The prompts used in the experiment are presented in Appx. \u00a7B."}, {"title": "BASELINES", "content": "We consider the following categories of baselines. (1) No Context: RAG without any context. (2) Gold Document: RAG using directly relevant evidence from the retrieved documents if they exist. (3) Raw Document: RAG with raw documents that have not undergone any compression. (4) Generated Context (Yu et al., 2023): RAG with context generated by the same LM as the target model. This is equivalent to FAVICOMP with \u03b1 = 1, as we rely solely on the target model to generate context when \u03b1 = 1. (5) Reranking-based Methods: We rerank sentences in the evidence set and choose top-ranked sentences as the context. We utilize two rerankers-Sentence-BERT (Reimers &\nGurevych, 2020) and RECOMP-extractive (Xu et al., 2024). (6) Compression-based Methods: We employ four compressors-LongLLMLingua (Jiang et al., 2023a), RECOMP-abstractive (Xu et al.,\n2024), CompAct (Yoon et al., 2024), and Zero-shot Summarization. Zero-shot Summarization is instructed to summarize the evidence set into a concise summary based on the question, using the same LM as the target model. This is equivalent to FAVICOMP with \u03b1 = 0, as we depend entirely on the compression model without any intervention from the target model. A detailed explanation of the implementation of the baselines is provided in Appx. \u00a7A."}, {"title": "EXPERIMENTAL RESULTS", "content": "In this section, we compare the overall performance of FAVICOMP with other baselines across the five datasets (\u00a74.1), explore the impact of ensemble coefficient \u03b1 on performance and perplexity (\u00a74.2), investigate how effectively FAVICOMP incorporate parametric and non-parametric knowledge (\u00a74.3), and compare the compression rates with other baselines (\u00a74.4)."}, {"title": "MAIN RESULTS", "content": "The overall performance of FAVICOMP and the baselines across the five datasets is presented in Tab. 1. To start with, the compression-based methods consistently outperform the reranking-based methods, due to that the reranking-based methods are prone to losing more question-relevant information by discarding lower-ranked sentences. Next, FAVICOMP outperforms all other baselines across all the datasets, except for the Gold Document which is regarded as the upper bound of the performance. It is noteworthy that FAVICOMP, as a training-free, decoding-time strategy, outperforms supervised baselines, some of which are trained on the training set of the datasets. For the MuSiQue dataset, FAVICOMP even outperforms Gold Document baseline\u2014which can be viewed as a perfect compressor-with Llama3-8B-Instruct, and shows comparable results with Mistral-7B-Instruct. This demonstrates that explicitly incorporating parametric knowledge from the target model can significantly enhance performance in multi-document QA, even when the context is imperfect.\nMoreover, it is surprising that all the existing supervised compression-based methods are excelled by the Raw Document baseline. This indicates that existing methods are likely to fall short of retaining essential supportive information while compressing the evidence documents. Additionally, LongLLMLingua and RECOMP-abstractive perform worse than Zero-shot Summarization, possibly due to the use of smaller compression models (T5-large and Llama2-7B-Chat, respecitvely) compared to that used in Zero-shot Summarization (Llama3-8B-Instruct). However, it suggests that knowledge distillation from larger teacher LMs to the smaller compression models may not generalize well, as the context preferences and prior knowledge of the target model and the teacher model are likely to differ.\nFurthermore, despite using the same base model for the compression model (Mistral-7B-Instruct), the training-free FAVICOMP outperforms CompAct, which trains the compression model using knowledge distillation to generate and evaluate summaries of retrieved documents. This also indicates that knowledge distilled from a teacher model may not always be effectively transferable to the target model due to discrepancies in context preference and prior knowledge. In contrast, the superior performance of FAVICOMP is attributed to its ability to familiarize evidence with the target model and its effective incorporation of parametric knowledge from ensemble decoding.\nFinally, given that Zero-shot Summarization corresponds to FAVICOMP with \u03b1 = 0 and Generated Context corresponds to FAVICOMP with \u03b1 = 1, the fact that FAVICOMP outperforms both baselines highlights its ability to effectively incorporate tokens from both sources\u2014evidence summary and generated context. This results in superior performance compared to relying on just one source alone."}, {"title": "IMPACT OF ENSEMBLE COEFFICIENT ON PERFORMANCE AND PERPLEXITY", "content": "Fig. 2 illustrates how performance and perplexity change as the ensemble coefficient \u03b1 is varied across the values {0.0,0.1, 0.3, 0.5, 0.7, 0.9, 1.0} on NQ, HotpotQA and MuSiQue datasets. We calculate the perplexity of the compressed evidence conditioned on the preceding inputs, i.e. instruction, demonstrations, and the question. For all the datasets, performance is the highest when \u03b1 = 0.5, indicating that proactively lowering perplexity by equally weighting both input sources yields the best results. Additionally, the performance tends to improve as the perplexity of compressed evidence decreases, which aligns with the previous works (Liu et al., 2024; Gonen et al., 2023). Interestingly, when \u03b1 is equal to 0.9 or 1.0, there is a slight increase in perplexity. At high \u03b1 values, FAVICOMP is more likely to generate context without referring to external knowledge. As a result, the LM's increased uncertainty when generating context with limited evidential knowledge is likely the cause of the slight rise in perplexity. Results for other datasets are included in Fig. 5."}, {"title": "INTEGRATION OF PARAMETRIC AND NON-PARAMETRIC KNOWLEDGE", "content": "The effective integration of parametric and non-parametric knowledge is crucial for complex tasks such as multi-document QA, where the evidence set may not contain all the necessary information. To this end, we evaluate how effectively FAVICOMP incorporates parametric knowledge from the target model and non-parametric knowledge from the compression model on the multi-document QA datasets. We begin by dividing the test samples of each dataset into evidence-relevant and evidence-irrelevant subsets, using the Hits metric. The Hits metric is set to 1 (evidence-relevant) if the retrieved evidence set contains the correct answer, and 0 (evidence-irrelevant) if it does not. We then assess the downstream performance of each subset. The underlying intuition is that if a method performs better on the evidence-relevant subset, it suggests that the method is more effectively utilizing the provided evidential knowledge. Conversely, if a method excels on the evidence-irrelevant subset, it indicates that the method is more effectively leveraging parametric knowledge without relying on potentially irrelevant evidence.\nThe left figure of Fig. 3 compares the accuracy in Hits = 0 and Hits = 1 subsets across the datasets. We compare FAVICOMP with the top-performing unsupervised compression method, Zero-shot Summarization, and the most competitive supervised compression method, CompAct. Compared to the other two baselines, FAVICOMP performs better in the Hits = 0 subset while performing comparably in the Hits = 1 subset. This proves that FAVICOMP effectively relies on parametric knowledge rather than evidential knowledge when faced with irrelevant evidence, while maintaining similar effectiveness in utilizing evidential knowledge when relevant evidence is present.\nInterestingly, even though CompAct generally performs better on the Hits = 1 subset compared to Zero-shot Summarization, it underperforms relative to Zero-shot Summarization on the Hits = 0 subset. This suggests that the training may have been biased towards utilizing solely evidential knowledge, rather than effectively leveraging both sources in synergy.\nWe also evaluate the performance of FAVICOMP with various \u03b1 values under this setting. The right figure of Fig. 3 shows that \u03b1 = 0.5 performs the best on the Hits = 0 subset, while performance declines as \u03b1 deviates further from 0.5. This pattern in the Hits = 0 subset mirrors the overall performance trend, suggesting that appropriately utilizing parametric knowledge when the evidence is irrelevant is crucial to the overall performance. In the Hits = 1 subset, performance remains consistent for \u03b1 values up to 0.5 but decreases significantly when \u03b1 exceeds 0.5 due to the diminished utilization of the relevant evidential context."}, {"title": "COMPRESSION RATE COMPARISONS", "content": "Since one of the functionalities of compression-based RAG is to reduce the number of tokens from the evidence while keeping its essential information, we report the compression rate, calculated as\n$\\frac{\\text{# of tokens in retrieved documents}}{\\text{# of tokens in compressed documents}}$. Overall, FAVICOMP and RECOMP-abstractive consistently scores highest compression rates. RECOMP-abstractive exhibits significantly high compression rates because the compression model is trained to output an empty string when no relevant evidence is found, which is often the case in multi-document QA datasets. FAVICOMP compresses the evidence that is familiar to the target model by lowering the perplexity at each decoding step, typically resulting in a shorter context. Specifically, when compared to Zero-shot Summarization, which is equivalent to FAVICOMP with \u03b1 = 0, FAVICOMP consistently achieves higher compression rates. This demonstrates that the ensemble decoding strategy, combining token logits from both evidence compression and context generation, leads to greater compression efficiency."}, {"title": "CASE STUDY", "content": "Tab. 3 presents two examples from HotpotQA to illustrate how FAVICOMP effectively familiarizes evidence while seamlessly integrating both parametric and non-parametric knowledge during evidence compression. We compare its output with Raw Document, which does not apply any compression, and Zero-shot Summarization, which is equivalent to FAVICOMP with \u03b1 = 0.\nIn both examples, Raw Document fails to produce the correct answer, even though the evidence contains the necessary information, highlighting the need for effective evidence compression. In the first example, while the difference between the compressed evidence from Zero-shot Summarization and FAVICOMP appears subtle, FAVICOMP delivers the correct answer with a lower perplexity in compression, underscoring the significance of evidence familiarization. The second example highlights the importance of parametric knowledge when the retrieved evidence set lacks complete information. Since the evidence set does not mention \u201cSkeptic,\u201d Zero-shot Summarization introduces irrelevant information (\u201cPhilanthropy magazine\"), ultimately leading to an incorrect answer. In contrast, FAVICOMP integrates parametric knowledge about \"Skeptic\" and incorporates it into the evidence compression. Notably, FAVICOMP selects the arg max token from the target model only when the token's probability is higher than that of the compression model, demonstrating that FAVICOMP draws on parametric knowledge only when necessary-potentially when the compression model is uncertain about the next token.\""}, {"title": "RELATED WORKS", "content": "Evidence Compression for RAG. Standard RAG retrieves textual evidence related to the prompt from the external corpora or knowledge bases and incorporates it as a part of the input to the LM (Lewis et al., 2020; Izacard & Grave, 2021; Guu et al., 2020). However, retrieved evidence pieces may contain inconsistent or irrelevant information to the question, potentially confusing the target model in downstream tasks (Shi et al., 2023). To tackle this problem, traditional approaches aim to rerank the textual evidence based on its relevance to the question and then select a top-ranked subset to include as part of the input to the LM (Nogueira et al., 2020; Zhuang et al., 2023). However, this approach lose more question-relevant information by discarding lower-ranked sentences.\nRecent efforts on evidence compression seek to compress retrieved evidence pieces to filter out unnecessary information and retain only the essential context (Wang et al., 2023c; Li et al., 2024c; Jiang et al., 2023a; Xu et al., 2024; Cao et al., 2024; Yoon et al., 2024). Wang et al. (2023c) filter query-relevant context using relevance metrics and Li et al. (2024c) extract query-relevant information and restructure them to form a consistent context. Jiang et al. (2023a) and Cao et al. (2024) conduct token-level or embedding-based compression to preserve only the query-relevant information using a trained compressor. Xu et al. (2024) and Yoon et al. (2024) train a compression model to generate an abstractive summary of the documents by distilling knowledge from larger language models. While these methods are successful to some extent, they often achieve suboptimal performance because the compressed context may be unfamiliar to the LM used in the downstream task due to differences in internal knowledge and prompt preferences between the compression and the target model. In contrast, FAVICOMP proactively lowers the perplexity of the compressed evidence pieces using ensemble decoding technique without any training, thereby improving the downstream performance.\nParametric and Non-parametric Knowledge in RAG. While there have been studies on the phenomena of LM's utilization of both parametric and non-parametric knowledge sources (Longpre et al., 2021; Wadhwa et al., 2024; Wu et al., 2024; Zhang et al., 2024; Zhou et al., 2023; Wang et al., 2023a; Fang et al., 2024), there is a lack of research focused on effectively synergizing both sources. A few of these efforts introduces counterfactual augmentation (Longpre et al., 2021; Fang et al., 2024; Zhang et al., 2024) and causal intervention (Zhou et al., 2023; Wang et al., 2023a) to mitigate knowledge conflict, which, however, requires explicitly knowing the features of the input that causes such conflict. Zhang et al. (2023) seek to address this issue by incorporating LM-generated context into the LM's input along with the retrieved documents, thereby integrating both sources of knowledge. However, merely concatenating both contexts is a suboptimal solution, as LMs may still show bias toward one source over the other when generating responses (Longpre et al., 2021; Wu et al., 2024). To address this, FAVICOMP employs ensemble decoding during the evidence compression, ensuring that both types of knowledge are seamlessly fused together to create a consistent context.\nConstrained Decoding. Constrained decoding has been previously proposed in text generation tasks for various purposes, including optimizing prompts (Liu et al., 2024), enhancing plausibility (Li et al., 2023) or controllability (Meng et al., 2022; Huang et al., 2023), and reducing hallucination (Shi et al., 2024). Contrastive Decoding (Li et al., 2023) enforces a plausibility constraint during generation by inducing the difference in token log-probabilities between expert and amateur LMs. Context-aware Decoding (Shi et al., 2024) uses contrastive decoding to amplify the probability differences between outputs with and without evidence, encouraging the LM to prioritize the evidential knowledge. Our work is closely connected with the method by Liu et al. (2024) which employs ensemble decoding to paraphrase prompts to enhance zero-shot LM prompting and generalization. Their approach focuses on the robustness and generalizability of instruction prompts for tasks without retrieval augmentation. In contrast, our approach compresses externally retrieved evidence while integrating parametric knowledge during compression, specifically targeting knowledge-intensive tasks that require balancing both evidential and parametric knowledge."}, {"title": "CONCLUSION", "content": "In this study, we introduce FAVICOMP, a training-free evidence compression method designed to enhance the RAG by making retrieved evidence set more familiar to the target model, while seamlessly integrating parametric knowledge. By leveraging ensemble decoding, FAVICOMP proactively lowers the perplexity of the compressed evidence making it more favorable to the target model. Moreover, FAVICOMP effectively balances the target model's parametric knowledge and the retrieved knowledge, improving performance on complex tasks where the retrieved evidence set may not contain all the necessary information. Our extensive experiments validate the effectiveness of FAVICOMP on open-domain QA tasks, showing significant improvements over existing baselines in multiple datasets. Additionally, FAVICOMP's model-agnostic nature allows it to be effortlessly incorporated into various RAG workflows without additional training, making it a versatile tool for enhancing LMs in complex tasks."}, {"title": "IMPLEMENTATION DETAILS", "content": "(1) Gold Document: We evaluate only on HotpotQA, 2WikiMQA, and MuSiQue, as these datasets contain gold documents. To identify the gold documents within the top-5 retrieved documents, we compare each gold document with the retrieved ones. If 50% or more of the content matches, we classify it as a gold document. If none of the retrieved documents are identified as gold, we utilize the entire set of retrieved documents as the context for the evaluation. This approach is necessary because the documents are chunked, and the retrieved documents may not exactly match the gold documents.\n(2) Generated Context: We use the context generation prompt in Tab. 5 to generate the context.\n(3) Zero-shot Summarization: We use the evidence compression prompt in Tab. 5 to compress the retrieved documents.\n(4) RECOMP-extractive: We utilize the same Contriever models trained by the authors for each dataset, to encode both the question and the sentences in the evidence set. For 2WikiMQA and MuSiQue, since there are no fine-tuned models available, we use the Contriever fine-tuned on HotpotQA. Following the original paper, we select one sentence as the context for NQ and TQA, whereas for the other datasets, we utilize two sentences.\n(5) RECOMP-abstractive: Similar to RECOMP-extractive, we use the same T5-large models trained by the authors for each dataset, to generate compress the retrieved evidence. For the 2WikiMQA and MuSiQue, we employ the T5-large model fine-tuned on HotpotQA.\n(6) LongLLMLingua: We use $Llama2-7B^5$ trained by the authors as the prompt compressor model. We use the default hyperparameters in the original paper, where dynamic context compression rate is set to 0.3 and maximum compression rate is set to 0.5.\n(7) CompAct: We use the same $Mistral-7B-Instruct^6$ model instruction-tuned by the authors for evidence compression. The number of documents per segment is set to 5 with 1 iteration."}, {"title": "PROMPT TEMPLATES", "content": "B.1 EVALUATION\nThe evaluation prompt template is shown in Fig. 4. For all the evaluations throughout the experiment, we switch the positions of the Question and Context if doing so results in better performance. System prompts and demonstrations used in the evaluation are presented in Tab. 4 and Tab. 6, respectively.\nB.2 FAVICOMP\nThe prompt templates for evidence compression and context generation of FAVICOMP are presented in Tab. 5"}]}