{"title": "Evaluating and Improving Graph-based Explanation Methods for Multi-Agent Coordination", "authors": ["Siva Kailas", "Shalin Jain", "Harish Ravichandar"], "abstract": "Graph Neural Networks (GNNs), developed by the graph learning community, have been adopted and shown to be highly effective in multi-robot and multi-agent learning. Inspired by this successful cross-pollination, we investigate and characterize the suitability of existing GNN explanation methods for explaining multi-agent coordination. We find that these methods have the potential to identify the most-influential communication channels that impact the team's behavior. Informed by our initial analyses, we propose an attention entropy regularization term that renders GAT-based policies more amenable to existing graph-based explainers. Intuitively, minimizing attention entropy incentivizes agents to limit their attention to the most influential or impactful agents, thereby easing the challenge faced by the explainer. We theoretically ground this intuition by showing that minimizing attention entropy increases the disparity between the explainer-generated subgraph and its complement. Evaluations across three tasks and three team sizes i) provides insights into the effectiveness of existing explainers, and ii) demonstrates that our proposed regularization consistently improves explanation quality without sacrificing task performance.", "sections": [{"title": "1. Introduction", "content": "Graph neural networks (GNNs) were originally developed to analyze complex relational data (Wu et al., 2020). However, they were quickly adopted by various other communities due to their ability to capture structural information and reason over non-euclidean spaces while remaining invariant to certain distractors. The fields of multi-agent and multi-robot learning were among the beneficiaries of these powerful techniques, enabling scalable policies that encode team size-invariant strategies for inter-robot communication and coordination. Indeed, researchers have demonstrated that GNNs are effective in solving a variety of core challenges in multi-agent coordination (Li et al., 2021b), such as information aggregation (Nayak et al., 2023), decentralization (Ji et al., 2021), and learning to communicate (Sheng et al., 2022). This has instantiated into adoption of RL trained GNN-based policies with overall similar design choices in the multi-robot community to tackle practical applications such as cooperative navigation (Li et al., 2020; 2021a), coverage control (Gosrich et al., 2022), autonomous driving (Cai et al., 2022), and real-world multi-robot coordination (Blumenkamp et al., 2022).\nDespite the impressive strides in multi-agent learning fueled by GNNs, the inner workings of the learned policies remain opaque to most users. GNN-based coordination policies are hard to introspect and analyze, giving rise to unexplained agent behaviors and decision making strategies. In contrast, other fields that analyze and study graph-structured data (e.g., material science (Reiser et al., 2022) and chemistry (Yang et al., 2021)) enjoy improved transparency and explainability, thanks to recent developments in graph-based explanation methods (Ying et al., 2019). These aforementioned works, and the graph learning community in general, have been pursuing explanations of GNNs with respect to supervised learning, and typically do so on large graph data. Thus, analogous to how the multi-agent/multi-robot community has evaluated and transposed the use of GNNs from the graph learning community for learning coordination for different tasks (as evidenced by the aforementioned related works), we seek to do a similar evaluation with GNN-based post-hoc explainers. This evaluation, combined with the insights presented to potentially improve the explanation quality, can be used to address a much-needed explanation paradigm for multi-agent coordination (see (Brandao et al., 2022). and section 2).\nIn this work, we explore whether one could adopt existing GNN explanation methods to explain multi-agent coordination. We systematically investigate and characterize the suitability of existing GNN explanation methods for graph attention network (GAT) based policies in multi-agent coordination. If we could explain GNN-based coordination policies, users can effectively debug the learning algorithm by comparing observed coordination strategies against their expectations. Further, explanations would help non-experts gain insights into learned coordination policies.\nWe study GNN explainers for multi-agent coordination since they estimate parsimonious yet representative subgraphs as a means to explain complex decision making over graphs. When translated to multi-agent coordination, extracting such subgraphs can help identify the most influential inter-agent interactions that can effectively approximate and distill coordination strategies learned across the entire team. In fact, identifying such influential interactions was found to be a key challenge in explaining coordination strategies by a recent user study focused on multi-agent navigation (Brandao et al., 2022). Further, these explanation methods are agnostic to the learning algorithm or paradigm used to train GNNs and do not interfere with training or task performance.\nIn particular, we analyzed three prominent post-hoc graph-based explainers (Graph Mask (Schlichtkrull et al., 2021), GNN-Explainer (Ying et al., 2019) and Attention Explainer (Fey & Lenssen, 2019)) across three multi-agent tasks (blind navigation, constrained navigation, and search and rescue) and measured the quality of their explanations using established explanation metrics (e.g., fidelity (Amara et al., 2022) and faithfulness (Agarwal et al., 2023)).\nOur findings suggest that graph-based explainers have the potential to explain learned inter-agent influences in GNN-based coordination policies. However, our analysis also revealed that there is room for improvement in the quality of explanations generated by these approaches.\nIn addition to our systematic analyses, we propose a simple regularization technique for training graph-based coordination policies in an effort to make learned policies more amenable to existing explanation methods. Specifically, we introduce an attention entropy minimization objective that can be used as a regularizer when training graph attention networks (GATs). Intuitively, minimizing attention entropy will \"narrow\" each agent's attention to the most influential or impactful neighbors. In turn, this more focused attention simplifies the core problem faced by graph-based explainers: identifying the most-influential inter-agent interactions.\nOur theoretical analysis shows that minimizing attention entropy increases the disparity between the subgraph generated based on attention values and its complement. This aligns well with intuition and helps explain the proposed regularizer's potential to improve explanation quality.\nRigorous empirical evaluations across three tasks and differing team sizes suggest that our regularization approach is remarkably effective on Attention Explainer, transforming a simple explanation method into the best-performing method in terms of explanation quality across all tasks. We find that pairing minimizing attention entropy with the other two explainers tends to improve at least one aspect of their explanation quality. Further, we find our regularization's improvements to explanation quality come with negligible impact on task performance. Our theoretical and empirical analyses provide a strong foundation and take the first steps towards interpretable and explainable graph-based policies for multi-agent coordination.\nIn summary, our contributions include: i) insights from a systematic evaluation of existing post-hoc graph-based explainers when used to explain GNN-based multi-agent coordination, and ii) a theoretically-grounded attention entropy regularization scheme that is shown to improve the explainability of learned GNN-based policies."}, {"title": "2. Related Works", "content": "Explainable multi-agent coordination: Since explainable multi-agent coordination was proposed as a new research direction recently (Kraus et al., 2020), only a few works have explored this problem. One line of work investigates explanations via policy abstraction using a multi-agent MDP (Boggess et al., 2022; 2023). Similarly, policy summarization and the use of landmarks to condition and convey the high-level strategy is also being explored (Pandya et al., 2024). These works rely on text and visual modalities, and are focused more on high-level explanations. As a result, they do not focus on explaining or distilling key interactions among agents. In fact, explanations that capture critical and affected agents (and agent-agent influences) were found to be desirable in a user-study by (Brandao et al., 2022) for multi-agent/multi-robot navigation-based tasks. While a different line of work has proposed generating easily verifiable multi-agent path finding plans as explainable (Almagor & Lahijanian, 2020; Kottinger et al., 2022; Kottinger et al.), they inherently do not capture key agents or interactions as desired and are limited to specific problem representations of multi-agent navigation. These gaps in the literature strongly motivate the idea of identifying subgraphs of agents to outline the most relevant agents and their interactions among one another. Toward this, we investigate and improve the utility of graph-based explanations of GNN-based multi-agent policies.\nGraph-based explanation methods: The prevalent approach to explaining GNNs is to identify the subgraph most relevant to its prediction (Schlichtkrull et al., 2021; Ying et al., 2019; Luo et al., 2020; Yuan et al., 2022). In addition to these methods, other approaches include generating probabilistic graphical models to explain GNN predictions (Vu & Thai, 2020), identifying graph patterns and motifs (Yuan et al., 2020), and generating counterfactual subgraphs representing the minimal perturbation such that the GNN prediction changes (Lucic et al., 2022). These methods have demonstrated success in explaining the predictions of GNNs on synthetic graph datasets such as Tree-Cycles and BA-shapes (Ying et al., 2019), and large real-world graph datasets such as MUTAG (Debnath et al., 1991). However, their ability to explain graph-based multi-agent policies remains unexplored. Motivated by the potential of subgraph explanations to identify the most influential channels of communication within a team, we focus on subgraph identification methods. To explore differing subgraph explanations, we use GraphMask to obtain binary mask subgraphs (Schlichtkrull et al., 2021), GNN-Explainer to obtain continuous mask subgraphs (Ying et al., 2019), and Attention-Explainer to obtain attention-based subgraphs for Graph Attention Networks (Fey & Lenssen, 2019)."}, {"title": "3. Preliminaries", "content": "Multi-Agent Coordination: We consider a class of multi-agent coordination problems that can be modeled as Decentralized Partially Observable Markov Decision Processes (POMDP). They are described by the tuple (D, S, A, T,R, O), where D is the set of N agents, S is the set global states, A is the set of actions each agent can take, T is the state transition model, R is the global reward function, and O is the observation model. We aim to learn decentralized action policies \\(\\pi(a_i|O_N(i))\\) that maximize the expected return \\(E[\\Sigma_{t=0}^{T}r_t]\\), where each agent's policy is conditioned its own observations and the messages received from its neighbors through a graph network. We include the ego agent in its own set of neighbors.\nGraph Neural Networks: We consider policies that contain a convolutional Graph Neural Network (GNN). GNNs consist of L layers of graph convolutions, followed by a nonlinear activation (Kipf & Welling, 2016). GNNs facilitate learned communication between agents over the communication graph G(V, E), where \\(V = {v_1, ...v_N}\\) is the set of N nodes representing agents and \\((v_i,v_j) \\in E\\) indicates a communication link between agents i and j.\nA single Graph Convolution Layer is given by \\(h_i^1 = \\sum_{j\\in N(i)} \\sqrt{deg(i)deg(j)}\\phi_{\\theta}h_j\\); where \\(h_j\\) is the node feature of \\(v_j\\), \\(\\phi_{\\theta}\\) is a learned transformation of the node feature parameterized by \\(\\theta\\), and \\(N(i) = {j | (v_i, v_j) \\in E}\\). Early works in multi-robot coordination using GNNs adopted this base architecture (e.g., (Li et al., 2020)). However, this architecture implicitly assumes that all agents exhibit equivalent influence through the message passing/communication channels. This is generally not the case, and more recent works have pivoted to adopting graph attention layers into their architecture in lieu of standard GNN layers due to the improved expressivity and performance achieved in multi-agent/multi-robot tasks (e.g., (Li et al., 2021a; Cai et al., 2022; He et al., 2023)). In this work, every policy contains a single Graph Attention Layer (Brody et al., 2021), which extends the graph convolution layer with a learned attention function that weights each edge \\((v_i, v_j) \\in E\\). A single graph attention layer is given by \\(h_i^1 = \\sum_{j\\in N(i)} a_{ij} \\cdot \\phi_{\\theta}h_j\\), where \\(a_{ij}\\) is the edge weight computed by the attention function introduced by Brody et al. (2021).\nGNN-based Multi-Agent Coordination: There are fundamental design choices that have been adopted throughout the GNN-based multi-agent coordination literature (see Sec. 2). Most importantly, an agent-agent graph and a representation of each agent's local observations are provided as inputs to the GNN as the adjacency matrix and the node features. Within the GNN, single or several rounds of message passing are conducted through graph convolutional layers, allowing agents to integrate information from surrounding agents into their own node representation (such as local observations), analogous to communication channels. This node representation is then utilized for task completion, typically via a local decoder structure such as a multi-layer perceptron with shared parameters/weights. This effectively conditions the action of the ego agent on the observations of surrounding agents in addition to the ego agent. We adopt these fundamental design aspects in our model architecture, along with task-specific design choices informed by prior works (Bettini et al., 2024; 2022; Blumenkamp et al., 2022)."}, {"title": "4. Evaluating Graph Explainers", "content": "Below, we discuss the graph explainers we investigate along with the tasks and metrics we use to evaluate both explanation quality and task performance."}, {"title": "4.1. Graph-based Explainers", "content": "We investigate the following GNN explainers in terms of their ability to explain GNN-based coordination policies.\nGraph Mask: Graph Mask (Schlichtkrull et al., 2021) searches for the minimal-edge subgraph \\(G_s \\subseteq G\\) such that a measure of the divergence between the output un-"}, {"title": "4.2. Tasks", "content": "We evaluate the three graph-based explainers introduced above on three multi-agent coordination tasks implemented using the VMAS simulator (Bettini et al., 2022) across three team sizes. These tasks are all the same or harder versions of the tasks in BenchMARL (Bettini et al., 2024), a new MARL benchmark. Since most tasks in BenchMARL and VMAS are unsolved with GNNs (or in general), we adopted the model architecture and training parameters of prior works for tasks that have been solved using GNNs, and designed GNNs for other tasks based on related work.\nBlind Navigation: We adapted this task from (Bettini et al., 2022). The original task considers a team of N agents, each equipped with LIDAR and required to navigate to its assigned goal location while minimizing collisions with other agents. Both initial and goal locations are randomized. Our blind navigation task differs from the original version in that there are no LIDAR sensors equipped on the agents. This means that the agents only observe their own position, velocity, and goal location, and that each agent has no information about the other agents from local observations alone. Thus, effective communication is required to solve our blind version of this task. We consider three team sizes (N = 3, 4, 5). For all team sizes, we design a common policy with an Identity layer for the encoder, a 32-dim GATv2 layer with TanH activation, and a 2-layer decoder with 256 units each and TanH activation. We use the dense reward structure from (Bettini et al., 2022; 2024).\nBlind Passage: We adapted this task from (Blumenkamp et al., 2022; Bettini et al., 2022). The original passage task"}, {"title": "4.3. Metrics for Explanation Quality", "content": "We consider four metrics to quantify both the fidelity (Amara et al., 2022) and faithfulness (Agarwal et al., 2023) of generated explanations. These metrics and their variants have been studied and used extensively in the graph learning community to characterize the quality of subgraph-based explanations (Kosan et al.; Liu et al., 2021; Fey &\nLenssen, 2019; Pope et al., 2019; Bajaj et al., 2021; Yuan et al., 2022). In the definitions below, \\(F(\\cdot)\\) refers to the GNN-based model and \\(G_t = (X_t, A_t)\\) refers to the original graph input at timestep t, where \\(A_t\\) is the adjacency matrix at timestep t of \\(G_t\\) and \\(X_t\\) is set of all node features at timestep t in \\(G_t\\). We use \\(G_t^s\\) to refer to the explanation subgraph at timestept with \\(A_s\\) as its adjacency matrix at timestep t. Since \\(G_t\\) is a fully connected graph at every timestep in our experiments, \\(A_t = 1, \\forall t\\) where 1 is a matrix with each entry equal to 1. We define each of explanation quality metric for a single timestep, and we average across all timesteps and across multiple episodes, each with a random initial state, during the evaluation in section 6."}, {"title": "5. Improving Explanations", "content": "After analyzing the quality of explanations generated by existing explanation results (see Sec. 6), we observed that they could be improved by modifying how we train GNNs \u2013 by minimizing the entropy of the attention values.\nAttention entropy minimization can be motivated from two perspectives, one from a multi-agent coordination perspective and one from a graph learning perspective. From a multi-agent collaboration perspective, an agent who collaborates with other agents will intuitively desire to (1) filter out useless information and (2) focus on the information that is most crucial to the task at hand. This is akin to how humans use selective attention to filter unnecessary information and focus on the truly important information for decision-making (Moerel et al., 2024). Attention in GNNs serve a similar purpose when the nodes are agents and the edges are communication channels composed of agents providing information to other agents. Minimizing attention entropy in a multi-agent setting yields an attention distribution that is further from pure uniform attention, resulting in a set of agents that are given more focus than other agents.\nFrom a graph learning perspective, minimizing attention entropy in conjunction with task objective can be seen as a form of denoising for node-level learning tasks (e.g., node classification or regression). By incentivizing a set of attention values that have lower entropy, the model is likely to learn a stronger filter that starts removing extraneous or noisy information. The intuition behind minimizing attention entropy can be connected to more formal notions of information bottleneck over graphs (Yu et al.). But, optimizing such objectives tends to be computationally intensive and challenging, which will be likely exacerbated when combined with multi-agent learning. In contrast, integrating attention entropy minimization into learning GNNs is much simpler, especially within MARL frameworks like MAPPO (Yu et al., 2022a). Formally, let \\(L_{PPO}(\\theta)\\) be the standard clip PPO loss used in MAPPO (Yu et al., 2022a; Schulman et al., 2017) to update the policy weights \\(\\theta\\) and let \\(a_t(\\theta)\\) be the attention values generated from the model parameters \\(\\theta\\). We can define the new regularized loss as\n\\[L_{PPO}^{ATTN} = E_t[L_{PPO}(\\theta) + \\lambda H(a_t(\\theta))]  \\quad(1)\\]\nwhere \\(H(p) = \\sum_{i} p_i \\log(p_i)\\) denotes the entropy of p."}, {"title": "5.1. Theoretical Analysis", "content": "Let G be the fully-connected input graph of N nodes or agents (with A = 1 as its adjacency matrix) to a GNN-based model that learns a set of attention values \\(a_{ij}\\) for each pair of vertices i, j in G. Moreover, let \\(G_s(a)\\) be the subgraph generated by Attention Explainer using an attention-based adjacency matrix \\(A_s(a)\\) whose i,jth element is given by \\(a_{ij}\\). The complement of \\(G_s(a)\\) is \\(G \\setminus G_s(a)\\) with the corresponding adjacency matrix \\(1 - A_s(a)\\). Below, we show that minimizing the attention entropy corresponds to maximizing the dissimilarity of the subgraph \\(G_s\\) and its complement \\(G \\setminus G_s\\) under a global measure characterized by an entry-wise matrix norm.\nDistances between two graphs \\(G_1\\) and \\(G_2\\) are best described in terms of the adjacency matrix \\(A_{G_1-G_2} = A_{G_1} - A_{G_2}\\), and mismatch norms are matrix norms applied to \\(A_{G_1-G_2}\\) (Gervens & Grohe, 2022). It turns out that global norms such as edit distance and its direct generalizations correspond to entrywise matrix norms, and that the entry-wise matrix 1-norm \\(||A||_1 = (\\sum_{i,j} |A_{i,j}||)\\) is a suitable mismatch norm (Gervens & Grohe, 2022). As such, the distance between \\(G_s(a)\\) and \\(G \\setminus G_s(a)\\) (denoted as \\(D(a)\\)) can be represented as the entry-wise 1-norm of\n\\[A_s(a) - (1 - A_s(a)), \\text{yielding}\\]\n\\[D(a) = ||A_s(a)_{i,j} - (1 - A_s(a)_{i,j}||_1\\]\n\\[= \\sum_{i,j} |A_s(a)_{i,j} - (1 - A_s(a)_{i,j})| \\]\n\\[= \\sum_{i,j} |a_{i,j} - (1 - a_{i,j})| = \\sum_{i,j} |2a_{i,j} - 1| \\]\nNote that D(a) is convex as it is a sum of convex functions. As such, we can compute its lower as shown below using Jensen's inequality (McShane, 1937) and the fact that \\(\\sum_j a_{i,j} = 1, \\forall i\\) due to softmax normalization.\n\\[D(a) = \\sum_{i,j} |2a_{i,j} - 1| > 2 |2 E[a_{i,j}] - 1| = N^2 |2 \\frac{1}{N} -1| = |2N - N^2|\\]\nNow, suppose \\(a_{i,j} \\sim U\\) \\(\\forall i, j\\) (that is, a uniform attention distribution is learned for all nodes in G). We can determine\n\\[D(a = [\\frac{1}{N} ... \\frac{1}{N}]) = \\sum_{i,j} |2\\cdot \\frac{1}{N} - 1| = N^2 |\\frac{2}{N} - 1| = |2N - N^2|\\]\nFrom the above, we can see that a uniform attention distribution realizes the lower bound of D(a), implying that a uniform attention distribution yields the lowest distance between the induced subgraph explanation \\(G_s(a)\\) and the complement of the subgraph \\(G \\setminus G_s(a)\\) (i.e., a global minima under simplex constraint on \\(\\sum_j a_{i,j} = 1:N\\)). We can additionally analyze what happens to D(a) at the extreme points of the simplex constraint on \\(\\sum_j a_{i,j} = 1:N\\) (w.l.o.g., let \\(a = a_{i,j=1:N} = [1, 0, . . . 0], \\forall i\\)) as follows\n\\[D(a = [1,0,...,0], \\forall i) = \\sum_{i,j} 1 = N^2 > 2N - N^2, \\forall N \\geq 2\\]\nFrom this, we can see that the minima is not persistent throughout the simplex on \\(\\sum_j a_{i,j} = 1:N\\). Note that through a nearly-identical analysis of negative attention entropy -H(a), the uniform distribution achieves a global minima under simplex constraint on \\(\\sum_j a_{i,j} = 1:N\\) for -H(\\(a_{i,j}\\)) (i.e., the uniform distribution of attention results in the largest entropy). Moreover, as -H is strictly convex, this minima is unique to the uniform distribution.\nGiven that D(a) and -H(a) are both convex functions that share a global minima and that minima is not persistent over the simplex constraint on \\(\\sum_j a_{i,j} = 1:N\\), we can conclude that minimizing attention entropy widens the separation between \\(G_s(a)\\) and \\(G \\setminus G_s(a)\\).\nWe hypothesize that increasing the distance D(a) is a desirable trait to have within the GNN-based policy structure for explainability. Intuitively, increasing D(a) increases the correlation of the original model output F(G) and the model output using the subgraph induced by the attention values \\(F(G_s(a))\\). This can be attributed to attention entropy minimization pruning noisy and detractive edges when jointly guided by the task reward. This improved alignment should manifest as an improvement in negative fidelity, as supported by empirical results.\nConversely, increasing D(a) decreases the correlation of the original model output F(G) and the model output using the complement of the subgraph induced by the attention values \\(F(G \\setminus G_s(a))\\). Since the distance D(a) between \\(G_s(a)\\) and \\(G \\setminus G_s(a)\\) is maximized under the regularization, coupled with the fact that \\(G \\setminus G_s(a)\\) will likely have the noisy and detractive edges as mentioned earlier, it is likely that F(G) and \\(F(G \\setminus G_s(a))\\) will diverge, improving positive fidelity (also supported by our empirical experiments). In effect, the derived relationship between D(a) and attention entropy minimization (i.e. minimizing attention entropy \u2192 maximizing D(a)) suggests that our regularization likely incentivizes the attention-induced subgraph to be both necessary and sufficient via the above mechanisms.\nTo summarize, since negative fidelity is proportional to how much the subgraph explanation approximates the original graph's predictions, we will likely see a reduction in negative fidelity when D(a) increases. Similarly, since positively fidelity is proportional to how much useful information is taken away when the subgraph explanation is deleted from the original graph, we will likely see an improvement in positive fidelity when D(a) increases.\nFinally, upon inspection of the objective D(a), we find that the gradient within an \\(\\epsilon\\)-neighborhood of the uniform distribution (i.e., the minima) has a gradient of 0. As such, unless the gradient produced by the learning objective guides the parameters to leave this neighborhood, the distance between \\(G_s\\) and \\(G \\setminus G_s\\) will not increase. Under the assumption that this property is desirable (which we confirm in our empirical analysis), a surrogate objective such as entropy H will provide a better gradient signal due to its strict concavity."}, {"title": "6. Empirical Results and Discussion", "content": "Our empirical evaluation has a twofold purpose. First, to characterize and compare the performance of state-of-the-art graph-based explainers when deployed in cooperative MARL settings as opposed to large graph data and supervised learning settings as done in prior work. Second, to investigate how minimizing attention entropy within GAT-based policies affects both explanation quality and task performance. For all evaluations, we run 50 rollouts in each environment for each independent variable of interest. All relative comparisons made below were validated via Mann-Whitney U-tests with Bonferroni correction."}, {"title": "6.1. Results on Explanation Fidelity", "content": "Figure 2 shows the delta fidelity results for each of the three team sizes and each of the three tasks. Note that a higher delta fidelity is desirable, as this indicates that the explanation subgraph is both necessary and sufficient.\nComparing explainers: We first analyze each explanation method without attention entropy with respect to delta fidelity (blue violin plots in Figure 2). We observe that Graph-Mask tends to have the least delta fidelity, followed by AttentionExplainer, with GNNExplainer yielding the highest delta fidelity. The low delta fidelity of GraphMask is likely due to its sole reliance on hard binary masks, which constrains the space of possible subgraph explanations. GNNExplainer and AttentionExplainer likely outperform than GraphMask since they produce more expressive subgraphs and allow for varying degrees of inter-agent influences as they use soft edge masks (c.f. binary hard-masks in GraphMask). Unlike AttentionExplainer, GNNExplainer is compatible with any GNN (i.e. GCN, GAT, etc) and does not rely on the model being self-interpretable. Without attention entropy regularization, the attention values are more noisy and likely cause AttentionExplainer to perform worse than GNNExplainer. These observations suggest that GNNExplainer provides the best explanations with respect to delta fidelity when employed out-of-the-box.\nImpact of regularization: Now we examine the impact of attention entropy minimization on each explainer's fidelity measures and also compare them with one another. We observe that the regularization has had no discernible impact on GraphMask as it still tends to have the worst delta fidelity measures. This is because GraphMask gains an improvement in negative fidelity, but also incurs a deterioration in positive fidelity (see appendix). This means that the explanations generated by GraphMask on a model trained with attention entropy minimization contain a larger subset of the salient edges but do not capture all the salient edges due to the restriction of hard binary masks. GNNExplainer and AttentionExplainer continue to perform better than GraphMask after the regularization.\nHowever, unlike without attention entropy minimization, AttentionExplainer now outperforms GNNExplainer. This is likely because inclusion of attention entropy boosts the correlation between the attention values (which can be interpreted as a subgraph) and the GNN model behavior, in line with the insights from the theoretical analysis. Further, entropy minimization likely obtains mixed results for GNNExplainer with respect to fidelity due to the difficulty of the optimization problem (Ying et al., 2019). The original objective of GNNExplainer is intractable to solve, requiring assumptions that tend to work well for the graph datasets for which it was designed. Thus, the inclusion of attention entropy minimization may not inherently improve the optimization landscape that GNNExplainer attempts to solve despite the fact that the subgraphs induced by the attention values themselves better represent the underlying model."}, {"title": "6.2. Results on Explanation Faithfulness", "content": "Figure 3 shows the unfaithfulness results for each of the three team sizes and each of the three tasks. Note that a lower unfaithfulness is generally desirable, as this indicates that the explanation subgraph is more faithful to the underlying prediction distribution.\nComparing explainers: We first analyze each explanation method without attention entropy with respect to unfaithfulness (blue violin plots in Figure 3). We observe that, out of the three explanation methods, GraphMask tends to have the worst unfaithfulness measures. Of the 9 experimental configurations, 5 configurations show AttentionExplainer having marginally better unfaithfulness measures compared to GNNExplainer, and the remaining 4 configurations show the opposite. Once again, the high unfaithfulness measures from GraphMask are likely due to its use of hard binary masks. Especially when the attention values are diffuse, GNNExplainer and AttentionExplainer are more likely to be able to capture varying degrees of inter-agent influences via soft edges. On the other hand, GraphMask either preserves or drops an edge, making it difficult to capture more diffuse inter-agent influences, which are more likely to occur when the model is trained without regularization.\nImpact of regularization: Now we examine the impact of attention entropy minimization on each explainer's faithfulness and also compare them with one another. The regularization consistently reduces GraphMask's unfaithfulness. This is likely due to the regularization making the attention distribution sparser, resulting in agent-agent influences that are closer to a subgraph composed of only a binary adjacency matrix. As such, GraphMask can capture more of the salient edges even when employing a hard mask. However, this boost from regularization seems to be insufficient to overcome all the limitations of hard masks since GraphMask continues to underperform the other two explainers with respect to faithfulness. In contrast, the regularization significantly improves the faithfulness of AttentionExplainer across all team sizes and tasks, enabling it to now consistently outperform GNNExplainer. Once again, this can likely be attributed to the fact that the minimizing attention entropy boosts the correlation between the subgraph induced by the attention values and the inter-agent influences considered by the underlying model. Similar to what we observed for fidelity, minimizing attention entropy seems to have a mixed impact on GNNExplainer's faithfulness."}, {"title": "6.3. Impact on Task Performance", "content": "We also measured the task performance comparison between the presence and absence of attention entropy minimization when training the policy (see Appendix for metrics). Generally, it is common to expect some performance loss in pursuit of explainability, but a model that is more explainable but does not perform the task well is not useful. Moreover, a potential concern of augmenting the RL objective with attention entropy is the reduction in policy performance. Reassuringly, Table 1 shows that, in general, little to no task performance is lost when including attention entropy minimization into the learning objective."}, {"title": "6.4. Additional Experiments", "content": "We conduct two additional experimental studies in conjunction with the above evaluation. The first study looks at the explanation quality over the course of model training, results can be found in A.7. The second study assesses the explanation quality in zero-shot deployment of trained policies on larger team sizes than the training team size, results can be found in A.8. The trends and findings from these studies are generally similar to the trends and findings of the aforementioned results. In particular, we continue to observe that GraphMask improves in faithfulness and AttentionExplainer improve in overall explanation quality while GNNExplainer garners mixed improvements when the model is trained using attention entropy minimization."}, {"title": "7. Conclusion", "content": "Our work lays the foundation for the adoption of GNN explainers to explain graph-based multi-agent policies. We conducted the first empirical evaluation of GNN explanation methods in explaining policies trained on established multi-agent tasks across varying team sizes. We find that existing GNN explanation methods have the potential to identify the most influential communication channels impacting the team's decisions. Further, we proposed attention entropy minimization as an effective regularization to improve explanation quality for graph attention-based policies, with"}]}