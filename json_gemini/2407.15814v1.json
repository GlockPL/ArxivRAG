{"title": "Perceptions of Linguistic Uncertainty by Language Models and Humans", "authors": ["Catarina G. Belem", "Markelle Kelly", "Mark Steyvers", "Sameer Singh", "Padhraic Smyth"], "abstract": "Uncertainty expressions such as \"probably\" or \"highly unlikely\" are pervasive in human language. While prior work has established that there is population-level agreement in terms of how humans interpret these expressions, there has been little inquiry into the abilities of language models to interpret such expressions. In this paper, we investigate how language models map linguistic expressions of uncertainty to numerical responses. Our approach assesses whether language models can employ theory of mind in this setting: understanding the uncertainty of another agent about a particular statement, independently of the model's own certainty about that statement. We evaluate both humans and 10 popular language models on a task created to assess these abilities. Unexpectedly, we find that 8 out of 10 models are able to map uncertainty expressions to probabilistic responses in a human-like manner. However, we observe systematically different behavior depending on whether a statement is actually true or false. This sensitivity indicates that language models are substantially more susceptible to bias based on their prior knowledge (as compared to humans). These findings raise important questions and have broad implications for human-AI alignment and AI-AI communication.", "sections": [{"title": "1 Introduction", "content": "Uncertainty is ubiquitous in human communication - in relaying predictions (\"it is likely to rain tomorrow\u201d), conveying imperfect knowledge (\u201cI think I have a copy in my desk\"), and describing unknown information (\u201cthe artifact could be more than 500 years old\"). Expressing uncertainty is critical in fields such as medicine, law, and politics, where statements including uncertainty expressions (e.g., \u201clikely,\u201d \u201cdoubtful\") are frequently used to support medical, judicial, and political"}, {"title": "2 Related Work", "content": "Human Perceptions of Uncertainty Expressions. In fields like medicine, finance, law, and politics, where it is impossible to make predictions with complete certainty, decisions are often informed by subjective probabilities (Karelitz and Budescu, 2004; Dhami and Wallsten, 2005; Fore, 2019). Subjective probabilities can be communicated quantitatively, through numerical probabilities (e.g., odds, percentages, intervals), or qualitatively, through the use of uncertainty expressions or epistemological markers (e.g., \u201cI believe\u201d, \u201cAccording to\") (Dhami and Mandel, 2022).\nAlthough being less precise than numerical probabilities (Wallsten et al., 1986b; Brun and Teigen, 1988; Budescu et al., 2014), humans generally prefer to use linguistic expressions, rather than numbers, to communicate uncertainty (Erev and Cohen, 1990; Wallsten et al., 1993). Interested in the efficacy of how humans communicate uncertainty linguistically, researchers have examined how individuals map uncertainty expressions into numerical values across different fields and expertise levels (Windschitl and Wells (1996); Karelitz and Budescu (2004); Wallsten et al. (2008, 1986a); Fore (2019); inter alia). Although there can be considerable variation in responses at the individual level, these studies have revealed that there are consistent patterns relating uncertainty expressions and numerical probabilities that can be observed systematically at the population level (Budescu and Wallsten, 1985; Clarke et al., 1992; Wallsten et al., 2008; Willems et al., 2019; Fagen-Ulmschneider, 2019).\nUncertainty Quantification in LLMs. The need for more reliable LLMs has prompted researchers to investigate new methods for communicating the internal uncertainty of LLMs. Proposed methods differ in the information used to estimate the model's uncertainty, ranging from methods that use token-level information (Jiang et al., 2021; Kuhn et al., 2023; Duan et al., 2024), to dissimilarities across multiple samples (Si et al., 2022; Chen and Mueller, 2023; Xiong et al., 2024; Hou et al., 2024; Lin et al., 2024; Aichberger et al., 2024), to training external classifiers using the inputs and/or LLMs' representations (Jiang et al., 2021; Mielke et al., 2022; Shrivastava et al., 2023), or even"}, {"title": "3 Baseline Human Study", "content": "As a baseline for how people map uncertainty expressions to numerical probabilities, we first conducted an experiment in which 94 humans were shown uncertainty expressions and asked to provide corresponding numerical responses. We focused on a set of 14 uncertainty expressions (e.g., \"almost certain\u201d, \u201cunlikely\u201d\u2014the full list is provided in Appendix B and is also shown on the y-axis in Figure 4), drawn from Wallsten et al. (2008) and Wallsten et al. (1986a). In this initial experiment, our goal is to assess how people perceive these uncertainty expressions \u201cin the wild,\u201d putting them in the context of plausible real-world statements. In addition, we also aim to use statements that minimize the potential for people to conflate their own beliefs about these statements with their assessment of the confidence of the person making the statement.\nTo this end, we constructed a set of statements (u, s, e) which include uncertainty expressions u \u2208 U used by speakers s \u2208 S to convey their degree of certainty about the truthfulness or falsehood of a statement or event e \u2208 E. By presenting statements about a specific speaker s, we are asking participants to use \u201ctheory of mind\" to estimate how likely is that the speaker believes that the statement is true. We then query participants about the speaker's strength of belief, clearly distinguishing this notion from the participant's own beliefs. For instance, given the statement \u201cSonia"}, {"title": "4 Methodology", "content": "Our full set of experiments includes experiments with both humans and LLMs as participants and with both non-verifiable (NV) and verifiable (V) statements. The baseline experiment, as described in Section 3, consists of human participants and non-verifiable statements (denoted human + NV). In this section, we describe extensions to include verifiable statements and LLM participants, resulting in three additional sets of experiments: human + V, LLM + V, LLM + NV. To help us draw"}, {"title": "4.1 Verifiable Statements", "content": "In addition to the non-verifiable statements described in Section 3, our dataset also includes verifiable statements, for the purpose of assessing the effects of prior knowledge on quantifying linguistic uncertainty. To this end, we create 60 verifiable statements based on a multiple-choice question- answering dataset from The Question Company. Starting with 30 of the dataset's \u201ceasy\u201d questions and corresponding multiple-choice options, we write true statements that use the correct answer and"}, {"title": "4.2 Numerical Responses from LLMs", "content": "To obtain uncertainty estimates from LLMs, we create prompts similar to the queries provided to humans (see list of prompts in Appendix B.4). Our goal is to estimate an empirical distribution, per uncertainty expression u, over the LLM's generated responses, in a manner similar to how empirical distributions for humans are generated (e.g., see Figure 4). In the results in this paper we focus on greedy decoding, where we select the numerical response that has the highest probability in the next-token probability distribution generated by the LLM conditioned on the prompt, i.e., decoding with temperature=0. Because this sampling approach requires no knowledge about the weights or next-token probabilities, it is applicable to any model, including those behind black-box APIs, such as Gemini (Anil et al., 2024) and GPT-4 (Achiam et al., 2024). We focus primarily on this greedy sampling approach since it aligns more closely with the human responses, i.e., we want to assess the ability of each LLM as if it were an individual human providing a single response rather than asking it to match a population distribution of human responses. We obtain an empirical distribution of LLM responses, for each uncertainty expression u, by generating responses in this manner over multiple statements e and speakers s. We refer the interested reader to Appendix C for additional information on the extraction methodologies used."}, {"title": "4.3 Metrics", "content": "We treat the empirical distribution obtained for the non-verifiable statements with human participants (described in Section 3) as our reference distribution for evaluation purposes, since it reflects human perceptions of uncertainty expressions in a setting that is designed to be free of prior information or biases about the corresponding statements. For every uncertainty expression u \u2208 U, we define a reference conditional probability distribution P(k|u), k = 0,5,10, . . ., 95, 100. where P(k|u) is the empirical distribution from the baseline experiment. Given a response from any agent, human or LLM, in the context of a particular uncertainty expression u we measure the quality of the response using the reference distribution P(k|u).\nThe primary quality metric that we propose is Proportional Agreement (PA). PA can be defined as follows: if an agent's response matches bin k for uncertainty expression u, then the PA value for that response is defined as P(k|u), where P is the reference (population) distribution defined above. Intuitively, for an expression u, this PA score P(k|u) represents the probability that the agent's response k agrees with that of a randomly selected individual, and is upper bounded for any expression by arg maxk P(k|u), i.e., by the mode of the P(k|u). The higher the PA value, the better the quality of the response in terms of agreement with the aggregate human population (as reflected by P(klu)). To compute a single score for a particular LLM or individual human, we average the PA score over multiple responses and over the 14 uncertainty expressions7.\nAn alternative to the PA metric would be to compare histograms of responses, e.g., based on multiple responses from agents for a particular uncertainty expression u. We provide numerical results for histogram comparisons (using the Wasserstein distance between histograms) in Appendix D, but we consider this to be of secondary interest since we are not requiring any LLM or individual human to necessarily replicate the full population variability of responses.\nAs an additional measure of alignment between the reference distribution and the agent's dis- tribution, we also compute the Mean Absolute Error (MAE), for each uncertainty expression u, defined as the absolute difference between (i) the mean of the responses across statements involving u for an agent, and (ii) the mean of the reference distribution for u, P(k|u). We then average across the 14 expressions u to get a single score per agent."}, {"title": "5 Results", "content": "This section examines the ability of several well-known LLMs to interpret uncertainty expressions8. We begin by assessing models' abilities to produce numerical responses that resemble human-like trends (e.g., higher numerical responses assigned to higher certainty expressions and vice-versa). We then study the effect of prior knowledge in the perception of uncertainty of both humans and models. We conclude with an assessment of the generalizability of our findings."}, {"title": "5.1 How well do LLMs perceive uncertainty?", "content": "As established in prior work (Wallsten et al., 1986b; Fagen-Ulmschneider, 2019; Willems et al., 2019) and in our baseline experiment (Section 3), humans show population-level agreement in mapping uncertainty expressions to numerical responses. In this section, we assess whether LLMs possess a similar ability to ascribe numerical responses to uncertainty expressions. To this end, we prompt LLMs to provide responses for the same non-verifiable (NV) statements as in the baseline experiment. Figure 5 shows the expression-wise histograms for these responses for two LLMs: GPT-40 and OLMO (7B), each of which can be directly compared to the histogram for humans in Figure 4.\nVisually, we observe that GPT-40 matches the human distributions well, with smaller variance per distribution, while OLMO (7B) is less well-matched. In Figure 13 in Appendix D, we see that most LLMs map uncertainty expressions to numerical responses in a manner consistent with human behavior, with higher values for expressions that are perceived by humans as higher-certainty (e.g., \"almost certain,\u201d \u201chighly likely\u201d) and lower values for lower-certainty expressions (e.g., \u201cvery unlikely\"). Only two of the LLMs evaluated, OLMO (7B) and Gemma (2B), fail to reproduce this \"increasing\" pattern across expressions (relative to the human pattern). One clear difference between humans and LLMs is that the conditional distributions of LLMs have lower entropy (or variance) relative to the human distributions, with the LLM distributions tending to be much more concentrated on a small number of responses compared to the variance in responses from a population of humans.\nThese observations are reflected more precisely by the PA scores in Table 1. We observe that larger and newer LLMs (in particular, GPT-4,LLama3 (70B),and Gemini) perform especially well on this task under the PA metric, being at 85% or above in terms of matching the modal scores that a human population assigns to each uncertainty expression. In fact, 7 out of the 10 LLMs evaluated are significantly better matched to population modal responses than are individual humans on average. This aligns with the high-level findings of Maloney et al. (2024), in particular, that the difference between the numerical responses of GPT-4 and humans were similar to (or smaller than) inter-human differences. In the context of our experiments, these high scores reflect that LLMs tend to be more consistent than individual humans in terms of agreement with aggregate human responses.\""}, {"title": "5.2 Does knowledge affect uncertainty perceptions of LLMs?", "content": "In this section, we assess the extent to which LLMs, and humans, are biased by their prior knowledge or beliefs in mapping uncertainty expressions to numerical responses. To investigate this question we collect responses from humans and LLMs on our verifiable (V) dataset, which includes both true and false common-knowledge statements (based on correct or incorrect answers, respectively, to multiple-choice questions). We find that average PA scores for both humans and LLMs are systematically lower for verifiable statements compared to the non-verifiable responses (Table 2). This suggests that prior knowledge about a statement makes it more difficult to quantify the beliefs of someone else about that statement. While humans show a small drop in their PA score, this reduction in PA is particularly pronounced for LLMs: all 10 LLMs demonstrated a significant reduction in PA, averaging a 4.3 point drop in score (across all models), compared to a 0.9 point drop for humans.\nTo investigate these differences in more detail, we consider the mean response values produced by the 6 models exhibiting the highest PA score. These values differ systematically depending on whether the statement is true or false: across the 6 LLMs in Figure 6, the mean response is 7.0 percentage points lower for false than true statements. This indicates that the models assign higher response values to the same uncertainty expression when they believe the associated statement it refers to is true than when they believe it to be false (providing a quantitative validation of the ChatGPT example in Figure 1 in Section 1).\nResults for a subset of specific uncertainty expressions are shown in Figure 7. We observe that the prior-knowledge bias is remarkably different depending on the uncertainty expression:"}, {"title": "5.3 How generalizable are our findings?", "content": "In the previous sections, our analyses are conducted on a manually curated set of 120 statements, com- prised of 60 NV statements and 60 V statements. To further validate our findings concerning LLMs' prior knowledge biases, we re-assess the impact of knowledge in LLMs' perceptual capabilities by obtaining their responses for 400 additional verifiable statements. Similarly to the original study, Figure 15 shows that, on average, all models except Gemma (2B) exhibit significant perceptual differences between true and false statements\u2014 between 5.87 (OLMO (7B)) and 17.26 (LLama3 (70B)) percentage points. While the magnitude of the difference is different across the two datasets (potentially due to semantic differences between the two QA datasets used to curate the verifiable statements), the directionality of the results with this larger dataset nonetheless corroborates our knowledge bias finding by showing that these perceptual differences persist in a different context.\nWe refer the reader to Appendix E for a more detailed description of the experimental setup and additional results."}, {"title": "5.4 How does decoding impact our findings?", "content": "The previous analyses employ greedy decoding (i.e., temperature=0) when obtaining numerical responses from the LLMs. In this section, we investigate the impact of the decoding technique in the model's abilities to perceive linguistic uncertainty, by considering richer probability information (i.e., temperature=1) when obtaining the response10.\nTable 3 summarizes the change in agreement between LLM and human responses between the verifiable and non-verifiable settings (in terms of change in PA and MAE) when using probabilistic decoding. Validating the results reported in Section 5.2 with greedy decoding, we observe a clear difference in the PA score between non-verifiable and verifiable statements when using probabilistic decoding. Further, comparing responses across true and false statements, we observe large mean response differences of 11.4, 11.7, and 27.9 percentage points for GPT-40, GPT-4, and ChatGPT, respectively (see Figure 8 and Figure 17 in the Appendix for a breakdown across expressions). Although GPT-40 mean responses are considerably lower than in the greedy decoding setting (with a 20 percentage points drop), the gap between true and false statements persists. Ultimately, this analysis confirms the robustness of our previous findings to the decoding strategy."}, {"title": "6 Discussion", "content": "Theory of mind. A growing body of work aims to assess the theory of mind capabilities of LLMs in different contexts (e.g., (Street et al., 2024; Verma et al., 2024; Sap et al., 2022; Zhou et al., 2023)). Our task of mapping uncertainty expressions to numerical probabilities, from the perspective of some speaker, is one component of a general theory of mind ability. Our results indicate that LLMs have\nroom for improvement in this area, in particular, that they are prone to confusing their own belief about a statement with the belief of someone else.\nConnection to human behavior simulation using LLMs. Our experiments reveal that, despite agreeing with population-level perceptions of linguistic uncertainty, models do not capture the full diversity of human behavior. Given the recent interest in using LLMs to simulate human participants (Aher et al., 2023; Gui and Toubia, 2023; Dillion et al., 2023; Park et al., 2023b; Namikoshi et al., 2024), our work raises important questions about whose opinions and behaviors are being simulated (Santurkar et al., 2023; Motoki et al., 2023) and reveals a new dimension in which human and model diversity differ.\nLimitations\nBiases in Interpreting Uncertainty Expressions: Prior work has raised several concerns about the consistency of humans' interpretations of uncertainty expressions, demonstrating that they are subject to a number of biases and nuances. For example, people may conflate the speaker's confidence with the speaker's estimated uncertainty (Fleiner and Vennekens, 2024), statements worded in terms of confidence (\u201cI am almost certain\u201d) or likelihood (\u201cI believe it is almost certain\") are interpreted as primarily communicating different types of uncertainty (epistemic and aleatoric, respectively)\""}, {"title": "7 Conclusions", "content": "We introduce the task of assessing the abilities of LLMs to interpret uncertainty in language and evaluate a number of models in this context. We observe that many LLMs can competently map uncertainty expressions to numerical responses in a way that aligns with population-level human perceptions, although the probabilities they choose are much less diverse than those by humans. Additionally, we find that LLMs are more susceptible to conflating their own uncertainty about a statement with the statement speaker's uncertainty, resulting in performance that is biased by the LLM's belief about the statement.\nIn proposing this task, we do not take a stance on whether LLM behavior should mirror the diversity of human behavior\u2014which is a broader philosophical discussion\u2014but focus on charac- terizing LLMs in comparison to human patterns that arise at the population-level. By highlighting systematic inconsistencies related to the perceptions of linguistic uncertainty in the presence of knowl- edge, we shed light into overlooked model behaviors that are critical for understanding human-AI communication and downstream LLM performance."}, {"title": "Appendix A. Motivating example", "content": "In the main paper, we discuss the divergence of model behavior in the use case of news headline generation when prompted with text containing uncertainty expressions. In particular, Figure 1 shows two different levels of conviction in the generated text: the first headline contains confident- sounding words like \u201cconclude\" and \"comprehensive\u201d whereas the second generated headline uses less confident language like \u201csuggests\u201d and \u201cpossible\u201d. To further understand the extent to which these differences could be explained by the models' own knowledge, we prompt the model a second time to assess its agreement or disagreement with the topic being discussed. The answers are illustrated in Figure 9."}, {"title": "Appendix B. Experiment Details", "content": "This section describes in greater detail various aspects of the experiments conducted in this paper, including the list of uncertainty expressions, the name selection strategy, the list of prompts, a list of statements, as well as additional details on the human experiments."}, {"title": "B.1 Uncertainty Expressions", "content": "The uncertainty expressions are a subset of the expressions proposed in Wallsten et al.; Wallsten et al.; Willems et al.; Fore. The final list of uncertainty expressions used in this paper is listed below:\n\u2022 almost certain, highly likely, very likely, likely, probable, somewhat likely, somewhat unlikely, uncertain, possible, unlikely, not likely, doubtful, very unlikely, highly unlikely"}, {"title": "B.2 Name Selection", "content": "All names used in our experiments were collected from a random name generator\u00b9\u00b9, which we ran iteratively until we obtained 32 unique names, half of each biological gender (as determined by the random generator)."}, {"title": "B.3 Human Experiments", "content": "Human responses were collected using Prolific (https://www.prolific.com/). We recruited 100 participants for the non-verifiable experiment and 100 different participants for the second verifiable experiment. One of the 100 responses was not received due to a technical issue in both the first and second experiment, leaving a total of 99 responses for each. We recruited participants whose first language was English that were located in the United States. Participants were paid $2 for completing the study and the average completion time was 8 minutes and 48 sections; the average payment rate was $13.64/hour. The University of California, Irvine Institutional Review Board (IRB) approved the experimental protocol. Prior to the experiment, participants were given detailed instructions outlining the experimental procedure as well as how to understand and interact with the user interface. Participants were asked to sign an integrity pledge after reading all of the instructions, stating that they would complete the experiment to the best of their abilities. After submitting their integrity pledge, participants were granted access to the experiment.\nWe filtered out low-quality responses with the following procedure. For each participant, we computed the Spearman correlation between the participant's responses and the overall ranking of uncertainty statements in the non-verifiable experiment. We removed participants with p < 0.2, a threshold chosen empirically to filter out only no-signal, spam-like responses. This filter removed 5 participants in the first experiment and 10 in the second experiment. In total, we remain with 94 participants in the non-verifiable experiment and 89 in the verifiable experiment."}, {"title": "B.4 Prompts", "content": "In our main paper, we conduct experiments using 2 demonstrations, since this better replicates the human setup.\nDuring the course of our experiments, we carried experiments with varying assumptions: non- verifiable setup assessed models (and humans) perceptions in the absence of strong prior knowledge about the statements, whereas the verifiable setup focused on the evaluation of the same perceptions when knowledge was present. We used two different sets of exemplars in our experiments to reflect these differences. The set of exemplars used in the non-verifiable setting are defined in terms of the following:\n\u2022 speaker: \u201cKathleen\u201d, uncertainty: \u201cimpossible\u201d, statement: \u201cthe cafe made a profit in the last 6 months\".\n\u2022 speaker: \u201cCedric\u201d, uncertainty: \u201ccertain\u201d, statement: \u201cthe new treatment will improve the patient's condition\u201d.\nAs for the verifiable exemplars, we experimented with various orderings of the statements and found minimal differences in the obtained distribution of numerical responses. As a result, we report the results using a false statement as the first example and a true statement as the second example in the prompt. These were associated with the following speaker names and uncertainty expressions:"}, {"title": "B.5 Non-Verifiable Statements", "content": "In this section, we list a random selection of 20 statements (5 per each statement type). These statements are incorporated in the prompts listed in the main paper and the placeholders [[they]] and [[their]] are replaced by pronouns matching the gender of the statement speaker's name.\nForecasting of future events. Verbal probabilities are often used to communicate uncertainty about future events.\n1. [[they]] will buy a new watch this Thanksgiving weekend.\n2. [[they]] will be offered a promotion this fall.\n3. the company will have another round of lay-offs by mid July.\n4. there will be vegetarian options at the barbecue.\n5. [[they]] will visit New York over winter break.\nImperfect knowledge. Verbal probabilities can also be used to communicate uncertainty imprecise information about events or outcomes.\n1. the restaurant near [[their]] apartment accepts reservations.\n2. the new museum is offering complimentary admission.\n3. there is a yoga studio within 2 miles of [[their]] workplace.\n4. there are more than eighty students in the auditorium right now.\n5. the temperature in the office is at least 72 degrees Fahrenheit."}, {"title": "Possession.", "content": "Alternatively, verbal probabilities can be used to convey uncertainty about acquain- tances, be it in terms of the objects they own or in terms of their preferences.\n1. [[their]] boss owns a blue car.\n2. [[their]] friend has a leather jacket.\n3. [[their]] cousin has a vegetable garden.\n4. [[their]] classmate owns a guitar.\n5. [[their]] boss has a stereo amplifier."}, {"title": "Preference.", "content": "1. [[their]] cousin prefers spinach over broccoli.\n2. [[their]] boss prefers coffee over tea.\n3. [[their]] friend prefers running over cycling.\n4. [[their]] neighbor prefers the beach over the mountains.\n5. [[their]] coworker prefers reading books over watching movies."}, {"title": "B.6 Verifiable Statements", "content": "In this section, we list a random selection of 9 true statements (3 per each topic) and their false counterparts. These statements are incorporated then used as part of the test examples the prompts listed in Section B.4.\nGeography. One of the topics of the experiment involves geography, as well as knowledge about landmarks and monuments. These statements were curated from a set of easy trivia questions provided by The Question Company (as described in Section 4). For each question-answer pair in the trivia dataset, we create both a true and a false statement using the correct and one incorrect answer choice, respectively.\nGiven our interest in attesting the knowledge effect in the models' and humans' perceptions of linguistic uncertainty, we purposely decided to use easy trivia questions as the basis for our facts (as opposed to more difficult ones), since this subset constitutes a good proxy for facts that LLMs and humans may have strong prior beliefs about.\n1. Great Britain directly borders 0 countries.\n2. the Colosseum, a famous landmark in Rome, was originally built as an Amphitheatre.\n3. New York is known as the Big Apple.\n4. Great Britain directly borders 2 countries.\n5. the Colosseum, a famous landmark in Rome, was originally built as an Cathedral.\n6. New York is known as the Big Orange."}, {"title": "History of Art.", "content": "One of the topics of the experiment involves history of arts. For each fact we include both a true and one false variation of that fact.\n1. the Mona Lisa is a famous painting by Leonardo da Vinci.\n2. the Scream is the best known painting by Edvard Munch.\n3. Andy Warhol became a famous artist in the 1960s for painting soup cans and soap boxes.\n4. the Mona Lisa is a famous painting by Tintoretto.\n5. the Scream is the best known painting by Jackson Pollock.\n6. Frida Kahlo became a famous artist in the 1960s for painting soup cans and soap boxes.\nScience. These include facts concerning chemistry, biology, and astronomy. For each fact we include both a true and one false variation of that fact.\n1. water's chemical formula is H2O.\n2. pH is a measure of the acidity or basicity of a substance.\n3. the nearest planet to the sun is Mercury.\n4. carbon monoxide's chemical formula is H2O.\n5. OG is a measure of the acidity or basicity of a substance.\n6. the nearest planet to the sun is Mars."}, {"title": "B.7 Language Models", "content": "Throughout our paper, we use OpenAI to obtain the results for ChatGPT, GPT-4, and GPT-40; Google's Vertex AI APIs to obtain results for Gemini, TogetherAI12 to run LLama3 (70B), Mixtral 8x7B, and Mixtral 8x22B. We run LLama3 (8B) OLMO (7B) and Gemma (2B) locally on a single GPU 8 RTX A6000 (48 GB).\nDuring the paper, we shorten the name of the studied models for simplicity. All our experiments consider the instruction-tuned or RLHF version of the mentioned models. All experiments were conducted from April through June. For reproducibility, we list below the mapping from model name to exact version of the model used:\n\u2022 ChatGPT:gpt-3.5-turbo-0125\n\u2022 GPT-4: gpt-4-turbo-2024-04-09\n\u2022 GPT-40: gpt-40-2024-05-13\n\u2022 LLama3 (8B):meta-llama/Meta-Llama-3-8B-Instruct\n\u2022 LLama3 (70B):meta-llama/Llama-3-70b-chat-hf"}, {"title": "Appendix C. Extracting LLMs Numerical Responses", "content": "In this section, we elaborate on the methodologies to extract the numerical responses. There are many different level of information that we can use to characterize numerical responses from LLMs. When estimating empirical distributions using point estimates, it suffices to use greedy decoding, since it represents the best guess that a model would give on average. However, in some cases, one may be interested in more faithful estimates of the empirical probability distribution for each uncertainty expression. To that end, one can take advantage of the information available in different platforms, which we describe next."}, {"title": "C.1 Full Next-Token Probability Information", "content": "By definition, our task elicits a numerical response from LLMs, which resembles the setup in verbalized confidence (Tian et al., 2023). The adoption of single digit tokenization (Singh and Strouse, 2024) by autoregressive models (e.g., Gemma (2B),LLama3 (70B),and OLMO (7B)) creates some challenges in the computation of numerical responses non-trivial for autoregressive models. In practice, due to the left-to-right nature of LLMs, single digit tokenization implies that the probability of a number between [0, 9] is always greater or equal to the probability of any number in [10, 100]. To circumvent this problem, we report the corrected probability during our experiments as follows:\nPmodel(Yt = ix) - \u03a3(9,j=0)Pmodel(Yt = i, Yt+1 = j|x),\nwhere x is a prompt and j \u2208 [0, 9]. Intuitively, this means that we are computing the probability of i \u2208 [0, 9] and no other number following it. The details of what a number is change with tokenizer implementation. Unlike traditional greedy decoding, we condition the selection of the arg-max prediction to the set of strings representing the numbers between [0, 100] (followed by no other number)."}, {"title": "C.2 Partial Next-Token Probability Information", "content": "In order to work, this method requires two properties to be satisfied: (1) numbers between 0 and 100 were encoded with unique tokens (i.e., there are 101 unique integers that represent each individual token), and (2) exponentiating the log probabilities returned by the black-box API must lead to a"}, {"title": "Appendix D. Additional Results", "content": "In this section, we report additional results, including visualization of the empirical distributions for models and humans in Section D.1, additional measurements of similarity between non-verifiable and verifiable distributions in Section D.2, metrics discriminated by uncertainty expression (whenever applicable) in Sections D.3 and D.4."}, {"title": "D.1 Histograms", "content": "Figure 13 depicts the empirical distributions for the non-verifiable experiments."}, {"title": "D.2 Summary Metrics", "content": "For a more complete understanding of the differences among the distributions, we report distance metrics in Table 4.\n\u2022 Proportional Agreement: proposed in Section 4.3, measures the overall agreement between an agent's and a reference (population) distribution. We use the results of the human studies in the non-verifiable setting as our reference distribution throughout the whole paper.\n\u2022 Mean Absolute Error: proposed in Section 4.3, measures the average agreement across uncertainty expressions between a agent's distribution and a reference (population) distribution. In this case, we also use the human results from the non-verifiable setting as our reference distribution throughout the paper.\n\u2022 Wasserstein Distance: computed using scipy.stats.wasserstein_distance, mea- sures the distance between two conditional distributions."}, {"title": "D.3 Proportional Agreement", "content": "Tables 5 and 6 report the proportional agreement (PA) metric discriminated by uncertainty expression in the non-verifiable and verifiable settings, respectively. The results are reported in the filtered pool of human participants."}, {"title": "D.4 Mean Response", "content": "Figure 14 illustrates the mean rated probability metric discriminated by uncertainty expressions across the non-verifiable, as well as the true and false verifiable statements."}, {"title": "Appendix E. Generalization results", "content": "Diversity of grammatical and semantic structures is an important component of current evaluation practices in LLMs (Selvam et al., 2023; Seshadri et al., 2022), since it helps ensure that obtained results are not an artifact of the evaluation methodology and/or benchmarks used. The experiments described in the main paper were carefully crafted to cover various topics and situations where uncertainty expressions could be used. To further strengthen our analysis and validate our findings, we simultaneously run collect models perceptions of uncertainty expressions using a larger dataset."}, {"title": "Appendix F. Probabilistic decoding", "content": "=1. The probability distributions are obtained\nTable 8 summarizes the different metrics in the non-verifiable setting, when using probabilistic\ndecoding with OpenAI models, i.e., temperature\nusing the methodology described in Appendix C."}]}