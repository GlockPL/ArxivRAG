{"title": "From Voice to Value: Leveraging AI to Enhance Spoken Online Reviews on the Go", "authors": ["Kavindu Ravishan", "D\u00e1niel Szab\u00f3", "Niels van Berkel", "Aku Visuri", "Chi-Lan Yang", "Koji Yatani", "Simo Hosio"], "abstract": "Online reviews help people make better decisions. Review platforms usually depend on typed input, where leaving a good review requires significant effort because users must carefully organize and articulate their thoughts. This may discourage users from leaving comprehensive and high-quality reviews, especially when they are on the go. To address this challenge, we developed Vocalizer, a mobile application that enables users to provide reviews through voice input, with enhancements from a large language model (LLM). In a longitudinal study, we analysed user interactions with the app, focusing on AI-driven features that help refine and improve reviews. Our findings show that users frequently utilized the AI agent to add more detailed information to their reviews. We also show how interactive AI features can improve users' self-efficacy and willingness to share reviews online. Finally, we discuss the opportunities and challenges of integrating AI assistance into review-writing systems.", "sections": [{"title": "1 Introduction", "content": "Online restaurant reviews are a great resource for anyone looking for a place to eat or enjoy a cup of coffee. These user-generated evaluations not only influence dining decisions, but also shape the reputation and success of restaurants [1, 28, 55]. Writing good reviews requires skills that improve the quality and impact of feedback. These abilities include being able to clearly articulate thoughts, developing well-informed points of view, or presentation skills [46].\nThe recent commoditization of advanced voice input/output features in mobile devices (such as voice assistants and dictation features) and Large Language Models (LLMs) open up new opportunities to augment many of the skills and considerations that deal with reviews. These technologies can support human expression and improve review quality through, for instance, automatically correcting text. However, generating reviews is a different thing. For instance, a recent study has shown how AI-created reviews can be perceived as less useful, trustworthy, and authentic than human-created ones when people are aware that the content was AI-generated [4]. This is a common response to AI-generated content, reflecting broader concerns about its authenticity.\nIn our work, aim to strike a balance between AI-generated content and human-written content. To this end, we contribute Vocalizer: an Al-powered mobile application to assist users in leaving spoken reviews conveniently on the go. We integrated a Large Language Model-powered AI agent into the review creation process, allowing users to instruct the model to edit their review that has gone through an initial automatic cleansing process. Rather than using the LLM to generate the whole review for users, we allow users to freely ramble the initial review, and then exploit AI features to enhance the review.\nOur key contributions are:\n\u2022 A web application that enables people to edit their restaurant reviews with an Al agent and on the go,\n\u2022 An analysis of the strategies people prefer to employ for editing their reviews with LLM\n\u2022 A discussion on the use of LLMs in the context of leaving good restaurant reviews, highlighting its benefits and the tensions that arise when allowing AI to modify human-made reviews.\nThrough a one-month field study with two Vocalizer versions (one without the interactive Al features and one with them), we found that users used the AI agent to improve the quality of their reviews by adding more information, clarifying unclear points, and removing unnecessary content. The use of AI also led to an increase in users' confidence in writing good reviews, as measured through a simple self-efficacy metric. Through our results, we also discuss the potential of AI to aid and improve the review writing process, making it more efficient for a wider range of users."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Online Reviews", "content": "Past work has examined the role of online reviews in shaping consumer behaviour. Studies have consistently shown that online reviews influence purchasing decisions, particularly in the restaurant industry [3, 18, 49]. In addition, the content and sentiment of online reviews have been shown to impact a restaurant's reputation and overall success [1, 28]. The characteristics of the review can also significantly impact how consumers perceive the review and whether or not they trust it [15]. The key characteristics of a high-quality review include the following.\n\u2022 Detailed and informative: High-quality reviews provide specific examples of the reviewer's experience with the product or service being reviewed. For example, instead of saying, \"This restaurant was great,\" a high-quality review might say, \"I had the steak at this restaurant and it was cooked perfectly. It was also very tender and juicy. The service was also excellent.\" [30, 36, 46].\n\u2022 Objective and neutral: Reviews written in a neutral and objective tone are considered quality reviews [36, 46, 50]. They avoid using emotional language or making personal attacks. For example, instead of saying, \"I loved this product,\" a high-quality review might say, \"This product is well-made and durable. It also has a number of features that I find useful.\".\n\u2022 Readable and easy to read: High-quality reviews are well-written and easy to read [20]. They use proper grammar and spelling, and they are formatted in a way that is easy to scan. For example, a high-quality review might use bullet points or numbered lists to make it easier for readers to find the information they are looking for.\n\u2022 Recency, Length, and Quantity of reviews: Several factors affect consumer trust in online reviews. Review recency is a key element, with recent reviews holding more credibility as they better represent the current state of a product or service [52]. Longer reviews tend to be trusted more as they offer in-depth insights and comprehensive information [30, 36, 52]. Additionally, a high quantity of positive reviews contributes to increased consumer trust, signalling a larger satisfied customer base.\n\u2022 Sentiment of the review: Consumers are more likely to trust reviews that have very high or low ratings because they are often seen as more helpful. This is because individuals who experience strong emotions typically put significant effort into expressing their high satisfaction or intense dissatisfaction, leading to higher-quality reviews [30, 52].\nConsumers are more likely to trust reviews that are written by other consumers who have similar demographic characteristics, such as age, gender, and location [2]. This is because consumers are more likely to feel that they can relate to the reviewer's experience and that the reviewer's opinion is relevant to them [2]. For example, a young mother is more likely to trust a review of a children's product that is written by another young mother than a review that is written by an older person who does not have children.\nStudies have also investigated consumer trust and credibility in relation to online reviews [21], and recent findings from a non-academic survey [8] suggest a curious trend: consumers may in some cases even exhibit a preference for carefully crafted AI-generated review responses over those authored by humans."}, {"title": "2.2 AI-Assisted Writing", "content": "The integration of AI, particularly Large Language Models (LLMs), into various tasks with text enables a new class of applications. Modern LLMs can now summarize complex documents, help with creative writing, or provide answers to questions [25, 56]. Many people use commercial tools that rely on AI technologies such as LLMs to work with text. For instance, Grammarly\u00b9 is a tool shown to help with many aspects of language editing, such as grammar, sentence structure, spelling, and punctuation [19, 22, 48].\nLLMs have uses across various domains. For example, in educational environments, LLM tools can boost students' creative abilities, enhance their writing proficiency, and increase engagement in the learning process. They help learners generate story concepts, improve grammatical precision, and collaborate with LLM systems that serve as coauthors in the creative writing process [12].\nThe use of LLMs extends beyond creative writing to more structured and challenging tasks such as argumentative essay writing. Studies [29] have shown that LLM can reduce cognitive strain and speed up the writing process by offering well-organized content and relevant evidence. This is especially useful for tasks that require a logical structure and the integration of supporting arguments, as LLM can quickly generate content that satisfies these requirements [29]. The different types of writing tasks-be it creative or argumentative-highlight the various impacts of LLM assistance, notably boosting productivity in structured writing while still offering benefits in creative scenarios.\nIn addition, modern smartphones have contributed to the accessibility and efficiency of LLM-assisted writing through advanced voice input technologies. Integrating voice input with AI, as seen e.g. in the Rambler system, has been shown to improve the writing process by addressing common issues in voice dictation, such as disfluency, verbosity, and incoherence [31]. Users of Rambler reported that the system facilitated the development and refinement of spoken content, supported iterative revisions, and enhanced user control during editing. This synthesis of spontaneous speech and organized writing exemplifies how LLMs can bridge the gap between natural language expression and formalized text production.\nAlthough advances have been made, the development of AI-generated text presents challenges related to factual accuracy, bias, and the ethical consequences of creating synthetic content. The growing sophistication of AI technologies in producing text that resembles human writing prompts important questions about the authenticity and dependability of such content [33]. This issue is particularly noticeable in areas like online reviews, where it is becoming harder to distinguish between content created by humans and AI. Recent research indicates that human reviewers often find it difficult to separate Al-generated restaurant reviews from those written by humans [16]. This raises ethical issues in situations where the authenticity and humanness of content are important.\nConcerns about the potential misuse of AI in generating misleading content are increasing. Research by Bartoli et al. [6] showed that it is feasible to produce hundreds of fake reviews designed for specific ratings and restaurant categories, successfully deceiving users. Their experimental study with 39 human participants showed that about 30% of the AI-generated reviews were considered useful"}, {"title": "3 Methodology", "content": "We developed the mobile application, named Vocalizer, as a web application which was then evaluated through a deployment study conducted on a university campus with several different restaurants."}, {"title": "3.1 Vocalizer: A Voice-Based AI Review Application", "content": "For the purpose of the study, the application was designed with two versions that acted as the study conditions: Voice-Only Version (VOV) and LLM-Assisted Version (LAV). The VOV uses only voice input, while the LAV incorporates voice input alongside a LLM. The overall operation of Vocalizer is outlined in Figure 2. We used HTML for front-end development and Express\u00b2 and Node.js\u00b3 for server-side functionality."}, {"title": "3.1.1 Voice-only Version.", "content": "In the VOV, participants were asked to record their restaurant reviews via voice input after visiting a restaurant. The reviews were then immediately transcribed using the OpenAI Whisper model and presented to users for review and editing (see Figure 4-C), allowing them to correct any transcription errors and refine the content as needed.\nA dynamic waveform animation was added as visual feedback during the recording process (see Figure 4-B). These visual elements can improve user motivation and task participation, potentially leading to more detailed and thorough reviews [13]. We set a maximum of 5 minutes of recording time due to technical considerations, but the subtle countdown timer was shown after 30 seconds to create a relaxed and unrestricted recording environment while still ensuring sufficient data collection. Before submitting the review, users were asked for quick feedback using a slider input (scale 1-7, from not at all willing to extremely willing) about their willingness to share the review online."}, {"title": "3.1.2 LLM-Assisted Version.", "content": "The LAV builds on the voice-only version by including various language improvement functionalities, as detailed below, through the OpenAI GPT-45 model. A fundamental design principle in creating the LAV for the Vocalizer application was highlighting user autonomy and control.\nInitial Review Improvement: LAV refines the spoken review automatically by rephrasing sentences for improved clarity, correcting grammatical errors, removing unrelated content to ensure coherence, and eliminating filler words like 'um' and 'ah' while retaining the original content and tone (see Figure 3-B). As spoken text tends to be \"messy\" by nature, we sought to avoid the user having to rely on first using the Al agent to remove the filler words before considering more substantial edits to the review. Al agent can also identify if the user goes off-topic or talks about irrelevant details as part of the review, and remove these sections. The LAV version converts raw transcripts into polished text that is shown on the screen accompanied by a voice transcription. Users can compare the original voice transcription with the refined review text."}, {"title": "AI Agent:", "content": "A conversational AI assistant (see Figure 4-C, D) provides the ability to further edit the improved review through natural language commands typed by the user on the device's keyboard. This functionality is designed to study how users leverage Al to improve and customize their reviews. As the user instructs the Al agent on how to modify the review, it employs the GPT-4 model to make the desired adjustments (see Figure 1, panels 4 and 5). The user is allowed to repeat this step and iteratively improve their review until they are satisfied with the result."}, {"title": "Review Improvement Tips:", "content": "In order to improve the quality of user-generated reviews, our application offers enhancement suggestions based on the features of high-quality reviews that had been identified in related research. We provide the findings to the LLM through prompts, as detailed in the listing Listing 3. When users request assistance with review improvement at the tap of a button, the Al agent shows suggestions specific to their review as shown in Figure 4-E. For example, the brief review 'the soup was very tasty' may receive the following advice: 'Your review is short and sweet, and it's great that you mentioned the positive aspects of the dish. However, to enhance its quality, consider providing more details. You could specify what type of soup it was, and what made its flavor so delightful. Talk about the ingredients or any unique flavor it had. You could also elaborate on the restaurant's service or atmosphere to make the review more comprehensive. Remember that more explanatory and in-depth reviews tend to be more useful for readers'.\nFurthermore, users can restart the review process and go back to the beginning if they find the agent's revisions of their review unsatisfactory. All interactions are logged within the application for later examination.\nAgain, before submitting each review, participants were required to leave fast in-app feedback, as illustrated in Figure 4-F. This time, in addition to the item about willingness to share, the users were also asked about their overall satisfaction with the review, if the provided ideas were helpful (in case they used the suggestions module) and if the AI agent was useful in improving the review. The voice-only version contained one feedback question (share willingness), whereas the LLM-Assisted Version featured up to four questions (share willingness, review satisfaction, improvement idea helpfulness, Al agent usefulness) to assess the various Al components according to the levels of feature interaction. All feedback collected was stored in an online database for later evaluation."}, {"title": "3.2 Prompt Design", "content": "The efficacy of AI models depends on the specificity and clarity of the prompts [51]. This study used a systematic approach to prompt engineering, with the aim of optimizing AI performance by carefully considering prompt structure and content. The variable generatedText refers to the review that has already undergone initial improvement by the LLM, eliminating redundant information to improve clarity. The variable refineInstructions refers to the instructions given by the participants on how to refine this review. The GPT model receives the texts represented by these two identifiers along with their brief descriptions before receiving the prompt to process them.\nAlthough Vocalizer appears to be one cohesive Al system to the user, it is made up of three separate but complementing LLM-powered features that operate independently. In the following, we describe the differences in how the ChatGPT model is utilised for these three features. Table 1 presents the inputs, system prompts, and temperature settings for each AI feature. These three specify which textual data a certain LLM step in the system works on, what system prompt it receives (an essential part in making models like ChatGPT behave in specified ways) and what temperature the model runs on (how creative the model is allowed to be at the expense of factual accuracy) respectively."}, {"title": "3.3 Study Design", "content": "We used a mixed method research design to explore the use of AI to improve reviews. We captured data directly in the application and using a set of three questionnaires that were administered using Google Forms. The overall study design is shown in Figure 5 and in the following, we detail the exact data collection methodology."}, {"title": "3.3.1 Participants.", "content": "We recruited 14 participants, 7 women and 7 men, with an average age of 28.21 years (SD = 5.32). Participants were recruited through the university's research participation system and on-campus posters. This online participation system allows researchers to advertise a study and allows potential participants to register for projects that align with their interests. Each participant was promised approximately $20 worth of university merchandise as a reward for their time."}, {"title": "3.3.2 Protocol.", "content": "We used a counterbalanced within-subjects design, as shown in Figure 5. Participants were randomly assigned to begin the study period with either the VOV or the LAV condition. The lead author sent each participant a welcome email that contained instructions on how to start, how to install the application, and what their tasks were. The first task was to complete at least five restaurant reviews from any of the campus restaurants. After completing the first five reviews and the corresponding post-visit questionnaire, they continued the study with the alternate version of the application. Participants were asked to email the lead author after finishing the reviews with either version of the application. The lead author kept a record of participants' progress to confirm that the participants proceeded according to protocol. The participants did not interact with Vocalizer outside the context of this study, and all participants completed the study period within three weeks."}, {"title": "3.3.3 Questionnaires.", "content": "The background questionnaire included items on demographic information, prior experience with restaurant reviews and review platforms, and attitude toward restaurant reviews. The questionnaire was presented prior to the installation of the application, in accordance with the guidelines provided in the welcome email.\nAfter each review, we collected rapid in-app feedback, as follows:\n\u2022 Q1: How willing would you be to share the final review online in a public restaurant review platform? (VOV & LAV)\n\u2022 Q2: How satisfied are you with the final review? (LAV)\n\u2022 Q3: Were the provided improvement ideas helpful (LAV in case the \"Get Improvement Ideas\" functionality was used)\n\u2022 Q4: How useful was the AI agent in improving your review? (LAV)"}, {"title": "4 Results", "content": ""}, {"title": "4.1 Overview of the Review Experience of Participants", "content": "Participants reported eating on campus frequently, with an average of almost four visits per week (most restaurants are closed on weekends except for one fast-food spot), showing a significant engagement with the campus food scene. Participants rated their review experience on average 3.43 (SD = 1.91) on a scale from 1-7, indicating a range of levels of expertise. Google Reviews was the most used platform (N = 10), but the participants also mentioned using TripAdvisor, restaurant websites, Facebook, Reddit, Jodel, and Yelp to collect and share experiences. Results about favourite features on review platforms showed that traditional and lower-effort methods may be desirable. Four of the 14 participants mentioned that multimedia reviews are particularly important for them, and at least five thought that short, easy review formats such as stars are appreciated."}, {"title": "4.2 Vocalizer Usage Analysis", "content": "In the Voice-Only Version, we collected 75 reviews. 56 submitted the original transcript, while 19 reviews were manually edited by users, with an average of 2.11 edits per review among the latter.\nIn the LLM-Assisted Version, we collected a total of 82 reviews. These reviews have all been improved using LLM after transcription of the voice input. Examples of how users' transcribed reviews were enhanced at the initial LLM improvement step are shown in E. Out of these, users asked the AI agent to further modify 42 reviews, while the rest were submitted without the use of the AI agent. Users requested recommendations to improve their reviews 20 times by pressing the button 'Get improvement ideas' shown in Figure 4-E. Examples of reviews and the improved advice they received are shown in Appendix D."}, {"title": "4.2.1 Q1: Willingness to share [VOV & LAV].", "content": "A paired-sample t-test was performed to compare the willingness of the participants to share reviews produced by VOV and LAV. The results indicated a significant difference between the two conditions. Participants reported a higher willingness to share LAV reviews (M = 5.93, SD = 1.04) compared to VOV reviews (M = 4.53, SD = 1.85), t(14) = -2.39, p < 0.05. This suggests that the LAV was more effective in producing content that met user expectations for public sharing."}, {"title": "4.2.2 Q2: Review satisfaction (LAV).", "content": "Participants showed a high level of satisfaction with the reviews provided by the LLM-Assisted Version (N = 82). The average satisfaction score was 6.15 out of 7, with a standard deviation of 0.96, reflecting a positive reception of the quality of the review."}, {"title": "4.2.3 Q3: Usefulness of improvement ideas by the Al agent.", "content": "All of the 20 improvement suggestions were considered helpful, as indicated by thumbs up or down."}, {"title": "4.2.4 Q4: Usefulness of the Al agent (LAV).", "content": "The mean rating for agent usefulness was 6.09 (SD = 1.24, N = 42), indicating a generally positive perception. Although a majority of users expressed high levels of satisfaction (categories \"Very\" and \"Extremely\" useful accounted for 83.33% of responses), a smaller subset provided more moderate ratings. This indicates that although the Al agent was beneficial to most users, there could be particular situations or individual traits that affected its perceived usefulness."}, {"title": "4.3 Strategies for Revising Prompts for a Better Review", "content": "We adapted Braun and Clarke's thematic analysis [7] to fit our smaller team. The lead author and a coauthor first reviewed the prompts that the participants used to modify their reviews. Then, we collaboratively created the initial codes (Modification Types) using Google Sheets as the shared platform. In the second step, the codes were reworded when necessary and merged into broader themes (Categories). We noticed a dominant focus on content modification, with users seeking to add, remove, or rephrase information. In addition, users frequently requested sentiment adjustment, shifting between positive, negative, or neutral tones. Style control, such as improving readability or conciseness, was also observed. The types of modifications and their frequencies can be seen in Table 2."}, {"title": "4.4 Impact of Reviewer Experience on Prompt Length", "content": "We observed a positive correlation between the prior experience of users in leaving reviews and the length of the prompts they provided to the AI assistant. More experienced users tended to use longer prompts compared to less experienced users (Spearman correlation = 0.94, p < 0.01). Although further research into this is warranted, we believe that more experienced users simply know better what they want and how to write good reviews."}, {"title": "4.5 User Experience Analysis", "content": "Our post-visit questionnaires, using a 7-point Likert scale (1 = strongly disagree, 7 = strongly agree) based on the UEQ-S questionnaire, provided information on user experience with the VOV and LAV interfaces on our restaurant review platform (N = 28). The LAV seemed to outperform the VOV in all hedonic and pragmatic quality dimensions, as illustrated in Figure 8. The LAV achieved an average UEQ-S score of 6.17, compared to 5.52 for the VOV. The pragmatic score was 6.27 for LAV and 5.84 for VOV, while the hedonic score was 6.07 for LAV versus 5.20 for the VOV. However, none of these differences were statistically significant, as determined by Mann-Whitney U tests with Bonferroni corrections applied (adjusted significance threshold = 0.00625)."}, {"title": "4.6 Self-Efficacy for Writing Reviews", "content": "We evaluated self-efficacy on a scale from 0 to 10 in four aspects. First, we asked participants in the post-visit questionnaires to understand how each of the two conditions (VOV and LAV) help people. Second, in the final questionnaire, we also asked participants about their self-efficacy when they were not using any of these novel tools. Finally, beyond the two cases compared in this study, the final questionnaire was used to understand what people think about the overall concept of voice-input review creation (i.e. the concept of spoken reviews via a mobile application, with or without LLM assistance, looking beyond the implementation used in our study). Thus, we measured their self-efficacy in four aspects: Unaided review creation, Vocalizer VOV, Vocalizer LAV, and Overall. An overview of the development of people's self-efficacy is depicted in Figure 9\nAs illustrated in Figure 9, participants reported an average self-efficacy score of 4.86 out of 10 without any support (labeled as \"Unaided\" in Figure 9). The self-efficacy score reported by participants after using the VOV was 8.25, whereas it was 9.00 after using the LAV. An overall score (i.e. considering the whole concept, in the final questionnaire) was 9.14. To this end, we conducted a Friedman test, a non-parametric test for repeated measures to evaluate the differences. We observe a significant difference in self-efficacy scores across conditions, x\u00b2(3) = 20.15, p < 0.001. To explore which conditions differed significantly, we performed pairwise post-hoc comparisons using the Wilcoxon signed-rank test with a Bonferroni correction to account for multiple comparisons. We observe significant differences between the LAV condition and the Unaided condition (i.e. when people estimated their ability without any tools, in the final questionnaire) (p < 0.001) as well as between the Unaided and Overall (i.e. with a tool like ours) conditions (p < 0.001). This suggests that the LAV increased users' self-efficacy in their review writing abilities."}, {"title": "4.7 Qualitative Analysis of Post-Study Questionnaire", "content": "Following the same method described in Section 4.3, we analysed the responses in the Final Questionnaire. Most participants shared positive experiences regarding the application."}, {"title": "4.7.1 How did Vocalizer help, or did not help, leave reviews.", "content": "The first part of the analysis looks at easing the process of leaving reviews.\nEase and efficiency. First, almost all participants highlighted the effortlessness of verbal reviews enabled by Vocalizer. Users consistently highlighted that the process was easy and fast compared to traditional written reviews. As P17 noted, \"The speech-to-text feature saves time and is way more low-effort compared to written reviews. You can just explain the experience out loud as you would to a friend, and then you can just fix it up with the Al features. Genius.\" This preference for voice review indicates a trend toward minimizing the cognitive and physical effort involved in the review process [35]. Users appreciated the ability to articulate their experiences verbally without the need to focus on typing or grammatical accuracy, making the review process more accessible and less time-consuming.\nTransform spontaneous thoughts. The LLM-Assisted Version was also recognized for its ability to turn ideas and short comments into well-structured, informative reviews. Nine participants mentioned that the AI applications made the review leaving process more efficient by improving the overall quality of their submissions. P11 remarked, \"It is easier to leave a review as you can say the review in an unstructured way and the AI makes it structured.\". P5 repeated this sentiment, stating, \u201cIt is satisfying to see how this application turns your own random thoughts into a well-written, informative, and engaging piece of text.\" The ability of the AI to streamline the review process while enhancing the clarity and coherence of the content was a key factor in the positive reception of the tool.\nMotivational Impact. A few participants referred to how the tool encouraged them to leave more reviews. P14 expressed, \"When I use this app, it motivates me to do a review. I don't need to think about grammar mistakes, and the Al-given review is very good.\u201d This suggests that the Al's ability to simplify the review process and alleviate concerns about writing quality can potentially motivate people."}, {"title": "4.7.2 Which version did users prefer, and why.", "content": "We asked participants to tell us how they felt about each of the two versions of Vocalizer they tried, and to explain which one they preferred and why.\nEasier articulation. Every participant expressed a preference for the LAV. They highlighted several advantages of the LAV, including its ability to organizing and articulating user thoughts more effectively than the audio-only version. P2 exemplified this with a comments \"So it gathers your thoughts properly and has the function of changing the phrases as you want.\", and another participant stated \"I prefer to use the one with an Al agent. The AI agent will help me for the grammatical error, and clearly make the sentence which I really want to say.\"\nDialogue. The other clear reason for the participants' preference was the capability to have a two-way conversation about the review with the AI. Here, P15 said that \"It was fun to experiment with different change requests and see how the ai interpreted it.\""}, {"title": "4.7.3 Foreseeable opportunities and threats in using Al for review process.", "content": "Authenticity. First, several participants mentioned their concerns about the AI-enhanced reviews' ability to convey the author's true feelings, or in other words, their authenticity. They thought that while the LLM makes an objectively better-written review, it loses some of its meaning and nuances may be lost in the process: \"... there might be a difference in the intended feedback and Al-version of it\" (P11), and \"if AI corrects a lot of the sentences, that will hide the people's honest feelings\" (P12).\nSocietal Impact. Second, although these may not be specific to user-generated reviews and review platforms, participants pointed out several negative implications on society. Two participants explained that AI taking over small human tasks like this will harm individuals' thinking abilities: \u201cour imagination and creativity will not improve\" (P1) and \"it suppresses our critical thinking ability. Coming to restaurants, yeah it might be easy to give review but proof reading it will be better.\u201d (P4) One participant, P9 highlighted potential malicious use of the technology, as \u201cpeople find it too easy to write restaurant reviews that they would say malicious things without thinking, making it easier to defame restaurants without a justified cause.\u201d."}, {"title": "User Experience", "content": "On the positive side, participants thought the use of AI has the potential to enhance user experience in leaving reviews. For instance, comments like \u201cThe whole experience was pleasant and fun\" (P8) and \"It was really fun! I enjoyed rambling on my review\" (P12) reflect how using AI to assist in the task added an element of enjoyment to the experience.\nBridging Language Barriers. The potential of Al in enhancing communication was noted by users who could empathize with the struggle with language barriers or writing skills. Participants noted that AI can not only help in writing reviews, but as a byproduct teach people language skills. This was highlighted by comments such as \"Opportunities: people with broken English can create well written reviews\" (P6) and \u201c I think I would like to have the Al agent as my writing tutor, because it can help me generate ideas and improve my paragraphs by suggesting which areas to elaborate\" (P9). It is thus very probable that AI can advance accessibility and inclusion by allowing more people to leave reviews in different languages."}, {"title": "5 Discussion", "content": "AI can make people's lives easier in different ways. Currently, especially content creation is becoming more efficient, and in the case of writing, tools such as ChatGPT have been revolutionary. With restaurant reviews, similar tools can help people write more informative and better reviews. Previous work also highlights that reviews perceived as more informative are often considered more helpful by readers [14, 32]. Our findings suggest that users actively seek to produce such informative reviews when given the tools to do so. Indeed, users seem to have two primary motivations: they not only seek comprehensive reviews but are also committed to producing detailed content themselves. Our study also shows a correlation between users' experience levels in online review writing and the richness of detail in their prompts to the AI system (see Section 4.4). Although we measure this rather naively with just the word count, it is not far-fetched to suggest that users with greater expertise in writing online reviews typically produce more detailed prompts, indicating that experience can enhance the clarity and specificity with which users express their desires to the AI system. And that, in turn, should lead to people trusting their own abilities in writing."}, {"title": "5.1 User Self-efficacy in Providing Useful and Engaging Reviews with AI", "content": "Extensive research has been conducted to assess self-efficacy in writing tasks, and more recently artificial intelligence has been used to improve people's writing skills. For example, in educational contexts, AI chatbots can enhance self-efficacy, learning attitudes, and motivation [26]. The study indicated that AI chatbots outperform traditional methods in facilitating learning outcomes and emphasize the critical role of Al in boosting self-efficacy, reducing anxiety, and improving writing skills. Al plays an essential role in fostering these improvements [10]. In addition, it has been shown that undergraduates can learn narrative creation and self-efficacy in writing using similar AI tools [43]. Furthermore, the potential of AI applications as effective writing assistance tools for non-native post-graduate students in academic English writing has been demonstrated earlier [38].\nAlthough these studies primarily focus on learning contexts, our research extends this investigation to the domain of online restaurant review writing. As detailed in Section 4.6, our findings indicate improvement in user self-efficacy when utilising Vocalizer. Participants reported a self-efficacy score of 4.86 out of 10 when writing reviews without AI assistance, indicating substantial room for improvement. In the final questionnaire, participants reported a high self-efficacy score of 9.14 out of 10 when using Al-enhanced applications like ours for writing reviews. The quantitative data was additionally supported in our qualitative findings, where users indicated that the AI agent helped them arrange their thoughts and fix their language to something they were much more satisfied with.\nOur prototype is far from a perfect tool, but it acts as a concrete step toward helping people write reviews without perfect command of the English language and with a helpful agent that can structure their thoughts into a sensible review."}, {"title": "5.2 User Satisfaction and Preferences in AI-Assisted Review Generation", "content": "A study has found that AI-created reviews were less useful, trustworthy and authentic when people were aware that the content was Al-generated, compared to reviews that they knew were written by humans[4", "24": "."}]}