{"title": "Sequential Learning in the Dense Associative Memory", "authors": ["Hayden McAlister", "Anthony Robins", "Lech Szymanski"], "abstract": "Sequential learning involves learning tasks in a sequence, and proves challenging for most neural networks. Biological neural networks regularly conquer the sequential learning challenge and are even capable of transferring knowledge both forward and backwards between tasks. Artificial neural networks often totally fail to transfer performance between tasks, and regularly suffer from degraded performance or catastrophic forgetting on previous tasks. Models of associative memory have been used to investigate the discrepancy between biological and artificial neural networks due to their biological ties and inspirations, of which the Hopfield network is perhaps the most studied model. The Dense Associative Memory, or modern Hopfield network, generalizes the Hopfield network, allowing for greater capacities and prototype learning behaviors, while still retaining the associative memory structure. We investigate the performance of the Dense Associative Memory in sequential learning problems, and benchmark various sequential learning techniques in the network. We give a substantial review of the sequential learning space with particular respect to the Hopfield network and associative memories, as well as describe the techniques we implement in detail. We also draw parallels between the classical and Dense Associative Memory in the context of sequential learning, and discuss the departures from biological inspiration that may influence the utility of the Dense Associative Memory as a tool for studying biological neural networks. We present our findings, and show that existing sequential learning methods can be applied to the Dense Associative Memory to improve sequential learning performance.", "sections": [{"title": "1 Introduction", "content": "Sequential learning, sometimes continual learning or lifelong learning, is an important area of study in machine learning where models are exposed to several tasks in sequence and must learn new tasks without exposure to previous ones. Biological neural networks can perform extremely well in sequential environments, and can even use information from previous tasks to help learn new tasks quicker, \"forward transfer\", and achieve better-than-random performance on unseen tasks, \u201czero-shot\u201d performance. However, artificial neural networks struggle immensely in sequential environments, often forgetting previous tasks entirely (catastrophic forgetting) let alone showing any signs of forward transfer. There have been many techniques developed to improve the ability of artificial neural networks to learn sequential tasks. These include architectural methods that alter / add to the network architecture for each new task, regularization methods that create a surrogate loss term for previous tasks to use in future learning, and rehearsal based methods that replay data from previous tasks to prevent forgetting.\nThe Hopfield network is a well studied model of autoassociative memory. In the Hopfield network, states are learned to be attractors in an energy space such that probe states iterate towards and stabilize on them (within some constraints). Importantly, the Hopfield network has had significant presence in research bridging the gap between computer science, neuroscience, and psychology thanks to the biological inspiration of the Hopfield"}, {"title": "2 Literature Review", "content": null}, {"title": "2.1 The Hopfield Network", "content": "The classical Hopfield network is a relatively simple artificial neural network that operates as a model of autoassociative memory (Hopfield, 1982). Although earlier autoassociative memories exist (Steinbuch and Piske, 1963; Steinbuch, 1965; Kohonen, 1972; Kohonen, 1978), the Hopfield network is perhaps the most studied in the class. Typically, a network of dimension N operates over states that are drawn from a discrete domain, usually the binary ({0,1}N) or bipolar ({\u22121,1}N). The network can be operated over a continuous domain, but it has been shown that the interesting behavior of the network is the same whether operating over continuous or discrete domains (Hopfield, 1984), and hence the simpler discrete domain is more common. The network usually employs the Hebbian"}, {"title": "2.2 Sequential Learning in the Hopfield Network", "content": "The classical Hopfield network has been studied in sequential learning environments at length, alongside other network architectures. Nadal et al. (Nadal et al., 1986) analyzed the recall of the Hopfield network after training on a sequence of items. The weights of the network were rigorously analyzed as new items were added, and the magnitudes associated with each item were quantified. Notably, Nadal et al. found results that are strikingly similar to the spin glass literature (Amit, Gutfreund, and Sompolinsky, 1985b;\nToulouse, Dehaene, and Changeux, 1986; Personnaz et al., 1986). Notably, the catastrophic forgetting observed when the network's capacity is exceeded is linked directly to an item's associated weight magnitude dropping below a derived threshold something that occurs nearly simultaneously for all stored items above the network capacity. Burgess et al.\n(Burgess, Shapiro, and Moore, 1991) investigate list-learning in the Hopfield network, a form of sequential learning where items are presented one at a time for a single epoch, effectively forming tasks of a single item. Recall is tested over all previously presented items at the end of each epoch. Burgess found the accuracy is high for both early and late items, but lower for intermediate items. This was linked to the biological memory model of short-term and long-term memory (Shiffrin and Atkinson, 1969) as well as the concepts of recency and primacy/familiarity (Lund, 1925). Burgess et al. also experimented with small modifications to the learning algorithms in the Hopfield network, such as increasing weights in proportion to the weight magnitude. Thanks to the simplicity of Hebbian learning rules, this is effectively equivalent to rehearsal in this domain. When using this method, it was found that the network could be tuned to exhibit strong primacy or strong recency behavior easily, and with some difficulty both behaviors could be observed.\nRobins investigated a more explicit form of rehearsal, looking at feed-forward neural networks with new tasks alongside items from previous tasks. Of particular interest are the"}, {"title": "2.3 Sequential Learning Datasets", "content": "There are a variety of sequential learning datasets that are standard across the literature. A decent amount of sequential learning has focused on classification tasks, as these networks were and are prevalent in the current age of machine learning. Some such datasets have been based around well known, widely available datasets such as MNIST digit classification (LeCun, Cortes, and Burges, 1998) and CIFAR (Krizhevsky and Hinton, 2009). These are image based classification datasets which proved popular during the rise of image"}, {"title": "3 Sequential Learning Methods", "content": "Sequential learning methods can be broadly classified into three categories: architectural, rehearsal, regularization. Architectural sequential learning methods make some change to"}, {"title": "3.1 Naive Rehearsal", "content": "The first method we will investigate is by far the simplest naive rehearsal. While rehearsal-based methods may appear simple at first glance, there is some depth to these methods. The buffer items need not be taken directly from the training data of previous tasks and could instead be generated another way, meaning the assumption of access to previous tasks is not violated. Furthermore, the method in which rehearsal items are introduced during training may vary, from naively introducing the entire buffer at each epoch to sampling a new mini-buffer that is updated every epoch, known as sweep-rehearsal\n(Robins, 1995; Silver, Mason, and Eljabu, 2015).\nIn naive rehearsal we take some proportion of the previous task items and store them in a buffer to introduce during the next task's training. We take a constant proportion across all tasks, e.g. 100 items from task one, 100 from task two, and so on. Typically, some predefined buffer size is used, and the buffer is utilized entirely throughout learning (i.e. initially the entire buffer are items from the first task, after task two the buffer is half items from task one, half from task two, and so on). However, we found excellent results using a growing buffer, with considerably shorter training times. Our rehearsal implementation combined the buffer with the new task data, then shuffled and batched this. This means individual buffer items are seen as frequently as individual new items once per epoch and effectively implements a type of sweep rehearsal (as the buffer items are newly sampled each batch). We may be able to save some memory by replaying the same buffer items more than once, but our investigation here is foundational, and hence the best-case scenario with a large number of buffer items is still interesting. We parameterize naive rehearsal using the rehearsal proportion: a rehearsal proportion of 1.0 would be to present"}, {"title": "3.2 Pseudorehearsal", "content": "Pseudorehearsal (Robins, 1993; Robins, 1995), similar to naive rehearsal above, is a rehearsal-based sequential learning method. In pseudorehearsal, however, buffer items are not sampled from the training data of previous tasks, but instead from randomly generated data that is fed through the model. This removes the need for access to the previous training data, as only the model itself is required to generate buffer items. Pseudorehearsal has been studied in the Hopfield network (Robins and McCallum, 1998), both using both homogenous (a random state is relaxed to a stable point, which is taken as the buffer item) and heterogeneous methods (a random state is relaxed to a stable one, and the pair is taken as a buffer item to ensure the mapping remains consistent during training of the next task). We will focus on the homogenous method. After each task, we will generate some number of random probes (based on the amount of training data in the previous task, so we can compare the buffer size to the rehearsal method above) and relax these probes to a stable state. We then treat these relaxed probes exactly as if they were sampled from the training data using naive rehearsal combine with the data for the next task, shuffle, batch, and train.\nWe theorize that pseudorehearsal will be extremely effective at higher interaction vertices. It is known that the memories of the Dense Associative Memory undergo a feature to prototype transition as the interaction vertex is raised (Krotov and Hopfield, 2016; Krotov and Hopfield, 2018). When the memories are learned to be prototypes of the training data, we expect random probes to stabilize on or near those prototype states, meaning we get buffer items that are extremely representative of the previous task training data without requiring explicit access to the data itself. This should improve the efficacy of pseudorehearsal, which has always suffered from buffer items that are not representative of training data (Atkinson et al., 2021)."}, {"title": "3.3 Gradient Episodic Memories", "content": "The following methods are somewhat hybrid regularization and rehearsal-based methods. Gradient Episodic Memories (Lopez-Paz and Ranzato, 2022) aims to preserve performance on previous tasks by ensuring the gradient vector of the loss on the current task never points against the gradient of the loss on previous tasks. To measure the gradient on previous tasks, a buffer of previous task items is kept and evaluated each batch to find the gradients. Therefore, Gradient Episodic Memories regularizes the weight updates by rehearsing previous task items. One of the main benefits of GEM over other sequential learning methods, particularly the quadratic-penalty weight-importance methods we discuss below,"}, {"title": "3.4 Averaged Gradient Episodic Memory", "content": "Average Gradient Episodic Memory (Chaudhry et al., 2018) aims to solve some of the issues with Gradient Episodic Memories above by relaxing the conditions on the optimization problems. Rather than requiring that the projected gradient does not point against any previous task's gradient, leading to one constraint per previous task, we require the projected gradient to not point against the gradient of all previous tasks simultaneously. That is, rather than having a gradient per previous task, we have a single gradient after running the model on all tasks. This is the average in Averaged Gradient Episodic Memory. By reducing the number of constraints to one, we also greatly reduce the computation required to find the projected gradient \u011f; in fact, a closed form solution for this optimization problem exists. However, by only constraining the gradient to point in the same direction as the gradient on the combined task buffers we may allow learning that goes against the gradient for an individual task, potentially leading to forgetting, but only if the learning would benefit all previous tasks on average.\nA-GEM looks to learn a new task with the objective:"}, {"title": "3.5 L2 Regularization", "content": "Regularization-based methods add an additional term to the loss function that approximates the previous task's loss. Usually that additional term is proportional to the squared difference of the previous task's optimal weights and the current weights, meaning large shifts from the previous optimal weights are penalized heavily. Many regularization-based methods include a measure of weight importance to allow \"unimportant\" weights to shift more than \"important\" ones. We will investigate several such regularization methods with weight importance measures, but as a sanity check we will also investigate the degenerate case of assigning an equal importance to every weight. This is functionally equivalent to L2 regularization, also called weight decay.\nFormally, this class of quadratic term regularization methods look like"}, {"title": "3.6 Elastic Weight Consolidation", "content": "Elastic Weight Consolidation (Kirkpatrick et al., 2017) is the first regularization-based technique we will discuss with a nontrivial weight importance measure. In EWC, weight importance is measured using the Fisher information matrix of the network parameters, effectively quantifying how each parameter affects the prediction for a sample of the training data. The presence of previous task items means that this calculation must be done"}, {"title": "3.7 Memory Aware Synapses", "content": "Memory Aware Synapses (Aljundi et al., 2018) is another weight importance regularization-based sequential learning technique. In contrast to EWC, which uses the Fisher information and the loss function of the task, MAS measures the model sensitivity directly. A Taylor expansion of the model output with respect to each network parameter gives a weight importance measure of:"}, {"title": "3.8 Synaptic Intelligence", "content": "Synaptic Intelligence (Zenke, Poole, and Ganguli, 2017) is a sequential learning technique again focused around a quadratic regularization term with weight importances. The importance measure is now based on the path integral of the task's gradient along the path through parameter space taken during training. Formally:"}, {"title": "4 Hyperparameter Tuning and Experiment Design", "content": null}, {"title": "4.1 Experimental Design", "content": "In our experiments, we consistently use a series of five permuted MNIST tasks. For hyperparameter tuning we use tasks consisting of 2000 items, and for the final results we use 10000 items per task. Items are randomly sampled from the full MNIST dataset. We do not train on the full MNIST dataset in each task due to computational constraints. We use only five tasks due to instability of the network with additional training as the number of tasks is increased, we found the network weights fluctuate increasingly wildly. We found decreasing the learning rate and / or increasing the momentum helped reduce this unwanted behavior, but at the cost of sequential learning performance.\nEach task has 20% of its data set aside for validation, with the remaining 80% used for training. Each task has a different randomly generated permutation of pixels such that the classification of digits must be learned from scratch each time. We encode these items into binary vectors by flattening the pixel values into a vector, to which we append a one-hot encoded task ID, and ten neurons to represent the classes of digits. To conform to the literature that uses this dataset with the Dense Associative Memory (Krotov and Hopfield,"}, {"title": "4.2 Dense Associative Memory Formalization and Hyperparameters", "content": "The Dense Associative Memory is an autoassociative memory consisting of a set of memory vectors that learns a set of items \u00a7 via the gradient descent of the error of the initial item and the relaxed item. To relax a probe state & we update each neuron until all neurons are stable (further updates do not result in change). In contrast to the classical Hopfield"}, {"title": "4.3 Sequential Learning Methods Hyperparameters", "content": "Each sequential learning method in Section 3 has associated hyperparameters that must also be tuned to maximize sequential learning performance. Instead of fixing the hyperparameters across all interaction vertices as we have done for the more general hyperparameters of the Dense Associative Memory, we will tune the parameters per interaction vertex.\nFor rehearsal based methods, we varied the proportion of task items added to the rehearsal buffer. We conduct a linear search from a rehearsal proportion of 0.0 to 1.0. For naive rehearsal, this range corresponds to vanilla sequential learning (no sequential learning method) to effectively presenting the tasks non-sequentially. For pseudorehearsal, the lower end of the search again corresponds to no method, but the upper end no longer has a nice interpretation. We can generate as many buffer items as we like, including more than the number of items in the original task. Since we do not require unique generated items, we are likely to generate several repeated items in the buffer, but since the generated items are representative of the network's attractor space this is not only acceptable, but perhaps preferable to ensure important regions are well sampled. We do not explore beyond a pseudorehearsal proportion of 1.0 as we see a plateau in the performance.\nFor regularization based methods - L2 Regularization, Elastic Weight Consolidation, Memory Aware Synapses, and Synaptic Intelligence we must tune the regularization parameter A. Since the weight importances of each method are not guaranteed to be on the same order of magnitude as one another, we do not expect the values of A between each method to be comparable. However, the values of A within each method are comparable, so if we observe different optimal regularization parameters across interaction vertices that is an interesting result. Again, we perform a search across a wide range of regularization parameters for each method, although this time we employ a log-space search to ensure we explore a wide range of parameters across many orders of magnitude efficiently.\nFinally, for the two somewhat unique methods of Gradient Episodic Memories and Averaged Gradient Episodic Memories, we again tune the proportion of data from previous tasks to add to the memory buffer. This allows us to compare the rehearsal methods with"}, {"title": "5 Experiment Results", "content": null}, {"title": "5.1 Sequential Learning Methods Hyperparameter Tuning", "content": null}, {"title": "5.1.1 Rehearsal Methods", "content": null}, {"title": "5.1.2 GEM and A-GEM", "content": "Gradient Episodic Memories and Averaged Gradient Episodic Memories have the same reasonably interpretable hyperparameter space as rehearsal-based methods: how much of the previous task do we store. Although we only require the gradients of the buffer items instead of rehearsing them directly, we are still interested in how the size of the buffer effects the performance of the methods."}, {"title": "5.1.3 Regularization-Based Methods", "content": "In rehearsal-based methods we have a good understanding of how the rehearsal proportion should impact sequential learning performance and a relatively small space over which to search (0.0 through 1.0), so a grid search was reasonable. In the following regularization-based methods, the regularization hyperparameter A has a more nuanced impact on performance too small and the network will forget earlier tasks, too large and the network will not learn new tasks. Furthermore, the search space of A is much larger, and it is not clear where the optimal region will lie, whereas in rehearsal we could be sure that increasing the rehearsal proportion would improve performance. For these reasons, we have not conducted a grid search over A but instead use Optuna (Akiba et al., 2019), a hyperparameter optimization library, in conjunction with a Tree-structured Parzen Estimator sampler over the search space. This sampler attempts to estimate the optimal hyperparameter value using a Gaussian Mixture Model, resulting in dense sampling near the optimal region (of interest to us) and sparser sampling far from the optimal region. To estimate the error in the average accuracy across the A search space, we have used a moving window of size 20 and plot the average and standard deviation (as an error band). This gives us better error estimates close to the optimal region, which is more useful for our analysis. In each of the following methods we have conducted 100 trials for each interaction vertex."}, {"title": "5.2 Tuned Sequential Learning Methods", "content": "Now we have tuned the Dense Associative Memory and all sequential learning methods we can finally compare the peak performance of each method to one another. For rehearsal-based methods, GEM, and A-GEM, we investigate several values of the rehearsal / memory proportion. For the regularization-based methods, we use regularization A as shown in\nTable 1. We repeat the five Permuted MNIST task sequential learning experiment ten times, aggregating the results in Table 2."}, {"title": "6 Conclusions", "content": "We have investigated the Dense Associative Memory in the context of sequential learning. We believe the application of this network in a sequential learning environment is novel, and gives some insight into the behavior of this associative memory architecture. Our studies include rehearsal-based (naive rehearsal, pseudorehearsal), gradient-based (Gradient Episodic Memories, Averaged Gradient Episodic Memories) and regularization-based\n(L2 Regularization, Elastic Weight Consolidation, Memory Aware Synapses, Synaptic Intelligence) sequential learning methods. Our studies shed light on several aspects of the Dense Associative Memory; the performance of the network in sequential learning tasks, the most effective sequential learning method as the interaction vertex varies, and the properties of the network more broadly.\nWe found that the Dense Associative Memory responds as expected to rehearsal-based methods; increasing the average accuracy of a series of tasks as more of the previous tasks are rehearsed. Of particular note was pseudorehearsal, which improved dramatically at high interaction vertices, which we attribute to the generation of representative pseudoitems thanks to prototype-like memory structures. Gradient-based methods (GEM, A-GEM) were very effective at extreme interaction vertices but much less effective at transitory values, and also produced much more unstable learning behaviors, perhaps indicating the Dense Associative Memory is very sensitive to gradient alterations during the learning process.\nRegularization-based methods gave some understanding of weight importances in the Dense Associative Memory. From a sequential learning perspective, the network behaves typically in that constraining the weights retains performance on previous tasks, even for naive constraints such as L2 regularization. We discovered that the regularization hyperparameter in Elastic Weight Consolidation was sensitive to the amount of data used per task, particularly for higher interaction vertices, which may indicate that the prototype memory structures of high interaction vertex networks require increasing amounts of data to stabilize (at least, in a way that the Fisher Information can attribute importances to).\nWe found that Memory Aware Synapses and Synaptic Intelligence performed very well for transitory interaction vertices where other methods struggled, and regularization-based"}, {"title": "A General Dense Associative Memory Hyperparameter Search", "content": "We conducted a hyperparameter search over the general network hyperparameters. Our aim is to find the combination of hyperparameters that maximizes the minimum accuracy on these tasks. This ensures our network can not only learn the first task well (overcoming initialization) but also remain plastic enough to learn subsequent tasks. We present a select few of these gridsearches that performed best, searching over the initial learning rate and the temperature. We repeated these experiments to tune the learning rate decay and momentum values, although the results are excluded for brevity. Note that the optimal regions in the searches below do not move considerably as we increase the interaction vertex, something attributed to modifications to the Dense Associative Memory (McAlister,\nRobins, and Szymanski, 2024a). We also include the average accuracy over the same gridsearch for completeness. We determined the optimal general network hyperparameters to be a number of memories |5| = 512, a number of training epochs MaxEpochs = 500, an initial learning rate lrInit = 1 \u00d7 10-1, learning rate decay lrDecay = 0.999, momentum p=\n0.6, an initial and final temperature value } = T = Tf = 0.875, and an error exponent m = 1."}]}