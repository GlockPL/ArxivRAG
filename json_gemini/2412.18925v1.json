{"title": "HuatuoGPT-01, Towards Medical Complex Reasoning with LLMs", "authors": ["Junying Chen", "Zhenyang Cai", "Ke Ji", "Xidong Wang", "Wanlong Liu", "Rongsheng Wang", "Jianye Hou", "Benyou Wang"], "abstract": "The breakthrough of OpenAI 01 highlights the potential of enhancing reasoning to improve LLM. Yet, most research in reasoning has focused on mathematical tasks, leaving domains like medicine underexplored. The medical domain, though distinct from mathematics, also demands robust reasoning to provide reliable answers, given the high standards of healthcare. However, verifying medical reasoning is challenging, unlike those in mathematics. To address this, we propose verifiable medical problems with a medical verifier to check the correctness of model outputs. This verifiable nature enables advancements in medical reasoning through a two-stage approach: (1) using the verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning (RL) with verifier-based rewards to enhance complex reasoning further. Finally, we introduce HuatuoGPT-01, a medical LLM capable of complex reasoning, which outperforms general and medical-specific baselines using only 40K verifiable problems. Experiments show complex reasoning improves medical problem-solving and benefits more from RL. We hope our approach inspires advancements in reasoning across medical and other specialized domains.", "sections": [{"title": "Introduction", "content": "The release of OpenAI ol has marked a significant milestone in large language model (LLM) development, showcasing impressive capabilities [1-3]. This breakthrough highlights the potential of scaling Chain-of-Thought (CoT) and reinforcement learning to enhance LLM performance [4-6]. While subsequent research efforts attempt to replicate these advancements, they often remain limited to mathematical reasoning tasks [7-9, 6]. The application of o1-like methods to specialized fields, such as medicine, remains largely underexplored.\nMedical tasks often involve complex reasoning [10-12]. In real-world medical diagnoses or decisions, doctors often deliberate carefully. Such life-critical field necessitates meticulous thinking to ensure more reliable answers [13, 14]. Additionally, the medical domain offers unique advantages: compared to general domains, the medical domain is generally narrower in scope and easier to verify. Furthermore, medical reasoning closely resembles real-world applications in fields like finance, law, education, and security, making advancements in this area readily transferable [15, 16].\nDespite these advantages, a key challenge in medical reasoning is verifying the thought process, which often lacks clear steps. Inspired by mathematical problems that allow verification through their outcomes, we construct 40K verifiable medical problems reformatted from challenging, closed-set medical exam questions. These verifiable problems are characterized as open-ended with unique,"}, {"title": "Verifiable Medical Problems", "content": "objective ground-truth answers that allow an LLM verifier to check solution correctness. This enables a two-stage approach for advancing medical complex reasoning:\nStage 1: Learning Complex Reasoning We construct complex reasoning trajectories through strategy-based searches guided by verifier feedback (True or False). The LLM first initializes a CoT. If the verifier rejects the current CoT, the model extends the CoT by applying a strategy sampled from Backtracking, Exploring New Paths, Verification, and Correction until a correct answer is provided. Successful reasoning trajectories are then used to fine-tune the LLM, enabling it develop complex reasoning skills that embody iterative reflection.\nStage 2: Enhancing Complex Reasoning with RL After acquiring complex reasoning skills, reinforcement learning (RL) further refine this ability. Specifically, sparse rewards provided by the verifier guide self-improvement using the Proximal Policy Optimization (PPO) algorithm.\nUsing this approach, we present HuatuoGPT-01, a medical LLM capable of producing a long CoT to recognize its mistakes, try different strategies and refine the answer. Experiments demonstrate that our method (using only 40K data points) yields an 8.5-point improvement on medical benchmarks with an 8B model. Furthermore, our 70B model outperforms other open-source general and medical-specific LLMs across multiple medical benchmarks. The experiments further reveal that complex reasoning enhances medical problem-solving and boosts RL performance compared to standard or non-CoT methods. Our contributions are as follows:\n\u2022 To the best of our knowledge, this is the first work to advance medical complex reasoning in LLMs using verifiable medical problems and a medical verifier.\n\u2022 With verifiable medical problems, we propose a two-stage training approach, combining search strategies to construct reasoning pathways for fine-tuning, and further enhanced by RL with verifier feedback.\n\u2022 Using the proposed method, we developed HuatuoGPT-01, the first medical LLM capable of complex reasoning. HuatuoGPT-01 exhibits superior performance compared to the open-source general and medical-specific baselines.\n\u2022 Our experiments reveal that complex reasoning is effective for medical problem-solving and benefits RL enhancements.\nInspired by mathematical problems that enable verification of the solution process through the final result, we aim to create verifiable medical problems that allow reasoning verification through outcomes. These verifiable problems are characterized as open-formal with unique, objective ground-truth answers, as illustrated in Figure 1.\nSourcing from Medical Exam Questions To achieve this, we utilize closed-set real-world exam questions for two key reasons: 1) a large number of medical exam questions are available; and 2) these exam questions are typically objective and accurate. Specifically, we collected 192K medical multiple-choice exam questions from the training sets of MedQA-USMLE [17] and MedMcQA [18]."}, {"title": "Transforming to Verifiable Medical Problems", "content": "Transforming to Verifiable Medical Problems However, these medical questions are closed-set, meaning they provide limited options to choose from. This makes it easy for models to guess the correct answer without proper reasoning. Additionally, some questions are not suitable due to they may lack a unique correct answer for verification or are too simple to require reasoning.\nTo address this, we select and process the questions as follows:\n1. Selecting Challenging Questions We removed questions that three small LLMs (Gemma2-9B [19], LLaMA-3.1-8B [20], Qwen2.5-7B [21]) all answered correctly and discarded short questions to retain those requiring deeper reasoning.\n2. Ensure Unique Answers: We excluded questions asking for \"incorrect options\" or with multiple correct answers. A LLM (GPT-40) is further employed to remove questions where the correct answer might not be unique or could be ambiguous.\n3. Reformatting to Open-Ended Formal: Using LLMs (GPT-40), We reformatted each closed-set question into open-ended problem z and a ground-truth answer y*, as shown in Figure 1.\nThe prompt used for filtering and processing can be found in Appendix B. After this filtering and processing, we ultimately constructed a dataset of 40K verifiable medical questions denoted as\n$D = \\{(x, y^*)\\}$, where x is a verifiable problem and $y^*$ the ground-truth answer.\nDeveloping Medical Verifier With these verifiable problems, we propose a verifier to assess the correctness of model outputs. Given a medical verifiable problem x, the model generates a Chain-of-Thought (CoT) e and a result y. The verifier checks y against the ground-truth answer $y^*$ and provides binary feedback as:\n$Verifier(y, y^*) \\in \\{True, False\\}$\nThis feedback is essential for building a correct reasoning trajectory and improving reasoning performance. We use GPT-40 [22] as the verifier, prompting it to perform validation with the detailed prompt provided in Appendix C. Given the prevalence of aliases in the medical domain, exact match methods [8, 23] commonly applied in mathematics are impractical. Experiments in Section 4.2 confirm this and demonstrate the reliability of the LLM-based verifier."}, {"title": "Methodology", "content": "In this section, we present the method for training LLMs to performing medical complex reasoning to identify errors, and refine answers using deep thinking. As shown in Figure 1, the method has two stages: Stage One: master complex reasoning, and Stage Two, enhance complex reasoning with reinforcement learning (RL)."}, {"title": "Stage One: Learning Complex Reasoning", "content": "3.1 Stage One: Learning Complex Reasoning\nSearching for Correct Trajectories Given a verifiable medical problem as a tuple (x, $y^*$), i.e. (question, ground-true answer), the LLM (e.g., GPT-40) generates an initial CoT $e_0$ and answer $y_0$:\n$e_0, y_0 = LLM_{init}(x)$\nThe verifier checks if $y_0$ matches $y^*$. If incorrect, the model iteratively refines the answer by applying a randomly selected search strategy k \u2208 K on prior thoughts $[e_0, y_0, \u00b7 \u00b7 \u00b7, e_{i\u22121}, y_{i\u22121}]$, producing new reasoning $e_i$ and new answer $y_i$:\n$e_i, y_i = LLM_{k_i} (x, [e_0, y_0, ..., e_{i-1}, y_{i-1}])$\nwhere i denotes the i-th iteration. We define four search strategies K to guide the refinement process:\n\u2022 Exploring New Paths The LLM explores a new approach $e_i$, distinct from prior $e_0, ..., e_{i\u22121}$, to derive a new answer $y_i$.\n\u2022 Backtracking The LLM revisits a previous reasoning process $e_j$, $y_j$, where $j < i \u2212 1$, and continues reasoning from there. Note that Backtracking is sampled only if $i < 2$.\n\u2022 Verification The LLM evaluates the current reasoning $e_{i\u22121}$and result $y_{i\u22121}$, providing a validation process $e_i$ and the verified result $y_i$.\n\u2022 Corrections The LLM critiques and corrects the current reasoning $e_{i\u22121}$, yielding a revised reasoning $e_i$ and answer $y_i$.\nThe process iterates until $y_i$ is verified as correct. If the maximum iteration count N = 3 are reached, the search restarts. Each data point (x, $y^*$) is given up to T = 3 attempts; if all fail, the data point is discarded. The prompts for search reasoning trajectories can be found in Appendix D.\nConstructing SFT Training Data When a successful trajectory $[e_0, y_0,..., e_i, y_i]$ is found, it is reformatted into a coherent, natural language reasoning process $\\hat{e}$ (Complex CoT):"}, {"title": "Stage Two: Enhance Complex Reasoning with RL", "content": "In this stage, we further enhance the complex reasoning skills using reinforcement learning (RL). While the LLM learned successful reasoning trajectories in stage 1, these paths, derived via search, may not be optimal. On-policy learning in stage 2 aims to refine the model for better complex CoT reasoning."}, {"title": "Rewards of RL", "content": "Rewards play a crucial role in guiding the RL training target. Given a verifiable problem x and the generated response ($\\hat{e}, \\hat{y}$), the reward is assigned as:\nr' (x, \u0177, y*) = \\begin{cases}1 & \\text{if verifier(\u0177, y*) = True} \\\\0.1 & \\text{if verifier(y, y*) = False} \\\\0.5 & \\text{if y = null}\\end{cases}\nFollowing [24, 25, 8], correct answers receive a reward of 1, incorrect answers receive 0.1, and responses that lack think-before-answering behavior receive 0. Additionally, following related works, the total reward combines this function score with the Kullback-Leibler (KL) divergence between the learned RL policy $\u03c0_\u03b8$ and the initial policy $\u03c0_{ref}$, scaled by a coefficient \u03b2:\nr(x, y, y*) = r'(x, y, y*) + \u03b2KL(\u03b8)\nto stabilize training with sparse rewards [8].\nReinforcement Learning For RL, We use the Proximal Policy Optimization (PPO) [26] algorithm with a clipped objective. The fine-tuned model serves as the policy model $\u03c0_\u03b8$. Training is conducted on the remaining verifiable medical problems $D_{RL} = \\{(x, y^*)\\}$. The policy samples responses ($\\hat{e}, \\hat{y}$) for input x, computes the reward, and updates parameters \u03b8.\nThe full training process for both stages is summarized in Algorithm 1."}, {"title": "Experiments", "content": "4 Experiments\n4.1 Experimental Setup\nTraining Data Finally, We constructed a 40K medical verification dataset D = \\{(x,y^*)\\} from the training sets of MedQA-USMLE [17] and MedMCQA [27]. Of this, 20K is used for SFT in stage 1 and 20K for RL in stage 2. Additionally, 4K unconverted data (close-set questions with option answers) from D are included to enhance generalization. In line with prior work that integrates general-domain data to support medical adaptation [15, 28], we add 5K general verification questions sourced from MMLU-Pro [29] outside the medical-related tracks. All data were strictly screened to avoid contamination with the evaluation data using the filtering method of Med-PaLM2 [30] (filtering overlaps of 64 consecutive characters).\nModel Training Using the proposed method, we train our models HuatuoGPT-01-8B and HuatuoGPT-01-70B based on LLaMA-3.1-8B-Instruct and LLaMA-3.1-70B-Instruct [20], respectively. In Stage 1, the models are fine-tuned on the $D_{SFT}$ for 3 epochs with a learning rate of 5e-6 and a batch size of 128. In Stage 2, we employ PPO for RL with a learning rate of 5e-7, a batch size of 128, and \u03b2 set to 0.03. The PPO parameters are set as: 3 PPO epochs, a discount factor 1.0, a value coefficient 1.0, and a clip range 0.2.\nBaselines We compare our models with two type of LLMs: 1) General LLMs: Qwen-2.5 [31], LLaMA-3.1 [20], Gemma 2 [19], Yi [32], Mistral [33]; and 2) Medical-Specific LLMs: UltraMedical [28], OpenBioLLM [34], and BioMistral [35].\nBenchmarks We evaluate on standard medical benchmarks: MedQA (USMLE test set) [17], MedMCQA (validation set) [18], and PubMedQA (test set) [36]. Aditionally, we evaluated the medical sections of some challenging LLM benchmarks, including the health and biology tracks of MMLU-Pro [29], and the genetics and molecular biology tracks of GPQA [37]. Due to the limited number of GPQA questions, we ran this evaluation 5 times and averaged the results."}, {"title": "Experimental Results", "content": "4.2 Experimental Results\nMain Results We evaluated various open-source LLMs on medical tasks, as shown in Table 1. The results indicate that prior medical-specific LLMs, like UltraMedical, excel on traditional medical benchmarks (MedQA, MedMCQA, PubMedQA) but struggle on the newer, more challenging datasets,"}, {"title": "Ethical Statement", "content": "A Ethical Statement\nAlthough the proposed model is a medical LLM with complex reasoning capabilities, it may still produce content that includes hallucinations or inaccuracies. Therefore, the current model is not suitable for real-world applications. Consequently, we will impose strict limitations on the use of our model. The models are not permitted for use in clinical or other industry applications where such inaccuracies could lead to unintended consequences. We emphasize the ethical responsibility of users to adhere to these restrictions in order to safeguard the safety and integrity of their applications."}, {"title": "Constructing Medical Verifiable Problems", "content": "B Constructing Medical Verifiable Problems\nTo construct Medical Verifiable Problems, we begin by employing small models and rule-based methods to identify challenging questions. Subsequently, we leverage GPT-40 to perform data filtering, isolating questions that have been suitably transformed. The prompt used for this data filtering process is illustrated in Figure 5. After selecting appropriate data, we reformat multiple-choice medical exam questions into open-ended verifiable problems using the prompt provided in Figure 6."}, {"title": "The Prompt of Verifier", "content": "C The Prompt of Verifier\nGPT-40 serves as the verifier to assess the correctness of model-generated outputs. Using the prompt depicted in Figure 7, we present GPT-40 with both the model's output and the ground-truth answer to evaluate the correctness of the response. The verifier returns a Boolean value: True if the response is accurate and False otherwise."}, {"title": "Prompts for Searching Trajectories", "content": "D Prompts for Searching Trajectories\nThis section outlines the prompts used for constructing complex Chain-of-Thought (CoT) reasoning pathways. Initially, a question x is presented to GPT-40, which generates an initial CoT response using the prompt shown in Figure 8. If the verifier determines the response to be incorrect, GPT-40 employs one of several search strategies to iteratively refine the output until it is accurate. The prompts for these four search strategies \u2014 Backtracking, Exploring New Paths, Correction, and Verification \u2014 are detailed in Figures 10, 10, 11, and 12, respectively."}, {"title": "Prompts for Constructing SFT Training Data", "content": "E Prompts for Constructing SFT Training Data\nWhen a successful trajectory [e0, Yo, ..., ei, Yi] is found, it is reformatted into a coherent, natural language reasoning process \u00ea (Complex CoT) using the prompt shown in Figure 13. This reformatting avoids rigid structures, using smooth transitions (e.g., \u201chmm,\u201d \u201calso,\u201d \u201cwait\u201d) to streamline reasoning and reduce token usage. The model then generates a formal response \u0177 for for question x using the conclusion of \u00ea with the prompt in Figure 13."}, {"title": "Settings of other RL training", "content": "F Settings of other RL training\nwe further compared different RL-related algorithms with PPO. Specifically, we employed the preference-learning algorithm DPO and the REINFORCE-style algorithm RLOO.\nDPO For DPO, we had the model generate five answers for each question offline and used a verifier to identify pairs of one correct and one incorrect answer. If no such pairs were found, the data was discarded. Verified correct answers were used as positive examples, while failed verifications served as negative examples for training DPO. The hyperparameters for DPO training were set as follows: learning rate of 1e-6, batch size of 128, and a regularization parameter of 1.\nRLOO For RLOO, we used the same reward function as PPO. The parameters were also identical to those of PPO, with an additional parameter rloo_k set to 2."}, {"title": "Chinese Medical Model", "content": "G Chinese Medical Model\nModel Training For the Chinese medical domain, we replaced the exam questions from the CMB training set for Chinese medical verifiable problems. Based on the same training process as the English version of HuatuoGPT-01, we developed HuatuoGPT-01-7B-zh, built on the Qwen2.5-7B-Instruct model.\nChinese Medical Evaluation To assess the Chinese medical capabilities, we evaluated the model on three Chinese medical benchmarks, including the Chinese test set from MedQA (MCMLE) [17], the test set from CMB-Exam [84], and the test set from CMExam [85]. Additionally, we evaluated the model on the medical section of the Chinese general evaluation benchmark CMMLU [86], covering tracks of 'clinical knowledge,' 'agronomy,' 'college medicine,' 'genetics,' 'nutrition,' 'Traditional Chinese Medicine,' and 'virology'.\nComparison Models We compared the performance of three general Chinese models: Qwen2.5, GLM-4, and Yi. Additionally, we included a comparison with a Chinese medical model, HuatuoGPT-2-7B [15]."}]}