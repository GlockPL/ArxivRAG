{"title": "Deep Unlearn: Benchmarking Machine Unlearning", "authors": ["Xavier F. Cadet", "Anastasia Borovykh", "Mohammad Malekzadeh", "Sara Ahmadi-Abhari", "Hamed Haddadi"], "abstract": "Machine unlearning (MU) aims to remove the influence of particular data points\nfrom the learnable parameters of a trained machine learning model. This is a\ncrucial capability in light of data privacy requirements, trustworthiness, and safety\nin deployed models. MU is particularly challenging for deep neural networks\n(DNNs), such as convolutional nets or vision transformers, as such DNNs tend to\nmemorize a notable portion of their training dataset. Nevertheless, the community\nlacks a rigorous and multifaceted study that looks into the success of MU methods\nfor DNNs. In this paper, we investigate 18 state-of-the-art MU methods across\nvarious benchmark datasets and models, with each evaluation conducted over\n10 different initializations, a comprehensive evaluation involving MU over 100K\nmodels. We show that, with the proper hyperparameters, Masked Small Gradients\n(MSG) and Convolution Transpose (CT), consistently perform better in terms of\nmodel accuracy and run-time efficiency across different models, datasets, and\ninitializations, assessed by population-based membership inference attacks (MIA)\nand per-sample unlearning likelihood ratio attacks (U-LiRA). Furthermore, our\nbenchmark highlights the fact that comparing a MU method only with commonly\nused baselines, such as Gradient Ascent (GA) or Successive Random Relabeling\n(SRL), is inadequate, and we need better baselines like Negative Gradient Plus\n(NG+) with proper hyperparameter selection.", "sections": [{"title": "Introduction", "content": "Machine unlearning aims to remove the influence of a specified subset of training data points from\ntrained models [10]. This process is crucial for enhancing privacy preservation, model safety, and\noverall model quality. MU helps ensure compliance with the right to be forgotten [23], removes\nerroneous data points that negatively impact model performance [43], and eliminates biases introduced\nby parts of the training data [14]. Deep neural networks present significant challenges for MU due\nto their computationally intensive training requirements, highly non-convex loss landscapes, and\ntendency to memorize substantial portions of their training data [22, 12, 21]. The key open challenges\nin MU include: (1) A degradation in model accuracy often accompanies unlearning; (2) Some MU\nmethods require the model to be trained in specific ways, such as saving checkpoints, tracking\naccumulated gradients, or training with differential privacy, limiting their applicability to already\ndeployed models; (3) Assurance of information removal is difficult as there are no reliable metrics to\nmeasure it accurately; (4) There is no consensus on which methods are the most effective.\n\nA branch of MU known as exact unlearning aims to guarantee that the specified forget data have been\ncompletely removed from the model. The most reliable exact MU method is to retrain the model\nfrom scratch while excluding the forget data. Another exact MU that offers data removal guarantees is\nSISA (Sharded, Isolated, Sliced, Aggregated) [9]. However, exact MU is computationally prohibitive,\nemphasizing the need for more efficient methods. The alternative branch is approximate unlearning"}, {"title": "Background of Machine Unlearning", "content": "Setting. Starting with a training set $D = \\{(x_i, y_i)\\}_{i=1}^{N}$ and a trained model $f_0$, referred to as the\noriginal model, the objective of a MU method $U$ is to remove the influence of a particular subset\nof training set $D_F = \\{(x_i, y_i)\\}_{i=1}^{K} \\subset D$ referred to as the forget set where $K \\ll N$. The rest of\ntraining set $D_R = D \\setminus D_F$ is called the retain set. The forget set and the retain set are distinct and\ncomplementary subsets of the training set. The outcome of MU is an unlearned model $f_U$, the aim\nfor which is to perform on par with a model retrained from scratch on $D_R$; this latter model $f_R$ is\nreferred to as the retrained model. We denote the weights of the original model and the retrained\nmodel as $\\theta_0$ and $\\theta_R$, respectively. For evaluations, we consider two held-out sets: the validation set\n$D_V$ and test set $D_T$, both drawn from the same distribution as $D$. We consider the accuracy of the\nretrained model as the optimal accuracy. One critical assumption we make is that the MU method has\naccess to $\\theta_0$, $D_R$, $D_F$, and $D_V$.\n\n(1) Unlearning Evaluation. To evaluate the success of unlearning, one approach is to check whether\ndata points in $D_F$ still influence the predictions made by the unlearned model [13, 37, 31]. This is\ncommonly done via influence functions [35, 8, 29], membership inference attacks (MIA) [44]. MIA\nhas become one of the most common approaches for evaluating MU methods. It aims to determine\nwhether specific data points were part of the original training dataset based on the unlearned model."}, {"title": "Machine Unlearning Methods", "content": "We briefly discuss the main unlearning methods considered.", "subsections": [{"title": "Classical Baselines", "content": "FineTune (FT) finetunes the original model $f_0$ on only the retain set $D_R$ for several epochs.\n\nThe methodology for this follows the classic MIA: we compute the losses on $D_F$ and $D_V$, we shuffle and\ntrim them so that they are of equal size. We then train logistic regression models in a 10-fold cross validation,\nand compute the average accuracy across the folds."}, {"title": "State-of-the-art MU methods", "content": "Expanding upon the classical baselines, we additionally evaluate 15 recent MU methods. We first\ndiscuss the seven top-performing methods from the Machine Unlearning Competition 2023 on\nKaggle.\nForget-Contrast-Strengthen (FCS) [1] minimizes the Kullback-Leibler Divergence (KLD) between\nthe model's output on $D_F$ and a uniform distribution over the output classes, then alternatively\noptimizes a contrastive loss between the model's outputs on $D_R$ and $D_F$, and minimizes the cross-\nentropy loss on $D_R$.\nMasked-Small-Gradients (MSG)[2] accumulates gradients via gradient descent on the $D_R$ and\ngradient ascent on the $D_F$, then reinitialize weights with the smallest absolute gradients while\ndampening subsequent weights updates on the $D_R$ for the other weights.\nConfuse-Finetune-Weaken (CFW)[3] injects noise into the convolutional layers and then trains the\nmodel using a class-weighted cross-entropy on $D_R$, then injects noise again toward the final epochs.\nPrune-Reinitialize-Match-Quantize (PRMQ) [4] first prunes the model via L1 pruning, reinitializes\nparts of the model, optimises it using a combination of cross-entropy and a mean-squared-error on the\nentropy between the outputs of $f_0$ and $f_U$ on $D_R$ and finally converts $f_U$'s weights to half-precision\nfloats.\nConvolution-Transpose [5] simply transposes the weights in the convolutional layers and trains on\n$D_R$.\nKnowledge-Distillation-Entropy (KDE) [6] uses a teacher-student setup. Both student and teacher\nstart as copies of the original model, then the student's first and last layers are re-initialised. The\nstudent $f_U$ minimizes its Kullback-Leibler Divergence (KLD) with the $f_0$ over $D_V$, then minimizes\na combination of losses: a soft cross-entropy loss between $f_U$ and $f_0$, a cross-entropy loss on outputs\nof $D_R$ from $f_U$, and the KLD between $f_U$ and $f_0$ on $D_R$.\nRepeated-Noise-Injection (RNI) [7] first reinitialises the final layer of the model, then repeatedly\ninjects noise in different layer of the model while training on the $D_R$.\n\nWe further consider eight state-of-the-art methods introduced in the literature.\nFisher Forgetting (FF) [25, 20] adds noise to $f_0$ with zero mean and covariance determined by the\n4th root of Fisher Information matrix with respect to $\\theta_0$ on $D_R$.\nInfluence Unlearning (IU) [33, 47, 34] uses Influence Functions[18] to determine the change in $\\theta_0$\nif a training point is removed from the training loss. IU estimates the change in model parameters\nfrom $b_0$ to the model trained without a given data point. We use the first-order WoodFisher-based\napproximation from [34].\nCatastrophic Forgetting - K (CF-K) [24] freezes the first layers then trains the last $k$ layers of the\nmodel on $D_R$.\nExact Unlearning - K (EU-K) [24] freezes the first layers then restores the weights of the last $k$\nlayers to their initialization state. We randomly reinitialize the weights instead, so that the method no\nlonger requires knowledge about the training process of $f_0$.\nSCRUB [37] leverages a student-teacher setup where the model is optimised for three objectives:\nmatching the teacher's output distribution on $D_R$, correctly predicting the $D_R$ set and ensuring the\noutput distributions of the teacher and student diverge on the $D_F$\nSaliency Unlearning (SaLUN) [20] determines via gradient ascent which weights of $\\theta_0$ are the\nmost relevant to $D_F$, then trains the model simultaneously on $D_R$ and $D_F$ with random labels on\n$D_F$, while dampening the gradient propagation based on the selected weights.\nNegative Gradient Plus (NG+) [37] is an extension of the Gradient Ascent approach where\nadditionally a gradient descent step is taken over the $D_R$."}]}, {"title": "Experimental Evaluation", "content": "Experiments. We evaluate the 18 recent MU methods as described in Section 3 across 5 benchmark\ndatasets: MNIST [38], FashionMNIST [49], CIFAR-10 [36], CIFAR-100 [36], and UTK-Face [53].\nThese datasets vary in difficulty, number of classes, instances per class, and image sizes. We consider\ntwo model architectures: a TinyViT and a ResNet18 model. Hence, in total we evaluated nine\ndifferent combinations of models and architectures: ResNet18 and TinyViT on MNIST, Fashion-\nMNIST, CIFAR-10, CIFAR-100, and ResNet18 on UTKFace. More information on the data sets,\nhyperparameters, and data augmentations used to train the original and retrained models is provided\nin the appendix B. We construct the forget set by sampling 10% of $D$.\n\nThe performance of the MU methods can change across datasets, model configurations, and model\ninitializations; a reliable MU method remains consistent across these changes. For each method,\ndataset and model combination, we unlearn from Original models initialized using 10 different seeds\nand consider the average performance across seeds.\n\nA further observation is that prior research tends to compare MU methods with default hyperparame-\nters, potentially leading to a less competitive performance of the method. To ensure that we get the\nbest performance out of each method, we perform three hyperparameter sweeps to find the best set of\nhyperparameters for each method. To ensure a fair comparison, we use same number of searches for\neach method. Each hyper-parameter sweep uses 100 trials to minimize four loss functions: Retain\nLoss ($L_{Retain}$), Forget Loss ($L_{Forget}$), Val Loss ($L_{Val}$), and Val MIA ($L_{Val-MIA}$) given by\n\n$L_{Retain} = \\alpha \\times |RA(f_U) - RA(f_R)|,  L_{Forget} = \\beta \\times |FA(f_U) - FA(f_R)|,$\n$L_{Val} = \\gamma \\times |VA(f_U) - VA(f_R)|, L_{Val-MIA} = \\eta \\times Disc(D_V, f_U),$\n\nwhere the $L_{Retain}$ captures the divergence in accuracy between the retrained and unlearned model\nover the $D_R$, $L_{Forget}$ and $L_{Val}$ capture the divergence over $D_F$, $D_V$ respectively and $L_{Val-MIA}$ cap-\ntures whether the loss distributions over $D_F$ and $D_V$ are distinguishable from one another via the\ndiscernibility score defined in Section 2. We set $\\alpha = \\beta = \\gamma = \\frac{1}{4}$ and $\\eta = 1$ as we found these\nvalues to balance the importance of importance retention and the resilience to Membership Inference\nAttacks. Per unlearn method, we use the hyperparameter configuration that minimises the four loss\nterms when evaluating the method. Thus, for each unlearning method, we first unlearn 300 models to\ndo the hyper-parameter sweep, then unlearn 10 models with the best set of hyper-parameters, leading\nto 5, 580 per dataset for a given architecture, leading to a total of 50, 220 for the 9 dataset / model\ncombinations.\n\nRanking. A challenge in the comparison of MU method performance comes from the potential\nproximity of the evaluation metrics. As a simple example, suppose we have four methods $U_1, ..., U_4$\nwith accuracies: 98%, 99%, 50%, 1%, respectively; if we simply rank the methods, the rank itself\nwould not be representative of the fact that e.g. $U_1$ and $U_2$ are much above $U_3$ and $U_4$. In order to\nenable distinctions based on proximities, we use Agglomerative Clustering and define cut-off points\nsuch that we obtain three clusters: (1) Best performers (G1), (2) Average performers (G2), and (3)\nWorst performers (G3). If a method does not produce 10 usable models, one per original model, it is\nassigned to a Failed group (F). For each method, we count the number of times it appears in each\nof the three groups (with nine being the maximum). To obtain a final ranking of the methods, we\nfirst rank the methods using the number of times it appears in the Best Performers group (G1); if ties\noccur, we use the Average Performers (G2) group to break them. If ties persist, the Worst Performers\n(G3) group serves as the final tie-breaker. This method ensures a clear and fair ranking by considering\neach performance group in order of importance."}, {"title": "Main Results", "content": "Table 1 presents the main results of our evaluations on MU methods based on Retention Deviation\nand Indiscernibility. The results for the run-time efficiency are shown in Table 2."}, {"title": "Discussion and Conclusion", "content": "The increasing focus on data privacy and trustworthiness of machine learning models underscores the\nneed for robust and practical methods to unlearn and remove the influence of specific data from trained\nmodels. Due to the growing size of models, we require methods that avoid the computationally costly\nretraining from scratch. In this work we performed a comprehensive comparison of approximate\nunlearning methods across various models and datasets aimed to address this critical issue.\n\nWe experimentally compared 18 methods across different datasets and architectures, focusing on\nassessing the method's ability to maintain privacy and accuracy while being computationally efficient\nand reliable across datasets, architectures and random seeds. Our findings indicate that Masked-Small-\nGradients, which accumulates gradients via gradient descent on the data to remember and gradient\nascent on the data to forget to determine which weights to update, consistently outperforms for all\nmetrics across the studied datasets, architectures, and initialization seeds. Similarly, Convolution\nTranspose, which leverages the simple transposition in convolutional layers, performed strongly.\n\nBoth CT and MSG were resistant against both a population-based Membership Inference Attack\n(MIA) and a stronger, per-sample attack (U-LiRA). However, a core challenge of approximate\nunlearning is that these methods will only be as strong as the attacks against which they are tested. As\nstronger and more complex attacks emerge, some approximate unlearning methods might no longer\nbe as efficient as initially expected. This highlights the need for continuous evaluation and adaptation\nof unlearning methods to maintain their effectiveness. We also conducted experiments based on\nL2 distances, but found that no method consistently got close to the reference models' weights, we\nprovide further information in Appendix G.\n\nLimitations. Due to computational costs, we limited our analysis to Tiny Vision Transformers and\nResNet; a further investigation of other architectures could provide useful insights. We did not\ninvestigate different amounts of unlearning samples, which some methods are known to be sensitive\nto [37]. We did not consider repeated deletion, instead we assume that there is a single forget set and\nthat the unlearning process happens once, as is common in the literature, nonetheless, in practical\napplications one might need to unlearn different smaller forget sets over time and some unlearning\nmethods might not work well under such scenario. We finally remark once again on the difficulty\nof evaluation for approximate unlearning [31]: while these methods provide significant gains in\nefficiency, novel attacks might highlight yet unknown weaknesses of the unlearning processes.\n\nFuture work. First, we put our focus on natural image data, however, machine unlearning is relevant\nto other data types such as medical images or other modalities such as time series, audio and speech,\nor language data. Second, we focus on the classification task, however, other learning tasks would\ngreatly benefit from machine unlearning too. For instance removing concepts from generative models\nfor images [20] or poisoned data in language models [31]. Third, this work focuses on empirically\nbenchmarking approximate machine unlearning methods. We do not provide a theoretical analysis of\nthese methods or a rigorous comparison with exact unlearning algorithms.\n\nImpact statement. This paper aims to highlight the importance of effectively assessing approximate\nmachine unlearning methods. Our goal is to stress the need for evaluating new unlearning methods\nagainst more reliable baselines and experimental setups. Additionally, it is crucial to assess the\nconsistency of a new unlearning method across various datasets and model architectures. Without\nsuch a thorough evaluations, proposed unlearning methods may provide a false sense of privacy and\nsafety, ultimately limiting their effectiveness for data regulation."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the fields of Machine Learning and Machine\nUnlearning. There are many potential societal consequences of our work. We aim to raise awareness"}, {"title": "Related Works", "content": null, "subsections": [{"title": "Machine Unlearning", "content": "Machine Unlearning is often first associated with the work from Cao et al. [11], followed by Bourtoule\net al. [9], which proposes SISA (Sharded, Isolated, Sliced, Aggregated) as an exact unlearning method.\nFor a recent overview of MU, we refer to the survey from Xu et al. [50], which provides a taxonomy of\ncommon unlearning methods. Furthermore, Zhang et al. [52] review MU through privacy-preserving\nand security lenses. The authors cover the Confidentiality, Integrity, and Availability security triad\nand the need for Data Lineage, which relates to following the movement of data in a machine learning\npipeline and understand from where it originates, where it is stored, and how it percolates in the\nsystem through transformation. Some might have information on common MU verification methods,\nprivacy evaluation metrics, and datasets, we defer to the work of Nguyen et al. [39], and Shaik et\net al. [43].\n\nIn the following, we focus on the MU taxonomy from Xu et al. [50], which considers Data Reorgani-\nzation and Model Manipulation:\n(1) Data Reorganization methods focus on directly modifying the data to perform unlearning. It is\ndivided into Data Obfuscation, Data Pruning, and Data Replacement.\n\nData Obfuscation refers to modifying the dataset to obscure the influence of the data to be unlearned:\nrandom relabeling and retraining [28], SRL (Successive Random Labels), and Saliency Unlearning\n(SalUN).\n\nData Pruning usually relies on dividing the dataset into multiple sub-datasets and training sub-models\non these subsets. This is the category to which SISA [9] relates. Our work does not consider methods\nassociated with this setting as they assume the training process.\n\nData Replacement attempts to unlearn by replacing the original dataset with transformed data that\nsimplifies unlearning specific samples. For instance, Cao et al. [11], replace the training data with\nsummations of efficiently computable transformations. Like data pruning, these methods tend to\nmake strong assumptions about the training process.\n(2) Model Manipulation methods directly adjust the model parameters to remove the influence of\nspecific data points. Model manipulation is divided into Model Shifting, Model Replacement, and\nModel Pruning.\n\nModel Shifting directly updates the model parameters to offset the influence of the unlearned samples,\nsuch as using a single step of Newton's method on model parameters [30] or decremental updates\n[40], in our benchmark Fisher Forgetting (FF), Influence Unlearning (IU), and SalUN would represent\nthese approaches.\n\nModel Replacement uses pre-calculated parameters that do not reflect the data to forget to replace\nparts of the trained model. For instance, when using decision trees, one can replace nodes affected by\nthe forget set by pre-calculated node [41].\n\nThese methods often make strong assumptions about the training process and the overall model.\n\nModel Pruning prunes specific parameters from the trained models to remove the influence of certain\nsamples [34] or Prune-Reinitialize-Match-Quantize (PRMQ) [4] which prunes the model via L1\npruning, reinitializes parts of the model then train the model on DR."}, {"title": "Machine Unlearning for Deep Neural Networks", "content": "Initially, MU research primarily focused on linear models such as linear regression and logistic\nmodels. Such models allow for the design of methods that assume the convexity of the loss function,\nrendering them less practical for DNN-based approaches. Since DNNs can memorize parts of their\ntraining data, they are particularly relevant targets for MU, even more so when they have been trained\non large amounts of potentially personal data. For Deep Learning models, unlearning raises additional\nchallenges: 1) the non-convexity of the loss function of Deep Neural Networks [15], 2) the size of\nthe models inducing large computational costs, 3) the randomness coming from the model's training\nprocess, such as the initialization seed, randomness in the mini-batch generation process, and 4) the"}, {"title": "Post-Hoc Machine Unlearning", "content": "While proactively designing deep-learning pipelines with built-in unlearning methods such as SISA\ncan greatly simplify the unlearning process, many contemporary services relying on DNNs were\nnot deployed with unlearning in mind. This motivates searching for methods that can unlearn from\nalready trained models without making assumptions about the training process.\n\nThus, we focus on post-hoc MU, a scenario where we assume that the unlearning method is agnostic\nto the original training process of the model. Under such a scenario, differences exist in terms of\ndata availability at unlearning time. For instance, whether one has access to the original training\ndata D, the retain set DR, the forget set DF, or even some external set such as the validation set DV.\nTherefore, careful consideration should be given to the data requirement associated with an unlearning\nmethod. Indeed, some might require having access to both DR and DF at the unlearning time, while\nothers assume that DR is no longer available [17] making them more practical in real-world scenarios.\nThroughout our benchmark, we make the same assumption as the NeurIPS2023 Unlearning Challenge\n[45], where the unlearning methods had access to fo, DR, DF, DV"}, {"title": "Machine Unlearning and Differential Privacy", "content": "We based our Unlearning definition on Sekhari et al. [42] and refer to their work on the distinction\nbetween Differential Privacy and the objective of Machine Unlearning. Differential privacy, in a\nhigh-level picture, is a method for publicly sharing aggregated information about a population by\ndescribing the patterns discovered among the groups within the dataset while withholding specific\ninformation about individual data points. A randomized algorithm A is ($\\epsilon$, $\\delta$)-differentially private if\nfor all datasets $D_1$ and $D_2$ that differ on a single data point, and all $S \\subset Range(A)$,\n\n$Pr[A(D_1) \\in S] \\le e^{\\epsilon} \\cdot Pr[A(D_2) \\in S] + \\delta.$\n\nIn this definition, $\\epsilon$ (epsilon) is a non-negative parameter that measures the privacy loss, with smaller\nvalues indicating stronger privacy. The parameter $\\delta$ represents the probability of breaking differential\nprivacy, ideally close to or equal to 0.\n\nDespite enabling provable error guarantees for Unlearning methods, Differential Privacy requires\nstrong model and algorithmic assumptions, making MU, derived from it, potentially less effective\nagainst practical adversaries [34]."}]}, {"title": "Datasets", "content": "CIFAR-10 CIFAR-10 is a widely used dataset in computer vision and machine learning. It\ncomprises 60,000 32x32 color images in 10 different classes, with 6,000 images per class. The dataset\nis divided into 50,000 training images and 10,000 testing images. CIFAR-10 represents a diverse\nrange of everyday objects, such as airplanes, automobiles, birds, and cats, making it a challenging\ntask for image classification. The simplicity of the images combined with the variety of categories\nmakes CIFAR-10 a suitable dataset to test the efficacy of machine unlearning algorithms in effectively\nunlearning information without compromising the model's performance on the remaining data.\n\nData Augmentations: random cropping to 32x32 with 4-pixel padding, 50% random horizontal flip-\nping, and per-channel normalization with a mean of [0.4919, 0.4822, 0.4465] and standard deviation\nof [0.2023, 0.1994, 0.2010]. At test time, we resize to 32x32 and normalize."}, {"title": "Neural Network Architectures", "content": "We consider two families, ResNet (Residual Network) [32] and ViT (Vision Transformer) [19], which\nare prominent architectures in computer vision. We consider ResNet18 and a TinyViT[48] with\napproximately 11M learnable parameters for a fair comparison between two fundamentally different\narchitectures. This provides insights into how architectural differences impact the unlearning process\nand helps understand the trade-offs between convolutional and transformer-based models regarding\nreliability and computational efficiency.\n\nResNet: ResNet18 Introduced by He et al. [32], it facilitates the training of deep networks through\nshortcut connections, which mitigates the problem of vanishing gradients. The ResNet18 is known\nfor its balance between performance and computational efficiency."}, {"title": "Privacy Evaluation", "content": null, "subsections": [{"title": "Unlearning-Membership Inference Attack (U-MIA)", "content": "A common approach to evaluate the quality of unlearning methods is to attack the unlearned models\nwith a form of Membership inference Attack (MIA). Membership Inference Attacks attempt to\ndetermine whether a specific data point was part of the model train data. The efficacy of the\nMembership Inference Attack has been used as a metric to evaluate the success of unlearning\nalgorithms. A general approach to such an attack is as follows. Assume $f_\\theta$ is a trained model with\nparameters $\\theta$, and let $L$ be a loss function, such as the cross-entropy loss. Then, compute the losses\nfor each sample from two sets of data $A$ and $B$ (of equal size) and train a binary classification model\nsuch as logistic regression with labels $y = 1$ for points $i$ in $A$ and $y = 0$ for points $i$ in $B$. An\naccuracy score from the classifier close to 1.0 indicates that the classifier can perfectly distinguish\nbetween samples from $A$ and $B$ based on the loss values. A score of 0.5 indicates that the ability to\ndistinguish is close to random."}, {"title": "Unlearning -Likelihood Ratio Attack (U-LIRA)", "content": "The performance of a general MIA can be improved by considering, e.g., a per-sample attack such as\nLIRA [13, 31]. For any given point, we wish to determine whether the outputs from the unlearned\nmodels differ from those of models that have never seen the data point. To assess the attack robustly,\nwe evaluate it across multiple models, using shadow models trained on various retain/forget sets.\nSpecifically, we first train n models based on n splits of the training data. This train data is then\nsplit into 10 random retain and forget splits, and hence, we unlearn a total of 10n models. We then\nperform hyper-parameter sweeps, similar to what we do in the original results and unlearn using the\noptimal hyper-parameters, except that we consider sweeps and conduct 200 trials per sweep to\ndetermine the best hyper-parameters. In our setting, we set n = 64."}]}, {"title": "Per dataset results", "content": "Here, we present the results for both ResNet18 and the TinyViT across datasets.\n\nResNet18 We provide the tables with Retain Accuracy (RA), Forget Accuracy (FA), Test Accuracy\n(TA), Retain Retention (RR), Forget Retention (FR), Test Retention (TR), Performance Retention\nDeviation (RetDev), Indiscinerbility concerning the Test Set (Indisc), U-MIA on the Test set (T-MIA)\nand RunTime Efficiency (RTE) for every dataset using the ResNet18 model on MNIST (Table 4),\nFashionMNIST (Table 5), CIFAR-10 (Table 6), CIFAR-100 (Table 7) and UTKFace (Table 8). In"}, {"title": "Per architectures rankings", "content": "Here, we present the rankings across datasets for ResNet18 and TinyVit. We\nnote that some methods, such as RNI or NG+, are less efficient on the ViT architectures regarding\nIndiscernibility. However, methods such as SCRUB are less efficient regarding Retention Deviation\non the ViT architecture."}, {"title": "L2 Distances between model weights", "content": "The distance between the Unlearned and Retrained models has also been considered in the literature\nto evaluate MU. Nevertheless, we observe that models end up at a similar distance to the Retrained\nmodel, with significant differences in performance. We further note that one challenging aspect of\nthe L2 distance comparison is the different factors of Weight Decay used by the MU method. The\nhyper-parameter searches determine these Weight Decay factors, which can significantly vary from\none unlearning method to another, making it challenging to compare methods. Furthermore, the\nbest-performing method, MSG, is usually at the same distance as both the Original and Retrained\nmodel. For each method, for each initialization seed, we computed the L2 distance between the\nunlearned model fu and the retrained model fr, as well as between the fu and fo (Figure 2).\n\nAlthough having the same weight as the Retrained model would indicate that the unlearned model has\nunlearned DF, our evaluations show that distance to the Retrained model might not be an adequate\nevaluation metric for MU."}, {"title": "Requirements", "content": "We ran the experiments on compute clusters with different capacities. Nonetheless, each method was\ntested on devices with the same specifications when recording run times: 1 NVIDIA L4 24GB GPU\nand 4 Intel(R) Xeon(R) CPU @ 2.20GHz."}]}