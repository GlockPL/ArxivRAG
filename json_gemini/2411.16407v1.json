{"title": "A Study on Unsupervised Domain Adaptation for Semantic Segmentation in the Era of Vision-Language Models", "authors": ["Manuel Schwonberg", "Claus Werner", "Hanno Gottschalk", "Carsten Meyer"], "abstract": "Despite the recent progress in deep learning based computer vision, domain shifts are still one of the major challenges. Semantic segmentation for autonomous driving faces a wide range of domain shifts, e.g. caused by changing weather conditions, new geolocations and the frequent use of synthetic data in model training. Unsupervised domain adaptation (UDA) methods have emerged which adapt a model to a new target domain by only using unlabeled data of that domain. The variety of UDA methods is large but all of them use ImageNet pre-trained models. Recently, vision-language models have demonstrated strong generalization capabilities which may facilitate domain adaptation. We show that simply replacing the encoder of existing UDA methods like DACS by a vision-language pre-trained encoder can result in significant performance improvements of up to 10.0% mIoU on the GTA5\u2192Cityscapes domain shift. For the generalization performance to unseen domains, the newly employed vision-language pre-trained encoder provides a gain of up to 13.7% mIoU across three unseen datasets. However, we find that not all UDA methods can be easily paired with the new encoder and that the UDA performance does not always likewise transfer into generalization performance. Finally, we perform our experiments on an adverse weather condition domain shift to further verify our findings on a pure real-to-real domain shift.", "sections": [{"title": "1 Introduction", "content": "Computer vision has experienced several breakthroughs in the past decade enabled by deep neural networks (DNNs) [11, 18, 30, 44]. Domain shifts, i.e. when the training and inference distribution differ, are still a major challenge for DNNs and can cause severe performance drops, e.g. when models are trained on synthetic data and inference is done on real data [21,\n54, 60]. These can significantly hamper the application especially to safety-critical areas like autonomous driving and medical image analysis. Consequently, the mitigation of domain shifts is a major objective and several research fields emerged.\nThe most popular field is unsupervised domain adaptation (UDA) where only unlabeled samples from the target domain are available and the objective is to adapt the model towards this specific target domain. A broad variety of UDA methods have been developed in the past years utilizing methods like adversarial adaptation [60, 61], contrastive learning [28, 67, 72], self-training [59, 75] and knowledge distillation frameworks [21, 55, 71]. Next to unsupervised domain adaptation so-called domain generalization (DG) methods gained increasing research attention [4, 7, 13, 25, 29, 32, 41, 53, 58, 70, 73]. Here, no target data is available and the objective is to obtain a model which generalizes well across multiple unseen target domains. Very recently, the utilization of vision-language models (VLMs) enabled a major performance increase as well as methodological progress in the field of domain generalization with works like Rein [66], CLOUDS [2], and VLTSeg [26]. These approaches demonstrate that large-scale vision-language pre-training like CLIP [44] can be leveraged to significantly improve domain generalized semantic segmentation as well as detection [63] performance. Surprisingly, the field of UDA research falls back behind DG research because so far all existing UDA methods use ImageNet pre-trained models leaving the strong potential of vision-language models unexplored except a very recent study from Englert et al. [12]. For this reason, we equip selected UDA methods with a state-of-the-art vision-language pre-trained encoder and show their strong potential for unsupervised domain adaptation. We also evaluate the domain generalization capabilities of the UDA methods on unseen datasets and show that a simple UDA method with a vision-language pre-trained backbone provides state-of-the-art generalization capabilities. Next to the established synthetic-to-real and real-to-real benchmarks which contain a mixture of different domain shifts (e.g. GTA5\u2192 Cityscapes contains synthetic and geolocation shift) we evaluate both the adaptation and generalization performance for a single pure adverse weather condition shift on the ACDC [50] dataset. This is motivated by the results from Sakaridis et al. [50] that UDA methods perform very different on different domain shifts and in some cases worsen the performance. Overall, our study makes the following contributions and novel findings:\n\u2022 Extensive evaluation of UDA methods, in particular DACS [59], equipped with a state-of-the-art vision-language pre-trained backbone demonstrating that this can boost the UDA target performance by 10.0% mIoU and across three unseen datasets by 13.7% mIoU\n\u2022 Analysis of UDA methods revealing that not all methods are similarly compatible with a VLM-based encoder; this indicates the need for new UDA methods tailored towards vision-language models\n\u2022 Extensive evaluation on unseen datasets showing that the UDA and generalization performance are not necessarily correlated and that recent DG methods can provide better generalization than UDA methods"}, {"title": "2 Related Work", "content": "Unsupervised Domain Adaptation (UDA) Methods There exists a broad variety of UDA methods which can be coarsely clustered into input, feature, output space and hybrid adap-"}, {"title": "3 Method", "content": "In this section, we describe the details of our study, focusing on the evaluated UDA methods,\nthe model architectures and the domain shifts of the ACDC dataset."}, {"title": "3.1 UDA Methods", "content": "In contrast to Englert et al. [12] we decide to not only focus on the most recent and usu-ally more complex UDA methods, but deliberately also include previous methods. That shows how previous UDA methods and principles like adversarial adaptation benefit from the new vision-language pre-trained backbone and how they perform on a pure domain shift. We select previous, highly influential UDA methods: AdaptSegNet [60], ADVENT [64] and DACS [59]. In addition, we include the recent state-of-the-art methods SePiCo [67], DAFormer [21], and MIC [24]. AdaptSegNet [60] is one of the earliest UDA works and utilizes adversarial domain adaptation by employing a domain discriminator in both the feature and output space. Similar to AdaptSegNet, ADVENT [64] applies adversarial learning and self-training on the entropy maps of the output space. DACS [59] is an easy-to-apply method which is incorporated by several subsequent UDA methods, combining input space cross domain image mixing and adaptive self-training. SePiCo [67] as one of the current state-of-the-art methods proposes multiple contrastive losses along with a teacher-student framework to align the source and target domains. DAFormer [21] was the first work using a vision transformer backbone for UDA and applied self-training, rare class sampling and an ImageNet feature distance loss to preserve ImageNet knowledge. We include both the initial DAFormer method and its follow-up approach MIC [24] with HRDA [22]."}, {"title": "3.2 UDA Architectures", "content": "Encoder & Initialization For the encoder choice, we follow recent domain generalization approaches [26, 66] and employ the EVA02-CLIP-L-14 vision encoder [57] which has shown strong generalization capabilities for segmentation. EVA02-CLIP [57] relies on a sequence of CLIP and masked image modeling pre-training. Note that we only use the vision encoder of the pre-training and refer to it as EVA02-L. H\u00fcmmer et al. [26] demonstrated the strong generalization capabilities of the EVA02-CLIP encoder which makes it a natural candidate for our study. Both Wei et al. [66] and Englert et al. [12] focused on DINOv2 [42] pre-trained weights as their initialization. We are not using DINOv2 pre-training in our study since the Cityscapes dataset, which is one of our main target domains, is used to sample the pre-training dataset of DINOv2, reducing the significance of evaluations. Moreover, we include the established UDA architectures DeepLabv2 with a ResNet-101 backbone [5] and the DAFormer architecture with a MiT-B5 backbone [21] as it is common practice in UDA and DG benchmarking [21, 23, 41, 67, 72].\nDecoder We employ an ASPP-based decoder with different dilation rates from the DAFormer architecture [21]. The ASPP-decoder receives multi-level features from different levels of the encoder and performs up-sampling to obtain a common size of the feature maps in case of a hierarchical encoder like a ResNet-101 or a MiT-B5. When using the EVA02-L encoder this upsamling has no effect since the encoder is non-hierarchical."}, {"title": "3.3 Domain Shift Datasets", "content": "Most of the real-to-real domain shifts for benchmarking are a mixture of at least two differ-ent domain shifts like Cityscapes\u2192 ACDC and Cityscapes DarkZurich. Those benchmarks contain a geolocation shift, a weather/condition shift and also have been recorded with different sensor setups. In contrast, we aim to evaluate UDA methods in scenarios which ex-clusively cover only a single, well defined domain shift, e.g. only a geolocation or only an"}, {"title": "3.4 Experimental Settings", "content": "Implementation Details All experiments are based on the open source framework MMSeg-mentation [8] and were conducted on a single A100 GPU with 80GB memory. The crop resolution for all experiments was fixed to 512\u00d7512 except for MIC [24], where a 1024\u00d71024 resolution was used. The number of training iterations was set to 40k as common practice and a batch size of four was used. For ADVENT and AdaptSegNet with a ResNet-101 backbone the SGD optimizer with a learning rate of 2.5e \u2013 03 was used. In all other cases, the AdamW [35] optimizer was selected in line with previous approaches [21, 26, 66]. For the MiT-B5 backbone a learning rate of 6e \u2013 05 and for the EVA02-L encoder of 1e \u2013 05 was used. Hyperparameters specific to the respective UDA methods were set as given by the authors without change.\nMetric As the evaluation metric we use the mean intersection over union (mIoU) averaged across 19 classes which are shared among all synthetic and real datasets. Only for SYNTHIA the mIoU across 13 classes is reported as common standard in UDA [55, 64, 67, 72]."}, {"title": "4 Results", "content": "In this section, we show the results and start with the evaluation of the UDA performance with vision-language pre-training followed by the domain generalization evaluation. All results obtained with the vision-language pre-trained encoder EVA-02-CLIP will be high-lighted with this green color."}, {"title": "4.1 UDA with vision-language pre-training", "content": "We equipped four of the selected UDA methods with the vision-language pre-trained EVA02-L encoder and compared it to the current two standard architectures ResNet-101 and MiT-B5, both initialized with ImageNet pre-trained weights. We could not include the combination of MIC and EVA02-CLIP in our study, but future work should investigate this combination."}, {"title": "4.2 Domain generalization of UDA methods", "content": "The domain generalization performance of UDA approaches to entirely unseen domains is rarely evaluated but highly relevant because the adaptation to e.g. a certain real target domain should intuitively also improve the generalization to other unseen target domains. For this reason, we evaluate the domain generalization performance across different backbones following the same protocol as pure DG approaches [14, 26, 41] and compare it with state-of-the-art DG approaches similar to [43]."}, {"title": "4.3 Discussion", "content": "We did not apply any changes to the UDA methods like e.g. changing hyperparameters, a different resolution or disabling the FD-loss. This may have improved the performance of the UDA methods. However, in contrast to Englert et al. [12] our aim was to evaluate the UDA methods without any changes to assess how well they transfer to a different domain shift and a new encoder architecture with vision-language initialization. Modifying existing UDA methods may not be trivial because removing or adapting certain components will likely influence the performance and behavior."}, {"title": "5 Conclusion", "content": "We equipped existing UDA methods with a state-of-the-art vision-language pre-trained encoder and studied the target performance and the generalization to unseen domains. The results demonstrate the potential of vision-language pre-training for UDA by reaching a competitive target domain performance with a simple UDA method. They also indicate strong generalization capabilities for both established benchmarks and a pure \u2192 adverse weather condition domain shift based on ACDC. We show that recent state-of-the-art UDA methods rely on a loss function which cannot be directly used for the vision-language pre-trained encoder. Our results indicate that similar to domain generalization new UDA methods are required to fully exploit the potential of vision-language models for UDA. Our domain generalization evaluations showed two novel findings. First, the target domain performance is not necessarily an indicator for their generalization capabilities and second, that recent, pure DG methods are performing in parts similarly or even superior than UDA methods."}]}