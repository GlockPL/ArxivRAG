{"title": "ADAPT-\u221e: SCALABLE LIFELONG MULTIMODAL INSTRUCTION TUNING VIA DYNAMIC DATA SELECTION", "authors": ["Adyasha Maharana", "Jaehong Yoon", "Tianlong Chen", "Mohit Bansal"], "abstract": "Visual instruction datasets from various distributors are released at different times\nand often contain a significant number of semantically redundant text-image pairs,\ndepending on their task compositions (i.e., skills) or reference sources. This\nredundancy greatly limits the efficient deployment of lifelong adaptable multi-\nmodal large language models, hindering their ability to refine existing skills and\nacquire new competencies over time. To address this, we reframe the problem\nof Lifelong Instruction Tuning (LiIT) via data selection, where the model auto-\nmatically selects beneficial samples to learn from earlier and new datasets based\non the current state of acquired knowledge in the model. Based on empirical\nanalyses that show that selecting the best data subset using a static importance\nmeasure is often ineffective for multi-task datasets with evolving distributions,\nwe propose Adapt-\u221e, a new multi-way and adaptive data selection approach that\ndynamically balances sample efficiency and effectiveness during LiIT. We first\nconstruct pseudo-skill clusters by grouping gradient-based sample vectors. Next,\nwe select the best-performing data selector for each skill cluster from a pool of\nselector experts, including our newly proposed scoring function, Image Ground-\ning score. This data selector samples a subset of the most important samples from\neach skill cluster for training. To prevent the continuous increase in the size of the\ndataset pool during LiIT, which would result in excessive computation, we fur-\nther introduce a cluster-wise permanent data pruning strategy to remove the most\nsemantically redundant samples from each cluster, keeping computational require-\nments manageable. We validate the effectiveness and efficiency of Adapt-\u221e over\na sequence of various multimodal instruction tuning datasets with various tasks,\nincluding (Knowledge) VQA, multilingual, grounding, reasoning, language-only,\nand multi-image comprehension tasks. Training with samples selected by Adapt-\nalleviates catastrophic forgetting, especially for rare tasks, and promotes for-\nward transfer across the continuum using only a fraction of the original datasets.", "sections": [{"title": "INTRODUCTION", "content": "Multimodal instruction tuning (Liu et al., 2023b; Zhang et al., 2023a; Liu et al., 2023a; Gan et al.,\n2024; Yoon et al., 2024) has been actively explored to enhance visual reasoning or the generation\nability of Multimodal Large Language Models (MLLMs) (Zhu et al., 2023; Li et al., 2023a; Tang\net al., 2023; Team et al., 2023; Munasinghe et al., 2023; Yu et al., 2024; Zhang et al., 2024) following\nuser intents by training models on human or machine-generated multi-task visual instruction tuning\ndatasets (Li et al., 2023c; Yin et al., 2023; Xu et al., 2024; Chen et al., 2024). While many distributors\ncontinue to release new high-quality instruction-tuning tasks and datasets, continually adapting large\nmodels to these massive datasets over time is prohibitively costly and inefficient. Given a pre-trained\nMLLM and the continuous expansion of the dataset pool with a stream of instruction-tuning datasets,\nas commonly observed in the research community today, the challenge lies in developing an ever-\nevolving, instruction-following MLLM in the most data- and computation-efficient manner. This\nresearch question poses a realistic, sustainable instruction tuning scenario for MLLMs, distinct from"}, {"title": "2 RELATED WORK", "content": "Multimodal Instruction Tuning Datasets. While multimodal data, such as image-text pairs, has\nincreased significantly, multimodal instruction-following data remains relatively scarce due to the\ntime-consuming nature of human data collection. To address this, recent works (Liu et al., 2023b;\nZhang et al., 2023a; Chen et al., 2023; Zhu et al., 2023; He et al., 2024a) have leveraged generative\nmodels to collect such data from existing image datasets. Li et al. (2023c) introduce the M3IT\ndataset with 2.4 million instances and 400 task instructions, translated into 80 languages. Xu et al.\n(2024) develop VISION-FLAN, a large-scale dataset of 187 tasks with expert-written instructions,\nensuring diversity through iterative refinement. MultiInstruct (Xu et al., 2023) features 62 tasks\nacross 10 categories, sourced from 21 open datasets.\nContinual Instruction Tuning. In the era of multimodal LLMs, instructional datasets have raised\nseveral timely research problems. Therefore, it is crucial to develop sustainable models that can ad-\ndress emerging data and real-world challenges. Inspired by continual learning (Van de Ven & Tolias,\n2019; Srinivasan et al., 2022; Lee et al., 2024b), a paradigm focused on enabling models to adapt\nto non-i.i.d., time-variant tasks, continual instruction tuning (CIT) (Chen et al., 2024; Zhang et al.,\n2023b) has recently been studied for (multimodal) LLMs that allows the model to adapt to multiple\ninstruction tuning datasets sequentially without costly retraining. KPIG (He et al., 2024b) introduces\na new CIT method that helps LLMs capture task-specific information and avoid overfitting general\ninstructions by computing key-part information gain on masked parts to replay data and refine train-\ning dynamically. EProj (He et al., 2023) and Fwd-Prompt (Zheng et al., 2024) expand the CIT to\nthe training of large multimodal models. EProj introduces new regularization and model expansion\nmethods based on task correlations for continual instruction tuning of LMMs. Fwd-Prompt proposes\na prompt-based approach that projects the prompt gradient into the residual space to minimize task\ninterference while utilizing pre-trained knowledge, reducing negative forward transfer."}, {"title": "3 LIMITATIONS OF SCORE-BASED DATA SELECTION IN LIFELONG MULTIMODAL INSTRUCTION TUNING", "content": "3.1 LOCALITY OF THE SAMPLE IMPORTANCE: DATA SELECTION DEPENDS ON THE DATA\nScore-based selection methods are widely used to assess the importance of training samples across\nmodalities (Zheng et al., 2023; Liu et al., 2024; Marion et al., 2023; Evans et al., 2024; Gadre et al.,\n2024). We analyze two importance scores in multimodal instruction tuning: the EL2N score (Paul\net al., 2021) and entropy score (Coleman et al.). EL2N measures the L2-norm of the output error\nvector, while entropy reflects the uncertainty in the output probabilities. Using the M3IT dataset (Li\net al., 2023b), which includes eight tasks: captioning, multilingual (Chinese), classification, gener-\nation, knowledge VQA (kvqa), reasoning, videoQA, and VQA, we compute score distributions. As\nshown in Figure 2A, relying on a single importance score metric, such as EL2N or entropy, is in-\nsufficient to differentiate meaningful samples across a diverse range of tasks. For instance, selecting\nhigher EL2N scores tends to favor for generalization (Paul et al., 2021), captioning samples over\nkvqa, leading to a skewed dataset.\nIn addition, the effectiveness of different importance scores varies based on the task and dataset at\nhand. The perplexity score is effective for filtering out low-quality samples in VQA that generally\noccur in the tail end of its distribution (see Figure 2C). Tasks such as kvqa (in purple) and captioning\n(in blue) are more separable via their entropy scores than EL2N scores. Moreover, Zheng et al.\n(2023); Swayamdipta et al. (2020) show that the most effective training subset contains a balanced\nmix of easy, difficult, and ambiguous samples. A biased score estimator may assign higher or lower\nscores to too many samples, making it hard to select the most effective subset. Thus, we need a"}, {"title": "3.2 IMPORTANCE OF VISION-LANGUAGE DATA WITH IMAGE GROUNDING SCORE", "content": "The perplexity score measures the likelihood of a given sequence of tokens as predicted by an\nautoregressive generative model and has been used for selecting samples with higher instruction\nfollowing difficulty in language datasets (Li et al., 2024). For MLLMs, the perplexity function\nadditionally conditions on the input image tokens within the model. A lower perplexity score implies\nthat the model assigns a higher probability to the output tokens. We compute the perplexity of a\nmultimodal data instance zi for an MLLM with weights 0 as:\n$PPL(z_i) = exp(\\frac{1}{\\sum_{e_j \\in z_i}}NLL(e_j)),$\nwhere $NLL(e_j) = -log(P(e_j|e_{<j}, I; \\theta))$.\nwhere $NLL(e_j)$ indicates the negative log-likelihood of token $e_j$ in the sequence $z_i$ comprising of\nimage $I$ and tokens $e$. As discussed in the previous section, perplexity is useful for detecting low-\nquality multimodal samples (see Figure 2C top). Motivated by the effectiveness and generalizability\nof the perplexity score (Marion et al., 2023; Li et al., 2024), we further modify this scoring func-\ntion to distill the importance of the image in a multimodal data instance. We compute the image\ngrounding score of a multimodal data instance zi with image I and tokens e as:\n$IG(z_i) = \\frac{PPL(e)}{PPL(e, I)}$\nA higher IG score is assigned when the model assigns a higher probability to the text when condi-\ntioned on the image, compared to when the image is absent. Conversely, a lower IG score indicates\nthat the image has little to no effect on the text's probability. As shown in Figure 2C bottom, an\nimage-query pair with a higher IG score requires the model to carefully understand the visual scene\n(e.g., reading the text on a sign in the image). In contrast, examples where the model can predict the\nanswer without seeing the images represent lower IG scores. Thus, the IG scoring function allows\nus to discard multimodal samples that do not leverage the multimodal functionality of MLLMs."}, {"title": "4 LIFELONG MULTIMODAL INSTRUCTION TUNING VIA MULTI-WAY DATA SELECTION", "content": "4.1 PROBLEM STATEMENT\nThis paper tackles the problem of LiIT over a sequence of multiple large datasets. Let $D_0, ..., D_{T\u22121}$\nbe a set of accessible datasets where $D_t = \\{x_i, p_i\\}_{i=1}^{N_t}$ denotes the dataset released in the timestep\nt, composed of $N_t$ image-text pairs. Formally, we aim to train a multimodal model over multiple\nobserved datasets for a given computational budget, such as FLOPs or training iterations. Given the\nmodel f parameterized by 0, the training objective at time step T is formulated as follows:\n$arg \\min_{\\theta} \\frac{1}{T} \\sum_{t=0}^{T-1} \\frac{1}{\\tilde{N}_t} \\sum_{i=0}^{\\tilde{N}_t -1} L(f_{\\theta}(x_i), \\hat{y}_i)$ s.t. $T (N_t-1) \\leq \\tau,$\nwhere $\\tilde{N}_t = r(N_t, T, \\tau) \\in N, \\tilde{N}_t < N_t$, and \u03c4 is the computational budget. r(\u00b7) denotes a decay\nfunction conditioned on T and \u03c4, and $\\hat{y}_i$ indicates the ground truth answer corresponding to the\ninput data sample. Here, we constrain the minibatch iterations for multimodal instruction tuning per\ntraining timestep, by subsampling $\\tilde{D_t} = \\{x_i, p_i\\}_{i=1}^{\\tilde{N}_t}$, where $\\tilde{D_t} \\subseteq D_t, D_t \\sim P(\\tilde{D_t} | D_t, T, \\tau)$. When\na new multimodal instruction tuning dataset $D_T$ is released, $\\{D_t\\}_{t=0}^{T-1}$ is (re-) drawn for finetuning \u03b8.\nWe propose the Adapt-\u221e data selection method for the problem of lifelong multimodal instruction\ntuning and describe each of its steps in detail in the following sections."}, {"title": "4.2 PSEUDO-TASK CLUSTERING VIA GRADIENTS", "content": "In the LiIT scenario, the model continuously updates its weights to incorporate new knowledge\nand refine its capabilities in specific tasks by training on unseen, meaningful data. The relative"}, {"title": "4.3 ENTROPY-BASED MULTI-WAY DATA SELECTION", "content": "As discussed in Section 3.1, different scoring functions provide distinct advantages when select-\ning meaningful samples from a wide range of vision-language instruction tuning tasks, such as\nin-domain VQA, open-ended VQA, multilingual QA, visual grounding, commonsense reasoning,\nscientific/physical understanding, and OCR. To address the limitations of relying on a single data\nselection (pruning) method and promote collaborative decision-making when evaluating sample im-\nportance, based on the model's current state and data diversity, we propose a novel and versatile,\nmulti-way data pruning strategy. First, we construct a function pool S = {$s_0(\u00b7)$, ..., $s_{N\u22121}(\u00b7)$} where\n$s_n(\u00b7)$ denotes n-th sample selection operation based on the corresponding importance score func-\ntion. Here, we aim to identify the function \u015d that maximizes the entropy of the distribution of scores\nover the samples of each pseudo-task cluster. For the k-th cluster Ck with m samples, we first ex-\ntract the corresponding scores for each $s_n$ using model weights @ at time step t. We approximate the\ndistribution of scores, $P_h$, by binning the range of normalized scores and calculating densities $p_h$\nover the $B^{(k)}$bins. Omitting cluster index k for brevity, the selection of \u015d is formulated as:\n$\\hat{s}^{(k)} = arg \\max_{s_n} H (P_h^{(k)})$, where $P = \\{p_h^{(k)}(x)\\}$, $\\forall b \\in B^{(k)}$ and $x \\in C_k$.\nA score function that yields higher average entropy in its distribution compared to other functions\nindicates a better ability to assess the uncertainty of the system (i.e., the model @ at timestep t).\nMoreover, we employ the CCS (Zheng et al., 2023) sampling strategy that aims to select a balanced\nrepresentation of seen and unseen samples as well as frequent and rare tasks by sampling uniformly\nover the range of a score function. Hence, it is even more imperative to use a score function that\nis discriminative over its entire range. Since the resulting pseudo-task clusters may vary in size,\nwe define a training budget T and divide it uniformly over the k clusters. The leftover data budget\nfrom clusters with sizes $|c_i|$ smaller than T/k are equally distributed over other clusters. This results\nin the selection of a skill-balanced subset from the pool of multi-task datasets in lifelong learning.\nImportantly, our proposed multi-way approach is highly flexible since the scoring function pool can\nbe seamlessly extended with new scoring functions based on users' needs."}, {"title": "4.4 COMBINED PERMANENT-TEMPORAL DATA PRUNING BY REMOVING REDUNDANCY", "content": "The steps of Adapt-\u221e outlined in the previous sections enable the model to effectively select the\nmost beneficial samples for each pseudo-task (i.e., skill) and train on them adaptively, based on\nthe current model and evolving dataset distributions. However, as the data pool grows with the\nrelease of new instruction-tuning datasets, the computational burden of updating gradient-based\nsample vectors and ranking their importance increases. To keep the computational requirements\nmanageable over time, we further implement a permanent data pruning strategy to iteratively reduce\nthe size of the entire dataset pool. At the end of training at each timestep, we measure pairwise\ncosine similarities between samples within each cluster and prune those with maximum similarities,\nas they are highly redundant and contain overlapping knowledge. The similarity is computed in\nthe semantic space which is well represented using hidden layer outputs of the model as shown in"}, {"title": "5 EXPERIMENTS", "content": "5.1 EXPERIMENTAL SETUP\nModel. We conduct our experiments using the LLaVA 1.5 multimodal large language model (Liu\net al., 2023a). It is trained on top of the Vicuna LLM (Chiang et al., 2023) using a corpus of approx-\nimately 600K image-text caption pairs for pretaining of the vision projectors from the pre-trained\nCLIP visual encoder ViT-L/14 (Radford et al., 2021). Further, it is trained on the LLaVA instruc-\ntion tuning dataset consisting of 665K text-only and vision-language instances. We adopt LORA\nfinetuning (Hu et al., 2021) of the LLaVA-1.5-7B model with the recommended hyperparameters.\nDatasets. For training, in addition to the LLaVA-665K instruction tuning dataset at t = 0, we\nconsider the following order of datasets: M3IT (Li et al., 2023c), MiniGPT4 (Zhu et al., 2023),\nMANTIS (Jiang et al., 2024), LAMM (Yin et al., 2023) and VisionFLAN (Xu et al., 2024). Each\ndataset's temporal order, size, and skill composition are summarized in Table 1. We select standard\nevaluation datasets to measure performance on the skills enumerated in Table 1. These datasets and\ntheir corresponding task-specific evaluation metrics are listed in Table 4.\nMetrics. We report various evaluation metrics from existing literature designed for understand-\ning the continual learning phenomena in machine learning. The Average Accuracy (acc) at final\ntimestep t = T is the average of the model's performance across all skills (and across datasets\nwithin each skill). The Relative Gain (r) metric (Scialom et al., 2022) is the average of skill\nperformances as a % of the respective upper bounds i.e., $r_T = \\frac{\\sum_{s=1}^{S} p_t}{\\sum_{s=1}^S upper \\: bounds} \\times 100%$.\nWe consider the best performances in each skill group in the sequential learning setting to be the\nupper bound in performances and report r for the final time step T. We also report the Forget-\nting Rate(f) which is the % drop in performance averaged across all skills and timesteps i.e.,\n$f = \\sum_{t=1}^T \\frac{\\sum_{s=1}^S min(p_t-p_{t-1},0)}{p_{t-1}} \\times 100%$.\nAdapt-x Setup. The optimal value of k in the pseudo-task clustering step is computed from a grid\nsearch over values of k between 5 and 50, and selected absed on the WSS value of clusters. In the\nscore-based sample selection step, we use a bin size of 50 and discard the top and bottom 5% of\nsamples for computing entropy as well as for CCS sampling, to remove outliers, low-quality sam-\nples, and uninformative data (Zheng et al., 2023). We use perplexity, image grounding (Section 3.2),\nEL2N (Paul et al., 2021) and entropy (Coleman et al.) score functions for S throughout the paper.\nBaselines. We present baseline numbers on (1) Sequential and Multi-task training, (2) Random\nselection for experience replay (10% of past datasets), (3) Score-based selection methods, including\nRandom, EL2N (Paul et al., 2021), Entropy (Coleman et al.), Perplexity (Marion et al., 2023),\nand (3) recent competitive data pruning baselines: SemDeDup (Abbas et al., 2023), Density-based\nPruning (Abbas et al., 2024), and COINCIDE (Lee et al., 2024a). See Appendix for details."}, {"title": "5.2 MAIN RESULTS", "content": "We present the main experimental results in Table 2 and show a breakdown of skill-wise accuracies\nat each time step for various methods in Figure 3. Our main findings are as follows:\nSequential training leads to catastrophic forgetting of skills. Multiple skills learned at t=0\n(LLaVA) are forgotten at t=1,2 upon training the model on datasets containing a different set of\nskills. The M3IT dataset (t=1) does not contain grounding tasks and results in large drops in per-\nformance for the same. Similarly, the MiniGPT4 dataset (t=2) predominantly contains high-quality\ncaptions and causes forgetting of all other skills. The MANTIS dataset (t=3) improves performance\non the MMMU dataset because it contains training instances with documents, charts, and visualiza-\ntion images. At t=4, the LAMM dataset contains object detection and keypoint detection tasks that\npromote recovery of the referring comprehension skill learned at t = 0, presumably due to a sim-\nilarity in their non-textual output modalities. Finally, VisionFLAN (t=5) recovers performance on\nall skills except grounding and MMMU. Surprisingly, VisionFLAN also induces recovery of multi-\nlingual and unimodal skills (MMLU) despite not containing those tasks in its dataset composition.\nThis method incurs nearly 26% forgetting across all timesteps and retains only 68% of its all-time\nhigh performances at the final timestep (see Table 2).\nRandom experience replay and pruning alleviate forgetting. Random experience replay using\n10% data from past datasets significantly improves skills retention over sequential training, bringing"}, {"title": "6 ANALYSIS", "content": "Relative gains per skill. We present the skill-wise breakdown of relative gains of each method\ndiscussed in Table 2, in Figure 5A. The largest increases are observed for the language-only skill"}, {"title": "7 CONCLUSION", "content": "Valuable visual instruction tuning datasets from various sources are released over time and often\ncontain overlapping text-image pairs. To efficiently train lifelong adaptive MLLMs on these growing\ndatasets, a scenario we call Lifelong Instruction Tuning (LiIT), we reformulate data selection so\nthe model automatically selects meaningful samples from both old and new datasets, maintaining\nbalance when incorporating new data. We observe that assessing sample informativeness with a\nstatic importance measure is challenging in LiIT, as it depends on the model's evolving capabilities\nand the shifting dataset distribution. To address this, we propose a scalable lifelong multimodal\ninstruction tuning approach that dynamically balances sample efficiency and effectiveness through\ntemporal multi-way data pruning. We show that training with samples selected by this method\nreduces catastrophic forgetting and enhances forward gain, using only a fraction of the original\ndataset, particularly for rare tasks with limited resources."}, {"title": "A BASELINES", "content": "Sequential Training. In this method, the model is naively trained on the stream of instruction\ntuning datasets without any experience replay. Generally, this method sets a lower bound on the\nperformance of the model on each evaluation task.\nMulti-task Training. This method comprises pooling all of the datasets across time steps and\ntraining the model on this pooled dataset in one go. Generally, this method sets an upper bound on\nthe performance of the model on various skill sets. However, if there are low-resource tasks in the\ndataset, it can lead to low performance on those tasks.\nCoverage-based Coreset Selection (CCS). Zheng et al. (2023) introduces the method CCS where\nthey divide a range of difficulty scores into equal-sized bins and randomly data samples from each\nbin with a uniform budget. This approach is motivated by maximizing coverage over the semantic\nspace while ensuring an equal distribution of easy and difficult samples in the selected subset. Easy\nsamples promote learning whereas difficult samples are information-dense and promote generaliza-\ntion. We use this method in Adapt-\u221e for score-based selection.\nScore-based Selection. Multiple importance score functions have been proposed over the years\nfor various data types. We select the following for baseline experiments: (1) Perplexity: This metric\nis widely used for filtering language corpora (Marion et al., 2023), (2) EL2N (Paul et al., 2021):\nThis metric is the L2-norm of the output error vector and is effective at low pruning ratios (or high\nretention rates), (3) Entropy (Coleman et al.): This score function is the entropy value of the output\nprobability vector.\nEmbedding-based Selection. SemDeDup (Abbas et al., 2023) extracts semantic embeddings for\npertaining datasets using a universal embedding transformer such as CLIP (Radford et al., 2021) for\nimage-text pairs or Sentence Transformer for natural language corpora and performs deduplication\nwithin k-means clusters. DBP (Abbas et al., 2024) assigns pruning budget to the clusters in SemD-\neDup using a cluster complexity score. COINCIDE (Lee et al., 2024a) clusters feature vectors into a\nlarge number of cluster e.g., k=10,000 to identify skill-concept clusters and samples non-uniformly\nfrom the clusters using a difficulty score metric."}, {"title": "B EXPERIMENTAL SETUP", "content": "Additional Details on Adapt-\u221e Setup. We use random projections (Park et al., 2023; Xia et al.,\n2024) to reduce the dimensionality of gradient vectors extracted for the pseudo-task clustering step.\nWe use a constant projection dimension of 8192 throughout our experiments."}]}