{"title": "MotiF: Making Text Count in Image Animation with Motion Focal Loss", "authors": ["Shijie Wang", "Samaneh Azadi", "Rohit Girdhar", "Saketh Rambhatla", "Chen Sun", "Xi Yin", "Gopal Meta"], "abstract": "Text-Image-to-Video (TI2V) generation aims to generate a video from an image following a text description, which is also referred to as text-guided image animation. Most existing methods struggle to generate videos that align well with the text prompts, particularly when motion is specified. To overcome this limitation, we introduce MotiF, a simple yet effective approach that directs the model's learning to the regions with more motion, thereby improving the text alignment and motion generation. We use optical flow to generate a motion heatmap and weight the loss according to the intensity of the motion. This modified objective leads to noticeable improvements and complements existing methods that utilize motion priors as model inputs. Additionally, due to the lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V Bench, a dataset consists of 320 image-text pairs for robust evaluation. We present a human evaluation protocol that asks the annotators to select an overall preference between two videos followed by their justifications. Through a comprehensive evaluation, MotiF outperforms nine open-sourced models, achieving an average preference of 72%. The TI2V Bench is released in the project website.", "sections": [{"title": "1. Introduction", "content": "From creating short clips of moving MNIST digits [27] to producing a minute-long high-definition video featuring a stylish walking woman [5], Text-to-Video (T2V) generation [3, 18, 30, 37] has progressed remarkably over the past decade. Meanwhile, video generation with additional conditions [15, 22, 31, 43\u201345] has also gained considerable attention. In this paper, we focus on Text-Image-to-Video (TI2V) generation, a task first introduced in [19] aiming to generate a video based on an initial image and a text description [26, 46, 50, 53]. Our goal is to enhance text adherence in open-domain TI2V generation. Commonly known as Image-to-Video (I2V) generation [50, 53] or image animation [25, 46], this task often overlooks the text guidance. In many cases, text merely describes the image or video [13, 21], or is unnecessary [6, 23, 49], underscoring the need for focused improvement.\nThe main challenge in TI2V modeling is that while the image offers a strong spatial signal to learn the video content, the model must rely mainly on the text to infer motion. When given an image and multiple different prompts, most prior works generate videos with limited motion. Zhao et al. [53] identified this issue, termed as conditional image leakage, where the model overly relies on the image condition. To address this issue, some studies propose to reduce the strength of the image condition by adding noise or masking the image [26, 53]. Others seek to derive more motion priors to facilitate motion learning [10, 51]. These methods all emphasis on enhancing the input signals, hoping that the model can learn to leverage them implicitly. In contrast, we propose to improve the training objective to enable the model to focus on the regions with more motion explicitly, which is an orthogonal direction to prior works and can be easily combined with existing techniques.\nWe hypothesize that the model's difficulty in following motion-based instructions may stem from insufficient attention to motion patterns. As shown in Figure 1 (a), in a video with a static background, 97% of the pixels remain unchanged over time, while only 3% exhibit meaningful motion. This subtle motion pattern might be neglected by the model especially considering that all regions are treated equally in the L2 loss. Motivated by this, we introduce Motion Focal loss (MotiF), a term inspired from the focal loss in dense object detection [35]. Specifically, we first use optical flow [42] to create a motion heatmap that represents the motion intensities of the video. We then use the motion heatmap to assign loss weight for the video to focus on regions with more motion. Additionally, we analyze commonly used image conditioning mechanisms and observe that concatenating the conditioning image with the diffusion input works best for both image and text alignment.\nBeyond training, evaluating TI2V generation is also challenging due to the lack of a suitable evaluation set and reliable metrics. Existing evaluation sets are limited, they are either too small [51] or the text is used as a description of the starting image rather than specifying the intended motion for animation [13]. Therefore, we introduce a new benchmark, TI2V Bench, that consists of 320 image-text pairs covering 22 different scenarios. Each scenario includes 3 to 5 images with similar contents in different styles, and 3 to 5 prompts to animate these images to produce different motion. We intentionally include challenging scenarios such as novel object introduction and fine-grained object references. We use the Emu model [11], publicly available through meta. ai, to generate the initial images.\nPrevious studies have employed various automatic metrics to assess different quality aspects, such as video quality, motion intensity, and text alignment. We observed that these metrics may not always align with human perception. Following Movie Gen [30], we mainly rely on human evaluation with A-B testing to evaluate the performance. Our human evaluation protocol is motivated by the JUICE protocol [15], where annotators are asked to indicate an overall preference and justify their choices across several axes, including image alignment, text alignment, object motion, and overall quality. Through our comprehensive human evaluations, we show that MotiF enhances TI2V generation capabilities, notably improving text alignment and motion quality. In summary, we make the following contributions:\n\u2022 We present Motion Focal Loss (MotiF) that encourages TI2V generation to concentrate on regions with larger motion, and is complementary to existing techniques.\n\u2022 We present a new benchmark, TI2V Bench, including synthetic images and texts that cover a wide range of complex scenarios, as well as a human evaluation protocol designed to assess TI2V generation performance.\n\u2022 Through comprehensive comparisons with nine previous methods, we demonstrate the effectiveness of MotiF, achieving an average preference of 72%."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Text-Image-to-Video Generation", "content": "With the rapid advancements in Text-to-Image (T2I) and Text-to-Video (T2V) generation, TI2V generation has also gained significant attention. Although training-free methods [28, 49] exist that utilize pretrained T2I or T2V models with inference optimization to enable zero-shot TI2V generation, we focus on approaches that require training, especially on the most widely adapted latent diffusion models [1, 37, 54]. The techniques in prior TI2V generation methods can be mainly grouped into two categories: 1) design novel architectures to integrate the image condition, 2) derive motion priors to improve motion learning. Our approach falls into the second category.\nImage Condition Integration. Most recent works use ei-"}, {"title": "2.2. Video Generation Evaluation", "content": "Benchmarks and Automatic Metrics. Make-a-Video is the first to report FVD/IS on the UCF-101 dataset [41] and FID/CLIP similarities on the MSR-VTT dataset [47] under the zero-shot setting, which is followed by later works in both T2V [1, 4, 14, 54] and TI2V generation [25, 33, 46, 53]. Re-purposing existing video understanding datasets for TI2V evaluation is undesirable due to their unconstrained data collection procedures, which may not capture the nu-"}, {"title": "3. Approach", "content": "Given an image \\(x_i\\) and a text prompt \\(c\\), TI2V task aims to generate an L-frame video \\(x = \\{x_0, x_1, ..., x_L\\}\\) that follows the text description. In this work, we specifically tackle a common scenario when the image \\(x_i\\) is the first frame (i.e. \\(i = 0\\)). The generated video should maintain visual coherence with the starting image and produce motion that is driven by the text description. In this section, we present the details of our model architecture, the proposed motion focal loss, and the procedure of generating the motion heatmaps."}, {"title": "3.1. Preliminaries", "content": "Video Diffusion Models. Diffusion models [17, 38, 40] are probabilistic generative models designed to create images and videos from random Gaussian noise. In the forward pass, a diffusion process gradually transforms a data sample \\(x_0 \\sim P_{data}(x)\\) into Gaussian noise \\(x_T \\sim N(0, I)\\) over \\(T\\) timesteps by sequentially adding noise according to \\(q(x_t | x_{t-1})\\). The diffusion model learns the reverse denoising process, where it iteratively removes noise to reconstruct the data, predicting a less noisy \\(x_{t-1}\\) from \\(x_t\\)."}, {"title": "3.2. Modeling", "content": "As shown in Figure 2, different from prior works that all focused on incorporating more motion priors into the model, and thus may require additional inputs during inference, we focus on improving the training objective to explicitly focus on the motion learning during training.\nMotion Focal Loss. The diffusion loss treats all the latents equally across the spatial and temporal dimensions. However, the motion is not evenly distributed across the video. It's common that most of the video stays static while only a small region has meaningful motion. As a result, successive frames tend to closely resemble the initial frame, meaning that a \"still video\", i.e. duplicating the initial frame, can yield a relatively low loss. To address this, we introduce the motion focal loss, \\(\\mathcal{L}_{motif}\\), to explicitly focus on high-motion regions during training.\nWe begin first by generating a motion heatmap \\(m \\in \\mathbb{R}^{L \\times H \\times W}\\) for each video, where each entry \\(m[l, h, w] \\in [0, 1]\\) represents the motion intensity at position \\([h, w]\\) in the \\(l\\)-th frame of the input video \\(x\\). The motion heatmap \\(m\\) is down-sampled to \\(m' \\in [\\mathbb{R}^{L \\times H' \\times W'}\\) to align with video latents \\(z\\). The motion focal loss is defined as:\n\\begin{equation}\n\\mathcal{L}_{motif} = \\mathbb{E}_{t, x \\sim P_{data}, \\epsilon \\sim N(0, 1)} ||m' \\cdot (\\epsilon - \\epsilon_{\\theta}(z_t, c, t)) ||_2.   \n\\end{equation}\nThe model is trained with a joint loss with \\(\\lambda\\) scaling the motion focal loss relative to the diffusion loss:\n\\begin{equation}\n\\mathcal{L} = \\mathcal{L}_{diffusion} + \\lambda \\mathcal{L}_{motif}   \n\\end{equation}\nMotion Heatmaps. The use of motion heatmaps is to enhance the model's ability to generate motion by concentrating on areas with significant activity. Although there may be several methods to achieve this, we first explore using optical flow to create these motion heatmaps. To generate the motion heatmap \\(m_l\\) for the \\(l\\)-th frame \\(x_l\\), we first compute the optical flow intensity \\(f_l\\) between \\(x_l\\) and the subsequent"}, {"title": "4. TI2V Bench", "content": ""}, {"title": "4.1. Evaluation Set", "content": "While it's relatively easy to collect a diverse set of prompts for T2V evaluation, evaluating TI2V is not straightforward as it requires a conditional image. As shown in Table 1, recent TI2V benchmarks can be classified into three categories: 1) video-text pairs; 2) image-text pairs with realistic images; 3) image-text pairs with synthetic images. Re-purposing existing video understanding datasets [2, 41, 47] for TI2V evaluation is non ideal, as these datasets assume only one possible outcome from the initial frame, and the text descriptions typically do not emphasize the intended motion. Existing images are also limited in their diversity and potential motion changes. Thus, we resort to image-text pairs with synthetic images to curate TI2V Bench. AIGCBench [13] and Animate Bench [51] are most relevant to our work. However, the texts in AIGCBench are used to generate the images but not meant to describe the motion in the videos, which is critical for TI2V task. Animate Bench employs motion-based text descriptions, but its evaluation set is relatively small and limited, where the images often feature a single object or scene, and the text description lacks diversity.\nOur goal is to curate an evaluation set that has a diverse set of images, and each image has a diverse set of prompts"}, {"title": "4.2. Human Evaluation", "content": "Most T2V and TI2V works conduct human evaluation to assess the overall quality and the text alignment of the generated videos using various metrics. The issue with evaluating separate, independent questions is that mixed results make it challenging to reach a clear conclusion. Therefore, we aim to have a single metric to evaluate the performance while allow more detailed examination of different quality aspects for further analysis, following the JUICE protocol [15].\nSpecifically, each A-B comparison consists of two questions. We intentionally do not have the equal option so annotators are forced to choose the best one and justify their choices. First, annotators are asked to indicate their overall preference between two videos in the context of text-guided image animation. Second, they are required to justify their choice based on four different aspects: 1) object motion (not only camera movement); 2) alignment with the text prompt; 3) alignment with the starting image; 4) overall quality. Annotators can select any combinations of these aspects. The overall preference is used to indicate the model's performance in TI2V generation, which we refer to as TI2V score. This evaluation protocol allows us to generate a single metric to draw conclusions for A-B comparisons while enabling a more detailed analysis."}, {"title": "5. Experiment", "content": ""}, {"title": "5.1. Implementation Details", "content": "Model Architecture. We build our model based on a pretrained T2V model, VideoCrafter2 [8] (@512 resolution). This model uses CLIP as the text embedding, a Variational Auto-Encoder (VAE) as the video encoder and decoder, and a spacetime-factorized U-Net as the denoising model. We concatenate the condition image with the noised video and expand the first embedding layer, similar to prior works.\nTraining and Inference. We use an internal licensed dataset of 1M video-text pairs that is similar to [46] for training. Videos are center-cropped and sampled at 320 \u00d7 512 resolution of 16 frames with a dynamic frame stride ranging from 1 to 6 following [46]. The model is optimized in the v-prediction mode [36] by a combination of the diffusion loss and the motion focal loss with (\\(\\lambda = 1\\)). The optical flow intensity is normalized using a sigmoid-like function \\(\\sigma(x) = 1/(1 + e^{100(0.05 - x)})\\), which generates a continu-"}, {"title": "5.2. Comparison to Prior Works", "content": "We compare to nine open-sourced TI2V generation methods. To the best of our knowledge, this is the most comprehensive TI2V human evaluation. We ensure fair comparisons by following each method's pre-processing and post-processing pipeline and ensure the generated videos are natural after resizing to the same resolution (320 \u00d7 320). The results are shown in Figure 4. Our method wins by a considerable margin to prior works. From the justification selections, the main reasons for our method to win are on the object motion and text alignment, which is exactly the motivation of MotiF to improve text-driven motion learning."}, {"title": "6. Conclusions", "content": "In this paper, we focus on the often overlooked problem of text alignment in text-guided image animation. We hypothesis that the reason prior works struggle to follow the text prompts is because the model may not pay attention to the motion patterns during training. Thus, we present MotiF to guide the model's learning on regions with more motion. In addition, we curate a challenging benchmark to evaluate TI2V generation. Although MotiF have shown considerable advantages over prior works, it is still limited in generating high quality videos with coherence motion in challenging scenarios when there are multiple objects or new object is expected to enter the scene. We hope this work will attract more attention in solving this challenging problem."}]}