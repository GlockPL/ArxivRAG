{"title": "BATAYAN: A Filipino NLP benchmark for evaluating\nLarge Language Models", "authors": ["Jann Railey Montalan", "Jimson Paulo Layacan", "David Demitri Africa", "Richell Isaiah Flores", "Michael T. Lopez II", "Theresa Denise Magsajo", "Anjanette Cayabyab", "William Chandra Tjhi"], "abstract": "Recent advances in large language models\n(LLMs) have demonstrated remarkable\ncapabilities on widely benchmarked\nhigh-resource languages; however, linguistic\nnuances of under-resourced languages remain\nunexplored. We introduce BATAYAN, a holistic\nFilipino benchmark designed to systematically\nevaluate LLMs across three key natural\nlanguage processing (NLP) competencies:\nunderstanding, reasoning, and generation.\nBATAYAN consolidates eight tasks, covering\nboth Tagalog and code-switched Taglish\nutterances. Our rigorous, native-speaker-driven\nannotation process ensures fluency and\nauthenticity to the complex morphological and\nsyntactic structures of Filipino, alleviating\na pervasive translationese bias in existing\nFilipino corpora. We report empirical\nresults on a variety of multilingual LLMs,\nhighlighting significant performance gaps that\nsignal the under-representation of Filipino\nin pretraining corpora, the unique hurdles\nin modeling Filipino's rich morphology and\nconstruction, and the importance of explicit\nFilipino language support and instruction\ntuning. Moreover, we discuss the practical\nchallenges encountered in dataset construction\nand\npropose principled solutions for\nbuilding culturally and linguistically-faithful\nresources in under-represented languages.\nWe also provide a public benchmark and\nleaderboard as a clear foundation for iterative,\ncommunity-driven progress in Filipino NLP.", "sections": [{"title": "Introduction", "content": "Spurred on by recent advances in computing\npower, big data, and machine learning, LLMs have\ncome into widespread use due to the emergence\nof a variety of novel and useful capabilities at\nscale (Hadi et al., 2023). These capabilities have\nmade user applications based on LLMs some\nof the fastest-growing consumer applications in\nhuman history, but have also rendered previous\nbenchmarks insufficiently difficult and diverse\n(Wey, 2024; Yang et al., 2023).\nBenchmarks are standard datasets used to\nmeasure and compare the performance of models\nagainst one another. In particular, the increasingly\ngeneral capabilities of LLMs have necessitated\nholistic benchmarks which test a diversity of\nmetrics like fairness, truthfulness, and robustness,\nas well as a variety of tasks like text summarization,\ncasual reasoning, and translation (Yang et al., 2023;\nGuo et al., 2023; Liu et al., 2023). The vast majority\nof LLM benchmarks evaluate tasks in English, with\nnon-English works being much fewer (Liu et al.,\n2021; Son et al., 2024).\nFilipino, despite being the national language\nof the Philippines and being spoken by over\n80 million people, remains an under-resourced\nlanguage. It is under-represented in multilingual\nLLMs, and existing corpora are domain-specific,\nnon-multilingual (Cajote et al., 2024), and\ncreated by non-native speakers (Quakenbush,\n2005; Dita et al., 2009). Specifically for LLM\nbenchmarks, Filipino is either excluded from\nmultilingual benchmarks, suffer from limited\ntask diversity, or have serious deficiencies\nin grammatical correctness, completeness, and\ndiversity (Bandarkar et al., 2024; NLLB Team et al.,\n2022).\nMoreover, Filipino is a complex language which\nexhibits a highly complex linguistic structure,\nparticularly in its rich morphological system\n(Ramos, 2021; Go and Nocon, 2017). Its\nagglutinative nature allows for extensive use of\naffixation and creating nuance through prefixes,\ninfixes, suffixes, and circumfixes (Archibald and\nO'Grady, 2001; Jubilado, 2004). These affixes,\ncombined with root words, allow intricate verb\nconjugations for marking tense, focus, and mood\n(Zamar, 2022). Filipino incorporates elements\nfrom a wide array of linguistic influences, such\nas Spanish (Bowen, 1971; Wolff, 2001), Chinese\n(Gonzales, 2022; Reid, 2018; Chan-Yap, 1980),\nand Malay (Wardana et al., 2022; Baklanova, 2017),\nwith codeswitching between English and Filipino\nbeing common (Bautista, 1991, 2004).\nOur threefold contributions attempt to address\nthese challenges. First, we present BATAYAN,\u00b9 a\nholistic Filipino benchmark for evaluating large\nlanguage models across 8 distinct tasks spanning\nnatural language understanding, reasoning, and\ngeneration. We place a rigorous emphasis on\nauthenticity to natural Filipino language use\nthrough native speaker translation and annotation,\naddressing the limitations of existing Filipino\ndatasets. BATAYAN is released as part of\nSEA-HELM\u00b2, a leaderboard for comprehensive\nevaluation of LLMs across linguistic and reasoning\ntasks for Southeast Asian languages. Second, we\nprovide extensive evaluation results from testing\n8 prominent LLMs on BATAYAN, revealing\nsignificant disparities in model performance\nacross different linguistic capabilities in Filipino.\nThird, we document systematic challenges\nand methodological considerations in creating\nhigh-quality Filipino NLP datasets, particularly\nhighlighting issues in translation fluency,\nvocabulary adaptation, and the preservation of\nFilipino's rich morphological features. These\ninsights provide a practical framework for future\ndevelopment of Filipino language resources."}, {"title": "Considerations for Filipino Evaluations", "content": "While Filipino is the official language of the\nPhilippines, it has been argued in literature that\nits de facto lingua franca is Taglish, the practice of\ncode-switching between English and Tagalog (Go\nand Gustilo, 2013). Naturally-occurring datasets\nin Filipino (e.g., those mined from social media,\nrecorded interviews) contain some code-switching\n(Bautista, 2004). We defer the problematization\nof the debated difference between Tagalog and\nFilipino, and use the two terms interchangeably.\nStandard Tagalog, while text-rich, is considered\nunder-resourced, lacking the linguistic data, tools,\nand resources for effective natural language\nprocessing (Miranda, 2023b). To address\ndata scarcity, developers take advantage of\nhigh-resource languages such as English through\ntranslations (Goyal et al., 2022; Doddapaneni\net al., 2023). Furthermore, leveraging multilingual\ndatasets is potent not only because it enhances the\nperformance of models trained on limited resources\nbut also because code-switching and bilingualism\nis a common phenomena in the Philippines (Tupas\nand Martin, 2017).\nTaglish is characterized to possess\ncommunicative efficiency. Compared to\nStandard Tagalog or English, Taglish, provides\na more convenient way of communicating a\nmessage (Bautista, 2004). For this consideration,\napplication of NLP becomes more helpful as users\nare more likely to interact freely when using a\nlanguage familiar to them."}, {"title": "Language Design Principles", "content": "The design of BATAYAN adhered to principles\nthat emphasize linguistic authenticity and\nrepresentativeness in word choice, sentence\nstructure, and grammar.\nIn terms of language use and word choice, we\nemployed common Filipino and Taglish vocabulary,\nbalancing loanwords with native Filipino terms by\nprioritizing colloquial usage. This was also done\nwhen words had both English and Filipino variants,\ntaking into account spelling and orthographic\npreferences prevalent among native speakers.\nRegarding sentence structure, we prioritized\nsentences with attention to natural syntax and\nthe choice of ayos (sentence arrangement),\nnamely direct (karaniwang ayos, KA, lit. usual\norder) or inverted (di-karaniwang ayos, DKA,\nlit.\nunusual order) forms (Tanawan et al.,\n2008). In KA, Filipino constructions follow\nthe typical predicate-initial word order (Malicsi,\n2013). In contrast, DKA, which is also referred\nto linguistically as an ay-inversion, is a type of"}, {"title": "Task and Dataset Curation", "content": "Previous work on Filipino language model\nevaluation has been largely fragmented. While\nprior studies have made important contributions in\nassessing large-scale language models (Cruz and\nCheng, 2020, 2022), and various researchers have\ndeveloped task-specific datasets for named entity\nrecognition (Miranda, 2023a), sentiment analysis\n(Villavicencio et al., 2021), and other isolated tasks,\nthe field lacks a unified, comprehensive benchmark\nfor systematic evaluation of model capabilities\nacross different linguistic dimensions.\nThrough BATAYAN, we aim to significantly\nexpand the scope of Filipino language evaluation\nby introducing tasks that assess a gamut\nof linguistic capabilities. Specifically, we\nfocus on integrating more challenging Natural\nLanguage Understanding (NLU) tasks alongside\nNatural Language Reasoning (NLR) tasks, while\nintroducing novel Natural Language Generation\n(NLG) tasks previously unexplored in the Filipino\ncontext. This comprehensive approach allows for\na more thorough assessment of models' ability\nin Filipino given its rich morphology and unique\nlinguistic characteristics.\nOur benchmark design and selection of tasks are\ninformed by established multilingual evaluation\nframeworks, particularly BHASA (Leong et al.,\n2023), which provides systematic evaluation across\na number of Southeast Asian languages. We\nalso draw insights from widely-adopted language\nbenchmarks such as the XTREME multilingual\nbenchmark (Hu et al., 2020) and the IndoNLU\nIndonesian benchmark (Wilie et al., 2020), which\nhave demonstrated the importance of testing\nvarious aspects of linguistic competence.\nFor this study, we carefully selected eight tasks\nthat span three key competencies of NLP. For NLU:\nParaphrase Identification (PI), Question Answering\n(QA), Sentiment Analysis (SA), Toxicity Detection\n(TD). For NLR: Causal Reasoning (CR), Natural\nLanguage Inference (NLI). For NLG: Abstractive\nSummarization (AS), Machine Translation (MT).\nThese tasks are detailed in Appendix B."}, {"title": "Dataset Collection and Annotation", "content": "To construct BATAYAN, we curated open-source\ncorpora with clear provenance when possible,\nprioritizing datasets that exhibit authentic language\nuse across various domains such as social media,\nnews articles, and other publicly available texts.\nWe identified existing Filipino-language datasets\nfor MT, QA, and TD. For the SA task, we chose\nto repurpose an existing dataset and annotate them\nfor the aforementioned task.\nTasks such as AS, CR, NLI, and PI lacked\na native Filipino corpus. As such, we first\nidentified existing English corpora, generated\ninitial translations using automatatic translation\ntools, then conducted a manual review and revision\nto generate high-quality and fluent translations.\nThe authors\u00b3 acted as annotators and raters for this\nstudy. They were grouped into teams of three and\nperformed translations and reviews, focusing on\ncultural and linguistic relevance to ensure that the\ntranslated texts resonate with Filipino contexts.\nFor each task, we randomly sampled 5n entries\nfor a target size of n, maintaining balanced\nclass distributions. Entries were filtered by\nlength (20\u20132000 characters) and quality (<50%\ngrammatical errors). Afterwards, each sample"}, {"title": "Issues with Developing a Filipino\nBenchmark", "content": "This section discusses issues in creating BATAYAN,\nsplit into datasets that required adaptation and\ndatasets where the content were kept as-is. The\npurpose of this section is to shed light on issues\nthat future researchers may encounter in creating\nnew Filipino datasets or adapting previous ones."}, {"title": "Issues with Adapted Data", "content": "Most of the tasks presented in this study were\nsourced from existing English language datasets\nthat were natively translated and adapted into\nFilipino by the authors, whom are all native\nspeakers of Filipino.\nCreating more relevant summaries for\nXL-Sum. Previous work has noted that a\nsignificant portion of reference summaries in the\nXL-Sum dataset is highly abstractive, demonstrates\nfactual errors, or contains information not\nmentioned in the provided articles (Guo et al.,\n2022), putting to question its factuality and\nvalidity. To address this, we developed new\nFilipino summaries for each article. We ensured\nrelevance and fluency by including only the\nmost important information that can be directly\nlifted from the article using natural-sounding and\ngrammatically-correct constructions.\nLimitations in adapting vocabulary. Adapting\nthe English tasks to Filipino revealed issues in\nmaintaining the intelligibility of the text. Technical\nterms that exist in English were difficult to translate\ninto Filipino, with several of them having no\ndirect translations. For example, an instance\nfrom Belebele included the term \"rule of thirds\",\nwhich describes a specific photography technique.\nOriginally, the translation of this phrase in the\nTagalog subset of Belebele was \u201ctuntunin ng mga\nsangkatlo\u201d (lit. \"rule of thirds\"), which is does\nnot make sense in Filipino and does not convey\nthe intended meaning. For jargon such as this, we\nchose to keep the original English phrasing.\nEnglish idiomatic expressions were also\nprevalent and required a more flexible approach\nin adaptation to ensure the integrity of meaning."}, {"title": "(Dis)fluency in expert and machine\ntranslations.", "content": "Our rigorous review and\nannotation of existing datasets with machine\nand expert-guided translations in Filipino revealed\nweaknesses in their adaptation. The authors\nfound that the automatic translations were\nunnatural, disfluent, and showed characteristics\nof translationese (Gellerstam, 1986; Riley et al.,\n2020) despite being grammatically correct.\nNotably, we observed that these initial translations\ndemonstrated an unusual preference with using\nay-inversion. We also observed that these\ntranslations used Filipino terms that were\nsynonymous to their English counterparts but\npragmatically-awkward. Hence, we applied\nrephrasing and re-translation to ensure that the\nsamples used sound more native and natural.\nFor example, one passage from the English\nsubset of Belebele was, \u201cThe CCTV would\ncertainly send a strong signal...\u201d The original\nTagalog (machine) translation was, \u201cAng CCTV\nay tiyak na magpapadala ng malakas na hudyat...\u201d\nThe term \u201cmagpapadala\u201d here means \u201cto send\u201d in\nthe sense of transporting a thing from one place to\nanother, while the term \u201chudyat\u201d implies a \u201cstarting\nsign\u201d. It also uses the unnatural DKA construction\nor ay-inversion. Within the context of the original\npassage, a more fluent (human) translation would\nbe, \u201cTiyak na maghahatid ang CCTV ng malakas\nna pahiwatig...\u201d, where \u201cmaghahatid\u201d means \u201cto\nbring about\u201d and \u201cpahiwatig\u201d conveys a sense of\n\u201creminder\u201d or \u201cwarning\u201d, and the sentence follows\nthe usual KA construction."}, {"title": "Inconsistencies in ay-inversions in translation\nconstruction.", "content": "Styles in translation vary from\nperson-to-person, depending on their valuations\nand reading of the original text (Castagnoli, 2020).\nLikewise, ay-inversion in the context of translation\nboils down to a stylistic choice of the translator.\nHence, while the KA is deemed more natural over\nthe DKA, this valuation of the usual/natural and\nunusual/unnatural is debated.\nOn one hand, university stylebooks on writing\nsuch as that of Yapan (2017) considers KA to\nmean the usual pattern of speech for Filipino\nspeakers. Also, Magracia (2001) notes that\nwhile the DKA is correct in \u201cmeaning, syntactic\norder, and grammaticality,\u201d it is not the \u201cnatural\u201d\nway of speaking for Filipinos. Furthermore,\nshe argues that the DKA reflects a speaker's\nlanguage of thought\u2014in this case, the influence of\nEnglish. The same justification, in terms of source\nlanguage for translated corpora as demonstrated\nin Visweswariah et al. (2011), can then be said on\nthe machine-translated text, though this remains an\nopen question for Filipino.\nOn the other hand, other research argues that\nDKA is not an abnormality, but is instead a\nnecessary aspect for \u201cdiscourse continuity\u201d (Bolata,\n2022). Hence, the more complex the sentence, the\nhigher the chance of utilizing the ay-inversion (Fox,\n1985). The same can be observed in the datasets.\nFor example, the short sentences in the CR task\nallowed for the sentences to be changed to the KA,\nwhile longer, more context-rich sentences in the\nNLI task required the use of the DKA.\nThese frameworks demonstrate the subjective\nnature of the selection of sentence structure. Hence,\nsuch inconsistencies may still be observed in\nthe tasks in BATAYAN despite being rigorously\ntranslated by native speakers."}, {"title": "Issues with Natively-Sourced Data", "content": "While the issues with most datasets in BATAYAN\nwere related with fluency and naturalness, the\nPhilippine election-related tweets (Cabasag et al.,\n2019) dataset used for the SA and TD tasks\npresented unique challenges.\nIncomplete entries. Due to the contextual\nnature of Filipino and social media communication,\nmany samples lacked sufficient contextual\ninformation. To maintain dataset authenticity, we\nprioritized entries with adequate information for\nsentiment or toxicity analysis rather than complete\nremoval. This was accomplished by selecting"}, {"title": "Evaluation", "content": "BATAYAN serves as the Filipino component of\nthe SEA-HELM leaderboard. The evaluation\nframework of SEA-HELM follows prior systems\nsuch as HELM (Liang et al., 2023), and is\ncomposed of tasks, prompts, and metrics."}, {"title": "Evaluation Design", "content": "Tasks. The selection of tasks in BATAYAN\naforementioned in Section 3.1 aims to\ncomprehensively evaluate language capabilities\nof LLMs in Filipino. Each task is a collection\nof instances composed of an input string, a list\nof references, accompanying metadata, and the\ngroundtruth label. We divide instances into two\nsets: the evaluation set, and a small 5-instance set\nthat can serve as exemplars for few-shot prompting.\nOverall, BATAYAN provides 8 distinct tasks with\n3,800 test instances.\nPrompts. For BATAYAN, we developed prompt\ntemplates for each task written in Filipino. We\nensured that the instructions for each task are\nconsistent with the prompt template design already\nused in SEA-HELM. A comprehensive list of these\nprompt templates can be found in Appendix C.\nDuring model evaluation, input prompts are\nconstructed using evaluation instances and the\ncorresponding prompt template. The default\nevaluation setting for BATAYAN is zero-shot\nprompting, where in-context input-label pairs are\nnot included in the model prompt. Given the input"}, {"title": "Results", "content": "Analysis of model performance across tasks\nreveals several insights about current multilingual\nLLMs' capabilities in Filipino. Models with\nexplicit Filipino language support generally\noutperform those without dedicated instruction\ntuning, though the margin varies significantly\nby task type. In NLU and NLR tasks (see Table\n2),\ngemma2-9b-cpt-sea-lionv3-instruct\ndemonstrates superior performance, achieving the\nhighest scores in SA (75.99%), TD (73.12%), and\nNLI (68.31%). This suggests that targeted regional\npretraining and instruction tuning can capture\nthe nuances of Filipino sentiment and toxicity,\nwhich often involve code-switching and cultural\ncontext. However, for PI, Qwen2.5-7B-Instruct\nachieves the best performance (84.45%), indicating\nthat semantic similarity tasks may benefit more\nfrom general multilingual pretraining than from\nregion-specific tuning.\nThe performance gap between models\nwith Filipino language support and general\nmultilingual models is most pronounced in CR,\nwhere gemma2-9b-cpt-sea-lionv3-instruct\n(92.75%) significantly outperforms models without\ndedicated Filipino support. This suggests that\nunderstanding causality in Filipino requires strong\ngrasp of the language's complex morphological\nsystem, particularly in how verb affixes encode\ncausative relationships.\nIn MT (see Table 3), Filipino-supported\nLLMs\nshow stronger performance\noverall. For English\u2192Filipino translation,\ngemma2-9b-cpt-sea-lionv3-instruct leads\nin both ChrF++ (57.80) and MetricX-24\n(87.23). However, for Filipino\u2192English\ntranslation, while gemma-2-9b-it achieves\na slightly higher ChrF++ (67.54) than\ngemma2-9b-cpt-sea-lionv3-instruct (65.93),\nit is substantially overtaken by the latter on\nMetricX-24 (90.55 vs. 76.49). This performance\ngap suggests that region-specific tuning not only\naids comprehension of Filipino source text but can\nalso the semantic fidelity of English outputs\u2014an\naspect captured more effectively by MetricX-24.\nIn AS, Llama-3.1-8B-Instruct achieves\nthe highest BERTScore (74.33), despite lacking\nexplicit Filipino support. This unexpected\nresult may indicate that general abstraction\nand summarization capabilities transfer well\nacross languages, even without language-specific\ntuning. However, when examining ROUGE-L\nscores, Filipino-supported models show stronger\nperformance, suggesting they better preserve\nFilipino-specific\ncoherence patterns.\ndiscourse\nstructures\nand\nIn our experiments, we found that the\naya-expanse-8b model mostly failed to handle\nFilipino instructions, resulting in low scores\nparticularly in CR (1.96%) and QA (2.97%)."}, {"title": "Conclusion", "content": "In this paper, we introduced BATAYAN for\nholistically evaluating LLMs on a gamut of\nFilipino language tasks covering natural language\nunderstanding, reasoning, and generation. Our\nfindings show that LLMs with explicit Filipino\nlanguage support and finetuned on Filipino\ninstructions demonstrate better performance on\nBATAYAN compared to other models.\nAs one of the major challenges in developing\nBATAYAN is maintaining the naturalness and\nfluency of the language, we plan to develop\nmetrics and tools that can help discriminate from\ntranslationese and natural texts, inspired by prior\nresearch (Lovenia et al., 2024; Riley et al., 2020)."}, {"title": "Limitations", "content": "While our work provides a unique and\ncomprehensive Filipino language benchmark, we\nalso reported in previous sections the challenges\nand limitations in developing high-quality NLP\nresources for the under-represented language.\nAdditionally, prosodic characteristics such as\nstress and sarcasm are not immediately obvious\nin the datasets utilized. We account for this by\nselecting only high-agreement samples. We also\nnote that the domain of both SA and TD tasks\n(which were tweets surrounding political events),\nlimits the distributions of these tasks, and as such\nwe recommend utilizing other datasets that cover a\nwider set of domains and use cases."}, {"title": "Ethical Considerations", "content": "This project was approved by the principal\ninvestigator's university internal review board.\nDue to the nature of the toxicity detection task,\nwe note that the authors were exposed to offensive\nmaterial. Nonetheless, they were encouraged to\nreport inappropriate samples and were given the\noption to stop work if desired."}]}