{"title": "Long-Tailed Backdoor Attack Using Dynamic Data Augmentation Operations", "authors": ["Lu Pang", "Tao Sun", "Weimin Lyu", "Haibin Ling", "Chao Chen"], "abstract": "Recently, backdoor attack has become an increasing security threat to deep neural networks and drawn the attention of researchers. Backdoor attacks exploit vulnerabilities in third-party pretrained models during the training phase, enabling them to behave normally for clean samples and mis-predict for samples with specific triggers. Existing backdoor attacks mainly focus on balanced datasets. However, real-world datasets often follow long-tailed distributions. In this paper, for the first time, we explore backdoor attack on such datasets. Specifically, we first analyze the influence of data imbalance on backdoor attack. Based on our analysis, we propose an effective backdoor attack named Dynamic Data Augmentation Operation (D2AO). We design D\u00b2AO selectors to select operations depending jointly on the class, sample type (clean vs. backdoored) and sample features. Meanwhile, we develop a trigger generator to generate sample-specific triggers. Through simultaneous optimization of the backdoored model and trigger generator, guided by dynamic data augmentation operation selectors, we achieve significant advancements. Extensive experiments demonstrate that our method can achieve the state-of-the-art attack performance while preserving the clean accuracy.", "sections": [{"title": "1 Introduction", "content": "Deep Neural Networks (DNN) have showcased remarkable achievements across various computer vision tasks, including image classification (He et al. 2016), object detection (Chen et al. 2023a), and visual object tracking (Wei et al. 2023; Blatter et al. 2023). However, the state-of-the-art models like foundation models (Radford et al. 2021; Brown et al. 2020) typically demand large scale training data and expensive computational resources. It is a common practice to outsource DNN training processes to third-party platforms or utilize pre-trained large-scale foundation models (Radford et al. 2021; Brown et al. 2020) from such platforms. The opaque training process raises concerns regarding trustworthiness and introduces security risks.\nBackdoor attack (Barni, Kallas, and Tondi 2019; Gu et al. 2019; Chen et al. 2017; Doan et al. 2021; Liu et al. 2020) usually injects backdoor behaviors by attaching triggers to a portion of training samples and altering their labels into specified target labels. Backdoored models make correct predictions on clean samples but predict samples with triggers as target labels. Despite a rich literature of backdoor"}, {"title": "2 Related Work", "content": "Backdoor Attacks. Backdoor attacks (Gu et al. 2019; Turner, Tsipras, and Madry 2019; Barni, Kallas, and Tondi 2019; Nguyen and Tran 2020; Wang, Zhai, and Ma 2022; Nguyen and Tran 2021; Liu et al. 2020; Liu et al.; Salem et al. 2022) introduce malicious behaviors by implanting specific triggers onto a small fraction of clean images, and subsequently altering their labels into a predefined target label. Models trained with such backdoored datasets exhibit accurate predictions for clean images, but misclassify images containing triggers as the target label during inference. Most existing backdoor attacks focus on designing an effective trigger generator, which can be categorized into two main types: fixed trigger generators (Gu et al. 2019; Turner, Tsipras, and Madry 2019; Barni, Kallas, and Tondi 2019; Liu et al.; Chen et al. 2017) and dynamic trigger generators (Nguyen and Tran 2020, 2021; Wang, Zhai, and Ma 2022; Liu et al. 2020; Doan et al. 2021). Early works often employ fixed trigger generators to generate trigger patterns with consistent appearance and location across all samples, e.g., a checkerboard positioned at the bottom right corner of an image (Gu et al. 2019). Such fixed triggers are not stealthy (Nguyen and Tran 2020, 2021) and easily detected by humans. In constrast, recent works (Nguyen and Tran 2020, 2021; Wang, Zhai, and Ma 2022; Liu et al. 2020; Doan et al. 2021) use dynamic trigger generators to generate sample-specific trigger patterns that are stealthy and difficult to be detected. To further evade human inspection, clean-label backdoor attacks (Turner, Tsipras, and Madry 2019; Barni, Kallas, and Tondi 2019; Liu et al. 2020; Shafahi et al. 2018) directly poison samples from the target label and do not modify ground-truth labels of backdoored samples.\nDespite these efforts, current backdoor attacks all focus on balanced datasets. However, real-world datasets often exhibit long-tailed distributions. Head classes and tail classes exhibit different properties during training and inference, which hinder the attack efficacy of existing backdoor attacks. In this paper, for the first time, we study backdoor attack for long-tailed visual recognition.\nLong-tailed Visual Recognition. Methods for long-tailed visual recognition focus on balancing clean classes, and can be divided into two categories: re-weighting methods and re-sampling methods.\nRe-weighting methods (Alshammari et al. 2022; Cao et al. 2019; Menon et al. 2020; Yu et al. 2022; Guo et al. 2022; Park et al. 2021; Chen et al. 2023b; Zhang and Pfister 2021) increase the loss weights of tail classes to address the over-fitting issue of tail classes. Logits Adjustment (LA) (Menon et al. 2020) incorporates offsets from class label frequencies into the logits of cross-entropy loss. Label-Distribution-Aware Margin (LDAM) loss (Cao et al. 2019) enlarges margins of tail classes through margin regularization. AREA (Chen et al. 2023b) calculates class weights inversely proportional to the effective statistical area of each class. Other methods (Yu et al. 2022; Park et al. 2021; Zhang and Pfister 2021) focus on instance-level re-weighting by dynamically adjusting probabilities (Yu et al. 2022) of samples or identifying high-influence samples (Park et al. 2021; Zhang and Pfister 2021).\nThese methods are designed for traditional long-tailed learning, and do not address the issue of imbalance between backdoored and clean samples. Our method can tackle this problem by applying appropriate data augmentation operations to clean and backdoored samples, respectively.\nRe-sampling methods (Ahn, Ko, and Yun 2022; Van Hulse, Khoshgoftaar, and Napolitano 2007; Sarafianos, Xu, and Kakadiaris 2018; Mullick, Datta, and Das 2019; Kim, Jeong, and Shin 2020; Li et al. 2021; Park et al. 2022; Chu et al. 2020) balance classes among imbalanced training datasets by either oversampling tail classes or undersampling head classes. Undersampling head classes (Van Hulse, Khoshgoftaar, and Napolitano 2007), however, may discard valuable information. A simple oversampling technique is random oversampling, but it often leads to overfitting (Sarafianos, Xu, and Kakadiaris 2018). More sophisticated methods (Ahn, Ko, and Yun 2022; Mullick, Datta, and Das 2019; Kim, Jeong, and Shin 2020; Li et al. 2021; Park et al. 2022) synthesize samples to construct a bal-"}, {"title": "3 Long-tailed Backdoor Attack", "content": "Threat Model. We consider the same attack setting in previous works (Saha, Subramanya, and Pirsiavash 2020; Nguyen and Tran 2020, 2021; Turner, Tsipras, and Madry 2019) including the state-of-the-art attack called WaNet (Nguyen and Tran 2021). Attacker can totally control the training process, and change training dataset for backdoor injection. Then, the backdoored model will be delivered to the victim user. In the attack mode of image classification, a successfully backdoored model will predict correct label for clean images, but predict target label for images with triggers.\nProblem Definition. Assuming there is a clean long-tailed training dataset $D = \\{(x_i, y_i)\\}$ with input image $x_i \\in \\mathbb{R}^d$ and the corresponding label $y_i \\in \\{1,2,..., K\\}$. $D_k = \\{(x,y) \\in D|y = k\\}$ is the subset of k-th class. We assume that the number of samples decrease with class indexes, i.e., $N_{max} = |D_1| \\geq |D_2| \\geq \\geq |D_K| = N_{min}$. The imbalance ratio is defined as $IR = N_{max}/N_{min}$.\nGiven a backdoor injection rate $p$, a subset $D_m \\subset D$ is randomly selected with $|D_m| = p * |D|$. Then, the set of backdoored samples is constructed as $D_u = \\{(G'(x), \\eta(y))|G'(x) = (1 \u2212 \\alpha)x + \\alpha G(x), (x,y) \\in D_m\\}$, where $\\eta$ is a function mapping ground-truth label into target label, and G is a backdoor trigger generator. $\\alpha$ is the weight of blending generated trigger with the original image. The objective function of long-tailed backdoor attack is:\n$\\min \\{E_{(x,y)\\sim D\\backslash D_m} L_c(f_\\theta(x), y) + E_{(x,y)\\sim D_m}L_b(f_\\theta(G'(x)), \\eta(y))\\}$\nwhere $f_\\theta$ is a classification network parameterized by $\\theta$. $L_c$ and $L_b$ are classification loss functions for clean samples and backdoor samples, respectively.\nProblem Analysis. Since $D_m$ is randomly selected from $D$ with the same probability for every sample, it contains more samples from head classes and fewer samples from tail classes. The consequent imbalance in the constructed backdoor samples makes it difficult to effectively flip tail classes into target label, which thereby increases the difficulty of optimizing $L_b$. Additionally, the imbalance between clean samples and backdoored samples affects the optimization of both $L_b$ and $L_c$. As illustrated in Fig. 1, when the target label is in head or medium classes, the number of backdoored samples and clean samples with the target label are comparable. This makes it challenging for the model to classify two distinct types of samples into the same label. When the target label is in tail classes, the class with the target label becomes dominated by backdoored samples. In both cases, there exists a trade-off between optimization of $L_b$ and $L_c$. Besides, the imbalanced sizes among clean classes also hinder the optimization of $L_c$. The above analysis motivates us to adopt different data augmentation operations for clean and backdoored samples, respectively."}, {"title": "4 Method", "content": "Overview\nTo successfully train a backdoored model on long-tailed datasets, we propose a method that finds Dynamic Data Augmentation Operations (D2AO) for clean and backdoored samples simultaneously. Shown in Figure 2, our framework includes backdoored model training and operation selectors training. The two training procedures are conducted alternatively. At each epoch, we first train operation selectors and then train backdoored model.\nAt model training stage, the Clean Selector can choose class-specific data augmentation operations with varying augmentation strengths for clean samples. For backdoored samples, the Backdoored Selector chooses sample-specific data augmentation operations based on predicted probabilities over these operations. Augmented images are feed into a trigger generator to generate perturbation triggers, which are attached on the original images to construct backdoored images. Both augmented clean images and backdoored images are utilized to train the backdoored model.\nAt operation selectors training stage, we sample a fraction of images from training dataset to update the data augmentation operation selectors. For the Clean Selector, we increase data augmentation strength if the model can classify weakly augmented images with a good performance. For the Backdoored Selector, we learn a network to predict the probabilities of data augmentation operations by leveraging the classification accuracy.\nBackdoored Model Training\nData Augmentation for Clean Samples. Since clean samples is class-imbalanced, we design a Clean Selector (CS) to select class-specific data augmentation operations for balancing different clean classes. Assume there are $N$ available data augmentation operators (e.g. Flip and Rotation). A data augmentation operator with a specific augmentation strength level $s$ is considered as a data augmentation operation $m^s_i$. The set of data augmentation operations is defined as $M = \\{m^s_i|i \\in \\{1, 2, ..., N\\}, s \\in \\{1, 2, ..., s_{max}\\}\\}$, where $s_{max}$ is the maximum level of data augmentation strength. For class $k$, we introduce a learned parameter $s(k)$ to indicate the strength of data augmentation applied to samples from class $k$. We will randomly choose $n(k)$ data augmentation operations from the data augmentation operation set $M_{s(k)} = \\{m^j_i | m^j_i \\in M, j \\leq s(k)\\}$. Following a state-of-the-art data augmentation based long-tailed method CUDA (Ahn, Ko, and Yun 2022), we set $n(k) = s(k)$ in our experiments. We will discuss the update of $s(k)$ in Sec 4.\nData Augmentation for Backdoored Samples. To balance clean samples and backdoored samples, we adopt different data augmentation operations for clean and backdoored samples separately. Since backdoored samples can come from all classes and each backdoored sample is critical for attack performance, we design a Backdoored Selector (BS) to choose sample-specific data augmentations. Previous works (Qiu et al. 2021; Borgnia et al. 2021) have shown that strong augmentations can mitigate backdoor attack to some extent. Therefore, our designed backdoored selector should choose weak sample-wise operations. Specifically, we utilize a trained data augmentation selection network $h$ to predict probabilities which indicate the strength of data augmentations. High probabilities means weak augmentations while low probabilities correspond to strong augmentations. Given a fixed data augmentation strength $q$, we use the trained data augmentation selection network $h$ to predict probabilities of all $N$ data augmentation operations in $M_q = \\{m^j_i | m^j_i \\in M, j = q\\}$. Based on these predicted probabilities, $n(q)$ data augmentation operations are randomly chosen from $M_q$ to apply to each backdoored sample. Similar to clean augmentation, we set $n(q) = q$ in our experiments. The update of data augmentation selection network $h$ will be discussed in Sec 4.\nSample-Specific Trigger Generator. To evade human inspection and ensure the stealthiness of triggers, we utilize an auto-encoder based trigger generator $G$ to generate sample-specific global perturbation triggers conditioned on specific clean images. The generated trigger is then attached with blending strength $\\alpha$ on the clean image to construct a backdoored image. To ensure to generate sample-specific triggers, we follow IAB (Nguyen and Tran 2020) to enforce trigger diversity by using a diversity loss:\n$L_{div} = E_{(x,y)\\sim D_m} \\frac{||x - x'||}{||G(BS(x)) \u2013 G(BS(x'))||}$\nwhere for each $x$, $x'$ is randomly chosen from $D\\backslash D_m$.\nWe attach triggers after conducting data augmentation on samples from $D_m$ to ensure that trigger generator $G$ adapts to data augmentation operations. Both augmented clean and backdoored samples are used to train the backdoored model according to the objective function in Eq. 1.\nData Augmentation Operation Selectors Training\nThe data augmentation operation selectors and backdoored model are trained alternatively. At the $t$-th epoch, we first sample a fraction of images from each class to construct a temporary dataset $D_t$ for updating the operation selectors. Then we train the classifier $f_\\theta$ and the trigger generator $G$ guided by fixed selectors at the $t$-th epoch. The following describes how the selectors are updated at each epoch.\nClean Selector Training. The Clean Selector is designed to determine how to choose appropriate data augmentation operations for clean samples. The core idea is to first sufficiently learn representations of weakly augmented images, and then learn representations of strongly augmented images. We denote $s(k)$ as the augmentation strength score of class $k$ at the $t$-th epoch. Thus, we update $s(k)_t$ for class k based on $D_t$ as follows:\n$s(k)_t = \\begin{cases}\ns(k)_{t-1} +1 & \\text{if acc(k) > $\\gamma$}\ns(k)_{t-1} -1 & \\text{otherwise}\n\\end{cases}$\nwhere acc(k) denotes the average accuracy of class k in $D_t$. When calculating acc(k), images of class k are augmented by randomly selecting $s(k)_{t-1}$ data augmentation operations from the operation set $M_{s(k)_{t-1}} = \\{m^j_i | m^j_i \\in M, j \\leq s(k)_{t-1}\\}$. Once $s(k)_t$ for the $t$-th epoch is determined, we utilize the updated Clean Selector to augment images in $D\\backslash D_m$ for training the backdoored model $f_\\theta$.\nBackdoored Selector Training. The Backdoored Selector is employed to choose proper data augmentation operations for backdoored samples. Given a fixed strength $q$, the core idea is to determine which operations are weak data augmentation operations for each input image $x$. We use an operation selector network $h$ to predict probabilities of all operations in the operations set with the fixed strength $q$. For an input image $x$, the network outputs high probabilities for weak data augmentation operations and low probabilities for strong data augmentation operations.\nIn comparison to strongly augmented samples, weakly augmented samples are easy to achieve higher classification accuracy. Therefore, we can optimize an operation selector network $h$ by using a classification loss, e.g., cross entropy loss. For a given input image $x$ and a strength $q$, we augment $x$ with $N$ data augmentation operations $(m^q_1, m^q_2, ..., m^q_N)$ respectively. $N$ augmented images are then fed into a feature generator $f_\\theta'$ to get $N$ features. The aggregated feature of $x$ is obtained by adding $N$ features weighted by output probabilities of $h$ (rescaled with temperature $T$). Then, the aggregated feature is input into a classifier $f'$ to obtain classification logits. The final optimization objective of $h$ is:\n$L_h = E_{(x,y)\\sim D_t} l_{ce}(f'(\\sum_{i=1}^{N} Softmax(h(f(x)), T) [i]f_\\theta (m^q_i(x)))), y)$\nwhere $l_{ce}$ is the cross entropy loss. Through the loss, $h$ is encouraged to output high probabilities for data augmentation operations that are less likely to change the ground-truth labels. Consequently, we obtain probabilities that indicate which data augmentations are weak for trigger learning."}, {"title": "5 Experiments", "content": "Experiments Details\nDatasets. We conduct experiments on two long-tailed benchmarks called CIFAR10-LT and CIFAR100-LT, which are constructed from balanced CIFAR10 (Krizhevsky, Hinton et al. 2009) and CIFAR100 (Krizhevsky, Hinton et al. 2009). The Imbalance Ratios (IR) of CIFAR10 and CIFAR100 are set as 50 and 10, respectively.\nEvaluation Metrics. The evaluation metrics consist of clean accuracy (ACC) and attack success rate (ASR). ACC is the classification accuracy on clean samples and ASR is the classification accuracy on backdoored images. We further follow traditional long-tailed visual recognition to divide all classes into three groups: \"Many\", \"Medium (Med.)\" and \"Few\". The classes are sorted descending based on the number of class samples. \"Many\" consists of classes with first 1/3 classes. \"Medium (Med.)\" comprises those of second 1/3 classes. \"Few\" composes of last 1/3 classes. We use three types of target labels to attack models. For each kind of target label, we compute the average ACC of three groups, and the average ASR for backdoored samples from three kinds of source labels. We also compute the average ACC and ASR across \"ALL\" classes.\nBackdoor Attack Methods. Considering the practical application, we compare with four state-of-the-art stealthy backdoor attacks including Label-consistent backdoor attack (LC) (Turner, Tsipras, and Madry 2019), Sinusoidal signal backdoor attack (SIG) (Barni, Kallas, and Tondi 2019), Input-aware backdoor attack (IAB) (Nguyen and Tran 2020) and Warp-based backdoor attack (WaNet) (Nguyen and Tran 2021). IAB (Nguyen and Tran 2020) and WaNet (Nguyen and Tran 2021) represent dirty-label attacks with dynamic trigger generators. LC (Turner, Tsipras, and Madry 2019) and SIG (Barni, Kallas, and Tondi 2019) are two classic clean-label attacks.\nLong-tailed Visual Recognition Methods. We choose a state-of-the-art data augmentation based re-sampling method called CUDA (Ahn, Ko, and Yun 2022) and a classic re-weighting method called Logit Adjustment (LA) (Menon et al. 2020). The two methods are integrated with backdoor attack methods for comparing performance.\nTraining Settings. The architecture of classification model $f_\\theta$ is ResNet18 (He et al. 2016), and the architecture of trigger generator $G$ is an auto-encoder similar to that in IAB (Nguyen and Tran 2020). We optimize classification model $f_\\theta$ using Stochastic Gradient Descent (SGD) optimizer with a momentum of 0.9 and a weight decay of 0.0005. The learning rate is set as 0.01. We optimize trigger generator $G$ using Adam optimizer with same learning rate 0.01. The backdoored operation selector network $h$ consists of a FC layer. The network $h$ is optimized using Adam optimizer with learning rate 0.01. In the implementations, $f_\\theta'$ is the last FC layer in $f_\\theta$ and $f_\\theta$ is the rest part of $f_\\theta$ excluding last FC layer $(f)$. The setting of other hyperparameters areas follows: $\\Lambda_{div}$ = 0.01, q = 1 and $\\alpha$ = 0.1. We use data augmentation with probability of 0.5 for clean samples. We conduct experiments on a A6000 GPU (48G memory).\nResults compared with other attacks\nWe compare with four state-of-the-art stealthy backdoor attacks including LC (Turner, Tsipras, and Madry 2019), SIG (Barni, Kallas, and Tondi 2019), IAB (Nguyen and Tran 2020) and WaNet (Nguyen and Tran 2021). To ensure fair comparison, we integrate these four attacks with a state-of-the-art data augmentation based long-tailed method called CUDA (Ahn, Ko, and Yun 2022). CUDA (Ahn, Ko, and Yun 2022) utilizes class-wise data augmentation strategies based on the features of clean samples. Therefore, a backdoored sample will be augmented using data augmentation operations specific to its original class when integrating backdoor attacks with CUDA (Ahn, Ko, and Yun 2022). We also conduct experiments combining all data augmentation based methods, including our method, with a classic re-weighting long-tailed method called Logits Adjustment (LA) (Menon et al. 2020). LA adjusts decision boundaries between head classes and tail classes by adjusting logits. We use all-to-one attacks and compute average results on three kinds of target labels: \"Many\u201d, \u201cMedium (Med.)\" and \u201cFew\". For each target label category, we compute the average results across three groups of classes. We also compute average ACC and ASR across \"ALL\" classes."}, {"title": "6 Conclusion", "content": "In this paper, for the first time, we address the backdoor attack for long-tailed visual recognition. We propose a backdoor attack with Dynamic Data Augmentation Operations (D2AO). The clean and backdoored selectors are designed to choose proper data augmentation operations for clean and backdoored samples separately. An auto-encoder trigger generator is used to generate stealthy trigger patterns. We conduct experiments on two long-tailed benchmarks and achieve state-of-the-art attack performance compared with other backdoor attacks."}, {"title": "Appendix", "content": "A Experimental Details of Backdoor Attacks\nWe compare our method with other four state-of-the-art stealthy backdoor attacks, including Label-consistent Backdoor Attack (LC), Sinusoidal Signal Backdoor Attack (SIG), Input-aware Backdoor Attack (IAB) and Warp-based Backdoor Attack (WN). LC and SIG are classic clean-label attacks. IAB and WaNet are representative sample-specific attacks. All experiments are conducted under the all-to-one setting. The implementation of our code is based on BackdoorBench (Wu et al. 2022) V1 1. We follow the original paper (or code) of these attacks to conduct experiments, and the details of attack setting are introduced below:\nLabel-consistent Backdoor Attack (LC). LC is a clean-label backdoor attack, and utilizes a trigger comprising four 3 \u00d7 3 checkerboards positioned at the four corners of an image. We follow the paper (Turner, Tsipras, and Madry 2019) to generate adversarial perturbations using projected gradient descent (PGD) 2. The adversarial model is trained with bounded in linf norm. For CIFAR10-LT, we follow the paper (Turner, Tsipras, and Madry 2019) to set \\epsilon = 16. For CIFAR100-LT, we set \\epsilon = 8 to attack model successfully. We poison 50% samples from target label to attack model for CIFAR10-LT. For CIFAR100-LT, we poison 80% samples from target label to successfully attack models.\nSinusoidal Signal Backdoor Attack (SIG). SIG is a clean-label attack with sinusoidal signal as triggers. In the case of CIFAR10-LT, we follow original paper (Barni, Kallas, and Tondi 2019) to set A = 20 and f = 6. For CIFAR100-LT, we set \\Delta = 40 and f = 6 to successfully attack the model. Similar to LC, we poison 50% samples from target label to attack model for CIFAR10-LT, while for CIFAR100-LT, we poison 80% samples from target label to achieve successfully attacks.\nInput-aware Backdoor Attack (IAB). IAB is a sample-specific backdoor attack using two auto-encoders, one for generating triggers and the other for producing masks. Following the original code 3, we first train a mask generator and then train the classification model and trigger generator simultaneously. The injection rate p is set as 0.1. We follow the paper to set $p_a$ as 0.1 and $p_e$ as 0.1 for both datasets.\nWarp-based Backdoor Attack (WN). WaNet employs elastic warping triggers to poison images. We follow the original code 4 to train backdoored model with $p_a$ = 0.1 and $p_n$ = 0.2 for both CIFAR10-LT and CIFAR100-LT. The injection rate p is also set as 0.1.\nB Experiments of Resilience Against Backdoor Defenses\nBackdoor defense can be categorized into two main types: backdoor detection and backdoor mitigation. Backdoor detection focuses on identifying whether a given model is a backdoored model. On the other hand, backdoor mitigation aims to reduce the impact of backdoor attacks, typically by decreasing the Attack Success Rate (ASRs) while maintaining clean accuracies (ACCs) at an acceptable level. To assess the resilience against these defense approaches, we select representative methods from both categories and evaluate defense performance against five backdoor attacks, including our proposed method, on the CIFAR10-LT dataset. For a fair comparison, we integrate the other four attacks with the state-of-the-art data augmentation based long-tailed method called CUDA (Ahn, Ko, and Yun 2022).\nResilience Against Backdoor Detection. Neural Cleanse (NC) (Wang et al. 2019), Meta Neural Trojan Detection (MNTD) (Xu et al. 2021) and K-ARM (Shen et al. 2021) represent three classic methods for backdoor detection. Neural Cleanse (NC) (Wang et al. 2019) optimizes a trigger pattern for each label to convert all clean samples into this specified label. It then employs an outlier detection algorithm to identify triggers smaller than others. NC uses the Anomaly Index for evaluation. If the Anomaly Index exceeds 2, the model is flagged as backdoored. The results of five backdoor attacks' resiliences against NC are shown in Fig. 4a. All backdoor attacks are integrated with the state-of-the-art data augmentation based long-tailed method CUDA (Ahn, Ko, and Yun 2022). The target label is set as 0 (head class). We train two types of clean models: \u201cCL1", "CL2": "enotes training with CUDA. We can observe that using CUDA increases Anomaly Index from the clean model.\nC Comparison Results Using Logits Adjustments (LA)\nWe present comparison results on CIFAR10-LT and CIFAR100-LT using Logits Adjustments (LA) in Table 9 and Table 10, respectively. Comparing to the results without using LA in the main paper, we observe that LA can increase ACC largely. This is consistent with the traditional long-tailed visual recognition methods, which usually integrate re-sampling and re-weighting methods to improve the classification accuracy. However, LA sometimes hinders the attack performance (ASR). For example, the average attack performance of IAB using LA decreases. A possible reason is that LA changes the decision boundary between clean images and backdoored images.\nD More Experimental Analysis\nVisualization of backdoored images\nWe visualize the backdoored images generated by different backdoor attacks in Fig. 5. It's evident that the triggers used by two clean backdoor attacks, LC and SIG, lack stealthiness. The stealthiness of these two attacks is that they do not change ground-truth labels. While IAB is a sample-specific backdoor attack, its trigger is still discernible to human inspectors. In contrast, the triggers generated by WaNet and our method exhibit a higher level of stealthiness. The difference is that WaNet subtly alters the shape of objects (the dog is thiner than that in the original image), while our method introduces slight color changes..\nAnalysis on other target labels\nIn our main paper, we conduct analysis experiments on CIFAR10-LT with target label as 0 (\"Many\" classes). we extend our analysis with the target label set as 4 (\"Medium\" classes).. These experiments follow a similar structure to those presented in the main paper. Below are the details of our ablation studies:\nData Augmentation Strength q in Backdoored Operation Selector. Similar to the experiments conducted in the main paper, we keep the other hyperparameters fixed. The results are presented in Tab. 11. We can observe that the model gets higher ASRs when q is set as 2 or 4. Compared to models made in the main paper.\nStrength \u03b1 of Trigger. We conducted experiments with fixed hyperparameters while adjusting \u03b1, and the results are summarized in Tab. 14. It is observed that ASRs will increase when increasing trigger strength \u03b1. This observation can be attributed to the heightened visibility of the trigger as its strength increases. A stronger trigger is more conspicuous, making it easier for the classifier to discern and learn its features, thus resulting in higher ASRs.\nClass-wise Performance. We further analyzed the class-wise performance, comparing our method with two other sample-specific backdoor attacks, namely IAB (Nguyen and Tran 2020) and WaNet (Nguyen and Tran 2021). Class-wise ASR and ACC are depicted in Figure 6a and Fig. 6b, respectively. From Fig. 6a, it is evident that our method achieves state-of-the-art ASRs for most classes. Additionally, our method demonstrates superior performance on head classes and some tail classes (e.g., class 9) in terms of ACC, as illustrated in Fig. 6b.\nE Negative Impact and Limitations.\nWe brings attention to the threat of this practical long-tailed backdoor attack task. While our method achieves a good attack performance, we mainly focus on data augmentation-based methods, and does not consider re-weighting-based techniques. We leave them for an interesting future work."}]}