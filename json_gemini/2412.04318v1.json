{"title": "THE HYPERFITTING PHENOMENON: SHARPENING\nAND STABILIZING LLMS FOR OPEN-ENDED TEXT\nGENERATION", "authors": ["Fredrik Carlsson", "Fangyu Liu", "Daniel Ward", "Murathan Kurfali", "Joakim Nivre"], "abstract": "This paper introduces the counter-intuitive generalization results of overfitting\npre-trained large language models (LLMs) on very small datasets. In the setting\nof open-ended text generation, it is well-documented that LLMs tend to gener-\nate repetitive and dull sequences, a phenomenon that is especially apparent when\ngenerating using greedy decoding. This issue persists even with state-of-the-art\nLLMs containing billions of parameters, trained via next-token prediction on large\ndatasets. We find that by further fine-tuning these models to achieve a near-zero\ntraining loss on a small set of samples \u2013 a process we refer to as hyperfitting \u2013 the\nlong-sequence generative capabilities are greatly enhanced. Greedy decoding with\nthese Hyperfitted models even outperform Top-P sampling over long-sequences,\nboth in terms of diversity and human preferences. This phenomenon extends to\nLLMs of various sizes, different domains, and even autoregressive image gen-\neration. We further find this phenomena to be distinctly different from that of\nGrokking and double descent. Surprisingly, our experiments indicate that hyper-\nfitted models rarely fall into repeating sequences they were trained on, and even\nexplicitly blocking these sequences results in high-quality output. All hyperfit-\nted models produce extremely low-entropy predictions, often allocating nearly all\nprobability to a single token.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite the recent rapid advancements in artificial intelligence spearheaded by Transformer-based\nlarge language models (LLMs) and their emergent phenomena (Wei et al., 2022b; Bubeck et al.,\n2023), models trained on next-token pre-training objectives often degenerate when producing longer\ntexts. This is particularly true for greedy decoding, and has resulted in mitigation strategies such as\nrepetition penalties (Keskar et al., 2019) and nucleus sampling (Holtzman et al., 2020). However,\nwhen removing these heuristics and simply picking the top-1 candidate at each time-step, LLMs\ndisplay strong tendencies to repeat themselves at the token, phrase, and sentence level (Holtzman\net al., 2020), as is exemplified in Figure 1. This is a recurrent phenomenon for which there are many\nproposed hypotheses but, to the best of our knowledge, no definitive explanation exists."}, {"title": "2 RELATED WORK", "content": "Various recent studies report phenomena regarding neural networks that break the conventional prac-\ntice of early stopping. For instance, \"double descent\" shows a second rise and decline in test loss\nbeyond the classical bias-variance trade-off Belkin et al. (2019); Nakkiran et al. (2020). Overfitting\nnetworks for a prolonged duration may lead to \u201cgrokking\u201d, resulting in strong delayed generaliza-\ntion Power et al. (2022); Liu et al. (2023). There are recorded cases of benign overfitting, where a\nmodel that perfectly fits noisy training data still generalizes well to unseen scenarios Zhang et al.\n(2017); Belkin et al. (2019). These phenomena, which seemingly break the foundations of statistical\nlearning theory, are often attributed to the over-parameterization of large networks in relation to the\ntraining data volume He et al. (2016); Zhang et al. (2021).\nIt is well documented that in long-sequence text generation LLMs exhibit degenerative tendencies\nsuch as repetition (Welleck et al., 2019; Holtzman et al., 2020; Fu et al., 2020; Brown et al., 2020;\nXu et al., 2022). While mitigation strategies like repetition penalties (Keskar et al., 2019) and\nsophisticated sampling strategies (Holtzman et al., 2020) exist, no definitive explanation for why\nthis occurs has been given. Interestingly, repetitive texts tend to occur less frequently for conditional\ngeneration tasks, such as machine translation and summarization (Holtzman et al., 2020).\nAlthough next-token prediction is the dominant training objective for LLMs, it is not perfectly\naligned with the requirements of sequence generation (Welleck et al., 2019; Bachmann & Nagara-\njan, 2024). This is evident in practical scenarios, where alternative approaches may score higher on\nhuman preference but achieve lower perplexity (Carlsson et al., 2024). Even in situations of pure\nlanguage modeling, the next-token prediction loss and its exponentiated version, perplexity, only\ncapture a subset of the statistical properties of language (Meister & Cotterell, 2021).\nOur work also relates to the dominant LLM paradigm of pre-training on large datasets, followed\nby additional fine-tuning in which scaling up next-token prediction training gives rise to emergent\nand unpredictable capabilities (Wei et al., 2022a; Srivastava et al., 2023). Often, further adjustments\nto the LLM are made using reinforcement learning for instruction-following (Ouyang et al., 2022).\nWhile the main idea of this may be to modify the LLM's interaction API, it can significantly increase\nlong-sequence generation capabilities, as consistently seen in code generation (Guo et al., 2024;\nLozhkov et al., 2024). Contrary to these methods, hyperfitting allows us to observe the effects of\ncollapsing the modeling space attained during pre-training, without incorporating any new data or\ntraining methods."}, {"title": "3 HYPERFITTING", "content": "Hyperfitting is conceptually straightforward and consists of fine-tuning a pre-trained model on a\nsmall set of samples until the model achieves a near-zero training loss. This is done using a small\nlearning rate in order to preserve as much knowledge as possible from the pre-training, but nev-\nertheless leads to a poor validation loss. This is exemplified in Figure 2, where the training and\nvalidation losses for Tiny Llama 1.1b (Zhang et al., 2024) is shown next to its type-token ratio\n(TTR) for sequences generated on held-out contexts. TTR is a simple metric measuring the ratio of\nunique tokens, defined as TTR = $\\frac{\\text{Number of Unique Tokens (Types)}}{\\text{Total Number of Tokens}}$. Although a high TTR does not guaran-\ntee textual quality, the average TTR has been shown to correlate well with human preferences for\nlong-sequence text generation (Carlsson et al., 2024), and is further discussed in Appendix A.1s.\nTo verify that hyperfitting has a reproducible effect, we perform this training on various model\ninstances, datasets and modalities. Specifically, we fine-tune one instance for each of the following\nmodels: Tiny Llama 1.1b, DeepSeek 7b (Bi et al., 2024), Llama 3.1 8b & 70B (Dubey et al., 2024),\nand ImageGPT-Large (Chen et al., 2020) for image generation. Notably, the text models cover a\nrange of quality levels; Llama 3.1 and DeepSeek are, at the time of writing, considered state of the\nart for open-source LLMs, while TinyLlama is less competitive.\nFor all our experiments we train the model via the next-token prediction objective, with specific\ndetails regarding the image experiments found in Section 7.1. Unless otherwise specified, all LLMs\nuse the following training setup: 20 epochs on 2000 randomly selected sequences from a given\ndataset, with a length of 256 tokens. We update all the model's parameters using the Adam optimizer\nwith a learning rate of le-6 without weight decay, and use a batch size of 8. All hyperfitted LLMs use\nthe identical samples from the Fiction-Stories dataset (Forsythe, 2024). However, as demonstrated\nin Section 6.2 and Section 7.1, hyperfitting occurs for various datasets and modalities.\nDue to the obvious concern of a hyperfitted model only repeating the data it has been fine-tuned\non, we additionally generate texts using a citation blocker. This means the model is prohibited\nfrom repeating longer subsequences appearing in the hyperfitting dataset. Via a straightforward\npattern matching approach, we continuously check if the 5 most recently generated tokens exist as\na sequence in the training data. If this is the case, we zero out the probability of the next token, as\nsoon as the current word is completed."}, {"title": "4 OPEN-ENDED TEXT GENERATION", "content": "To thoroughly evaluate the models' ability to generate text in an open-ended setting, we conduct an\nextensive human evaluation study with verified English speakers independently hired as freelancers.\u00b9\nEach sample consists of a textual context and two possible continuations, with one continuation\nalways coming from the original text and the other generated by a model. The annotator's task is to\ndetermine the preferred continuation, or classify them as equally good options. Further details about\nthe annotations are available in Appendix A."}, {"title": "4.1 TEXT GENERATION RESULTS", "content": "Hyperfitting drastically increases the human preference ratio. This is particularly true for the 256-\ntoken scenario, where the initially worst performing TinyLlama increases from 4.9% to 34.4%,\nputting it on par with Llama 3.1 70b. While all models perform worse on the 256 scenario, the drop\nis less drastic for hyperfitted models. Indeed, greedy decoding with hyperfitted models produce both\nhigher human ratings, and a higher ttr when compared with their nucleus sampling counterparts-a\nmethod proposed specifically to mitigate repetitions. Citation blocked models see no noticeable\ndrop in performance.\nThere is a clear correlation with the average TTR and human preference, as the hyperfitted models\ndemonstrate a comparably small drop in both metrics as sequence length increases. Interestingly, all\nhyperfitted models perform abysmally in terms of perplexity, corroborating the lack of correlation\nbetween this metric and a model's ability to generate longer. This is discussed further in Section 5."}, {"title": "4.2 DIVERSITY AND DATASET OVERLAP", "content": "As hyperfitting is by definition overfitting a model on a tiny set of samples, we investigate how this\nimpacts the model's diversity and overlap with the training dataset. For diversity between generated\nsequences we apply Self-BLEU, which measures the highest pairwise BLEU score for all pairwise"}, {"title": "5 SHARPENED PREDICTIONS", "content": "Given the boost in textual quality brought about by hyperfitting, we investigate why hyperfitted mod-\nels achieve such poor perplexity on held-out data. Using the 300 texts and their original continua-\ntions from the experiment reported in Section 4, we collect information on the predicted vocabulary\ndistributions of different models. As we observe similar trends across all our hyperfitted models, the\nresults in Table 3 display information only on a subset of the models.\nThe hyperfitted models exhibit significantly lower entropy in their predicted vocabulary distributions\ncompared to the non-hyperfitted models. This entails that almost all of the probability mass is\nattributed to a single token. Given the poor perplexity on withheld texts, this sharpened prediction\nbehavior persists even when these predictions are wrong. This persistence is exemplified in Figure 4,\nwhere the hyperfitted model assigns \"United\" a 92.8% probability, although it assigned the previous\nword \"Manchester\" a near 0 probability. Neither of these words occur in the hyperfitting dataset.\nHyperfitted models produce vocabulary predictions with extremely low entropy. Moreover, the low\ntraining loss indicates that almost all of the probability is consistently assigned to the correct next\ntoken during training. This sharpened prediction pattern is, to a degree, transferred to unseen data\nwhere the model continues to heavily favor certain candidates. When evaluating these predictions\nagainst the unseen data, the low-entropy predictions assign very low probability to words that occur\nin the new sequences but are not favored by the model, which in turn results in very high perplexity\nregardless of the quality of the texts they generate. It is worth noting here that, although we follow\nstandard practice and report performance on held-out data using the exponentiated perplexity metric,\nthe key point is really that predictions with low inherent entropy result in high cross-entropy when\nmeasured against unseen sequences."}, {"title": "6 DATA INFLUENCE", "content": "The following experiments aim to investigate the effect and importance of the data used during\nhyperfitting. For this endeavor we alter only the data used during hyperfitting and, unless stated\notherwise, apply the same training procedure as Section 3. These experiments focus on a single\nproperty at a time and do not account for potential relationships between these properties."}, {"title": "6.1 DETERMINACY OF DATA", "content": "As an initial experiment, we evaluate the extent to which the set of training samples deterministically\ndictates the outcome of the hyperfitting process. To this end, we produce two additional versions\nof the fiction dataset: 'Shuffle-1' and 'Shuffle-All'. For Shuffle-1 the order of only two samples\nis switched, and for Shuffle-All dataset, the entire order is shuffled. For both datasets we hyperfit\nLlama 3.1 using the same fixed random seed as used in Section 3, meaning all models train on the\nsame data, but in a different order.\nUsing the full original texts from Section 3, we calculate how often two models produce the same\ntop-1 prediction. This is displayed in the left similarity matrix of Figure 5. All models differ in\napproximately 30% of their top rank predictions; this is a noticeably large difference from training\non the same data. This is especially noteworthy considering that some portion of these predictions\nwill be for subwords, which are almost guaranteed to have the same top rank. Conclusively, the data\ndoes not deterministically account for which tokens emerge as top candidates from the hyperfitting\nprocess."}, {"title": "6.2 TYPE OF DATA", "content": "Although Section 6.1 shows that data does not fully determine the resulting model, we nevertheless\nexplore whether any trends emerge between the hyperfitting datasets and downstream dataset ca-\npabilities. Therefore, we additionally hyperfit Llama 3.1 with data from both Wikipedia and BBC\nNews separately and measure per-dataset human preference for the 256-token task in Section 3.\nQualitative examples of these models are available in Appendix C.2."}, {"title": "6.3 QUANTITY OF DATA", "content": "Finally, we measure the effect of the number of training samples from the Fiction dataset when\nhyperfitting TinyLlama. To this end we keep the number of updates constant at 5000, entailing\nmore epochs as the number of samples decreases. The right part of Figure 5 displays the resulting\nTTR of the first 96 tokens when greedily generating from the 300 contexts used in Section 4. Since\nhuman annotations are costly, TTR is intended as a crude estimate of quality by measuring the\nrepetitiveness of generations. Further discussion regarding TTR as an automatic metric is available\nin Appendix A.1.\nAlthough there is an initial decline in TTR when decreasing from 2000 samples, the TTR remains\nabove 50 up until there are only 8 training samples. This means that in terms of producing less\nrepetitive output, improvements may be seen from very few samples. Further, we note that 8 samples\nequals our hyperfitting batch size, meaning that at this point all batch updates are identical, and may\nhence be indicative of why this is drastically worse than 16 samples."}, {"title": "7 HYPERFITTING AND THE BIGGER PICTURE", "content": null}, {"title": "7.1 IMAGE GENERATION", "content": "To investigate the hyperfitting phenomenon for an additional modality, we hyperfit ImageGPT-Large\n(774M parameters) (Chen et al., 2020) on 2,000 randomly selected images from CIFAR-10. Besides\nusing visual tokens, ImageGPT is a standard Transformer architecture and was pre-trained using\nnext-token prediction on 32x32 images. Figure 6 contains a qualitative comparison of the greedy\ngeneration when the models receive the first 25% of an image. More details and results are available\nin Appendix B.4.\nFrom visual inspection it is clear that the hyperfitted model produces higher quality images that more\nresemble actual objects and subjects (see Appendix B.4 for more examples). Although the gener-\nated image quality is unimpressive compared to contemporary diffusion based models, the relative\nimprovement allows us to conclude that the hyperfitting phenomenon extends to other modalities\nbeyond just text. Moreover, we note that greedily generating with ImageGPT results in repetitive\npatterns analogous to the repetitive texts of LLMs. This strongly indicates that the repetitive nature\nof Transformer LLMs is not an artifact of the repetitions found in natural language, as posed by Fu\net al. (2020) and Holtzman et al. (2020)."}, {"title": "7.2 RELATIONSHIP TO GROKKING AND DOUBLE DESCENT", "content": "The hyperfitting phenomenon differs in several key ways from the reported work on grokking and\ndouble descent. (1) As seen in Figure 2, the positive effect of hyperfitting occurs as training loss\napproaches zero. In contrast, previous phenomena occur during prolonged exposure to low train-\ning loss. (2) All our hyperfitted models are pre-trained LLMs with billions of parameters, whereas\nprevious work utilizes comparably small and randomly initialized networks. (3) Hyperfitting is ob-\nserved in the task of sequence generation, where the model's predictions are recursively added to its\ninput. Previously observed phenomena have focused on single output tasks, such as classification\nand regression. (4) Hyperfitting sees improvements in terms of TTR after only a few epochs, as evi-\ndent in Figure 2, significantly faster than the reported occurrence of double descent and the delayed\nrewards of grokking. (5) None of the hyperfitting training utilizes any form of weight decay, which\nis speculated to be a main contributor to delayed generalization in grokking (Liu et al., 2023).\nFrom (2) and (3), we conclude that hyperfitting is observed at a higher level of model and task\ncomplexity. One may argue that if grokking were to occur in large pre-trained models, it would\nhappen quickly and would therefore reconcile (1) and (4). However, this is yet to be achieved,\nand it is unclear if such speed would even be compatible with the slow progress of weight-decay\n(5). Therefore, besides all phenomena seemingly contradicting early stopping, we currently find no\nevidence of a commonality. We therefore argue for treating hyperfitting as a separate phenomenon.\nFinally, one may note that the validation loss for hyperfitting never decreases, distinguishing it from\nthe hallmark of double descent and grokking. However, as the next-token prediction loss is not fully\naligned with our task of sequence generation, we do not consider this to be a reasonable comparison.\nAdmittedly, this entails we cannot track an aligned validation score, preventing us from proving that\nhyperfitting fundamentally differs from previous discoveries."}, {"title": "7.3 TOP-RANK ENCOURAGEMENT", "content": "This subsection explores our observation that scenarios with low training loss cause desirable tokens\nto be ranked higher even when validation loss is poor. For this, we use the term 'top-ranks' to refer\nto the set of most probable tokens in a predicted distribution, and perplexity as the exponential of\nthe log loss for the next token.\u00b3 The notion of a \u201cdesirable\u201d token entails that, if generated, the token\nwould extend the current sequence in a manner acceptable by a human.\nSince next-token prediction does not factor in the order and rank of predictions, we note that a\nhigher loss leaves more room for undesired candidates to reside within the predicted top-ranks. In\na scenario of moderate perplexity, this means that two models achieving identical perplexity on all\ntime-steps, can still have different top-ranks. This notion is visualized in Figure 7.\nA low training perplexity entails distributions during training with low entropy. As observed in\nSection 5, this entropy behaviour is transferred to validation data. But simply lowering entropy does\nnot by itself entail that top-rank predictions improve. Indeed, we can freely modify the entropy of\nany (non-uniform) distribution by applying temperature to it, without any change in the predicted\norder. It follows that something additional happens to the model as it achieves a low training loss."}, {"title": "8 DISCUSSION AND CONCLUSIONS", "content": "We introduce the hyperfitting phenomenon: where pre-trained LLMs consistently see significant im-\nprovements in open-ended text generation by overfitting to a very small set of samples. The textual\nquality of the models are assessed via human verified English speakers, resulting in a new dataset\nwith over 20,000 annotations. We provide extensive evidence that the hyperfitting phenomenon is\nreproducible across various model sizes, data types, and extends to autoregressive image generation.\nIn all these scenarios, the hyperfitted models predict very sharp distributions, with the candidates\nseemingly emerging from knowledge acquired during pre-training.\nFor text generation, the hyperfitted models produce texts that are rated higher by human annotators.\nInterestingly, using greedy decoding with hyperfitted models results in less repetitive texts than using\nnucleus sampling with the original models. This showcases a key flaw in sampling-only methods:\nthey doesn't change predicted probabilities, so while it reduces the chance of repetition, the risk\nstill increases with longer sequences. Furthermore, we find that our hyperfitted models rarely repeat\nlonger subsequences from the training data. Even when explicitly blocking all such subsequences,\nthe models still produce high-quality texts.\nWe find that hyperfitting on the same data, but shuffled, results in a model with 30% different top-1\npredictions. This indicates that the stochastic hyperfitting process itself is responsible for a large\npart of which top-1 candidates emerge. Additionally, we found no correlation between the training\ndata and downstream generation capabilities. However, models hyperfitted on Wikipedia and BBC\nNews outperform our model using fiction data. Due to these nuanced results, further work is needed\nto discern the impact of the data used during hyperfitting.\nAll our experiments (besides the nucleus sampling baseline in Section 4) are centered around greedy\ndecoding. This is intended to remove as many elements of uncertainty as possible, and allow us\nto investigate the underlying model directly. Further investigation of combining hyperfitting with\nother sampling strategies and heuristics is left to future work. However, we note that sampling\nwithout any temperature may result in a near-deterministic generative behaviour, considering the\nsharp distributions of the hyperfitted models.\nFinally, through our observations of the extreme scenario that hyperfitting poses, we hypothesize\nthat the behavior of predicting good tokens in the top ranks is itself a learnable behavior. We refer\nto this as top-rank encouragement and speculate that it is more likely to occur in scenarios with low\ntraining loss. To what extent such a hypothesis is true, and why hyperfitting results in generative\ncapabilities that generalize, are important open questions for future work."}, {"title": "ETHICS STATEMENT", "content": "The research reported in this paper involves an extensive human evaluation. In the interest of fairness\nas well as data quality, annotators were hired as freelancers through Fiverr\u00b9 and paid 10 USD per\nhour of annotation."}]}