{"title": "Generating bilingual example sentences with large language models as lexicography assistants", "authors": ["Raphael Merx", "Ekaterina Vylomova", "Kemal Kurniawan"], "abstract": "We present a study of LLMs' performance in generating and rating example sentences for bilingual dictionaries across languages with varying resource levels: French (high-resource), Indonesian (mid-resource), and Tetun (low-resource), with English as the target language. We evaluate the quality of LLM-generated examples against the GDEX (Good Dictionary EXample) criteria: typicality, informativeness, and intelligibility (Kilgarriff et al., 2008). Our findings reveal that while LLMs can generate reasonably good dictionary examples, their performance degrades significantly for lower-resourced languages. We also observe high variability in human preferences for example quality, reflected in low inter-annotator agreement rates. To address this, we demonstrate that in-context learning can successfully align LLMs with individual annotator preferences. Additionally, we explore the use of pre-trained language models for automated rating of examples, finding that sentence perplexity serves as a good proxy for typicality and intelligibility in higher-resourced languages. Our study also contributes a novel dataset of 600 ratings for LLM-generated sentence pairs, and provides insights into the potential of LLMs in reducing the cost of lexicographic work, particularly for low-resource languages.", "sections": [{"title": "1 Introduction", "content": "Example sentences in bilingual dictionaries play a crucial role in language learning, helping both to understand the meaning of headwords (words that mark a separate entry in the dictionary), and their usage in context (Potgieter, 2012; Nielsen, 2014; Caballero, 2024). What makes candidate sentences good as examples is the subject of linguistic research, with Kilgarriff et al. (2008) proposing the GDEX (Good Dictionary EXample) framework, which qualifies good examples as typical (\"exhibiting frequent and well-dispersed patterns of usage\"), intelligible (\"avoiding gratuitously difficult lexis and structures\"), and informative (\"helping to elucidate the definition\"), as illustrated in Table 1.\nThe extensive work required to come up with example sentences increases the cost of compiling lexicographic resources (He and Yiu, 2022). This has prompted research into the automatic selection of example sentences from existing corpora (Kilgarriff et al., 2008; Frankenberg-Garcia, 2014). However, existing corpora might not always contain sentences that are suited to language learning, as their text can be overly complex, fail to further explain the meaning of the headword, or not be licensed for reproduction. As a result, researchers have begun exploring models tailored for the generation of dictionary example sentences from a headword and its dictionary definition (He and Yiu, 2022).\nLarge language models (LLMs) trained on a wide range of texts (Gao et al., 2020) might be well suited to formulate generic and informative example sentences that benefit language learning. In particular, their capacity to adapt to new, unseen tasks (Radford et al., 2019; Kojima et al., 2023) means that they might be well suited to generate sentences against specific criteria. However, questions about the quality of the sentences they generate, and their ability to understand what makes a good example, remain.\nIn this paper, we review LLMs capability to generate and rate example sentences in a bilingual lexicography context, against the GDEX criteria. We work with three language pairs, with English on the target side, and source sides that cover a range of language resource levels: French (high-resource), Indonesian (mid-resource), Tetun (low-resource). The paper makes the following contributions:"}, {"title": "2 Background", "content": "LLMs for synthetic data generation. While hallucinations can make LLMs unreliable for tasks that require factual accuracy (Azamfirei et al., 2023), the text they generate can be of high quality, in some cases preferred over human-generated text by human annotators (West et al., 2023; Almeman et al., 2024; Cai et al., 2024a). LLM generation of synthetic data has several downstream applications, including the creation of corpora for subsequent training of specialised models (Li et al., 2023; Whitehouse et al., 2023) and the generation of examples to aid learning (Jury et al., 2024; Nam et al., 2024). In lower resource scenarios, LLMs exhibit an increased tendency to generate inaccurate or poor quality information (Cahyawijaya et al., 2024; Benkirane et al., 2024). However, this limitation is not entirely prohibitive; recent research has demonstrated that LLMs can be leveraged to generate synthetic resources when authentic materials are scarce (Santoso et al., 2024). This dual nature of LLMs in low-resource contexts\u2014their proneness to hallucination and their potential for synthetic data generation\u2014presents both challenges and opportunities for their application in bilingual lexicography.\nAutomated extraction and generation of dictionary examples. The identification, rating, and generation of dictionary examples has been the subject of previous research. Using the GDEX criteria, Almeman and Anke (2022) found that many WordNet examples (Miller, 1995) are of poor quality, often because they are too short, in comparison with those from the Oxford English Dictionary (1989). A subsequent study found that ChatGPT-generated examples are rated higher by human annotators than those from the Oxford Dictionary (Almeman et al., 2024). Cai et al. (2024a) further introduced OxfordEval, an evaluation metric defined as the win rate between generated sentences and the Oxford Dictionary, and found that LLM-generated examples have over 80% win rate. They also introduced the selection of candidate sentences through a masked language model to marginally improve the win rate. In non-English settings, results were found to be more mixed: working with Japanese, Benedetti et al. (2024a) found human examples were still preferred by annotators, with high rates of disagreement between annotators about example quality. In a low-resource setting, working with Singlish, Chow et al. (2024) found that ChatGPT could be leveraged to produce draft dictionary entries, including example sentences, but authors did not rate the examples independently of generated definitions.\nResearch gap. Despite the growing body of research on LLMs in lexicography, several areas remain unexplored. First, there has been no structured evaluation of LLM capabilities in generating example sentences for bilingual dictionaries."}, {"title": "3 LLM generation of bilingual example sentences", "content": "This section describes our methodology for generating bilingual example sentences using LLMs, and results from human annotation of these generated sentences."}, {"title": "3.1 Methodology for generation", "content": "Figure 1 provides an overview of our proposed methodology for generating and rating examples.\nWord selection For each source language (French, Indonesian, Tetun), we randomly select 50 words from the top 10,000 most frequent words. We use existing word lists for French\u00b9 and Indonesian, and generate that list for Tetun by finding the top 10,000 words in the Labadain 30k dataset (de Jesus and Nunes, 2024), the largest available Tetun dataset audited by native speakers. We then manually translate each of the 50 words to their English equivalent. When words have multiple translations, we select the one that we deem the most frequent. This results in 50 word pairs for each language pair.\nExample generation We work with two LLMs, GPT-40 (OpenAI team, 2024) and Llama 3.1 405b (Dubey et al., 2024). The former is the highest rated model overall on the Chatbot Arena as of September 2024 (Chiang et al., 2024), the latter is the highest rated among open weights models. For generating example sentence pairs, we use the OpenAI API\u00b3 for GPT-40, and the Replicate API4 for Llama 3.1 405b, using a prompt that describes the GDEX criteria and includes the word pairs, shown in Appendix A.1. Both the source and target side sentences are generated jointly in the same output.\nAnnotator selection and training All annotators are native speakers of the source language they rate, and are advanced speakers of English as a second language. We recruit two annotators per source language, one with a computational linguistics background, and one with no background in"}, {"title": "3.2 Quality of LLM-generated examples", "content": "Table 2 shows an example of LLM-generated sentences for each language pair, with their associated ratings.\nPer language Mean overall ratings and annotation distribution are presented in Table 3 and Figure 2 respectively. LLM-generated examples get a medium to high overall rating across language pairs. However, there is a clear drop in quality when language is less-resourced. French examples, representing a high-resource language, received the highest ratings (mean 4.68 out of 5), followed by Indonesian (mid-resource, mean 4.41), and then Tetun (low-resource, mean 3.74). This pattern is consistent with previously observed LLM performance degradation on lower-resourced languages (Li et al., 2024), likely due to the reduced amount of training data available for these languages. For example, the MADALAD-400 corpus (Kudugunta et al., 2023), which has documents from Common Crawl tagged by language, has almost 6 times more French documents (~220M) than Indonesian documents (~38M), and over 5,000 times more French documents than Tetun documents (~40k).\nPer LLM Comparing overall rating for the two LLMs used in the study, we find that GPT-40 outperforms Llama3.1 for French (4.79 vs. 4.57), with a statistically significant t-statistic of over 3 indicating a substantial difference between the two models relative to variation in the data. For Indonesian and Tetun however, the paired t-test indicated that the difference between the two models is not statistically significant compared to the variation in the data. We therefore observe variability in LLM output quality that is uneven across language depending on resource level and shows that performance degradation is not always predictable from resource level.\nPer GDEX criteria Comparing qualitative ratings (typical / intelligible / informative / translation correct), we find a consistent degradation across criteria as the resource level of the language decreased (Figure 2). For example, 95% of examples are rated as \"typical\" for French, but this decreased to 92% for Indonesian and 69% for Tetun. The trend was particularly pronounced for the \"Informative\" criterion (fra 95%, ind 77%, tdt 56%), highlighting the challenges LLMs face in maintaining accurate and relevant examples for lower-resourced languages."}, {"title": "3.3 A note on inter-annotator agreement", "content": "Table 5 shows relatively low rates of inter-annotator agreement for French and Indonesian, measured through Krippendorff's alpha (Castro, 2017), both for overall rating (where individual judgement is encouraged) and for qualitative GDEX criteria (where standard rating is encouraged). For Tetun, however, we observe relatively high inter-annotator agreement across all criteria, including overall rating. We hypothesise that this is due to the more pronounced mistakes in Tetun sentences, which means both that ratings rely less on subtlety of judgement, and that there is more signal to measure. For example, in French, all GDEX criteria are rated \"Yes\" in over 95% of examples, giving little room to measure disagreement.\nWe note that low inter-annotator agreement for rating examples was observed in previous studies (Benedetti et al., 2024b). This finding guides our further experiments: (1) when working with in-context learning, we favour aligning LLM rating with one annotator's judgement at a time, rather than aligning with contradicting ratings coming from multiple annotators (Section 4.1); (2) when working with pretrained language models, which are not fine-tuned to annotator preference, we only measure alignment with the annotator who has a computational linguistics background (Section 4.2)."}, {"title": "4 Automated rating of example sentences", "content": "This section describes our experiments for automating the rating of LLM-generated sentences, in alignment with human ratings."}, {"title": "4.1 Rating through LLM in-context learning", "content": "For each annotator, we study whether in-context learning can successfully teach the annotator's preferences to an LLM, measured through alignment in overall rating (1-5 score).\nData preparation and model choice Given 100 annotated example sentence pairs from a specific annotator, we randomly sample 10 pairs as in-context examples and 90 pairs for evaluation. To avoid bias linked to model self-preference (Panickssery et al., 2024), we choose against working with one of the two LLMs used for generating sentences and instead rely on Gemini 1.5 Pro (Gemini Team, 2024) for this task, given that it is the second best ranked model for instruction following on the Chatbot Arena as of September 2024.\nPreprocessing through reasoning generation For each sentence pair in the sample of 10 pairs, we first ask the LLM to reason about what led to the annotator's rating, given their comment (if any), their ratings of the GDEX criteria, and the translation"}, {"title": "4.2 Rating through pre-trained language models", "content": "In this section, we aim to determine if computationally derived metrics can effectively approximate human judgements of example sentence quality along GDEX criteria.\nData preparation We work exclusively with ratings from annotators who have a background in computational linguistics. We map each rating to a number between 0 and 1, where No = 0, Somewhat = 0.5, Yes = 1, allowing us to represent the gradations in quality along a continuous scale.\nMetrics and hopythesis For each source-side sentence, we compute several metrics using pre-trained language models to test various hypotheses. We examine whether the probability of the entry word (when masked) can serve as a predictor of the \"Informative\" rating, hypothesising that a lower probability might indicate a more informative context. We also investigate if sentence perplexity can be a good predictor of both the \"Intelligible\" and \"Typical\" ratings, with the assumption that lower perplexity could indicate a more intelligible and typical sentence. Additionally, we explore whether context entropy at the position of the entry word could be another predictor of the \"Informative\" rating, positing that higher entropy might suggest a more informative context.\nChoice of models To test the hypotheses, we use pre-trained encoder-only language models: CamemBERT-large for French (Martin et al., 2019), IndoBERT for Indonesian (Koto et al., 2020). For Tetun, given the absence of existing encoder-only models for the language, we fine-tune XLM-ROBERTa-large (Conneau et al., 2019) on MADLAD-400 (Kudugunta et al., 2023) which is the largest Tetun monolingual corpus available, using the hyperparameters in Adelani et al. (2021). We release the weights of this model for future researchers.\nResults As Table 7 demonstrates, the probability of the target word serves as a fair predictor of informativeness for French, with a correlation of 0.21, but this relationship does not hold for other languages. High perplexity proves to be a moderately good predictor of low intelligibility for both French and Indonesian, with correlations of -0.57 and -0.52 respectively. Similarly, high perplexity is a good predictor of low typicality for French (correlation of -0.41) and moderately good for Indonesian (-0.32). Notably, no significant correlations are found for Tetun across these metrics. Contrary to our hypothesis, context entropy at the target word (when masked) does not serve as a good predictor for informativeness across any of the languages studied.\nImplications Our results show the potential of sentence perplexity for estimating example sentence typicality and intelligibility, for middle- to high-resource languages. The lack of significant results for Tetun demonstrates that the amount of available corpora in this low-resource language is not sufficient to get a pre-trained language model that captures sentence quality with a high degree of accuracy."}, {"title": "5 Discussion", "content": "Our study provides several insights into the capabilities and limitations of LLMs for generating and evaluating bilingual dictionary examples. First, we demonstrate that LLMs are capable of producing reasonably good quality example sentences across multiple language pairs. However, there is a clear degradation in performance as we move from high-resource languages like French to low-resource languages like Tetun. The variability in output quality across languages underscores the need for careful evaluation and potential supplementary techniques when applying LLMs to lexicographic tasks, especially for less-represented languages.\nA notable challenge revealed in our study is the high variance in personal preferences for example sentence quality, as evidenced by low inter-annotator agreement rates. This variability poses difficulties in establishing a single, universally accepted metric for evaluating dictionary examples. However, our experiments with in-context learning demonstrate that LLMs can be successfully aligned with individual annotator preferences, even for low-resource languages like Tetun. This finding suggests a promising avenue for tailoring LLM outputs to specific lexicographic standards or individual annotator judgements, potentially facilitating the example generation and evaluation process.\nThe low inter-annotator agreement observed in our study highlights the need for annotations from multiple annotators before drawing conclusions about the quality (or lack thereof) of example sentences. This multi-annotator approach can help capture a more comprehensive range of perspectives and mitigate individual biases. Additionally, our findings, particularly for French where most GDEX criteria were rated \"Yes\" due to the high quality of generated sentences, suggest the need for finer measures of criteria to better capture nuanced levels of quality. We recommend developing more granular rating scales or additional sub-criteria, especially for high-resource languages where LLMs perform well. This refinement in evaluation methods could provide more discriminative assessments of LLM-generated example sentences."}, {"title": "6 Conclusion", "content": "We contribute a first evaluation of LLM capability to generate bilingual example sentences, across languages of various resource levels. We show that although LLMs are capable of generating good bilingual example sentences on average, their performance degrades with language resource level. We further show that even when using a shared framework for sentence evaluation (GDEX), annotators tend to disagree with each other on sentence quality, but that in-context learning can be leveraged to align LLMs with a specific annotator's ratings.\nOur findings highlight the potential of LLMs in lowering the cost of lexicographic work, and their ability in aligning with human judgement in a field where human judgement can be highly variable. This is of particular value in low-resource lexicographic work, where lack of human resources may prevent the widespread compilation of lexicographic resources."}, {"title": "Limitations", "content": "While our study shows LLMs can play a helpful role in the generation and rating of bilingual dictionary examples, our choice of experiment constraints can limit the reach of our results. We work exclusively with languages that use Latin script, and with English on the target side, which raises the question of how our results would hold for languages that use other scripts and with lower-resource target languages. We did not include part of speech information when generating examples, and do not study performance on words that have several definitions; both choices may have skewed the quality of generated example downwards.\nThe low inter-annotator agreement, while part of the experiment, and expected in this lexicographic context, raises questions about how we could have better aligned annotators, for example by using pre-qualifying questions, or by exclusively relying on linguists for annotation.\nWe identify several areas for future work. First, LLM rating of example sentences could be integrated in the example generation pipeline, for instance by having an LLM generate a number of candidate examples, and another LLM automatically rank them, similar to the approach by Cai et al. (2024b). Second, the quality of LLM-generated example sentences could be compared against sentences retrieved from a corpus. Last, the incorporation of retrieved sentences in the LLM prompt could guide the LLM to generate more typical or informative sentences."}, {"title": "A Prompts used", "content": "In the prompts below, the parts in brackets (e.g. {SRC_NAME}) are templated out."}, {"title": "A.1 Generating examples", "content": "You are assisting in the creation of a bilingual {SRC_NAME}-{TGT_NAME} dictionary. Your task is to generate example sentences for dictionary entries to help users understand the usage of words in context.\nYou will be provided with a {SRC_NAME} word and its {TGT_NAME} equivalent.\n<{SRC_NAME} entry>\n{{src_word}}\n</{SRC_NAME} entry>\n<{TGT_NAME} entry>\n{{tgt_word}}\n</{TGT_NAME} entry>\nPlease create a pair of example sentences for each entry. The sentences should be:\nTypical: Show typical usage of the word\nInformative: Add value by providing context or additional information\nIntelligible: Be clear, concise, and appropriate for a general audience\nUsing the entries provided above (the {SRC_NAME} and {TGT_NAME} words)\nFormat your response as follows:\n<example_sentence_pair >\n{SRC_NAME}: [Your {SRC_NAME} sentence here]\n{TGT_NAME}: (Your {TGT_NAME} sentence here]\n</example_sentence_pair >\nPlease provide your example sentences based on the given {SRC_NAME} and { TGT_NAME} entries."}, {"title": "A.2 Reasoning about a specific annotator's rating", "content": "<example>\nSrc Entry: {src_entry}\nTgt Entry: {tgt_entry}\nSrc Example: {src_example}\nTgt Example: {tgt_example}\nComment: {comment}\nTypical: {typical}\nInformative: {informative}\nIntelligible: {intelligible}\nTranslation correct: {\ntranslation_correct}\n</example>\nReasoning: what is the reasoning for the above ratings? Give your response in one paragraph."}, {"title": "A.3 In-context learning for aligning an LLM with an annotator", "content": "A.3.1 Prompt construction\nTEMPLATE_EXAMPLE = \"\"\"<example >\n<data>\nSrc Entry: {src_entry}\nTgt Entry: {tgt_entry}\nSrc Example: {src_example}\nTgt Example: {tgt_example}\n</data>\n{reasoning}\n{rating}\n</example>\"\"\"\ndef get_templated_example(row):\nreturn TEMPLATE_EXAMPLE. format(\nsrc_entry=row[SRC_LANG],\ntgt_entry=row[TGT_LANG],\nsrc_example=row['src_example'],\n)\ntgt_example=row['tgt_example'],\nreasoning=row['reasoning'],\nrating=row['Overall rating']\nAUGMENTED_SYSTEM_PROMPT = SYSTEM\nfor row in sample:\nAUGMENTED_SYSTEM_PROMPT +=\nget_templated_example(row)\nAUGMENTED_SYSTEM_PROMPT += '\\n\\n'"}, {"title": "A.3.2 Prompt example", "content": "An example constructed prompt with two examples. Note that our experiments used 10 examples.\nYou are assisting in the creation of a bilingual English-Indonesian dictionary. Your task is to rate a candidate sentence pair that illustrates dictionary entries to help linguists select an appropriate example pair.\nExample sentences should should be:\nTypical: Show typical usage of the word\nInformative: Add value by providing context or additional information\nIntelligible: Be clear, concise, and appropriate for a general audience\nTranslation correct: Are sentences a good translation of each other, with fluent grammar and correct usage of words in both languages\nYou are rating the example sentences, not the dictionary entries.\n<example>\n<data>\nSrc Entry: meriam\nTgt Entry: cannon\nSrc Example: Meriam itu ditempatkan di atas bukit untuk melindungi kota dari serangan musuh.\nTgt Example: The cannon was placed on the hill to protect the city from enemy attacks.\n</data>\nThe example sentences are typical because they demonstrate a standard use of the word \"cannon\" in a military context. However, they are only somewhat informative because the statement about cannons being used for defense, while not entirely inaccurate, might not be the most common understanding. The sentences are intelligible due to their clear and concise language, and the translation is accurate, reflecting the meaning and grammar of both the source and target languages.\n4 Good</rating >\n</example>\n<example>\n<data>\nSrc Entry: menanyai\nTgt Entry: question\nSrc Example: Polisi menanyai saksi mata untuk memperoleh informasi lebih lanjut tentang kejadian itu.\nTgt Example: The police questioned the eyewitness to obtain more information about the incident.\n</data>\nThe ratings are justified because the sentences demonstrate typical usage of the words \"menanyai\" and \"questioned\" in the context of a police investigation. They are informative by providing context about the purpose of the questioning. Both sentences are clear and concise, making them intelligible. However, the translation is slightly off because \"keterangan\" would be a more natural choice than \"informasi\" in Indonesian, making the translation somewhat less accurate.\n4 Good</rating >\n</example>\n<data>\nSrc Entry: sehari-hari\nTgt Entry: everyday\nSrc Example: Saya menggunakan sepeda sebagai alat transportasi sehari-hari karena lebih ramah lingkungan.\nTgt Example: I use a bicycle as my everyday mode of transportation because it's more environmentally friendly.\n</data>"}]}