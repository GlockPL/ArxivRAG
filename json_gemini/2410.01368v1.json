{"title": "Theoretical Lower Bounds for the Oven Scheduling Problem", "authors": ["Francesca Da Ros", "Marie-Louise Lackner", "Nysret Musliu"], "abstract": "The Oven Scheduling Problem (OSP) is an NP-hard real- world parallel batch scheduling problem arising in the semiconductor industry. The objective of the problem is to schedule a set of jobs on ovens while minimizing several factors, namely total oven runtime, job tardiness, and setup costs. At the same time, it must adhere to various constraints such as oven eligibility and availability, job release dates, setup times between batches, and oven capacity limitations. The key to obtaining efficient schedules is to process compatible jobs simultaneously in batches. In this paper, we develop theoretical, problem-specific lower bounds for the OSP that can be computed very quickly. We thoroughly examine these lower bounds, evaluating their quality and exploring their integration into existing solution methods. Specifically, we investigate their contribution to exact methods and a metaheuristic local search approach using simulated annealing. Moreover, these problem-specific lower bounds enable us to assess the solution quality for large instances for which exact methods often fail to provide tight lower bounds.", "sections": [{"title": "1 Introduction", "content": "The semiconductor manufacturing sector has been identified as one of the most energy-intensive industries [18], particularly in the context of hardening elec- tronic components in specialized heat treatment ovens. To mitigate energy con- sumption, one strategy involves grouping and processing compatible jobs to- gether in batches to optimize resource utilization. Such scheduling tasks that aim to increase efficiency by processing multiple jobs simultaneously in batches are known as batch scheduling problems.\nOver the last three decades, the scientific community has extensively inves- tigated batch scheduling problems, as witnessed by the surveys by Mathirajan and Sivakumar [15], Fowler and M\u00f6nch [4]. A multitude of problem variants, in the single or parallel machine setting, and each with distinct constraints and"}, {"title": "2 The Oven Scheduling Problem", "content": "The OSP aims to group compatible jobs into batches and devise an optimal schedule for these batches across a set of ovens. We report an abridged descrip-"}, {"title": "2.1 Solution methods for the OSP", "content": "In the literature, the OSP has been solved with a construction heuristic [10], exact methods [10], and a SA algorithm [11] which we very briefly describe here.\nThe construction heuristic introduced to solve the OSP [10] is a dispatching rule that prioritizes jobs based on their release dates and then on their due dates. The algorithm starts at time 0. At each time step, it compiles the list of currently available machines and currently released jobs that have not yet been scheduled.\nThe algorithm then selects the job with the earliest due date from this pool and greedily assigns it to one of the eligible machines. Once a job is scheduled, other available jobs are included in the same batch, provided that the job's attribute, processing time, and the machine's capacity allow it. If no job can be scheduled, the time is incremented by one, and the process is repeated. This heuristic has been used to warm-start the exact methods with some of the solvers [10] and as an initial solution for the SA approach [11].\nTwo exact modeling approaches which were formulated as CP and ILP mod- els were proposed by Lackner et al. [10]. The first approach is based on batch positions: each job is assigned to one of the possible batches, which are uniquely characterized by their machine and the batch position on this machine. The constraints are formulated on the level of batches and an optimal schedule of the batches needs to be found. The second uses a unique representative job for each batch and seeks an optimal schedule for these jobs. These two model- ing approaches are implemented both in the high-level solver-independent mod- eling language MiniZinc [16] and using interval variables in the Optimization Programming Language (OPL) [5] used by CP Optimizer. Moreover, different state-of-the-art solvers, search strategies, and a warm-start approach leverag- ing the construction heuristic were employed. Ultimately, the best results were achieved with CP Optimizer and the OPL-model using representative jobs as well as with Gurobi and the MiniZinc-model with batch positions. In what fol- lows, we will refer to these two solution methods as \"cpopt\" and \"mzn-gurobi\" (as well as \"cpopt-WS\u201d and \u201cmzn-gurobi-WS\u201d for the variants with warmstart).\nA SA algorithm for the OSP was proposed by Lackner et al. [11]. In this algorithm, a solution to the OSP is represented by the assignments of jobs to ovens and by the processing order of the jobs on their respective machines. The schedule of the batches on the ovens is then deterministically constructed"}, {"title": "3 Lower bounds on the optimal solution cost", "content": "In this section, we describe a procedure to calculate lower bounds on the optimal solution cost for a given instance of the OSP. Our main focus lies in bounding the number of batches required in any feasible solution. At the same time, we derive bounds on the cumulative batch processing time. These lower bounds serve as a basis for deriving lower bounds on the cumulative setup costs. Finally, we provide a brief discussion on the number of tardy jobs."}, {"title": "3.1 Minimum number of batches required and minimal cumulative batch processing time", "content": "Since jobs can only be combined in a batch if they share the same attribute, bounds on the number of batches required are calculated independently for all attributes. For a given attributer \u2208 A, we denote by $b_r$ the number of batches in a feasible solution and by $p_r$ the minimal cumulative processing time of batches.\nDue to the capacity constraints of machines, a simple bound on the number of batches required is\n$b_r \\geq \\frac{\\sum_{j\\in J: a_j=r} s_j}{\\text{max}_{m\\in M}{C_m}}$,\n(1)\nas stated by Koh et al. [7]. This corresponds to the minimal number of batches required if we assume that jobs can be split into smaller jobs of unit size and that all jobs can be scheduled on the machine with the largest machine capacity.\nThis bound can be tightened by distinguishing between \"large\" and \"small\" jobs (in a similar fashion as Damodaran and V\u00e9lez-Gallego [1], Li et al. [12, 13]). Large jobs are those jobs that are so large that they cannot accommodate any other jobs in the same batch and thus need to be processed in a batch of their own. All other jobs are referred to as small jobs. For a given attribute r, the sets of large jobs $J_l^r$ and small jobs $J_s^r$, with attributer are thus defined as follows:\n$J_l^r = \\{j \\in J: a_j=r, s_j + s_i > \\text{max}_{m\\in E_j}(C_m) \\forall i \\in I \\text{ with } i\\neq j \\text{ and } a_i = r\\}$,"}, {"title": "3.2 Bounds on the other components of the objective function", "content": "If we assume that the setup costs before batches of a given at- tribute are always minimal, we obtain the following bound on the setup costs:\n$sc \\geq \\sum_{r=1}^a b_r \\min_{s\\in\\{1,...,a\\}} \\{sc(s,r)\\}$.\n(8)\nA similar bound can be derived assuming that the setup costs after batches are always minimal. For this case, we include initial setup costs for all machines to which batches are scheduled and ignore the last batch on every machine. Since a prior it is not known which machines are used in a schedule, we cre- ate the list setup_costs as follows. For every attribute r, we add $b_r$ copies of $\\min_{s\\in \\{1,...,a\\}}\\{sc(r, s)\\}$ to setup_costs. Moreover, for every machine m, we add the element $\\min_{s\\in\\{1,...,a\\}}\\{sc(i_{am},s)\\}$ to setup_costs. The list is then sorted in non-decreasing order and the sum of the first b elements is taken:\n$SC \\geq \\sum_{i=1}^b setup_costs(i)$.\n(9)\nAltogether, we have the following lower bound on the setup costs\n$sc > max (\\sum_{r=1}^a b_r \\min_{s\\in\\{1,...,a\\}} \\{sc(s,r)\\}, \\sum_{i=1}^b setup\\_costs(i))$.\n(10)\nNote that it is impossible to obtain a lower bound on the setup costs by ar- ranging the minimum number of batches per attribute (as calculated previously) in an order that minimizes the cumulative setup costs. Indeed, if the matrix of setup costs does not fulfill the triangle inequality, it can be advantageous to introduce additional batches if the sole objective is to reduce setup costs.\nRegarding the number of tardy jobs, direct inference from the instance itself may be limited. However, we can obtain a lower bound on the number of tardy jobs by independently scheduling each job in a batch on its own on the first available machine and computing the completion time. Any job finishing after its latest end date is necessarily tardy in every solution."}, {"title": "4 Including lower bounds in solution methods", "content": "A recommended practice to build efficient exact models is to tightly restrict and bound the domain of variables (as suggested, for instance, by the MiniZinc guide on efficient modeling practices\u00b3).\nBy employing tighter variable bounds, algorithmic efficiency can be signifi- cantly enhanced, facilitating faster convergence to optimal solutions or the iden- tification of unfeasible regions. When solving the OSP with one of the exact methods, the lower bounds derived in Section 3 can be calculated in a prepro- cessing step and can then be provided to the model as part of the input data. The range of the variables corresponding to the individual objective components as well as the variable for the aggregated objective function can thus be bounded from below. Moreover, the aggregated objective value of the solution delivered by the construction heuristic can be used to bound the range of the objective function from above.\nProblem-specific lower bounds can also have practical applications in meta- heuristic algorithms, e.g., in SA. Lower bounds can be used to guide the search, e.g., as part of the termination criterion. This strategy allows for early interrup- tion of the process, sparing computational resources while still achieving satis- factory solution quality"}, {"title": "5 Experimental evaluation", "content": "In this experimental evaluation, we aim to analyze the quality of the theoretically derived lower bounds and their practical usefulness in helping to solve the OSP."}, {"title": "5.1 Benchmark instances", "content": "We consider the 80 benchmark instances by Lackner et al. [9], which differ per number of jobs (10, 25, 50, or 100), number of machines (2 or 5), and number of attributes (2 or 5).\nMoreover, we consider 40 new instances featuring a larger number of jobs (250 or 500) to reflect real-world scenarios better. This new set is generated using the specifications of the random instance generator provided by Lackner et al. [8].\nFor tuning purposes (i.e., when using SA), we generate 25 additional instances with similar characteristics as the initial benchmark set."}, {"title": "5.2 Experimental setup", "content": "We consider the following methods for the OSP:"}, {"title": "5.3 Lower bounds quality", "content": "Our objective is to assess the tightness of the calculated lower bounds. We ex- amine the bound on the overall cost (obj) as well as the bounds on its three components individually (t, p, and sc). For the smaller benchmark instances with up to 100 jobs and the aggregated objection function, we refer to the best results per instance obtained by Lackner et al. [10] with their proposed exact methods. For the larger benchmark instances with 250 or 500 jobs, we rerun the best-performing exact methods (\u201cmzn-gurobi\u201d and \u201cmzn-gurobi-WS\u201d as well as \"cpopt\" and \"cpopt-WS\u201d) and retrieve the best result per instance. Moreover, we run the exact models with the task of optimizing just one of the three com- ponents for the entire benchmark set. In our analysis of the lower bounds, we differentiate between those instances and objectives where an optimal solution cost is known and those where we do not know the optimum.\nFor those instances and objectives where the optimum solution is known, given an instance i, we compute the relative gap(i) between the calculated lower bound b(i) and the optimal cost s(i); specifically gap(i) = 100\u00b7 (s(i) \u2013 b(i))/s(i).\nResults show the general tendency that the larger the instances, the smaller the gap (see Figure 2). Concerning the individual components, we observe that most room for improvement is left for the simple bounds for sc and t. Nonetheless, the gap for sc is less than 25% for more than half of the instances and the gap for t is less than 10% for 74% of the instances. For the cumulative processing times, the gap is less than 25% for 88% of instances and less than 10% for 61%. The"}, {"title": "5.4 Measuring solution quality", "content": "In this section, we use the lower bounds to assess the solution quality and benchmark the best-known solutions for the OSP with the best lower bounds. On the one hand, we consider the best solution found for each instance by the methods described in Section 2.1. On the other hand, we consider the best lower bound per instance among the calculated bounds and the ones retrieved by the exact methods. Then we calculate the relative gap between the best solution and the best lower bound per instance. Results are shown in Figure 3. Almost all small instances with 25 or 50 jobs could be solved optimally. For larger instances, the solution methods find very good solutions (with a relative gap[%] \u2264 1%) for roughly half the instances. For most of the remaining instances, the gap is larger than 5%, showing that there is still room for improvement-both in terms of the solution quality and in terms of the lower bound quality."}, {"title": "5.5 Application of lower bounds", "content": "We aim to understand whether using the calculated problem- specific lower bounds allows the exact methods to improve their results. As described in Section 4, we perform experiments where the objective function and its components are bounded from below by the calculated lower bounds. Moreover, we perform experiments additionally supplying the solvers with the upper bound on the objective obtained from the greedy construction heuristic.\nTable 2 presents results categorized by methods and types of bounds included; it displays: the number of instances for which the optimal solution, when known, was reached (\"optimal\"); the number of instances for which a feasible solution was found (\"solved\"), the number of instances for which the method could prove optimality (\"proven opt\"); the number of instances for which the best solution was found (\"best\"); the number of instances for which the best lower bound could be found (\"best lower bound\"); the average run time (\"avg rt\") and its standard deviation (\"std rt\") in seconds. Note that for the number of best solutions found and of best lower bounds found, the comparison is made among a single solution method, i.e., comparing results obtained when no non-trivial bounds are pro- vided, when lower bound and when lower and upper bounds are provided. The statistics regarding runtime are calculated for the subset of instances for which"}, {"title": "Local search", "content": "Lower bounds provide a means to assess whether it is feasible to halt the search before reaching the termination criterion\u2014in our case, the timeout. We aim to discern under which circumstances this is viable and how much time is necessary. Considering the overall cost, for 50 out of 120 instances, the gap[%] is lower than 1% (average time required 15.52 \u00b1 39.85 s); for 60, the gap[%] is lower than 5% (average time required 3.86 \u00b1 20.21 s), and for 67, it is lower than 10% (average time required 11.13 \u00b1 34.92 s). This means that for roughly half of the benchmark instances, the search could be terminated early, delivering a solution of good quality. It is worth pointing out that this is merit also of a demonstrably good initial solution (see Appendix A.5)."}, {"title": "6 Conclusion", "content": "In this study, we introduced a procedure for calculating theoretical lower bounds for the OSP which can be calculated within a couple of seconds even for large instances. The experimental evaluation demonstrated their quality and practical utility when incorporated into exact methods or LS approaches. Our bounds can help to find better solutions, to deliver more optimality proofs, and to find high-quality solutions in a shorter time.\nNotably, some of the bounds we developed are relatively simple, in particular those concerning job tardiness. This suggests that there is potential for further enhancements by refining these lower bounds with more sophisticated meth- ods. Therefore, future extensions will focus on improving the presented bounds. Additionally, we aim to explore adaptive local search techniques, wherein neigh- borhood probabilities dynamically adjust based on the proximity to the lower bounds. Moreover, investigating alternative use cases, such as employing differ- ent weight sets on the objective function, may offer valuable insights."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Example of an OSP Instance.", "content": "To better exemplify the problem, let us consider the following randomly created instance consisting of 10 jobs (n = 10), 2 machines (k = 2), and 2 attributes (a = 2). It presents the following characteristics:"}, {"title": "A.2 Formal statement and proof of the correctness of the GAC-bounds described in Section 3.1", "content": "In the following, we formulate the bounds described in the Section entitled Alter- native refinement of the bound for small jobs based on compatible job processing times (starting on page 8) more formally and prove the correctness of the algo- rithm GAC+.\nFirst, let us recall the compatibility requirement expressed in equation (5).\nNow let us consider the following special case of the OSP:"}, {"title": "OSP*", "content": "Given a set of jobs I of unit size defined by their minimal and maximal processing times, i.e. j = [mintj,maxt;] for all j \u2208 I, and a single machine with capacity c\u2208 N, how many batches do we need at least in order to process all jobs if jobs can only be processed in the same batch if the compatibility condition (5) is fulfilled?\nSeveral variants of this problem have been studied in the literature, e.g. by Finke et al. [3]; the variant that we are interested in corresponds to the problem (P2) there. Solving the problem OSP* will allow us to obtain lower bounds for the OSP: Indeed, as in equation (1), we obtain lower bounds on the number of batches required and their processing times if we assume that jobs can be split into smaller jobs of unit size and that all jobs can be scheduled on the machine with largest machine capacity.\nAs stated in Section 3, equation (5) between jobs can be represented with the help of a compatibility graph G = (V,E), where V is the set of all jobs I and (i, j) \u2208 E if and only if the jobs i and j have compatible processing times. In this graph, a batch forms a (not necessarily maximal) clique. The problem of solving an OSP instance with unit-sized jobs and a single machine with capacity c is thus equivalent to covering the nodes of the compatibility graph with the smallest number of cliques with size no larger than c.\nA simple greedy algorithm to solve this problem is provided by Finke et al. [3] and referred to as the algorithm GAC (greedy algorithm with compatibility). By adapting the order in which jobs are processed by the GAC algorithm, we obtain an algorithm that minimizes both the number of batches and the cumulative batch processing time. We call this algorithm GAC+.\nAlgorithm GAC+: Consider the set of jobs I in non-increasing order j1, j2, ..., \u00cen of their minimal processing times mint;, breaking ties arbitrarily. Construct one batch per iteration until all jobs have been placed into batches. In iteration i, open a new batch Bi and label it with the first job j* that has not yet been placed in a batch. Starting with j* = [mint;*, maxt;*], place into B\u2081 the first c not yet scheduled jobs j for which mintj* \u2208 [mintj,maxtj] (or all of them if there are fewer than c).\nFor a set I of unit size jobs and a maximum batch size c\u2208 N, we denote by GACb(I, c) the number of batches returned by the GAC+ algorithm above. Similarly, let GACp(I, c) denote the minimal processing time returned by the GAC+ algorithm for this instance.\nTheorem 1. For any given set of unit size jobs I and for any given constant c \u2208 N, Algorithm GAC+ solves the problem OSP*, i.e., GACb(I, c) is the minimum number of batches required under the condition that a batch may not contain more than c jobs. Moreover, the cumulative batch processing time GACp(I, c) is minimal."}, {"title": "A.3 Detailed example for the calculation of lower bounds", "content": "We consider the example instance described in Appendix A.1 to exemplify the calculation of the problem-specific lower bounds on the objective function as derived in Section 3.\nThe values of the lower bounds for the number of batches required and the cumulative batch processing times are summarized in Table 4 on page 22. We explain their calculation in what follows. The sets of large jobs are J\u2081 = \u00d8 and J\u2082 = {1, 2, 3, 6}, we thus need 4 batches for the large jobs of attribute 2 and none for attribute 1. The processing times for large batches are given by the minimal processing times of the large jobs and contribute 11+10+19+19 = 59 to the cumulative batch processing time.\nFor the processing time of small jobs, we exemplify the calculation of the bound based on eligible machines for attribute 1 and the one of the bound based on compatible processing times for attribute 2. For attribute 1, we have three small jobs (4, 9, and 10) of which job 4 can only be processed on machine 1 and job 9 only on machine 2. Two different batches are thus required for these jobs.\nAfter the warm start a machine can be used every moment"}, {"title": "A.4 Details concerning the experimental setup", "content": "We consider the theoretical lower bounds as presented in Section 3. The code is implemented in C#. The experiments are executed on a machine featuring an Intel Core i7-1185G7 processor with 3.00GHz. Each run is executed on a single thread.\nWe consider the construction heuristic proposed by Lackner et al. [10] (see Section 2.1). The solution method is implemented in C++. The code is compiled with Clang++15. All experiments are executed on a machine featuring 2x Intel Xeon Platinum 8368 2.4GHz 38C, 8x64GB RDIMM. Each run is executed on a single thread.\nWe consider the exact methods proposed by Lackner et al. [10] (see Sec- tion 2.1). The \u201ccpopt\u201d is implemented with CPLEX Studio 22.11, whereas \u201cmzn- gurobi\u201d uses Minizinc 2.8.2 Gurobi 10.0.1. All experiments are executed on a machine featuring 2x Intel Xeon CPU E5-2650 v4 (12 cores @ 2.20GHz, no hyperthreading).\nWe consider the SA proposed by Lackner et al. [11] (see Section 2.1). The SA is implemented using EasyLocal++, a C++ framework for LS algorithms [2]. The code is compiled with Clang++15. All experiments are executed on a machine featuring 2x Intel Xeon Platinum 8368 2.4GHz 38C, 8x64GB RDIMM. Each algorithm is executed on a single thread. The algorithm is tuned using irace (v.3) [14]."}, {"title": "A.5 Evaluation of the upper bounds provided by the construction heuristic", "content": "It is important to note that if the construction heuristic successfully schedules all jobs, as is the case for all our benchmark instances, the resulting solution"}]}