{"title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models", "authors": ["Bozhong Tian", "Xiaozhuan Liang", "Siyuan Cheng", "Qingbin Liu", "Mengru Wang", "Dianbo Sui", "Xi Chen", "Huajun Chen", "Ningyu Zhang"], "abstract": "Large Language Models (LLMs) trained on extensive corpora inevitably retain sensitive data, such as personal privacy information and copyrighted material. Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters. Experimental results show that MemFlex is superior to existing methods in both precise knowledge unlearning and general knowledge retaining of LLMs.", "sections": [{"title": "1 Introduction", "content": "Forgetting is a crucial brain function that eliminates unnecessary information to maintain neural system integrity (Small, 2021; Farrell, 2022). In parallel, Large Language Models (LLMs) (Ouyang et al., 2022; Zhao et al., 2023; OpenAI, 2023) inevitably incorporate sensitive data during training, which is not essential for their functionality (Yao et al., 2023a, 2024; Li et al., 2024b; Zhang et al., 2024a; Liu et al., 2024b). Therefore, removing sensitive knowledge from LLMs is imperative for ensuring the safety and integrity of these systems. The most straightforward solution involves removing such data from pre-training corpora and retraining LLMs, although this method is expensive and time-consuming. Another approach, alignment methods like reinforcement learning from human feedback (RLHF) (Bai et al., 2022), is computationally expensive and requires extensive, high-quality human feedback (Casper et al., 2023).\nConsequently, recent research has primarily focused on knowledge unlearning (Chen and Yang, 2023; Eldan and Russinovich, 2023; Si et al., 2023; Liu, 2024; Li et al., 2024a; Huang et al., 2024; Zhao et al., 2024b; Sha et al., 2024), which facilitates efficient, post-training forgetting in models. However, current evaluation paradigms are limited, typically failing to consider the extent of forgetting, instead simply unlearning all related knowledge regarding factual instances. Psychological research (ROEDIGER III et al., 2010; Storm, 2011) emphasizes that forgetting is a natural and necessary process that helps focus on essential knowledge. Education literature (Sharek and Wiebe, 2011; Sha et al., 2024) also suggests that regulating the extent of forgetting can enhance learning. Under the United States Code (USC) (U.S., 2018), specifically 17 U.S.C. \u00a7\u00a7 106(2), 107, 302, copyright owners are granted protections, yet the \u201cfair use\u201d principle permits certain uses such as criticism and commentary without explicit permission. Additionally, \u201cRight to Deletion\" and \"Right to Access\u201d under California Consumer Privacy Act (CCPA) (California, 2018), along with \u201cRight to Erasure\u201d and \u201cData Minimization\" under General Data Protection Regulation (GDPR) (Europe, 2016), mandate protecting users' privacy while still allowing the retention of necessary public information. These principles underline the importance of carefully considering how to retain or erase data. For instance, as shown in Figure 1, knowledge related to \"Where is J.K. Rowling currently living?\u201d involves personal information and should be forgotten, whereas knowledge for answering \"What is J.K. Rowling's most representative work?\u201d falls in the public domain and should be retained for understanding her contribution.\nHowever, it remains unclear whether existing unlearning methods can adequately differentiate the unlearning and retaining knowledge of instances. Thus, we propose Knowledge Unlearning with Differentiated Scope in LLMS (KnowUnDo), a novel benchmark for more nuanced evaluations of knowledge unlearning methods, particularly in copyrighted content and user privacy domains. KnowUnDo categorizes knowledge regarding instances into Unlearn Scope and Retention Scope based on copyright and privacy laws. Unlearning methods should forget knowledge in Unlearn Scope while retaining knowledge in Retention Scope. We have also developed metrics, including Unlearn Success and Retention Success to evaluate the differentiation performance of unlearning methods under our benchmark. Current unlearning methods, such as Gradient Ascent (GA) (Jang et al., 2023), unlearn factual instance knowledge but also result in the loss of general knowledge. To address this, the GA with Mismatch method (Yao et al., 2023a) improves by introducing KL divergence or Gradient Descent on general knowledge. However, these methods suffer from updating parameters indiscriminately, which fails to differentiate the scope between unlearning and retaining.\nTo this end, we introduce MemFlex, a novel strong baseline that utilizes gradient information to pinpoint the Unlearn Scope and Retention Scope within the model's parameter space. MemFlex precisely erases sensitive parameters, enabling LLMs to have a more flexible memory. Our experimental results demonstrate that MemFlex outperforms existing methods in identifying these scopes with minimal impact on the model's general capabilities. Additionally, it significantly reduces the consumption of training resources. Specifically, MemFlex improves the Success by an average of 7.97% when unlearning LLaMA2-7B-Chat and Qwen-1.5-7B-Chat in both domains. Furthermore, it achieves an 11.76% reduction in training time per step."}, {"title": "2 Benchmark Construction", "content": "2.1 Task Definition\nWe denote an LLM as M, characterized by its parameters \u03b8, forming Me. Specifically, Me is represented by a function that maps the input x to its corresponding prediction y, as described below:\n$y = M_\\theta(x)$\n$\\qquad\\qquad\\qquad \\qquad\\qquad y = \\prod_{i=1}^|y| P_\\theta \\big(y_i \\mid y_{<i}, x\\big),$\t(1)\nwhere $P_\\theta$ denotes the probability of generating the next token in the sequence, and $y_{<i} = \\{ y_1, \\dots, y_{i-1} \\}$. Given an unlearned descriptor $(x_u, y_u)$ related to an unlearning instance $I$ (e.g., copyrighted content or public figures). Current approaches often indiscriminately update \u03b8 to \u03b8' to ensure that all responses, $y'_u = M_\\theta'(x_u)$, related to $I$ are non-harmful. However, not all knowledge associated with $I$ needs to be forgotten. Thus, we define the unlearning process as follows:\n$M_{\\theta'}(x) =\\begin{cases}y_u & \\text{if } x \\in U(x_u, y_u) \\\\M_{\\theta}(x) & \\text{if } x \\in R(x_u, y_u) \\\\M_{\\theta}(x) & \\text{Otherwise,} \\end{cases}$\t(2)\nwhere $U(x_u, y_u)$ and $R(x_u, y_u)$ are the Unlearn Scope and Retention Scope for $(x_u, y_u)$ shown in Figure 2. \"Otherwise\" pertains to knowledge outside these scopes."}, {"title": "2.2 Dataset Construction", "content": "We develop a more practical benchmark equipped with valid evaluation metrics. To the best of our knowledge, we are the first to introduce a benchmark that explores the unlearning and retaining scopes of knowledge regarding factual instances. We further classify such knowledge, unlearning only those within the Unlearn Scope and allowing responses within the Retention Scope, as shown in Figure 2. Additionally, current benchmarks (Maini et al., 2024; Yao et al., 2024) typically consider copyrighted content and user privacy separately. Our benchmark integrates both aspects to provide a comprehensive evaluation of unlearning methods. Our dataset construction is illustrated in Figure 3. We also manually verify the datasets in both domains of our benchmark.\n2.2.1 Copyrighted Content\nSampling Copyrighted Instances. In constructing the dataset, our initial step involves selecting copyrighted books from the GoodReads \u201cBest Books Ever\" list, and choosing books based on popularity and genre diversity to ensure a representative sample. After identifying the target books, we input their titles into GPT-4 API to generate related author information and book overviews for checking. We then cross-referenced this generated information with Wikipedia to assess the accuracy of GPT-4's comprehension. As GPT-4 is the most powerful LLM, we only filter two erroneous books.\nEnsuring Unlearn and Retention Scope. Under the United States Code (USC) (U.S., 2018), 17 U.S.C. \u00a7 106(2) grants copyright owners the exclusive right to prepare derivative works based on the protected work. Unauthorized Revision or Extension of such works may infringe this right and are thus categorized under the Unlearn Scope. Conversely, 17 U.S.C. \u00a7 107 establishes the \"fair use\" principle, permitting the use of copyrighted material without authorization for purposes like criticism, and commentary. Review, Recommendation and non-creative Meta-Info typically qualify as fair use and are placed within the Retention Scope. Additionally, instances that have entered the public domain due to copyright expiration, as outlined in 17 U.S.C. \u00a7 302, are also classified under the Retention Scope.\n2.2.2 User Privacy\nDue to the risks associated with using real privacy data, we construct a dataset of fictitious author information following Maini et al. (2024) and fine-tune the model on this dataset to establish a foundation for conducting further experiments.\nThe process of constructing fictitious author information is as follows. First, we manually construct examples of fictitious authors and use these as a demonstration for prompting GPT-4 to generate data of fictitious authors based on predefined attributes such as Name, Genre, Born, Awards, Parents, Email, and Address. According to the \u201cRight to Deletion\u201d and \u201cRight to Access\u201d under CCPA (California, 2018), and the \u201cRight to Erasure\" and \"Data Minimization\u201d principles under GDPR (Europe, 2016), we should retain essential information about public figures, such as their Name, Genre, Born, and Awards, which are categorized under the Retention Scope. These details are necessary to understand their contributions. Conversely, their private information, including Parents, Email, and Address, does not contribute to this understanding and therefore falls into the Unlearn Scope. Using these categories, we prompt GPT-4 to generate corresponding question-answer pairs, with the specific template provided in the Appendix B.2. The generated question-answer pairs (xu, Yu) form our dataset Dpriv, which includes {D, D},"}, {"title": "2.3 Evaluation Metrics", "content": "2.3.1 Evaluation for Unlearning\nOur evaluation metrics, as referenced in Meng et al. (2022); Mitchell et al. (2022); Zhang et al. (2024a); Yao et al. (2024), include Unlearn Success, Retention Success, and Perplexity.\nUnlearn Success: We define a metric named Unlearn Success to measure the success of unlearning by the average accuracy of the Unlearn cases:\n$\\mathbb{E}_{x_u,y_u\\sim D^{UL}}\\mathbb{I}\\big[argmax_y P_\\theta'(y \\mid x_u) \\neq y_u \\big],$t(3)\nwhere $D^{UL}$ refers to $D^{UL}_{cpyr}$ and $D^{UL}_{priv}$. The unlearned model $M_{\\theta'}$ should not be able to predict correctly for unlearned knowledge.\nRetention Success: We also define a metric named Retention Success to measure the success of retaining, assessed by the average accuracy in the Retention cases:\n$\\mathbb{E}_{x_u,y_u\\sim D^{RT}}\\mathbb{I}\\big[argmax_y P_\\theta'(y \\mid x_u) = y_u \\big]$\t(4)\nIdeally, $M_{\\theta'}$ should retain its performance on Retention Scope with the original one $M_\\theta$, indicating that the unlearning process is under control.\nPerplexity: We use Perplexity to measure the model's prediction complexity, defined as:\n$\\text{Perplexity} = 2^{-\\frac{1}{|y|}\\sum_{i=1}^{|y|}\\log_2 P_\\theta \\big(y_i \\mid y_{<i},x'\\big)}$\t(5)\n2.3.2 General Task Performance\nThe unlearning process may unintentionally introduce side effects to LLMs in unrelated areas. Therefore, to assess the impact comprehensively, we also evaluate the capabilities of the unlearned model across a variety of general tasks, which span Knowledge Understanding, Truthfulness, and Knowledge Reasoning, referring to the classification schema of the related works (Contributors, 2023; Gao et al., 2023; Beeson, 2024).\nKnowledge Understanding. We use Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) and ARC Challenge (Clark et al., 2018) to evaluate the LLM's understanding and application of knowledge.\nTruthfulness. The TruthfulQA (Lin et al., 2022) dataset assesses the LLM's ability to generate truthful and reliable answers to questions.\nKnowledge Reasoning. The SIQA (Sap et al., 2019) measures the model's commonsense reasoning in social contexts, testing its ability to reason logically. We also select ReAding Comprehension Dataset From Examinations (RACE) (Lai et al., 2017) for evaluation, which focuses on the model's capability to analyze complex texts.\nAll general tasks are evaluated using The Language Model Evaluation Harness tool (Gao et al., 2023) for fair comparisons."}, {"title": "3 Baselines", "content": "3.1 Overview\nAs discussed in Section 2.1, LLM unlearning ensures the model effectively forgets the data in the Unlearn Scope while retaining performance in the Retention Scope. We use an unlearning framework for LLMs (Yao et al., 2024) under MIT License. To unlearn sequences in DUL, we update the current model Me using the gradient derived from:\n$\\sum_{x_u,y_u\\in D^{UL}} \\sum_{i=1}^{|y_u|} \\log P_\\theta(y \\mid y_{<i}, x_u)$\t(6)\n$+\\sum_{x_u,y_u\\in D^{RT}} \\sum_{i=1}^{|y_u|} \\log P_\\theta'(y' \\mid y_{<i}, x_u)$\nWe focus on the first-order approximate unlearning methods, which rely on gradient information and are often more efficient than exact unlearning and second-order methods.\n3.2 Approximate Unlearning Methods\nGradient Ascent Removing the secondary component from Eq. 6 and reversing the gradient's direction leads to the gradient ascent method, used to forget specific data subsets. Effective for small datasets, it is applied for a few epochs to avoid degrading overall model performance (Golatkar et al., 2020; Jang et al., 2023).\nFine-tuning with Random Labels This method updates the model's weights by training on randomly labeled data, which ignores the second term in Eq. 6 and simulates the effect of removing targeted knowledge, typically reducing general performance. Similarly to the gradient ascent, it is applied for a few epochs.\nUnlearning with Adversarial Samples This method generates adversarial tokens to confuse the model and unlearn specific sequences effectively."}, {"title": "3.3 The Proposed Strong Baseline: MemFlex", "content": "Inspired by knowledge localization of model editing (Dai et al., 2022; Meng et al., 2022; Yao et al., 2023b; Chen et al., 2024b), we introduce a novel unlearning method that identifies pivotal parameter regions for \u201cforgetting\u201d and \u201cretaining\u201d. Building on this, and further inspired by Yu et al. (2023) and Fan et al. (2023), we leverage gradient information to enhance the precision of localization. For instance, to pinpoint for forgetting, we proceed as follows:\nGiven $(x_u, y_u) \\in D^{UL}$, the label $y_u$ is substituted with a random one to form $(x_u, \\tilde{y_u})$.\nGradient information $g \\gets \\nabla_\\theta L(x_u, \\tilde{y_u})$ is harvested through back-propagation.\nThis process of random substitution and back-propagation is iterated five times, culminating in an average that yields a stable Unlearn gradient matrix $G^{UL} = \\frac{1}{N} \\sum_{i=1}^N g_i$.\nA similar procedure is applied to pinpoint for retaining to obtain a Retention matrix $G^{RT}$. Following Liu et al. (2024a) and Tian et al. (2024), we analyze the gradient information by its two constituents: direction and magnitude. We hypothesize that a close resemblance in direction between Retention and Unlearn Scopes suggests potential disruption with retention knowledge during the unlearning process, measured as:\n$\\text{COS} \\big(G^{UL}, G^{RT} \\big) = \\frac{\\big(G^{UL}, G^{RT}\\big)}{\\|G^{UL}\\| \\|G^{RT}\\|}$\t(8)"}, {"title": "4 Experiment", "content": "4.1 Settings\nWe conduct experiments using LLaMA-2-7B-Chat (Touvron et al., 2023) and Qwen-1.5-7B-Chat (Bai et al., 2023), fine-tuning these models on our datasets with LoRA (Hu et al., 2021), a method enhancing model adaptation without extensive training, as our base models.\n4.2 Results\nResults on User Privacy. As shown in Tables 2 and 4, the base models perform well with high success and low perplexity, showing effective knowledge integration, while the unlearned models show a decline in performance. GA and Fine-tuning with Random Labels (Random Labels) successfully unlearn sensitive knowledge but fail to retain essential information, leading to significant drops in Retention Success. This performance degradation underscores the challenge of distinguishing between Unlearn and Retention Scopes.\nUnlike GA and Random Labels, which cause high perplexity by altering learning distributions, Unlearning with Adversarial Samples (Adversarial, Adv) mimics the original distribution, maintaining general knowledge and low perplexity but struggles with unlearning or retaining. A combined approach of gradient ascent and descent achieves moderate success in differentiating scopes while maintaining stable performance on general tasks. Additionally, applying gradient descent to in-distribution (Retention, ID) rather than out-of-distribution (OOD) data more effectively distinguishes scopes but slightly lowers general performance. Our method, which identifies the most effective differentiation between Unlearn and Retention Scopes, achieves the best balance in retaining the model's retention and general knowledge, despite only modest Unlearn Success. This indicates that our approach not only distinguishes scopes more clearly but also retains the model's essential functionality. The case study is shown in Tables 9 and 11.\nResults on Copyrighted Content. As shown in Tables 3 and 6, these unlearning methods demonstrate similar trends for copyright as observed for privacy, confirming their general applicability. Notably, since copyright knowledge is in both the extension module and original model parameters, focusing unlearning solely on the extension results in confusion and higher perplexity compared to privacy-related unlearning. The case study is shown in Tables 8 and 10.\nEfficiency. Knowledge unlearning should minimize the training time and GPU resources without degrading performance. As shown in Table 5, our method significantly improves the unlearning performance with enhanced efficiency by updating parameters within Unlearn Scope instead of updating all parameters.\n4.3 Analysis\nIn this section, we explore why knowledge localization effectively improves LLM unlearning.\nFinding 1: Knowledge Localization Ensures High Retention Success. We compare Unlearn Success, Retention Success, and Perplexity across different methods during the unlearning process. As illustrated in Figure 4, our method maintains high Retention Success with a stable curve throughout the process, whereas other methods significantly degrade overall performance due to excessive parameter updates. Our method's stability stems from precisely localizing critical regions necessary to retain overall performance. In contrast, other approaches tune all model modules indiscriminately, causing irreversible performance disruptions that are hard to recover from.\nFinding 2: True Differentiation is Difficult. In Section 3.2, we highlight how using GA on Retention Scope enhances the model's ability to differentiate scopes. To further assess this capability, we prepend the prompt \"You are a helpful assistant...\" to the evaluation request. This setup aims to test the model's response stability under conditions that mimic normal usage. As shown in Figure 5, while GA + GD on ID leads to significant performance drops and nonsensical responses, our method maintains more stable performance. The observed difference can be attributed to the shortcoming of the GA + GD method, which erases and then forces the model to re-learn the retention knowledge. This method disrupts the model's understanding of retention knowledge. In contrast, our method preserves high stability by freezing parameters well-aligned with retention knowledge, thus avoiding the disruptive effects observed with the GA + GD. Furthermore, the results shown in Tables 2, 3, 4 and 6 demonstrate that both the Adversarial and GA methods fail to differentiate between unlearning and retaining scopes, evidenced by their subpar performance.\nFinding 3: Classifier Struggle with Scope Differentiation. We use a RoBERTa classifier (Liu et al., 2019) to distinguish unlearning scopes, labeling the Unlearn Scope as 1 and the Retention Scope as 0. In User Privacy, Unlearn Success reaches 83.63% and Retention Success 96.29%, setting a new state-of-the-art. However, when we prepend the same prompt with Finding 2 to the evaluation request, the Unlearn Success drops to 51.25%. This indicates that the classifier lacks the generality to effectively differentiate the unlearning scope, in contrast to unlearning methods that can utilize the robust text comprehension capabilities of LLMs."}, {"title": "5 Related Work", "content": "5.1 Large Language Models Unlearning\nMachine unlearning in LLMs has recently gained significant attention, with contributions from various studies (Wang et al., 2023; Zhang et al., 2024b; Wang et al., 2024e; Gundavarapu et al., 2024; Wang et al., 2024d; Liu, 2024; Stoehr et al., 2024; Pochinkov and Schoots, 2024; Lu et al., 2024; Chen et al., 2024a; Wang et al., 2024b; Zhao et al., 2024c; Wang et al., 2024a; Zhao et al., 2024a; Jin et al., 2024; Jin and Ren, 2024; Venditti et al., 2024; Hong et al., 2024). Numerous methods have been developed for knowledge unlearning for LLMs. Eldan and Russinovich (2023) apply preference optimization for unlearning, training the model to reject sensitive responses. Additionally, Pawelczyk et al. (2023) and Thaker et al. (2024) utilize in-context learning and system prompts, respectively, to promote unlearning. However, the unlearning scope remains unexplored.\n5.2 Knowledge Localization\nMany works focus on mapping knowledge to the parameters of LLMs, known as knowledge localization. Knowledge neurons (Dai et al., 2022) localize specific facts by adjusting neuron activation, inspired by the idea that \u201cMLP module is actually key-value memory\u201d (Geva et al., 2021). Despite its innovation, this approach has sparked debate regarding its efficacy and validation (Chen et al., 2024b; Wang et al., 2024f). For layer-wise localization, causal tracing (Meng et al., 2022) locates critical layers through denoising operations and has influenced many studies (Tan et al., 2023; Zhang et al., 2024a; Meng et al., 2023). Other methods use gradient information or hidden states (Yu et al., 2023; Fan et al., 2023; Wang et al., 2024c) for less constrained knowledge localization."}, {"title": "6 Conclusion", "content": "We formally investigate over-forgetting in knowledge unlearning and establish the novel benchmark KnowUnDo. We also propose MemFlex, an efficient method for precisely targeting and unlearning sensitive knowledge. However, our localization approach is confined to the modules of LLMs. Further research can extend this to individual neurons to achieve more precise unlearning and control."}, {"title": "Limitations", "content": "Law. There are differences between the laws of various countries; we only consider the USC (U.S., 2018), \u0421\u0421\u0420\u0410 (California, 2018), and GDPR (Europe, 2016) and do not take other laws into account.\nScopes. The division of scope does not include all categories, which can be further investigated in future studies.\nComputational Resources. Due to computational resource limitations, experiments on more diverse and larger models could not be conducted.\nProtected Types. In the future, we will consider including more types of copyrighted content (e.g., audio, video) and addressing user privacy, rather than being limited to text."}, {"title": "A Experimental Details", "content": "We utilize Pytorch to conduct experiments on a single A100 GPU (40G). The max sequence length is set to 256. All methods' optimizations are performed using the Adam optimizer. Our hyperparameters are in Table 7."}, {"title": "B Prompt Template", "content": "B.1 Copyrighted Content Construction\nB.1.1 Rewrite Query Generation\nPROMPT: Please generate 5 queries for me based on continue writing the story about [TOPIC].\nDEMONSTRATIONS:\n1. As a fan of [TOPIC], please continue writing the story about [TOPIC].\\n2. Please continue expanding the plot regarding [TOPIC].\\n3. I would love to see more of the story developed around [TOPIC].\nB.1.2 Continued Writing Query Generation\nPrompt used for generating continued writing examples."}]}