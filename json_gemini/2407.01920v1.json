{"title": "To Forget or Not?\nTowards Practical Knowledge Unlearning for Large Language Models", "authors": ["Bozhong Tian", "Xiaozhuan Liang", "Siyuan Cheng", "Qingbin Liu", "Mengru Wang", "Dianbo Sui", "Xi Chen", "Huajun Chen", "Ningyu Zhang"], "abstract": "Large Language Models (LLMs) trained on\nextensive corpora inevitably retain sensitive\ndata, such as personal privacy information and\ncopyrighted material. Recent advancements in\nknowledge unlearning involve updating LLM\nparameters to erase specific knowledge. How-\never, current unlearning paradigms are mired\nin vague forgetting boundaries, often erasing\nknowledge indiscriminately. In this work, we\nintroduce KnowUnDo, a benchmark contain-\ning copyrighted content and user privacy do-\nmains to evaluate if the unlearning process in-\nadvertently erases essential knowledge. Our\nfindings indicate that existing unlearning meth-\nods often suffer from excessive unlearning. To\naddress this, we propose a simple yet effec-\ntive method, MemFlex, which utilizes gradient\ninformation to precisely target and unlearn sen-\nsitive parameters. Experimental results show\nthat MemFlex is superior to existing methods in\nboth precise knowledge unlearning and general\nknowledge retaining of LLMs\u00b9.", "sections": [{"title": "Introduction", "content": "Forgetting is a crucial brain function that elimi-\nnates unnecessary information to maintain neural\nsystem integrity (Small, 2021; Farrell, 2022). In\nparallel, Large Language Models (LLMs) (Ouyang\net al., 2022; Zhao et al., 2023; OpenAI, 2023) in-\nevitably incorporate sensitive data during training,\nwhich is not essential for their functionality (Yao\net al., 2023a, 2024; Li et al., 2024b; Zhang et al.,\n2024a; Liu et al., 2024b). Therefore, removing\nsensitive knowledge from LLMs is imperative for\nensuring the safety and integrity of these systems.\nThe most straightforward solution involves remov-\ning such data from pre-training corpora and re-\ntraining LLMs, although this method is expensive\nand time-consuming. Another approach, alignment"}, {"title": "Benchmark Construction", "content": "We develop a more practical benchmark equipped\nwith valid evaluation metrics. To the best of our\nknowledge, we are the first to introduce a bench-\nmark that explores the unlearning and retaining\nscopes of knowledge regarding factual instances.\nWe further classify such knowledge, unlearning\nonly those within the Unlearn Scope and allowing\nresponses within the Retention Scope, as shown in\nFigure 2. Additionally, current benchmarks (Maini\net al., 2024; Yao et al., 2024) typically consider\ncopyrighted content and user privacy separately.\nOur benchmark integrates both aspects to provide\na comprehensive evaluation of unlearning methods.\nOur dataset construction is illustrated in Figure 3.\nWe also manually verify the datasets in both do-\nmains of our benchmark."}, {"title": "Task Definition", "content": "We denote an LLM as M, characterized by its\nparameters \u03b8, forming Me. Specifically, Me is\nrepresented by a function that maps the input x to\nits corresponding prediction y, as described below:\ny = M\u03b8(x)\n\u220f P\u03b8 (yi | y<i, x),\ni=1\nwhere P\u03b8 denotes the probability of generating\nthe next token in the sequence, and y<i =\n{y1,\u2026\u2026,yi\u22121}. Given an unlearned descriptor\n(xu, yu) related to an unlearning instance I (e.g.,\ncopyrighted content or public figures). Current ap-\nproaches often indiscriminately update \u03b8 to \u03b8' to\nensure that all responses, yu = M\u03b8' (xu), related\nto I are non-harmful. However, not all knowledge\nassociated with I needs to be forgotten. Thus, we\ndefine the unlearning process as follows:\nM\u03b8'(x) = \n{yu if x \u2208 U(xu, yu) \nM\u03b8(x) if x \u2208 R(xu, yu)\nM\u03b8(x) Otherwise,\nwhere U(xu, yu) and R(xu, yu) are the Unlearn\nScope and Retention Scope for (xu, yu) shown in\nFigure 2. \"Otherwise\" pertains to knowledge out-\nside these scopes."}, {"title": "Copyrighted Content", "content": "Sampling Copyrighted Instances. In construct-\ning the dataset, our initial step involves select-\ning copyrighted books from the GoodReads \u201cBest\nBooks Ever\" list, and choosing books based on\npopularity and genre diversity to ensure a represen-\ntative sample. After identifying the target books,\nwe input their titles into GPT-4 API to generate\nrelated author information and book overviews for\nchecking. We then cross-referenced this generated\ninformation with Wikipedia to assess the accuracy\nof GPT-4's comprehension. As GPT-4 is the most\npowerful LLM, we only filter two erroneous books.\nEnsuring Unlearn and Retention Scope. Un-\nder the United States Code (USC) (U.S., 2018),\n17 U.S.C. \u00a7 106(2) grants copyright owners the\nexclusive right to prepare derivative works based\non the protected work. Unauthorized Revision\nor Extension of such works may infringe this\nright and are thus categorized under the Unlearn\nScope. Conversely, 17 U.S.C. \u00a7 107 establishes\nthe \"fair use\" principle, permitting the use of\ncopyrighted material without authorization for pur-\nposes like criticism, and commentary. Review,\nRecommendation and non-creative Meta-Info typ-\nically qualify as fair use and are placed within the\nRetention Scope. Additionally, instances that have\nentered the public domain due to copyright expi-\nration, as outlined in 17 U.S.C. \u00a7 302, are also\nclassified under the Retention Scope."}, {"title": "User Privacy", "content": "Due to the risks associated with using real privacy\ndata, we construct a dataset of fictitious author\ninformation following Maini et al. (2024) and fine-\ntune the model on this dataset to establish a foun-\ndation for conducting further experiments.\nThe process of constructing fictitious author in-\nformation is as follows. First, we manually con-\nstruct examples of fictitious authors and use these\nas a demonstration for prompting GPT-4 to gen-\nerate data of fictitious authors based on prede-\nfined attributes such as Name, Genre, Born, Awards,\nParents, Email, and Address. According to the\n\u201cRight to Deletion\u201d and \u201cRight to Access\u201d under\nCCPA (California, 2018), and the \u201cRight to Erasure\"\nand \"Data Minimization\u201d principles under GDPR\n(Europe, 2016), we should retain essential informa-\ntion about public figures, such as their Name, Genre,\nBorn, and Awards, which are categorized under the\nRetention Scope. These details are necessary to\nunderstand their contributions. Conversely, their\nprivate information, including Parents, Email,\nand Address, does not contribute to this under-\nstanding and therefore falls into the Unlearn Scope.\nUsing these categories, we prompt GPT-4 to gen-\nerate corresponding question-answer pairs, with\nthe specific template provided in the Appendix B.2.\nThe generated question-answer pairs (xu, yu) form\nour dataset Dpriv, which includes {DUpriv, DRpriv},"}, {"title": "Evaluation Metrics", "content": "Our evaluation metrics, as referenced in Meng et al.\n(2022); Mitchell et al. (2022); Zhang et al. (2024a);\nYao et al. (2024), include Unlearn Success, Reten-\ntion Success, and Perplexity.\nUnlearn Success: We define a metric named Un-\nlearn Success to measure the success of unlearning\nby the average accuracy of the Unlearn cases:\n\u2211(xu,yu)\u2208DUL 1{argmaxy P\u03b8' (y | xu) != yu},\nwhere DUL refers to DULcpyr and DULpriv. The un-\nlearned model M\u03b8' should not be able to predict\ncorrectly for unlearned knowledge.\nRetention Success: We also define a metric\nnamed Retention Success to measure the success\nof retaining, assessed by the average accuracy in\nthe Retention cases:\n\u2211(xu,yu)\u2208DRT 1{argmaxy P\u03b8' (y | xu) = yu}\nIdeally, M\u03b8' should retain its performance on Re-\ntention Scope with the original one M\u03b8, indicating\nthat the unlearning process is under control.\nPerplexity: We use Perplexity to measure the\nmodel's prediction complexity, defined as:\nPerplexity = 2^(\u2212\u2211i(log2 P\u03b8' (yi|y<i,x')))\nGeneral Task Performance"}, {"title": "General Task Performance", "content": "The unlearning process may unintentionally in-\ntroduce side effects to LLMs in unrelated areas.\nTherefore, to assess the impact comprehensively,\nwe also evaluate the capabilities of the unlearned\nmodel across a variety of general tasks, which\nspan Knowledge Understanding, Truthfulness, and\nKnowledge Reasoning, referring to the classifica-\ntion schema of the related works (Contributors,\n2023; Gao et al., 2023; Beeson, 2024).\nKnowledge Understanding. We use Mas-\nsive Multitask Language Understanding (MMLU)\n(Hendrycks et al., 2021) and ARC Challenge (Clark\net al., 2018) to evaluate the LLM's understanding\nand application of knowledge.\nTruthfulness. The TruthfulQA (Lin et al., 2022)\ndataset assesses the LLM's ability to generate truth-\nful and reliable answers to questions."}, {"title": "Baselines", "content": "As discussed in Section 2.1, LLM unlearning en-\nsures the model effectively forgets the data in the\nUnlearn Scope while retaining performance in the\nRetention Scope. We use an unlearning framework\nfor LLMs (Yao et al., 2024) under MIT License. To\nunlearn sequences in DUL, we update the current\nmodel M\u03b8 using the gradient derived from:\n\u2211(xu,yu)\u2208DUL(\u2211i=1yu log P\u03b8(y|y<i, xu))\n+ \u2211(xu,yu)\u2208DRT(\u2211i=1yu log P\u03b8(y'|y<i, xu))\nWe focus on the first-order approximate unlearning\nmethods, which rely on gradient information and\nare often more efficient than exact unlearning and\nsecond-order methods.\nApproximate Unlearning Methods"}, {"title": "Approximate Unlearning Methods", "content": "Gradient Ascent Removing the secondary com-\nponent from Eq. 6 and reversing the gradient's di-\nrection leads to the gradient ascent method, used\nto forget specific data subsets. Effective for small\ndatasets, it is applied for a few epochs to avoid de-\ngrading overall model performance (Golatkar et al.,\n2020; Jang et al., 2023).\nFine-tuning with Random Labels This method\nupdates the model's weights by training on ran-\ndomly labeled data, which ignores the second term\nin Eq. 6 and simulates the effect of removing tar-\ngeted knowledge, typically reducing general per-\nformance. Similarly to the gradient ascent, it is\napplied for a few epochs.\nUnlearning with Adversarial Samples This\nmethod generates adversarial tokens to confuse the\nmodel and unlearn specific sequences effectively."}, {"title": "The Proposed Strong Baseline: MemFlex", "content": "Inspired by knowledge localization of model edit-\ning (Dai et al., 2022; Meng et al., 2022; Yao et al.,\n2023b; Chen et al., 2024b), we introduce a novel\nunlearning method that identifies pivotal parameter\nregions for \u201cforgetting\u201d and \u201cretaining\u201d. Building\non this, and further inspired by Yu et al. (2023) and\nFan et al. (2023), we leverage gradient informa-\ntion to enhance the precision of localization. For\ninstance, to pinpoint for forgetting, we proceed as\nfollows:\n\u2022 Given (xu, yu) \u2208 DUL, the label yu is substi-\ntuted with a random one to form (xu, \u02dcyu).\n\u2022 Gradient information g \u2190 VoL(xu, \u02dcyu) is\nharvested through back-propagation.\n\u2022 This process of random substitution and back-\npropagation is iterated five times, culminating\nin an average that yields a stable Unlearn gra-\ndient matrix GUL = 1/N \u2211i=1 gi\nA similar procedure is applied to pinpoint for retain-\ning to obtain a Retention matrix GRT. Following\nLiu et al. (2024a) and Tian et al. (2024), we analyze\nthe gradient information by its two constituents: di-\nrection and magnitude. We hypothesize that a close\nresemblance in direction between Retention and\nUnlearn Scopes suggests potential disruption with\nretention knowledge during the unlearning process,\nmeasured as:\nCOS (GUL, GRT) = (GUL\u00b7 GRT)/|| GUL ||||GRT||"}, {"title": "Experiment", "content": "We conduct experiments using LLaMA-2-7B-\nChat (Touvron et al., 2023) and Qwen-1.5-7B-\nChat (Bai et al., 2023), fine-tuning these models\non our datasets with LoRA (Hu et al., 2021), a\nmethod enhancing model adaptation without exten-\nsive training, as our base models."}, {"title": "Results", "content": "Results on User Privacy. As shown in Tables 2\nand 4, the base models perform well with high suc-\ncess and low perplexity, showing effective knowl-\nedge integration, while the unlearned models show\na decline in performance. GA and Fine-tuning with\nRandom Labels (Random Labels) successfully un-\nlearn sensitive knowledge but fail to retain essential\ninformation, leading to significant drops in Reten-\ntion Success. This performance degradation un-\nderscores the challenge of distinguishing between\nUnlearn and Retention Scopes.\nUnlike GA and Random Labels, which cause\nhigh perplexity by altering learning distributions,\nUnlearning with Adversarial Samples (Adversarial,\nAdv) mimics the original distribution, maintaining\ngeneral knowledge and low perplexity but struggles\nwith unlearning or retaining. A combined approach"}, {"title": "Analysis", "content": "In this section, we explore why knowledge local-\nization effectively improves LLM unlearning.\nFinding 1: Knowledge Localization Ensures\nHigh Retention Success. We compare Unlearn\nSuccess, Retention Success, and Perplexity across\ndifferent methods during the unlearning process.\nAs illustrated in Figure 4, our method maintains\nhigh Retention Success with a stable curve through-\nout the process, whereas other methods signifi-\ncantly degrade overall performance due to exces-\nsive parameter updates. Our method's stability\nstems from precisely localizing critical regions nec-\nessary to retain overall performance. In contrast,\nother approaches tune all model modules indis-\ncriminately, causing irreversible performance dis-\nruptions that are hard to recover from."}, {"title": "Conclusion", "content": "We formally investigate over-forgetting in knowl-\nedge unlearning and establish the novel benchmark\nKnowUnDo. We also propose MemFlex, an effi-\ncient method for precisely targeting and unlearning\nsensitive knowledge. However, our localization\napproach is confined to the modules of LLMs. Fur-\nther research can extend this to individual neurons\nto achieve more precise unlearning and control."}, {"title": "Limitations", "content": "Law. There are differences between the laws of\nvarious countries; we only consider the USC (U.S.,\n2018), \u0421\u0421\u0420\u0410 (California, 2018), and GDPR (Eu-\nrope, 2016) and do not take other laws into account.\nScopes. The division of scope does not include\nall categories, which can be further investigated in\nfuture studies.\nComputational Resources. Due to computa-\ntional resource limitations, experiments on more\ndiverse and larger models could not be conducted.\nProtected Types. In the future, we will consider\nincluding more types of copyrighted content (e.g.,\naudio, video) and addressing user privacy, rather\nthan being limited to text."}, {"title": "Experimental Details", "content": "We utilize Pytorch to conduct experiments on a\nsingle A100 GPU (40G). The max sequence length\nis set to 256. All methods' optimizations are per-\nformed using the Adam optimizer. Our hyperpa-\nrameters are in Table 7."}, {"title": "Copyrighted Content Construction", "content": "Rewrite Query Generation\nPROMPT: Please generate 5 queries for me\nbased on continue writing the story about\n[TOPIC].\nDEMONSTRATIONS:\n1. As a fan of [TOPIC], please continue writing\nthe story about [TOPIC].\\n2. Please continue\nexpanding the plot regarding [TOPIC].\\n3. I\nwould love to see more of the story developed\naround [TOPIC].\nContinued Writing Query Generation\nPrompt used for generating continued writing ex-\namples."}]}