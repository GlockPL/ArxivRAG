{"title": "Federated Domain Generalization via Prompt Learning and Aggregation", "authors": ["Shuai Gong", "Chaoran Cui", "Chunyun Zhang", "Wenna Wang", "Xiushan Nie", "Lei Zhu"], "abstract": "Federated domain generalization (FedDG) aims to improve the global model's generalization in unseen domains by addressing data heterogeneity under privacy-preserving constraints. A common strategy in existing FedDG studies involves sharing domain-specific knowledge among clients, such as spectrum information, class prototypes, and data styles. However, this knowledge is extracted directly from local client samples, and sharing such sensitive information poses a potential risk of data leakage, which might not fully meet the requirements of FedDG. In this paper, we introduce prompt learning to adapt pre-trained vision-language models (VLMs) in the FedDG scenario, and leverage locally learned prompts as a more secure bridge to facilitate knowledge transfer among clients. Specifically, we propose a novel FedDG framework through Prompt Learning and AggregatioN (PLAN), which comprises two training stages to collaboratively generate local prompts and global prompts at each federated round. First, each client performs both text and visual prompt learning using their own data, with local prompts indirectly synchronized by regarding the global prompts as a common reference. Second, all domain-specific local prompts are exchanged among clients and selectively aggregated into the global prompts using lightweight attention-based aggregators. The global prompts are finally applied to adapt VLMs to unseen target domains. As our PLAN framework requires training only a limited number of prompts and lightweight aggregators, it offers notable advantages in computational and communication efficiency for FedDG. Extensive experiments demonstrate the superior generalization ability of PLAN across four benchmark datasets. We have released our code at https://github.com/GongShuai8210/PLAN.", "sections": [{"title": "I. INTRODUCTION", "content": "N today's era of distributed data sources and edge comput- ing, there exists a substantial demand for collaboratively training machine learning models across multiple clients. Federated learning (FL) [1], [2] has emerged as a promising solu- tion, enabling the development of accurate and robust models while maintaining data privacy. Unlike traditional centralized approaches, FL allows each local client to learn from its own data. Then, a central server periodically aggregates the local model parameters from all clients to generate a global model.\nDespite significant progress, traditional FL approaches [3]- [6] primarily concentrate on enhancing model performance within the federation of clients. These methods often assume that data across all clients are identically distributed. In reality, however, clients independently collect local data, which natu- rally form mutiple source domains with distinct distributions. During deployment, the test data may also come from a target domain with previously unseen distributions. Therefore, how to generalize the federated model under domain shifts remains an underexplored issue.\nAs a common solution to the domain shift issue, domain generalization (DG) [7], [8] techniques have been developed to enable models trained on source domains with heterogeneous distributions to generalize well to unseen target domains. Nevertheless, applying existing DG methods in the FL setting is not straightforward, as they typically operate in a centralized manner that requires full access to data from different source domains. To overcome this challenge, federated domain gen- eralization (FedDG) [9] has been further introduced to syner- gize FL and DG. As illustrated in Fig. 1, FedDG facilitates collaborative model learning across diverse source domains for effective generalization to unseen target domains while keeping data privacy.\nMany prior studies on FedDG aim to share domain knowl- edge among clients under the privacy-preserving constraint, thereby enhancing the generalization ability of each local model by integrating knowledge from diverse domains. For"}, {"title": "II. RELATED WORK", "content": "Federated learning (FL) is a distributed machine learning paradigm that enables multiple clients to collaboratively train a model while keeping their data localized. In a single iteration of FL, the server sends the current global model to all participating clients. Subsequently, each client independently trains the model using its local dataset. Once the local training is completed, each client uploads its model parameters back to the server, allowing for the aggregation of these parameters to form a new global model.\nFedAvg [1] is a fundamental FL algorithm that aggregates locally trained model parameters through weighted averaging, with weights determined by each client's data size. However, due to its underlying assumption of the IID data distribution, FedAvg often encounters difficulties in settings character- ized by data heterogeneity among clients. To mitigate the challenges posed by data heterogeneity in FL, FedProx [3] extends FedAvg by incorporating a proximal term into the local objective functions, which prevents local updates from"}, {"title": "B. Domain Generalization", "content": "The above issue can be addressed by the domain generaliza- tion (DG) technique. DG aims to train a model on one or more source domains that can generalize well to the unseen target domain. Representative methods either learn domain-invariant features across multiple source domains [8], [23]\u2013[25] or adopt the idea of meta-learning [26], [27]. Most conventional DG methods train models in a centralized manner. However, these methods cannot be directly applied in FL, since the centralized server is restricted from accessing multi-source domains to comply with data privacy requirements.\nSuch limitation of centralized methods leads to the emer- gence of FedDG. Federated domain alignment is crucial in FedDG, as it aims to reduce domain shift and enhance the model's generalization ability on unseen domains [28]. To achieve this, a core idea is to facilitate domain knowledge sharing across multiple clients. Liu et al. [9] exchanged the amplitude spectrum information in frequency space among clients to transfer multi-client data distribution. Huang et al. [10] allows the sharing of class prototypes of local data among clients. Chen et al. [11] introduced CCST, which facil- itates image style sharing by exchanging pixel-level channel- wise mean and standard deviation of image features among clients. The shared information of these methods is generated directly from the local data. Consequently, sharing such sensi- tive information among clients could be considered as a form of data leakage. Our approach, PLAN, introduces prompts learned from local data as a more secure bridge for transferring domain knowledge among clients in FedDG."}, {"title": "C. Prompt Learning in VLMs", "content": "Prompt learning [29], [30] is a kind of parameter-efficient fine-tuning techniques, which only introduces a small number of learnable prompt tokens as extra inputs while freezing the parameters of VLMs. As a pioneering effort, CoOp [15] converts the input context tokens in the text encoder into learn- able prompts for adapting CLIP [13] to downstream image recognition. MaPLe [16] further introduce visual prompts to enhance alignment between the text and visual representations in CLIP. In recent years, researchers have introduced the prompt learning methods into FL because they naturally meet the requirements for efficient communication. For example, PromptFL [17] and Diprompt [31] independently learn text prompts on each local client, then aggregate the locally trained prompts at the server with fixed weights. In contrast, PLAN concurrently performs both text and visual prompt learning on the local client through a reference-based prompt learning mechanism, which facilitates the indirect synchronization of local prompts across clients. Additionally, PLAN is equipped with lightweight, attention-based prompt aggregators to selec- tively aggregate local prompts into global prompts."}, {"title": "III. PRELIMINARIES", "content": "In this section, we first present the formulation of the FedDG problem. We then briefly describe CLIP [13], a typical representative of VLMs."}, {"title": "A. Federated Domain Generalization", "content": "FedDG aims to develop models that can generalize well in unseen domains while preserving data privacy across distributed sources. As a common practice in FedDG, we generally consider the C-way image classification task, where heterogeneous data is partitioned among K clients.\nFormally, we denote (X, Y) as the joint image and label space. Each client independently collects local data, resulting in data heterogeneity among clients and thus forming multiple source domains. Let $\\mathcal{S} = \\{\\mathcal{S}_k\\}_{k=1}^K$ represent the dataset of K clients, where $\\mathcal{S}_k$ consists of $N_k$ image and label pairs for client k, i.e., $\\mathcal{S}_k = \\{(x_i^k, y_i^k) \\in (\\mathcal{X}, \\mathcal{Y})\\}_{i=1}^{N_k}$. For any two distinct clients k and j, their data distributions are different: $P_{\\mathcal{S}_k} \\neq P_{\\mathcal{S}_j}, \\forall k, j \\in \\{1, ..., K\\}$. FedDG aims to learn a global model $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$ from the combined dataset $\\mathcal{S}$, with the goal of achieving high performance in unseen target domains.\nThe standard FedDG paradigm involves two key stages: (1) local training, where each client trains a model on its own domain-specific data to capture local patterns, and (2) global aggregation, where the locally trained models are sent to a central server for aggregation, producing a global model that generalizes to unseen domains. Typically, these two stages alternate multiple times, with each full cycle referred to as a federated round.\nOur work focuses on effectively adapting VLMs through prompt learning in the FedDG scenario. We use CLIP [13] as the VLM of choice due to its broad popularity. CLIP comprises a text encoder and an image encoder. We utilize a Transformer [32] as the text encoder. Since visual prompt learning is also considered, a Vision Transformer (ViT) [33] is selected as the image encoder.\nThe text encoder processes a category description, typically in the format \"a photo of a [CLASS].\", where [CLASS] denotes the category name. The description is first tokenized into a sequence of tokens and projected into word embeddings"}, {"title": "IV. METHOD", "content": "In this section, we elaborate our proposed FedDG frame- work based on Prompt Learning and AggregatioN (PLAN). PLAN simultaneously performs both text and visual prompt learning for CLIP on all clients and leverages the learned prompts as a secure bridge for transferring domain knowl- edge among clients. Following the standard FedDG paradigm, PLAN integrates two training stages in each federated round: first, a reference-based prompt learning mechanism is intro- duced to enable the indirect synchronization of local prompts across clients; second, the lightweight attention-based prompt aggregators are implemented to selectively aggregate local prompts into global prompts. The global prompts are finally used to adapt CLIP to unseen target domains. For clarity, Table I presents a summary of the key notations and their def- initions used throughout this paper. The conceptual structure of PLAN is illustrated in Fig. 2."}, {"title": "A. Prompt Designing", "content": "Prior research [16] has confirmed the advantages of multi- modal prompts in guiding VLMs. Additionally, appending prompts to the deeper encoder blocks helps to the learning of stage-wise feature representations. Building on these insights, we incorporate both text and visual prompts into each block of the CLIP encoders.\nSuppose the text encoder comprises L Transformer blocks $\\{B_l\\}_{l=0}^{L-1}$. For client k, we design $m_t$ text prompt tokens for block $B_l$, denoted as $T_l^k = [t_{l,1}^k][t_{l,2}^k]...[t_{l,m_l}^k]$. These tokens are inserted into $B_l$ on client k and computed as follows:\n$[cls]_{l+1}, E_{l+1} = B_l ([cls]_l, T_l^k, E_l),$\nwhere $[cls]_l$ denotes the input class token for $B_l$, and $E_l$ denotes the embedding of the description of category c. Finally, the text representation of category c on client k is generated by\n$w_c^k = \\text{Proj}_t ([cls]_l),$\nwhere $\\text{Proj}_t ()$ is a projection layer for the text encoder. The vision encoder also consists of L Transformer blocks $\\{I_l\\}_{l=0}^{L-1}$. For client k, we insert $m_v$ visual prompt tokens"}, {"title": "B. Reference-based Prompt Learning", "content": "For local training, our PLAN method updates the local text and visual prompts using domain-specific data on each client. Specifically, given a labeled sample from client k, i.e., $(x, y) \\in \\mathcal{S}_k$, the probability distribution of image x over categories, denoted as $p^k(x) \\in [0, 1]^C$, where the c-th"}, {"title": "C. Attention-based Prompt Aggregation", "content": "For global aggregation, a typical approach is to average the local prompts to obtain the global prompts [17], [18]. However, in the context of FedDG, local prompts encapsu- late diverse domain knowledge from various clients. Simply assuming equal contribution from each local prompt to the global prompts may compromise the model's generalization to unseen domains. To overcome this challenge, our PLAN method introduces two lightweight attention-based aggregators to measure the importance of local prompts and selectively aggregate them into the global prompts.\nFrom all clients, the server collects the set of local text prompts $\\{T^k\\}_{k=1}^K$ and local visual prompts $\\{V^k\\}_{k=1}^K$. Our objective is to learn two corresponding prompt aggregators, $A_t$ and $A_v$, which effectively aggregate the local prompts into the global text prompt $T^g$ and global visual prompt $V^g$, respectively:\n$T^g = A_t (T^1, T^2, ..., T^K),$\n$V^g = A_v (V^1, V^2, ..., V^K).$\nSince $A_t$ and $A_v$ follow similar procedures, we will use $A_t$ as an example to outline the process of prompt aggregation. Specifically, to capture the differences in the impacts of $\\{T^k\\}_{k=1}^K$, $A_t$ quantifies the attention weight allocated to each local text prompt $T^k$ by assessing its degree of similarity to a learnable query Q:\n$\\gamma_k = \\frac{\\text{exp} ((Q, \\mathcal{F}_q (T^k)))}{\\sum_{i=1}^K \\text{exp} ((Q, \\mathcal{F}_q (T^i)))},$\nwhere $(,)$ denotes the inner product similarity, and $\\mathcal{F}_q$ serves as a transformation function that maps $T^k$ into the same space as Q. For computational efficiency, $\\mathcal{F}_q$ is designed as a simple two-layer perceptron with a bottleneck structure [40]. The global text prompt $T^g$ is then generated via the weighted aggregation of the local prompts:\n$T^g = \\sum_{k=1}^K \\gamma_k \\mathcal{F}_a (T^k),$\nwhere $\\mathcal{F}_a$ is a mapping function that shares the same struc- ture as $\\mathcal{F}_q$. Through this mechanism, the text aggregator $A_t$ selectively emphasizes informative local prompts while"}, {"title": "V. EXPERIMENTS", "content": "In this section, we offer a detailed description of the ex- perimental settings and report a series of experimental results, demonstrating the effectiveness of our PLAN method."}, {"title": "A. Datasets", "content": "We evaluate the performance of PLAN on four DG bench- mark datasets, including:\n\u2022 PACS [41] contains 9,991 images across 7 classes, spanning four domains: Photo, Sketch, Cartoon, and Art Paintings.\n\u2022 OfficeHome [42] consists of 24,788 images in 65 classes, distributed among four domains: Art, Clipart, Product, and Real World.\n\u2022 VLCS [43] integrates four photographic domains, i.e., VOC2007, LabelMe, Caltech-101, and SUN09, and com- prises 10,729 instances across five classes.\n\u2022 DomainNet [44] combines six photographic domains- Clipart, Infograph, Painting, Quickdraw, Real World, Sketch-and includes 569,010 images in 345 classes.\nWe conducted evaluations on all datasets using the leave- one-domain-out setting [45], where each domain is chosen as the unseen target domain for testing, while the remaining domains are allocated to different clients as source domains for training. The split of training and validation sets within each source domain follows the same configuration as described in [46]\u2013[48] for PACS, OfficeHome, and VLCS, and [44] for DomainNet. We report the classification accuracy for each target domain, along with the overall average accuracy for each dataset."}, {"title": "B. Baselines", "content": "We compare PLAN against 15 recently proposed methods across four categories:\n1) Centralized learning (CL)-based DG methods aim to develop a well-generalized model using data from all available domains in a centralized manner.\n\u2022 SWAD [49] demonstrates that flatter minima lead to better generalization in unseen domains and proposes a stochastic weight averaging densely algorithm to identify these flatter minima.\n\u2022 HCVP [50] improves domain generalization by introduc- ing a hierarchical prompt generation system that captures both domain-specific and task-specific features, which are integrated into a ViT via contrastive learning.\n\u2022 Doprompt [51] enhances generalization by generating prompts for input images. It first learns domain-specific prompts from source domains, and then trains a prompt adapter to produce tailored prompts for each target image.\n2) FL-based DG methods enable multiple decentralized clients to collaboratively train a model without directly sharing their data.\n\u2022 FedAvg [1] is a classic FL algorithm that derives a global model by averaging locally trained model updates from multiple clients.\n\u2022 FedProx [3] extends FedAvg by incorporating a proximal term into the objective function to address challenges such as statistical heterogeneity among client data.\n3) Conventional FedDG methods combine the principles of FL and DG to develop models that generalize well to unseen domains while ensuring data privacy.\n\u2022 FedSR [52] employs both $L_2$-norm and conditional mu- tual information regularizations to achieve a simple yet generalizable data representation.\n\u2022 FedADG [12] adopts adversarial learning to align data distributions across source domains with a dynamically generated reference distribution.\n\u2022 CCST [11] transfers domain-specific styles across clients, allowing the model to capture a broader range of domain variations.\n\u2022 ELCFS [9] facilitates privacy-preserving data exchange among clients by transmitting information in the fre- quency space, thereby enhancing model generalizability.\n\u2022 GA [45] dynamically recalibrates the aggregation weights of local models by minimizing the variance in general- ization gaps across clients.\n4) Parameter-efficient fine-tuning (PEFT)-based methods leverage techniques such as prompt learning and adapters to enhance the generalization of VLMs in FL scenarios."}, {"title": "C. Implementation Details", "content": "In our implementation of PLAN, we utilized the pre- trained ViT-Base/16 CLIP model for prompt learning, with text and visual prompt token dimensions set to 512 and 768, respectively. The prompt token length was configured to be 8, while the depth of the Transformer blocks incorporating the prompts was fixed at 12. The mapping functions, $\\mathcal{F}_q$ and $\\mathcal{F}_a$ in Eq. (12) and Eq. (13), were implemented using a two-layer bottleneck network with a reduction ratio of 1/8.\nDuring training, we set the number of federated rounds to 20 and the number of local update epochs to 1, i.e., R = 20 and E = 1 in Algorithm 1. We adopted the SGD optimizer with a batch size of 32 and a learning rate of 0.0015. The trade-off hyperparameter \u03b1 in Eq. (10) was selected as 1. The influence of key hyperparameters on model performance will be discussed in a later section. For the sake of reproducibility, the code of our PLAN method has been made publicly available at https://github.com/GongShuai8210/PLAN.\nFor CL-based DG baselines, we cited the results using the ViT backbone from the previously published paper [50]. For the remaining three categories of baselines, we reproduced their results based on CLIP to ensure a fair comparison. The hyperparameters of baseline methods were determined either by following the settings in the original papers or according to the results obtained on the validation set."}, {"title": "D. Main Results", "content": "Table II, Table III, and Table IV present the results of PLAN compared to the baseline methods across different categories on PACS, OfficeHome, and VLCS, respectively. Given the large scale of DomainNet, reproducing all models on this dataset is prohibitively expensive. Therefore, only the results of PEFT-based methods on DomainNet are shown in Table V. All results represent the average of three runs, with the best performance in each generalization task highlighted in bold.\n1) Compared with CL-based DG methods: On different datasets, CL-based DG methods exhibit superior performance, significantly surpassing FL-based DG methods as well as conventional FedDG methods. However, this improvement comes at the expense of data privacy, as CL-based DG methods require a centralized framework with unrestricted access to data from all clients. Our PLAN method addresses this issue by leveraging VLM prompts as a privacy-preserving bridge"}, {"title": "E. Ablation Study", "content": "Table VI presents the average accuracy of several variants of our PLAN method when different key components are omitted.\nAs described in Eq. (9), we align the local prompts of different clients with the global prompts by minimizing the KL divergence between their respective prediction distributions."}, {"title": "F. Hyperparameter Analysis", "content": "We evaluate the influence of different hyperparameters on the performance of our PLAN method on OfficeHome.\n1) Effect of Loss Weight: In Eq. (10), the hyperparameter \u03b1 controls the weight of the KL loss $\\mathcal{L}_{kl}^k$ in optimizing the local prompts. Table VII presents the results achieved by PLAN with different values of \u03b1. Notably, even a small weight, such as \u03b1 = 0.1, yields an average performance improvement of 0.38%. Such an observation reaffirms the importance of the KL loss in PLAN. The performance gradually improves as \u03b1 increases from 0.1 to 1, but begins to decline with any further increase. This trend is intuitive, as an excessively high value of \u03b1 may cause the KL loss to dominate the training process, thereby preventing the local prompts from effectively capturing domain-specific knowledge on each client.\n2) Effect of Prompt Depth and Length: Fig. 3 displays the effects of adjusting the prompt depth and length on the performance of PLAN. For simplicity, we assume that text and visual prompts have identical depth and length [16]. In Fig. 3a,"}, {"title": "G. Cost Analysis", "content": "We assess the efficiency of PLAN by examining the commu- nication and computation costs during training on OfficeHome. The communication cost is measured by the size of parameters transmitted per federated round, while the computation cost is quantified by the GPU training time required per round.\nAs shown in Fig. 4a, we observe that compared to traditional FedDG methods such as FedAvg using ViT-Base/16 and ResNet-50, PLAN reduces the communication cost by up to 15.29 times and 4.55 times, respectively. In contrast to the previous leading prompt learning method, FedMaPLe, PLAN incurs a slightly higher communication cost, as it requires two communications between the server and clients in each round. Similarly, as illustrated in Fig. 4b, PLAN exhibits only a slightly lower computational efficiency per round in comparison with FedMaPLe. However, as depicted in Fig 4c, when comparing the accuracy of PLAN and FedMaPLe after each round, we find that PLAN consistently achieves con- siderably higher performance. Notably, after just one training round, PLAN surpasses FedMaPLe's peak performance over the entire training process. The faster convergence speed of PLAN significantly reduces its total communication and computational costs relative to FedMaPLe during training."}, {"title": "H. Comparison in Few-shot Setting", "content": "In FL scenario, each client may have limited data samples. To evaluate PLAN's effectiveness in few-shot settings, we compare its average accuracy against that of PromptFL and FedMaPLe on PACS and OfficeHome. Fig. 5 shows the results of the three methods across varying numbers of samples per category. As expected, the performance of all methods consistently improves with the increase of training data. Ex- cept for the one-shot setting, PLAN demonstrates superior performance compared to both PromptFL and FedMaPLe in all other cases. While all three methods are based on CLIP, these results demonstrate that PLAN more effectively facilitates the adaptation of CLIP in few-shot settings."}, {"title": "I. Visualization", "content": "We present some visualization results to offer an intuitive understanding of PLAN's advantages.\nIn Fig. 6, we apply the t-SNE algorithm [55] to visualize the features of target samples extracted by CLIP and PLAN, respectively, when \"Real World\" is designated as the target domain on OfficeHome. Samples from the same category are marked with identical colors. Compared to CLIP, the target features extracted by PLAN are more clustered, and features associated with different categories exhibit greater separation. This visualization suggests that PLAN achieves a more distinct feature representation across categories.\nIn Fig. 7, we visualize the Class Activation Maps (CAMs) [56] generated by CLIP and PLAN for some target samples on DomainNet. To produce the CAMs, we first compute the model's output logits and then use their gradients to weight the attention maps of each block within the vision encoder. The final CAMs are obtained by averaging these weighted attention maps across all blocks. Each sample is annotated with its category label. It is evident that CLIP fre- quently focuses on background elements or objects irrelevant to the given category, whereas PLAN effectively concentrates on the objects of interest."}, {"title": "VI. CONCLUSION", "content": "In this paper, we explored the emerging field of feder- ated domain generalization (FedDG) with prompt learning techniques. We proposed that learned prompts can act as a more secure bridge for knowledge transfer among clients, as they are not directly generated from local data. Based on the prompt learning technique in pre-trained vision-language models, we introduced a communication-efficient framework called PLAN, which employs a two-stage collaborative train- ing strategy to generate reference-based local prompts towards each client and selectively aggregates these local prompts into global prompts. The entire framework optimizes only a small number of parameters for the prompts and lightweight attention-based prompt aggregators, thereby ensuring commu- nication efficiency. Our extensive experiments on four bench- mark datasets demonstrate that PLAN achieves new state-of- the-art performance in FedDG. One limitation of our study is the assumption that the source and unseen domains share the same category space, which might not always hold in real-world applications. Future work will focus on extending PLAN to accommodate open-set scenarios, making it more applicable to practical settings."}]}