{"title": "Interpretable Differential Diagnosis with Dual-Inference Large Language Models", "authors": ["Shuang Zhou", "Sirui Ding", "Jiashuo Wang", "Mingquan Lin", "Genevieve B. Melton", "Rui Zhang"], "abstract": "Methodological advancements to automate the generation of differential diagnosis (DDx) to predict a list of potential diseases as differentials given patients' symptom descriptions are critical to clinical reasoning and applications such as decision support. However, providing reasoning or interpretation for these differential diagnoses is more meaningful. Fortunately, large language models (LLMs) possess powerful language processing abilities and have been proven effective in various related tasks. Motivated by this potential, we investigate the use of LLMs for interpretable DDx. First, we develop a new DDx dataset with expert-derived interpretation on 570 public clinical notes. Second, we propose a novel framework, named Dual-Inf, that enables LLMs to conduct bidirectional inference for interpretation. Both human and automated evaluation demonstrate the effectiveness of Dual-Inf in predicting differentials and diagnosis explanations. Specifically, the performance improvement of Dual-Inf over the baseline methods exceeds 32% w.r.t. BERTScore in DDx interpretation. Furthermore, experiments verify that Dual-Inf (1) makes fewer errors in interpretation, (2) has great generalizability, (3) is promising for rare disease diagnosis and explanation.", "sections": [{"title": "Introduction", "content": "Differential diagnosis (DDx) is a list of possible conditions that could cause a patient's symptoms [1]. In clinical scenarios, the importance of conducting DDx is multifaceted. Firstly, it provides a more comprehensive assessment of a clinical case than a single diagnosis, enabling the identification of less obvious but critical conditions. Secondly, it guides appropriate diagnostic examinations and optimizes resource allocation. Thirdly, by offering a clear list of potential conditions, clinicians can effectively communicate the diagnostic process and involve patients in decision-making, thereby earning their trust in the healthcare system. However, the benefits of DDx come at the cost of requiring additional time and effort from clinicians, which can overwhelm an already overburdened workforce.\nOver these years, many automatic DDx algorithms [2, 3] have been developed to alleviate human efforts. Generally, these methods adopted deep learning (DL) techniques for supervised learning and then provided DDx prediction. For example, Liu et. al [4] developed a DL-based system to distinguish among 26 of the most common skin conditions. Hwang et. al [5] proposed a convolutional neural network-long short-term memory (CNN-LSTM) algorithm for cardiovascular diseases. Another DL-based platform was developed to detect 39 types of common referable fundus diseases and conditions [6]. However, merely providing diagnosis predictions to clinicians is still insufficient as the black-box nature of DL models renders them hard to be trusted [7]. It is crucial to further explain the reasons behind the diagnoses to enhance their trustworthiness [8]. Thereby, interpretable DDx is desired in clinical scenarios. This task takes patients' symptom descriptions as input, generates a set of potential diseases as differentials, and provides explanations for the diagnoses. Despite its importance, interpretable DDx is less explored due to various challenges. Firstly, there is a lack of a DDx dataset with interpretation of the diagnoses for model development and evaluation [9, 10]. Secondly, diagnosing diseases and providing reasonable explanations is a complex process, typically requiring large models with extensive medical knowledge. However, such models are costly in terms of data collection and training.\nFortunately, large language models (LLMs), e.g., GPT-4 and ChatGPT, have been trained on billions of corpora (including medical data) and demonstrated exceptional capabilities across numerous natural language processing (NLP) tasks [11-14]. Additionally, many studies have reported the effectiveness of LLMs in various clinical scenarios, such as medical question answering (QA) [15-17], clinical text summarization [18], and disease diagnosis [19-21]. For example, Med-PaLM 2 [22] was an instruction prompt-tuned LLM with over 340 billion parameters and achieved comparable performance with clinicians on medical QA datasets. Similarly, DRG-LLaMA [23] was fine-tuned on the LLaMA model [24] with 236,192 MIMIC-IV discharge summaries and achieved about 0.99 on the AUC score for Diagnosis-Related Group (DRG) prediction. Motivated by this, it is promising to use LLMs for interpretable DDx. More recently, Daniel et. al [25] proposed to fine-tune PaLM 2 on medical domain data and provided an interactive interface to generate DDx for assisting clinicians. Tu et. al [26] developed an LLM-based system that can perform clinical history-taking and diagnostic dialogue by simulating patients with a dialogue generator for clinical conversation. However, our study differs from the above works as we elicit the exceptional capabilities of LLMs with prompts for interpretable DDx instead of training or fine-tuning on massive data. Although a recent study [27] has employed Chain-of-thought (CoT) [28] to harness LLMs' reasoning ability for boosting diagnosis accuracy, how to leverage LLMs for interpretable DDx and thoroughly evaluate LLMs' interpretation performance is still under-explored.\nIn this work, we formally investigated prompting LLMs for interpretable DDx. Our contributions are as follows. Firstly, we curated a new dataset with 570 clinical notes spanning nine specialties. Specifically, the data was extracted from publicly available medical corpora, including MedQA USMLE (United States Medical Licensing Exam) dataset [29] and professional medical books (e.g., First Aid for the USMLE Step 2 CS); each note was annotated by domain experts with a set of differentials and the interpretation of the diagnoses. Secondly, we proposed a prompt-based framework called Dual-Inf that enables multiple LLMs to conduct bidirectional inference (i.e., from symptoms to diagnoses and vice versa) for this task. Specifically, Dual-Inf consists of (1) a prediction module, which makes initial diagnoses based on patients' notes, (2) a backward-inference module, which objectively recalls representative symptoms (i.e., from diagnoses to symptoms), and (3) an examination module, which receives patients' notes and the outputs from the other two modules for correctness examination and decision making. Thirdly, we comprehensively evaluated the proposed framework for interpretable DDx, including model interpretability, robustness on rare diseases, and error analysis. The results showed that Dual-Inf achieved superior performance for disease diagnosis. Moreover, our framework also provided better interpretations, surpassing the baseline methods by 26% w.r.t. accuracy, 32% w.r.t BERTScore, 18% w.r.t. SentenceBert, and 31% w.r.t. METEOR score. Additionally, the results demonstrated that Dual-Inf made fewer errors than the baselines (e.g., CoT and SC-CoT). In conclusion, our work verified the feasibility of using LLM for interpretable DDx and the released well-annotated DDX dataset could prompt the development of the research field."}, {"title": "Results", "content": "We established a well-annotated text-based interpretable DDx dataset, Open-XDDx. This dataset consisted of 570 notes from publicly available medical exercises, covering 9 clinical specialties, including cardiovascular disease, digestive system disease, respiratory disease, endocrine disorder, nervous system disease, reproductive system disease, circulatory system disease, skin disease, and orthopedic disease. Each note contained patients' symptom descriptions, differential diagnoses, and the corresponding interpretations that supported the potential diagnosis. Considering the raw data did not contain interpretations, we annotated the note with clinicians from the University of Minnesota. The annotation procedure was described in Supplementary Data 1. \n\nWe evaluate the diagnosis accuracy with automatic evaluation by comparing the ground-truth diagnoses in our dataset with the predicted ones. The results with GPT-4 and GPT-4o are respectively depicted in Figure 1(b) and Supplementary Data 2. As shown in Figure 1(b), Dual-Inf achieved superior performance than the baselines over the nine specialties. Specifically, when taking GPT-4 as the LLM, the diagnosis accuracy of Dual-Inf on cardiovascular, digestive, respiratory, nervous, circulatory, and orthopedic diseases surpassed CoT by more than 9% absolute percentage points. The highest difference is observed in cardiovascular (0.536 vs. 0.419). Moreover, the overall performance of SC-CoT is significantly higher than CoT (difference of 0.032, 95% CI 0.021 to 0.043, p=0.001) and Diagnosis-CoT (difference of 0.019, 95% CI 0.001 to 0.028, p=0.004). Additionally, Dual-Inf further outperforms SC-CoT (0.533 vs. 0.472, difference of 0.061, 95% CI 0.055 to 0.062), and the improvement was statistically significant at p=<0.001. Similarly, when taking GPT-4o as the LLM, Dual-Inf achieved an accuracy of over 0.55 on nervous, skin, and orthopedic diseases, surpassing that of SC-CoT by over 9%. The overall performance enhancement of Dual-Inf over SC-CoT (0.546 vs. 0.487, difference 0.059, 95% CI 0.053 to 0.064) was statistically significant at p=<0.001."}, {"title": "Interpretation performance", "content": "We assessed the model interpretability with automatic evaluation and human evaluation. Regarding automatic evaluation, we used GPT-4o to compare the ground-truth interpretation with the interpretation from models and judged the consistency. The prompt for interpretation comparison is shown in Supplementary Data 3. We implemented the methods using two LLM models, GPT-4 and GPT-4o. The results with GPT-4 are shown in Figure 2, and the results with GPT-4o are presented in Supplementary Data 4. In Figure 2, the interpretation accuracy of Diagnosis-CoT and SC-CoT was 0.305 and 0.334, surpassing that of CoT 0.011 (95% CI 0.004 to 0.019, p=0.012) and 0.04 (95% CI 0.038 to 0.043, p=<0.001), respectively. Furthermore, Dual-Inf achieved a higher accuracy of 0.446 with improvement over SC-CoT 0.112 (95% CI 0.105 to 0.118, p=<0.001). As for BERTScore, SentenceBert, and METEOR, Diagnosis-CoT, and SC-CoT outperformed CoT; the score differences were 0.02 (95% CI 0.016 to 0.024, p=<0.001) and 0.05 (95% CI 0.047 to 0.052, p=<0.001). Dual- Inf further outperformed SC-CoT with performance comparison as 0.345 vs. 0.258 (difference of 0.087, 95% CI 0.083 to 0.090, p=<0.001), 0.427 vs. 0.356 (difference of 0.071, 95% CI 0.067 to 0.076) and 0.333 vs. 0.251 (difference of 0.082, 95% CI 0.076 to 0.088). When taking GPT-4o as LLM, the interpretation accuracy of Dual-Inf reached 0.488, while the performance of CoT and SC-CoT were respectively 0.366 and 0.408. As for the other three metrics, the score comparison between Dual-Inf and SC-CoT were respectively 0.351 vs. 0.268 (difference of 0.083, 95% CI 0.079 to 0.086), 0.432 vs. 0.368 (difference of 0.064, 95% CI 0.058 to 0.070) and 0.344 vs. 0.264 (difference of 0.08, 95% CI 0.077 to 0.083).\n\nWe also manually examined the interpretation by clinicians at three different dimensions, including correctness, completeness, and usefulness. Figure 3(a) showed that the correctness scores of Dual-Inf mainly ranged from 3 to 4, while that of the strong baseline SC-CoT were mainly in the scope of 2 to 3. Furthermore, regarding the completeness of the interpretation, the count number of 3-score and 4-score on Dual-Inf is 38 and 21, while the count number on SC-CoT was respectively 19 and 3. As for the overall usefulness, Dual-Inf had the count number of 33 and 25 on the 3-score and 4-score in the 100 predictions, whereas the count number of SC-CoT was respectively 26 and 10."}, {"title": "Error analysis on interpretation", "content": "In our analysis, we further analyzed the error types of the interpretation generated by LLMs. Specifically, we randomly selected 100 samples that the baseline method SC-CoT had incorrect interpretation and compared the types of errors between SC-CoT and Dual-Inf. As per related papers [30, 31], the errors were divided into three categories: (1) missing content, defined as missing at least two pieces of evidence for a differential; (2) factual error, where the interpretation is not medically correct; (3) low relevance, indicating that the mentioned evidence was less relevant to the differential. As shown in Figure 3(b), when taking GPT-4 as the LLM, over 89 of the predictions of SC-CoT suffered from missing content, whereas Dual-Inf had only about 76 (difference 13.4, 95% CI 11.5 to 15.2). When using GPT-4o as the LLM, the count number comparison is 83.6 vs. 71.2 (difference 12.4, 95% CI 10.7 to 14.1). Meanwhile, regarding factual error and low relevance on the interpretation, the count number comparison between Dual-Inf and SC-CoT with GPT-4 is 17 vs. 8.2 (difference 8.8, 95% CI 7.8 to 9.8) and 15.4 vs. 10.8 (difference 4.6, 95% CI 3.9 to 5.3). Furthermore, when taking GPT-4o as the LLM, Dual-Inf still exhibited fewer errors than SC-CoT on the latter two types, and the difference was statistically significant (p=<0.001)."}, {"title": "Performance on rare disease", "content": "We further investigated the performance of Dual-Inf on the rare disease cases in the dataset. The results of taking GPT-4 as LLM are shown in Table 2 and the results with GPT-4o as LLM can be found in Supplementary Table 5. In terms of diagnosis accuracy, the comparison between CoT and Diagnosis-CoT was 0.396 vs. 0.402 (difference 0.006, 95% CI -0.001 to +0.012, p=0.073), and the score difference between CoT and SC-CoT was 0.029 (95% CI 0.017 to 0.040, p=0.002). The DDx accuracy of Dual-Inf was 0.483, exceeding that of SC-CoT by (0.058) 13%. As for the interpretation metrics, the scores of Dual-Inf were 0.379, 0.284, 0.371, and 0.275, respectively, while those of SC-CoT were 0.285, 0.207, 0.316, and 0.220, respectively. When taking GPT-4o as LLM, we observed similar results. Concretely, the DDx performance comparison between Dual-Inf and SC-CoT was about 0.491 vs. 0.431 (difference of 0.060, 95% CI 0.052 to 0.068, p=<0.001), and the comparison on the interpretation metrics was respectively 0.426 vs. 0.354, 0.287 vs. 0.211, 0.378 vs. 0.326, and 0.286 vs. 0.235."}, {"title": "Ablation study", "content": "We further conducted an ablation study to analyze the effect of each component in our framework. Specifically, we built four model variants for comparison: (1) a model with merely the forward-inference module (FI), (2) Dual-Inf without the backward-inference module (FI-EM), (3) FI-EM further drops the \u201cturn back\u201d mechanism (FI-EM*), (4) Dual-Inf without the \u201cturn back\u201d mechanism (Dual-Inf*). The results with GPT-4 as the LLM are shown in Table 3 and the results based on GPT-4o are presented in Supplementary Table 6. In Table 3, we observed that the diagnosis accuracy of FI, FI-EM*, and Dual-Inf* was less than 0.46, while that of FI-EM was about 0.5. Compared with the FI-EM, Dual-Inf further boosted the diagnosis accuracy by about 7%. As for interpretability, FI and FI-EM* achieved comparable performances on the four metrics. The interpretation accuracy of Dual-Inf* and FI-EM were about 0.358 and 0.373 respectively. Besides, Dual-Inf presented superior interpretability, with BERTScore boosted by about 24% and METEOR boosted by about 28% over the Dual-Inf*."}, {"title": "Discussion", "content": "The first observation of our study was that Dual-Inf boosted DDx accuracy. This was because Dual-Inf was capable of filtering out erroneous diagnoses by examining the quality of interpretation. Notably, the examination module in Dual-Inf integrated the output from the other two modules for correctness checking and filtered out low-confidence differentials. The \"turn back\" mechanism enabled the forward-inference module to receive feedback from the erroneous prediction and \u201cthink twice\u201d for disease diagnosis, thus boosting accuracy. We further visualized the distribution of the DDx accuracy with GPT-4 on each case in Figure 4. We observed that the median and three-quarters percentile of diagnosis accuracy on Dual-Inf was about 0.504 and 0.751, exceeding that of the SC-CoT (0.434 and 0.652) and Diagnosis-CoT (0.421 and 0.628). The improvements over the baselines were statistically significant, i.e., p<0.001. The results verified that Dual-Inf boosted the diagnosis accuracy for most of the notes. Besides, we observed that, in Figure 1(b), all the methods obtained higher DDx accuracy than the overall performance on four specialties (i.e., skin disease, reproductive disease, orthopedic disease, and nervous system disease). We explain the phenomenon from the perspective of training data of the LLMs. The results may indicate that GPT-4 has learned more medical knowledge on the four specialties, thus achieving superior accuracy.\nSecond, we found that Dual-Inf provided a better interpretation for DDx. For example, the manual examination of 100 cases (Figure 3(b)) showed that Dual-Inf achieved higher scores across all evaluation metrics. This was because the framework enabled LLMs to make bidirectional inferences and work collaboratively. Specifically, the forward-inference module made initial diagnoses based on the patient's symptoms, the backward-inference module reasoned in an opposite direction by objectively recalling the symptoms of the initial diagnoses, and the summarization module examined the correctness and completeness of the prediction. We further provided a distribution visualization of the interpretation on each note in Figure 5; the models were implemented with GPT-4. We observed that the median and quarter percentiles of the metric scores (e.g., BERTScore and METEOR) on Dual-Inf were larger than the baseline method SC-CoT. Besides, some interpretation results from Dual-Inf were of a high score, shown as outliers in Figure 5. For example, some values of BERTScore and METEOR on Dual-Inf approached 0.8, surpassing that of SC-CoT. It revealed that Dual-Inf generated better interpretations for most of the cases.\nWe further provided case studies to demonstrate the superior interpretability of our framework. The example in Table 4 showcased that Diagnosis-CoT and SC-CoT only provided three correct interpretations for a differential, i.e., Pneumothorax, while Dual-Inf generated more correct interpretations. This was because the framework conducted a bi-directional inference that further recalled all the related symptoms from diagnoses, thus providing comprehensive explanations. Besides, compared to the baseline methods, Dual-Inf had one more correct differential, i.e., Hemothorax, with three correct interpretations. The reasons were as follows: (1) Dual-Inf could filter out the low-confidence diagnosis, i.e., Pulmonary Contusion, which had a limited number of interpretations as evidence support, (2) the \u201cturn back\u201d mechanism gave the LLM another chance for diagnosis and took the low-confidence diagnosis as feedback to elicit a correct answer. Third, this study verified that using multiple LLMs could reduce interpretation errors for DDx. As shown in Figure 3(b), SC-CoT presented fewer errors in low-relevance interpretation than CoT. This indicated considering the interpretations from multiple LLMs helped to alleviate the hallucination issue. Compared to the baseline method, Dual-Inf made fewer erroneous interpretations of the three error types. We attributed it to the dual-inference mechanism of the framework. Concretely, the forward-inference module made inferences (i.e., from symptoms to diagnoses), the backward-inference module recalled the medical knowledge objectively (i.e., from diagnoses to symptoms), the examination module supplemented the interpretations and filtered low-confidence prediction, and the \u201cturn back\u201d mechanism further provided another chance to \u201cthink twice\u201d. These designs helped to alleviate the hallucination issue and boosted the quality of DDx interpretation.\nFinally, our study demonstrated a promising tendency for using Dual-Inf for rare disease diagnosis. Generally, patients with rare diseases have higher a probability of underdiagnosis or misdiagnosis [32, 33]. Although recent works investigated using LLMs for rare disease diagnosis [34-36], the feasibility of prompting LLMs to provide interpretation on rare diseases was under-explored. To the best of our knowledge, our work is the first one to explore the problem. We assume the explanation for DDx can boost the trustworthiness of the prediction and assist doctors in identifying less obvious but critical diagnoses for patients with rare diseases. We found that SC-CoT, which leveraged multiple LLMs, outperformed CoT on disease diagnosis, and Dual- Inf further outperformed SC-CoT (Table 2). As for interpretation, Dual-Inf brought about a 10% performance improvement over SC-CoT on BERTScore and METEOR. The above results indicated that GPT-4 had the medical knowledge on the rare disease and Dual-Inf could effectively elicit the diagnostic ability of LLMs, e.g., GPT-4. Since the proposed framework is flexible and generalizes well to arbitrary LLMs, it is feasible to replace GPT-4 with advanced LLMs in the future, thus achieving satisfying performance on rare diseases.\nIn summary, this study established an annotated dataset for interpretable DDx and proposed a prompted-based framework Dual-Inf that enabled multiple LLMs to conduct bi-directional inference (i.e., from diagnoses to symptoms and vice versa), thus providing trustworthy predictions. The findings revealed that existing prompt methods suffered from sub-optimal performance for generating DDx and interpretations, rendering it hard to assist clinical physicians in real-world scenarios. The experiments verified the effectiveness of Dual-Inf for providing accurate DDx, generating comprehensive interpretations, and reducing prediction errors. The results highlighted the potential of employing LLMs for interpretable DDx. Moreover, the released dataset with ground-truth DDx and interpretations supported automatic evaluation for future studies, thus prompting the development of the research field."}, {"title": "Methods", "content": "The data source for this study is publicly available medical exercises collected from MedQA USMLE (United States Medical Licensing Exam) dataset [29] and professional medical books, including First Aid for the USMLE Step 2 CS and First Aid Q&A for the USMLE Step 1. The raw data was multiple-choice question answering exercises. We transformed the questions into free text by preserving the patients' symptom descriptions and removing the multiple-choice options. The texts were further preprocessed including (1) removing duplicate notes, (2) unifying all characters into UTF-8 encoding and removing illegal UTF-8 strings; (3) correcting or removing special characters; (4) filtering out notes with fewer than 130 characters. Lastly, we collected 570 clinical notes in total, among which 10 notes were used for prompt development and 560 notes were preserved as the test set for evaluation. \n\nAs shown in Figure 1(a), we developed a framework called Dual-Inference Large Language Model (Dual-Inf) for interpretable DDx. Its input was clinical notes, and the output involved a set of potential diagnoses and the reasons that supported the prediction. Dual-Inf consisted of four components: (1) a prediction module, which made initial diagnoses, i.e., from patients' symptoms to diagnoses, (2) a backward-inference module, which conducted an inverse inference via recalling all the representative symptoms of the initial diagnoses, i.e., from diagnoses to symptoms, (3) an examination module, which received patients' notes and the output from the two modules for prediction assessment and decision making, and (4) a \u201cturn back\u201d mechanism, which took low-confidence diagnoses as feedback for the forward-inference module to \u201cthink twice\u201d.\nThe pipeline is as follows. First, the forward-inference module exploited the clinical note to infer initial diagnoses and provide interpretations. Then, the backward-inference module conducted backward inference, i.e., from diagnoses to symptoms. It received the initial diagnoses as input and recalled the corresponding symptoms (including medical examination and laboratory test results). Afterward, all the information was fed into the examination module for verification and decision-making. Specifically, (i) it checked the correctness of the interpretation from the forward-inference module and deleted the erroneous ones by taking the recalled knowledge as ground-truth, (ii) it supplemented the interpretations by considering both the patient's note and the recalled knowledge, (iii) it further determined the decision-making, i.e., either output or filter out the predictions, depending on the predictions' quality. The underlying idea was that a predicted diagnosis with only a few interpretations was not trustworthy. Regarding this, we set a threshold \u1e9e to assess the confidence of a diagnosis. In detail, if the number of interpretations (symptoms) that supported a diagnosis was smaller than the threshold \u1e9e, the diagnosis was regarded as erroneous. Vice versa, if the number of interpretations exceeds the threshold \u1e9e, the diagnosis and interpretation were assumed trustworthy and taken as output. Later, the \u201cturn back\u201d mechanism took the low-confidence diagnoses as feedback for the forward-inference module to \"think twice\u201d. The above steps formed an iterative procedure. We assigned a maximum iteration number \u03bb to control it; once this limit was reached, the framework would output the final results. In this study, we implemented the three modules with publicly available LLMs, i.e., GPT-4 and GPT-4o, and reported the performance on our dataset. \n\nWe adopted several baseline methods: CoT [28], Diagnosis-CoT [27], and Self-consistency CoT (SC-CoT) [37]. In our implementation, SC-CoT generated five predictions for the same medical note and then selected the most consistent diagnoses and interpretations. The prompt of the baselines can be found in Supplementary Data 9. For a fair comparison, all the methods were implemented with the same LLM. We chose two representative LLMs, i.e., GPT-4 and GPT-4o; the API was \u201cgpt-4-turbo-preview\u201d and \u201cgpt-4o\" from the OpenAI company. We further analyzed the impact of the hyper-parameter \u1e9e on the performance and presented the results in Supplementary Data 13.\""}, {"title": "Automatic evaluation metric", "content": "Considering that our established dataset contains ground-truth labels, we conduct automatic evaluation by comparing the ground-truth annotation with the predicted ones. To evaluate the performance of disease diagnosis, we followed related papers [38] and adopted accuracy as the metric. It is calculated as:\nDiagnosis accuracy =  $\\frac{Cumulative \\ number \\ of \\ correct \\ diagnoses}{Total \\ number \\ of \\ diagnoses}$ (1)\nAs for examining the performance on interpretation, we employed accuracy, BERTScore [39], SentenceBert [40], and METEOR [41], which were widely used in related tasks, e.g., text summarization and generation [42, 43]. Concretely, the computation of interpretation accuracy is as follows:\nInterpretation accuracy = $\\frac{Cumulative \\ number \\ of \\ correct \\ interpretations}{Total \\ number \\ of \\ interpretations}$ (2)\nBERTScore [39] employs the BERT model to determine the semantic similarity between the reference text and the generated text. It is proficient at evaluating the semantic alignment within the context, providing a semantic-based analysis of model performance. SentenceBert [40] measures sentence similarity using a fine- tuned BERT model that generates dense vector representations, facilitating efficient and accurate semantic comparisons. METEOR [41] assesses the harmonic mean of unigram precision and recall, utilizing stemmed forms and synonym equivalence."}, {"title": "Design of human evaluation", "content": "We also evaluated the predicted DDx and interpretation with experts' efforts. As for the diagnosis, physicians compared the consistency of the predicted results and ground-truth. If the physicians thought the predicted diagnosis was a synonym or sub-type of the provided ground-truth diagnosis, the prediction was marked as correct. As for the generated interpretation, we followed related works [21, 44] and evaluated the answers from three dimensions: correctness, completeness, and usefulness. In particular, correctness measures whether the statements are medically correct. Completeness measures whether the rationales are comprehensive based on the symptom description. Usefulness is the overall usefulness of the predicted rationales for making the diagnosis. Three physician authors evaluated the results. Each note was assessed by two physicians. If there was a discrepancy in grading, the final decision would be made by a third physician."}, {"title": "Rare disease cases", "content": "We extracted cases of rare diseases for evaluation. We first determined the scope of rare diseases by following the definition from the National Organization for Rare Disorders (NORD) and obtained a list of 1092 rare diseases. See the full list of rare diseases in Supplementary Data 10. Then, we extracted the cases with ground- truth diagnoses involving any rare disease in the list. Following this extraction guideline, we obtained 145 rare disease cases as a subset of the data."}]}