{"title": "Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors", "authors": ["Georgios Chochlakis", "Alexandros Potamianos", "Kristina Lerman", "Shrikanth Narayanan"], "abstract": "In-context Learning (ICL) has become the primary method for performing natural language tasks with Large Language Models (LLMs). The knowledge acquired during pre-training is crucial for this few-shot capability, providing the model with task priors. However, recent studies have shown that ICL predominantly relies on retrieving task priors rather than \"learning\" to perform tasks. This limitation is particularly evident in complex subjective domains such as emotion and morality, where priors significantly influence posterior predictions. In this work, we examine whether this is the result of the aggregation used in corresponding datasets, where trying to combine low-agreement, disparate annotations might lead to annotation artifacts that create detrimental noise in the prompt. Moreover, we evaluate the posterior bias towards certain annotators by grounding our study in appropriate, quantitative measures of LLM priors. Our results indicate that aggregation is a confounding factor in the modeling of subjective tasks, and advocate focusing on modeling individuals instead. However, aggregation does not explain the entire gap between ICL and the state of the art, meaning other factors in such tasks also account for the observed phenomena. Finally, by rigorously studying annotator-level labels, we find that it is possible for minority annotators to both better align with LLMs and have their perspectives further amplified.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Radford et al., 2019; Ouyang et al., 2022a; Touvron et al., 2023; Dubey et al., 2024; Brown et al., 2020; Achiam et al., 2023) have become to dominate language processing as generalists that can perform many tasks. This dominance comes from the emergence of methods such as In-Context Learning (ICL; Brown et al. 2020) and Chain-of-Throught prompting (CoT; Wei et al. 2022), wherein LLMs perform tasks by leveraging input-output demonstrations and task instructions in the prompt only, without any parameter updates.\nWhile ICL is often contrasted with traditional in-weights learning (i.e., gradient-based updates of the models' parameters) (Kossen et al., 2023; Chan et al., 2022), the ICL abilities of LLMs depend on their general, in-weights prior knowledge of several concepts, allowing them to perform many tasks in a zero-shot or few-shot manner. Therefore, studying how LLMs leverage the context in relation with their existing knowledge is a prerequisite to understanding ICL.\nPrior work found evidence that LLMs may be overly reliant on their prior knowledge, disregarding the demonstrations in the prompt. Specifically, Min et al. (2022) demonstrated that, under certain circumstances, LLMs ignore the provided signal in their prompt in the form of the mapping between inputs and outputs, and instead act as a database of tasks; they focus on the examples and the labels independently to fetch the underlying task (Xie et al., 2021): Min et al. (2022) sampled examples and labels independently, and showed very little change in performance. Since no annotations are provided, the setting is given the status of \"zero-shot\" inference, namely task-recognition zero-shot. While follow-up work (Kossen et al., 2023; Wei et al., 2023; Pan et al., 2023; Yoo et al., 2022) has further studied this phenomenon and questioned the generality of the phenomenon, more recent work (Chochlakis et al., 2024b,a) has provided further, quantitative evidence for the relative importance of prior and evidence in the posteriors of the models. Specifically, in complex subjective tasks like multilabel emotion or morality recognition, LLMs seem to virtually disregard evidence from the dataset's mapping in their posterior predictions, even in the form of Chain-of-Thought prompting (CoT; Wei et al. 2022), performing significantly worse than traditional algorithms (Al-"}, {"title": "2 Related Work", "content": "2.1 In-Context Learning and Priors\nICL (Brown et al., 2020) has been used extensively to evaluate LLMs on standard benchmarks (Srivastava et al., 2022). It requires no gradient-based interventions, which are otherwise costly to perform for large models, and usually achieves competitive or state-of-the-art performance. The existence of commercial APIs (Achiam et al., 2023), coupled with open-weights alternatives (Touvron et al., 2023; Dubey et al., 2024) have made ICL an accessible generalist for language tasks and more (Liu et al., 2024). Previous work has further studied controlling the reliance on context and in-weights knowledge through the distribution of the training data (Chan et al., 2022), examining how to optimally select examples for the prompt (Rubin et al., 2022; Gupta et al., 2023), integrating instructions explicitly during training (Touvron et al., 2023; Ouyang et al., 2022b), etc. Relevant to our work, prior work has elicited ICL priors by providing random labels for the examples of the prompt (Min et al., 2022). The resulting minimal variations in performance indicates that LLMs recognize and retrieve their prior knowledge of the task in the prompt rather than doing any \u201clearning\u201d. Subsequent results challenged the view that LLMs mostly perform task recognition, showcasing a significant degradation in performance when scaling the prompt (Kossen et al., 2023) or when focusing on specific tasks instead of aggregates (Yoo et al., 2022), and analyzed behavior with certain label manipulations (Kossen et al., 2023; Pan et al., 2023; Wei et al., 2023). More recent work, however, suggests that in complex subjective tasks like emotion and morality classification, the prior understanding of the task dominates posterior predictions (Chochlakis et al., 2024b).\nOne potential way to augment ICL and overcome the prior bias is with CoT (Wei et al., 2022). CoT incorporates the derivation process from input to output explicitly in the prompt, presenting a more human-like reasoning process. This has several advantages, such as making some patterns in the data explicit in the prompt, making model responses more explainable, and potentially directing more computing resources towards more complex problems. However, detailed analysis has cast some doubt on the reliability and the faithfulness of this reasoning technique (Lanham et al., 2023; Turpin et al., 2024). Nonetheless, CoT seems to improve the robustness and performance of LLMs across a plethora of tasks, yet follow-up work on subjective tasks showed no improvements and the same bias towards a reasoning prior, especially for larger models (Chochlakis et al., 2024a). Methods such as Tree of Thoughts (Yao et al., 2024) or self-consistency (Wang et al., 2022) have experimented"}, {"title": "2.2 Annotator Disagreement and Modeling", "content": "Many works have attempted to model individual annotator perspectives. Researchers used the EM algorithm (Dawid and Skene, 1979) to assign confidence to each annotator's evaluations (Hovy et al., 2013). Recently, Gordon et al. (2022) concatenated features derived from Transformers (Vaswani et al., 2017) with annotator embeddings that incorporate demographics to model individual perspectives. Demographic information has also been incorporated in word embeddings by Garten et al. (2019). Demographic information and psychological profiles have also been statistically examined in text annotations to derive insights into systematic biases (Sap et al., 2022). Recent work has tried to filter annotators based on deep learning methods (Mokhberian et al., 2022), to model annotators on top of common representations (Davani et al., 2022; Mokhberian et al., 2023), and to decrease annotation costs based on agreement (Golazizian et al., 2024). Modeling annotators with LLMs has shown limited success, and LLM biases have also been explored (Dutta et al., 2023; Abdurahman et al., 2024; Hartmann et al., 2023; Tao et al., 2024)."}, {"title": "3 Methodology", "content": "We closely follow the methodology and notation of Chochlakis et al. (2024a). For a set of examples $X$, and a set of labels $y$, a dataset $D_a$ defines a mapping $f_a: X \\rightarrow Y$, where $a$ denotes a specific annotator or the aggregate, as well as reasoning chains $R^a(x) = r, x \\in X$ that explicitly describe $f_a$, and therefore $D^a = \\{(x, y, r): x \\in X, y = f^a(x), r = R^a(x)\\}$, from which we can sample demonstrations with $p(x, y, r)$. We do not differentiate between splits for brevity. Given CoT prompt $S = \\{(x_i, y_i, r_i): (x_i, y_i, r_i) \\sim P_S, i \\in [k]\\}$ with $k$ demonstrations sampled with distribution $p_S$ from $D^a$ without replacement, an LLM produces its own mapping and predictions for the task, denoted as $f_k(.; p_s): X \\rightarrow Y$. When using regular ICL, we simply drop the reasoning text. For all our experiments, we set the temperature $T = 0$ to derive deterministic predictions, and"}, {"title": "3.1 Similarity and Performance Metrics", "content": "To keep evaluations consistent when using API-based LLMs, we rely on similarity measures calculated directly on the final predictions rather than probabilistic measures like logits. Therefore, we use the Jaccard Score, Micro and Macro F1 metrics (Mohammad et al., 2018) to evaluate the performance of the models. For consistency, we also use them to quantify the similarity between the predictions from different model runs or annotators, since they are symmetric functions which we apply to interchangeable sets."}, {"title": "3.2 Task Prior Knowledge Proxies via Zero-shot Inference", "content": "Here, we precisely define the priors that we use for ICL and CoT. First, we have the true reasoning task-recognition zero-shot\u00b9 prior, where the prompt contains $k$ demonstrations sampled with $p^1(x,y,r) = p(x)p(y)p(r)$, so text, labels, and reasoning are sampled independently from each other from $D_a$, hence labels and reasoning are irrelevant to the text and each other. This effectively maintains the higher-order relationships between labels, which are strong in such multilabel tasks (Cowen and Keltner, 2017). For ICL, we have the corresponding task-recognition zero-shot prior sampled with $p^1(x, y) = p(x)p(y)$, so text and labels are also sampled independently. The similarity of the priors to annotators and aggregate are, therefore, measured by comparing (as described in Section 3.1) the prior predictions $f_k(.; P^1)$ with the annotator's labels, which is equivalent to the prior performance for the annotator, and"}, {"title": "3.3 Prompt Design", "content": "Because the specific examples and their order in the prompt can affect the output of the model, we use exactly the same examples and in the same order across corresponding experiments. To achieve that, we find groups of annotators with significant overlap in the train and in the evaluation sets and use the same samples, including for aggregate and prior. Since the only degree of freedom is the labels, we eliminate many potential confounding factors. We show more details in the Appendix."}, {"title": "4 Experiments", "content": "4.1 Datasets\nMFRC (Trager et al., 2022): Multilabel moral foundation corpus with annotations for six moral foundations: care, equality, proportionality, loyalty, authority, and purity. We use annotators 00 through 04 (common examples between groups 00-01-03 and 02-04).\nGoEmotions (Demszky et al., 2020): Multilabel emotion recognition benchmark with 27 emotions. For efficiency and conciseness, we pool the emotions to the following seven \"clusters\u201d by using hierarchical clustering: admiration, anger, fear, joy, optimism, sadness, and surprise. We use annotator triplets 4-37-61 and 7-36-60."}, {"title": "4.2 Implementation Details", "content": "We use the 4-bit quantized versions of the open-source LLMs through the HuggingFace (Wolf et al., 2020) interface for PyTorch. We use LLaMA-2 7B and 70B, LLaMA-3 8B and 70B, GPT-3.5 Turbo, and GPT-40 mini. We chose only models with RLHF (Ouyang et al., 2022b) to further reduce confounding factors. We perform 3 runs for each LLM experiment, varying the examples used. Statistical significance is calculated with permutation tests and measured by considering all 3 runs as separate data points. We use random retrieval of examples. We use less shots for CoT given that the reasoning in the prompt increases its length. We generated reasoning chains for each example per annotator and for the aggregate, and use one triplet for CoT. We use one NVIDIA A100 and one V100."}, {"title": "4.3 Baselines", "content": "To establish baseline performance of LLMs compared to smaller, gradient-based methods, we present performance with and without CoT prompting compared to BERT-based (Devlin et al., 2018) SpanEmo (Alhuzali and Ananiadou, 2021). In Figure 1, we demonstrate the significant difference in performance across all the LLMs (45 shots for ICL, 15 shot for CoT) and SpanEmo. In fact, given the very high Jaccard Score and very low F1 scores, results for MFRC indicate close to random performance for the model, even with CoT.\nNevertheless, we argue that this is an artifact of the inconsistent mapping used in the prompt, caused by the aggregation of labels for different annotators. Next, we evaluate whether aggregation does create annotation artifacts, and the extend to which they influence model behavior."}, {"title": "4.4 Main Results", "content": "In this section, we present our experiments, aimed at disentangling the role of aggregation in subjective tasks. First, we focus on 45-shot ICL (Section 4.4.1), and analyze how the similarity of each annotator and of the aggregate with the models' prior affects the relative improvement of the posterior $p_S$ over the prior $p_1$, as well as absolute posterior performance. Then, we analyze how the majority and minority (or idiosyncratic) annotators fare"}, {"title": "4.4.1 Annotator Modeling with ICL", "content": "Similarity to Prior In Figure 3, we first see the performance of the models for each annotator and the aggregate w.r.t. the similarity of each to the prior of each model. For the performance of the model, we see that, as expected, similarity with the prior correlates positively with final performance, with all results but one being statistically significant. It is interesting to see that the aggregate ranks low both for MFRC and GoEmotions in terms of posterior (from left to right and top to bottom: 5/6, 4/6, 6/6, 6/6, 5/6, 6/6, 5/7, 5/7, 6/7, 5/7, 5/7, 2/7; average is 22nd percentile) and prior (5/6, 2/6, 6/6, 6/6, 4/6, 5/6, 6/7, 4/7, 5/7, 2/7, 5/7, 2/7; average is 33rd percentile). Looking at the relative improvement, it is interesting to see that the only significant trends are negative trends, meaning that the LLMs tend to boost opinions they disagree with more. Despite the aggregate being among the worst performing mappings, with the expectation being that it receives significant gains in performance, it ranked below average (4/6, 4/6, 2/6, 6/6, 3/6, 5/6, 4/7, 5/7, 3/7, 6/7, 1/7, 4/7; average is 39th percentile).\nAgreement with Aggregate By switching to examining at the similarity of each annotator to the aggregate, and how that correlates with absolute and relative performance, in Figure 4, we see strongly negative trends. In fact, 17 out of the 24 cases are negative, 6 of which are statistically significant, and only one positive trend is statistically significant. Therefore, we see that idiosyncratic annotators both have better performance and are more amplified.\nOverall, we see very strong correlational evidence that not only are aggregates misaligned with the models' priors, they also benefit less from ICL with their labels in the prompt. This is happening in spite of annotators with worse alignment frequently receiving significant gains in performance. Consequently, by combining our findings from the priors and the aggregates, we conclude that the aggregate mapping inherently has inconsistencies that inject sufficient noise in the prompt."}, {"title": "4.4.2 Annotator Modeling with CoT", "content": "We now switch to CoT, and evaluate consistency across prompting techniques. We note that because of the decreased number of runs in this setting, the confidence in these findings is similarly decreased.\nSimilarity to Prior In Figure 5, we observe similar trends to the equivalent setting for ICL (Figure 3), namely that final (posterior) performance positively correlates with the prior similarity (10 of 12 cases are positive, 6 of which are statistically significant), and that relative improvement of the posterior compared to the prior is negatively correlated with similarity with the prior (10 of 12 cases are negative, 4 of which are statistically significant). That being said, the differences in performance in Figure 5 tend to be smaller than in Figure 3. The aggregate is among the worst performers in MFRC, but the results in GoEmotions are equivocal.\nAgreement with Aggregate In Figure 6, we present CoT results and correlate them with the similarity to the aggregate. Our experiments here seem to be split between negative and positive trends. Here, due to the decreased number of experiments, it is difficult to extract concrete findings, yet we can ascertain that the findings here do not seem to contradict our previous findings, and do no show improved performance compared to ICL modeling."}, {"title": "4.5 Detailed Analysis", "content": "Based on observations during our manual annotation efforts, we identified clear patterns that annotator 01 in MFRC provides the Authority label more frequently even when one authority figure is mentioned in the input, in contrast to the rest of the annotators. Therefore, if any learning is achieved from ICL and CoT, we expect the accuracy for Authority to be visibly improved compared to the prior baseline due to the consistency and the clear pattern. To achieve and test that, we made this implicit bias clear in the generated reasoning chains. We present the change in performance in Table 1 in comparison to another label, Equality, chosen at random, since we did not observe any other clear patterns in other labels. We do indeed observe that the gains in Authority F1 score are consistent and tend to be significant across models and settings (with only GPT-40 mini CoT presenting an insignificant drop), in opposition to Equality, where gains tend to be small and insignificant, and large drops in performance are observed. This does indicate an ability for the models to somewhat learn and revise priors from the prompt when the mapping and/or the rationale presented are consistent and clear."}, {"title": "4.6 Best Performing Annotators", "content": "Finally, and as a best case scenario, we present the performance of the best annotator with ICL (based on performance across models as can be seen in Figures 3 and 4) and compare that to the aggregate. Results are shown in Figure 7. The benefits from modeling a specific annotator are evident, as we observe large gains in performance in two out of"}, {"title": "5 Conclusion", "content": "In summary, we see that aggregated labels tend to align less with the LLMs' prior for each task. Furthermore, and in spite of worse aligned annotators receiving larger posterior performance increases, the aggregate posterior appears to collapse to the prior, resulting in significantly worse performance to several annotators. This result indicates that the majority is not necessarily well-aligned with models. Then, it is evident that interpretable and consistent mappings can be modeled by LLMs and improve upon the prior, even though the model might not align with the specific annotator a priori. Finally, we see that individuals do indeed result in better performance on the task.\nGiven the commonsense reasoning capabilities of LLMs, the emotional and moral capabilities of LLMs demonstrated in settings different from traditional machine learning settings (Tak and Gratch, 2024), as well as best-case results, we conclude that the aggregation process introduces artifacts in the labels that cause LLMs to ignore the mapping as noise. It is interesting to note that in our prompts, the aggregate in rarely a hodgepodge of disparate opinions, wherein the aggregate does not match any individual, but factors like different annotator mixtures, especially between train and test splits, as well as different annotator groups prevailing in different examples, introduce sufficient noise in the annotations that cannot be modeled with ICL and CoT. That being said, the performance gap with gradient-based methods remains large, suggesting that other factors like task complexity also majorly account for the observed biasing effects.\nFinally, we question what it means to model these aggregate opinions. Namely, since they might not reflect the opinion of any individual, even if in a minority of cases, then what rationale should be provided and how should it be generated in a sound manner? We advocate, therefore, as previous work has done (Prabhakaran et al., 2021; Dutta et al., 2023), for releasing and modeling annotator-level labels instead of aggregates, and suggest that the field of subjective modeling should move away from aggregate modeling in the age of LLMs and of more elaborate modeling methods such as CoT."}, {"title": "6 Limitations", "content": "Given our constraints to standardize the prompt and remove other degrees of freedom that can constitute potential confounding factors in our evaluations, but also for computational efficiency, the evaluations sets contain a small number of examples (namely, 100 and 71 for the triplets in GoEmotions, and 100 for both groups in MFRC), increasing the noise in our findings. Nonetheless, this practice has become standard in the evaluation of LLMs.\nIn our study of specific labels and the effects of what we perceive as consistency on the performance of LLMs (Section 4.5), a potential confounding factor in analysis is the increased frequency of the label, since the studied annotator was more sensitive to specific stimuli in the input, as described. We specifically chose to perform a more detailed analysis in this positive pattern, however, because we expect the models, as did we, find it easier to distinguish positive patterns in the data.\nIt is important to note that we have performed experiments in only two problems and benchmarks, as we opted in favor of presenting more LLMs. Therefore, our findings may not generalize to other, highly subjective tasks. Furthermore, other datasets with stricter annotation manuals that aim to resolve all ambiguities may not present similar behavior, as annotator agreement is artificially raised by removing some of the subjectivity of the semantic interpretations of the annotators in favor of following stricter, highly specific instructions.\nWe also want to note that we do not perform Bonferonni correction across models and datasets given the small number of datapoints we have to compute our correlations, yet we believe it is important to highlight the settings with smaller p-values.\nMoreover, datasets with data derived from social media have been criticized as lacking the context for a model or even humans\u2014to make appropriate judgments about the emotion or morality expressed in them (Yang et al., 2023), and techniques to evaluate the correctness of the labels in a dataset have been designed to discard noisy samples (Swayamdipta et al., 2020; Mokhberian et al., 2022). Given the subjective nature of the task and the lack of context, such tools could be used to perhaps improve performance. That being said, the interpretation by humans is sufficiently consistent for SpanEmo to achieve better performance than every LLM. We also note that by removing ambiguous examples, we may also remove that which makes these tasks challenging (Aroyo and Welty, 2015)."}, {"title": "7 Ethical Considerations", "content": "Our focus on traditional machine learning benchmarks, as well as our takeaways, should complement and not compete with quantifying bias in LLMs using other tools and techniques (Caliskan et al., 2017; Gonen and Goldberg, 2019; Ferrara, 2023; Abdurahman et al., 2024). It is also important to emphasize that improving affective and moral capabilities in LLMs entails perils, since better catering the emotional and moral responses to more contexts and personalizing them to specific individuals can lead to improved manipulation of users by LLMs."}, {"title": "A Annotators", "content": "We reiterate that we group annotators together based on their overlap in the train and evaluation sets so as to standardize the prompt across them. We use 5 annotators in MFRC, one triplet and one pair, both with enough common examples in the evaluation set to present somewhat robust results. For both annotator groups, we use 100 evaluation examples. For GoEmotions, we use 6 annotators, as these were the only groups with enough evaluation examples, 100 and 71 respectively. We did not find any possible quadruplets or larger groups."}, {"title": "B Prompt", "content": "For completeness, we showcase example prompts we use in MFRC for ICL and CoT that illustrate the prompt format we utilize across all experiments. One example for each setting is shown in Table 2. We note that the prompts do not utilize the conversational format even though it is available across all our models because we found it to work worse in terms of performance (in terms of the performance of the aggregate in GoEmotions) and ability to decode (e.g., including explanations without being prompted to, diverging in terms of output format, or even predicting emojis rather than emotion words) compared to the used one."}]}