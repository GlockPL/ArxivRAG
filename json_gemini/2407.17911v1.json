{"title": "ReCorD: Reasoning and Correcting Diffusion for HOI Generation", "authors": ["Jian-Yu Jiang-Lin", "Kang-Yang Huang", "Ling Lo", "Yi-Ning Huang", "Terence Lin", "Jhih-Ciang Wu", "Hong-Han Shuai", "Wen-Huang Cheng"], "abstract": "Diffusion models revolutionize image generation by leveraging nat-ural language to guide the creation of multimedia content. Despite significant advancements in such generative models, challenges persist in depicting detailed human-object interactions, especially regarding pose and object placement accuracy. We introduce a training-free method named Reasoning and Correcting Diffusion (ReCorD) to address these challenges. Our model couples Latent Diffusion Models with Visual Language Models to refine the gener-ation process, ensuring precise depictions of HOIs. We propose an interaction-aware reasoning module to improve the interpretation of the interaction, along with an interaction correcting module to refine the output image for more precise HOI generation deli-cately. Through a meticulous process of pose selection and object positioning, ReCorD achieves superior fidelity in generated images while efficiently reducing computational requirements. We conduct comprehensive experiments on three benchmarks to demonstrate the significant progress in solving text-to-image generation tasks, showcasing ReCorD's ability to render complex interactions ac-curately by outperforming existing methods in HOI classification score, as well as FID and Verb CLIP-Score. Project website is available at https://alberthkyhky.github.io/ReCorD/.", "sections": [{"title": "Introduction", "content": "In recent years, diffusion models have become a cornerstone in the field of multimedia processing, demonstrating remarkable success across a wide array of generative tasks [26, 36, 37, 55, 64, 72]. Among them, text-to-image (T2I) generation has attracted significant attention [8, 12, 42, 56, 63] due to its user-friendly nature lever-aging natural language guidance. While state-of-the-art (SOTA) T2I diffusion models, such as SDXL [44] and DALL-E 3, have greatly improved image realism and expanded the conceptual possibilities of generation, ongoing challenges persist. In particular, they often encounter difficulties with text prompts that contain intricate human-object interactions (HOI) [23].\nAs illustrated in Figure 1, despite their training on extensive datasets, T2I methods like SDXL and DALL-E 3 exhibit flaws in rendering human poses or object placements. In the example of \"a boy is lying on a bench,\" although DALL-E 3 precisely captures the lying pose, it incorrectly locates the boy, whereas SDXL places the boy on the bench accurately but does not manage to render the lying down pose. These inaccuracies may stem from the inher-ent biases or assumptions about the interaction between the given human and object embedded within the large-scale datasets [52] used to train T2I models [16]. Such biases can lead to hallucina-tion problems [35], resulting in models failing to generate images matching the intended interactions accurately. For instance, given the prompt \"a man is carrying a bicycle\" as in Figure 1, SDXL and DALL-E 3 might err in posture and object placement because the most common association in the datasets is \"riding,\" leading to inaccuracies in depicting the intended interaction.\nTo enhance the accuracy of T2I models in generating interac-tions, a possible avenue is to ensure the correct positioning of both the human and the object within the image. Previously, several layout-to-image (L2I) models [13, 27, 30, 61, 71] have been proposed to include the layout of each object as additional input for diffusion models, aiming to gain more precise control over the out-put images. For instance, GLIGEN [30] retrain the model with the layout-annotated dataset by the Gated Self-Attention. DenseDif-fusion [27] and BoxDiff [61] exemplify training-free L2I methods that necessitate supplementary inputs for operation. However, the requirement for user-specific layouts can be time-consuming and inconvenient for users. In addition, especially in scenarios involv-ing HOI, simplistic inputs like boxes prove inadequate in capturing complex attributes such as posture and body orientation, which are crucial for the accurate depiction of interaction, ultimately resulting in suboptimal images. The deficiency is evident in Figure 1, where BoxDiff struggles to generate realistic interactions despite being provided with layout information as additional input.\nOn the other hand, alternative approaches have integrated Large Language Models (LLMs) to augment diffusion models, aiming better to grasp the nuances of textual prompts in image gener-ation [15, 31, 45, 60, 62, 65]. Innovations such as LMD [31] and LayoutLLM-T2I [45] have pioneered to employ LLMs for creating more intuitive and accurate image outputs. LMD utilizes a dual-phase approach, initially using a pre-trained LLM to create a scene layout with captioned bounding boxes. It further proceeds with a layout-grounded controller to guide diffusion models. Addition-ally, LayoutLLM-T2I starts by generating a coarse layout and then implements a specially trained transformer module within the de-noising UNet for fine-grained generations. Despite the progress made with LLM-assisted methods in image generation, a pivotal limitation arises when handling HOI. Predominantly dependent on textual prompts, these methods exhibit two possible shortcom-ings. Firstly, they may overlook the intricate spatial dynamics and nuanced interactions within an image due to the limited informa-tion the prompt provides. Secondly, in some cases, the LLMs may over-analyze the textual prompt, leading to hallucinations where they generate fabricated content that is not grounded in reality. An evident illustration of this phenomenon is presented in the first-row of Figure 1. Despite the prompt suggesting a man carrying a bicycle, the LLM-assisted LayoutLLM-T2I tends to overanalyze and assume an additional man riding the bike instead, resulting in the generation of human-like artifacts riding the bicycle, which is not the intended interaction. Such deficiency underscores the critical need for advanced approaches to interpret information from the concise HOI prompts better, ensuring image generation that closely aligns with the intended interactions.\nIn this paper, we propose Reasoning and Correcting Diffusion (ReCorD) for generating HOI. We argue that proper HOI genera-tion requires both the correct human posture and precise object positioning to facilitate realistic interaction. To achieve this, we in-troduce an innovative pipeline depicted in Figure 2. Our approach includes three key steps. First, we employ the Latent Diffusion Model (LDM) to produce a set of human candidates performing the verb in the text prompt, emphasizing the correct posture by employ-ing intransitive prompts. Next, as Visual Language Models (VLMs) excel at comprehending image contents, we harness their visual reasoning capabilities to select candidates with optimal posture and determine the appropriate placement of the object based on their interpretation of the interaction scene. Finally, we introduce a refinement mechanism to adjust object positions while preserv-ing accurate human posture. By implementing inverse attention masks and bounding box constraints, we prevent the overlap of attention maps between humans and objects during image gen-eration, enhancing the fidelity of the final image. Our proposed ReCorD guarantees precise control over the depicted interactions, effectively mitigating the risks of hallucinations. This work pio-neers training-free interaction generation, eliminating the need for extra labeled data and bypassing training-related computational challenges. We summarize our main contributions as follows:\n(1) We introduce a novel reasoning framework that integrates LDM with VLMs to overcome the challenges of generating realistic HOI, mitigating issues presented in previous ap-proaches, such as LLMs overanalyzing simple text prompts and training data biases in LDM.\n(2) To enhance human figure depiction accuracy, we design a correction mechanism within LDM for dynamic image ad-justment, enabling precise control and refinement of human interactions in generated images as well as enhancing the portrayal accuracy significantly.\n(3) The extensive experiments demonstrate our training-free ReCorD's proficiency in creating captivating and realistic HOI scenes, outperforming state-of-the-art techniques."}, {"title": "Related Work", "content": ""}, {"title": "Conditioned T2I Diffusion Models", "content": "Recent advancements in diffusion models [9, 11, 21, 25, 38, 54] have significantly improved the capabilities of large-scale T2I gen-eration models such as DALL-E [47, 48], Imagen [51], and Stable Diffusion [44, 50]. While these models guided solely by plain text demonstrate the ability to generate high-quality images, they strug-gle with prompts that demand detailed attribute specifications and a nuanced understanding of spatial relationships [49]. Consequently, recent research on diffusion models extended beyond text-based conditions and incorporated advanced conditioning mechanisms such as inpainting masks [27], sketches [58], key points [30], seg-mentation maps [10] and layouts [16], facilitating enhanced spa-tial manipulations. The modification of models marks this evolu-tion to include additional encoders, achieved through strategies like fine-tuning [3, 30, 41, 67], or constructing new models from scratch [24]. For instance, SpaText [3] and GLIGEN [30] introduce spatial modulations into pre-trained models, employing fine-tuning with adapters to perform layout constraints. Despite the enhance-ments, the requirement for model retraining for each new condition type remains a significant challenge. Building on the foundation of the previous diffusion models, we propose a training-free frame-work designed explicitly to enhance the proficiency of LDM in interpreting and visualizing the intricate relationships of HOIs."}, {"title": "Image Generation with Spatial Control", "content": "As T2I models are traditionally trained on datasets featuring brief text captions, they frequently struggle to capture the nuances of more complex captions that contain multiple phrases [53]. Following the insight from Prompt-to-Prompt [18] that trained T2I models inherently provide token-region associations through their atten-tion maps, several works [2, 4, 7, 16, 22, 27, 61] have been proposed to mitigate this issue. For instance, MultiDiffusion [4] choose to conduct individual denoising procedures for every phrase, respec-tively, at every timestep. However, this independent generation technique frequently stumbles over lifelike compositions and is easily hindered by bias toward specific actions or objects. Attend-and-Excite [7] strategically manipulates the noise map to enhance the activation of previously overlooked tokens in cross-attention maps. Yet, a limitation arises since the mere intensification of at-tention to certain tokens does not always lead to a holistic repre-sentation of the intended information within the generated output. BoxDiff [61] comes up with three spatial constraints to optimize the cross-attention layers given instinctive inputs, e.g., bounding box or scribble, during the sampling process. InteractDiffusion [22] introduces a novel approach to generating images with precise HOI by tokenizing interaction using a conditioning self-attention layer for the accuracy of the complexities of interaction represen-tation. To better impose regulations on the generated human, the rectangular-shaped constraints cannot be applied since bounding boxes cannot properly convey the details of human action, such as body orientation, facial direction, etc. Building on these training-free approaches, our pipeline assures that the object is matched with a sufficiently strong attention map and the human can be generated in a legitimate pose. Thus, a complex scene containing interaction can be precisely rendered."}, {"title": "LLM-assisted Image Generation", "content": "The integration of Large Language Models (LLMs) with diffusion models has significantly transformed T2I generation, capitalizing on the superior generalization abilities of LLMs [14, 31, 32, 60, 62, 68]. LayoutGPT [14] adopt LLMs for layout generation via in-context learning. VisorGPT [62] takes one step further by fine-tuning to em-brace diverse modalities, including key points, semantic masks, etc. LMD [31] represents pioneering efforts in this integration, utilizing LLMs to interpret object locations from text prompts, thus enhanc-ing the accuracy and quality of generated images. LayoutLLM-T2I [45] queries ChatGPT for text-to-layout induction and intro-duces a Layout-aware Spatial transformer with a view to improve layout and image generation simultaneously. Emphasizing the cen-tral role of LLMs, these advancements bypass the need for addi-tional information inputs, allowing LLMs to shape the initial layout configurations and interpret user prompts directly. Despite the suc-cessful outcomes of using LLMs, they fail to address the challenge of generating the explicit posture of humans given specific actions. Compared to LLMs, LDMs demonstrate a better understanding of scene kinetics based on simple text prompts. Therefore, we propose to leverage such advantage of LDMs alongside the robust visual reasoning abilities of VLMs to generate accurate HOI."}, {"title": "Method", "content": "We introduce the ReCorD, an interaction-aware model that main-tains training-free superiority. The resulting images hinge on the human adopting the appropriate pose and ensure the object is lo-cated in a suitable position according to the given text prompt. Our generative pipeline comprises three modules: Coarse Candidates Generation, Interaction-aware Reasoning, and Interaction Correct-ing. To abbreviate these modules, we term them as Mg, Mr, and Mc, respectively. We decomposed the denoising process T into two stages, i.e. T\u2081 and T2, by observing that the diffusion model captures the initial layout during the early denoising steps and refines the de-tails in the later iterations [4]. In the former stage, Mg generates k coarse candidates, while M, suggests the ideal pose and layout w.r.t. the text prompt. Subsequently, Mc corrects object locations while preserving selected poses to refine the cursory images into desired ones. Importantly, ReCorD empowers the diffusion model to create images aligned with text prompts, highlighting complex spatial conditions and intricate interactions without additional training. The overall pipeline is depicted in Figure 2, and we elaborate on the details of each module in the following sections."}, {"title": "Coarse Candidates Generation Module", "content": "Given a text prompt y describing a HOI triplet (i.e., \"a subject is verbing an object\"), we enhance interaction representation by adopt-ing distinct attention mechanisms [57] within Mg. More precisely, we manipulate cross-attention and self-attention maps to generate candidate images k associated with the action subject to the prompt.\nCross-Attention Maps Manipulation. To facilitate image gen-eration concerning the textual information, we incorporate such conditions into LDMs using cross-attention maps. During denoising, LDMs initially sample a latent vector z\u0142 from a Gaussian distribu-tion N(0, 1) and progressively remove noise to obtain zt-1 at each step t\u2208 [T\u2081,..., 0]. After encoding the prompt y into text tokens via text encoder, the cross-attention map is defined as follows:\n$$A = Softmax (QK^T / \\sqrt{d})$$,\nwhere Q = $q(\u03c6(z)) and K = \u00a2k(\u03c8(y)) represent the query and key embeddings derived by corresponding projection functions. q and y are spatial normalization [70] and the text encoder from CLIP [46] yielding intermediate representations and N text tokens (y) = {w1w}. For simplicity, we omit the subscript t that represents the denoising step while manipulating attention maps.\nModeling the cross-attention map for the interaction (verbing in y) is challenging when generating HOI scenes, leading to an ambiguous representation of the verb token. To address this issue, we propose an alternative intransitive prompt \u1ef9, which typically excludes object-related descriptions in y. The cross-attention maps A using y can be derived by substituting K in eq. (1) with K = k(\u03c8(\u1ef9)). As illustrated in Figure 3, A captures more informative clues, especially for the verb token, compared to A when using \u1ef9, resulting in interactive representations. Accordingly, we formulate the final cross-attention maps by rearranging them as follows:\n$$A_{cross} = \\begin{cases}\nA_n & \\text{if } w_n \\in \\varOmega(y)\\\\\nA_n & \\text{otherwise},\n\\end{cases}$$\nwhere n denotes the index of text tokens. Ideally, we embrace the attention maps if the text token exists in the intransitive prompt.\nSelf-Attention Maps Manipulation. In contrast to cross-attention maps, self-attention maps lack direct token associations but still in-fluence the spatial layout and appearance of generated images [34]. Therefore, we manipulate the self-attention maps similarly to eq. (2) for latent representation once the denoising step t > y for obtaining Aself, where y is a predefined parameter ensuring that scenes and objects from original tokens (y) can be generated effectively."}, {"title": "Interaction-Aware Reasoning Module", "content": "As an intermediate module bridging the others, we present the Interaction-Aware Reasoning module M, (see Figure 4) following the generation of coarse candidates in Mg. This module comprises two components powered by VLM: the Pose Selection Agent and the Layout Agent. Specifically, the Pose Selection Agent selects an image aligning with the prompt y, while the Layout Agent adjusts the object's location and preserves human key points P and further determines the target position bo for the correction module Mc.\nPose Selection Agent. As the pose plays a vital characteristic in the HOI generation, we first couple an agent to select the appro-priate pose conditioned on the prompt. The Pose Selection Agent integrates the initial prompt y with previously generated candidates to create the pose template. Leveraging the visual comprehension capabilities of VLMs, this agent excels in identifying the precise pose corresponding to y, enhancing the model's ability to inter-pret visual data beyond relying solely on textual cognition as in LLMs. This pivotal step ensures that the pose information initially obtained from LDMs is meticulously refined for subsequent phases.\nLayout Agent. To address the issue of LLM-assisted methods being overly reliant on prompts for sampling layouts, we incorporate the identified key points P and the bounding box bh for humans as additional data. Recognizing that an interaction involves both the relation to human and the object, we collect the crucial information of P using 33 key points in (x, y) coordinates and represent bh = (\u0425\u0442\u0456\u043f, \u0423\u0442\u0456\u043f, \u0425\u0442\u0430\u0445, \u0423\u0442\u0430\u0445). Additionally, we use the image selected by the former agent as inputs to VLMs for layout suggestion tasks.\nWe first extract the object's location bo using Otsu's algorithm [43], an automatic thresholding technique applied to the object's cross-attention map Across to isolate regions with higher values. Subse-quently, we detect human key points using MediaPipe Pose Land-marker to create the segmentation mask mp. Consecutively, we establish a series of guidelines and fixed protocols for VLMs adher-ence, including constraints on bh to maintain the integrity of the intended human poses and an overlapping reduction strategy to improve the quality of generated images containing multiple ob-jects. Furthermore, inspired by the Chain-of-Thought approach [59], we enhance the logical coherence by guiding VLMs to construct visual attribute information for human posture. We ground the VLMs in logical reasoning across multiple Visual Attributes such as pose types, body orientation, object relations, etc. Drawing on insights from previous research [5, 39], we prepare VLMs with three examples, aiding in the clarification of visual representation and preventing over-analysis and hallucinations to construct the interaction template. Eventually, we extract the proposed location bo for the box-constrained loss eq. (4) by text scraping and integrate a checking mechanism to determine whether the alteration in bo falls within a predetermined threshold. If the change is minimal which indicates a minor difference, M, would signal no changes to Mc. This mechanism is crucial for maintaining a streamlined and resource-efficient generation process, ensuring only significant location adjustments prompt further action."}, {"title": "Interaction Correcting Module", "content": "We delicately refine the candidate image provided by the dual agents while preserving the original human pose in Mc, as shown in Figure 5. To combine the generative capabilities of LDMs with the reasoning abilities of VLMs, we incrementally update the latent zt to adjust the object's position and size based on bounding boxes bo related to the interaction. Notably, we conduct the denoising process for t \u2208 [T2,..., 0], including modulation of cross-attention and self-attention maps, as described in Section 3.1.\nSimultaneously modifying the object's location requires con-sideration of potential overlap with the human body since cross-attention maps from different tokens may exhibit strong values in the same region, which will deteriorate image quality. To address this challenge, we introduce a mechanism for eliminating attention overlap. Specifically, given the token index of the object denoted as m, we use the cross-attention map Am to construct an inverse mask at each time step t, denoted as Am = 1 - Am, where 1 is the tensor of the same dimension as Am containing all elements equal to 1. This inverse attention map is then applied to the remaining maps using an element-wise product operation defined as\n$$\\hat{A}_n = \\overline{A}_m \\cdot A_n, \\forall n \\neq m.$$\nThrough eq. (3), we can mitigate the issue of attention overlap between humans and objects while updating the object positions, ensuring the successful generation of updated objects.\nConditioned Spatial Constraints Since our ReCorD is training-free and does not involve additional learnable networks for knowl-edge transfer, we employ box constraints [61] to regularize the denoiser, which can be formulated as\n$$L = L_{IB} + L_{OB} + L_{CC}$$,\nwhere each term in sequence order represents the inner-box, outer-box, and corner constraint, respectively. We apply eq. (4) to update the latent at each time step t with corresponding weight at as\n$$z_t \\leftarrow z_t - \\alpha_t \\nabla L.$$\nWith slight updates to z\u0142 at each step, we ensure the object holds sufficient mutual information with the box region and con-forms to the specified size, i.e. bo, thereby accurately correcting the object's position to represent the interaction. As LDM aims to denoise iteratively and involve the attention maps as intermedi-ate, LDM(zt, y, t, s) is the diffusion process at time step t before manipulation, which seeks the corresponding attention maps. Af-ter going through our proposed ReCorD, the denoising UNet is reused to predict the latent representation at the next step. For-mally, LDM(2t, y, t, s) denotes the diffusion model that adopts the manipulated attention maps, resulting in the prediction, i.e. zt-1."}, {"title": "Experiments", "content": ""}, {"title": "Experimental Setup", "content": "Datasets. Given the absence of a standard benchmark crafted for HOI generation, we assess the efficacy of our approach by extracting HOI triplets from two established HOI detection datasets, namely HICO-DET [6] and VCOCO [17], to form the input text prompts. HICO-DET includes 600 triplets across 80 object categories and 117 verb classes, while VCOCO contains 228 triplets, spanning 80 object classes and 29 verb types. For a comprehensive assess-ment, we incorporate the non-spatial relationship category in T2I-CompBench [23], which is characterized by 875 interaction terms. We select prompts that exclusively involve HOIs in T2I-CompBench. To enhance diversity, we apply random subject augmentation to each verb and object pair extracted from the datasets to form the input prompt. Accordingly, our experiments are conducted across three datasets: HICO-DET, with 7,650 HOI prompts; VCOCO, con-tributing 2,550 prompts; and the non-spatial relationship category of T2I-CompBench, adding 465 prompts.\nBaselines. We report comparisons with nine strong-performing models, 1) T2I models: Stable Diffusion (SD) [50], Attend-and-Excite (A&E) [7], SDXL [44], and DALL-E 3. 2) L2I models: BoxDiff [61], MultiDiffusion [4] and InteractDiffusion [22]. 3) LLM-assisted T2I models: LayoutLLM-T2I [45] and LMD [31]. We utilized the official implementations and the default settings for each baseline. For L2I models, we provide the actual bounding box data from HICO-DET and VCOCO datasets in addition to the text prompts as inputs. For LLM-assisted methods, the input layouts are exclusively generated by LLMs, rather than being sourced from the datasets.\nEvaluation Metrics. To measure the interaction in the generated images, we utilize the CLIP-Score SCLIP [19] evaluating the similar-ity between the input text and the generated images. While this met-ric is commonly applied to estimate fidelity to text prompts, we note its inclination towards a noun or object bias, with CLIP often unable to differentiate among verbs, relying instead on nouns [40, 66]. To address this, we specifically extract verbs from the text prompts and calculate the Verb CLIP-Score Suer. In addition, we introduce a HOI classification score to evaluate the interaction depiction. By transforming a pre-trained, SOTA HOI detector [69] into a classi-fier, we evaluate HOI instances in generated images and compare them against the ground truth of HICO-DET and VCOCO. The accuracy of HOI classification is evaluated based on the top three accuracy scores. HOIFull and HOIRare represent scores for the full and rare set, respectively, on the HICO-DET dataset. The rare set is selected based on having fewer than 10 instances across the dataset.\nMoreover, we employ the Fr\u00e9chet Inception Distance (FID) [20] and PickScore [28] to assess image quality. FID compares the Fr\u00e9chet dis-tance distribution of Inception features between real and generated images, whereas PickScore, a text-image scoring metric, exceeds human performance in predicting user preferences.\nImplementation Details. We choose the Stable Diffusion [50] model as the default backbone and GPT4V [1] as the VLM in Mr. We set the ratio of classifier-free guidance to 7.5, denoising steps T\u2081 = 10, T2 = 50, and use the DDIM [54] scheduler within denoising steps. The number of coarse candidates k = 5, and the hyperparameter y = 5 initials the operation for self-attention maps manipulation. For evaluation, one image is generated per triplet for HICO-DET and VCOCO datasets and three images per triplet for T2I-CompBench."}, {"title": "Qualitative Results", "content": "We provide a qualitative comparison to assess the generated HOIs. As depicted in Figure 6, ReCorD outperforms other SOTA methods by generating realistic human poses and object placements that align with text prompts, proofing its proficiency in depicting object interactions with high fidelity. In contrast, baseline methods often tend to misplace objects or fail to capture the nuances of intended ac-tions. For L2I models with additional layout inputs, BoxDiff achieves object size requirements but struggles to accurately depict inter-action poses; InteractDiffusion fails to accurately portray subtle activities despite the fine-tuning, as seen in (a), (d), (e), and (f); Mul-tiDiffusion strives for precise object placement despite generating images in various sizes. On the other hand, LayoutLLM-T2I, despite leveraging language models for improved layout generation, often produces objects disproportionate to humans, which is evident in (e) and (f). Furthermore, MultiDiffusion defines a new optimiza-tion process for generation but it heavily depends on the prior knowledge of the pre-trained models. Especially, SDXL struggles with action poses (a), (b), (d), and (e), and DALL-E 3 with object sizing and placement (a), (c), (e), and (f), illustrating key areas where ReCorD advances beyond the limitations of existing solutions."}, {"title": "Quantitative Results", "content": "We present the quantitative comparison of the generated results, with prompts sourced from HICO-DET and VCOCO in Table 1, and results using prompts formed from T2I-CompBench in Table 2.\nCLIP-based Image-Text Similarity. The results of CLIP-Score SCLIP reveal that our ReCorD outperforms other methods on HICO-DET and T2I-CompBench, and it is comparable to MultiDiffusion on VCOCO. Furthermore, our ReCorD achieves the best results across all three datasets in terms of Verb CLIP-Score Suer, this confirms our ability to generate more closely matched interactions.\nEvaluation of Image Quality. As per PickScore evaluation, our ReCorD model is comparable with the SDXL model and outper-forms other methods. This demonstrates that after incorporating our designed Interaction-Correcting Module with SD models. Our ReCorD can maintain the image generation quality of the model while achieving more authentic interaction. Furthermore, when comparing the generated images with real ones in HICO-DET and VCOCO datasets using FID scores, our model outperforms other methods except InteractDiffusion. Notably, given that InteractD-iffusion is fined-tuned using HICO-DET and COCO datasets [33], the performance of our ReCorD is particularly remarkable as it operates without the need for training or additional HOI data.\nEvaluation of Interaction Accuracy. Table 1 validates our method significantly enhances the accuracy of HOI generation in both datasets, revealing the efficacy in synthesizing more precise HOI."}, {"title": "Generation Speed and Memory Usage", "content": "For generating one image, we use an Nvidia RTX 6000 GPU, and the memory costs are 14/42 GB when using SD/SDXL as the backbone, with total inference times of 40.66/61.48 seconds."}, {"title": "Comparing MLLMs for Layout Suggestions", "content": "We evaluate BLIP-2 [29] by randomly adjusting the size and position of ground truth bounding boxes in HICO-DET. However, BLIP-2 often misinterprets real-world distributions, providing irrelevant answers and invalid mIoU scores. Conversely, GPT-4V achieves a mIoU score of 49.72%, demonstrating superior layout suggestion accuracy, making it highly suitable for our ReCorD."}, {"title": "GPT-4V for Evaluation", "content": "Following T2I-CompBench for non-spatial relationship evaluation, our ReCorD achieves a GPT score of 98.16, outperforming SOTA T2I methods like SDXL (97.87), MultiDiffusion (97.43), and LayoutLLM-T2I (96.75). This demonstrates ReCorD's ability to generate accurate HOI images aligned with foundation model knowledge."}, {"title": "Ablation Studies", "content": "For the ablation studies, Figure 7 displays the HOI generation results of the SD incorporating modules in our ReCorD: (a) with only SD, (b) with SD and Mg, and (c) with SD and Mg + Mr + Mc. With only SD, the generated outputs appear to be suboptimal, likely influenced by biases inherent in its training data, leading to misinterpretation of the intended interaction described in the text prompts. From the results in (b), it shows that with the inclusion of Mg, the accuracy of the generated human action is significantly improved due to our intransitive prompt altering technique, highlighting that sim-plifying the prompt to focus on the core action enables the model to generate the intended human poses with enhanced precision. However, it still struggles to accurately position objects in relation to humans in the images, resulting in a mistaken interaction. For our ReCorD, M, helps select appropriate poses and retain suitable candidates while Me refines images to obtain accurate interaction with the correct object size and position while maintaining the selected pose. As a result, HOI generation of our complete pipeline in (c) exemplifies the most successful outcomes achieved."}, {"title": "Conclusion", "content": "We have introduced ReCorD framework, tailored explicitly for HOI image generation. This method comprises three interaction-specific modules that synergistically interact with each other. Our core idea revolves around reasoning layout and correcting attention maps using VLM-based agents and an LDM to address this challenge. Extensive experiments demonstrate the effectiveness of our approach in enhancing image accuracy and semantic fidelity to the input text prompts, particularly in capturing intricate concepts of interactions that several baseline generative models struggle with. Additionally, we quantify our improvements through various protocols and a user survey focused on HOI generation, providing valuable insights and paving the way for future explorations in this domain."}]}