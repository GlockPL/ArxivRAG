{"title": "ReCorD: Reasoning and Correcting Diffusion for HOI Generation", "authors": ["Jian-Yu Jiang-Lin", "Kang-Yang Huang", "Ling Lo", "Yi-Ning Huang", "Terence Lin", "Jhih-Ciang Wu", "Hong-Han Shuai", "Wen-Huang Cheng"], "abstract": "Diffusion models revolutionize image generation by leveraging nat- ural language to guide the creation of multimedia content. Despite significant advancements in such generative models, challenges persist in depicting detailed human-object interactions, especially regarding pose and object placement accuracy. We introduce a training-free method named Reasoning and Correcting Diffusion (ReCorD) to address these challenges. Our model couples Latent Diffusion Models with Visual Language Models to refine the gener- ation process, ensuring precise depictions of HOIs. We propose an interaction-aware reasoning module to improve the interpretation of the interaction, along with an interaction correcting module to refine the output image for more precise HOI generation deli- cately. Through a meticulous process of pose selection and object positioning, ReCorD achieves superior fidelity in generated images while efficiently reducing computational requirements. We conduct comprehensive experiments on three benchmarks to demonstrate the significant progress in solving text-to-image generation tasks, showcasing ReCorD's ability to render complex interactions ac- curately by outperforming existing methods in HOI classification score, as well as FID and Verb CLIP-Score.", "sections": [{"title": "1 Introduction", "content": "In recent years, diffusion models have become a cornerstone in the field of multimedia processing, demonstrating remarkable suc- cess across a wide array of generative tasks. Among them, text-to-image (T2I) generation has attracted signifi- cant attention due to its user-friendly nature lever- aging natural language guidance. While state-of-the-art (SOTA) T2I diffusion models, such as SDXL and DALL-E 3, have greatly improved image realism and expanded the conceptual possibilities of generation, ongoing challenges persist. In particular, they of- ten encounter difficulties with text prompts that contain intricate human-object interactions (HOI) . As illustrated in Figure 1, despite their training on extensive datasets, T2I methods like SDXL and DALL-E 3 exhibit flaws in rendering human poses or object placements. In the example of \"a boy is lying on a bench,\" although DALL-E 3 precisely captures the lying pose, it incorrectly locates the boy, whereas SDXL places the boy on the bench accurately but does not manage to render the lying down pose. These inaccuracies may stem from the inher- ent biases or assumptions about the interaction between the given human and object embedded within the large-scale datasets used to train T2I models. Such biases can lead to hallucina- tion problems , resulting in models failing to generate images matching the intended interactions accurately. For instance, given the prompt \"a man is carrying a bicycle\" as in Figure 1, SDXL and DALL-E 3 might err in posture and object placement because the most common association in the datasets is \"riding,\" leading to inaccuracies in depicting the intended interaction. To enhance the accuracy of T2I models in generating interac- tions, a possible avenue is to ensure the correct positioning of both the human and the object within the image. Previously, several layout-to-image (L2I) models have been pro- posed to include the layout of each object as additional input for diffusion models, aiming to gain more precise control over the out- put images. For instance, GLIGEN retrain the model with the layout-annotated dataset by the Gated Self-Attention. DenseDif- fusion  and BoxDiff exemplify training-free L2I methods that necessitate supplementary inputs for operation. However, the requirement for user-specific layouts can be time-consuming and inconvenient for users. In addition, especially in scenarios involv- ing HOI, simplistic inputs like boxes prove inadequate in capturing complex attributes such as posture and body orientation, which are crucial for the accurate depiction of interaction, ultimately resulting in suboptimal images.  On the other hand, alternative approaches have integrated Large Language Models (LLMs) to augment diffusion models, aiming better to grasp the nuances of textual prompts in image gener- ation . Innovations such as LMD  and LayoutLLM-T2I  have pioneered to employ LLMs for creating more intuitive and accurate image outputs. LMD utilizes a dual-phase approach, initially using a pre-trained LLM to create a scene layout with captioned bounding boxes. It further proceeds with a layout-grounded controller to guide diffusion models. Addition- ally, LayoutLLM-T2I starts by generating a coarse layout and then implements a specially trained transformer module within the de- noising UNet for fine-grained generations. Despite the progress made with LLM-assisted methods in image generation, a pivotal limitation arises when handling HOI. Predominantly dependent on textual prompts, these methods exhibit two possible shortcom- ings. Firstly, they may overlook the intricate spatial dynamics and nuanced interactions within an image due to the limited informa- tion the prompt provides. Secondly, in some cases, the LLMs may over-analyze the textual prompt, leading to hallucinations where they generate fabricated content that is not grounded in reality. An evident illustration of this phenomenon is presented in the first- row of Figure 1. Despite the prompt suggesting a man carrying a bicycle, the LLM-assisted LayoutLLM-T2I tends to overanalyze and assume an additional man riding the bike instead, resulting in the generation of human-like artifacts riding the bicycle, which is not the intended interaction. Such deficiency underscores the critical need for advanced approaches to interpret information from the concise HOI prompts better, ensuring image generation that closely aligns with the intended interactions. In this paper, we propose Reasoning and Correcting Diffusion (ReCorD) for generating HOI. We argue that proper HOI genera- tion requires both the correct human posture and precise object positioning to facilitate realistic interaction. To achieve this, we in- troduce an innovative pipeline depicted in Figure 2. Our approach includes three key steps. First, we employ the Latent Diffusion Model (LDM) to produce a set of human candidates performing the verb in the text prompt, emphasizing the correct posture by employ- ing intransitive prompts. Next, as Visual Language Models (VLMs) excel at comprehending image contents, we harness their visual reasoning capabilities to select candidates with optimal posture and determine the appropriate placement of the object based on their interpretation of the interaction scene. Finally, we introduce a refinement mechanism to adjust object positions while preserv- ing accurate human posture. By implementing inverse attention masks and bounding box constraints, we prevent the overlap of attention maps between humans and objects during image gen- eration, enhancing the fidelity of the final image. Our proposed ReCorD guarantees precise control over the depicted interactions, effectively mitigating the risks of hallucinations. This work pio- neers training-free interaction generation, eliminating the need for extra labeled data and bypassing training-related computational challenges.", "subsections": [{"title": "Contributions", "content": "We summarize our main contributions as follows:\n(1) We introduce a novel reasoning framework that integrates\nLDM with VLMs to overcome the challenges of generating\nrealistic HOI, mitigating issues presented in previous ap-\nproaches, such as LLMs overanalyzing simple text prompts\nand training data biases in LDM.\n(2) To enhance human figure depiction accuracy, we design a\ncorrection mechanism within LDM for dynamic image ad-\njustment, enabling precise control and refinement of human\ninteractions in generated images as well as enhancing the\nportrayal accuracy significantly.\n(3) The extensive experiments demonstrate our training-free\nReCorD's proficiency in creating captivating and realistic\nHOI scenes, outperforming state-of-the-art techniques."}]}, {"title": "2 Related Work", "content": null, "subsections": [{"title": "2.1 Conditioned T2I Diffusion Models", "content": "Recent advancements in diffusion models have significantly improved the capabilities of large-scale T2I gen- eration models such as DALL-E , Imagen , and Stable Diffusion . While these models guided solely by plain text demonstrate the ability to generate high-quality images, they strug- gle with prompts that demand detailed attribute specifications and a nuanced understanding of spatial relationships . Consequently, recent research on diffusion models extended beyond text-based conditions and incorporated advanced conditioning mechanisms such as inpainting masks , sketches , key points , seg- mentation maps and layouts , facilitating enhanced spa- tial manipulations. The modification of models marks this evolu- tion to include additional encoders, achieved through strategies like fine-tuning , or constructing new models from scratch . For instance, SpaText and GLIGEN introduce spatial modulations into pre-trained models, employing fine-tuning with adapters to perform layout constraints. Despite the enhance- ments, the requirement for model retraining for each new condition type remains a significant challenge. Building on the foundation of the previous diffusion models, we propose a training-free frame- work designed explicitly to enhance the proficiency of LDM in interpreting and visualizing the intricate relationships of HOIs."}, {"title": "2.2 Image Generation with Spatial Control", "content": "As T2I models are traditionally trained on datasets featuring brief text captions, they frequently struggle to capture the nuances of more complex captions that contain multiple phrases . Follow- ing the insight from Prompt-to-Prompt  that trained T2I models inherently provide token-region associations through their atten- tion maps, several works have been proposed to mitigate this issue. For instance, MultiDiffusion choose to conduct individual denoising procedures for every phrase, respec- tively, at every timestep. However, this independent generation technique frequently stumbles over lifelike compositions and is easily hindered by bias toward specific actions or objects. Attend- and-Excite strategically manipulates the noise map to enhance the activation of previously overlooked tokens in cross-attention maps. Yet, a limitation arises since the mere intensification of at- tention to certain tokens does not always lead to a holistic repre- sentation of the intended information within the generated output. BoxDiff comes up with three spatial constraints to optimize the cross-attention layers given instinctive inputs, e.g., bounding box or scribble, during the sampling process. InteractDiffusion introduces a novel approach to generating images with precise HOI by tokenizing interaction using a conditioning self-attention layer for the accuracy of the complexities of interaction represen- tation. To better impose regulations on the generated human, the rectangular-shaped constraints cannot be applied since bounding boxes cannot properly convey the details of human action, such as body orientation, facial direction, etc. Building on these training- free approaches, our pipeline assures that the object is matched with a sufficiently strong attention map and the human can be generated in a legitimate pose. Thus, a complex scene containing interaction can be precisely rendered."}, {"title": "2.3 LLM-assisted Image Generation", "content": "The integration of Large Language Models (LLMs) with diffusion models has significantly transformed T2I generation, capitalizing on the superior generalization abilities of LLMs . LayoutGPT  adopt LLMs for layout generation via in-context learning. VisorGPT takes one step further by fine-tuning to em- brace diverse modalities, including key points, semantic masks, etc. LMD represents pioneering efforts in this integration, utilizing LLMs to interpret object locations from text prompts, thus enhanc- ing the accuracy and quality of generated images. LayoutLLM- T2I queries ChatGPT for text-to-layout induction and intro- duces a Layout-aware Spatial transformer with a view to improve layout and image generation simultaneously. Emphasizing the cen- tral role of LLMs, these advancements bypass the need for addi- tional information inputs, allowing LLMs to shape the initial layout configurations and interpret user prompts directly. Despite the suc- cessful outcomes of using LLMs, they fail to address the challenge of generating the explicit posture of humans given specific actions. Compared to LLMs, LDMs demonstrate a better understanding of scene kinetics based on simple text prompts. Therefore, we propose to leverage such advantage of LDMs alongside the robust visual reasoning abilities of VLMs to generate accurate HOI."}]}, {"title": "3 Method", "content": "We introduce the ReCorD, an interaction-aware model that main- tains training-free superiority. The resulting images hinge on the human adopting the appropriate pose and ensure the object is lo- cated in a suitable position according to the given text prompt. Our generative pipeline comprises three modules: Coarse Candidates Generation, Interaction-aware Reasoning, and Interaction Correct- ing. To abbreviate these modules, we term them as $M_g$, $M_r$, and $M_c$, respectively. We decomposed the denoising process $T$ into two stages, i.e. $T_1$ and $T_2$, by observing that the diffusion model captures the initial layout during the early denoising steps and refines the de- tails in the later iterations . In the former stage, $M_g$ generates $k$ coarse candidates, while $M_r$ suggests the ideal pose and layout w.r.t. the text prompt. Subsequently, $M_c$ corrects object locations while preserving selected poses to refine the cursory images into desired ones. Importantly, ReCorD empowers the diffusion model to create images aligned with text prompts, highlighting complex spatial conditions and intricate interactions without additional training. The overall pipeline is depicted in Figure 2, and we elaborate on the details of each module in the following sections.", "subsections": [{"title": "3.1 Coarse Candidates Generation Module", "content": "Given a text prompt $y$ describing a HOI triplet (i.e., \"a subject is verbing an object\"), we enhance interaction representation by adopt- ing distinct attention mechanisms within $M_g$. More precisely, we manipulate cross-attention and self-attention maps to generate candidate images $k$ associated with the action subject to the prompt. Cross-Attention Maps Manipulation. To facilitate image gen- eration concerning the textual information, we incorporate such conditions into LDMs using cross-attention maps. During denoising, LDMs initially sample a latent vector $z_t$ from a Gaussian distribu- tion $N(0, 1)$ and progressively remove noise to obtain $z_{t-1}$ at each step $t\\in[T_1,..., 0]$. After encoding the prompt $y$ into text tokens via text encoder, the cross-attention map is defined as follows:\n$A = \\text{Softmax}(QK^T / \\sqrt{d}) = \\text{Softmax}(Q K^T/\\sqrt{d}),$ (1)\nwhere $Q = \\phi_q(\\varphi(z))$ and $K = \\phi_k(\\psi(y))$ represent the query and key embeddings derived by corresponding projection functions. $\\varphi$ and $\\psi$ are spatial normalization  and the text encoder from CLIP  yielding intermediate representations and $N$ text tokens $(y) = \\{w_1 ... w_N\\}$. For simplicity, we omit the subscript $t$ that represents the denoising step while manipulating attention maps. Modeling the cross-attention map for the interaction (verbing in $y$) is challenging when generating HOI scenes, leading to an ambiguous representation of the verb token. To address this issue, we propose an alternative intransitive prompt $\\tilde{y}$, which typically excludes object-related descriptions in $y$. The cross-attention maps $\\tilde{A}$ using $\\tilde{y}$ can be derived by substituting $K$ in eq. (1) with $K = \\phi_k(\\psi(\\tilde{y}))$. As illustrated in Figure 3, $\\tilde{A}$ captures more informative clues, especially for the verb token, compared to $A$ when using $y$, resulting in interactive representations. Accordingly, we formulate the final cross-attention maps by rearranging them as follows:\n$A_{\\text{cross}} = \\begin{cases}  \\tilde{A}_n & \\text{if } w_n \\in \\varphi(\\tilde{y}) \\\\ A_n & \\text{otherwise}, \\end{cases}$ (2)\nwhere $n$ denotes the index of text tokens. Ideally, we embrace the attention maps if the text token exists in the intransitive prompt. Self-Attention Maps Manipulation. In contrast to cross-attention maps, self-attention maps lack direct token associations but still in- fluence the spatial layout and appearance of generated images . Therefore, we manipulate the self-attention maps similarly to eq. (2) for latent representation once the denoising step $t > y$ for obtaining $A_{\\text{self}}$, where $\\gamma$ is a predefined parameter ensuring that scenes and objects from original tokens $(y)$ can be generated effectively."}, {"title": "3.2 Interaction-Aware Reasoning Module", "content": "As an intermediate module bridging the others, we present the Interaction-Aware Reasoning module $M_r$ (see Figure 4) following the generation of coarse candidates in $M_g$. This module comprises two components powered by VLM: the Pose Selection Agent and the Layout Agent. Specifically, the Pose Selection Agent selects an image aligning with the prompt $y$, while the Layout Agent adjusts the object's location and preserves human key points $P$ and further determines the target position $b_o$ for the correction module $M_c$. Pose Selection Agent. As the pose plays a vital characteristic in the HOI generation, we first couple an agent to select the appro- priate pose conditioned on the prompt. The Pose Selection Agent integrates the initial prompt $y$ with previously generated candidates to create the pose template. Leveraging the visual comprehension capabilities of VLMs, this agent excels in identifying the precise pose corresponding to $y$, enhancing the model's ability to inter- pret visual data beyond relying solely on textual cognition as in LLMs. This pivotal step ensures that the pose information initially obtained from LDMs is meticulously refined for subsequent phases. Layout Agent. To address the issue of LLM-assisted methods being overly reliant on prompts for sampling layouts, we incorporate the identified key points $P$ and the bounding box $b_h$ for humans as additional data. Recognizing that an interaction involves both the relation to human and the object, we collect the crucial information of $P$ using 33 key points in (x, y) coordinates and represent $b_h = (x_{min}, y_{min}, x_{max}, y_{max})$. Additionally, we use the image selected by the former agent as inputs to VLMs for layout suggestion tasks. We first extract the object's location $b_o$ using Otsu's algorithm , an automatic thresholding technique applied to the object's cross- attention map $A_{\\text{cross}}$ to isolate regions with higher values. Subse- quently, we detect human key points using MediaPipe Pose Land- marker to create the segmentation mask $m_p$. Consecutively, we establish a series of guidelines and fixed protocols for VLMs adher- ence, including constraints on $b_h$ to maintain the integrity of the intended human poses and an overlapping reduction strategy to improve the quality of generated images containing multiple ob- jects. Furthermore, inspired by the Chain-of-Thought approach , we enhance the logical coherence by guiding VLMs to construct visual attribute information for human posture. We ground the VLMs in logical reasoning across multiple Visual Attributes such as pose types, body orientation, object relations, etc. Drawing on insights from previous research , we prepare VLMs with three examples, aiding in the clarification of visual representation and preventing over-analysis and hallucinations to construct the interaction template. Eventually, we extract the proposed location $b_o$ for the box-constrained loss eq. (4) by text scraping and integrate a checking mechanism to determine whether the alteration in $b_o$ falls within a predetermined threshold. If the change is minimal which indicates a minor difference, $M_r$ would signal no changes to $M_c$. This mechanism is crucial for maintaining a streamlined and resource-efficient generation process, ensuring only significant location adjustments prompt further action."}, {"title": "3.3 Interaction Correcting Module", "content": "We delicately refine the candidate image provided by the dual agents while preserving the original human pose in $M_c$, as shown in Fig- ure 5. To combine the generative capabilities of LDMs with the reasoning abilities of VLMs, we incrementally update the latent $z_t$ to adjust the object's position and size based on bounding boxes $b_o$ related to the interaction. Notably, we conduct the denoising process for $t \\in [T_2,..., 0]$, including modulation of cross-attention and self-attention maps, as described in Section 3.1. Simultaneously modifying the object's location requires con- sideration of potential overlap with the human body since cross- attention maps from different tokens may exhibit strong values in the same region, which will deteriorate image quality. To address this challenge, we introduce a mechanism for eliminating attention overlap. Specifically, given the token index of the object denoted as $m$, we use the cross-attention map $A_m$ to construct an inverse mask at each time step $t$, denoted as $\\bar{A}_m = 1 - A_m$, where 1 is the tensor of the same dimension as $A_m$ containing all elements equal to 1. This inverse attention map is then applied to the remaining maps using an element-wise product operation defined as\n$\\hat{A}_n = \\bar{A}_m \\cdot A_n, \\forall n \\neq m.$ (3)\nThrough eq. (3), we can mitigate the issue of attention overlap between humans and objects while updating the object positions, ensuring the successful generation of updated objects. Conditioned Spatial Constraints Since our ReCorD is training- free and does not involve additional learnable networks for knowl- edge transfer, we employ box constraints to regularize the denoiser, which can be formulated as\n$L = L_{IB} + L_{OB} + L_{CC},$ (4)\nwhere each term in sequence order represents the inner-box, outer- box, and corner constraint, respectively. We apply eq. (4) to update the latent at each time step $t$ with corresponding weight $\\alpha_t$ as\n$z_t \\leftarrow z_t + \\alpha_t \\nabla L.$ (5)\nWith slight updates to $z_t$ at each step, we ensure the object holds sufficient mutual information with the box region and con- forms to the specified size, i.e. $b_o$, thereby accurately correcting the object's position to represent the interaction. As LDM aims to denoise iteratively and involve the attention maps as intermedi- ate, LDM($z_t, y, t, s$) is the diffusion process at time step $t$ before manipulation, which seeks the corresponding attention maps. Af- ter going through our proposed ReCorD, the denoising UNet is reused to predict the latent representation at the next step. For- mally, LDM($z_t, y, t, s$) denotes the diffusion model that adopts the manipulated attention maps, resulting in the prediction, i.e. $z_{t-1}$."}]}, {"title": "4 Experiments", "content": null, "subsections": [{"title": "4.1 Experimental Setup", "content": "Datasets. Given the absence of a standard benchmark crafted for HOI generation, we assess the efficacy of our approach by extracting HOI triplets from two established HOI detection datasets, namely HICO-DET  and VCOCO , to form the input text prompts. HICO-DET includes 600 triplets across 80 object categories and 117 verb classes, while VCOCO contains 228 triplets, spanning 80 object classes and 29 verb types. For a comprehensive assess- ment, we incorporate the non-spatial relationship category in T2I- CompBench , which is characterized by 875 interaction terms. We select prompts that exclusively involve HOIs in T2I-CompBench. To enhance diversity, we apply random subject augmentation to each verb and object pair extracted from the datasets to form the input prompt. Accordingly, our experiments are conducted across three datasets: HICO-DET, with 7,650 HOI prompts; VCOCO, con- tributing 2,550 prompts; and the non-spatial relationship category of T2I-CompBench, adding 465 prompts. Baselines. We report comparisons with nine strong-performing models, 1) T2I models: Stable Diffusion (SD) , Attend-and-Excite (A&E) , SDXL , and DALL-E 3. 2) L2I models: BoxDiff , MultiDiffusion  and InteractDiffusion . 3) LLM-assisted T2I models: LayoutLLM-T2I  and LMD . We utilized the official implementations and the default settings for each baseline. For L2I models, we provide the actual bounding box data from HICO-DET and VCOCO datasets in addition to the text prompts as inputs. For LLM-assisted methods, the input layouts are exclusively generated by LLMs, rather than being sourced from the datasets. Evaluation Metrics. To measure the interaction in the generated images, we utilize the CLIP-Score $S_{\\text{CLIP}}$ evaluating the similar- ity between the input text and the generated images. While this met- ric is commonly applied to estimate fidelity to text prompts, we note its inclination towards a noun or object bias, with CLIP often unable to differentiate among verbs, relying instead on nouns . To address this, we specifically extract verbs from the text prompts and calculate the Verb CLIP-Score $S_{\\text{verb}}$. In addition, we introduce"}, {"title": "4.4 Generation Speed and Memory Usage", "content": "For generating one image, we use an Nvidia RTX 6000 GPU, and the memory costs are 14/42 GB when using SD/SDXL as the backbone, with total inference times of 40.66/61.48 seconds."}, {"title": "4.5 Comparing MLLMs for Layout Suggestions", "content": "We evaluate BLIP-2  by randomly adjusting the size and position of ground truth bounding boxes in HICO-DET. However, BLIP-2 often misinterprets real-world distributions, providing irrelevant answers and invalid mIoU scores. Conversely, GPT-4V achieves a mIoU score of 49.72%, demonstrating superior layout suggestion accuracy, making it highly suitable for our ReCorD."}, {"title": "4.6 GPT-4V for Evaluation", "content": "Following T2I-CompBench for non-spatial relationship evaluation, our ReCorD achieves a GPT score of 98.16, outperforming SOTA T2I methods like SDXL (97.87), MultiDiffusion (97.43), and LayoutLLM- T2I (96.75). This demonstrates ReCorD's ability to generate accurate HOI images aligned with foundation model knowledge."}, {"title": "4.7 Ablation Studies", "content": "For the ablation studies, Figure 7 displays the HOI generation results of the SD incorporating modules in our ReCorD: (a) with only SD, (b) with SD and $M_g$, and (c) with SD and $M_g + M_r + M_c$. With only SD, the generated outputs appear to be suboptimal, likely influenced by biases inherent in its training data, leading to misinterpretation of the intended interaction described in the text prompts. From the results in (b), it shows that with the inclusion of $M_g$, the accuracy of the generated human action is significantly improved due to our intransitive prompt altering technique, highlighting that sim- plifying the prompt to focus on the core action enables the model to generate the intended human poses with enhanced precision. However, it still struggles to accurately position objects in relation to humans in the images, resulting in a mistaken interaction. For our ReCorD, $M_r$ helps select appropriate poses and retain suitable candidates while $M_c$ refines images to obtain accurate interaction with the correct object size and position while maintaining the selected pose. As a result, HOI generation of our complete pipeline in (c) exemplifies the most successful outcomes achieved."}]}, {"title": "5 Conclusion", "content": "We have introduced ReCorD framework, tailored explicitly for HOI image generation. This method comprises three interaction-specific modules that synergistically interact with each other. Our core idea revolves around reasoning layout and correcting attention maps using VLM-based agents and an LDM to address this challenge. Ex- tensive experiments demonstrate the effectiveness of our approach in enhancing image accuracy and semantic fidelity to the input text prompts, particularly in capturing intricate concepts of interactions that several baseline generative models struggle with. Additionally, we quantify our improvements through various protocols and a user survey focused on HOI generation, providing valuable insights and paving the way for future explorations in this domain."}]}