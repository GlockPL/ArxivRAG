{"title": "Empowering the Deaf and Hard of Hearing Community: Enhancing Video Captions Using Large Language Models", "authors": ["Nadeen Fathallah", "Monika Bhole", "Steffen Staab"], "abstract": "In today's digital age, video content is prevalent, serving as a primary source of information, education, and entertainment. However, the Deaf and Hard of Hearing (DHH) community often faces significant challenges in accessing video content due to the inadequacy of automatic speech recognition (ASR) systems in providing accurate and reliable captions. This paper addresses the urgent need to improve video caption quality by leveraging Large Language Models (LLMs). We present a comprehensive study that explores the integration of LLMs to enhance the accuracy and context-awareness of captions generated by ASR systems. Our methodology involves a novel pipeline that corrects ASR-generated captions using advanced LLMs. It explicitly focuses on models like GPT-3.5 and Llama2-13B due to their robust performance in language comprehension and generation tasks. We introduce a dataset representative of real-world challenges the DHH community faces to evaluate our proposed pipeline. Our results indicate that LLM-enhanced captions significantly improve accuracy, as evidenced by a notably lower Word Error Rate (WER) achieved by ChatGPT-3.5 (WER: 9.75%) compared to the original ASR captions (WER: 23.07%), ChatGPT-3.5 shows an approximate 57.72% improvement in WER compared to the original ASR captions.", "sections": [{"title": "1 Introduction", "content": "The Deaf and Hard of Hearing (DHH) community represents a significant portion of the global population. According to the World Health Organization (WHO), over 5% of the world's population - or 430 million people - are estimated to have some degree of hearing loss and require rehabilitation services for disabling hearing loss (including 34 million children). The prevalence of hearing impairment is expected to rise due to factors such as population growth, aging demographics, and increased exposure to harmful noise levels. In particular, it is projected that by 2050, nearly 700 million people will require some form of hearing assistance [36]. The challenges faced by the DHH community in accessing multimedia content are substantial. Many individuals in this group rely on captions as a primary means of accessing auditory information. However, the quality of captions often falls short of the accuracy required for complete comprehension, significantly affecting the community's ability to fully participate in educational, social, and professional settings. The need to bridge this gap is critical, as video content is increasingly used for education, communication, and entertainment, making accessible captioning an essential aspect of ensuring inclusivity for all. Although video captions have enhanced accessibility to some extent, there remains a considerable deficit in their precision, user-friendliness, and overall effectiveness. Our research initiative explores the challenges related to the DHH community's use of video captions and improving the quality of captions generated by assistive tools. Video content is a primary education, communication, and entertainment medium, facilitating knowledge sharing and communication. Barriers to video content accessibility arise when it has inaccurate captions. Incomplete or inaccurate captions pose a significant challenge for the DHH community, who rely on these captions to access auditory information. Accurate captions are essential to ensure that video content is inclusive and accessible to all individuals, regardless of hearing ability. Traditional video captioning"}, {"title": "RQ. Can Large Language Models (LLMs) be used to enhance the quality of video captions for the DHH community?", "content": "Our research question is:\nRQ. Can Large Language Models (LLMs) be used to enhance the quality of video captions for the DHH community?\nThe primary contributions of this research project are:"}, {"title": "2 Related work", "content": "The DHH community relies on captioning solutions such as ASR systems to access auditory information. This section presents current ASR systems for video captioning, challenges in ASR systems for video captioning, and why LLMs are suitable to address those challenges."}, {"title": "2.1 ASR Systems for Video Captioning", "content": "Automatic Speech Recognition (ASR) is a technology that uses Machine Learning/Artificial Intelligence to convert human speech into text. It is crucial for the DHH community as it provides a cost-effective, real-time solution for accessing spoken content, enhancing communication, and enabling participation in various activities. Captioning with ASR systems has seen significant advancements in generating high-quality captions, making them a cost-effective alternative to human captioning services [8], [24]. Deep learning, particularly end-to-end models like Connectionist Temporal Classification (CTC) and Sequence-to-Sequence (Seq2Seq) architectures, has notably increased transcription accuracy [10], [5]. Transformer-based models improve performance by handling long range dependencies in audio sequences [34]. Notable implementations that provide a cost-efficient solution for the DHH community in educational settings include the tabletop tool [33] to facilitate DHH users' communication, the APEINTA system [13] to generate captions on multiple platforms, and the E-Scribe's web-based solution [4]. Despite these advancements, challenges in ASR, such as accuracy in diverse environments, ambient noise, and contextual understanding, remain [18]."}, {"title": "2.2 Challenges in Video Captioning", "content": "Despite improvements in ASR technology, the suitability of automatic captions for the DHH community remains a contentious issue. Even minor inaccuracies can significantly impact comprehension, particularly for individuals who rely solely on captions to access auditory information. Many members of the DHH community consider automatic captions to be only a starting point rather than a complete solution. For instance, a seemingly small error rate-such as 0. 1%-could lead to the misinterpretation of critical content, particularly when it involves homophones, specialized terminology, or idiomatic expressions. The reliance on ASR alone without further correction can, therefore, result in incomplete or confusing information. This concern highlights the importance of"}, {"title": "2.3 LLMs for Improving Video Captions Quality", "content": "Large Language Models (LLMs) are machine learning models that can comprehend and generate human language text [14]. Earlier studies have typically utilized models like BERT to enhance ASR through methods like rescoring and distillation. Rescoring involves improving the quality of generated text by re-evaluating multiple possible outputs produced by an ASR model. For instance, ASR systems often generate an n-best list-a set of multiple possible transcriptions ranked by likelihood\u2014and a rescoring model can be used to select the most accurate one. For example, [30] applied BERT for n-best list rescoring, while [37] used Minimum Word Error Rate (MWER) loss to train a BERT-based rescoring model that minimizes errors across the top-ranked transcriptions. Distillation, on the other hand, is a training approach where a smaller, less complex model (the \"student\") is trained using the knowledge transferred from a larger, more complex model (the \"teacher\"). This is done by using the teacher's outputs as \"soft labels\" to provide more informative learning signals. In [9], BERT was employed in a distillation framework to create such soft labels for training ASR models, effectively teaching the student model nuanced semantic representations. Additionally, [17] explored how to transfer the semantic knowledge embedded in large models, improving ASR performance by enriching the distilled information. Although these techniques demonstrated the effectiveness of LLMs in improving ASR outputs, they were limited to older LLMs with comparatively less advanced language capabilities."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Dataset", "content": ""}, {"title": "3.1.1 Dataset Collection.", "content": "While existing datasets like LibriSpeech [23], TED-LIUM [11], and Common Voice [2] offer valuable resources for improving ASR system-generated captions, they are not entirely suitable for our task due to their limitations in domain diversity and capturing specific captioning challenges. These datasets often lack the varied contexts and errors in real-world video captions. To address this, we generated our open-domain dataset, comprising videos from diverse domains such as education, cooking, travel and tourism, entertainment, and news. The duration of collected videos ranges from 1-3 minutes."}, {"title": "3.1.2 Dataset Annotation.", "content": "Numerous studies have considered ASR an assistive tool for DHH to access auditory information [16], [15]. However, inaccurate captions are a significant limitation of ASR. After collecting 52 videos from diverse domains, we utilized the YouTubeTranscriptApi [25] to retrieve the ASR-generated captions from YouTube. Thus, the assistive tool under investigation in this study is YouTube's ASR automatic captioning feature, and the retrieved captions serve as the input to our proposed model. To evaluate our model's caption correction performance, we manually generated the ground truth captions for these videos, enabling us to compare the corrected captions produced by our model to the actual words spoken in the videos. Details about the dataset variables are presented in Table 2."}, {"title": "3.2 Model Selection", "content": "This study aims to integrate LLMs with assistive tools commonly used by DHH students to generate and correct automatic captions. The captions produced by these assistive tools serve as inputs for the LLMs, after that we prompt the LLM to correct any"}, {"title": "3.3 Pipeline for improving video caption quality using LLMs", "content": "To improve the quality of ASR system-generated captions, we propose a pipeline that leverages LLMs' advanced capabilities, including contextual understanding, text generation, text correction, and semantic coherence. Our proposed pipeline is illustrated in figure 3.\nInput: Our dataset of video content covers various domains like education, cooking, travel and tourism, entertainment (such as movies), and news, collected from YouTube. Videos range is between 1 and 3 minutes. We chose YouTube's automatic caption generation feature as the assistive tool under investigation to generate captions. The input to the LLM is the captions generated by YouTube's automatic caption generation feature as text.\nImproving captions with LLMs: We prompt LLMs to correct input inaccurate captions without altering the word sequence. To construct the prompts, we use zero-shot prompting, which refers to the model's ability to perform a task without any prior examples or specific training on that task, based solely on the instructions given in the prompt [29], with their vast pre-existing knowledge, LLMs can generate accurate and contextually appropriate captions without needing additional task-specific information or training. We conducted experiments with two LLMs, ChatGPT-3.5 and Llama213B, based on their performance, as discussed in 3.2.\nOutput: The output generated by LLM is the corrected captions. The output generated by LLM is then compared with manually created ground truth. The generated captions are more accurate and contextually relevant than the initial captions.\nImplementation: We implemented our project using Python within a Jupyter Notebook environment. Link to GitHub project repository:\nhttps://github.com/monikabhole001/Improving-the-Quality-of-Video-Captions-for-the-DHH-Community-Using-LLM"}, {"title": "4 Evaluation", "content": "The final step is the evaluation of our caption correction pipeline. We employ two approaches for evaluation: how the selected LLMs (Llama2-13B and ChatGPT-3.5) perform on different challenges in captions, such as homophones, inaccurate words, and domain-specific terminology, and the use of quantitative metrics for a more objective assessment of caption accuracy and quality."}, {"title": "4.1 LLM Performance on Captioning Challenges", "content": "Prompt: \"Correct the caption according to English standards. Don't change the word sequence\".\n\u2022\tHomophones and inaccurate words:\nInput caption: \"I was walkng in the son. The day was bright and pleasant because of son y weather.\"\nChatGPT output caption: \"I was walking in the sun. The day was bright and pleasant because of sunny weather.\"\nLlama2-13B output caption: \"I was walking in the sun. The day was bright and pleasant because of sunny weather.\"\n\u2022\tDomain-specific terminology:\nInput caption: \"Today, we'll explore advanced concepts in Sea Plus Plus programming, focusing on poly more fizz 'em.\""}, {"title": "4.2 Quantitative evaluation metrics", "content": "We employed three standard metrics to measure the performance of our caption correction pipeline: Word Error Rate, BLEU Score, and ROUGE Score. These metrics are essential for understanding how closely the generated captions match the ground truth captions.\nTo clarify the terms used in our evaluation, here are the definitions:\n\u2022\tGenerated caption: Automatically generated caption by an assistive tool (e.g., YouTube's ASR system).\n\u2022\tGround truth caption: The correct caption of the video, which is generated manually.\n\u2022\tPredicted caption: This is the caption generated by LLM (Corrected caption)."}, {"title": "Word Error Rate (WER)", "content": "Word Error Rate (WER) [1]: Measures the percentage of errors in the predicted caption compared to the reference caption. It accounts for incorrect, omitted, or inserted words, calculated using the formula in Equation 1. A lower WER indicates a higher accuracy of the predicted captions, and a WER of 0% indicates a perfect match.\n$WER = \\frac{S+D+I}{N}$  (1)\n\u2022\tSubstitutions ( S): Incorrect words in the predicted caption.\n\u2022\tDeletions (D): Missing words in the predicted caption present in the ground truth caption.\n\u2022\tInsertions (I): Extra words in the predicted caption, not in the ground truth caption.\n\u2022\tN: Total number of words in the ground truth caption."}, {"title": "Bilingual Evaluation Understudy (BLEU)", "content": "Bilingual Evaluation Understudy (BLEU) [20]: Calculates the precision of n -grams; n-grams are consecutive sequences of n words, measuring the percentage of n-grams in the predicted caption that matches the ground truth caption. It evaluates how well the predicted caption captures the exact word sequences of the ground truth caption. Commonly used n-grams include unigrams (single words), bigrams (two-word sequences), and trigrams (three-word sequences). The final BLEU score is calculated by aggregating the precision of unigrams, bigrams, trigrams, and four-grams, balancing the contributions of shorter and longer sequences. The BLEU score ranges from o to 1, with higher values indicating better quality."}, {"title": "Recall-Oriented Understudy for Gisting Evaluation (ROUGE)", "content": "Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [20]: Measures overlapping n-grams between predicted caption and ground truth caption. The ROUGE score determines the recall of n-grams, ensuring the LLM-predicted caption includes essential content from the ground truth caption. The score ranges from o to 1, with higher values indicating better quality. ROUGE scores are categorized as follows:\n\u2022\tROUGE-N: Measures overlap of n-grams.\n\u2022\tROUGE-L: Measures the longest common subsequence (LCS).\nWER measures word-level errors, which do not account for the overall meaning and context of the captions; it severely penalizes minor errors that do not change the sentence's meaning. BLEU and ROUGE help address this limitation by evaluating the precision and recall of word sequences. BLEU measures the precision of n-grams, capturing the overlap of words between the predicted and ground truth caption. However, it can struggle with longer sequences and recall. ROUGE emphasizes recall and the longest common subsequence, making it better suited for assessing the overall structure and meaning. Together, these metrics ensure that captions are not only error-free but also accurate and comprehensive, capturing both lexical and semantic similarities."}, {"title": "5 Results", "content": "Leveraging LLMs to improve the quality of ASR video captioning tools has shown to be a promising approach. In our evaluation, we tested two different LLMs, ChatGPT-3.5 and Llama2-13B, using our dataset of 52 videos. The experiments were conducted using Google Colab, specifically utilizing its Python notebook environment. The models' performances were assessed using standard natural language processing metrics: WER, BLEU, and ROUGE scores, as detailed in Table 4.\nOur results demonstrate that ChatGPT-3.5 (WER: 9.75%) shows a significant improvement over the original Youtube-ASR-Caption system (WER: 23.07%), highlighting its superior accuracy in reducing errors per word. Additionally, the BLEU score for ChatGPT-3.5 is notably high at 0.85 compared to the original Youtube-ASR-Caption system BLEU score of 0.67, suggesting superior precision in n-gram matching and, thus, higher accuracy in caption generation. ChatGPT-3.5 achieved a high ROUGE-1 score of 0.98, similar to the Youtube-ASR-Caption system, indicating a strong recall of unigrams (single words). Moreover, ChatGPT-3.5 slightly outperforms the ROUGE-2 (0.97) Youtube-ASR-Caption system."}, {"title": "6 Conclusion", "content": "In conclusion, integrating LLMs into caption correction systems can significantly enhance the accuracy and coherence of captions, improving accessibility for the DHH community. Our evaluation shows that ChatGPT-3.5 outperforms Llama2-13B regarding caption quality, capturing both individual words and longer sequences effectively and ensuring the generated captions are detailed and contextually accurate.\nHowever, a limitation of this project is that LLMs might miss or misinterpret voice intonations, cultural references, and idioms that manual captioning can understand and convey accurately. For instance, a human captioner can appropriately interpret the emphasis in \"Really?!\" to express surprise, while an LLM might transcribe it as \"Really\". Similarly, cultural references like \"Diwali\" (the Hindu festival of lights) or phrases like \"namaste\" (a traditional Hindu greeting) could be misunderstood by LLMs but accurately captured by human captioners. To address these limitations, we propose employing multi-modal LLMs and incorporating ML models designed to understand complex human communication nuances."}, {"title": "7 Future Work", "content": "Future research and development efforts can enhance the effectiveness of video captioning through several key areas. Expanding our current dataset beyond YouTube ASR system-generated captions to include other widely used platforms like Microsoft Teams, and Zoom would increase the applicability of our solutions to the DHH community.\nIn addition, considering the potential scalability of our approach is crucial for maximizing its impact. Investigating how well the LLM-based captioning system can handle larger datasets and diverse input types will be essential for scaling up the solution to different environments and applications. Exploring the deployment of this solution in low-resource settings or environments with limited computational capabilities is a necessary step to ensure accessibility across a broad range of users. Optimization of model efficiency, such as using quantization techniques or lighter model variants, could further facilitate the scalability of this approach to a production-level tool.\nThe applicability to other assistive technologies is another avenue worth exploring. Beyond video captioning, the methods developed here could extend to applications like real-time speech-to-text systems used in classrooms or workplaces, benefiting not only the DHH community but also those with auditory processing disorders or language learners. Integrating LLMs for enhanced subtitle generation in augmented reality (AR) or virtual reality (VR) environments could also pave the way for immersive and accessible experiences, improving real-time communication. Exploring these broader applications will further demonstrate the flexibility and impact of LLM-based solutions in assistive technology.\nAddressing code-switching is another essential area of focus. YouTube's ASR system feature currently handles only one language at a time, which presents a significant limitation. Existing codeswitching ASR datasets such as [7], [22], [21] handle switching between two languages only, which is not suitable for evaluating LLM performance on audio containing more than two languages. Curating a dataset to address this challenge requires using tools such as Whisper by OpenAI [26], designed to handle multiple languages within a single audio stream. We conducted some preliminary experiments to evaluate LLMs' performance for the code-switching challenge; a sample of the experiments is shown in figure 4. Our results show that ChatGPT-3.5 and Llama2-13B can successfully handle mixed-language inputs and produce accurate English captions. Furthermore, experimenting with more advanced prompt engineering techniques, such as context-aware prompting. Context-aware prompting is a prompt engineering technique that includes relevant information in the prompt to improve LLM responses. Contextual information includes but is not limited to the domain of the video and its metadata.\nLastly, user experiments on the pipeline will be conducted with the DHH community to gather feedback and make necessary modifications. This feedback can also improve LLM performance through reinforcement learning, ensuring a user-centered solution that effectively meets their needs."}]}