{"title": "Towards Personalized Federated Learning via Comprehensive Knowledge Distillation", "authors": ["Pengju Wang", "Bochao Liu", "Weijia Guo", "Yong Li", "Shiming Ge"], "abstract": "Federated learning is a distributed machine learning paradigm designed to protect data privacy. However, data heterogeneity across various clients results in catastrophic forgetting, where the model rapidly forgets previous knowledge while acquiring new knowledge. To address this challenge, personalized federated learning has emerged to customize a personalized model for each client. However, the inherent limitation of this mechanism is its excessive focus on personalization, potentially hindering the generalization of those models. In this paper, we present a novel personalized federated learning method that uses global and historical models as teachers and the local model as the student to facilitate comprehensive knowledge distillation. The historical model represents the local model from the last round of client training, containing historical personalized knowledge, while the global model represents the aggregated model from the last round of server aggregation, containing global generalized knowledge. By applying knowledge distillation, we effectively transfer global generalized knowledge and historical personalized knowledge to the local model, thus mitigating catastrophic forgetting and enhancing the general performance of personalized models. Extensive experimental results demonstrate the significant advantages of our method.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid evolution of distributed intelligent systems has brought data privacy to the forefront. Federated Learning (FL), a distributed machine learning paradigm, enables collaborative model training through parameter sharing rather than raw data exchange, effectively reducing the risk of exposing sensitive data [1]. By leveraging FL, we can efficiently utilize data from distributed clients to collectively train high-performance models. FL has demonstrated its significant value in a variety of fields, including medical health [2], financial analytics [3], and social network [4]. In traditional FL, multiple clients collaborate to train a global model to achieve an optimal universal solution for all clients. However, the non-independent and identically distributed nature of the data distribution [5], known as data heterogeneity, often causes a decline in the performance of distributed clients and can even lead to catastrophic forgetting. Fig. 1 depicts this phenomenon. It is evident that the data distribution varies in each communication round. During each training round, distributed clients update their local model with the global model. Unfortunately, the post-update local model exhibits a significant performance decline compared to the pre-update local model, indicating the phenomenon of forgetting previously learned knowledge.\nPersonalized Federated Learning (PFL) is an innovative approach to tackle data heterogeneity in FL [6]. It involves the collaborative training of a global model by all clients, after which each client develops a personalized model through personalized strategies, reflecting the distinctive characteris-tics of its local data. However, while existing PFL methods excel in model personalization, they often neglect model generalization. For instance, pFedSD [7] employs knowledge distillation to transfer knowledge from the historical model to the local model for achieving model personalization. Nevertheless, this method may hinder the local model's generalization, as the historical model represents the previous local model and incorporates personalized knowledge. The primary limitation of existing PFL methods lies in their design principles, which overemphasize personalized learning and may lead to model overfitting on individual clients, thereby reducing their adaptability to varied client [8].\nIn order to alleviate catastrophic forgetting and achieve a balance between generalization and personalization, we propose Personalized Federated Learning via Comprehensive Knowledge Distillation (FedCKD). Our method integrates multi-teacher knowledge distillation [9] into FL for comprehensive knowledge transfer. The global model represents the aggregated model from the last round of server aggregation, containing global generalized knowledge, while the historical model represents the local model from the last round of client training, containing historical personalized knowledge. By employing the multi-teacher knowledge distillation, we utilize global and historical models as teachers and the local model as the student. This method effectively transfers both global generalized knowledge and historical personalized knowledge to the local model. Global generalized knowledge enhances model performance, whereas historical personalized knowledge addresses the issue of catastrophic forgetting.\nIn summary, our primary contributions are as follows: (1) We propose a novel PFL method called FedCKD. Through multi-teacher knowledge distillation, our method effectively transfers global generalized knowledge and historical per-sonalized knowledge to the local model, thereby addressing catastrophic forgetting and enhancing model performance. (2) We introduce the annealing mechanism in knowledge distillation that dynamically adjusts the weight factor in the loss function, facilitating a smooth transition of training from knowledge transfer to local learning. This mechanism enhances the model's personalization ability and improves the training process's stability. (3) We validate the superior performance of our method through an extensive series of experiments, surpassing existing state-of-the-art methods."}, {"title": "II. RELATED WORK", "content": "Federated Learning allows multiple clients to collabora-tively train local models without sharing their data, addressing data privacy concerns. However, FL faces the challenge of data heterogeneity, which implies that data distribution across clients can vary significantly, potentially leading to subpar performance of traditional FL methods like FedAvg [1] and FedProx [10]. In response to this challenge, PFL has emerged. By introducing personalized strategies such as parameter decoupling, model regularization, personalized aggregation, or knowledge distillation, PFL aims to enhance the model's ability to learn from the unique data characteristics of each client while ensuring global generality. Through parameter decoupling, FedPer [11] develops a global representation on the server while retaining a unique head for each client, LG-FedAvg [12] merges the benefits of localized represen-tation learning with the training of a unified global model. Through model regularization, pFedMe [13] leverages moreau envelopes as a regulatory loss to streamline local model training. Through personalized aggregation, FedFomo [14] implements first-order model optimization in each client's update phase to refine personalization. Through knowledge distillation, pFedSD [7] facilitates the transfer of personalized knowledge from the previous round of models to improve the current round. Although existing PFL methods have made significant progress, they frequently concentrate solely on enhancing the model personalization for individual clients, overlooking the vital aspect of maintaining model generaliza-tion. Future research should aim to achieve a balance between generalization and personalization, exploring novel strategies to enable models to offer personalized solutions for each client while maintaining adaptability to various clients."}, {"title": "III. METHODOLOGY", "content": "In Traditional FL, training involves a server and n clients. Each client, denoted as $C_k$ $(k = 1, 2, ..., n)$, possesses its own private data $D_k = \\{x_k, y_k\\}$, with $x_k$ denotes the sample and $y_k$ denotes the label. The collective data across all clients is represented as $D = \\cup_{k=1}^{n}D_k$. The goal in FL is to derive a global model $w_g$ that minimizes the overall loss function,\n$$\\min_{w_g} \\sum_{k=1}^{n} \\frac{|D_k|}{|D|} L_k(D_k; w_g),$$\nwhere $L_k(D_k; w_g)$ is the empirical loss for client $C_k$.\nThe training process comprises critical phases. In the server broadcast phase, the server sends its global model $w_g$ to all clients. In the client update phase, each client replaces its local model, denoted as $w_k \\leftarrow w_g$, and conducts local updates by applying $w_k \\leftarrow w_k - \\eta \\nabla L_k(D_k; w_k)$ to train its local model with own data. In the client upload phase, each client uploads its updated local model $w_k$ back to the server. In the global aggregation phase, the server aggregates the local models to generate a global model $w_g \\leftarrow \\sum_{k=1}^{n} \\frac{|D_k|}{|D|} w_k$. These above phases are iteratively executed until the model converges or a predefined number of training rounds is reached.\nDespite its clear advantages in overall performance, the global model $w_g$ suffers a significant performance decline in heterogeneous data scenarios. This decline is largely due to the global model's objective in FL to be universally applicable, which often does not cater to the unique data of individual clients. To address this, PFL customizes a personalized model $w_k$ for each client, effectively mitigating the challenges of data heterogeneity and enabling a tailored exploration of each client's unique data characteristics. The optimization problem in PFL is formulated as follows:\n$$\\min_{w_1,..., w_n} \\sum_{k=1}^{n} \\frac{|D_k|}{|D|} L_k(D_k; w_k).$$\nShould we focus on the model's generalized performance, representing its performance across extensive data, or on its personalized performance, representing its adaptability to specific data? Striking a balance between these two aspects is essential in crafting personalized strategies that effectively capture the unique data distribution characteristics of each client while maintaining generality."}, {"title": "B. Comprehensive Knowledge Distillation", "content": "Knowledge distillation (KD) transfers knowledge from a trained teacher model to an untrained student model [15]. Let $p_t$ and $p_s$ denote the outputs of the teacher and student models, respectively. To smooth the output distribution, a temperature parameter $T$ is used in the softmax function\n$$p_i = \\frac{\\exp(z_i/T)}{\\sum_{j} \\exp(z_j/T)},$$\nwhere $z_i$ is the i-th output element. KD aims to minimize the discrepancy between the teacher and student models. The loss function is defined as follows:\n$$L = L_{CE}(p_s, y) + \\lambda L_{KL}(p_t, p_s),$$\nwhere $L_{CE}$ is the Cross-Entropy loss between the soft labels $p_s$ and the hard labels $y$, $L_{KL}$ is the Kullback-Leibler divergence loss between the soft labels $p_t$ and the soft labels $p_s$, $\\lambda$ is the weight factor.\nRecently, KD has become a critical technique in FL [16], offering innovative solutions to tackle the challenge of catastrophic forgetting, exemplified by methods such as"}, {"title": "IV. EXPERIMENTS", "content": "We simulate two heterogeneous settings using three datasets: FMNIST [17], CIFAR10 [18], and CIFAR100 [18].\nIn the practical setting, data distribution reflects real-world di-versity, characterized by a Dirichlet distribution $Dir(\\alpha)$, with a default $\\alpha = 0.10$ for all datasets. In the pathological setting, data distribution is extremely unbalanced, where each client is assigned imbalanced data from s = 2/2/20 classes out of the 10/10/100 classes in the FMNIST/CIFAR10/CIFAR100 datasets. Fig. 3 illustrates the distribution of the CIFAR100 dataset among 20 clients, showcasing sample numbers by circle size and sample labels by circle color.\nWe utilize two commonly used models: a simple CNN for the FMNIST dataset, and a five-layer CNN for the CIFAR10 and CIFAR100 datasets, adhering to the model outlined in [7] to guarantee fairness.\nWe contrast our method FedCKD with state-of-the-art methods. For traditional FL methods, we evaluate against FedAvg [1] and FedProx [10]. For PFL methods, we evaluate against FedPer [11], LG-FedAvg [12], pFedMe [13], FedFomo [14], and pFedSD [7].\nWe explore two scenarios: (1) n = 20 clients with a participation rate of r = 1.00 over T = 50 communication rounds; (2) n = 100 clients with a participation rate of r = 0.10 over T = 100 communication rounds. The training process involves executing local epochs E = 5 and batch sizes B = 64. The SGD optimizer is used with a learning rate of 0.01, a momentum factor of 0.90, and a weight decay of 1e - 5. For comparability, our method sets the weight factor to $\\lambda$ = 0.50 and distillation temperature to T = 3 in line with [7], while the decay rate for the annealing mechanism is set to $\\gamma$ = 0.99. When employing alternative methods, we strictly adhere to the settings detailed in [7]. We repeat each experiment three times, with the recording of both mean and standard deviation. Evaluations are conducted based on the average test accuracy across all clients."}, {"title": "B. Performance Comparison", "content": "We assess the test accuracy across three datasets in the practical and pathological settings, as presented in Table I- II. Our method, FedCKD, consistently outperforms other methods in various settings. A notable highlight is its performance in the practical setting on the CIFAR100 dataset with 20 clients, where FedCKD exceeded the closest baseline, pFedSD, by a significant margin of 1.98%. Furthermore, FedCKD exhibits remarkable stability, as indicated by its low standard deviation in most cases. On the contrary, the performance of FedAvg and FedProx is subpar because they fail to adequately account for the diverse distribution of data.\nFedPer and LG-FedAvg solely share partial parameters of the local model, overlooking vital local knowledge embedded in other parameters. pFedMe enhances the model's alignment with each client's data by incorporating regularization terms. Nevertheless, an excessive emphasis on personalization could compromise the global model's generalization prowess. Fed-Fomo relies on client-specific weights for model aggregation, potentially diminishing the cohesive integration of global knowledge. pFedSD leverages personalized knowledge from historical models to enhance performance, yet overlooks generalized knowledge within the global model. The learning curve illustrates how a model's accuracy evolves with the increasing number of communication rounds. In the practical setting, as shown in Fig. 4, FedCKD consistently outperforms pFedSD across all datasets and communication rounds, affirming the effectiveness of FedCKD. Overall, FedCKD significantly improves model accuracy in heterogeneous settings by balancing personalized and generalized knowledge.\nIndividual personalization is critical in PFL because it allows the model to take into account each client's unique data characteristics and distribution. Fig. 5 shows the difference in performance between our method FedCKD and the baseline pFedSD in terms of individ-ual accuracy for each client. In the practical setting, FedCKD demonstrates superior accuracy compared to pFedSD in 17 clients while showing lower accuracy in 3 clients, indicating that FedCKD outperforms pFedSD in 85.00% of the cases. In the pathological setting, FedCKD's accuracy exceeds that of pFedSD in 14 clients and lags behind in 6 clients, resulting in higher accuracy in 70.00% of the clients when compared to pFedSD. The findings suggest that FedCKD exhibits strong personalized performance, showcasing its robust capability in adapting to varying client data distributions.\nThe main objectives of individual fairness are to achieve high average accuracy and ensure an even accuracy distribution among clients, and the metric for individual fairness is the standard deviation of model accuracy across all clients. In our experiments in the practical setting with 100 clients, we recorded their model accuracy and computed the standard deviation. As shown in Table III, Our method, FedCKD, has higher test accuracy and lower standard deviation in most cases compared to other PFL methods. Notably, on the CIFAR10 dataset, FedCKD attains the highest average accuracy of 81.02% and the lowest standard deviation of 8.84%. Moreover, on the FMNIST dataset, FedCKD attains a standard deviation similar to pFedSD. Despite a slight increase in standard deviation on the CIFAR100 dataset, FedCKD significantly surpasses other PFL methods in terms of average accuracy. Overall, our method showcases improved fairness, leading to enhanced performance across all clients."}, {"title": "C. Ablation Study", "content": "We investigate the contribution of the annealing mechanism employed in KD. The results on the CIFAR100 dataset among 20 clients in the practical setting are illustrated in Table IV. Upon implementing the annealing mechanism, the mean accuracy in-creases from 57.08% to 57.25%, while the standard deviation decreases from 0.15% to 0.07%. These results demonstrate that the annealing mechanism not only enhances the model's performance but also increases its stability."}, {"title": "D. Sensitivity Analysis", "content": "We conduct a sensitivity analysis to investigate the robust-ness to data heterogeneity, participation rate, and model archi-tecture on the CIFAR100 dataset. The default hyperparameters used, unless stated otherwise, are $\\alpha = 0.1$, n = 20, r = 1.0.\nRobustness to data heterogeneity. Data heterogeneity is a fundamental factor that directly influences the model's learn-ing efficiency and ultimate performance. In the experiments, we set $\\alpha = \\{0.01, 0.10, 1.00\\}$. Lower $\\alpha$ values increase data distribution heterogeneity. FedCKD consistently outperforms other methods in these settings, illustrating its robustness across different levels of heterogeneity, detailed in Table V. Remarkably, when data heterogeneity $\\alpha = 1.00$, FedCKD exceeds pFedSD by 2.83%. However, LG-FedAvg, pFedMe, and FedFomo exhibit subpar performance compared to the traditional FL methods.\nRobustness to participation rate. Participation rate plays a vital role in PFL as it determines the proportion of clients contributing to the global model update. In the experiments, we set r = {0.20, 0.60, 1.00}. Table V shows that most methods exhibit reduced performance as the participation rate decreases. This decline is due to the restricted data participation in each round, which results in the model that cannot fully utilize the overall data. Noteworthy, pFedSD's performance declines when the participation rate reaches 1.00, likely because the wide variances in client data distributions introduce extra noise into the model updates. Conversely, our method, FedCKD, effectively mitigates the impact of noise.\nRobustness to model architecture. Model architecture also plays a significant role in PFL because different model architectures may exhibit varying adaptability and learning capabilities when handling specific types of data. In the experiments, we utilize two more complex models, namely ResNet-8 [19] and MobileNetV2 [20]. Table V demonstrates that our method, FedCKD, significantly outperforms the baseline methods across a variety of model architectures. This superiority is evident through improved mean accuracy and reduced standard deviation, suggesting its ability to maintain consistent and efficient performance in all model architectures."}, {"title": "V. CONCLUSION", "content": "This paper presents FedCKD, a comprehensive knowledge distillation method for PFL, designed to address catastrophic forgetting and achieve a balance between generalization and personalization. Utilizing multi-teacher knowledge distilla-tion, we effectively transfer knowledge from global and historical models to the local model. The global model contains generalized knowledge, while the historical model holds personalized knowledge. Employing the distillation mechanism, we achieve a balance and integration of these two types of knowledge, significantly boosting the model's performance. We also implement an annealing mechanism to further enhance performance and stability. Extensive experiments demonstrate the superiority of FedCKD over existing methods. However, it has inherent limitations as the KD process often requires additional computational resources. Future research will concentrate on implementing more efficient KD techniques in PFL to reduce computational costs."}]}