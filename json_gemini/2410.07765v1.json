{"title": "GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language Models Through Traversing 2D Game Maps", "authors": ["Muhammad Umair Nasir", "Steven James", "Julian Togelius"], "abstract": "Large language models (LLMs) have recently demonstrated great success in generating and understanding natural language. While they have also shown potential beyond the domain of natural language, it remains an open question as to what extent and in which way these LLMs can plan. We investigate their planning capabilities by proposing GameTraversalBenchmark (GTB), a benchmark consisting of diverse 2D grid-based game maps. An LLM succeeds if it can traverse through given objectives, with a minimum number of steps and a minimum number of generation errors. We evaluate a number of LLMs on GTB and found that GPT-4-Turbo achieved the highest score of 44.97% on GTB_Score (GTBS), a composite score that combines the three above criteria. Furthermore, we preliminarily test large reasoning models, namely o1, which scores 67.84% on GTBS, indicating that the benchmark remains challenging for current models. Code, data, and documentation are available at https://github.com/umair-nasir14/Game-Traversal-Benchmark.", "sections": [{"title": "Introduction", "content": "Large language models, built atop the transformer architecture [1], are widely influential in the field of natural language processing [2], and have shown great promise in a variety of applications that were not originally envisioned as target domains, thereby, hinting towards more general Artificial Intelligence models [3, 4]. In recent years, numerous LLMs have emerged, each competing for state-of-the-art performance by continuously expanding the limits of their capabilities [5]. As a result, the accurate evaluation of LLMs has become a key focus for researchers [6\u20138].\nLLMs are trained to predict the next token based on their current context, but an ongoing debate is whether these next-token predictors are capable of planning [9, 10]. Planning may include tasks in games [11, 12], robot control [13], questioning answering [14], or visual programming [15]. While these works show the capabilities of LLMs to plan, multiple complementary planning benchmarks are required to know whether these task-specific examples of planning are a common LLM ability. Therefore, we present a benchmark designed to evaluate the planning abilities of LLMs from a novel perspective. This benchmark serves as a complement to existing LLM assessments, focusing on tasks that are unlikely to be extensively represented in the training datasets of current LLMs. While prior evaluations of planning abilities have been conducted in natural language environments [16, 17], our work introduces a task represented as a string of characters that depicts a 2D grid-based map. This representation is infrequently encountered by LLMs during training, preventing them from merely performing a lookup to obtain answers; instead, they must engage in effective planning to determine the optimal path.\nPrevious works have shown that LLMs can generate 2D grid-based maps with fine-tuning and training [18\u201320], and without [21, 12]. Consequently, the question of whether LLMs can successfully create 2D grid-based maps has been addressed. However, while an LLM can generate sequences of characters devoid of natural language meaning, we seek to investigate whether it can effectively plan using these same character sequences. This inquiry is the focus of our research.\nA natural question arises: Why evaluate LLMS on a 2D map? The rationale is that a 2D map, represented as a sequence of characters, is interpretable by LLMs. Given adequate instruction, LLMs should therefore be capable of processing and comprehending the map's state, which encompasses the map itself, the agent's position, and the locations of the objectives. Previous studies have demonstrated that LLMs can process instructions that are out-of-distribution from their training data. Therefore, when provided with the map's state and the available actions for moving the agent, an LLM capable of planning should be able to generate a sequence of actions to reach the target. To generate the correct action sequence, the LLM must plan each move. Similar to how humans remain aware of their surroundings while traversing a path, an LLM needs to continuously observe the environment to produce the correct actions. But do LLMs actually observe and plan as they generate action sequences? To answer this, we introduce the GameTraversalBenchmark (GTB), which consists of a dataset of diverse maps.\nIn the following sections, we begin by describing the dataset collection process and the creation of GTB. Next, we evaluate several LLMs on the GTB benchmark, demonstrating that while the state-of-the-art GPT-4 achieves the highest performance, it still falls short of reaching 50%. We conclude with a discussion on how traversal problems can be made more challenging to encourage the research community to improve LLMs' planning capabilities."}, {"title": "GameTraversalBenchmark", "content": "In this section, we will discuss how we curated the dataset and the GameTraversalBenchmark (GTB). The dataset is generated via the pipeline introduced by our previous work, named as Word2World [12]. Word2World is an LLM-based game design system that creates a story and narrative for the game by extracting useful information, such as tileset and character descriptions, and goals for the player. After extracting useful information, Word2World first places alphanumeric characters for the environment of the world in a 2D grid-based map. After that, it places alphanumeric characters for the game characters, such as the protagonist and antagonist of the story, and the interactive object tiles needed for the protagonist to play the game. Then Word2World generates the coordinates of objectives needed to complete the game. The setting of the environment and game characters is iteratively refined to generate more coherent maps. These worlds are generated using GPT-4-Turbo, GPT-4, GPT-3.5, Claude-3-Opus and Claude-3-Haiku.\nWe extract 150 of these maps, an example of which is illustrated by Figure 1. Each of the objectives for the LLM is to reach a certain coordinate and then to reach another coordinate, which serves as the next objective. These maps make good environments for judging the traversal abilities of an agent, as they have diverse patterns and different sizes. While there are datasets available from previous games [22, 23], GTB provides many different sized levels with deceptive paths to traverse."}, {"title": "Benchmark", "content": "We consider a generated map as one data point for the benchmark. The map's generated objective coordinates are the points where the LLM agent needs to traverse to attain the most rewards. Since we are interested in observing an LLM's planning ability from a traversal agent perspective, we compute the optimal path using A* [24] search, which is set to find the optimal (shortest) path within 25000 iterations.\nThe goal of the LLM agent is to traverse the world through these objectives. The LLM agent is required to generate a sequence of actions that should take the agent from current position to the position of the objective. Therefore, LLM agent can generate a sequence of actions for the current objective once. All generated actions in the sequence are then rolled out in simulation and the position of the LLM agent decides the rewards it gets. Table 1 shows how rewards are distributed. This new position of the LLM agent is the starting positon for the next objective. The LLM agent also needs to find the shortest path to each objective. Thus, an LLM agent receives the highest rewards for achieving the objective while minimising the number of actions and making the fewest errors while generating the solution. Such errors include incorrect actions generated by the LLM, failure to adhere to imposed constraints, or the generation of syntactically incorrect outputs."}, {"title": "Baseline Results", "content": "To show the relevance of GTB in the current state-of-the-art LLMs, we evaluate a few families of LLMs. Table 2 shows results for different LLMs. The best-performing LLMs, such as GPT-4-Turbo and Claude-3-Opus, are among the state-of-the-art LLMs as well when it comes to benchmarks for natural language understanding tasks, code generation tasks, as well as on PlanBench [16]. Thus, the results are consistent with other benchmarks, making GTB a uniques option to evaluate planning in LLMs. These results demonstrate that the benchmark is a tough challenge for LLMs and there is a big room for improvement in the planning abilities of today's LLMs.\nFurthermore, we can observe from Table 2 that all LLMs except the first 4 LLMs are worse than a Random-FP agent. The Random-FP agent computes the difference in distance between two objectives, using this value to determine the length of the required action sequence. Then it produces that many actions uniformly at random. MGE, MPL and MAT are not applicable in this case as there will be no generation error and the path length is fixed. Questions therefore arise as to whether LLMs with lesser capacity are just randomly generating sequences of actions. Observation of the behaviour exhibited by the various LLMs suggests that the actions taken by LLMs other than GPT-4 and Claude-3-Opus do not appear meaningful, and could very well be random. The most likely explanation for this is that these LLMs are unable to internally construct a map or representation of the environment's state. A truly random agent, Random-RP generates random action sequence of random lengths and achieves a GTBS of 3, implying that generating the right sequence length is not a random task it needs to be planned. LLMs performing better than Random-RP agent also means the LLM may have a sense of how long the path should be, but were not able to produce the right actions. This suggests that a less capable LLM might grasp the relationship between the start and target positions of the objective, but fails to comprehend the path planning process required to navigate between them."}, {"title": "Preliminary Results On Large Reasoning Models", "content": "We further test GTB on o1 [25], a recent large reasoning model (LRM), to see how the current state-of-the-art performs on GTB. The underlying LLM of o1 is trained using reinforcement learning to curate its outputs through a private Chain-of-Thought reasoning process [26], which enhances its ability to simulate a \u201cthinking\" phase before generating responses. Furthermore, o1 is a reasoning model, meaning that the inference stage, which contains many more steps than the typical LLM, is computationally and monetarily expensive. For this reason, it cannot be straightforwardly compared with other LLMs - it is fundamentally a different kind of model. For the same reason, we only include preliminary results based on a single run in this paper. In the single run, we found that the model outperformed the top-performing LLM on GTB. However, despite surpassing GPT-4-Turbo, it was unable to exceed a score of 70 on GTBS, as demonstrated in Table 4."}, {"title": "Related Work", "content": "Conventionally, LLMs are evaluated for their natural language understanding and generation abilities. In this context, Hendrycks et al. [6] introduced the Massive Multitask Language Understanding (MMLU) benchmark which consists of 57 tasks spanning subjects such as elementary mathematics, computer science, and law. Similarly, Beyond the Imitation Game benchmark (BIG-bench) [7] was also introduced to evaluate LLMs across 204 diverse tasks. Other evaluations, such as Chatbot Arena [27] and MT-Bench [8], focus on measuring the general capabilities of chatbots. While these benchmarks assess overall performance across multiple tasks, there are numerous specialised benchmarks that evaluate specific downstream tasks. For example, SocKET [28] assesses social knowledge, TRUSTGPT [29] focuses on ethics, MATH [30] evaluates mathematical problem-solving, APPS [30] and HumanEval [31] test code generation abilities, FreshQA [32] and TRUTHFULQA [33] examine question-answering capabilities, and SafetyBench [34] evaluates the safety performance of LLMs.\nBenchmarks specifically designed to evaluate the planning abilities of LLMs are also related to GTB. API-Bank [35] evaluates an LLM on the ability to continuously plan, retrieve, and call multiple APIs. The most closely related works are PlanBench [16] and AutoPlanBench [17] which uses Planning Domain Description Language (PDDL) [36] and convert them into natural language to evaluate the planning abilities of LLMs. While previous works have demonstrated the capacity of LLMs to play games [12, 37\u201340] to the best of our knowledge, no prior work has focused on evaluating LLMs specifically within the context of games."}, {"title": "Limitations", "content": "Although GTB has provided valuable insights into the planning abilities of LLMs, it is important to acknowledge its limitations, which, if addressed, could further advance the field. The 2D game maps are static, which is still challenging but can be increased in difficulty, such as moving non-player characters, enemies that may attack, tiles that may end in terminating conditions etc. Furthermore, GTB focuses solely on pathfinding abilities, which, while critical, represent only one dimension of planning. The action space is also limited to only 4 discrete actions. GTB has a static prompt that evaluates all the LLMs, which may be easier for larger LLMs to follow instructions then smaller ones."}, {"title": "Conclusion And Future Directions", "content": "This research introduces GameTraversalBenchmark (GTB), a novel benchmark for evaluating LLMs planning abilities through traversal in a 2D grid-based map. We provide extensive metrics to give insights towards planning abilities in LLMs. We then evaluate many LLMs on GTB_Score. We also evaluate LRMs on GTB_Score. We believe that there is a lot of room in this direction for research as LLMs have not been evaluated as traversal agents before but have been used in previous research as game-playing abilities. For future work, we would like to see how LLMs of all scales perform when they are fine-tuned on GTB. We would like to see how much smaller LLMs can improve upon fine-tuning, and after fine-tuning, how much such fine-tuning will generalise. Once we have LLMs that can perform well on GTB, we would like to extend the work by letting LLMs generate the state representation as well. It would also be valuable to have a prompt generation mechanics that can create prompts for all LLMs separately. A mechanism introduced by [41] could be useful to enhance LLMs on GTB. We hope that our contribution will push the boundaries of current state-of-the-art LLMs planning abilities as they are not able to achieve high scores on GTB, yet."}]}