{"title": "Music2P: A Multi-Modal Al-Driven Tool for Simplifying Album Cover Design", "authors": ["Joong Ho Choi", "Geonyeong Choi", "Ji-Eun Han", "Wonjin Yang", "Zhi-Qi Cheng"], "abstract": "In today's music industry, album cover design is as crucial as the music itself, reflecting the artist's vision and brand. However, many AI-driven album cover services require subscriptions or technical expertise, limiting accessibility. To address these challenges, we developed Music2P, an open-source, multi-modal AI-driven tool that streamlines album cover creation, making it efficient, accessible, and cost-effective through Ngrok. Music2P automates the design process using techniques such as Bootstrapping Language Image Pre-training (BLIP), music-to-text conversion (LP-music-caps), image segmentation (LoRA), and album cover & QR code generation (ControlNet). This paper demonstrates the Music2P interface, details our application of these technologies, and outlines future improvements. Our ultimate goal is to provide a tool that empowers musicians and producers, especially those with limited resources or expertise, to create compelling album covers.", "sections": [{"title": "1 INTRODUCTION", "content": "In the contemporary music industry, album cover design plays a pivotal role, serving as a visual embodiment of an artist's vision and brand. Traditionally, this process has required collaborative efforts among musicians, graphic designers, and marketing teams. However, independent artists and smaller labels face significant challenges due to limited time, resources, and specialized design expertise. The key challenge is creating a visual representation that captures the music's essence, resonates with the target audience, and stands out in a competitive market-all within tight time and budget constraints.\nIn response to these challenges, recent years have witnessed the emergence of AI-driven solutions for album cover generation. However, these solutions often suffer from uni-modal constraints, typically accepting only textual input. This limitation restricts users' creativity and the breadth of information they can provide. Furthermore, due to high computational costs, these services often limit users to a small number of attempts, leaving independent artists and smaller labels with minimal opportunities to generate satisfactory album covers.\nTo address these limitations, we introduce Music2P, a system that leverages recent Al technologies to provide a more comprehensive and adaptable solution. As illustrated in Figure 1, Music2P integrates multiple components to create an attractive album cover generation pipeline. The Music2P's system architecture includes image captioning (BLIP [10]), music-to-text conversion (LP-music-caps [9]), image segmentation (LoRA [8]), album covers & QR code generation (ControlNet [13]). These interconnected components work together to produce aesthetically pleasing and semantically relevant album covers from multi-modal inputs, offering a holistic approach to the design process."}, {"title": "2 MUSIC2P: MULTI-MODAL ALBUM COVER GENERATION", "content": "As illustrated in Figure 1, Music2P is an open-source, multi-modal Al system designed to streamline the creation of personalized album covers by integrating various input modalities, including text, image, and audio. This system addresses accessibility challenges in the music industry by offering a cost-effective, subscription-free solution. Music2P automates the album cover design process using advanced techniques, including Bootstrapping Language Image Pre-training (BLIP) [10] for image captioning, LP-music-caps [3] for converting music to text, image segmentation with Low-Rank Adaptation (LoRA) [8], and ControlNet [13] for generating album covers & QR codes."}, {"title": "2.1 Image to Text Conversion", "content": "We use Bootstrapping Language-Image Pre-training (BLIP) as a composite caption generator for images. It has an advanced encoder-decoder structure that can handle multi-modal tasks. Specifically, BLIP uses self-attention and cross-attention to improve the understanding of the given input image for album cover design. BLIP also includes a filter to remove noisy or unrelated captions, allowing us to use the most appropriate and influential descriptions as input when creating album covers."}, {"title": "2.2 Music to Text Conversion", "content": "We employ LP-music-caps [3] as its Audio-to-Caption model, a sophisticated system designed to transform musical input into descriptive text. This model's development follows a two-phase approach: initial training on a GPT 3.5 Turbo-generated pseudo dataset of music-caption pairs, followed by fine-tuning with human-crafted captions to enhance output naturalness. LP-music-caps generates segmented captions at ten-second intervals, offering nuanced insights into the music's evolving themes, moods, and expressions. This sequential captioning method effectively captures the musical piece's dynamic and temporal progression, translating auditory nuances into textual form."}, {"title": "2.3 Segmentation with LoRA", "content": "In album cover creation, extracting a Canny edge map and performing image segmentation on user-provided images are crucial for aligning visual design with musical aesthetics and themes. The Canny map provides a simplified edge-based abstraction, but its monochromatic nature inherently limits its ability to differentiate color-defined objects, as evidenced in Figure 2(b). This limitation becomes critical in contexts requiring color differentiation. The Canny map's reduction of images to binary edge representations results in the loss of vital chromatic information essential for object identification and differentiation. Conversely, image segmentation algorithms excel in parsing images into distinct segments based on color, intensity, and texture, as demonstrated in Figure 2(e).\nBoth Canny detection and image segmentation, despite their individual strengths, exhibit notable limitations. Canny detection effectively identifies edges (Figure 2(b)) akin to the original image (Figure 2(a)) but fails in color distinction, rendering it ineffective for differentiating objects like apples and oranges. Image segmentation faces similar challenges when applied to images outside its training domain, as illustrated by the ineffective segmentation resulting from applying a cityscape-trained model to a fruit image (Figure 2(c)).\nAddressing these limitations necessitates fine-tuning the segmentation network, a process involving the adaptation of a pre-existing network with approximately 90 million parameters. To manage this computational challenge efficiently, we employed Low-Rank Adaptation (LoRA) [8]. LoRA enables efficient retraining of large neural networks by adjusting only a small parameter subset, significantly reducing computational demands and training time. The LoRA technique is mathematically expressed as:\n$h = W_0x + \\Delta Wx = W_0x + BAx,$\nwhere the hidden state h comprises the original transformation $W_0x$ and a low-rank update $\\Delta Wx$, decomposed into the product of smaller matrices $B \\in R^{d\\times r}$ and $A \\in R^{r\\times k}$, with rank r substantially"}, {"title": "2.4 Album Cover Generation using ControlNet", "content": "ControlNet [13], a suite of neural networks fine-tuned on Stable Diffusion, forms the core of Music2P's image generation. Its architecture seamlessly integrates pretrained parameters from a Diffusion model-specifically, the latent space of Stable Diffusion's U-Net architecture-with separately maintained pre-trained parameters as locked copies. This unique design enables precise control over both structural and artistic elements in the generated images. Specifically, the ControlNet methodology employs a dual-copy strategy, creating two versions of a large image diffusion model: one trainable and one with frozen weights. This approach serves a dual purpose: preserving the foundational knowledge encoded in the Diffusion model's latent space while allowing adaptability through the trainable copy. \"Zero convolution\" layers bridge these two copies, facilitating seamless information flow and enabling network fine-tuning to meet the specific demands of album cover design.\nThis architecture empowers ControlNet to perform task-specific conditioning, significantly enhancing control over various aspects of image generation, including pose, edge detection, and depth maps. For Music2P, we further refined the pre-trained ControlNet through additional fine-tuning, focusing on datasets for edge detection and image segmentation, tailoring it specifically for album cover creation. To generate album covers, Music2P integrates ControlNet by leveraging a triad of inputs:\n(1) BLIP-generated captions derived from the input image"}, {"title": "2.5 QR Code Generation", "content": "The QR code generation feature in Music2P allows users to create visually integrated QR codes within their album covers. The process involves the following steps:\n(1) The user provides a base image and a QR code.\n(2) The user inputs text describing the desired aesthetic.\n(3) The system generates an AI-enhanced QR code integrated with the base image.\nFor example, if a user provides the base image shown in Figure 4(a) and specifies \"realistic, 8K\" as the text input along with techno music, Music2P's QR code service will produce an output as illustrated in Figure 4(b). It is important to note that the aesthetic quality and functionality of the output QR code may be compromised if the user submits a personal photograph or an image containing patterned objects. The QR code generation model utilizes several key hyperparameters that significantly influence the output:\n*   Guidance scale: Higher values result in sharper final images, affecting both the QR code and the base image.\n*   ControlNet conditioning scale: This parameter adjusts the balance between the QR code and the base image. Values range from 0 to 5, where lower values result in a less visible QR code, and higher values make the QR code more prominent.\n*   Strength: This parameter also controls the visibility of the QR code over the base image. Lower values result in a subtle QR code, while higher values make it more dominant.\nThese hyperparameters allow for fine-tuning the visual integration and readability of the QR code within the album cover design.\nAfter we trained the model to generate QR codes and album covers, we utilized Ngrok to avoid running the code to use our service. Ngrok provides the IP address and the port number to use our service. After the user uploads the image, mp3 file, and the style for the album cover on the website generated by Ngrok, the user can get the album cover without running our code."}, {"title": "3 SYSTEM IMPLEMENTATION & USER GUIDE", "content": "Music2P is an open-source web system solution accessible to anyone. The system is designed to be user-friendly, requiring minimal"}, {"title": "4 CONCLUSION & FUTURE WORK", "content": "This paper introduces Music2P, a multi-modal system for automated album cover generation that integrates advanced AI techniques including BLIP for image captioning, LP-music-caps for music-to-text conversion, LoRA for image segmentation, and ControlNet for album cover and QR code generation. We've also explored a cost-effective deployment using Ngrok, making the solution accessible to a wider range of users. For future directions, we would need to focus on scalable infrastructure deployment. Additionally, we need to train more customized LoRA for our Music2P solution. This is essential because when generating album covers, if user input images consist of faces or objects of pattern, the aesthetic quality of output deteriorates significantly."}]}