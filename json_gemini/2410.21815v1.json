{"title": "GNOTHI SEAUTON: EMPOWERING FAITHFUL SELF-INTERPRETABILITY IN BLACK-BOX MODELS", "authors": ["Shaobo Wang", "Hongxuan Tang", "Mingyang Wang", "Hongrui Zhang", "Xuyang Liu", "Weiya Li", "Xuming Hu", "Linfeng Zhang"], "abstract": "The debate between self-interpretable models and post-hoc explanations for black-box models is central to Explainable AI (XAI). Self-interpretable models, such as concept-based networks, offer insights by connecting decisions to human-understandable concepts but often struggle with performance and scalability. Conversely, post-hoc methods like Shapley values, while theoretically robust, are computationally expensive and resource-intensive. To bridge the gap between these two lines of research, we propose a novel method that combines their strengths, providing theoretically guaranteed self-interpretability for black-box models without compromising prediction accuracy. Specifically, we introduce a parameter-efficient pipeline, AutoGnothi, which integrates a small side network into the black-box model, allowing it to generate Shapley value explanations without changing the original network parameters. This side-tuning approach significantly reduces memory, training, and inference costs, outperforming traditional parameter-efficient methods, where full fine-tuning serves as the optimal baseline. AutoGnothi enables the black-box model to predict and explain its predictions with minimal overhead. Extensive experiments show that AutoGnothi offers accurate explanations for both vision and language tasks, delivering superior computational efficiency with comparable interpretability.", "sections": [{"title": "1 INTRODUCTION", "content": "Explainable AI (XAI) has gained increasing significance as AI systems are widely deployed in both vision (Dosovitskiy, 2020; Radford et al., 2021; Kirillov et al., 2023) and language domains (Devlin et al., 2019; Brown, 2020; Achiam et al., 2023). Ensuring interpretability in these systems is vital for fostering trust, ensuring fairness, and adhering to legal standards, particularly for complex models such as transformers. As illustrated in Figure 1(a), the ideal paradigm for XAI involves designing inherently transparent models that deliver superior performance. However, given the challenges in achieving this, current XAI methodologies can be broadly classified into two main categories: developing self-interpretable models and providing post-hoc explanations for black-box models.\nDesigning Self-Interpretable Models: Several notable efforts have focused on designing self-interpretable models that are grounded in solid mathematical foundations or learned concepts. Among these, concept-based networks have emerged as a representative approach linking model decisions to predefined, human-understandable concepts (Kim et al., 2018; Koh et al., 2020; Alvarez-Melis & Jaakkola, 2018). However, incorporating hand-crafted concepts often introduces a trade-off between interpretability and performance, as these models typically compromise the performance of the primary task. Moreover, such methods are often closely tied to specific architectures, which limits their scalability and transferability to other tasks. Furthermore, the explanations generated by concept-based models often lack a rigorous theoretical foundation, raising concerns about their reliability and overall trustworthiness.\nExplaining Black-Box Models: Given the challenges of designing self-interpretable models for practical applications, post-hoc explanations for black-box models have become a widely adopted alternative. Among these, Shapley value-based methods (Shapley, 1953) are particularly valued for their theoretical rigor and adherence to four principled axioms (Young, 1985). However, calculating exact Shapley values involves evaluating all possible feature combinations, which scales exponentially with the number of features, making direct computation impractical for models with high-dimensional inputs. To alleviate this, methods like Fast-SHAP (Jethani et al., 2021) and ViT-Shapley (Covert et al., 2022) employ proxy explainers that estimate Shapley values during inference, significantly reducing the number of evaluations needed. While these approaches reduce some computational costs, training a separate explainer remains resource-intensive.\nTo bridge the gap between existing methods and address the aforementioned challenges, the core ob-jective of our research is to achieve theoretically guaranteed self-interpretability in advanced neu-ral networks without sacrificing prediction performance, while minimizing training, memory, and inference costs. To this end, we propose a novel paradigm, AutoGnothi, which leverages parameter-efficient transfer learning (PETL) to substantially reduce the high training, memory, and inference costs associated with obtaining explainers. As depicted in Figure 3(a), traditional model-specific methods require two training stages: (i) fine-tuning a pre-trained model into a surrogate model, and (ii) training an explainer using the surrogate model. During inference, the original model is used for prediction, while a separate explainer network generates explanations, leading to two inference passes and double the storage overhead. In contrast, AutoGnothi utilizes side-tuning to reduce both training and memory costs, as shown in Figure 3(b). By incorporating an additive side branch parallel to the pre-trained model, we efficiently obtain a surrogate side network through side-tuning. We then apply the same strategy to develop the explainer side network, enabling simultaneous predic-"}, {"title": "2 RELATED WORK", "content": "Explaining Black-Box Models with Shapley Values: Among post-hoc explanation methods, the Shapley value (Shapley, 1953) is widely recognized as a faithful and theoretically sound metric for feature attribution, uniquely satisfying four key axioms: efficiency, symmetry, linearity, and dummy."}, {"title": "3 BACKGROUND", "content": "3.1 SHAPLEY VALUES\nThe Shapley value, originally introduced in game theory (Shapley, 1953), provides a method to fairly distribute rewards among players in coalitional games. In this framework, a set function assigns a value to any subset of players, corresponding to the reward earned by that subset. In machine learning scenarios, input variables are typically regarded as players, and a deep neural network (DNN) serves as the value function, assigning importance (saliency) to each input variable.\nLet $s \\in \\{0,1\\}^d$ be an indicator vector representing a specific variable subset for a sample $x = [x_1,x_2,...,x_d] \\in \\mathbb{R}^d$. Specifically, $x_s$ denotes the variables indicated by $s$, while those not in $s$ are replaced by a masked value (e.g., a baseline value). Let $e_i \\in \\mathbb{R}^d$ denote the vector with a one in the $i$-th position and zeros elsewhere. For a game involving $d$ players\u2014or equivalently, a DNN $v : \\{0,1\\}^d \\rightarrow \\mathbb{R}$ with $d$ input variables\u2014the Shapley values are denoted by $\\phi_v(x_1),..., \\phi_v(x_d)$. Each $\\phi_v(x_i) \\in \\mathbb{R}$ represents the value attributed to the $i$-th input variable $x_i$ in the sample $x$. The Shapley value $\\phi_v(x_i)$ is computed as follows:\n$\\phi_v(x_i) = \\sum_{s:s_i=0} \\frac{1}{d} {d-1 \\choose |s|} (v(x_{s+e_i}) - v(x_s)).$\\nIntuitively, Eq. (1) captures the average marginal contribution of the i-th player to the overall reward by considering all possible subsets in which player i could be included. Shapley values satisfy four key axioms: linearity, dummy player, symmetry, and efficiency (Young, 1985). These axioms ensure a fair and consistent distribution of the total reward among all players.\n3.2 MODEL-BASED ESTIMATION OF SHAPLEY VALUES\nCalculating Shapley values to explain individual predictions presents substantial computational challenges (Chen et al., 2023a). To mitigate this burden, these values are typically approximated using sampling-based estimators, such as those in (Lundberg, 2017; Covert & Lee, 2020), though the sampling cost remains considerable. Recently, a more efficient model-based approach, introduced in (Jethani et al., 2021), accelerates the approximation by training a proxy explainer to compute Shapley values through a single model inference. However, this method has not been validated on advanced neural architectures such as transformers."}, {"title": "4 \u041c\u0415\u0422\u041dOD", "content": "4.1 EFFICIENTLY TRAINING THE SHAPLEY VALUE EXPLAINER FOR BLACK-BOX MODELS\nAs discussed in Section 3.2, existing methods for approximating Shapley values require two stages. First, the black-box model $f$ is fully fine-tuned to obtain a surrogate model $g$ with the same trainable parameters and memory cost as $f$. Then, an explainer & is fully trained using $g$. These stages at least double the training, memory, and inference costs compared to using the black-box model $f$ alone, making these methods impractical for large models."}, {"title": "4.1.1 \u041e\u0432TAINING THE SURROGATE", "content": "To obtain the surrogate model, AutoGnothi applies LST directly to the black-box model $f$, utilizing the additive side branch $g$ with parameters $B$ to predict the masked inputs $x_s$ of sample $x$. Let $f^{(i)}$ and $g^{(i)}$ denote the $i$-th MSA block of the main model $f$ and the surrogate branch $g$, respectively. Assume there are $N$ MSA blocks in total. Let $z^{main}$ denote the output for masked input $x_s$ of the first MSA layer $f^{(1)}$ of $f$, i.e., $z^{main} = f^{(1)}(x_s)$. The forward process of the frozen main model $f$ with the side-tuning branch $g$ is:\n$z_{surr}^{(i)} = g^{(i)}(FC^{(i)}(z^{main})).$\nAfter $N$ MSA blocks, an FC head is applied to generate the prediction for the partial information, i.e., $y_{surr} = FC^{head}(z_{surr}^{(N)})$. The convergence of the surrogate model is analyzed as follows:\nTheorem 1 (Proof in Appendix B). Let the surrogate model be trained using gradient descent with step size $a$ for $t$ iterations. The expected KL divergence between the original model\u2019s predictions $f(x)$ and the surrogate model\u2019s predictions $g_B(x_s)$ is upper-bounded by:\n$E_{x~p(x),s~U(0,d)} [D_{KL} (f(x)||g_B(x_s))] \\leq \\frac{1}{2\\mu} (1 - \\mu a)^t (L_{surr}(B_0) - L_{surr}^*),$\nwhere $B_0$ is the initial parameter value, $L_{surr}^*$ is the optimal value during optimization, and $\\mu$ is the minimal eigenvalue of the Hessian of $L_{surr}$."}, {"title": "4.1.2 \u041e\u0412TAINING THE EXPLAINER", "content": "For the explainer model, AutoGnothi uses a similar LST feature backbone as in the surrogate model $g$, consisting of $N$ MSA blocks for feature extraction. Let $\\phi^{(i)}$ represent the i-th MSA block of the explainer branch \u03c6. In addition to the lightweight backbone blocks in the side branch, we add M extra FC layers as the explanation head. Together, the side network $\u03c6$ generates explanations based on the backbone features from the main branch. Let $z^{main}$ denote the output for input x from the first MSA layer $f^{(i)}$ of the main branch f, i.e., $z^{main} = f^{(1)}(x)$. The forward process of the explainer is:\n$\\forall i \\in \\{1, ...,N\\}, z_{exp}^{(i)} = \\phi^{(i)}(FC^{(i)}(z^{main})) , $\n$\\phi_{exp}(x) = FC_{head}^{(M)}(FC_{head}^{(M-1)} (... (FC_{head}^{(1)}(z_{exp}^{(N)}))) ,$\nwhere the main branch f remains uncontaminated. We provide a theoretical guarantee for the con-vergence of the trained side branch \u03c6 as follows:\nTheorem 2 (Proof in Appendix B). Let $\u03c6_v(x|y)$ denote the exact Shapley value for input-output pair (x, y) in game v. The expected regression loss Lexp(\u03b8) upper bounds the Shapley value estimation error as follows,\n$E_{p(x,y)} [||\u03c6_\u03b8(x,y) - \u03c6_v(x|y)||^2] \\leq \\frac{2}{H_{d-1}} (L_{exp}(\u03b8) - L_{exp}^\\ast),$\nwhere $L_{exp}^\\ast$ represents the optimal loss achieved by the exact Shapley values, and $H_{d\u22121}$ is the $(d \u2212 1)$-th harmonic number.\nTheorem 2 provides a theoretical guarantee that a side-tuned explainer can achieve performance on par with a fully trained explainer. The complete proof is presented in Appendix B."}, {"title": "4.1.3 GENERATING EXPLANATION FOR BLACK-BOX MODELS", "content": "After obtaining the explainer, we now detail the explanation procedure. In most post-hoc explana-tion methods"}, {"title": "4.2 DIFFERENCE BETWEEN AutoGnothi AND PREVIOUS PETL METHODS", "content": "In this section, we provide an empirical analysis of why AutoGnothi outperforms previous PETL approaches. We highlight that AutoGnothi is not a simple extension of classical PETL, where full fine-tuning is typically the optimal baseline. Additionally, AutoGnothi exploits the intrinsic corre-lation between the prediction task and its explanation, enabling black-box models to become self-interpretable without sacrificing prediction accuracy. We elaborate on these two points below.\nFull fine-tuning poses challenges for achieving self-interpretability and is not the optimal base-line for AutoGnothi. For classical PETL, the goal is often to match the performance of fully fine-tuned models on standard tasks (Houlsby et al., 2019; Chen et al., 2023b; Hu et al., 2021; Mercea et al., 2024). However, in XAI scenarios, the challenge shifts: it becomes difficult, if not impossible, to train a model that balances both prediction accuracy and explanation quality (Arrieta et al., 2020; Gunning et al., 2019; Do\u0161ilovi\u0107 et al., 2018). In fact, fine-tuning models to adapt interpretability without forgetting pre-trained knowledge can be difficult (Li & Hoiem, 2017). Additionally, full fine-tuning the original model also puts us in the Theseus's Paradox: we won't be sure if we are ex-plaining the very same model anymore. Even if full fine-tuning were practical, it would contradict the goal of interpreting the pre-trained model. In contrast, AutoGnothi pipeline addresses this issue by freezing the primary model and training only a side network to generate explanations, offering an efficient solution that enables self-interpretability without degrading prediction performance."}, {"title": "5 EXPERIMENTS", "content": "5.1 EXPERIMENTAL SETTINGS\nDatasets and Black-box Models. For image classification, we used the ImageNette (Howard & Gugger, 2020), Oxford IIIT-Pets (Parkhi et al., 2012), and MURA (Rajpurkar et al., 2017) datasets, following (Covert et al., 2022). For sentiment analysis, we utilized the Yelp Review Polarity dataset (Zhang et al., 2015). In terms of black-box models, we employed the widely used ViT models (Dosovitskiy, 2020) for vision tasks, including ViT-tiny, ViT-small, ViT-base, and ViT-large. For language tasks, we used the BERT-base model Implementation Details. For surrogates and ex-plainers, AutoGnothi incorporates the same num-ber of MSA blocks as the black-box model being explained in the side network and utilizes a re-duction factor of $r = 8$ for the lightweight side branch on both the ImageNette and Oxford IIIT-Pets datasets, and $r = 4$ for MURA and Yelp Review Polarity. Surrogates use the same task head as the black-box classifiers with one addi-tional FC layer as classification head for handling partial information. Explainers utilize three addi-tional FC layers as the explanation head after the side network backbone. For attention masking, we employed a causal attention masking strategy, setting attention values to a large negative number before applying the softmax operation (Brown, 2020). More detailed training settings are pro-vided in Appendix A.\nEvaluation Metrics for Explanations. We used the widely adopted insertion and deletion met-rics (Petsiuk, 2018) to evaluate explanation qual-ity. These metrics are computed by progressively"}, {"title": "6 CONCLUSION", "content": "This paper introduces AutoGnothi to bridge the gap between self-interpretable models and post-hoc explanation methods in Explainable AI. Inspired by parameter-efficient transfer-learning, AutoG-nothi incorporates a lightweight side network that allows black-box models to generate faithful Shapley value explanations without affecting the original predictions. Notably, AutoGnothi out-performs directly finetuning the all the parameters in the model for explanation by a clear margin. This approach empowers black-box models with self-interpretability, which is superior to standard post-hoc explanations that require generating predictions and explanations in two separate, heavy in-ferences. Experiments on ViT and BERT demonstrate that AutoGnothi achieves superior efficiency on computation, storage and memory in both training and inference periods."}, {"title": "A FURTHER EXPERIMENTAL DETAILS", "content": "A.1 ENVIRONMENT\nOur experiments were conducted on one 128-core AMD EPYC 9754 CPU with one NVIDIA GeForce RTX 4090 GPU with 24 GB VRAM. No multi-card training or inference was involved. We implemented the training and inference pipelines for image classification tasks under the Py-Torch Lightning framework, and for the sentiment analysis task with just PyTorch. For evaluations on baseline methods we leverage the SHAP library (Lundberg, 2017), with minor modifications applied to bridge data format differences between Numpy and PyTorch.\nA.2 FLOPS AND MEMORY\nWe used the profile function from the thop library to evaluate the FLOPs for each model during the inference stage. Memory consumption was manually calculated during the training stage based on the model parameters, activations, and intermediate results. Our memory estimations were made under the assumption that the memory was evaluated under a batch size of 1 and uses 32-bit floating point precision (torch.float32).\nA.3 CLASSIFIER\nThe training of all tasks is split into 3 stages. In the first stage parameters of the classifier are inherited from the original base model verbatim, with the exception of AutoGnothi adding additional parameters for the new side branch, initialized with Kaiming initialization.\nWe fine-tune this classifier model on the exact same dataset with the AdamW optimizer, using a learning rate of $10^{-5}$ for 25 epochs, and retain the best checkpoint rated by minimal validation loss. Classification loss is minimum square error with respective to the predicted classes and the ground-truth labels. For AutoGnothi the classes come from the side branch, while all remaining models use the classes from the main branch. We train and evaluate these methods on 1 Nvidia RTX 4090, and use a batch size of 32 samples provided that it fits inside the available GPU memory.\nWe freeze parameters in all stages likewise. As is described in 4.2, AutoGnothi only trains the side branch and all parameters from the original model are frozen. In ViT-shapley (Covert et al.) and Duo pipelines, all parameters are trained, whilst the Froyo pipeline only trains the classification head.\nA.4 SURROGATE\nSurrogate models have the same model architecture as the classifiers, with a different recipe. We load all parameters from the classifier without any changes or additions, further fine-tune the model for 50 epochs, and retain the best checkpoint with minimal validation loss.\nUnlike the classifier, the surrogate model focuses on mimicking the classifier model's behavior under a masked context. Consider the logits $p(y|x_o)$ from the classifier model, where x is the input and y is the corresponding class label. The surrogate model aims at closing in its masked logits distribution, $p(y|x_s)$, with the original distribution:\n$L_{surr}(\u03b2) = E_{x~p(x),s~U(0,d)} [D_{KL} (f(x)||g_B(x_s))],$\nThe mask is selected on an equi-categorical basis. We first pick an integer $n_s = |s|$ at uniform distribution, denoting the number of tokens that shall be masked. $n_s$ mutually exclusive indices are then randomly chosen from the input at uniform distribution. To avoid inhibiting model capabilities, special tokens like the implicit class token in ViT or the [CLS] token applied by the BERT tokenizer are never masked.\nIn order to selectively hide or mask inputs from the model, we apply causal attention masks for both the image models and text models in our experiments. However, it's worth noting that while they may confuse the transformer's attention mechanisms, certain other methods are also capable of concealing these input tokens, primarily zeroing or assigning random values to the said pixels"}, {"title": "A.5 EXPLAINER", "content": "We load explainer model parameters from surrogate model checkpoints, such that all are copied from the surrogate model to the explainer model, except for the last classification head, which is replaced with an explainer head. The explainer head contains 3 MLP layers and a final linear layer, with GeLU (Hendrycks & Gimpel, 2017) activations from in between. For AutoGnothi only the classification head on the side branch is replaced. We train the explainer model for 100 epochs with the AdamW optimizer, using a learning rate of $10^{-5}$, and keep the best checkpoint.\nIn our implementation, we took 2 input images in each mini-batch and generated 16 random masks for each image, resulting in a parallelism of 32 instances per batch. A slight change is applied to the masking algorithm in the explainer model from the surrogate model, in order to reduce variance during gradient descent. Specifically, in addition to generating masks uniform, we follow prior work (Covert et al., 2022) and use the paired sampling trick (Covert & Lee, 2020), pairing each subset s with its complement 1 s. This algorithm equally applies to both image and text classification models.\nThe explainer model is trained to approximate the Shapley value \u03c6o(x, y). Let $g_B(x_s|y)$ be the surrogate values respective to the input x and the class y, masked by the indicator vector s, and"}, {"title": "A.6 DATASETS", "content": "In this section we explain in more detail which datasets are selected and how they are processed for our experiments. Three datasets are used for the image classification task. The ImageNette dataset includes 9, 469 training samples and 3, 925 validation samples for 10 classes. MURA (mus-culoskeletal radiographs) has 36, 808 training samples and 3, 197 validation samples for 2 classes. The Oxford-IIIT Pets dataset contains 5, 879 training samples, 735 validation samples and 735 test samples in 37 classes. For the text classification (sequence classification) task, we use the Yelp Polarity dataset, which originally contains 560, 000 training samples and 38, 000 test samples.\nFor each epoch, image classifiers (ViT) iterate through all available images in either of the train or test dataset. Specifically to during training, images are normalized by the mean value and standard deviation of each corresponding training dataset, before being down-sampled to 224 x 224 pixels. For text classifiers, each of the epoch is trained on exactly 2048 training samples randomly chosen from the dataset, and validated on 256 equally random test samples. This is primarily done to serve our needs in frequent checkpoints for more thorough data analysis such as on CKA or parameter gradients.\nDue to the sheer cost from some metrics on certain tasks, we reduced the size of our test set during evaluation. We selected 1000 test samples for datasets ImageNette, MURA and Yelp Review Polar-ity, and randomly selected 300 samples for the Oxford-IIIT Pets dataset. For each dataset this subset stays the same between different explanation methods and different model sizes. We emphasize that these samples are deliberately independent from the training set to avoid potential bias from the results."}, {"title": "B PROOFS OF THEOREMS", "content": "Here we provide detailed proof for Theorem 1 and Theorem 2, which provide theoretical guarantee for the performance of surrogates and explainers. Our proof follows (Simon & Vincent, 2020) and (Covert et al., 2022), which exhibit similar results for a single data point.\nLemma 1. For a single input x, the expected loss under Eq. (2) is \u03bc-strongly convex, where \u03bc is the minimal eigenvalue of the Hessian of Lsurr(\u03b2).\nProof. The expected loss for a single input x under the new objective function is given by:\n$L_{surr}(\u03b2) = E_{s~U(0,d)} [D_{KL}(f(x)||g_\u03b2(x_s))].$\nThis loss function is convex in \u03b2 because the KL divergence $D_{KL}(f(x)||g_\u03b2(x_s))$ is convex in $g_\u03b2(x_s)$, and $g_\u03b2(x_s)$ is a smooth function of \u03b2. The Hessian of $L_{surr}(\u03b2)$ with respect to \u03b2 is:\n$\\nabla_\u03b2^2 L_{surr}(\u03b2) = E_{s~U(0,d)} [\\nabla_\u03b2^2 D_{KL}(f(x)||g_\u03b2(x_s))].$\nThe convexity of $L_{surr}(\u03b2)$ is determined by the smallest eigenvalue of this Hessian, \u03bc. Since the KL divergence is strictly convex, the minimum eigenvalue is positive, which implies \u03bc-strong convexity. \u03a0\nTheorem 1. Let the surrogate model be trained using gradient descent with step size a for t itera-tions. The expected KL divergence between the original model\u2019s predictions f(x) and the surrogate model\u2019s predictions $g_\u03b2(x_s)$ is upper-bounded by:\n$E_{x~p(x),s~U(0,d)} [D_{KL} (f(x)||g_\u03b2(x_s))] \\leq \\frac{1}{2\u03bc} (1 - \u03bc a)^t (L_{surr}(\u03b2_0) - L_{surr}^*),$\nwhere $\u03b2_0$ is the initial parameter value, and $L_{surr}^*$ is the optimal value during optimization.\nProof. Let the surrogate model be trained using gradient descent with step size a for t iterations. The optimization process for minimizing the expected KL divergence between f(x) and $g_\u03b2(x_s)$ can be written as:\n$\u03b2_{t+1} = \u03b2_t - \u03b1 \\nabla_\u03b2 L_{surr}(\u03b2_t).$\nBecause $L_{surr}(B)$ is \u00b5-strongly convex, we can apply the standard result for gradient descent conver-gence on strongly convex functions, which gives the following bound:\n$L_{surr}(\u03b2_t) - L_{surr}^* \\leq (1 - \u03bc \u03b1)^t (L_{surr}(\u03b2_0) - L_{surr}^*),$\nwhere $L_{surr}^*$ is the optimal value, and $\u03b2_0$ is the initial parameter value.\nSince the KL divergence is bounded by the expected loss, we have:\n$E_{x~p(x),s~U(0,d)} [D_{KL}(f(x)||g_\u03b2(x_s))] \\leq L_{surr}(\u03b2_t).$\nSubstituting the bound on $L_{surr}(\u03b2_t)$, we obtain:\n$E_{x~p(x),s~U(0,d)} [D_{KL} (f(x)||g_\u03b2(x_s))] \\leq \\frac{1}{2\u03bc} (1 - \u03bc \u03b1)^t (L_{surr}(\u03b2_0) - L_{surr}^*).\nLemma 2. For a single input-output pair (x, y), the expected loss under Eq.(3) for the prediction $\u03c6_\u03b8(x, y)$ is \u00b5-strongly convex with $\u00b5 = H_{d\u22121}^{\u22121}$, where $H_{d\u22121}$ is the $(d \u2212 1)$-th harmonic number.\nProof. For an input-output pair (x, y), the expected loss for the prediction $\u03a6 = \u03c6_\u03b8(x, y)$ is defined as\n$L_\u03b8(x, y) = E_{s~p(s)} [(g_\u03b2(x_s|y) - g_\u03b2(x_0|y) - s^T \u03a6)^2]$\n$=\\\u03a6^T E_{s~p(s)}[ss^T] \u03a6 - 2 E_{s~p(s)}[s(g_\u03b2(x_s|y) - g_\u03b2(x_0|y))]\u03a6$\n$+ E_{s~p(s)} [(g_\u03b2(x_s|y) - g_\u03b2(x_0|y))^2].$"}, {"title": "C ADDITIONAL RESULTS FOR AutoGnothi", "content": "In addition to the results on ImageNette included in the main paper, we provide more detailed results on other datasets ranging from image classification tasks to text classification tasks, against a number of baseline explanation methods, with respect to other model sizes in this section."}]}