{"title": "Ten Challenging Problems in Federated Foundation Models", "authors": ["Tao Fan", "Hanlin Gu", "Xuemei Cao", "Chee Seng Chan", "Qian Chen", "Yiqiang Chen", "Yihui Feng", "Yang Gu", "Jiaxiang Geng", "Bing Luo", "Shuoling Liu", "Win Kent Ong", "Chao Ren", "Jiaqi Shao", "Chuan Sun", "Xiaoli Tang", "Hong Xi Tae", "Yongxin Tong", "Shuyue Wei", "Fan Wu", "Wei Xi", "Mingcong Xu", "He Yang", "Xin Yang", "Jiangpeng Yan", "Hao Yu", "Han Yu", "Teng Zhang", "Yifei Zhang", "Xiaojin Zhang", "Zhenzhe Zheng", "Lixin Fan", "Qiang Yang"], "abstract": "Federated Foundation Models (FedFMs) represent a distributed learning paradigm that fuses general competences of foundation models as well as privacy-preserving capabilities of federated learning. This combination allows the large foundation models and the small local domain models at the remote clients to learn from each other in a teacher-student learning setting. This paper provides a comprehensive summary of the ten challenging problems inherent in FedFMs, encompassing foundational theory, utilization of private data, continual learning, unlearning, Non-IID and graph data, bidirectional knowledge transfer, incentive mechanism design, game mechanism design, model watermarking, and efficiency. The ten challenging problems manifest in five pivotal aspects: \"Foundational Theory,\" which aims to establish a coherent and unifying theoretical framework for FedFMs. \"Data,\" addressing the difficulties in leveraging domain-specific knowledge from private data while maintaining privacy; \"Heterogeneity,\" examining variations in data, model, and computational resources across clients; \"Security and Privacy,\" focusing on defenses against malicious attacks and model theft; and \"Efficiency,\" highlighting the need for improvements in training, communication, and parameter efficiency. For each problem, we offer a clear mathematical definition on the objective function, analyze existing methods, and discuss the key challenges and potential solutions. This in-depth exploration aims to advance the theoretical foundations of FedFMs, guide practical implementations, and inspire future research to overcome these obstacles, thereby enabling the robust, efficient, and privacy-preserving FedFMs in various real-world applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Foundation Models (FMs) [1] have emerged as a groundbreaking force in the realm of artificial intelligence. Renowned FMs, including GPT-4 and LLaMa, have exhibited an extraordinary ability to understand context and nuances, enabling them to skillfully handle a wide range of tasks across diverse fields such as natural language processing (NLP) and computer vision (CV). On the other hand, Domain Models (DMs), often deployed remotely on edge devices, are trained locally using private data that cannot be shared with the FMs. However, DMs also have limitations stemming from their restricted generalization capacities. The dilemma raises a question: How can we effectively harness the power of FMs for domain-specific knowledge while simultaneously ensure that DMs possess adequate generalization capabilities and adhere to privacy protection requirements?\nOne promising solution to integrate capabilities of FMs and DMs is through Federated Foundation Models (FedFMs) [2, 3, 4]. This paper provides a comprehensive summary of definitions and challenges in this new machine learning paradigm. Specifically, as depicted in Fig. 1, FedFMs is defined as a distributed learning framework which includes at least one pretrained FM and a multitude of DMs. Federated learning (FL) [5] methods are adopted in FedFMs to ensure that data privacy are well-respected during the training of FedFMs, especially, for healthcare, finance and IoTs applications.\nDespite their promising applications, FedFMs face significant challenges for widespread adoption. In this paper, we delve into the intrinsic challenges of FedFMs from five key aspects: \"Foundational Theory\u201d, \u201cData\u201d, \u201cHeterogeneity\u201d, \"Security and Privacy\u201d and \u201cEfficiency\".\nFoundational Theory addresses the optimization objectives at the highest level in FedFMs. The theory must address issues such as how to harmonize privacy, utility, and efficiency within the framework of FedFMs [108], guiding algorithm design (Problem 1).\nData, challenges arise from data distribution across organizations or individuals. FedFMs must adapt to unique private data characteristics, balancing data utility and privacy risks [3] (Problem 2). Data also varies over time; thus, FedFMs must adapt to new task requirements without forgetting previously learned knowledge [109] (Problem 3). Additionally, FedFMs need mechanisms for \"unlearning\" when participants exit [110] (Problem 4).\nHeterogeneity in FedFMs includes data, model, and computational aspects. Non-IID data variations across clients challenge effective generalization and pattern capture, necessitating advanced methodologies [47] (Problem 5). Additionally, collaboration between large FMs and small domain models introduces bidirectional knowledge transfer challenges [64] (Problem 6). Lastly, evaluating and fairly rewarding participants' contributions remain critical [74] (Problem 7).\nSecurity and Privacy are vital for FedFMs' practical application. FedFMs face two primary security threats: privacy attacks and local data/model theft. Addressing these requires both defensive technical measures and game-theoretic mechanisms to deter attackers [111] (Problem 8). To combat model theft, leveraging model watermarking for full life-cycle tracking, auditing, and detection is crucial [92] (Problem 9).\nEfficiency refers to optimized training and inference times and computational resources. There are still significant improvements needed in three key areas to ensure efficiency: training efficiency, communication efficiency, and parameter efficiency [112, 106, 107] (Problem 10).\nThis paper critically assesses recent advancements in FedFMs from the aforementioned five aspects and identifies the ten challenging problems, summarized in TABLE I."}, {"title": "II. PROBLEM 1: HOW TO ESTABLISH A FOUNDATIONAL THEORY FOR FEDFMs?", "content": "Although FedFMs hold great potential, they currently lack a solid theoretical foundation. This section defines the foundational theory of FedFMs as a multi-dimensional trade-off framework, focusing on balancing various goals such as privacy, utility, efficiency, interpretability, fairness, robustness and aggregation methods. The aim is to emphasize the complex interactions between these dimensions and seek to achieve an optimal equilibrium that accommodates the diverse requirements of FedFMs.\nThe foundational theory of FedFMs involves three key trade-offs. First, the trade-off between privacy, utility, and efficiency. It provides a multi-objective optimization framework to ensure that improvements in one dimension do not compromise others, enabling the simultaneous optimization of all objectives in practical applications. Second, the trade-off between global and personalized model performance, which explores how to optimize the global model while maintaining flexibility for client-specific customizations. Finally, the trade-off between different types of FedFMs, which examines the differences between open-source (white-box) and closed-source (black-box) models, aiming to optimize the aggregation process for different model types and deployment scenarios.\nProblem Formulation. Consider K clients, each possessing private data $D_k$, who participate in the FedFMs training process. Let $w_s$ represent the model of the server, and $w_c = {w_1,...,w_K}$ denote the set of models of K clients. We define $w_g = {w_s, w_c}$. We use $w$ as either $w_s, w_c$ or $w_g$.\nThe foundation theory in FedFMs can be regarded as a multiobjective optimization [119, 108], which aims to achieve a balance among privacy loss, utility loss, and efficiency loss as:\n$\\min_{w} (l_u (w), l_p (w), l_e(w))$,\ns.t. $l_u(w) + l_p(w) + l_e(w) > 0$\nwhere $l_i(w) = F (l_{i,1}(w, D_1), ..., l_{i,K}(w, D_K))$ for $i \\in {u,p,e}$, $F$ is the aggregation mechanism such as FedAvg [112]. $l_{u,k}(w, D_k), l_{p,k}(w, D_k)$, and $l_{e,k}(w, D_k)$ denote the loss functions corresponding to the privacy, utility, and efficiency of client $k$ for the data $D_k$ respectively. Note that the sum of loss in Eq. (1) for privacy, utility and efficiency is larger than zero, denoting the \"no free lunch\" aspect of the tradeoff [108].\nIn previous FL research, multi-objective optimization problems are often represented as tasks where the \"optimal balance point\" lies on the Pareto front or surface.\nFirst, we examine the trade-offs between privacy and other dimensions. In FL, numerous studies have addressed the privacy-utility trade off [108, 119]. For instance, Zhang et al. [108] proposed the NFL theorem within the FL framework, indicating an inherent trade-off between privacy and utility, expressed as:\n$C_1 l_p + C_2 l_u$,\nwhere $l_p$ represents the amount of privacy leakage, and $l_u$ represents utility loss. Zhang et al. [6] further applied the NFL framework to LLMs. Unlike the privacy leakage defined by gradients exchanged between clients and servers in [108], this framework focuses more on how much an attacker can infer the original data through protected embeddings. The core expression is:\n$\\frac{C_2}{C_1} l_p + l_u \\geq C_2 \\cdot TV(P || \\bar{P})$,\nwhere $TV(P || \\bar{P})$ represents the total variation distance between the distribution of the undistorted embedding and the distribution of the embedding independent of the client's prompt embedding. $TV(P || \\bar{P})$ is a constant that has nothing to do with the protection mechanism. Eq. (3) indicates that when interacting with an LLM using protected prompts, it is impossible to minimize both privacy leakage and utility loss simultaneously.\nSeveral studies have explored privacy-utility trade-offs in FedFMs. In FedFMs, privacy protection can be categorized into two scenarios: 1) absolute privacy protection, where no leakage is allowed, and 2) privacy protection with cost, where only specific data (e.g., names) is protected, focusing on key information. Common methods for absolute privacy protection include homomorphic encryption [120] and differential privacy [8]. For privacy with cost, techniques like knowledge distillation [121] and tunable soft prompts [7] can be applied.\nGiven the large number of parameters in FMs, optimizing efficiency without sacrificing performance is a critical challenge in FedFMs foundational theory. To this end, several methods [10, 122, 123] have been proposed to enhance the efficiency of FedFMs systems. For instance, Kuang et al. [9] introduced a federated parameter-efficient fine-tuning (PEFT) framework. Additionally, Yue et al. [123] and Su et al. [122] focused on selecting key parameters or subsets of clients to minimize communication costs, and Wu et al. [10] leverages model compression for FedFMs.\nIn FedFMs, addressing system heterogeneity is crucial, as variations in data, resources, and personalization needs affect both global model generalization and client fairness. To tackle this, yiyuan yang et al. [49] employed global and local adapters to handle distribution shifts and personalized needs.\nFMs can be categorized as closed-source (black-box) and open-source (white-box) based on parameter accessibility and usage rights. Closed-source FMs (e.g., GPT-4 [124]) are proprietary, with their architecture, weights, and training processes kept confidential. These models are typically accessible only via APIs, limiting users' control over internal workings. In contrast, open-source FMs (e.g., LLaMA [125]) provide full access to architecture, weights, and training methods, enabling free access, modification, and redistribution by the community.\nAggregation plays a crucial role in training and fine-tuning FedFMs, requiring efficient knowledge integration while maintaining data privacy. However, aggregation methods differ between closed-source and open-source FMs, impacting how organizations deploy, train, and share models. These differences involve trade-offs in complexity, resource and feasibility.\nIn FedFMs, aggregation methods vary between open-source and closed-source models. For white-box FedFMs, where model weights and architectures are accessible [11, 12], traditional parameter-based aggregation methods, such as FedAvg [112], can be used. These methods allow for customized aggregation at different layers based on specific needs [11, 126]. However, implementing open-source FedFMs requires significant AI expertise, computational resources, and infrastructure, which may not be feasible for all organizations.\nIn contrast, black-box FedFMs, where model parameters are inaccessible [13, 14, 127], focus on prompt-level aggregation. This involves aggregating high-level representations or prompt patterns [13, 14, 127], rather than direct model parameters. This \"prompt-centric\u201d approach reduces technical barriers and operational overhead, making it accessible to organizations with limited Al expertise or resources, though it offers less customization than weight-based methods [128]."}, {"title": "III. PROBLEM 2: HOW TO UTILIZE PRIVATE DATA IN FEDFMS?", "content": "Private data utilization in FedFMs enhances FMs by leveraging local private data while preserving privacy. In real-world scenarios, valuable private data is often isolated within organizations due to privacy regulations. For example, medical institutions maintain separate patient data repositories, creating \u201cdata islands\" that limit training diversity. Traditional centralized training is impractical or restricted due to privacy concerns, which particularly affects FMs that rely on extensive, diverse datasets for robust performance. FedFMs overcome these challenges by allowing organizations to train models locally, sharing only model updates instead of raw data with a central server for aggregation. This approach effectively bridges data islands while ensuring strong privacy protections, addressing both data scarcity and regulatory constraints in FM development.\nProblem Formulation. The utilization of private data in FedFMs involves clients contributing their private data for the model's training while preserving data privacy [112, 5, 129]. Consider a scenario where a central aggregation server and K clients collaborate to learn a global FM. Each client is in a data-isolated environment, meaning that data is not shared among them. Let the k-th client possess data $D_k$. The optimization problem for obtaining the model $w_g$ with respect to data utilization can be formulated as:\n$\\min_{w_g} l_u (w_g)$\ns.t. $l_{p,k}(w_g) < \\delta_k$, for $k = 1,...,K$\nwhere $l_u(w_g) = F (l_{u,1}(w_1), ..., l_{u,K}(w_K))$, $l_{u,k}()$ represents the loss function related to data utilization for client $k$ and $F$ is the aggregation mechanism, such as FedAvg [112]. Additionally, $l_{p,k}(w)$ denotes the privacy leakage of client k with respect to the model $w$, and it must be within a predefined threshold $\\delta_k$.\nExisting methods for utilizing private data in FedFMs fall into two methods: data-centric and model-centric. Data-centric methods enhance distributed private data utility while ensuring privacy, addressing data quality, efficiency, and performance. In contrast, model-centric methods adapt FM architectures and training paradigms to effectively leverage private data, focusing on instruction tuning and model splitting."}, {"title": "IV. PROBLEM 3: HOW TO DESIGN CONTINUAL LEARNING IN FEDFMs?", "content": "Existing FL setups assume static models and data, neglecting the dynamic nature of real-world data collection. FedFMs, with their numerous parameters and complex training, struggle to meet real-time requirements without retraining. As the number of clients and data samples grows, FedFMs face scalability issues, potentially increasing communication overhead and slowing convergence. To address these, Continual Learning (CL) can be integrated into FedFMs for incremental updates without full retraining. A crucial overlooked issue in FCL is the interaction between spatial and temporal heterogeneity, leading to spatial-temporal catastrophic forgetting (ST-CF) [109]. Catastrophic Forgetting (CF) describes a deep model forgetting previous tasks after learning new ones, reducing accuracy [137, 138]. In FCL, clients experience temporal CF as they continually learn new tasks, while non-IID data causes spatial CF in the global model, decreasing performance on local test sets. Spatial forgetting interacts with temporal forgetting as clients use the global model for subsequent tasks [109].\nThe goals of FCL are threefold: (1) Continuously update and optimize local models to adapt to changing local data. (2) Enhance client knowledge through interaction with other clients via FL. (3) Apply knowledge gained from other clients for further learning.\nProblem Formulation. This section extends the traditional FL framework to FCL to address the challenges posed by strong spatial-temporal data heterogeneity. Regarding spatial heterogeneity [27, 28, 29]. K clients have distinct data denoted as $D_k$. For temporal heterogeneity, in the federated system, clients are involved in learning m tasks. The corresponding datasets for each client k are {$D_k^1$,...,$D_k^m$}.\nWhen all clients have collaboratively completed learning task 1,..., T and are about to embark on learning the new task T + 1, the goal of federated continual learning encompasses two key aspects. Firstly, it is to effectively learn the new task T+1. Secondly, it is to ensure that knowledge of the previous tasks 1,..., T is not forgotten. Specifically, the optimization objective is formulated as:\n$\\min_{w_g} F (l_{u,1}(w_g, {D_1^t\\}_{t=1}^{T}),\u00b7\u00b7\u00b7, l_{u,K}(w_g, {D_K^t\\}_{t=1}^{T}),\\newline l_{u,1}(w_g, D_1^{T+1}), \u00b7\u00b7\u00b7, l_{u,K}(w_g, D_K^{T+1}))$"}, {"title": "V. PROBLEM 4: HOW TO DESIGN UNLEARNING IN FEDFMS?", "content": "Data is crucial for intelligent development across industries, but data leakage risks during circulation and sharing have raised privacy concerns. Global governments and legislators have imposed stringent data privacy regulations, such as GDPR and CCPA, mandating digital service providers to grant users the \"Right to Be Forgotten\" (RTBF) [143] and establish mechanisms for data removal from models [38].\nThis opt-out mechanism, known as machine unlearning (MU) [37], is vital in FedFMs. Despite FL not requiring direct user data access, it retains implicit information from training, posing privacy risks from model inversion attacks [144]. Additionally, training data may include unauthorized or biased content, raising ethical and copyright issues [145]. Addressing these concerns is essential for responsible FedFM deployment. Due to the high retraining cost, developing a federated unlearning (FU) technique that aligns with legal frameworks is a key challenge for FedFM advancement.\nThe unlearning process in FedFMs involves identifying and flagging clients and data records associated with data deletion requests, then applying active forgetting techniques to remove specified data contributions while preserving model integrity.\nProblem Formulation. The objective of FU is to mitigate the impact of a specific subset of data, denoted as $\\bar{D_j}$, belonging to the target client j [33, 34, 37].\nIn the context of FU, we need to construct a new model $w_{un}$. To do this, we first define a reference model $w_{re}$. The model $w_{re}$ is obtained by minimizing a weighted sum of loss functions. Here, F is the aggregation mechanishm such as FedAvg [112]. The model $w_{re}$ is given by:\n$w_{re} = arg \\min_{w} F (l_1(w, D_1), ... ,l_{j-1}(w, D_{j-1}), \\newline l_j (w, D_j - \\bar{D_j}),..., l_k (w, D_K - \\bar{D_K}))$"}, {"title": "VI. PROBLEM 5: HOW TO ADDRESS NON-IID ISSUES AND GRAPH DATA IN FEDFMS", "content": "The Non-IID issues stem from heterogeneous data distributions across clients, where each client may have distinct feature sets, label distributions, and sample sizes. These variations pose significant challenges to global model training, particularly in FedFMs [4, 151, 152]. The primary types of Non-IID data are outlined as follows: 1) Feature distribution Skew, where clients have distinct feature distributions, leading to data-specific patterns; e.g., hospitals serving different age groups. 2) Label distribution Skew, where differing label distributions hinder model generalization, as seen in transaction classification with varying data types; 3) Quantity Skew, where data volume disparities across clients bias aggregation, reducing model representativeness. 4) Temporal Skew, when clients collect data at different times, complicating model generalization, such as regional weather sensors recording at non-synchronized times.\nBeyond these static skews, dynamic shifts like concept drift (same label, different features) and concept shift (same features, different label) introduce additional complexity. Concept drift occurs when $P_k(x|y)$ changes while P(y) remains consistent, as seen in regional variations of \"winter\" images. Concept shift arises when $P_k(y|x)$ varies despite a stable P(x), such as differing labels for similar behaviors across cultures.\nThe federated graph of models in FedFMs presents two key perspectives: FedFMs with Graphs and Graphs of FedFMs. In the former, each client's data forms graph structures capturing internal and external dependencies, requiring models to learn both intra- and inter-graph relationships [153]. In the latter, clients act as nodes in a federated network, with edges representing relationships like geographic proximity or data similarity [154]. This Internet of Models enables selective information sharing under Non-IID conditions but demands advanced aggregation strategies to ensure model consistency.\nProblem Formulation. There are two different settings in this section involving a server and multiple clients: Non-IID issues and FedFMs with Graphs.\nSetting 1. Non-IID issues. In the FedFMs framework [153, 154], the optimization problem with Non-IID data across K clients is formulated as follows:\n$\\min_{w_g} F (l_{u, 1} (w_g, D_1),..., l_{u,K} (w_g, D_K))$\nwhere dist($D_{k_1}$, $D_{k_2}$) > 0 $\\forall k_1 \\neq k_2$\nwhere $l_{u,k}()$ represents the utility loss function of client k, F is the aggregation mechanishm and dist(,) represent the distance of two dataset.\nSetting 2. FedFMs with Graphs. In this setting, each client k possesses a local graph $G_k = (V_k, E_k)$. The set $V_k$ contains the nodes, and $E_k$ represents the set of edges within the local graph. K clients consist the whole graph as G = (V, E), where vertices are K nodes [155]. The objective of learning a federated graph neural network (GNN) model $w_g$ is given by:\n$\\min_{w_g} F (l_{u, 1} (w_g, G_1, G), ..., l_{u,K} (w_g, G_K, G))$\ns.t. $l_{p,k}(w_g, G_k, G) < \\delta_k$, for $k = 1,...,K$"}, {"title": "VII. PROBLEM 6: HOW TO ACHIEVE THE BIDIRECTIONAL KNOWLEDGE TRANSFER BETWEEN FMS AND DMS IN FEDFMS?", "content": "Bidirectional knowledge transfer within FedFMs denotes the reciprocal exchange and adaptation of knowledge between the server-hosted FMs and the client-owned DMs. This dynamic interplay entails not only the transfer of knowledge from the FM to the DMs to bolster their capacities but also the converse flow of domain-specific insights from the DMs back to the FMs, thereby enriching its comprehension and performance across a spectrum of domains. This interaction between the server-based FMs and client-based small DMs is analogous to the teaching and learning interactions between a teacher and students; knowledge flows both ways while privacy is preserved.\nThe framework of bidirectional knowledge transfer can be delineated into three distinct settings [3, 66, 68], as illustrated in Fig. 7. Setting 1 aims to facilitate the transfer and adaptation of knowledge from the server's FM to the clients' DMs. Setting 2 focuses on harnessing the domain-specific expertise of clients to refine the server's FM. Setting 3 strives for the co-optimization of both the FM and the DMs, fostering a synergistic enhancement of their respective capabilities.\nProblem Formulation. There are three different settings in this section involving a server and multiple clients. Here, $w_s$ represents the server's FM, $w_k$ represents the DM of client k, $D_k$ is the local private dataset of client k, and $D_p$ is the public data available at the server.\nSetting 1: Optimizing clients' DMs using server's FM and local data. The goal is to optimize the DMs ($w_k$) of the client k by taking advantage of the knowledge of the server's model $w_s$ and the local private dataset ($D_k$) of each client. The main objective is formulated as:\n$\\min_{w_1,...,w_K} F(l_{u, 1} (w_1/w_s, D_1),\u00b7\u00b7\u00b7, l_{u,k}(w_k/w_s, D_K))$,\nwhere $l_{u,k}(w_k/w_s, D_k)$ represents the utility loss function of client k.\nSetting 2: Optimizing the server's FM using clients' domain-specific knowledge and public data. This setting aims to optimize the server's FM ($w_s$) by leveraging the domain-specific knowledge of the clients' models ($w_k$, where k \u2208 {1,\u2026\u2026, K}) and the public data ($D_p$). The main objective is given by:\n$\\min_{w_s} F (l_{u,1} (w_s/w_1, D_p),..., l_{u,k} (w_s/w_K, D_p))$,"}, {"title": "VIII. PROBLEM 7: HOW TO DESIGN INCENTIVE MECHANISMS THROUGH CONTRIBUTION EVALUATION IN FEDFMS?", "content": "As the scaling law shows, the FMs' performance relies on high-quality data and substantial computational resources, which need appropriate compensation to be contributed in the real-world. Thus, it is crucial for FedFMs to attract data providers and incentive them to contribute high-quality data or other resources. The contribution evaluation for FedFMs aims to enable the client's participation by measuring the importance of datasets or computational resources. The incentive mechanisms for FedFMs aim to ensure the data providers' active and sustained participation by providing them with fair rewards. Contribution evaluation in FedFMs refers to measuring the importance or influence of heterogeneous datasets, model quality, or computational resources, which is vital for identifying the free riders and outliers within the FedFMs ecosystem and can serve as a prerequisite for the incentive mechanisms in FedFMs.\nProblem Formulation. Consider a scenario with K clients, where each client k has a private dataset $D_k$. A contribution evaluation mechanism C is designed to compute the contribution C(w, Dk) for client k by introducing data Dk in FedFMs. Assume an Oracle knows the true contribution $C_k^*$ for each client k. The objective of contribution evaluation is to look for the contribution evaluation mechanishm C such that this evaluation is accurate [73, 74], formulated as:\n$\\min \\sum_{k=1}^{K} ||C(w_s, D_k) - C_k^*||$"}, {"title": "IX. PROBLEM 8: HOW TO DESIGN GAME MECHANISMS IN FEDFMS?", "content": "Before introducing game mechanisms in FedFMs, it is essential to clearly define the roles of the various participants. FedFMs participants are often categorized into three types: honest, semi-honest and malicious. Honest participants are the most ideal, as they strictly adhere to the established FedFMs protocols and follow the prescribed procedures for each step. Semi-honest participants, on the other hand, do not fully comply with the protocols. They might discreetly collect private data without violating the FedFMs protocol to infer other participants' private information, but they do not actively launch attacks or collude with other participants to undermine the protocol. They pose a significant threat to the privacy of the protocol because they are not actively attacking and are difficult to detect. Malicious participants are easily compromised by attackers or are themselves attackers disguised as legitimate FedFMs participants, to expose sensitive private data. Semi-honest attacks are the most common.\nProblem Formulation Assume K clients participate in FedFMs, and let $A_k$ represent the set of actions available to participant k, where $a_k \\in A_k$ is the action chosen by participant k. These actions may include contributing data, reporting malicious activity, or abstaining from malicious behavior. The utility function $U_k(a_k, a_{-k})$ for participant k depends on their own action $a_k$ and the collective actions of all other participants, denoted by $a_{-k}$. To capture the interplay between utility, privacy, and efficiency, the utility function is defined as follows:\n$U_k(a_k, a_{-k}) = r_k-c_k(a_k)+\\lambda_k R_k(a_k)-\\alpha P_k(a_k)-\\beta E_k(a_k)$"}, {"title": "X. PROBLEM 9: HOW TO DESIGN MODEL WATERMARKING IN FEDFMS?", "content": "Model watermarking in FedFMs plays a crucial role in safeguarding intellectual property (IP), helping to ensure that FedFMs ownership is protected and preventing unauthorized use by individual clients during local training and model theft by embedding unique watermark for each client.\nWatermarking in FedFMs refers to the process by which both clients and the central server collaboratively embed a watermark into a shared model to assert ownership and ensure traceability. Given that FedFMs involve multiple clients contributing to a jointly trained model while a central server orchestrates training, watermarking serves as a mechanism for both clients and the server to embed unique identifiers. This enables clients to verify their contributions and claim ownership in case of disputes, while also allowing the server to track model integrity and detect unauthorized modifications.\nProblem Formulation. There are two categories of model watermarking: White-Box and Black-Box.\nWhite-Box. A binary watermark B\u2208 {0,1} is embedded directly into the model parameters 0 (e.g., batch normalization layers) during training by adding specialized regularization terms to the loss function [92]. During verification Vw, an extractor E retrieves the watermark B from 0. The watermark is verified if the Hamming distance H(B,B) is below a predefined threshold ew as illustrated in Fig. 10.\nBlack-Box. Nt trigger-based samples T = {(x, y)}=1 are embedded into the model @ using a trigger objective during training [92]. In the verification step (V6), trigger inputs Xt are fed into the model M, and the outputs are compared to the corresponding labels yt to compute the trigger accuracy At. The watermark is confirmed if At exceeds a predefined threshold eb as illustrated in Fig. 10.\nThe watermark embedding process for client k minimizes a loss comprising the main task on Dk and two regularization terms, $l_{u,Tx} (w_s)$ and $l_{u, B_k} (w_s)$, to embed trigger samples Tk and feature-based watermarks Bk in terms of the server's global model ws. Given the global model w at communication round t, the objective formulation is defined as:\n$\\min F_{k=0,1,..., K}a l_{u,k}(w_s) + (1 - a)l_{m,k}(w_s)$"}, {"title": "XI. PROBLEM 10: HOW TO IMPROVE THE EFFICIENCY IN FEDFMS?", "content": "Borrowing the definitions in [201], the efficiency of FedFMs corresponds to the three fundamental aspects of computation, communication, and storage. Thus, the efficiency of FedFMs refers to its capability to minimize the weighted sum of these three factors, facilitating scalable and high-performance training in resource-constrained environments [202, 4]. Challenges such as high update frequencies, large model sizes, and network unreliability hinder scalability, while the resource demands of complex FMs exacerbate the strain on devices. Therefore, improving computation, communication, and storage efficiency is vital for enabling FedFMs' deployment in sectors like healthcare, finance, and IoT.\nProblem Formulation. This section considers three types of efficiency in FedFMs [112, 106, 107]: the computation loss, communication loss, and model size for model w at time slot t as $l_c^t(w)$, $l_{cm}^t(w)$, and $l_s^t(w)$, respectively. The objective is to minimize the overall efficiency consumption, expressed as:\n$\\min \\sum_{t=0}^{T} \\alpha \\cdot l_c^t(w) + \\beta \\cdot l_{cm}^t(w) + \\gamma \\cdot l_s^t(w)$\ns.t. $l_{p,k}(w) < \\delta_k$\nwhere T is training episodes; $\\alpha, \\beta, \\gamma$ are the respective weights reflecting the relative importance of each efficiency component and meet $\\alpha + \\beta + \\gamma = 1$. This formulation ensures a balanced trade-off among the three efficiency dimensions, optimizing FedFMs for real-world, resource-constrained environments."}, {"title": "XII. SUMMARY", "content": "This paper provides a broad and quantitative examination of ten challenging problems inherent in FedFMs, encompassing issues of foundational theory, utilization of private data, continual learning, unlearning, Non-IID and graph data, bidirectional knowledge transfer, incentive mechanism design, game mechanism design, model watermarking, and efficiency. In this section, we unify the objective functions of these ten challenging problems in FedFMs into the following equation:\n$\\min_{\\omega_g=(\\omega_{s},{\\omega_{k}\\}_{k=1}^{K}),\\{a_k\\}_{k=1}^{K},F,C} (\\alpha_1 l_{u,k} (\\omega_s, \\omega_k, a_k, C, {D_k\\}_{k=1}^{K}, G) \\newline + \\alpha_2 l_{e,k}(\\omega_s, \\omega_k, a_k, C, {D_k\\}_{k=1}^{K}, G) + \\alpha_3 l_{m,k}(\\omega_s, \\omega_k, a_k, C, {D_k\\}_{k=1}^{K}, G) \\newline + \\alpha_4 l_{c,k}(\\omega_s, \\omega_k, a_k, C, {D_k\\}_{k=1}^{K}, G) + \\alpha_5 l_{p,k}(\\omega_s, \\omega_k, a_k, C, {D_k\\}_{k=1}^{K}, G))$\\ns.t. $l_u(w_g)+l_p(w_g) + l_e(w_g) > 0$"}]}