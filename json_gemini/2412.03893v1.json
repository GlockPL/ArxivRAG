{"title": "Dual-Branch Subpixel-Guided Network for Hyperspectral Image Classification", "authors": ["Zhu Han", "Jin Yang", "Lianru Gao", "Zhiqiang Zeng", "Bing Zhang", "Jocelyn Chanussot"], "abstract": "Deep learning (DL) has been widely applied into hyperspectral image (HSI) classification owing to its promising feature learning and representation capabilities. However, limited by the spatial resolution of sensors, existing DL-based classification approaches mainly focus on pixel-level spectral and spatial information extraction through complex network architecture design, while ignoring the existence of mixed pixels in actual scenarios. To tackle this difficulty, we propose a novel dual-branch subpixel-guided network for HSI classification, called DSNet, which automatically integrates subpixel information and convolutional class features by introducing a deep autoencoder unmixing architecture to enhance classification performance. DSNet is capable of fully considering physically nonlinear properties within subpixels and adaptively generating diagnostic abundances in an unsupervised manner to achieve more reliable decision boundaries for class label distributions. The subpixel fusion module is designed to ensure high-quality information fusion across pixel and subpixel features, further promoting stable joint classification. Experimental results on three benchmark datasets demonstrate the effectiveness and superiority of DSNet compared with state-of-the-art DL-based HSI classification approaches. The codes will be available at https://github.com/hanzhu97702/DSNet, contributing to the remote sensing community.", "sections": [{"title": "I. INTRODUCTION", "content": "Hyperspectral imaging is a technique for exploring the spectral properties of ground targets with the fine resolution of scene radiance [1]. Owing to its rich spectral characteristics, each pixel in hyperspectral images (HSIs) can be regarded as an approximately continuous spectral curve, enabling various materials to be effectively identified and discriminated [2], [3]. Benefited from inherent values within the cubic data architecture, HSI has been widely utilized in many fields, including precision agriculture [4], [5], urban planing [6], [7], target detection [8], [9] and mineral monitoring [10], [11]. As one of the primary HSI processing technologies, HSI classification is an essential foundation and aims at generating high-precision classification maps that reflect the ground distribution information.\nIn the past few decades, numerous traditional machine learning methods have be proposed for HSI classification, such as the support vector machine (SVM) [12], random forest (RF) [13], and the Bayesian model [14]. Nevertheless, these traditional methods cannot consider sufficient spectral-spatial features to establish the relationship between pixels in the spatial dimension. A large number of advanced dimensionality reduction and feature extraction methods have been further introduced to improve the classification accuracy, including subspace learning [15], [16], principal component analysis (PCA) [17], [18], morphological profiles [19]\u2013[21], Gabor filters [22], [23], ensemble learning [24] and superpixel-based analysis [25], [26]. However, the extracted features by these above-mentioned approaches rely on manual design by experts and tend to be shallow, making the recognition and robustness in complex scenarios unsatisfactory.\nWith the explosive growth of computer vision and artificial intelligence theory, deep learning (DL) solves this bottleneck and can automatically extract robust and high-level representations from a data-driven perspective [27]\u2013[31]. The most prominent DL-based HSI classification methods are convolutional neural networks (CNNs) [32]\u2013[34], graph convolutional networks (GCNs) [35]\u2013[37], recurrent neural networks (RNNs) [38]\u2013[40] and Transformers [41], [42], which explores the spectral and spatial feature representations from different views. Specifically, CNNs aim at extracting spatial contextual representations by employing two-dimensional (2-D) or three-dimensional (3-D) convolution kernels, whereas the fixed receptive field limits the model learning ability to retrieve large-scale contextual information. GCNs and RNNs can capture topological associations among samples and model spectral sequences in the HSI, respectively. Following the adoption of the self-attention mechanism, Transformer-based methods can fully learn global spectral-spatial structure information, but the quadratic complexity limits its computational efficiency. Furthermore, recent studies have demonstrated that the hybrid architecture of these DL models can solve the imperfection of feature exploration and greatly improve the performance of HSI classification in complex scenes [43]\u2013[45]. Although existing DL-based approaches have achieved great success in the field of HSI classification, these methods usually treat each pixel as a pure spectrum for classifier training and ignore the existence of mixed pixels owing to the impact of low spatial resolution in actual scenarios. In general, mixed pixels reflect the spectral mixing characteristics of different ground objects in the scene, but existing studies fail to consider the subpixel information that characterizes the spatial distribution of ground objects within a single pixel, which inevitably leads to misclassification by the classifier. In addition, subpixel in the HSI usually has certain physical mixing constraints [46], [47], thus how to naturally integrate information provided by discriminative classifier and subpixel information to improve classification performance still faces great challenges.\nThe rise of hyperspectral unmixing (HU) technology supports the realization of intelligent subpixel interpretation. HU deals with this challenging issue by separating the mixed pixel into a set of endmember signatures and their corresponding fractional abundances [48], [49]. Depending on geometry, spectral reflection and refraction characteristics, HU approaches mainly rely on two mixing assumptions to describe photo interactions underlying the observations: linear mixing model (LMM) and nonlinear mixing model (NLMM) [50]. LMM considers the macroscopic mixing scale and assumes the linear combination of different endmember spectra in the scene [51]. NLMM introduces the multiple light scatterings in the microscopic scale and achieves a suitable approximation to the actual mixing [52], [53]. Nevertheless, NLMM usually needs a large number of reliable metrics and prior knowledge, which is difficult to be calculated in real scenarios [54]\u2013[56]. In recent decades, DL-based unmixing methods have gradually become the mainstream for handling mixed pixels in the HSI. As an important representative of DL, autoencoder (AE) can achieve blind HU by imposing certain abundance and endmember constraints with an encoder-decoder architecture, which has been proven to be effective in the field of HU [57]\u2013[64]. Nevertheless, it should be noted that existing HSI classification and unmixing methods based on DL are conventionally performed independently, lacking a robust and effective connection. For some hyperspectral satellite data with low spatial resolution, such as Airborne Visible/Infrared Imaging Spectrometer (AVIRIS), Environmental Mapping and Analysis Program (EnMAP) or Gaofen-5 (GF-5) satellite, there are a large number of mixed pixels in the scene, which inevitably degrades the subsequent per-pixel fine classification processing [65]\u2013[67]. Only relying on increasing the complexity of DL-based HSI classification models to enhance the modeling ability cannot be applied in actual emergency scenarios due to high training time and model parameters. Although some fusion-based methods have been proposed as a prepossessing technique to promote classification performance, the prior knowledge of other remote sensing data sources is required to assist model learning and the adopted deterministic degradation modeling cannot handle realistically-blurred HSIs [68]\u2013[70], which further affects the application of subsequent downstream tasks. Potential subpixel information is often ignored and not fully mined into existing DL-based classification approaches. Therefore, it is necessary to explore an united and intelligent data-driven framework for both HSI classification and unmixing tasks rather than being implemented step-wise. Furthermore, introducing additional subpixel information into DL models usually leads to information redundancy and underutilization, so exploring the efficient fusion strategy during the joint optimization procedure can facilitate better interpretation of class label distribution.\nMotivated by the above concerns, we propose a dual-branch subpixel-guided network for HSI classification, called DSNet, in which a deep AE unmixing architecture with physically nonlinear properties is incorporated into the CNN-based classifier network, to effectively fuse subpixel and class-wise representations of HSI to enhance classification performance. Fig. 1 briefly illustrates the similarities and differences between the existing HSI classification method and the proposed subpixel-guided HSI classification method using DL. Different from previous DL-based HSI classification studies that generally focus on pixel-level spectral and spatial information extraction through complex network architecture design, this is the first attempt to adopt the reconstruction (RE) loss and cross-entropy (CE) loss for DSNet training, to collaboratively explore latent correlations from subpixel and pixel information in the HSI. The proposed subpixel-guided HSI classification method can mine inherent spatial distribution of different materials from their corresponding spectral attributes to improve the discrimination ability of the classifier. For this purpose, the subpixel fusion module is designed to aggregate diagnostic abundances and class-wise representations to further guide the classifier network toward a more accurate classification direction. In brief, the major contributions of this paper can be summarized as follows.\n\u2022 We propose a subpixel-guided deep network by introducing a deep AE unmixing architecture for HSI classification tasks, called DSNet. DSNet is capable of estimating subpixel-level abundances and generating discriminative class-wise representations more automatically and efficiently, thereby yielding a significant classification performance improvement.\n\u2022 The deep AE unmixing architecture considers a general unmixing modeling consisting of a linear mixture component and a physically nonlinear mixture component, which provides a complete and physically meaningful subpixel prior information for the CNN-based classifier network.\n\u2022 The subpixel fusion module is developed to ensure high-quality information fusion across pixel and subpixel features, which further achieves stable joint classification and facilitates a better separation of different classes.The remaining part of this paper is organized as follows. Section II elaborates the implementation details of our DSNet. Section III presents the extensive experiments and analyses on three HSI benchmark datasets. Finally, conclusions are drawn and summarized in Section IV."}, {"title": "II. PROBLEM FORMULATION AND METHOD", "content": "In this section, we start with a review of the existing AE-based unmixing approaches, and then provide a detailed description of the proposed DSNet method.\nAs a simple and commonly used spectral mixing model, the LMM assumes that a given spectral vector of i-th pixel in the HSI is generated by the linear combination of different end-member spectra and their corresponding abundances, which can be formulated as follows:\n$$y_i = Ma_i + n_i$$ \nwhere $y_i \\in R^L$ represents the input spectral vector with $L$ spectral bands. $M = [m_1, m_2,\u2026\u2026,m_P] \\in R^{L \\times P}$ is the endmember matrix with $P$ endmember categories and $m_i$ denotes the i-th endmember vector. $a \\in R^P$ is the fractional abundance vector for different endmembers in the i-th observed pixel. $n \\in R^L$ denotes the noise vector in the HSI.\nUnder the matrix notation, we can rewrite (1) in a compact matrix form as follows:\n$$Y = MA + N$$ \nwhere $Y \\in R^{L \\times N}$ and $N \\in R^{L \\times N}$ denote the input HSI with $N$ pixels and the noise matrix, respectively. $A = [a_1,a_2,\u2026,a_v] \\in R^{P \\times N}$ is the abundance matrix, where each abundance vector $a_i$ should satisfy the abundance sum-to-one constraint (ASC) and the abundance non-negativity constraint (ANC), that is:\n$$1^TA = 1^T$$\n$$A \\geq 0$$\nFurthermore, due to the existence of multiple scattering interactions between different materials, NLMM is proposed to delineate this complex mixture mechanism. The general form of NLMM is regarded as the sum of a linear mixture and a nonlinear fluctuation that can be parameterized by fractional abundances and endmembers [58], [71]. Its mathematical formulation can be written as\n$$Y = MA + \\Phi(MA) + N$$ \nwhere $\\Phi$ is the nonlinear mapping function applied to the linear transform MA. To achieve an adaptive spectral mixing mechanism from a data-driven perspective, DL-based methods are widely applied in the field of HU. As a typical unsupervised method of DL, AE has been applied to model the spectral mixture process owing to its powerful learning and reconstruction capability. In general, the AE consists of two parts, namely, an encoder and a decoder.\n1) Encoder: The encoder part is designed to transform the input pixel $y_i$ into a hidden representation $v_i$ by utilizing some trainable network parameters, which can be written as\n$$v_i = f_e(y_i) = f(W^{(e)}y_i +b^{(e)}),$$ \nwhere $f(\\cdot)$ is the nonlinear activation function, such as the rectified linear unit (ReLU), the leaky ReLU (LReLU) and the sigmoid function. $W^{(e)}$ and $b^{(e)}$ denote the weight and bias in the e-th encoder part.\n2) Decoder: Based on different mixing assumptions, the decoder part employs one or more hidden layers to reconstruct the input pixel from the hidden representation $v_i$ and the reconstruction process is expressed as\n$$\\hat{y}_i = f_d(v_i) = W^{(d1)}v_i + \\Phi(W^{(d)}v_i)$$\nwhere $\\hat{y} \\in R^L$ is the reconstructed spectral vector. $W^{(d1)}$ and $W^{(d)}$ represent the weight matrix in the first and other d-th decoder part. $\\Phi(\\cdot)$ is the nonlinear interaction among the endmembers. When the observed scene is based on LMM, the decoder part has one hidden layer and $f_d(v_i) = W^{(d1)}v_i$. Since the structure of the decoder satisfies the spectral mixing process, the results of the extracted abundance vector $\\hat{a}_i$ and endmember matrix M in the AE can be regarded as $v_i$ and $W^{(d1)}$, respectively."}, {"title": "B. Dual-Branch Subpixel-Guided Network", "content": "To effectively aggregate diagnostic subpixel information for HSI classification tasks, the proposed DSNet method is used to introduce a deep AE unmixing architecture with physically nonlinear properties to fully explore inherent spectral and spatial correlations in the HSI. As illustrated in Fig. 2, our DSNet is composed of a deep AE unmixing network and CNN-based classifier network, in which the subpixel fusion module is designed to ensure efficient utilization of different levels of discriminative information. The overall network configuration in the proposed DSNet is shown in Table I.\n1) Deep AE Unmixing Network: The structure of the deep AE unmixing network aims at extracting subpixel-level abundance information by building a general mixing decoder with physically nonlinear constraints during the reconstruction process. As illustrated in Table I, block 1-3 represent the encoder part in the deep AE unmixing architecture, and block 4-5 are the general mixing decoder part. In essence, deep AE unmixing network can realize blind unmixing process from HSIs and extract useful abundance information as internal subpixel knowledge for the classifier. Given the input patch $\\{x_i\\}_{i=1}^{N_s} \\in R^{L \\times H \\times H}$ with the H \u00d7 H spatial size and $N_s$ training samples, the encoder part of AE is served as a feature extractor to map the input into high-dimensional abundance representations by the following transformation:\n$$h^{(e)} = \\begin{cases}\nf(BN_{\\gamma,\\beta}(W^{(e)}x_i + b^{(e)})),\\ \\ \\ \\ \\ \\ \\ \\ e = 1 \\\nf(BN_{\\gamma,\\beta}(W^{(e)}h^{(e-1)} + b^{(e)})),\\ \\ \\ \\ \\ e = 2 \\\nW^{(e)}h^{(e-1)}+b^{(e)},\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ e=3\n\\end{cases}$$\nwhere $h^{(e)} \\in R^{P \\times H \\times H}$ denotes the encoded hierarchical representation of HSI data in the e-th encoder layer. $f(\\cdot)$ is the ReLU activation function. $BN_{\\gamma,\\beta}(x_i) = \\gamma x_i + \\beta represents the batch normalization (BN) layer to speed up the parameter learning and avoid the problem of vanishing gradients in the training phase. $\\{W^{(e)}, b^{(e)}\\}$ is the set of the corresponding weight and bias matrix in the 1 \u00d7 1 convolution operation. It is emphasized that 1 \u00d7 1 convolution is equivalent to the fully connected structure in the traditional AE network, which can further ensure the integrity of the spatial structure for the extracted abundance maps and fully explore the spectral relationship between different ground objects.\nTo guarantee ANC and ASC constraints, the absolute value rectification and summed normalization are adopted to the encoded hierarchical representation $h^{(3)}$, which can be written as\n$$v_i = \\frac{|h^{(3)}|}{\\sum_{i=1}^{P} |h^{(3)}|}$$\nwhere $v_i \\in R^{P \\times H \\times H}$ represents the extracted abundance results from the encoder part.\nUnlike previous works that set the fixed number of decoder layers for unmixing, the proposed general mixing decoder is designed to reconstruct the input with different network architectures and takes into account the number of decoder layers K on unmixing performance. Based on (5) and (7), a novel weight matrix $G \\in R^{L \\times K \\times P}$ is proposed to unify and simplify $W^{(d1)}$ and $W^{(d)}$, so that the unmixing performance is insensitive to the number of decoder layers while satisfying physically nonlinear properties, denoted as follows:\n$$W^{(d1)} = \\sum_{k=1}^{K}G_k$$\n$$W^{(d)} = TL_{K \\rightarrow L}(G)$$\nwhere $G_k \\in R^{L \\times P}$ is the each layer element in G. $W^{(d1)}$ can be regarded as the joint interaction of each layer in the decoder."}, {"title": "2) CNN-based Classifier Network", "content": "The spectral-spatial information within the HSI is considered and extracted by the CNN-based classifier network to obtain pixel-level class features $c_i \\in R^P$. In this paper, we only adopt a simple 2-D convolution architecture to extract the spectral-spatial information of HSIs, which can better reflect the advantage of introducing subpixel information. As illustrated in Table I, two 3 x 3 convolution layers are initially deployed to extract feature maps with rich spatial information, followed by the ReLU activation function. Then, the extracted features are flattened and fed into two linear layers to output the pixel-level class feature $c_i$. $c_i$ can provide the discriminative class probabilities for DSNet to aid subsequent training of the subpixel fusion module. This process can be expressed as\n$$c_i = f_{CNN}(x_i)$$\nwhere $f_{CNN}(\\cdot)$ denotes the transformation process in the CNN-based classifier network."}, {"title": "3) Subpixel Fusion Module", "content": "To achieve efficient combination of subpixel and pixel information, the subpixel fusion module is proposed to generate discriminative class-wise representations for the supervised training of DSNet. Given the input abundance patch $v_i$ and class feature $c_i$, due to the difference of the spatial dimension, the front part of subpixel fusion module is designed to maintain the consistency of spatial size by converting the abundance patch into one-dimension space through one 3 \u00d7 3 convolution layer with a stride of 2. Then, the concatenation operation is adopted to fuse the transformed abundance and class feature, so that the integrity of abundance information is preserved as much as possible in the training process. The formula of the subpixel fusion module can be summarized as\n$$s_i = [Flatten(f(BN_{\\gamma,\\beta}(W_{sub}v_i + b_{sub}))), c_i]$$\nwhere $s_i \\in R^S$ is the fused joint representation derived from the subpixel fusion module. $[\\,\\cdot\\,]$ and $Flatten(\\cdot)$ stand for concatenation and flatten operation, respectively. $W_{sub}$ and $b_{sub}$ denote the weight and bias matrix for spatial dimension reduction in the 3 \u00d7 3 convolution layer."}, {"title": "C. Objective Function", "content": "As stated before, the objective function of the proposed DSNet is realized by two phases. One phase is to train the deep AE unmixing network by minimizing the RE loss based on spectral angle distance (SAD), given by\n$$\\mathcal{L}_{RE} = \\frac{1}{N_s}\\sum_{i=1}^{N_s}arccos(\\frac{x_i^T\\hat{x_i}}{\\|x_i\\|_2\\|\\hat{x_i}\\|_2})$$\nwhere $N_s$ represents the number of training samples. $x_i$ and $\\hat{x_i}$ denote the i-th patch in the input HSI and the reconstructed HSI, respectively.\nThe second phase aims at training the CNN-based classifier network by adopting the CE loss between the fused class-wise representation and true label in the ground truth (GT). The standard CE loss is calculated as\n$$\\mathcal{L}_{CE} = - \\frac{1}{N_s}\\sum_{i=1}^{N_s}p_i\\log(p_i)$$\nwhere $p_i$ and $\\hat{p_i}$ are the one-hot encoding of the true label and the corresponding class prediction of DSNet.\nThe overall loss of DSNet can be formulated as\n$$\\mathcal{L} = \\lambda \\mathcal{L}_{RE} + (1 - \\lambda)\\mathcal{L}_{CE}$$\nwhere $\\lambda$ is a hyperparameter to balance different objective functions, and the range of \u03bb is [0, 1]."}, {"title": "IV. CONCLUSION", "content": "This paper proposes a dual-branch subpixel-guided framework (DSNet) for HSI classification in the remote sensing community. We explore the relationship between unmixing and HSI classification and design a joint classification network to deal with mixed pixels to enhance HSI classification performance in actual scenarios. Unlike existing DL-based classification approaches rely on complex network architecture design, the proposed DSNet is capable of fully exploiting inherent subpixel information and designing suitable fusion strategy to improve classification accuracy from a data-driven perspective. Based on a general mixture model, the deep AE unmixing network can achieve automatic subpixel-level abundance extraction with certain physically significance in an unsupervised manner. Then, the subpixel fusion module is developed to ensure high-quality information fusion across pixel and subpixel features, which further achieves stable joint classification and facilitates better class separation. Experimental results performed on several real HSI classification datasets demonstrate the superior performance of our proposed method compared with state-of-the-art DL-based HSI classification methods. Future work will integrate more complete spectral-spatial classifier, such as the hybrid architecture of CNN and Transformer, into the AE network to further enhance its performance."}]}