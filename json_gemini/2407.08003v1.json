{"title": "Machine Learning for ALSFRS-R Score Prediction: Making Sense of the Sensor Data", "authors": ["Ritesh Mehta", "Aleksandar Pramov", "Shashank Verma"], "abstract": "Amyotrophic Lateral Sclerosis (ALS) is characterized as a rapidly progressive neurodegenerative disease that presents individuals with limited treatment options in the realm of medical interventions and therapies. The disease showcases a diverse range of onset patterns and progression trajectories, emphasizing the critical importance of early detection of functional decline to enable tailored care strategies and timely therapeutic interventions. The present investigation, spearheaded by the iDPP@CLEF 2024 challenge, focuses on utilizing sensor-derived data obtained through an app. This data is used to construct various machine learning models specifically designed to forecast the advancement of the ALS Functional Rating Scale-Revised (ALSFRS-R) score, leveraging the dataset provided by the organizers. In our analysis, multiple predictive models were evaluated to determine their efficacy in handling ALS sensor data. The temporal aspect of the sensor data was compressed and amalgamated using statistical methods, thereby augmenting the interpretability and applicability of the gathered information for predictive modeling objectives. The models that demonstrated optimal performance were a naive baseline and ElasticNet regression. The naive model achieved a Mean Absolute Error (MAE) of 0.20 and a Root Mean Square Error (RMSE) of 0.49, slightly outperforming the ElasticNet model, which recorded an MAE of 0.22 and an RMSE of 0.50. Our comparative analysis suggests that while the naive approach yielded marginally better predictive accuracy, the ElasticNet model provides a robust framework for understanding feature contributions.", "sections": [{"title": "1. Introduction", "content": "Amyotrophic Lateral Sclerosis (ALS), also known as Lou Gehrig's disease, is a progressive neurodegen-erative disorder that affects motor neurons in the brain and spinal cord. The ALS Functional RatingScale-Revised (ALSFRS-R), widely utilized in clinical and research settings, stands as a critical metricemployed by healthcare professionals to evaluate the functional state of ALS patients. The preciseforecasting of ALSFRS-R scores is essential for assessing disease progression and the effectiveness oftherapeutic measures. Recent developments in sensor technology have opened up new avenues forcontinuous, non-invasive monitoring of ALS symptoms. The fusion of sensor data with predictive mod-eling presents the potential for more accurate and timely forecasts of disease progression, significantlybenefiting patient care and treatment management.\nThe iDPP@CLEF 2024 competition is an initiative aimed at leveraging sensor data [1] [2] and machinetechniques to predict ALS progression. Task 1 involves predicting the ALSFRS-R scores assignedby medical professionals using sensor data collected via a dedicated app. Task 2 focuses on predictingself-assessment scores recorded frequently by patients. These tasks aim to enhance the accuracy andtimeliness of ALS symptom monitoring and forecasting, providing valuable insights for patient careand treatment management. We hypothesized that the relatively small dataset, coupled with a highnumber of features, will pose significant challenges in our modeling efforts, necessitating the use ofstrong regularization techniques to avoid overfitting.\nTo address the prediction of ALSFRS-R scores, we implemented several techniques. We started with anaive model to form a baseline and establish a reference for comparison. The naive model simply carriesthe last observed value forward. We then explored various Machine Learning algorithms for regression,"}, {"title": "2. Related Work", "content": "In recent years, the integration of sensor data and machine learning (ML) techniques [3] has shownpromising results in improving the accuracy and reliability of ALSFRS-R score predictions. [4] devel-oped machine learning models to objectively measure ALS disease severity using voice samples andaccelerometer data, while [5] further focuses specifically on on deep learning methods to predict ALSdisease progression. Outside of the ALS prediction field, [6] demonstrated the potential of temporalmodels in the healthcare domain by integrating EHR data; [7] applied various ML models to sensor datafrom accelerometers attached to dairy cattle for disease prediction and [8] demonstrates how temporalpatterns from clinical and imaging data can be used to predict residual survival for cancer patients.\nA more technical field of the literature deals specifically with the longitudinal aspect of the dataand its effect on ML and DL methods. The recent wider application of ML methods in biomedical datahas necessitated adapting traditional models to handle repeated measurements over time. [9]. [10]extends Random Forests to handle fixed and random effects. [11, 12] give a more general overview ofexisting methods, while [13] focuses specifically on a neural network adaptation that can handle fixedand random effects."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Data Description and Preprocessing", "content": "The raw features dataset provided for the competition consists of two parts: static data and sensor data.\nThe former contains patient-specific baseline (constant) data, such as sex, age etc. The latter contains 90time-series of patient-specific sensor data, collected over an average of nine months through a dedicatedapp developed by the BRAINTEASER project, using a fitness smartwatch in the context of clinical trials.\nFor some patients, the clinical data starts before the app data, and some patients whose app data goesbeyond the last observed clinical data. This required some synchronization between the clinical and theapp data, in order to avoid look-ahead bias. Let \\(t_1\\) be the first sensor time point in the raw data, \\(t_1\\) bethe first clinical time point in the raw data, as defined by the days from diagnosis for a given patient.\nOverall, we consider the time-overlap between clinical and sensor data. Some special cases are handledas follows:\n\u2022 If \\(t_1 > t_1\\), then we discard clinical data prior to one step back from \\(t_1\\). For example, consider apatient that has clinical observations at days 690, 780, 873, and sensor data from 800 onwards.We discard the clinical observation 690, but keep the rest. As we will see later, the reason for thisis that we will use the previous clinical score as a feature and hence the first value to be predictedwould be the target at day 873. That first value will be predicted using the sensor data between800 and 872, as well as previous clinical visit from the 780th day since diagnosis.\n\u2022 If \\(t_1 < t_1\\), then we use \\(t_1\\) as the starting point for the sensor data and we do not discard anyclinical data.\n\u2022 Sensor data after the last clinical observation is discarded.\n\u2022 If the clinical data ends after the sensor data, we consider that point only if it is at maximum of60 days from the last observed sensor data point. This is done so that we do not use too distantsensor data as a feature.\n\u2022 Some patients have only one clinical observation. As we will use the previous clinical questionresponses in the set of features for the models, those patients will be discarded due to the missingprevious response value.\nSome patients had a few missing static data observations as well. Those were imputed by a simplemedian over all other patients for the respective feature.\nThe target variable (i.e., questionnaire responses) provides highly informative insights when visualizedand analyzed over time, as shown in Figure 1. The visualization is done on the actual dataset we willuse, i.e. after the pre-processing steps described above.\nThe curves reveal an initial period of slow degradation followed by a more rapid decline for manyquestions. We also notice that the scores change pretty slowly for most patients over the course of 100days (the average number of days between consecutive visits to the clinician). Additionally, only a fewpatients manage to recover their scores to better levels. That observation has to be cautioned by theobservation that the amount of patients with a longer follow-up decreases strongly over time. While atthe beginning there are n = 51 observed patients (i.e. the full sample), this number drops to n = 37 forthe second follow-up and decreases further to just n = 5 and n = 2 for the fifth and sixth follow-uprespectively. This is also evident by the higher CI (Confidence Interval) bars on Figure 1.\nA key insight here, as a result of above mentioned observations, is that the previous value of the scorecould be a useful engineered feature, which will also dictate the data pre-processing steps. Additionally,the heterogeneity in question types supports two modeling approaches: (1) treating the data as paneldata with \"question type\" as a grouping variable for random effects, and (2) modeling each questionseparately."}, {"title": "3.2. Features generation", "content": "For our modeling, we considered three groups of features: target-based, static, and sensor-data. Whilewe used the static data as-is, we originally tried to build a baseline model based only on the target dataand treat it as an autoregressive problem with some additional engineered simple transformations ofthe target dataset, e.g., the difference in days between two diagnosis, the first value of each patient,the previous value, one-hot encoding of the question type, the follow-up time-index (i.e. the x-axis inFigure 1). Based on this initial analysis, we quickly realized our initial hypothesis for the importanceof the previous target value, stated in previous section, is valid, and we kept it as a feature for anysubsequent modeling.\nA central challenge in this study was the handling of the sensor data features. Those are observeddaily (with some gaps), compared to the target (and related engineered variables such as the previousvalue), which is observed once every three months. To address this mismatch in the frequencies, wefollowed three different approaches:\nApproach 1: By simply taking the median over a window of each sensor data column, where thewindow was defined to be between each two consecutive clinical visits - \\([t \u2013 1, t)\\).\nApproach 2: By generating a vast array of features derived from a window in each sensor data column,where the window is between each two consecutive clinical visits - \\([t \u2013 1, t)\\), inspired by afeature-based time series analysis approach [14].\nApproach 3: By using LSTM (Long Short Term Memory) cells for the sensor data and letting thenetwork define the relevant transformations of the input.\nNote that Approach 1 is a special and simple case of Approach 2. Here, we effectively express eachsensor time window \\([t \u2013 1, t)\\), with just one number and a set of numbers respectively. Doing thisacross all the windows of all patients builds the sensor data feature set. Approach 3 on the other handdoes not have pre-defined (set of) transformation(s) on the input sensor data and we let the neuralnetwork itself pick up the relevant transformation of the input. Its details are provided in section 3.5where the LSTM model is described.\nFor Approach 2, we employed the tsfresh python library for systematic feature engineering fromtime-series and other sequential data [15]. Simple examples of those can be e.g. the maxima/mini-ma/median/mean etc. over each sensor data window per each patient between two clinical visits. Afull outline of the feature extraction procedure and the considered extracted features can be foundon the documentation website of the package. Note that, when one considers different parametersetups of the various extracted features, one would end up with multiple extracted features per timeseries. We have around 100 sensor time series and many of the extracted features by tsfresh hadproblems like high amount of missing values, low variability etc. and so were removed from the set offeature candidates. On each of those extracted features we performed filtering, by calculating tsfreshSpearmann correlation coefficient between the feature and the (one-step-ahead) clinical values per eachquestion, over all patients. The hypothesis tests for significance are adjusted following the procedure in[16]. For the final feature set based on the app data, we experimented with either a) keeping all thefeatures that were deemed significant per question or b) by fixing the number of the top k (e.g. 10)in terms of lowest p-value to keep in the sensor feature set. As the final modeling was done for eachquestion separately, we settled on the former choice."}, {"title": "3.3. Data augmentation", "content": "As already mentioned, one of the major challenges in modeling for Task1 was the lack of sufficientdatapoints. Given the high amount of features, this made the models more prone to overfitting. Fittingcomplex deep models would exacerbate this problem further. To circumvent this issue, we decided toaugment the Task1 data with that of Task2. (Note that doing this reduces the average number of daysbetween any two consecutive visits.) This gave us ALSFRS-R scores progression as shows in Figure 2.\nThere are a couple of advantages of doing this:"}, {"title": "3.4. Modeling", "content": "Unless explicitly specified, all modeling efforts described are for Task1 i.e. predicting clinician's assignedscores. Our modeling setup used RMSE as the scoring function at all stages."}, {"title": "3.4.1. CV setup", "content": "The small dataset (129 datapoints per question only) posed several challenges with regards to choosingthe appropriate cross-validation procedure for the hyperparameter tuning for our models. One keyaspect is that the true (unseen) test set contains patients that are not present in the training set, ratherthan containing unseen data of the same patients. Hence, to assess the ability of our models to generalizewell, we used a nested k-fold cross validation strategy [18, 19]. This consists of two loops - an inner,and an outer one.\nIn each inner loop, we set aside a test set of 10% containing complete data of the patients (i.e. allobservations). For the remaining data, we perform a further k-fold cross-validation, whereby we alsoadapted it to contain a complete set of observations of each patient in both the training and the validationsets. That ensured that there is no information leakage between different times of the patient's data, asthe true out of sample test (for the final submission) also contains data on unseen patients."}, {"title": "3.4.2. Traditional ML models", "content": "We experimented with a wide range of modeling techniques for predicting ALSFRS-R scores. Thisincluded traditional ML models as well as deep learning models, features described in Approach 1 aswell as Approach 2 mentioned in section 3.2, with and without augmenting the training data fromTask1 with that of Task2 as described in section 3.3. We achieved good results with traditional as wellas deep learning models. We'll go over the details of LSTM modeling in the next section.\nNa\u00efve Model\nAs described in section 3.1, the scores for individual questions changes infrequently for an averagepatient over the course of two consecutive clinician visits (this is especially true since the scores areintegers and don't allow partial progression from say 4 to 3). This suggests a Naive Model wherethe predicted score is the same as previous visit's score to serve as a baseline for all modeling approaches.\nModeling Approaches\nIn its given form, this is a multi-label multi-class classification problem. However, we can treatquestion_type as a covariate and transform this into a multi-class classification (with 12x datapoints).\nWe can also train a separate model for each of the 12 questions which would also transform it into a"}, {"title": "3.4.3. LSTM", "content": "In our study, we employed a Long Short-Term Memory (LSTM) neural network to predict ALSFRS-Rscores based on time-series sensor data. The LSTM model is well-suited for handling sequential datadue to its ability to capture long-term dependencies. As depicted in Figure 4, our model processessensor data collected over multiple time points using a series of LSTM cells. Each cell captures temporalpatterns in the data at different time steps, which are then fed into subsequent cells. To handle theuneven number of days between baseline and target scores, padding was applied to standardize theinput sequences. The output from the final LSTM cell is concatenated with baseline scores and staticpatient data to enhance the model's predictive capability. This combined feature set is then passedthrough multiple linear layers, each dedicated to predicting one of the twelve ALSFRS-R sub-scores.This architecture allows the model to leverage both dynamic sensor inputs and static information,providing robust predictions for each functional domain assessed by the ALSFRS-R score."}, {"title": "4. Results", "content": ""}, {"title": "4.1. Task1", "content": "The results for Task1 are shown in Table 2 below.\nThe table displays the RMSE of various models on each question on test data split from the train+valset. As can be seen here as well as Figure 5, previous_value has by far the most predictive power. For"}, {"title": "4.2. Task2", "content": "Due to time constraints, we didn't manage to submit our model for Task2. However, using the groundtruth that was released after the competition deadline, we ran our ElasticNet + Naive model and it gavean RMSE of 0.5594 and MAE of 0.2803. This is better than the top leaderboard submission with RMSEof 0.5774 and MAE of 0.2879."}, {"title": "4.3. Takeaways", "content": "Following is a summary of our takeaways:\n\u2022 Previous score is an overwhelming contributor in prediction of the future score\n\u2022 No single model could capture the essence of all questions better than the Naive model\n\u2022 Small training set means that simpler models that focus on regularization perform well\n\u2022 Having more granularity in scores either through having floating point scores or larger spanlike [0, 10] rather than [0, 4] would have allowed better modeling results, as 5 integer scale forscores seemed to mask inherent trends.\n\u2022 Ablating sensor data in training didn't deteriorate the model performance much, which is evidentfrom the Feature Importance scores graph in Figure 5\nNotable features that showed up in Feature Importances (other than previous_value) acrossmultiple questions include:\nstatic features: age_at_diagnosis and FVC\nmedian sensor features: agg_respiration_alpha2_DimMean, agg_respiration_RMSSD,\nagg_respiration_SD1 and agg_total_steps\ntsfresh sensor features: beat_to_beat_cvsd_benford_correlation,\nbeat_to_beat_cvsd__quantile___q_0.6 and total_steps__quantile__q_0.1"}, {"title": "5. Future Work", "content": "Future work will focus on clustering patient data to capture distinct patient phenotypes, allowing formore personalized predictions and treatment plans. We also plan to utilize freely available datasetssuch as PRO-ACT to enhance the robustness and generalizability of our models. Additionally, access tomultimodal data sources such as audio data for speech, accelerometer data for muscle function, geneticdata, imaging data, etc could help further improve the accuracy and reliability of ALSFRS-R scorepredictions. These efforts aim to refine our models and contribute to more effective ALS managementand patient care.\nAnother point we plan to work on is integrating models for panel data. We've already experimentedwith this approach, where instead of fitting separate models for each question, we fitted a panel model"}, {"title": "6. Conclusions", "content": "We explored various machine learning approaches to predict ALSFRS-R scores. The main goal of ourstudy was to determine if the app data provides additional value that enhances the predictive accuracyof the models, based on evidence found in the dataset that we analyzed.\nThe methodology involved extensive data preprocessing to synchronize clinical and sensor data,ensuring the integrity and reliability of the dataset. We generated features from both static and dynamicdata sources, with a particular focus on leveraging temporal dependencies in the sensor data throughtechniques like median aggregation, feature-based time series analysis, and LSTM neural networks.\nOur comparative analysis revealed that while on Task1 the naive baseline model achieved slightlybetter predictive accuracy, the ElasticNet+Naive regression model offered a robust framework forunderstanding feature contributions. Moreover, on Task2 our model (though not submitted) was able toperform slightly better than the Naive model. Nonetheless, the previous value of the ALSFRS-R scoreemerged as a significant predictor across all models. Thus, our analysis did not find significant evidencethat the app data provides an overall enhancement to the predictive accuracy of the model. Based onthe leaderboard at the end of the competition, it would appear that the other teams reached a similarconclusion.\nWe caution against discarding the potential usefulness of the app data at this stage however, as thedataset brought several methodological challenges with it, the biggest of which was its very small size.We addressed it through rigorous cross-validation strategies and data augmentation techniques, ensuringthat our models generalize well to unseen patients. The feature importance analysis highlighted andadded value of certain static features and, for some questions, the importance of isolated sensor-derivedfeatures.\nOverall, this work demonstrates the potential of integrating sensor data with machine learningmodels to enhance the monitoring and prediction of ALS progression, within the limits of a smalldataset.\nGiven the strong persistence of the \u201cprevious value\" in both our modeling, and the lack of significantimprovement to the baseline model, we hypothesise that the dataset would benefit from a largerand more heterogeneous set of patients, not only to increase the amount of observations, but to alsointroduce more variability in the target variable, perhaps spread across more regions and with clinicalevaluations done by a multitude of clinicians.\"\n    }"}]}