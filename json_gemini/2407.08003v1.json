{"title": "Machine Learning for ALSFRS-R Score Prediction: Making Sense of the Sensor Data", "authors": ["Ritesh Mehta", "Aleksandar Pramov", "Shashank Verma"], "abstract": "Amyotrophic Lateral Sclerosis (ALS) is characterized as a rapidly progressive neurodegenerative disease that presents individuals with limited treatment options in the realm of medical interventions and therapies. The disease showcases a diverse range of onset patterns and progression trajectories, emphasizing the critical importance of early detection of functional decline to enable tailored care strategies and timely therapeutic interventions. The present investigation, spearheaded by the iDPP@CLEF 2024 challenge, focuses on utilizing sensor-derived data obtained through an app. This data is used to construct various machine learning models specifically designed to forecast the advancement of the ALS Functional Rating Scale-Revised (ALSFRS-R) score, leveraging the dataset provided by the organizers. In our analysis, multiple predictive models were evaluated to determine their efficacy in handling ALS sensor data. The temporal aspect of the sensor data was compressed and amalgamated using statistical methods, thereby augmenting the interpretability and applicability of the gathered information for predictive modeling objectives. The models that demonstrated optimal performance were a naive baseline and ElasticNet regression. The naive model achieved a Mean Absolute Error (MAE) of 0.20 and a Root Mean Square Error (RMSE) of 0.49, slightly outperforming the ElasticNet model, which recorded an MAE of 0.22 and an RMSE of 0.50. Our comparative analysis suggests that while the naive approach yielded marginally better predictive accuracy, the ElasticNet model provides a robust framework for understanding feature contributions.", "sections": [{"title": "1. Introduction", "content": "Amyotrophic Lateral Sclerosis (ALS), also known as Lou Gehrig's disease, is a progressive neurodegenerative disorder that affects motor neurons in the brain and spinal cord. The ALS Functional Rating Scale-Revised (ALSFRS-R), widely utilized in clinical and research settings, stands as a critical metric employed by healthcare professionals to evaluate the functional state of ALS patients. The precise forecasting of ALSFRS-R scores is essential for assessing disease progression and the effectiveness of therapeutic measures. Recent developments in sensor technology have opened up new avenues for continuous, non-invasive monitoring of ALS symptoms. The fusion of sensor data with predictive modeling presents the potential for more accurate and timely forecasts of disease progression, significantly benefiting patient care and treatment management.\nThe iDPP@CLEF 2024 competition is an initiative aimed at leveraging sensor data [1] [2] and machine learning techniques to predict ALS progression. Task 1 involves predicting the ALSFRS-R scores assigned by medical professionals using sensor data collected via a dedicated app. Task 2 focuses on predicting self-assessment scores recorded frequently by patients. These tasks aim to enhance the accuracy and timeliness of ALS symptom monitoring and forecasting, providing valuable insights for patient care and treatment management. We hypothesized that the relatively small dataset, coupled with a high number of features, will pose significant challenges in our modeling efforts, necessitating the use of strong regularization techniques to avoid overfitting.\nTo address the prediction of ALSFRS-R scores, we implemented several techniques. We started with a naive model to form a baseline and establish a reference for comparison. The naive model simply carries the last observed value forward. We then explored various Machine Learning algorithms for regression,"}, {"title": "2. Related Work", "content": "In recent years, the integration of sensor data and machine learning (ML) techniques [3] has shown promising results in improving the accuracy and reliability of ALSFRS-R score predictions. [4] developed machine learning models to objectively measure ALS disease severity using voice samples and accelerometer data, while [5] further focuses specifically on on deep learning methods to predict ALS disease progression. Outside of the ALS prediction field, [6] demonstrated the potential of temporal models in the healthcare domain by integrating EHR data; [7] applied various ML models to sensor data from accelerometers attached to dairy cattle for disease prediction and [8] demonstrates how temporal patterns from clinical and imaging data can be used to predict residual survival for cancer patients.\nA more technical field of the literature deals specifically with the longitudinal aspect of the data and its effect on ML and DL methods. The recent wider application of ML methods in biomedical data has necessitated adapting traditional models to handle repeated measurements over time. [9]. [10] extends Random Forests to handle fixed and random effects. [11, 12] give a more general overview of existing methods, while [13] focuses specifically on a neural network adaptation that can handle fixed and random effects."}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Data Description and Preprocessing", "content": "The raw features dataset provided for the competition consists of two parts: static data and sensor data. The former contains patient-specific baseline (constant) data, such as sex, age etc. The latter contains 90 time-series of patient-specific sensor data, collected over an average of nine months through a dedicated app developed by the BRAINTEASER project, using a fitness smartwatch in the context of clinical trials. For some patients, the clinical data starts before the app data, and some patients whose app data goes beyond the last observed clinical data. This required some synchronization between the clinical and the app data, in order to avoid look-ahead bias. Let \\(t_1\\) be the first sensor time point in the raw data, \\(t'_1\\) be the first clinical time point in the raw data, as defined by the days from diagnosis for a given patient. Overall, we consider the time-overlap between clinical and sensor data. Some special cases are handled as follows:\n\u2022 If \\(t_1 > t'_1\\), then we discard clinical data prior to one step back from \\(t'_1\\). For example, consider a patient that has clinical observations at days 690, 780, 873, and sensor data from 800 onwards. We discard the clinical observation 690, but keep the rest. As we will see later, the reason for this is that we will use the previous clinical score as a feature and hence the first value to be predicted would be the target at day 873. That first value will be predicted using the sensor data between 800 and 872, as well as previous clinical visit from the 780th day since diagnosis.\n\u2022 If \\(t_1 < t'_1\\), then we use \\(t_1\\) as the starting point for the sensor data and we do not discard any clinical data.\n\u2022 Sensor data after the last clinical observation is discarded.\n\u2022 If the clinical data ends after the sensor data, we consider that point only if it is at maximum of 60 days from the last observed sensor data point. This is done so that we do not use too distant sensor data as a feature.\n\u2022 Some patients have only one clinical observation. As we will use the previous clinical question responses in the set of features for the models, those patients will be discarded due to the missing previous response value.\nSome patients had a few missing static data observations as well. Those were imputed by a simple median over all other patients for the respective feature.\nThe target variable (i.e., questionnaire responses) provides highly informative insights when visualized and analyzed over time, as shown in Figure 1. The visualization is done on the actual dataset we will use, i.e. after the pre-processing steps described above.\nThe curves reveal an initial period of slow degradation followed by a more rapid decline for many questions. We also notice that the scores change pretty slowly for most patients over the course of 100 days (the average number of days between consecutive visits to the clinician). Additionally, only a few patients manage to recover their scores to better levels. That observation has to be cautioned by the observation that the amount of patients with a longer follow-up decreases strongly over time. While at the beginning there are n = 51 observed patients (i.e. the full sample), this number drops to n = 37 for the second follow-up and decreases further to just n = 5 and n = 2 for the fifth and sixth follow-up respectively. This is also evident by the higher CI (Confidence Interval) bars on Figure 1.\nA key insight here, as a result of above mentioned observations, is that the previous value of the score could be a useful engineered feature, which will also dictate the data pre-processing steps. Additionally, the heterogeneity in question types supports two modeling approaches: (1) treating the data as panel data with \"question type\" as a grouping variable for random effects, and (2) modeling each question separately."}, {"title": "3.2. Features generation", "content": "For our modeling, we considered three groups of features: target-based, static, and sensor-data. While we used the static data as-is, we originally tried to build a baseline model based only on the target data and treat it as an autoregressive problem with some additional engineered simple transformations of the target dataset, e.g., the difference in days between two diagnosis, the first value of each patient, the previous value, one-hot encoding of the question type, the follow-up time-index (i.e. the x-axis in Figure 1). Based on this initial analysis, we quickly realized our initial hypothesis for the importance of the previous target value, stated in previous section, is valid, and we kept it as a feature for any subsequent modeling.\nA central challenge in this study was the handling of the sensor data features. Those are observed daily (with some gaps), compared to the target (and related engineered variables such as the previous value), which is observed once every three months. To address this mismatch in the frequencies, we followed three different approaches:\nApproach 1: By simply taking the median over a window of each sensor data column, where the window was defined to be between each two consecutive clinical visits - \\([t' - 1, t')\\).\nApproach 2: By generating a vast array of features derived from a window in each sensor data column, where the window is between each two consecutive clinical visits - \\([t' - 1, t')\\), inspired by a feature-based time series analysis approach [14].\nApproach 3: By using LSTM (Long Short Term Memory) cells for the sensor data and letting the network define the relevant transformations of the input.\nNote that Approach 1 is a special and simple case of Approach 2. Here, we effectively express each sensor time window \\([t' - 1, t')\\), with just one number and a set of numbers respectively. Doing this across all the windows of all patients builds the sensor data feature set. Approach 3 on the other hand does not have pre-defined (set of) transformation(s) on the input sensor data and we let the neural network itself pick up the relevant transformation of the input. Its details are provided in section 3.5 where the LSTM model is described.\nFor Approach 2, we employed the tsfresh python library for systematic feature engineering from time-series and other sequential data [15]. Simple examples of those can be e.g. the maxima/minima/median/mean etc. over each sensor data window per each patient between two clinical visits. A full outline of the feature extraction procedure and the considered extracted features can be found on the documentation website of the package. Note that, when one considers different parameter setups of the various extracted features, one would end up with multiple extracted features per time series. We have around 100 sensor time series and many of the extracted features by tsfresh had problems like high amount of missing values, low variability etc. and so were removed from the set of feature candidates. On each of those extracted features we performed filtering, by calculating tsfresh Spearmann correlation coefficient between the feature and the (one-step-ahead) clinical values per each question, over all patients. The hypothesis tests for significance are adjusted following the procedure in [16]. For the final feature set based on the app data, we experimented with either a) keeping all the features that were deemed significant per question or b) by fixing the number of the top k (e.g. 10) in terms of lowest p-value to keep in the sensor feature set. As the final modeling was done for each question separately, we settled on the former choice."}, {"title": "3.3. Data augmentation", "content": "As already mentioned, one of the major challenges in modeling for Task1 was the lack of sufficient datapoints. Given the high amount of features, this made the models more prone to overfitting. Fitting complex deep models would exacerbate this problem further. To circumvent this issue, we decided to augment the Task1 data with that of Task2. (Note that doing this reduces the average number of days between any two consecutive visits.) This gave us ALSFRS-R scores progression as shows in Figure 2. There are a couple of advantages of doing this:"}, {"title": "3.4. Modeling", "content": "Unless explicitly specified, all modeling efforts described are for Task1 i.e. predicting clinician's assigned scores. Our modeling setup used RMSE as the scoring function at all stages."}, {"title": "3.4.1. CV setup", "content": "The small dataset (129 datapoints per question only) posed several challenges with regards to choosing the appropriate cross-validation procedure for the hyperparameter tuning for our models. One key aspect is that the true (unseen) test set contains patients that are not present in the training set, rather than containing unseen data of the same patients. Hence, to assess the ability of our models to generalize well, we used a nested k-fold cross validation strategy [18, 19]. This consists of two loops - an inner, and an outer one.\nIn each inner loop, we set aside a test set of 10% containing complete data of the patients (i.e. all observations). For the remaining data, we perform a further k-fold cross-validation, whereby we also adapted it to contain a complete set of observations of each patient in both the training and the validation sets. That ensured that there is no information leakage between different times of the patient's data, as the true out of sample test (for the final submission) also contains data on unseen patients."}, {"title": "3.4.2. Traditional ML models", "content": "We experimented with a wide range of modeling techniques for predicting ALSFRS-R scores. This included traditional ML models as well as deep learning models, features described in Approach 1 as well as Approach 2 mentioned in section 3.2, with and without augmenting the training data from Task1 with that of Task2 as described in section 3.3. We achieved good results with traditional as well as deep learning models. We'll go over the details of LSTM modeling in the next section.\nNa\u00efve Model\nAs described in section 3.1, the scores for individual questions changes infrequently for an average patient over the course of two consecutive clinician visits (this is especially true since the scores are integers and don't allow partial progression from say 4 to 3). This suggests a Naive Model where the predicted score is the same as previous visit's score to serve as a baseline for all modeling approaches.\nModeling Approaches\nIn its given form, this is a multi-label multi-class classification problem. However, we can treat question_type as a covariate and transform this into a multi-class classification (with 12x datapoints). We can also train a separate model for each of the 12 questions which would also transform it into a"}, {"title": "3.4.3. LSTM", "content": "In our study, we employed a Long Short-Term Memory (LSTM) neural network to predict ALSFRS-R scores based on time-series sensor data. The LSTM model is well-suited for handling sequential data due to its ability to capture long-term dependencies. As depicted in Figure 4, our model processes sensor data collected over multiple time points using a series of LSTM cells. Each cell captures temporal patterns in the data at different time steps, which are then fed into subsequent cells. To handle the uneven number of days between baseline and target scores, padding was applied to standardize the input sequences. The output from the final LSTM cell is concatenated with baseline scores and static patient data to enhance the model's predictive capability. This combined feature set is then passed through multiple linear layers, each dedicated to predicting one of the twelve ALSFRS-R sub-scores. This architecture allows the model to leverage both dynamic sensor inputs and static information, providing robust predictions for each functional domain assessed by the ALSFRS-R score."}, {"title": "4. Results", "content": null}, {"title": "4.1. Task1", "content": "The results for Task1 are shown in Table 2 below.\nThe table displays the RMSE of various models on each question on test data split from the train+val set. As can be seen here as well as Figure 5, previous_value has by far the most predictive power. For"}, {"title": "4.2. Task2", "content": "Due to time constraints, we didn't manage to submit our model for Task2. However, using the ground truth that was released after the competition deadline, we ran our ElasticNet + Naive model and it gave an RMSE of 0.5594 and MAE of 0.2803. This is better than the top leaderboard submission with RMSE of 0.5774 and MAE of 0.2879."}, {"title": "4.3. Takeaways", "content": "Following is a summary of our takeaways:\n\u2022 Previous score is an overwhelming contributor in prediction of the future score\n\u2022 No single model could capture the essence of all questions better than the Naive model\n\u2022 Small training set means that simpler models that focus on regularization perform well\n\u2022 Having more granularity in scores either through having floating point scores or larger span like [0, 10] rather than [0, 4] would have allowed better modeling results, as 5 integer scale for scores seemed to mask inherent trends.\n\u2022 Ablating sensor data in training didn't deteriorate the model performance much, which is evident from the Feature Importance scores graph in Figure 5\nNotable features that showed up in Feature Importances (other than previous_value) across multiple questions include:\nstatic features: age_at_diagnosis and FVC\nmedian sensor features: agg_respiration_alpha2_DimMean, agg_respiration_RMSSD,\nagg_respiration_SD1 and agg_total_steps\ntsfresh sensor features: beat_to_beat_cvsd_benford_correlation,\nbeat_to_beat_cvsd__quantile___q_0.6 and total_steps__quantile___q_0.1"}, {"title": "5. Future Work", "content": "Future work will focus on clustering patient data to capture distinct patient phenotypes, allowing for more personalized predictions and treatment plans. We also plan to utilize freely available datasets such as PRO-ACT to enhance the robustness and generalizability of our models. Additionally, access to multimodal data sources such as audio data for speech, accelerometer data for muscle function, genetic data, imaging data, etc could help further improve the accuracy and reliability of ALSFRS-R score predictions. These efforts aim to refine our models and contribute to more effective ALS management and patient care.\nAnother point we plan to work on is integrating models for panel data. We've already experimented with this approach, where instead of fitting separate models for each question, we fitted a panel model"}, {"title": "6. Conclusions", "content": "We explored various machine learning approaches to predict ALSFRS-R scores. The main goal of our study was to determine if the app data provides additional value that enhances the predictive accuracy of the models, based on evidence found in the dataset that we analyzed.\nThe methodology involved extensive data preprocessing to synchronize clinical and sensor data, ensuring the integrity and reliability of the dataset. We generated features from both static and dynamic data sources, with a particular focus on leveraging temporal dependencies in the sensor data through techniques like median aggregation, feature-based time series analysis, and LSTM neural networks.\nOur comparative analysis revealed that while on Task1 the naive baseline model achieved slightly better predictive accuracy, the ElasticNet+Naive regression model offered a robust framework for understanding feature contributions. Moreover, on Task2 our model (though not submitted) was able to perform slightly better than the Naive model. Nonetheless, the previous value of the ALSFRS-R score emerged as a significant predictor across all models. Thus, our analysis did not find significant evidence that the app data provides an overall enhancement to the predictive accuracy of the model. Based on the leaderboard at the end of the competition, it would appear that the other teams reached a similar conclusion.\nWe caution against discarding the potential usefulness of the app data at this stage however, as the dataset brought several methodological challenges with it, the biggest of which was its very small size. We addressed it through rigorous cross-validation strategies and data augmentation techniques, ensuring that our models generalize well to unseen patients. The feature importance analysis highlighted and added value of certain static features and, for some questions, the importance of isolated sensor-derived features.\nOverall, this work demonstrates the potential of integrating sensor data with machine learning models to enhance the monitoring and prediction of ALS progression, within the limits of a small dataset.\nGiven the strong persistence of the \u201cprevious value\" in both our modeling, and the lack of significant improvement to the baseline model, we hypothesise that the dataset would benefit from a larger and more heterogeneous set of patients, not only to increase the amount of observations, but to also introduce more variability in the target variable, perhaps spread across more regions and with clinical evaluations done by a multitude of clinicians.\"\n    }"}]}