{"title": "When Personalization Meets Reality: A Multi-Faceted Analysis of Personalized Preference Learning", "authors": ["Yijiang River Dong", "Tiancheng Hu", "Yinhong Liu", "Ahmet \u00dcst\u00fcn", "Nigel Collier"], "abstract": "While Reinforcement Learning from Human Feedback (RLHF) is widely used to align Large Language Models (LLMs) with human preferences, it typically assumes homogeneous preferences across users, overlooking diverse human values and minority viewpoints. Although personalized preference learning addresses this by tailoring separate preferences for individual users, the field lacks standardized methods to assess its effectiveness. We present a multi-faceted evaluation framework that measures not only performance but also fairness, unintended effects, and adaptability across varying levels of preference divergence. Through extensive experiments comparing eight personalization methods across three preference datasets, we demonstrate that performance differences between methods could reach 36% when users strongly disagree, and personalization can introduce up to 20% safety misalignment. These findings highlight the critical need for holistic evaluation approaches to advance the development of more effective and inclusive preference learning systems.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning from human feedback (RLHF) has been effective in aligning pre-trained Large Language Models (LLMs) with human preferences, improving their helpfulness, harmlessness, and instruction-following abilities (Ouyang et al., 2022). However, standard RLHF assumes a homogeneous set of preferences, failing to account for the diverse and sometimes conflicting nature of human values (Casper et al., 2023). This leads to biases toward the perspectives of a western, democratic, postgraduate-educated demographic (Santurkar et al., 2023), even though LLM users represent a wide range of cultural and ideological backgrounds, with a majority being non-U.S. users across the world (Liu and Wang, 2023)."}, {"title": "2 Related Work", "content": "Personalization in machine learning refers to tailoring systems to generate predictions that align with each individual's preferences and needs. This concept has been extensively studied in Recommendation Systems (Sarwar et al., 2001; He et al., 2017) and Dialogue Systems (Zhang et al., 2018; Li et al., 2016). With the widespread adoption of LLMs, personalization has become even more critical to ensure these models effectively serve diverse global users with varying preferences\u2014a challenge that remains underexplored in current alignment pipelines (Sorensen et al., 2024). Unlike traditional task-specific ML systems, LLMs are general-purpose models designed to handle a wide range of tasks and domains. This versatility makes personalization both more important and more challenging, as the model must adapt its broad capabilities to each user's specific needs and preferences. Several approaches have been proposed, including prompting (Hwang et al., 2023), user embedding learning (Li et al., 2024; Feng et al., 2024), latent variable modeling (Poddar et al., 2024; Siththaranjan et al., 2023), meta-learning (Zhao et al.), multi-objective reinforcement learning (Jang et al., 2023), preference elicitation (Li et al., 2025), prompt optimization (Kim and Yang, 2024), and context compression (Kim et al.). However, these methods have typically been evaluated on different datasets which prohibits a fair comparison between them. Evaluation of Personalization presents unique challenges beyond traditional preference learning. While domains like recommender systems have established evaluation frameworks using per-user interaction histories (Harper and Konstan, 2015), evaluating natural language outputs and collecting general-domain preference data at scale remains challenging (Zhou et al., 2022; Clark et al., 2021; Dong et al., 2024). Existing survey-based datasets, such as OpinionQA (Santurkar et al., 2023) and GlobalOpinionQA (Durmus et al., 2023), provide large-scale, real-world general-domain data but are limited to multiple-choice formats, which fail to capture realistic LLM usage scenarios. In contrast, generation-based datasets such as Salemi et al. (2024); Wang et al. (2024); Stiennon et al. (2020) contain preferences for open-ended generations but remain restricted to narrow domains. Other sources, like Personal Reddit (Staab et al.) and Persona-DB (Sun et al., 2025), scrape Reddit and Twitter data but cannot be publicly released due to privacy concerns. PRISM (Kirk et al., 2024b) offers diverse preference data for LLM generations but remains limited in size to effectively model individual annotators. In the absence of large-scale, general-domain preference datasets, recent research has explored synthetic data generation via role-playing agents and LLM-as-a-Judge evaluations (Zheng et al., 2023; Jang et al., 2023; Zollo et al., 2024; Castricato et al., 2024; Shao et al., 2023; Liu et al., 2024b). While these methods may not fully capture real user preferences (Hu and Collier, 2024), recent works suggest that synthetic benchmarks can serve as viable testbeds for evaluating personalization, even if they don't comprehensively represent all human preference variations (Castricato et al., 2024; Zollo et al., 2024). As noted in Balog and Zhai (2025), perfect simulations of human preferences may not be necessary for these simulation to provide valuable insights and help develop better algorithms."}, {"title": "3 Preliminaries on Personalized Preference Learning", "content": "Preference learning systems can take various forms, including reward models (RMs), where a model assigns a numerical preference score; preference ranking models, which make comparative judgments between multiple candidates; and generation-based policy models, where the model explicitly generates preference judgments, sometimes accompanied by explanations or feedback. In this section, we review previous approaches to learning personalized preferences, with a particular focus on reward models, which constitute the majority of existing methods."}, {"title": "3.1 Vanilla Reward Modeling", "content": "Consider n annotators U1, U2, ..., Un who provide preference feedback on outputs Y1, Y2 for a given prompt x. The preferred and dispreferred response is denoted as y+ and y-, respectively. This yields a personalized preference dataset Dp:\nDp = \\cup_{u=1}^{n} \\{(x_i, y_i^+, y_i^-, u)\\}_{i=1}^{m}\nwhere m is the number of samples. Current preference tuning literature assumes homogeneous human preference (Ouyang et al., 2022; Stiennon et al., 2020; Liu et al., 2024a), and thus aggregates Dp via majority voting or rank aggregation, yielding:\nD = \\{(x_i, y_i^+, y_i^-)\\}_{i=1}^{m}\nNext, a reward model r(x, y) \u2192 R is trained to approximate human's satisfaction level of response y given prompt x. Following the Bradley-Terry (BT) model (Bradley and Terry, 1952), the probability of preferring y+ over y\u00ae is given by:\nP(y^+ > y^- | x) = \\sigma(r(x,y^+) \u2013 r(x,y^-)),\nwhere \\sigma is the logistic function. The reward model r(x, y) is then optimized via maximum likelihood estimation by as a binary classification problem:\nr = arg \\min_r E_{(x,y^+,y^-)\\sim D} [-log P(y^+ > y^- | x)]."}, {"title": "3.2 Personalized Reward Modeling", "content": "To capture individual preferences, the reward model must adapt its predictions based on user identity. Formally, this means extending the vanilla reward model r(x, y) to incorporate user information, yielding r(x, y, u). Below we summarize baseline approaches and recent methods from the literature that we consider in our evaluation. Individual Reward Modeling trains a dedicated reward model ru for each user u using only their personal preference data Du. As shown in Equation 1, each model maximizes the likelihood of its user's observed preferences and thus would in theory obtain optimal personalization provided there are sufficient preference data for each user."}, {"title": "4 Evaluation", "content": "Given the challenges and costs of collecting large-scale, open-domain personalized preference datasets, researchers have explored both carefully curated narrow-domain human annotated and general-domain synthetic data generation approaches (Stiennon et al., 2020; Jang et al., 2023; Zollo et al., 2024; Castricato et al., 2024). We focus on three datasets that provide pairwise preference annotations - a format particularly suited for preference learning:"}, {"title": "5 Results", "content": "Personalized RM Achieves the Best Performance across All Datasets. As shown in Figure 2,, in terms of reward modeling accuracy, personalized RM consistently outperforms all methods across all datasets. Its success over individual reward modeling can be attributed to the its collaborative learning - leveraging signals for all users. Individual reward models, while serving as simple yet effective baselines, achieve the second-best performance. Both of them surpass other baselines by a significant margin on Personal LLM and performs even better on P-SOUPS. We attribute this to its superior ability to handle the high interpersonal disagreement nature of P-SOUPS. On TL;DR, all methods-except RAG-perform comparably. RAG, in contrast, exhibits the weakest performance among all personalization methods across all datasets, with accuracy approaching that of random guessing. This is likely due to the limitations of the 7B model in capturing nuanced user preferences through in-context learning. Dataset Properties Predict Personalization Gains. Figure 2d compares three representative preference learning approaches across all evaluation datasets, ranging from no personalization (Vanilla RM) to simple personalization (Individual RM) to complex personalization (PRM). The results demonstrate that personalization gains strongly correlate with our proposed room for personalization metric. P-SOUPS, with the highest room for personalization (Table 2), shows the greatest improvement from personalization methods. In contrast, TL;DR's low inter-personal disagreement limits the gains from personalization appraoches. These empirical results validate our analytical framework for characterizing personalization datasets. Personalization Methods can Scale with More Training Samples. As expected, increasing the number of training samples can generally improves RM accuracy for all methods when they are capable of learning personalized RMs. However, since Conditional RM and GPO are not effective at learning personalized preferences from P-SOUPS, their performance does not improve with the addition of more training data. We attribute this to these methods' limitations in modeling high interpersonal disagreement, a defining characteristic of the P-SOUPS dataset. These findings highlight that different personalization methods exhibit varying levels of robustness when faced with increasingly divergent preference data. Personalization Protects Minority Viewpoints. While prior work has primarily focused on average performance metrics, we argue that a crucial function of personalization is protecting minority viewpoints that diverge from majority preferences. Figure 5 reveals that Vanilla RM fails to capture preference for such minority users. While Individual RM successfully preserves these minority preferences through dedicated per-user models, Personalized RM achieves only partial success. Through this analysis, we would like to point out a critical limitation in current personalization research: existing evaluation frameworks often treat all preference groups as equal, which can overlook the significance of minority groups due to their smaller sizes. This undermines the core objective of personalization, which is to preserve preference diversity. We argue that a personalization method's ability to preserve minority viewpoints should also be considered a critical evaluation metric for assessing personalization approaches. Adaptation to New Users. As discussed in Section 4.3, a critical challenge in real-world deployment is adapting personalization methods to new users with limited preference data. We evaluate this capability in scenarios where only 30-100-300 preference pairs are available per new user. Since RM fine-tuning approaches, including Personalized RM, do not inherently support this cold-start setup, we implement two additional baselines for comparison: (1) Retrieve Similar User RM: we identify the existing user whose preferences most similar to the new user and directly apply the reward model of that user. (2) Further Fine-Tune Trained RM: We take the Vanilla RM trained on aggregated existing users preference data and fine-tune it for one epoch using the new user's limited data. The results shown in Figure 3 demonstrate that GPO significantly outperforms these baselines, approaching the upper bound (individual RMs trained on complete 100K user data) with just 30-300 samples. The Similar-User RM performs only marginally better than Vanilla RM, indicating that simple user-matching strategies are insufficient for effective personalization. These findings reveal the power of meta-learning-based approaches and urge further exploration of making reward modeling more effective in limited data settings. Personalization Can Hurt Model Safety and Reasoning To investigate potential negative impacts of personalization on core LLM capabilities, we evaluate models before and after personalization across the three dimensions of RewardBench (Lambert et al., 2024). Specifically, we fine-tune a pre-trained model (initially optimized for safety and reasoning) using individual reward modeling, with results shown in Figure 4. The effects of personalization vary substantially across datasets, aligning with our theoretical framework. For TL;DR, both preference prediction accuracy and safety/reasoning performance remain largely stable, consistent with our finding of limited room for personalization in Section 4.2. In contrast, Personal-LLM and P-SOUPS exhibit a concerning trade-off: while preference prediction accuracy improves significantly, we observe substantial degradation in both reasoning ability and safety performance. This degradation suggests that optimizing for individual preferences can compromise fundamental model capabilities, a phenomenon we term the \u201cpersonalization tax.\u201d These findings raise important concerns about the deployment of personalized LLM systems and underscore the need for careful balancing of personalization benefits against potential risks (Kirk et al., 2024a; Hui et al., 2024; Ai et al., 2024)."}, {"title": "6 Conclusion", "content": "This work addresses gaps in LLM personalization research by introducing a systematic evaluation framework. We establish a principled methodology for characterizing preference datasets through: inter-user disagreement, intra-user consistency, and minority representation. Our analysis across P-SOUPS, TL;DR, and Personal-LLM datasets reveals distinct challenges that personalization methods must address, from high disagreement to varying levels of minority viewpoint representation. Our comprehensive evaluation framework extends beyond accuracy to address practical constraints and potential risks. Through this lens, we evaluate eight representative personalization methods, finding that Individual RM provides a strong baseline while collaborative approaches like PRM achieve up to 6% improvement. Notably, some methods successfully preserve minority preferences that standard RLHF would overlook. However, we also identify a \"personalization tax,\" where optimizing for individual preferences can degrade model safety and reasoning capabilities. These findings demonstrate both the promise and challenges of personalization. We hope this work's systematic framework and empirical insights will guide the development of more robust, inclusive, and responsible personalization approaches that can better serve diverse global user."}, {"title": "Limitation", "content": "Firstly, two datasets that we evaluated on (P-SOUPS and Personal-LLM), are synthetically generated. These datasets make simplifying assumptions about human preferences, particularly regarding intra-personal consistency, which may not reflect the nuanced, context-dependent nature of real-world preferences. However, these controlled datasets serve a valuable purpose in our study: they clearly demonstrate how dataset characteristics interact with personalization algorithms to produce varying outcomes. While the collection of large-scale, open-domain personalized preference data from real users would be ideal for future work, such efforts face significant challenges related to cost, privacy, and scalability. Secondly, we evaluated 8 methods where 3 of them, VPL (Poddar et al., 2024), GPO (Zhao et al.), Personalized RM (Li et al., 2024) are specifically developed for personalized preference learning. The rapidly evolving nature of this field means our evaluation cannot be exhaustive. Recent developments in prompt optimization (Kim and Yang, 2024) and context compression (Kim et al.) suggest promising new directions that warrant investigation. Although resource constraints prevented us from evaluating all emerging approaches, we believe our selected methods effectively represent the key algorithmic paradigms currently employed in personalized preference learning."}, {"title": "Ethical Statement", "content": "Current LLM alignment approaches, where a relatively small group of researchers and organizations dictate alignment targets, raise significant concerns about procedural justice and representation (Santurkar et al., 2023). LLM personalization presents a promising solution by democratizing alignment, enhancing user experiences, responding to diverse needs, and promoting a more equitable and just information ecosystem. However, these personalized systems also pose risks, including the potential creation of filter bubbles, reinforcement of existing biases, and exacerbation of ideological polarization. Additionally, while our study does not involve personally identifiable information, real-world deployment of personalized LLMs requires strong privacy safeguards to prevent the misuse of sensitive user data. Our findings further show that optimizing for individual preferences may lead to safety misalignment as discussed in Section 5. The central challenge, then, becomes how to balance the benefits and risks of LLM personalization (Kirk, 2024). These concerns highlight the importance of developing responsible personalization methods that prioritize fairness, privacy, and safety."}, {"title": "A Appendix", "content": "A.1 Hyperparameter Selection For Vanilla RM, Individual RM, and Conditional RM, we fine-tune the model with learning rate of 3e-4 with LoRA rank of 16 and LoRA alpha of 32. Following the optimization literature (McCandlish et al., 2018), the total number of optimization steps for training with different sample size should be kept the same. Thus we do hyperparameter search of the training eposes, we train 1 epoch on 100,000 samples. We search over 1,3,10 epoch on 10,000 samples and 1, 10, 100 epoch on 1,000 samples. For VPL, GPO, PRM, we use the same hyper-parameter setup as their paper except we search over the number of training epochs as above."}]}