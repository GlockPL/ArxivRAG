{"title": "EXPONENTIAL TOPOLOGY-ENABLED SCALABLE COMMUNICATION IN MULTI-AGENT REINFORCEMENT LEARNING", "authors": ["Xinran Li", "Xiaolu Wang", "Chenjia Bai", "Jun Zhang"], "abstract": "In cooperative multi-agent reinforcement learning (MARL), well-designed communication protocols can effectively facilitate consensus among agents, thereby enhancing task performance. Moreover, in large-scale multi-agent systems commonly found in real-world applications, effective communication plays an even more critical role due to the escalated challenge of partial observability compared to smaller-scale setups. In this work, we endeavor to develop a scalable communication protocol for MARL. Unlike previous methods that focus on selecting optimal pairwise communication links\u2014a task that becomes increasingly complex as the number of agents grows\u2014we adopt a global perspective on communication topology design. Specifically, we propose utilizing the exponential topology to enable rapid information dissemination among agents by leveraging its small-diameter and small-size properties. This approach leads to a scalable communication protocol, named ExpoComm. To fully unlock the potential of exponential graphs as communication topologies, we employ memory-based message processors and auxiliary tasks to ground messages, ensuring that they reflect global information and benefit decision-making. Extensive experiments on large-scale cooperative benchmarks, including MAgent and Infrastructure Management Planning, demonstrate the superior performance and robust zero-shot transferability of ExpoComm compared to existing communication strategies.", "sections": [{"title": "INTRODUCTION", "content": "Cooperative multi-agent reinforcement learning (MARL) has recently emerged as a promising approach for complex decision-making tasks across diverse real-world applications, such as resource allocation (Ying & Dayong, 2005), package delivery (Seuken & Zilberstein, 2007), autonomous driving (Zhou et al., 2021), robot control (Swamy et al., 2020), and infrastructure management planning (Leroy et al., 2024). Under the widely adopted centralized training and decentralized execution (CTDE) paradigm (Kraemer & Banerjee, 2016; Lyu et al., 2021), algorithms like MADDPG (Lowe et al., 2017), COMA (Foerster et al., 2018), MATD3 (Ackermann et al., 2019), QMIX (Rashid et al., 2020), and MAPPO (Yu et al., 2022) have achieved notable success.\nTo enhance agent collaboration in partially observable scenarios, communication mechanisms have been incorporated into multi-agent systems (MASs) to assist in decentralized decision-making (Sukhbaatar et al., 2016). Enabling information exchange during execution helps MARL algorithms to address non-stationarity and partial observability prevalent in these environments. Building upon this foundation, researchers have devoted efforts to designing effective communication protocols, focusing on three core considerations: 1) whom the agents should communicate with (Ding et al., 2020; Hu et al., 2024); 2) when communication should occur (Hu et al., 2021; Kim et al., 2019); and 3) how the agents should design and utilize the communication messages"}, {"title": "RELATED WORK", "content": "Communication in MASS Communication among agents in MARL was first introduced by Sukhbaatar et al. (2016); Foerster et al. (2016) and has since become an active research area due to its potential to enhance cooperation and improve task performance. The flexibility of communication protocols makes finding effective solutions for the MARL paradigm challenging (Zhu et al., 2024). To address this difficulty, many studies have focused on optimizing communication components, such as message generators, message aggregators, and connectivity among agents, through end-to-end training (Peng et al., 2017). From the sender side, ToM2C (Wang et al., 2022) and MAIC (Yuan et al., 2022) enhance message generation through teammate modeling, while CACL (Lo et al., 2024) uses contrastive learning techniques to learn communication encoding in a decentralized training paradigm. From the receiver side, TarMAC (Das et al., 2019), G2ANet (Liu et al., 2020), and MASIA (Guan et al., 2022) improve message aggregation using attention-based strategies.\nRecently, researchers have addressed challenges posed by real-world communication systems. Notably, NDQ (Wang et al., 2020b) and TMC (Zhang et al., 2020) reduce communication costs by crafting succinct messages, while ATOC (Jiang & Lu, 2018), IC3 (Singh et al., 2019), I2C (Ding et al., 2020), and CommFormer (Hu et al., 2024) manage overhead by pruning unnecessary communication links. Additionally, Freed et al. (2020) propose a stochastic encoding/decoding scheme to handle noisy channels, and DACOM (Yuan et al., 2023) introduces delay-aware communication to account for the high latency of wireless channels.\nDespite these advancements, scalability in communication mechanisms has been largely overlooked, often due to the quadratically increasing communication cost associated with fully-connected graphs as the number of agents grows. Although few works explicitly address the scalability issue, efforts to design communication topologies among agents offer potential solutions. These can be categorized into fully-connected, rule-based, and learned topologies. Early works (Sukhbaatar et al., 2016; Foerster et al., 2016; Peng et al., 2017) typically adopt fully-connected topologies to demonstrate communication benefits, but at the cost of high bandwidth requirements. Later on, to reduce the overall communication overhead, Jiang et al. (2020) and Weil et al. (2024) restrict communication to nearby neighbors based on distance, while NeurComm (Chu et al., 2020) limits communication to neighboring agents in networked MASs. In spite of achieving significant performance gains, their further applicability may be limited since they require extra information beyond local observation to determine the communication topology. In contrast, learned topology methods assume no such requirements and offer high flexibility. In particular, ATOC (Jiang & Lu, 2018), IC3 (Singh et al., 2019), I2C (Ding et al., 2020) locally deploy gates for agents to decide if they should engage in communication. However, these methods may result in uncontrollable overall communication costs due to individual control schemes. Alternatively, MAGIC (Niu et al., 2021) utilizes graph attention mechanisms to learn the communication topology, while CommFormer (Hu et al., 2024) extends the idea and enables control over the overall communication sparsity. Although effective in small-scale MASs, peer-wise connectivity becomes increasingly difficult to learn in large-scale MASs, and high sparsity may impair performance, as discussed by Hu et al. (2024).\nOur proposed ExpoComm, which incorporates rule-based topologies for rapid information dissemination among all agents, complements existing efforts in MAS communication by explicitly addressing scalability challenges.\nExponential Graphs Exponential graphs are a class of graph topologies that exhibit strong scalability properties with respect to the number of nodes. They have been primarily used in distributed learning to periodically synchronize model updates across devices. Assran et al. (2019) investigate exponential graphs with gossip algorithms and achieve high consensus rates for decentralized learning. Follow-up works (Wang et al., 2020a; Ying et al., 2021; Kong et al., 2021; Yuan et al., 2021) build upon this topology, optimizing model weight update algorithms and providing empirical evidence and theoretical guarantees for the effectiveness of exponential graphs. Beyond distributed learning, exponential graphs also have applications in chip design (Wang et al., 2015; 2016). Overall, exponential graphs demonstrate efficient information dissemination across many nodes, making them a promising candidate topology for achieving scalable communication in MARL."}, {"title": "SCALABLE COMMUNICATION WITH EXPONENTIAL GRAPH IN MARL", "content": "In this section, we propose ExpoComm, which leverages exponential graphs as communication topologies among agents in MARL to enable scalable communication. We structure the following subsections to address three key questions: 1) Why and how should exponential graphs be adapted for agent communication? 2) How can the corresponding neural network architecture be designed to effectively utilize the messages transmitted through these topologies? 3) How can messages propagated among agents be grounded to ensure their usefulness?\nIn Section 3.1, we outline the requirements for scalable communication: effective information dissemination among agents and low communication overhead. We translate these requirements into the challenge of identifying topologies with small diameters and sizes, key properties of exponential topologies. In Section 3.2, we discuss how memory-based message processors can enable meaningful message encoding, leveraging the small-diameter property over multiple timesteps within exponential topologies. In Section 3.3, we adopt a global perspective to ground messages using a global state reconstruction auxiliary task and contrastive learning, as ExpoComm aims to facilitate message flow across the entire graph rather than focusing on local features."}, {"title": "EXPONENTIAL GRAPH AS THE COMMUNICATION TOPOLOGY", "content": ""}, {"title": "PROBLEM SETTING", "content": "In this work, we consider a fully cooperative partially observable multi-agent task, which can be modeled as a decentralized partially observable Markov decision process (Dec-POMDP) (Oliehoek & Amato, 2016). The Dec-POMDP is defined by a tuple M = (S, A, P, R, \u03a9, \u039f, \u039d, \u03b3) with N being the number of agents and \u03b3\u2208 (0, 1] being the discount factor. At each timestep t, with the"}, {"title": "COMMUNICATION TOPOLOGIES", "content": "To design an effective and scalable communication protocol in many-agent systems, it is essential to determine whom to communicate with, i.e., to construct the communication topology so that communication is both beneficial for decision-making and cost-effective. While previous work (Hu et al., 2024) assumes a static communication topology, we adopt a more flexible, time-varying directed graph \\(G_t = \\langle V,E_t\\rangle\\), where node \\(v_i \\in V\\) denotes agent i and edge \\(e_{ij} \\in E_t\\) indicates a communication link from agent i to agent j at timestep t.\nFrom a graph perspective, we consider the following desiderata for the communication topology:\n\u2022 Small graph diameter for fast information dissemination: Formally defined as \\(diameter(G_t) = \\max_{v_i,v_j\\in V} d(v_i, v_j)\\) with d(vi, vj) representing the shortest path distance from vi to vj, the graph diameter indicates how quickly messages travel through the graph. Since communication aids multi-agent decision-making by providing the locally observant agents with global information and alleviating the non-stationarity, a graph with a small diameter can expedite message exchange and is therefore desirable.\n\u2022 Small size for low communication overhead: Formally defined as |Et|, the size of a graph denotes the total number of edges, corresponding to the number of communication links in an MAS. We assume that any message transmission incurs the same overhead, therefore the total overhead scales with the number of links. Given the high hardware requirement for communication modules and the potential delays induced by densely connected communication topologies, we prefer graphs with a small size in many-agent settings."}, {"title": "EXPONENTIAL GRAPHS", "content": "Based on the desiderata above for the communication topologies, we draw inspiration from graph literature and choose exponential graphs (Assran et al., 2019; Ying et al., 2021) as a promising candidate for communication topology in many-agent systems. Below, we introduce two variants of exponential graphs and demonstrate their small-diameter and small-size properties through an illustrative example.\nStatic Exponential Graph Assuming a randomly sequential ordering of agents 0, 1, . . ., N \u2013 1 and the corresponding adjacency matrix E \u2208 {0,1}N\u00d7N, in the static exponential graph, each agent communicates with peers that are 20, 21, . . ., 2[log2 (N-1)] hops away, which is illustrated by Figure 1a. Formally, we have\n\n\\(E^{t(stat)}_{ij} = \\begin{cases} 1 & \\text{if } log_2((j \u2013 i) \\mod N) \\text{ is an integer or } i = j \\\\ 0 & \\text{otherwise} \\end{cases}\\)\nOne-peer Exponential Graph In the one-peer exponential graph, each agent iterates through different peers that are 20, 21, . . ., 2[log2 (N-1)] hops away, which is illustrated by Figure 1b. Formally, we have\n\\(E^{t(one-peer)}_{ij} = \\begin{cases} 1 & \\text{if } log_2((j \u2013 i) \\mod N) = t \\mod [log_2(N - 1)] \\text{or } i = j \\\\ 0 & \\text{otherwise} \\end{cases}\\)\nProperties Using the adjacency matrices defined above, we verify that the graph diameter for both static and one-peer exponential graphs is [log2 (N \u2212 1)] (see Appendix A for details). As discussed in Section 3.1.2, a small diameter facilitates efficient information dissemination, especially when the number of agents N is large.\nRegarding communication costs, static exponential graphs have a size of N \u22c5[log2 (N \u2212 1)], while one-peer exponential graphs have a size of N. Notably, the size of one-peer exponential graphs scales linearly with the number of agents, meaning the overall communication overhead also scales linearly.\nTo illustrate these properties, we provide a toy example in Figure 2. We visualize the message dissemination abilities of different communication topologies under varying communication budgets. In this example, with N = 256 agents, graph sizes (communication budgets) |Et| are set to N \u22c5 log2 N and N, respectively. In Figure 2, we observe that for each communication topology, reducing the graph sizes (as shown in Figures 2b, 2d and 2f) typically slows down dissemination speed due to increased graph diameters. This illustrates a trade-off between graph diameter and size, reflecting the trade-off between communication performance and overhead in many-agent systems. Sparser graphs with smaller sizes result in slower message dissemination but lighter communication overhead. However, exponential topologies strike a balance in this trade-off, demonstrating strong information diffusion even with a minimal communication budget of N.\nBased on these observations, we conclude that exponential topologies are well-suited for many-agent communication because: 1) In exponential topologies, any two agents can exchange messages in"}, {"title": "NEURAL NETWORK ARCHITECTURE DESIGN", "content": "With exponential graphs serving as the communication topology in ExpoComm, we we elaborate on the neural network architecture to help agents utilize received messages for better decision-making. The overall architecture is illustrated in Figure 3. ExpoComm is based on the concept of facilitating message flow across all agents within a certain timeframe, where the graph diameter indicates the length of such timeframe. To capitalize on the small graph diameter of exponential graphs, the message-processing module at each agent should ideally preserve all information received within diameter(Gt) timesteps. However, preserving all messages across multiple timesteps is not memory-efficient, so we employ sequential neural networks, such as attention blocks and recurrent neural networks (RNNs), for message processing."}, {"title": "TRAINING AND EXECUTION DETAILS", "content": "Following the QMIX (Rashid et al., 2020) algorithm, we update the network parameters \u03b8 with the objective of minimizing the temporal difference (TD) error loss:\n\n\\(L^{TD}(\u03b8) = E_{(s_t,o_t,a_t,r_t,s_{t+1},o_{t+1})\\sim D} [(y^{tot} - Q^{tot}(s_t, o^*_t, a^*_t;\u03b8))^2]\\)\n\nwhere \\(y^{tot} = r + \\gamma \\max_a Q^{tot}(s_{t+1}, o_{t+1}, a; \u03b8^-)\\) and \u03b8\u00af represents the parameters of the target network as in DQN.\nHowever, communication inevitably enlarges the policy space, making it more challenging to find the optimal policy relying solely on the MARL training objective (Li & Zhang, 2024). To facilitate learning meaningful messages, we introduce auxiliary tasks to restore global information from local messages. From a message perspective, we aim for it to traverse among agents over multiple timesteps, accumulating new information along the way, and ultimately reflecting global information useful for decision-making.\nMessage grounding with the global state In scenarios where the global state is available during training, the auxiliary loss is given by the prediction error of the current global state:\n\n\\(L^{pred}_{aux}(\u03b8, \\phi) = E_{(s_t,o_t)\\sim D} [s_t - f(m^i_t; \\phi))^2]\\)\n\nwhere the learnable auxiliary network for prediction f(\u00b7; \u03c6) is used to ground the messages and can be discarded after training.\n\n\nMessage grounding without the global state Alternatively, when the global state is unavailable during training, we use contrastive learning for meaningful message encoding, similar to Lo et al. (2024). Specifically, we treat messages from different agents at the same timestep as positive pairs and messages with intervals larger than diameter(Gt) as negative pairs, encouraging local messages m to reflect the current global latent state. The corresponding auxiliary loss is given as the InfoNCE loss (Oord et al., 2018):\n\n\\(L^{cont}_{aux}(\u03b8) = -E_{i,j,t,t^{'}} \\log \\frac{\\exp{(g(m^i_t) \\cdot g(m^j_t)/\\tau)}}{\\sum_{m \\in M} \\exp{(g(m^i_t) \\cdot g(m)/\\tau)}}\\)\n\nwhere i is uniformly sampled from {0, . . ., N}, j is uniformly sampled from {0,..., N : j \u2260 i}, \\(M = \\{m_{t'}^k : k \\in \\{0,..., N\\}, t^{'} \\notin [t-\\text{diameter}(G_t), t+\\text{diameter}(G_t)]\\} \\cup \\{m^j_t\\}\\) with |M| = M + 1 and m is uniformly sampled from M. g(\u00b7) is the normalization function, M is the hyperparameter indicating the number of negative pairs and \u03c4 is the temperature hyperparameter. The overall training loss is:\n\n\\(L^{TD}(\u03b8) = L^{TD}(\u03b8) + \\alpha \\cdot L_{aux}(\u03b8; \\phi)\\)\n\nwhere \u03b1 is the hyperparameter and \\(L_{aux}(\\cdot)\\) is the auxiliary loss defined by Equation (4) or Equation (5), depending on whether global information is available during training. The training and execution procedures are summarized in Algorithm 1."}, {"title": "EXPERIMENTAL RESULTS", "content": "In this section, we evaluate ExpoComm on two large-scale multi-agent benchmarks: MA-gent (Zheng et al., 2018) and Infrastructure Management Planning (IMP) (Leroy et al., 2024). All experiments are averaged over five random seeds and the shaded areas represent the 95% confidence interval. Details on network architecture and the training hyperparameters are available in Appendix B.1."}, {"title": "EXPERIMENTAL SETUPS", "content": "Environment descriptions In this section, We test ExpoComm and baselines across twelve scenarios in two large-scale benchmarks, with the number of agents ranging from 20 to 100. Specifically, MAgent is a particle-based gridworld environment representative of the typical MARL gaming benchmarks. To expand the variety of tasks, we also include the IMP benchmark, with tasks oriented from real-world applications. More details regarding the environment settings are provided in Appendix B.2."}, {"title": "RESULTS", "content": "Benchmark results We present the comparative performance of ExpoComm and baselines in MAgent and IMP environments with Figure 4 and Table 1, respectively. Overall, ExpoComm demonstrates superior performance in these large-scale benchmarks under both communication budgets, underscoring the scalability and robustness of ExpoComm strategies. Notably, the one-peer version of ExpoComm achieves the best performance in most scenarios, despite communication costs that only grow linearly with the number of agents. This makes it the most suitable method for handling large-scale MARL communication problems under very low communication budgets. Additional visualization results to illustrate the learned policies are provided in Appendix \u0421.1.\nZero-shot transfer Similar to the experimental settings suggested by Wang et al. (2022), we test the zero-shot transfer ability of our proposed ExpoComm and the baseline methods, reporting the"}, {"title": "CONCLUSIONS", "content": "In this work, we explored scalable communication strategies in MARL and introduced ExpoComm, an exponential topology-enabled communication protocol. We proposed a framework with communication topologies featuring small diameters for fast information dissemination and small graph sizes for low communication overhead. This framework is complemented by memory-based message processors and message grounding through auxiliary objectives to achieve effective global information representation. Despite requiring only (near-)linear communication costs relative to the number of agents, ExpoComm demonstrated superior performance and strong transferability on large-scale benchmarks like MAgent and IMP. This study highlights the potential for enhancing the scalability of MARL communication strategies through the explicit design of communication topologies."}, {"title": "A THEORETICAL ANALYSIS", "content": "In this section, we analyze the communication effect of exponential topologies.\nTheorem 1. Suppose that \\(E^t\\) is defined by Equation (2). Let \\(T = [log_2(N - 1)]\\). Then, the following holds:\n\n\\(E^t_{ij}(E_{ij}^{t+1})^b ... (E_{ij}^{t+T-1})^b = 11^T\\)\n\nwhere \u00d7b denotes logical (Boolean) matrix multiplication.\nRemark 1. If the information at each agent remains valid within \u03c4 timesteps and there is no information loss during aggregation, the one-peer exponential topology ensures information exchange between any two agents in the system with \u03c4 timesteps.\nRemark 2. Static exponential topologies exhibit a similar communication effect as described in Theorem 1. Specifically, \u2200i, j that \\(E_{ij}^{t(one-peer)} = 1\\), it holds that \\(E_{ij}^{t(stat)} = 1\\).\nProof. Define function Z : R+ \u2192 {0,1} such that\n\n\\(Z(x) = \\begin{cases} 1, x > 0, \\\\ 0, x = 0. \\end{cases}\\)\n\nThen, for all x, y, u, v \u2265 0, the following equivalence holds:\n\n\\(xy + wv = 0 \u21d4 (Z(x) \\times_b Z(y)) +_b (Z(u) \\times_b Z(v)) = 0\\)\n\nwhere \u00d7b denotes logical (Boolean) And, and +b denotes logical (Boolean) Or. Now, consider the connection between Z(x) and the structure of an all-one matrix. For a non-negative matrix X, it holds that Xij \u2208 R+, \u2200i, j \u2194 Z(X) = 11T.\nTherefore, by applying Appendix A to E1 E+1... E+-1 = 2117 (Ying et al., 2021), we have"}, {"title": "EXPERIMENT DETAILS", "content": ""}, {"title": "NETWORK ARCHITECTURE AND HYPERPARAMETERS", "content": "Codebase Our implementation of ExpoComm and baseline algorithms is based on the following codebase:\n\u2022 EPyMARL (Papoudakis et al., 2021): https://github.com/uoe-agents/epymarl\n\u2022 CommFormer Hu et al. (2024): https://github.com/charleshsc/CommFormer\n\u2022 CommNet (Sukhbaatar et al., 2016): https://github.com/isp1tze/MAProj\nThe code for ExpoComm is publicly available at https://github.com/LXXXXR/ ExpoComm.\nNeural network architecture Following previous work Papoudakis et al. (2021), we employ deep neural networks consisting of multilayer perceptrons (MLPs) with rectified linear unit (ReLU) activation functions and gated recurrent units (GRUs) to parameterize the agent networks. In ExpoComm, the message memory blocks described in Section 3.2 are implemented using a single GRU or an attention block. The prediction network f(\u00b7; \u03c6) described in Section 3.3 is implemented using a two-layer MLP.\nHyperparameters To ensure a fair comparison, we implement our method and self-constructed baselines using the same codebase with the same set of hyperparameters, with the exception of method-specific ones and the learning rate. In general, we follow the common settings provided by Papoudakis et al. (2021) for MAgent benchmark and adopt the settings in IMP paper (Leroy et al., 2024) for the IMP benchmark. The common hyperparameters are listed in Table 2. The ExpoComm-specific hyperparameters are provided in Table 3. For learning rate, we search among"}, {"title": "ENVIRONMENTAL DETAILS", "content": "Codebase The environments used in this work are listed below with descriptions in Table 4.\n\u2022 MAgent (Zheng et al., 2018; Terry et al., 2020):\nhttps://github.com/Farama-Foundation/MAgent2\n\u2022 IMP (Leroy et al., 2024): https://github.com/moratodpg/imp_marl\nMAgent MAgent is a highly scalable gridworld gaming benchmark shown in Figure 7. In AdversarialPursuit, agents aim to tag adversaries while adversaries try to escape. Agents"}, {"title": "IMPLEMENTATION DETAILS", "content": "In MAgent, we implement our proposed ExpoComm along with baselines DGN+TarMAC and ER on top of the IDQN base algorithm. In IMP, these are implemented on top of QMIX. For ExpoComm, we use Equation (4) for MAgent benchmark because the global state is provided in this environment, and Equation (5) for IMP, as the global state is a concatenation of all observations and is not compact or suitable for message grounding."}, {"title": "EXPERIMENTAL INFRASTRUCTURE", "content": "The experiments were conducted using NVIDIA GeForce RTX 3080 GPUs and NVIDIA A100GPUs. Each experimental run required less than 2 days to complete."}, {"title": "MORE RESULTS AND DISCUSSION", "content": ""}, {"title": "VISUALIZATION RESULTS", "content": "We visualize the final trained policies of ExpoComm and IDQN in dversarialPursuit and Battle with Figure 9 and Figure 10 respectively to demonstrate how ExpoComm enhances cooperation among agents. As shown in Figure 9a and Figure 10a, agents adopt a global perspective and act cooperatively with ExpoComm policies, demonstrating effectiveness even under extreme communication budgets(K = 1). In comparison, IDQN agents focus only on local observations and often become trapped in suboptimal solutions due to lack of coordination."}, {"title": "COMPARISON WITH PROXY-BASED COMMUNICATION", "content": "Although we primarily focus on decentralized communication-based MASs without centralized proxies, we also compare ExpoComm against the proxy-based CommNet (Sukhbaatar et al., 2016). As seen in Figure 11 and Table 6, ExpoComm outperforms CommNet in most scenarios, especially in IMP benchmarks. However, CommNet achieves comparable performance on the AdversarialPursuit tasks. This implies that a global perspective is more crucial for success in these scenarios, possibly explaining ExpoComm's larger advantage over other baselines in this scenario."}, {"title": "LIMITATIONS AND FUTURE WORK", "content": "While ExpoComm demonstrates strong performance and scalability in cooperative multi-agent tasks, some limitations remain.\nFirst, ExpoComm does not explicitly incorporate agent heterogeneity or properties of the underlying environmental MDP when constructing the communication topology. This could result in suboptimal performance in scenarios requiring targeted messaging between specific agents (Yuan et al., 2022) or in networked MDPs (Zhang et al., 2018; Ma et al., 2024). Therefore, Incorporating factors like agent identities or relationships presents a promising direction for further improvements in such settings.\nSecond, we evaluated ExpoComm primarily in fully cooperative tasks. Partially competitive settings requiring agents to learn to communicate only when necessary remain challenging. Examining ExpoComm's capabilities and limitations in such partially competitive tasks presents an important avenue for future work.\nFinally, communication scalability in multi-agent systems remains an under-explored area despite the attempt of this work. For instance, incorporating finer graph topologies beyond exponential graphs may enhance performance, and exploiting temporal communication sparsity could further reduce costs. There are still many open questions in scaling communication efficiently."}]}