{"title": "Achieving Peak Performance for Large Language Models: A Systematic Review", "authors": ["Zhyar Rzgar K Rostam", "S\u00e1ndor Sz\u00e9n\u00e1si", "G\u00e1bor Kert\u00e9sz"], "abstract": "In recent years, large language models (LLMs) have achieved remarkable success in natural language processing (NLP). LLMs require an extreme amount of parameters to attain high performance. As models grow into the trillion-parameter range, computational and memory costs increase significantly. This makes it difficult for many researchers to access the resources needed to train or apply these models. Optimizing LLM performance involves two main approaches: fine-tuning pre-trained models for specific tasks to achieve state-of-the-art performance, and reducing costs or improving training time while maintaining similar performance. This paper presents a systematic literature review (SLR) following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement. We reviewed 65 publications out of 983 from 2017 to December 2023, retrieved from 5 databases. The study presents methods to optimize and accelerate LLMs while achieving cutting-edge results without sacrificing accuracy. We begin with an overview of the development of language modeling, followed by a detailed explanation of commonly used frameworks and libraries, and a taxonomy for improving and speeding up LLMs based on three classes: LLM training, LLM inference, and system serving. We then delve into recent optimization and acceleration strategies such as training optimization, hardware optimization, scalability and reliability, accompanied by the taxonomy and categorization of these strategies. Finally, we provide an in-depth comparison of each class and strategy, with two case studies on optimizing model training and enhancing inference efficiency. These case studies showcase practical approaches to address LLM resource limitations while maintaining performance.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, dense deep learning models have seen an extraordinary growth in the number of parameters [1]\u2013[3]. Transformer as an effective deep learning architecture has been widely used over the recent years, and transformer- based models have achieved notable success and recognition in various fields including language modeling compared to the existing models [4]-[13].\nTo achieve significant accuracy in deep learning, large models with billions to trillions of parameters are essential. Therefore, deep learning models continue to grow in com- plexity with an array of large-scale models ranging from Bidirectional Encoder Representations from Transformers (BERTlarge, 340 million parameters) [8], Generative Pre- trained Transformer-3 (GPT-3, 175 billion parameters) [14], to General Language Model (GLM-3, 1.75 trillion parame- ters) [15]. With models now reaching trillions of parameters, even the most powerful GPUs are struggling to keep up [1]. This resource-intensive requirement is making it difficult for many researchers to access the computational resources they need to train these models [1], [4], [16]. Also, handling, managing, and fitting these models into device memory is a daunting challenge due to memory limitations, and this tremendous size of data brings complexity, and requires high- end computing resources with significant memory require- ments to process [5], [17]\u2013[19]. Training large-scale models effectively require significant adjustments [20]-[24], espe- cially in terms of increasing training throughput and loading these kinds of large models into GPU memory [18].\nAs a result, developing frameworks, libraries and propos- ing new techniques to overcome the mentioned challenges has become an essential task. There are many studies that have worked on possibilities for optimization and acceleration with large models and using various techniques to achieve state- of-the-art (SOTA) results without sacrificing accuracy. These remarkable advancements in the field of language models (LMs) required a systematic review of recent LM optimiza- tion and acceleration techniques. To address these challenges and guide future research, this SLR paper aims to:\n\u2022 Analyze recent optimization and acceleration techniques for LLMs.\n\u2022 Identify challenges associated with training, inference, and system serving for LLMs (billions/trillions of pa- rameters).\n\u2022 Develop a structured taxonomy to categorize LLM opti- mization techniques.\n\u2022 Review and evaluate recent libraries and frameworks designed for LLM optimization.\n\u2022 Identify promising areas for future research in LLM development, focusing on efficiency, scalability, and flexibility.\nIn this SLR we are making the following contributions:\n\u2022 Comprehensive overview: We offer a comprehensive overview of the development of language modeling (Section II), detailing commonly used frameworks and libraries (Section IV), and recently used techniques and strategies (Sections V, VI, VII). This serves as a valu- able resource for understanding the current landscape of LLM optimization.\n\u2022 Taxonomy of optimization strategies: We categorize optimization strategies into three classes: training op- timization, hardware optimization, and scalability and reliability. This taxonomy helps clarify the various ap- proaches and their specific applications (presented in Fig 4, Sections V, VI, VII).\n\u2022 Detailed analysis of techniques: Our analysis explores recent optimization and acceleration strategies, we pro- vide two comparative analyses regarding performance, cost, and scalability for reviewed strategies (presented in Tables 6 and 7) and their core categories: training optimization, hardware optimization, and scalability and reliability (presented in Table 5). In the latter analysis, we also consider the focus of classes.\n\u2022 Case studies: We include two in-depth case stud- ies that demonstrate practical approaches to optimiz- ing model training and enhancing inference efficiency. These case studies highlight how resource limitations can be addressed while maintaining performance (Sec- tions VIII-A, VIII-B).\n\u2022 Future direction: We explore a range of promising future directions for LLM development. These areas, detailed in specific sections, focus on enhancing efficiency, scal- ability, and flexibility for LLMs (Section X).\nThis review paper is organized as follows: an overview of lan- guage modeling development (Section II), followed by an in-depth explanation of the most commonly utilized frameworks and libraries specifically designed for optimizing and accel- erating large language models (LLMs) (Section IV, Tables 3 and 4), accompanied by taxonomy and categorization. Addi- tionally, it delves into recent optimization and acceleration strategies employed within LLMs, including the taxonomy and categorization of these strategies (presented in Fig. 1) (Section V, VI, VII), Table 8 summarizes the reviewed papers, excluding those already covered in Tables 3 and 4 or the main text. Moreover, we present an individual comparison in terms of performance, cost, and scalability for reviewed strategies discussed in Tables 6, and 7, and the classes (training opti- mization, hardware optimization, scalability and reliability) presented in Table 5. In addition to the mentioned factors, we consider the classes' focus in this comparison. Finally, we illustrate these concepts with two real-world examples: optimizing model training and improving inference efficiency through case studies (Section VIII)."}, {"title": "A. RELATED WORKS", "content": "In this section, we will present the related studies that inves- tigate optimization and acceleration with dense deep learn- ing models and LLMs. Jahan et al., in [25] present a sys- tematic literature review (SLR) by comparing 31 language models inspired by BERT, published between 2018 and 2020, to help researchers choose the best model based on their requirements. By analyzing each model's performance against ROBERTa, the study identified seven models that performed better, and the rest of the studies investigated with different parameter settings. The outperforming models varied in dataset size, suggesting that both large and small datasets can be effective depending on the model's archi- tecture. Ultimately, this research provides valuable insights for researchers seeking the optimal language model for their specific tasks. Yu et al [26] conduct a survey that explores the growing challenges and opportunities for optimizing large- scale deep learning systems. By highlighting recent advances in optimization techniques, it proposes a new way to cate- gorize and explain the different computing approaches used. Zhao et al [27] carry out a survey that focuses on the recent advancements in LLMs. The study concentrates on four major dimensions of LLMs: pre-training, adaptation tuning, uti- lization, and capacity evaluation. The survey emphasizes the techniques or discoveries essential for LLMs' success. Ad- ditionally, it provides an overview of available development resources and offers valuable guidelines for successful LLM implementation, drawing from the latest research. Bai et al., in [28] provide a systematic survey that provides an overview of LLM resource efficiency. It focuses on LLM significant resource consumption in computational, memory, energy, and financial aspects. It categorizes techniques aimed at improv- ing LLMs' resource efficiency. Standardized evaluation met-"}, {"title": "II. LANGUAGE MODELING DEVELOPMENT", "content": "Language modeling is a fundamental approach to enhancing the ability of machines to understand and process human language. It is a computational model that can learn and predict the possibilities of incoming (or missing) tokens [24]. The development of language models can be classified as follows (see Fig. 3):\n\u2022 N-gram language models, like bigrams and trigrams, are basic methods that learn from the frequency of word sequences in text [36], [37]. However, their limited context window restricts their ability to capture long- range dependencies and understand the deeper semantic relationships between words.\n\u2022 Markov assumption language models, refers to those models that predict the next word based on the most recent in the context [24]. Both n-gram and Markov assumption language models are commonly used to im- prove task performance in NLP, and information re- trieval (IR) [38].\n\u2022 Machine learning models, these models investigate ma- chine learning algorithms to enhance language compre- hension. They are trained on extensive text corpora to discern patterns and relationships [39]. The adoption of machine learning in NLP introduced a more advanced methodology, enabling the creation of applications such as spam detection [40] and sentiment analysis [41].\n\u2022 Neural language models, these models are developed based on NN for working with a sequence of data. They have a special ability of learning effective features for words or sentences. These studies [42]\u2013[44] have initiated the use of language models for representation learning (beyond word sequence modeling), and show\n\u2022 Transformer language models refer to those models that leverage the capabilities of a deep learning architecture called Transformer to process and understand human language [46], [47]. These models achieved remarkable results by using \"special attention mechanism\" to un- derstand the relationship between words and sentences. These models capture context-aware representation in- stead of learning fixed word representations, first pre- training then fine-tuning according to specific down- stream tasks [2], [8], [24], [48]. Transformer architecture has been used to build PLMs such as BERT [8], GPT-2 [49], and BART [50]. These models underwent training using bidirectional language models and specifically de- signed pre-training tasks applied to extensive unlabeled datasets. The growth in model size and data size has revolutionized the way we approach downstream tasks, enabling large-sized PLMs to achieve remarkable per- formance gains. These models exhibit unique character- istics compared to smaller PLMs, such as 330M-BERT and 1.5B-GPT-2, demonstrating exceptional abilities in solving complex tasks. As a result, LLM is the term used to refer to large-sized PLMs [48], [51], [52]."}, {"title": "III. MACHINE LEARNING MODELS", "content": "The process of building, deploying, and managing a machine learning model involves three distinct phases: training, in- ference, and system serving. Training is the foundation of machine learning, where a vast dataset of labeled data is used to develop a model that can identify patterns and relationships within the data. Inference is the application of the trained model, where new, unseen data is fed into the model to obtain predictions or classifications based on the learned patterns. System serving ensures the model's longevity and effective- ness in real-world applications, handling large volumes of requests, monitoring the model's performance, and providing continuous updates or modifications as needed [11], [19], [53]. In the section IV, we provide a categorization of the most recent frameworks and libraries utilized for LLMs optimiza- tion, structured into three primary classes: training, inference, and deployment and system serving (presented in Fig. 4). However, certain studies can be classified into two categories simultaneously, owing to their ability to handle multiple tasks, such as LightSeq2 (section IV-A4), TurboTransformers (sec- tion IV-B4), and PetS (section IV-B5)."}, {"title": "IV. FRAMEWORKS AND LIBRARIES", "content": "As most LLMs are designed based on Transformers, these models are a powerful type of neural network that have achieved SOTA results on a wide range of applications. To achieve these results the models are required to have a huge model size with hundreds of billions, even trillion of parame- ters. Training LLMs requires distributed training algorithms, which employ parallel processing techniques to efficiently train these massive models. To streamline distributed training, various optimization frameworks have been developed, pro- viding tools and infrastructure for implementing and deploy- ing parallel algorithms [24], [54], [56]. In this section, we will provide the most recent frameworks and libraries designed to overcome those limitations."}, {"title": "A. LLM TRAINING FRAMEWORKS AND LIBRARIES", "content": "This section will delve into the objectives and outcomes of LLM frameworks and libraries employed in the training phase. Additionally, a summary of each framework/library will be provided individually (see Table 3).\n1) GPipe\nGPipe [3] introduces a novel pipeline parallelism framework based on batch partitioning. It divides each mini-batch ex- ample into smaller micro-batches, which are subsequently executed in sequence across the cells. During training, it employs synchronous mini-batch gradient descent, where gradients from all micro-batches are aggregated and applied to the model at the end of the mini-batch. GPipe has been shown to train two large-scale models: a convolutional model for image classification and a transformer model for ma- chine translation. The convolutional model, AmoebaNet, was trained on 480\u00d7480 input from the ImageNet 2012 dataset. To enhance its performance, the model width was expanded, and its parameters were scaled up to 557 million. The model achieved a top-1 validation accuracy of 84.4%. Meanwhile, the transformer model a single 128-layer, 6-billion-parameter multilingual model trained across 103 languages, was also evaluated. GPipe achieved superior performance compared to training 350-million-parameter bilingual transformer big models individually across 100 language pairs. The model presents its efficiency by boosting the performance on a vari- ety of devices, with the support of flexibility on any deep net- work architectures, utilizing the synchronous gradient decent, and ensuring consistent training regardless of the number of partitions.\n2) ByteTransformer\nByteTransformer [4] is a transformer framework for GPU ac- celeration with an efficient and high performance optimized for variable-length inputs in NLP problems. The framework uses an algorithm for overcoming the redundant computa- tions on zero-padding tokens, and variable input length. Fur- thermore, the model proposed a fused Multi-Head Attention (MHA) to reduce the memory overhead of the intermediate matrix. This model manually optimizes the memory sizes of layer normalization by introducing bias and activation to maximize the overall system performance. It has been used by some famous applications including TikTok and Douying of ByteDance. The model was evaluated on an NVIDIA A100, focusing on the forward pass of BERT-like transform- ers, including BERT [8], ALBERT [57], DistilBERT, and DeBERTa. It showcased a significant improvement, enhanc- ing the fused MHA mechanism by 6.13\u00d7 compared to Py- Torch attention. Additionally, ByteTransformer outperformed PyTorch, TensorFlow, Tencent TurboTransformer [11], Mi- crosoft DeepSpeed [5], and NVIDIA FasterTransformer by 87%, 131%, 138%, 74%, and 55%, respectively, in terms of the end-to-end performance of a standard BERT transformer.\n3) Megatron-LM\nMegatron-LM [19] is a deep learning library for training LLMs efficiently and effectively. It enables the library for the training of very large transformer models with billions of parameters. It offers a set of optimization methods for dis- tributed training, it includes strategies like intra-layer model parallelism, and mixed-precision training. These optimiza- tion techniques significantly enhance training efficiency and speed, facilitating effective distributed training across mul- tiple GPUs. Megatron-LM operates independently without requiring new compiler or library changes. This makes it orthogonal and complementary to pipeline model parallelism, allowing for seamless integration and flexibility within exist- ing NLP frameworks.\nThe library has been shown to be highly effective for training LLMs. A Megatron-LM model with 8.3 billion pa- rameters was trained on 512 NVIDIA V100 GPUs using 8- way model parallelism and achieved sustained performance of up to 15.1 PetaFLOPs across the entire application. This is significantly faster than previous approaches to training"}, {"title": "4) LightSeq2", "content": "LightSeq2 [54] proposes a software library that accelerates the training of transformer-based models within GPUs. It is a system-level optimization while maintaining accuracy and training behavior. The system works with BERT (encoder), GPT (decoder), Transformer (encoder-decoder), and vision transformer. The system uses three techniques for improv- ing training speed and efficiency. First (layer-specific ker- nels), after analyzing Transformer-specific layers in detail, rewriting the kernels with dependencies and other techniques to improve parallelism, and using small kernels to improve GPU utilization. Second (mixed-precision trainer), instead of applying batch updates to many individual full-precision updates, it applies batch updates to reduced-precision param- eters. Finally, introduced an efficient memory management technique to minimize the need for frequent allocation and release calls. This strategy involves recycling the memory space of tensors that remain unused during the backward pass. The system accelerates the entire training process for trans- former models. LightSeq2 achieves significant performance improvement on a variety of NLP tasks, including machine translation, on the WMT14 English-German machine transla- tion task, it achieved a 308% speedup on the WMT14 English- German machine translation task compared to PyTorch."}, {"title": "5) COLLIE", "content": "COLLIE [55] introduces a library designed to efficiently facil- itate the collaborative training of LLMs using 3D parallelism [24] (Sections V-C4, V-C1, V-C2), parameter-efficient fine- tuning (PEFT) methods, and optimizers. The library demon- strated significantly improved training efficiency compared to existing solutions. The study empirically evaluates the cor- relation between model size and GPU memory consumption under different optimization methods and analyzes through- put. Additionally, the study investigates training methods to improve the abilities of the LLaMA-65B model, specif- ically focusing on following user instructions. Techniques like LoRA [35], LOMO [58] (Section V-A4), AdaLomo [59], and AdamW demonstrated success in boosting the model's instruction following capabilities without sacrificing its over- all performance. Notably, LoRA and AdaLomo achieved im- pressive results, enabling the model to achieve an average score of 56.9."}, {"title": "6) LLM Training Frameworks and Libraries: Challenges and Key Findings", "content": "This section explores five prominent frameworks and li- braries: GPipe [3], ByteTransformer [4], Megatron-LM [19], LightSeq2 [54], and CoLLiE [55]. Each offers unique func- tionalities to overcome limitations in LLM training.\nAddressing Training Challenges:\n\u2022 Distributed training: As LLMs grow complex, training them on a single device becomes impractical. Frame- works like Megatron-LM [19] and CoLLiE [55] em- ploy distributed training algorithms that split the model across multiple GPUs, enabling parallel processing and faster training.\n\u2022 Efficiency and speed: LightSeq2 [54] tackles train- ing speed through system-level optimizations. It uti- lizes techniques like layer-specific kernels and mixed- precision training to enhance GPU utilization and reduce memory usage. Similarly, ByteTransformer [4] accel- erates transformer models for variable-length inputs in NLP tasks.\n\u2022 Memory management: Efficient memory allocation is crucial for LLM training. CoLLIE [55] overcomes mem- ory constraints in LLM training by utilizing 3D paral- lelism to efficiently distribute memory across training machines and GPUs, enabling the training of large mod- els even in resource limited environments.\n\u2022 Fine-tuning and performance: CoLLiE [55] investigates methods to improve specific LLM capabilities, such as following user instructions. It explores parameter- efficient fine-tuning methods that enhance model perfor- mance in targeted areas without compromising overall functionality.\nKey Findings:\n\u2022 GPipe [3] demonstrates successful training of a large multilingual transformer model, achieving superior re- sults compared to training individual smaller models.\n\u2022 ByteTransformer [4] significantly outperforms existing frameworks in terms of performance for BERT-like transformers on various benchmarks.\n\u2022 Megatron-LM [19] facilitates training of LLMs with billions of parameters, achieving SOTA on NLP tasks while offering high throughput.\n\u2022 LightSeq2 [54] accelerates transformer model training by up to 308%, showcasing substantial performance improvements.\n\u2022 COLLIE [55] introduces a library for collaborative LLM training, demonstrating improved efficiency and effec- tiveness in training large models like LLaMA-65B. It explores methods to enhance specific functionalities without impacting overall performance."}, {"title": "B. LLM INFERENCE FRAMEWORKS AND LIBRARIES", "content": "This section will introduce the LLM frameworks and libraries designed particularly for inference tasks, followed by a sum- mary of each one (see Table 4).\n1) DeepSpeed Inference\nDeepSpeed Inference [5] presents a comprehensive system solution for efficient transformer inference. It has the poten- tial to enable new and innovative applications of transformer models in cloud datacenters and other resource-constrained environments. The system consists of two main parts: Deep- Speed Transformer and ZeRO-Inference [1]. The model is a GPU-only solution that leverages a variety of optimiza- tions to achieve SOTA (minimize) latency and (maximize) throughput for transformer models of all sizes. Specifically, in the first phase DeepSpeed Transformer uses tensor-slicing and inference-optimized pipeline parallelism to scale dense transformer models across GPUs.\nFor sparse transformer models, it has developed a massive- GPU sparse transformer layer that can extend the scalability of Mixture-of-Experts (MoE) transformer layers to hundreds of GPUs. This is achieved through a combination of par- allelism techniques and optimization strategies for commu- nication. Then, DeepSpeed Transformer employs optimized sparse kernels to reduce the computational burden on a single GPU. ZeRO-Inference [1] is a heterogeneous solution that leverages GPU, CPU, and NVMe memory to enable massive transformer inference with limited GPU resources.\nIt is particularly useful for inferring models that are too large to fit in GPU memory. It works by partitioning the model"}, {"title": "C. LLM DEPLOYMENT AND SERVING LIBRARIES", "content": "As mentioned in section IV, some of the frameworks and libraries are utilized for multiple purposes. Besides vLLM [69] (Section IV-C1), the models used for deployment and serving purposes are mentioned in these sections LightSeq2 IV-A4, TurboTransformer IV-B4, PetS IV-B5.\n1) vLLM\nVLLM [69] is a high performance system that efficiently handles LLMs at a large scale. The model tackles the memory limitations of existing LLM serving systems through a novel algorithm called PagedAttention (Section V-A1). PagedAt- tention splits the KV cache into manageable blocks, mini- mizing wasted memory and enabling efficient sharing across requests. VLLM is a distributed system that supports popu- lar LLMs and even models exceeding single GPU memory. Evaluations present vLLM significantly improves throughput by 2-4\u00d7 faster compared to existing systems, especially for complex tasks involving long sequences, large models, and intricate decoding algorithms. This makes vLLM a significant advancement for efficient LLM processing, enabling faster and more scalable LLM applications."}, {"title": "2) LLM Deployment and Serving Libraries: Challenges and Key Findings", "content": "As explored in previous sections (Sections IV-A6 and IV-B12) a variety of LLM frameworks exist that hold promise for deployment and serving applications. This section will discuss the key challenges and findings associated with LLM deployment and serving.\nChallenges of LLM Deployment and Serving:\n\u2022 Memory limitations: Large LLMs can easily overwhelm the memory capacity of a single GPU. This limits their deployment and serving for real-world applications.\n\u2022 Scalability: Effectively handling multiple user requests simultaneously with large LLMs requires efficient scal- ing solutions.\n\u2022 Variability of input: LLM performance can suffer when dealing with input sequences of varying lengths, requir- ing dynamic memory allocation strategies.\n\u2022 Ease of deployment: Integrating complex LLM serving systems into existing workflows can be challenging for researchers and practitioners.\nKey Findings:\n\u2022 PagedAttention: This algorithm (introduced by vLLM [69]) breaks down the KV cache into manageable blocks, minimizing wasted memory and enabling effi- cient sharing across requests. This is a significant im- provement for processing large LLMs.\n\u2022 Efficient GPU utilization: TurboTransformers [11] uti- lize techniques like parallel GPU kernels and dynamic batch scheduling to optimize performance on GPUs. This translates to faster inference for transformer-based models.\n\u2022 System-level optimizations: LightSeq2 [54] demon- strates how system-level optimizations within the train- ing process can significantly improve training speed and efficiency for transformer models. This translates to faster deployment of LLMs in general.\nThese findings from vLLM [69], TurboTransformers [11], and LightSeq2 [54] offer promising solutions for overcoming challenges in LLM deployment and serving. By focusing on memory management, efficient GPU utilization, user-friendly tools, and co-optimization."}, {"title": "V. TRAINING OPTIMIZATION", "content": "Training optimization in LLMs involves improving the effi- ciency and effectiveness of the training process. This encom- passes a range of techniques and strategies aimed at improv- ing factors such as convergence speed, model generalization, and resource utilization. The goal of training optimization is to achieve the desired model performance with faster training times, reduced resource requirements, and improved overall training effectiveness. In this section, we will focus on model optimization, size reduction, distributed training, and hetero- geneous training (Fig. 5)."}, {"title": "A. MODEL OPTIMIZATION", "content": "Model optimization in LLMs refers to the process of im- proving the model's architecture, structure, or parameters to enhance its overall performance. We stated various tech- niques aimed at achieving better accuracy, efficiency, or both. Common model optimization strategies for LLMs include al- gorithmic optimization (section V-A1), layer-specific kernels (section V-A2), model partition (section V-A3), fine-tuning (section V-A4), and scheduler optimization (section V-A5).\n1) Algorithmic Optimization\nFlexGen [17] devised a linear programming-based search algorithm to optimize throughput within the search space. This model can identify efficient patterns for tensor saving and access.\nBuilding on techniques for efficient model execution, Swa- pAdvisor [70], proposes a novel approach to deep learning memory management, that enables the training and serv- ing of large models despite limited GPU memory. Through smart swapping between CPU and GPU memory, it opti- mizes scheduling, memory allocation, and swap planning to maximize computational efficiency. This approach allows training models up to 12\u00d7 beyond the usual GPU memory limit while maintaining significant performance. It stands as an innovative solution for deep learning with limited GPU resources.\nNLP-Fast [60] employs algorithmic optimization tech- niques to enhance the performance of large-scale heteroge- neous NLP models. One of the techniques is cross-operation zero skipping, which eliminates unnecessary computations by skipping zero or near-zero values across multiple operations. By leveraging these techniques, NLP-Fast can significantly improve the overall performance of NLP models on various hardware platforms.\nByteTransformer [4] was developed to address the chal- lenges of redundant computations and memory overhead in transformer models, it employs a combination of algorithmic optimizations and memory-efficient techniques, including a padding-free algorithm, fused MHA, and manually optimized memory sizes of layer normalization. These techniques effec- tively eliminate unnecessary computations, minimize mem- ory footprint, and reduce the cost of accessing GPU global memory, leading to significant performance gains compared"}, {"title": "B. SIZE REDUCTION OPTIMIZATION", "content": "Minimizing the size or complexity of LLMs is a crucial optimization technique known as size reduction optimiza- tion. This approach is essential for addressing challenges associated with memory demands", "16": "investigates the trade-offs involved in scal- ing down language model training", "78": "were almost true as it was observed for performance in large-compute settings. As a predictable outcome of these laws", "analyzed": "one utilizing a classical rtx2080ti GPU", "17": "Sections IV-B", "10": "proposes a model for training a large model with unreliable heterogeneous devices with low network bandwidth by using dynamically generated", "79": "is a compression and execution framework that reduces memory usage significantly. This is achieved through a scalable compression algorithm that shrinks trillion parameter MoEs down to less than 1 bit per parameter. This impressive compression is facilitated by a custom format specifically designed to work with bespoke GPU kernels", "76": "is a compression-aware parameter-efficient adaptation method for large-scale PLMs. It com- bines the quantization of PLMs with fine-tuning", "80": "proposes a new highly accurate and highly efficient post-training quantization method based on approx- imate second-order information which is called a new one- shot weight quantization. This model reaches a level that is considered acceptable to precisely quantize models to 3 or 4 bits per parameter", "81": "also proposes a novel post-training quantization technique to address the deploying LLM challenge. This technique effectively compresses LLMs into a format using 4-bit weights and 8-bit activations (W4A8). This approach achieves SOTA performance on popular LLMs like BLOOM [82", "14": "and LLaMA-2 without requiring further fine-tuning. FPTQ offers a significant advantage by optimiz- ing both memory usage and computation efficiency during the inference stage without sacrificing accuracy. This technique simplifies the deployment process for LLMs and makes them more practical for real-world use. The model was validated on various datasets", "scores": 73.38, "56": "method introduces a novel technique in quantization, specifically for LLMs. While existing quan- tization methods like GPTQ [80"}]}