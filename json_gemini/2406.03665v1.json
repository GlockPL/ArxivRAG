{"title": "Towards Dynamic Trend Filtering through Trend Point Detection\nwith Reinforcement Learning", "authors": ["Jihyeon Seong", "Sekwang Oh", "Jaesik Choi"], "abstract": "Trend filtering simplifies complex time series data\nby applying smoothness to filter out noise while\nemphasizing proximity to the original data. How-\never, existing trend filtering methods fail to reflect\nabrupt changes in the trend due to 'approximate-ness,' resulting in constant smoothness. This ap-\nproximateness uniformly filters out the tail distri-\nbution of time series data, characterized by extreme\nvalues, including both abrupt changes and noise. In\nthis paper, we propose Trend Point Detection for-\nmulated as a Markov Decision Process (MDP), a\nnovel approach to identifying essential points that\nshould be reflected in the trend, departing from ap-\nproximations. We term these essential points as\nDynamic Trend Points (DTPs) and extract trends\nby interpolating them. To identify DTPs, we uti-\nlize Reinforcement Learning (RL) within a discrete\naction space and a forecasting sum-of-squares loss\nfunction as a reward, referred to as the Dynamic\nTrend Filtering network (DTF-net). DTF-net in-\ntegates flexible noise filtering, preserving critical\noriginal subsequences while removing noise as re-\nquired for other subsequences. We demonstrate\nthat DTF-net excels at capturing abrupt changes\ncompared to other trend filtering algorithms and en-\nhances forecasting performance, as abrupt changes\nare predicted rather than smoothed out.", "sections": [{"title": "1 Introduction", "content": "Trend filtering emphasizes proximity to the original time se-\nries data while filtering out noise through smoothness [Leser,\n1961]. Smoothness in trend filtering simplifies complex pat-\nterns within noisy and non-stationary time series data, mak-\ning it effective for forecasting and anomaly detection [Park\net al., 2020]. While smoothness achieves the property of\nnoise filtering, an 'abrupt change' denotes a point in a time\nseries where the trend experiences a sharp transition, sig-\nnaling a change in slope. Given that abrupt changes deter-\nmine the direction and persistence of the slope, it is crucial\nto incorporate them into the trend. Traditional trend filtering\nemploys a sum-of-squares function to reflect abrupt changes\nwhile utilizing second-order differences as a regularization\nterm to attain smoothness [Hodrick and Prescott, 1997; Kim\net al., 2009]. However, we found that the constant nature of\nsmoothness filters out abrupt changes, making it challenging\nto distinguish them from noise.\nThe issue of constant smoothness arises from the reliance\non the property of \u2018approximateness.' Evidence presented by\n[Ding et al., 2019] suggests that the sum-of-squares function\neliminates tail distribution as outliers since it approximates a\nGaussian distribution with a light-tail shape. As both abrupt\nchanges and noise reside within the tail distribution, filtering\nout only noise becomes challenging. This uniform filtering\nresults in the loss of valuable abrupt changes that should be\nreflected in the trend [Wen et al., 2019].\nIn this paper, we propose Trend Point Detection formulated\nas a Markov Decision Process (MDP), aiming to identify es-\nsential points that should be reflected in the trend, departing\nfrom approximateness [Sutton and Barto, 2018]. These es-\nsential points are termed Dynamic Trend Points (DTPs), and\ntrends are extracted by interpolating them. We utilize the Re-\ninforcement Learning (RL) algorithm within a discrete action\nspace to solve the MDP problem, referred to as a Dynamic\nTrend Filtering network (DTF-net) [Schulman et al., 2017].\nRL can directly detect essential points through an agent with-\nout being constrained by fixed window sizes or frequencies\nwithin the time series data domain. This dynamic approach\nenables the adjustment of noise filtering levels for each sub-\nsequence within the time series.\nBuilding on prior research regarding reward function learn-\ning based on Gaussian Process (GP) [Biyik et al., 2020], we\ndefine the reward function as the sum-of-squares loss function\nfrom Time Series Forecasting (TSF). This choice is supported\nby [Ding et al., 2019], which suggests that the sum-of-squares\nfunction approximates a Gaussian distribution and functions\nsimilarly to a Gaussian kernel. Note that using a Gaussian\nkernel function as a reward leverages RL to effectively op-\ntimize the agent while learning the full distribution of time\nseries data. Through the TSF reward, temporal dependencies\naround DTPs can be captured, and the level of smoothness is\ncontrolled by adjusting the forecasting window size. Addi-\ntionally, to address the overfitting issue, we apply a random\nsampling method to both the state and the reward.\nWe compare DTF-net with four categorized baselines:\ntrend filtering (TF), change point detection (CPD), anomaly\ndetection (AD), and time series forecasting (TSF) algorithms."}, {"title": "2 Related Work", "content": "Traditional trend-filtering algorithms have employed vari-\nous methods to capture abrupt changes. H-P [Hodrick and\nPrescott, 1997] and l\u2081 [Kim et al., 2009] optimize the sum-\nof-squares function, a widely used cost function for trend fil-\ntering. However, they often face challenges in the delayed\ndetection of abrupt changes due to the use of second-order\ndifference operators for smoothness. To address this issue,\nthe TV-denoising algorithm [Chan et al., 2001] was intro-\nduced, relying on first-order differences. Nevertheless, this\nstrategy introduces delays in detecting slow-varying trends\nwhile overly focusing on abrupt changes. These methods en-\ncounter difficulties in handling heavy-tailed distributions due\nto the use of the sum-of-squares function [Wen et al., 2019].\nContrary to sum-of-squares function methods, alternative\napproaches to trend filtering exist. For example, frequency-\nbased methods like Wavelet [Craigmile and Percival, 2002]\nare designed for non-stationary signals but are susceptible to\noverfitting. The Empirical Mode Decomposition (EMD) al-\ngorithm [Wu et al., 2007] decomposes a time series into a\nfinite set of oscillatory modes, but it generates overly smooth\ntrends. Lastly, the Median filter [Siegel, 1982] is a non-linear\nfilter that selects the middle value from the sorted central\nneighbors; therefore, outlier values that deviate significantly\nfrom the center of the data are excluded."}, {"title": "2.2 Extreme Value Theorem", "content": "Abrupt changes in a time series reside in the tail of the data\ndistribution, making them rare events. However, their impact\nis significant, as they can alter the slope of the time series and\naffect the consistency of trends. Once an abrupt change oc-\ncurs, its effects are often permanent until the next one occurs.\nTherefore, detecting abrupt changes is crucial to minimize\nfalse negative rates and capture important information.\nReal-world time series data commonly exhibit a long-\nheavy tail distribution. Formally, the tail distribution is de-\nfined as follows:\n$\\lim_{\\overline{I}\\rightarrow\\infty}P\\{max(y_1, \\dots, y_T) \\leq y\\} = \\lim_{\\overline{I}\\rightarrow\\infty}F_T (y) = 0,$ (1)\nwhere T random variables {91, \u2026, YT} are i.i.d. sampled\nfrom distribution Fy [von Bortkiewicz, 1921; Ding et al.,\n2019]. Furthermore, extreme values within the tail distribu-\ntion can be modeled using Extreme Value Theory.\nTheorem 1 (Extreme Value Theory [Fisher and Tippett,\n1928; Ding et al., 2019]). If the distribution in Equation (1)\nis not degenerate to 0 under a linear transformation of y, the\ndistribution of the class with the non-degenerate distribution\nG(y) should be as follows:\n$G(y) =\\begin{cases}\nexp(-(1-\\gamma y)^{-\\frac{1}{\\gamma}}), y \\neq 0, 1-\\gamma y \\geq 0, \\\\\nexp(-\u0435^{-y}),  \\gamma = 0.\n\\end{cases}$(2)\nExtreme Value Theory (EVT) has demonstrated that ex-\ntreme values exhibit a limited degree of freedom [Lorenz,\n1963]. This implies that the occurrence patterns of extreme\nvalues are recursive and can be memorized by a model [Alt-\nmann and Kantz, 2005]. Essentially, a model with substantial\ncapacity and temporal invariance can effectively learn abrupt\nchanges, which are categorized as extreme values.\nHowever, extreme values are typically either unlabeled or\nimbalanced, making them challenging to predict. In classifi-\ncation tasks, previous research [Raj et al., 2016] has high-\nlighted the susceptibility of deep networks to the data im-"}, {"title": "2.3 Markov Decision Process and Reinforcement\nLearning", "content": "MDP is a mathematical model for decision-making when an\nagent interacts with an environment. It relies on the first-\norder Markov property, indicating that the future state de-\npends solely on the current state. MDP comprises compo-\nnents denoted as (S, A, P, R, \u03b3). Here, S denotes the set of\nenvironment states, while A represents the set of actions un-\ndertaken by the agent at state S. The transition probability,\nP = Pr(S' S, A), signifies the probability of transitioning\nfrom the current state S to the next state S'. The reward,\nR = E[R(S, A, S')|S, A], where R(S, A, S') represents the\nimmediate reward obtained when transitioning from state S\nto S' by taking action A. The discount factor \u03b3\u2208 (0,1] gov-\nerns the trade-off between current and future rewards [Sutton\nand Barto, 2018]. We can formulate any time series data with\nan MDP for Trend Point Detection, as detecting points al-\nways adheres to the first-order Markov property [Wu and Or-\ntiz, 2021]. These points are determined solely by the current\ntime step and remain unaffected by past observations, sharing\nproperties similar to those of predicting stock trading points.\nIn RL, actions are predicted through a policy network de-\nnoted as \u03c0(AIS) = Pr(AS) for each state, representing\nthe probability of action A at state S. The state-value func-\ntion \u03c5\u03c0(S) = E[G|S] estimates the expected reward value\nfor a state S under policy \u03c0, where G = \u2211k=0YkRk de-\nnotes the expected sum of future rewards starting from the\nnext reward R'. In RL of discrete action spaces, methods\nlike Advantage Actor-Critic (A2C) [Mnih et al., 2016] and\nProximal Policy Optimization (PPO) [Schulman et al., 2017]\ndirectly train the policy using the estimated state-value\nfunction v. In contrast, Deep Q-Network (DQN) [Mnih et\nal., 2015] finds the optimal action-value function, denoted\nas q\u03c0(S, \u0391) = E\u201e[G|S, A]. This function represents the ex-\npected cumulative reward for taking action A in state S un-\nder policy \u03c0 and is determined through the Bellman equation\n(Appendix B). DTF-net utilizes RL to extract flexible trends\nthrough dynamic action prediction from a deep policy net-\nwork \u03c0, learning within the time series data environment for-\nmulated as an MDP of the Trend Point Detection problem."}, {"title": "3 Dynamic Trend Filtering Network", "content": "Time series data is defined as T = {(X1,Y1), (X2,Y2), ..., (XN, YN)}, where X \u2208 RD represents the input, y \u2208 Rd\nrepresents the output, and the dataset comprises a total of\nN \u2208 Z+ samples. Here, D and d denote the input and output\ndimensions, respectively, both of which are positive integers.\nThe Trend Point Detection problem formulation takes in-\nput X representing the environment and outputs DTPs, en-\ncompassing abrupt changes, midpoints of distribution shifts,\nand other critical points influencing trend slope changes oc-\ncurring at both short and long intervals. The output consists\nof specific univariate time series y(i) labeled with binary val-\nues, where i \u2208 d of target."}, {"title": "3.1 Trend Point Detection\nEnvironment Definition", "content": "$\\bullet$ State S = [Xt, At]: the positional encoded vector set of\ntime series data X and action A with horizon t.\n$\\bullet$ Action A: a discrete set with (a = 1) for detecting DTP\nand (a = 0) for smoothing.\n$\\bullet$ Reward R(S, A, S'): the change in forecasting sum-of-\nsquares function value when action A is taken at state S\nand results in the transition to the next state S'.\n$\\bullet$ Policy \u03c0(A/S): the probability distribution of A at S.\nThe RL algorithm, named DTF-net, employs a policy net-\nwork within the defined MDP. It receives the state S as in-\nput and outputs the binary labeled target y(i), also denoted as\nA, learned through the maximization of cumulative rewards\nR. DTF-net is designed to extract dynamic trends by interpo-\nlating detected essential trend points, referred to as DTPs and\nrepresented by the set {y(i) = 1} or {A|a=1}."}, {"title": "Episode and State for DTF-net", "content": "Previous studies in RL for time series [Liu et al., 2022a] have\ngenerally adopted a sequential approach. In contrast, DTF-\nnet introduces dynamic segmentation with variable lengths\ncomprising one episode through random sampling. The dis-\ncrete uniform distribution is specifically chosen to ensure that\nall sub-sequences are considered equally:\n$s \\sim unif\\{0, N\\},\\\\\nl \\sim unif\\{h + p, H\\},$ (3)\nwhere s represents the starting points of the sub-sequence, l\ndenotes the sub-sequence length, h denotes the forecasting\nlook-back horizon, p denotes the forecasting prediction hori-\nzon, and H represents the maximum length comprising one\nepisode. With sampling, the length and starting point of the\nsub-sequence are defined, resulting in a non-sequential and\nrandom progression of the episode. This sampling approach\nmitigates the overfitting issue by allowing the model to use\nonly a portion of the sequence.\nWithin a single episode, DTF-net cumulatively constructs\nthe state S. To maintain a constant state length within an\nepisode, we employ positional encoding as follows:\n$PE(pos,2i) = sin(pos/10000^{2i/dmodel}),\\\\\nPE(pos,2i+1) = cos(pos/10000^{2i/dmodel}).$\nThe cumulative state progression is achieved by gradually\nexpanding the state representation St as the step unfolds as\nfollows,\n$St = PE(\\{Xs:s+t, A0:t\\}), where t < l.$(4)\nThrough cumulative state construction, the agent can learn\nsequential information in the time series (Appendix D.1)."}, {"title": "3.2 Reward Function of DTF-net\nGP and Reward Function Learning", "content": "Traditional trend filtering methods utilize the sum-of-squares\nfunction, also known as the Mean Squared Error (MSE), to\napproximate abrupt changes when extracting trends. How-\never, [Ding et al., 2019] provided evidence that minimizing\nthe sum-of-squares function assumes that the model output\ndistribution determined using the MSE cost function denoted\nas P(Y), follows a Gaussian distribution with variance \u0442,\ngrounded in Bregman's theory [Banerjee et al., 2005].\n$P(Y) = min \\sum_{t=1}^{T} || y_t - Ot||^2,\\\\\n= max \\prod_{t=1}^{T}P(Y_txt, \\theta),\\\\\n= max \\prod_{t=1}^{T}N(Yt, \\theta),$\nwhere o \u2208 Y represents the output from a model parameter-\nized by 6. This also suggests that model @ operates in a man-\nner similar to a Kernel Density Estimator (KDE) employing\na Gaussian kernel [Rosenblatt, 1956].\nContrary to approximations, DTF-net utilizes a policy net-\nwork \u03c0 to predict DTPs, including abrupt changes. However,\ndefining a reward function in general time series data is chal-\nlenging but is the most crucial task in RL training for opti-\nmizing the policy network. To tackle this challenge, DTF-\nnet draws inspiration from previous works, which employ the\nGaussian Process (GP) for reward function learning [Kuss\nand Rasmussen, 2003; Biyik et al., 2020].\nFormally, GP [Williams and Rasmussen, 1995] assumes\nnoisy targets y\u2081 = f(xi) + ei that are jointly Gaussian with a\ncovariance function k:\n$P(y|x) \\sim N(0, K), where K_{pq} = k(x_p, x_q).$(6)\nWith a Gaussian covariance function,\n$k(x_p, x_q|\\theta) = v^2exp(-(\\frac{(x_p-x_q)A^{-1}(x_p-x_q)}{2}))+\\delta_{pq}\\sigma^2,$\nwhere diagonal matrix A, v, and o are hyperparameters in 0,\nthe predictive distribution for input x* follows Gaussian:\n$P(ox|x^*, x, y, \\theta) \\sim N(k(x^*,x)K^{-1}y,\nk(x^*, x^*) - k(x^*, x)K^{-1}k(x,x^*)).$(7)\nThe GP model inherently learns a full distribution of time\nseries data, enabling RL to effectively optimize the policy net-\nwork (Appendix B). Leveraging these insights, DTF-net's re-\nward function is defined as the sum-of-squares function from"}, {"title": "3.3 Dynamic Trend Filtering", "content": ""}]}