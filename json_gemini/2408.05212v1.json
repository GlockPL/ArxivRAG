{"title": "Preserving Privacy in Large Language Models:\nA Survey on Current Threats and Solutions", "authors": ["Michele Miranda", "Elena Sofia Ruzzetti", "Andrea Santilli", "Fabio Massimo Zanzotto", "S\u00e9bastien Brati\u00e8res", "Emanuele Rodol\u00e0"], "abstract": "Large Language Models (LLMs) represent a significant advancement in artificial intelligence,\nfinding applications across various domains. However, their reliance on massive internet-\nsourced datasets for training brings notable privacy issues, which are exacerbated in critical\ndomains (e.g., healthcare). Moreover, certain application-specific scenarios may require fine-\ntuning these models on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize and inadver-\ntently reveal sensitive information. We explore current threats by reviewing privacy attacks\non LLMs and propose comprehensive solutions for integrating privacy mechanisms through-\nout the entire learning pipeline. These solutions range from anonymizing training datasets\nto implementing differential privacy during training or inference and machine unlearning af-\nter training. Our comprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This work aims to\nguide the development of more secure and trustworthy AI systems by providing a thorough\nunderstanding of privacy preservation methods and their effectiveness in mitigating risks.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) represent a significant advancement in artificial intelligence (AI), demon-\nstrating remarkable capabilities in understanding and generating human-like text, and fueling innovation\nacross various domains (Brown et al., 2020a; Touvron et al., 2023; Zhao et al., 2023). However, their reliance\non massive internet-sourced datasets for training raises significant privacy concerns considering that these\nmodels are prone to memorize segments of their training data (Feldman, 2020; Feldman & Zhang, 2020).\nThis is further exacerbated if we consider certain application-specific scenarios (e.g, healthcare) that may\nrequires the fine-tuning of publicly available models on user private data (Tram\u00e8r et al., 2022). Since lan-\nguage models are specifically trained to generate text, this poses a fundamental threat considering that they\nmight inadvertently disclose memorized private data at any moment (Carlini et al., 2019; 2021)."}, {"title": "2 Preliminaries", "content": "In this section, we present preliminary concepts essential for comprehending the subsequent content of the\ndocument. Readers already acquainted with these concepts may proceed directly to the next section.\nMachine Learning Machine learning involves the development of algorithms that can learn from and\nmake predictions or decisions based on data by recognizing patterns in it, utilizing various techniques that\ninclude both linear and nonlinear models. Classical machine learning often relies on handcrafted features\nthat require domain knowledge to be created. Classical machine learning models include decision trees,\nsupport vector machines, linear regression, and ensemble methods like random forests, designed for specific\ndata types and tasks. Models are composed of parameters, which are what is learned in the learning process.\nThese are variables that the learning algorithm adjusts during training to fit the model to the data as closely\nas possible. Parameters can be weights, as they affect how much of the input is passed between neurons\n(as a multiplication factor), or biases, shifting the outputs along the value axis (as an addition factor).\nNetwork parameters are very influential and their magnitude defines the model complexity and its capacity\nto recognize more intricate patterns in the data and generalize better. More complex networks fall under\ndeep learning, see 2. There are three different types of learning: supervised learning, where there is a ground\ntruth, and the model tries to output an answer as close as possible to that; unsupervised learning, where\nthere is no answer and the model groups inputs based on some similarity; reinforcement learning, where"}, {"title": "2.1 Differential Privacy", "content": "According to Dwork & Roth (2014): \"Differential privacy describes a promise, made by a data holder, or\ncurator, to a data subject: \"you will not be affected, adversely or otherwise, by allowing your data to be used\nin any study or analysis, no matter what other studies, data sets, or information sources, are available. \"\nThe fundamental goal of Differential Privacy is to enable the extraction of useful insights from data while\nminimizing the potential impact on any individual's privacy within that dataset. The core idea is to introduce\ncontrolled noise or randomness into the analysis or results in a way that prevents the identification of specific\nindividuals. In simpler terms, when a system or algorithm is differentially private, the inclusion or exclusion\nof any individual's data should not significantly affect the overall outcome or conclusions of the analysis.\nThere are several kinds of Differential Privacy definitions, but the original one is (\u03b5, \u03b4)-Differential Privacy.\nA randomized algorithm M with domain $\\mathcal{N}$ is (\u03b5, \u03b4)-differentially private if for all S\u2286Range(M)\nand for all x, y \u2208 $\\mathcal{N}$ such that ||x - y||1 \u2264 1 :\n$\\Pr[M(x) \\in S] \\leq \\exp(\\epsilon) \\Pr[M(y) \\in S] + \\delta$\nThe formal guarantee provided by (\u03b5, \u03b4)-Differential Privacy is that for any two datasets x and x' that differ\nby only one element (these are often referred to as \"adjacent datasets\"), and for any subset of outputs S of the\nalgorithm's range Y, the probability that the algorithm outputs something in S given x is not substantially\nhigher than given x'.\ne is the parameter that measures the privacy loss associated with a query in the system. A smaller e provides\nstronger privacy guarantees. The use of e in the inequality indicates that the probability of a certain output\ndoes not increase by more than a multiplicative factor of e when any single individual's data is added or\nremoved.\nThe privacy budget is the total loss of privacy sustained by the system during the execution, and it can be\ncomputed by compounding the privacy loss indicated by e over the total number of executions/iterations.\nThis is done with the composition theorems, which are explained in more detail later in this section.\nd is the parameter that allows for a small probability of failure of the privacy guarantee. Ideally, 8 should\nbe very small, close to 0, which indicates a lower chance of the algorithm failing to maintain privacy.\nThis means that for all subsets S of the output range Y, the probability that the algorithm on input x gives\nan output in S is at most e raised to the power e times the probability that the algorithm on input x' gives\nan output in S, plus the small probability \u0431.\nAnother prominent definition of Differential Privacy, which is actually a relaxation of the original definition,\nis R\u00e9nyi Differential Privacy (Mironov, 2017)."}, {"title": "R\u00e9nyi Differential Privacy", "content": "A randomized algorithm M : D \u2192 R is said to have e-R\u00e9nyi differential privacy of order a, or\n(\u03b1, \u20ac)-RDP for short, if for any adjacent D, D' \u2208 D it holds that\n$D_{\\alpha}(f(D)||f (D')) \\leq \\epsilon$\nHere, Da(P||Q) represents the R\u00e9nyi divergence of order a between two probability distributions P and Q,\nand is defined as:\n$D_{\\alpha}(P||Q) = \\frac{1}{\\alpha-1} \\log \\frac{1}{log} \\sum_{x \\in X} P(x)^{\\alpha}Q(x)^{1-\\alpha}$\nfor all a > 1, where X is the support of the distributions P and Q. The parameter a here controls the\nsensitivity of the divergence measure to differences between P and Q, with higher values of a focusing on\nevents that are more probable under P than under Q. The parameter e still quantifies the privacy guarantee,\nwith smaller values indicating stronger privacy.\nBy providing a mathematically rigorous definition of privacy guarantees, Differential Privacy has become\na key principle for designing privacy-preserving algorithms, especially in scenarios where personal data is\ninvolved. From this mathematical definition, some interesting properties arise. Among these, there are\ncomposition theorems that allow the computation of the exact privacy loss through many applications of\nthe DP algorithm, which corresponds to the privacy budget. Sequential composition theorem (Kairouz et al.,\n2015) applies when multiple differentially private mechanisms are applied to the same dataset in sequence.\nThe theorem quantifies how the privacy guarantees accumulate over the sequence of operations. Parallel\ncomposition theorem (McSherry, 2009) applies when multiple differentially private mechanisms are applied\nto disjoint subsets of the dataset. This theorem shows that the privacy guarantee of the overall process is\ngoverned by the worst-case privacy guarantee of any single mechanism. Differential Privacy also exhibits\na Post-Processing property that states that once data has been processed through a DP mechanism, the\noutput retains its privacy guarantees regardless of any further processing.\nDifferential privacy was born as a statistic analysis technique for databases. Initially, it was used to query\ndatabases without compromising the privacy of anybody enrolled, and that is why we discussed about queries\nand compounding privacy over them to compute the total privacy loss or privacy budget. Luckily, all this\ncame in handy in recent years, with the worldwide diffusion of deep neural networks, when differential\nprivacy was applied to them. That is when databases became datasets and queries became (mostly training)\niterations. We will see more in detail what is the result of this transition in the next paragraph \u00a72.1."}, {"title": "2.2 Deep Learning with Differential Privacy", "content": "Differential privacy was initially devised for database analysis, but it is possible to apply it to deep learning\nmodels in several ways. In practice, since we primarily work with deep learning models, we will often refer\nto datasets rather than databases. However, this distinction does not entail any practical difference in the\ncontext of our discussion. shows how Differential Privacy can be incorporated in 1) the input data,\n2) the gradients, 3) the output data, 4) the labels or even in the loss function 5). The most used technique\nis by training using DP-SGD (Differentially Private Stochastic Gradient Descent), while DP-Adam is less\nused we discuss the reasons for this in \u00a75.1.2. Both of these techniques involve gradient manipulation,\nfalling under the point 2) in Initially presented by Song et al. (2013), DP-SGD is a variation of\nstandard Stochastic Gradient Descent (SGD) that enhances privacy by incorporating gradient clipping and\nadding Gaussian noise. In DP-SGD, gradients are first clipped to a maximum norm to limit the influence\nof any single data point. Then, Gaussian noise is added to the average of these clipped gradients, ensuring\nthat the updates to the model parameters maintain differential privacy by obscuring the contribution of\nindividual data points. It is possible to consider the several layers separately in order to have different\nclipping thresholds, and the amount of noise should be calibrated over the sensitivity of the function (in this"}, {"title": "3 Privacy Attacks", "content": "As the training data becomes huge, it is increasingly urgent to understand whether models tend to expose\nthe data on which they have been trained since it is more likely for them to capture sensitive information.\nA broad research area is devoted to understanding whether deep learning algorithms memorize training\nexamples and, hence, can be used to reconstruct (potentially sensitive) training information. While some\nworks question the effect of memorization during generation - meaning that texts generated starting from\nprompts extracted from training are different from the original ones (McCoy et al., 2023) - the discussion\naround memorization and its consequent risks is growing as larger models become more popular. In some\ncases, memorization is discussed as the key to good performance since data are often characterized by long-\ntailed distributions, hence, for a model, it is hard to distinguish between outliers and actually informative\ndata points (Feldman & Zhang, 2020). However, memorization of training data by a model allows an attacker\nto reconstruct sensitive information or to understand whether a certain example was used during training. It\nhas been shown by Carlini et al. (2023a) that it is possible to reconstruct training data with only black-box\naccess.\nFor LLMs, it might be sufficient to input the right prompt, which is not necessarily an adversarial one\n(Carlini et al., 2021), in order to condition a model to generate sequences that are exactly memorized from\nthe training data. Attacks that allow verbatim extraction of training data are generally referred to as Training\nData Extraction attacks. When LLMs are fine-tuned with Reinforcement Learning with Human Feedback\n(RLHF), they tend to be more robust and avoid revealing the training set examples and, in particular,\nrevealing sensitive information. However, some adversarial prompts have emerged to condition a model to\nshow some unintended behaviour: since sensitive information is memorized by a model, RLHF is used to"}, {"title": "3.1 Training Data Extraction", "content": "In Training Data Extraction attacks, an adversary aims to reconstruct training data having only access to a\ntarget Language Model. In particular, those attacks aim to extract the entire example by querying the target\nmodel (Carlini et al., 2021). Retrieval of examples is based on the assumption that they were memorized\nby the target models, and the definitions of memorization underlying these attacks may vary. However, the\nvarious definitions revolve around the key concept that a textual example or part of it can be extracted from\na language model. In the original definition of Carlini et al. (2021), an example is defined to be extractable\nif there is a prefix used to condition the generation of the model so that the string can be exactly generated\nfrom the model. This definition is first introduced by Carlini et al. (2021) and then largely adopted with\nslight modifications (Huang et al., 2022; Carlini et al., 2023a; Nasr et al., 2023).\nExtracting Training Data consists of conditioning a LM to the right prompt so that it is likely that\nit will generate a training example to complete it. Formally, a textual example x is extractable from\na LM g if there exists a prompt p such that: $x \\approx \\arg \\max_{x'} g(x'|p)$. Once the right prompt has\nbeen identified, this procedure can be used to reconstruct training data, and hence, it can be used to\nextract sensitive information like Personally Identifiable Information (PII) from a model."}, {"title": "3.1.1 Non-adversarial extraction", "content": "Using training data extraction techniques, it has been shown that Language Models are prone to reveal\nprivate information when prompted with sentences that require the disclosure of such information. While\ntrying to recover memorized training examples, Carlini et al. (2021) demonstrated that GPT-2 tends to\nmemorize personal information such as Twitter handles, emails and UUIDs (Universal Unique Identifiers).\nIn particular, in their experiments the language model generation is conditioned to some prompts that the\nattacker predefined: to maximize the probability of success of the attack, they choose prompts that are\nlikely to be in the training data, like samples from the Common Crawl 2. Then, once the generation is\ncomplete, the attacker tries to figure out which sequences are most likely to be generated as a result of the\nmemorization, that is, which of the generated strings exactly matches the training data. In order to identify\nsequences that are more likely to be generated because of memorization, Carlini et al. (2021) compare the\nlikelihood that is assigned to each sequence by two models: one is the model under attack - GTP-2 \u2013 and\nthe other is a reference model, another LM. The attacker retains as possibly memorized sentences only those\nthat have a high likelihood in the target model when compared to the likelihood assigned by the reference"}, {"title": "3.1.2 Adversarial prompting", "content": "There have been initiatives to ensure LLMs are aligned with human values and used appropriately. For\nexample, the OpenAI usage policy requires that users do not use the models to compromise the privacy of\nothers, including collecting or generating personal data. Moreover, OpenAI's ChatGPT has been aligned\nwith Reinforcement Learning from Human Feedback (Ouyang et al., 2022) and avoids answering questions\nregarding its training data. In this scenario, the techniques based on non-adversarial prompts may not be\neffective: Nasr et al. (2023) discuss how ChatGPT tends to not expose training data when prompted with\nprefixes that are likely seen during training. For these reasons, some attacks against LLMs define adversarial\nprompts that force the model to diverge from its originally intended use. For example, asking the model\nto repeat indefinitely a word, Nasr et al. (2023) notice that ChatGPT diverges and starts emitting training\ndata, also containing personally identifiable information.\nA new form of adversarial prompt attacks are jailbreak prompts. These prompts are designed to circumvent\nsafeguards and manipulate LLMs into generating harmful content. In some cases, a goal hijacking is per-\nformed, meaning that the original goal of a prompt is changed to a new goal, like generating a target phrase,\nby injecting malicious text in the original prompt (Perez & Ribeiro, 2022). Shen et al. (2023) characterize a\nlarge set of jailbreak prompts and evaluate their effectiveness against different LLMs. They notice that most\nof these prompts encourage the model to act like another fictional assistant, such as DAN (short for Doing\nAnything Now), which has no limitation in the answers. Along with other potentially harmful scenarios, they\nshow that this type of prompt allows a user to receive suggestions on how to collect personal information.\nIn this context, Li et al. (2023b) focused on the usage of adversarial prompts to pose privacy threats: they\npropose a multi-step jailbreak prompt to extract personal information, such as emails, based on Chain-of-\nThought (Kojima et al., 2023). They study the effect of adversarial prompting against ChatGPT. They first\nnotice that the model tends to avoid sharing private information if direct data extraction prompts are used.\nThen they propose the Multi-step Jailbreaking Prompt in three steps: they assume the role of the user to\ninput the jailbreaking prompt, then act as the assistant to confirm that the jailbreak mode is enabled and,\nlastly, they play the role of the user and query the assistant with a non-adversarial prompt. They show that\nmore frequent emails are less difficult to recover: experiments on the Enron dataset show that more than\n50% of frequent emails in the dataset can be extracted."}, {"title": "3.2 Membership Inference Attacks (MIA)", "content": "Membership inference attacks (MIA) (Shokri et al., 2016) are currently a de facto standard when it comes to\nverifying model privacy (Murakonda & Shokri, 2020). In this kind of attack, the attacker has access to (1)\nsome records (supposed to come from a similar distribution of the training data) and (2) to the model,\nwhich could be provided as an API, in a black box setting, or be fully available as a white box. The aim of the"}, {"title": "3.2.1 \u039c\u0399\u0391 with Shadow Models", "content": "Stemming from the general setting described in Shokri et al. (2016), MIAs have been successfully tailored for\nLanguage Models to extract sensitive information from them. Song & Shmatikov (2019) argue that Language\nModels tend to memorize training data and, therefore, a user can tell whether their data have been used\nalso in a black box setting. Their attack positively exploits the tendency of these models to rank relatively\nrare words higher when they are in the same context in which they were seen during training. Hisamoto\net al. (2020) try to make this type of attack more general by training an attack classifier not on the output\nprobabilities but only on features that can be extracted from the output sequences of a sequence-to-sequence\nTransformer model. Carlini et al. (2022) observe that MIAs could be better framed as the procedure of\ncomparing models that are trained on a given example and models that are not trained on it, and to find\na difference in their behaviour, for example having a likelihood of the example much higher in the first\ncase rather than in the other. They train numerous shadow GPT-2 models to measure the probability of\nobserving a certain likelihood of an example in models trained and not trained on it. However, training\nshadow models may be too expensive in the context of LLMs: the original formulation of MIA can be used\nas long as it is feasible for the attacker to construct shadow models that are similar to the target model."}, {"title": "3.2.2 \u039c\u0399\u0391 with Thresholds", "content": "Since the construction of shadow models can be too expensive, MIAs have also been formulated without\nrelying on them. In fact, some MIAs have been designed to exploit some measure of overfit over the training\ndata in order to understand whether a given example has already been seen by a model: overfitting the\ntraining data is a sufficient condition to detect memorized examples (Yeom et al., 2018). Shi et al. (2023)\nobserve that the probability distribution is more spread over low probability words for an unseen example:\nthey design attacks that threshold the average log-likelihood of the top-k rarest words to ascertain whether\nor not the example is part of the training data. Language models trained on clinical data have been shown\nto be sensitive to MIAs. Mireshghallah et al. (2022) demonstrate the effectiveness of MIAs designed against\nLanguage Models trained with Masked Language Modeling objectives, obtaining a substantial gain both in"}, {"title": "3.3 Model Inversion", "content": "Model inversion attacks generally exploit the output of the model and use this information to reconstruct the\ninput data that generated the observed outputs. In the initial formulation (Fredrikson et al., 2014; 2015), an\nadversary is given access to the trained model and to a partially masked input, for which only non-sensitive\nfeatures are disclosed. The attacker aims to infer the value of the sensitive attributes that serve as input to\nthe model. While the initial formulation is implemented for linear regression models (Fredrikson et al., 2014),\nthis type of attack has been successfully used to extract sensitive input information from different models.\nFredrikson et al. (2015) applied for the first time a pattern inversion attack against a neural network, trained\nfor facial recognition: they were able to partially reconstruct the faces of people in the training set exploiting\nthe prediction of the system, framing the inversion as an optimization problem that requires reconstruction\nof the image that justifies the observed output distribution. Model inversion attacks are often performed\nagainst distributed models: in fact, some protocols like federated learning ensure that private data are not\nshared across multiple servers (see Section 2.2 for further details). However, with some access to model\noutputs, it is possible for an attacker to reconstruct the input data that causes the observed output.\nModel Inversion Attacks\nThis family of attacks threatens models especially when they are distributed across multiple servers:\nin fact, while data holders do not directly share private data which are thus maintained locally\nthey may be willing to share seemingly opaque information related to a machine learning model,\nsuch as model outputs, embeddings or gradients. However, Model Inversion attacks are designed to\nreconstruct private data from these apparently opaque sources."}, {"title": "3.3.1 Model Output Inversion", "content": "The outputs of a model, like the representation of textual data that a Transformer-based LM produces in\nthe last layer, can be totally or partially inverted to reconstruct the text that is originally the input of the\nmodel. In particular, these representations often called embeddings are only apparently opaque and can\nbe inverted to reconstruct the entire sentence or the words that it contains. Pan et al. (2020) design the first\nmodel inversion attack against Transformer-based Language Models. They observe that directly framing the\ninversion problem as an optimization problem is challenging due to the discrete nature of sentences. They\ntrain an attack classifier that is designed to extract fixed patterns and keywords like, for example, location\nor medical information \u2013 from sentence representations of models. Similarly, Song & Raghunathan (2020)\npredict unordered sets of words from sentence embeddings: their attack model is a neural network trained\nwith the multiset prediction loss (Welleck et al., 2018) so that it is possible for the model to predict each\nword in the embedding also conditioned on the words already predicted."}, {"title": "3.3.2 Gradient Inversion", "content": "In federated learning (Ray et al., 2021) multiple nodes train a model cooperatively without directly sharing\ndata samples: the system is designed to make training feasible while exchanging model updates rather than\ndata across nodes (see $5.3 for further details). Gradient Inversion attacks are directed against federated\nlearning environments that perform training by sharing gradients. Those attacks can be used to reconstruct\ntraining data from seemingly opaque sources.\nZhu et al. (2019) demonstrated that if the gradients are openly accessible, it is possible to reconstruct the\ntraining data. The attacker aims to reconstruct the training data by minimizing the distance between some\ngenerated gradients and the ground truth one. In particular, the attacker initially generates gradients from\nrandom inputs and labels. Then, having access to the ground truth gradients solves the optimization problem\nof minimizing the L2 distance between the generated gradients and the ground truth one, parameterized by\nreal inputs and labels. In their experiments, Zhu et al. (2019) applied this attack against a BERT model.\nThis idea was further explored by inverting gradients for image classification tasks (Zhu & Blaschko, 2020;\nGeiping et al., 2020; Yin et al., 2021).\nIn reconstructing textual data, Deng et al. (2021) further attempted to perform a gradient attack against\nTransformer-based language models introducing also a L\u2081 term in the reconstruction loss. Experiments on\na Transformer model, BERT model and TinyBERT demonstrate the effectiveness of this gradient attack,\nrecovering up to 50% of the original tokens on average. In LAMP (Balunovic et al., 2022), more effective\nresults are achieved by simultaneously training the attack model to minimize the difference between the\nreconstruction gradients and choosing at each iteration only sequences that have low perplexity according\nto an external language model (like GPT-2). Reconstructing the correct word order is, in fact, one of the\nmain challenges: Gupta et al. (2022) propose to recover from the gradients a bag of words for the sentence\nto extract and then perform a beam search to effectively reconstruct the sentence."}, {"title": "4 Solutions for Preserving Privacy in LLMs: Data", "content": "After identifying the key vulnerabilities and potential attacks on LLMs as outlined in \u00a73, in this section, we\nintroduce the reader to the current possible solutions to the problem of preserving privacy in the context\nof LLMs and text generative models. We have to say that the field is very fragmented as this is a very\nrecent concern and the field is evolving fast. There is not, at the moment of writing this survey, a single\ngold standard solution to the problem of preserving privacy in LLMs and text generative models. The main\nsolutions available can be broadly categorized into two categories: i) Preserving Privacy in Data \u00a74\nand ii) Preserving Privacy in Model \u00a75. In the former section, we collected all the methods that try\nto solve the problem at its source i.e., they act directly on the privacy-sensitive information in the dataset,\nbefore the training happens, while in the latter we selected all the methods that address the problem from\nthe model perspective i.e., try to remove from the model problematic information after the training. The\ntwo approaches fit different use cases, so it is important to be aware of both.\nIn this section, we propose several solutions that allow preserving privacy from the data perspective i.e., we\npreserve privacy directly in the training data used in machine learning models. This includes any approach\nthat involves manipulating, carefully choosing, or generating training data in order to avoid the memorization\nand subsequent leakage of private information inside a large language model or any model that learns from\ndata. This basically puts the focus on preserving privacy in the data used to train the model. Given\nthat a machine learning model is moulded by its training dataset, training it on a privacy-preserving dataset\nensures that these methods offer equivalent privacy assurances to the dataset itself. The section is structured\nin the following way: we first show classical anonymization techniques \u00a74.1, then we focus on anonymization\ntechniques built with Differential Privacy \u00a74.1.2. Putting this last section in relation with Figure 5, the point\nof application of DP would be (1), the input, because all these techniques aim to apply Differential Privacy\non the data before feeding it to the LLM (for either training or inference).\nPreserving Privacy in Data\nKey Idea: Preserve privacy directly on the dataset used to train the model."}, {"title": "4.1 Anonymization", "content": "Data anonymization is not a new concept, with seminal work dating back to the 1990s: a pivotal develop-\nment was the introduction of k-anonymity in 1997 (Sweeney, 1997), which means ensuring that data cannot\nbe re-identified to fewer than k individuals. This foundational concept has spurred extensive research, lead-\ning to a plethora of anonymization techniques. Given the breadth and depth of the field, this section is\norganized into two main parts: the first, titled Classical Anonymization 4.1.1, explores traditional methods\nsuch as rule-based approaches and includes a discussion of how recent advancements in language models\nhave been leveraged for anonymization purposes. These traditional methods have been the cornerstone of\nanonymization for decades. In more recent developments, the rapid adoption of Differential Privacy has"}, {"title": "4.1.1 Classical Anonymization", "content": "As already mentioned, anonymity definitions can be traced back to 1997 with k-anonymity. Anonymiza-\ntion aims to make impossible the re-identification of an individual, that is the process of reconstructing\nthe identity of an individual given a record that contains their information. Different attributes could be\nanonymized. While effectively masking PII - like names, emails, and social security numbers - is necessary,\nsome features that are not exactly identifiers can be used in combination to identify an individual: those\nfeatures are called quasi-identifiers. Quasi-identifiers are apparently non-personal attributes - like gender,\ndate of birth, and zip codes - that can be used in combination with external resources to identify a person\nfrom its anonymized records. The definition of k-anonymity (Sweeney, 1997) ensures that quasi-identifier\ncannot be used for re-identification. Essentially, a dataset is said to have k-anonymity if any given record in\nthe dataset is indistinguishable from at least k - 1 other records with respect to certain identifying attributes\ncalled quasi-identifiers. Upon this concept, several implementation techniques were proposed by Samarati &\nSweeney (1998), like generalization (modifying attribute values to less specific but accurate representations),\nsuppression (i.e., entirely removing data that are outliers or too identifiable), minimal Generalization (en-\nsuring data are not generalized more than necessary to meet k-anonymity). Of course, this method is highly\nsusceptible to the choice of k (even inapplicable sometimes) and gives coarse data, making it impossible to\ncarry out fine-grained analysis. It is also important to mention that, due to the reliance on quasi-identifiers,\nthis method is much less effective with unstructured text.\nSince anonymization has such a long history with the first paper theorizing k-anonymity dating back to\n1997 (Sweeney, 1997) much research was devoted to this field, resulting in the development of several\ntechniques. Larbi et al. (2022) give an overview of many anonymization techniques, with their different pros\nand cons based on the use case. In this section, we will show the most relevant ones that are also available\nin a synthetic tabular format in Table 2.\nThe standard process is outlined in Figure 4. After any necessary preprocessing phase, the first challenge to\nsolve is to identify the entities that may cause a violation of privacy. This can be done via a Named Entity\nRecognition (NER) tool. To maintain the coherence of the text, a Co-reference Resolution (CRR) system can\nbe used to resolve references between entities should be identified if the anonymization framework aims to\nensure consistent anonymization across the document. Finally, private information should be replaced with\nanonymous information. The anonymization methods can vary. Suppression is used to completely remove\nthe information. Tagging allows the replacement of sensitive information with artificial labels -like the one\nobtained by the NER system- that still retain information about the class of the data and an identifier.\nThe sensitive information can be also replaced with other textual information: random substitution and\ngeneralization can both be used. Random substitution uses another random entity of the same class, while\ngeneralization replaces an entity with a more general term.\nMamede et al. (2016) present a modular anonymization framework for text documents in Portuguese that\nclosely resembles the general workflow for this task. The system consists of four modules: pre-processing,\nNamed Entity Recognition (NER), Co-reference Resolution (CRR), and Anonymization. They notice that\nthe effectiveness of automatic text anonymization methods relies heavily on the performance of Named Entity\nRecognition (NER) and Co-reference Resolution (CRR) modules and while CRR shows good performance,\nthe NER can be very lacking depending on the dataset. They also implement and test a number of different\nanonymization strategies: all the anonymization methods presented can be effective in some cases, but\nthey all exhibit some drawbacks. Suppression, while dependent on NER accuracy, often compromises text\ncomprehension due to loss of information, challenging readers to grasp entity references. Tagging helps\nmaintain references between anonymized entities but detracts from natural text flow, necessitating improved\nNER and CRR accuracy. Random substitution maintains natural language output but can lead to semantic\ndrifts due to arbitrary replacements, suggesting a need for a curated list of vague yet contextually appropriate\nentities. Generalization offers a balanced approach by replacing specific entities with broader categories and"}, {"title": "4.1.2 Anonymization with Differential Privacy", "content": "In this section", "rewriting": "a text is modified to\npreserve privacy by perturbing the original text representations. The main advantage is that the Differential\nPrivacy formal guarantees are met. All the works in this section are also available in tabular format in\nTable 3. Furthermore, referring to , we identify the works in this section as examples of Differential\nPrivacy applied on the input (1), as they all use DP to pre-process the input data before actually feeding it\nto the LLM, regardless of this being for training or inference.\nFeyis"}]}