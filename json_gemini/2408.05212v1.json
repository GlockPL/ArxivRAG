{"title": "Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions", "authors": ["Michele Miranda", "Elena Sofia Ruzzetti", "Andrea Santilli", "Fabio Massimo Zanzotto", "S\u00e9bastien Brati\u00e8res", "Emanuele Rodol\u00e0"], "abstract": "Large Language Models (LLMs) represent a significant advancement in artificial intelligence, finding applications across various domains. However, their reliance on massive internet- sourced datasets for training brings notable privacy issues, which are exacerbated in critical domains (e.g., healthcare). Moreover, certain application-specific scenarios may require fine- tuning these models on private data. This survey critically examines the privacy threats associated with LLMs, emphasizing the potential for these models to memorize and inadver- tently reveal sensitive information. We explore current threats by reviewing privacy attacks on LLMs and propose comprehensive solutions for integrating privacy mechanisms through- out the entire learning pipeline. These solutions range from anonymizing training datasets to implementing differential privacy during training or inference and machine unlearning af- ter training. Our comprehensive review of existing literature highlights ongoing challenges, available tools, and future directions for preserving privacy in LLMs. This work aims to guide the development of more secure and trustworthy AI systems by providing a thorough understanding of privacy preservation methods and their effectiveness in mitigating risks.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) represent a significant advancement in artificial intelligence (AI), demon- strating remarkable capabilities in understanding and generating human-like text, and fueling innovation across various domains (Brown et al., 2020a; Touvron et al., 2023; Zhao et al., 2023). However, their reliance on massive internet-sourced datasets for training raises significant privacy concerns considering that these models are prone to memorize segments of their training data (Feldman, 2020; Feldman & Zhang, 2020). This is further exacerbated if we consider certain application-specific scenarios (e.g, healthcare) that may requires the fine-tuning of publicly available models on user private data (Tram\u00e8r et al., 2022). Since lan- guage models are specifically trained to generate text, this poses a fundamental threat considering that they might inadvertently disclose memorized private data at any moment (Carlini et al., 2019; 2021)."}, {"title": "PII", "content": "Personal Identifiable Information (PII) refers to any data that can be used to identify a specific individual. This may include direct information such as a person's name, address, email address, social security number, phone number, genome, fingerprints, and face, as well as any other data that, when combined, could lead to the identification of an individual.\nThe survey is organized as follows and divided into two main categories: privacy attacks (\u00a73) and solutions (\u00a74-5). The taxonomy in Figure 1 provides a structured view of our survey on preserving privacy in LLMs. For readers unfamiliar with the terminology, we provide a preliminaries section (\u00a72) that explains the basic concepts related to language models, differential privacy, and federated learning.\nPrivacy attacks encompass training data extraction (\u00a73.1), membership inference (\u00a73.2), and model inversion (\u00a73.3). Training data extraction involves techniques such as non-adversarial extraction (\u00a73.1.1) and adversar- ial prompting (\u00a73.1.2), where attackers can potentially extract sensitive information from the training data. Membership inference utilizes shadow models (\u00a73.2.1) or thresholds (\u00a73.2.2) to infer whether specific data points were part of the training set. Model inversion includes model output inversion (\u00a73.3.1) and gradient inversion (\u00a73.3.2) methods that can reconstruct the input data from model outputs.\nSolutions to these privacy challenges are categorized into privacy in data (\u00a74) and privacy in model (\u00a75). Privacy in data includes classical anonymization techniques to anonymize data before it is used in training (\u00a74.1.1) and anonymization with differential privacy (DP) (\u00a74.1.2), which ensures that individual data points cannot be distinguished from the dataset. Privacy in model encompasses the application of DP during different phases such as pre-training (\u00a75.1), fine-tuning (\u00a75.1.2), Parameter-Efficient Fine-Tuning (PEFT) (\u00a75.1.3), and inference (\u00a75.2). Federated learning with DP (\u00a75.3) is implemented to maintain privacy across distributed datasets, and machine unlearning (\u00a75.4) involves techniques to effectively remove specific data from trained models. Finally, we present available tools and frameworks to implement privacy in language models (\u00a76).\nIn summary, this survey serves as a critical resource for researchers, and practitioners providing insights into the current privacy threats in LLMs and the currently available solutions to address these challenges. By systematically categorizing privacy issues and their corresponding mitigation strategies, this paper not only highlights the importance of preserving privacy in LLMs but also paves the way for future advancements in the field. All the papers in this survey are also available at the following GitHub repository\u00b9."}, {"title": "2 Preliminaries", "content": "In this section, we present preliminary concepts essential for comprehending the subsequent content of the document. Readers already acquainted with these concepts may proceed directly to the next section.\nMachine Learning Machine learning involves the development of algorithms that can learn from and make predictions or decisions based on data by recognizing patterns in it, utilizing various techniques that include both linear and nonlinear models. Classical machine learning often relies on handcrafted features that require domain knowledge to be created. Classical machine learning models include decision trees, support vector machines, linear regression, and ensemble methods like random forests, designed for specific data types and tasks. Models are composed of parameters, which are what is learned in the learning process. These are variables that the learning algorithm adjusts during training to fit the model to the data as closely as possible. Parameters can be weights, as they affect how much of the input is passed between neurons (as a multiplication factor), or biases, shifting the outputs along the value axis (as an addition factor). Network parameters are very influential and their magnitude defines the model complexity and its capacity to recognize more intricate patterns in the data and generalize better. More complex networks fall under deep learning, see 2. There are three different types of learning: supervised learning, where there is a ground truth, and the model tries to output an answer as close as possible to that; unsupervised learning, where there is no answer and the model groups inputs based on some similarity; reinforcement learning, where"}, {"title": "2.1 Differential Privacy", "content": "According to Dwork & Roth (2014): \"Differential privacy describes a promise, made by a data holder, or curator, to a data subject: \"you will not be affected, adversely or otherwise, by allowing your data to be used in any study or analysis, no matter what other studies, data sets, or information sources, are available. \" The fundamental goal of Differential Privacy is to enable the extraction of useful insights from data while minimizing the potential impact on any individual's privacy within that dataset. The core idea is to introduce controlled noise or randomness into the analysis or results in a way that prevents the identification of specific individuals. In simpler terms, when a system or algorithm is differentially private, the inclusion or exclusion of any individual's data should not significantly affect the overall outcome or conclusions of the analysis. There are several kinds of Differential Privacy definitions, but the original one is (\u03b5, \u03b4)-Differential Privacy.\n(\u03b5, \u03b4)-Differential Privacy\nA randomized algorithm M with domain $||$ is (\u03b5, \u03b4)-differentially private if for all $S \\subseteq Range(M)$ and for all x, y \u2208 N|x| such that $||x - y||_1 \\leq 1$ :\n$Pr[M(x) \\in S] \\leq exp(\\varepsilon) Pr[M(y) \\in S] + \\delta$\nThe formal guarantee provided by (\u03b5, \u03b4)-Differential Privacy is that for any two datasets x and x' that differ by only one element (these are often referred to as \"adjacent datasets\"), and for any subset of outputs S of the algorithm's range Y, the probability that the algorithm outputs something in S given x is not substantially higher than given x'.\ne is the parameter that measures the privacy loss associated with a query in the system. A smaller e provides stronger privacy guarantees. The use of e in the inequality indicates that the probability of a certain output does not increase by more than a multiplicative factor of e when any single individual's data is added or removed.\nThe privacy budget is the total loss of privacy sustained by the system during the execution, and it can be computed by compounding the privacy loss indicated by e over the total number of executions/iterations. This is done with the composition theorems, which are explained in more detail later in this section.\nd is the parameter that allows for a small probability of failure of the privacy guarantee. Ideally, 8 should be very small, close to 0, which indicates a lower chance of the algorithm failing to maintain privacy.\nThis means that for all subsets S of the output range Y, the probability that the algorithm on input x gives an output in S is at most e raised to the power e times the probability that the algorithm on input x' gives an output in S, plus the small probability \u0431.\nAnother prominent definition of Differential Privacy, which is actually a relaxation of the original definition, is R\u00e9nyi Differential Privacy (Mironov, 2017)."}, {"title": "R\u00e9nyi Differential Privacy", "content": "R\u00e9nyi Differential Privacy\nA randomized algorithm M : D \u2192 R is said to have e-R\u00e9nyi differential privacy of order a, or (\u03b1, \u20ac)-RDP for short, if for any adjacent D, D' \u2208 D it holds that\n$D_\\alpha (f(D)||f (D')) \\leq \\epsilon$\nHere, $D_\\alpha(P||Q)$ represents the R\u00e9nyi divergence of order a between two probability distributions P and Q, and is defined as:\n$D_\\alpha(P||Q) = \\frac{1}{\\alpha-1}log \\frac{\\sum_{x \\in X} P(x)^\\alpha}{Q(x)^{1-\\alpha}}$\nfor all a > 1, where X is the support of the distributions P and Q. The parameter a here controls the sensitivity of the divergence measure to differences between P and Q, with higher values of a focusing on events that are more probable under P than under Q. The parameter e still quantifies the privacy guarantee, with smaller values indicating stronger privacy.\nBy providing a mathematically rigorous definition of privacy guarantees, Differential Privacy has become a key principle for designing privacy-preserving algorithms, especially in scenarios where personal data is involved. From this mathematical definition, some interesting properties arise. Among these, there are composition theorems that allow the computation of the exact privacy loss through many applications of the DP algorithm, which corresponds to the privacy budget. Sequential composition theorem (Kairouz et al., 2015) applies when multiple differentially private mechanisms are applied to the same dataset in sequence. The theorem quantifies how the privacy guarantees accumulate over the sequence of operations. Parallel composition theorem (McSherry, 2009) applies when multiple differentially private mechanisms are applied to disjoint subsets of the dataset. This theorem shows that the privacy guarantee of the overall process is governed by the worst-case privacy guarantee of any single mechanism. Differential Privacy also exhibits a Post-Processing property that states that once data has been processed through a DP mechanism, the output retains its privacy guarantees regardless of any further processing.\nDifferential privacy was born as a statistic analysis technique for databases. Initially, it was used to query databases without compromising the privacy of anybody enrolled, and that is why we discussed about queries and compounding privacy over them to compute the total privacy loss or privacy budget. Luckily, all this came in handy in recent years, with the worldwide diffusion of deep neural networks, when differential privacy was applied to them. That is when databases became datasets and queries became (mostly training) iterations. We will see more in detail what is the result of this transition in the next paragraph \u00a72.1."}, {"title": "2.2 Deep Learning with Differential Privacy", "content": "Differential privacy was initially devised for database analysis, but it is possible to apply it to deep learning models in several ways. In practice, since we primarily work with deep learning models, we will often refer to datasets rather than databases. However, this distinction does not entail any practical difference in the context of our discussion. Figure 2 shows how Differential Privacy can be incorporated in 1) the input data, 2) the gradients, 3) the output data, 4) the labels or even in the loss function 5). The most used technique is by training using DP-SGD (Differentially Private Stochastic Gradient Descent), while DP-Adam is less used we discuss the reasons for this in \u00a75.1.2. Both of these techniques involve gradient manipulation, falling under the point 2) in Figure 2. Initially presented by Song et al. (2013), DP-SGD is a variation of standard Stochastic Gradient Descent (SGD) that enhances privacy by incorporating gradient clipping and adding Gaussian noise. In DP-SGD, gradients are first clipped to a maximum norm to limit the influence of any single data point. Then, Gaussian noise is added to the average of these clipped gradients, ensuring that the updates to the model parameters maintain differential privacy by obscuring the contribution of individual data points. It is possible to consider the several layers separately in order to have different clipping thresholds, and the amount of noise should be calibrated over the sensitivity of the function (in this"}, {"title": "3 Privacy Attacks", "content": "As the training data becomes huge, it is increasingly urgent to understand whether models tend to expose the data on which they have been trained since it is more likely for them to capture sensitive information. A broad research area is devoted to understanding whether deep learning algorithms memorize training examples and, hence, can be used to reconstruct (potentially sensitive) training information. While some works question the effect of memorization during generation - meaning that texts generated starting from prompts extracted from training are different from the original ones (McCoy et al., 2023) - the discussion around memorization and its consequent risks is growing as larger models become more popular. In some cases, memorization is discussed as the key to good performance since data are often characterized by long- tailed distributions, hence, for a model, it is hard to distinguish between outliers and actually informative data points (Feldman & Zhang, 2020). However, memorization of training data by a model allows an attacker to reconstruct sensitive information or to understand whether a certain example was used during training. It has been shown by Carlini et al. (2023a) that it is possible to reconstruct training data with only black-box access.\nFor LLMs, it might be sufficient to input the right prompt, which is not necessarily an adversarial one (Carlini et al., 2021), in order to condition a model to generate sequences that are exactly memorized from the training data. Attacks that allow verbatim extraction of training data are generally referred to as Training Data Extraction attacks. When LLMs are fine-tuned with Reinforcement Learning with Human Feedback (RLHF), they tend to be more robust and avoid revealing the training set examples and, in particular, revealing sensitive information. However, some adversarial prompts have emerged to condition a model to show some unintended behaviour: since sensitive information is memorized by a model, RLHF is used to"}, {"title": "3.1 Training Data Extraction", "content": "In Training Data Extraction attacks, an adversary aims to reconstruct training data having only access to a target Language Model. In particular, those attacks aim to extract the entire example by querying the target model (Carlini et al., 2021). Retrieval of examples is based on the assumption that they were memorized by the target models, and the definitions of memorization underlying these attacks may vary. However, the various definitions revolve around the key concept that a textual example or part of it can be extracted from a language model. In the original definition of Carlini et al. (2021), an example is defined to be extractable if there is a prefix used to condition the generation of the model so that the string can be exactly generated from the model. This definition is first introduced by Carlini et al. (2021) and then largely adopted with slight modifications (Huang et al., 2022; Carlini et al., 2023a; Nasr et al., 2023).\nTraining Data Extraction\nExtracting Training Data consists of conditioning a LM to the right prompt so that it is likely that\nit will generate a training example to complete it. Formally, a textual example x is extractable from\na LM g if there exists a prompt p such that: x arg $maxx' g(x'p)$. Once the right prompt has\nbeen identified, this procedure can be used to reconstruct training data, and hence, it can be used to\nextract sensitive information like Personally Identifiable Information (PII) from a model."}, {"title": "3.1.1 Non-adversarial extraction", "content": "Using training data extraction techniques, it has been shown that Language Models are prone to reveal private information when prompted with sentences that require the disclosure of such information. While trying to recover memorized training examples, Carlini et al. (2021) demonstrated that GPT-2 tends to memorize personal information such as Twitter handles, emails and UUIDs (Universal Unique Identifiers). In particular, in their experiments the language model generation is conditioned to some prompts that the attacker predefined: to maximize the probability of success of the attack, they choose prompts that are likely to be in the training data, like samples from the Common Crawl \u00b2. Then, once the generation is complete, the attacker tries to figure out which sequences are most likely to be generated as a result of the memorization, that is, which of the generated strings exactly matches the training data. In order to identify sequences that are more likely to be generated because of memorization, Carlini et al. (2021) compare the likelihood that is assigned to each sequence by two models: one is the model under attack - GTP-2 \u2013 and the other is a reference model, another LM. The attacker retains as possibly memorized sentences only those that have a high likelihood in the target model when compared to the likelihood assigned by the reference"}, {"title": "3.1.2 Adversarial prompting", "content": "There have been initiatives to ensure LLMs are aligned with human values and used appropriately. For example, the OpenAI usage policy requires that users do not use the models to compromise the privacy of others, including collecting or generating personal data. Moreover, OpenAI's ChatGPT has been aligned with Reinforcement Learning from Human Feedback (Ouyang et al., 2022) and avoids answering questions regarding its training data. In this scenario, the techniques based on non-adversarial prompts may not be effective: Nasr et al. (2023) discuss how ChatGPT tends to not expose training data when prompted with prefixes that are likely seen during training. For these reasons, some attacks against LLMs define adversarial prompts that force the model to diverge from its originally intended use. For example, asking the model to repeat indefinitely a word, Nasr et al. (2023) notice that ChatGPT diverges and starts emitting training data, also containing personally identifiable information.\nA new form of adversarial prompt attacks are jailbreak prompts. These prompts are designed to circumvent safeguards and manipulate LLMs into generating harmful content. In some cases, a goal hijacking is per- formed, meaning that the original goal of a prompt is changed to a new goal, like generating a target phrase, by injecting malicious text in the original prompt (Perez & Ribeiro, 2022). Shen et al. (2023) characterize a large set of jailbreak prompts and evaluate their effectiveness against different LLMs. They notice that most of these prompts encourage the model to act like another fictional assistant, such as DAN (short for Doing Anything Now), which has no limitation in the answers. Along with other potentially harmful scenarios, they show that this type of prompt allows a user to receive suggestions on how to collect personal information.\nIn this context, Li et al. (2023b) focused on the usage of adversarial prompts to pose privacy threats: they propose a multi-step jailbreak prompt to extract personal information, such as emails, based on Chain-of- Thought (Kojima et al., 2023). They study the effect of adversarial prompting against ChatGPT. They first notice that the model tends to avoid sharing private information if direct data extraction prompts are used. Then they propose the Multi-step Jailbreaking Prompt in three steps: they assume the role of the user to input the jailbreaking prompt, then act as the assistant to confirm that the jailbreak mode is enabled and, lastly, they play the role of the user and query the assistant with a non-adversarial prompt. They show that more frequent emails are less difficult to recover: experiments on the Enron dataset show that more than 50% of frequent emails in the dataset can be extracted."}, {"title": "3.2 Membership Inference Attacks (MIA)", "content": "Membership inference attacks (MIA) (Shokri et al., 2016) are currently a de facto standard when it comes to verifying model privacy (Murakonda & Shokri, 2020). In this kind of attack, the attacker has access to (1) some records (supposed to come from a similar distribution of the training data) and (2) to the model, which could be provided as an API, in a black box setting, or be fully available as a white box. The aim of the"}, {"title": "MIA as Classification of Behavioural Changes", "content": "MIA can be framed as a classification problem: the attacker determines whether a certain example\nhas been used during the training phase of the model or not. Attacks are based on detecting changes\nin the target model when it is exposed to data points that are in the training set versus when it is\nexposed to examples that are not part of it."}, {"title": "3.2.1 \u039c\u0399\u0391 with Shadow Models", "content": "Stemming from the general setting described in Shokri et al. (2016), MIAs have been successfully tailored for Language Models to extract sensitive information from them. Song & Shmatikov (2019) argue that Language Models tend to memorize training data and, therefore, a user can tell whether their data have been used also in a black box setting. Their attack positively exploits the tendency of these models to rank relatively rare words higher when they are in the same context in which they were seen during training. Hisamoto et al. (2020) try to make this type of attack more general by training an attack classifier not on the output probabilities but only on features that can be extracted from the output sequences of a sequence-to-sequence Transformer model. Carlini et al. (2022) observe that MIAs could be better framed as the procedure of comparing models that are trained on a given example and models that are not trained on it, and to find a difference in their behaviour, for example having a likelihood of the example much higher in the first case rather than in the other. They train numerous shadow GPT-2 models to measure the probability of observing a certain likelihood of an example in models trained and not trained on it. However, training shadow models may be too expensive in the context of LLMs: the original formulation of MIA can be used as long as it is feasible for the attacker to construct shadow models that are similar to the target model."}, {"title": "3.2.2 \u039c\u0399\u0391 with Thresholds", "content": "Since the construction of shadow models can be too expensive, MIAs have also been formulated without relying on them. In fact, some MIAs have been designed to exploit some measure of overfit over the training data in order to understand whether a given example has already been seen by a model: overfitting the training data is a sufficient condition to detect memorized examples (Yeom et al., 2018). Shi et al. (2023) observe that the probability distribution is more spread over low probability words for an unseen example: they design attacks that threshold the average log-likelihood of the top-k rarest words to ascertain whether or not the example is part of the training data. Language models trained on clinical data have been shown to be sensitive to MIAs. Mireshghallah et al. (2022) demonstrate the effectiveness of MIAs designed against Language Models trained with Masked Language Modeling objectives, obtaining a substantial gain both in"}, {"title": "3.3 Model Inversion", "content": "Model inversion attacks generally exploit the output of the model and use this information to reconstruct the input data that generated the observed outputs. In the initial formulation (Fredrikson et al., 2014; 2015), an adversary is given access to the trained model and to a partially masked input, for which only non-sensitive features are disclosed. The attacker aims to infer the value of the sensitive attributes that serve as input to the model. While the initial formulation is implemented for linear regression models (Fredrikson et al., 2014), this type of attack has been successfully used to extract sensitive input information from different models. Fredrikson et al. (2015) applied for the first time a pattern inversion attack against a neural network, trained for facial recognition: they were able to partially reconstruct the faces of people in the training set exploiting the prediction of the system, framing the inversion as an optimization problem that requires reconstruction of the image that justifies the observed output distribution. Model inversion attacks are often performed against distributed models: in fact, some protocols like federated learning ensure that private data are not shared across multiple servers (see Section 2.2 for further details). However, with some access to model outputs, it is possible for an attacker to reconstruct the input data that causes the observed output.\nModel Inversion Attacks\nThis family of attacks threatens models especially when they are distributed across multiple servers:\nin fact, while data holders do not directly share private data - which are thus maintained locally-\nthey may be willing to share seemingly opaque information related to a machine learning model,\nsuch as model outputs, embeddings or gradients. However, Model Inversion attacks are designed to\nreconstruct private data from these apparently opaque sources."}, {"title": "3.3.1 Model Output Inversion", "content": "The outputs of a model, like the representation of textual data that a Transformer-based LM produces in the last layer, can be totally or partially inverted to reconstruct the text that is originally the input of the model. In particular, these representations - often called embeddings are only apparently opaque and can be inverted to reconstruct the entire sentence or the words that it contains. Pan et al. (2020) design the first model inversion attack against Transformer-based Language Models. They observe that directly framing the inversion problem as an optimization problem is challenging due to the discrete nature of sentences. They train an attack classifier that is designed to extract fixed patterns and keywords like, for example, location or medical information \u2013 from sentence representations of models. Similarly, Song & Raghunathan (2020) predict unordered sets of words from sentence embeddings: their attack model is a neural network trained with the multiset prediction loss (Welleck et al., 2018) so that it is possible for the model to predict each word in the embedding also conditioned on the words already predicted."}, {"title": "3.3.2 Gradient Inversion", "content": "In federated learning (Ray et al., 2021) multiple nodes train a model cooperatively without directly sharing data samples: the system is designed to make training feasible while exchanging model updates rather than data across nodes (see $5.3 for further details). Gradient Inversion attacks are directed against federated learning environments that perform training by sharing gradients. Those attacks can be used to reconstruct training data from seemingly opaque sources.\nZhu et al. (2019) demonstrated that if the gradients are openly accessible, it is possible to reconstruct the training data. The attacker aims to reconstruct the training data by minimizing the distance between some generated gradients and the ground truth one. In particular, the attacker initially generates gradients from random inputs and labels. Then, having access to the ground truth gradients solves the optimization problem of minimizing the L2 distance between the generated gradients and the ground truth one, parameterized by real inputs and labels. In their experiments, Zhu et al. (2019) applied this attack against a BERT model. This idea was further explored by inverting gradients for image classification tasks (Zhu & Blaschko, 2020; Geiping et al., 2020; Yin et al., 2021).\nIn reconstructing textual data, Deng et al. (2021) further attempted to perform a gradient attack against Transformer-based language models introducing also a L\u2081 term in the reconstruction loss. Experiments on a Transformer model, BERT model and TinyBERT demonstrate the effectiveness of this gradient attack, recovering up to 50% of the original tokens on average. In LAMP (Balunovic et al., 2022), more effective results are achieved by simultaneously training the attack model to minimize the difference between the reconstruction gradients and choosing at each iteration only sequences that have low perplexity according to an external language model (like GPT-2). Reconstructing the correct word order is, in fact, one of the main challenges: Gupta et al. (2022) propose to recover from the gradients a bag of words for the sentence to extract and then perform a beam search to effectively reconstruct the sentence."}, {"title": "4 Solutions for Preserving Privacy in LLMs: Data", "content": "After identifying the key vulnerabilities and potential attacks on LLMs as outlined in \u00a73, in this section, we introduce the reader to the current possible solutions to the problem of preserving privacy in the context of LLMs and text generative models. We have to say that the field is very fragmented as this is a very recent concern and the field is evolving fast. There is not, at the moment of writing this survey, a single gold standard solution to the problem of preserving privacy in LLMs and text generative models. The main solutions available can be broadly categorized into two categories: i) Preserving Privacy in Data \u00a74 and ii) Preserving Privacy in Model \u00a75. In the former section, we collected all the methods that try to solve the problem at its source i.e., they act directly on the privacy-sensitive information in the dataset, before the training happens, while in the latter we selected all the methods that address the problem from the model perspective i.e., try to remove from the model problematic information after the training. The two approaches fit different use cases, so it is important to be aware of both.\nIn this section, we propose several solutions that allow preserving privacy from the data perspective i.e., we preserve privacy directly in the training data used in machine learning models. This includes any approach that involves manipulating, carefully choosing, or generating training data in order to avoid the memorization and subsequent leakage of private information inside a large language model or any model that learns from data. This basically puts the focus on preserving privacy in the data used to train the model. Given that a machine learning model is moulded by its training dataset, training it on a privacy-preserving dataset ensures that these methods offer equivalent privacy assurances to the dataset itself. The section is structured in the following way: we first show classical anonymization techniques \u00a74.1, then we focus on anonymization techniques built with Differential Privacy \u00a74.1.2. Putting this last section in relation with Figure 5, the point of application of DP would be (1), the input, because all these techniques aim to apply Differential Privacy on the data before feeding it to the LLM (for either training or inference).\nPreserving Privacy in Data\nKey Idea: Preserve privacy directly on the dataset used to train the model."}, {"title": "4.1 Anonymization", "content": "Data anonymization is not a new concept, with seminal work dating back to the 1990s: a pivotal develop- ment was the introduction of k-anonymity in 1997 (Sweeney, 1997), which means ensuring that data cannot be re-identified to fewer than k individuals. This foundational concept has spurred extensive research, lead- ing to a plethora of anonymization techniques. Given the breadth and depth of the field, this section is organized into two main parts: the first, titled Classical Anonymization 4.1.1, explores traditional methods such as rule-based approaches and includes a discussion of how recent advancements in language models have been leveraged for anonymization purposes. These traditional methods have been the cornerstone of anonymization for decades. In more recent developments, the rapid adoption of Differential Privacy has"}, {"title": "4.1.1 Classical Anonymization", "content": "As already mentioned, anonymity definitions can be traced back to 1997 with k-anonymity. Anonymiza- tion aims to make impossible the re-identification of an individual, that is the process of reconstructing the identity of an individual given a record that contains their information. Different attributes could be anonymized. While effectively masking PII - like names, emails, and social security numbers - is necessary, some features that are not exactly identifiers can be used in combination to identify an individual: those features are called quasi-identifiers. Quasi-identifiers are apparently non-personal attributes - like gender, date of birth, and zip codes - that can be used in combination with external resources to identify a person from its anonymized records. The definition of k-anonymity (Sweeney, 1997) ensures that quasi-identifier cannot be used for re-identification. Essentially, a dataset is said to have k-anonymity if any given record in the dataset is indistinguishable from at least k - 1 other records with respect to certain identifying attributes called quasi-identifiers. Upon this concept, several implementation techniques were proposed by Samarati & Sweeney (1998), like generalization (modifying attribute values to less specific but accurate representations), suppression (i.e., entirely removing data that are outliers or too identifiable), minimal Generalization (en- suring data are not generalized more than necessary to meet k-anonymity). Of course, this method is highly susceptible to the choice of k (even inapplicable sometimes) and gives coarse data, making it impossible to carry out fine-grained analysis. It is also important to mention that, due to the reliance on quasi-identifiers, this method is much less effective with unstructured text.\nSince anonymization has such a long history with the first paper theorizing k-anonymity dating back to 1997 (Sweeney, 1997) much research was devoted to this field, resulting in the development of several techniques. Larbi et al. (2022) give an overview of many anonymization techniques, with their different pros and cons based on the use case. In this section, we will show the most relevant ones that are also available in a synthetic tabular format in Table 2.\nThe standard process is outlined in Figure 4. After any necessary preprocessing phase, the first challenge to solve is to identify the entities that may cause a violation of privacy. This can be done via a Named Entity Recognition (NER) tool. To maintain the coherence of the text, a Co-reference Resolution (CRR) system can be used to resolve references between entities should be identified if the anonymization framework aims to ensure consistent anonymization across the document. Finally, private information should be replaced with anonymous information. The anonymization methods can vary. Suppression is used to completely remove the information. Tagging allows the replacement of sensitive information with artificial labels like the one obtained by the NER system- that still retain information about the class of the data and an identifier. The sensitive information can be also replaced with other textual information: random substitution and generalization can both be used. Random substitution uses another random entity of the same class, while generalization replaces an entity with a more general term.\nMamede et al. (2016) present a modular anonymization framework for text documents in Portuguese that closely resembles the general workflow for this task. The system consists of four modules: pre-processing, Named Entity Recognition (NER), Co-reference Resolution (CRR), and Anonymization. They notice that the effectiveness of automatic text anonymization methods relies heavily on the performance of Named Entity Recognition (NER) and Co-reference Resolution (CRR) modules and while CRR shows good performance, the NER can be very lacking depending on the dataset. They also implement and test a number of different anonymization strategies: all the anonymization methods presented can be effective in some cases, but they all exhibit some drawbacks. Suppression, while dependent on NER accuracy, often compromises text comprehension due to loss of information, challenging readers to grasp entity references. Tagging helps maintain references between anonymized entities but detracts from natural text flow, necessitating improved NER and CRR accuracy. Random substitution maintains natural language output but can lead to semantic drifts due to arbitrary replacements, suggesting a need for a curated list of vague yet contextually appropriate entities. Generalization offers a balanced approach by replacing specific entities with broader categories and"}, {"title": "4.1.2 Anonymization with Differential Privacy"}]}