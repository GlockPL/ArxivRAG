{"title": "Revisiting Random Walks for Learning on Graphs", "authors": ["Jinwoo Kim", "Olga Zaghen", "Ayhan Suleymanzade", "Youngmin Ryou", "Seunghoon Hong"], "abstract": "We revisit a simple idea for machine learning on graphs, where a random walk\non a graph produces a machine-readable record, and this record is processed by a\ndeep neural network to directly make vertex-level or graph-level predictions. We\nrefer to these stochastic machines as random walk neural networks, and show\nthat we can design them to be isomorphism invariant while capable of universal\napproximation of graph functions in probability. A useful finding is that almost any\nkind of record of random walk guarantees probabilistic invariance as long as the\nvertices are anonymized. This enables us to record random walks in plain text and\nadopt a language model to read these text records to solve graph tasks. We further\nestablish a parallelism to message passing neural networks using tools from Markov\nchain theory, and show that over-smoothing in message passing is alleviated by\nconstruction in random walk neural networks, while over-squashing manifests as\nprobabilistic under-reaching. We show that random walk neural networks based on\npre-trained language models can solve several hard problems on graphs, such as\nseparating strongly regular graphs where the 3-WL test fails, counting substructures,\nand transductive classification on arXiv citation network without training. Code is\navailable at https://github.com/jw9730/random-walk.", "sections": [{"title": "1 Introduction", "content": "The most widely used class of neural networks on graphs are message passing neural networks where\neach vertex keeps a feature vector and updates it by propagating and aggregating messages over\nneighbors [67, 51, 46]. These networks benefit in efficiency by respecting the natural symmetries of\ngraph functions, namely invariance to graph isomorphism [21]. On the other hand, message passing\ncan be viewed as implementing color refinement iterations of the one-dimensional Weisfeiler-Lehman\n(1-WL) graph isomorphism test, and consequently their expressive power is not stronger [112, 83].\nAlso, the inner working of these networks is tied to the topology of the input graph, which is related\nto over-smoothing, over-squashing, and under-reaching of features under mixing [106, 48, 85].\nIn this work, we (re)visit a simple alternative idea for learning on graphs, where a random walk on a\ngraph produces a machine-readable record, and this record is processed by a deep neural network that\ndirectly makes vertex-level or graph-level predictions. We call these stochastic machines random\nwalk neural networks. Using random walks for graph learning has received practical interest due\nto their scalability and compatibility with sequence learning methods. This has led to empirically\nsuccessful algorithms such as DeepWalk [94] and node2vec [50] which use skip-gram models on\nrandom walks and CRaWl [105] which uses convolutions on random walks. However, principled\nunderstanding and design of random walk neural networks have been studied relatively less.\nWe study the fundamental properties of random walk neural networks, and show that they can be\ndesigned from principle to be isomorphism invariant while capable of universal approximation of"}, {"title": "2 Random Walk Neural Networks", "content": "We define a random walk neural network as a randomized function $\\mathcal{X}_{\\Theta}(\\cdot)$ that takes a graph $G$ as\ninput and outputs a random variable $X_{\\Theta}(G)$ on the output space $\\mathbb{R}^d$. It can be optionally queried\nwith an input vertex $v$ which gives a random variable $X_{\\Theta}(G, v)$. It consists of the following:\n1. Random walk algorithm that produces $l$ steps of vertex transitions $v_0 \\rightarrow \\cdots \\rightarrow v_l$ on the input\ngraph $G$. If an input vertex $v$ is given, we fix the starting vertex by $v_0 = v$.\n2. Recording function $q : (v_0 \\rightarrow \\cdots \\rightarrow v_l, G) \\rightarrow z$ that produces a machine-readable record $z$ of\nthe random walk. It may access the graph $G$ to record auxiliary information such as attributes.\n3. Reader neural network $f_{\\Theta} : z \\rightarrow \\hat{y}$ that processes record $z$ and outputs a prediction $\\hat{y}$ in $\\mathbb{R}^d$. It\nis the only trainable component and is not restricted to specific architectures.\nTo perform graph-level tasks, we sample a prediction $\\hat{y} \\sim X_{\\Theta}(G)$ by running a random walk on $G$\nand processing its record using the reader neural network. For vertex-level predictions $\\hat{y} \\sim X_{\\Theta}(G, v)$,\nwe query the input vertex $v$ by simply starting the random walk from it.\nIn the sections below, we discuss the design of each component for graph-level tasks on finite graphs\nand then vertex-level tasks on possibly infinite graphs. The latter simulates learning problems on\nlarge graphs, such as transductive classification, and forces the model to operate independently of the\nsize of the whole graph $G$ [81]. Pseudocode and proofs can be found in Appendix A.2 and A.4."}, {"title": "2.1 Graph-Level Tasks", "content": "Let $\\mathcal{G}$ be the class of undirected, connected, and simple graphs\u00b2. Let $n \\geq 1$ and $\\mathcal{G}_n$ be the collection\nof graphs in $\\mathcal{G}$ with at most $n$ vertices. We would like to model a graph-level function $\\phi : \\mathcal{G}_n \\rightarrow \\mathbb{R}^d$\nusing a random walk neural network $X_{\\Theta}(\\cdot)$.\nSince $\\phi$ is a function on graphs, it is reasonable to assume that it is isomorphism invariant:\n$$\\phi(G) = \\phi(H), \\ \\forall G \\sim H.$$\nIncorporating the invariance structure to our model class would offer generalization benefit [76, 13].\nSince $X_{\\Theta}(\\cdot)$ is a randomized function, we accept the probabilistic notion of invariance [63, 12, 26, 25]:\n$$X_{\\Theta}(G) \\stackrel{d}{=} X_{\\Theta}(H), \\ \\forall G \\sim H.$$\nWe now claim that we can achieve probabilistic invariance by properly choosing the random walk\nalgorithm and recording function while not imposing any constraint on the reader neural network.\nTheorem 2.1. $X_{\\Theta}(\\cdot)$ is invariant if its random walk algorithm and recording function are invariant.\nBelow, we describe the choice of random walk algorithm, recording function, and neural networks\nthat comprise random walk neural networks as in Theorem 2.1. Pseudocode is given in Appendix A.2.\nRandom walk algorithm A random walk algorithm is invariant if it satisfies the following:\n$$\\pi(v_0) \\rightarrow \\cdots \\rightarrow \\pi(v_l) \\stackrel{d}{=} u_0 \\rightarrow \\cdots \\rightarrow u_l, \\ \\forall G \\sim H,$$\nwhere $v[\\cdot]$ is a random walk on $G$, $u[\\cdot]$ is a random walk on $H$, and $\\pi : V(G) \\rightarrow V(H)$ specifies the\nisomorphism from $G$ to $H$. It turns out that many random walk algorithms in literature are already\ninvariant. To see this, let us write the probability of walking from a vertex $u$ to its neighbor $x \\in N(u)$:\n$$\\text{Prob}[v_t = x / v_{t-1} = u] := \\frac{c_G(u, x)}{\\sum_{y \\in N(u)} c_G(u, y)},$$\nwhere the function $c_g : E(G) \\rightarrow \\mathbb{R}_+$ assigns positive weights called conductance to edges. If we use\nconstant conductance $c_g(\\cdot) = 1$, we have unbiased random walk used in DeepWalk [94]. We show:\nTheorem 2.2. The random walk in Equation (4) is invariant if its conductance $c[\\cdot](\\cdot)$ is invariant:\n$$c_G(u,v) = c_H(\\pi(u), \\pi(v)), \\ \\forall G \\sim H.$$\nIt includes constant conductance, and any choice that only uses degrees of endpoints $\\text{deg}(u), \\text{deg}(v)$.\nWe favor using the degree information since it is cheap while potentially improving the behaviors\nof walks. Among many instantiations, we find that the following conductance function called the\nminimum degree local rule [3, 29] is particularly useful:\n$$c_G(u, v) := \\frac{1}{\\text{min}[\\text{deg}(u), \\text{deg}(v)]}.$$This conductance is special because it achieves $O(n^2)$ cover time, i.e. the expected time of visiting all\n$n$ vertices of a graph, optimal among random walks in Equation (4) that use degrees of endpoints [29].\nIn practice, we find that adding non-backtracking property that enforces $v_{t+1} \\neq v_{t-1}$ is helpful while\nnot adding much cost. Formally, we can extend Equation (4) and Theorem 2.2 to second-order random\nw"}, {"title": "Recording function", "content": "A recording function $q: (v_0 \\rightarrow \\cdot \\rightarrow v_l, G) \\rightarrow z$ takes a random walk and\nproduces a machine-readable record $z$. We let $q(\\cdot, G)$ have access to the graph $G$ the walk is taking\nplace. This allows recording auxiliary information such as vertex or edge attributes. A recording\nfunction is invariant if it satisfies the following for any given random walk $v[\\cdot]$ on $G$:\n$$q(v_0 \\rightarrow \\cdots \\rightarrow v_l, G) = q(\\pi(v_0) \\rightarrow \\cdots \\rightarrow \\pi(v_l), H), \\ \\forall G \\sim H.$$\nInvariance requires that $q(\\cdot, G)$ produces the same record $z$ regardless of re-indexing of vertices of $G$\ninto $H$. For this, we have to be careful in how we represent each vertex in a walk $v_0 \\rightarrow \\cdot \\rightarrow v_l$ as a\nmachine-readable value, and which auxiliary information we record from $G$. We make two choices:\n\u2022 Anonymization. We name each vertex in a walk with a unique integer, starting from $v_0 \\rightarrow 1$ and\nincrementing it based on their order of discovery. For instance, a random walk $a \\rightarrow b \\rightarrow c \\rightarrow a$\ntranslates to a sequence $1\\rightarrow2\\rightarrow3\\rightarrow1$.\n\u2022 Anonymization + named neighborhoods. While applying anonymization for each vertex $v$ in\na walk, we record its neighbors $u \\in N(v)$ if they are already named but the edge $v \\rightarrow u$ has not\nbeen recorded yet. For instance, a walk $a \\rightarrow b \\rightarrow c \\rightarrow d$ on a fully-connected graph translates\nto a sequence $1 \\rightarrow 2 \\rightarrow 3(1) \\rightarrow 4(1, 2)$, where $(\\cdot)$ represents the named neighborhoods.\nThe pseudocode of both algorithms can be found in Appendix A.2. We now show the following:\nTheorem 2.3. A recording function $q : (v_0 \\rightarrow \\cdots \\rightarrow v_l, G) \\rightarrow z$ that uses anonymization, optionally\nwith named neighborhoods, is invariant.\nOur choice of anonymizing a walk is inspired by the observation model of Micali and Zhu (2015) [81].\nWhile initially motivated by privacy concerns [81, 60], we find it useful for invariance. Recording\nnamed neighborhoods is inspired by sublinear algorithms that probe previously discovered neighbors\nduring a walk [28, 23, 10]. In our context, it is useful because whenever a walk visits a set of vertices\n$S \\subseteq V(G)$ it automatically records the entire induced subgraph $G[S]$. As a result, to record all edges\nof a graph, a walk only has to visit all vertices. It is then possible to choose a random walk algorithm\nthat takes only $O(n^2)$ time to visit all $n$ vertices [3, 29], while traversing all edges, i.e. edge cover\ntime, is $O(n^3)$ in general [122, 89]. The degree-biasing in Equation (6) precisely achieves this.\nReader neural network A reader neural network $f_{\\Theta}: z \\rightarrow \\hat{y}$ processes the record $z$ of the random\nwalk and outputs a prediction $\\hat{y}$ in $\\mathbb{R}^d$. As in Theorem 2.1, there is no invariance constraint imposed\non $f_{\\Theta}$, and any neural network that accepts the recorded walks and has a sufficient expressive power\ncan be used. This is in contrast to message passing where invariance is hard-coded in feature mixing\noperations. Furthermore, our record $z$ can take any format, such as a matrix, byte sequence, or plain\ntext, as long as $f_{\\Theta}$ accepts it. It is appealing in this regard to choose the record to be plain text, such\nas \"1-2-3-1\", and choose $f_{\\Theta}$ to be a pre-trained transformer language model. This offers expressive\npower [114] as well as transferable representation from language domain [75, 97]. In our experiments,\nwe test both \"encoder-only\" DeBERTa [52] and \"decoder-only\" Llama [107] language models as $f_{\\Theta}$."}, {"title": "2.2 Vertex-Level Tasks", "content": "We now consider vertex-level tasks. In case of finite graphs, we may simply frame a vertex-level task\n$(G, v) \\mapsto y$ as a graph-level task $G' \\mapsto y$ where $G'$ is $G$ with its vertex $v$ marked. Then, we can\nsolve $G' \\mapsto y$ by querying a random walk neural network $X_{\\Theta}(G, v)$ to start its walk at $v_0 = v$.\nA more interesting case is when $G$ is an infinite graph that is only locally finite, i.e., has finite degrees.\nThis simulates learning problems on a large graph such as transductive classification. In this case, it\nis reasonable to assume that our target function depends on finite local structures [103, 81].\nLet $r \\geq 1$, and $\\mathcal{B}_r := {\\mathcal{B}_r(v)}$ be the collection of local balls in $G$ of radius $r$ centered at $v \\in V(G)$.\nWe would like to model a vertex-level function $\\phi : \\mathcal{B}_r \\rightarrow \\mathbb{R}^d$ on $G$ using a random walk neural\nnetwork $X_{\\Theta}(\\cdot)$ by querying the vertex of interest, $X_{\\Theta}(G, v)$. We assume $\\phi$ is isomorphism invariant:\n$$\\phi(\\mathcal{B}_r(v)) = \\phi(\\mathcal{B}_r(u)), \\ \\forall \\mathcal{B}_r(v) \\approx \\mathcal{B}_r(u).$$\nThe invariance of $X_{\\Theta}(\\cdot)$ in probability is defined as following:\n$$X_{\\Theta}(G,v) \\stackrel{d}{=} X_{\\Theta}(G, u), \\ \\forall \\mathcal{B}_r(v) \\approx \\mathcal{B}_r(u).$$\nWe may achieve invariance by choosing the random walk algorithm and recording function similar to\nthe graph-level case. As a modification, we query $X_{\\Theta}(G, v)$ with the starting vertex $v_0 = v$ of the\nrandom walk. Anonymization informs $v$ to the reader neural network by always naming it as 1.\nThen, we make an important choice of localizing random walks with restarts. That is, we reset a walk\nto the starting vertex $v_0$ either with a probability $\\alpha \\in (0, 1)$ at each step or periodically every $k$ steps.\nA restarting random walk tends to stay more around its starting vertex $v_0$, and was used to implement\nlocality bias in personalized PageRank algorithm for search [88, 44]. This localizing effect is crucial\nin our context since a walk may drift away from $\\mathcal{B}_r(v_0)$ before recording all necessary information,"}, {"title": "3 Analysis", "content": "In Section 2, we have described the design of random walk neural networks, primarily relying on the\nprinciple of invariance. In this section we provide in-depth analysis on their expressive power and\nrelations to issues in message passing such as over-smoothing, over-squashing, and under-reaching."}, {"title": "3.1 Expressive Power", "content": "Intuitively, if the records of random walks contain enough information such that the structures of\ninterest, e.g. graph $G$ or local ball $\\mathcal{B}_r(v)$, can be fully recovered, a powerful reader neural network\nsuch as a multi-layer perceptron [27, 55] or a transformer [114] on these records would be able to\napproximate any function of interest. Our following analysis shows this.\nWe first consider using random walk neural network $X_{\\Theta}(\\cdot)$ to universally approximate graph-level\nfunctions $\\phi(\\cdot)$ in probability, as defined in Abboud et al. (2021) [1]. While we consider scalar-valued\nfunctions as often done in universality proofs, extension to vector-valued functions can be done easily.\nDefinition 3.1. $X_{\\Theta}(\\cdot)$ is a universal approximator of graph-level functions in probability if, for all\ninvariant functions $\\phi : \\mathcal{G}_n \\rightarrow \\mathbb{R}$ for a given $n \\geq 1$, and $\\forall \\epsilon, \\delta > 0$, there exist choices of length $l$ of\nthe random walk and network parameters $\\Theta$ such that the following holds:\n$$\\text{Prob}[|\\phi(G) - X_{\\Theta}(G)| < \\epsilon] > 1 - \\delta, \\ \\forall G \\in \\mathcal{G}_n.$$\nWe show that, if the random walk is long enough and the reader neural network $f_{\\Theta}$ is powerful\nenough, a random walk neural network $X_{\\Theta}(\\cdot)$ is capable of graph-level universal approximation in\nprobability. In particular, we show that the length of the random walk $l$ controls the confidence $1 - \\delta$\nof the approximation, with cover time $C_V(G)$ or edge cover time $C_E(G)$ playing a central role.\nTheorem 3.2. A random walk neural network $X_{\\Theta}(\\cdot)$ with a sufficiently powerful $f_{\\Theta}$ is a universal\napproximator of graph-level functions in probability (Definition 3.1) if it satisfies either of the below:\n\u2022 It uses anonymization to record random walks of lengths $l > C_E(G)/\\delta$.\n\u2022 It uses anonymization and named neighborhoods to record walks of lengths $l > C_V(G)/\\delta$.\nWe remark that, while the cover time and edge cover time are $O(n^3)$ for uniform random walks [4,\n122, 24, 41, 15, 2, 89, 45], we make use of minimum degree local rule in Equation (6) to achieve a\ncover time of $O(n^2)$ [3, 29], in conjunction with named neighborhood recording. While universal\napproximation can be in principle achieved with simple uniform random walk, our design reduces\nthe length $l$ required for the desired reliability $> 1 - \\delta$, and hence improves learning and inference.\nWe now show an analogous result for universal approximation of vertex-level functions. Since our\ntarget function $\\phi$ is on a collection $\\mathcal{B}_r$ of finite graphs, the proof is similar to the graph-level case.\nDefinition 3.3. $X_{\\Theta}(\\cdot)$ is a universal approximator of vertex-level functions in probability if, for all\ninvariant functions $\\phi : \\mathcal{B}_r \\rightarrow \\mathbb{R}$ for a given $r \\geq 1$, and $\\forall \\epsilon, \\delta > 0$, there exist choices of length $l$ and\nrestart probability $\\alpha$ or period $k$ of the random walk and network parameters $\\Theta$ such that:\n$$\\text{Prob}[|\\phi(\\mathcal{B}_r(v)) - X_{\\Theta}(G, v)| < \\epsilon] > 1 - \\delta, \\ \\forall \\mathcal{B}_r(v) \\in \\mathcal{B}_r.$$\nWe show the following, which relates to local cover time $C_V(\\mathcal{B}_r(v))$ and edge cover time $C_E(\\mathcal{B}_r(v))$:\nTheorem 3.4. A random walk neural network $X_{\\Theta}(\\cdot)$ with a sufficiently powerful $f_{\\Theta}$ and any nonzero\nrestart probability $\\alpha$ or restart period $k > r + 1$ is a universal approximator of vertex-level functions\nin probability (Definition 3.3) if it satisfies either of the below for all $\\mathcal{B}_r(v) \\in \\mathcal{B}_r$:"}, {"title": "3.2 Over-smoothing, Over-squashing, and Under-reaching", "content": "In message passing neural networks, the model operates by passing features over edges and mixing\nthem, and thus their operation is tied to the topology of the input graph. It is now understood how this\nrelates to the issues of over-smoothing, over-squashing, and under-reaching [9, 106, 47, 48, 85, 111].\nWe connect these theories and random walk neural networks to verify if similar issues may take place.\nLet $G$ be a connected non-bipartite graph with row-normalized adjacency matrix $P$. We consider a\nsimplified message passing model, where the vertex features $h^{(0)}$ are initialized as some probability\nvector $x$, and updated by $h^{(t+1)} = h^{(t)} P$. This simple model is often used as a proxy to study the\naforementioned issues [48, 118]. For example, over-smoothing happens as the features exponentially\nconverge to a stationary vector $h^{(l)} \\rightarrow \\pi$ as $l \\rightarrow \\infty$, smoothing out the input $x$ [48]. Over-squashing\nand under-reaching occur when a feature $h_v^{(l)}$ becomes insensitive to distant input $x$. While under-\nreaching refers to insufficient depth $l < \\text{diam}(G)$ [9, 106], over-squashing refers to features getting\noverly compressed at bottlenecks of $G$, even with sufficient depth $l$. The latter is described by the\nJacobian $\\frac{\\partial h^{(l)}}{\\partial x} < [\\sum_{t=0}^l P^t]_{uv}$, as the bound often decays exponentially with $l$ [106, 11].\nWhat do these results tell us about random walk neural networks? We can see that, while $P$ drives\nfeature mixing in the message passing schema, it can be also interpreted as the transition probability\nmatrix of uniform random walk where $P_{uv}$ is the probability of walking from $u$ to $v$. This parallelism\nmotivates us to design an analogous, simplified random walk neural network and study its behavior.\nWe consider a simple random walk neural network that runs a uniform random walk $v_0 \\rightarrow \\cdot \\rightarrow v_l$,\nreads the record $x_{v_0} \\rightarrow \\cdot \\rightarrow x_{v_l}$ by averaging, and outputs it as $h^{(l)}$. Like simple message passing,\nthe model involves $l$ steps of time evolution through $P$. However, while message passing uses $P$ to\nprocess features, this model uses $P$ only to obtain a record of input, with feature processing decoupled.\nWe show that in this model, over-smoothing does not occur as in simple message passing\u2075:\nTheorem 3.5. The simple random walk neural network outputs $h^{(l)} \\rightarrow \\bar{x}_\\pi$ as $l \\rightarrow \\infty$.\nEven if the time evolution through $P$ happens fast (i.e. $P$ is rapidly mixing) or with many steps $l$, the\nmodel is resistant to over-smoothing as the input $x$ always affects the output. In fact, if $P$ is rapidly\nmixing, we may expect improved behaviors based on Section 3.1 as the cover times could reduce.\nOn the other hand, we show that over-squashing manifests as probabilistic under-reaching:\nTheorem 3.6. Let $h_u^{(l)}$ be output of the simple random walk neural network queried with $u$. Then:\n$$\\mathbb{E} \\bigg[ \\frac{\\partial h_u^{(l)}}{\\partial x_v} \\bigg] = \\frac{1}{l+1} \\sum_{t=0}^l [P^t]_{uv} \\rightarrow \\pi_v \\ \\text{as} \\ l \\rightarrow \\infty.$$\nThe equation shows that the feature Jacobians are bounded by the sum of powers of $P$, same as in\nsimple message passing. Both models are subject to over-squashing phenomenon that is similarly\nformalized, but manifests through different mechanisms. While in message passing the term is related\nto over-compression of features at bottlenecks [106], in random walk neural networks it is related to\nexponentially decaying probability of reaching a distant vertex $v$, i.e., probabilistic under-reaching.\nIn message passing, it is understood that the topology of the input graph inevitably induces a trade-off\nbetween over-smoothing and over-squashing [85, 48]. Our results suggest that random walk neural\nnetworks avoid the trade-off, and we can focus on overcoming under-reaching e.g. by accelerating\nthe walk, while not worrying much about over-smoothing. Our design choices such as degree-biasing\n(Equation (6)) and non-backtracking [5] can be understood as achieving this."}, {"title": "4 Related Work", "content": "Random walks on graphs Our work builds upon theory of random walks on graphs, i.e. Markov\nchains on discrete spaces. Their statistical properties such as hitting and mixing times have been\nwell-studied [4, 74, 24, 41, 86, 93], and our method (Section 2) is related to cover time [4, 62, 33, 2],\nedge cover time [122, 15, 89, 45], and improving them, using local degree information [59, 3, 29],\nnon-backtracking [5, 64, 7, 39], or restarts in case of infinite graphs [35, 79, 61]. Random walks\nand their statistical properties are closely related to structural properties of graphs, such as effective\nresistance [34, 17], Laplacian eigen-spectrum [74, 102], and discrete Ricci curvatures [87, 32]. This\nhas motivated our analysis in Section 3.2, where we transfer the prior results on over-smoothing and\nover-squashing of message passing based on these properties [9, 106, 47, 48, 11, 85, 91] into the\nresults on our approach. Our work is also inspired by graph algorithms based on random walks, such\nas anonymous observation [81], sublinear algorithms [28, 23, 10], and personalized PageRank for\nsearch [88]. While we adopt their techniques to make our walks and their records well-behaved, the\ndifference is that we use a deep neural network to process the records and directly make predictions.\nRandom walks and graph learning In graph learning, random walks have received interest due to\ntheir scalability and compatibility to sequence learning methods. DeepWalk [94] and node2vec [50]\nuse shallow skip-gram models on random walks to learn vertex embeddings. While CRaW1 [105] is\nthe most relevant to our work, it uses convolutions on walks recorded in a different way from ours.\nHence, its expressive power is controlled by the kernel size and does not always support universality.\nAgentNet [78] learns agents that walk on a graph while recurrently updating their features. While\noptimizing the walk strategy, this method may trade off efficiency as training requires recurrent\nroll-out of the entire network. Our method allows pairing simple and fast walkers with parallelizable\nnetworks such as transformers, which suffices for universality. WalkLM [104] proposed a fine-tuning\nmethod for language models on random walks on text-attributed graphs. While in a similar spirit, our\nresult provides more principles as well as extending to purely topological graphs and training-free\nsetups. Our method is also related to label propagation algorithms [120, 121, 49] that perform\ntransductive learning on graphs based on random walks, which we further discuss in Section 5.3.\nProbabilistic invariant neural networks Whenever a learning problem is compatible with sym-\nmetry, incorporating the associated invariance structure to the hypothesis class often leads to general-\nization benefit [13, 36, 37]. This is also the case for probabilistic invariant neural networks [76, 12],\nwhich includes our approach. Probabilistic invariant networks have recently gained interest due to\ntheir potential of achieving higher expressive powers compared to deterministic counterparts [26, 101].\nIn graph learning, this is often achieved with stochastic symmetry breaking between vertices using\nrandomized features [73, 95, 1, 65], vertex orderings [84, 66], or dropout [90]. Our approach can be\nunderstood as using random walk as a symmetry-breaking mechanism for probabilistic invariance,\nwhich provides an additional benefit of natural compatibility with sequence learning methods.\nLanguage models on graphs While not based on random walks, there have been prior attempts\non applying language models for problems on graphs [108, 20, 117, 40, 113], often focusing on\nprompting methods on problems involving simulations of graph algorithms. We take a more principle-\noriented approach based on invariance and expressive power, and thus demonstrate our approach\nmainly on the related tasks, e.g. graph separation. We believe extending our work to simulating graph\nalgorithms requires a careful treatment [109, 31, 30, 98] and plan to investigate it as future work."}, {"title": "5 Experiments", "content": "We perform a series of experiments to demonstrate random walk neural networks. We implement\nrandom walk algorithms in C++ based on Chenebaux (2020) [22], which produces good throughput\neven without GPU acceleration [105]. Likewise, we implement recording functions in C++ which\nperform anonymization with named neighborhoods and produce plain text. Pseudocode of our\nimplementation is given in Appendix A.2. We choose reader neural networks as pre-trained language\nmodels, with a prioritization on long inputs in accordance to Section 3.1. For fine-tuning experiments,\nwe use DeBERTa-base [52] that supports up to 12,276 tokens, and for training-free experiments, we\nuse instruction-tuned Llama 3 [80] that support up to 8,192 tokens. We implement our pipelines in\nPyTorch [92] with libraries [42, 38, 110, 70] that support multi-GPU training and inference."}, {"title": "5.1 Graph Separation", "content": "We first verify the claims on expressive power in Section 3.1 using three synthetic datasets where\nthe task is recognizing the isomorphism type of an input graph. This is framed as fitting an N-way\nclassification problem on N non-isomorphic graphs. The graphs are chosen such that a certain WL\ngraph isomorphism test fails"}, {"title": "Revisiting Random Walks for Learning on Graphs", "authors": ["Jinwoo Kim", "Olga Zaghen", "Ayhan Suleymanzade", "Youngmin Ryou", "Seunghoon Hong"], "abstract": "We revisit a simple idea for machine learning on graphs, where a random walk\non a graph produces a machine-readable record, and this record is processed by a\ndeep neural network to directly make vertex-level or graph-level predictions. We\nrefer to these stochastic machines as random walk neural networks, and show\nthat we can design them to be isomorphism invariant while capable of universal\napproximation of graph functions in probability. A useful finding is that almost any\nkind of record of random walk guarantees probabilistic invariance as long as the\nvertices are anonymized. This enables us to record random walks in plain text and\nadopt a language model to read these text records to solve graph tasks. We further\nestablish a parallelism to message passing neural networks using tools from Markov\nchain theory, and show that over-smoothing in message passing is alleviated by\nconstruction in random walk neural networks, while over-squashing manifests as\nprobabilistic under-reaching. We show that random walk neural networks based on\npre-trained language models can solve several hard problems on graphs, such as\nseparating strongly regular graphs where the 3-WL test fails, counting substructures,\nand transductive classification on arXiv citation network without training. Code is\navailable at https://github.com/jw9730/random-walk.", "sections": [{"title": "1 Introduction", "content": "The most widely used class of neural networks on graphs are message passing neural networks where\neach vertex keeps a feature vector and updates it by propagating and aggregating messages over\nneighbors [67, 51, 46]. These networks benefit in efficiency by respecting the natural symmetries of\ngraph functions, namely invariance to graph isomorphism [21]. On the other hand, message passing\ncan be viewed as implementing color refinement iterations of the one-dimensional Weisfeiler-Lehman\n(1-WL) graph isomorphism test, and consequently their expressive power is not stronger [112, 83].\nAlso, the inner working of these networks is tied to the topology of the input graph, which is related\nto over-smoothing, over-squashing, and under-reaching of features under mixing [106, 48, 85].\nIn this work, we (re)visit a simple alternative idea for learning on graphs, where a random walk on a\ngraph produces a machine-readable record, and this record is processed by a deep neural network that\ndirectly makes vertex-level or graph-level predictions. We call these stochastic machines random\nwalk neural networks. Using random walks for graph learning has received practical interest due\nto their scalability and compatibility with sequence learning methods. This has led to empirically\nsuccessful algorithms such as DeepWalk [94] and node2vec [50] which use skip-gram models on\nrandom walks and CRaWl [105] which uses convolutions on random walks. However, principled\nunderstanding and design of random walk neural networks have been studied relatively less.\nWe study the fundamental properties of random walk neural networks, and show that they can be\ndesigned from principle to be isomorphism invariant while capable of universal approximation of"}, {"title": "2 Random Walk Neural Networks", "content": "We define a random walk neural network as a randomized function $\\mathcal{X}_{\\Theta}(\\cdot)$ that takes a graph $G$ as\ninput and outputs a random variable $X_{\\Theta}(G)$ on the output space $\\mathbb{R}^d$. It can be optionally queried\nwith an input vertex $v$ which gives a random variable $X_{\\Theta}(G, v)$. It consists of the following:\n1. Random walk algorithm that produces $l$ steps of vertex transitions $v_0 \\rightarrow \\cdots \\rightarrow v_l$ on the input\ngraph $G$. If an input vertex $v$ is given, we fix the starting vertex by $v_0 = v$.\n2. Recording function $q : (v_0 \\rightarrow \\cdots \\rightarrow v_l, G) \\rightarrow z$ that produces a machine-readable record $z$ of\nthe random walk. It may access the graph $G$ to record auxiliary information such as attributes.\n3. Reader neural network $f_{\\Theta} : z \\rightarrow \\hat{y}$ that processes record $z$ and outputs a prediction $\\hat{y}$ in $\\mathbb{R}^d$. It\nis the only trainable component and is not restricted to specific architectures.\nTo perform graph-level tasks, we sample a prediction $\\hat{y} \\sim X_{\\Theta}(G)$ by running a random walk on $G$\nand processing its record using the reader neural network. For vertex-level predictions $\\hat{y} \\sim X_{\\Theta}(G, v)$,\nwe query the input vertex $v$ by simply starting the random walk from it.\nIn the sections below, we discuss the design of each component for graph-level tasks on finite graphs\nand then vertex-level tasks on possibly infinite graphs. The latter simulates learning problems on\nlarge graphs, such as transductive classification, and forces the model to operate independently of the\nsize of the whole graph $G$ [81]. Pseudocode and proofs can be found in Appendix A.2 and A.4."}, {"title": "2.1 Graph-Level Tasks", "content": "Let $\\mathcal{G}$ be the class of undirected, connected, and simple graphs\u00b2. Let $n \\geq 1$ and $\\mathcal{G}_n$ be the collection\nof graphs in $\\mathcal{G}$ with at most $n$ vertices. We would like to model a graph-level function $\\phi : \\mathcal{G}_n \\rightarrow \\mathbb{R}^d$\nusing a random walk neural network $X_{\\Theta}(\\cdot)$.\nSince $\\phi$ is a function on graphs, it is reasonable to assume that it is isomorphism invariant:\n$$\\phi(G) = \\phi(H), \\ \\forall G \\sim H.$$\nIncorporating the invariance structure to our model class would offer generalization benefit [76, 13].\nSince $X_{\\Theta}(\\cdot)$ is a randomized function, we accept the probabilistic notion of invariance [63, 12, 26, 25]:\n$$X_{\\Theta}(G) \\stackrel{d}{=} X_{\\Theta}(H), \\ \\forall G \\sim H.$$\nWe now claim that we can achieve probabilistic invariance by properly choosing the random walk\nalgorithm and recording function while not imposing any constraint on the reader neural network.\nTheorem 2.1. $X_{\\Theta}(\\cdot)$ is invariant if its random walk algorithm and recording function are invariant.\nBelow, we describe the choice of random walk algorithm, recording function, and neural networks\nthat comprise random walk neural networks as in Theorem 2.1. Pseudocode is given in Appendix A.2.\nRandom walk algorithm A random walk algorithm is invariant if it satisfies the following:\n$$\\pi(v_0) \\rightarrow \\cdots \\rightarrow \\pi(v_l) \\stackrel{d}{=} u_0 \\rightarrow \\cdots \\rightarrow u_l, \\ \\forall G \\sim H,$$\nwhere $v[\\cdot]$ is a random walk on $G$, $u[\\cdot]$ is a random walk on $H$, and $\\pi : V(G) \\rightarrow V(H)$ specifies the\nisomorphism from $G$ to $H$. It turns out that many random walk algorithms in literature are already\ninvariant. To see this, let us write the probability of walking from a vertex $u$ to its neighbor $x \\in N(u)$:\n$$\\text{Prob}[v_t = x / v_{t-1} = u] := \\frac{c_G(u, x)}{\\sum_{y \\in N(u)} c_G(u, y)},$$\nwhere the function $c_g : E(G) \\rightarrow \\mathbb{R}_+$ assigns positive weights called conductance to edges. If we use\nconstant conductance $c_g(\\cdot) = 1$, we have unbiased random walk used in DeepWalk [94]. We show:\nTheorem 2.2. The random walk in Equation (4) is invariant if its conductance $c[\\cdot](\\cdot)$ is invariant:\n$$c_G(u,v) = c_H(\\pi(u), \\pi(v)), \\ \\forall G \\sim H.$$\nIt includes constant conductance, and any choice that only uses degrees of endpoints $\\text{deg}(u), \\text{deg}(v)$.\nWe favor using the degree information since it is cheap while potentially improving the behaviors\nof walks. Among many instantiations, we find that the following conductance function called the\nminimum degree local rule [3, 29] is particularly useful:\n$$c_G(u, v) := \\frac{1}{\\text{min}[\\text{deg}(u), \\text{deg}(v)]}.$$This conductance is special because it achieves $O(n^2)$ cover time, i.e. the expected time of visiting all\n$n$ vertices of a graph, optimal among random walks in Equation (4) that use degrees of endpoints [29].\nIn practice, we find that adding non-backtracking property that enforces $v_{t+1} \\neq v_{t-1}$ is helpful while\nnot adding much cost. Formally, we can extend Equation (4) and Theorem 2.2 to second-order random\nw"}, {"title": "Recording function", "content": "A recording function $q: (v_0 \\rightarrow \\cdot \\rightarrow v_l, G) \\rightarrow z$ takes a random walk and\nproduces a machine-readable record $z$. We let $q(\\cdot, G)$ have access to the graph $G$ the walk is taking\nplace. This allows recording auxiliary information such as vertex or edge attributes. A recording\nfunction is invariant if it satisfies the following for any given random walk $v[\\cdot]$ on $G$:\n$$q(v_0 \\rightarrow \\cdots \\rightarrow v_l, G) = q(\\pi(v_0) \\rightarrow \\cdots \\rightarrow \\pi(v_l), H), \\ \\forall G \\sim H.$$\nInvariance requires that $q(\\cdot, G)$ produces the same record $z$ regardless of re-indexing of vertices of $G$\ninto $H$. For this, we have to be careful in how we represent each vertex in a walk $v_0 \\rightarrow \\cdot \\rightarrow v_l$ as a\nmachine-readable value, and which auxiliary information we record from $G$. We make two choices:\n\u2022 Anonymization. We name each vertex in a walk with a unique integer, starting from $v_0 \\rightarrow 1$ and\nincrementing it based on their order of discovery. For instance, a random walk $a \\rightarrow b \\rightarrow c \\rightarrow a$\ntranslates to a sequence $1\\rightarrow2\\rightarrow3\\rightarrow1$.\n\u2022 Anonymization + named neighborhoods. While applying anonymization for each vertex $v$ in\na walk, we record its neighbors $u \\in N(v)$ if they are already named but the edge $v \\rightarrow u$ has not\nbeen recorded yet. For instance, a walk $a \\rightarrow b \\rightarrow c \\rightarrow d$ on a fully-connected graph translates\nto a sequence $1 \\rightarrow 2 \\rightarrow 3(1) \\rightarrow 4(1, 2)$, where $(\\cdot)$ represents the named neighborhoods.\nThe pseudocode of both algorithms can be found in Appendix A.2. We now show the following:\nTheorem 2.3. A recording function $q : (v_0 \\rightarrow \\cdots \\rightarrow v_l, G) \\rightarrow z$ that uses anonymization, optionally\nwith named neighborhoods, is invariant.\nOur choice of anonymizing a walk is inspired by the observation model of Micali and Zhu (2015) [81].\nWhile initially motivated by privacy concerns [81, 60], we find it useful for invariance. Recording\nnamed neighborhoods is inspired by sublinear algorithms that probe previously discovered neighbors\nduring a walk [28, 23, 10]. In our context, it is useful because whenever a walk visits a set of vertices\n$S \\subseteq V(G)$ it automatically records the entire induced subgraph $G[S]$. As a result, to record all edges\nof a graph, a walk only has to visit all vertices. It is then possible to choose a random walk algorithm\nthat takes only $O(n^2)$ time to visit all $n$ vertices [3, 29], while traversing all edges, i.e. edge cover\ntime, is $O(n^3)$ in general [122, 89]. The degree-biasing in Equation (6) precisely achieves this.\nReader neural network A reader neural network $f_{\\Theta}: z \\rightarrow \\hat{y}$ processes the record $z$ of the random\nwalk and outputs a prediction $\\hat{y}$ in $\\mathbb{R}^d$. As in Theorem 2.1, there is no invariance constraint imposed\non $f_{\\Theta}$, and any neural network that accepts the recorded walks and has a sufficient expressive power\ncan be used. This is in contrast to message passing where invariance is hard-coded in feature mixing\noperations. Furthermore, our record $z$ can take any format, such as a matrix, byte sequence, or plain\ntext, as long as $f_{\\Theta}$ accepts it. It is appealing in this regard to choose the record to be plain text, such\nas \"1-2-3-1\", and choose $f_{\\Theta}$ to be a pre-trained transformer language model. This offers expressive\npower [114] as well as transferable representation from language domain [75, 97]. In our experiments,\nwe test both \"encoder-only\" DeBERTa [52] and \"decoder-only\" Llama [107] language models as $f_{\\Theta}$."}, {"title": "2.2 Vertex-Level Tasks", "content": "We now consider vertex-level tasks. In case of finite graphs, we may simply frame a vertex-level task\n$(G, v) \\mapsto y$ as a graph-level task $G' \\mapsto y$ where $G'$ is $G$ with its vertex $v$ marked. Then, we can\nsolve $G' \\mapsto y$ by querying a random walk neural network $X_{\\Theta}(G, v)$ to start its walk at $v_0 = v$.\nA more interesting case is when $G$ is an infinite graph that is only locally finite, i.e., has finite degrees.\nThis simulates learning problems on a large graph such as transductive classification. In this case, it\nis reasonable to assume that our target function depends on finite local structures [103, 81].\nLet $r \\geq 1$, and $\\mathcal{B}_r := {\\mathcal{B}_r(v)}$ be the collection of local balls in $G$ of radius $r$ centered at $v \\in V(G)$.\nWe would like to model a vertex-level function $\\phi : \\mathcal{B}_r \\rightarrow \\mathbb{R}^d$ on $G$ using a random walk neural\nnetwork $X_{\\Theta}(\\cdot)$ by querying the vertex of interest, $X_{\\Theta}(G, v)$. We assume $\\phi$ is isomorphism invariant:\n$$\\phi(\\mathcal{B}_r(v)) = \\phi(\\mathcal{B}_r(u)), \\ \\forall \\mathcal{B}_r(v) \\approx \\mathcal{B}_r(u).$$\nThe invariance of $X_{\\Theta}(\\cdot)$ in probability is defined as following:\n$$X_{\\Theta}(G,v) \\stackrel{d}{=} X_{\\Theta}(G, u), \\ \\forall \\mathcal{B}_r(v) \\approx \\mathcal{B}_r(u).$$\nWe may achieve invariance by choosing the random walk algorithm and recording function similar to\nthe graph-level case. As a modification, we query $X_{\\Theta}(G, v)$ with the starting vertex $v_0 = v$ of the\nrandom walk. Anonymization informs $v$ to the reader neural network by always naming it as 1.\nThen, we make an important choice of localizing random walks with restarts. That is, we reset a walk\nto the starting vertex $v_0$ either with a probability $\\alpha \\in (0, 1)$ at each step or periodically every $k$ steps.\nA restarting random walk tends to stay more around its starting vertex $v_0$, and was used to implement\nlocality bias in personalized PageRank algorithm for search [88, 44]. This localizing effect is crucial\nin our context since a walk may drift away from $\\mathcal{B}_r(v_0)$ before recording all necessary information,"}, {"title": "3 Analysis", "content": "In Section 2, we have described the design of random walk neural networks, primarily relying on the\nprinciple of invariance. In this section we provide in-depth analysis on their expressive power and\nrelations to issues in message passing such as over-smoothing, over-squashing, and under-reaching."}, {"title": "3.1 Expressive Power", "content": "Intuitively, if the records of random walks contain enough information such that the structures of\ninterest, e.g. graph $G$ or local ball $\\mathcal{B}_r(v)$, can be fully recovered, a powerful reader neural network\nsuch as a multi-layer perceptron [27, 55] or a transformer [114] on these records would be able to\napproximate any function of interest. Our following analysis shows this.\nWe first consider using random walk neural network $X_{\\Theta}(\\cdot)$ to universally approximate graph-level\nfunctions $\\phi(\\cdot)$ in probability, as defined in Abboud et al. (2021) [1]. While we consider scalar-valued\nfunctions as often done in universality proofs, extension to vector-valued functions can be done easily.\nDefinition 3.1. $X_{\\Theta}(\\cdot)$ is a universal approximator of graph-level functions in probability if, for all\ninvariant functions $\\phi : \\mathcal{G}_n \\rightarrow \\mathbb{R}$ for a given $n \\geq 1$, and $\\forall \\epsilon, \\delta > 0$, there exist choices of length $l$ of\nthe random walk and network parameters $\\Theta$ such that the following holds:\n$$\\text{Prob}[|\\phi(G) - X_{\\Theta}(G)| < \\epsilon] > 1 - \\delta, \\ \\forall G \\in \\mathcal{G}_n.$$\nWe show that, if the random walk is long enough and the reader neural network $f_{\\Theta}$ is powerful\nenough, a random walk neural network $X_{\\Theta}(\\cdot)$ is capable of graph-level universal approximation in\nprobability. In particular, we show that the length of the random walk $l$ controls the confidence $1 - \\delta$\nof the approximation, with cover time $C_V(G)$ or edge cover time $C_E(G)$ playing a central role.\nTheorem 3.2. A random walk neural network $X_{\\Theta}(\\cdot)$ with a sufficiently powerful $f_{\\Theta}$ is a universal\napproximator of graph-level functions in probability (Definition 3.1) if it satisfies either of the below:\n\u2022 It uses anonymization to record random walks of lengths $l > C_E(G)/\\delta$.\n\u2022 It uses anonymization and named neighborhoods to record walks of lengths $l > C_V(G)/\\delta$.\nWe remark that, while the cover time and edge cover time are $O(n^3)$ for uniform random walks [4,\n122, 24, 41, 15, 2, 89, 45], we make use of minimum degree local rule in Equation (6) to achieve a\ncover time of $O(n^2)$ [3, 29], in conjunction with named neighborhood recording. While universal\napproximation can be in principle achieved with simple uniform random walk, our design reduces\nthe length $l$ required for the desired reliability $> 1 - \\delta$, and hence improves learning and inference.\nWe now show an analogous result for universal approximation of vertex-level functions. Since our\ntarget function $\\phi$ is on a collection $\\mathcal{B}_r$ of finite graphs, the proof is similar to the graph-level case.\nDefinition 3.3. $X_{\\Theta}(\\cdot)$ is a universal approximator of vertex-level functions in probability if, for all\ninvariant functions $\\phi : \\mathcal{B}_r \\rightarrow \\mathbb{R}$ for a given $r \\geq 1$, and $\\forall \\epsilon, \\delta > 0$, there exist choices of length $l$ and\nrestart probability $\\alpha$ or period $k$ of the random walk and network parameters $\\Theta$ such that:\n$$\\text{Prob}[|\\phi(\\mathcal{B}_r(v)) - X_{\\Theta}(G, v)| < \\epsilon] > 1 - \\delta, \\ \\forall \\mathcal{B}_r(v) \\in \\mathcal{B}_r.$$\nWe show the following, which relates to local cover time $C_V(\\mathcal{B}_r(v))$ and edge cover time $C_E(\\mathcal{B}_r(v))$:\nTheorem 3.4. A random walk neural network $X_{\\Theta}(\\cdot)$ with a sufficiently powerful $f_{\\Theta}$ and any nonzero\nrestart probability $\\alpha$ or restart period $k > r + 1$ is a universal approximator of vertex-level functions\nin probability (Definition 3.3) if it satisfies either of the below for all $\\mathcal{B}_r(v) \\in \\mathcal{B}_r$:"}, {"title": "3.2 Over-smoothing, Over-squashing, and Under-reaching", "content": "In message passing neural networks, the model operates by passing features over edges and mixing\nthem, and thus their operation is tied to the topology of the input graph. It is now understood how this\nrelates to the issues of over-smoothing, over-squashing, and under-reaching [9, 106, 47, 48, 85, 111].\nWe connect these theories and random walk neural networks to verify if similar issues may take place.\nLet $G$ be a connected non-bipartite graph with row-normalized adjacency matrix $P$. We consider a\nsimplified message passing model, where the vertex features $h^{(0)}$ are initialized as some probability\nvector $x$, and updated by $h^{(t+1)} = h^{(t)} P$. This simple model is often used as a proxy to study the\naforementioned issues [48, 118]. For example, over-smoothing happens as the features exponentially\nconverge to a stationary vector $h^{(l)} \\rightarrow \\pi$ as $l \\rightarrow \\infty$, smoothing out the input $x$ [48]. Over-squashing\nand under-reaching occur when a feature $h_v^{(l)}$ becomes insensitive to distant input $x$. While under-\nreaching refers to insufficient depth $l < \\text{diam}(G)$ [9, 106], over-squashing refers to features getting\noverly compressed at bottlenecks of $G$, even with sufficient depth $l$. The latter is described by the\nJacobian $\\frac{\\partial h^{(l)}}{\\partial x} < [\\sum_{t=0}^l P^t]_{uv}$, as the bound often decays exponentially with $l$ [106, 11].\nWhat do these results tell us about random walk neural networks? We can see that, while $P$ drives\nfeature mixing in the message passing schema, it can be also interpreted as the transition probability\nmatrix of uniform random walk where $P_{uv}$ is the probability of walking from $u$ to $v$. This parallelism\nmotivates us to design an analogous, simplified random walk neural network and study its behavior.\nWe consider a simple random walk neural network that runs a uniform random walk $v_0 \\rightarrow \\cdot \\rightarrow v_l$,\nreads the record $x_{v_0} \\rightarrow \\cdot \\rightarrow x_{v_l}$ by averaging, and outputs it as $h^{(l)}$. Like simple message passing,\nthe model involves $l$ steps of time evolution through $P$. However, while message passing uses $P$ to\nprocess features, this model uses $P$ only to obtain a record of input, with feature processing decoupled.\nWe show that in this model, over-smoothing does not occur as in simple message passing\u2075:\nTheorem 3.5. The simple random walk neural network outputs $h^{(l)} \\rightarrow \\bar{x}_\\pi$ as $l \\rightarrow \\infty$.\nEven if the time evolution through $P$ happens fast (i.e. $P$ is rapidly mixing) or with many steps $l$, the\nmodel is resistant to over-smoothing as the input $x$ always affects the output. In fact, if $P$ is rapidly\nmixing, we may expect improved behaviors based on Section 3.1 as the cover times could reduce.\nOn the other hand, we show that over-squashing manifests as probabilistic under-reaching:\nTheorem 3.6. Let $h_u^{(l)}$ be output of the simple random walk neural network queried with $u$. Then:\n$$\\mathbb{E} \\bigg[ \\frac{\\partial h_u^{(l)}}{\\partial x_v} \\bigg] = \\frac{1}{l+1} \\sum_{t=0}^l [P^t]_{uv} \\rightarrow \\pi_v \\ \\text{as} \\ l \\rightarrow \\infty.$$\nThe equation shows that the feature Jacobians are bounded by the sum of powers of $P$, same as in\nsimple message passing. Both models are subject to over-squashing phenomenon that is similarly\nformalized, but manifests through different mechanisms. While in message passing the term is related\nto over-compression of features at bottlenecks [106], in random walk neural networks it is related to\nexponentially decaying probability of reaching a distant vertex $v$, i.e., probabilistic under-reaching.\nIn message passing, it is understood that the topology of the input graph inevitably induces a trade-off\nbetween over-smoothing and over-squashing [85, 48]. Our results suggest that random walk neural\nnetworks avoid the trade-off, and we can focus on overcoming under-reaching e.g. by accelerating\nthe walk, while not worrying much about over-smoothing. Our design choices such as degree-biasing\n(Equation (6)) and non-backtracking [5] can be understood as achieving this."}, {"title": "4 Related Work", "content": "Random walks on graphs Our work builds upon theory of random walks on graphs, i.e. Markov\nchains on discrete spaces. Their statistical properties such as hitting and mixing times have been\nwell-studied [4, 74, 24, 41, 86, 93], and our method (Section 2) is related to cover time [4, 62, 33, 2],\nedge cover time [122, 15, 89, 45], and improving them, using local degree information [59, 3, 29],\nnon-backtracking [5, 64, 7, 39], or restarts in case of infinite graphs [35, 79, 61]. Random walks\nand their statistical properties are closely related to structural properties of graphs, such as effective\nresistance [34, 17], Laplacian eigen-spectrum [74, 102], and discrete Ricci curvatures [87, 32]. This\nhas motivated our analysis in Section 3.2, where we transfer the prior results on over-smoothing and\nover-squashing of message passing based on these properties [9, 106, 47, 48, 11, 85, 91] into the\nresults on our approach. Our work is also inspired by graph algorithms based on random walks, such\nas anonymous observation [81], sublinear algorithms [28, 23, 10], and personalized PageRank for\nsearch [88]. While we adopt their techniques to make our walks and their records well-behaved, the\ndifference is that we use a deep neural network to process the records and directly make predictions.\nRandom walks and graph learning In graph learning, random walks have received interest due to\ntheir scalability and compatibility to sequence learning methods. DeepWalk [94] and node2vec [50]\nuse shallow skip-gram models on random walks to learn vertex embeddings. While CRaW1 [105] is\nthe most relevant to our work, it uses convolutions on walks recorded in a different way from ours.\nHence, its expressive power is controlled by the kernel size and does not always support universality.\nAgentNet [78] learns agents that walk on a graph while recurrently updating their features. While\noptimizing the walk strategy, this method may trade off efficiency as training requires recurrent\nroll-out of the entire network. Our method allows pairing simple and fast walkers with parallelizable\nnetworks such as transformers, which suffices for universality. WalkLM [104] proposed a fine-tuning\nmethod for language models on random walks on text-attributed graphs. While in a similar spirit, our\nresult provides more principles as well as extending to purely topological graphs and training-free\nsetups. Our method is also related to label propagation algorithms [120, 121, 49] that perform\ntransductive learning on graphs based on random walks, which we further discuss in Section 5.3.\nProbabilistic invariant neural networks Whenever a learning problem is compatible with sym-\nmetry, incorporating the associated invariance structure to the hypothesis class often leads to general-\nization benefit [13, 36, 37]. This is also the case for probabilistic invariant neural networks [76, 12],\nwhich includes our approach. Probabilistic invariant networks have recently gained interest due to\ntheir potential of achieving higher expressive powers compared to deterministic counterparts [26, 101].\nIn graph learning, this is often achieved with stochastic symmetry breaking between vertices using\nrandomized features [73, 95, 1, 65], vertex orderings [84, 66], or dropout [90]. Our approach can be\nunderstood as using random walk as a symmetry-breaking mechanism for probabilistic invariance,\nwhich provides an additional benefit of natural compatibility with sequence learning methods.\nLanguage models on graphs While not based on random walks, there have been prior attempts\non applying language models for problems on graphs [108, 20, 117, 40, 113], often focusing on\nprompting methods on problems involving simulations of graph algorithms. We take a more principle-\noriented approach based on invariance and expressive power, and thus demonstrate our approach\nmainly on the related tasks, e.g. graph separation. We believe extending our work to simulating graph\nalgorithms requires a careful treatment [109, 31, 30, 98] and plan to investigate it as future work."}, {"title": "5 Experiments", "content": "We perform a series of experiments to demonstrate random walk neural networks. We implement\nrandom walk algorithms in C++ based on Chenebaux (2020) [22], which produces good throughput\neven without GPU acceleration [105]. Likewise, we implement recording functions in C++ which\nperform anonymization with named neighborhoods and produce plain text. Pseudocode of our\nimplementation is given in Appendix A.2. We choose reader neural networks as pre-trained language\nmodels, with a prioritization on long inputs in accordance to Section 3.1. For fine-tuning experiments,\nwe use DeBERTa-base [52] that supports up to 12,276 tokens, and for training-free experiments, we\nuse instruction-tuned Llama 3 [80] that support up to 8,192 tokens. We implement our pipelines in\nPyTorch [92] with libraries [42, 38, 110, 70] that support multi-GPU training and inference."}, {"title": "5.1 Graph Separation", "content": "We first verify the claims on expressive power in Section 3.1 using three synthetic datasets where\nthe task is recognizing the isomorphism type of an input graph. This is framed as fitting an N-way\nclassification problem on N non-isomorphic graphs. The graphs are chosen such that a certain WL\ngraph isomorphism test fails, and so does their related family of message passing neural networks.\nThe Circular Skip Links (CSL) graphs dataset [84", "21": "nand 1-WL and 2-WL tests fail. The 4 \u00d7 4 rook's and Shrikhande graphs [6", "78": "and 3-WL test fails [6", "8": "contains 15 strongly regular graphs\nwith 25 vertices of degree 12, on which 3-WL test fails [6", "6": ".", "72": "with 2e-5 learning rate and 0.01 weight decay on on 8\u00d7 RTX 3090 GPUs. While the\nmodel supports 12,276 tokens in maximum, we use 512 tokens which is the maximum allowed by\nmemory constraint. We use batch size 256, and accumulate gradients for 8 steps for SR25. At test\ntime, we ensemble 4 stochastic predictions of the network by averaging classification logits.\nThe results are in Table 1. Message passing neural networks that align with certain WL test fail when\nasked to solve harder problems, e.g. GIN aligns with 1-WL and fails in CSL. Recent algorithms\nthat use subgraphs and structural features show improved performance, with state-of-the-art such\nas ELENE-L (ED) solving SR25. Alternative approaches based on stochastic symmetry-breaking\ne.g. random node identification, often fail on CSL although universal approximation is possible in\ntheory [1"}]}]}