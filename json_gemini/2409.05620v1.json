{"title": "Joint Input and Output Coordination for Class-Incremental Learning", "authors": ["Shuai Wang", "Yibing Zhan", "Yong Luo", "Han Hu", "Wei Yu", "Yonggang Wen", "Dacheng Tao"], "abstract": "Incremental learning is nontrivial due to severe catastrophic forgetting. Although storing a small amount of data on old tasks during incremental learning is a feasible solution, current strategies still do not 1) adequately address the class bias problem, and 2) alleviate the mutual interference between new and old tasks, and 3) consider the problem of class bias within tasks. This motivates us to propose a joint input and output coordination (JIOC) mechanism to address these issues. This mechanism assigns different weights to different categories of data according to the gradient of the output score, and uses knowledge distillation (KD) to reduce the mutual interference between the outputs of old and new tasks. The proposed mechanism is general and flexible, and can be incorporated into different incremental learning approaches that use memory storage. Extensive experiments show that our mechanism can significantly improve their performance.", "sections": [{"title": "Introduction", "content": "In recent years, incremental learning has attracted much attention since it can play an important role in a wide variety of fields, including unmanned driving [Santoso and Finn, 2022] and human-computer interaction [Tschandl et al., 2020]. Incremental learning is nontrivial since the parameters of deep models in the old tasks are often destroyed in the process of learning new tasks. This leads to the occurrence of catastrophic forgetting [French and Chater, 2002]. How to well preserve past information and fully explore new knowledge has become a major challenge of incremental learning.\nExisting incremental learning approaches mainly focus on memory storage replay [Ahn et al., 2021; Li and Hoiem, 2017; Wu et al., 2019; Rebuffi et al., 2017; Yan et al., 2021], model dynamic expansion [Serra et al., 2018; Mallya and Lazebnik, 2018], and regularization constraints design [Aljundi et al., 2019]. Memory store replay has been demonstrated to be very effective, and it alleviates the destruction of old task weights by storing past data or simulating human memory. However, due to the privacy restriction and limited memory, the data to be accessed from old tasks are often quite scarce. This makes incremental learning models suffer from severe inter-task class bias, or known as the class imbalance issue between old and new tasks.\nThere exist some recent approaches [Ahn et al., 2021; Rebuffi et al., 2017; Yan et al., 2021] that alleviate the problem of class imbalance between old and new tasks by utilizing rescaling, balanced scoring, or softmax separating. Although these approaches can improve the performance to some extent, the problem of category imbalance still exists, since during the incremental learning progresses, the category imbalance becomes more severe as the number of sample categories continuously increase. Moreover, the mutual interference between old and new tasks has not been well addressed. That is, only the predictions in old tasks are tried to be maintained, and the output scores of old task data on the classification heads of new tasks are not well suppressed. The output consistency of new task data on old classification heads before and after updating the new task model is also not considered. Besides, none of the existing approaches deal with the class bias within tasks."}, {"title": "Related Work", "content": "Incremental learning [De Lange et al., 2021] has received extensive attention in recent decades. In incremental learning, input data in new tasks are continuously used to extend the knowledge of existing models. This makes incremental learning manifest as a dynamic learning technique. An incremental learning model can be defined as one that meets the following conditions: (1) The model can learn useful knowledge from new task data; (2) The old task data that has been used to train the model does not need to be accessed or has a small amount of access; (3) It has a memory function for the knowledge that has been learned. The current study on incremental learning mainly focuses on domain incremental learning [Mirza et al., 2022; Garg et al., 2022; Mallya et al., 2018], class-incremental learning [Ahn et al., 2021; Rebuffi et al., 2017; Yan et al., 2021; Zhang et al., 2020; Liu et al., 2021], and small sample incremental learning [Tao et al., 2020; Cheraghian et al., 2021].\nThere are many works on class-incremental learning (CIL), and most of these works overcome catastrophic forgetting by using knowledge distillation (KD) together with a small amount of old task data accessed. For example, DMC [Zhang et al., 2020] utilizes separate models for the new and old classes and trains the two models by combining double distillation. SPB [Liu et al., 2021] utilizes cosine classifier and reciprocal adaptive weights, and a new method of learning class-independent knowledge and multi-view knowledge is designed to balance the stability-plasticity dilemma of incremental learning.\nAlthough the above approaches can achieve promising performance sometimes, none of them address class bias within tasks, nor adequately address class bias between old and new tasks. Therefore, we propose joint input and output coordination (JIOC) mechanism that enables incremental learning models to alleviate class imbalance and reduce interference between the predictions of old and new tasks."}, {"title": "Human Inductive Memory", "content": "The inductive memory method is a unique ability of human beings. It causes the memorized content to be induced according to different attributes or categories; Subsequently, these contents are memorized by different categories or attributes. As early as 1999, Williams et al. [Williams, 1999] investigated the relationship between memory for input and inductive learning of morphological rules relating to functional categories in a semi-artificial form of Italian. The ability to perform induction appears in the early age of human, while the underlying mechanisms remain unclear. Therefore, Fisher et al. [Fisher and Sloutsky, 2005] demonstrated that category- and similarity-based induction should result in different memory traces and thus different memory accuracy. Hayes et al. [Hayes et al., 2013] examined the development of the relationship between inductive reasoning and visual recognition memory, and demonstrated it through two studies. Inspired by human inductive memory, Geng et al. [Geng et al., 2020] proposed a Dynamic Memory Induction Network (DMIN) to further address the small-sample challenge. These examples of inductive memory inspire us to propose an output distribution coordination mechanism."}, {"title": "Method", "content": "In CIL, data for new tasks are arriving constantly, which are represented as $\\mathcal{D} = {\\mathcal{D}_1, \\mathcal{D}_2, ..., \\mathcal{D}_t,..., \\mathcal{D}_T }$. The data in the t-th new task is $\\mathcal{D}_t = \\{(x_{i,j}, Y_{i,j}) \\}_{i=1,2,\\dots,m;j=1,2,\\dots,n_m }$, where $m$ is the number of classes, $n_m$ is the number of samples for the $m$-th category, $x$ is the input data, and $y$ is the corresponding data label. The number of samples may vary for different categories in the new task. When learning the $t$-th new task, we assume that there are a small amount of data stored for the old tasks, i.e.,\n$\\mathcal{D}_{old}^{t-1} = \\{(x_{i,j}, y_{i,j})\\}_{i=1,2,\\dots, m^{t-1} , j=1,2,...,n^{old} },\\quad N^{old} \\ll n \\tag{1}$\nwhere $i = 1,2,\\dots, m$ and $j = 1,2,\\dots,n^{old}, N^{old} \\ll n$. That is, the number of old data $\\mathcal{D}_{old}$ in the repository is much smaller than that of $\\mathcal{D}_t$. In CIL, a feature extractor $f(\u00b7)$ (such as ResNet [He et al., 2016]) and a fully connected layer (FCL) together with a softmax classifier is generally adopted, i.e.,\n$\\mathbf{z}_{i,j} = f(x_{i,j}; \\Theta), \\tag{2}$\n$\\mathbf{p}_{i,j} = \\text{softmax}(\\mathbf{z}_{i,j}; W), \\tag{3}$\nwhere $T = \\{1,2,...,t\\}, \\Theta$ is the parameter of the feature extractor, $W$ is the parameter of the classifier, and $\\mathbf{p}$ is a vector of output scores. When incremental learning proceeds to the t-th task, all the data in $\\mathcal{D}_t \\cup \\mathcal{D}_{old} = \\{(\\mathbf{x},y), (t=1, 2, ..., t)\\}$ are utilized for training, and the following cross-entropy loss is usually adopted:\n$\\mathcal{L}_{ce,t} = \\frac{1}{N^{old} + N^{new}}\\sum_{i,j,k,\\tau=1}^t Y_{i,j,k} \\log(P_{i,j,k}), \\tag{4}$\nwhere $N^{old}$ is total number of stored data for old tasks, $N^{new}$ is the total number of samples for the new task, and $P_{j,k}^i$ is the output score at the $k$-th neuron."}, {"title": "Overview", "content": "According to the above problem setup, it can be seen that when performing incremental learning, only a limited number of samples from the old tasks will be retained. Due to the large number of samples in the new task, incremental learning suffers from the class imbalance issue between the old and new tasks. The class imbalance issue also exists within the new task, but this is ignored by existing CIL approaches [Ahn et al., 2021; Rebuffi et al., 2017; Yan et al., 2021].\nTherefore, we propose the joint input and output coordination (JIOC) mechanism, as shown in Figure 2, where we assign different weights to different input data according to their the absolute value of the gradient for output scores. In addition, in order to prevent the mutual interference of output distributions between old and new tasks, we split the softmax layer inspired by the principle of human inductive memory. This is similar to the SSIL [Ahn et al., 2021] approach, but has several significant differences, as shown in Figure 3: 1) for each of the old tasks, we utilize KD to maintain the output distribution of each task. To make the output scores of new task data on the classification heads of old tasks consistent, we also employ KD to enforce the outputs after updating the old task models agree with the scores before the update; 2) to suppress the outputs of old task data on the classification heads of new tasks, their ground-truth target values are directly set to be zero for training."}, {"title": "Input Coordination", "content": "As we know, the class imbalance issue may lead to significant bias in the learned weights of the fully connected layers [Li et al., 2020]. Therefore, $P_{j,k}^i$ may deviate greatly from its corresponding true value $p_{i,j,k}$, and hence it is necessary to balance the weight of fully connected layers between tasks and within tasks.\nDue to the severe bias in the weights of the fully connected layer, we propose to utilize the outputs of fully connected layer's previous layer to adjust the weights. Suppose that $\\mathbf{q}_{j}$ is the vector of the previous layer that outputs scores $p_i$. The derivative of $\\mathcal{L}_{ce,t}$ w.r.t. $\\mathbf{q}_j$ (we refer to the supplementary material for the detailed calculation) can be given by:\n$\\frac{\\partial \\mathcal{L}_{ce,t}}{\\partial \\mathbf{q}_j} =  \\frac{\\partial \\mathcal{L}_{ce,t}}{\\partial \\mathbf{p}_i}  \\frac{\\partial \\mathbf{p}_i}{\\partial \\mathbf{q}_j} = [\\frac{P_{i,1,1} - Y_{i,1,1}}{\\alpha^{i}_{1,1}}\\vdots\\frac{P_{i,j,m^t}-Y_{i,j,m^t}}{\\alpha^{i}_{j,m^t}}]  \\tag{5}$\nThen the absolute value of the gradient of the output score for the input data when $k = i$ is:\n$d_{i,j,k=i} = |P_{i,j,k=i} \u2013 Y_{i,j,k=i}|. \\tag{6}$\nWhen the number of data is large for a certain category, the model tends to bias to this category and thus the absolute value of the gradient in Eq. (6) tends to be small in the learning process. To alleviate the bias issue, we propose to regard the absolute value as the weight for the corresponding input sample and add it into the loss during the training. That is, smaller weights will be adaptively assigned to the samples of the category that has more input data, and hence the model would focus more on the category that has fewer samples.\nBased on the above analysis, we utilize the absolute values $\\delta$ of the gradient to induce a weight for each input data during the training. First of all, we incorporate the absolute value of the gradient of the input data into the traditional cross-entropy loss (Eq. (4)), i.e.,\n$\\mathcal{L}_{IC,t} = \\frac{1}{N^{old} + N^{new}} \\sum\\delta^{t}_{i,j,k} \\log(P_{i,j,k}). \\tag{7}$\nThen, we can use Eq. (7) to balance the loss of each category. In this way, the category weights of the fully connected layer can be balanced according to the absolute value $d_{ij}$ of the gradient of each input data. It not only alleviates the category bias between old and new tasks in incremental learning, but also greatly reduces the within-task bias. The main procedure is summarized in Algorithm 1 1."}, {"title": "Output Coordination", "content": "According to the above analysis, it is necessary to keep the output distribution of the new task data $\\mathcal{D}_t$ on the old task classification heads consistently before and after updating the old task models 2. Also, it is necessary to suppress the output scores of the old task data $\\mathcal{D}_{old}$ on the classification heads of the new task (In Figure 1, this is to keep the blue solid line consistent with the dotted line, and make the green solid line approach to 0).\nWhen the model trains the t-th task, we suppose that the output score of the data $\\mathcal{D}_t \\cup \\mathcal{D}_{old}$ without going through softmax layer is given by $\\tilde{z}_{i,j,k}$. Before updating the old tasks models and training the t-th task, the output score of the data $\\mathcal{D}_t \\cup \\mathcal{D}_{old}$ is $\\hat{z}_{i,j,k}$. By considering the principle of human inductive memory, KD is used to enforce the output consistency of the new task data on the classification heads of each old task before and after updating the corresponding model, i.e.,\n$\\mathcal{L}_{OC,1\\rightarrow t-1} = \\sum_{\\tau=1}^{t-1}\\sum_{i,j,k} P_{KL} (\\hat{z}^{t}_{i,j,k}||\\tilde{z}^{t}_{i,j,k})  \\tag{8}$\nwhere $P_{KL} (\\cdot)$ is the distillation loss, and $e$ is a temperature scaling parameter.\nThe output of the old task data $\\mathcal{D}_{old}^{t}$ on the classification head of the new task can be adjusted according to:\n$\\mathcal{L}_{OC,t}^{old} = \\frac{1}{N^{new}} \\sum_{i} (P_{i,j,k} - 0), \\quad i \\in \\{1,\\dots,m^{(t-1)}\\} \\tag{9}$\nAlthough the principle of Eq. (8) is similar to the SSIL [Ahn et al., 2021] approach, the output coordination mechanism proposed in this paper is different from the SSIL approach, as shown in Figure 3. Combining the output coordination loss $\\mathcal{L}_{IC,t}$ of Eq. (7), the overall loss function $\\mathcal{L}_{JIOC,t}$ of the method proposed can be obtained, i.e.,\n$\\mathcal{L}_{JIOC,t} = \\mathcal{L}_{IC,t} + \\gamma_1\\mathcal{L}_{OC,1\\rightarrow t-1} + \\gamma_2\\mathcal{L}_{OC,t}^{old} \\tag{10}$\nwhere $\\gamma_1 \\ge 0$ and $\\gamma_2 \\ge 0$ are trade-off hyper-parameters."}, {"title": "Experiment", "content": "In this paper, we not only validate the effectiveness of our method on unbalanced CIFAR10-LT and CIFAR100-LT datasets but also conduct corresponding validation on"}, {"title": "Ablation Studies", "content": "In this section, we first separately investigate the effectiveness of input and output coordination strategies, and then analyze that the proposed output coordination strategy exhibits a more pronounced effect in mitigating forgetting compared to the SSIL method."}, {"title": "Study on the effectiveness of Input and Output Coordination", "content": "To validate the effectiveness of input and output coordination strategies, we exclusively employed ResNet18 as the feature extractor on the CIFAR100-LT dataset,, as shown in Tables 4. Furthermore, on the CIFAR100 dataset, we conducted experiments separately using ResNet18 and ResNet32 as feature extractors, as shown in Table 5 and Table 6 (we refer to the supplementary material for Table 5 and Table 6).\n(1) The results are reported in Table 4, where we can see that the average accuracy of the ICARL algorithm with the proposed input coordination is 32.34 on the old tasks, 76.22 on the new task, and 43.16 overall. Compared with the original ICARL approach, the improvements are 27.22%, 3.50%, and 12.19%, respectively. The results from the old task, new task, and overall task in Table 5 and Table 6 also illustrate the competitiveness of the input coordination strategy. This demonstrates that the input coordination strategy can alleviate class imbalance in incremental learning. Besides, the ICARL algorithm only uses KD to maintain the output distribution on the old task classification heads. It does not take into account the human inductive memory mechanism for coordinating output distribution across different tasks. In Table 4, the ICARL algorithm achieved average performances of 34.45, 74.83, and 45.13 on the old tasks, new tasks, and overall, respectively. Our proposed output coordination mechanisms improved these performances by 35.52%, 1.62%, and 17.31%, respectively. It can be concluded that the input and output coordination strategy proposed in this paper yields significant improvements, whether applied to the CIAFR100 dataset, the CIAFR100-LT dataset, or different network architectures with varying depths.\n(2) In the CIART100-LT dataset, the input coordination strategy demonstrates notable enhancements in the outcomes for old tasks (27.22%), new tasks (3.50%), and overall task (12.19%) performance when compared to the original ICARL algorithm, as shown in Table 4. Similarly, In the CIART100 dataset, the input coordination strategy further improves the original ICARL algorithm by 42.05%, 3.61%, and 19.51% on the old tasks, new tasks, and overall tasks, as shown in Table 5. According to the description and corresponding improvement effects of CIART100-LT and CIART100, it can be seen that the input coordination strategy has a good regulating effect on the imbalance of old and new categories."}, {"title": "Experiment Comparison between Output Coordination Strategy and the SSIL", "content": "To quantitatively analyze the differences between the output coordination strategy and the SSIL, we conducted corresponding experimental results based on different feature extractors and datasets, including class-imbalanced and balanced datasets, as shown in Figure 4. From the results in Figure 4, it is evident that using the output distribution coordination strategy leads to significant improvements in each stage task on class-imbalanced datasets and in deep feature networks (ResNet32). This also indicates that the output distribution coordination strategy enables new task data to maintain consistent output distributions on the old task classification head and suppresses old task data on the new task classification head during the incremental learning process. This avoidance of interference between new and old tasks is achieved. However, SSIL does not avoid interference from old task outputs, which results in its performance being inferior to the output distribution coordination strategy."}, {"title": "Conclusion", "content": "Although the existing approaches address the class bias issue in class-incremental learning (CIL) to a certain extent by scaling and dividing the softmax layer, they all ignore the bias within the task. In addition, the mutual interference between old and new tasks has not been well resolved. Therefore, we propose a joint input and output coordination (JIOC) mechanism to enable incremental learning models to simultaneously reduce the interference between predictions for these tasks and alleviate the class imbalance issue between and within tasks. From the extensive experiments on multiple popular datasets, we observe significant improvements when incorporating the proposed mechanism into the existing CIL approaches that utilize memory storage. In the future, we intend to design more sophisticated strategies to reweight the inputs, and develop a general end-to-end framework for CIL."}, {"title": "Appendix", "content": "According to the above problem setup, the data {$\\left(x_{u}, y_{u}\\right)\\left(u=1,2, \\ldots, m\\cdot t\\right)$} in the t-th task is simplified to $\\left\\{\\left(x_{u}, y_{u}\\right),\\left(u=1,2, \\ldots, m\\cdot t\\right)\\right\\}$, where  t is the number of classes. Besides, the corresponding output score $p_k$ is simplified to $p_u$. The previous layer\u2019s output score for softmax is $q_u$, i.e.,\n$\\begin{equation}\np_{u}=\\frac{e^{q_{u}}}{\\sum_{r=1}^{m\\cdot t} e^{q_{r}}}\\tag{12}\n\\end{equation}\nIf the k-th neuron is the correct output label, $y_k$ = 1 in $\\left[y_{1}, y_{2}, \\ldots, y_{m\\cdot t}\\right]$ and others are 0. The derivative of $L_{ce,t}$ w.r.t. $q_u$ can be given by:\n$\\begin{equation}\n\\frac{\\partial L_{c e, t}}{\\partial q_{u}}=\\frac{\\partial L_{c e, t}}{\\partial p_{k}} \\frac{\\partial p_{k}}{\\partial q_{u}}=\\frac{\\partial \\left(-\\sum_{u=1}^{m\\cdot t} y_{u} \\log p_{u}\\right)}{\\partial p_{k}} \\frac{\\partial p_{k}}{\\partial q_{u}}\\tag{13}\n\\end{equation}\nSince $y_k$ = 1, $\\frac{\\partial L_{c e, t}}{\\partial p_{u}} \\neq 0$, and the others are 0. The above Eq. 13 can be further simplified as:\n$\\begin{equation}\n\\frac{\\partial L_{c e, t}}{\\partial q_{u}}=y_{k} \\frac{\\partial p_{k}}{\\partial q_{u}}\\tag{14}\n\\end{equation}\nThe solution of $\\frac{\\partial p_{k}}{\\partial q_{u}}$ in Eq. 14 needs to be divided into two cases (u = k and u $\\neq$ k).\nu = k,\n$\\begin{equation}\n\\frac{\\partial p_{k}}{\\partial q_{u}}=\\frac{\\partial p_{k}}{\\partial q_{k}}=\\frac{e^{q_{k}}}{\\sum_{r=1}^{m \\cdot t} e^{q_{r}}}-\\frac{\\left(e^{q_{k}}\\right)^{2}}{\\left(\\sum_{r=1}^{m \\cdot t} e^{q_{r}}\\right)^{2}}=p_{k}\\left(1-p_{k}\\right)\\tag{15}\n\\end{equation}\nu$\\neq$k,\n$\\begin{equation}\n\\frac{\\partial p_{u}}{\\partial q_{k}}=-p_{k} p_{u}\\tag{16}\n\\end{equation}\nFrom the above Eq. 15 and 16, The derivative of $L_{ce,t}$ w.r.t. $q_u$ can be given by:\n$\\begin{equation}\n\\frac{\\partial L_{c e, t}}{\\partial q_{u}}=\\left[\\begin{array}{c}\np_{1} \\\\\n\\ldots \\\\\np_{k}-1 \\\\\n\\ldots \\\\\np_{m \\cdot t}\\end{array}\\right]\\tag{17}\n\\end{equation}\nSince $y_k$ = 1 and others are 0, the above Eq. 17 can be further rewritten, i.e.,\n$\\begin{equation}\n\\frac{\\partial L_{c e, t}}{\\partial q_{u}}=\\left[\\begin{array}{c}\np_{1}-y_{1} \\\\\n\\ldots \\\\\np_{k}-y_{k} \\\\\n\\ldots \\\\\np_{m \\cdot t}-y_{m \\cdot t}\\end{array}\\right]\\tag{18}\n\\end{equation}"}, {"title": "B Ablation Studies Table", "content": "On the CIFAR100 dataset, we conducted experiments separately using ResNet18 and ResNet32 as feature extractors, as shown in Table 5 and Table 6."}]}