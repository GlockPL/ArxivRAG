{"title": "LEARNING DISCRIMINATIVE FEATURES FROM SPECTROGRAMS USING CENTER\nLOSS FOR SPEECH EMOTION RECOGNITION", "authors": ["Dongyang Dai", "Zhiyong Wu", "Runnan Li", "Xixin Wu", "Jia Jia", "Helen Meng"], "abstract": "Identifying the emotional state from speech is essential for\nthe natural interaction of the machine with the speaker. How-\never, extracting effective features for emotion recognition is\ndifficult, as emotions are ambiguous. We propose a novel ap-\nproach to learn discriminative features from variable length\nspectrograms for emotion recognition by cooperating soft-\nmax cross-entropy loss and center loss together. The soft-\nmax cross-entropy loss enables features from different emo-\ntion categories separable, and center loss efficiently pulls the\nfeatures belonging to the same emotion category to their cen-\nter. By combining the two losses together, the discrimina-\ntive power will be highly enhanced, which leads to network\nlearning more effective features for emotion recognition. As\ndemonstrated by the experimental results, after introducing\ncenter loss, both the unweighted accuracy and weighted ac-\ncuracy are improved by over 3% on Mel-spectrogram input,\nand more than 4% on Short Time Fourier Transform spectro-\ngram input.", "sections": [{"title": "1. INTRODUCTION", "content": "Speech emotion recognition (SER) is crucial for natural\nhuman-computer interaction. An SER system extracts fea-\ntures from the speech waveform and then classifies them into\nthe corresponding emotion categories. And how to extract\nfeatures containing enough emotional information has drawn\ngrowing interest.\nFor SER, traditional methods extract frame-level features\nfrom overlapped frames on speech signals and apply statistic\nfunctions on them to get additional features [1]. Since deep\nneural network (DNN) can learn high-level invariant features\nfrom raw data [2] and deep learning brings a lot of break-\nthroughs in many fields [3], more and more methods utilizing\nneural networks to extract valid features from raw data have\nemerged. In [4], DNN and extreme learning machine were\nutilized to extract high-level features from low-level features.\nA bi-directional Long Short-Term Memory model was used\nin [5] to extract high level feature representations for SER. In\n[6], representation learning was performed on raw waveform\nfor end-to-end SER. Convolutional and recurrent neural net-\nworks were applied to learn high-level representations from\nspectrograms in SER task [7].\nEmotions are naturally ambiguous [8], different types of\nemotions might be confusing, increasing the difficulty of ex-\ntracting effective features [9]. A trending methodology to re-\nlease the ambiguity of emotion is to design an appropriate\nloss function instructing the neural network to learn discrim-\ninative features which have smaller intra-class variance and\nlarger inter-class variance. A \u201cpairwise discriminative task\u201d\nwas introduced in [10] to learn the similarity and distinction\nbetween two audios, which utilized cosine similarity loss to-\ngether with binary cross entropy loss. In the task, pairwise\naudios were fed into an audio encode networks to extract au-\ndio vectors, and a following discriminative network judged\nwhether the pairwise audios belong to the same emotion cat-\negory using binary cross-entropy loss. The extracted audio\nvectors from the same class were made \"close\" by the effect\nof cosine similarity loss and they were classified by a sup-\nport vector machine (SVM). In [11], a triplet framework was\nproposed to extract discriminative features by using triplet\nloss[12], whose input was triplets including two utterances\nfrom the same emotion class and one utterance from other\nclasses. Then, similarly as [10], an SVM fed with extracted\nfeatures was used for classifying.\nRecent methods to learn discriminative features for SER\nvia using cosine similarity loss[10] or triplet loss[11] adopt a\ntwo-step strategy. These methods extract discriminative fea-"}, {"title": "2. THE PROPOSED APPROACH", "content": "Fig.1 depicts the framework of the proposed model, which\nincludes several 2-D Convolutional Neural Network layers\n(CNN layers), a bidirectional Recurrent Neural Network layer\n(Bi-RNN) and two fully-connected layers (FC1 and FC2).\nSoftmax cross-entropy loss and center loss are utilized in our\nmodel.\nCNN layers extract spatial information from a variable\nlength spectrogram to get a variable length sequence, Bi-\nRNN compresses the variable length sequence down to a\nfixed-length vector. FC1 projects Bi-RNN's output to the\ndesired dimensionality. FC2, whose output denotes the pos-\nterior class probabilities, is used to calculate softmax cross-\nentropy loss. Softmax cross-entropy loss enables the network\nto learn separable features, and center loss reduces features'\nintra-class variation simultaneously."}, {"title": "2.1. Model details", "content": "The model takes a Short Time Fourier Transform (STFT)\nspectrogram or Mel-spectrogram as input, whose size is\nLT \u00d7 LF. LT is variable depending on the length of audio,\nand LF is the dimension related to the frequency domain.\nAccording to the experience of computer vision, the con-\nvolutional network, whose first layer uses large convolution\nkernels and the remaining layers use small convolution ker-\nnels, perform well [14, 15]. Besides, after dozens of tests, we\ndetermined the details of CNN layers as Fig.2-(a).\nThe Bi-RNN compresses variable length sequence pro-\nduced by CNN layers to a fixed-length vector by concate-\nnating the last output of forward RNN and backward RNN,\nas shown in Fig.2-(b). Bi-RNN is implemented with 128-\nwidth Gated Recurrent Unit (GRU)[16], so the dimension of\nBi-RNN's output is 256.\nFC1 projects Bi-RNN's output to the desired feature space\nof target dimension d (d = 64 in our experiments), with\nPRELU[17] activation function. We take FC1's output z \u2208 Rd\nas the learned feature and calculate center loss according to z.\nThe output of FC2 denotes the predicted posterior probabil-\nities to corresponding emotion categories. And the parame-\nters in FC2 is used for calculating softmax cross-entropy loss,\nwhich will be described in detail in section 2.2."}, {"title": "2.2. Softmax cross-entropy loss", "content": "Softmax cross-entropy loss instructs the model to learn sepa-\nrable features, and it is common in multi-classification tasks.\nThe softmax loss function is presented as equation 1:\n$L_s = - \\frac{1}{m} \\sum_{i=1}^{m} log(\\frac{e^{W_{y_i}^T z_i + b_{y_i}}}{\\sum_{j=1}^{n} e^{W_j^T z_i + b_j}})$       (1)\nwhere m means the size of mini-batch and n is the number of\nemotion categories. zi \u2208 Rd is the i-th deep feature, belong-\ning to y\u2081-th emotion category (yi \u2208 {1,2, ..., n}). Wj \u2208 Rd\ndenotes the j-th column of the weights W \u2208 Rd\u00d7n in FC2,\nb\u2208 Rn is the bias in FC2 and bj is the j-th term of b."}, {"title": "2.3. Center loss", "content": "To reduce intra-class variation of learned features, we intro-\nduce center loss in our model. Our model keeps a global cen-"}, {"title": "ter for each class and pulls features closer to their correspond-ing centers. The formula of center loss is given as follows:\n$L_c = \\frac{1}{m} \\sum_{i=1}^{m} ||z_i - c_{y_i}||^2$                                           (2)\nwhere cj (j\u2208 {1,2,..., n}) denotes the global class cen-\nter of features corresponding to the j-th emotion category.\nThrough optimizing the center loss, the distance between fea-\ntures within the same class becomes smaller. Cj is initial-\nized with 0 and updated per mini-batch iteration based on\n\u010bj, which is the j-th class center of features from a mini-\nbatch, caculated by equation 3 when \u03a3\u03b91 \u03b4(Yi = j) > 0.\n\u03b4(condition) = 1 if the condition is satisfied, otherwise\n\u03b4(condition) = 0.\n$\\dot{c_j} = \\frac{\\sum_{i=1}^{m} \\delta(Y_i = j)z_i}{\\sum_{i=1}^{m} \\delta(Y_i = j)}$                                                                                     (3)\nThe global class center cj is updated as equation 4. \u03b1\nis a hyperparameter controlling the update rate of cj when\nthere are features corresponding to j-th emotion category in\nthe new mini-batch, while cj keeps its previous value when\nno corresponding features in the new mini-batch. c and c\ndenotes the t-th iteration's value of cj and \u010bj respectively.\n$c_j^{t+1} = \\begin{cases}\n(1-\\alpha)c_j^t + \\alpha \\dot{c_j} & \\sum_{i=1}^{m} \\delta(Y_i = j) > 0 \\\\\nc_j^t & \\sum_{i=1}^{m} \\delta(Y_i = j) = 0\n\\end{cases}$                                               (4)"}, {"title": "2.4. Weighted loss and joint loss", "content": "Because of class imbalance, instead of using Ls and Lc di-\nrectly, we assigned weights to softmax cross-entropy loss and\ncenter loss in our experiments, shown in equation 5 and equa-\ntion 6. The weight wj(j \u2208 {1, 2, ..., n}) is in inverse propor-\ntion to the sample number of the j-th class in training set.\n$L_s = - \\frac{1}{m} \\sum_{i=1}^{m} w_{y_i}log(\\frac{e^{W_{y_i}^T z_i + b_{y_i}}}{\\sum_{j=1}^{n} e^{W_j^T z_i + b_j}})$                                             (5)\n$L_c = \\frac{1}{m} \\sum_{i=1}^{m} w_{y_i}||z_i - c_{y_i}||^2$                                                                            (6)\nOur neural network is trained using a joint loss comprised\nof softmax cross-entropy loss and center loss, calculated as\nequation 7. A is a hyperparameter trading off center loss\nagainst softmax cross-entropy loss. When x = 0, the model\nis trained using only softmax cross-entropy loss.\n$L = L_s + \\lambda L_c$                                                                          (7)"}, {"title": "3. EXPERIMENTS AND ANALYSIS", "content": "3.1. Experimental setup\nOur model was tested on the Interactive Emotional Dyadic\nMotion Capture (IEMOCAP) [18] dataset, which was de-\nsigned for studying multimodal expressive dyadic interac-\ntions. It contains approximately 12 hours of audiovisual data,\nincluding video, speech, motion capture of face and text tran-\nscriptions. For training and evaluation, we used categorical\nemotions neutral, angry, happy, sad and excited which rep-\nresent the majority of the emotion categories in the database\n(5531 utterances), happy and excited were merged since they\nare close in the activation and valence domain [18].\nAs there is data imbalance between classes - neutral\n(30.9% of the total dataset), angry (19.9%), happy (29.6%),\nand sad (19.6%), we adopted both the unweighted accuracy\n(UA, the mean value of the recall for each class) and the\nweighed accuracy (WA, the number of correctly classified\nsamples divided by the total amount of samples) as metrics\nto evaluate SER performance. In our experiments, the dataset\nwas divided into 5 subsets randomly keeping the emotion\ndistribution, 4 subsets were used for training, half of the\nlast subset was used as development set and half as test set.\nWe repeated cross-validation 5 times to get the final average\nresult.\nOur experiments were conducted on log scale Mel-\nspectrogram and log scale STFT spectrogram respectively.\nTo get spectrogram, a sequence of overlapping Hamming\nwindows were applied to the speech signal, with window\nshift of 10msec, and window size of 40msec. The speech\nsignal was sampled at 16kHz and the DFT length was 1024.\nThe number of Mel bands was 128 when calculating Mel-\nspectrogram. We assumed that 14s long utterance contains\nenough emotional information. So for utterance whose dura-\ntion is longer than 14s (2.07% of the total dataset), we only\nextracted the middle 14s to calculate spectrogram.\nDuring the training phase, we used Adam [19] optimizer,\nset learning rate to 0.0003, and the size of mini-batch was\n32. We applied the parameters maximizing the UA of the\ndevelopment set as the model's final parameters."}, {"title": "3.2. Experiments on Mel-Spectrogram", "content": "As \u03b1 controls the update rate of class centers and A dom-\ninates the weight of center loss, we conducted experiments\nto investigate the effect of hyperparameter a and A on Mel-\nspectrogram input. The experimental results are shown in\nFig.3. Fig.3-(a) illustrates that the UA and WA are not sen-\nsitive to a, Fig.3-(b) demonstrates that the SER performance\ncan be significantly improved with proper value of A. When\n\u03bb = 0 (setting 1), the UA is 63.80% and the WA is 61.83%. The UA and WA is 66.86% and 65.40% respectively when\n\u03bb = 0.3, \u03b1 = 0.5 (setting 2). The UA and WA are both\nincreased by over 3% when using center loss with proper hy-\nperparameters.\nTo illustrate the discriminative power provided by center\nloss, we applied Principal Component Analysis(PCA) to em-\nbed learned features. The PCA embeddings (of features pro-\nduced by once experiment in cross-validations on setting 1\nand on setting 2 respectively) are drawn in Fig.4. Comparing\nFig.4-(b) with Fig.4-(a) or Fig.4-(d) with Fig.4-(c), we could\nfind that features belonging to the same class are more com-\npact when using center loss. After introducing center loss\ntogether with softmax cross-entropy loss to train the model,\nthe discriminative power can be enhanced, which leads to the\nmodel learning more effective features for SER.\nThe final confusion matrix was calculated by averaging\nconfusion matrices from 5 times cross-validations, the results\non setting 1 and setting 2 are shown in table 1-(a) and table 1-\n(b). As can be seen, after introducing center loss for enhanc-\ning discriminative power, the accuracy of each emotion has\nbeen improved to different degrees."}, {"title": "3.3. Experiments on STFT spectrogram", "content": "We conducted experiments to prove that introducing center\nloss is also useful for learning effective features for SER on\nSTFT spectrograms input, by comparing experiment result\nwhen x = 0 (setting 3) with \u03bb = 0.3, \u03b1 = 0.5 (setting 4).\nThe final confusion matrices are shown in table 2-(a) and ta-\nble 2-(b), which show that each emotion's accuracy has been\nimproved after introducing center loss. The UA and WA are\n60.98% and 58.93% on setting 3, while 65.13% and 62.96%\non setting 4. Both the UA and WA are improved by more than\n4% after introducing center loss.\nWe presented the UA and WA on setting 1 ~ setting 4\nshown in Fig.5. We can conclude that introducing center loss\ncould effectively improve the SER performance by compar-\ning setting 2 with setting 1 or setting 4 with setting 3 in Fig.5.\nBy comparing experimental results on setting 1 with setting 3\nor setting 2 with setting 4 in Fig.5, we could find that learning\neffective features for SER on Mel-spectrogram input, which\nreduces the dimension based on human hearing characteris-\ntics, is easier than on STFT spectrogram input."}, {"title": "4. CONCLUSIONS", "content": "In this paper, we presented an approach to learn discrimina-\ntive features from variable length spectrograms by integrat-\ning center loss in the SER model. The 2-D PCA embedding\nillustrated the discriminative power when using center loss,\nwhich enables the neural network to learn more effective fea-\ntures for SER. Our experiment results demonstrated that cen-\nter loss with proper hyperparameters is useful for improving\nthe performance of SER on both Mel-spectrogram input and\nSTFT spectrogram input. As center loss mainly focuses on\nreducing the intra-class variation of features, in future work,\nwe will explore more in the loss function, hoping to increase\nthe features' inter-class variation to further improve the SER\nperformance."}]}