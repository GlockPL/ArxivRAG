{"title": "Suppress Content Shift: Better Diffusion Features via Off-the-Shelf Generation Techniques", "authors": ["Benyuan Meng", "Qianqian Xu", "Zitai Wang", "Zhiyong Yang", "Xiaochun Cao", "Qingming Huang"], "abstract": "Diffusion models are powerful generative models, and this capability can also be applied to discrimination. The inner activations of a pre-trained diffusion model can serve as features for discriminative tasks, namely, diffusion feature. We discover that diffusion feature has been hindered by a hidden yet universal phenomenon that we call content shift. To be specific, there are content differences between features and the input image, such as the exact shape of a certain object. We locate the cause of content shift as one inherent characteristic of diffusion models, which suggests the broad existence of this phenomenon in diffusion feature. Further empirical study also indicates that its negative impact is not negligible even when content shift is not visually perceivable. Hence, we propose to suppress content shift to enhance the overall quality of diffusion features. Specifically, content shift is related to the information drift during the process of recovering an image from the noisy input, pointing out the possibility of turning off-the-shelf generation techniques into tools for content shift suppression. We further propose a practical guideline named GATE to efficiently evaluate the potential benefit of a technique and provide an implementation of our methodology. Despite the simplicity, the proposed approach has achieved superior results on various tasks and datasets, validating its potential as a generic booster for diffusion features.", "sections": [{"title": "1 Introduction", "content": "Diffusion models (DMs) [15, 34] are a prevalent family of generative models for various tasks [36, 35, 25]. This strong generative capability can be applied to discrimination [21]. Diffusion Feature (DF), a popular approach, extracts inner activations from a pre-trained diffusion model as vision features [1, 50, 60, 57, 37, 62], similarly to how ResNet [14] serves as a feature extractor. Extracting features with a vastly pre-trained diffusion model grants this approach strong robustness and generalizability. Furthermore, it enjoys the philosophy of hitchhiking, a solid research paradigm in the age of large base models [44]: advancements in diffusion models can all be transformed into better feature quality. It promises this research direction a bright future."}, {"title": "2 Related Work", "content": "The introduction to diffusion models can be found in [6]. In this section, we solely focus on diffusion features. So far, the research topics in this direction fall into two categories: task exploration and method improvement, i.e., applying the paradigm to different tasks and studying the approach itself for better feature quality, respectively.\nTask exploration. There have been attempts on semantic segmentation [1, 50], semantic correspon- dence [57, 37, 22], hyperspectral image classification [62], domain generalization [9], training data synthesizing [45, 55], zero-shot referring image segmentation [30], visual grounding [23], a novel task involving personalization [47], co-salient object detection [48], and open-world segmentation [38]. This extensive and extending list shows the discriminative capability of DMs.\nMethod improvement. The way to enhance diffusion features can be divided according to the three important factors of a diffusion model: prompt, layer, and timestep. In a basic pipeline, the prompt is manually designed and simple, the layer only refers to the activations between convolutional blocks, and the timesteps are manually set. (i) To enhance the use of prompt, a typical improvement is prompt tuning [50, 60, 57, 22], which equivalently fine-tunes the text encoder along with the downstream discriminative task. Another novel method is auto-captioning [19], replacing manual prompt design with an automatic captioner. (ii) To dig more information out of UNet layers, researchers choose to additionally take attention layers into consideration. However, we find that cross-attention is more frequently used [50, 60, 57, 30] while self-attention is less popular [47]. (iii) As for better usage of timesteps, [62] proposes to extract features from many timesteps and dynamically assign weights to them, while [52] employs reinforcement learning for better timestep selection.\nOf the two directions, our work aims for better methods instead of new tasks. Furthermore, most existing diffusion feature approaches suffer from content shift, an example of which is the visualization of attention features in Figure 1. Therefore, our GATE can serve as a generic performance booster to other diffusion feature approaches, showing its potential for broad application."}, {"title": "3 Preliminaries: Diffusion Feature", "content": "Diffusion models consist of a neural network module and a diffusion scheduler. The network is an end-to-end network, which can be formally denoted as $\\epsilon_{\\theta}$, where $\\theta$ is the parameters. The diffusion scheduler is the core of diffusion models. With the scheduler, diffusion models generate images progressively, during which the network module is re-used on each timestep, each time only predicting an incremental noise. Generally, we use a smaller / larger timestep to indicate less / more noises. A typical generation process starts from t = T (total noises) and ends at t = 0 (clean images).\nNext, we will explain how features are extracted using a common pipeline for diffusion features, with the visual illustration in Figure 2. Given an input image $x \\in [R^{3 \\times h \\times w}$, where h, w are height and width, the extraction process includes: (i) A pre-trained VAE encodes the input image into the latent space, inducing $x_0 \\in R^{4 \\times h' \\times w'}$, as a common practice presented in [34, 33]. (ii) $x_0$ acts as the input of the forward diffusion process, i.e., timestep t = 0. As suggested in [1], it is beneficial to extract diffusion features at non-zero timestep. Following this practice, we set t = 50 and get $x_t$. (iii) $x_t$, along with the timestep t and a textual prompt c, is sent into the pre-trained diffusion UNet $\\epsilon_{\\theta}$, i.e., $\\epsilon_{\\theta}(x_t, t, c)$. The generation techniques selected by our method are also applied here to modify $\\theta$, c. (iv) Convolutional and attention features are gathered during the computation of the backbone. For convolutional features, we gather the output activations of each resolution in the upsampling stage of the UNet. For attention features, we obtain the mean value of the similarity maps between query and key in all cross-attention layers."}, {"title": "4 Exploration of Content Shift", "content": "In Figure 1, we can qualitatively observe content shift from feature visualization. However, this visualization is intentionally amplified for better observation by extracting features at large timesteps. Now we aim to examine the impact of content shift on quantitative performance under more practical scenarios, i.e., when timesteps are small. To this end, we need to toggle the magnitude of content shift in features. Describing the quality of the image is a widely adopted way to control generation. A prompt accurately describing the quality of input images is considered to suppress content shift, while intentionally describing something different from the image will cause more severe content shift. We select the semantic segmentation task [1] as an example of high-quality images and image classification on CIFAR10 [20] for low-quality images. The results in Figure 3 seem counter-intuitive at first glance, as low-quality prompts surprisingly can improve the performance on CIFAR10, but this in fact complies with the quality of input images. Therefore, these results can demonstrate the negative impact of content shift on feature quality at small timesteps."}, {"title": "4.1 Impact of Content Shift", "content": "In Figure 1, we can qualitatively observe content shift from feature visualization. However, this visualization is intentionally amplified for better observation by extracting features at large timesteps. Now we aim to examine the impact of content shift on quantitative performance under more practical scenarios, i.e., when timesteps are small. To this end, we need to toggle the magnitude of content shift in features. Describing the quality of the image is a widely adopted way to control generation. A prompt accurately describing the quality of input images is considered to suppress content shift, while intentionally describing something different from the image will cause more severe content shift. We select the semantic segmentation task [1] as an example of high-quality images and image classification on CIFAR10 [20] for low-quality images. The results in Figure 3 seem counter-intuitive at first glance, as low-quality prompts surprisingly can improve the performance on CIFAR10, but this in fact complies with the quality of input images. Therefore, these results can demonstrate the negative impact of content shift on feature quality at small timesteps."}, {"title": "4.2 Cause of Content Shift", "content": "After the negative impact of content shift is confirmed, we next aim to find its cause. Unlike more conventional feature extractors such as ResNet [14], the inputs to diffusion models are not the original image (Figure 4(a)), but its noisy version (Figure 4(b)), as enforced by the diffusion process [15]. The early layers of diffusion models even further add more noise to the feature maps (Figure 4(c)). However, the diffusion UNet gains the ability from vast pre-training to reconstruct clean inner representations from noisy inputs (Figure 4(d)), roughly at the middle of the UNet structure. Additionally, the shortcut structures in UNet also help the reconstruction by passing some high-frequency details. Afterward, the diffusion UNet will further predict noises based on the reconstructed representations (Figure 4(e)).\nNotably, the diffusion features we are using are in fact the reconstructed representations, which answers why clean diffusion features can be obtained even though the inputs are noisy. Despite the reconstruction ability, however, many high-frequency details are potentially blurred out by input noises, and thus their reconstruction is mostly based on \"imagination\". This leads to possible drift from the original image during reconstruction [7]. Naturally, the content shift phenomenon in extracted diffusion features reflects the drift during reconstruction. Consequently, content shift is an inherent characteristic of diffusion models and diffusion features, which suggests its broad existence across models and timesteps."}, {"title": "5 Suppression of Content Shift", "content": "According to the cause of content shift, we need to steer the reconstruction process back to the original image to suppress content shift. While it is viable to design new methods for this purpose, we find it also possible and more efficient to adopt off-the-shelf generation techniques. Specifically, as an inherent characteristic of diffusion models, content shift affects not only features but also the original generative purpose of diffusion models. Hence, there have been techniques for generation that are already capable of toggling content shift by steering reconstruction [7, 58]. For example, ControlNet [58] introduces an additional reference image and steers reconstruction by directly modulating activations, pushing the reconstructed representation towards the reference image. It inspires us to utilize ControlNet in a different way: we can set the same input image simultaneously as the reference image, enforcing the recovered image to be more similar to the input one and thus suppressing content shift. Similarly, it is possible to adopt other generation techniques to suppress content shift.\nFurthermore, utilizing off-the-shelf generation techniques is more consistent with the intuition of diffusion feature than designing a new method. Specifically, this field is dependent on the generative capability of diffusion models, and thus it is important to stay updated with new advancements in the generation field. Utilizing techniques from the generation field helps with this goal, while a method that is newly designed solely for diffusion feature might hinder it. Considering it, we decide not to devise new methods, but to develop more detailed guidelines for suitably integrating these off-the-shelf generation techniques.\nWe next illustrate the guidelines for utilizing off-the-shelf generation techniques. In the field of generation, the abundant techniques may not all be suitable for suppressing content shift, suggesting the necessity of examining the effect of a given technique on feature quality. Although it is possible"}, {"title": "5.1 Utilization of Generation Techniques", "content": "According to the cause of content shift, we need to steer the reconstruction process back to the original image to suppress content shift. While it is viable to design new methods for this purpose, we find it also possible and more efficient to adopt off-the-shelf generation techniques. Specifically, as an inherent characteristic of diffusion models, content shift affects not only features but also the original generative purpose of diffusion models. Hence, there have been techniques for generation that are already capable of toggling content shift by steering reconstruction [7, 58]. For example, ControlNet [58] introduces an additional reference image and steers reconstruction by directly modulating activations, pushing the reconstructed representation towards the reference image. It inspires us to utilize ControlNet in a different way: we can set the same input image simultaneously as the reference image, enforcing the recovered image to be more similar to the input one and thus suppressing content shift. Similarly, it is possible to adopt other generation techniques to suppress content shift.\nFurthermore, utilizing off-the-shelf generation techniques is more consistent with the intuition of diffusion feature than designing a new method. Specifically, this field is dependent on the generative capability of diffusion models, and thus it is important to stay updated with new advancements in the generation field. Utilizing techniques from the generation field helps with this goal, while a method that is newly designed solely for diffusion feature might hinder it. Considering it, we decide not to devise new methods, but to develop more detailed guidelines for suitably integrating these off-the-shelf generation techniques.\nWe next illustrate the guidelines for utilizing off-the-shelf generation techniques. In the field of generation, the abundant techniques may not all be suitable for suppressing content shift, suggesting the necessity of examining the effect of a given technique on feature quality. Although it is possible"}, {"title": "5.2 Quantitative Evaluation", "content": "We have also developed a quantitative metric for evaluating generation techniques. We set feature extracted at t = 0 as reference $FEAT_{ref} \\in R^{c \\times h \\times w}$ as it is less affected by noises. Then we use the Laplacian operator to evaluate the contour difference between feature FEAT and the reference:\n$diff = \\sum_{i,j}^{hxw} \\Sigma | || Laplacian(FEAT_{ref,i,j})||_2 - ||Laplacian(FEAT,i,j)||_2 | \\in (-\\infty,1]$ (1)\nWe then set a feature with stronger content shift as an anchor and compare $dif f_{anchor}$ with other features.\n$Score = \\frac{(dif f_{anchor} - dif f)}{dif f_{anchor}}$ (2)\nScore = 1 means an exact match, and a smaller value indicates more shift. In this way, we can measure the extent of content shift in extracted features using different generation techniques and thus evaluate the suppression effect of techniques. Noticeably, the evaluation of this quantitative metric can be well approximated by the previously proposed qualitative evaluation, so we recommend the qualitative evaluation if efficiency is desired."}, {"title": "5.3 Selected Generation Techniques", "content": "We select three generation techniques as our implementation of GATE. Their Img2Img generation results are provided in Figure 6. Integrating the techniques only slightly impacts efficiency, which will be discussed in Appendix A along with technique details. Additionally, we analyze two failed"}, {"title": "5.4 Feature Amalgamation", "content": "The three techniques above are able to improve feature quality individually and can be applied simultaneously for stronger suppression effects. While this can lead to a single high-quality feature, it is a common practice in previous diffusion feature approaches to amalgamate multiple features for further improvement [1, 24, 62, 29]. The conventional way extracts features at different timesteps to obtain more diverse information. We find that, compared to the conventional amalgamation of timesteps, the amalgamation of different combinations of generation techniques can bring stronger diversity, as indicated in Figure 7. Therefore, we additionally amalgamate features obtained with"}, {"title": "6 Experimental Validation", "content": "Typically, diffusion feature studies [1, 50, 60] prefer fine-grained pixel-level tasks for evaluation. Following this practice, we select three tasks for experiments: semantic correspondence using SPair-71k [26] dataset, label-scarce semantic segmentation using Bedroom-28 [54] and Horse-21 [54] datasets, and standard semantic segmentation using ADE20K [61] and CityScapes [5] datasets."}, {"title": "6.1 Experimental Settings", "content": "Task & Dataset. Typically, diffusion feature studies [1, 50, 60] prefer fine-grained pixel-level tasks for evaluation. Following this practice, we select three tasks for experiments: semantic correspondence using SPair-71k [26] dataset, label-scarce semantic segmentation using Bedroom- 28 [54] and Horse-21 [54] datasets, and standard semantic segmentation using ADE20K [61] and CityScapes [5] datasets.\nEvaluation Metrics. (i) For semantic correspondence, PCK@0.1img(\u2191) and PCK@0.1bbox(\u2191) are used, following the widely-adopted protocol reported in [26]. (We omit @0.1 to save some space in Table 1.) These two metrics mean the percentage of correctly predicted keypoints, where a predicted keypoint is considered to be correct if it lies within the neighborhood of the corresponding annotation with a radius of 0.1 \u00d7 max(h, w). For PCK@0.1img/PCK@0.1bbox, h, w denote the dimension of the entire image/object bounding box, respectively. (ii) For semantic segmentation, we use mIoU metric, which is the mean over the IoU performance across all semantic classes [12]. For each image, IoU (Intersection over Union, \u2191) is defined by #(overlapped pixels between the prediction and the ground truth) / #(union pixels of them). In addition, we also use aAcc and mAcc, where aAcc is the classification accuracy of all pixels and mAcc averages the accuracy over categories.\nFeature Extraction. All tasks extract features at t = 50. When ControlNet is applied, except for standard semantic segmentation, we additionally start multi-step denoising from t = 60. For feature amalgamation, we extract multiple convolutional features and one attention feature per image:\nSemantic correspondence: We obtain six in total convolutional features using individual fine-grained prompt, ControlNet, and LoRA techniques, and one attention feature using a prompt including all object categories, with ControlNet and LoRA.\nLabel-scarce semantic segmentation: We obtain one convolutional feature using fine-grained prompts, one convolutional feature using ControlNet, and one (Bedroom-28) to two (Horse- 21) features using different LoRA weights. One attention feature is extracted with all three techniques applied.\nStandard semantic segmentation: One convolutional feature is obtained using only fine- grained prompts and two more are extracted additionally with ControlNet and different LORA weights. One attention feature is extracted with all three techniques applied.\nNotably, the ADE20K dataset for standard semantic segmentation contains images of varying scenes, which can test how well the fine-grained prompt technique can generalize in this scenario. To this end, we use a prompt that can cover different scenarios: \u201ca highly realistic photo of the real world. It can be an indoor scene, or an outdoor scene, or a photo of nature. high quality\". This prompt covers various scenes for generalizability and describes image quality for fine-grained effect.\nFor more experimental settings, including more detailed feature extraction methods and implementa- tion details, please refer to Appendix C."}, {"title": "6.2 Comparison with SO\u0422\u0410", "content": "The experimental results are shown in Table 1 and Table 2. For most SOTA competitors, we borrow the reported results from their original studies. However, MaskCLIP [8] and ODISE [50] only provide results on ADE20K and it is hard to extend their implementations to CityScapes, so their results on CityScapes are missing. Furthermore, the original results reported by VPD [60] are based on full-scale fine-tuning of diffusion UNet, which is not fair as we do not train the diffusion model. Therefore, we re-evaluated VPD with the diffusion UNet frozen and reported our results.\nSemantic Correspondence. It is a pity that the related studies do not perform experiments under exactly the same setting. For fairness, we mainly compare GATE against a baseline method, which uses one feature extracted without any technique, under a unified setting. For reference, we still"}, {"title": "6.3 Qualitative Analysis", "content": "In Figure 8, we provide feature visualization for qualitative analysis of GATE. The visualization is obtained using PCA analysis, reducing the channels of features to 3, which are regarded as RGB for visualization. We can observe: (i) The attention features become clearer and closer to the input image when more generation techniques are applied according to GATE, showing the suppression effect on content shift. (ii) Notably, for the second image where a person is riding a horse, the baseline attention feature fails to follow the instruction, i.e., attending only to the horse and ignoring the person. In contrast, generation techniques applied according to GATE help attention features attend to the correct object. (iii) From convolutional features, we can see the application of generation techniques brings stronger diversity."}, {"title": "6.4 Ablation Study: Effect without Feature Amalgamation", "content": "For ablation study, we aim to evaluate the effect of selected techniques without feature amalgamation. The discriminative performance is shown at the bottom line of Figure 8, which is obtained on a single Horse-21 split instead of five random repeats for faster evaluation. We can observe: (i) Every individual technique can improve feature quality over baseline. (ii) When multiple techniques are applied simultaneously, stronger improvement can be obtained. This demonstrates that all three selected techniques can benefit feature quality, and their benefits can be combined together."}, {"title": "7 Conclusion and Future Work", "content": "In this paper, we reveal a phenomenon named content shift that has been causing degradation in diffusion features. Based on the analysis of its cause, we propose to suppress it with off-the-shelf generation techniques, which allows hitchhiking the advancements in generative diffusion models. This approach, while enjoying simplicity, is experimentally demonstrated to be generically effective.\nHowever, the effectiveness of GATE relies on the selected techniques, for which we propose both a qualitative evaluation guideline and a quantitative metric. Though we selected three effective techniques and reported failed cases, there still is more to explore, which might potentially lead to more effective implementations. Furthermore, we only experimented with three tasks, so the full potential of GATE might remain under-explored."}, {"title": "A Implementation Details and Efficiency Concerns", "content": "In this section, we will explain the implementation details of each selected technique and feature amalgamation. Further discussion will also be provided on how these techniques manage to suppress content shift and the efficiency impact of our implementation."}, {"title": "A.1 Fine-Grained Prompts", "content": "Fine-grained prompts can be integrated into diffusion feature very simply. We only need to replace the simple and short prompts, such as \"a photo of a horse\" in most DF approaches [37, 9, 45, 55, 21], with more complex ones. Since prompts are a built-in function of current diffusion models, the only required integration of fine-grained prompts is to generate these prompts. To fulfill our goal, we need detailed descriptions of the image content, and many image captioning models can serve this purpose, such as Kosmos-2 [31]. It is also possible to manually design prompts that can cover various images, which is found to be almost as effective as auto captioners. When this approach is used, our observation is only in the training set to avoid data leaks. While this is effective for convolutional features, attention features require more consideration. To be specific, using specific prompts for each image causes inconsistency in the structure of attention features. To fill this gap, we always use manual and fixed prompt for attention features, even when convolutional features utilize auto-captioners. This fixed prompt is designed by observing images and describing the common objects. For example, \"bedroom, a bed, some bedroom furniture, lights, a door, ceiling, floor, walls, pillow, quilt, chair and table, window, in good quality\".\nRegarding the efficiency of integrating fine-grained prompts, we do not make any changes to the original feature extraction pipeline but only introduce the overhead of image captioning. Theoretically, this is still linear complexity. In practice, the actual time consumption depends on what auto-captioner is used. Moreover, these prompts can be re-used for the extraction of many groups of features, further reducing the proportion of overhead in the total time."}, {"title": "A.2 ControlNet", "content": "ControlNet requires a parameter, the control signal, which should be set as the input image itself to suppress content shift. More specifically, the control image ControlNet requires is not an ordinary image, but a specially processed one, such as a depth image or a canny image. Among all control image types, we find that canny images are exceptionally efficient because their process does not use any neural network model. Therefore, we use canny as the sole control type for feature extraction.\nAlthough the simple usage is already yielding satisfying results, we find an additional way to further exploit ControlNet. It is reported that taking multiple denoising steps before feature extraction can make the attention feature less blurry [51]. However, taking multiple denoising steps is not a common practice for diffusion features, as it also brings severe performance drops caused by content shift. With ControlNet, it becomes possible to harness this good property without failing to suppress content shift. We only do this for attention features, so convolutional features are still extracted in a one-step manner.\nRegarding efficiency, processing an image into a canny image takes a very small overhead. So the major overhead introduced by ControlNet should be extracting attention features with multiple denoising steps. Whilst this does take obviously longer time than one-step feature extraction, its impact is limited due to two factors: (i) We empirically find that the optimal effect comes with less than 10 denoising steps. If more steps are taken, attention features will meet unacceptable content shift even with ControlNet applied. (ii) Different from the diversity effect in convolutional features, attention features do not see obvious enhancement when amalgamated. Therefore, we only extract one attention feature per image and concatenate it to the amalgamation result of convolutional features. Considering the two factors, taking multiple denoising steps does not in fact contain many steps and its use is relatively rare in the overall feature extraction process. Thus, the efficiency impact of integrating ControlNet is small."}, {"title": "A.3 LORA", "content": "The integration of LoRA is also simple. Integrating LoRA into feature extraction only requires switching the base model weights to LoRA weights, so the major effort might be training a good LORA weight. Fortunately, the community has offered many handy tools for ordinary users, such as kohya_ss\u00b3, which can be directly utilized for our purpose. Following the tradition from the community, we choose 30 random images from the training set to train a LoRA weight. Specifically, we choose the LoCon type of LoRA, which additionally inserts trainable parameters between convolutional layers, and follow the corresponding guides for training.\nThe major overhead introduced by LoRA is its training. As a popular tool for AIGC players, the training for LoRA is feasible even on personal computers, showing its efficiency. In our experiments, training one LoRA weight usually takes less than 30 minutes. Moreover, the trained LoRA weights can be re-used, further reducing the proportion of overhead in the feature extraction process."}, {"title": "A.4 Regularized Weight Assignment for Feature Amalgamation", "content": "Feature amalgamation can be accomplished by simply concatenating different features together. However, this is found suboptimal, leading to a superior strategy for feature amalgamation, which is weight assignment [24, 62]. Roughly, it assigns weights to each feature and amalgamates the features via a linear combination according to the weights. We next formalize this process. We have b feature maps $r_1,r_2,\u2026,r_b \\in R^{c \\times w \\times h}$ for each image, with c, w, h being channel, width, and height, respectively. Weight assigners are simple MLP or CNN networks parameterized by $\\theta$ and can be denoted as $f(\u00b7; \\theta)$. Considering that the downstream task might benefit from various perspectives, there are n assigners. Assigning weights $w^j$ is denoted as:\n$w^j := f^j (r_i; \\theta^j) \\in R, s.t. \\sum_{i=1}^b w_i^j = 1, j = 1,...,n$ (3)\nAfterward, we take the linear combinations of features and concatenate them as the final feature:\nr= Concat($\\sum_{i=1}^b w_i^1 r_i,\\sum_{i=1}^b w_i^2 r_i,\u2026,\\sum_{i=1}^b w_i^n r_i$) (4)\nWe notice that this weight assignment strategy is suboptimal. To be specific, the assigners tend to converge to a trivial solution, where they assign weights almost equally to all features and different assigners share a similar prediction. To tackle this issue, we introduce two extra regularization terms. The first one encourages sparsity, meaning the assigners should concentrate on only a few features. The second one, named diversity, promotes a larger discrepancy between the predictions of different weight assigners. Formally, the loss function is:\n$\\mathcal{L}_{fin} = \\mathcal{L} + \\gamma_1 \\sum_{j=1}^{n} |w^j||_1 - \\frac{\\gamma_2}{2} \\sum_{j=1}^{n} \\sum_{k \\neq j}^{n} ||w^j - w^k||_2^2,$\nwhere $\\mathcal{L}$ is the original loss of the downstream task; $\\gamma_1$ and $\\gamma_2$ are the hyper-parameters of the regularization terms; $w_i := (w_1^j, w_2^j, \u2026\u2026\u2026, w_b^j)^T$, and $|| \u00b7 ||_2$ denotes the $l_2$-norm. Appendix D.5 will visualize the direct effect of the proposed regularization on weight distribution.\nSince attention features do not show the same diversity effect as convolutional features, the two types of features are processed differently. To be specific, we obtain many convolutional features but only one attention feature per image. Feature amalgamation is conducted among convolutional features, while the attention feature is concatenated to the final amalgamation result.\nNext, we aim to explain why the two proposed regularization terms can achieve their design purpose. Note that the weights of all features are restricted to sum up to 1, which means the $l_1$-norm of w is always 1. For a vector with a fixed $l_1$-norm, a larger $l_2$-norm means concentrating on less axes among all dimensions. Therefore, we use the negative value of its $l_2$-norm as sparsity regularization to promote a larger $l_2$-norm. As for diversity regularization, it simply measures and promotes the discrepancy among the predictions of different weight assigners."}, {"title": "B Classifier-Free Guidance and IP-Adapter: Failed Case", "content": "Classifier-free guidance [16] is a widely adopted generation technique in current diffusion models. Its design purpose is to sacrifice diversity for better fidelity, similar to low-temperature sampling [18] in other generative models.\nWe first show its Img2Img generation results according to the GATE guideline in Figure 9. It is clear that it makes little difference to the similarity between outputs and the input, already suggesting that it lacks the potential for suppressing content shift. Since it can also be implemented simply, we integrate it into feature extraction as well and actually examine its effect, leading to the same conclusion as the previous quick evaluation following the GATE guideline.\nWhy is classifier-free guidance unable to suppress content shift? It is designed to gain fidelity at the cost of diversity. While better fidelity is irrelevant to content shift, it might be helpful to lower the diversity of reconstruction given the blank caused by noises. However, the less diverse reconstruction is still not guaranteed to be centered on the original content. Therefore, even though classifier-free guidance can reduce diversity, content shift will not be suppressed. We hope this failed case can provide more insights into how generation techniques affect content shift and how to select them.\nIP-Adapter is a generation technique designed for image variation, which shares a similar architecture to ControlNet. By inputting images, IP-Adapter helps generate new images with some elements taken from the reference one. Since the goal of IP-Adapter is image variation instead of strict control, it is less effective in mitigating content shift than ControlNet. The weaker effectiveness of IP-Adapter is experimentally demonstrated on the Horse-21 dataset, as shown in Table 3. IP-Adapter is not entirely ineffective for content shift, but less effective than ControlNet and thus not included in our implementation."}, {"title": "C Experimental Details", "content": "For the diffusion model to extract features, we choose Stable Diffusion v1.5 [34] to be consistent with SOTA competitors. For semantic correspondence, two settings are examined: (i) directly performing"}, {"title": "C.1 Implementation Details", "content": "For the diffusion model to extract features, we choose Stable Diffusion v1.5 [34] to be consistent with SOTA competitors. For semantic correspondence, two settings are examined: (i) directly performing"}, {"title": "C.2 Feature Extraction Details for Each Task", "content": "Convolutional features include two features per setting under different random seeds:\n(i) Two with fine-grained prompt \"a photo of aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, dog, horse, motorbike, person, potted plant, sheep, train, tv monitor, high quality, best quality, highly realistic, masterpiece, high resolution\".\n(ii) Two with LoRA.\n(iii) Two with ControlNet.\nAttention features are obtained using a prompt including all object categories, with ControlNet (denoising from t 60) and LoRA also applied."}, {"title": "C.2.1 Semantic Correspondence", "content": "Convolutional features include two features per setting under different random seeds:\n(i) Two with fine-grained prompt \"a photo of aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, dog, horse, motorbike, person, potted plant, sheep, train, tv monitor, high quality, best quality, highly realistic, masterpiece, high resolution\".\n(ii) Two with LoRA.\n(iii) Two with ControlNet.\nAttention features are obtained using a prompt including all object categories, with ControlNet (denoising from t 60) and LoRA also applied."}, {"title": "C.2.2 Semantic Segmentation on Horse-21", "content": "Convolutional features include:\n(i) A feature using fine-grained prompt \"a horse, high quality, best quality, highly realistic, masterpiece\".\n(ii) A feature using ControlNet, with a simple prompt \"a horse\".\n(iii) Two features using different LoRA weights and a simple prompt. The first LoRA weight is trained to generate high-quality images, while the second LoRA weight is trained until it slightly overfits.\nAttention features are obtained using a prompt \"a photo of a single horse running in a sports field, with a well-equipped rider on its back, seems they are in a competition, high quality, best quality, highly realistic, masterpiece\", with ControlNet and the first LoRA weight applied. Additionally, we concatenate the output feature of amalgamation with a feature extracted using DDPM [1]."}, {"title": "C.2.3 Semantic Segmentation on Bedroom-28", "content": "Convolutional features include:\n(i) A feature using fine-grained prompt \u201ca photo of a tidy and well-designed bedroom\u201d. It is found that quality prompts such as \"high quality\" will be interpreted as the quality of the room instead of the image, so such prompts are not utilized.\n(ii) A feature using ControlNet, with a"}]}