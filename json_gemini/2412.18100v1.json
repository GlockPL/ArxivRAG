{"title": "EvoPAT: A MULTI-LLM-BASED PATENTS SUMMARIZATION AND ANALYSIS AGENT", "authors": ["Suyuan Wang", "Xueqian Yin", "Menghao Wang", "Ruofeng Guo", "Kai Nan"], "abstract": "The rapid growth of scientific techniques and knowledge is reflected in the exponential increase in new patents filed annually. While these patents drive innovation, they also present significant burden for researchers and engineers, especially newcomers. To avoid the tedious work of navigating a vast and complex landscape to identify trends and breakthroughs, researchers urgently need efficient tools to summarize, evaluate, and contextualize patents, revealing their innovative contributions and underlying scientific principles. To address this need, we present EvoPat, a multi-LLM-based patent agent designed to assist users in analyzing patents through Retrieval-Augmented Generation (RAG) [1] and advanced search strategies. EvoPat leverages multiple Large Language Models (LLMs), each performing specialized roles such as planning, identifying innovations, and conducting comparative evaluations. The system integrates data from local databases, including patents, literature, product catalogous, and company repositories, and online searches to provide up-to-date insights. The ability to collect information not included in original database automatically is also implemented. Through extensive testing in the natural language processing (NLP) domain, we demonstrate that EvoPat outperforms GPT-4 [2] in tasks such as patent summarization, comparative analysis, and technical evaluation. EvoPat represents a significant step toward creating AI-powered tools that empower researchers and engineers to efficiently navigate the complexities of the patent landscape.", "sections": [{"title": "Introduction", "content": "Patents serve as critical repositories for technical innovation, detailing unique methodologies, designs, and applications.\nHowever, the explosion of intellectual property information has brought both opportunities and challenges for researchers\nand practitioners seeking to retrieve, analyze, and utilize patent knowledge. A tool to efficiently distill key insights from\npatents, such as identifying innovations, analyzing strengths and weaknesses, and comparing them with related patents\nis urgently needed. Nowadays, it is possible to craft such tools using artificial intelligence technology.\nAs the most potential member of artificial intelligence, Large Language Models (LLMs) are transformative tools for\nprocessing and understanding complex information across various domains [3, 4]. Their ability to comprehend nuanced\ntextual data, synthesize insights, and perform comparative analyses makes them particularly well-suited for patent\nanalysis. Although recent advancements in LLM-based systems have demonstrated their potential in summarizing\nacademic articles and generating research ideas, their application to the patent domain remains underexplored [5].\nExisting systems for patent analysis often focus on single-dimensional tasks [6], such as keyword extraction or text\nsummarization, failing to provide a comprehensive, structured understanding of the patent's content and its relationship\nto other patents.\nTo address these challenges, we introduce a multi-agent architecture that leverages the collaborative power of multiple\nLLMs to provide a comprehensive understanding of patents is required needily. The system is designed to analyze\npatent content holistically, extracting key innovations, pinpointing technical difficulties, identifying strengths and\nweaknesses, and performing horizontal comparisons with similar patents. Additionally, it offers structured summaries\ntailored to various user needs, including researchers, industry practitioners, and intellectual property analysts."}, {"title": "System Architecture", "content": "This section introduces EvoPat, a novel Multi-LLM-Based patents summarization and analysis agent that consists of\nthree parts: Data Preprocessing, Patent Analysis, and Output Integration."}, {"title": "Overview", "content": "As shown in Fig 1, EvoPat consists of three main phases: Data Preprocessing, Patent Analysis, and Output Integration.\nThe input to EvoPat is the source patent in any field, and the output would be a report including analysis and\nsummarization from multiple perspectives.\n\u2022 Data Preprocessing: Given the source patent, we first need to extract and normalize text to remove irrelevant\ncontent. Finally, we embed the text and store it in the Faiss database for future retrieval.\n\u2022 Patent Analysis: Our agent would analyze and summarize the processed patent text from multiple perspectives\nusing LLMs from 5 different roles: innovation points, implementation methods, technical details, horizontal\ncomparisons, and academic directions.\n\u2022 Output Integration: Our final step is to output a clear and easy-to-read patent report. We first convert the\noutput into Markdown [13] format based on different levels, then integrate the output and unify the format,\nand finally generate a PDF file as the patent analysis report."}, {"title": "Data Preprocessing", "content": "Traditional LLMs make it difficult to read patent PDF information directly, and patents still contain some irrelevant\ninformation and characters. Therefore, to improve the effectiveness of patent analysis, we need to preprocess the source\npatent by extracting and filtering the text from it."}, {"title": "Text Extraction", "content": "\u2022 Text-based PDF: Most existing patents are in text PDF format, which has a neat patent structure and can be\ndirectly extracted using existing open source tools [14, 15], with low time consumption and generally accurate\nextraction results.\n\u2022 Image-based PDF: A small number of patents are in image-based PDF format, characterized by each page of\nthe PDF resembling an image. Traditional PDF extraction tools cannot extract the contents, and currently, they\nrely on Optical Character Recognition(OCR) technology for extraction. However, OCR has some issues, such\nas high time consumption and insufficient accuracy in extracting text."}, {"title": "Text Filtering", "content": "The text directly extracted from the patent contains irrelevant content, which can affect the accuracy performance of\nLLM, reducing its effectiveness of response. Moreover, it would increase the time cost of LLM's response. Therefore,\nwe need to use normalized regular expressions to filter the text. The normalization criteria are as follows.\n\u2022 Remove special characters from the text except for normal punctuation marks.\n\u2022 Remove redundant information from text, such as HTML tags and URL links.\n\u2022 Remove common stop words such as 'the', 'is', etc."}, {"title": "Text Embedding", "content": "We need to store the patents in a database to facilitate subsequent retrieval. Since storing raw text directly would be\ninefficient, we opt to embed the text and store it in a vector database for a faster and more effective search.\nWe use the embedding model of the BGE-M3 [16] model. BGE-M3 is an open-source model designed for natural\nlanguage processing (NLP) tasks, particularly in semantic search, vector-based retrieval, and other applications requiring\ndense embeddings. It leverages pretraining and fine-tuning to produce bidirectional embeddings for capturing rich\nsemantic information in sentences or paragraphs. At the same time, it supports over 100 languages and has leading\nmultilingual and cross-language search capabilities, making it very suitable for patent embedding in various languages.\nFaiss [17] is an open-source library developed by Meta [18], designed for efficient similarity search and clustering\nof dense vectors at scale. It supports exact and approximate nearest neighbor (ANN) search, making it ideal for\napplications like recommendation systems, semantic search, and large-scale retrieval in natural language processing and\ncomputer vision. Faiss provides a variety of indexing methods to balance speed, memory usage, and accuracy. With\nGPU acceleration and support for billions of vectors, Faiss achieves high performance and scalability. Its integration\nwith machine learning workflows and adaptability to diverse datasets make it a robust tool for patent retrieval systems."}, {"title": "Patent Analysis", "content": "In this phase, EvoPat can analyze, summarize, and expand patent content utilizing Large Language models to analyze\npatents from five different perspectives."}, {"title": "Long Context Input", "content": "Considering that the content of patent text extracted and filtered by us may still be too long, if used directly as contextual\nprompts for extensive model analysis, it will inevitably face the problem of model token limitations, resulting in higher\ncosts and slower response times.\nCurrently, there are two different approaches to solving this problem. First, Autogen [19] introduced a method named\nTransform Messages. The Transform Messages capability is designed to modify incoming messages before the LLM\nprocesses them. This can include Message History Limitation and Token Limitation. For Message HistoryLimitation,\nthis strategy reduces the length of conversation history by keeping only the most recent messages, focusing on essential\ncontext, and improving processing efficiency. For Token Limitation, this strategy ensures that the input adheres to the\ntoken limits by controlling both the per message and the total token counts. It calculates the number of tokens in each\nmessage and truncates those exceeding the set limit. Therefore, we can combine these two strategies to ensure robust\nhandling of long conversation histories while adhering to model constraints.\nHowever, despite the effectiveness of the Transform Messages strategy in addressing long-text input issues by segmenting\nand treating previous segments as historical context, it faces two major challenges. First, treating the entire text as\nhistorical information can lead to forgetting, especially when the text is excessively long. Large models may easily\nforget or overlook portions of the information, resulting in less accurate and detailed results. Second, this approach is\ncostly, requiring sending many tokens each time, making the analysis more expensive and less efficient.\nAs a result, recent work has started focusing on text compression, aiming to retain only the key and essential information\nin the text, thereby meeting the model's token limit while maintaining analysis efficiency. LLMLingua [20] is a tool\ndesigned to compress prompts effectively, enhancing the efficiency and cost-effectiveness of LLM operations. Its goal is\nto construct a language exclusive to LLMs that may be hard for humans to grasp but can be easily understood by LLMs.\nSpecifically, LLMLingua leverages well-aligned, smaller language models like GPT-2 Small [21] and LLaMA-7B [22]\nto identify and remove unimportant tokens from prompts. This process transforms the prompt into a compressed format\nthat may be difficult for humans to interpret but remains entirely understandable for LLMs. The compressed prompts\ncan be directly applied to black-box LLMs, achieving up to 20x reduction in prompt size while maintaining nearly\nidentical performance in downstream tasks. This includes preserving LLM-specific capabilities such as in-context\nlearning (ICL) and reasoning. Therefore, using LLMLingua can effectively solve the problem of excessively long patent\ncontent and better assist in LLMs analysis."}, {"title": "Multi-agent System", "content": "The automated multi-LLM-based patent analysis agent comprises a group of large language models. In this study, we\nutilize the advanced GPT-40 from the GPT-4 family [2], accessed via the OpenAI API[23]. Each agent in the system\nis assigned a specific role and task, described by a unique configuration profile. The introduction of the agents in the\nteam is as follows: Here, we take Patent US20170263445A1 [24]as an example. Fig.2 illustrates the workflow of the\nmulti-agent system and the outputs provided by each scientist.\n\u2022 Innovation Points Scientist: The Innovation Points Scientist is responsible for identifying the most valuable\ninnovative methods within the patent. These innovations are critical for users as they determine whether they\nwish to explore the patent further.\n\u2022 Implementation Method Scientist: The Implementation Method Scientist presents the patent's implementation\nprocess to users. This helps users quickly understand the patent's workflow and enables them to assess the\ncomplexity of its realization.\n\u2022 Technical Detail Scientist: The Technical Details Scientist provides users with supplementary technical details\nof the patent's methods, such as specific numerical values, environmental conditions, and unique processes.\n\u2022 Horizontal Comparison Scientists: The Comparative Analysis Scientist offers the function in conducting\ninternet searches for similar patents using the Google Patents API [25]. This enables a comparative analysis\nthat highlights a patent's uniqueness relative to others.\n\u2022 Academic Direction Scientists: The Academic Direction Scientist is primarily responsible for conducting\nonline searches for related papers using the Semantic Scholar API [26]. This API facilitates the analysis of\ncurrent research trends in the academic community within this field, broadening the user's perspective."}, {"title": "Tool invoking", "content": "Due to LLMs' limited knowledge base, accessing data outside their training corpus often leads to hallucination, which\nis unacceptable for patent analysis tasks. Beyond local Retrieval-Augmented Generation(RAG) [1] methods, one of\nthe most common approaches is to retrieve relevant knowledge by calling external APIs, effectively expanding the\nmodel's knowledge base, reducing hallucination, and improving result reliability. In our work, we also leverage the\nGoogle Patents and Semantic Scholar APIs to retrieve related patents and papers. We adopted the AutoGen framework\nto register relevant tools for the agents. Each tool is defined as a Python function with a name, a description, and\nappropriately described input attributes, enabling the agents to call these tools accurately and effectively."}, {"title": "Output Integration", "content": "To facilitate user reading, all agent responses will be standardized into Markdown format. Markdown, as a lightweight\nmarkup language, allows users to write documents in an easy-to-read and easy-to-write plain text format, which is then\nconverted into valid HTML documents and ultimately transformed into a well-structured PDF file for further analysis"}, {"title": "Experiments", "content": "In this section, our experiments are centered on answering the following Research Questions (RQs):\n\u2022 RQ1: How does EvoPat perform in patent analysis?\n\u2022 RQ2: How significant is the impact of LLMLingua and Transform Messages on patent analysis?"}, {"title": "Experiment Settings", "content": ""}, {"title": "Dataset", "content": "We collected 5000 patents from the past decade in the field of science and engineering from Google Patents as the\ndataset for our experiments, covering four languages: Chinese, English, Japanese, and Korean. Google Patents is a\ncomprehensive patent database that includes the majority of patents worldwide. Its well-structured indexing facilitates\nefficient searches, and it provides high-quality original patent texts for download, making it an ideal resource for\nevaluating EvoPat."}, {"title": "Implementations", "content": "We run all experiments on a machine with 128G RAM, 16 cores of CPU and a RTX 4090 GPU. Phases of EvoPat are\nimplemented with OpenAI, BGE-M3, and AutoGen."}, {"title": "Metrics", "content": "In the field of automatic text summarization, evaluating the quality of a generated summary typically involves comparing\nit against a set of reference summaries, commonly known as gold summaries. Among the most widely used evaluation\nmetrics is Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [28], a comprehensive suite of metrics designed\nto quantify the overlap of n-grams (word sequences of length n) between generated and reference summaries. ROUGE\nhas gained popularity for its simplicity, effectiveness, and strong correlation with human judgment in summarization\ntasks. Notably, ROUGE-1, ROUGE-2, and ROUGE-L are frequently employed to assess both extractive and abstractive\nsummarization systems.\n\u2022 ROUGE-1 : ROUGE-1 evaluates the overlap of unigrams (single words) between a generated summary and its\nreference counterpart. It quantifies the shared words between the two summaries, providing an indication of\nthe extent to which the generated summary reflects the content of the reference summary.\n$$ROUGE-1=\\frac{\\sum_{w \\in generated} Count_{match}(w)}{\\sum_{w \\in reference} Count(w)}$$\nWhere:\nCountmatch(w) is the number of times word w appears in both the generated and reference summaries.\nCount(w) is the total number of times word w appears in the reference summary.\n\u2022 ROUGE-2 : ROUGE-2, akin to ROUGE-1, measures the overlap of bigrams (sequences of two consecutive\nwords) between the generated and reference summaries. By assessing the presence of common word pairs,\nthis metric goes beyond individual word matching to evaluate the generated summary's ability to preserve\nboth fluency and higher-order content structure, offering a more nuanced assessment than ROUGE-1.\n$$ROUGE-2=\\frac{\\sum_{bigram(w_1,w_2) \\in generated} Count_{match}(w_1, w_2)}{\\sum_{bigram(w_1,w_2) \\in reference} Count(w_1, w_2)}$$\nWhere:\nCountmatch (W1,W2) refers to the number of times the bigram (w1, W2) appears in both the reference and\ngenerated summaries.\nCount(w\u2081, w2) refers to the total number of occurrences of the bigram (w1, w2) in the reference summary."}, {"title": "", "content": "\u2022 ROUGE-L : ROUGE-L evaluates the Longest Common Subsequence (LCS) between a generated summary\nand its reference counterpart. In contrast to ROUGE-1 and ROUGE-2, which emphasize n-gram overlap,\nROUGE-L identifies the longest sequence of words shared by both summaries in their original order. By\ncapturing structural similarity, this metric is especially effective for assessing the fluency and coherence of the\ngenerated summary.\n$$ROUGE-L = \\frac{LCS \\ length}{Reference \\ length}$$\nWhere:\nLCS length is the length of the longest common subsequence between the generated and reference\nsummaries.\nReference length is the total number of words in the reference summary.\nBERTScore [29] is an embedding-based evaluation metric that builds upon METEOR [30]. It leverages cosine similarity\nto measure the alignment between tokens or n-grams in the generated summary and those in the reference. The metric\ncomprises three core components: Precision, Recall, and F1 score, each derived from the similarity of token embeddings\nproduced by a pre-trained BERT model.\n\u2022 BERTScore Precision : BERTScore Precision represents the average cosine similarity between each token in\nthe generated output and its closest counterpart in the reference summary. A higher cosine similarity indicates\na greater degree of alignment between the token in the generated summary and its corresponding token in the\nreference.\n$$BERTScore \\ Precision = \\frac{1}{N_{generated}} \\sum_{i=1}^{N_{generated}} cos(v^{(i)}_{generated}, V_{reference})$$\nWhere:\nNgenerated is the total number of tokens in the generated summary.\ncos(v generated, V reference) represents the cosine similarity between the embedding of the i-th token in the\ngenerated summary and its nearest match in the reference summary (denoted as i*).\n\u2022 BERTScore Recall : BERTScore Recall is defined as the average cosine similarity between each token in the\nreference summary and its most similar counterpart in the generated output. This metric evaluates the extent to\nwhich the generated summary effectively captures the tokens from the reference summary.\n$$BERTScore \\ Recall= \\frac{1}{N_{reference}} \\sum_{i=1}^{N_{reference}} cos(v^{(i)}_{reference}, V_{generated})$$\nWhere:\nNreference is the total number of tokens in the reference summary.\ncos(v reference, V generated) represents the cosine similarity between the embedding of the i-th token reference,\nin the\nreference summary and its nearest match in the generated summary.\n\u2022 BERTScore F1: BertScore F1 is calculated as the harmonic mean of Precision and Recall, providing a\nbalanced evaluation of these two metrics. It ensures that both the similarity between tokens in the generated\nand reference summaries, as well as the coverage of reference tokens, are adequately reflected.\n$$BERTScore \\ F1 = 2 \\times \\frac{BERTScore \\ Precision \\times BERTScore \\ Recall}{BERTScore \\ Precision + BERTScore \\ Recall}$$"}, {"title": "RQ1: Evaluation of Patent Analysis", "content": "We calculate the scores of patent analysis generated by EvoPat and GPT-40 based on the metrics in section 3.1.3. The\nresults are shown in Table 1\nThis demonstrates that the quality of patent analysis generated by EvoPat has significantly surpassed that of GPT-40,\nparticularly in terms of ROUGE score. The improvement can be largely attributed to our Multi-LLM-Based patent\nanalysis agent system, in which each agent focuses on a specific perspective, and agents collaborate by sharing historical\ninformation, thereby enhancing the quality and depth of the analysis. In contrast, GPT-4o is limited by its internal\nmodel and can only provide basic analysis."}, {"title": "RQ2: The Impact of Long-Text Processing", "content": "As discussed in Section 2.3.1, patent documents are often lengthy, frequently exceeding the token limits of large\nlanguage models. Strategies such as message transformation and text compression can help mitigate this issue. The\nTransform Messages strategy involves limiting the number of historical tokens and segments, sending the text in smaller\nparts to the LLMs for analysis. While this approach is generally accurate and minimizes the risk of information loss, its\nmain drawbacks are high costs and the potential for important historical context to be overlooked due to the length of\nthe text. In contrast, LLMLingua is a well-established open-source text compression tool that effectively compresses\ntext while preserving key information understandable by LLMs. This method not only reduces analysis costs but also\nenhances the efficiency of the analysis. Given that long-text processing primarily impacts ROUGE scores, as well as the\nInformative, Rich, and Attributable metrics, these indicators were selected for evaluation. The results are detailed in\nTable 3."}, {"title": "Conclusion and Discussion", "content": "In this paper, we introduced EvoPat, a Multi-LLM-Based Patent Summarization and Analysis Agent. EvoPat is\ndesigned with three main components: Data Preprocessing, Patent Analysis, and Output Integration. This system\npreprocesses and embeds patent files, analyzes the novelty of proposals from both scientific and market perspectives,\nand generates comprehensive reports. These reports include reviews, novelty evaluations, and summaries, offering\na holistic perspective on patent content. The unique architecture of EvoPat, which incorporates multiple specialized\nLLM systems, enables it to process and analyze thousands of patents within minutes. Extensive evaluations using\ndiverse metrics and expert assessments demonstrate that EvoPat outperforms GPT-4 in key dimensions, including\ninformativeness, richness, coherence, attribution, and extensibility.\nDespite these achievements, EvoPat faces limitations that will pave the way for future work. One major challenge is\ndata preprocessing, particularly with patent figures and multilingual text. Extracting meaningful connections between\nfigures and content from PDF files remains a significant hurdle. Improving figure recognition and context alignment\nwill be a priority. Another critical area of improvement is enhancing the connections between patents and academic\npublications. Identifying and explaining the scientific principles underlying patents requires robust knowledge graph\nconstruction and the integration of specialized agent roles.\nAdditionally, the temporal gap between emerging scientific trends in publications and their subsequent appearance in\npatents necessitates advanced time-series algorithms. These algorithms will help EvoPat generate more precise and\nforward-looking reports while mitigating issues like AI hallucination. By addressing these challenges, EvoPat aims to\nfurther advance patent analysis and provide even greater value to researchers, engineers, and decision-makers."}]}