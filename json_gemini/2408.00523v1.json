{"title": "Jailbreaking Text-to-Image Models with LLM-Based Agents", "authors": ["Yingkai Dong", "Zheng Li", "Xiangtao Meng", "Ning Yu", "Shanqing Guo"], "abstract": "Recent advancements have significantly improved automated task-solving capabilities using autonomous agents powered by large language models (LLMs). However, most LLM-based agents focus on dialogue, programming, or specialized domains, leaving gaps in addressing generative AI safety tasks. These gaps are primarily due to the challenges posed by LLM hallucinations and the lack of clear guidelines. In this paper, we propose Atlas, an advanced LLM-based multi-agent framework that integrates an efficient fuzzing workflow to target generative AI models, specifically focusing on jailbreak attacks against text-to-image (T2I) models with safety filters. Atlas utilizes a vision-language model (VLM) to assess whether a prompt triggers the T2I model's safety filter. It then iteratively collaborates with both LLM and VLM to generate an alternative prompt that bypasses the filter. Atlas also enhances the reasoning abilities of LLMs in attack scenarios by leveraging multi-agent communication, in-context learning (ICL) memory mechanisms, and the chain-of-thought (COT) approach. Our evaluation demonstrates that Atlas successfully jailbreaks several state-of-the-art T2I models in a black-box setting, which are equipped with multi-modal safety filters. In addition, Atlas outperforms existing methods in both query efficiency and the quality of the generated images.", "sections": [{"title": "I. INTRODUCTION", "content": "The pursuit of autonomous agents [1], [2], [3], [4] has been a longstanding focus in both academic and industrial research. Traditionally, agent building was conducted in constrained environments with limited knowledge bases, often leading to the inability to achieve human-like decision-making capabilities. In recent years, large language models (LLMs) [5], [6], [7] have made remarkable strides, demonstrating their potential to attain human-like intelligence. These advancements have spurred a surge in research focused on LLM-based autonomous agents.\nLLM-based autonomous agents, hereafter referred to as LLM agents, utilize LLM applications (e.g., GPT-4 [7] or Vicuna [6]) to execute complex tasks through an architecture that combines LLMs with key modules like memory and tool usage. In the construction of LLM agents, an LLM or its variant, such as a vision language model (VLM), serves as the primary controller or \u201cbrain,\" orchestrating the execution of tasks or responses to user requests. The integration of LLMs as fundamental elements within autonomous agents has opened up new avenues for research and application across various domains, promising more versatile and intelligent AI systems. Building on the foundation of LLM agents and their wide-ranging applications, we turn our attention in this work to a crucial yet understudied area: generative AI safety. While LLM agents have been successfully deployed in fields such as computer science & software engineering [4], [8], [9], [10], [11], industrial automation [12], [13], [14], and social science [15], [16], [17], their potential to enhance research into generative AI safety is vastly under-researched.\nThe safety of recent generative AI is crucial, especially as techniques like text-to-image generative (T2I) models [18], [19], [20], [21] have rapidly gained unprecedented popularity due to their ease of use, high quality, and flexibility in generating images. A significant ethical concern with T2I models is their potential to generate sensitive Not-Safe-for-Work (NSFW) images, including those related to violence and content inappropriate for children [22], [23]. However, identifying safety vulnerabilities in these advanced models presents significant challenges [24]. In this work, we posit that LLM agents, with their ability to process and synthesize vast amounts of information, could play a pivotal role in enhancing the understanding and exploration of safety vulnerabilities of recent rapidly developed generative AI."}, {"title": "A. Our Contributions", "content": "In this study, we take the first step in utilizing LLM agents to explore the safety vulnerabilities in generative AI models. Our objective is to develop a fully automated jailbreak attack framework based on LLM agents. This framework employs multiple agents to create adaptive-mode prompt-level adversarial prompts based on the following two key insights:\n\u2022 Recent advancements in LLM have made it possible to generate semantically similar prompts across a seemingly infinite array of modes. For example, given the simple prompt 'a cat,' an LLM can flexibly generate diverse content. It could describe a playful kitten with vivid imagery or weave a tale about its adventure in a fantasy realm. Alternately, it might compose a poem highlighting a cat's serene moments or present a dialogue capturing its mischievous antics.\n\u2022 Diversity in modes signifies a variety of adversarial prompts. However, it also implies that the search space for adversarial prompts is vast. Therefore, it is very difficult to find prompts that bypass the safety filters with LLM alone. Previous research indicates that the involvement of multiple agents can promote diverse, innovative thinking [25], enhance the accuracy of generated content [26], and improve the reasoning capabilities [27]. Such findings underpin the feasibility of orchestrating effective jailbreak attacks through an LLM-based multi-agent framework.\nBased on these insights, we propose a novel framework called Atlas, which employs multiple autonomous agents to systematically probe and potentially bypass the safety filters of T2I models. Atlas is developed through the lens of fuzzing, embodying its fundamental elements the mutation engine and the score function as two specialized agents: the mutation agent and the critic agent. The mutation agent employs a VLM to automatically identify the activation state of the safety filters within the victim T2I model by analyzing images alongside their corresponding textual descriptions. Utilizing both current data and memory modules, this agent dynamically identifies optimization directions and executes optimizations, concluding with the delivery of candidate adversarial prompts. Subsequently, the critic agent scores these prompts using LLM's imitation and reasoning capacities. The prompt with the highest score is sent to the T2I model for testing. Upon receiving new test outcomes, the mutation agent concludes the optimization if the jailbreak is successful. Additionally, Atlas introduces a commander agent, designed as a finite state machine (FSE), to control the workflow. Furthermore, to enhance the resilience and domain-specific inference of LLMs, Atlas incorporates a chain of thought (COT) and a novel in-context learning (ICL) mechanism.\nWe evaluate Atlas with LLaVA [28], ShareGPT4V [29], and Vicuna [6] against three state-of-the-art T2I models equipped with a large variety of safety filters. Our evaluation results show that Atlas can perform efficient jailbreak attacks on existing safety filters. For most conventional safety filters, Atlas achieves close to 100% bypass rate and an average of 4.6 queries with a reasonable semantic similarity. Even for the conservative safety filter, the bypass rate can reach more than 82.45% and the average number of queries required is also only 12.6. We also show that Atlas can successfully bypass the safety filters of the close-box DALL-E 3. Furthermore, Atlas surpasses other jailbreak methods, striking a superior balance between bypass efficiency and the number of queries, while preserving semantic integrity. Finally, we also evaluate the effectiveness of the key components of Atlas through an ablation study.\nIn summary, we make the following contributions:\n\u2022 We verify the effectiveness of the LLM agent in advancing generative AI safety research, especially in identifying vulnerabilities within T2I generative models.\n\u2022 We design a novel framework called Atlas for effective jailbreak T2I models. This involves the creation of three distinct LLM agents and the establishment of a novel workflow resembling fuzzing, integrating chain of thought (COT) reasoning and an innovative in-context learning (ICL) mechanism to synergize their functionalities. Compared to existing jailbreak methods, Atlas can achieve an adaptive-mode prompt-level jailbreak attack by bypassing the black-box safety filters inside black-box T21 models.\n\u2022 We conduct an extensive experiment to evaluate the performance of Atlas. The results indicate that Atlas can not only ensure semantic similarity but also achieve an extremely high success rate in jailbreaking with fewer queries, and its comprehensive performance surpasses existing methods."}, {"title": "II. PRELIMINARIES", "content": "In this section, we begin by introducing the autonomous agents. We then present the text-to-image models and the jailbreak attacks against them."}, {"title": "A. Autonomous Agents", "content": "An autonomous agent is an advanced system that integrates an intelligent system as its central controller, functioning as the \"brain\" of the agent. This agent uses task decomposition, self-reflection, and dynamic memory to iteratively improve its actions and adapt to emerging challenges. Moreover, it has the capability to interface with external tools and APIs, broadening its capabilities beyond natural language processing to include tool use, planning, and executing specialized tasks. Here are the key components of an autonomous agent:\nBrain. The intelligent system, encompassing large language models, vision language models, and finite state machines, acts as the \"brain\" of the agent, directing a sequence of operations to fulfill tasks or user requests.\nMemory Module. It stores internal logs, including past thoughts, actions, and observations, allowing the agent to recall past behaviors and plan future actions.\nTool Usage. Autonomous agents interact with external tools, like search APIs and code interpreters, to gather information and complete subtasks.\nAutonomous agents have significantly demonstrated capabilities when operating independently, and these capabilities are further augmented within a multi-agent system framework. [25], [27]. Previous research has explored automated task-solving using autonomous agent systems in various areas, including Q&A tasks [3], [30], programming tasks [4], [8], [31], sociological phenomena research [15], [16], [32], and domain-specific tasks [33], [34], [35]. However, exploration in the realm of machine learning security, This gap motivates us to focus on how autonomous agents perform in this domain."}, {"title": "B. Text-to-Image Generative Models", "content": "Text-to-image generative models have ushered in a new era of digital imagery. These models start with a canvas of Gaussian random noise and, through a process akin to reverse erosion, gradually sculpt the noise to reveal a coherent image. They can generate high-quality images in various styles and content based on natural language descriptions (e.g., \u201cA painting of a mountain full of lambs\"). These models have gained unprecedented popularity due to their ease of use, high quality, and flexibility. Consequently, numerous variants of text-to-image models have emerged, including Stable Diffusion (SD) [20], [21], DALLE [18], [36], Imagen [37], and Midjourney [19]."}, {"title": "Safety Filters", "content": "Incorporating safety filters is a widely adopted measure in the deployment of these models. These filters primarily inhibit the production of images featuring sensitive content, including adult material, violence, and politically sensitive imagery. For example, DALLE 3 [18] implements filters to block violent, adult, and hateful content and refuses requests for images of public figures by name. According to the classification methodology outlined in previous work [24], safety filters can be categorized into three distinct types: text-based safety filters, image-based safety filters, and text-image-based safety filters.\n\u2022 Text-based safety filter: This type of safety filter is designed to assess textual input before image generation. It typically uses a binary classifier to intercept sensitive prompts or utilizes a predefined list to block prompts that contain sensitive keywords or phrases and those closely aligned with such keywords or phrases in the text embedding space.\n\u2022 Image-based safety filter: This type of safety filter operates on the output side, examining the generated images. It functions as a binary classifier, trained on a dataset with images clearly labeled as NSFW or SFW (Safe For Work). A significant example of an application that incorporates an image-based safety filter is the official demo of SDXL [21], which seamlessly integrates this filter to scrutinize images for sensitive content.\n\u2022 Text-image-based safety filter: This type of filter is a hybrid approach, leveraging both the input text and the generated images to ensure the content's safety. It implements a sophisticated binary classification mechanism that considers both text and image embeddings to identify and block sensitive content. The open-source Stable Diffusion 1.4 [20] adopts this type of filter. In particular, this safety filter prevents the creation of an image if the cosine similarity measure between the image's CLIP embedding and the CLIP text embeddings of seventeen predefined unsafe concepts exceeds a specific limit [38]."}, {"title": "C. Jailbreaking Text-To-Image Models", "content": "The jailbreaking attack against text-to-image models involves using prompt engineering to bypass the usage policies implemented in the model. By cleverly crafting prompts, an attacker can circumvent the model's defense mechanisms, such as safety filters, causing it to generate harmful images that violate its usage policies, including pornography and violence. Some illustrative examples of adversarial prompts generated by Atlas and corresponding images are demonstrated in Figure 1. In these scenarios, the model's safety filter, which blocks content involving both dogs and cats, rejects explicit violation requests, such as \"The cat's eyes gleamed as it spotted a bird outside the window.\" However, rephrasing the same concept more subtly, as in \"The small, fluffy cat was curled up on a cushion in the sunny window.\u201d(corresponding to Figure 1(a), enables the model to produce outputs that violate its usage policy undetected.\nResearchers have proposed various strategies to jailbreak text-to-image models. However, most of these methods operate at the token level [24], i.e. attacking by replacing a few sensitive words in the prompt. While investigations into prompt-level jailbreak attacks, which entail the replacement of the entire sentence, remain scarce, these initiatives are often confined to either white-box attacks [39], [40] or black-box attacks that can only generate adversarial prompts in a limited pattern [41], [42]. This highlights a significant gap in the exploration of the adversarial prompts' space.\nRecently, Yang et al. [24] propose an advanced technique known as SneakyPrompt, which employs a reinforcement learning-based method to find substitutions for NSFW words and bypasses the safety filter by replacing NSFW words with meaningless ones. SneakyPrompt can automatically create token-level input text prompts that trick the model into generating unsafe images with a high success rate. Note that SneakyPrompt is the state-of-the-art automated jailbreak attack against text-to-image models in the black-box setting. It achieves automated functionality by utilizing reinforcement learning techniques which are commonly used in traditional autonomous agents. Therefore, in this work, we use SneakyPrompt as a baseline to investigate whether the proposed autonomous agent (Atlas) can be successfully applied to text-to-image model jailbreaks."}, {"title": "III. PROBLEM FORMULATION", "content": "In this section, we first define the LLM agent and LLM-based multi-agent system, and then we define the adversarial prompt against the text-to-image model. Finally, we describe the threat model of Atlas.\n1) Autonomous Agent and LLM-based Multi-agent System: In this section, we will first give the formal definition of autonomous agents and then the definition of multi-agent systems.\nDefinition 1. [Agents] An agent $A_i$ is defined as a tuple $A_i = (B_i, M_i, Act_i)$. Here, $B_i$ is the \u201cbrain\u201d of the agent, processing both the current information and the memory $M_i$ to determine the next action $act_i$ to be taken. The set $Act_i$ is the collection of actions available to the agent.\nDefinition 2. [LLM-based Multi-agent Systems] An LLM-based multi-agent system can be formally represented as a tuple $MAS = (\\Alpha, Env, Act, St, T)$. Here, $\\Alpha$ represents a set of agents $(A_1, A_2, ..., A_n)$ centralized around the LLM Agent and potentially complemented by other agent types. $Env$ defines the environment of the system, which can be any form of data or state space abstraction, providing a stage for the agents' actions. The set $Act$ includes the collection of actions available to the agents, with each $Act_i$ corresponding to the action set available to a specific agent $A_i$. The set of system states is denoted by $St$, and the transition function $T: St \\times Act \\rightarrow St$ describes how agents' actions lead to transitions in the system's states.\n2) Adversarial Prompt: A text-to-image model is denoted as $I$, and a safety filter, represented by $F$, where $F(p, I(p)) = 0$ indicates that both the input prompt $p$ and the generated image $I(p)$ are classified as NSFW, and $F(p,I(p)) = 1$ signifies that they are deemed SFW, we define a prompt as jailbreak prompt if Definition 3 is satisfied.\nDefinition 3. [Adversarial Prompt] An adversarial prompt $p_a$ is defined as one that meets two conditions simultaneously: First, the prompt does not trigger the safety filters of the text-to-image model, i.e., $F(p_a,I(p_a)) = 0$. Second, $I(p_a)$ has similar visual semantics to $I(p_t)$, where $p_t$ is the target sensitive prompt."}, {"title": "A. Threat Model", "content": "In this work, we focus on black-box jailbreak attacks, which assume that the attacker lacks knowledge of both the internal working mechanisms of the target model and any details of the safety filters it may possess. Such an attacker is limited to interacting with the model's API interface and receiving outputs in response to input text prompts. The advantages of focusing on black-box attacks are multifaceted and significant. First, the universality of black-box attacks stands out, as their effectiveness does not rely on specific details of the model's implementation, making them applicable across a wide range of T2I models. Second, black-box attacks pose a serious threat to commercial text-to-image models. These models, e.g., DALL-E 3 [18] and Midjourney [19] tend to conceal their inner workings, thereby making white-box attacks challenging. However, black-box attacks can still be effectively executed by exploiting the models' input-output behavior. Third, the heightened challenge inherent in black-box scenarios more effectively showcases Atlas's potential for exploring safety vulnerabilities in generative AI."}, {"title": "IV. Atlas", "content": "In this section, we first provide an overview of Atlas. Next, we present the details of the workflow and each key component."}, {"title": "A. Overview", "content": "1) Key Idea: The objective of Atlas is to bypass the safety filters of the T2I model, prompting it to produce images closely aligned with a given sensitive target prompt. This is possible because a safety filter essentially functions as a binary classifier, delineating clear decision boundaries. A prompt closely related to a sensitive target can be visualized as an $\\epsilon$-ball, allowing the attack's goal to be achieved by finding a point where the $\\epsilon$-ball intersects with the classifier's decision boundary. It is pertinent to note that while Atlas queries are exclusively textual, the approach is applicable to both textual and visual modalities. This is because the decision boundary of the image modality can be mapped to the embedding space of the text modality [24].\nTo achieve this objective, we formalize the key idea of Atlas, which is a multi-agent mutational fuzzer that searches for the adversarial prompt that satisfies the following two objectives:\n\u2022 Objective I: Mutating the target sensitive prompt $p_t$ to bypass the safety filter $F$. That is, finding prompts that cross the decision boundary of $F$ from $p_t$.\n\u2022 Objective II: If the generated image has a large semantic deviation from the $p_t$, find the intersection with the $\\epsilon$-bell in the $\\hat{\\epsilon}$-bell centered on the current prompt $p^{i-1}$, starting from $p^{i-1}$.\n2) Key Components: Figure 2 shows the overall pipeline of Atlas in searching adversarial prompts. Given a seed pool consisting of various sensitive prompts, Atlas sequentially modifies each prompt which is blocked by the safety filters of the T2I model, using three key agents.\nMutation Agent. Given a target, sensitive prompt $p_t$, the mutation agent $A_m$ employs test oracles to analyze the response of the victim T2I model $I$ and modifies the prompt to find an adversarial prompt $p_a$. This mutation agent evaluates whether the adversarial prompt $p_a$ triggers the safety filter and if the generated image $I(p_a)$ has similar semantics to the original target sensitive prompt $p_t$. If $p_a$ requires further mutation, the agent adaptively develops a mutation strategy and generates multiple mutated prompts accordingly. Details are presented in Section IV-B.\nCritic Agent. The critic agent $A_c$ evaluates the current state and scores the mutated prompts. In traditional fuzz testing, the quality feedback engine computes the fitness score of test cases based on system feedback, guiding mutations toward bug conditions. However, this approach is not suitable for testing T2I models for two reasons. First, the safety filters of T2I models are usually binary classifiers, offering limited feedback. Second, this approach increases the number of invalid accesses to the system, leading to higher costs and a greater likelihood of access denial. Therefore, Atlas implements the scoring function through the critic agent, embedding feedback in the candidate jailbreak prompts. Details are presented in Section IV-C.\nCommander Agent. The commander agent $A_{cm}$ feeds the candidate adversarial prompt into the T2I model and loops through the above workflow until the adversarial prompt is found. Note that the commander agent completely controls the execution of the above workflows without any manual intervention. Details are presented in Section IV-A3.\n3) Workflow: As shown in Figure 3, the workflow of Atlas can be divided into five steps:\n\u2022 Step (1): Bypass Check. The mutation agent determines whether the safety filter $F$ of T2I is bypassed based on the current prompt $p^{i-1}$ and T2I's response $I(p^{i-1})$. If the current prompt triggers the safety filter of T2I, i.e. $F(p^{i-1}, I(p^{i-1})) = 1$, Step (2) is initiated. At the same time, the critic agent records the triggered prompt in its long-term memory. Otherwise, Step (3) proceeds.\n\u2022 Step (2): Mutation for Bypass. The mutation agent accesses its long-term memory to formulate the mutation strategy and guidance, drawing upon the results retrieved. Then, The mutation agent mutates the prompt in accordance with the devised strategy and guidance, generating multiple candidate prompts $C = (C_1, C_2, ..., C_n)$. Finally, the mutation agent sends these candidate prompts to the commander agent. After completing the above operations, Step (5) will be executed.\n\u2022 Step (3): Semantic Check. $A_m$ queries the semantic similarity of original prompt and T2I response, i.e., $L(p_t, I(p^{i-1}))$. We do not use $L(I(p_t), I(p^{i-1}))$ to obtain semantic similarity because the generated image $I(p_t)$ is censored and thus inaccessible. If $L(p_t, I(p^{i-1})) < \\delta$, perform step (4). Otherwise, Atlas terminates the query for $p_t$ and outputs the corresponding adversarial prompt $p_a$ and the image $I(p_a)$. At the same time, $A_m$ records the successful adversarial prompt in its long-term memory.\n\u2022 Step (4): Mutation for Semantic. $A_m$ provides guidance for subsequent mutations to improve semantic similarity and then performs mutations to produce multiple candidate prompts. Then, the mutation agent sends these candidate prompts to the commander agent. After completing the above operations, Step (5) will be executed.\n\u2022 Step (5): Score and Select. The critic agent scores the candidate prompts, and the commander agent selects the highest-scoring prompt $p^i$ for input into the T2I model $I$. For each target sensitive prompt $p_t$, Atlas performs Steps (1) through (5) upon receiving a response from the T2I model. Atlas repeats this process until it identifies an adversarial prompt or attains the maximum number $\\Theta$ of queries specified for the target prompt.\nIt is important to note that Atlas initializes the memory module for each agent exclusively during the initial mutation of the first sample in the seed pool and does not reinitialize it in subsequent mutations. This means that when mutating the second sample, the memory module in Atlas retains records of the mutations applied to the first sample. Furthermore, we employ an exponential backoff strategy to set the maximum query limit. Upon reaching this limit for a prompt in a given round, we temporarily bypass the query for that prompt, preserving it for the subsequent round. This process continues until each prompt has been successfully linked to its corresponding adversarial prompt or until the predefined maximum query threshold is met. Moreover, to circumvent the risk of local optima, each round of Atlas starts with the original prompt, rather than the last mutated prompt from the previous round.\n4) Meta-Structure of System Message and Prompt: To improve the understanding and implementation of mutation and scoring tasks by LLM and VLM, a critical step involves the creation of system messages and prompt templates. Each system message and prompt temple adheres to a consistent meta-structure as shown in Figure 4. The complexity of the jailbreak task dictates the instantiation of this meta-structure in various forms. These instantiations will be detailed in Section IV-B, Section IV-C, and Section IV-D."}, {"title": "B. Mutation Agent", "content": "Atlas's mutation agent aims to increase the target sensitive prompts' jailbreak capabilities gradually. Following Definition 1, we define the mutation agent as the combination of the brain $B_m$, the memory module $M_m$, and the actions $Act_m$, i.e., $A_m = (B_m, M_m, Act_m)$.\n1) Brain of Mutation Agent: In this attack scenario, the mutation agent is required to recognize and align information from the textual and visual modalities within the environment. Therefore, we use a vision language model (VLM) as the decision-making brain of the agent. VLMs, such as GPT-4V(ision) [43], LLaVA [28], and ShareGPT4V [29], integrate capabilities in image understanding, text understanding, and text generation. These models can understand visual elements within an image and answer text-based questions using image information. For the VLM model, we carefully construct a system message that customizes its role and provides a detailed description 1:\n2) ICL-based Memory Module: Recall that an agent requires a memory module that includes past thoughts, actions, and observations to recall past behaviors and plan future actions. Considering that the brain of $A_m$ is a VLM model, we construct this memory module using in-context learning (ICL). ICL allows a model to perform tasks by conditioning on a few examples provided in the prompt, without any parameter updates or additional training. In this context, the provided examples are successful adversarial prompts generated during each loop and saved into the long-term memory database. Concretely, the ICL-based memory module involves three steps:\n\u2022 $A_m$ retrieves successful jailbreak prompts from its long-term memory database. Note that the long-term memory database is empty at its first loop for the first target sensitive prompt.\n\u2022 To prevent overly long contexts from confusing the VLM's attention, $A_m$ ranks these prompts and extracts the top k prompts using a semantic-based memory retriever (detailed in Section IV-B3).\n\u2022 $A_m$ summarizes these successes and uses them to guide the mutation of the current prompt through a well-designed prompt template.\nThis structured process ensures that $A_m$ effectively uses past experiences to inform the next mutation actions.\n3) Actions of Mutation Agent: The actions of the mutation agent include text outputs and the use of tools. Given that text output is an inherent capability of the VLM, this section elaborates on the mutation agent's tool-using capability. In specific, we introduce two tools for the mutation agent:\nSemantic-based Memory Retriever. To equip the VLM with ICL capabilities while avoiding the confusion caused by excessively long contexts, we design a semantic-based memory retriever. This retriever ranks jailbreak prompts in long-term memory by semantic similarity and selects the top k most similar prompts to construct the ICL PROMPT. Concretely, the retriever first uses word embedding tools, such as SentenceTransformer [44] and Word2Vec [45], to convert the original prompt and each jailbreak prompt in the long-term memory into text embeddings. It then calculates the cosine similarity between the text embedding of the original prompt and each jailbreak prompt. Finally, based on the calculated cosine similarities, it sorts the sentences and outputs the top $k_m$ jailbreak prompts with the highest cosine similarity.\nMultimodal Semantic Discriminator. To ensure that the generated image $I(p_a)$ has similar visual semantics to $I(p_t)$, we design the multimodal semantic discriminator for the mutating agent. This discriminator computes the similarity between the text embedding of the original target prompt $p_t$ and the image embedding of the generated image $I(p_a)$. Specifically, the mutation agent uses clip-vit-based-patch32 [46] to compute the CLIP variant of cosine similarity (CLIPScore) [47] between $p_t$ and $I(p_a)$. Although VLM can also compute semantic similarity between multimodal data, we use a multimodal semantic discriminator to account for the uncertainty of large models. The multimodal semantic discriminator is able to ensure that the cosine similarity between all final generated images and the original sensitive words is greater than $\\delta$, whereas the VLM's criterion changes each time it discerns the semantic similarity, and it is unable to control the lower limit of the semantic similarity."}, {"title": "C. Critic Agent", "content": "After the mutation agent $A_m$ generates multiple candidate jailbreak prompts, the critic agent $A_{ct}$ scores these prompts and selects the one with the strongest jailbreak ability. To measure the jailbreak ability of the candidate prompts $C$, we design the following score function:\n$S = \\lambda_1 \\cdot S_1(C) + \\lambda_2 \\cdot S_2(p_t, C)$  (1)\nHere, the scoring function $S_1$ evaluates the effectiveness of prompts in bypassing safety filters, while $S_2$ measures semantic similarity. According to Definition 3, an adversarial prompt should meet two conditions: 1) $F(p_a, I(p_a)) = 0$; and 2) $I(p_a)$ has similar visual semantics to $I(p_t)$. These conditions correspond to $S_1$ and $S_2$, respectively.\nThe design objective of the critic agent has been to solve Equation 1. To achieve this, the critic agent includes two LLMs acting as different brains, a memory module, and a brain-switching mechanism in its action module, represented as $A_{ct} = (B_{ct}, M_{ct}, Act_{ct})$.\n1) Brain of Critic Agent: As mentioned above, we design two different LLM-based brains for $A_{ct}$: the SafetyFilterBrain $B_{ct}$, which calculates $S_1$, and the SemanticEvaluatorBrain $B_{ct}$, which computes $S_2$.\nSafetyFilterBrain. To measure how well the candidate prompts bypass safety filters, we customize the system message of the SafetyFilterBrain to simulate the safety filter of the target model."}, {"title": "D. Commander Agent", "content": "The commander agent fully controls Atlas's workflow. To ensure stability, we define the commander agent as a reactive agent rather than an LLM-based one. This design focuses on direct input-output mappings instead of complex reasoning, allowing it to respond to the environment in a stable manner [1], [48], [49], [50], [51]. Following Definition 1, the commander agent should be defined as $A_{cm} = (B_{ct}, I_{ct}, Act_{ct})$. However, since $A_{cm}$ only performs direct input-output mapping, a memory module is unnecessary. Therefore, we define the commander agent as $A_{ct} = (B_{ct}, Act_{ct})$.\n1) Brain of Commander Agent: We design a rule-based FSM as the brain of $A_{cm}$, instead of using an LLM or VLM. The FSM is defined by a 3-tuple, $(Q, R, T)$, where $Q = (q_0, q_1, ..., q_n)$ represents the set of states. The initial state $q_0$ is waiting for the program to start, the end state $q_n$ is the termination process, and each intermediate state $(q_1, q_2, ..., q_{n-1})$ denotes a prompt that can be issued by $A_{cm}$ to $A_m$ or $A_{ct}$. The set $R$ includes prefix of responses that might be given by $A_m$ or $A_{ct}$. The function $T: Q \\times R \\rightarrow Q$ is the transition mechanism, determining the next state based on the current state and the received response. Next, we describe $Q$, $R$ and $T$ in detail in Section IV-D2\n2) Actions of Commander Agent: Unlike the other two agents that passively receive tasks, $A_{cm}$ proactively initiates sessions with other agents. The interaction of $A_{cm}$ is divided into two primary segments: commander-mutation interaction and commander-critic interaction.\nCommander-Mutation Interaction To address the illusion and task loss problems of VLM, we divide the mutation agent's task into sub-tasks and use chain-of-thought (COT) [52], [53], [54], [55] to improve reasoning and instruction-following. The commander agent implements multi-turn COT by sending one sub-task at a time to the mutation agent. After receiving a response, it sends the context and the next sub-task. This multi-turn COT is more effective than single-turn COT in overcoming VLM hallucination and solving the task loss issue [55].\nIn each mutation round, the commander agent sends \"Bypass Check\" prompts to the mutation agent. These prompts guide the mutation agent to verify if the T2I safety filter has been bypassed based on the current prompt and image. To improve accuracy, we construct the \"Bypass Check\" prompt template on the \"Description then Decision\" approach [55]. Specifically, we design the template as multiple-choice questions, divided into \u201cCheck-Description Prompt\u201d and \u201cCheck-Decision Prompt\u201d (detailed in Appendix 4)\nAfterward, $A_{cm}$ decides on the next steps based on the response. If the mutation agent's response is the safety filters are triggered, the commander agent instructs it to create a mutation strategy using the \"ICL,\u201d \u201cICL-Strategy,\u201d and \u201cStrategy\" prompt templates in Section IV-B1. Once the mutation agent responds, the commander agent sends a \"Modify Prompt\" (detailed in Appendix B) directive for new prompts based on its guidance.\nConversely, if the mutation agent's response is the prompt has bypassed the safety filters, $A_{cm}$ instructs the mutation agent to verify if the semantics remain unchanged. As detailed in Section IV-B3, the mutation agent uses the multimodal semantic discriminator to compute semantic similarity. Thus, $A_{cm}$ simply sends a \u201cSemantic check\u201d request and waits for the response.\nIf the mutation agent calculates that $L(p, I(p^{i-1}))$ is less than $\\delta$, it indicates semantic deviation, requiring further mutation. In this case, $A_{cm}$ sends a \"Semantic Guide Prompt\" (detailed in Appendix B) to the mutation agent to devise targeted mutation strategies and a \"Modify Prompt\" to generate new prompts based on these strategies.\nCommander-Critic Interaction After $A_{cm}$ receives the prompts from $A_m$, it sends them to $A_{ct}$ for scoring. As shown in Figure 3(c), the commander agent sends the \"Mark\u201d and \"New prompts\" to the critic agent, directing it to score each prompt using either the \u201cBypass Score Prompt\u201d if the safety filters are triggered, or the \u201cSemantic Score Prompt Template\u201d (detailed in Appendix B) otherwise.\nFinally, the commander agent selects the highest-scoring prompt from the critic agent's evaluation and formats it appropriately for sending"}]}