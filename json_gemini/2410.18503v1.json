{"title": "SFB-NET FOR CARDIAC SEGMENTATION: BRIDGING THE SEMANTIC GAP WITH ATTENTION", "authors": ["Nicolas Portal", "Nadjia Kachenoura", "Thomas Dietenbeck", "Catherine Achard"], "abstract": "In the past few years, deep learning algorithms have been widely used for cardiac image segmentation. However, most of these architectures rely on convolutions that hardly model long-range dependencies, limiting their ability to extract contextual information. In order to tackle this issue, this article introduces the Swin Filtering Block network (SFB-net) which takes advantage of both conventional and swin transformer layers. The former are used to introduce spatial attention at the bottom of the network, while the latter are applied to focus on high level semantically rich features between the encoder and decoder. An average Dice score of 92.4 was achieved on the ACDC dataset. To the best of our knowledge, this result outperforms any other work on this dataset. The average Dice score of 87.99 obtained on the M&M's dataset demonstrates that the proposed method generalizes well to data from different vendors and centres.", "sections": [{"title": "1. INTRODUCTION", "content": "Segmentation of cardiac structures on medical images gives crucial information to diagnose cardiomyopathies as it allows to accurately delineate structures targeted by the disease and monitor their remodelling. For example, inherited or acquired cardiomyopathies can be more easily diagnosed by doctors assisted by effective segmentation software. Magnetic resonance imaging (MRI) is useful to detect such diseases as it provides high contrast, resolution, and anatomical coverage, all without radiation and thus with low risk for patients. Nowadays, many segmentation algorithms rely on deep learning methods as they achieved good results on computer vision tasks. The U-net architecture [1] is often used in practice as it proved to be an effective design to perform semantic segmentation. This architecture effectively fuses high-resolution information carried by the encoder with semantically-rich features of the decoder. However, the simple concatenation of feature maps coming from the encoder and the decoder has been shown to be suboptimal [2]. Indeed, feature maps coming from the encoder, though containing very fine-grained local details, carry less semantically rich features than feature maps of the decoder. As a result they present a non-negligible amount of noise, but cannot be ignored because they contain information on details, with a high spatial precision. This phenomenon is known as the semantic gap. Accordingly, this work focuses on designing a new mechanism able to bridge this semantic gap by filtering out noise found in the encoder feature maps and focusing training on regions with strong response in the semantically-rich feature maps of the decoder."}, {"title": "1.2. Related work", "content": "Ronneberger et al. [1] introduced the U-net architecture which consists of an encoder to aggregate contextual information, a symmetric decoder to enable precise localization, and skip connections between the encoder and the decoder to exploit local information contained in high-resolution feature maps.\nRecently, attention mechanisms have been extensively used to improve neural networks performance. [3] integrated both spatial and channel attention at the end of the network to benefit from global contextual information. Following [4] who showed that the multi head self-attention mechanism used by transformers has the ability to increase the receptive field of the network even at shallow layers, [5] used these transformer blocks at the bottom of the U-net network either in a 2D or 3D configuration improving the performance of medical image segmentation algorithms. Differently, [6] use convolutions to reduce the spatial resolution of feature maps, to subsequently incorporate self-attention at higher levels in the encoder. [7] introduced a window and shifted window attention mechanism which improves performance over traditional transformer blocks while reducing the computational complexity. As a result, these blocks have been applied successfully to both 2D [8] and 3D [9] medical image segmentation. Attention has also been applied in the skip connection paths of the U-net, first using summation [10], and more recently through cross-attention mechanisms, either channel-wise [11] or spatially, as a way to filter out noise and focus on semantically rich features from deeper layers [12]."}, {"title": "2. METHOD", "content": "Our SFB-net is based on the U-net architecture [1] with an encoder, decoder, and skip-connections in-between, as illustrated in Figure 1. Convolutional blocks, depicted in blue are used throughout the network. These blocks contain 2 convolutions, each followed by a batch normalization layer and a Gaussian Error Linear Unit (gelu) [14] activation. As proposed by [15], an encoder/decoder architecture is used where the number of convolutional blocks in the encoder is doubled as compared to the decoder to improve the model encoding ability. The number of filters is doubled at each layer of the encoder and halved for corresponding layers of the decoder. Strided convolutions are used instead of pooling layers to down sample feature maps. The number of down sampling is limited resulting in a feature map at the bottleneck 8 times smaller than the input image size. Up-sampling is carried out using 2D transposed convolutions. To compensate for the resulting shallowness of the network, which may reduce its receptive field, a conventional transformer layer is introduced at the bottleneck (depicted in purple). This enables the network to take advantage of global contextual information. Note that transformer blocks are not used in the encoder and decoder since keeping convolutions at higher resolutions was shown to give better results [16]. Indeed, convolutions generalize better to unseen images than transformers and extract local information found at higher resolutions more effectively.\nDeep supervision is applied at each stage of the decoder. More precisely ground truth segmentations are down sampled to match the size of the network's outputs. The loss weight aidi \u2208 {1,2,3} for each resolution is halved when the image size is reduced. The final loss is the sum of successive stages loss and is defined as:\nL = \u03b11 \u00d7 Lseg(H,W) + a2 \u00d7 Lseg(#,W) + 03 \u00d7 Lseg(HW)"}, {"title": "2.2. Swin Filtering Blocks", "content": "A filtering mechanism is introduced in the skip connection paths between the encoder and the decoder. This process is described in Figure 2. The goal is to enable the decoder, to filter out irrelevant information originating from the encoder. More precisely, the encoder's feature maps contain noise that should be discarded before concatenation with the decoder. To do so, local information contained in high-resolution feature maps of the encoder that can be found in semantically-rich areas underlined by the decoder are highlighted and emphasized, while response in other noisy areas are toned down. Similar to [12], Multi-Head-Self-Attention (MHSA) is used in this process. However the window and shifted window version of MHSA introduced by [7] is preferred for its lower computational load, reducing training time and enabling to use the saved GPU memory in other parts of the network. Windowed Multi Head Self Attention (W-MHSA) performs attention in windows of M by M equal-sized patches. When performing Shifted Window Multi-Head Self Attention (SW-MHSA), windows are shifted by | \rfloor patches both in the x and y direction so that attention can be conducted between patches belonging to different windows. W-MHSA is described as:\nW-MHSA(Q, K, V) = Softmax(QKT/\u221ad + B)V\nWhere Q, K and V\u2208 RM2\u00d7d are the query, key and value respectively. d is the query, key and value dimension. B\u2208 RM2\u00d7M\u00b2 is the learnable relative position bias added to each head which encode the spatial relationship between patches. Q, K and V are tensors generated using separate linear layers. W-MHSA and SW-MHSA blocks are favorably used to perform cross-attention between the encoder and the decoder's feature maps. Cross-attention uses the same pro-"}, {"title": "3. EXPERIMENTS", "content": "Experiences are conducted on two datasets:\n\u2022 Automated cardiac diagnosis challenge dataset (ACDC) [19]: this dataset comprises 100 patients corresponding to overall 1902 annotated slices. Patients are divided into 5 groups according to specific diseases (normal, hypertrophic cardiomyopathies, dilated cardiomyopathies, abnormal right ventricle and myocardial infarction). For each patient, right ventricular cavity (RV), myocardium (MYO) and left ventricular cavity (LV) at end systole and end diastole are labelled on slices covering the heart from its base to its apex. Only annotated slices are used in our experiments. A 5-fold cross validation is performed and dice score is used to compare SFB-net with literature."}, {"title": "3.2. Implementation details", "content": "SFB-net is implemented with Pytorch and trained using a 16GB Tesla v100 SXM2. The nnUnet [17] framework is used as a starting point for this work. The AdamW optimizer and cosine annealing scheduler are used for training. The initial learning rate and weight decay are both set to 0.0001. For fair comparison with [17, 18], the number of training epochs is set to 1000. Each epoch is made up of 250 iterations. Batch size is 10 for ACDC and 6 for M&M's. Before training, all images are resampled in the x and y directions based on the median pixel spacing of the dataset. As a post processing step, the largest connected component is kept in the binarized prediction (classes are merged). Please refer to [17] for additional details. A wide range of data augmentations is applied: rotation, scaling, gamma adjustment, brightness adjustment, mirroring, contrast modification, low-resolution simulation, noise, and blur. Mirroring is also applied at testing time. The number of heads in SFBs are 2, 4 and 8. The number of heads in the bottom transformer layer is set to 16. The maximum number of filters at the bottleneck of the network is 512. Image size is set to 224x224 pixels for ACDC and 288x288 pixels for M&M's. SFB-net is made up of 23 million parameters. In Table 3 GigaFLOPS (Gflops) are computed with a batch size of 1. The throughput is computed by averaging the time taken to process the largest possible batch size on the GPU over 100 iterations."}, {"title": "3.3. Results", "content": "Comparison with literature on the ACDC dataset is summarized in Table 1 while comparison on the M&M's dataset is presented in Table 2. Results for SFB-net and nn-Unet are obtained using a 5-fold cross-validation and reported after post-processing. Other reported results come from their respective manuscripts.\nAs compared to other work presented in Table 1, we achieved the best overall Dice score (92.4%), as well as the best Dice score for the myocardium, while Dice scores of the LV and RV cavities were slightly lower. Of note, our"}, {"title": "4. CONCLUSION", "content": "This work introduced a new deep learning network architecture relying on spatial attention. Attention was applied both"}, {"title": "5. COMPLIANCE WITH ETHICAL STANDARDS", "content": "This research study was conducted retrospectively using human subject data made available in open access. Ethical approval was not required as confirmed by the license attached with the open access data."}]}