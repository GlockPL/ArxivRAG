{"title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance", "authors": ["Xuanfan Ni", "Liyan Xu", "Chenyang Lyu", "Longyue Wang", "Mo Yu", "Lemao Liu", "Fandong Meng", "Jie Zhou", "Piji Li"], "abstract": "To alleviate memory burden during inference of large language models (LLMs), numerous studies have focused on compressing the KV cache by exploring aspects such as attention sparsity. However, these techniques often require a pre-defined cache budget; as the optimal budget varies with different input lengths and task types, it limits their practical deployment accepting open-domain instructions. To address this limitation, we propose a new KV cache compression objective: to always ensure the full-cache performance regardless of specific inputs, while maximizing KV cache pruning as much as possible. To achieve this goal, we introduce a novel KV cache compression method dubbed DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance, then halting the pruning process. Empirical evaluation spanning diverse context lengths, task types, and model sizes suggests that our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average. Furthermore, our method is easy to integrate within LLM inference, not only optimizing memory space, but also showing reduced inference time compared to existing methods.", "sections": [{"title": "Introduction", "content": "As most large language models (LLMs) follows autoregressive generation (Minaee et al., 2024; Chang et al., 2023; OpenAI, 2023), they rely on KV Cache to store intermediate states during inference. Specifically, LLMs access the cached key and value vectors of past tokens during the attention calculation, thereby speeding up inference by re-using these KV cache (Vaswani et al., 2017; Ainslie et al., 2023; Shazeer, 2019). However, as the model size and the input length increase, the memory required to maintain the KV cache also grows proportionally. Since these vectors are usually stored within GPU memory, managing the KV cache efficiently has become crucial to mitigate the overall memory consumption and computational overhead during LLM inference. For instance, Llama3 8B model (Dubey et al., 2024) requires 1 GB of KV cache memory for 2K-token inputs, while its 70B counterpart demands a gigantic memory up to 50GB for 20K tokens.\nConsequently, recent works have proposed to effectively reduce KV cache through pruning, leveraging the sparsity of the attention mechanism: certain positions are more pivotal during the generation process, while those less important ones could be pruned with minimal performance degradation; thus, the full KV cache is not always necessary. These observations have led to several optimization methods, such as H2O (Zhang et al., 2023), ScissorHands (Liu et al., 2023), StreamingLLM (Xiao et al., 2024), and FastGen (Ge et al., 2024), where they aim to identify and retain only the most salient token positions, discarding less critical ones based on certain pruning criteria. Alongside, other orthogonal paradigms have also been proposed without hard-pruning, such as KVMerger (Wang et al., 2024) and D2O (Wan et al., 2024) that merge KV vectors, and MLA (DeepSeek-AI et al., 2024) that operates attention in the latent space.\nWhile previous pruning methods have successfully demonstrated that a decent size of KV cache could be discarded in practice, there exists one common limitation that has not been addressed yet: these pruning techniques typically require a pre-defined KV cache budget, of which its optimal threshold could vary significantly according to specific tasks or inputs. Hence, in real-world scenarios, one would have to carefully tune many budget thresholds for diverse domains, or set a uniform threshold but likely suffering large degradation on certain tasks. We reckon that neither choice is a satisfactory solution, hindering the widespread adoptation of KV pruning techniques.\nTo further illustrate, Table 1 shows that when the budget is set to 50%, Llama3-8B-Instruct using H2O achieves over 98% of the full-cache performance on NarrativeQA (Kocisk\u00fd et al., 2018), whereas under the same conditions on GSM8K (Cobbe et al., 2021), a widely-used math benchmark, the performance drastically falls to less than 42% of the full-cache performance.\nTo address the aforementioned limitation, in this work, we introduce a new objective for KV cache compression: our method aims to strike towards full-cache performance regardless of specific inputs, while maximizing KV cache pruning as much as possible. Towards this objective, we propose an adaptive pruning method, dubbed DBudgetKV, which Dynamically adjusts KV cache retention per input, without the need of pre-defined memory Budget. Particularly, DBudgetKV automatically identifies input-dependent KV cache pruning that leads to minimal performance degradation. A key implication is that the method tends to realize a higher compression ratio for simpler tasks, while allocating more cache resources for complicated tasks.\nOur method features a two-step process: 1) after the prefilling stage, rank all input tokens according to an importance metric; 2) discard the KV cache of each token sequentially, until hitting a stopping criterion. In choosing the importance metric and the stopping criterion, DBudgetKV takes into account two important factors from previous works: i) position bias, where tokens at the beginning and the end are generally more significant than those in the middle (Xiao et al., 2024; Jiang et al., 2024b); ii) attention score, where tokens receiving higher scores usually contribute more information to subsequent generation (Minaee et al., 2024; Chang et al., 2023; Zhang et al., 2023).\nWe then propose DBudgetKV of empirical efficacy and low overhead. Concretely, tokens are ranked solely by their positions, with no additional computation involved. The ranking strategy signals high significance to the beginning and the end positions, where the initial tokens are the most important, followed by the remaining tokens in a reversing order. Next in the pruning process, we design an attetion-based metric to determine the stopping condition, such that the pruning halts when this metric signals that the remaining KV cache is unlikely to match the full-cache capacity, thereby fulfilling our objective to approximate full performance with dynamic cache pruning.\nMore specifically, our attention metric compares the difference between the Frobenius form of the full attention matrix and the reduced matrix by setting pruned positions' score to zero. When the difference exceeds a universal threshold, the process is halted. Especially, this metric empirically correlates well with the generation performance regardless of inputs, serving as an estimator to bridge the pruning state and the subsequent generation. Furthermore, the entire process is implemented efficiently through PyTorch's built-in operators, where we show that DBudgetKV not only optimizes memory space, but also acheives time reduction compared to the full-cache inference and three KV pruning methods.\nExperimental results on 13 datasets varying diverse context lengths and tasks, e.g. mathematical and commonsense reasoning, reading comprehension and coding, demonstrate that DBudgetKV achieves our objective effectively and robustly. The resulting inference is on par or even surpasses the full-cache performance with multiple LLMs of different sizes, including Llama3 (Dubey et al., 2024), Qwen2.5 (Yang et al., 2024), and Mistral (Jiang et al., 2024a). Meanwhile, the dynamic pruning of DBudgetKV allocates relatively high budget on math benchmarks but attains high compression ratio by up to 85% on comprehension tasks. The average budget size reaches 63.7% using Llama3-8B. The overall results suggest that DBudgetKV is able to accomplish lossless KV cache pruning with strong generalizability. Besides, it does not rely on specific model architectures and can coexist with other KV cache optimization techniques. We further conduct more experiments for in-depth analysis and ablation studies, depicting the rationality of our various design choices and hyperparameters.\nOur contributions can be summarized below:\n\u2022 To the best of our knowledge, we are the first to propose the objective that ensures full-cache performance while maximizing the KV cache pruning, which holds greater practical values and deployment potentials.\n\u2022 We introduce DBudgetKV that employs a straightforward two-step process, where its dynamic pruning is designed with minimal degradation to the generation performance."}, {"title": "Related Work", "content": "KV Cache Compression Recent advancements in KV cache compression have emerged to address memory constraints for model inference. Scissorhands (Liu et al., 2023) observes the repetitive nature of attention patterns and the persistence of token importance. It then preserves the most important tokens based on attention scores to achieve KV cache compression. H2O (Zhang et al., 2023) introduces dynamic eviction policies that strategically balance retention of recent tokens and historically significant \"heavy hitters\". StreamingLLM (Xiao et al., 2024) enables infinite-length sequence processing without fine-tuning by stabilizing attention through landmark tokens. SnapKV (Li et al., 2024) enhances compression efficiency through attention score-based selection and clustering of critical KV positions. FastGen (Ge et al., 2024) proposes an adaptive KV cache management system that customizes retention strategies per attention head.\nHowever, these methods require setting a fixed memory budget, which makes it difficult to consistently maintain full performance across different domains and datasets, limiting their practical application. In contrast, our method imposes no such constraints, targeting full-cache performance while allowing for dynamic budget size."}, {"title": "Methodology", "content": "We first elaborate our objective in Sec. 3.1 as a new direction for KV cache compression. Distinct from previous works, we aim to compress the KV cache as much as possible while ensuring LLMs' performance remains intact. We then delineate our proposed approach along with its motivation behind in Sec. 3.2. Important implementation details are next discussed in Sec. 3.3."}, {"title": "Objective of KV Cache Compression", "content": "As mentioned in Sec. 1, most previous works on KV cache pruning require a pre-defined memory budget beforehand, either by fixing the number of cached positions directly (Cai et al., 2024; Li et al., 2024), or retaining positions by a fixed ratio of input length (Zhang et al., 2023; Wan et al., 2024). While these approaches are effective in certain circumstances, they may encounter practical challenges when deployed in real-world scenarios.\nThe biggest issue is that the optimal budgets for LLM inference in real-world scenarios are evidently infeasible to enumerate, as they could vary across tasks and domains, especially for open-domain instructions. For example, mathematical tasks are typically concise and require inference based on all given conditions, thus many positions may logically contribute to the generation, necessitating a relatively higher memory budget. Conversely, for article reading comprehension, LLM may only need a small set of critical positions for the answer generation, featuring a smaller budget in many cases.\nAs a side effect, inference with previous KV cache pruning is likely to experience performance instability across different inputs, which can be observed by our analysis in Sec. 4.2. Overall, the practical value is thereby diminished.\nIn this work, we resort to a dynamic paradigm that eliminates the need for manually setting a memory budget. We introduce a new objective for KV cache compression, aiming to automatically reduce the KV cache as much as possible, while fulfilling the condition that the method should always strike towards full-cache performance. In this way, one could utilize such pruning method off-the-shelf, without spending time and resources on tuning usage-specific hyperparameters, which we hope would further advance the research development of KV cache compression techniques."}, {"title": "DBudgetKV", "content": "Towards our objective, we design a novel ranking-based pruning method applied after the prefilling stage, consisting of two steps. First, all tokens are ranked according to an importance metric per Transformers layer. Then, the system keeps removing the KV cache sequentially, until hitting a stopping criterion. Ultimately, our proposed approach does not need to fix an optimal budget for storing KV cache. Rather, the stopping criterion is tied to the norm of attention matrices that works uniformly regardless of the inputs.\nThe motivation behind our two-step procedure comes from two phenomena brought by previous works. First, there exists position bias such that positions in the beginning and the end are typically important for the subsequent generation, as observed by StreamingLLM (Xiao et al., 2024). Second, attention scores are usually an effective indicator of the token importance, as leveraged by many pruning techniques (Liu et al., 2023; Zhang et al., 2023; Li et al., 2024). For our objective, we propose to combine the two important properties in our approach, described in details as follows.\nImportance Ranking Denote a LLM input sequence as $X = \\{x_1,x_2,...,x_n\\}$, where each Transformers layer originally consists of n positions of KV vectors per attention head. As the pruning process starts from the least important KV vectors, all tokens are firstly ranked according to an importance strategy.\nTo this end, we resort to a simple ranking strategy leveraging the position bias, without undergoing any additional computation. As identified by StreamingLLM, a significant portion of attentions is allocated to the initial tokens, regardless of their relevance to the language modeling task, known as attention sinks. Inspired by this, we employ a purely position-based importance evaluation strategy: we assume that the first m tokens are always crucial; while among the remaining n \u2212 m tokens, those towards the end are more significant. By this strategy, the importance in X is ranked from the highest to the lowest as $x_1, x_2,..., x_m, x_n, x_{n\u22121},...,x_{m+1}\\}$, denoted as $\\tilde{X}$, where m is a chosen hyperparameter that works regardless of specific input sequences.\nIt should be noted that our choice on importance ranking is empirically supported rather than theoretically based. We have experimented other attention-based importance ranking, which are provided in Sec. 4.4. Our position-based strategy is shown both empirically effective and superior on computational overhead.\nStopping Criterion With the initially ranked KV vectors, our method then sequantially decides whether to continue or terminate pruning, which needs an evaluation metric serving as the stopping criterion. Since we evidently cannot know the precise impact of a KV cache position before the generation, we require a metric that correlates well with the resulting performance change when discarding a position, serving as a bridge to ensure a minimal degradation upon the full KV-cache performance.\nInspired by Devoto et al. (2024), we utilize the Frobenius norm of the attention matrix that fits the requirement well empirically. For a matrix $M\\in \\mathbb{R}^{m\\times n}$, the Frobenius norm of M is the matrix-version of the L2 norm, expressed as $||M||_F = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}|M_{i,j}|^2}$.\nWe conduct preliminary experiments to validate its reliability. We employ Llama2-7B and randomly mask a certain portion of the KV cache, setting the corresponding values in the attention matrix $A \\in \\mathbb{R}^{n\\times n}$ to zero. As illustrated in Figure 2, there is a certain degree of positive correlation between its Frobenius norm and the model's performance, which are shown generalized across different domains.\nAs the input sequence length n increases, the time and space overhead for norm calculation also grows significantly by $O(n^2)$. Hence, we next aim to reduce the computational scale from the naive norm calculation. We observe that the Frobenius norm of the last row of A also correlates with the model's performance; equivalently saying, the attention distribution of the latest token is good enough to serve as an estimator. Formally, we utilize the last k rows (latest k tokens) of A and reduce to a single attention vector $A' \\in \\mathbb{R}^{1\\times n}$. The score $s_j$ for a position $j \\in [1, n]$ in $A'$ is:\n$s_j = \\frac{\\sum_{i=k}^{i=1} A_{i,j}}{\\sum_{i=k}^{i=1} 1\\{A_{i,j} \\neq 0\\}}$ (1)\nWe then use the Frobenius norm of $A'$, equivalently its L2 norm, as the evaluation metric, which reduces the overall complexity to $O(n)$.\nFinally, for each position in the ranking order, our method compares the difference between the Frobenius norm of the original attention matrix A, and the Frobenius norm of the reduced attention vector A' of which the corresponding positions are set to 0. We set a universal threshold as 1%, such that when the new norm exceeds 1% difference from the original norm, it hits the stopping criterion. The KV vectors of the remaining positions are then all kept, while the preceding positions would have their KV cache discarded. In this way, our method achieves dynamic KV cache pruning while targeting to maintain the full-cache performance, assisted by the norm metric."}, {"title": "Implemetation Details", "content": "Though our proposed pruning process is conceptually sequential, it is implemented efficiently by PyTorch's operators, such that the stopping positions of all layers are identified directly in parallel without any looping operations. Empirical analysis in Sec. 4.5 shows that our proposed method achieves higher efficiency compared to three popular baselines.\nSpecifically, we reorder $A'$ based on Token Importance Ranking $\\tilde{X}$ to obtain $A'_{sort}$, where $A'_{sort}[0]$ corresponds to the least important token and $A'_{sort}[-1]$ corresponds to the most important token. Next, we compute the cumulative square-sum of each element in $A'_{sort}$:\n$A'_{cumsum}[i] = \\sqrt{\\sum_{k=i}^{n}|A'_{sort}[k]|^2}$ (2)\nsuch that $A'_{cumsum}[i]$ represents the Frobenius Norm of the attention matrix after removing all tokens to the left of $i^{th}$ token. We then divide each element in $A'_{cumsum}$ by $||A'||_F$ and subtract this value from 1 to determine the difference between each new matrix and A'. The torch where operation allows us to directly identify the position of the token that satisfies the condition of not exceeding a 1% difference, ultimately yielding the set of tokens for which the KV cache is called to retain. The final kv cache compression algorithm of DBudgetKV is presented in Algorithm 1. Since PyTorch allows specifying the dimension for operations, DBudgetKV can run in parallel across multiple attention heads.\nRetaining Bottom Layers Additionally, we have discovered that applying DBudgetKV to the model's 0th and 1st layers results in poor performance when the model operates under a high budget. We hypothesize that this may be due to the relatively uniform distribution of attention scores in these initial layers, leading to the premature discarding of tokens that are crucial for subsequent layers, ultimately causing the entire model's inference to collapse. A more detailed analysis of this issue is provided in Appendix B. Based on this finding, we initiate the KV cache compression operations starting from the model's 2nd layer."}, {"title": "Experiments", "content": "Our experiments are conducted with three LLMs: Llama3-Instruct with size of 8B/70B, Mistral-7B-Instruct-V0.3, and Qwen2.5-Instruct with size of 7B/32B/72B. We implement DBudgetKV upon the codebase of SnapKV\u00b9. For the reduced attention matrix A', we set k = 1 in practice (ablation provided in Sec. 4.4), and set m = 4 for importance ranking."}, {"title": "Main Results", "content": "The main results are shown in Table 1 according to our evaluation protocol, where DBudgetKV demonstrates strong advantages with robust performance across diverse tasks and models:\n\u2022 DBudgetKV is able to fulfill our objective, capable of performing near-lossless dynamic compression across different models, varying input lengths, and diverse task types. Interestingly, with Llama3-8B and Qwen2.5-7B, DBudgetKV even surpasses the full-cache performance by 0.12% and 2.63% respectively, utilizing an average of 63.68% and 76.02% KV cache. With Mistral, DBudgetKV also manages to achieve nearly 20% KV cache compression with only a 1.5% performance reduction. These results indicate that our proposed method can be practically deployed in real-world scenarios without worrying domain-specific budget thresholds. In stark contrast, previous methods achieve a consistent full-cache performance only when manually determined a high budget ratio, e.g. 90%. However, when the budget is reduced, e.g. 50%, the degradation can become severe on certain datasets, distinct from DBudgetKV that automatically adjusts the pruning to always attain full performance.\n\u2022 DBudgetKV natually reflects the difficulty of the generation task. As in Table 1, the dynamic budget ratio is high on Math&Science datasets (over 90%), while much lower on QA or Summarization datasets (as low as 15%). This observation is in line with our intuition, where inference on concise but hard tasks, such as math problems, requires more context and more precise calculation, resulting in higher budget allocation.\n\u2022 Besides the dynamic compression, DBudgetKV also outperforms the three 90%-budget baselines, while itself uses less than 90% budget, and achieves the highest score in 20 out of 39 comparisons. This suggests that by taking into account both the position bias and attention scores, our method accompolishes more effective pruning than the baselines that only consider one aspect, highlighting the importance of integrating both dimensions for effective KV cache compression."}, {"title": "DBudgetKV's Generalization Ability", "content": "We next conduct experiments to examine whether our design and hyperparameters can be generalized to LLMs of larger sizes. On five datasets in Table 2, DBudgetKV with Llama3-70B and Qwen2.5-32B/72B demonstrates consistent near full-cache performance with the same DBudgetKV setting. Notably, the averaged compression ratio increases on datasets of longer context, achieving nearly a 50% lossless compression at its peak, validating the robustness and versatility for its general applicability."}, {"title": "Ablation Study", "content": "The ablation study is conducted to explore the impact of various configurations of DBudgetKV. We select Llama-3-8B-Instruct and perform experiments across five datasets presented in Table 3.\nAttention Matrix Reduction The reduced attention matrix A' in Sec. 3.2 aggregates attention scores from the last k positions. The upper part of Table 3 illustrates the model's performance and the actual budget when setting k = 1, 1%n, 5%n, and 10%n (n being the number of input tokens). It is evident that setting k as 1 achieves a significantly reduced budget, thus a higher compression ratio, with almost no change in performance compared to 1%n and 5%n. On the other hand, while 10%n can compress more KV cache, it fails to maintain performance (for instance, on NarrativeQA, the former achieves a performance of 9.74 using 32% of the budget, whereas the latter scores 21.44 using 48.7% of the budget). What's even better is that since k = 1 requires the least amount of computation, relying solely on the scores from the last token, the complexity of obtaining A' becomes O(1), independent of the sequence length. The advantages of both high efficacy and low overhead make k = 1 a solid design choice for calculating the norm metric.\nImportance Ranking Strategies DBudgetKV simply regards positions as the significance of tokens, while another intuitive strategy is to utilize attention scores to rank their importance. We investigate its potentials through ranking by each token's average attention score received from other tokens, similar to previous approaches such as H2O. The results are shown in the middle of Table 3: under the same experimental settings, the attention-based ranking struggles to maintain the model's performance. Though it achieves a high compression ratio, the performance is not stable with possible severe degradation, which deviates from our objective. Therefore, the position-based strategy is more suited for our goal.\nAttention Norm Threshold As we adopt the universal threshold as 1% for the attention norm difference, we comprehensively study the effects of smaller or larger thresholds, as shown in the bottom part of Table 3. Intuitively, a larger threshold allows for a higher compression ratio, while a smaller threshold does the opposite. The conclusion is clear that when the threshold is set to a smaller 0.1%, the average budget increases as expected, yet the model's performance sees little improvement. Conversely, when the threshold is set to 10%, it prunes more KV cache, but the model's performance significantly deteriorates. Thus, we deem 1% as a reasonable threshold for universal uses across models and tasks.\nAppendix D presents additional results and analysis from the ablation study."}, {"title": "Efficiency Analysis", "content": "In this section, a quantitative study is conducted on whether DBudgetKV improves inference time, apart from the space reduction from KV cache pruning. We compare the time usage on six datasets with Llama3-8B/70B using DBudgetKV, along with three baselines with the 50% budget setting. Table 4 reports the average generation time after the prefilling stage of each sample (Overall), as well as the time needed to complete pruning (Prune) before the generation.\nFrom the results, DBudgetKV achieves the best performance in 8 out of 12 comparisons of overall generation time, and achieves the best average time for both 8B and 70B LLMs, suggesting that although DBudgetKV requires a slightly longer time for pruning itself compared to three other approaches, the total generation speed of DBudgetKV is evidently advantageous. It also performs consistently across models of different scales, underscoring its time efficiency."}, {"title": "Conclusion", "content": "In this study, we introduce an innovative KV cache compression objective designed to approximate the full-cache performance, independent of specific inputs, while optimizing resource utilization through targeted KV cache pruning. Our approach, termed DBudgetKV, employs a straightforward yet effective two-step process for each layer of LLMs. Tokens are initially ranked solely based on their positions, followed by the eviction of the least significant tokens as determined by the norm value of the reduced attention matrix. Comprehensive experiments conducted across diverse datasets, encompassing a variety of tasks and context lengths, demonstrate that DBudgetKV achieves nearly lossless compression, while with notable space and time reduction, successfully fulfilling our objective for greater practical values."}, {"title": "Limitations", "content": "The main limitation of DBudgetKV stems from the gap between its current compression budget and the true optimal budget. This can be seen in Table 1 that for certain scenarios, e.g. QMSum with Mistral-7B, DBudgetKV reaches 84.3% budget while 50% budget is also viable with no performance degradation. We regard this gap as room for improvement in future work, facilitating more aggressive KV cache compression while ensuring the full-cache performance.\nAnother limitation of DBudgetKV is that though it demonstrates almost lossless compression, we rely on empirical experiments and there is no hard guarantee on the full-performance constraint. In Table 1, while both Llama3-8B and Qwen2.5-7B even surpass the full-cache performance, DBudgetKV with Mistral-7B shows trivial degradation of 1.5%. More robust methods could be developed as future work to further strengthen this constraint."}, {"title": "A Full Algorithm of DBudgetKV", "content": "Algorithm 1 DBudgetKV\nInput: Prompt, Threshold t\nOutput: Compressed KV Cache\nCreate Empty List Kc, Ve\nfor Transformer Layer Li in LLM do\n$Q^{i}, K^{i}, V^{i} \\leftarrow L_{i}$(Prompt)\nR$\\leftarrow$ Postion-Based Importance Rank\n$A^{last}$Attention$(Q^{i}[\\dots,:, -1, :] , K^{i}T)$\nF$\\leftarrow$ Frobenius$(A^{last})$\n$\\tilde{A^{last}}$$\\leftarrow$ Square$(A^{last})$\nReorder $A^{last}$ by Rank R\n$A^{cumsum}$$\\leftarrow$ Cumsum$(\\tilde{A^{last}})$\n$A^{cumsum}$$\\leftarrow$ Sqrt($A^{cumsum}$)\n$A^{ratio}$$\\leftarrow$ (F-$A^{cumsum}$)/F\nIndex$\\leftarrow$ Max(Where($A^{ratio}$ <= t))\n$K^{i}$$\\leftarrow$ Compress $K^{i}$byR\u00b9[I :]\n$V^{i}$$\\leftarrow$ Compress $V^{i}$byR\u00b9[I :]\nAppend $K^{i}$, $V^{i}$ to $K_{c}, V_{c}$\nend for\nreturn $K_{c}, V_{c}$\nIn this section, we elaborate the necessity to freeze\nthe first and second layers in DBudgetKV as men-\ntioned in Section 3. In our preliminary studies, we\napply DBudgetKV to Llama2-7B-Chat and con-\nduct experiments on GSM8K and CoQA. We first\nperform a case study, followed by a comparison\nwith the effects of not freezing the bottom layers.\nAs shown in Table 5, when the threshold is set\nto 1% and no layers are frozen, the outputs for two\nexamples on GSM8K and CoQA are incorrect and\nlack logical coherence. However, the budget ex-\nceeds 90% in both cases. By observing the actual\nbudget of each layer, we can see that for these two\nexamples, the budgets for layer 0 and 1 are rela-\ntively low, while the budgets from layer 2 to layer\n30 become very high, with the last layer being\nlow again. We hypothesize that the model is still\nencoding the global semantics in its early layers,\nthus not yet able to identify truly important tokens\nin the first two layers, of which the attention dis-\ntribution is relatively uniform. This phenomenon\nhas also been observed by early works on Trans-\nformers analysis (Ethayarajh, 2019; Gar\u00ed Soler and\nApidianaki, 2021). Not freezing early layers may\nlead to early eviction of important tokens, result-\ning in subsequent generations being unable to ac-"}, {"title": "B Freeze the first two layers of LLM", "content": "In this section, we elaborate the necessity to freeze the first and second layers in DBudgetKV as mentioned in Section 3. In our preliminary studies, we apply DBudgetKV to Llama2-7B-Chat and conduct experiments on GSM8K and CoQA. We first perform a case study, followed by a comparison with the effects of not freezing the bottom layers.\nAs shown in Table 5, when the threshold is set to 1% and no layers are frozen, the outputs for two examples on GSM8K and CoQA are incorrect and lack logical coherence. However, the budget exceeds 90% in both cases. By observing the actual budget of each layer, we can see that for these two examples, the budgets for layer 0 and 1 are relatively low, while the budgets from layer 2 to layer 30 become very high, with the last layer being low again. We hypothesize that the model is still encoding the global semantics in its early layers, thus not yet able to identify truly important tokens in the first two layers, of which the attention distribution is relatively uniform. This phenomenon has also been observed by early works on Transformers analysis (Ethayarajh, 2019; Gar\u00ed Soler and Apidianaki, 2021). Not freezing early layers may lead to early eviction of important tokens, resulting in subsequent generations being unable to access this information, ultimately causing the output to fail.\nBased on the above case study, we attempt to freeze certain layers of the model and explore the optimal freezing configuration. We continue to validate the results of freezing different layers, and the resuls are shown in Table 6. We can observe that freezing the first two layers achieves a balance between model performance and budget. Moreover, freezing the last layer does not significantly enhance the model's performance and instead leads to an increase in budget. DBudgetKV ultimately opts to begin KV cache compression from the 2nd layer of the model."}, {"title": "C Datasets Used in Experiments", "content": "In this section, we provide a comprehensive overview of all the tasks and datasets utilized in the experiments in this paper.\nMath & Science This task evaluates the model's ability to tackle mathematical and scientific problems. By directly inputting questions and comparing the model's output with the correct answers, we calculate the model's Accuracy on these datasets: GSM8K is a dataset for evaluating model's math-solving skills, featuring 8,000 elementary-level math word problems requiring basic arithmetic and reasoning. GPQA tests model's understanding of physics concepts and problem-solving across various topics, assessing scientific reasoning abilities. TheoremQA evaluates model's grasp and application of mathematical theorems, ranging from simple applications to complex proofs, testing advanced math skills.\nCommonsense Reasoning (CR) This task evaluates model's ability to make deductions and understand everyday situations using implicit knowledge and logical inference. TruthfulQA (ThQA) evaluates model's ability to generate accurate and truthful responses, testing models on distinguishing fact from fiction, especially in areas prone to misconceptions. We use BLEU as the metric. CoQA assesses model's ability to understand and respond to questions in a conversational context, focusing on maintaining coherence and context throughout a dialogue. We use F1 Score as the metric.\nSingle Document QA (Single-Doc QA) This task assesses the model's reading comprehension skills when dealing with a single, extended document. NarrativeQA (Kocisk\u00fd et al., 2018) is a dataset designed to evaluate model's ability to comprehend and answer questions based on narrative texts, focusing on understanding stories and their underlying themes. Qasper (Dasigi et al., 2021) is a dataset aimed at assessing model's capability to extract and answer questions from academic papers, emphasizing understanding complex scientific information. We employ F1 Score as the metric for above two datasets.\nMulti-Document QA (Multi-Doc QA) This task evaluates the model's reading comprehension capabilities across multiple extended documents. 2WikiMultiHopQA (2WKMQA) (Ho et al., 2020) is a dataset designed to test model's ability to perform multi-hop reasoning and answer complex questions using information from multiple Wikipedia articles. MuSiQue (Trivedi et al., 2022) evaluates model's skill in integrating and reasoning over information from multiple sources to answer comprehensive questions accurately. We leverage F1 Score as the metric for above two datasets.\nSummarization This task examines the model's ability to comprehend and summarize lengthy documents. QMSum (Zhong et al., 2021) is a dataset for evaluating model's ability to generate concise summaries of meeting transcripts, focusing on capturing the key points from multi-party discussions. Multi-News (M-News) (Fabbri et al., 2019) is a dataset that challenges models to create coherent summaries by synthesizing information from multiple news articles on the same topic. We use Rouge-L as the metric for above two datasets.\nFew-Shot Learning (FSL) This task assesses the model's few-shot learning capabilities. TriviaQA (Joshi et al., 2017) is a dataset designed to assess model's ability to retrieve and answer questions based on large collections of trivia, emphasizing comprehension and factual recall. We use F1 Score as the metric."}, {"title": "D Ablation", "content": "In this section, we present additional ablation study results. By setting various values for k, we expand upon the results shown in Table 3. The expanded results are displayed in Table 7. These experiments facilitate a deeper understanding of how different parameter settings impact model performance and provide a basis for optimizing parameter selection.\n= 1 not only conserves pruning time but also achieves better model performance with a reduced budget.\nAs shown in Table 7, setting k"}]}