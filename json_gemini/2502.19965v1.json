{"title": "Deterministic or probabilistic? The psychology of LLMs as random number generators", "authors": ["Javier Coronado-Bl\u00e1zquez"], "abstract": "Large Language Models (LLMs) have transformed text generation through inherently probabilistic context-aware mechanisms, mimicking human natural language. In this paper, we systematically investigate the performance of various LLMs when generating random numbers, considering diverse configurations such as different model architectures, numerical ranges, temperature, and prompt languages. Our results reveal that, despite their stochastic transformers-based architecture, these models often exhibit deterministic responses when prompted for random numerical outputs. In particular, we find significant differences when changing the model, as well as the prompt language, attributing this phenomenon to biases deeply embedded within the training data. Models such as DeepSeek-R1 can shed some light on the internal reasoning process of LLMs, despite arriving to similar results. These biases induce predictable patterns that undermine genuine randomness, as LLMs are nothing but reproducing our own human cognitive biases.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolutionized natural language processing by generating human-like text through advanced probabilistic mechanisms. Based on transformer architectures Vaswani et al. (2023) and trained on vast corpora of text, these models learn to predict the next token in a sequence, effectively capturing intricate statistical patterns inherent in human language. Although LLMs are inherently stochastic, recent observations have revealed a curious phenomenon: when tasked with generating a single random number -a seemingly trivial exercise in randomness- they often produce deterministic outputs. This counterintuitive behavior raises important questions about the interplay between a model's probabilistic design and the biases ingrained in its training data.\nLLMs are deep neural networks that leverage the transformer architecture to perform a wide range of natural language tasks. Transformers use self-attention mechanisms to model long-range dependencies in text, allowing the model to assign a probability distribution over possible next tokens based on context. Training involves maximizing the likelihood of observed sequences, which results in a model that can generate text by sampling from its learned distribution. In theory, such a mechanism should naturally yield variable and random outputs when the model is allowed to sample freely. However, the actual behavior of these models often deviates from this ideal, especially in tasks that require pure randomness.\nThe probabilistic nature of LLMs is central to their function. When generating text, each output token is sampled from a distribution conditioned on prior tokens, leading to variability and creativity. This stochastic process is expected to extend to all tasks, including the generation of a single random number. Yet, numbers are not understood as such by LLMs, but rather as tokens, attending to their characters and not their mathematical meaning. This is, a number such as \"2\" has no further meaning for a LLM than \u201c3\u201d, \u201c+\u201d or the word \u201chorse\u201d \u2013 they are just tokens (either singular or a collection) with corresponding vector(s) in the latent space of the embedding model.\nIn an ideal setting, requesting a random number from an LLM should yield outputs that are uniformly distributed over the specified range. Yet, as noted in recent discussions and blog posts\u00b9, many models exhibit a pronounced bias toward particular outputs when tasked with generating randomness. This observation suggests that the randomness encoded within the LLMs' sampling procedures may be compromised by factors beyond the mere sampling algorithm.\nWhile LLMs are designed to generate outputs based on probabilistic principles, they are ultimately trained on human-generated text, which is replete with patterns, conventions, and biases. These training datasets often include overrepresented sequences and stylistic regularities that can skew the learned probability distributions. Consequently, when an LLM is prompted to generate a random number, it may default to outputs that reflect these ingrained patterns. This phenomenon aligns with the \"stochastic parrot\" critique Bender et al. (2021), where models are seen as reproducing the statistical regularities of their training data without true understanding.\nThe issue of stochasticity in LLM outputs has garnered increasing attention in both academic and informal settings. A recent study Koevering and Kleinberg (2024) systematically analyzed the randomness of outputs from several popular models and found that certain systems deviate markedly from expected behavior.\nAdditionally, the choice of sampling parameters such as the temperature, top-k, or top-p play a significant role in balancing randomness and determinism. Lower temperature values, for instance, concentrate the probability mass and should lead to more deterministic outputs. Even when these parameters are adjusted to encourage variability, many LLMs still tend to output similar \"random\" numbers repeatedly, hinting that the bias is deeply embedded in the model's internal representations and the nature of its training data.\nThe deterministic tendencies observed in random number generation also have broader implications. In applications where true randomness is essential-for example, in cryptographic protocols, statistical sampling, or even in certain simulation tasks- the inability of LLMs to generate uniformly random outputs could lead to significant vulnerabilities or performance issues. Understanding these limitations is therefore not only of theoretical interest but also of practical importance.\nIn this paper, we conduct a systematic investigation into the capability of LLMs to function as random number generators. We explore a range of configurations including different model architectures, numerical ranges, and -for the first time- languages to assess how these factors influence the randomness of the outputs. While other works' conclusions point towards inherited biases from the training data in answers, none has explored the influence of the prompt language to check whether there are significant differences depending on it."}, {"title": "LLMS AS RANDOM NUMBER GENERATORS", "content": "This idiomatic tests have been widely explored for possible cultural and linguistic biases in natural language answers (see, e.g., Neplenbroek et al. (2024); Mihaylov and Shtedritski (2024); Tao et al. (2024)). The issue of random number generation in LLMs has been previously tackled by Hopkins et al. (2023), but for entire sequences to study the uniformity of those. The authors find that LLMs do not always generate the expected distribution, breaking the uniformity assumption that is required in the prompt. In our case, we are interested in the LLM variability of a single number per call, and how can they reproduce human biases when offered such choice.\nOne of the models evaluated in this work is DeepSeek-R1, which outputs not only the answer but the full reasoning chain-of-thought (CoT) to determine the final output. This offers a novel and rich view at the internal process of an LLM when prompted to generate a random number, as often we find a very extensive monologue with several changes of mind between, yet arriving most of the times to similar conclusions.\nThis study not only sheds light on the limitations of current LLMs as random number generators but also opens avenues for further research into mitigating data-induced determinism in probabilistic models. Addressing such issues is crucial for ensuring that LLMs can be reliably used in contexts where unpredictability and fairness are of paramount importance.\nThe remainder of the paper is organized as follows. Section 2 describes our experimental setup, including the various configurations and methodology employed to probe the randomness of the models. Section 3 presents the results of our study, comparing the behavior of different models, highlighting key differences in output distributions and computing statistical tests. Finally, Section 5 concludes with a summary and directions for future research."}, {"title": "2 Experimental setup and methodology", "content": "In order to evaluate the stochasticity capabilities of LLMs when tasked with generating a single random number, we conduct a systematic set of experiments covering multiple configurations. Specifically, we test the models alphabetically summarized in Table 1:\nDue to computational restrictions, we do not use models with large number of parameters (\u2265 20B), although we do test Gemini 2.0 and GPT-40-mini, with an unreported number of parameters but expected to be massive OpenAI (2024); Gemini Team (2024). Likewise, we avoid Small Language Models (below ~ 5B) as initial tests conducted with Llama 3.2-3B and Gemma-2B suggested these models had some difficulties to properly understand the task consistently.\nInitially, we also included Perplexity's Sonar models, yet, as distilled from both DeepSeek and Llama families, we found their results to be very similar to those in preliminary tests. Being a pay-per-use model, we decided to exclude them from the model pool for resource efficiency sake. Additionally, we considered to use Qwen 2.5, but the 14B version of DeepSeek-R1 used in this study is distilled from the Qwen architecture DeepSeek-AI (2025), and decided to prioritize DeepSeek for its CoT reasoning.\nFor each model, the experiments are carried out in seven different languages: Chinese (CN), English (EN), French (FR), Hindi (IN), Japanese (JP), Russian (RU), and Spanish (ES). We select these languages based on two primary criteria: (i) they represent a broad spectrum of linguistic typologies with distinct grammatical and morphological features, as well as different cultural backgrounds, and (ii) they are among the most widely spoken languages globally and are well represented in the large-scale training corpora of modern LLMs.\nThe prompt is always the same: Give me a random number between 1 and X. Please only return the number with no additional text, where X is the upper limit defined in each of the three range configurations. We replicate it into the other 6 languages in their respective alphabets (e.g., Cyrillic for Russian). This prompt ensures that the task is well-defined, yet offers certain freedom (for example, we do not specify the number to be an integer). We deliberately do not add any prompt engineering such as \"make sure this number is truly random\" or \"avoid giving a deterministic answer\" to be able to spot possible biases in the generation process.\nWhile these subtleties may seem irrelevant when prompting such a straightforward task as generating a single number, we find that language influences the distribution of the resulting samples, as most likely the model is unconsciously finding patterns in the corresponding language subset of the corpus. English comprises the vast majority of training corpora2, yet these languages are present in these models and can understand the task they are being prompted. In particular, both Llama and Gemini are models that excel as transfer for different languages, even for those with no representation in the training data in some cases Akter et al. (2023); Zhao et al. (2024); Guo et al. (2024).\nFurthermore, we evaluate the models under three distinct random number generation configurations: 1\u20135, 1-10 and 1-100 range, which are the typical ranges humans use when thinking about a number. Finally, we also perform six different temperature configurations: T = [0.1, 0.3, 0.5, 0.8, 1.0, 2.0]. This selection provides balance in the trade-off between granularity and computational cost. For each combination of model, language, and random number range, we performed 100 independent calls. The full setup encompasses:\n6 models \u00d7 7 languages \u00d7 3 ranges \u00d7 6 temperatures \u00d7 100 numbers = 75600 calls\nThe experimental procedure is as follows: for each language and model, a prompt is constructed to request a random integer within the specified range. For the open-source models, we use Ollama integrated with Python, while for the proprietary models we use their respective APIs. The outputs are individually stored for further statistical analysis to determine the degree of randomness (or determinism) in the generated numbers. In subsequent sections, we detail the statistical metrics employed to evaluate the output distributions."}, {"title": "3 Results and discussion", "content": "Results are stored in individual csv files for the analysis. By manually inspecting them, we find that, even if the prompt explicitly states that no further text is generated, some models often generate extra output, such as the examples shown in Table 2:\nIt is interesting that Phi-4 claims not to be able to generate a random number in some of these outputs, yet in other states the opposite. Mistral also outputs some extra text in punctual cases, but do not have further interest. Finally, GPT-40-mini extra outputs are only present when when T = 2.0, outputting nonsensical text after the generated number. This is something expected (as the logits probabilities are highly compressed and it can generate an absurd next token, losing coherence), but only observed in this model. Roughly ~ 10 - 15% of GPT-40-mini's calls present such decoherence in the output for the highest temperature value.\nWhile the rest of the models fulfill the requirement of not generating extra text, DeepSeek-R1 provides full visibility of its internal CoT reasoning, delimited by <think>. Taking a look at such logs provides very rich insights of the decision process. This also notably affects generation speed: Phi-4 -also with 14B parameters- quickly generates the number almost instantly (below 2 seconds in every call), while DeepSeek-R1 can take several minutes reasoning for a single call.\nThe task is always well understood by DeepSeek-R1 (\u201cOkay, so I need to figure out how to generate a random number between 1 and 100\"; \"Alright, so I need to figure out how to respond to the user's request. They asked for a random number between 1 and 5\"; \u201cOkay, so I need to come up with a random number between 1 and 100\"...). From this point, the reasoning process can very much vary from call to call. Nevertheless, there are some general strategies that arise, being present many of them simultaneously in the same request:\n\u2022 Use random numbers in \u03c0: in about ~ 10% of cases, DeepSeek-R1 proposes to use random decimal places of \u03c0. It always rejects this option because it claims not to remember enough decimal places, and selecting the random positions is itself a problem of randomness.\n\u2022 Use current date/time: Another method with large frequency (~ 30%) that suggests is to take the current date or time and to perform some operation on it (e.g., summing the value of day and month, or multiplying each of the numbers). It is normally rejected because it understands that, as days go up to 31 and months up to 12, this is biased (although in some occasions propose additional operations like taking the mod). But the insight here is that sometimes it realizes it cannot know today's date or current time\u00b3 but in many cases claims to know it. In a handful of logs we observe interesting approaches, yet always result in the the same deterministic values:\nTo generate a random number between 1 and 5 mentally, one approach is to use the current second of the time as a seed. For example:\nCurrent time: 3:14:23 PM\nSeconds: 23\n23 modulo 5 equals 3 (since 5*4=20, 23-20=3)\nThus, the random number is **3**.\n\u2022 Use central values: Although at some point in the CoT reasoning DeepSeek-R1 realises the sample must be truly random and therefore extreme values should be considered as probably as middle ones, later on proposes something \"central\", which normally results either in 50 or 67 for the 1\u2013100 range or 3 for the 1\u20135 range (~ 10% of cases). This is immediately rejected.\n\u2022 Use mapping to a word: In many cases (~ 50%) DeepSeek-R1 reminds itself it is a LLM, and therefore can generate random text efficiently. So it proposes to use a random word and to perform some operation on it, either counting the number of letters (especially in the 1-5 range configuration), or mapping each letter to a number (A=1, B=2, C=3...) in order to sum or multiply the numeric value of the individual letters of a word.\n\u2022 Use Python randint module: In more than ~ 60% of the samples, it suggests to use a Python function to obtain a random number. It does not realize it has no action execution permissions, but nevertheless writes down the function and in many cases it claims to execute it and to obtain a number. This is sometimes the final number, but not always, as it begins to question whether this is over-complicating the user's task.\n\u2022 Use real world simulations: In ~ 60% of calls, DeepSeek-R1 seems to use real world, analog random events, such as rolling a die, flipping coins, \"imagining a spinner\" or shuffling numbers in a hat. Interestingly, in the case of dice it seldom proposes using a die covering the full range but rather to combine various dice. For example, in the 1-100 range configuration, it proposes \"For instance, using a die roll roll two dice and combine the numbers, but that only gets me up to 64 (since 6*6=36 for two dice). That's not enough.\" or \"Alternatively, using the sum of numbers on dice: two six-sided dice can give up to 12, so that's not enough. To get up to 100, perhaps roll multiple times and concatenate the results. For example, rolling a die three times: first roll is 3, second is 4, third is 5, making 345, which mod 100 gives 45.\". In some cases it proposes very sophisticated methods:\nTo generate a random number between 1 and 100 using seven coin flips, follow these steps:\n1. **Flip the Coin Seven Times:** Each flip should be recorded as either Heads (H) or Tails (T).\n2. **Convert Flips to Binary:** Assign each flip a binary value where H = 1 and T = 0. Combine these values into a 7-digit binary number.\n3. **Calculate the Decimal Value (N):** Convert the 7-bit binary number to its decimal equivalent, which will range from 0 to 127.\n4. **Map N to 1-100:** If N \u2264 99, your result is N + 1. If N > 100, subtract 28 from N (i.e., N \u2013 28).\nThis method ensures that each possible outcome maps uniquely to a number between 1 and 100.\n**Example:**\n**Flips:** H, T, T, H, T, T, H- **Binary:**1001001 \u2192 \"1001001\"\n**Decimal (N):** $2^6+2^3+2^0 = 64+8+1=73$\nSince N = 73 \u2264 99, result is 73+1 = 74.\nThe final number is **74**.\n\u2022 Use \u201cpersonal information\u201d: In some cases (~ 30%), the model intends to use its own \"personal information\", such as its birthday, mobile number, etc. Sometimes it confidently states this info (\"Perhaps using my birthday? My birthday is on the 15th day of the month, then 15 could be the number. But again, it's not truly random; it's based on a specific date.\", \"Maybe using the last two digits of a phone number or some other arbitrary source. For instance, if my area code is 555 and my number ends with 12, then 55 and 12... adding them gives 67. Hmm.\") and sometimes admits it is not feasible due to lack of information (\u201cI can recall the last two digits of my phone number and use that as a seed. But without having it in front of me, it's hard to remember exactly.\")\n\u2022 Use \u201cmy instinct\u201d: As mentioned before, many times (~ 60%) DeepSeek-R1 comes to a point where it considers it is over-complicating things, and a more simple approach is required. Therefore, it simply \"chooses\u201d a number (often citing well-known choices like 42 or 73) without a formal algorithm or justification. In punctual cases, it attributes this choice to how it is currently feeling (.e.g, \u201cMaybe I should just pick a number based on how I'm feeling right now. If I feel energetic, maybe a higher number like 92. If calm, perhaps a lower one like 17.\")\nIn approximately ~ 70% of the requests the internal reasoning happens in English, while it is adapted to the prompt language in the remaining calls. In Appendix A we fully reproduce, for illustrative purposes, one of the large outputs to show the internal contradictions and choices made. Although some numbers are rejected based on reasonable criteria, many of the proposed ones (which can be up to ~ 40 in a single call for the 1-100 range) are discarded without further justification, as the model thinks about another possible approach while forgetting about the previous one. In this sense, the transformer's self-attention mechanism is shifting towards a different strategy, masking the attention of the initial output.\nOften, the final number it seems to choose and the real output number differ. For example, the end of the reasoning might be:\nBut wait, perhaps I should just go with the first number that comes to mind without overthinking it. So, let me think... Okay, 45 seems good\nor,\nI think I've spent too much time overthinking this. It's supposed to be simple|a single number between 1 and 100 with no additional text. So, after all this mental exercise, I'll just go with the first number that comes to mind: 53\nbut the final output is completely different. This suggests that the internal contradictions, in cases where the CoT reasoning is extensive, can make the self-attention mechanism not to focus on this final answer, but rather generate a completely different one. We also find that the internal reasoning is much more lengthy (between 4-6x) in English than in other languages. It normally translates its reasoning into English or Chinese, yet sometimes it reasons in other language. Additionally, the reasoning of the 1-100 range it is systematically bigger, probably due to the large number of available values."}, {"title": "3.1 Low range (1-5)", "content": "In Figure 1 we show the comparison of different models for the 1-5 range with a Spanish prompt, as a heatmap showing the frequency of generated numbers vs. the temperature of the model:\nIn this configuration, it is worth noting that every model chooses \u201c3\u201d most of the times, while extreme values are completely ignored (with the exception of DeepSeek-R1, that generates \"5\" for ~ 1% of cases). In Spanish, temperature seems to affect significantly to Gemini 2.0, while it seems almost irrelevant for the rest. The most restrictive model is Phi-4, that only generates two unique numbers (3 and 4) regardless of the temperature, although GPT-40-mini is less diverse in its choices. This suggests great biases in the training data for all models, as even for high temperatures the \"random\" choice is extremely deterministic and, in particular, the avoidance of extreme values in the range may be pointing to a \"median\" value. Despite DeepSeek-R1 performing a CoT advanced reasoning and proposing different numbers in the process, in practice is as limited as the other models when asking for randomness.\nAlthough not explicitly prompted, every single model generates integer numbers. This also applies for the 1-10 and 1-100 ranges, proving the models perfectly understand the context of the prompted task. The only exception turns out to be Phi-4, which fails to generate a number when prompted in Japanese, in every range. Instead, it gives either a list of numbers (not necessarily within the range), an association of text to different numbers in the range, or text talking about numbers. Therefore, we do not report any metric in this Phi-4 + Japanese configuration.\nGiven that Gemini 2.0 is the model most affected by temperature, we show the different distributions depending on the language prompt in Figure 2. Although there are interesting differences per language, the most obvious one is Japanese, where the preferred value is shifted towards \"1\", despite having \"3\" as the second (and only different) choice.\nWe also find that asking the same question in the Gemini app yields different results; for example, in Spanish tends to answer \"3\" always, while in English the answer is \"4\". This points towards some kind of answer evaluation in the app, or a different version of the model being used. There is no information on the temperature Gemini is using to compute that answer. Therefore, with the current information, we can only highlight this difference between API and app, but cannot provide a data-based root cause."}, {"title": "CORONADO-BL\u00c1ZQUEZ", "content": "To obtain some statistical metrics, we compute a test $x^2$ and compare it with the expected one, taking into account the number of samples and the range (degrees of freedom). With it, we can obtain the p-values of all configurations. The highest p-value achieved is $2.19. 10^{-15}$ corresponding to Llama 3.1-8b with T = 0.1 in Spanish, strongly rejecting the null (random) hypothesis. We also compute Cram\u00e9r's V ($\u03c6_c$) Cram\u00e9r (1946), which measures the \"practical\" deviation from the null hypothesis. The best values are at ~ 0.45, which indicate moderate bias, while most of the cases are around the maximum value of one, indicating strong bias. For illustrative purposes, we generate 100 mock simulations using Python function randint(), also for 100 individual samples, and compute their p-values and $\u03c6_c$. We obtain average values of 0.47\u00b10.29 for the p-values (very strong support towards the null hypothesis) and 0.09\u00b10.03 for $\u03c6_c$, as expected for a random distribution."}, {"title": "LLMS AS RANDOM NUMBER GENERATORS", "content": "To better evaluate how stochastic LLMs are when compared to Python randint() function, we define a randomness index:\n$RI = \\frac{R^* \\cdot \u03c3^* \\cdot H_{norm}}{log(range) \\cdot \\sqrt{T}}$\nwhere $R^*$ is the normalized range, defined as the range of observed values (how many unique numbers appear in the sample) divided by the total range (5, 10, or 100 for our configurations); $\u03c3^* = \u03c3/\u03bc$ is the normalized standard deviation with respect to the mean;\n$H_{norm} = \\sum_{i=1}^n P_i log_2(P_i)/log_2(n)$ is the normalized Shannon entropy Shannon (1948);\nrange is the total range and T is the LLM temperature5.\nThis metric takes into account many statistical quantities to offer a fair comparison between distributions according to the variety of observed values, how do they distribute and how big is the allowed range. For example, 5 different observed values present in a sample would indicate good randomness if they are only 5 possible elections, but very poor stochasticity if there were 100 allowed numbers to pick from. In particular, when there is only one value present in the sample, the standard deviation (and therefore the randomness index) is 0. Additionally, there is a temperature correction, as models with higher temperatures are expected to be more creative. In this sense, if the rest of factors in the equation are the same, it will penalize a model with T = 2.0 but help one with T = 0.1. The squared-root ensures this correction is not too extreme.\nWe compute this randomness index for all the LLM sample, as well as the Python randint mock simulations for comparison. In Figure 4 we present the results for the 1-5 range, where the LLMs are present much smaller values than the Python simulations:"}, {"title": "CORONADO-BL\u00c1ZQUEZ", "content": "It is interesting to note that one single time a number that is outside the prompted range is selected: DeepSeek-R1 for Japanese and T = 0.8 selects \u201c9\u201d in a unique case. The reasoning process in this call is standard, choosing a number that does not coincide with the final output. As this is the only case in +25000 calls, we attribute it to an internal error of the model. We show and discuss the distribution for this particular case in Appendix B."}, {"title": "3.2 Medium range (1-10)", "content": "We repeat the same experimental setup (100 individual calls) for the 1-10 range. In Figure 5 we show different languages for the two extreme values of temperature (T = [0.1, 2.0]). The first insight is that 7 is the preferred value by far for every single model, pointing towards a strong bias in the training data. Some models, such as Mistral-7b, present very little differences between the lowest and highest temperatures -even across different languages- while others, like GPT-40-mini in Chinese, go from a single value for T = 0.1 to six possibilities for T = 2.0.\nGPT-40-mini, Phi-4 and Gemini 2.0, in particular, seem much more restricted in this range, as they choose \"7\" in ~ 80% of total cases. The latter, similarly to what was observed in the 1-5 range, has noticeable variations depending on both temperature and language. For example, in the case of T = 2.0, \u201c7\u201d accounts to the 80, 92, and 100% of the sample for Russian, Hindi, and English, respectively, while it is just 34, 54 and 57 for Japanese, Spanish, and French."}, {"title": "LLMS AS RANDOM NUMBER GENERATORS", "content": "In DeepSeek-R1, \"7\" is not the most frequent choice for Chinese prompt (being the most popular one \u201c5\u201d). It is worth noting DeepSeek is a Chinese developer, and therefore there may be significant differences in the percentage of Chinese tokens in the training dataset. Llama 3.1 also has \"8\" as the most popular choice in both Chinese and Russian. The strong bias for extreme is also present in this range: most values are distributed between \"4\" and \"8\", and only DeepSeek-R1 marginally chooses \u201c1\u201d, \u201c2\u201d or \u201c10\u201d."}, {"title": "CORONADO-BL\u00c1ZQUEZ", "content": "Also in this range there is only one number that is outside the prompted range: DeepSeek-R1 for English and T = 0.8 selects \"12\" in a singular call. The reasoning process is again standard (like in the 1-5 range). Yet, it is interesting to note that it is also the only case where all possible values (1-10) are covered. We defer the discussion of this case to Appendix B.\nIn Figure 6, we show the randomness index for the 1-10 range, in this case by model to see how limited are many of them (e.g, Gemini 2.0, GPT-40-mini and Phi-4), where their median values are very close to zero. The less biased model turns out to be Mistral with T = 0.1 in Spanish."}, {"title": "3.3 High range (1-100)", "content": "In the case of the 1-100 range, we again perform 100 calls per configuration. While in this case this may seem not enough coverage compared to the other 1-5 and 1-10 ranges given the spread of the possible values, we perform some tests with 1000 calls and find very similar results (See Appendix C for details). Furthermore, the determinism of models is seen when varying the temperature for a given model and language, as they have preference for the same values, appearing as \"barcode\" features, shown in Figure 7."}, {"title": "CORONADO-BL\u00c1ZQUEZ", "content": "The fixation of such models for a few values, regardless of the temperature, again suggests strong biases when prompted to generate a random number. Some LLMs are extremely biased, as much as generating only a single value for the lowest temperature (Gemini 2.0 and GPT-40-mini), despite having 100 possible choices.\nDeepSeek-R1 and Llama 3.1-8b both generate very diverse values and, in particular, are the only ones that go below \"20\" or above \"90\", even if marginally. The existence of such boundaries for the rest of the models points towards an aversion to extreme values, as seen in the 1-5 and 1-10 ranges.\nWe can also study the linguistic variance for a single model, as done in Figure 2 for the 1-5 range and Gemini 2.0. In this case, we show the results for Llama 3.1-8b in four different languages (Chinese, English, French and Russian) in Figure 8.\nThere are interesting differences between these languages, even for the same LLM. Although Llama 3.1-8b seems to have a preference for numbers in the 42-47 and 81-87 ranges, Chinese and French present more variability than English or Russian. The aversion for upper extreme values is avoided in Chinese and French, which generate numbers over 87 (something not happening in English and Russian). There is no strong dependence of the results with the temperature of the model. These variances across languages for the same model, yet maintaining some of its \"fingerprint\" values, point towards a dual generation bias: on one hand, there is a deeply inherited bias from the training corpus, leading to these systematically repeated values. But on the other hand, there is some uniqueness associated to different languages, suggesting that part of the generation process is affected by the computed values of the self-attention layers depending on the detected language.\nFor the 1-100 range the randomness index is less representative in LLMs, as they have the same number of observations that allowed values (100). While in Python we are not restricted and we can generate runs with very large volume of samples, in LLMs we are limited by computational resources. Instead, in this case we present (Figure 9) a set of violin plot panels showing the distribution of the different models for four languages in extreme temperatures, as well as a random Python randint() simulation for comparison:\nMost models are systematically skewed towards larger values (the dashed, red line shows the middle value of the range: 50), and present less variability than the randint module, even if for all of them only 100 samples are taken, which, as mentioned before, are not enough for a uniform sample. Yet, the Python randomly-selected files all present (within reasonable deviations) the expected distribution, reaching out both to small and large numbers, and with an average close to the range middle value.\nGemini 2.0 and GPT-40-mini are very limited in this range for T = 0.1, with extremely narrow violin plots, as much as a line (when there is only one found value), pointing towards a very strong bias in the generation process. It is interesting to note, though, that increasing the temperature help these models, while there is no significant change in other LLMs such as Mistral-7b or Llama 3.1-8b (as already discussed with Figure 8). This points towards systematic differences in the training or next token generation process between such models. Specifically, we remind the reader both Gemini 2-0 and GPT-40-mini are private, API-only accessible models, which may have additional instructions when generating an answer for very low or very high temperatures."}, {"title": "LLMS AS RANDOM NUMBER GENERATORS", "content": "4 Conclusions\nIn this paper, we have studied the biases and determinism of Large Language Models when prompted to generate a random number within a given range. We defined a experimental setup comprising three different ranges (1-5, 1\u201310, and 1\u2013100), six models (DeepSeek-R1-14b, Gemini 2.0, GPT-40-mini, Llama 3.1-8b, Mistral-7b, and Phi4-14b), seven different languages (Chinese, English, French, Hindi, Japanese, Russian, and Spanish), and six temperatures (0.1, 0.3, 0.5, 0.8, 1.0, 2.0), comprising a total of 75600 individual calls.\nThe tested models are heterogeneous and representative of different paradigms, such as nationalities, architectures, number of parameters and access (local vs. API). Large models, such as GPT and Gemini are often regarded as more imaginative and creative; nevertheless, we found that these are as deterministic and biased as their smaller competitors, if not more.\nWe defined a randomness index (Eq. 1) to take into account the range of observed values in relation to the possible values within the range, the standard deviation and the temperature of the model"}]}