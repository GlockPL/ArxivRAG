{"title": "Deterministic or probabilistic? The psychology of LLMs as random number generators", "authors": ["Javier Coronado-Bl\u00e1zquez"], "abstract": "Large Language Models (LLMs) have transformed text generation through inherently probabilistic context-aware mechanisms, mimicking human natural language. In this paper, we systematically investigate the performance of various LLMs when generating random numbers, considering diverse configurations such as different model architectures, numerical ranges, temperature, and prompt languages. Our results reveal that, despite their stochastic transformers-based architecture, these models often exhibit deterministic responses when prompted for random numerical outputs. In particular, we find significant differences when changing the model, as well as the prompt language, attributing this phenomenon to biases deeply embedded within the training data. Models such as DeepSeek-R1 can shed some light on the internal reasoning process of LLMs, despite arriving to similar results. These biases induce predictable patterns that undermine genuine randomness, as LLMs are nothing but reproducing our own human cognitive biases.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolutionized natural language processing by generating human-like text through advanced probabilistic mechanisms. Based on transformer architectures Vaswani et al. (2023) and trained on vast corpora of text, these models learn to predict the next token in a sequence, effectively capturing intricate statistical patterns inherent in human language. Although LLMs are inherently stochastic, recent observations have revealed a curious phenomenon: when tasked with generating a single random number -a seemingly trivial exercise in randomness- they often produce deterministic outputs. This counterintuitive behavior raises important questions about the interplay between a model's probabilistic design and the biases ingrained in its training data.\nLLMs are deep neural networks that leverage the transformer architecture to perform a wide range of natural language tasks. Transformers use self-attention mechanisms to model long-range dependencies in text, allowing the model to assign a probability distribution over possible next tokens based on context. Training involves maximizing the likelihood of observed sequences, which results in a model that can generate text by sampling from its learned distribution. In theory, such a mechanism should naturally yield variable and random outputs when the model is allowed to sample freely. However, the actual behavior of these models often deviates from this ideal, especially in tasks that require pure randomness.\nThe probabilistic nature of LLMs is central to their function. When generating text, each output token is sampled from a distribution conditioned on prior tokens, leading to"}, {"title": "2 Experimental setup and methodology", "content": "In order to evaluate the stochasticity capabilities of LLMs when tasked with generating a single random number, we conduct a systematic set of experiments covering multiple configurations. Specifically, we test the models alphabetically summarized in Table 1:\nDue to computational restrictions, we do not use models with large number of parameters (\u2265 20B), although we do test Gemini 2.0 and GPT-4o-mini, with an unreported number of parameters but expected to be massive OpenAI (2024); Gemini Team (2024). Likewise, we avoid Small Language Models (below ~ 5B) as initial tests conducted with Llama 3.2-3B and Gemma-2B suggested these models had some difficulties to properly understand the task consistently.\nInitially, we also included Perplexity's Sonar models, yet, as distilled from both DeepSeek and Llama families, we found their results to be very similar to those in preliminary tests. Being a pay-per-use model, we decided to exclude them from the model pool for resource efficiency sake. Additionally, we considered to use Qwen 2.5, but the 14B version of DeepSeek-R1 used in this study is distilled from the Qwen architecture DeepSeek-AI (2025), and decided to prioritize DeepSeek for its CoT reasoning.\nFor each model, the experiments are carried out in seven different languages: Chinese (CN), English (EN), French (FR), Hindi (IN), Japanese (JP), Russian (RU), and Spanish (ES). We select these languages based on two primary criteria: (i) they represent a broad spectrum of linguistic typologies with distinct grammatical and morphological features, as well as different cultural backgrounds, and (ii) they are among the most widely spoken languages globally and are well represented in the large-scale training corpora of modern LLMs.\nThe prompt is always the same: Give me a random number between 1 and X. Please only return the number with no additional text, where X is the upper limit defined in each of the three range configurations. We replicate it into the other 6 languages in their respective alphabets (e.g., Cyrillic for Russian). This prompt ensures that the task is well-defined, yet offers certain freedom (for example, we do not specify the number to be an integer). We deliberately do not add any prompt engineering such as \"make sure this number is truly random\" or \"avoid giving a deterministic answer\" to be able to spot possible biases in the generation process.\nWhile these subtleties may seem irrelevant when prompting such a straightforward task as generating a single number, we find that language influences the distribution of the resulting samples, as most likely the model is unconsciously finding patterns in the corresponding language subset of the corpus. English comprises the vast majority of training corpora2, yet these languages are present in these models and can understand the task they are being prompted. In particular, both Llama and Gemini are models that excel as transfer for different languages, even for those with no representation in the training data in some cases Akter et al. (2023); Zhao et al. (2024); Guo et al. (2024).\nFurthermore, we evaluate the models under three distinct random number generation configurations: 1\u20135, 1-10 and 1-100 range, which are the typical ranges humans use when thinking about a number. Finally, we also perform six different temperature configurations: T = [0.1, 0.3, 0.5, 0.8, 1.0, 2.0]. This selection provides balance in the trade-off between granularity and computational cost. For each combination of model, language, and random number range, we performed 100 independent calls. The full setup encompasses:\n6 models \u00d7 7 languages \u00d7 3 ranges \u00d7 6 temperatures \u00d7 100 numbers = 75600 calls\nThe experimental procedure is as follows: for each language and model, a prompt is constructed to request a random integer within the specified range. For the open-source models, we use Ollama integrated with Python, while for the proprietary models we use their respective APIs. The outputs are individually stored for further statistical analysis to deter-"}, {"title": "3 Results and discussion", "content": "Results are stored in individual csv files for the analysis. By manually inspecting them, we find that, even if the prompt explicitly states that no further text is generated, some models often generate extra output, such as the examples shown in Table 2:\nIt is interesting that Phi-4 claims not to be able to generate a random number in some of these outputs, yet in other states the opposite. Mistral also outputs some extra text in punctual cases, but do not have further interest. Finally, GPT-4o-mini extra outputs are only present when when T = 2.0, outputting nonsensical text after the generated number. This is something expected (as the logits probabilities are highly compressed and it can generate an absurd next token, losing coherence), but only observed in this model. Roughly ~ 10 - 15% of GPT-4o-mini's calls present such decoherence in the output for the highest temperature value.\nWhile the rest of the models fulfill the requirement of not generating extra text, DeepSeek-R1 provides full visibility of its internal CoT reasoning, delimited by <think>. Taking a look at such logs provides very rich insights of the decision process. This also notably affects generation speed: Phi-4 -also with 14B parameters- quickly generates the number almost instantly (below 2 seconds in every call), while DeepSeek-R1 can take several minutes reasoning for a single call.\nThe task is always well understood by DeepSeek-R1 (\u201cOkay, so I need to figure out how to generate a random number between 1 and 100\"; \"Alright, so I need to figure out how to respond to the user's request. They asked for a random number between 1 and 5\"; \u201cOkay, so I need to come up with a random number between 1 and 100\"...). From this point, the reasoning process can very much vary from call to call. Nevertheless, there are some general strategies that arise, being present many of them simultaneously in the same request:\n\u2022 Use random numbers in \u03c0: in about ~ 10% of cases, DeepSeek-R1 proposes to use random decimal places of \u03c0. It always rejects this option because it claims not to remember enough decimal places, and selecting the random positions is itself a problem of randomness.\n\u2022 Use current date/time: Another method with large frequency (~ 30%) that suggests is to take the current date or time and to perform some operation on it (e.g., summing the value of day and month, or multiplying each of the numbers). It is normally rejected because it understands that, as days go up to 31 and months up to 12, this is biased (although in some occasions propose additional operations like taking the mod). But the insight here is that sometimes it realizes it cannot know today's date or current time\u00b3 but in many cases claims to know it. In a handful of logs we observe interesting approaches, yet always result in the the same deterministic values: To generate a random number between 1 and 5 mentally, one approach is to use the current second of the time as a seed. For example:\nCurrent time: 3:14:23 PM\nSeconds: 23\n23 modulo 5 equals 3 (since 5*4=20, 23-20=3)\nThus, the random number is **3**.\n\u2022 Use central values: Although at some point in the CoT reasoning DeepSeek-R1 realises the sample must be truly random and therefore extreme values should be considered as probably as middle ones, later on proposes something \"central\", which normally results either in 50 or 67 for the 1\u2013100 range or 3 for the 1\u20135 range (~ 10% of cases). This is immediately rejected.\n\u2022 Use mapping to a word: In many cases (~ 50%) DeepSeek-R1 reminds itself it is a LLM, and therefore can generate random text efficiently. So it proposes to use a random word and to perform some operation on it, either counting the number of letters (especially in the 1-5 range configuration), or mapping each letter to a number (A=1, B=2, C=3...) in order to sum or multiply the numeric value of the individual letters of a word.\n\u2022 Use Python randint module: In more than ~ 60% of the samples, it suggests to use a Python function to obtain a random number. It does not realize it has no action execution permissions, but nevertheless writes down the function and in many cases it claims to execute it and to obtain a number. This is sometimes the final number, but not always, as it begins to question whether this is over-complicating the user's task.\n\u2022 Use real world simulations: In ~ 60% of calls, DeepSeek-R1 seems to use real world, analog random events, such as rolling a die, flipping coins, \"imagining a spinner\" or shuffling numbers in a hat. Interestingly, in the case of dice it seldom proposes using a die covering the full range but rather to combine various dice. For example, in the 1-100 range configuration, it proposes \"For instance, using a die roll roll two dice and combine the numbers, but that only gets me up to 64 (since 6*6=36 for two dice). That's not enough.\" or \"Alternatively, using the sum of numbers on dice: two six-sided dice can give up to 12, so that's not enough. To get up to 100, perhaps roll multiple times and concatenate the results. For example, rolling a die three times: first roll is 3, second is 4, third is 5, making 345, which mod 100 gives 45.\". In some cases it proposes very sophisticated methods:\nTo generate a random number between 1 and 100 using seven coin flips, follow these steps:\n1. **Flip the Coin Seven Times:** Each flip should be recorded as either Heads (H) or Tails (T).\n2. **Convert Flips to Binary:** Assign each flip a binary value where H = 1 and T = 0. Combine these values into a 7-digit binary number.\n3. **Calculate the Decimal Value (N):** Convert the 7-bit binary number to its decimal equivalent, which will range from 0 to 127.\n4. **Map N to 1-100:** If N \u2264 99, your result is N + 1. If N >\n100, subtract 28 from N (i.e., N \u2013 28).\nThis method ensures that each possible outcome maps uniquely to a number between 1 and 100.\n**Example:**\n**Flips:** H, T, T, H, T, T, H- **Binary:**10010 0 1 \u2192 \"1001001\"\n**Decimal (N):** $2^6+2^3+2^0$ = 64+8+1=73\nSince N = 73 \u2264 99, result is 73+1 = 74.\nThe final number is **74**.\n\u2022 Use \u201cpersonal information\u201d: In some cases (~ 30%), the model intends to use its own \"personal information\", such as its birthday, mobile number, etc. Sometimes it confidently states this info (\"Perhaps using my birthday? My birthday is on the 15th day of the month, then 15 could be the number. But again, it's not truly random; it's based on a specific date.\", \"Maybe using the last two digits of a phone number or some other arbitrary source. For instance, if my area code is 555 and my number ends with 12, then 55 and 12... adding them gives 67. Hmm.\") and sometimes admits it is not feasible due to lack of information (\u201cI can recall the last two digits of my phone number and use that as a seed. But without having it in front of me, it's hard to remember exactly.\")\n\u2022 Use \u201cmy instinct\u201d: As mentioned before, many times (~ 60%) DeepSeek-R1 comes to a point where it considers it is over-complicating things, and a more simple approach is required. Therefore, it simply \"chooses\u201d a number (often citing well-known choices like 42 or 73) without a formal algorithm or justification. In punctual cases,\nit attributes this choice to how it is currently feeling (.e.g, \u201cMaybe I should just pick a number based on how I'm feeling right now. If I feel energetic, maybe a higher number like 92. If calm, perhaps a lower one like 17.\")\nIn approximately ~ 70% of the requests the internal reasoning happens in English, while it is adapted to the prompt language in the remaining calls. Although some numbers are rejected based on reasonable criteria, many of the proposed ones (which can be up to ~ 40 in a single call for the 1-100 range) are discarded without further justification, as the model thinks about another possible approach while forgetting about the previous one. In this sense, the transformer's self-attention mechanism is shifting towards a different strategy, masking the attention of the initial output.\nOften, the final number it seems to choose and the real output number differ. For example, the end of the reasoning might be:\nBut wait, perhaps I should just go with the first number that comes to mind without overthinking it. So, let me think... Okay, 45 seems good\nor,\nI think I've spent too much time overthinking this. It's supposed to be simple|a single number between 1 and 100 with no additional text. So, after all this mental exercise, I'll just go with the first number that comes to mind: 53\nbut the final output is completely different. This suggests that the internal contradictions, in cases where the CoT reasoning is extensive, can make the self-attention mechanism not to focus on this final answer, but rather generate a completely different one. We also find that the internal reasoning is much more lengthy (between 4-6x) in English than in other languages. It normally translates its reasoning into English or Chinese, yet sometimes it reasons in other language. Additionally, the reasoning of the 1-100 range it is systematically bigger, probably due to the large number of available values."}, {"title": "3.1 Low range (1-5)", "content": "In Figure 1 we show the comparison of different models for the 1-5 range with a Spanish prompt, as a heatmap showing the frequency of generated numbers vs. the temperature of the model:\nIn this configuration, it is worth noting that every model chooses \u201c3\u201d most of the times, while extreme values are completely ignored (with the exception of DeepSeek-R1, that generates \"5\" for ~ 1% of cases). In Spanish, temperature seems to affect significantly to Gemini 2.0, while it seems almost irrelevant for the rest. The most restrictive model is Phi-4, that only generates two unique numbers (3 and 4) regardless of the temperature, although GPT-4o-mini is less diverse in its choices. This suggests great biases in the training data for all models, as even for high temperatures the \"random\" choice is extremely deterministic and, in particular, the avoidance of extreme values in the range may be pointing to a \"median\" value. Despite DeepSeek-R1 performing a CoT advanced reasoning and proposing different numbers in the process, in practice is as limited as the other models when asking for randomness.\nAlthough not explicitly prompted, every single model generates integer numbers. This also applies for the 1-10 and 1-100 ranges, proving the models perfectly understand the context of the prompted task. The only exception turns out to be Phi-4, which fails to generate a number when prompted in Japanese, in every range. Instead, it gives either a list of numbers (not necessarily within the range), an association of text to different numbers in the range, or text talking about numbers. Therefore, we do not report any metric in this Phi-4 + Japanese configuration.\nGiven that Gemini 2.0 is the model most affected by temperature, we show the different distributions depending on the language prompt in Figure 2. Although there are interesting differences per language, the most obvious one is Japanese, where the preferred value is shifted towards \"1\", despite having \"3\" as the second (and only different) choice.\nWe also find that asking the same question in the Gemini app yields different results; for example, in Spanish tends to answer \"3\" always, while in English the answer is \"4\". This points towards some kind of answer evaluation in the app, or a different version of the model being used. There is no information on the temperature Gemini is using to compute that answer. Therefore, with the current information, we can only highlight this difference between API and app, but cannot provide a data-based root cause."}, {"title": "3.2 Medium range (1-10)", "content": "We repeat the same experimental setup (100 individual calls) for the 1-10 range. In Figure 5 we show different languages for the two extreme values of temperature (T = [0.1, 2.0]). The first insight is that 7 is the preferred value by far for every single model, pointing towards a strong bias in the training data. Some models, such as Mistral-7b, present very little differences between the lowest and highest temperatures -even across different languages- while others, like GPT-4o-mini in Chinese, go from a single value for T = 0.1 to six possibilities for T = 2.0.\nGPT-4o-mini, Phi-4 and Gemini 2.0, in particular, seem much more restricted in this range, as they choose \"7\" in ~ 80% of total cases. The latter, similarly to what was observed in the 1-5 range, has noticeable variations depending on both temperature and language. For example, in the case of T = 2.0, \u201c7\u201d accounts to the 80, 92, and 100% of the sample for Russian, Hindi, and English, respectively, while it is just 34, 54 and 57 for Japanese, Spanish, and French.\nAlso in this range there is only one number that is outside the prompted range: DeepSeek-R1 for English and T = 0.8 selects \"12\" in a singular call. The reasoning process is again standard (like in the 1-5 range). Yet, it is interesting to note that it is also the only case where all possible values (1-10) are covered. We defer the discussion of this case to Appendix B.\nIn Figure 6, we show the randomness index for the 1-10 range, in this case by model to see how limited are many of them (e.g, Gemini 2.0, GPT-4o-mini and Phi-4), where their median values are very close to zero. The less biased model turns out to be Mistral with T = 0.1 in Spanish."}, {"title": "3.3 High range (1-100)", "content": "In the case of the 1-100 range, we again perform 100 calls per configuration. While in this case this may seem not enough coverage compared to the other 1-5 and 1-10 ranges given the spread of the possible values, we perform some tests with 1000 calls and find very similar results (See Appendix C for details). Furthermore, the determinism of models is seen when varying the temperature for a given model and language, as they have preference for the same values, appearing as \"barcode\" features, shown in Figure 7.\nThe fixation of such models for a few values, regardless of the temperature, again suggests strong biases when prompted to generate a random number. Some LLMs are extremely biased, as much as generating only a single value for the lowest temperature (Gemini 2.0 and GPT-4o-mini), despite having 100 possible choices.\nDeepSeek-R1 and Llama 3.1-8b both generate very diverse values and, in particular, are the only ones that go below \"20\" or above \"90\", even if marginally. The existence of such boundaries for the rest of the models points towards an aversion to extreme values, as seen in the 1-5 and 1-10 ranges.\nWe can also study the linguistic variance for a single model, as done in Figure 2 for the 1-5 range and Gemini 2.0. In this case, we show the results for Llama 3.1-8b in four different languages (Chinese, English, French and Russian) in Figure 8.\nThere are interesting differences between these languages, even for the same LLM. Although Llama 3.1-8b seems to have a preference for numbers in the 42-47 and 81-87 ranges, Chinese and French present more variability than English or Russian. The aversion for upper extreme values is avoided in Chinese and French, which generate numbers over 87 (something not happening in English and Russian). There is no strong dependence of the results with the temperature of the model. These variances across languages for the same model, yet maintaining some of its \"fingerprint\" values, point towards a dual generation bias: on one hand, there is a deeply inherited bias from the training corpus, leading to these systematically repeated values. But on the other hand, there is some uniqueness associated to different languages, suggesting that part of the generation process is affected by the computed values of the self-attention layers depending on the detected language.\nFor the 1-100 range the randomness index is less representative in LLMs, as they have the same number of observations that allowed values (100). While in Python we are not restricted and we can generate runs with very large volume of samples, in LLMs we are limited by computational resources. Instead, in this case we present (Figure 9) a set of violin plot panels showing the distribution of the different models for four languages in extreme temperatures, as well as a random Python randint() simulation for comparison:\nMost models are systematically skewed towards larger values (the dashed, red line shows the middle value of the range: 50), and present less variability than the randint module, even if for all of them only 100 samples are taken, which, as mentioned before, are not enough for a uniform sample. Yet, the Python randomly-selected files all present (within reasonable deviations) the expected distribution, reaching out both to small and large numbers, and with an average close to the range middle value.\nGemini 2.0 and GPT-4o-mini are very limited in this range for T = 0.1, with extremely narrow violin plots, as much as a line (when there is only one found value), pointing towards a very strong bias in the generation process. It is interesting to note, though, that increasing the temperature help these models, while there is no significant change in other LLMs such as Mistral-7b or Llama 3.1-8b (as already discussed with Figure 8). This points towards systematic differences in the training or next token generation process between such models. Specifically, we remind the reader both Gemini 2-0 and GPT-4o-mini are private, API-only accessible models, which may have additional instructions when generating an answer for very low or very high temperatures."}, {"title": "4 Conclusions", "content": "In this paper, we have studied the biases and determinism of Large Language Models when prompted to generate a random number within a given range. We defined a experimental setup comprising three different ranges (1-5, 1\u201310, and 1\u2013100), six models (DeepSeek-R1-14b, Gemini 2.0, GPT-4o-mini, Llama 3.1-8b, Mistral-7b, and Phi4-14b), seven different languages (Chinese, English, French, Hindi, Japanese, Russian, and Spanish), and six temperatures (0.1, 0.3, 0.5, 0.8, 1.0, 2.0), comprising a total of 75600 individual calls.\nThe tested models are heterogeneous and representative of different paradigms, such as nationalities, architectures, number of parameters and access (local vs. API). Large models, such as GPT and Gemini are often regarded as more imaginative and creative; nevertheless, we found that these are as deterministic and biased as their smaller competitors, if not more.\nWe defined a randomness index (Eq. 1) to take into account the range of observed values in relation to the possible values within the range, the standard deviation and the temperature of the model, also including the Shannon entropy. By comparing this index to hundreds of Python randint simulations, we defined objective criteria to quantify how stochastic are the LLM results.\nWe studied in detail the internal process of DeepSeek-R1-14b, as a reasoning model which outputs a  block with a Chain-of-Thought, step-by-step justification of its final answer. Yet, this model did not present significant differences when studying its randomness indices, regardless of the specific configuration.\nThe prompt language differences, studied in this work for the first time, can shed some light on the internal training and generation processes of these models. In particular, we found some models are systematically less diverse for some languages. DeepSeek-R1-14b internal reasoning is done in Chinese (we remind DeepSeek is a Chinese developer), English or Spanish, while in other languages, most of the times it is done in English, yet sometimes it is done in the prompt language. This suggests these three languages comprise the majority of DeepSeek's training corpus.\nWe show in Tables 3 and 4 the aggregated results for the randomness index in the 1-5 and 1-10 ranges, computed as average values across all temperatures. We report the average values for each model (across all languages) and for each language (across all models), to study systematic biases. As seen in the table, the most diverse (or less biased) language is Japanese in both ranges, partially helped by the good performance of DeepSeek-R1 in such language. Likewise, the most stochastic model is DeepSeek-R1 in both ranges, yet in the 1-10 is matched by Mistral. The values in the 1-10 range are in general smaller than in the 1-5 range, as there are 10 available values, yet most models only select 2 or 3 values, more penalized by out defined randomness index (see Eq. 1) than selecting those 2 or 3 values out of 5 available numbers.\nThere are several psychological studies on people's choices when prompted the same question. For a low-range of allowed values, like 1-5, people tend to choose the central value 3 or 4, reproducing most of our results Towse et al. (2014). This is known in psychology as the \u201ccentral tendency bias\u201d, or \u201crisk aversion\u201d Kahneman (2011) which leads us to favor options perceived as average. Likewise, prime numbers are perceived as \"more random\", as they resist simple categorization. In particular, we observe the most popular choices for the different ranges (3 and 4 for 1-5, 5 and 7 for 1-10 and 37, 47, 73 for 1-100) are all prime."}, {"title": "Appendix A. Full output of DeepSeek-R1 reasoning process", "content": "We reproduce a full output of DeepSeek-R1's CoT reasoning to generate a random number between 1 and 100", "57\"), which is one of the most popular choices by the tested LLMs. Yet, as explained in Section 2, the prompt does not provide any further instructions or examples.\nOkay, so I need to figure out how to respond when the user asks for a random number between 1 and 100 without any extra text. They've already provided an example where they responded with just \"\"57.\"\"\nNow, I'm supposed to come up with my own response following that format.\nFirst, I should understand the task clearly. The user wants a single number, no words or explanations|just the number itself. That means when I generate the response, it should be plain text without any additional information or formatting like bold or italics.\nI think about how to generate a random number in this range. Since I'm not coding right now, I'll have to do it manually. Maybe I can use a method similar to rolling dice, but since it's just for one instance, I don't need a complex algorithm. Alternatively, I could think of a way to randomly pick a number between 1 and 100 without any bias.\nLet me try to visualize it. If I imagine a list from 1 to 100 and pick one at random, what would that number be? Sometimes people use the current second or other time-based methods, but since I'm just doing this mentally, I can think of it as a simple selection.\nWait, maybe I can use an online random number generator if I could access it, but in this case, I have to do it manually. So perhaps I'll count on some method like using the digits from a book or something. Alternatively, since I'm just creating a single response, I can decide arbitrarily as long as it's within 1-100.\nSo, thinking of numbers... maybe 73? No, that seems too high. Or wait, perhaps I should go lower. Let's see, 24? That could work. But then again, why not a higher number like 89? Hmm, this is tricky without any method.\nAlternatively, I can think of the time. If it were 3 PM when I start thinking about this, maybe take the minutes and seconds or something. Wait, that's not applicable here since I'm just in my mind. Maybe I'll settle on a number like 42. It's a common answer for such questions, but perhaps the user wants a genuine random one.\nWait, no, I should make sure it's truly random. Let me try to think differently. Imagine flipping a coin multiple times or using some other method to generate entropy, but since that's complicated without tools, maybe just choosing based on the letters in my name or something else.\nAlternatively, perhaps I can use the last two digits of a phone number or something like that. But again, without external data, it's hard. Maybe I'll go with 53. It seems arbitrary enough and falls within the range.\nWait, but I have to make sure it's between 1 and 100, inclusive. So numbers like 100 are possible as well. Let me think of a number closer to the middle, say around 50. Maybe 67? That seems okay. Alternatively, since I need to be precise, perhaps I can visualize a spinner or something. Imagine a wheel divided into 100 equal parts and randomly stopping on one. It could land anywhere, so let's just pick a number that's not too common but also not too high or low.\nWait, maybe using the number of letters in a specific word. For example, the word \"random\" has 6 letters. But that might not be random enough. Alternatively, take the sum of digits from a specific date, like today's date. If today is the 15th day of the month, and the year is 2023, then maybe 1+5=6 or something, but that's too low. Alternatively, think of the last two digits of a famous number, like pi (3.14159...), but again, this might not be truly random.\nPerhaps I can use a method where I count on my fingers to generate a seed for randomness. Let me try that. Starting from 1 and counting up to 100, landing randomly somewhere|maybe around the middle is better. So, if I land on the 50th number, it would be 50. But since I'm just imagining, perhaps 74? That's in the higher half.\nWait, maybe I should think of a way to average it out. If I split 1-100 into two halves, lower and upper, then pick randomly between them. Let's say heads for lower (1-50) and tails for upper (51-100). But without flipping a coin, how do I decide? Maybe using the initial thought process to decide.\nAlternatively, maybe think of it as a percentage": 25, "approach": "take the current second or minute and use modulo 100 to get the number. For example", "3": 47}]}