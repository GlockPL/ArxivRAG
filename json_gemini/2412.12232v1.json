{"title": "You Only Submit One Image to Find the Most Suitable Generative Model", "authors": ["Zhi Zhou", "Peng-Xiao Song", "Lan-Zhe Guo", "Yu-Feng Li"], "abstract": "Deep generative models have achieved promising results in image generation, and\nvarious generative model hubs, e.g., Hugging Face and Civitai, have been developed\nthat enable model developers to upload models and users to download models.\nHowever, these model hubs lack advanced model management and identification\nmechanisms, resulting in users only searching for models through text matching,\ndownload sorting, etc., making it difficult to efficiently find the model that best\nmeets user requirements. In this paper, we propose a novel setting called Generative\nModel Identification (GMI), which aims to enable the user to identify the most\nappropriate generative model(s) for the user's requirements from a large number of\ncandidate models efficiently. To our best knowledge, it has not been studied yet.\nIn this paper, we introduce a comprehensive solution consisting of three pivotal\nmodules: a weighted Reduced Kernel Mean Embedding (RKME) framework for\ncapturing the generated image distribution and the relationship between images and\nprompts, a pre-trained vision-language model aimed at addressing dimensionality\nchallenges, and an image interrogator designed to tackle cross-modality issues.\nExtensive empirical results demonstrate the proposal is both efficient and effective.\nFor example, users only need to submit a single example image to describe their\nrequirements, and the model platform can achieve an average top-4 identification\naccuracy of more than 80%.", "sections": [{"title": "1 Introduction", "content": "Recently, stable diffusion models [5, 17, 19, 14] have achieved state-of-the-art performance in image\ngeneration and become one of the popular topics in artificial intelligence. Various model hubs, e.g.,\nHugging Face and Civitai, have been developed to enable model developers to upload and share\ntheir generative models. However, existing model hubs provide trivial methods such as tag filtering,\ntext matching, and download volume ranking [18], to help users search for models. However, these\nmethods cannot accurately capture the users' requirements, making it difficult to efficiently identify\nthe most appropriate model for users. As shown in Figure 1, the user should submit their requirements\nto the model hub and subsequently, they must download and evaluate the searched model one by one\nuntil they find the satisfactory one, causing significant time and computing resources.\nThe above limitation of existing generative model hubs inspires us to consider the following question:\nCan we describe the functionalities and utilities of different generative models more precisely in some"}, {"title": "2 Problem Setup and Notions", "content": "In this paper, we explore a novel problem setting called GMI, where users identify the most appro-\npriate generative models for their specific purposes using one image. We assume there is a model\nplatform, consisting of M generative models {fm}M. Each model is associated with a specification\nSm to describe its functionalities for future model identification. The platform consists of two stages:\nthe submitting stage for model developers and the identification stage for users, respectively.\nm=1\nIn the submitting stage, the model developer submits a generative model fm to the platform. Then,\nthe platform assigns a specification Sm to this model. Here, the specification Sm = As (fm,P)\nis generated by a specification algorithm As using the model fm and a prompt set P = {pi} =1\nIf the model developer can provide a specific prompt set for the uploaded model, the generated\nspecification would be more precise in describing its functionalities. In the identification stage, the\nusers identify models from the platform using only one image x7. When users upload an image\nx7 to describe their purposes, the platform automatically calculates the pseudo-prompt p, and then\ngenerates requirements R\u2081 = Ar(x+, P+) using a requirement algorithm Ar. Users can optionally\nprovide corresponding prompt p7, setting pr = p\u2081, to more precisely describe their purposes. During"}, {"title": "3 Proposed Method", "content": "In this section, we present our solution for the GMI setting. Due to space limitations, we explain the\nRKME framework and its failure in GMI in the Appendix B. Our solution adopts a novel weighted\nterm to capture the relationship between images and prompts in RKME, thereby enabling the model\nto be more precise in describing the functionalities of generative models. However, there are two\nissues remain: 1) High dimensionality of images brings intractability of efficiency and similarity\nmeasurement; 2) Cross-modality issue causes difficulties in calculating weight. To address these\nchallenges, we employ a large pre-trained vision model G(\u00b7) to map images from image space to\na common feature space. Subsequently, an image interrogator I(\u00b7) is adopted to convert x, to\ncorresponding pseudo prompt p7, thereby mitigating the cross-modality issues. Consequently, the\nsimilarity in the common feature space can be computed with the help of a large pre-trained language\nmodel T(.). We provide a detailed description of our proposal as follows.\nSubmitting Stage The algorithm As first samples images from the generative model fm using the\nprompt set: Xm = {fm(p)|p\u2208 P}. The developer can optionally replace P with a specific prompt\nset to generate a more precise specification. Then, the large pre-trained vision model G(.) is adopted\nto encode Xm as follows. The obtained feature representation Zm is efficient and robust to compute\nthe similarity between images, i.e., Zm = {G(x)|x \u2208 Xm}. Subsequently, As encodes prompt set P\nto the common feature representation using T(\u00b7): Qm = {T(p)|p\u2208 P}. Finally, the specification\nSm of generative model fm is defined as follows: Sm = As(fm;Pm) = {Zm; Qm}. Note that Sm\nis automatically computed inside the platform, which is very convenient for developers to use and\ndeduce their burden of uploading models. Additionally, the specification does not occupy a large\namount of storage space on the platform since the only feature representation is storage.\nIdentification Stage The users upload one single image x, to describe their requirements and the\nplatform describes the requirements with R\u2081 from x7. Specifically, the requirement algorithm Ar\nfirst generates feature representations of x, using G(\u00b7), i.e., z\u2081 = G(x\u012b). Subsequently, the pseudo-\nprompt p, is generated by I(\u00b7), i.e., p\u2081 = I(x7), and converted to feature representations using T(\u00b7),\ni.e., qr = T(pr). The user can optionally replace p, with a prompt p, built on his understanding\nto precisely describe the requirement. Finally, the requirement is:R\u2081 = A\u2084(x) = {z+; q}. Note\nthat R is automatically computed inside the platform, which is very easy to use for users. After\nthe platform generates the requirement R\u2081, it will calculates the similarity score for each model fm\nusing evaluation algorithm A:"}, {"title": "3.1 Discussion", "content": "It is evident that our proposal for the GMI scenario achieves a higher level of accuracy and efficiency\nwhen compared to model search techniques employed by existing model hubs. For accuracy, our\nproposal elucidates the functionalities of generated models by capturing both the distribution of\ngenerated images and prompts, which allows for more accurate identification compared to the"}, {"title": "4 Experiments", "content": "In this section, we briefly introduce the experiment settings and main results. Detailed information\nabout experiments is additionally provided in Appendix C.\nSettings We conduct experiments on a benchmark dataset described in subsection C.1. Our proposal\nis compared with three baseline methods: 1) Download: The model is ranked based on the download\nvolume [18], representing methods that ignore model capabilities. 2) RKME-Basic: The model is\nidentified using the basic RKME paradigm [7, 22]. 3) RKME-CLIP: The model is identified based on\nthe combination of the RKME paradigm and CLIP model [15]. Two metrics, i.e., accuracy and rank,\nare adopted for evaluation. Accuracy evaluates the ability of methods to identify the most appropriate\nmodel, the higher the better. Rank evaluates the user's efforts in identifying the most appropriate\nmodels, the lower the better. Additionally, Top-k accuracy is reported to indicate how many attempts\ncan users find their satisfied models in major cases.\nEmpirical Results As shown in Figure 2, our proposal achieves the best performance in both\naverage accuracy and average rank, which demonstrates the effectiveness of our proposal. The\nDownload and RKME-Basic methods cannot work in our setting because they do not consider the\nchallenges of GMI. The performance of the RKME-CLIP method improves significantly, indicating\nthat the CLIP model can address the high dimensionality issue. Our proposal captures the relation\nbetween images and prompts, thereby giving the best performance. Table 1 presents the results\nof Top-k accuracy. These results show that our proposal achieves 80% top-4 accuracy on the\nbenchmark dataset, indicating that user only requires four attempts to satisfy their needs in major\ncases using our proposal to identify generative models. Finally, we show the visualization in Figure 3.\nThe requirements are shown in the first column, and the generated images of each method using\npseudo-prompts are shown in the remaining columns. Our proposal gives the most similar images."}, {"title": "5 Conclusion", "content": "In this paper, for the first time, we propose a novel problem called Generative Model Identification.\nThe objective of GMI is to describe the functionalities of generative models precisely and enable the\nmodel to be accurately and efficiently identified in the future by users' requirements. To this end,\nwe present a systematic solution including a weighted RKME framework to capture the generated\nimage distributions and the relationship between images and prompts, a large pre-trained vision-\nlanguage model aimed at addressing dimensionality challenges, and an image interrogator designed to\ntackle cross-modality issues. Moreover, we built and released a benchmark platform based on stable\ndiffusion models for GMI. Extensive experiment results on the benchmark clearly demonstrate the\neffectiveness of our proposal. For example, our proposal achieves more than 80% top-4 identification"}, {"title": "A Related Work", "content": "Generative modeling [8] is a field of machine learning that focuses on learning the underlying\ndistribution and generation of new samples for corresponding distribution. Recently, significant\nprogress has been made in image generation with various methods. Generative Adversarial Networks\n(GANs) [1-3, 6] apply an adversarial approach to learn the data distribution. It consists of a generator\nand a discriminator playing a min-max game during the training process. Variational Autoencoders\n(VAEs) [10, 24, 25] is a variant of Auto-Encoder (AE) [26], where both consist of the encoder\nand decoder networks. The encoder in AE learns to map an image into a latent representation.\nThen, the decoder aims to reconstruct the image from that latent representation. Diffusion Models\n(DMs) [14, 5, 17] leverages the concept of the diffusion process, consisting of forward and reverse\ndiffusion processes. Noise is added to an image during the forward process and the diffusion model\nlearns to denoise and reconstruct the image. With the development of the generative model, various\ngenerative model hubs/pools, e.g., HuggingFace, Civitai, have been developed. However, they lack\nmodel management and identification mechanisms, resulting in inefficiency for users to find the most\nsuitable model. Lu et al. [12] adopts a contrastive learning method to explore the search for deep\ngenerative models in terms of their contents.\nAssessing the transferability of pre-trained models is related to the problem studied in this paper.\nNegative Conditional Entropy (NCE) [23] proposed an information-theoretic quantity [4] to study\nthe transferability and hardness between classification tasks. LEEP [13] is primarily developed\nwith a focus on supervised pre-trained models transferred to classification tasks. You et al. [29]\ndesigns a general algorithm, which is applicable to vast transfer learning settings with supervised\nand unsupervised pre-trained models, downstream tasks, and modalities. However, these methods\nare not suitable for our GMI problem because they impose significant computational overhead\nin terms of model inference during the identification process. Learnware [30] presents a general\nand realistic paradigm by assigning a specification to models to describe their functionalities and\nutilities, making it convenient for users to identify the most suitable models. Model specification\nis the key to the learnware paradigm. Recent studies [21] are designed on Reduced Kernel Mean\nEmbedding (RKME) [27], which aims to map the training data distributions to points in Reproducing\nKernel Hilbert Space (RKHS), and achieves model identification by comparing similarities in the\nRHKS. Subsequently, Guo et al. [7] improves existing RKME specifications for heterogeneous label\nspaces. Tan et al. [22, 21] make their efforts to solve heterogeneous feature spaces. However, these\nstudies primarily focus on classification tasks, overlooking the relationship between images and\nprompts, which is crucial for identifying generative models. Therefore, existing techniques are\ninadequate for addressing the GMI problem, underscoring the pressing need for the development of\nnew technologies specifically tailored to generative models."}, {"title": "B Problem Analysis", "content": "B.1 Reduced Kernel Mean Embedding.\nA baseline method to describe the model's functionality is the RKME techniques [27]. It maps\ndata distribution of each model fm as corresponding specification SRKME = {xRKMEN, where\nNRKME is the reduced set size of fm. For one query image x, from the users, the baseline method\ndefines the requirement as RRKME = {x}. Finally, the platform computes the similarity score in\nRKHS Hk using evaluation algorithm ARKME."}, {"title": "Example B.1.", "content": "Suppose that there are two simplified generative models f1 and f2 on the platform.\nf1 generates scatter points following x = cos (p\u03c0), y = sin (\u03c1\u03c0). f2 generates scatter points"}, {"title": "Remark.", "content": "Example B.1 shows us that overlooking the interplay between images and prompts leads to\nimpossible cases for distinguishing generative models effectively. Existing RKME studies mainly\nfocus on classification tasks, which can implicitly model the tasks through data distribution since the\nclass space is discrete and small. For generative models, we have to explicitly model the model's\nfunctionality, i.e., the relation between images and prompts, to achieve satisfied identification results."}, {"title": "B.2 Weighted RKME Framework", "content": "Motivated by our analysis, how to incorporate the relationship between images and prompts in model\nspecification and identifying process is the key challenge for our GMI setting. Inspired by existing\nstudies [11, 16] about the conditional maximum mean discrepancy, we propose to consider the above\nrelation using a weighted formulation of Equation 2:\nwhere Wm = {Wm,i}m are required to measure the relation between user image x, and prompt\nset P. Here, we make the simplifications RWeighted = x, and Sweighted = Xm in Equation 3. This\nraises challenges inherent in dimensionality since stable diffusion models produce high-quality\nimages. Moreover, measuring the relation using Wm is also a challenging problem and encounters\ncross-modality issues."}, {"title": "C Detailed Experiment Settings and Results", "content": "C.1 Model Platform and Task Construction\nIn practice, we expect model developers to submit their models and corresponding prompts to the\nmodel platform. And we expect users to identify models for their real needs. In our experiments, we\nconstructed a model platform and user identification tasks respectively to simulate the above situation.\nFor the construction of the model platform, we manually collect M = 16 different stable diffusion\nmodels {f1,..., fm } from one popular model platform, CivitAI, as uploaded generative models on\nthe platform. Note that these collected models belong to the same category to simulate the real process\nin which users first trigger category filters and then select the models. We construct 55 prompts"}, {"title": "C.2 Comparison Methods.", "content": "Initially, we compare it with the traditional model search method called Download. This method is\nused to simulate how users search generative models according to their downloading volumes [18],\nwhere users will try models with high downloading volume first. This baseline method can represent\na family of methods that employ statistical information without regard to model capabilities. We also\nconsider the basic implementation of the RKME specification [27] as a baseline method RKME-Baisc\nfor our GMI problem. The details of generating specifications, and identifying models are presented\nin subsection B.1. Furthermore, we compare our proposed method with a variant of the basic RKME\nspecification, that is, RKME-CLIP, which calculates specifications in the feature representation\nspace encoded by the CLIP model [15]. The results obtained from RKME-CLIP further support our\nviewpoint on the critical challenges posed by dimensionality."}, {"title": "C.3 Implementation Details.", "content": "We adopt the official code in Wu et al. [27] to implement the RKME-Basic method and the\nofficial code in Radford et al. [15] to implement the CLIP model. For RKME-Basic and\nRKME-CLIP methods, we follow the default hyperparameter setting of RKME in previous\nstudies [7]. We set the size of the reduced set to 1 and choose the RBF kernel [28] for\nRKHS. The hyperparameter y for calculating RBF kernel and similarity score is tuned from\n{0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.03, 0.04, 0.05} and set to 0.02 in our experiments.\nExperiment results below show that our proposal is robust to y."}, {"title": "C.4 Ablation Study", "content": "In order to comprehensively evaluate the effectiveness of our proposal, we investigate whether each\ncomponent contributes to the final performance. We additionally compare our proposal with two\nvariants, called RKME-CLIP and RKME-Concat. RKME-CLIP adopts the CLIP model to extract the\nfeature representation for constructing RKME specifications. RKME-Concat adopts both vision and\ntext branches of the CLIP model to extract representations of images and prompts. It combines two\nmodes of representation for constructing RKME specifications. We report accuracy and rank metrics\nin Table 2. The performance of RKME-CLIP demonstrates that employing large pre-trained models\nis an effective approach for addressing dimensionality issues. The performance of RKME-Concat\ndemonstrates the benefits of considering both images and prompts for model identification. Our\nresults achieve the best performance, and demonstrate the effectiveness of our weighted formulation\nin Equation 3 and our specifically designed algorithm in Equation 1."}, {"title": "C.5 Hyperparameter Robustness", "content": "We evaluate the robustness of each method to the hyperparameter y in Figure 5. The results\ndemonstrate that our proposed method exhibits robust performance across a wide range of y values.\nHowever, as y continues to increase, the performance of both our proposal and the baseline methods\nbegins to degrade. This observation highlights the importance of tuning the hyperparameter y before\ndeploying our method in practical applications. Once y is properly tuned, our method can operate\nrobustly due to its hyperparameter robustness within a broad range."}], "equations": ["Ae(Sm, Rr) = \\frac{1}{N_m} \\sum_{i=1}^{N_m}  w_{m,i} \\cdot (k(z_{m,i}, z_r) - k(q_{m,i}, q_r))", "A^{RKME}(S^{RKME}, R^{RKME}) = \\sum_{i=1}^{N^{RKME}} \\frac{1}{N^{RKME}} k(x^{RKME}_{m,i}, x) \\qquad \\qquad (2)", "A_{Weighted} (S_{Weighted}, R_{Weighted}) = \\sum_{i=1}^{N_m} \\frac{1}{N_m} w_{m,i} \\cdot (k(x_{m,i}, x_r)) \\qquad (3)"]}