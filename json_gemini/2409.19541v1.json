{"title": "Unlabeled Debiasing in Downstream Tasks via Class-wise\nLow Variance Regularization", "authors": ["Shahed Masoudian", "Markus Frohman", "Navid Rekabsaz", "Markus Schedl"], "abstract": "Language models frequently inherit societal\nbiases from their training data. Numerous tech-\nniques have been proposed to mitigate these\nbiases during both the pre-training and fine-\ntuning stages. However, fine-tuning a pre-\ntrained debiased language model on a down-\nstream task can reintroduce biases into the\nmodel. Additionally, existing debiasing meth-\nods for downstream tasks either (i) require la-\nbels of protected attributes (e.g., age, race, or\npolitical views) that are often not available or\n(ii) rely on indicators of bias, which restricts\ntheir applicability to gender debiasing since\nthey rely on gender-specific words. To address\nthis, we introduce a novel debiasing regulariza-\ntion technique based on the class-wise variance\nof embeddings. Crucially, our method does\nnot require attribute labels and targets any at-\ntribute, thus addressing the shortcomings of\nexisting debiasing methods. Our experiments\non encoder language models and three datasets\ndemonstrate that our method outperforms ex-\nisting strong debiasing baselines that rely on\ntarget attribute labels while maintaining perfor-\nmance on the target task.", "sections": [{"title": "1 Introduction and Background", "content": "Language Models (LMs) based on encoders are\nused for a variety of purposes such as document\nclassification (Founta et al., 2018; De-Arteaga et al.,\n2019), job recommendation (Kumar et al., 2023a),\ntext generation (Eldan and Li, 2023), or as text\nencoder for multimodal models such as text-to-\naudio (Liu et al., 2023) or text-to-image (Bahani\net al., 2023) models. These models often encode\nsocietal biases rooted in the corpora used for train-\ning (Mehrabi et al., 2022; Rekabsaz et al., 2021a),\nwhich causes a distributional shift of embeddings,\nhence affecting their outputs either with dispropor-\ntionate misclassification of documents belonging\nto minority groups or unfair ranking of the docu-\nments (Rekabsaz et al., 2021b; Melchiorre et al.,\n2021).\nSeveral works focus on reducing the effect of\nthese biases by improving model performance re-\nlated to some specific fairness metric (empirical\nfairness) or by making the model blind to the ex-\nistence of a certain attribute (representational fair-\nness) (Shen et al., 2022). For instance, Shen et al.\n(2022) leverage contrastive learning to improve\nempirical fairness. Recent works focus mostly\non efficiency and user flexibility when it comes\nto debiasing using modular approaches such as\nsub-networks or adapters (Houlsby et al., 2019).\nHauzenberger et al. (2023) introduce a modular\ndebiasing scheme with adversarial training (Elazar\nand Goldberg, 2018) and mutual information reduc-\ntion (Colombo et al., 2021) to control the bias in\nencoder LMs. Kumar et al. (2023b) use adversarial\ntraining with adapters (Pfeiffer et al., 2021) to im-\nprove representational fairness. Finally, Masoudian\net al. (2024) used gated adapters to improve rep-\nresentational fairness while preserving task perfor-\nmance for classification and retrieval tasks.\nAlthough these methods effectively reduce\nsensitive attribute information and enhance fair-\nness (Zerveas et al., 2022; Shen et al., 2022)\nthrough blindness, they depend on attribute labels\nto align the distribution of the target attribute. Since\nthe user input data contains numerous nuanced pro-\ntected attributes, such as age, race, religion, etc., it\nis challenging to collect labeled data for each indi-\nvidual attribute across every task. Moreover, super-\nvised debiasing methods typically require training\non each attribute individually, scaling linearly with\nthe number of attributes. This complexity high-\nlights the need for more efficient and scalable ap-\nproaches to handle multiple protected attributes in\ndebiasing efforts.\nTo address this limitation, some works attempt\nto debias language models without using attribute"}, {"title": "2 Problem Formulation", "content": "In recent years, adapter networks (Houlsby et al.,\n2019; Pfeiffer et al., 2021) have emerged as an effi-\ncient way of training models on downstream tasks.\nIn addition to their improved training efficiency,\nadapters keep the backbone LM weights fixed, help-\ning preserve information within the model.\nThis provides strong motivation for using debi-\nasing methods during fine-tuning, even when using\nan already debiased pre-trained LM. However, as\nsurveyed in \u00a7 1, existing debiasing methods either\nrely on attribute labels or are limited to attributes\nwith explicit indicators in the text, such as gender.\nFurthermore, there exists a plethora of sensitive at-\ntributes, and labeling them all is challenging across\ntasks. This increase in number also affects debi-\nasing complexity as it scales with the number of\nattributes. Thus, a method that addresses this gap\nwould be highly desirable. In the following, we\noutline how we solve this gap."}, {"title": "3 Low Variance Regularization (LVR)", "content": "We formulate our regularization scheme based on\nk centers, each representing a class in the dataset\nwith d dimension {C1, C2, ...Ck|C; \u2208 Rd}, where\nd is the model's embedding size. We aim to adjust\nthe parameters of the network if the variance of the\nembeddings in a batch is high, which intuitively\nresults in the mitigation of any undesirable distri-\nbutional shift that might exist in the embeddings.\nSince we have k classes, class-wise variance is a\ngood proxy for this regularization loss.\nWe define the regularization loss as the distance\nbetween embeddings (Z \u2208 Rd) of class i in a given\nbatch from their corresponding center. For each\nbatch, we calculate the center of embeddings that\nbelong to the same class (Ci), which results in k\ncenters. To account for noisy data points and empty\nbatches, we use the weighted sum of the current\nbatch center Co and the normalized weighted sum\nof previous batch centers C-1 where w is a hy-\nperparameter to control the influence of previous\nbatch and found through grid search. The centers\nare calculated as follows:\nC = (1 \u2013 \u03c9) \\frac{Z1+Z2... Zm}{m}+wCb-1, (1)\nwhere m is the number of samples for the ith class\nin a batch. In practice, if there are no samples of\na class within a batch, we ignore it; and if only\none sample of the class is in the batch, the center\nbecomes the sample itself. We then define the reg-\nularization loss as the sum of distances for each\nspecific sample belonging to class i from the esti-\nmated center of the batch:\nLc = \\sum_{i=1}^{k} \\sum_{r=1}^{m} \\sum_{j=1}^{d} (z_{ir}^{j} - c_{i}^{j})^2, (2)\nwhere c is the center value for the jth dimension of\nthe ith class and zir is the value for the jth dimen-\nsion of the rth embedding for the ith class. This\ncorresponds to reducing the class-wise variance of\nthe embeddings created by the model, which in\nturn reduces distributional shift that might exist in\nthe data points of the same class and results in the\nalignment of the embeddings. We also use the cal-\nculated centers as extra input for the classification\ntask and calculate the loss of the centers. We show\nlater in \u00a7 5 that this added loss term is essential to\nmitigate degradation in task performance.\nThe overall loss then becomes a linear combination:\nLtotal = Lt + XL\u2084 + Lc, (3)\nwhere Lt is the classification loss, L\u2081 is the reg-\nularization loss, and Le is the loss to classify the\ncalculated centers belonging to each class."}, {"title": "4 Experimental Setup", "content": "For our experiments, we follow previous works\nand focus on transformer-based language models.\nWe use BERT-BASE and ROBERTA-BASE, in\ncombination with adapters (Pfeiffer et al., 2021) for\neach task. Trained in this way, we denote models\nusing our debiasing method as ADPLVR.\nWe use the following document classification\ndatasets: occupation prediction (BIOS; De-Arteaga\net al. (2019)) with gender as protected attribute,\nhate speech detection (FCDL18; Founta et al.\n(2018)) with race for protected attribute, and men-\ntion detection (PAN16; Rangel et al. (2016)), cor-\nresponding to a multi-attribute setting with age and\ngender as protected attributes. For each dataset,\nwe remove all explicit indicators of protected at-\ntributes following previous works (Hauzenberger\net al., 2023; Kumar et al., 2023b; Masoudian et al.,\n2024) from the text.\nBaselines. We choose baselines as follows: FT,\nADP and ADPNLI as fine-tuned versions of the\nentire model and adapter-based training of the\nmodel and adapter-based training for BERT model\ntrained on debiased NLI, respectively, without us-\ning any additional bias mitigation method. We\nalso select recent in-process debiasing algorithms\nas strong baselines, relying either on adversarial\ntraining (Elazar and Goldberg, 2018, FTADV; AD-\nPADV) or mutual information reduction (Colombo\net al., 2021, ADPMMD) to reduce the bias en-\ncoded within the embeddings and increase repre-"}, {"title": "5 Results and Discussion", "content": "Table 2 shows the task and probe performance of\nthe baselines and ADPLVR.\nIn our single attribute experiments on\nFCDL18 and BIOS, using both BERT-BASE and\nROBERTA-BASE, ADPLVR is able to remove\ninformation about protected attributes considerably\nbetter than all the baselines on BIOS and FCDL18.\nAs for task performance, we observe a decrease in\naccuracy with ADPLVR compared to the baselines\non BIOS. Remarkably, our regularization method\neven shows an improvement in task performance\non FCDL18, demonstrating the robustness of its\nembeddings.\nIn our multi-attribute experiment on PAN16 (2\nprotected attributes), we observe that ADPLVR\nperforms slightly worse than the best-performing\nmodel, ADPADV, on the main task. However, un-\nlike ADPADV, which has access to the protected\nattribute label during training, ADPLVR crucially\ndoes not rely on attribute labels for bias mitiga-\ntion; yet, it still outperforms the baselines in pro-\ntected attribute information removal. Overall, we\nobserve that, with BERT-BASE, ADPLVR shows\nslightly higher balanced accuracy compared to AD-\nPADV for both protected attributes, while with\nROBERTA-BASE, ADPLVR similarly shows im-\nproved mitigation performance.\nNotably, other debiasing methods show similar\ndecreases in task performance. Still, on FCDL18,\nADPLVR clearly outperforms all supervised base-\nlines on the main task and information removal.\nAblation Study. To ensure all parts of our\nmethod are necessary to achieve its performance,\nwe conduct an ablation study where we remove\n(i) the memory of the previous batch, w, and (ii)\nthe center loss Le introduced in section 3. Table 3\nshows the result of this ablation study. By remov-\ning w, the balanced accuracy of the probe consider-\nably increases, meaning that the robustness of the\nembeddings toward protected attributes is reduced.\nThus, more information about unknown, unrelated\nattributes influences the final output of the model\nto a larger extent.\nMoreover, we observe that task performance\nclearly degrades when removing Le. Overall, the\nbest-performing model, both in terms of task perfor-\nmance and probe balanced accuracy, is the one that\nhas both w acting as memory of previous batches\nfor the model and Le, corresponding to our class-\ncenter-based loss."}, {"title": "6 Conclusion", "content": "In this work, we focus on representational fairness\nand introduce a novel regularization and optimiza-\ntion scheme to debias encoder LMs without ac-\ncessing protected attribute labels. We show the\neffectiveness of our method using two encoder\nLMs across three datasets and multiple protected\nattributes. We demonstrate that our method en-\nhances debiasing while maintaining task perfor-\nmance compared to strong baselines. To the best\nof our knowledge, our method is the first that can\nmitigate bias of any arbitrary target attribute by\ngenerating robust embeddings best suited for the\nclassification task. Since our method does not rely\non attribute labels, we hope it paves the future for\nmore accessible, effective, and efficient debiasing\nof encoder-based transformer models."}, {"title": "Limitations", "content": "One limitation of this work is the definition of gen-\nder used in all datasets, which is limited to binary\nfemale/male, lacking an inclusive and nuanced def-\ninition of gender. Moreover, although our method\nproved independent of attribute labels, a thorough\nevaluation would require more datasets with a vari-\nety of defined attributes. Another limitation of this\nwork is the task in which we narrowed our study to\nclassification tasks. We acknowledge that the find-\nings of this paper might not be applicable to other\ntasks such as retrieval or recommendation. Further-\nmore, our study is focused on transformer-based\nlanguage models which put an additional limitation\non the generalization of the work to other models\nsuch as CNNs or LSTM-based language models.\nDue to the lack of suitable datasets, we relied on\ndatasets commonly used in the debiasing literature.\nIn FDCL18, race is restricted to African Ameri-\ncan and White American, which does not reflect\nreal-life scenarios. Furthermore, we follow previ-\nous works (Sap et al., 2019; Ravfogel et al., 2020;\nZhang et al., 2021) and use labels of protected\nattributes assigned using another model, making\nthem not fully representational of the real data dis-\ntribution. A final limitation of this work is the lack\nof suitable datasets for multi-attribute settings, in\nwhich we could demonstrate that our approach can\nhandle even more attributes than demonstrated with\nPAN16 simultaneously."}]}