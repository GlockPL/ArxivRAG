{"title": "DifFRelight: Diffusion-Based Facial Performance Relighting", "authors": ["MINGMING HE", "PASCAL CLAUSEN", "AHMET LEVENT TA\u015eEL", "LI MA", "OLIVER PILARSKI", "WENQI XIAN", "LASZLO RIKKER", "XUEMING YU", "RYAN BURGERT", "NING YU", "PAUL DEBEVEC"], "abstract": "We present a novel framework for free-viewpoint facial performance relighting using diffusion-based image-to-image translation. Leveraging a subject-specific dataset containing diverse facial expressions captured under various lighting conditions, including flat-lit and one-light-at-a-time (OLAT) scenarios, we train a diffusion model for precise lighting control, enabling high-fidelity relit facial images from flat-lit inputs. Our framework includes spatially-aligned conditioning of flat-lit captures and random noise, along with integrated lighting information for global control, utilizing prior knowledge from the pre-trained Stable Diffusion model. This model is then applied to dynamic facial performances captured in a consistent flat-lit environment", "sections": [{"title": "1 INTRODUCTION", "content": "Today, photorealistic digital humans often play starring roles alongside live-action actors, and appear with increasing fidelity in video games and virtual reality. However, constructing photorealistic digital human models is difficult and expensive, and animating them realistically - even with performance-capture data \u2013 is still challenging and prone to error.\nVolumetric performance capture (volcap) systems use an array of inward-pointing cameras to record dynamic human performances in three dimensions, avoiding the need for traditional 3D character modeling, rigging, and animation. Volcap captures all the nuances of a performance, allowing it to be rendered into any 3D scene from any angle. However, most capture techniques record the subject under a single flat lighting condition, which hampers the achievement of cinematic lighting effects and complicates the seamless integration of characters into new scenes. To address this, some advanced systems [Guo et al. 2019] use time-multiplexed lighting to record performances under multiple lighting conditions alternating each frame. This approach enables data-driven relighting, sometimes enhanced by machine learning inference [Meka et al. 2020; Zhang et al. 2021a].\nIn this work, we achieve high-quality relighting of volumetrically captured facial performances as shown in Fig. 1. This is accomplished by fine-tuning a diffusion-based image-to-image translation model using subject-specific relighting examples. In addition to capturing the performance under flat lighting, we record each subject under a dense array of lighting directions across various facial expressions. We train a diffusion model for precise lighting control, enabling the generation of high-fidelity relit facial images from flat-lit inputs. This process involves spatially-aligned conditioning of flat-lit captures and random noise, integrated with lighting information for global control, leveraging prior knowledge from the pretrained stable diffusion model. Additionally, we use dynamic 3D Gaussian Splatting (3DGS) to extrapolate the performance to novel viewpoints.\nOur work includes the following contributions:\n(1) A novel framework for facial performance relighting that trains on a subject-specific paired dataset and generalizes to novel lighting conditions, enabling relighting from free viewpoints and unseen facial expressions.\n(2) A diffusion-based relighting model that spatially conditions the flat-lit input image, utilizing lighting information as global control to generate high-quality relit results.\n(3) A scalable dynamic 3DGS technique for reconstructing long sequences, ensuring temporal consistency in flat-lit inputs for coherent inference by the relighting model.\n(4) A unified lighting control that combines a new area lighting representation with directional lighting, offering versatile lighting controls as well as enabling the composition of complex environment lighting."}, {"title": "2 RELATED WORK", "content": "Parametric reflectance modeling measures a subject's geometry and reflectance properties from different views and lighting directions but struggles with complex materials and dynamic subjects [Goesele et al. 2004; Sato et al. 1997; Weyrich et al. 2006]. These techniques struggle with complex materials, such as hair, and are difficult to apply to dynamic subjects since observations under many lighting conditions are required.\nImage-based relighting photographs the subject under various lighting conditions to create new lighting scenarios, which is effective but costly and challenging for dynamic subjects [Debevec et al. 2000; Einarsson et al. 2006; Nimeroff et al. 1995; Peers et al. 2007].\nIntrinsic image relighting decomposes an image into intrinsic components and recomposes it with modified lighting [Barron and Malik 2014; Le and Kakadiaris 2019; Li et al. 2018; Luan et al. 2017; Mei et al. 2024; Sengupta et al. 2018; Shih et al. 2014; Shu et al. 2017a,b; Sun et al. 2019; Tewari et al. 2021a, 2020, 2021b; Wang et al. 2008; Zhou et al. 2019]. Despite advancements, these methods face challenges with complex shading effects and detail preservation. Recently, Kim et al. [2024] propose a physically based rendering model for computing diffuse and specular reflectance, inspired by [Pandey et al. 2021]. They can generalize to novel subjects but lack detail for skin and hair reflectance, although refining with a neural renderer. Rao et al. [2022] develop a model with a reflectance network trained on OLAT images to learn a light-conditioned volumetric reflectance field from a single photo, but it is limited to static faces and does not address dynamic sequences.\nNeural relighting approaches either use Neural Radiance Fields (NeRFs) to reconstruct material properties and lighting effects in 3D but face challenges in novel poses and detail preservation [Boss et al. 2021; Li et al. 2023b; Liu et al. 2023; Mildenhall et al. 2020; Sarkar et al. 2023; Srinivasan et al. 2021; Wang et al. 2023a; Xu et al. 2023b; Yao et al. 2022; Zeng et al. 2023; Zhang et al. 2023b, 2021b], or introduce 3D Gaussian Splatting (3DGS) for relighting because of the fine detail reconstruction [Gao et al. 2023; Kerbl et al. 2023]. Recent research on relightable head avatars uses OLAT data and various geometry representations, such as volumetric primitives [Yang et al. 2023], neural fields [Xu et al. 2023a], and 3DGS [Saito et al. 2023], to enhance the rendering quality of facial details. The subject-specific model [Saito et al. 2023] achieves promising facial performance relighting by combining 3D Gaussians with a relightable appearance model based on learnable radiance transfer. However, it necessitates a significantly denser capture settings for expressions and requires precise track of facial mesh and gaze motion and explicit modeling of face and eyes, making this method difficult to generalize."}, {"title": "2.2 Diffusion-Based Relighting Methods", "content": "Diffusion models [Karras et al. 2022; Ke et al. 2023; Preechakul et al. 2022; Rombach et al. 2022; Song et al. 2020a,b; Wang et al. 2023b; Zhang et al. 2023a] have demonstrated remarkable capabilities in generating high-quality images by sampling from a learned distribution of images, particularly when conditioned on spatial control via image-to-image translation [Brooks et al. 2023; Ke et al. 2023; Meng et al. 2021; Parmar et al. 2023; Wang et al. 2023b]."}, {"title": "3 METHOD", "content": "Fig. 2 illustrates our dynamic facial performance relighting pipeline with two main components: dynamic 3D performance reconstruction and diffusion-based relighting. Our approach begins with the acquisition of multi-view performance sequences in a flat-lit environment. Utilizing our scalable approach of dynamic 3D Gaussian Splatting (3DGS), we first reconstruct deformable 3D Gaussians to render the sequence from novel perspectives. Then we employ the diffusion-based relighting model to generate new lighting for the rendered image sequence, based on a specified lighting direction. This model has been trained with subject-specific paired data of flat-lit and OLAT images captured using a customized LED-panel stage (Sec. 3.1), incorporating a new design of feeding flat-lit images and lighting directions as spatial and global conditions into the pretrained latent diffusion model (Sec. 3.2). To maintain temporal consistency in the dynamic relighting, we introduce a partitioning and combining scheme to train temporally-coherent 3DGS for lengthy sequences and also apply temporal blending on the dynamic relit results (Sec. 3.3). Additionally, we introduce a new variablesized area light representation for enhanced control over different lighting types for various lighting compositions (Sec. 3.4)."}, {"title": "3.1 Facial Data Capture", "content": "Capture Setup. To supervise the diffusion-based relighting model and facilitate learning the translation of facial reflectance from flat lighting to arbitrary lighting, we require paired training data where lighting is the only changing variable. We use a volumetric capture stage consisting of a multi-view array camera array placed within a capped cylinder of LED panels as in Fig. 3 (a). By turning on different LED panels, we capture the same subject under varying lighting conditions. Furthermore, customizing the display patterns of LED panels enables us to record the subject under various backgrounds and image-based lighting environments. We constructed this stage as a cylindrical capture rig using off-the-shelf ROE BP2v2 LED panels and 75 synchronized 4K Z-CAM e2 cinema cameras. The rig comprises a 16 \u00d7 5 array of 50cm \u00d7 50cm panels (16 columns, each consisting of 5 panels), forming a capture volume approximately 250 cm tall and 276 cm in diameter. The ceiling is covered with a 6 \u00d7 5 array of panels, and an additional 10 panels on the floor provide additional lighting angles. The cameras peer in through 5cm gaps in the walls and ceiling.\nLighting setup. Following LeGendre et al. [2020], we capture the reflectance field as OLAT images using each panel in sequence as a single light source at 24 frames per second in sync with the camera array. As Debevec and LeGendre [2022] demonstrate, LED panels are effective for image-based lighting on actors, so we use them to simulate OLATs, with one light (=OL) equivalent to one LED panel. Each panel approximates a 20-degree cone from the stage center, producing less aliased shadows than point lights while still capturing eye highlights and sharp specular reflections. We minimize the panel size while maintaining sufficient illumination and try to ensure uniformity in size and shape. Despite the large angular coverage of each panel, in practice, our OLAT-based framework works reasonably well when generalizing to environment lighting, as shown in Fig. 7 and Fig. 16. We also capture flat-lit lighting to have paired data for training the diffusion-based relighting model.\nData collection. We collected OLAT sequences for four individuals, each demonstrating 30 facial expressions and head poses. Each OLAT sequence has 123 lighting conditions with 13 interspersed flat-lit tracking frames. This same flat-lit lighting is used to capture the dynamic facial performance. Additionally, we recorded one OLAT sequence of a clean plate without a subject to facilitate background removal in 3DGS reconstruction. To compensate for the individual's slight movements during one OLAT sequence, we conduct optical flow alignment in image space with respect to the flat-lit frames for each view, following Meka et al. [2019]. When training the relighting model, we convert linear images to sRGB color space for better compatibility with the pretrained model weights."}, {"title": "3.2 Diffusion-Based Relighting", "content": "After data acquisition and preprocessing, we obtain a paired dataset {IFlatLit, IOLAT, d}, where IFlat Lit represents the flat-lit image, and IOLAT denotes the OLAT image under light direction d. d is defined as a directional vector pointing from the stage center to the center of each panel in camera space. Our objective is to train a personalized model capable of relighting the flat-lit image of the same subject under novel views, novel lightings, and novel expressions/poses. Given the limited set of training pairs we have captured, a strong prior is essential for the model to generalize to unseen conditions. Rather than relying on physically-based lighting models, which often fail to accurately simulate complex shading effects such as anisotropic materials and subsurface scattering, we employ a purely data-driven prior to ensure the photorealism of our output. We use a diffusion-based approach that leverages the prior knowledge encapsulated in the pre-trained Stable Diffusion model [Rombach et al. 2022].\nWe formulate our problem as an image-to-image translation that transfers a flat-lit image IFlat Lit to the corresponding OLAT image IOLAT under the condition d. Inspired by recent monocular depth estimation works [Ke et al. 2023], we adapt Stable Diffusion's architecture to fine-tune it with our paired data conditioned on lighting information, achieving robust performance in relighting tasks.\nThe Stable Diffusion model [Rombach et al. 2022] is originally designed as a latent diffusion model for text-to-image generation. It comprises a Variational Autoencoder (VAE) that encodes an image I into the latent space z = &(I) and decodes it back to I \u2248 D(z). Additionally, it includes a diffusion U-Net \u00ea (z(t); s, t), which predicts the noise of a partially denoised latent z(t) conditioned on some text embedding s and the diffusion timestep t. By iteratively removing noise from random noise, we obtain a clean image latent z(0), which can then be decoded back into an image D(z(0)).\nTo transform the image generation framework into an image-to-image translation model and more effectively leverage the spatial information in the flat-lit image, we modify the U-Net by concatenating the latent of the flat-lit image with the random noise map. This necessitates a minor change in the network structure: specifically, doubling the input channel count of the first convolutional layer. These minimal adjustments enable us to maximize the retention of the pre-trained weights from Stable Diffusion. Additionally, concatenating the latents with random noise improves the alignment of the spatial structure between the relit result and the input.\nIn addition to the flat-lit image, which offers spatial cues to the resulting output, the lighting information acts as a crucial global control signal that influences the entire image. We encode light direction d into higher dimensions and replace text embeddings by introducing it into the U-Net through cross-attention in diffusion models. To increase the frequency band of d and enhance the conditioning, we use Spherical Harmonics (SH) to encode d, similar to techniques used by Tancik et al. [2023]. We also zero-pad the SH encoding to match the length of the text embedding:\nsd = 0 (d), (1)\nwhere indicates concatenation, and y indicates the SH encoding. Initially, this may appear unusual as the SH encoding is nothing like a text embedding, which is predicted by some text encoder [Schuhmann et al. 2022]. However, we find that the fine-tuning is adequate for filling the domain gap between the text embedding and SH encoding. We set the SH degree to 3 for all model training, as it showed the best performance in our experiments (see Sec. 5.1 in the supplemental material).\nAt training time, we freeze the VAE encoder and decoder, as both the flat-lit and OLAT-lit images lie within the manifold of the VAE's latent space. We randomly select training pairs from the dataset and fine-tune the diffusion U-Net using the denoising diffusion objective:\nLdiffusion = ||\u00ea (ZOLATE(IFlatLit); $d, t) \u2013 \u20ac||2, (2)"}, {"title": "3.3 Dynamic Performance Relighting", "content": "To ensure temporally consistent relit results when applying the pre-trained diffusion-based relighting model to flat-lit facial performances, our initial goal is to minimize the domain gap between training and testing by generating photorealistic novel-view renderings. Additionally, maintaining temporal consistency in the flat-lit inputs enhances the diffusion model's performance under consistent conditions. In line with these requirements, we utilize 3D Gaussians [Kerbl et al. 2023] as the geometry representation for capturing fine details in real-time rendering. 3D Gaussian Splatting (3DGS) can be extended to the temporal domain [Jung et al. 2023; Li et al. 2023a; Luiten et al. 2023; Wu et al. 2023]. Deformable 3DGS [Jung et al. 2023] learns a deformation field for each frame within the canonical space of 3D Gaussians that is shared across all frames. The deformation field takes the positions of the 3D Gaussians x and the current time t as inputs, outputting the offset for position x, rotation r, and scaling s as \u03b4\u03b9\u03c7, \u03b4\u03b5r, and 8ts. While preserving details in short videos, this method faces challenges in accurately representing motion and maintaining intricate details in long sequences due to its use of globally shared Gaussians, which struggle with the complexities of extended sequences.\nWe introduce a novel scalable method based on the deformable 3DGS [Jung et al. 2023] to optimize 3D Gaussians for lengthy performance sequences, thereby generating temporally consistent reconstruction. Instead of training all frames together with one shared set of Gaussians, we partition it into small segments with an equal number of frames, allowing varying Gaussians across segments. To minimize temporal inconsistency at the transition frame between segments and to preserve a similar level of reconstruction details across segments, we design a two-stage training strategy. As shown in Fig.4, in Stage 1, we evenly distribute K keyframes in the entire sequence, and then train a deformable Gaussian splatting model based on the keyframes only, generating the pretrained K-frame model. In Stage 2, using the K frames as transition points, we partition the entire sequence into K - 1 segments, each containing two keyframes (one at the beginning and the other at the end). Inside every segment, we initialize the 3D Gaussians and the deformation network using the pretrained K-frame model based on the time of its first frame in the global time range. The geometry information of 3D Gaussians (such as positions x, rotations r and scalings s) is initialized as the pretained K-frame Gaussians transformed by the deformation offset of the first frame tko. In this way, we ensure the initial states of 3D Gaussians in different segments are temporally consistent with similar level of details. To further enhance the consistency, we fix the initial Gaussians and train the deformation network only for the warm-up iterations (we use 3000 iterations as default), with the regularization term defined as L2 loss on the deformation offset of keyframes ko and k\u2081:\nLreg = ||Stko - Stko ||2 + ||Stk\u2081 \u2013 Stk\u2081 ||2, (4)\nwhere Stko and Stk\u2081 are initial deformation offset of tko and tk\u2081 while \u03b4tko and \u03b4tk\u2081 indicates the learned offset with respect to x, r and s.\nThis warm-up training allows the Gaussians to freely deform to reconstruct motions while being restricted to the deformation of keyframes at transition points. After this stage, most Gaussians are in the correct position, and we relax the constraints to allow Gaussians to clone, split, and prune for detailed reconstruction for a number of iterations (defaulting to 10000 iterations). After the densification, we fix the number of Gaussians and jointly optimize the deformation network and the Gaussians. The ablation of the two-stage training method is included in the supplemental material.\nAlthough we ensure temporal consistency in the reconstructed flat-lit sequence used as input for our relighting model, the image-based diffusion model may still produce temporally inconsistent high frequencies. To address this, we adopt the method from [Jamri\u0161ka et al. 2019], applying a temporal blending approach to interpolate relit results between keyframes. We use a small step size of 4 to sample keyframes and preserve details and lighting accuracy."}, {"title": "3.4 Unified Lighting Control", "content": "To provide the users with flexible lighting control, we propose a novel area lighting representation, which includes both lighting direction and a variable light size. This is integrated with our directional lighting into a unified lighting control to guide the diffusion-based relighting model. Based on the single-lighting inference, we can enable HDRI environment lighting reconstruction by compositing multiple single-lighting inferences.\nArea-light training. To infer lights of various sizes - from point lighting to diffuse we modify our training in two ways. First, we multiply the light direction by a light size factor 1 - a, with a ranging from 0 (indicating a single OLAT) to 1 (representing the flat-lit), which represents the spread of the light and thus its sharpness. Second, we construct our ground truth target by combining OLAT images to simulate isotropic Spherical Gaussian (SG) illumination. Then the light size factor is linked to the Spherical Gaussian's sharpness by the formula \u03bb = , with 0 = \u0430 \u00b7 (\u04e8\u0442\u0430\u0445 \u2013 Omin) + Omin, being the inflection point of an SG and min = 1. 180, \u04e8\u0442\u0430\u0445 = 89. 180.\nHDRI reconstruction. To relight a performance under an HDRI environment using our model conditioned on a single lighting, we first map an HDRI latitude-longitude (lat-long) representation to the OLATs, with each OLAT's weighting coefficient being an average from its corresponding area on the HDRI lat-long. During inference, we generate an image for each OLAT direction and compute a weighted sum of these OLATs in linear space using their fitted coefficients. Additionally, the HDRI map can be reconstructed using our model conditioned on both light direction and size by first fitting Spherical Gaussians onto the HDRI lat-long representation and then inferring for each light direction and size. Similarly to the OLATs, we compute a weighted sum of the predictions using the fitted coefficients of these Spherical Gaussians. If the HDRI is animated, we adjust the conditioned directions of the OLATS or Spherical Gaussians to match the rotation of the HDRI."}, {"title": "4 EXPERIMENTS", "content": "We have conducted multiple experiments to evaluate the performance of our diffusion-based relighting model, particularly its generalization across various dimensions including novel lightings, expressions, and views. We compare our model with two baselines constructed using different network structures from the related methods. Additionally, we perform ablation studies to verify various technical designs within our system and conduct a simple extension to assess model generalization to novel subjects. For more results on dynamic performance relighting and detailed experiments, please refer to the accompanying video and supplemental material."}, {"title": "4.1 Implementation Details", "content": "We capture 30 expressions (30E), 123 OLATs (123L), and 75 views (75V) for each of the 4 subjects and downsample 4K captures to 1080 \u00d7 1920. For training, we use the Cartesian product of 27E \u00d7 115L \u00d7 69V, with the rest reserved for validation. Capturing all OLAT data for each subject takes a few minutes. For inference, we record 10-15 seconds of performances in a multi-view capture stage under flat lighting. We implement our diffusion-based relighting system using Diffusers [Face 2024] based on Stable Diffusion v2.1. Our custom models are trained on 8 NVIDIA A100 GPUs with 40GB memory each, for 100K iterations with a batch size of 8. We use an Adam optimizer with a learning rate of 3 \u00b7 10\u22125. We use a DDPM scheduler with 1000 steps for training and a DDIM scheduler with 30 steps for inference which takes about 5-6 seconds per frame on a single A100 GPU. Our dynamic 3DGS is also trained in full HD resolution. Given a long sequence, we divide it into segments based on the number of keyframes, typically including 20 frames per segment. We then train the deformable 3DGS for 40K iterations for each segment."}, {"title": "4.2 Results", "content": "We showcase the effectiveness of our diffusion-based relighting method on the testing data of the four captured subjects. The visual results in Fig. 1, Fig. 10, and Fig. 11 demonstrate that our method reproduces realistic skin texture and reflectance, eye highlights, and fine hair structures while maintaining the subject-specific identity features. The novel lighting animation creates a realistic interplay of shadows and shading across the face. Fig. 11 shows our unified lighting control with both directional and area lights, and as an application to our lighting control, Fig. 7 shows the results relit by HDRI environment maps. In addition to varying lighting conditions, expression changes are illustrated in Fig. 10."}, {"title": "4.3 Comparisons", "content": "Since no existing works employ the same training and testing settings as our method, we establish baselines using two different network structures: a diffusion-based model built on ControlNet [Zhang et al. 2023a] and a U-Net-based model inspired by a state-of-the-art image-based relighting technique of [Pandey et al. 2021]. We adapt ControlNet to condition on a 3-channel flat-lit image and a 1-channel diffuse shading map, obtained by creating a diffuse shading map using the dot product between the lighting direction and photometric normals. Photometric normals are computed using a standard method [Ma et al. 2007] for subjects captured under gradient illumination. This model is trained under the same settings as ours at 1080 \u00d7 1920 resolution using 8 A100 GPUs for 100K iterations with a batch size of 8. The other baseline, a U-Net-based image-to-image translation network, is directly conditioned on the lighting direction, diverging from the use of intrinsic features (normals, albedo, specular) in the referenced architecture to minimize the negative impact caused by inaccurate intrinsic decomposition on a small dataset. Both models are trained using the same data, and we evaluate all methods across four validation configurations: Novel Light, Novel Expression, Novel View, and Novel Light+Expression+View (LEV), introducing complexities in one or three dimensions. We quantify differences between the relit images and ground truth by computing PSNR, SSIM, LPIPS [Zhang et al. 2018], and FLIP [Andersson et al."}, {"title": "4.4 Ablation Studies", "content": "4.4.1 Ablation studies on diffusion-based relighting. We conducted various ablation studies to evaluate the key technical designs in our diffusion-based relighting model. We first try different methods to integrate lighting control by providing the model with a rough shading estimation. We generate a shading map akin to our strategy in the ControlNet-based relighting model, i.e. dot product between light direction and photometric normal. A visualization of the normal and shading map can be seen in Fig. 14. We then concatenate the latent of the shading map with the flat-lit latent and random noise as input for the diffusion U-Net. We compare the lighting condition with shading map and SH encoding and results are shown in Fig. 14 and Tab. 2. The numerical results indicate that the shading-map-conditioned model slightly outperforms ours due to the use of its richer shading and geometric detail as spatial control. However, capturing photometric normals in multi-view dynamic settings is challenging due to hardware constraints. Notably, our method does not require time-multiplexed lighting during performance capture, allowing for more comfortable flat lighting and reducing costs by eliminating the need for high-speed globalshutter cameras. Therefore, our approach can achieve comparable performance without relying on photometric normals, making it suitable for more applications.\nTo determine which factor contributes the most to color fidelity and overall result quality, we disabled the training strategy that uses pyramid noise (\"w/o pyramid noise\") and removed the initialization that uses the pre-trained Stable Diffusion (\"w/o pre-trained\"). Quantitative results are detailed in Tab. 2. The model performance noticeably degrades without the pyramid noise, as reflected by a decrease in all evaluation metrics. A visual comparison in Fig. 5 further demonstrates that pyramid noise enables more accurate predictions of darker pixels with less color shifting, which likely explains why our method better preserves color fidelity. Additionally, fine-tuning the network based on pre-trained stable diffusion is crucial as it provides strong prior knowledge to generating photorealistic relit results. Without pre-trained weights, the model tends to generate more artifacts, as illustrated in Fig. 6.\n4.4.2 Ablation studies on scalable dynamic 3D Gaussian Splatting. To demonstrate the effectiveness of our scalable dynamic 3DGS, we compare it with three baselines: (1) the original deformable 3DGS model [Jung et al. 2023]; (2) a model partitioning long sequences for separate training without our K-frame initialization (K-frame) or deformation offset regularization (Lreg); and (3) a model using partitioning and K-frame but without Lreg. We evaluate by holding out"}, {"title": "5 REAL-WORLD HDRI RELIGHTING", "content": "Since our method infers OLAT images from any viewpoint that closely match the ground truth, it can reproduce real-world lighting conditions from HDRI maps, following [Debevec et al. 2000]. To evaluate this, we capture reference images of one trained subject in complex indoor and outdoor environments, using HDRI maps to reconstruct the lighting. For comparison, we render the flat-lit subject from similar viewpoints and input these into our relighting model. For each HDRI map, we generate two results: one using the OLAT-based model and the other using the area-light model."}, {"title": "6 MODEL GENERALIZATION", "content": "Although our model is primarily trained on the subject-specific data, which limits its generalization capacity for novel subjects, the model learns to translate from flat to directional lighting, a process that is somewhat subject-agnostic. However, subject-specific information from the model, especially high-frequency details, may remain in the results, as in Fig. 8. To assess whether training on multiple subjects' data reduces such subject-specific information, we train a model using data from three subjects. This model shows improved performance on a novel subject, better preserving the subject's identity, though color shifts are still prominent. This suggests the potential for training a general relighting model using more subjects."}, {"title": "7 LIMITATIONS", "content": "Our technique has a few limitations; for one, we do not completely resolve the temporal consistency issue of image-based diffusion models due to the absence of video training data, and our optical flow post-processing occasionally introduces artifacts during rapid movements, as illustrated in Fig. 9. Emerging video diffusion models hold promise for solving this issue. Secondly, our subject-specific training is not designed to generalize to unseen subjects and can alter identity features in the relit results, as shown in our extension. We believe this limitation could be addressed by training on a diverse, multi-person dataset and/or implementing identity disentanglement techniques. Lastly, we would need to add full-body training data for our relighting technique to be applied to full-body performances."}, {"title": "8 CONCLUSION", "content": "The proposed method shows promising results in relighting freeviewpoint flat-lit facial performances to various lighting conditions using subject-specific OLAT training data. It demonstrates enhanced performance in replicating eye highlights, skin luster, and hair details compared to prior image-based methods, which start with a single diffuse lighting condition. Leveraging an underlying diffusion model allows us to achieve well-conditioned results while maintaining the original subject's appearance and identity by finetuning on subject-specific data. These findings suggest a promising direction for advancing relightable volumetric capture production and facilitating postproduction relighting of any flat-lit footage."}, {"title": "A.1.1 3D Gaussian Splatting Reconstruction with Background Segmentation", "content": "As some post-composition like the generation of HDR environment lighting or training data of our U-Net based network necessitates foreground matting, we propose employing the clean background captured by our stage as the known background in the 3DGS reconstruction. Implementation details are provided here. In the 3D Gaussian Splatting reconstruction, we start with a set of multi-view images, along with corresponding camera calibration and sparse point cloud generated by Metashape 1. Following [Kerbl et al. 2023], we initialize the 3D Gaussians with the sparse point cloud and then optimize the Gaussians, which are defined by a center position x, a covariance matrix \u2211 obtained from rotation r and scaling s, opacity a, and color c represented by spherical harmonic coefficients. Given a view direction transformation matrix W, the convariance matrix \u2211 can be projected to camera space following [Zwicker et al. 2002] and converted to:\n\u03a3' = JWEWT JT, (5)\nwhere J is the Jacobian of the affine approximation of the projective transformation and \u2211 is expressed as follows:\n\u03a3 = RSST RT (6)\nwith the rotation and scaling vectors r and s transformed into matrices R and S. The color of each pixel p on the image is defined as alpha blending of 3D Gaussians that covers this pixel and are sorted in depth:\nCp = \u2211 Tiaici  \u03b1\u2081 = \u03c3\u03b5 (p-qi) \u03a3' (p-qi), (7)\nwhere T\u012f is the transmittance defined as T\u2081 = \u041f}=1(1 \u2212 aj) and qi is the 2D projection position of the ith Gaussian. To optimize Gaussians, the loss function is L1 combined with a D-SSIM term same as the original loss in [Kerbl et al. 2023].\nTo obtain precise alpha mattes for the foreground subject, we employ the captured clean plate as a known background. This facilitates the separation of foreground and background Gaussians, directing the optimization to focus only on the foreground Gaussians. Therefore, the final pixel color is computed as:\nCp = \u2211 Tiaici + (1 - \u2211 Tiai)bp, (8)"}, {"title": "A.4 U-Net Training Pipeline", "content": "To train our U-Net [Ronneberger et al. 2015", "2016": "architecture. Matching this architecture allows weight initializations from a pre-trained model, however we elected to not use these weights as frozen. We observed that convergence is relatively faster with this initialization strategy, but a random weight initialization yields a similar convergence at longer training times. For the decoder, we use style modulations described in StyleGAN2 [Karras et al. 2020"}]}