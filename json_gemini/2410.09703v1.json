{"title": "Universal scaling laws in quantum-probabilistic machine learning by tensor network towards interpreting representation and generalization powers", "authors": ["Sheng-Chen Bai", "Shi-Ju Ran"], "abstract": "Interpreting the representation and generalization powers has been a long-standing issue in the field of machine learning (ML) and artificial intelligence. This work contributes to uncovering the emergence of universal scaling laws in quantum-probabilistic ML. We take the generative tensor network (GTN) in the form of a matrix product state as an example and show that with an untrained GTN (such as a random TN state), the negative logarithmic likelihood (NLL) $L$ generally increases linearly with the number of features $M$, i.e., $L \\sim kM + const.$. This is a consequence of the so-called \u201ccatastrophe of orthogonality,\u201d which states that quantum many-body states tend to become exponentially orthogonal to each other as $M$ increases. We reveal that while gaining information through training, the linear scaling law is suppressed by a negative quadratic correction, leading to $L \\sim \\beta M - M^2 + const$. The scaling coefficients exhibit logarithmic relationships with the number of training samples and the number of quantum channels $\\chi$. The emergence of the quadratic correction term in NLL for the testing (training) set can be regarded as evidence of the generalization (representation) power of GTN. Over-parameterization can be identified by the deviation in the values of $\\alpha$ between training and testing sets while increasing $\\chi$. We further investigate how orthogonality in the quantum feature map relates to the satisfaction of quantum probabilistic interpretation, as well as to the representation and generalization powers of GTN. The unveiling of universal scaling laws in quantum-probabilistic ML would be a valuable step toward establishing a white-box ML scheme interpreted within the quantum probabilistic framework.", "sections": [{"title": "Introduction.", "content": "Developing \u201cwhite-box\u201d (or interpretable) schemes of machine learning (ML) and artificial intelligence is a long-concerned issue that is key to ensuring robustness, stability, fairness, privacy protection, etc. [1-3]. Among others, characterizing and interpreting representation and generalization powers, in other words, the abilities of ML models to handle both learned and unlearned samples, is a fundamental topic in both academic research and real-life applications.\nTensor network (TN) has been considered as a promising candidate for developing \"white-box\" ML due to its solid and systematic theoretical foundation in quantum information sciences and quantum mechanics (see, e.g., Refs. [4-13] and a recent review in Ref. [14]). Originating from quantum many-body simulation, TN has been regarded as an interpretable and efficient numerical tool [15-20], with its representation power characterized by the scaling laws of entanglement entropy (see, e.g., [21-24]). The representation power in quantum many-body simulation can be understood as how the simulation accuracy of the physical properties scales with the parameter complexity. For instance, matrix product state (MPS) [20, 25] provides a faithful representation of the states satisfying the one-dimensional (1D) area law of entanglement entropy, offering a solid foundation for the efficient simulation of 1D quantum systems with local interactions by MPS.\nIn the field of ML, the representation and generalization powers can be characterized by the loss functions or prediction accuracies on the training and testing sets, respectively. Recent researches have uncovered the power laws in the scaling of the loss function in recursive neural networks [26] and Transformer models [27], which have been applied in different scenarios such as multi-modality [28, 29], compute-constrained environments [30], data engineering [31, 32], and reinforcement learning [33]. There are also some pioneering works on revealing the scaling behaviors of TN-based ML [10, 34], which have preliminarily demonstrated the significance of this topic."}, {"title": null, "content": "This work contributes to uncovering the scaling laws in quantum-probabilistic ML, which provides interpretations of the representation and generalization powers. The idea of quantum-probabilistic ML is to model the joint probabilistic distributions by quantum many-body states that satisfy Born's quantum probabilistic interpretation [14]. We take the generative TN (GTN) in the MPS form as an example [6]. With a GTN that is irrelevant to the data (e.g., untrained or random), the negative logarithmic likelihood (NLL) $L$ universally obeys a linear scaling law with respect to the number of features $M$ as"}, {"title": null, "content": "$L \\sim kM + b$. \t\t (1)\nThis suggests the exponentially-vanishing probabilities of the samples obtained from the GTN, which is a consequence of the \"catastrophe of orthogonality (COO)\" for the quantum many-body states. By training the GTN, the acquired information suppresses the linear increase of NLL with a negative quadratic correction term as\n$L \\sim \\beta M - \\alpha M^2 + \\gamma$.\t\t\t\t\t (2)\nSee Fig. 1 for an illustration.\nThe scaling coefficients in $L$ exhibit logarithmic scaling behaviors against the virtual dimension $\\chi$ of the GTN. These coefficients' scaling laws are consistent with the logarithmic scaling of NLL versus $\\chi$, which explains how the powers of GTN improve as the number of quantum channels increases. The deviation of the coefficients' scaling behaviors between the training and testing sets indicates over-parameterization. We further investigate how the orthogonality of the quantum feature map relates to the satisfaction of quantum probabilistic interpretation, as well as to the scaling laws, and to the representation and generalization powers of GTN."}, {"title": "Generative tensor network for quantum-probabilistic machine learning", "content": "The central idea of quantum-probabilistic machine learning is to represent the joint probabilistic distribution of $M$ features (denoted as $P(x)$ with $x = (x_1, x_2, ..., x_M)$) by a quantum many-body state (denoted as $|\\Psi\\rangle$) consisting of $M$ spins (or qubits). The state is related to $P(x)$ via protective measurement as\n$P(x) = |\\langle x | \\Psi \\rangle|^2$, \t\t\t\t\t\t\t (3)\nwhere the product state $|x\\rangle = |x_1, x_2, ..., x_M\\rangle$ is obtained from the sample $x$ by adopting the so-called quantum feature map (QFM).\nThe definition of QFM can be flexible. Here, we consider a widely-used one that reads\n$|x_1, x_2, ..., x_M\\rangle = \\prod_{m=1}^M \\left[ cos(\\frac{\\theta \\pi x_m}{2}) |0\\rangle + sin(\\frac{\\theta \\pi x_m}{2}) |1\\rangle \\right]$\t\t\t (4)\nwith $\\theta$ a hyper-parameter that tunes the orthogonality, and $|0\\rangle$ and $|1\\rangle$ being the eigenstates of the Pauli matrix $\\sigma^z$ [4]. The above formulation means that with a well-trained $|\\Psi\\rangle$, the probability of any given sample $x$ can be obtained by quantum measurement [Eq. (3)].\nObviously, the complexity of $|\\Psi\\rangle$ scales exponentially with the number of spins, i.e., with the number of features $M$ while using the QFM given by Eq. (4). The use of TN to represent $|\\Psi\\rangle$ lowers the complexity from exponential to polynomial. The TN state that correctly represents the joint probability distribution is referred to as GTN."}, {"title": null, "content": "Below, we take MPS as the representation of GTN, which is written as\n$|\\Psi\\rangle = \\sum_{s_1...s_M} \\sum_{a_1...a_{M-1}} A_{s_1}^{a_1} A_{s_2 a_1}^{a_2} ... A_{s_{M-1} a_{M-2}}^{a_{M-1}} A_{s_M a_{M-1}} |s_1 ... s_M\\rangle$, \t\t\t\t\t\t\t (5)\nwith $s_m = 0,1$ and thus dim($s_m$) = $d$ = 2 (called physical dimension). The indexes {$a_m$} are called virtual indexes. We take their dimensions to be uniformly dim($a_m$) = $\\chi$, which is dubbed as virtual dimension. The complexity of an MPS just scales linearly with the number of spins $M$ as\n$\\#|\\Psi\\rangle \\sim O(Md\\chi^2)$.\t\t\t\t\t\t\t\t (6)\nTo train $|\\Psi\\rangle$ so that Eq. (3) can correctly give the joint probability distribution, one can update the local tensors {$A^{[m]}$} of MPS to minimize the NLL\n$L = - \\frac{1}{N} \\sum_n ln P(x^{(n)})$, \t\t\t\t\t\t\t (7)\nwith $x^{(n)}$ denoting the $n$-th sample in a provided training set. Taking the gradient descent method as an example, the local tensors of MPS are updated as\n$A^{[m]} \\leftarrow A^{[m]} - \\eta \\frac{\\partial L}{\\partial A^{[m]}}$, \t\t\t\t\t\t\t\t (8)\nwith $\\eta$ a small positive number known as the gradient step or learning rate."}, {"title": "Catastrophe of orthogonality", "content": "The COO of quantum many-body states says that the fidelity between two \u201cdifferent\u201d quantum states generally decreases exponentially with the number of spins $M$ as\n$|\\langle \\Psi | \\Phi \\rangle | \\sim k^M$, \t\t\t\t\t\t\t\t (9)\nwith $0 < k < 1$.\nBelow, let us focus on the COO of MPS's. We utilize the orthogonal form of MPS and place the orthogonal center on the right end. Then all tensors except the last one are isometries, satisfying the orthogonal conditions $\\sum_{s_m a_{m-1}} A_{s_m a_{m-1}}^{a_m} A_{s_m a_{m-1}}^{a'_m} = I_{a_m a'_m}$, with $I$ denoting the identity. The rightmost tensor satisfies the normalization condition as $\\sum_{s_M a_{M-1}} A_{s_M a_{M-1}} A_{s_M a_{M-1}} = 1$, so that the whole MPS satisfies the normalization condition (say, $|\\langle \\Psi | \\Phi \\rangle| = 1$ for $|\\Psi\\rangle = e^{i \\zeta} |\\Phi\\rangle$ with $\\zeta$ an arbitrary phase factor).\nNote that any MPS can be transformed into such an orthogonal form by gauge transformations (meaning the transformations do not change the tensor obtained by contracting all virtual indices).\nWith $|\\Psi\\rangle$ and $|\\Phi\\rangle$ denoting two different MPS's (for instance, their local tensors, denoted as {$A^{[m]}$} and {$B^{[m]}$}, are randomly generated), $|\\langle \\Psi | \\Phi \\rangle|$ would generally satisfy Eq. (9). To show this, we may compute $|\\langle \\Psi | \\Phi \\rangle|$ by contracting the shared indexes from left to right, starting with"}, {"title": null, "content": "$\\sum_{s_1 a_1} A_{s_1}^{a_1} B_{s_1}^{a_1*}$. As $A^{[1]}$ and $B^{[1]}$ are isometries, we have $|v| \\leq 1$, where the equal sign holds for $A^{[1]} = B^{[1]}$. We may normalize $v$ as $v \\leftarrow v/z_1$ with $z_1 = |v| < 1$ (assuming $A^{[1]} \\neq B^{[1]}$).\nSubsequently, we contract $v$ with $A^{[m]}$ and $B^{[m]}$ for $m = 2, 3, ..., M$. After each contraction, we will generally have $|v| < 1$ as $A^{[m]}$ and $B^{[m]}$ are different isometries. Then we normalize it as $v \\leftarrow v/z_m$ with $z_m = |v| < 1$. Finally, we have $|\\langle \\Psi | \\Phi \\rangle| = \\prod_{m=1}^M z_m$ with\n$\\sum_{s_M a_{M-1}} A_{s_M a_{M-1}} B_{s_M a_{M-1}} = |\\langle v | A^{[M]} B^{[M]} \\rangle|$, which is also generally less than 1. Obviously, $|\\langle \\Psi | \\Phi \\rangle| \\rightarrow 0$ exponentially with $M$, since it is the product of $M$ numbers that are generally less than 1.\nMore rigorous deductions about the COO can be given if we impose certain restrictions on the MPS's, such as translational invariance ($A^{[m]} = A$ and $B^{[m]} = B$ for any $m$). The exponential decay of $|\\langle \\Psi | \\Phi \\rangle|$ can be immediately seen by the fact that the dominant eigenvalue of the transfer matrix ($M_{aa',bb'} = \\sum_s A_{sab} B_{sa'b'}$) is in general less than 1. This corresponds to the \u201cshrinkage\u201d of norm in the contractions discussed above. Considering that the GTN's are obtained by updating the local tensors with ML data, we will not further seek for more rigorous deductions, but will assume the satisfaction of COO with GTN's."}, {"title": "Universal scaling laws of negative logarithmic likelihood", "content": "From Eqs. (3) and (9), the probability of obtaining a sample (x) from an irrelevant GTN should be"}, {"title": null, "content": "exponentially small. Substituting this into Eq. (7), one immediately obtains the linear scaling law of NLL as given by Eq. (1).  demonstrates such a scaling law on the Fashion-MNIST dataset [35] for the inter-class NLL, where we take the categories of the samples to differ from those of the (trained) GTN's. By linear fitting, we have $k$ = 0.1837, 0.1764, and 0.1758 for the training set ($k$ = 0.1835, 0.1762, and 0.1756 for the testing set) for $\\chi$ = 32, 64, and 128, respectively. This indicates the scaling law of NLL does not change by varying $\\chi$.\n shows that the intra-class NLL, where the categories of the samples should correspond to those of the GTN's, obeys the corrected scaling law given by Eq. (2). We have the quadratic coefficient $\\alpha = 3.653 \\times 10^{-5}, 3.252 \\times 10^{-5}$, and $3.041 \\times 10^{-5}$ for the training set ($\\alpha = 3.663 \\times 10^{-5}, 3.582 \\times 10^{-5}$, and $3.603 \\times 10^{-5}$ for the testing set) for $\\chi$ = 32, 64, and 128, respectively. As $M \\sim O(10^2)$, we have $M^2 \\sim O(10^4)$ or $O(10^5)$. Thus, the contribution of the quadratic term is about $O(1)$, which is ignorable particularly for large $M$. For the linear term, we have $\\beta = 7.365 \\times 10^{-2}, 6.582 \\times 10^{-2}$, and $6.006 \\times 10^{-2}$ for the training set ($\\beta = 7.617 \\times 10^{-2}, 7.277 \\times 10^{-2}$, and $7.196 \\times 10^{-2}$ for the testing set). The magnitudes of constant terms are about $O(1)$ or $O(10^{-1})$. These indicate the contributions from all three terms cannot be ignored. The universality of the scaling laws is further demonstrated on various datasets [36].\nBelow, we focus on the intra-class NLL, if not mentioned specifically.  we change the virtual dimension $\\chi$ of the GTN by fixing $M = 784$ and show the values of"}, {"title": null, "content": "$\\alpha$ in Eq. (2) for the intra-class NLL. Our results suggest the logarithmic scaling behavior of $\\alpha$ against $\\chi$ as\n$\\alpha \\sim p_\\alpha ln \\chi + q_\\alpha$.\t\t\t\t\t\t\t\t\t (10)\nSimilar logarithmic behaviors are observed for the coefficients $\\beta$ and $\\gamma$, as shown in Fig. 3(b) and (c).\nSubstituting such logarithmic relations of $\\alpha$, $\\beta$, and $\\gamma$ [one may refer to Eq. (10)] to the corrected scaling law, we have\n$L \\sim (-p_\\alpha M^2 + p_\\beta M + p_\\gamma) ln \\chi + (-q_\\alpha M^2 + q_\\beta M + q_\\gamma)$.\t\t\t\t\t\t\t\t\t (11)\nThis suggests the logarithmic scaling of $L$ against $\\chi$ as\n$L \\sim P_L ln \\chi + Q_L$, \t\t\t\t\t\t\t\t\t (12)\nwhich is verified in . It means logarithmic improvement of the ML powers versus the number of quantum channels (which will be discussed below).\nAnother important observation in  is the deviation between the curves for the training and testing sets. For about $\\chi > 30$, the coefficients in the corrected scaling law for the training set maintain the logarithmic scaling behavior given by Eq. (12). But those for the testing set start to deviate, leading to higher NLL as shown in . These results imply that by further increasing the parameter complexity of GTN for about $\\chi > 30$, the representation power improves by obeying the expected scaling law, but the improvement of generalization power decelerates. This can be regarded as a sign of over-parameterization. Notably, such a sign can be clearly observed with the deviation of the scaling of coefficients but can hardly be seen in NLL, which is one of the advantages of gaining the knowledge about the scaling laws.\nCombining Eqs. (11) and (12), we can immediately obtain the relations between the relevant coefficients as\n$P_L = -p_\\alpha M^2 + p_\\beta M + p_\\gamma$,\n$Q_L = -q_\\alpha M^2 + q_\\beta M + q_\\gamma$.\t\t\t\t\t\t\t\t\t (13)\nFig. 4 verifies such relations by comparing the $P_L$ and $Q_L$ obtained by Eq. (13) with those by fitting the data with Eq. (12), which show remarkable coincidence."}, {"title": "Orthogonality, quantum probabilistic interpretation, and their relations to generalization power", "content": "To gain a deeper"}, {"title": null, "content": "understanding of the representation and generalization powers of GTN, below we investigate the orthogonality of QFM (referring to the orthogonality of the states obtained by QFM of Eq. (4)) and the satisfaction of quantum probabilistic interpretation.\nConsider a simple case of binary images with $\\chi = N$ (with $N$ the number of training samples), which can provide fundamental information on the quantum probabilistic ML with small $N$ or sufficiently-large $\\chi$. Taking $\\theta = 1$ in Eq. (4), we exactly have the orthogonality $\\langle x | y \\rangle = 0$ for any two distinct samples $x \\neq y$. With a well-trained or constructed GTN, say $|\\Psi\\rangle = \\frac{1}{\\sqrt{N}} \\sum_{n=1}^N |x^{(n)}\\rangle$ (that can be exactly written as an MPS with $\\chi = N$), we have $P(x^{(n)}) = 1/N$ for any training sample.\nFor any samples (say a testing sample denoted as $y$) that do not belong to the training set, we have $P(y) = 0$ due to the orthogonality. One may understand $\\chi$ as the number of channels in the GTN. $N$ orthogonal states have to occupy $N$ quantum channels. For $\\chi = N$, all quantum channels in the GTN are taken by the training samples, which gives the ideal equal distribution of probabilities and minimal NLL $L = ln N$. No channel is left for the testing samples, giving vanishing probabilities and divergent NLL. This is demonstrated in and (b) with the binarized Fashion-MNIST dataset.\nIn this binary case with $\\theta = 1$ above, the normalization condition for the full-Hilbert-space quantum probabilities is satisfied, i.e., $\\sum P(x) = 1$ (since $\\sum P(x) = \\langle \\Psi | \\Psi \\rangle$) with the summation through all possible samples. Thus, the obtained NLL is always larger than the theoretical minimum $ln N$. But for $0 < \\theta < 1$, the product states obtained by QFM from different samples are generally not orthogonal to"}, {"title": null, "content": "each other. Consequently, reducing $\\theta$ will cause an \u201cimpertinent\u201d decrease of NLL, essentially due to the violation of the normalization condition of quantum probabilities, i.e., $\\sum_x P(x) > 1$. Consequently, the NLL can become lower than its theoretical minimum $L < ln N$ [see ].\nThe NLL for the testing set also decreases with $\\theta$, which is also \u201cimpertinent\u201d but might be a positive signal on the generalization power (we will show this later).  shows the unusually large average probability $P$ for both the training and testing sets while reducing $\\theta \\rightarrow 0$.\nNow we adopt the GTN classification (GTNC) scheme [8] to demonstrate the representation and generalization powers from another perspective. The idea is to use $G$ GTN's (with $G$ the number of categories) to parameterize the classification boundaries of samples in the Hilbert space of quantum states (the GTN's). Given a sample $y$, the GTNC predicts its classification as\n$g_{pred} (y) = argmax_g \\langle y | | \\Psi^{(g)} \\rangle$,\t\t\t\t\t\t (14)\nwith $| \\Psi^{(g)} \\rangle$ the GTN trained by the training samples in the $g$-th category.\nThe inset of  shows the classification accuracy against $\\theta$ for the training and testing sets of the binarized Fashion-MNIST dataset. For $\\theta = 1$, the training accuracy is theoretically 100%, while the testing accuracy is low. This is a typical example of over-parameterization, which can be the result of insufficient training data or redundant parameters (note that here we take $N = \\chi$).\nBy reducing $\\theta$, the normalization condition of the full-Hilbert-space quantum probabilities is violated, while the training accuracy decreases with $\\theta$. However, non-vanishing probabilities appear for the testing samples, which improve the testing accuracy (see the peak of testing accuracy around $\\theta = 0.5$). These results imply a balance between the violation of the quantum normalization condition and the generalization power of quantum states for ML, tuned by the orthogonality of QFM.\nThe above implication is meaningful for handling non-binary data, which are encountered more frequently than the binary ones. For instance, each feature (pixel) in an 8-bit gray-scale image can take $D = 256$ distinct values. If we rigorously require orthogonality such that $\\langle x | y \\rangle = 0$ for $x \\neq y$, we should map each pixel to a spin with $D$ levels, which is infeasible and impractical. Our results provide an interpretation of why the QFM with two-level ($d = 2$) spins works well on gray-scale images, though it clearly leads to the violation of the quantum normalization condition.\nIn  we show the NLL $L$ and average probability $P$ against $M$ for the original (gray-scale) Fashion-MNIST dataset. For $\\theta = 1$, we still do not have orthogonality; generally, $|\\langle x | y \\rangle| > 0$ for $x \\neq y$. This explains why the NLL of the training set is lower than the theoretical minimum $ln N$. However, we have $L \\rightarrow ln N$ and $P \\rightarrow 1/N$ for large $M$, suggesting the restoration of orthogonality (and the normalization condition) thanks to COO. In the inset of , we show that the testing accuracy increases with $M$, implying that for $D < d$, enhancing orthogonality will generally improve the generalization power [36]."}, {"title": "Summary", "content": "The main contribution of this work is the proposal of universal scaling laws in quantum probabilistic machine learning, where the joint probabilistic distributions are encoded as generative tensor networks in the form of matrix product states. We show that the gain of representation or generalization power suppresses the linear scaling law of negative logarithmic likelihood, which is a result of the catastrophe of orthogonality of quantum many-body states, by introducing a negative quadratic correction. The relations among the scaling laws, orthogonality of the quantum feature map, and the normalization condition of quantum probabilities are discussed, providing a systematic understanding of the representation and generalization powers of quantum probabilistic machine learning."}]}