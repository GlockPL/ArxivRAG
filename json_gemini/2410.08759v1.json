{"title": "Enhancing GNNs with Architecture-Agnostic Graph Transformations: A Systematic Analysis", "authors": ["Zhifei Li", "Gerrit Gro\u00dfmann", "Verena Wolf"], "abstract": "In recent years, a wide variety of graph neural network (GNN) architectures have emerged, each with its own strengths, weaknesses, and complexities. Various techniques, including rewiring, lifting, and node annotation with centrality values, have been employed as pre-processing steps to enhance GNN performance. However, there are no universally \u0430\u0441-cepted best practices, and the impact of architecture and pre-processing on performance often remains opaque.\nThis study systematically explores the impact of various graph transformations as pre-processing steps on the performance of common GNN architectures across standard datasets. The models are evaluated based on their ability to distinguish non-isomorphic graphs, referred to as expressivity.\nOur findings reveal that certain transformations, particularly those augmenting node features with centrality measures, consistently improve expressivity. However, these gains come with trade-offs, as methods like graph encoding, while enhancing expressivity, introduce numerical inaccuracies widely-used python packages. Additionally, we observe that these pre-processing techniques are limited when addressing complex tasks involving 3-WL and 4-WL indistinguishable graphs.", "sections": [{"title": "1 Introduction", "content": "Applying deep learning techniques to graph-structured data presents unique challenges due to the irregular and heterogeneous nature of graphs compared to the more regular structures found in data like images and text. Molecular graphs, in particular, preserve rich and complex structural information, making them essential for tasks like drug discovery, where understanding molecular in-teractions and structures is critical for identifying new therapeutic compounds. This complexity makes it difficult to apply traditional deep learning models to graph data, necessitating specialized approaches."}, {"title": "2 Background", "content": "Graph Isomorphism Given two graphs $G = (A, X)$ and $G' = (A', X')$ with adjacency matrices A and A' and feature matrices X and X' respectively, an isomorphism between these graphs is a bijective mapping $\u03c0 : V[G] \u2192 V[G']$ that satisfies $A_{uv} = A'_{\u03c0(u)\u03c0(v)}$ and $X_v = X'_{\u03c0(v)}$ [6]. We use $G = G'$ to denote that G and G' are isomorphic, where G' can be represented as $\u03c0(G)$. If there is no bijection \u03c0 that satisfies these conditions, we say that G and G' are non-isomorphic.\nGraph Neural Networks A Graph Neural Network (GNN) is a differentiable and learnable function designed to operate on graph-structured data [4]. A GNN that performs a graph classification or regression task is permutation invariant to the ordering or labeling of the nodes, meaning that if the nodes of the input graph are permuted, the output of the GNN remains unchanged. Thus, isomorphic graphs lead to the same GNN output (graph embedding) by design.\nMessage Passing Neural Networks (MPNNs) One of the most widely-used paradigms for designing GNNs is the MPNN framework [14]. This framework consists of multiple layers, where each layer computes a new embedding for a node by"}, {"title": "Input and Computational Graph", "content": "In the context of MPNNs, it is crucial to distinguish between the input graph and the computational graph. The in-put graph is the foundational structure upon which GNNs perform learning and inference tasks. A computational graph G* is the graph which is used for mes-sage passing derived from the input graph through an explicit or implicit graph transformation."}, {"title": "Graph Transformation Function", "content": "We consider transformation functions that serve as pre-processing steps, mapping an input graph to a computational graph. Formally, a transformation $f : G \u2192 G^*$ must preserve graph isomorphism, mean-ing that $G \\cong H$ implies $f(G) = f(H)$. Additionally, it is crucial that these trans-formations can be computed efficiently within polynomial time (with respect to the number of nodes)."}, {"title": "Separation Ability", "content": "Separation ability refers to the capacity of a GNN to dis-tinguish between non-isomorphic graphs [3,4]. Formally, given two non-isomorphic graphs $G_1 = (A_1, X_1)$ and $G_2 = (A_2, X_2)$, where $X_1 = X_2 = 1$ (i.e., the graphs lack node features), a GNN, $g_\u0398(\u00b7)$, exhibits separation ability if $g_\u0398(G_1) \u2260 g_\u0398(G_2)$, thereby correctly identifying the two graphs as distinct.\nThis metric is integral to assessing GNN expressivity, as it quantitatively eval-uates the model's ability to capture and differentiate structural differences be-tween graphs."}, {"title": "Weisfeiler-Leman (WL) Test", "content": "Definition and more details about WL Test are provided in Appendix A."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Datasets", "content": ""}, {"title": "EXP Dataset", "content": "The EXP dataset, introduced by Abboud et al. [12], consists of 600 pairs of non-isomorphic graphs that the 1-WL test fails to distinguish. Each graph is composed of two disconnected components: the \"core pair,\" designed to be non-isomorphic, and the \u201cplanar component,\" which is identical in both graphs to introduce noise.\nHowever, there are only three substantially different core pairs in the dataset, limiting its ability to thoroughly test model expressivity. This lack of diversity has led many recent GNNs to achieve near 100% accuracy on EXP, making it difficult to use for detailed comparisons. To address this limitation, we also use the BREC dataset, introduced by Wang et al. [13]."}, {"title": "BREC Dataset", "content": "The BREC dataset provides a more challenging benchmark with 800 non-isomorphic graphs, organized into 400 pairs across four categories:"}, {"title": "Task and Dataset Augmentation", "content": "The primary task for both datasets is to distinguish between non-isomorphic graph pairs while correctly identifying iso-morphic pairs. To further assess model expressivity, we extended both datasets by adding isomorphic graph pairs. This allows us to evaluate whether the model generates distinct embeddings for non-isomorphic graphs and identical embed-dings for isomorphic ones. The former measures the expressivity of the GNN, the latter is a sanity check to test the numerical stability of the underlying pipeline. Samples of these two datasets are shown in Appendix \u0412."}, {"title": "3.2 Graph Transformations", "content": "We evaluate nine graph transformations that preserve graph isomorphism and can be computed within polynomial time, to assess their impact on the per-formance of standard GNN architectures on the EXP and BREC datasets (|V| represents the number of nodes and |E| represents the number of edges.)"}, {"title": "Virtual Node", "content": "Proposed by Gilmer et al. [14], this method introduces a global virtual node that connects to all other nodes in the graph. This node acts as a shared scratch space for the entire graph, facilitating long-range information exchange during message passing.\nIn particular, a new node vo is added, and edges (vo, v) are created for all v \u2208 V(G). If two graphs G and H are isomorphic, their transformed versions G* and H* will also be isomorphic, as the isomorphism extends naturally to the virtual node. The transformation introduces O(1) additional nodes and O(|V|) edges, resulting in a time complexity of O(|V|)."}, {"title": "Centrality-Based Methods", "content": "We consider four centrality measures: degree, closeness, betweenness, and eigenvector centrality. Each centrality measure pro-vides different insights into the structure of a node's neighborhood and its influ-ence in the graph."}, {"title": "Degree Centrality", "content": "Computes the number of edges incident to each node. This method can be computed in O(|V| + |E|)."}, {"title": "Closeness Centrality", "content": "Measures how close a node is to all others in the graph, based on shortest path distances. For unweighted graphs, the com-plexity is O(|V|2) for sparse graphs."}, {"title": "Betweenness Centrality", "content": "Quantifies the number of shortest paths passing through a node, with a time complexity of O(|V|\u00b7 |E|), making it more computationally expensive for larger graphs."}, {"title": "Eigenvector Centrality", "content": "Evaluates the influence of a node based on the importance of its neighbors. Efficient approximation methods yield a time complexity of O(|E|)."}, {"title": "Distance Encoding", "content": "Distance encoding augments node features by incorpo-rating information about shortest path distances between nodes. This method enhances the graph's structural representation by introducing relative distance information as node features. The shortest path distances are computed using BFS, leading to a complexity of O(|V|(|V| + |E|))."}, {"title": "Graph Encoding with Laplacian Eigenvectors", "content": "Inspired by Dwivedi et al. [15], this method uses the top-k eigenvectors of the graph Laplacian to capture global structural information. These eigenvectors highlight topological features such as clusters and connectivity patterns. The computation of the eigenvectors through eigenvalue decomposition has a time complexity of O(|V|\u00b3), making it suitable for smaller graphs."}, {"title": "Subgraph Extraction", "content": "Following Zhao et al. [16], we extract subgraphs (specif-ically, \"ego graphs\") for each node by considering nodes within a fixed radius (e.g., 2 hops). The size of these subgraphs is then used as additional node fea-tures. It takes O(|V| + |E|) to extract an ego graph for a single node. For each node in the graph, the total complexity is O(|V|(|V| + |E|))."}, {"title": "Add Extra Node on Each Edge", "content": "This transformation adds an intermediary node on each edge, turning direct connections into two-step paths. This modi-fication allows GNNs to capture more fine-grained interactions and reduces the over-squashing problem. Specifically, for each edge (u,v), we introduce a new node w and create edges (u, w) and (w,v). The transformation preserves iso-morphisms since any isomorphism between G and H extends to their modified versions G* and H*. The complexity of this method is O(|E|)."}, {"title": "3.3 Evaluation Methodology", "content": "To assess the expressivity of GNN models, we utilize three key metrics: Equiv-alence Class Count (ECC), False Positives (FP), and False Negatives (FN). These metrics offer a comprehensive evaluation of how effectively the models differentiate between isomorphic and non-isomorphic graphs."}, {"title": "Equivalence Class Count (ECC)", "content": "The ECC metric measures how well the model distinguishes between graphs by counting the number of unique graph embeddings produced by the GNN. Specifically, if two graphs G1 and G2 produce different embeddings, they are assigned to different equivalence classes. The total number of such classes reflects the model's ability to distinguish non-isomorphic graphs while ideally grouping isomorphic graphs into the same class. This metric not only captures the model's expressivity but also highlights potential errors, such as incorrectly separating isomorphic graphs or failing to differentiate non-isomorphic ones."}, {"title": "4 Experiment", "content": "Experimental Setup We evaluate the effect of the graph transformation meth-ods described in this paper by applying them as pre-processing steps to graphs from EXP and BREC datasets. For each dataset, we employ three distinct GNN architectures to assess the impact of these transformations on model perfor-mance: Graph Isomorphism Networks (GIN) [4], Principal Neighbourhood Ag-gregation (PNA) [17], and Deep Set (DS) [18,19]. GIN relies on message passing and are as powerful as the 1-WL test for graph isomorphism. PNA extends GIN by using multiple aggregators and degree scalers to capture a broader range of structural information. In contrast, Deep Set operates on node features in a permutation-invariant manner, making it distinct from typical message-passing"}, {"title": "Observation 1: Enhanced Expressivity Through Pre-Processing", "content": "Table 1 and 2 reveal that the expressivity of GNN models, especially GIN and PNA, can be significantly enhanced by applying various graph transformation techniques as pre-processing steps. In particular, methods involving node feature augmentation consistently improve the models' ability to differentiate between non-isomorphic graphs. This insight underscores the potential of node feature augmentation to serve as a powerful tool in refining GNN architectures."}, {"title": "Observation 2: Challenges with Higher-Order Indistinguishability", "content": "Despite the clear gains in expressivity achieved through pre-processing, the re-sults also reveal a critical limitation: these enhancements fall short when address-ing more complex problems, such as those involving 3-WL and 4-WL indistin-guishable graphs, as shown under CFI sub-dataset in Table 3. This observation indicates that while pre-processing techniques can extend the expressivity of GNNs, they are not applicable to all graph-related challenges. The persistence of these limitations suggests that more advanced (i.e., computationally costly) methods may be required to overcome the expressivity barriers posed by higher-order graph structures."}, {"title": "Observation 3: Graph Encoding's Impact on Expressivity-Equivalence Trade-off", "content": "A critical observation from Table 1, 2, 3 involves the trade-off be-tween distinguishing non-isomorphic pairs and preserving structural equivalence for isomorphic pairs. Graph encoding methods show an increased ability to differ-entiate non-isomorphic graphs, improving the Equivalence Class Count (ECC). However, this enhanced expressivity comes at the cost of misclassifying isomor-"}, {"title": "5 Conclusion and Future Work", "content": "This study examined the role of graph transformations as preprocessing steps to enhance the expressivity of GNNs, with a focus on preserving graph isomor-phism. We showed that these transformations, particularly node feature aug-mentation, can significantly boost the expressivity of simple GNN architectures while keeping computations within polynomial time.\nHowever, our findings also highlight some limitations. While these transforma-tions improve expressivity, they struggle with more complex tasks, especially when dealing with graphs that remain indistinguishable by the 3-WL and 4-WL tests. This suggests that preprocessing alone may not be sufficient and may need to be combined with more sophisticated GNN architectures to address higher-order graph structures. Another key challenge is the trade-off observed with transformations like graph encoding: while they enhance the model's ability to distinguish non-isomorphic graphs, they simultaneously increase the likelihood of misclassifying isomorphic pairs. This trade-off underscores the need to bal-ance expressivity with robustness, pointing to future research that refines these methods or explores hybrid approaches to address both challenges.\nMoving forward, future work could involve combining these preprocessing tech-niques with advanced GNN models or assessing the benefits of integrating mul-tiple transformations. Comparative studies between graph transformation meth-ods and topology modification approaches could also offer valuable insights. Lastly, applying these methods to large-scale, real-world datasets will be critical for validating their practical effectiveness."}, {"title": "A Weisfeiler-Leman Test", "content": "1-WL Test The 1-dimensional Weisfeiler-Leman (1-WL) test [26] applies iter-ative color refinement to distinguish between non-isomorphic graphs. For each node $v \u2208 V$ in a graph $G = (V, E)$, the state $h_v^{(l)}$ of the node is refined iteratively, starting from initial labels (or colors) $h_v^{(0)}$ based on the node's features.\nAt iteration l, the refinement is computed as follows:\n1-WL : $h_v^{(l)}$ = hash$(h_v^{(l-1)}, \\{h_u^{(l-1)} : u \u2208 N(v)\\} )$ $\\forall v \u2208 V$ \nwhere N(v) represents the set of neighbors of node v in graph, and hash is a function that combines the current state of v with the states of its neighbors. The process repeats until the states of all nodes converge. If two graphs G and G' are non-isomorphic, then at least one node's state will differ, such that WL(G) \u2260 WL(G').\nThe WL algorithm effectively distinguishes the majority of graph pairs but it fails with certain fundamental examples. Figure 2 is a simple example. Although these graphs are non-isomorphic, they produce identical color sequences after the 1-WL refinement, making them indistinguishable by the test."}, {"title": "Connection Between GNNs and 1-WL Test", "content": "The following lemma is given by Morris et al. [3] and Xu et al. [4].\nLemma 1 Let $G_1$ and $G_2$ be any two non-isomorphic graphs. If a graph neural network $A : G \u2192 R^d$ maps $G_1$ and $G_2$ to different embeddings, the Weisfeiler-Leman graph isomorphism test also decides $G_1$ and $G_2$ are not isomorphic.\nk-WL Test [37] The k-dimensional Weisfeiler-Lehman test (k-WL) general-izes the classical test by assigning colors to k-tuples of nodes, denoted as v := $(v_1, v_2, ..., v_k) \u2208 V^k$. Initially, k-tuples are labeled based on their isomorphism types, and these labels are iteratively refined according to the i-neighbors $N_i(v)$ of each tuple.\n$N_i(v) = \\{(v_1, ..., v_{i\u22121}, u, v_{i+1}, ..., v_k) | u \u2208 V\\}$ \nTo update the label of each tuple, k-WL iterates as defined:\nk- WL : $h_v = HASH (h_v^{(l\u22121)}, \\{h_u^{(l\u22121)} | u \u2208 N_i(v)\\} | i \u2208 [k]) .$"}, {"title": "B EXP and BREC Datasets", "content": "BREC dataset consists 800 non-isomorphic graphs, organized into 400 pairs across four categories: Basics, Regular, Extension, and CFI."}, {"title": "Basics", "content": "There are 60 pairs of 1-WL indistinguishable graphs, which can be regarded as a complement to EXP dataset with similar difficulty."}, {"title": "Regular", "content": "140 pairs, including simple regular graphs, strongly regular graphs, 4-vertex condition graphs and distance regular graphs. All nodes of a regular graph possess the same degree and it is 1-WL indistinguishable."}, {"title": "Extension", "content": "Extension graphs consist of 100 pairs inspired by Papp and Wattenhofer [36] and generated and sampled by Wang et al. [13] between 1-WL and 3-WL distinguishing difficulty."}, {"title": "CFI", "content": "CFI graphs include 100 pairs. CFI graphs have larger graph sizes than other sections (up to 198 nodes) and can reach to 4-WL indistinguishable."}, {"title": "C Model Selection: Graph Isomorphism Networks, Principal Neighbourhood Aggregation, Deep Set", "content": "Graph Isomorphism Network (GIN) GIN was proposed by Xu et al. [4] which generalizes the WL test and hence achieves maximum discriminative power among GNNs and is as powerful as the 1-WL test. GIN updates node representa-tions as:\n$h_v^{(k)}$ = MLP$^\\Theta$((1+$\\epsilon^{(k)}$) \u00b7 $h_v^{(k-1)}$ + $\u2211_{u\u2208N(v)}$h$_u^{(k-1)}$).\nwhere $\u2208^{(k)}$ is a newly introduced learnable parameter used to adjust the weight of the central node.\nPrincipal Neighbourhood Aggregation(PNA) PNA was proposed by Corso et al. [17] which combines multiple aggregators with degree scalers to capture the diversity if structural roles of nodes in a graph.\nThe PNA layer updates the node feature h\u1d65 of node v as follows:\n$h_v^{(l+1)} = y(h_v^{(l)}$, AGGREGATORS ($\\{h_u^{(l)} : u \u2208 N (v)\\}$)\nwhere AGGREGATORS include mean, sum, max, min and std aggregation methods.\nDeepSets (DS) DeepSets, proposed by Zaheer et al. [18], can operate on sets: a sequence of input features, ensuring that the output is invariant to the permu-tation of the input set elements.\nGiven a set X = {$X_1,X_2, ..., X_n$}, where each xi \u2208 Rd, we define a DeepSet neural network as:\ny = MLP2 ($\u2211_{xi EX}$ MLP1(xi))\nwhere MLP1 maps each element xi of the set to a latent representation of size h and MLP2 takes the aggregated representation and maps it to the final output."}, {"title": "D Appendix D: Experiment Setup", "content": "Dataset Format EXP dataset is generated and stored using Pytorch Geomet-ric. BREC dataset can be stored and read in NetworkX library. To regulate and simplify the test, we store all datasets using Deep Graph Library (DGL). The conversion between libraries (PyTorch Geometric, NetworkX, and DGL) was handled carefully to ensure consistency in graph structures and node features.\nGraph Transformations For the Virtual Node and Graph Encoding transfor-mations, we utilized pre-built functions from the torch_geometric.transforms module. Specifically, we applied VirtualNode to add a global node connected to all other nodes, and AddLaplacianEigenvectorPE to compute and append the top eigenvectors of the Laplacian matrix as positional encodings to the node fea-tures. For methods such as Centrality, Distance Encoding, and Subgraph Extrac-tion, we used the NetworkX library to compute additional node features. These"}]}