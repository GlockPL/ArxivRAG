{"title": "Evaluation of Large Language Models\nvia Coupled Token Generation", "authors": ["Nina Corvelo Benz", "Stratis Tsirtsis", "Eleni Straitouri", "Ivi Chatzi", "Ander Artola Velasco", "Suhas Thejaswi", "Manuel Gomez-Rodriguez"], "abstract": "State of the art large language models rely on randomization to respond to a prompt. As an\nimmediate consequence, a model may respond differently to the same prompt if asked multiple times.\nIn this work, we argue that the evaluation and ranking of large language models should control for the\nrandomization underpinning their functioning. Our starting point is the development of a causal model\nfor coupled autoregressive generation, which allows different large language models to sample responses\nwith the same source of randomness. Building upon our causal model, we first show that, on evaluations\nbased on benchmark datasets, coupled autoregressive generation leads to the same conclusions as vanilla\nautoregressive generation but using provably fewer samples. However, we further show that, on evaluations\nbased on (human) pairwise comparisons, coupled and vanilla autoregressive generation can surprisingly\nlead to different rankings when comparing more than two models, even with an infinite amount of samples.\nThis suggests that the apparent advantage of a model over others in existing evaluation protocols may not\nbe genuine but rather confounded by the randomness inherent to the generation process. To illustrate and\ncomplement our theoretical results, we conduct experiments with several large language models from the\nLlama family. We find that, across multiple knowledge areas from the popular MMLU benchmark dataset,\ncoupled autoregressive generation requires up to 40% fewer samples to reach the same conclusions as\nvanilla autoregressive generation. Further, using data from the LMSYS Chatbot Arena platform, we find\nthat the win-rates derived from pairwise comparisons by a strong large language model to prompts differ\nunder coupled and vanilla autoregressive generation.", "sections": [{"title": "Introduction", "content": "One of the most celebrated aspects of state of the art large language models (LLMs) is that they can solve\nopen-ended, complex tasks across many different application domains such as coding, healthcare and scientific\ndiscovery [1-4]. However, this is crucially what also makes the evaluation and comparison of LLMs very\nchallenging it is very difficult, if not impossible, to create a single benchmark. As a consequence, in recent\nyears, there has been a flurry of papers introducing different benchmarks [5-22]. In fact, one of the flagship\nconferences in machine learning has even created a separate datasets and benchmarks track!\nIn this context, it is somehow surprising that, in comparison, there has been a paucity of work understanding,\nmeasuring or controlling for the different sources of uncertainty present in the evaluations and comparisons of\nLLMs based on these benchmarks [23-30]. In our work, we focus on one source of uncertainty that has been\nparticularly overlooked, the uncertainty in the outputs of the LLMs under comparison.\nGiven an input prompt, LLMs generate a sequence of tokens\u00b9 as output using an autoregressive process [31,\n32]. At each time step, they first use a neural network to map the prompt and the (partial) sequence of tokens"}, {"title": "A Causal Model for Coupled Autoregressive Generation", "content": "Let $V$ denote a vocabulary (set) of tokens, including an end-of-sequence token 1, $V^* = V \\cup V^2 \\cup...\\cup V^K$ be\nthe set of sequences of tokens up to length $K$, and $\u00d8$ be the empty token. An LLM $m\\in M$ takes as input a\nprompt sequence $s_q \\in V^*$ and responds with an output sequence $s \\in V^*$, generated using an autoregressive\nprocess. At each time step $i \\in [K]$ of the process, the LLM first takes as input the concatenation of the prompt\nsequence $s_q$ and the (partial) output sequence $s_{i-1}$, and generates a distribution over tokens $d_i \\in \\Delta(V)$.\nThen, it samples the next token $t_i \\sim d_i$ from the distribution $d_i$ and creates the output sequence $s_i = S_{i-1} ot_i$,\nwhere $o$ denotes the concatenation of a token or sequence with another sequence. If $t_1 = 1$, it terminates and\nreturns $s = s_i$, otherwise, it continues to the next step $i + 1$ in the generation. Once the process is completed,\nthe output sequence $s$ is assigned a scorer, which is subsequently used for model evaluation.\nFollowing the recent work by Chatzi et al. [34], we augment the above autoregressive process using a\nstructural causal model (SCM) [49, 50], which we denote as $\\mathcal{C}$. The SCM $\\mathcal{C}$ is defined by the following\nstructural equations:\n\n$S_0 = S_q,$\n\n$D_i = \\begin{cases}f_D(S_{i-1}, M) &\\text{if } \\text{last}(S_{i-1}) \\neq 1, \\\\ P_{\\O} &\\text{otherwise}\\end{cases},$\n\n$T_i = \\begin{cases}f_T(D_i, U_i) &\\text{if } D_i \\neq P_{\\O}, \\\\ \\O &\\text{otherwise}\\end{cases},$\n\n$S_i = S_{i-1} o T_i, \\quad S = S_K$, and $R = f_R(S, Z).$                                                (1)\n\nIn the above equations, $M, S_q, U = (U_i)_{i\\in{1,...,K}}$, and $Z$ are independent exogenous random variables, with\n$M \\sim P_M, S_q \\sim P_Q, U_i \\sim P_U$, and $Z \\sim P_Z$. Moreover, $f_D, f_T$ and $f_R$ are given functions, $P_{\\O}$ denotes the\npoint mass distribution on $\u00d8$, and $\\text{last}(S_{i-1})$ denotes the last token of the sequence $S_{i-1}$. Here, the function\n$f_D$ maps an input sequence $S_{i-1}$ to a distribution $D_i$ for the next token, using the architecture and network\nweights of the LLM $M$, the function $f_T$ and distribution $P_U$ specify the sampling mechanism that is used to\nsample the next token at each step of the generation process, following the distribution $D_i$, and the function\n$f_R$ and distribution $P_Z$ specify the exact scoring process by which the score $R$ is assigned to an output\nsequence $S$ during the evaluation of the LLM $M$.\nThroughout the paper, we focus on sampling mechanisms that satisfy counterfactual stability [34, 36, 37]\u2014\nan intuitive form of consistency between the next token $T_i$, its distribution $D_i$, and the corresponding noise\nvariable $U_i$. Moreover, we allow the score $R$ to be observable or unobservable, and its semantic meaning and\nsupport of its distribution to vary depending on the evaluation protocol. For example, in multiple-choice\nquestions [52], $R\\in {0,1}$ may represent whether an LLM outputs a correct ($R = 1$) or an incorrect ($R = 0$)\nresponse. In pairwise comparisons [16], $R \\in \\mathbb{R}^+$ may represent the level of user's satisfaction with the response\nprovided by an LLM. In this context, the noise variable $Z$ models any potential sources of uncertainty in the\nscoring process, e.g., uncertainty in users' preferences [53-55].\nBuilding upon the above causal model, we can now formally express what it means to sample (and\nevaluate) output sequences by different LLMs using the same source of randomness, a process we refer to as\ncoupled autoregressive generation. Consider a specific model $m$, a prompt $s_q$, and fixed noise values $u$\nand $z$. It is easy to see that specifying these values is sufficient to (deterministically) specify and compute\nthe exact value of the output sequence $S$ and its score $R$ using the autoregressive generation and scoring\nprocess given by Eq. 1. Then, we can formally express the coupled output sequences by two models $m$ and\n$m'$ and their corresponding scores as the result of interventions $do(M = m)$ and $do(M = m')$, respectively,\nwhere the $do(\\cdot)$ operator forcibly sets the value of $M$ while keeping the prompt $s_q$ and the noise values $u, z$\nfixed [56]. In what follows, we denote the respective scores $R_m(u, s_q, z)$ and $R_{m'}(u, s_q, z)$, following standard"}, {"title": "Evaluation based on benchmark datasets", "content": "In this section, we focus on the evaluation and comparison of LLMs based on benchmark datasets, e.g.,\nmultiple-choice questions [52], and theoretically investigate under which conditions coupled autoregressive\ngeneration requires fewer samples than independent autoregressive generation to reliably estimate the\ncompetitive advantage of one LLM over another.\nGiven a benchmark dataset characterized by an input prompt distribution $P_Q$, for each prompt $s_q \\sim P_Q$,\nlet $C(s_q) \\subset V^*$ denote the set of correct output sequences. In what follows, for ease of exposition, we consider\nbinary scores $R_m(u, s_q) = \\mathbb{1}_{{S_m(u, s_q) \\in C(s_q)}} \\in {0,1}$, where $S_m(u, s_q)$ denotes the output sequence of a\nmodel $m$ given a prompt $s_q$ under a realized sequence of noise values $u$ and $\\mathbb{1}{\\cdot}$ is the indicator function.\nThe standard approach to compare the performance of any pair of LLMs $m, m' \\in M$ using a benchmark\ndataset reduces to estimating the difference in their expected score, i.e.,\n\n$E_{U\\sim P_U, U'\\sim P_U, S_q\\sim P_Q}[R_m(U, S_q) \u2013 R_{m'}(U', S_q)],$\n\nIndependent generation\n\nwhere note that we use different noise variables $U$ and $U'$ for each LLM because, in the standard approach,\neach LLM generates outputs to each query independently (i.e., using independent autoregressive generation).\nAt first, one may think that, in this context, coupled autoregressive generation will not be helpful. Under\ncoupled autoregressive generation, the difference in the expected score adopts the following form:\n\n$E_{U\\sim P_U, S_q\\sim P_Q}[R_m(U, S_q) \u2013 R_{m'}(U, S_q)].$\n\nCoupled generation\n\nTherefore, based on the linearity of expectation and the fact that, under independent generation, both $U$ and\n$U'$ are sampled from the same distribution $P_U$, it is easy to see that Eqs. 2 and 3 are equivalent. However, as\nwe will show next, coupled autoregressive generation allows us to reliably estimate the difference in the two\nLLMs' scores from finite samples faster. More formally, we first start by characterizing the relation between\nthe variances of the difference of scores between LLMs using the following proposition:\nProposition 1 For any pair of LLMs $m, m' \\in M$, it holds that\n\n$Var [R_m (U, S_q) \u2013 R_{m'} (U', S_q)] = Var[R_m(U, S_q) \u2013 R_{m'}(U, S_q)] + 2 \u00b7 Cov[R_m(U, S_q), R_{m'}(U, S_q)]$         (4)"}, {"title": "Evaluation based on pairwise comparisons", "content": "In this section, we focus on the evaluation and comparison of LLMs according to their level of alignment\nwith human preferences, as elicited by pairwise comparisons between outputs of different LLMs to the same\nprompts. Such an evaluation protocol has become particularly popular to evaluate and compare LLMs in\nopen-ended, complex tasks in which, in contrast to benchmark datasets, there are no structured ground-truth\nEU\\sim PU,U'\\sim PU,Sq\\sim PQ[1{Rm(U,Sq) > Rm'(U', Sq)}]\n\nIndependent generation\n\nwhere 1{Rm (u, sq) > Rm' (u, sq)} = 1 (0) means that, for prompt sq and realized sequence of noise values u,\nthe output of m is (not) preferred over the output of m'.\nHere, similarly as in Eq. 2 in the evaluation based on benchmark datasets, we use different noise variables\nU and U' because, in this standard approach, each LLM generates outputs to each prompt independently\n(i.e., using independent autoregressive generation). Conversely, under coupled autoregressive generation, the\nwin-rate adopts the following form:\n\nEU\\sim PU,Sq\\sim PQ[1{Rm(U, Sq) > Rm'(U, Sq)}]\n\nCoupled generation\n\nHowever, in contrast with the comparison of the expected difference in scores under independent and\ncoupled autoregressive generation in the evaluation based on benchmark datasets, we cannot directly claim\nthat Eqs. 6 and 7 are equivalent because the win-rate is non-linear with respect to Rm (u, sq) and Rm' (u', sq).\nIn what follows, we will further analyze the difference between win-rates in two canonical settings similar to\nthose we used in Section 3.\nIn the first canonical setting, for each prompt, the response can only be one of two given single-token\nsequences and one of these sequences is preferred over the other by the user. Further, the LLMs under\ncomparison always output one of them as a response and the sampling mechanism used by the LLMs satisfies\ncounterfactual stability. Then, we can compute the win-rates achieved by each LLM m against any other\nLLM m' \u2260 m under independent and coupled autoregressive generation using the following proposition:\nProposition 4 Given a fixed prompt sq ~ PQ, assume that fR(s+) > fR(s-) for s+ = sq0t+ and s_ = sq0t_,\nwhere t+ and t_ are single-token sequences. Further, assume that the LLMs m and m' respond t+ with\nprobability pm and pm', respectively, and t_ with probability 1 - pm and 1 - pm', and the sampling mechanism\ndefined by fr and Pu satisfies counterfactual stability. Without loss of generality, assume pm' > pm. Then,\nunder coupled autoregressive generation, we have that\n\nEU~PU [1{Rm(U,sq) > Rm'(U, sq)}] = 0,\n\nEU~PU [1{Rm (U,sq) < Rm'(U, sq)}] = pm' - Pm.\nConversely, under independent autoregressive generation, we have that\n\nEU,U'~PU [1{Rm(U, 8q) > Rm'(U', sq)}] = pm(1 \u2212 Pm'),\n\nEU,U'~PU [1{Rm(U, 8q) < Rm'(U', $q)}] = pm'(1 \u2212 Pm)\n\nFrom the above proposition, we can readily conclude that, in general, the win-rates do differ under independent\nand coupled autoregressive generation. Nevertheless, we may be tempted to conclude that, for ranking LLMs,\nthis difference appears inconsequential because, for each fixed prompt sq, we have that\n\nEU~PU [1{Rm (U, 5q) < Rm'(U, $q)}] \u2013 EU\u223cPv [1{Rm(U,Sq) > Rm'(U,sq)}]\n= EU,U'~PU [1{Rm(U,sq) < Rm'(U', $q)}] \u2013 EU,U'~P\u028a[1{Rm(U,sq) > Rm'(U', $q)}]."}, {"title": "Experiments", "content": "In this section, we evaluate several large language models from the Llama family under coupled and independent\nautoregressive generation using: (i) the benchmark dataset MMLU [52] and (ii) pairwise comparisons between\noutputs of the LLMs when prompted using open-ended questions from the LMSYS Chatbot Arena platform [57].\nIn all our experiments, the LLMs use an implementation of the Gumbel-Max SCM [34] as a sampler both\nunder coupled and independent autoregressive generation. For details on hardware, datasets and models used\nfor experiments, refer to Appendix C."}, {"title": "Evaluation on the MMLU dataset", "content": "In this section, we compare three LLMs of different sizes, namely, Llama-3.1-8B-Instruct and Llama-\n3.2-{1B, 3B}-Instruct, using the MMLU benchmark dataset [52], which comprise 14,042 multiple choice\nquestions covering 52 knowledge areas. Recall that our theoretical results in Section 3 suggest that coupled"}, {"title": "Evaluation on the LMSYS-Chat-1M Dataset", "content": "In this section, we compare the same three LLMs as in the previous section as well as three quantized variants, namely, Llama-3.1-8B-Instruct-{AWQ-INT4, bnb-4bit, bnb-8bit}, using pairwise comparisons between\ntheir outputs by a strong LLM, when prompted with open-ended questions from the LMSYS Chatbot Arena\nplatform [57]. Similarly as in the previous section, here, our goal is to investigate to what extent the theoretical\nresults derived in Section 4, which show that the win-rates under coupled and independent autoregressive\ngeneration are different in certain canonical settings, generalize."}, {"title": "Discussion and Limitations", "content": "In this section, we discuss several aspects of our work, which we believe are important to consider and may\nserve as a basis for future research.\nModel assumptions. Our theoretical analysis of coupled autoregressive generation focuses on sampling\nmechanisms that satisfy counterfactual stability [36]. Although counterfactual stability has been shown to be\na desirable property for causal mechanisms in SCMs and, more specifically, for causal mechanisms used for\nsampling in LLMs [34], counterfactual stability may not always be appropriate and should be justified by\ndomain specific knowledge [60]. In this context, it is also worth mentioning that the Gumbel-Max SCM is not\nthe only SCM that satisfies counterfactual stability [60, 61]. Therefore, it would be interesting to understand\nthe sensitivity of coupled autoregressive generation to this specific choice of SCM as well as extending our\ntheoretical analysis to sampling mechanisms satisfying other alternative properties [62].\nPractical considerations. Our experimental results and theoretical analysis suggest that coupled autore-\ngressive generation is most advantageous over independent autoregressive generation whenever the LLMs\nunder comparison are sufficiently close in terms of their next-token distributions. Motivated by this obser-\nvation, it would be important to identify which parts of the LLM development pipeline (e.g., the LLMs'\narchitectures, training data, or fine-tuning process) lead, in practice, to sufficiently small changes in the\nnext-token distributions for coupled autoregressive generation to be most beneficial.\nOur causal model for coupled autoregressive generation assumes that the LLMs under comparison share\nthe same vocabulary. However, in practice, this may not hold since models use different tokenizers different\nfamilies of tokenizers may even use different low-level representations for tokens that appear to be the same at\nthe string level. One could think of naively lifting this assumption by merging the vocabularies of different\nLLMs, however, we empirically found that, using this strategy, different LLMs end up using different tokens\n(and thus noise values) to generate the same responses and thus coupled autoregressive generation provides\nsignificantly lower gains. Extending our causal model for coupled autoregressive generation to LLMs with\ndifferent tokenizers is an interesting, albeit challenging, direction for future work.\nEvaluation. We have conducted experiments using LLMs from the Llama family, namely Llama-3.1-\n8B-Instruct and Llama-3.2-{1B, 3B}-Instruct, and quantized versions thereof. It would be interesting\nto conduct experiments with LLMs from other families and also consider fine-tuned versions of them to\nunderstand how coupled autoregressive generation behaves in different settings. Furthermore, we have"}, {"title": "Conclusions", "content": "In this work, we have introduced a causal model of coupled autoregressive generation that enables the\nevaluation and comparison of different LLMs under the same source of randomness. In several canonical\nsettings, we have shown that, in evaluations based on benchmark datasets, coupled autoregressive generation\ncan provably reduce the number of samples required to reliably compare the performance of LLMs and, in\nevaluations based on pairwise comparisons, it can provably lead to different and, perhaps more intuitive,\nrankings of LLMs in comparison with independent autoregressive generation. Lastly, we have empirically\ndemonstrated that our theoretical results generalize to several state of the art LLMs and datasets commonly\nused for the evaluation and ranking of LLMs."}, {"title": "A Formal Definition of Counterfactual Stability", "content": "Counterfactual stability is a desirable property of SCMs [36] that has previously been used in the context of\nautoregressive generation of LLMs [34]. In the following, we provide its formal definition along with a simple\nexample to explain the intuition behind it. Throughout this section, $P_{\\mathcal{C}; do(\\cdot)}$ denotes the probability of the\ninterventional distribution entailed by an SCM $\\mathcal{C}$ under an intervention $do(\\cdot)$. Moreover, $P_{\\mathcal{C}|*;do(\\cdot)}$ denotes\nthe probability of the counterfactual distribution entailed by an SCM $\\mathcal{C}$ under an intervention $do(\\cdot)$ given\nthat an observed event has already occurred.\nDefinition 1 A sampling mechanism defined by fr and Pu satisfies counterfactual stability if for all LLMs\nm, m' \u2208 M, \u0456 \u2208 {1,2,..., \u039a} and tokens t1, t2 \u2208 V with t1 t2, the condition\nPC ; do(M=m')[T\u2081 = t1 | Di] PC ; do(M=m')[T\u2081 = t2 | Di]\nPC; do(M=m) [T\u2081 = t1|Di] \u2013 PC ; do(M=m)[Ti = t2 | Di\nimplies that PC|D\u2081,M=m,T\u2081=t1 ; do(M=m')[T; = t2] = 0."}, {"title": "Proofs", "content": "We can rewrite the variance of the difference in scores under independent generation in terms of the variance\nof the difference in scores under coupled generation as follows:\n\nVar[Rm(U, Sq) \u2013 Rm'(U', Sq)]] = Var[Rm(U, Sq) \u2013 Rm'(U, Sq) + Rm\u2032(U, Sq) \u2013 Rm' (U', Sq)]]\n\n= Var[Rm(U, Sq) \u2013 Rm' (U, Sq)] + Var[Rm' (U, Sq) \u2013 Rm' (U', Sq)]\n\n+2\u00b7 Cov[Rm(U, Sq) \u2013 Rm'(U, Sq), Rm' (U, Sq) \u2013 Rm' (U', Sq)].\n\nFor the variance of the difference in scores for the same LLM under independent noise values, we have that\nVar[Rm' (U, Sq) \u2013 Rm' (U', Sq)]@ E[(Rm (U, Sq) \u2013 Rm' (U', Sq))\u00b2] \u2013 E[Rm' (U, Sq) \u2013 Rm' (U', Sq)]2\n@E[Rm' (U, Sq)2 \u2013 2\u00b7 Rm' (U, Sq)Rm' (U', Sq) + Rm\u2032(U', Sq)\u00b2]\n2. E[Rm' (U, Sq)\u00b2] \u2013 2 \u00b7 E[Rm'(U, Sq)Rm' (U', Sq)],\nwhere (a) holds by the definition of variance, (b) is due to the subtraction term being 0, and (c) is due to the\nlinearity of expectation. Further, for the covariance of the difference in scores under independent generation\nand the difference in scores under coupled generation, we have that\nCov[Rm (U, Sq) \u2013 Rm' (U, Sq), Rm\u2032(U, Sq) \u2013 Rm' (U', Sq)]\n@E[(Rm(U, Sq) \u2013 Rm\u2032(U, Sq)) \u00b7 (Rm' (U, Sq) \u2013 Rm\u2032(U', Sq))]\n- E[Rm(U, Sq) \u2013 Rm\u2032(U, Sq)] \u00b7 E[Rm' (U, Sq) \u2013 Rm'(U', Sq)]"}, {"title": "Proof of Proposition 2", "content": "Due to Proposition 1, to show that Eq. 5 holds, it suffices to show that the covariance between the scores of\nthe different LLMs under coupled generation is non-negative, i.e., Cov[Rm(U, Sq), Rm' (U, Sq)] \u2265 0.\nTo this end, we first rewrite the covariance as\nCov[Rm (U, Sq), Rm' (U, Sq)] = P[Rm(U, Sq) = 1, Rm\u2032(U, Sq) = 1]-P[Rm(U, Sq) = 1]\u00b7P[Rm'(U, Sq) = 1]\n= \u2211P[Sq = $q] \u00b7 (P[Rm(U, sq) = 1, Rm'(U, sq) = 1] \u2013 P[Rm(U, 8q) = 1] \u00b7 P[Rm'(U, sq) = 1]) (11)\nNext, we note that the event Rm(U,sq) = 1 is equivalent to LLM m sampling the ground truth token for\nprompt sq. Without loss of generality, assume t\u2081 is the ground truth token, i.e., C(sq) = t1. Then, since only\ntokens {t1, t2} have positive probability under m and m', it must hold that either (i) one LLM assigns a\ngreater probability to t\u2081 and the other LLM assigns a greater probability to t\u2082, or (ii) both LLMs assign the\nsame probabilities. Further, since the sampling mechanism defined by fr and Pu satisfies counterfactual\nstability, we have that the condition in Eq. 10 holds in both (i) and (ii) and, under coupled generation, the\nLLM with greater (or equal) probability for t\u2081 will always sample t\u2081 when the LLM with lower (or equal)\nprobability does. This implies that\nP[Rm(U,sq) = 1, Rm' (U, sq) = 1] = min{P[Rm(U, sq) = 1], P[Rm' (U, sq) = 1]}\nFinally, since it holds that\nmin{P[Rm(U, sq) = 1], P[Rm'(U, 8q) = 1]} > P[Rm(U, sq) = 1]P[Rm'(U, sq) = 1]\nbecause P[Rm(U, sq) = 1] \u2208 (0,1) and P[Rm'(U,sq) = 1] \u2208 (0,1) by assumption, we can conclude from\nEq. 11 that\nCov[Rm (U, Sq), Rm' (U, Sq)] > 0."}, {"title": "Proof of Proposition 3", "content": "Using Proposition 1, we have that\nCov[Rm (U, Sq), Rm' (U, Sq)] = E[Rm(U, Sq) \u00b7 Rm' (U, Sq)] \u2013 E[Rm(U, Sq)] \u00b7 E[Rm' (U, Sq)]\n= P[Rm(U, Sq) = 1, Rm' (U, Sq) = 1] \u2013 P[Rm(U, Sq) = 1) \u00b7 P[Rm' (U, Sq) = 1] .\n(i)\n(ii)"}, {"title": "Proof of Proposition 4", "content": "Under coupled autoregressive generation, if the LLM m samples the preferred token t+, then the LLM m'\nmust also sample t+ because t+ is more likely under m' than under m and the sampling mechanism defined\nby fr and Pu satisfies counterfactual stability. This implies that the win-rate achieved by m against m' is\nEU~Pv [1{Rm(U, 8q) > Rm'(U,sq)}] = P[ft(fD(sq, m),U) = t+, fT(fp(sq, m'),U) = t_] = 0 (17)"}, {"title": "Proof of Proposition 5", "content": "We follow the notations and technique of Proposition 3. Fix query sq and consider first the case of independent\nautoregressive generation. Since each LLM can only assign a non-zero probability to single-token sequences,\nwe have:\nP[Rm(U, sq) = Rm'(U', $q)] = \u2211P[fT(fD($q,m),U) = tk]P[fT(fD($q, m'),U) = tk]\nIn the case of coupled autoregressive generation, since\nwe obtain:\nP[Rm(U,sq) = Rm' (U,sq)]\n\u2211kP[{fT(fD(sq,m), U) = tk, fr(fD(sq, m'),U) = tk}]"}, {"title": "Calculation of average win-rates in the example used in Sections 1 and 4", "content": "In this section, we provide detailed calculations of the win-rates for the example in Sections 1 and 4. Recall\nthat in this example, we are given three LLMS m1, m2 and m3, and we need to rank them according to their\nability to answer correctly two types of input prompts, q and q', picked uniformly at random. We assume\nthat the true probability that each LLM answers correctly each type of input prompt is given by:\n\nUsing Proposition 4, the win-rates under independent autoregressive generation are given, for each LLM\nmk, by:\n\u03a3EU,U'~PU,Sq~PQ[1{Rmx (U, Sq) > Rm; (U', Sq)}] =\nSubstituting the numerical values we obtain:\n\u03a3EU,U'~PU,Sq~PQ[1{Rm1(U,Sq) > Rm; (U', Sq)}] = 0.1545\n\u03a3EU,U'~PU,Sq~PQ [1{Rm2(U,Sq) > Rm; (U', Sq)}] = 0.15675\nSimilarly, using Proposition 4, the win-rates using coupled autoregressive generation can be written, for\neach LLM mk, as:\nwhere ()+ = max(0,) denotes the positive part. Substituting the numerical values we obtain"}, {"title": "Additional Experimental Details", "content": "Hardware setup. Our experiments are executed on a compute server equipped with 2 x Intel Xeon Gold\n5317 CPU, 1,024 GB main memory, and 2 \u00d7 A100 Nvidia Tesla GPU (80 GB, Ampere Architecture). In\neach experiment a single Nvidia A100 GPU is used.\nDatasets. As a benchmark dataset, we use Measuring Massive Multitask Language Understanding dataset\n(MMLU) [52] consisting of 14,042 questions covering 52 diverse knowledge areas with each question offering\nfour possible choices indexed from A to D, and a ground-truth answer. For pairwise comparison tasks, we use\nthe first 500 questions from the LMSYS-Chat-1M dataset [58].\nModels. In our experiments, we use Llama-3.1-8B-Instruct, its quantized variants Llama-3.1-8B-\nInstruct-{AWQ-INT4, bnb-4bit, bnb-8bit} and Llama-3.2-{1B, 3B}-Instruct models. The models\nare obtained from Hugging Face, and the quantised LLM variants Llama-3.1-8B-Instruct-{bnb-4bit,\nbnb-8bit} are built using the bitsandbytes library [64]."}, {"title": "Additional Experimental Results on the MMLU Dataset", "content": "Additional Experimental Results on the LMSYS-Chat-1M Dataset"}]}