{"title": "A Cognitive Writing Perspective for Constrained Long-Form Text Generation", "authors": ["Kaiyang Wan", "Honglin Mu", "Rui Hao", "Haoran Luo", "Tianle Gu", "Xiuying Chen"], "abstract": "Like humans, Large Language Models (LLMs) struggle to generate high-quality long-form text that adheres to strict requirements in a single pass. This challenge is unsurprising, as successful human writing, according to the Cognitive Writing Theory, is a complex cognitive process involving iterative planning, translating, reviewing, and monitoring. Motivated by these cognitive principles, we aim to equip LLMs with human-like cognitive writing capabilities through CogWriter, a novel training-free framework that transforms LLM constrained long-form text generation into a systematic cognitive writing paradigm. Our framework consists of two key modules: (1) a Planning Agent that performs hierarchical planning to decompose the task, and (2) multiple Generation Agents that execute these plans in parallel. The system maintains quality via continuous monitoring and reviewing mechanisms, which evaluate outputs against specified requirements and trigger necessary revisions. CogWriter demonstrates exceptional performance on LongGenBench, a benchmark for complex constrained long-form text generation. Even when using Qwen-2.5-14B as its backbone, CogWriter surpasses GPT-4o by 22% in complex instruction completion accuracy while reliably generating texts exceeding 10,000 words. We hope this cognitive science-inspired approach provides a paradigm for LLM writing advancements: CogWriter.", "sections": [{"title": "1 Introduction", "content": "LLMs like ChatGPT (Achiam et al., 2023) have begun to mirror human-like writing capabilities across diverse natural language processing tasks (Xi et al., 2023). From crafting concise summaries (Zhang et al., 2024b) to composing structured reports (Schmidgall et al., 2025; Wang et al., 2024d), these models can generate coherent text in a single pass (Rasheed et al., 2025; Minaee et al., 2024) with a fluency that often rivals human writers. Recent advances have led to models with expanded context windows of up to 128K tokens (Pawar et al., 2024), theoretically enabling the generation of extensive documents (Bai et al., 2024). However, these models face significant challenges when tasked with generating constrained long-form text under complex constraints, such as following detailed instructions over 10,000 words (Wu et al., 2024a). This limitation poses a crucial barrier for applications requiring extended (Shi et al., 2024), well-structured content, including creative design proposals, technical documentation, and comprehensive research reports.\nTo understand the disparity between LLMs and human writers, we refer to Cognitive Writing Theory (Flower, 1981), which emphasizes how humans succeed in writing through a recursive activity that dynamically integrates multiple cognitive processes. As shown in the top part of Figure 1, these processes include planning, where writers establish high-level goals and develop structural outlines; translating, where writers transform abstract ideas into coherent text; and reviewing, where writers continuously evaluate and refine their generated content. Crucially, writers control these components through continuous monitoring, allowing them to assess and adjust text to better align with evolving objectives throughout the writing process. Current LLMs excel at generating fluent text, effectively performing the translating function of converting internal token vectors into textual content. However, they fundamentally conflict with key cognitive principles in three ways, as shown in the bottom part of Figure 1: 1) They treat long-form text generation merely as an end-to-end task, overlooking the crucial hierarchical planning process that should guide content generation; 2) Their autoregressive architecture renders generated tokens as immutable context, preventing the reviewing and restructuring capabilities essential to hu-"}, {"title": "2 A Cognitive Writing Perspective", "content": "The challenge of constrained long-form text generation extends far beyond simply producing more words. Just as a novelist crafts an intricate narrative or an architect designs a towering structure, long text generation requires the coordination of multiple cognitive processes working together. Through the lens of cognitive writing theory, three fundamental processes emerge: hierarchical planning, continuous monitoring, and dynamic reviewing (Flower, 1981), as illustrated in Figure 1.\nHierarchical Planning Long-form writing requires a delicate cognitive balance between maintaining local coherence and global structure. Human writers cope with this constraint, as working memory cannot simultaneously retain every detail of a complex narrative (Kellogg, 2013). Skilled writers manage this limitation through hierarchical decomposition, systematically structuring the writing process into multiple levels (e.g., chapters, sections, and paragraphs). This approach enables them to alternate between top-down thematic planning and bottom-up content development, ensuring alignment with high-level objectives while refining details (Hayes and Flower, 2016).\nLLMs encounter a similar limitation: they generate text in a linear, autoregressive manner without an independent planning module to iteratively refine outlines or adapt strategies in real time (Xie et al., 2023). Consequently, their direct prompt-to-text generation process often struggles with complex, multi-threaded narratives. Without structured guidance, LLMs are prone to losing coherence over long spans, as their finite computational capacity quickly becomes overwhelmed (Hu et al., 2024).\nContinuous Monitoring Effective planning in writing requires continuous oversight. Human writers naturally monitor their work, acting like their own editors. They pay attention to both small details\u2014such as word choice and sentence flow\u2014and the larger structure, ensuring the text maintains a clear theme and purpose (Kellogg, 2013).\nIn contrast, current mainstream LLMs generate text in a linear, close-loop manner, without the ability to review or refine their output. They lack a built-in system to check their progress against the intended goals, making it difficult to spot and correct issues during generation. Without external monitoring, LLMs struggle to detect when the content drifts off-topic, when the style becomes inconsistent, or when repetition occurs\u2014problems that are especially common in extended long-form writing (Wang et al., 2024c; Ping et al., 2025).\nDynamic Reviewing While monitoring continuously tracks the writing process by detecting small errors, inconsistencies, or deviations, reviewing takes this feedback and applies it to make necessary adjustments, such as reorganizing content or improving logical flow. Human writers naturally engage in this iterative reviewing process, refining their work by revisiting earlier content and making adjustments (Bereiter and Scardamalia, 2013).\nHowever, LLMs lack this ability due to their left-to-right, single-pass generation (Yao et al., 2023; Wu et al., 2024b). Without the ability to revisit or reorganize previous content, LLMs struggle with global revisions, such as restructuring sections or ensuring consistency across distant parts of the text (Bae and Kim, 2024; Cheng et al., 2024; Zhang et al., 2024a). This absence of dynamic reviewing often results in long-form outputs with accumulated errors, inconsistencies, or redundant content."}, {"title": "3 Problem Formulation", "content": "Based on the analysis in Section 2, successfully generating long-form text requires addressing key deficiencies in current LLMs. We propose a new paradigm that equips LLMs with essential abilities to handle long, complex, and instruction-driven text generation. To achieve this, we formally define the constrained long-form text generation task, specifying the types of instructions and requirements the model must meet.\nFollowing Wu et al. (2024a), we formally define constrained long-form generation as the task of generating a sequence of interrelated text segments $D = {D_1, D_2, ..., D_n}$, where each $D_i$ represents a coherent unit of text that must satisfy certain constraints. Each segment $D_i$ must achieve a target $L$ words and adhere to a set of instructions $T$. The instructions $T$ guide the generation process and are classified into three types: 1. Single Instruction (SI): This instruction specifies content that must appear at exact, predefined positions. It is denoted as $T_S = {T_{S1},T_{S2},...}$, where each $T_{Si}$ indicates specific content that must be placed in a precise position within the generated descriptions. 2. Range Instruction (RI): This instruction specifies the content that must be included in each description within a designated range. It is represented as $T_R = {T_{i},T_{i+1},..., T_{i+j}}$, ensuring that the specified content is sequentially assigned within the range $[i, i + j]$. 3. Periodic Instruction (PI): This instruction mandates the periodic repetition of specific content at regular intervals. It is defined as $T_P = {T_{n}, T_{2n}, ..., T_{m \\cdot n}}$, where n is the interval length and m specifies the number of repetitions. These instructions are unified into a comprehensive Check Set: $T = {T_S,T_R,T_P}$.\nThe versatility of this framework extends to various practical applications. For example, in architectural planning for a 100-floor building, Single Instructions determine specific facilities like a medical center on the 20th floor, Range Instructions define functional zones like corporate offices spanning floors 5-12, and Periodic Instructions maintain consistent amenities such as security checkpoints on every fifth floor. Each floor description must meet a target length of 200 words."}, {"title": "4 Methodology", "content": "Drawing upon our analysis of cognitive writing processes and the identified limitations of single-pass generation approaches, in this section, we propose\nCogWriter, a training-free framework that equips LLM with cognitive writing capabilities.\n4.1 Framework Overview\nAs shown in Figure 2, CogWriter is designed to bridge the gap between current LLMs and human-like writing processes by integrating planning, monitoring, and reviewing mechanisms into the generation workflow. At its core, CogWriter employs a specialized Planning Agent that hierarchically decomposes the task and create structured plans, breaking down complex writing tasks into manageable components while maintaining their intricate relationships. Generation Agents execute these plans while monitoring mechanisms continuously evaluate the output to detect deviations in content, structure, or requirements. When issues are identified by monitor or LLM, a review process is triggered to revise and refine the output, ensuring overall coherence and adherence to instructions.\n4.2 Planning Agent\nThe Planning Agent serves as the strategic brain of the system. Similar to how an experienced writer begins with a detailed outline, this agent analyzes task requirements and generates a structured initial plan $P_{initial}$ under strict format constraints:\n$P_{initial} \\leftarrow \\text{GenerateInitialPlan}(p_{plan}),$\nwhere $P_{plan}$ is the task-specific prompt incorporating instruction descriptions $T$. The target plan is hierarchical, comprising unit plans: $P_{initial} = {P_{initial1}, ..., P_{initialn}}$.\nAfter generating the initial plan, the monitoring mechanism supervises the process and relays signals to the reviewing mechanism for evaluation and validation. The reviewing mechanism evaluates the plan through two key checks: First, it verifies if the generated content satisfies the task-specific constraints $T$. Second, it checks the plan's structure for any syntax errors and applies necessary corrections. If any issues are detected, a revision process is triggered to refine the plan:\n$P_{revised} \\leftarrow \\text{PlanRevise}(p_{revise}, P_{initial}), (1)$\n$P \\leftarrow \\text{FormatRevise}(P_{revised}), (2)$\nwhere $P_{revised}$ includes the revision prompt for the task instructions $T$. This iterative refinement ensures that the final plan is not only of high quality but also optimally structured to guide robust and effective content generation."}, {"title": "4.3 Generation Agents", "content": "Once the global plan $P = {P_1, ..., P_n}$ is finalized by the Planning Agent, multiple Generation Agents take over, each responsible for generating content for a specific description task $D_i$. The process begins with validating and refining the local plan $P_i$, through monitoring and reviewing similar to the Planning Agent to ensure it aligns with the instruction requirements $T$. Concretely, if discrepancies are detected, adjustments are applied to update the plan, as shown in the following equation:\n$P \\leftarrow \\text{PlanAdjust}(p_{adjust}, P_i), (3)$\nwhere $p_{adjust}$, encompasses the specialized prompt designed for reviewing each local plan $P_i$ against the residual informing from $T$.\nUpon validation of $P$, the agent generates content by executing the plan:\n$D_{initial} \\leftarrow \\text{Generate}(p_{write}, P), (4)$\nwhere $p_{write}$ is the prompt to generate content following the guidance of the plan $P$. Based on our preliminary study, this process generally produces content that meets most instruction criteria. However, length constraints may still require further refinement due to the limitations of most current LLMs in controlling output length precisely. To address this, a revision function adjusts the content to meet the specified length $L$:\n$D_i \\leftarrow \\text{LengthRevise}(p_{length}, D_{initial}), (5)$\nwhere $P_{length}$ is the prompt used to adjust the content length to $L$ by expanding or compressing the generated text while preserving key details, semantic integrity, and overall coherence.\nBy following this process, each segment $D_i$ seamlessly integrates with the overall narrative structure, ensuring both local coherence and global thematic consistency."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nDataset We evaluated CogWriter using Long-GenBench-16K (Wu et al., 2024a), a benchmark specifically designed for assessing a model's complex constrained long-form text generation capabilities. The dataset features four scenarios, each requiring approximately 16,000 tokens: (1) Diary Writing and (2) Menu Design assess temporal consistency by requiring coherent content organization across weeks of a year, while (3) Skyscraper Design and (4) Urban Planning evaluate spatial reasoning through detailed facility arrangements across floors or city blocks. The benchmark includes 400 test instances, with 100 instances per scenario. Each scenario involves three instruction types (defined in Section 3): single instructions, range instructions, and periodic instructions. For temporal tasks, Diary Writing and Menu Design require at least 200 words per weekly entry, totaling 10,400 words (52 weeks \u00d7 200 words). For spatial tasks, Skyscraper Design and Urban Planning mandate 15,000 words (100 units \u00d7 150 words).\nEvaluation Metrics We evaluate model performance using three key metrics from LongGenBench. Main Task Completion Rate (Comp. Rate) assesses whether all designated subtasks are completed in sequence (e.g., generating entries for every week in a diary without omissions). Instruction Following Accuracy measures adherence to single (Acc. Once), range (Acc. Range), and periodic (Acc. Periodic) instructions, with their average reported as Avg. Acc. We utilize the official evaluation scripts to ensure consistency with reported benchmarks. Additionally, we track Word Count, ensuring a minimum average threshold of 12,700 words to meet the combined task requirements.\nModels We first present the performance of several single-pass generation baseline models from the official LongGenBench repository, including LongWriter-Llama3.1-8B (Bai et al., 2024), Llama-"}, {"title": "5.2 Main Results", "content": "Table 1 highlights the main performance outcomes of our experiments. Firstly, our results reveal that LongWriter-Llama3.1-8B, despite being specifically designed and trained from Llama-3.1-8B-Instruct for long-form generation, struggles con-"}, {"title": "6 Discussion", "content": "Ablation Study We conduct an ablation study to evaluate the impact of different components in our proposed CogWriter framework, as shown in Table 2. Removing the PlanRevise module resulted in a noticeable performance drop across key metrics, with the average accuracy decreasing from\n0.55 to 0.50. This demonstrates that refining the initial plan through iterative revisions is crucial for maintaining effective task decomposition and alignment with task-specific constraints. Disabling the PlanAdjust mechanism further impacted performance, reducing the average accuracy to 0.45, particularly affecting Acc. Once and Acc. Range. Finally, removing the LengthReview module led to a drop in content generation quality due to unmet length constraints, highlighting its role in fine-tuning the output to meet requirements. Overall, the results emphasize the importance of each component, with PlanRevise and PlanAdjust playing key roles in ensuring task decomposition, plan refinement, and overall accuracy of generation.\nLength Control Performance As specified in Section 3, each description $D_i$ must achieve a target word count of $L$. To evaluate compliance with this requirement, we conducted an analysis of word count distributions across different models. Taking the Diary Writing task as an example, Figure 3 illustrates the performance of LLama-3.3-70B-Instruct and Qwen-2.5-14B-Instruct. The box plot reveals that these base models struggle to meet the word count requirement, with high variance and frequent deviations from the target length. In contrast, CogWriter achieves superior length control, as shown by its tighter, more stable distribution of word counts. The explicit monitoring mechanism within CogWriter effectively reduces variance and ensures consistent compliance with the length requirement. We provide further analysis results of other models and tasks in Appendix A.1.\nChallenges in Handling Complex Instructions As shown in Figure 4, our experiments reveal that for all baselines and our model, the average performance follows a consistent ranking: Single Instructions (SI) outperform Range Instructions (RI), while Periodic Instructions (PI) show the lowest success rate. This indicates that, despite task decomposition simplifying the overall process, LLMs still face difficulties in understanding and execut-"}, {"title": "7 Related Work", "content": "Long-form Text Generation Recent advances in long-form generation have focused on improving models through architectural enhancements and specialized training techniques (Salemi et al., 2025a; Que et al., 2024; Liu et al., 2023; Li et al., 2023). Approaches like Re3 (Yang et al., 2022) use recursive reprompting for extended story generation, while DOC (Yang et al., 2023) and hierarchical outlining (Wang et al., 2024c) improve narrative coherence through structured task decomposition. Personalized long-form generation has also gained attention (Salemi et al., 2025a; Wang et al., 2024a), with methods like LongLaMP (Kumar et al., 2024) and reasoning-enhanced techniques (Salemi et al., 2025b) adapting models to meet user-specific needs. Similarly, long-form question answering focuses on producing detailed responses to complex queries (Dasigi et al., 2021; Stelmakh et al., 2022; Lee et al., 2023; Tan et al., 2024). While these methods have improved generation capabilities (Wu et al., 2024a; Que et al., 2024), our work addresses a critical gap by examining long-form generation through the lens of cognitive writing theory."}, {"title": "8 Conclusion and Future Work", "content": "In this paper, we analyzed the challenges of constrained long-form text generation from a cognitive writing perspective. Building on these insights and empirical observations, we proposed CogWriter, a novel writing framework that transforms LLM constrained long-form text generation into a systematic cognitive paradigm. CogWriter bridges the gap between human writing cognition and LLM capabilities, leading to substantial and consistent improvements in both instruction completion and generation length across different LLMs, as demonstrated through extensive experiments on LongGenBench. Looking forward, we plan to optimize agent communication cost and develop specialized models that better align with the unique requirements of each cognitive stage in the writing process."}, {"title": "Limitations", "content": "While demonstrating superior performance, Cog-Writer exhibits two primary limitations. First, while our approach achieves higher quality output, it necessitates more computational resources. As detailed in Appendix A.2, this additional cost stems from multiple rounds of planning, generation, and reviewing. Second, our current implementation utilizes a single LLM across all cognitive writing stages (planning, generation, and reviewing). This uniform approach may not fully leverage the model's capabilities, as each stage only activates specific aspects of the model's knowledge and abilities. Future research directions include exploring specialized models for different cognitive stages and investigating Mixture-of-Experts architectures to enhance both domain expertise and parameter efficiency in the cognitive writing process."}, {"title": "Ethical Considerations", "content": "Like other LLMs, our CogWriter framework may inherit biases from training data. It may generate inaccurate content despite its enhanced control mechanisms, emphasizing the need for human oversight in practical applications. While the multi-step cognitive process increases computational costs, the structured planning approach improves efficiency and could be further optimized for sustainability. As with any advanced text generation system, CogWriter could potentially be misused for generating deceptive content, highlighting the importance of responsible deployment and appropriate safeguards in real-world applications."}, {"title": "A Appendix", "content": "A.1 Further Length Control Performance\nTo comprehensively demonstrate CogWriter's length control capabilities across different scenarios, we present the generated length distribution of LLama-3.3-70B-Instruct, Qwen-2.5-14B-Instruct,\nGPT-40, and GPT-40-mini in Figures 5a-5d. We evaluate two distinct task types: spatial tasks (150"}, {"title": "A.2 Inference Time and Token Consumption Analysis", "content": "To evaluate and analyze the computational efficiency of CogWriter, we conducted comprehensive experiments examining inference time and token consumption amount.\nInference Time For ensure reliable evaluation, we used LLaMA-3.3-70B as our test model, as\nQwen exhibited incomplete text generation issues\nand GPT's API calls were subject to network latency variations. All experiments were performed on 4 NVIDIA A100 GPUs, with each condition"}]}