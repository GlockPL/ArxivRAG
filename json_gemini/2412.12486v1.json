{"title": "Boosting Long-Context Information Seeking via Query-Guided Activation Refilling", "authors": ["Hongjin Qian", "Zheng Liu", "Peitian Zhang", "Zhicheng Dou", "Defu Lian"], "abstract": "Processing long contexts poses a significant challenge for large language models (LLMs) due to their inherent context-window limitations and the computational burden of extensive key-value (KV) activations, which severely impact efficiency. For information-seeking tasks, full context perception is often unnecessary, as a query's information needs can dynamically range from localized details to a global perspective, depending on its complexity. However, existing methods struggle to adapt effectively to these dynamic information needs.\nIn the paper, we propose a method for processing long-context information-seeking tasks via query-guided ACtivation REfilling (ACRE). ACRE constructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache compactly captures global information, and the layer-2 (L2) cache provides detailed and localized information. ACRE establishes a proxying relationship between the two caches, allowing the input query to attend to the L1 cache and dynamically refill it with relevant entries from the L2 cache. This mechanism integrates global understanding with query-specific local details, thus improving answer decoding. Experiments on a variety of long-context information-seeking datasets demonstrate ACRE's effectiveness, achieving improvements in both performance and efficiency. We will release our source codes in this repository.", "sections": [{"title": "Introduction", "content": "Recently, large language models (LLMs) have become widely used for daily information-seeking tasks, such as ChatGPT (OpenAI, 2023). However, their capabilities are inherently limited by the difficulty of updating parametric knowledge. To address this, incorporating external knowledge as a context has become a common approach (Zhao et al., 2024). In practice, this external knowledge often involves long contexts, such as long documents or novels, which pose significant challenges due to the large KV activations accumulated during inference, demanding substantial computational resources and reducing efficiency (Xu et al., 2023; Bai et al., 2024b; Zhang et al., 2024c).\nTo address the challenges posed by excessive KV activations, previous works have proposed various strategies: reducing the precision of activation tensors (Liu et al., 2024; Xu et al., 2024), dividing long contexts into smaller chunks for independent processing (Lee et al., 2024; Yoon et al., 2024), or compressing KV activations into shorter representations through selection or sparse attention (Zhang et al., 2023; Li et al., 2024; Xiao et al., 2024; Jiang et al., 2024). Retrieval-Augmented Generation (RAG) has also emerged as a promising approach, retrieving precise evidence from long contexts to support answer generation (Gao et al., 2024).\nHowever, most existing methods follow a unilateral strategy: either compromising the semantic richness of KV activations to create compact global representations, such as with quantized activations (Liu et al., 2024), or concentrating solely on detailed local information, such as RAG methods (Gao et al., 2024). Moreover, most lightweight KV methods remain constrained by the native context length limit, leading to significant performance degradation when processing contexts that exceed this limit (Zhang et al., 2024b).\nIn information-seeking tasks, we argue that the information needs of a user query can dynamically range from localized details to a global perspective, depending on the query's complexity. For instance, given a novel, the query \"What are the main characters' names?\" involves localized information needs and can be answered using specific local evidence. In contrast, the query \"How do the main characters drive the story's development?\" requires a global understanding of the entire book.\nTo address dynamic information needs in information-seeking tasks, we propose ACRE, a method that employs a bilateral strategy to capture a global perspective across the full context and enhance local details using query-guided activation refilling. Figure 1 presents an overview of ACRE's framework along with a comparison against efficient long LLMs and RAG methods.\nSpecifically, ACRE constructs a bi-layer KV activation cache for long contexts, comprising an L1 cache and an L2 cache. The L1 cache captures compact yet global information from the full context, while the L2 cache retains localized, detailed information. Notably, the L1 cache is significantly smaller than the L2 cache. During the forward pass of the LLM, the L1 and L2 caches are interleaved into a nested structure, with each L1 tensor optimized to proxy the semantics of its corresponding L2 cache. To enhance efficiency, we replace the original full attention mechanism\u2014where each token attends to all preceding tokens\u2014with a tailored selective attention mechanism. In this approach, tokens perform full attention on recent L1 and L2 tokens but only attend to distant L1 tokens. This selective attention mechanism significantly reduces computational costs, enabling ACRE to process long contexts more efficiently.\nAfter the forward pass, the nested KV cache is decomposed back into separate L1 and L2 caches. For an input query, ACRE first uses the query to attend to the compact L1 cache. Based on the resulting attention score distribution, ACRE selectively refills key entries of the L1 cache with the corresponding L2 cache entries, thereby enriching local details. This process is referred to as query-guided activation refilling."}, {"title": "Method", "content": "The process of solving information-seeking tasks using LLMs can be succinctly described as $y = M(X)$, where $M(\u00b7)$ denotes the LLM, $y$ represents the output answer and $X$ represents the input sequence. $X$ can take various forms, ranging from a standalone query to a complex instruction prompt. In this paper, we focus on information-seeking tasks with long contexts. Therefore, we define the input sequence $X$ as comprising a query $q$ and a long context $C$, denoted by $X = (C, q)$.\nFor the input $X$, a Transformer-based LLM computes multi-head attention (MHA) as follows:\n$Q = X \\cdot W_Q$,\n$K = XW_K$,\n$V = X W_V$,\n$A(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d}})V$,\nwhere $X$ represents the hidden states of the input sequence $X$, and $W_Q$, $W_K$, and $W_V$ are the projection weight matrices for the query $Q$, key $K$, and value $V$, respectively (Vaswani et al., 2023). The attention function $A(\u00b7)$ is applied iteratively across multiple layers and attention heads. For simplicity, we omit the layer and head indices.\nThe inference process of LLMs can be divided into two stages: (1) prefilling and (2) decoding (Liu et al., 2024). During the prefilling stage, the input sequence $X$ is processed through each layer using MHA, and the layer-wise key-value activations $[K, V]$ are cached. These cached activations are reused in the decoding stage to avoid redundant computations, enabling efficient processing. However, as MHA computation has quadratic complexity with respect to the sequence length n, handling long contexts becomes computationally expensive. This often results in slow processing speeds and out-of-memory issues, particularly when dealing with long input contexts (Dong et al., 2023).\nTo address the challenges posed by oversized KV caches for long contexts, we propose ACRE, a framework that constructs a Bi-layer KV Cache and employs a Query-Guided Refilling mechanism to enable a flexible KV cache that captures both global context and query-specific local details, ensuring efficient and high-quality answer decoding."}, {"title": "Overview of ACRE", "content": "Figure 2 provides an overview of ACRE. Specifically, for a information-seeking task with a long context $C$, ACRE organizes the long context into a bi-layer KV activation cache during the pre-filling stage, as shown in Figure 2 (a).\nThe construction of the Bi-layer KV Cache begins by interleaving newly introduced L1 tokens into the input context. Through model forwarding, a nested KV cache $[K, V]$ is obtained. This nested KV cache is then decomposed into a Bi-layer KV cache: the layer-1 (L1) cache, which is compact and stores global information from the full long context, and the layer-2 (L2) cache, which holds detailed and localized information. Each tensor in the L1 cache serves as a semantic proxy for a corresponding sequence of tensors in the L2 cache.\nWe denote the L1 KV cache as $[K^{L1}, V^{L1}] \\in \\mathbb{R}^{m \\times d}$ and the L2 KV cache as $[K^{L2}, V^{L2}] \\in \\mathbb{R}^{n \\times d}$. Here, the length of the L1 KV cache, $m$, is significantly smaller than $n$, the length of the L2 KV cache. To optimize memory usage, the L2 cache can be offloaded to CPU memory, while the L1 cache is retained in GPU memory as a constant cache after constructing the bi-layer KV cache. This design significantly improves memory efficiency in practical applications.\nThe Bi-layer KV Cache is constructed exclusively for input contexts, enabling it to be reused across different information-seeking tasks that share the same context. Given an input query $q$, ACRE utilizes $q$ to attend to the L1 cache, computing attention scores. Based on these scores, ACRE selectively refills the L1 cache by retrieving the most informative entries from the L2 cache, which are proxied by the corresponding most attentive L1 cache tensors. This process recovers a partial nested cache to support answer decoding and is referred to as query-guided activation refilling, which is shown in Figure 2 (b).\nBy leveraging both the L1 KV cache and the query-specific L2 KV cache, the final KV cache captures global information from the full long context while preserving local details. This design significantly enhances the performance of long-context information-seeking tasks. In the following sections, we provide the technical details of ACRE."}, {"title": "Bi-Layer KV Cache", "content": "To construct the bi-layer KV cache, we introduce a new type of token, called L1 tokens, denoted as $X^{L1} = (x_1^{l1}, \\dots, x_m^{l1})$. The original tokens of the input sequence are referred to as L2 tokens, denoted as $X^{L2} = (x_1, \\dots, x_n)$. By interleaving the L1 and L2 tokens, the input sequence $X$ is transformed into a nested sequence $\\tilde{X}$:\n$\\tilde{X} = (x_1, \\dots, x_l, x_1^{l1}, x_{l+1}, \\dots, x_{2l}, x_2^{l1}, \\dots, x_n, x_m^{l1})$,"}, {"title": "Query-Guided Activation Refilling", "content": "After constructing the bi-layer KV cache for the context, we obtain the L1 KV cache $[K^{L1}, V^{L1}]$, which serves as a global yet compact representation of the full long context, and the L2 KV cache $[K^{L2}, V^{L2}]$, which provides detailed but memory-intensive representations. To optimize memory usage, the L1 KV cache is retained as a constant cache in GPU memory, while the L2 KV cache is offloaded to CPU memory.\nFor an input query $q$, relying solely on the L1 KV cache is feasible but lacks query-specific detailed information. To address this limitation, ACRE proposes refilling the compact L1 KV cache with selected entries from the L2 KV cache that are most relevant for answering the query. Specifically, the query state $Q_q$ for the input query $q$ is computed as $Q_q = qW_Q$. Using this query state, the attention distribution is calculated as:\n$A = softmax(\\frac{Q_q K^{L1^T}}{\\sqrt{d}})$,"}, {"title": "Model Optimization", "content": "ACRE is characterized by its Bi-layer KV Cache structure and Query-Guided Activation Refilling mechanism. Its effectiveness relies on two key abilities: (1) the L1 KV activations must faithfully represent the L2 KV activations, and (2) given an input query q, the most relevant L2 KV activations must be efficiently retrieved. To optimize these abilities, we employ a two-stage optimization strategy.\nIn stage 1, the objective is to maximize the semantic volume of the L1 KV activations to effectively represent the corresponding L2 KV activations. This is achieved by predicting the next token using the previously accumulated L1 tokens and the recent L2 tokens. The optimization can be expressed through a cross-entropy loss:\n$L_{stage-1} = - \\sum_{t=1}^T log P(x_t | \\tilde{X}_{[1:t-1]}^{L1}, x_{[j:t-1]}^{L2})$,\nIn stage 2, the objective is to enable ACRE to retrieve the most relevant L2 KV activations for refilling the L1 KV cache based on an input query q. Since the L2 KV cache is proxied by the L1 KV cache, accurately attending to the most useful L1 KV activations allows retrieval of the corresponding L2 KV activations via the proxying relationship. To achieve this, we optimize ACRE using task-specific data comprising long contexts and input queries. The optimization employs the following loss function:\n$L_{stage-2} = - \\sum_{t=1}^T log P(y_t | X^{L2}, q)$,\nwhere $y$ represents the ground-truth answer, and $q$ is the input query. This loss ensures that ACRE learns to produce accurate answers solely based on the L1 KV cache while maintaining its ability to retrieve the most relevant KV activations."}, {"title": "Experiments", "content": "We evaluate ACRE and all baseline models across 12 information-seeking tasks from three public long-context benchmarks: LongBench (Bai et al., 2024b), InfiniteBench (Zhang et al., 2024c), and UltraDomain (Qian et al., 2024b). These 12 datasets are categorized as follows: (1) Complex QA (Qian et al., 2024b): Financial, Legal, Physics, Biology, Math, and CS. These tasks involve practical, high-level queries with extra-long contexts spanning specialized domains. Many queries demand a global and in-depth understanding of the full context, making them especially challenging. (2) Single-Document QA: NarrativeQA (Kocisk\u00fd et al., 2018), Qasper (Dasigi et al., 2021), MultiFieldQA (Bai et al., 2024b), and En.QA (Zhang et al., 2024c). (3) Multi-Document QA: 2WikiMQA (Ho et al., 2020), and MuSiQue (Trivedi et al., 2022)."}, {"title": "Baseline Models", "content": "We compare ACRE with the following baselines:\nOriginal: Directly fits the maximum context length of the underlying LLMs. KIVI (Liu et al., 2024): Quantizes KV activations into 4-bit precision. Beacon (Zhang et al., 2024a): Compresses the full KV activations into beacon activations. SelfExtend (Jin et al., 2024): Applies hierarchical positional encoding to extend the model's context window. MInference (Jiang et al., 2024): Dynamically applies different sparse attention mechanisms across all attention heads. StreamingLLM (Xiao et al., 2024): Attends only to recent tokens and sink tokens. RAG: Uses standard RAG pipelines to retrieve relevant evidence from the full context. RQRAG (Chan et al., 2024): Rewrites the input query into sub-queries and retrieves evidence for each sub-query. MemoRAG (Qian et al., 2024b): Applies a memory model to form a compact global memory over the full context, providing answer clues that assist the retrieval process for better evidence retrieval.\nIn the main experiments (Section 3.3), we use Qwen2.5-3B-Instruct as the underlying model. To analyze the impact of using different underlying models, we also experiment with Llama3.2-3B-Instruct and Qwen2.5-7B-Instruct in Section 3.4. All three LLMs have a native context window of 128K (Yang et al., 2024; MetaAI, 2024). The implementation details of ACRE and all baselines are in Appendix A."}, {"title": "Main Results", "content": "In Table 3.3, we present the results of the main experiments, demonstrating that ACRE outperforms all baselines across most datasets. These results highlight the effectiveness of ACRE's design. Specifically, we derive the following findings: (1) ACRE consistently outperforms the baseline approach of feeding the full context directly into LLMs. This improvement stems not only from ACRE's ability to process contexts exceeding the native LLM's context window but also from its precise focus on query-relevant local information, effectively filtering out irrelevant details through query-guided activation refilling. (2) Baselines in the second block generally perform worse than directly feeding the full context into LLMs. This is attributed to semantic loss caused by compressing full KV activations. In contrast, ACRE leverages its bi-layer KV cache and query-guided activation refilling to recover local detailed semantics from the L2 cache that are absent in the L1 cache, resulting in superior performance. (3) Baselines in the third block use retrieval tools to extract precise evidence from long contexts. While effective for queries with clear information needs, these methods struggle with complex queries that require a higher-level understanding of the full context. ACRE overcomes this limitation by utilizing the global information in the L1 cache and dynamically refilling it with query-relevant local details from the L2 cache, thereby adapting to the varying information needs of different queries."}, {"title": "Ablation Study", "content": "To thoroughly validate the effectiveness of our method design, we perform detailed ablation studies as follows:\n(1) Method Design and Model Selection: Figure 3 presents ablation results across different LLMs and variations in model design. First, we evaluate the role of training stages in model performance. Without the two-stage training process, ACRE reverts to a vanilla LLM, which performs significantly worse than ACRE. Stage-1 training enables ACRE to construct the bi-layer KV activation cache, thereby improving its long-context processing capabilities. When both stages are applied, ACRE achieves the best performance, demonstrating the effectiveness of its optimization design.\nSecond, to determine if ACRE's effectiveness stems from its training data, we fine-tune a vanilla model using ACRE's training data via SFT, producing SFT Vanilla. While SFT improves the vanilla model by enhancing its QA capabilities, it still underperforms compared to ACRE. This highlights the unique advantages of ACRE'sdesign.\nLastly, we replace ACRE's underlying LLM with Qwen2.5-7B (a scaled-up version of the same model) and Llama3.2-3B (a model of similar scale but different architecture). As shown in Figure 3, ACRE's design consistently proves effective across models of varying scales and architectures, confirming its generalizability.\n(2) Impact of Parameter Choice: As described in Section 2, ACRE's performance may be influenced by two hyperparameters: the maximum refilling length of KV activations \u03b7 and the L1/L2 interval l. To investigate their impact, we conduct experiments with different values of n and l. Figure 4 presents the results of this analysis.\nSpecifically, in the left figure, we observe that the impact of the refilled activation length varies by task. For tasks with queries requiring explicit information (e.g., nar and en. qa), answer decoding relies on precise local information. Here, ACRE's performance peaks at a reasonable refilled length but declines as excessive refilling introduces noise, which biases the decoding process. Conversely, for tasks with queries requiring the integration of global information, ACRE's performance consistently improves with longer refilled lengths. This is because the L1 cache already provides global information, and additional refilled activations enhance local context.\nThe right figure shows the impact of the L1/L2 interval. We find that ACRE's performance generally decreases as the L1/L2 interval increases. Larger intervals require L1 tokens to summarize more semantics from subsequent L2 tokens, potentially overloading the L1 cache. However, larger intervals result in a compact L1 KV cache, offering efficiency. In practical applications, users can adjust parameters to balance efficiency and effectiveness based on available resources.\nIn summary, ACRE outperforms directly using vanilla LLMs in most parameter settings, requiring significantly fewer computational resources while achieving higher efficiency."}, {"title": "Efficiency Analysis", "content": "To evaluate ACRE's efficiency compared to baselines in processing long contexts at different scales, we conduct comparative experiments using the vanilla LLM, the efficient attention method MInference, and ACRE.\nThe results, presented in Table 2, lead to the following conclusions: (1) ACRE consistently processes long contexts at different scale with comparable or lower GPU resource usage. This efficiency is attributed to the bi-layer KV activation design, which avoids directly processing the full KV activations. (2) ACRE's efficiency advantage becomes more pronounced with extremely long contexts (e.g., over 512K), where the vanilla LLM runs out of memory, and MInference faces a high risk of out of memory while require longer latency than ACRE. (3) Thanks to its query-guided activation refilling mechanism, ACRE utilizes only the compact L1 KV activations and query-relevant L2 KV activations for answer decoding. This enables ACRE to process contexts longer than the native window of the LLM while maintaining answer quality. In contrast, baseline models generate nonsensical answers when exceeding LLM's native context length.\nIn summary, ACRE demonstrates significant advantages in handling long contexts efficiently and reliably compared to baseline methods."}, {"title": "Related Work", "content": "Long-context processing is a critical capability of LLMs (Zhao et al., 2024). The most fundamental approach to enhancing this ability is training LLMs on long texts, either sampled from raw corpora or synthesized (Xiong et al., 2024; Mohtashami and Jaggi, 2024; Fu et al., 2024; Bai et al., 2024a). Consequently, the native context window of popular LLMs has increased significantly, from the earlier 4K to the current 128K (Peng et al., 2023; Touvron et al., 2023; Yang et al., 2024).\nIn addition to directly increasing the context window, some methods employ strategic positional encoding to enable LLMs to process contexts longer than their native window, as demonstrated by (Chen et al., 2023b; Song et al., 2023; Liu et al., 2023; Jin et al., 2024). However, when processing long contexts, LLMs generate large key-value (KV) activations, which consume substantial resources and reduce efficiency. To address this, many works aim to make KV activations more compact and lightweight (Liu et al., 2024; Xu et al., 2024). For example, KIVI focuses on reducing the precision of KV activations to 2-bit, resulting in significantly lighter KV representations (Liu et al., 2024). Other methods selectively attend to a small portion of KV activations through compression or sparse attention mechanisms. For instance, StreamingLLM proposes attending only to recent tokens and sink tokens to maintain compact KV activations (Xiao et al., 2024), similar idea also adopted by (Li et al., 2024; Zhang et al., 2023; Jiang et al., 2024; Zhang et al., 2024a). Beyond optimizing KV activations, alternative methods such as agent-based approaches (Qian et al., 2024a; Lee et al., 2024) and retrieval-augmented generation (Xu et al., 2023; Zhu et al., 2024) have been applied to facilitate long-context processing. These methods split the long context into chunks and retrieve evidence using retrievers or agents. They work well for explicit queries but struggle with implicit ones requiring full-context aggregation.\nMost existing methods either compact global KV activations into a lightweight form or prune them into shorter forms, often failing to balance global perspective with local informativeness. This limitation can compromise performance in information-seeking scenarios, where information needs may dynamically range from global to local."}, {"title": "Conclusion", "content": "In this paper, we propose a method, ACRE, designed to adapt to the dynamic information needs of long-context information-seeking tasks. ACRE constructs a bi-layer KV activation cache structure for long contexts, where the L1 KV cache stores compact, global information, and the L2 KV cache captures detailed, local information. Using query-guided activation refilling, ACRE identifies query-specific evidence from the L2 KV cache and refills this local information into the L1 KV cache, resulting in nested KV activations that effectively combine a global perspective with local details. Through experiments on a wide range of information-seeking datasets, we demonstrate the effectiveness of ACRE in simultaneously improving the performance and efficiency of long-context processing for information-seeking tasks."}, {"title": "Limitation", "content": "In this paper, we propose ACRE, a method designed to adapt to the dynamic information needs of long-context information-seeking tasks. ACRE constructs a bi-layer KV activation cache to balance global context perception and local detail preservation, leveraging query-guided activation refilling to enhance performance and efficiency. While ACRE demonstrates significant advancements, several limitations are worth noting:\n(1) Our method is primarily designed for information-seeking tasks, a major subset of long-context processing. This focus is largely driven by the availability of training data, as information-seeking tasks benefit from abundant QA datasets. While ACRE has the potential to adapt to general long-context tasks, further exploration with diverse task-specific data would be necessary to validate its broader applicability.\n(2) ACRE introduces additional parameters for constructing the bi-layer KV cache, increasing the model size. For example, using Qwen2.5-3B-Instruct, ACRE adds approximately 17.2% more parameters, requiring additional GPU memory to load the model. However, in long-context tasks, the majority of GPU memory is consumed by KV activations rather than model parameters. Our efficiency analysis confirms that ACRE reduces overall GPU memory consumption when processing long contexts, mitigating this limitation to some extent.\n(3) A portion of our training data is synthetically generated by commercial LLMs (e.g. GPT-4), which may introduce biases inherited from the original corpus or the LLMs used. While such biases could impact performance, many current commercial LLMs incorporate robust safeguards that help mitigate these issues. Nonetheless, addressing potential biases in synthetic data remains an area for future improvement."}, {"title": "Implementation details", "content": "For ACRE training, in stage 1, we sample long text spans from the RedPajama (Soboleva et al., 2023) dataset to create a training set of 2 billion tokens. The sampled text lengths are limited to a minimum of 4K and a maximum of 64K tokens. We randomly choose L1/L2 interval from $l \\in \\{8, 16, 32, 64, 128\\}$. The model is trained for one epoch with a batch size of 8 and a learning rate of $5 \\times 10^{-5}$. In stage 2, we collect 28,400 QA SFT data points from LongAlpaca (Chen et al., 2024) and synthetic data from (Zhang et al., 2024a; Qian et al., 2024b). We apply the same L1 token insertion strategy during training. The model is trained for three epochs with a batch size of 8 and a learning rate of $1 \\times 10^{-5}$ for two epochs. Stage-1 training takes around 7 hours while stage-2 training takes around 13 hours.\nDuring the two-stage training process, we optimize only the newly initialized parameters, keeping the original parameters frozen. The number of trainable parameters varies depending on the model. For instance: (1) When using Qwen2.5-3B-instruct, ACRE has around 503M trainable parameters, accounting for 17.2% of the original parameters. (2) When using Llama3.2-3B-instruct, ACRE has around 780M trainable parameters, accounting for 25.6% of the original parameters. This difference arises from variations in the implementation of multi-head attention."}, {"title": "Prompt for Bi-Layer KV Cache Construction", "content": "You are provided with a long article. Read the article carefully.\nAfter reading, you will be asked to perform specific tasks based on the content of the article.\nNow, the article begins:\n**Article Content:** [context]\nThe article ends here.\nNext, follow the instructions provided to complete the tasks.\nFor the main experiments, we configure ACRE with an L1/L2 interval l of 16, a maximum refilling length \u03b7 of 4,096, and the maximum working context window W of 32K tokens. For the Bi-Layer KV Cache construction, we utilize the following prompt. During the Query-Guided Activation Refilling process, we adopt task-specific prompts from the official benchmark repositories, without inserting the context into the task prompt.\nFor RAG, RQ-RAG, and MemoRAG, we employ BGE-M3 (Chen et al., 2023a) as the retriever and set the hit number to 5. For methods that divide the long context into chunks, we use the semantic-text-splitter tool, chunking the context to a maximum length of 512 tokens.\nFor KIVI, we quantize the KV activations to 4-bit precision. For Beacon, we use the official training code to fine-tune Qwen2.5-3B-Instruct, setting the compression ratio to 8 during inference. For SelfExtend, we set the group size to 32 and the window size to 2048, which is approximate by the official recommended strategy. For StreamingLLM, we use the SinkCache implementation from Transformers, configuring the window size to 4096 and the number of sink tokens to 8. Lastly, for MemORAG, we utilize the officially released memorag-qwen2-7b-inst as the memory model.\nAll methods are evaluated using the task prompts provided in the official repositories of their corresponding benchmarks\u00b9. Additionally, we use the same generation hyper-parameters (task-dependent) for ACRE and all baseline models.\nAll training and evaluation experiments were conducted using 8 NVIDIA A800-80G GPUs."}]}