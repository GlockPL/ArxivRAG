{"title": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making", "authors": ["Dayuan Fu", "Biqing Qi", "Yihuai Gao", "Che Jiang", "Guanting Dong", "Bowen Zhou"], "abstract": "Long-term memory is significant for agents, in which insights play a crucial role. However, the emergence of irrelevant insight and the lack of general insight can greatly undermine the effectiveness of insight. To solve this problem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an embodied agent designed to improve LLMs' planning and decision-making ability by summarizing and utilizing insight effectively across different scales. MSI achieves this through the experience selector, insight generator, and insight selector. Leveraging a three-part pipeline, MSI can generate task-specific and high-level insight, store it in a database, and then use relevant insight from it to aid in decision-making. Our experiments show that MSI outperforms another insight strategy when planning by GPT3.5. Moreover, We delve into the strategies for selecting seed experience and insight, aiming to provide LLM with more useful and relevant insight for better decision-making. Our observations also indicate that MSI exhibits better robustness when facing domain-shifting scenarios.", "sections": [{"title": "1 Introduction", "content": "Creating agents that can make autonomous decisions in the environment has always been a promising and interesting research direction. (Significant-Gravitas, 2023; Sun et al., 2023) With the emergence of ChatGPT and GPT-4 (Achiam et al., 2023), large language models (LLMs) have transformed from specialized models to a general model that can complete multiple types of tasks, hence it can make decisions for agents. (Xi et al., 2023; Yang et al., 2024; Wang et al., 2023b). This type of agent will transform multi-modal information into natural language as short-term memory. It then prompts large language models with short-term memory and long-term memory to plan and make decisions. With these capabilities, the agent can generate a series of actions that are executable within a given environment. (Yao et al., 2023; Park et al., 2023; Gao et al., 2023; Zheng et al., 2023)\nInsight\u00b9, as a form of long-term memory, has gradually become a crucial part of guiding LLM planning and decision-making. (Shinn et al., 2023; Zhao et al., 2023; Fu et al., 2024; Wang et al., 2023a; Xi et al., 2023; Zeng et al., 2024). Relative to other long-term memory such as examples, insight is more concise and higher-level. Although previous work has proposed a method of using LLM to summarize and utilize insights (Zhao et al., 2023), it either provides LLM with too many irrelevant insights or can not summarize the high-level insights, as shown in Figure 1. The former can interfere with decision-making (Liu et al.,"}, {"title": "2 Related Work", "content": "2.1 Embodied AI\nEmbodied AI focuses on leveraging multi-model information for decision and execution of actions. Diverging from traditional reinforcement learning approaches (Schulman et al., 2017), current research endeavors employ language models as decision-makers for action decisions. Specifically, the model transforms information from non-natural language modalities into natural language through a modality transformer (Inoue and Ohashi, 2022; Sarch et al., 2023), using natural language information as input to guide the Large Language Model in decision-making (Song et al., 2023; Singh et al., 2023, 2022; Suglia et al., 2021; Fu et al., 2024). Some methods involve fine-tuning the language model to map language inputs to action sequences at different hierarchical levels (Zhang et al., 2022; Zheng et al., 2022; Koshti and Bhavsar, 2023), while others prompt a frozen LLM to predict action plans, relying on the instruction-following and context-learning properties of the LLM to simulate new tasks during testing (Wu et al., 2023; Sarch et al., 2023; Song et al., 2023; Singh et al., 2023, 2022; Dong et al., 2024a). By relying on action(s) generated by the model, the robot can accomplish the designated tasks in the environment."}, {"title": "2.2 LLM Long-term Memory", "content": "When making decisions, humans often recall past cases to assist in decision-making. Due to the limited input length, the LLM Agent cannot receive infinite historical experiences. Therefore, efficiently utilizing existing success/failure experiences becomes crucial. The LLM Long-term Memory is designed to address this challenging issue (Zhao et al., 2023; Wen et al., 2023; Majumder et al., 2023; Qian et al., 2024). Currently, the LLM Agent Memory operates in two modes: example memory and insight memory. Example memory involves manually crafting experience examples that were successful in tasks. During usage, similar examples are retrieved based on the current task, using methods such as vectors or BM25, to prompt the large language model (Wang et al., 2023a; Wen et al., 2023; Dong et al., 2024b; Song et al., 2023; Zhong et al., 2023). Insight memory, on the other hand, summarizes success/failure experiences into insights through the LLM. When new tasks occur, the insights are directly input as a part of the prompt into the LLM for helping planning and decision-making. (Majumder et al., 2023; Zhao et al., 2023)."}, {"title": "3 Method", "content": "Figures 2 and 3 illustrate our approach. Initially, utilizing historical task data (train set), we employ the task execution module to collect a sufficient number of experiences. (\u00a73.1) These experiences are then subjected to the experience selector, which identifies experiences/experience pairs suitable for generating insights. (\u00a73.2) Subsequently, the multi-scale insights will be generated and stored in the insight database. (\u00a73.3) When a new task arises, we retrieve relevant sights from the database based on predefined rules. (\u00a73.4) These insights, along with task background, and user queries, are provided to the task execution module to facilitate execution. We refer to the process from experience collection to insight generation as insight summarization, and the subsequent insight selection and task execution as insight utilization."}, {"title": "3.1 Experience Generation", "content": "As shown in Figure 2, we regard training data as history tasks. For each history task, the executor leverages LLM to generate a plan based on task background and user queries. Subsequently, the robot employs first-order logic to decompose the plan into atomic actions (e.g., moving forward, picking up objects) and execute them in an environment. In some tasks or cases, the executor may replan based on the environment feedback. Upon completion, task background, user queries, agent's plans, environmental feedback, and execution results are stored as experiences for summarization. Detailed information can be found in Appendix A."}, {"title": "3.2 Experience Selection", "content": "The selection of experiences is crucial in summarizing insights, as it determines the quality of insights the model consolidates. As shown in Figure 3, our Experience Selection employs two modes:\nSuccess mode: We select experiences with successful execution results as the success mode experiences."}, {"title": "Pair mode", "content": "For each successful experience $s_s$, we identify a corresponding experience $s_f$ from the unsuccessful experience database $S_f$ by:\n$s_f = \\underset{s \\in S_f}{argmax} \\frac{emb(s) \\cdot emb(s_s)}{||emb(s)||_2 ||emb(s_s)||_2}$ \t(1)\nWhere $emb$ is the embedding of the experience's user query and the $(s_s, s_f)$ is the final experience pair in the pair mode.\nThese two types of selected experience (pair) collections are subsequently preserved and utilized as seed experience for insight summarization."}, {"title": "3.3 Multi-Scale Insight Generation", "content": "Multi-Scale Insight We categorize the insights into several scales. For all tasks, we will generate general scale and subtask scale insights. If the task provides a specific environment category (for example, kitchen), we will also generate environment scale insights. General insight refers to the knowledge required for all tasks, which should be high-level. Environment insight pertains to the knowledge needed in a specific environment, and subtask insight involves the understanding of executing particular subtasks. The overall pipeline can be seen in Figure 3's Insight Generation module.\nInsight Generation We initialize the insight database to be empty. Whenever a seed experience merges, we select all insights in the order of general, subtask.2 as a pool of candidate experience for updating.\nSubsequently, we prompt the LLM with templates containing the candidate insight, all experience information, and descriptions of 5 atomic actions: adding, removing, editing, agreeing on an insight, and moving an insight between scales, requesting the LLM to update the insight database through these atomic actions (Zhao et al., 2023). For subtask insight, we also require the LLM to additionally generate a subtask name corresponding to the insights. 3\nAfter the LLM generation is complete, we update the insight database in the order of general, environment (if have), and subtask, according to the atomic actions.\nAlign with Expel, we also employ a scoring mechanism in insight generation. Specifically, each"}, {"title": "3.4 Multi-Scale Insight Selection", "content": "Similar to the generation process, we use general and subtask insights2 as candidate insights. For subtask insights, we adopt two modes for further selection:\nHashmap indexing: We extract all subtask names from the subtask insight database, combine them with user queries, and provide them to the LLM, requiring the LLM to return all task names related to the user query. Subsequently, we consider all insights under returned subtask names as the subtask insights for this user query. The prompt of hashmap subtask selection can be seen in Appendix D\nVector indexing: We compute the cosine similarity between all subtask insights and the user query, selecting insights with at most 2000 tokens.4\nUltimately, we provide the different scales of insights, and the user query to the task execution module to accomplish the task."}, {"title": "4 Experiment", "content": "We evaluate MSI on the 2 benchmarks5: TEACH TfD benchmark (Padmakumar et al., 2022) and AgentBench Alfworld benchmark (Shridhar et al., 2020; Liu et al., 2023b). Our experiments are designed to address the following research questions (RQs): RQ1: Does MSI outperform other insights methods? RQ2: What kind of seed experience selection strategy should be chosen when facing different insight generation strategies and tasks? RQ3: What kind of insight selection strategy should be adopted for different future tasks? RQ4: How does the robustness of the MSI system evolve with the domain shifts?"}, {"title": "4.1 Experimental Setup", "content": "Evaluation metrics For TEACH, we calculate accuracy (ACC) and path length weighted (PLW) metrics under two settings: Task Success Rate (SR) and Goal Condition Success Rate (GC)."}, {"title": "Aligned with HELPER, these four metrics are", "content": "$SR_{ACC} = E_{x\\sim p} (1(SCN_x = GCN_x))$\t\t(2)\n$GC_{ACC} = \\frac{\\sum_{x\\sim p} SCN_x}{\\sum_{x\\sim p} GCN_x}$\t\t(3)\n$SR_{PLW}= \\frac{\\sum_{x\\sim p} \\frac{1(SCN_x = GCN_x)*L_{refx}}{Exp Max(L_{predx}, Left)}}{\\frac{\\sum_{x\\sim p} L_{refx}}{Exp L_{refx}}}$\t(4)\n$GC_{PLW} = \\frac{\\sum_{x\\sim p} \\frac{(SCN_x/GCN_x)*L_{refx}}{Exp Max(L_{predx}, refr}}{\\frac{\\sum_{x\\sim p} L_{refx}}{Exp L_{refx}}}$\t\t(5)\n$SCN$ and $GCN$ refer to the success condition number and goal condition number respectively, $L_{pred}$ refers to the step used to execute the task by the executor while $L_{ref}$ refers to the step used to execute the task by a human annotator, p refers to the distribution of the datasets and x is the sample of the distribution of the datasets.\nFor Alfworld, we calculate the $SR_{ACC}$ metric."}, {"title": "4.2 Executor", "content": "TEACH We use HELPER (Sarch et al., 2023) as the TEACh's executor. HELPER (Sarch et al., 2023) is an executor framework built on top of TEACh. As shown in Figure 2, it provides the task background, user query (i.e., the dialogue), and other relevant information to the LLM in a fixed format, allowing the LLM to generate a piece of code as the plan(Chen et al., 2021) and create a sequence of subtasks to guide the robot. Initially, the robot will walk around the environment to observe and obtain a spatial plan map that includes information about the objects it has observed, as well as its location (Blukis et al., 2022). At each time step, the robot receives an RGB image through its camera. It will then determine an atomic action based on the image, location, and subtask, and execute it in the simulation environment. (Sarch et al., 2023; Zhang et al., 2022) If the execution fails, the robot will call upon the VLM model (Li et al., 2023) to provide the most likely reason for the failure based on the image and attempt a second try or replan (Yao et al., 2022; Shinn et al., 2023). In the MSI, we include the environment, dialogue, planned subtasks, actual executed subtasks, and the VLM-provided failure reasons during replanning as part of the experience. (Note that: The EXPERIENCE in the prompt refers to insight in the paper. )\nAlfworld We use AgentBench as the Alfworld's executor. AgentBench (Liu et al., 2023b) is executor frameworks with ReAct format (Yao et al., 2022), Alfworld is one of its subtask. As shown in Figure 2, AgentBench provides the task background (as shown below), user query (i.e., the dialogue), and other relevant information to the LLM in a fixed format, allowing the LLM to generate a thought and an action (as the plan) in each turn. After the action's execution, the environment will give the feedback to the agent and the agent will replan another action based on feedback and new thoughts until the task is finished. In the MSI, we include the task background, user query, and all thought-action-observations in the task as the experience. The introduction of HELPER and AgentBench can be seen in Appendix A"}, {"title": "4.3 Hyperparameter", "content": "Our insight generation and decision-making components are aligned with Expel. We have chosen ChatGPT (gpt-3.5-turbo-1106) as the LLM for selecting insight subtasks. GPT-4 (gpt-4-1106-preview) as the LLM for insight generation. During the experience selection phase, we use text-embedding-ada-002 to establish a vector library for failed experiences for retrieval purposes.\nTEACh We have chosen ChatGPT (gpt-3.5-turbo-1106) as the decision-maker for planning. The settings for experience memory enhancement, PreCheck, Correction, and locator are all aligned with HELPER. Due to the time limitation and budget, we do not use GPT4 as the decision-maker for planning.\nAlfworld We have chosen ChatGPT (gpt-3.5-turbo-1106) and GPT-4 (gpt-4-1106-preview) as the decision-maker for planning. The examples are all aligned with AgentBench."}, {"title": "4.4 Baseline", "content": "For TEACH, We consider the following baselines:\nFine-Tune Based Model: Episodic Transformer (E.T.) (Padmakumar et al., 2022) is an end-to-end multimodal transformer that can predict the action by language inputs like dialogue and images in the environment. Jarvis (Zheng et al., 2022) and FILM (Min et al., 2022) use a multimodal transformer to predict subgoals and transform them into atomic actions by rules. DANLI (Zhang et al., 2022) uses an LM to encode language inputs to high-level subgoals and uses a PDDL model (Lamanna et al., 2021) to transform sub-"}, {"title": "4.5 Main Result (RQ1)", "content": "TEACh The performance of MSI on TEACh is displayed in Table 1. Notably, MSI gains 12.70% in IND data and 14.54% in OOD data, which outperforms all results among LM and ChatGPT. In contrast, Expel performs below other LLM Agent-"}, {"title": "4.6 Experience Select Strategy (RQ2)", "content": "Table 3 shows the results of the two strategies under two long-term memory methods. From the perspective of the optimization goal of insights (i.e. $SR_{ACC}$), Expel performs 8.28% and 8.99% on HELPER IND and OOD data when using insights summarized from successful experiences alone compared to using success-failure pairs with 9.94% and 11.60% respectively. In contrast, MSI performs better when summarizing insights from success-failure pairs rather than just successful experiences, the former reaches 12.70% and 14.54% in HELPER IND and OOD data while the latter only gains 10.65% and 13.39%. Alfworld's GPT3.5 version has the same trend in Table 3. The reason for this outcome may be that Expel's method of summarizing and utilizing insights provides the LLM with many fine-grained insights that are problematic yet related to the issue or irrelevant insights(as shown in the red part of Figure 4), leading to decreased accuracy.\nConversely, when MSI summarizes the insights, it does so at multiple scales and only selects a portion for actual use by the LLM. This approach separates general insights with strong generality from fine-grained insights, ensuring that when the LLM uses insights from success-failure pairs, it can benefit from the strong generality of general insights while reducing the interference of irrelevant fine-grained insights through selective insight use. Due to this characteristic of MSI, its effectiveness in summarizing and utilizing insights from success-failure experience pairs is better than using successful experiences alone.\nThe above analysis indicates that the Experience Select Strategy is related to the method of generating and utilizing insights. If strong generality and"}, {"title": "4.7 Insights Select Strategy (RQ3)", "content": "Table 4 shows the comparison of multi-scale insights versus only general insights used under two different Insight Select Strategies. In most cases, the use of multi-scale insights provides a stronger improvement to LLM planning than the use of general insights alone. However, when dealing with OOD problems in pair mode, the general insights gain 14.86% in TEACh and 20% in Alfworld, which outperforms the multi-scale insights' result of 14.54% and 16% respectively. This may be due to task-specific insights summarized in-domain not aligning with OOD tasks, resulting in fine-grained mismatches. Pair mode is more susceptible to fine-grained mismatches, which is why using only general insights can be more helpful to model decision-making than using multi-scale insights. Consistent with the conclusions of Section 4.4, the effectiveness of MSI when summarizing insights in pair mode is always better than in success mode.\nTable 5 presents the impact of two different methods of refining task-specific insights on LLM decision-making in TEACh. Across both data types, results using hashmap pair retrieval are over 20% higher on Success Rate (SR) than those using vector similarity retrieval (from 10.05% to 12.70% in IND and 11.43% to 14.54% in OOD). This is because vector similarity retrieval may introduce irrelevant insights, as shown in Figure 1. If the task is \"water plants with a bowl\", the top three insights retrieved by vector similarity are classified as \"Water Plant\", \"Retrieve and Prepare\" and \"Prepare Beverage\". The first two seem to align with the task requirements, while the third is unrelated. The \"Prepare Beverage\" can be retrieved because the word 'bowl' is in the task whose semantic space is associated with cooking, leading to the retrieval of irrelevant insights. This also explains why the method of vector similarity retrieval, used to retrieve schemes as examples, cannot be employed when utilizing insight.\nThe results from Tables 4 and 5 collectively illustrate the strategy for selecting insight:\nThe agent system needs to first determine whether the current task aligns with the seed task experiences for insight generation. If there is no alignment, then only general insights in the MSI should be used to assist LLM decision-making. Conversely, if there is alignment, multi-scale in-"}, {"title": "4.8 Robustness in Domain Adaptation (RQ4)", "content": "Agents can adjust to new environments by constantly updating their insights repository. However, the distribution of new tasks may differ from that of old tasks that have already been summarized into insights, which can lead to \"catastrophic forgetting\" of old tasks when the insights undergo domain transfer, resulting in decreased model performance on old tasks. Therefore, it is crucial to have robust agents for Domain Adaptation.\nFigure 5 illustrates the robustness of MSI and Expel under domain shifting in TEACh. We fed the training data into the insight summarizer in the order of environments: kitchen, living room, and bedroom, unlike the original MSI and Expel, which shuffle the training data before input. We selected the kitchen task in the valid unseen set as \"original domain tasks\" for testing. insights summarized solely on kitchen data are more beneficial in assisting the model with decision-making in the kitchen. However, as new OOD data is introduced, the model insights a degree of forgetting, leading to a decline in performance on kitchen tasks. Compared to Expel, which declines 2.11% after summarizing the living room and bedroom scheme, MSI shows a smaller degree of performance decline and faster convergence with only a decline of about 0.38%, proving that MSI possesses better robustness in handling domain transfer."}, {"title": "4.9 Conclusion", "content": "In this paper, we propose MSI, which is capable of summarizing and utilizing multi-scale insights to enhance the decision-making ability of embodied agents. MSI can assist agents in making higher-quality decisions and is better equipped to handle insight distribution shifting that may occur with continuous insight updating.\nOur experiments demonstrate that for MSI, success-failure experience pairs are better seed data for insights, while the strategy for insight selection needs to be determined based on a comprehensive assessment of the future task distribution and the distribution of tasks for which insights have been summarized.\nIt sets a new state-of-the-art result for the TEACh using agents based on ChatGPT as the foundation and beat another insight mechanism in the Alfworld. We believe our work contributes new insights into the summarization, storage, and utilization of long-term memory, especially insights."}, {"title": "Limitations", "content": "While MSI achieves significant improvements over existing baselines, there are still directions to explore for future work.\n(1) Although the General and Subtask scale can be used in all tasks, the environment scale can only be used in some embodied scenarios. In the future, we will expand the idea of multi-scale insight by designing different scales in other tasks.\n(2) We only explore one type of long-term memory, insight. In the future, we will explore the combination of different types of long-term memory."}]}