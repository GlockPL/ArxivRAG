{"title": "Stack Trace Deduplication: Faster, More Accurately, and in More Realistic Scenarios", "authors": ["Egor Shibaev", "Denis Sushentsev", "Yaroslav Golubev", "Aleksandr Khvorov"], "abstract": "In large-scale software systems, there are often no fully-fledged bug reports with human-written descriptions when an error occurs. In this case, developers rely on stack traces, i.e., series of function calls that led to the error. Since there can be tens and hundreds of thousands of them describing the same issue from different users, automatic deduplication into categories is necessary to allow for processing. Recent works have proposed powerful deep learning-based approaches for this, but they are evaluated and compared in isolation from real-life workflows, and it is not clear whether they will actually work well at scale.\naTo overcome this gap, this work presents three main con-tributions: a novel model, an industry-based dataset, and a multi-faceted evaluation. Our model consists of two parts (1) an embedding model with byte-pair encoding and approximate nearest neighbor search to quickly find the most relevant stack traces to the incoming one, and (2) a reranker that re-ranks the most fitting stack traces, taking into account the repeated frames between them. To complement the existing datasets collected from open-source projects, we share with the community Slow Ops a dataset of stack traces from IntelliJ-based products developed by JetBrains, which has an order of magnitude more stack traces per category. Finally, we carry out an evaluation that strives to be realistic: measuring not only the accuracy of categorization, but also the operation time and the ability to create new categories. The evaluation shows that our model strikes a good balance it outperforms other models on both open-source datasets and SlowOps, while also being faster on time than most. We release all of our code and data, and hope that our work can pave the way to further practice-oriented research in the area.", "sections": [{"title": "I. INTRODUCTION", "content": "In major software companies, developers receive a lot of error reports from individual users. While detailed bug reports can help developers fix the issues, users often do not have the time or the desire to write them [1]. In this case, software projects often rely on automatic bug reports [2], which in turn rely heavily on stack traces, i.e., stacks of method calls called during the error, accompanied by various metadata [3].\nDifferent bug reports often relate to the same error, and since there are tens and hundreds of thousands of them in large companies, an automatic deduplication [4] is necessary, which involves putting the new incoming report into one of the existing categories with similar reports. In the case when bug reports are represented by stack traces, this requires a similarity measure to compare them [5].\nThis problem is well-studied in literature, and there exist numerous solutions [6]\u2013[16]. The simplest methods are string-based, using different techniques such as Levenshtein distance, prefix match, etc. [6]-[8], [12]. Also popular are information retrieval-based approaches [10]-[12], with one of the most popular ones developed by Lerch and Mezini [11], employing the TF-IDF measure to find similar reports. With the rise of deep learning, new solutions were introduced that utilized it. Khvorov et al. proposed the first such model called S3M [13], which used a biLSTM [17] architecture with aggregation on top of outputs to encode stack traces and a linear transforma-tion on top to compare two stack traces. Subsequently, Liu et al. [15] improved upon this approach to achieve better results.\nDespite these advancements, there are still significant chal-lenges that need to be addressed. Firstly, the current methods still do not provide sufficient accuracy for a completely au-tomatic usage in real projects, requiring further enhancement. The accuracy is critical to ensure that developers can rely on these systems. Current state-of-the-art deep learning methods often process each stack frame individually, and only afterwards are the embeddings compared. To address this issue, the use of a reranker a model that processes two entities simultaneously may be reasonable, as it is often used in retrieval tasks to enhance the precision of the results [18].\nAlso, better evaluation methods are critical. Firstly, the evaluations are limited by the narrow scope of existing open-source datasets [19], [20], which affects their generalizability and robustness. Secondly, the vast majority of existing works only measure the general accuracy of putting reports into the necessary category, without considering two aspects crucial for practical use in industry: the ability to correctly create new cat-egories and the operation time. To the best of our knowledge, only two works [15], [16] studied these aspects. However, their analysis of time did not account for the possibility to pre-compute and cache embeddings, which is commonly done with embedding-based systems in large software projects. As for the evaluation of creating new categories, it was carried out before the advent of modern deep learning-based approaches, and it is thus necessary to also compare them on this task.\nIn this paper, we aim to fill all these gaps in research by developing a new model that performs better than the current state of the art, presenting a novel dataset collected from large-scale industrial products, and carrying out a more comprehensive, multi-faceted evaluation. Our model consists of two stages. The first stage is a biLSTM-based embedding model designed to quickly encode stack traces into compact feature vectors. The main features of this stage are Byte Pair Encoding (BPE) [21] and the faster approximate nearest"}, {"title": "II. BACKGROUND & RELATED WORK", "content": "A stack trace is a crucial component of an error report in software development, depicting the sequence of function calls that led to an error or an exception. It consists of a series of stack frames, each representing a specific function call within the source code. A stack trace provides a systematic breakdown of the operations leading to a failure, allowing developers to trace back through the execution process.\nEven though stack trace is only a part of an error report, most of the described approaches, as well as our one, do not utilize other metadata. Because of this, in this paper, we sometimes use the terms stack trace, bug report, error report, and just report interchangeably for easier presentation.\nA. Similarity Models for Stack Trace Deduplication\nThe problem of searching for similar reports based on stack traces has been approached with non-deep learning methods since the 2000s. Most of these approaches use string matching algorithms. A notable method outlined by Brodie et al. [27] adopts a biological sequence searching algorithm, specifically, a modified Needleman-Wunsch algorithm [28]. This method involves preprocessing the stack trace by removing typical error-handling and common entry routines, as well as elim-inating recursive function calls.\nModani et al. [6] explore various methods for comparing stack traces, including techniques based on edit distance, the longest common subsequence, and prefix matching. Addition-ally, they introduce an indexing strategy for all available stack traces to expedite the search process.\nRelated techniques are discussed by Bartz et al. [7] and by Dhaliwal et al. [8], with the latter being particularly noteworthy. This method employs a two-step process where signatures are first created for each stack trace, followed by the computation of Levenshtein distance between these signatures to determine similarity. This combination enhances the precision and efficiency of stack trace comparison."}, {"title": "III. APPROACH", "content": "In real-world scenarios, when a new error report arrives, the system for report grouping needs to find the most relevant category for the given report or create a new category. This is a retrieval task. To address this, we propose a two-stage approach: retrieval using an embedding model followed by more accurate re-ranking using a more advanced model, a well-known approach in retrieval tasks. The overall pipeline of the proposed approach is presented in Figure 1.\nThe advantage of the embedding model is that embeddings of the already arrived reports can be precomputed and stored in an index such as FAISS [22]. Then, when a new error report arrives, only its embedding needs to be computed. After the most relevant candidates are quickly retrieved, the reranker can be used to rank them more accurately. This way, the system combines the speed of the embedding model with the accuracy of the reranker. Let us now describe each of the two stages in greater detail.\nA. Embedding Model\nThe first part of our approach is the embedding model, the goal of which is to process all N pre-computed embeddings of the existing stack traces and select K most perspective ones that are similar to the incoming one, so that K \u226a N.\n1) Preprocessing and tokenization of stack frames: In the S3M model [13], each frame is assigned an embedding, and in the DeepCrash model [15], each package is assigned an embedding. This can cause problems as new frames and packages may be added to the repository and their embeddings will be initialized randomly. To handle this problem, we use Byte Pair Encoding (BPE) [3] to split package names into tokens. With BPE, if a new package is added, its name will be split into tokens known to the model, and it also allows for a pre-determined vocabulary size. The preprocessing workflow is streamlined as follows:\n1) Split each string into package, class, and method names.\n2) Further split each package name using camelCase split-ting, suitable for Java and Kotlin projects. This method can be adjusted for other languages, e.g., in Python, one might split based on underscores (snake_case).\n3) Apply BPE to each resulting string, converting sequences into tokens and mapping each to a token index.\nThe BPE tokenizer is trained using the same dataset as the model. This tokenization procedure enables controlled vocab-ulary sizing, setting the number of tokens prior to training. In this study, the limit was set to 10,000 tokens, as selected in preliminary experiments, and we leave further experiments for future work. Training BPE on the same dataset makes this tokenization more dataset-specific, enhancing the relevance of the generated tokens for the given source of reports. This is also the reason for training a new BPE instead of fine-tuning an existing one, since stack traces contain a lot of unique terminology, and the datasets provide enough data.\n2) Creating embeddings of stack frames: Once each stack frame is tokenized, it can be converted into a vector repre-sentation. While initial methods like DeepCrash used a bag-of-words (BOW) model, averaging embeddings of all tokens in a frame, we considered more advanced techniques for this transformation, similar to the work by Pradel et al. [30]. Since further on, similar to previous works, we use a bi-directional LSTM (biLSTM) to combine the embeddings of frames into the embedding of an entire stack trace (see next Section III-A3), we decided to try the same approach to combine the embeddings of tokens into the embedding of a frame on this previous step. It showed a slightly better result, so we decided to use it. Since it is more critical in the next step, we will describe it in detail for combining the embeddings of frames into the embedding of an entire stack trace.\n3) Creating embeddings of stack traces: After converting each stack frame into a vector, the next step involves com-puting the embedding for the entire stack trace, represented as a sequence of vectors. Like we just mentioned, we used a bidirectional LSTM (biLSTM) to encode these sequences, following recent state-of-the-art approaches [13], [15].\nIn our biLSTM model, each input frame's embedding is processed bidirectionally, generating a pair of output vectors $O_i$ (one for each direction) for each frame, which are then concatenated. Additionally, the final hidden states from both directions are concatenated to form $h$, representing the overall context of the sequence. To aggregate these outputs into a"}, {"title": "B. Reranker", "content": "The second stage of our approach is the reranker. It receives K most similar stack traces from the embedding model and re-ranks them, taking into account the similar frames between them and the incoming query stack trace. This is an entire separate model, and it serves to enhance the precision of stack trace similarity assessments.\n1) Preprocessing and tokenization of stack frames: The preprocessing and tokenization steps for the reranker are iden-tical to those used for the main embedding model, ensuring consistency in handling stack trace data.\n2) Creating embeddings of stack frames: The process of converting stack frames to embeddings in the reranker fol-lows the same procedure as described previously, maintaining uniformity in feature extraction across models.\n3) Creating embeddings of stack traces with cross-encoder: A cross-encoder is a powerful mechanism in machine learning, specifically designed to process pairs of inputs simultaneously to produce a single output, which is highly beneficial for the retrieval task [18], [34]\u2013[36]. We use a cross-encoder as a reranker in our approach to enhance the precision of our similarity assessments, ensuring that the most relevant stack traces are accurately aligned and compared.\nWe utilize a cross-encoder model based on a biLSTM architecture to identify identical frames across stack traces. The architecture of the cross-encoder is shown in Figure 2. This approach begins by encoding each stack frame into a fixed-length vector. If a frame from the query stack trace Q, denoted as $Q_i$, is also found in the second stack trace K, its representation is enhanced by adding a learned significance vector, $V$, which signifies the frame's presence in both traces. This procedure converts two independent sequences of frame embeddings, $[F_{Q1}, F_{Q2}, ..., F_{Q_i}]$ from the first stack trace and $[F_{K1}, F_{K2}, ........., F_{K_j}]$ from the second, into two interdepen-dent sequences, $[F_{Q1}, F_{Q2} + V,..., F_{Q_i} + V]$ and $[F_{K_1} + V, F_{K_2},..., F_{K_j} + V]$. Now, the embeddings are enriched with mutual information between corresponding stack frames.\nIn the next step, a model, equivalent to the embedding model described above, is used to aggregate these two sequences into two embeddings. Then, these embeddings are concatenated and fed into an MLP to obtain a single number a similarity score for the input pair. This method allows the model to emphasize frames that appear in both stack traces, potentially increasing the relevance and accuracy of the similarity score. Besides adding a significance vector $V$ to the frame embed-dings and utilizing it in the final MLP, the two stack traces do not interact directly. Interestingly, even this limited interaction can significantly improve the accuracy of the method.\n4) Training the cross-encoder: The training data was con-structed using triplets of stack traces: an anchor stack trace, a positive example (a stack trace from the same category), and a negative example (a stack trace from a different category). The positive pairs were sampled as described in Section III-A4. The negative example was added to each pair by sampling a random stack trace from a different random category.\nThe Binary Cross-Entropy (BCE) loss was used for training the cross-encoder. The loss function was defined as follows:\n$L_{BCE}$ = log(1 + exp($-s_p$)) + log(1 + exp($s_n$))\nwhere $s_p$ is the similarity score between the anchor and the positive example and $s_n$ is the similarity score between the anchor and the negative example. This loss function allows the model to learn to distinguish between similar and dissimilar stack traces effectively, improving the accuracy.\n5) Final decision: The reranker processes K pairs the incoming stack trace Q paired with each of the K stack traces selected by the embedding model. In then re-ranks these K"}, {"title": "IV. EVALUATION", "content": "This section presents the multi-faceted evaluation that we conducted of our approach, as well as the existing state-of-the-art approaches. Striving for evaluating the performance needed in practice, we posed the following research questions:\nRQ1 How accurate is our approach in predicting category for the given error report?\nRQ2 How well can our model distinguish between a situ-ation when error reports are attached to the existing category and when a new category is created?\nRQ3 How fast is our method when incorporated into the retrieval process?\nBelow, we describe the datasets we used, including the new one we present, the evaluation setup, the baselines, the metrics, and the results of our experiments.\nA. Datasets\n1) Existing datasets: The majority of evaluations pre-sented in previous works utilize the following public datasets:"}, {"title": "B. Evaluation Procedure", "content": "Our evaluation procedure aims to model real-world scenar-ios. Each dataset is first sorted by the arrival time of error reports and then split into training, validation, and test sets in the proportions of 70/10/20 based on the number of reports.\nAll reports from the test split of the dataset are sorted by their arrival time. We then iterate over all reports in the test segment and use the similarity model to rank categories for each new report. Specifically, we compute similarities between each previously arrived report and the newly arrived one. For each category, a similarity score is calculated based on the highest similarity score among its stack traces:\nSimilarity(q, category) = max Similarity(q, k)\nk\u2208category\nwhere q represents the current stack trace, and k is a stack trace within the category. This part is common in the evaluation, because in all the compared approaches, the incoming report is assigned to the category that has the single most similar report to the incoming one. Based on this ranking of categories, we compute the metrics presented in Section IV-D.\nA significant change we made compared to the previous evaluation procedure used in the work by Khvorov et al. [13] is that if, during the iteration over reports in the test split, we encounter a report that already has an identical report in an existing category, we ignore this report in the evaluation process. There are two main reasons for this adjustment.\nFirstly, in real-world scenarios, e.g., at JetBrains, when a report arrives identical to an existing one, it is automatically added to the same category without even employing the similarity model. Thus, it does not make sense to evaluate the model in situations where it would not be used."}, {"title": "C. Baselines", "content": "We consider several baselines for comparison with our proposed method, categorized into supervised, unsupervised, and large language models (LLMs). Below, we detail the specific models used in each category.\n1) Supervised baselines: Supervised methods require train-ing data to learn and typically involve models that adjust their parameters based on annotated datasets to enhance their predictive accuracy. Our solution falls into this category.\nS3M [13]: S3M is the first deep learning model proposed by Khvorov et al. At the time of its introduction, it outperformed non-deep learning solutions and was considered a state-of-the-art approach. It utilizes a biLSTM model to encode stack traces, but does not employ BPE, FAISS, or reranking.\nDeepCrash [15]: Proposed by Liu et al., DeepCrash is another supervised solution with an advanced architecture that outperforms S3M. The core ideas of the paper are to switch to the embedding model and use skip-gram for obtaining embeddings of the stack frames.\n2) Unsupervised baselines: Unsupervised models do not require labeled data and are advantageous in scenarios where such data is unavailable. However, they still rely on unlabeled data, such as a large collection of reports, to train components like IDF (Inverse Document Frequency)."}, {"title": "D. Metrics", "content": "1) RQ1: Attach accuracy metric: To address RQ1 and study the general accuracy of our approach in choosing the necessary category for incoming stack traces, we employ the Acc@1 metric. In our evaluation, there are two types of reports: those that are attached to an existing category and those that create a new category. The Acc@1 metric is applicable only to reports that are attached to some existing category, while the correctness of creating new categories is studied in RQ2. Acc@1 measures the ratio of cases where the model correctly predicts the most suitable category for a given report:\nAcc@1 =  \u2211 r\u2208A [Most relevant category predicted] / ||A||\nwhere A is the set of attached reports, and r is one such report.\n2) RQ2: Correctness of creating new categories: To ad-dress RQ2 and study how well the models can decide that the new category must be introduced, we use the ROC-AUC score, following Rodriguez et al. [16]. As we mentioned in Section IV-B, we treat this problem as a binary classification task and our approach uses the threshold T that balances the precision and recall of this binary classification using an F1-score. Evaluating this task using ROC-AUC allows to compare the models in general, before selecting the threshold.\n3) RQ3: Speed metric: To address RQ3, we measure the time required for the model to compute the similarity scores for all the necessary reports. Specifically, the time taken to calculate the embeddings for deep learning based approaches is not taken into account, because in large software systems they are pre-computed from the previous runs of the model. Since the incoming report gets sent to the category with the single most similar report (or creates a new category), the whole process involves only finding this most similar report, which is precisely what we are measuring. For our model, we distinguish between the time taken with the reranker and"}, {"title": "E. Results", "content": "1) RQ1: Assigning categories: As shown in Table II, our method outperforms all baselines across all datasets. Specifically, on the Eclipse and Ubuntu datasets, our method with the reranker achieves Acc@1 scores of 0.75 and 0.65, respectively, which are higher than the closest competitor, the text-embedding-3-small model. It can also be seen that on open-source datasets, text-embedding-3-small is the strongest among other models, showcasing the power of LLMs. Finally, it is evident that the results on SlowOps are much more positive than on open-source ones. This can be due to the specific nature of the issues in it or due to many more reports per category, which ensures a more robust classification. In any case, this highlights how important it is to evaluate the models on different kinds of data.\n2) RQ2: Creating new categories: In terms of ROC-AUC, which measures the ability to distinguish when a new cate-gory should be created, our method again outperforms most baselines, as shown in Table III. On the Eclipse dataset, our approach with the reranker achieves a ROC-AUC of 0.86, and on the Gnome dataset, it reaches 0.71. Notably, our method without the reranker achieves the highest ROC-AUC score of 0.99 on our dataset, suggesting that even without reranking, our model is highly effective at identifying new categories. Again, the second-best model is text-embedding-3-small, demonstrating excellent results.\n3) RQ3: Speed: Finally, as detailed in Table IV, the speed of our model indicates a trade-off between accuracy and com-putation time. While our method with the reranker provides the best predictive performance, it requires 144.5 ms per report, which is an order of magnitude faster than some baselines like S3M (1722.8 ms) but similarly an order of magnitude slower than others like DeepCrash (7.6 ms). However, when using only the embedding model without reranking, our method is significantly faster, processing each report in just 8.7 ms, making it suitable for real-time applications where speed is critical. While DeepCrash is excellent in terms of time, Tables II and III show that it struggles with performance. In contrast, text-embedding-3-small, which shows great results, is very slow, which is to be expected of an LLM."}, {"title": "V. DISCUSSION", "content": "A. Balance Between Accuracy and Performance\nOverall, our approach can be seen as a great practical compromise that can perform in both accuracy and time. The two-staged architecture comprising an embedding model followed by a reranker proves to be effective, with the reranker enhancing the precision of category predictions. This approach improves upon the challenges identified in previous works, and it can still perform well without the reranker if time is an issue in a particular task.\nOther evaluated models demonstrate very different perfor-mance. The tested LLM showed very good accuracy, close to our approach, but it is really slow. Two fastest approaches disregarding ours, although on a very different scale, are DeepCrash and the approach by Lerch and Mezini, however, their accuracy is significantly worse. It is thus important to continue research in this area, but taking into account the practical aspects and ensuring that new \"state-of-the-art\" approaches are good not only in base accuracy.\nB. Large Language Models (LLMs)\nGiven their entirely different nature and importance, it is worth discussing the LLMs separately. While LLMs like the text-embedding-3-small model perform well in our evalua-tions, they have certain limitations. They often require making an API call, which can introduce latency, making them less suitable for real-time applications. Additionally, software com-panies may worry about their privacy and may not be willing to send the data over the internet to the most powerful models. While local-based LLMs can be used to overcome these issues, running a biLSTM locally is usually cheaper and easier, thus, our model offers a more tailored solution for this specific task. Nonetheless, the provided results clearly indicate that LLMs deserve further study in this field.\nC. Performance on SlowOps\nAn interesting result from our evaluation is just how differ-ent the results are on our new industrial dataset. The reports in this dataset all relate to one error type, Slow Operation Assertion, and the dataset has an order of magnitude more reports per category, which might explain why it is \"easier\u201d for the models to correctly classify them. Still, the good per-formance of the models on it is not a reason for complacency, because it represents just one specific data source. Rather, this difference of results indicates the importance of carrying out the evaluation on diverse data, to obtain a more exhaustive picture of how models will perform in different scenarios."}, {"title": "VI. THREATS TO VALIDITY", "content": "The large-scale nature of our work makes it subject to the following threats to validity.\nHyperparameters. When evaluating our approach, we used certain hyperparameters, such as the size of BPE vocabulary of 10,000 and the number of the most similar stack traces passed from the Embedding model to the Reranker K = 10. While we carried out some preliminary experiments to select them, we did not conduct exhaustive evaluations, and so these hyperparameters might not be optimal. Such evaluations are a part of our future work.\nModel generalization. Our model is tailored to the dataset it was trained on, which could lead to overfitting to the specific characteristics of that dataset. To address this, we tested the model on a variety of datasets from different sources, helping to ensure its robustness. However, generalizability may still vary in significantly different environments.\nData source bias. The effectiveness of our model is closely tied to the specific dataset used for its training and evalua-tion. Since SlowOps was obtained from a particular software company, JetBrains, it may not fully represent the variability found in other environments, such as open-source projects or smaller companies. The characteristics of stack traces, coding practices, and error reporting can vary significantly across different domains, potentially leading to different outcomes if the model is applied outside the context of our dataset. For this reason, we encourage further researchers to collect even more diverse datasets and share them with the community.\nLanguage and framework dependency. The stack traces used in our study come primarily from JVM-based languages and C++. This selection might influence the performance of our model when applied to other programming languages or frameworks. The structure of stack traces and the nature of errors can vary significantly depending on the programming environment, potentially affecting the accuracy and effective-ness of our approach. Future work is necessary to explore the model's adaptability to a broader range of programming languages and development environments.\nWhile these threats to validity are important to acknowl-edge, we believe they do not invalidate the overall conclusions of our study or its practical relevance."}, {"title": "VII. CONCLUSION", "content": "In this work, we presented a novel model for deduplicating stack traces. The proposed approach consists of two steps: a base embedding model that employs the approximate nearest neighbors search to quickly find the most similar stack traces and a reranker that re-ranks them more accurately, taking into account the information about repeated individual frames. To facilitate more detailed comparison of our model with existing approaches, we collected a dataset of stack traces from JetBrains, complimenting the existing open-source datasets. Finally, we carried out a multi-faceted evaluation, comparing the accuracy of categorization, but also evaluating the ability of models to create new categories and their operation time. Our approach shows the best results in terms of accuracy and is efficient in terms of time. We release the dataset [25] and the code [26] to the community to facilitate further research.\nIn the future work, we plan to continue to improve our approach by conducting more experiments with different hy-perparameters and architectures. We also want to explore even broader evaluations, considering different types of industrial projects, languages, and frameworks. We believe this is crucial for ensuring the applicability of the research in practice."}]}