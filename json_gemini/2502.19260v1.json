{"title": "EMT: A Visual Multi-Task Benchmark Dataset for Autonomous Driving in the Arab Gulf Region", "authors": ["Nadya Abdel Madjid", "Murad Mebrahtu", "Abdelmoamen Nasser", "Bilal Hassan", "Naoufel Werghi", "Jorge Dias", "Majid Khonji"], "abstract": "This paper introduces the Emirates Multi-Task (EMT) dataset the first publicly available dataset for autonomous driving collected in the Arab Gulf region. The EMT dataset captures the unique road topology, high traffic congestion, and distinctive characteristics of the Gulf region, including variations in pedestrian clothing and weather conditions. It contains over 30,000 frames from a dash-camera perspective, along with 570,000 annotated bounding boxes, covering approximately 150 kilometers of driving routes. The EMT dataset supports three primary tasks: tracking, trajectory forecasting and intention prediction. Each benchmark dataset is complemented with corresponding evaluations: (1) multi-agent tracking experiments, focusing on multi-class scenarios and occlusion handling; (2) trajectory forecasting evaluation using deep sequential and interaction-aware models; and (3) intention benchmark experiments conducted for predicting agents' intentions from observed trajectories. The dataset is publicly available at avlab.io/emt-dataset, and pre-processing scripts along with evaluation models can be accessed at github.com/AV-Lab/emt-dataset.", "sections": [{"title": "I. INTRODUCTION", "content": "As autonomous driving technology advances, the ability of data-driven models to generalize across diverse road environments and conditions is essential for safe operation, but remains a significant challenge. To achieve robust generalization, it is critical to train models on datasets that capture a wide range of traffic scenes and characteristics. Current autonomous driving datasets provide extensive coverage of regions like the USA [1-5], Europe [6, 7], and parts of Asia, including China and Singapore [1, 8]. However, the Arab Gulf region, with its unique driving conditions, remains underrepresented. To address this gap, we introduce the Emirates Multi-Task (EMT) dataset, collected in the Unighted Araba Emirates (UAE) to capture the region's distinct traffic conditions. This region offers diverse driving challenges due to its range of road layouts, including expansive highways, urban areas, and complex city junctions. Additionally, driving behavior in the UAE reflects a blend of modern regulations and traditional practices. Our dataset was gathered using dash cameras mounted on two vehicles driving across the UAE's largest cities and on intercity routes, covering over 150 kilometers. The annotated dataset supports multiple benchmarks, including tracking, trajectory prediction, and intention prediction, aimed at advancing models robustness in complex driving environments.\nThe tracking benchmark dataset is designed to evaluate the ability of algorithms to accurately identify and maintain consistent object tracking over time in a complex driving en- vironment. Similar to current state-of-the-art (SOTA) tracking benchmarks [1, 9, 10], it focuses on the motion of vehicles, pedestrians, cyclists, and motorbikes, captured from a frontal camera perspective. The benchmark is designed to test tracking models under varying levels of traffic congestion and frequent lane changes. The dataset contains 8,806 unique tracking IDs, including 8,076 vehicles, 568 pedestrians, 158 motorbikes and 14 cyclists, and with a mean tracking duration of 6.5 seconds."}, {"title": "II. RELATED WORK", "content": "Related work is summarized through an overview of au- tonomous driving datasets, as well as models for multi-object tracking, trajectory forecasting, and intention prediction."}, {"title": "A. Autonomous Driving Datasets", "content": "Evaluating the effectiveness of autonomous driving algo- rithms requires testing across a diverse range of scenarios to assess performance under various conditions. Each scenario presents unique challenges and variability that models must navigate to be considered viable for real-world application. In recent years, numerous datasets, such as KITTI [6], KITTI-360 [7], ApolloScape [8], PIE [11], NuScenes [1], WAYMO [10], Argoverse I [3], Argoverse II [4, 5], and Lyft Level 5 [2], have been introduced to support research in tasks such as detection, segmentation, tracking, prediction, intention prediction, and planning.\nAmong intention prediction datasets, the NGSIM\u00b9 and HighD [12] datasets focus on highway scenarios. NGSIM pro- vides intention labels for lane-changing, lane-keeping, merg- ing, and yielding, while HighD includes labels for free-driving, vehicle following, critical maneuvers, and lane-changing. HighD offers high-resolution highway data with trajectories spanning approximately 13 seconds per vehicle. NuScenes [1] and Argoverse [3] datasets cover urban driving, providing multimodal sensor data. The NuScenes dataset includes inten- tions such as moving, stopped, and parked. Argoverse contains maneuvers for lane changes, lane keeping, and left/right turns.\nThe INTERACTION dataset [13] focuses on challenging environments like roundabouts, intersections, and highways, providing intention labels for lane changes, merging, and yielding, which are essential for interaction-aware trajectory prediction. BLVD dataset [14] offers diverse behavior and interaction annotations for urban driving, capturing a wide range of maneuvers such as lane changing, lane keeping, accelerating, decelerating, stopping, and left/right turns. This dataset supports both tracking and maneuver intention tasks through multi-camera video data. Lastly, the InD dataset [15] includes intention labels for actions such as merging, yielding, and left/right turns, with annotations similar to BLVD for vehicles, pedestrians, and riders.\nThe designed EMT dataset complements existing datasets by introducing data collected from a previously underrep- resented geographical location. By including diverse driving scenarios typical for the Arab Gulf-region traffic, the EMT dataset provides a unique context for improving models' predictive accuracy in culturally and regionally distinct driving behaviors."}, {"title": "B. Multi-object Tracking", "content": "Multi-object tracking (MOT) methods encompass a variety of approaches, including tracking-by-detection [16, 17] and joint detection and tracking [18, 19]. Tracking-by-detection methods first detect objects in each frame and then associate them across frames. SORT [16] serves as a lightweight and efficient baseline in this category, combining object detections with a Kalman Filter for motion prediction and the Hungarian algorithm for data association. Tracktor [17], on the other hand, eliminates the need for explicit data association by reusing regression heads in object detectors to refine and"}, {"title": "C. Trajectory Prediction Methods", "content": "The field of trajectory prediction is extensively studied and encompasses various methodologies, including physics-based models, probabilistic approaches, and learning-based tech- niques. The presented related work focuses on deep-learning methods relevant to our evaluation framework: LSTM-based models, Transformer-based architectures, and Graph Neural Networks (GNNs).\n1) LSTM-based methods: LSTM-based models have proven effective in capturing the temporal dependencies inherent in trajectory data. Early approaches, such as Social-LSTM [25], introduced the concept of social pooling to model interactions among agents, establishing a foundation for socially-aware trajectory prediction. Subsequent variants, including SSCN- LSTM [26] and SR-LSTM [27], extended these capabilities by integrating spatial context and message-passing mechanisms, thereby enhancing their ability to represent dynamic interac- tions in multi-agent environments.\nSeveral works have improved LSTM architectures by inte- grating environmental and spatial factors. Scene-LSTM [28] utilized a grid-based scene representation to capture interac- tions between agents and static objects, while MX-LSTM [29] combined trajectory data with head pose estimations to model multimodal behavior. GC-VRNN [30] extended this concept further by incorporating graph-based representations to man- age incomplete data and complex temporal dependencies. TraPHic [31] demonstrated the effectiveness of combining CNNs with LSTMs to handle heterogeneous traffic conditions. Architectural advancements have also enabled multimodal and hierarchical designs. For example, stacked LSTMs [32], hierarchically process trajectory data to predict maneuver- specific behaviors, while encoder-decoder frameworks [33], utilize Gaussian Mixture Models for probabilistic trajectory predictions. Other works, like STS-LSTM [34] and Highway- LSTM [35] highlight the adaptability of LSTMs in specialized use cases, such as spectral trajectory modeling or highway motion forecasting.\n2) Transformers: Transformer-based models are highly effective in capturing long-range dependencies, which are crucial for understanding agent motion over time. STAR [36] leverages Transformers to model individual pedestrian dynamics and graph-based spatial Transformers for crowd interactions, using alternating layers for joint spatio-temporal modeling. Similarly, GA-STT [37] employs cross-attention to fuse spatial and temporal embeddings, effectively capturing both individual and group-level motion features. HiVT [38] adopts a two-stage approach, with the first stage focusing on extracting local context and the second stage integrating global interactions.\nFor social interaction modeling, AgentFormer [39] utilizes an agent-aware Transformer with tailored attention mech- anisms for intra-agent and inter-agent interactions. Latent- Former [40] introduces hierarchical attention to capture so- cial interactions and employs Vision Transformers to extract contextual scene features. Several models integrate GNNS with Transformers, such as Graph-Based Transformers [41\u2013 43], which model structured representations of agent-agent and agent-environment relationships, highlighting the synergy between graph representations and Transformer architectures for structured interaction data.\nIn the context of multimodal integration, the literature ex- plores combining heterogeneous input modalities, such as map information, bounding boxes, and scene semantics. mmTrans- former [44] integrates a motion extractor for past trajectories, a map aggregator for road topology, and a social constructor to model agent interactions. MacFormer [45] builds upon this by incorporating map constraints and Crossmodal Transformers [46] further advance this approach by fusing cross-relation features between modality pairs, supported by a modality attention module.\n3) GNN-based methods: GNN models represent traffic scenes as graphs, where nodes correspond to agents and edges encode relationships, typically based on proximity. Graph Convolution Networks (GCN)-based models focus on spatial relationships and dynamic interaction modeling. GRIP [47] alternates between temporal convolutional layers and graph operations to encode motion features and spatial interactions, while its extension, GRIP++ [48], enhances this by incorporat- ing dynamic edge weights and scene context. LaneGCN [49] integrates HD map data using lane graphs, and Trajectron++ [50] combines GCNs with recurrent architectures to process spatiotemporal graphs enriched with semantic features.\nAnother line of work relies on Graph Attention Networks (GAT). GAT-based models use attention mechanisms to prior- itize influential interactions dynamically. Social-BiGAT [51] combines GATs with generative frameworks for multimodal forecasting. GATraj [52] introduces a Laplacian mixture de- coder to enhance prediction diversity while modeling spatial- temporal dependencies, and GraphTCN [53] integrates GATs with temporal convolutional networks (TCNs) to efficiently capture long-term dependencies.\nHierarchical and hybrid approaches combine multiple tech- niques for comprehensive modeling. MFTraj [54] uses dy- namic geometric graphs and adaptive GCNs to model spa- tiotemporal dependencies. GOHOME [55] and MTP-GO [56] adopt graph-based methods for agent-map interactions, with MTP-GO employing neural ODEs to manage dynamic mo- tion constraints. EqMotion [57] ensures interaction invariance and geometric equivariance, providing stable and consistent trajectory predictions."}, {"title": "D. Intention Prediction Methods", "content": "Intention-aware models represent an important direction in trajectory prediction, aiming to enhance accuracy by incorpo- rating agents' maneuver intentions. These methods [58, 59] treat maneuvers as short-term, goal-driven decisions that con- sist of a sequence of continuous states working toward a global objective. Maneuvers are typically categorized into lateral decisions, such as lane-keeping, lane-changing, or turning, and longitudinal decisions, such as maintaining speed, accel- erating, or braking. By integrating maneuver awareness, these models introduce an intermediate reasoning layer that informs predictions with planning-based logic.\nSeveral LSTM-based models have demonstrated success in intention prediction. Occupancy-LSTM [60] generates occu- pancy grid maps by modeling surrounding vehicle motions, capturing likely maneuvers such as lane changes. Similarly, Zyner et al. [61] and Phillips et al. [62] employ LSTMs to predict driver intentions, using features like heading, position, and velocity, with a particular focus on intersections. MX- LSTM [29] extends this capability by incorporating head pose data, providing an additional layer of contextual awareness to better infer maneuver intentions."}, {"title": "III. DATASET", "content": "In this section, the key aspects of the EMT dataset are intro- duced, including the data collection process, design method- ology, and annotations format."}, {"title": "A. Data collection", "content": "The EMT dataset was collected in two major cities in the UAE Abu Dhabi and Dubai, as well as on the roads connecting these cities. Data collection was conducted using two vehicles equipped with front-facing cameras. Each vehicle was outfitted with a VANTRUE 3-Channel Dash Camera, recording at 1080P Full HD. The video footage captures a variety of road topologies common in the region, including highways, roundabouts, bridges, city junctions, intercity high- ways, and narrow urban streets. The dash cameras recorded video sequences at a frame rate of 30 fps, saved as 3-minute clips. For the annotation process, frames were extracted at 10 fps. No pre-processing was applied to remove flashes or blurring caused by bumpy sections along some routes, as this aspect was left for future research to integrate as part of model design. Videos were carefully selected to represent a range of weather conditions, times of day, and scene types. This selection includes recordings during bright daylight, evening, and nighttime, under clear and rainy weather conditions. This diverse collection ensures the inclusion of the most common scenes encountered in the Arab Gulf region. Each video clip lasts between 2.5 and 3 minutes, totaling approximately 57 minutes of video data in the dataset."}, {"title": "B. Data Annotations", "content": "Every significant object and actor within each frame is annotated, including vehicles, pedestrians, small motorized ve- hicles, motorbikes, and cyclists. For objects classification and description we relied on convention presented in [63]. Small motorized vehicles refer to any motorized vehicle smaller than a car, such as mobility scooters or quad bikes. Medium vehicles include those larger than a car, such as vans, while large vehicles refer to lorries, typically characterized by having six or more wheels. Emergency vehicles include ambulances, police cars, and fire engines equipped with red and blue flashing lights, excluding vehicles with yellow flashing lights, such as road maintenance vehicles. Each object is assigned an intention label based on its observed behavior. Table I provides for each class description, number of bounding boxes and number of agents throughout the whole dataset.\nFor vehicles, the intention reflects the current maneuver being performed, while for pedestrians, it represents their activity. The most common maneuver for vehicles is lane- keeping, where the vehicle maintains its current lane and follows a steady trajectory without deviation. The labels \"merge left\" and \"merge right\" are used when a vehicle moves to an adjacent lane, merges, or exits a main road. Turning maneuvers, such as \"turn left\" and \"turn right,\" are assigned when a vehicle turns at an intersection or road junction, typically involving a reduction in speed and a change in direction. Braking indicates deceleration, often observable through activated brake lights in image-based data. Reversing refers to backward movement, which is commonly observed in parking areas or near road edges. Stop is used when a vehicle has come to a complete halt, such as at an intersection waiting for a green light or in parking areas. For pedestrians, the intention label walking denotes movement along a road or sidewalk at a steady pace without any apparent intent to cross the road. Waiting to cross is used when a pedestrian is facing oncoming traffic, signaling potential preparation to cross. Crossing applies when a pedestrian moves across the road, whether at junctions, designated crossings, or random locations, as long as the movement involves passing in front of a vehicle. The stop label indicates that the pedestrian is stationary, such as standing on a pavement or waiting at a bus stop. This annotation approach provides a comprehensive foundation for analyzing and understanding dynamic interac- tions between agents within road scenes.\nThe most common road topologies for the region are illustrated in Figure 3. One observed scenario involves bridges appearing above the ego vehicle in the images. Vehicles on these bridges are ignored as they do not influence the ego vehicle's planning. Figure 3(a) depicts a typical large city junction common in the region. A key feature of such junctions, compared to those in other areas, is the inclusion of a free right turn to alleviate traffic congestion. Cases (3) and (4) highlight this scenario: vehicles can make a free right turn from the traffic lights if no pedestrians are present and the turn does not interfere with other traffic. Otherwise, they must wait to complete the turn and merge safely onto the intended road. For clarity, the figure also includes other typical scenarios: (1) a car waiting at the junction for the green light; (2) a bus crossing the junction from left to right while staying in the same lane; (3) a car performing a free right turn to merge into a lane; (4) a car slowing down to allow a pedestrian to cross, tagged with the action"}, {"title": "C. Annotations Format", "content": "For Multi-Object Tracking, annotations are provided in two formats: GMOT and KITTI. Each video's annotation file includes objects detected across frames, with bounding boxes defined by the x and y coordinates of the top-left and bottom- right corners. These files also include the agent's class and consistent tracking IDs for all objects. For trajectory and intention prediction datasets, the annotation format closely follows the PIE dataset [11]. Each video's annotation file contains all recorded objects, along with their unique IDs, agent classes, sequences of frames in which they appear, and corresponding sequences of bounding boxes. In intention prediction, each object additionally has an \"intention\" attribute that stores the sequence of intentions throughout its entire trajectory:\nTo enhance usability, we supplement the annotations with a parsing script capable of generating custom settings by varying the size of past trajectories, prediction horizon, and overlaps. The script iterates through all objects and segments their trajectories into samples of past and future trajectories using a sliding window approach. For example, if the sliding"}, {"title": "IV. EVALUATION", "content": "This section presents the experimental results conducted on all three task-specific datasets. For each set of experiments, we provide a description of the evaluated models, specify the evaluation protocol, outline the implementation details, and report the results."}, {"title": "A. Multi-Agent Tacking", "content": "Multi-agent tracking experiments are conducted using Kalman filter-based trackers in a tracking-by-detection setup. Two SOTA trackers are evaluated to assess their ability to handle occlusions and detection errors, particularly when dealing with far objects.\n1) Trackers:\n\u2022 ByteTrack [21] is a lightweight real-time multi-object tracking model employing a two-level data association method. ByteTrack processes both high-confidence and low-confidence detections, enabling more robust tracking while maintaining computational efficiency. The method first associates tracklets with high-confidence detections, then matches unmatched tracklets with low-confidence detections to recover lost tracks.\n\u2022 BOT-SORT [20] is a multi-object tracker that enhances motion-based tracking through camera-motion compensa- tion and an improved Kalman filter state representation. Like ByteTrack, it employs a two-level association strat- egy. The tracker employs a cascade matching approach that prioritizes motion information for data association, with an optional ReID extension that adds appearance features as a secondary matching cue.\n2) Evaluation Protocol: We evaluate the trackers using three different detector configurations:\n\u2022 ground-truth detections assuming the presence of perfect detector, providing an upper bound on performance;\n\u2022 off-the-shelf YOLO detector, representing readily avail- able solutions;\n\u2022 fine-tuned YOLO detector, optimized for our specific use case.\nEach detector configuration serves a distinct analytical pur- pose. Ground-truth detections allow us to evaluate the tracker's association algorithm under ideal conditions, isolating its core tracking capabilities from detection errors. The off-the-shelf YOLO detector helps assess tracker robustness under challeng- ing conditions with missing and erroneous detections. Finally, the fine-tuned YOLO detector represents a more realistic production scenario, where the detector is optimized for the specific domain but still maintains some inherent errors. We conduct the evaluation using F1-score, Identity Switches and Higher Order Tracking Accuracy (HOTA):\n\u2022 Multi-Object Tracking Accuracy (MOTA): Evaluates over- all tracking performance by accounting for false positives, false negatives, and identity switches.\n\u2022 False Positives (FP): Instances where the tracker incor- rectly identifies an object that is not present in the ground truth (ghost detections).\n\u2022 False Negatives (FN): Cases where the tracker fails to identify an object present in the ground truth due to detection or tracking errors (missed detections).\n\u2022 Identity Switches (IDs): The total number of instances where a tracker incorrectly reassigns identities between objects.\n\u2022 Identity F1 Score (IDF1): Measures the tracker's ability to maintain consistent object identities, considering identity switches and fragmentation.\n\u2022 Higher Order Tracking Accuracy (HOTA): A compre- hensive metric that balances detection and association accuracy, decomposed into detection accuracy (DetA) and association accuracy (AssA) components.\nFor evaluation purposes, we group detection classes into four superclasses: pedestrian, motorbike, cyclist, and vehicle. The vehicle superclass encompasses all other classes in the dataset, including cars, buses, small motorized vehicles, medium and large vehicles, and emergency vehicles. Tracking performance metrics are broke down for each superclass to provide a detailed analysis of tracker behavior across different object categories. The evaluation was done on the full dataset, i.e., including train and test splits.\n3) Implementation Details: As an off-the-shelf detector, the YOLOX-L model from ByteTrack was utilized. For the fine-tuned setting, YOLOX-L was trained on a train-test split with an input image resolution of 1280\u00d71280. Since the original implementations of both trackers were designed for single-class pedestrian tracking, we extended their capabilities to support multi-class MOT. Our implementation maintains separate tracking instances for each class, ensuring that ob- ject association occurs only between detections of the same class. This preserves the core tracking logic while enabling simultaneous tracking of multiple object categories.\nAll experiments followed our evaluation protocol using standard tracking metrics. To ensure reproducibility, we main- tained consistent tracking parameters throughout testing. Byte- Track was configured with a detection confidence threshold of 0.5, a track buffer size of 10 frames, and an IoU matching threshold of 0.8. BOT-SORT used these same core parame- ters while incorporating additional high and low confidence thresholds of 0.6 and 0.1, respectively. These configurations were based on the trackers' original implementations and optimized for our specific tracking scenario while preserving their fundamental operational characteristics.\n4) Results: After fine-tuning YOLOX-L on our dataset, we evaluated both the fine-tuned and off-the-shelf models as detection backends for tracking. Table IV presents the per-class detection results. All evaluations used a detection confidence threshold of 0.4 and an NMS threshold of 0.5. While NMS is typically set higher, the densely packed objects and frequent bounding box overlaps in our dataset required a lower threshold to prevent excessive suppression. The fine- tuned YOLOX-L model significantly improved recall across all object classes, leading to higher F1 scores, particularly for pedestrians, motorbikes, and cyclists. The F1 score increased by approximately 2.5x across classes compared to the off-the- shelf model. While for vehicles, the precision dropped from 90.14 to 85.53, this trade-off resulted in a substantial reduction"}, {"title": "B. Trajectory Prediction", "content": "For trajectory prediction, three deep-learning architectures are evaluated, each differing in their ability to capture temporal dependencies and interaction dynamics.\n1) Predictors:\n\u2022 LSTM-based models assume that motion follows tem- porally causal dependencies, making recurrent networks a common choice for trajectory prediction. By leveraging hidden states to incorporate past information, LSTMs capture dynamic motion properties and temporal corre- lations in prediction sequences.\n\u2022 Transformer-based models employ self-attention lay- ers within an encoder-decoder architecture to capture dependencies comprehensively. The encoder processes input trajectories with positional encodings to preserve temporal order, while self-attention mechanisms com- pute pairwise relationships across timesteps. We adopt the Transformer configuration proposed by [64], which integrates encoder-decoder Transformer layers to model sequential dependencies from observed trajectories, com- bined with a Mixture Density Network (MDN) to esti- mate Gaussian Mixture Model (GMM) parameters. For generating a deterministic unimodal trajectory, the mean (\u03bc) of the mixture component with the highest mixing coefficient (\u03c0) at each timestep is selected. For multi- modal predictions, trajectory points can either be sampled from the mixtures or taken directly as their mean values (\u03bc), considering only mixtures with mixing coefficients \u03c0 > \u03c4. The trajectory module then generates paths by connecting these points across time steps, where each point at time t + 1 is connected to its closest neighbor at time t.\n\u2022 Graph Neural Networks (GNNs) model complex spatial and temporal interactions by operating on graphs con- structed from each scene, where nodes represent traffic agents and edges encode pairwise relationships, such as proximity. Each node contains information about an agent's past trajectory. We evaluate two message-passing strategies: Graph Convolutional Networks (GCNs), which uniformly aggregate information from neighboring nodes, and Graph Attention Networks (GATs), which dynamically compute attention weights to prioritize more influential interactions. Both GNNs are tested in two configurations: (1) sequence-to-sequence, where the past trajectory is flattened and directly used for future trajec- tory prediction, and (2) temporal, where a GNN encoder first captures spatial interactions, followed by an LSTM encoder-decoder to model temporal dynamics and gener- ate future trajectories in an autoregressive manner.\n2) Evaluation Protocol: Each predictor is trained on ground truth past trajectories. To compare performance, we compute the average and final displacement errors for each prediction setting in a unimodal mode, as well as for the Transformer coupled with the GMM model in a multimodal output set- ting. Let it and it denote the predicted and ground truth trajectories for the i-th agent at time step t, respectively.\n\u2022 Average Displacement Error (ADE) measures the average l2 distance between predicted and ground truth trajecto- ries over the entire prediction horizon At:\n$ADE = \\frac{1}{n \\times A_t} \\sum_{i=1}^{n} \\sum_{t=1}^{A_t} || S_{i,t} - G_{i,t} ||$\nFor multimodal predictions:\n$MinADE_k = \\frac{1}{n \\times A_t} min_k \\sum_{i=1}^{n} \\sum_{t=1}^{A_t} || S_{i,t} - G_{i,t} ||$\n\u2022 Final Displacement Error (FDE) measures the average l2 distance between predicted and ground truth trajectories at the final time step At:\n$FDE = \\frac{1}{n} \\sum_{i=1} || S_{i,A_t} - G_{i,A_t} ||$\nFor multimodal predictions:\n$MinFDE_K = \\frac{1}{n} \\sum_{i=1} min_k ||S_{i,A_t} \u2013 G_{i,A_t} ||$\nFor multimodal evaluation, the metric computes the 12 distance between the ground truth trajectory and the closest prediction among k possible trajectories, with k set to 5."}, {"title": "C. Intention Prediction", "content": "For intention prediction, the experiments involve providing the model with a past trajectory and generating future inten- tions at each timestamp over a prediction horizon of 10. The core evaluation model is an LSTM trained in a manner to classify and predict the most probable intention from a set of 11 classes. The model is evaluated in two settings: vanilla, where predicted intentions are assumed to be independent, and autoregressive, where dependencies between consecutive pre- dictions are considered. For instance, if a vehicle has stopped at a junction, it is less likely to change lanes immediately upon resuming movement; instead, it is more probable that it will continue with a lane-keeping intention. Similarly, for pedestrians, a \"waiting to cross\" intention is likely to be followed by \"crossing\".\n1) Predictors:\n\u2022 LSTM model corresponds to the vanilla setting, where the decoder generates the entire sequence of future inten- tions in a single forward pass. Predictions are evaluated against the ground truth using CrossEntropyLoss.\n\u2022 LSTMII model operates in an autoregressive mode with the capability of teacher forcing during training."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "This paper introduced the Emirates Multi-Task (EMT) dataset, which includes tracking, trajectory prediction, and intention prediction datasets. Each dataset is accompanied by evaluation code and results from conducted experiments. For tracking, the fine-tuned detector significantly improves performance, reinforcing the importance of using region- specific data for effective adaptation of deep learning algo- rithms in autonomous vehicle applications. This highlights the necessity of fine-tuning models on local data to achieve optimal performance. For trajectory prediction, we evaluate both sequential and interaction-aware models, providing train- ing pipelines for sequential and frame-based learning. The reported benchmark results serve as a reference for future research. For intention prediction, two evaluation settings are provided: cross-validation and train/test splits. We report cross-validation results as a benchmark, while the train/test setting is intended for assessing model generalization and performance on previously unseen intention classes. The EMT dataset is designed to facilitate the evaluation of models on the unique characteristics of the Gulf region, offering a valuable resource for advancing research in autonomous driving.\nThe evaluated algorithms operate solely on relative distances computed from absolute positions extracted from images. We leave it to the community to experiment with and design models that integrate visual cues for tracking and prediction. Additionally, the EMT dataset includes numerous highway scenarios that reflect regional traffic conditions. Balancing the dataset to address underrepresented patterns and improving model generalizability, considering the diversity gap between training and testing datasets, is left for researchers to explore.\nThe current dataset represents the first version collected from the region using a frontal camera. The primary direction for future work is to integrate Sim2Real scene generation to expand the dataset by including underrepresented scenarios. Additionally, we plan to collect a multimodal dataset incor- porating LiDAR, camera data, and localization information. This dataset will address underrepresented scenarios identified during the analysis of the EMT dataset, ensuring a more com- prehensive resource for the safe deployment of autonomous vehicles in the Gulf region. The dataset will also be sufficiently large to support large model training. For evaluation, future work will involve cross-evaluation by training models on multiple existing datasets and evaluating them on regional data. This process will incrementally include fine-tuning sam- ples to assess generalization and model performance on rare scenarios."}]}