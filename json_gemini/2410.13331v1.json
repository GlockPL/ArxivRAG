{"title": "IMPROVING DISCRETE OPTIMISATION VIA DECOUPLED STRAIGHT-THROUGH GUMBEL-SOFTMAX", "authors": ["Rushi Shah", "Michael Curtis Mozer", "Mingyuan Yan", "Dianbo Liu"], "abstract": "Discrete representations play a crucial role in many deep learning architectures, yet their non-differentiable nature poses significant challenges for gradient-based optimization. To address this issue, various gradient estimators have been developed, including the Straight-Through Gumbel-Softmax (ST-GS) estimator, which combines the Straight-Through Estimator (STE) and the Gumbel-based reparameterization trick. However, the performance of ST-GS is highly sensitive to temperature, with its selection often compromising gradient fidelity. In this work, we propose a simple yet effective extension to ST-GS by employing decoupled temperatures for forward and backward passes, which we refer to as Decoupled ST-GS. We show that our approach significantly enhances the original ST-GS through extensive experiments across multiple tasks and datasets. We further investigate the impact of our method on gradient fidelity from multiple perspectives, including the gradient gap and the bias-variance trade-off of estimated gradients. Our findings contribute to the ongoing effort to improve discrete optimization in deep learning, offering a practical solution that balances simplicity and effectiveness.", "sections": [{"title": "1 INTRODUCTION", "content": "Discrete representations have been widely used as a powerful tool in deep learning, offering several advantages compared to their continuous counterparts. These representations can lead to more efficient data compression (Ball\u00e9 et al., 2017; Toderici et al., 2017) as well as improved interpretability in comparison to their continuous counterparts (Vahdat & Kautz, 2020). Their advantage is that the structure of discrete variables often aligns with categorical semantics, making them easier to interpret in high-level tasks like image synthesis or reinforcement learning (Van Den Oord et al., 2017). Furthermore, discrete representations can serve as useful inductive bias, enhancing systematic generalisation across various architectural paradigms (Liu et al., 2021).\n\nThe use of discrete latents is prominent across a variety of applications. Notable examples include discrete latent variable models such as Vector Quantized Variational Autoencoders (VQ-VAEs) (Van Den Oord et al., 2017), categorical and Bernoulli VAEs (Jang et al., 2016), and hard attention mechanisms (Xu, 2015). In reinforcement learning, discrete actions are naturally used for action selection policies (Mnih et al., 2015). Other critical applications include neural architecture search (Liu et al., 2018), and model quantization for efficiency (Han et al., 2015).\n\nDespite their advantages, training models with discrete latent variables poses a significant challenge: non-differentiability. In deep learning, optimization is typically performed via gradient-based methods, which require differentiable operations to propagate error signals during backpropagation (Rumelhart et al., 1986). The discrete nature of latent variables, however, breaks this smoothness, making traditional gradient descent inapplicable. This challenge has spurred considerable research into techniques for bypassing the non-differentiability of discrete variables during optimization."}, {"title": "2 RELATED WORKS", "content": "Discrete optimization in deep learning has garnered significant attention due to its applicability in various tasks such as data compression, generative modelling, and reinforcement learning (RL). A common challenge in training discrete models is the non-differentiable nature of categorical variables, which obstructs the use of standard gradient-based optimization methods. To address this, three major approaches have been proposed: policy-based estimators, relaxation-based methods, and the Straight-Through Estimator (STE).\n\nPolicy-based estimators, such as the REINFORCE algorithm (Williams, 2004), provide a Monte Carlo-based gradient estimation for discrete variables. However, these methods often suffer from high variance, making them challenging to apply effectively in large-scale models. Previous works"}, {"title": "3 PRELIMINARIES", "content": ""}, {"title": "3.1 CATEGORICAL LATENT VARIABLE SETUP", "content": "We consider the problem of modelling categorical latent variables in a typical encoder-decoder setting. Let \u00e6 denote the input data, and z represent the latent variable, which is categorical. The encoder Eo(x) maps the input \u00e6 to a latent representation, while the decoder D\u00f8(z) reconstructs the input from the latent space.\n\nThe encoder outputs unnormalized logits, l = {11, 12, ..., lk}, where li corresponds to the unnormalized log-probability of the i-th category for a categorical variable with k possible categories. The logits are transformed into probabilities p = {P1, P2, ..., Pk } using the softmax operation:\n\n$p_i = \\frac{exp(l_i)}{\\sum_{j=1}^{k}exp(l_j)}\\ \\forall i \\in {1,2,...,k}.$\n\nNext, the categorical latent variable z is sampled from this distribution, i.e., z ~ Categorical(p). Formally, this involves selecting one of the k categories with probability pi for the i-th category. The resulting latent variable z can be represented as a one-hot vector:\n\n$z = [z_1, z_2,..., z_k]\\ \\ z_i \\in {0,1}\\ \\forall i \\in {1,2,..., k}\\ \\ \\sum_{i=1}^{k} i = 1.$\n\nIn this step, transitioning from probabilities p to the one-hot vector z is non-differentiable (Figure 1b). This presents a key challenge in training the model, as gradients cannot be directly propagated through the discrete sampling operation. The problem arises when attempting to backpropagate through the one-hot vector z, which introduces a discontinuity in the gradient flow. Specifically, the gradient \u018fL/dl, where L is the loss function, cannot be directly computed due to the discrete nature of z. This, in turn, obstructs the calculation of DL/80, which is necessary for gradient descent updates.\n\nTo address this issue, several estimators have been developed to allow gradient-based optimization with discrete variables, including the Straight-Through Estimator (STE) and the Gumbel-Softmax Estimator. We now describe these in detail."}, {"title": "3.2 STRAIGHT-THROUGH ESTIMATOR (STE)", "content": "The Straight-Through Estimator (STE) (Bengio et al., 2013) offers a simple workaround to the non-differentiability of discrete variables by treating the discrete sampling operation as identity during the backward pass. Once the categorical variable z is sampled as a one-hot vector, STE bypasses the sampling operation in the backward pass, allowing gradients to propagate through the logits l (Figure 1c).\n\nFormally, let z be the one-hot vector computed by sampling from p. In the forward pass, z is used directly:\n\n$z = one\\text{-}hot(p).$\n\nHowever, during the backward pass, instead of differentiating through the one-hot vector z, STE bypasses this step by using the softmax probabilities p directly for gradient calculation:\n\n$\\frac{\\partial L}{\\partial l} = \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial p} \\frac{\\partial p}{\\partial l}$\n\n$\\frac{\\partial L}{\\partial l} = \\frac{\\partial L}{\\partial p} \\frac{\\partial p}{\\partial l}\\ \\ (since \\frac{\\partial z}{\\partial p} \\approx 1)$\n\nIn other words, the gradients are computed as if the sampling step were differentiable. This approximation allows gradients to propagate through the logits I via the softmax function, enabling gradient-based optimization despite the non-differentiability of the one-hot vector z."}, {"title": "3.3 GUMBEL RELAXATION-BASED ESTIMATORS", "content": "The Gumbel-Softmax estimator (Jang et al., 2016) provides a differentiable approximation to sampling from a categorical distribution. The core idea is to replace the non-differentiable sampling step with a differentiable softmax function applied to logits perturbed by Gumbel noise during training. Specifically, let g be a vector of k independent samples from the Gumbel(0, 1) distribution. The"}, {"title": "4 METHODOLOGY", "content": "In this section, we discuss the limitation of using a single temperature in ST-GS and describe our proposed method, Decoupled ST-GS, which improves the original ST-GS by introducing separate independent temperature parameters for the forward and backward passes. Our approach is designed to increase gradient fidelity and performance by addressing the trade-offs inherent in the selection of a single temperature in the standard ST-GS estimator."}, {"title": "4.1 RETHINKING A SINGLE TEMPERATURE", "content": "The traditional ST-GS estimator relies on a single temperature parameter to control the smoothness of the relaxation in both the forward and backward passes. However, we argue that using a single temperature throughout the training process, even after performing a comprehensive hyperparameter search, cannot adequately capture the asymmetry between the forward and backward operations of the model. This limitation stems from the distinct roles played by the encoder and decoder: the encoder maps continuous inputs into discrete latent variables, while the decoder reconstructs continuous outputs from these discrete representations. Given this fundamental asymmetry, the gradients flowing through the encoder and decoder should be handled differently to optimize performance.\n\nSecondly, in the standard ST-GS estimator, a single temperature controls the sharpness of the categorical distribution in both the forward and backward passes. This shared temperature often presents a trade-off between sharpness and gradient fidelity. A low temperature 7 sharpens the categorical distribution, leading to a discrete-like sampling process, but can introduce high variance in the gradients. Conversely, a higher 7 smooths the distribution and reduces gradient variance, but"}, {"title": "4.2 DECOUPLING FORWARD AND BACKWARD TEMPERATURES", "content": "The core idea behind the Decoupled ST-GS approach is to use two distinct temperature parameters: Tf for the forward pass and th for the backward pass (Figure le). The forward temperature Tf governs the sharpness of the categorical distribution used when sampling the discrete latent variable, whereas the backward temperature \u03c4\u03bf controls the smoothness of the gradient approximation with respect to the logits."}, {"title": "4.2.1 FORWARD PASS: SAMPLING WITH TEMPERATURE T", "content": "The forward pass in our method follows the standard ST-GS process. Given the unnormalized logits I from the encoder, we compute the forward Gumbel-Softmax samples as:\n\n$\\tilde{z}^f = softmax\\left(\\frac{l + g}{\\tau^f}\\right)$\n\nwhere g represents the Gumbel noise sampled from the Gumbel(0, 1) distribution, and rf is the forward temperature.\u00b9 The forward pass then proceeds by discretizing the relaxed distribution using the argmax function to obtain a one-hot representation:\n\n$z = one\\text{-}hot(arg\\ max \\tilde{z}^f)$\n\nHere, Tf controls the sharpness of the categorical distribution. Lower values of rf produce sharper, more discrete samples, closely resembling the one-hot vector sampled from the original categorical distribution. In contrast, higher values of rf generate softer samples, which can still capture some uncertainty in the categorical assignment.\n\nAt this stage, the forward pass is identical to the ST-GS estimator: it uses the Gumbel-Max trick to sample discrete latent variables for use by the decoder. However, in contrast to ST-GS, we introduce a different temperature for the backward pass."}, {"title": "4.2.2 BACKWARD PASS: GRADIENT APPROXIMATION WITH TEMPERATURE T", "content": "During the backward pass, instead of using the same forward temperature for gradient calculation, we employ a distinct backward temperature \u03c4\u1f78 for the Gumbel-Softmax relaxation. This is critical for ensuring that the gradient estimates properly account for the encoder-decoder asymmetry without being overly constrained by the same temperature as used in the forward pass.\n\nAs in ST-GS, the backward pass operates by approximating the gradients using a Gumbel-Softmax relaxation. However, unlike the vanilla ST-GS, our method modifies the temperature used for this approximation, calculating the gradient with a relaxed Gumbel-Softmax sample 2b as follows:\n\n$\\tilde{z}^b = softmax\\left(\\frac{l + g}{\\tau^b}\\right)$\n\nThe gradient computation then proceeds similarly to the standard STE process, as explained in the preliminaries. Specifically, during backpropagation, we compute the gradients by treating the one-hot sampled vector z from the forward pass as if it were differentiable while using the relaxed b for gradient estimation:\n\n$\\frac{\\partial L}{\\partial l} = \\frac{\\partial \\tilde{z}^b}{\\partial l} \\frac{\\partial z}{\\partial \\tilde{z}^b} \\frac{\\partial L}{\\partial z}$"}, {"title": "4.2.3 OVERALL PROCESS", "content": "To clarify our approach, we provide a simplified pseudocode outlining the forward and backward passes of the proposed Decoupled ST-GS method.\n\nIn this pseudocode, we illustrate the use of different temperatures for the forward and backward passes. During the forward pass, the forward temperature rf is used to sample a one-hot vector from the Gumbel-Softmax distribution, while during the backward pass, the backward temperature \u03c4\u03bf controls the smoothness of the gradients.\n\nBy allowing the use of different temperatures, we can balance between the need for discreteness during the forward pass and smooth gradient approximations in the backward pass, tailoring the process to the encoder and decoder's respective roles. Through extensive experimentation, we show that this decoupled temperature approach leads to significant performance improvements and better gradient fidelity."}, {"title": "5 EXPERIMENTS AND RESULTS", "content": ""}, {"title": "5.1 PERFORMANCE ANALYSIS", "content": ""}, {"title": "5.1.1 RECONSTRUCTION TASK", "content": "We first evaluate our method on a standard reconstruction task using binary autoencoders across three datasets: MNIST (Deng, 2012), CIFAR10 (Krizhevsky, 2009), and SVHN (Netzer et al., 2011). The architecture used for this experiment consists of a convolutional encoder and decoder with residual connections (He et al., 2016) and a latent space dimension of 8 \u00d7 8 \u00d7 32. Each model was trained for 100 epochs with a batch size of 64.\n\nThe first set of results in Fig. 2 show the heatmaps of the validation loss for different datasets across various forward (rf) and backward (76) temperature settings, averaged over 5 seeds. The plots highlight that configurations with higher backward temperatures and lower forward temperatures yielded the best reconstruction performance. Specifically, performance plateaued when Tf was set to 0.3 and \u03c4\u03bf reached 3. Beyond these values, further improvements in reconstruction loss were minimal, suggesting the optimal temperature range is constrained within these bounds.\n\nWhile the heatmap provides visual insight into the effective temperature ranges, we further visualise these results using line plots, shown in the second row of Fig. 2. Notably, when only one temperature was tuned, performance improvements were relatively minor, as seen in the red markers on the line plot (which correspond to the heatmap's diagonal elements). This supports our hypothesis that a"}, {"title": "5.1.2 GENERATIVE MODELLING", "content": "Next, we evaluated our method in a generative modelling task using categorical variational autoencoders (VAEs) (Kingma, 2013) on MNIST. Two experimental setups were tested: one with 8 categorical dimensions and 4 latent dimensions and another with 16 categorical dimensions and 12 latent dimensions. Both models used a 3-layer MLP encoder and decoder and were trained for 160 epochs using 10 different seeds.\n\nAs in the reconstruction experiment, we varied the forward and backward temperatures across a grid of values. The resulting heatmaps and line plots (Figures 3 and 4) show the performance trends. In both setups, the best generative performance occurred in the lower triangle of the heatmap, where \u03c4f > \u03c4. Similar to the reconstruction task, tuning a single temperature led to limited improvements, as illustrated by the red markers in the line plot. These results emphasize the importance of independently optimizing both forward and backward temperatures for achieving optimal generative performance."}, {"title": "5.2 GRADIENT FIDELITY ANALYSIS", "content": ""}, {"title": "5.2.1 GRADIENT GAP", "content": "In the binary autoencoder experiments, due to the large latent space, computing exact gradients was computationally prohibitive (see appendix C for details about exact gradient calculation). Therefore, we used a proxy metric called the gradient gap (Huh et al., 2023) G to assess the fidelity of the gradients produced by the Gumbel-Softmax relaxation. The gradient gap measures the squared L2 norm of the difference between the gradients computed from forward passes using continuous relaxations and discrete samples (see appendix D for more details)."}, {"title": "5.2.2 BIAS-VARIANCE ANALYSIS", "content": "For the 8 \u00d7 4 categorical VAE, we conducted an exact gradient analysis, allowing for a detailed bias and variance study. Specifically, we calculated the bias of the estimated gradients by comparing them to the exact gradients obtained during the categorical VAE training process. These exact gradients were computed by considering all possible hidden states, calculating the error for each, and then computing the expected loss weighted by the probability of each configuration being selected (see appendix C for more details). This analysis provided insights into the accuracy of the gradient estimates generated by the Gumbel-Softmax relaxation.\n\nIn this experiment, we focused on the optimal configuration derived from the performance results: a forward temperature of Tf = 1.6 and a backward temperature of th = 1.3. We varied one temperature while keeping the other fixed and plotted the resulting bias and variance of the gradients. Figure 6 shows that increasing the backward temperature reduced both the bias and variance of the gradient estimates, indicating that the backward temperature has a stabilizing effect on gradient estimation, reducing noise and improving gradient reliability.\n\nConversely, increasing the forward temperature while holding the backward temperature constant led to an increase in both bias and variance, suggesting that higher forward temperatures introduce more variability into the gradient estimates. These findings highlight that forward and backward temperatures have distinct, complementary effects on gradient fidelity, reinforcing the need to tune each temperature separately."}, {"title": "6 CONCLUSION", "content": "In this work, we introduced the Decoupled ST-GS estimator, a simple but effective extension of the ST-GS method that allows for distinct temperature values in the forward and backward passes. Our approach addresses the limitation of using a single temperature, which often compromises perfor-"}, {"title": "A EXPERIMENTAL SETUP DETAILS", "content": ""}, {"title": "A.1 BINARY AUTOENCODER ARCHITECTURE", "content": "For the binary autoencoder experiments, we used a convolutional encoder and decoder with residual connections. The encoder consists of several convolutional layers which project the data into a latent space of dimension 8 \u00d7 8 \u00d7 32. The decoder mirrors this architecture, upsampling the latent space back to the input dimensionality.\n\nWe used the MNIST, CIFAR10, and SVHN datasets, without any form of input normalisation. All models were trained for 100 epochs with a batch size of 64, and we applied the Adam optimizer with a learning rate of 3 \u00d7 10\u20134. Each experiment was carried out with 5 different seeds."}, {"title": "A.2 CATEGORICAL VAE ARCHITECTURE", "content": "For the categorical VAE experiments, we employed a 3-layer MLP encoder and decoder. The encoder architecture consists of layers projecting the input from 784 dimensions (flattened images) down to 512, then to 256, and finally to the categorical latent space with dimensions 8 \u00d7 4 or 16 x 12. The decoder mirrors this structure, projecting from the latent space back to the original dimensionality.\n\nWe trained the models for 160 epochs using the RAdam optimizer without any input normalisation on MNIST. Two experimental setups were tested: 1) 8 categorical dimensions \u00d7 4 latent dimensions with a learning rate of 5 \u00d7 10-4. 2) 16 categorical dimensions \u00d7 12 latent dimensions with a learning rate of 7 \u00d7 10-4. Each experiment was carried out with 10 different seeds."}, {"title": "A.3 TEMPERATURE TUNING PROCESS", "content": "The forward (7f) and backward (76) temperatures were tuned by conducting grid searches over a set of predefined values. For the binary autoencoder experiments, rf was varied between 0.3 and 3.0, while th was varied between 0.3 and 6.0. For the categorical VAE, we experimented with values of Tf and th ranging from 0.3 to 3.0.\n\nThrough the heatmap visualizations (see Figures 2, 3 and 4 in the main text), it became clear that distinct temperature configurations were crucial for performance optimization and that symmetric temperature settings (i.e., \u03c4f = \u03c46) led to suboptimal results in both reconstruction and generative tasks."}, {"title": "B TEMPERATURE SCHEDULING EXPERIMENTS", "content": "In an effort to explore whether temperature scheduling could further improve performance, we conducted a series of experiments where forward (rf) and backward (76) temperatures were dynamically adjusted during training. The motivation for this approach stemmed from the observation that a fixed temperature combination of rf = 0.3 and T = 3 yielded the best performance in the CIFAR10 reconstruction task. We hypothesized that starting or ending with this combination, while varying the temperatures over time, might lead to additional improvements in reconstruction quality.\n\nIn conclusion, while temperature scheduling offered a flexible way to explore various configurations, it did not provide a meaningful improvement over the fixed best-performing setting. Therefore, we recommend using fixed temperatures for this particular task, as they offer simpler implementation without sacrificing performance."}, {"title": "C BIAS AND VARIANCE COMPUTATION", "content": "For the categorical VAE, it was possible to perform exact gradient analysis by directly computing the gradient of the exact categorical log-likelihood during training. The bias and variance of the estimated gradients were calculated by comparing the approximate gradient (obtained from the Gumbel-Softmax relaxation) to this exact gradient.\n\nThe exact gradient was computed by iterating over all possible configurations of the categorical latent variables and calculating their expectation. This allows for a precise reference point to which we can compare the approximate gradients.\n\nWe conducted this analysis by tracking the gradient of the encoder output across multiple forward and backward temperature configurations. The approximate gradient was calculated 1024 times using different random Gumbel noise draws, and its mean and standard deviation were computed. The bias was then defined as the difference between the exact gradient and the mean of the approximate gradients, normalized by the exact gradient:\n\n$Relative\\ Bias\\ Ratio = \\frac{Exact\\ Gradient \u2013 Mean\\ Approximate\\ Gradient}{Exact\\ Gradient}$\n\nThe variance was computed as the standard deviation of the approximate gradients. These metrics were visualized in a grid of temperature settings, and it was observed that increasing the backward temperature rh led to reduced bias and variance, while increasing rf resulted in higher bias and variance (see Figure 6)."}, {"title": "D GRADIENT GAP CALCULATION", "content": "Given the computational limitations in performing exact gradient analysis for the binary autoencoder's large latent space, we instead computed the gradient gap, denoted by G. The gradient gap is defined as the squared L2 norm of the difference between the gradients obtained from continuous relaxations (2f) and the gradients from discrete samples (z):\n\n$G =  $\n\nThis measure helps quantify the alignment between the gradients obtained from the Gumbel-Softmax relaxation and the true discrete gradients, particularly in cases where exact gradient computation is intractable. The backward temperature \u03c4\u03bf plays a critical role in reducing this gap, as shown in the main results (Figure 5)."}, {"title": "E COMPUTATIONAL RESOURCES", "content": "All experiments were conducted on a machine with an NVIDIA A100/4090 GPU and 40GB of RAM. Due to the computational demands of tuning both forward and backward temperatures, experiments were parallelized across multiple GPU cores where possible."}]}