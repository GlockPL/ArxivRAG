{"title": "Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages \u2013 A Singlish Case Study", "authors": ["Isaac Lim", "Shaun Khoo", "Watson Chua", "Goh Jiayi", "Jessica Foo"], "abstract": "To ensure safe usage, Large Language Models (LLMs) typically undergo alignment with human-defined values. However, this alignment often relies on primarily English data and is biased towards Western-centric values, limiting its effectiveness in low-resource language settings. In this paper, we describe our approach for aligning SEA-Lion-v2.1-Instruct (a Llama3-8B variant) to minimize toxicity in Singlish, an English creole specific to Singapore. We find that supervised fine-tuning and Kahneman-Tversky Optimization (KTO) on paired and unpaired preferences is more sample efficient and yields significantly better results than Direct Preference Optimization (DPO). Our analysis reveals that DPO implicitly enforces a weaker safety objective than KTO, and that SFT complements KTO by improving training stability. Finally, we introduce a simple but novel modification to KTO, KTO-S, which improves training stability through better gradient exploitation. Overall, we present a general approach for safety alignment conducive to low-resource languages, successfully reducing toxicity by 99% on our Singlish benchmark, with gains generalizing to the broader TOXIGEN dataset while maintaining strong performance across standard LLM benchmarks.", "sections": [{"title": "Introduction", "content": "Despite the unprecedented language capabilities of Large Language Models (LLMs), they are fundamentally machine learning models that perform best when in-distribution. With mainstream adoption of LLMs, there is an urgent need to ensure safety from a more robust perspective that is sensitive to edge-cases. One key vulnerability is safety in low-resource language settings, where linguistic nuances and cultural context must be carefully navigated to ensure respectful interaction with users.\nWhile post-training explicitly aligns LLMs to ensure that responses are helpful and harmless (Bai et al., 2022a), these processes typically rely on primarily English data. For instance, non-English languages only comprise 3% of Llama 3's supervised fine-tuning (SFT) data (Grattafiori et al., 2024). Probing these weaknesses, several studies highlight implicit LLM biases towards Western values (Ryan et al., 2024; Durmus et al., 2024; Benkler et al., 2023) and greater jailbreaking vulnerability in non-English contexts (Shen et al., 2024; Yong et al., 2024).\nIn this paper, we explicitly tackle low-resource language safety by fine-tuning to minimize toxicity in the context of Singlish. Singlish, being an English creole unique to Singapore, has its own phonology, vocabulary, and grammar (Ningsih and Rahman, 2023), incorporating elements from Chinese, Malay, Tamil and Chinese dialects. Aside from fundamental differences between English and standard Singlish, emergent terms in online Singlish lexicon further complicate the safety landscape(Foo and Khoo, 2024). Our main contributions in this paper are as follows:\nA general approach for safety alignment conducive to low-resource English languages We perform the first LLM safety alignment for Singlish, a low-resource English language, using parameter-efficient fine-tuning (PEFT) on SEA-Lion-v2.1-Instruct, a Llama3-8B based model. Our best model, combining Supervised Fine-Tuning (SFT) and Kahneman-Tversky Optimization (KTO), is the first to demonstrate KTO's effectiveness in practical safety alignment scenarios over the popular alternative of Direct Preference Optimization (DPO). Our analysis reveals that KTO's compatibility with unpaired preferences makes it more sample-efficient and particularly suited for low-resource regimes. Additionally, KTO enables the incorporation of original model responses to safe prompts, ensuring strong preservation of model helpfulness. We demonstrate this through experiments using curated Singlish texts with synthetic responses, successfully reducing Singlish toxicity by 99% while preserving helpfulness. These improvements generalize to the TOXIGEN dataset and minimally impact LLM benchmark performance. Overall, we address a critical research gap by providing a reference approach for real-world safety alignment in low-resource English language settings.\nKTO-S We introduce a simple but novel modification to KTO, KTO-S, which modifies the KL divergence with a SIGN coefficient. Because KTO only includes the KL term for loss saturation (Ethayarajh et al., 2024), KTO-S enables better training stability due to better gradient exploitation, evinced by faster loss convergence, smaller KL divergence and stable gradient norms during training."}, {"title": "Related Work", "content": "LLM safety is a rapidly growing field, with existing work generally falling into three groups.\nWe broadly describe the first group as safety dynamics. These studies aim to understand internal alignment mechanisms and failure modes to design new metrics (Peng et al., 2024), jailbreaking methods (Arditi et al., 2024; Zhou et al., 2024a) or alignment algorithms (Wei et al., 2023; Zhou et al., 2024b).\nThe second group is red-teaming. These studies directly aim to design more effective jailbreaking strategies, which can generate red-teaming datasets for more robust safety alignment. Approaches include gradient-based methods (Zou et al., 2023), general white-box methods (Hartvigsen et al., 2022; Arditi et al., 2024) and discrete prompt-based methods (Perez et al., 2022; Mehrotra et al., 2024).\nThe third group is safety alignment. Safety alignment is preference alignment with the goal of steering LLMs towards safe behavior. However, literature on safety alignment is frequently baked into foundation model technical reports (OpenAI et al., 2024; Grattafiori et al., 2024; Team et al., 2024) or more focused on scalable data generation (Bai et al., 2022b). In turn, discussion of safety alignment methods are frequently descriptive rather than comparative, with limited work highlighting optimal approaches for safety alignment even though domain-specific alignment is often desirable. Furthermore, general safety alignment can differ from real world use-cases, where safety is more targeted and data is limited."}, {"title": "Preference Alignment", "content": "Pretraining develops fundamental language understanding in an LLM as a next-token predictor, while post-training ingrains instruction-following behavior. Post-training typically has two phases, SFT and preference alignment - in the latter, LLMs are trained on preference datasets to steer them towards human-preferred responses, based on dimensions like style, quality and safety (Ziegler et al., 2020; Bai et al., 2022a).\nEarlier approaches frame preference alignment as reinforcement learning from human feedback (RLHF), where LLMs are trained to maximize rewards based on a pretrained reward model using Proximal Policy Optimization (PPO). (Ziegler et al., 2020; Ouyang et al., 2022; Bai et al., 2022a). Closed-form approaches have since gained popularity, starting with DPO (Rafailov et al., 2024), which reformulates RLHF as a supervised-learning objective to maximize the likelihood of preferences. The success of DPO in training state-of-the-art models like Llama 3 (Grattafiori et al., 2024) has inspired a wave of similar methods (Pang et al., 2024; Ethayarajh et al., 2024; Xu et al., 2024a; Azar et al., 2023) and comparative studies (Xu et al., 2024b)."}, {"title": "Safety for Low-Resource Languages", "content": "We observe limited literature directly addressing LLM safety in low-resource languages. Yong et al. (2024) covers important ground by investigating simple low-resource language jailbreaking attacks. Shen et al. (2024) is the closest to our work, as they fine-tune Llama 2-7B on low-resource language translations of the HH-RLHF dataset. We emphasize the following differences which make our work complementary:\nThey compare SFT with PPO (no SFT), whereas we compare SFT, DPO and KTO. Our experiments include SFT+DPO and SFT+KTO, typical combinations for more effective post-training. In doing so, we seek to identify the best method for safety alignment.\nTheir experiments compare the official Llama 2-Chat-7B with Llama 2-7B (base model) fine-tuned on translations of HH-RLHF, whereas we fine-tune and compare only post-trained Llama 3 models. This more accurately represents real-world deployment cases where post-trained foundation models undergo further finetuning.\nThey use machine translations of HH-RLHF, whereas we use curated Singlish texts from online sources, ensuring our data closely reflects the safety landscape of intended deployment."}, {"title": "Preliminaries on Alignment", "content": "Let $x$ denote an input prompt, $y$ the corresponding response, and $\\pi(y|x)$ the response of an LLM, $\\pi$, given $x$.\nGiven $D_{SFT} = \\{(x'_{SFT}, y'_{SFT})\\}_i$, where $x_{SFT}$ is typically an instruction prompt and $y_{SFT}$ the correct response, a pretrained model is trained to minimize standard cross-entropy loss.\nAssume that preferences are drawn from a Bradley-Terry model (Bradley and Terry, 1952) with underlying reward model $r^*$. Given a preference dataset $D_{pref} = \\{(x^i, y^i_w, y^i_l)\\}$, where $y_w > y_l$, the probability that $y_w$ is preferred to $y_l$ is:\n$p^*(y_w > y_l|x) = \\sigma(r^*(x, y_w) - r^*(x,y_l))$\nIn RLHF, a reward model $r_\\phi$ is estimated by maximizing the log-likelihood of preferences as follows:\n$L_{RM}(r_\\phi) = E_{(x,y_w,y_l)\\sim D_{pref}} [log p^* (y_w > y_l|x)]$\nDenoting the LLM being trained as $\\pi_\\theta$ and a reference model $\\pi_{ref}$, the following RL objective is optimized using PPO:\n$\\max E_{x\\sim D_{pref}, y\\sim \\pi_\\theta} [r_\\phi(x,y)-\\beta D_{KL}(\\pi_\\theta(y|x) || \\pi_{ref}(y|x))]$\nDPO is a popular closed-form alternative to PPO that is more stable and efficient (Rafailov et al., 2024). The key insight from DPO is that LLMs can implicitly estimate rewards based on the difference in log-probabilities of $y_w$ and $y_l$. This enables the log-likelihood of preferences to be maximized directly without RL:\n$L_{DPO}(\\pi_\\theta, \\pi_{ref}) = -E_{(x,y_w,y_l)\\sim D_{pref}} [log \\sigma (\\beta (log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}))]$ Typically, both $\\pi_\\theta$ and $\\pi_{ref}$ are initialized from $\\pi_{SFT}$.\nInstead of maximizing the log-likelihood of preferences, KTO maximizes the utility of LLM generations (Ethayarajh et al., 2024). It proposes a loss function inspired by Prospect Theory, where the utility of a given response is concave in gains and convex in losses, and evaluated against a reference point $\\z\\_0$. Given a dataset $D_{KTO} = \\{(x^i, y^i, L^i)\\};$ where $L = I(y \\sim Y_{positive}|x)$ indicates if y is a positive or negative response:"}, {"title": "Model Choice", "content": "We perform safety alignment on SEA-Lion-v2.1-Instruct (SEA-Lion), a fine-tuned variant of Llama 3-8B-Instruct (Llama 3) optimized for Southeast Asian languages. SEA-Lion was chosen for its training distribution, which is closer to Singlish, though it has not undergone direct safety alignment."}, {"title": "Training Data", "content": "We perform safety alignment using SGToxicityPrompts, a proprietary dataset of toxic and benign Singlish texts curated by Foo and Khoo (2024). These texts were collected from HardwareZone's Eat-Drink-Man Woman forum and selected subreddits on Singapore, all of which contain both innocuous and extremely toxic Singlish content.\nFrom SGToxicityPrompts, we sampled 8,000 texts, split equally between safe and unsafe statements. We further augmented each text with a prompt template to simulate a conversational format. We manually designed 21 templates with the intention of potentially, but not always, eliciting harmful responses when paired with an unsafe prompt. Upon review, we removed 10 templates from the safe subset because they elicited unsafe content even on safe prompts."}, {"title": "Experiments", "content": "We perform PEFT using LoRA (Hu et al., 2021) with r = a = 128, which performed the best in our preliminary analysis (Appendix B.1). We trained on 25,000 samples for each experiment, with equal safe and unsafe prompts. For each experiment with the same method, we use the same dataset (e.g. all experiments that involve SFT use DSFT)."}, {"title": "Evaluation", "content": "We hold out 12,500 safe and 12,500 unsafe prompts to evaluate Singlish toxicity. We score model responses on toxicity using LionGuard 3, a Singlish-based toxicity classifier,4 and refusal using distilroberta-base-rejection-v15. We also employ simple prefix string matching on responses starting with \"I cannot\" or \"I can't\" to catch false negatives from distilroberta. We then calculate toxicity rate (TR), refusal rate (RR) and false positive rate (FPR) as follows:\nTR = # unsafe with unsafe response / # unsafe\nRR = # unsafe with refusal response / # unsafe\nFPR = # safe with refusal response / # safe\nTOXIGEN is a large-scale machine-generated dataset of toxic and benign statements towards 13 minority groups (Hartvigsen et al., 2022). We use TOXIGEN to evaluate if any safety improvements generalize without compromising prior alignment. Specifically, we use a subset of strong examples (Appendix A.4) from the TOXIGEN test set and evaluate model responses using TOXIGEN-HateBert\u00b9 to calculate toxicity rate as in SGToxicityPrompts.\nWe evaluate general LLM performance using Open LLM Leaderboard v2 tasks, which span instruction-following, reasoning, and knowledge application. Including these benchmarks ensures more robust evaluation, as we do not want safety alignment to come at significant cost to model helpfulness (Bai et al., 2022a). We use a similar configuration to the published leaderboard, and report normalized scores8 (Appendix B.2)."}, {"title": "Results", "content": "SFT delivers significant safety gains. We present our SGToxicityPrompts and TOXIGEN results in table 1. SFT alone yields tremendous improvements in safety performance. Relative to the original SEA-Lion, \u03c0SFT reduces toxicity from 50.5% to 9.8% and increases refusals from 9.3% to 98.5% on SGToxicityPrompts, with a similar reduction on TOXIGEN toxicity from 19.5% to 9.8%. While there is a modest increase in FPR, it remains relatively low at 1.2%. Notably, SFT significantly outperforms \u03c0\u03ba\u03c4\u039f and \u03c0\u03c1\u03a1\u039f. These findings suggest that with a high-quality dataset, SFT alone is a viable and effective approach for safety alignment."}, {"title": "Analysis", "content": "We apply KTO and DPO to \u03c0SFT, resulting in #SFT+KTO and SFT+DPO. Both approaches show improvements in toxicity and refusal rates, indicating that preference alignment algorithms can still induce meaningful learning following SFT. Notably, #SFT+KTO achieves the highest refusal rate of 99.6% on SGToxicityPrompts, representing a 99.5% improvement over SEA-Lion, while also further reducing FPR. Although #SFT+DPO improves TR, it introduces a sharp increase in FPR, suggesting reduced ability to distinguish between unsafe and benign content.\nRecall that DPO only works on Dunsafe, while KTO also supports Dsafe. To evaluate KTO and DPO on equal terms, we perform KTO on just Dunsafe. Similar to #SFT+DPO, #SFT+KTO(Dunsafe) shows improvements to TR but suffers from an even larger increase in FPR to 30.6%. These findings highlight KTO's primary advantage: the ability to integrate both paired and unpaired preferences. This enhanced sample efficiency, combined with compatibility with more diverse data, is particularly valuable in low-resource language contexts where high-quality samples and labels are scarce.\nOpen LLM Leaderboard v2 performance is summarized in Table 4, with raw scores provided in Appendix B.2. On average, safety alignment has a minimal impact on model performance. While an inherent trade-off exists between helpfulness and harmlessness (Bai et al., 2022a), our findings indicate that applying safety alignment to balanced, high-quality data using PEFT results in disproportionately significant safety improvements with negligible performance trade-offs."}, {"title": "Alignment Mechanisms", "content": "DPO only supports Dunsafe, where increasing the likelihood of yw and decreasing the likelihood of yl are complementary objectives, resulting in an implicitly simpler training objective. This is simple to illustrate: generating a rejection to an unsafe prompt necessarily means not generating a compliant response, which improves loss on Dunsafe. In contrast, KTO supports Dsafe, which enforces a more challenging training objective, since bias toward Dunsafe risks accidentally rejecting safe prompts. This is evident when comparing rewards and losses during training for SFT+KTO and #SFT+KTO(Dunsafe) (Fig 1). Rewards and loss converge faster for #SFT+KTO(Dunsafe), with notably higher rewards on negative (unsafe) prompts.\nWhile KTO achieves notable safety improvements, its performance significantly benefits from pairing with SFT. During training, \u03c0\u03ba\u03c4\u03bf exhibits a peculiar decrease in rewards on negative examples accompanied by a pronounced spike in KL divergence, while rewards on positive examples steadily increase (Fig. 2). We hypothesize that this KL spike forces the model to prioritize positive samples over negative ones, as positive samples have less saturated rewards, ultimately leading to underfitting. In contrast, \u03c0SFT+KTO benefits from an initial SFT pass, which results in a naturally lower KL divergence and a smaller KL spike during training. Furthermore, the prioritization of positive and negative samples is reversed, offering additional evidence that \u03c0\u03ba\u03c4\u03bf underfits without SFT."}, {"title": "KTO-S", "content": "Seeking to address the instability observed in KTO, we introduce a simple modification to the KL divergence term, which we name KTO-S.\nThe design of KTO-S is motivated by KTO's sigmoid loss function (Eq 1), where gradients are flattest at the extremes. In KTO, the KL divergence is included solely for loss saturation (Ethayarajh et al., 2024), making it effectively a batch-specific constant.\nConsider a scenario where two samples have equal reward, but one has a larger KL divergence than the other. Intuitively, the sample with smaller KL divergence is more desirable. However, because the KL term in KTO does not backpropagate, the sample with larger KL divergence has larger gradients. KTO-S resolves this by introducing a SIGN coefficient to the KL term, ensuring the value function saturates in the correct direction:\nv(x, y)=\\begin{cases}  \\\\lambda \\eta \\sigma (\\beta (r_{\\theta}(x, y) + S\\z_{0})) & \\text{ if } y \\sim Y_{positive}|x\\\\\\\\ \\lambda \\upsilon \\sigma (\\beta (-S\\z_{0} - r_{\\theta}(x, y))) & \\text{ if } y \\sim Y_{negative}|x\\end{cases}\\end{align*} \\newline \\newline S = SIGN(r(x, y))\n#SFT+KTO-S performs similar to #SFT+KTO on our benchmarks (Table 1), but also presents faster loss convergence and a smaller KL spike (Fig 3)."}, {"title": "Conclusion", "content": "While LLM safety is a rapidly growing field, there remains a lack of published work on optimal methods for safety alignment, particularly for low-resource languages. Through our experiments, we provide a reference framework for safety alignment conducive to low-resource English languages. Our findings demonstrate that a combination of SFT and KTO achieves superior performance to DPO while being more sample efficient. Additionally, we also emphasize the importance of including paired and unpaired preferences derived from model responses to safe prompts, ensuring model helpfulness is preserved. Finally, we introduce KTO-S, a novel modification to KTO that improves training stability by applying loss saturation more effectively."}, {"title": "Dataset Details", "content": "This section contains offensive language used solely for research purposes. Reader discretion is advised."}, {"title": "SGToxicity Prompts", "content": "Sample texts from SGToxicityPrompts are shown in Fig 4 and 5."}, {"title": "Prompt Templates", "content": "Because texts from SGToxicityPrompts are individual statements or comments, we designed 21 prompt templates to adapt them to a conversational format. Prompt templates were manually generated by our team, with each template designed to potentially elicit a harmful response if paired with a toxic prompt from SGToxicityPrompts. This was determined through manual testing. Sample prompt templates are shown in Fig 6."}, {"title": "Prompt Template Filtering", "content": "After designing the prompt templates, we generated SEA-Lion-v2.1-Instruct responses and scored them using LionGuard. Based on these scores, we further filtered out prompts from the non-toxic subset if they failed to produce a clear partition between safe and unsafe prompts. In other words, templates that disproportionately produced harmful responses even on safe prompts were filtered out from the safe subset. As a simple but strict heuristic, we removed prompt templates that did not have at least 80% of safe prompts below the LionGuard high recall threshold. This led us to drop templates [1, 6, 7, 8, 14, 15,16,17,19, 20] from the safe subset."}, {"title": "TOXIGEN", "content": "In the TOXIGEN paper, the training set of TOXIGEN is used to fine-tune a HateBert classifier, resulting in TOXIGEN-HateBert, which we use to score model responses. While there is no concern of leakage since we are scoring model responses, for avoidance of doubt we evaluate our models on the annotated test set. TOXIGEN includes human annotations for the following fields:\ntoxicity_ai: perceived hatefulness assuming the text was AI-generated\ntoxicity_human: perceived hatefulness assuming the text was written by a human\nTo ensure a higher quality evaluation set, we only use samples where either both toxicity_ai and toxicity_human are less than or equal to 2 (safe), or where both toxicity_ai and toxicity_human are more than or equal to 4 (unsafe). From an initial dataset size of 940, this results in a final dataset size of 740. Samples are shown in Fig 8 and 9."}, {"title": "Additional Experiment Details", "content": "We conduct initial experiments with SFT to determine the best LoRA rank to use. For simplicity, we set r = a for all experiments. Furthermore, we prioritize RR and FPR as defining metrics to select the best model."}, {"title": "Implementation", "content": "We evaluate Open LLM Leaderboard v2 performance using similar configurations outlined by Huggingface. Specifically, we use the same task subsets on 1m-evaluation-harness. However, due to bugs in implementing Huggingface's fork of lm-evaluation-harness, we use the main branch instead."}, {"title": "Normalization", "content": "We normalize scores using the same approach as Huggingface, where baseline performance is determined relative to each sub-task. For instance, if a sub-task is a multi-choice format with 4 options, the baseline performance is 25%. Using sub-task baselines, we perform min-max normalization so that a score of 0 implies zero advantage over random guessing, while 100 indicates a perfect score."}, {"title": "Raw Scores", "content": "We report raw scores on Open LLM Leaderboard v2 tasks in Fig 4."}]}