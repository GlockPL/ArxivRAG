{"title": "A Survey on Trustworthiness in Foundation Models for Medical Image Analysis", "authors": ["Congzhen Shi", "Ryan Rezai", "Jiaxi Yang", "Qi Dou", "Xiaoxiao Li"], "abstract": "The rapid advancement of foundation models in medical imaging represents a significant leap toward enhancing diagnostic accuracy and personalized treatment. However, the deployment of foundation models in healthcare necessitates a rigorous examination of their trustworthiness, encompassing privacy, robustness, reliability, explainability, and fairness. The current body of survey literature on foundation models in medical imaging reveals considerable gaps, particularly in the area of trustworthiness. Additionally, extant surveys on the trustworthiness of foundation models fail to address their specific variations and applications within the medical imaging domain. This survey paper reviews the current research on foundation models in the major medical imaging applications, with a focus on segmentation, medical report generation, medical question and answering (Q&A), and disease diagnosis, which includes trustworthiness discussion in their manuscripts. We explore the complex challenges of making foundation models for medical image analysis trustworthy, associated with each application, and summarize the current concerns and strategies to enhance trustworthiness. Furthermore, we explore the future promises of these models in revolutionizing patient care. Our analysis underscores the imperative for advancing towards trustworthy AI in medical image analysis, advocating for a balanced approach that fosters innovation while ensuring ethical and equitable healthcare delivery.", "sections": [{"title": "1. INTRODUCTION", "content": "With the advancement of foundational models, the field of medical image analysis is poised at the brink of a revolution. Foundation models are large-scale machine learning models trained on extensive and diverse datasets. Following their initial training, these models can be adapted to specific downstream tasks with minimal adjustments. By leveraging their extensive pre-training on large-scale datasets, they offer unprecedented analytical depth, enabling the analysis and prediction of medical images ranging from radiology to pathology. The integration of foundation models into medical image analysis holds the potential to enhance diagnostic accuracy, expedite treatment schedules, and ultimately enhance patient outcomes.\nThe integration of foundation models into medical image analysis garnered significant interest, leading to a surge of impactful studies in the field. Notably, several perspective papers have emerged, highlighting the potential and future directions of leveraging foundation models in the medical domain. Research in foundation models for medical imaging has been explored in many areas, for example, precise segmentation and detection of tumors [9], auto-organ segmentation [10], generating clinical reports [11], extracting quantitative features from medical images using deep learning to predict disease characteristics and outcomes [2], and medical Q&A systems [12].\nHowever, the deployment of foundation models in such a critical sector raises significant concerns regarding their trustworthiness, which encompasses privacy, robustness, reliability, explainability, and fairness (detailed definition see Sec 3). In medical contexts, where decisions have profound implications on patient health, ensuring the trustworthiness of foundation models becomes paramount. It involves rigorous validation against clinical standards, continuous monitoring for performance drift, and mechanisms to interpret model decisions transparently. To fill this gap, we review and discuss recent advances of trustworthiness in foundation models for medical image analysis. To the best of our knowledge, this is the first survey of foundation models for medical image analysis from the trustworthiness perspective, which is different from the existing surveys of foundation models for medical image analysis.\nComparison with existing surveys. We compare existing surveys that discuss similar topics on foundation models, trustworthiness, and particularly those focused on medical imaging. Our survey paper exhibits several distinct advantages over the existing literature, as illustrated in Table I. Existing survey papers on foundation models in medical imaging exhibit significant gaps, particularly in addressing trustworthiness issues. For instance, while Azad et al. [1], Zhao et al. [2], and He et al. [3] discussed foundation models for medical imaging, they fall short in providing detailed analysis on trustworthiness. Whereas, He et al. [4], Sun et al. [5], and Liu et al. [6] survey trustworthiness in foundation models but do not focus on medical imaging applications. Similarly, Salahuddin et al. [7] and Hasani et al. [8] focus on trustworthiness issues in medical imaging but do not cover the foundation models. Furthermore, most of the existing survey papers fail to provide a comprehensive examination of trustworthiness and detailed insights into medical imaging application-specific challenges and solutions. In contrast, our survey uniquely integrates an in-depth analysis of trustworthiness across both LLMs and Vision foundation models, emphasizing privacy, robustness, reliability, explainability, and fairness, alongside a comprehensive review of their applications in medical imaging. This holistic approach ensures a more detailed and multifaceted understanding of the current landscape and addresses the critical need for trustworthy AI in medical image analysis, offering valuable insights that are not as extensively covered in other surveys.\nContributions. The contributions of this paper can be highlighted in the following aspects. First, we identify the trustworthiness concerns associated with foundation models for medical image analysis, classifying these issues into five distinct categories: privacy, robustness, reliability, explainability, and fairness. Second, we review the existing literature and categorize them according to the applications of medical image analysis, including segmentation, clinical report generation, medical question and answering (Q&A), and disease diagnosis. Third, we remark on the trustworthiness issues in the existing literature, noting the significant gaps in addressing these concerns. In addition, we propose future research directions, emphasizing the need for innovative approaches to enhance model trustworthiness. This includes advancing privacy techniques, improving model robustness, and fostering fairness and explainability in foundation models.\nRoadmap. The roadmap of this paper is shown in Figure 1. The subsequent sections are organized as follows: In Section 2, we introduce the background and usage of foundation models for medical image analysis. In Section 3, we define five categories of trustworthiness issues. In Section 4, we categorize the existing literature of foundation models for medical image analysis based on their application, and then by the reported trustworthiness concerns. Finally, we conclude with challenges and future directions in Section 5 before presenting our conclusion in Section 6.\nTo provide a comprehensive overview, Figure 2 illustrates the landscape of foundation models for medical image analysis studied in this survey. We have covered 31 recent foundation models with applications in medical imaging from 72 research papers, among which 44 are research papers particularly developing for or adapting to medical imaging, published from 2019 to 2024. Given that the type of foundation models is intricately linked to specific medical imaging applications, our survey is systematically categorized according to the prevalent tasks in medical imaging. In the four medical imaging areas we focused on-disease diagnosis, medical Q&A, report generation, and segmentation each foundation model is associated with multiple medical applications, demonstrating their versatility and applicability across various medical imaging tasks. For instance, models like MedCLIP and ChatGPT are extensively used for disease diagnosis and medical Q&A, while models like SAM [13] and its variants are specialized for Segmentation. LLaMA and Vicuna [14] are used in both medical Q&A and report generation. GPTs [15], [16] are used in all three applications except segmentation. CLIP [17] and its variants are popular choices for medical Q&A and disease diagnosis. In this survey, we also investigate the taxonomy of using foundation models, categorized by training from scratch, fine-tuning, prompt-tuning, or direct off-the-shelf use and how they are linked to trustworthiness aspects. The overlapping and intersecting lines between the use of foundation models and trustworthiness issues indicate the multi-faceted challenges and considerations in deploying these models effectively."}, {"title": "2. BACKGROUND AND TAXONOMY", "content": "2.1 Definition of Foundation Models\nFoundation models refer to massive machine learning models trained on extensive volumes of diverse datasets, which can later be fine-tuned for specific downstream tasks with relatively minimal additional data (e.g., ViT [18]). These models (e.g., GPT [15], [16], SAM [13], CLIP [17]) serve as versatile platforms that can be adapted to various specific tasks, ranging from language translation and content creation to complex problem-solving in healthcare domains. The adaptability and efficiency of foundation models make them a cornerstone in the development of cutting-edge technologies, providing a robust basis for innovation and research in the field of medical image analysis.\n2.2 The Use of Foundation Models\nThere are four major approaches for training and using foundation models in medical image analysis: training from scratch, fine-tuning, prompt-tuning, and using off-the-shelf models.\n1) Training From Scratch: Foundation models for medical image analysis, often using the transformer architecture, are trained on large, diverse datasets. Some, like the Segment Anything Model (SAM) [13], are pre-trained with labeled medical images and paired segmentation masks. Others undergo self-supervised learning (SSL), such as generative SSL (e.g., MAE [19]) and contrastive SSL (e.g., DINO [20], SimCLR [21], CLIP [17]), where they learn generic data patterns without labeled examples. Masked autoencoders (MAEs) [19] involve pre-text training to learn image representations by treating images as sequences of patches, masking out certain patches, and predicting masked parts of the image, thereby teaching the model to understand visual features such as shapes and textures. Among the contrastive SSL strategies, a popular and advanced approach is contrastive language-image pre-training (CLIP) [17], which uses the pre-training task of predicting which caption goes with which image. After learning a useful generalized representation of visual features, models are fine-tuned on specific downstream tasks, like tumor detection, using a smaller labeled dataset, thereby tailoring their broad capabilities to precise medical diagnosis.\n2) Fine-tuning: In medical image analysis, fine-tuning foundation models on specific datasets significantly enhances their ability to detect diseases in medical images such as X-rays, MRI scans by adjusting model parameters to the peculiarities of medical images. Adopting full fine-tuning for medical foundation models involves adjusting all the parameters of a pre-trained model to tailor it to a specific task or dataset. However, this will lead to a computational burden. To address the expensive computation cost, parameter-efficient tuning, such as Lora [22] is proposed to adjust a small set of parameters in a pre-trained model to optimize it for new tasks with minimal computational overhead. This approach is particularly useful in medical image analysis, as most hospitals lack sufficient and powerful computational resources. Another objective for fine-tuning is to align with clinical insights, refine diagnostic accuracy and reliability. As a popular alignment approach, Reinforcement Learning with Human Feedback such as InstructGPT [23] can be leveraged to use medical experts' feedback for foundation model alignment. Similarly, AI-generated feedback (e.g., Self-Refine [24]) simulates expert reviews, allowing continuous improvement in interpreting medical images and identifying complex conditions, even without direct human input.\n3) Prompt-Tuning: Prompt tuning, or prompt engineering, enhances foundation models for specific tasks by crafting input prompts while keeping the model backbone frozen [25]. The terms \"prompt tuning\" and \"prompt engineering\" are often used interchangeably, although their focuses are different. Prompt tuning focuses on updating continuous embeddings optimized for specific tasks, while prompt engineering tries out different prompts to find the best ones. Despite these differences, both approaches are commonly employed together in practice to direct the model's pre-trained knowledge to meet the precise needs of the downstream task, sometimes outperforming full fine-tuning (e.g., VPT [26]). In medical image analysis, well-designed prompts are specific inputs, such as text descriptions that guide the medical foundation model to focus on important features or regions within medical images. For example, a well-designed textual prompt for a medical foundation model analyzing chest X-rays might be, \"Focus on the lower lobe of the right lung for potential infiltrates.\" This can significantly improve a model's diagnostic accuracy and provide a resource-efficient way to tailor models for medical diagnostics.\n4) Off-the-Shelf: Due to the impressive generalization abilities acquired from the huge amount of training data, many general-purpose foundation models can be applied directly to the medical image analysis domain without any modification. For instance, SAM [13] can perform remarkably when applied directly to medical image analysis tasks [27]. For LLM-based foundation models, off-the-shelf models are generally used to do in-context learning [15], which refers to providing demonstrations or examples to the LLM at inference time to enhance its performance."}, {"title": "3. UNPACKING TRUSTWORTHINESS IN FOUNDATION MODELS FOR MEDICAL IMAGE ANALYSIS", "content": "3.1 Privacy (T1)\nPrivacy in medical image analysis is critical as patients expect their healthcare service providers to follow necessary safety measures to safeguard their inherent right to the privacy of their information, such as age and sex. This privacy concern can also exist in medical foundation models due to the sensitivity of the health information involved in the extensive training dataset for medical foundation model training [28], [29]. With the increase in data used for training and the complexity of foundation models, there is a growing interest in exploring privacy issues in medical foundation models in both academic and industrial fields.\n3.2 Robustness (T2)\nRobustness in medical image analysis refers to the ability to maintain performance when faced with various uncertainties and adverse conditions. Similarly, foundation models for medical image analysis also face challenges related to robustness concerns. This includes handling errors or imbalance in input text prompts [10], being generalizable to data from various distributions [30], defending against adversarial attacks designed to mislead the model [31], and resisting attempts at data poisoning where malicious entities might attempt to influence the model's training data [32]. Thus, the emphasis on robustness in foundation models for medical image analysis is crucial given the sensitive nature of medical diagnostics and the potential impact on patient care, requiring that these models have not only good performance but also resilient to a wide range of potential vulnerabilities.\n3.3 Reliability (T3)\nHealthcare is commonly seen as a high-stakes field, where reliability is a foundational requirement. Firstly, it is common that some foundation models such as LLMs can provide untruthful answers or generate misleading information, which may cause significant consequences in some scenarios such as Medical Q&A [33]. This is particularly concerning in healthcare, where the accuracy of the information can directly impact patient outcomes [34]. Secondly, hallucination can also happen when LLM's confidence is miscalibrated. In a medical context, such overconfidence in wrong information can be dangerous. For instance, an LLM might generate a very confident but incorrect interpretation of a patient's symptoms, leading healthcare providers down the wrong diagnostic path [35].\n3.4 Explainability (T4)\nThe explainability of foundation models refers to the ability to understand and interpret how these models make decisions or generate outputs [36], [37]. This property becomes crucial in medical image analysis, especially when applying foundation models in healthcare, due to the demand for trustworthy and actionable decision-making in clinical settings. To be specific, one common example of explainability in medical image analysis is highlighting the regions of an image that the model considers most indicative of a particular diagnosis. Another example involves delineating the boundaries of different anatomical structures or pathological regions, allowing practitioners to understand why certain areas were identified as significant.\n3.5 Fairness (T5)\nFairness in foundation models refers to the equitable performance of the model across different demographic groups, ensuring that no group experiences significantly lower performance. Similarly, it is also significant to eliminate fairness issues when adapting these foundation models to medical image analysis fields [38]. Due to the common issue of under-representation or imbalance in medical data, fairness in foundation models for medical image analysis is particularly critical. Therefore, it is essential to ensure that these medial foundation models perform consistently well for all populations, mitigating disparate impacts and reducing performance gaps between groups to maintain fairness."}, {"title": "4. TRUSTWORTHINESS IN FOUNDATION MODELS FOR MEDICAL IMAGE ANALYSIS", "content": "4.1 Overview of the Literature\nTable II lists the publications reviewed in this study on the development of medical imaging foundation models", "13": ".", "9": "MedLSAM [46", "10": "UA-SAM [45", "40": "ScribblePrompt [42", "76": ".", "Segmentation": "Privacy concerns in foundation models for medical image segmentation represent a critical challenge"}, {"13": "which asserts that the 11 M natural image dataset they use is \u201cprivacy respecting\" - which they specify as meaning that faces and license plates are blurred out. Similarly", "9": "the authors leveraged the de-identified public medical images for model training. Federated learning is where model training is done across multiple protected data sources without sharing data [77", "40": "describes"}, {"40": "presents a framework for federated SAM fine-tuing across multiple clients"}, {"40": ".", "Segmentation": "Robustness in foundation models for medical image segmentation is crucial for consistent and accurate results across various conditions. It ensures reliable outcomes", "modalities": "SAMs still lack sufficient robustness on medical images to see widespread use in the medical domain", "cannot stably and accurately implement zero-shot segmentation on multimodal and multi-object medical datasets\\\" [27]. To address this, Zhang et al. introduces uncertainty estimation for both SAM and MedSAM by using the change in segmentation boundaries as a function of prompt augmentation to generate uncertainty maps [10]. They propose that incorporating uncertainty estimations into SAMs builds trust through better error identification. Jiang et al. notes that features in medical images may have ambiguous boundaries, in contrast to the clear boundaries of most natural images. They propose UA-SAM [45], which integrates a probabilistic model into SAM training and fine-tuning. In addition to fine-tuning the SAM for domains with inherent uncertainty, the adapter in UA-SAM makes the SAM non-deterministic by outputting multiple \\\"plausible\\\" masks for a single input.\nImages taken during surgery itself represent an important use case for segmentation. These kinds of images differ in image quality from the natural images used to train segmentation foundation models like SAM. Wang et al. qualitatively assesses the performance of a base SAM on mid-surgery images, and finds that image features like blur or reflections result in inaccurate segmentation [41]. This is concerning as those features would be found throughout the surgery process. Additionally, Wang et al. quantitatively tests the performance of SAM on mid-surgery images from the EndoVis17 image set [78], while imposing 18 different types of noise (brightness, defocus, saturation, JPEG compression etc.) with different severity levels [41]. Some image corruptions, like saturation, only result in a slight decrease in performance, while others, like JPEG compression, cause near total failure [41].\nb) Robustness to different prompt inputs": "As mentioned", "prompts": "point prompts and box prompts. Prompting gives information to the model on what parts of the image are useful and where boundaries are. This allows a user to contribute to delineating a boundary that the model may not initially detect. This has relevance in the context of medical image segmentation in that the low contrast of medical images (as opposed to the SAM training set of natural images) often requires prompting to get to acceptable performance levels for clinical tasks.\nLei et al. introduces MedLSAM", "46": ".", "10": "."}, {"10": ".", "44": ".", "76": "a method for human-computer interaction for SAMs. GazeSAM collects the eye-movement behaviour of users (radiologists)"}, {"76": ".", "43": "."}, {"43": ".", "42": "and text-based [79", "Segmentation": "As with base SAM itself", "9": "."}, {"9": "a customization of the SAM fine-tuned on over 1M medical image-mask pairs covering 15 image modalities", "39": "examines the performance of MedSAM on a kidney CT scan set and looks for disparities in performance depending on patient characteristics. They use Dice similarity scores as their performance metric. No statistically significant relationship between patient age and performance is found"}, {"39": "also find that images from female patients yield better performance than from male patients.\n4.3 Report Generation\nThe adoption of foundation models in the generation of medical reports represents a pivotal advancement", "Generation": "Medical applications inherently come with significant concerns regarding patient data privacy. To satisfy privacy requirements", "11": "de-identified the MIMIC-CXR dataset by removing patient information when training their model. To preserve the utility of the model trained on de-identified data", "____\\\", while preserving the original meaning of the reports.\n2) Robustness in Report Generation": "Medical report generation is a high-risk field with a low tolerance for errors", "47": "propose ChatRadio-Valuer", "48": "."}, {"47": "starts with training on radiology reports from one institution to acquire knowledge of particular patterns and representations", "49": ".", "80": "RO-LLaMA also enhances the model's robustness and generalization abilities when faced with noisy inputs by adding random noise to the embedding vectors during training [61", "Generation": "Inheriting the issue of general foundation models", "54": [51]}, {"54": "propose CXR-ReDonE to remove prior references from medical reports by using GPT-3 to rewrite the reports and using token classification to remove words referring to priors. Another study observe that when customizing general-purpose foundation models for medical report generation", "50": ".", "53": "find that their proposed method for chest X-ray report generation may hallucinate reports with nonexistent findings. To mitigate this issue", "55": "proposes to enhance the reliability of radiology report generation by injecting additional knowledge with the current image in the generation process. Specifically", "Generation": "Explainability is also a crucial concern in medical report generation. To improve the interpretability of report generation systems", "56": "propose a two-step approach for generating the findings section of a radiology report from an automated interpretation of chest X-ray images: (1) detecting the abnormalities within the image using bounding-boxes with probabilities", "57": "propose an Adaptive patch-word Matching (AdaMatch) model which matches texts from the report with adaptive patches in the medical image to provide explainability. It utilizes an Adaptive Patch extraction module to dynamically capture abnormal regions of varying sizes and positions. To further improve its explainability", "58": "propose a region-guided radiology report generation model that describes individual regions to form the final report. Specifically", "63": [33], "11": [81], "Q&A": "Robustness in medical foundation models for Q&A tasks requires being able to handle errors in the input text prompts as well as be resilient against adversarial and backdoor attacks in medical images. Though some medical foundation models achieve success in their model performance", "60": "proposes a prefix tuning method for open-ended medical VQA of LLMs for performance improvement", "61": "incorporates consistency regularization", "80": "which adds random noise to the embedding vectors during training. This approach enhances the model's robustness and generalization abilities when faced with noisy inputs. Additionally", "65": "and PubMedCLIP [62", "31": "which show that using a modest set of wrongly labeled data and introducing a \u201cBad-Distance\" between the embeddings of clean and poisoned data can lead to successful backdoor attacks.\n2) Reliability in Medical Q&A: The reliability of foundation models is a significant concern in Q&A domains", "82": ".", "83": "are designed for general foundation hallucination evaluation"}, {"82": ".", "hallucinations": "catastrophic", "84": ".", "64": "propose MedPromptX and leverage few-shot prompting (FP) to eliminate hallucination in the foundation model for chest X-ray Diagnosis by guiding the output.\nAnother concern in reliability is truthfulness. A recent study highlights that Medical Visual Question Answering (Med-VQA) systems may be unreliable for medical diagnosis questions and could produce misleading information when existing state-of-the-art foundation models", "85": ".", "63": ".", "33": "propose Chat-CAD+", "Q&A": "In high-risk medical environments"}, {"63": ".", "35": ".", "Q&A": "Under-representation of certain groups in the training data or training data imbalance can lead to significantly lower accuracy or effectiveness of the model for certain populations, leading to disparate performance and fairness issues [86", "59": "propose PMC-VQA, a large-scale medical VQA dataset with 227k VQA pairs of 149k images covering various modalities and diseases. Despite their efforts to construct a comprehensive MedVQA dataset, the authors note the potential presence of biases, which may arise from data collection, annotation (inconsistencies or subjective interpretations from human annotators), or the underlying distribution of the medical images and questions. Figure 3 shows the word clouds for the questions and answers in the training set of the PMC-VQA dataset. The imbalance in word frequencies may be an indication of potential biases in the dataset. For instance, for the word cloud of answers, \u201cX-ray\u201d, \u201cMRI\u201d, and \"CT scan\" are the most prevalent words, suggesting that models trained on this dataset may perform better on well-represented modalities like X-ray, CT, and MRI, and"}]}