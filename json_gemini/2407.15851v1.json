{"title": "A Survey on Trustworthiness in Foundation Models\nfor Medical Image Analysis", "authors": ["Congzhen Shi", "Ryan Rezai", "Jiaxi Yang", "Qi Dou", "Xiaoxiao Li"], "abstract": "The rapid advancement of foundation models in\nmedical imaging represents a significant leap toward enhancing\ndiagnostic accuracy and personalized treatment. However, the\ndeployment of foundation models in healthcare necessitates a\nrigorous examination of their trustworthiness, encompassing\nprivacy, robustness, reliability, explainability, and fairness. The\ncurrent body of survey literature on foundation models in\nmedical imaging reveals considerable gaps, particularly in the\narea of trustworthiness. Additionally, extant surveys on the\ntrustworthiness of foundation models fail to address their specific\nvariations and applications within the medical imaging domain.\nThis survey paper reviews the current research on foundation\nmodels in the major medical imaging applications, with a focus\non segmentation, medical report generation, medical question\nand answering (Q&A), and disease diagnosis, which includes\ntrustworthiness discussion in their manuscripts. We explore the\ncomplex challenges of making foundation models for medical\nimage analysis trustworthy, associated with each application,\nand summarize the current concerns and strategies to enhance\ntrustworthiness. Furthermore, we explore the future promises\nof these models in revolutionizing patient care. Our analysis\nunderscores the imperative for advancing towards trustworthy\nAI in medical image analysis, advocating for a balanced approach\nthat fosters innovation while ensuring ethical and equitable\nhealthcare delivery.\nIndex Terms-Foundation Model, Trustworthy AI, Medical\nImage Analysis.", "sections": [{"title": "1. INTRODUCTION", "content": "With the advancement of foundational models, the field\nof medical image analysis is poised at the brink of a rev-\nolution. Foundation models are large-scale machine learning\nmodels trained on extensive and diverse datasets. Following\ntheir initial training, these models can be adapted to specific\ndownstream tasks with minimal adjustments. By leveraging\ntheir extensive pre-training on large-scale datasets, they offer\nunprecedented analytical depth, enabling the analysis and\nprediction of medical images ranging from radiology to pathol-\nogy. The integration of foundation models into medical image\nanalysis holds the potential to enhance diagnostic accuracy,\nexpedite treatment schedules, and ultimately enhance patient\noutcomes.\nThe integration of foundation models into medical image\nanalysis garnered significant interest, leading to a surge of\nimpactful studies in the field. Notably, several perspective\npapers have emerged, highlighting the potential and future\ndirections of leveraging foundation models in the medical\ndomain. Research in foundation models for medical imaging\nhas been explored in many areas, for example, precise segmen-\ntation and detection of tumors [9], auto-organ segmentation\n[10], generating clinical reports [11], extracting quantitative\nfeatures from medical images using deep learning to predict\ndisease characteristics and outcomes [2], and medical Q&A\nsystems [12].\nHowever, the deployment of foundation models in such a\ncritical sector raises significant concerns regarding their trust-\nworthiness, which encompasses privacy, robustness, reliability,\nexplainability, and fairness (detailed definition see Sec 3). In\nmedical contexts, where decisions have profound implications\non patient health, ensuring the trustworthiness of foundation\nmodels becomes paramount. It involves rigorous validation\nagainst clinical standards, continuous monitoring for perfor-\nmance drift, and mechanisms to interpret model decisions\ntransparently. To fill this gap, we review and discuss recent\nadvances of trustworthiness in foundation models for medical\nimage analysis. To the best of our knowledge, this is the\nfirst survey of foundation models for medical image analysis\nfrom the trustworthiness perspective, which is different from\nthe existing surveys of foundation models for medical image\nanalysis.\nComparison with existing surveys. We compare existing\nsurveys that discuss similar topics on foundation models, trust-\nworthiness, and particularly those focused on medical imaging.\nOur survey paper exhibits several distinct advantages over the\nexisting literature, as illustrated in Table I. Existing survey\npapers on foundation models in medical imaging exhibit sig-\nnificant gaps, particularly in addressing trustworthiness issues.\nFor instance, while Azad et al. [1], Zhao et al. [2], and He et\nal. [3] discussed foundation models for medical imaging, they\nfall short in providing detailed analysis on trustworthiness.\nWhereas, He et al. [4], Sun et al. [5], and Liu et al. [6]\nsurvey trustworthiness in foundation models but do not focus\non medical imaging applications. Similarly, Salahuddin et\nal. [7] and Hasani et al. [8] focus on trustworthiness issues\nin medical imaging but do not cover the foundation models.\nFurthermore, most of the existing survey papers fail to provide\na comprehensive examination of trustworthiness and detailed\ninsights into medical imaging application-specific challenges"}, {"title": "Contributions.", "content": "The contributions of this paper can be high-\nlighted in the following aspects. First, we identify the trust-\nworthiness concerns associated with foundation models for\nmedical image analysis, classifying these issues into five dis-\ntinct categories: privacy, robustness, reliability, explainability,\nand fairness. Second, we review the existing literature and\ncategorize them according to the applications of medical image\nanalysis, including segmentation, clinical report generation,\nmedical question and answering (Q&A), and disease di-\nagnosis. Third, we remark on the trustworthiness issues in\nthe existing literature, noting the significant gaps in address-\ning these concerns. In addition, we propose future research\ndirections, emphasizing the need for innovative approaches\nto enhance model trustworthiness. This includes advancing\nprivacy techniques, improving model robustness, and fostering\nfairness and explainability in foundation models.\nRoadmap. The roadmap of this paper is shown in Figure 1.\nThe subsequent sections are organized as follows: In Section 2,\nwe introduce the background and usage of foundation models\nfor medical image analysis. In Section 3, we define five\ncategories of trustworthiness issues. In Section 4, we cate-\ngorize the existing literature of foundation models for medical\nimage analysis based on their application, and then by the\nreported trustworthiness concerns. Finally, we conclude with\nchallenges and future directions in Section 5 before presenting\nour conclusion in Section 6.\nTo provide a comprehensive overview, Figure 2 illustrates\nthe landscape of foundation models for medical image analysis\nstudied in this survey. We have covered 31 recent foundation\nmodels with applications in medical imaging from 72 research"}, {"title": "2. BACKGROUND AND TAXONOMY", "content": "2.1 Definition of Foundation Models\nFoundation models refer to massive machine learning mod-\nels trained on extensive volumes of diverse datasets, which\ncan later be fine-tuned for specific downstream tasks with rel-\natively minimal additional data (e.g., ViT [18]). These models\n(e.g., GPT [15], [16], SAM [13], CLIP [17]) serve as versatile\nplatforms that can be adapted to various specific tasks, ranging\nfrom language translation and content creation to complex\nproblem-solving in healthcare domains. The adaptability and\nefficiency of foundation models make them a cornerstone in\nthe development of cutting-edge technologies, providing a\nrobust basis for innovation and research in the field of medical\nimage analysis."}, {"title": "2.2 The Use of Foundation Models", "content": "There are four major approaches for training and using\nfoundation models in medical image analysis: training from\nscratch, fine-tuning, prompt-tuning, and using off-the-shelf\nmodels.\n1) Training From Scratch: Foundation models for medical\nimage analysis, often using the transformer architecture, are\ntrained on large, diverse datasets. Some, like the Segment Any-\nthing Model (SAM) [13], are pre-trained with labeled medical\nimages and paired segmentation masks. Others undergo self-\nsupervised learning (SSL), such as generative SSL (e.g., MAE\n[19]) and contrastive SSL (e.g., DINO [20], SimCLR [21],\nCLIP [17]), where they learn generic data patterns without\nlabeled examples. Masked autoencoders (MAEs) [19] involve\npre-text training to learn image representations by treating\nimages as sequences of patches, masking out certain patches,\nand predicting masked parts of the image, thereby teaching\nthe model to understand visual features such as shapes and\ntextures. Among the contrastive SSL strategies, a popular and\nadvanced approach is contrastive language-image pre-training\n(CLIP) [17], which uses the pre-training task of predicting\nwhich caption goes with which image. After learning a useful\ngeneralized representation of visual features, models are fine-\ntuned on specific downstream tasks, like tumor detection,\nusing a smaller labeled dataset, thereby tailoring their broad\ncapabilities to precise medical diagnosis.\n2) Fine-tuning: In medical image analysis, fine-tuning\nfoundation models on specific datasets significantly enhances\ntheir ability to detect diseases in medical images such as\nX-rays, MRI scans by adjusting model parameters to the\npeculiarities of medical images. Adopting full fine-tuning\nfor medical foundation models involves adjusting all the\nparameters of a pre-trained model to tailor it to a specific\ntask or dataset. However, this will lead to a computational\nburden. To address the expensive computation cost, parameter-\nefficient tuning, such as Lora [22] is proposed to adjust a\nsmall set of parameters in a pre-trained model to optimize\nit for new tasks with minimal computational overhead. This\napproach is particularly useful in medical image analysis,\nas most hospitals lack sufficient and powerful computational\nresources. Another objective for fine-tuning is to align with\nclinical insights, refine diagnostic accuracy and reliability.\nAs a popular alignment approach, Reinforcement Learning\nwith Human Feedback such as InstructGPT [23] can be\nleveraged to use medical experts' feedback for foundation\nmodel alignment. Similarly, AI-generated feedback (e.g., Self-\nRefine [24]) simulates expert reviews, allowing continuous\nimprovement in interpreting medical images and identifying\ncomplex conditions, even without direct human input.\n3) Prompt-Tuning: Prompt tuning, or prompt engineering,\nenhances foundation models for specific tasks by crafting input\nprompts while keeping the model backbone frozen [25]. The\nterms \"prompt tuning\" and \"prompt engineering\" are often\nused interchangeably, although their focuses are different.\nPrompt tuning focuses on updating continuous embeddings\noptimized for specific tasks, while prompt engineering tries\nout different prompts to find the best ones. Despite these\ndifferences, both approaches are commonly employed together\nin practice to direct the model's pre-trained knowledge to\nmeet the precise needs of the downstream task, sometimes\noutperforming full fine-tuning (e.g., VPT [26]). In medical\nimage analysis, well-designed prompts are specific inputs,\nsuch as text descriptions that guide the medical foundation\nmodel to focus on important features or regions within medical\nimages. For example, a well-designed textual prompt for a\nmedical foundation model analyzing chest X-rays might be,\n\"Focus on the lower lobe of the right lung for potential\ninfiltrates.\" This can significantly improve a model's diagnostic\naccuracy and provide a resource-efficient way to tailor models\nfor medical diagnostics.\n4) Off-the-Shelf: Due to the impressive generalization abil-\nities acquired from the huge amount of training data, many\ngeneral-purpose foundation models can be applied directly to\nthe medical image analysis domain without any modification.\nFor instance, SAM [13] can perform remarkably when applied\ndirectly to medical image analysis tasks [27]. For LLM-\nbased foundation models, off-the-shelf models are generally\nused to do in-context learning [15], which refers to providing\ndemonstrations or examples to the LLM at inference time to"}, {"title": "3. UNPACKING TRUSTWORTHINESS IN FOUNDATION\nMODELS FOR MEDICAL IMAGE ANALYSIS", "content": "3.1 Privacy (T1)\nPrivacy in medical image analysis is critical as patients\nexpect their healthcare service providers to follow necessary\nsafety measures to safeguard their inherent right to the privacy\nof their information, such as age and sex. This privacy concern\ncan also exist in medical foundation models due to the sensitiv-\nity of the health information involved in the extensive training\ndataset for medical foundation model training [28], [29]. With\nthe increase in data used for training and the complexity of\nfoundation models, there is a growing interest in exploring\nprivacy issues in medical foundation models in both academic\nand industrial fields.\n3.2 Robustness (T2)\nRobustness in medical image analysis refers to the ability\nto maintain performance when faced with various uncertain-\nties and adverse conditions. Similarly, foundation models for\nmedical image analysis also face challenges related to ro-\nbustness concerns. This includes handling errors or imbalance\nin input text prompts [10], being generalizable to data from\nvarious distributions [30], defending against adversarial attacks\ndesigned to mislead the model [31], and resisting attempts\nat data poisoning where malicious entities might attempt to\ninfluence the model's training data [32]. Thus, the emphasis\non robustness in foundation models for medical image analysis\nis crucial given the sensitive nature of medical diagnostics\nand the potential impact on patient care, requiring that these\nmodels have not only good performance but also resilient to\na wide range of potential vulnerabilities."}, {"title": "3.3 Reliability (T3)", "content": "Healthcare is commonly seen as a high-stakes field, where\nreliability is a foundational requirement. Firstly, it is common\nthat some foundation models such as LLMs can provide\nuntruthful answers or generate misleading information, which\nmay cause significant consequences in some scenarios such\nas Medical Q&A [33]. This is particularly concerning in\nhealthcare, where the accuracy of the information can directly\nimpact patient outcomes [34]. Secondly, hallucination can also\nhappen when LLM's confidence is miscalibrated. In a medical\ncontext, such overconfidence in wrong information can be dan-\ngerous. For instance, an LLM might generate a very confident\nbut incorrect interpretation of a patient's symptoms, leading\nhealthcare providers down the wrong diagnostic path [35].\n3.4 Explainability (T4)\nThe explainability of foundation models refers to the ability\nto understand and interpret how these models make decisions\nor generate outputs [36], [37]. This property becomes crucial\nin medical image analysis, especially when applying founda-\ntion models in healthcare, due to the demand for trustworthy\nand actionable decision-making in clinical settings. To be\nspecific, one common example of explainability in medical\nimage analysis is highlighting the regions of an image that\nthe model considers most indicative of a particular diagnosis.\nAnother example involves delineating the boundaries of dif-\nferent anatomical structures or pathological regions, allowing\npractitioners to understand why certain areas were identified\nas significant.\n3.5 Fairness (T5)\nFairness in foundation models refers to the equitable per-\nformance of the model across different demographic groups,\nensuring that no group experiences significantly lower perfor-\nmance. Similarly, it is also significant to eliminate fairness\nissues when adapting these foundation models to medical\nimage analysis fields [38]. Due to the common issue of\nunder-representation or imbalance in medical data, fairness in\nfoundation models for medical image analysis is particularly\ncritical. Therefore, it is essential to ensure that these medial\nfoundation models perform consistently well for all popula-\ntions, mitigating disparate impacts and reducing performance\ngaps between groups to maintain fairness."}, {"title": "4. TRUSTWORTHINESS IN FOUNDATION MODELS FOR\nMEDICAL IMAGE ANALYSIS", "content": "4.1 Overview of the Literature\nTable II lists the publications reviewed in this study on the\ndevelopment of medical imaging foundation models, specif-\nically highlighting those that include explicit discussions or\nevidence regarding their trustworthiness. If several foundation\nmodels are used in a paper, the two most representative ones\nare listed. In addition, the usage of the foundation model\nis categorized into the four medical imaging applications\nwe focus on, as well as the trustworthiness issues. All the\nmodalities, datasets, and body parts or organs covered in each\nliterature are also identified. Through this structured approach,\nreaders can easily navigate the landscape of current research\non medical foundation models and identify key trends and gaps\nin trustworthiness.\n4.2 Segmentation\nImage segmentation is a crucial task in medical image\nanalysis. A substantial stride in the development of a foun-\ndation model for medical image segmentation came with the\nintroduction of the Segment Anything Model (SAM) [13].\nSAM is a vision transformer (ViT) model developed by Meta\nAI which demonstrates great zero-shot performance on diverse\nsets of natural images. Inspired by the generalizability gained\nthrough prompt engineering of LLMs, SAM was designed\nto accept prompts as points, boxes, or text. For now, only\npoint and box prompts are publicly available. These prompts\nare encoded using a CLIP encoder. Based on SAM, several\nmodels, modifications and frameworks have been developed\nfor medical image analysis, including MedSAM [9], MedL-\nSAM [46], UR-SAM [10], UA-SAM [45], FedSAM [40],\nScribblePrompt [42], and GazeSAM [76].\n1) Privacy in Segmentation: Privacy concerns in foundation\nmodels for medical image segmentation represent a critical\nchallenge, given the sensitive nature of patient data processed\nby these models. Most foundation models for medical image\nsegmentation are based on SAM architecture [13], which\nasserts that the 11 M natural image dataset they use is \u201cprivacy\nrespecting\" - which they specify as meaning that faces and\nlicense plates are blurred out. Similarly, in MedSAM [9] the\nauthors leveraged the de-identified public medical images for\nmodel training. Federated learning is where model training is\ndone across multiple protected data sources without sharing\ndata [77]. As FedSAM [40] describes, this is a relevant\ntechnique for models trained on medical data, as it avoids the\npatient confidentiality concerns of storing and sharing privacy-\nsensitive medical data. Despite this, federated learning is rarely\nused for medical image segmentation models. FedSAM [40]\npresents a framework for federated SAM fine-tuing across\nmultiple clients, each of which has access to a fraction of the\ntotal dataset. Using prostate cancer MRI, brain tumour MRI,\nnuclei slide images, and fundus photograph sets for training,\nFedSAM shows comparable performance to SAM, except with\nthe nuclei slide images, where the dataset size is the smallest\n[40].\n2) Robustness in Segmentation: Robustness in foundation\nmodels for medical image segmentation is crucial for consis-\ntent and accurate results across various conditions. It ensures\nreliable outcomes, regardless of changes in data input or com-\nputational environment. This issue is particularly vulnerable\nin SAM-based segmentation models, where users can input\ncustomized prompts, potentially leading to inconsistent results\ndue to the variability and unpredictability of these user-defined\ninputs.\na) Robustness to different image modalities: SAMs still\nlack sufficient robustness on medical images to see widespread\nuse in the medical domain, which has a lower tolerance for"}, {"title": "MedLSAM", "content": "Literature in SAM-based medical image segmentation has\nalso explored other types of prompts, including scribble-\nbased [42] and text-based [79], though extensive user testing\nis not covered or neither across extensive modalities.\n3) Fairness in Segmentation: As with base SAM itself,\nlimitations are identified in the ability of MedSAM to perform\nreliably with underrepresented image modalities. In the context\nof MedSAM and medical image analysis, the concern comes\nfrom the fact that the training set largely consists of CT, MRI,\nor Endoscopy images. A similar issue is found in the vari-\nability of clinician needs in segmentation tasks; not all image\nmodalities should be divided the same way. This presents a\nconcern in the fairness of these segmentation models as they\nare used in rare clinical scenarios [9].\nMedSAM [9], a customization of the SAM fine-tuned on\nover 1M medical image-mask pairs covering 15 image modal-\nities, has become a popular medical image analysis foundation\nmodel for segmentation. Xu et al. [39] examines the perfor-\nmance of MedSAM on a kidney CT scan set and looks for\ndisparities in performance depending on patient characteristics.\nThey use Dice similarity scores as their performance metric."}, {"title": "4.3 Report Generation", "content": "The adoption of foundation models in the generation of\nmedical reports represents a pivotal advancement, facilitating\nthe creation of patient reports and radiology interpretations.\nHowever, this technological leap has also brought to light\ntrustworthiness concerns, including data privacy, the reliability\nof generated reports, and the necessity for transparent, explain-\nable models. This section explores potential trustworthiness\nissues and solutions presented in the literature.\n1) Privacy in Report Generation: Medical applications\ninherently come with significant concerns regarding patient\ndata privacy. To satisfy privacy requirements, Thawkar et\nal. [11] de-identified the MIMIC-CXR dataset by removing\npatient information when training their model. To preserve the\nutility of the model trained on de-identified data, the authors\nused GPT-3.5-turbo to remove the de-defined symbols \u201c____\u201d,\n66\n\u0648\u0648\nwhile preserving the original meaning of the reports.\n2) Robustness in Report Generation: Medical report gen-\neration is a high-risk field with a low tolerance for errors,\nmaking it especially important to ensure the robustness of\nmedical foundation models. However, due to the complexity\nand variance across sources like different inspected body\nregions and institutions, ensuring the robustness in medial\nfoundation models still faces numerous challenges.\nTo improve the robustness of these models across different\nbody regions, Zhong et al. [47] propose ChatRadio-Valuer, a\nmodel based on LLMs that learns generalizable representa-\ntions from radiology reports of one institution and adapts to\nreport generation tasks across different body regions (chest,\nabdomen, muscle-skeleton, head, maxillofacial, and neck).\nAnother work on body-part robustness in report generation\nproposes a single-for-multiple (S4M) framework to make the\nmodel robust to six different body parts (chest, abdomen, knee,\nhip, wrist, and shoulder) while maintaining its performance\nor even outperforming other baseline methods [48]. This is\nachieved by incorporating general radiology knowledge with\nthe radiology-informed knowledge aggregation branch and\nenhancing the cross-modal alignment by the implicit prior\nguidance branch.\nAnother robustness issue comes from the heterogeneity of\ndifferent clinical institutions. To solve this problem, [47] starts\nwith training on radiology reports from one institution to\nacquire knowledge of particular patterns and representations,\nand then it undergoes supervised fine-tuning using data from\nother numerous institutions. This approach ensures that the\nmodel can effectively handle a wide range of clinical scenarios\nand diagnostic tasks, improving its generalization ability and\nrobustness to different institutions in real-world applications.\nIn addition, medical images with blurry boundaries or noise\ncan also lead the robustness concerns. To address the unstable"}, {"title": "AdaMatch", "content": "generalization issues", "Generation": "Inheriting the issue of\ngeneral foundation models"}, {"Generation": "Explainability is\nalso a crucial concern in medical report generation. To improve\nthe interpretability of report generation systems", "images": 1, "title": "A Survey on Trustworthiness in Foundation Models\nfor Medical Image Analysis", "authors": ["Congzhen Shi", "Ryan Rezai", "Jiaxi Yang", "Qi Dou", "Xiaoxiao Li"], "abstract": "The rapid advancement of foundation models in\nmedical imaging represents a significant leap toward enhancing\ndiagnostic accuracy and personalized treatment. However, the\ndeployment of foundation models in healthcare necessitates a\nrigorous examination of their trustworthiness, encompassing\nprivacy, robustness, reliability, explainability, and fairness. The\ncurrent body of survey literature on foundation models in\nmedical imaging reveals considerable gaps, particularly in the\narea of trustworthiness. Additionally, extant surveys on the\ntrustworthiness of foundation models fail to address their specific\nvariations and applications within the medical imaging domain.\nThis survey paper reviews the current research on foundation\nmodels in the major medical imaging applications, with a focus\non segmentation, medical report generation, medical question\nand answering (Q&A), and disease diagnosis, which includes\ntrustworthiness discussion in their manuscripts. We explore the\ncomplex challenges of making foundation models for medical\nimage analysis trustworthy, associated with each application,\nand summarize the current concerns and strategies to enhance\ntrustworthiness. Furthermore, we explore the future promises\nof these models in revolutionizing patient care. Our analysis\nunderscores the imperative for advancing towards trustworthy\nAI in medical image analysis, advocating for a balanced approach\nthat fosters innovation while ensuring ethical and equitable\nhealthcare delivery.\nIndex Terms-Foundation Model, Trustworthy AI, Medical\nImage Analysis.", "sections": [{"title": "1. INTRODUCTION", "content": "With the advancement of foundational models, the field\nof medical image analysis is poised at the brink of a rev-\nolution. Foundation models are large-scale machine learning\nmodels trained on extensive and diverse datasets. Following\ntheir initial training, these models can be adapted to specific\ndownstream tasks with minimal adjustments. By leveraging\ntheir extensive pre-training on large-scale datasets, they offer\nunprecedented analytical depth, enabling the analysis and\nprediction of medical images ranging from radiology to pathol-\nogy. The integration of foundation models into medical image\nanalysis holds the potential to enhance diagnostic accuracy,\nexpedite treatment schedules, and ultimately enhance patient\noutcomes.\nThe integration of foundation models into medical image\nanalysis garnered significant interest, leading to a surge of\nimpactful studies in the field. Notably, several perspective\npapers have emerged, highlighting the potential and future\ndirections of leveraging foundation models in the medical\ndomain. Research in foundation models for medical imaging\nhas been explored in many areas, for example, precise segmen-\ntation and detection of tumors [9], auto-organ segmentation\n[10], generating clinical reports [11], extracting quantitative\nfeatures from medical images using deep learning to predict\ndisease characteristics and outcomes [2], and medical Q&A\nsystems [12].\nHowever, the deployment of foundation models in such a\ncritical sector raises significant concerns regarding their trust-\nworthiness, which encompasses privacy, robustness, reliability,\nexplainability, and fairness (detailed definition see Sec 3). In\nmedical contexts, where decisions have profound implications\non patient health, ensuring the trustworthiness of foundation\nmodels becomes paramount. It involves rigorous validation\nagainst clinical standards, continuous monitoring for perfor-\nmance drift, and mechanisms to interpret model decisions\ntransparently. To fill this gap, we review and discuss recent\nadvances of trustworthiness in foundation models for medical\nimage analysis. To the best of our knowledge, this is the\nfirst survey of foundation models for medical image analysis\nfrom the trustworthiness perspective, which is different from\nthe existing surveys of foundation models for medical image\nanalysis.\nComparison with existing surveys. We compare existing\nsurveys that discuss similar topics on foundation models, trust-\nworthiness, and particularly those focused on medical imaging.\nOur survey paper exhibits several distinct advantages over the\nexisting literature, as illustrated in Table I. Existing survey\npapers on foundation models in medical imaging exhibit sig-\nnificant gaps, particularly in addressing trustworthiness issues.\nFor instance, while Azad et al. [1], Zhao et al. [2], and He et\nal. [3] discussed foundation models for medical imaging, they\nfall short in providing detailed analysis on trustworthiness.\nWhereas, He et al. [4], Sun et al. [5], and Liu et al. [6]\nsurvey trustworthiness in foundation models but do not focus\non medical imaging applications. Similarly, Salahuddin et\nal. [7] and Hasani et al. [8] focus on trustworthiness issues\nin medical imaging but do not cover the foundation models.\nFurthermore, most of the existing survey papers fail to provide\na comprehensive examination of trustworthiness and detailed\ninsights into medical imaging application-specific challenges"}, {"title": "Contributions.", "content": "The contributions of this paper can be high-\nlighted in the following aspects. First, we identify the trust-\nworthiness concerns associated with foundation models for\nmedical image analysis, classifying these issues into five dis-\ntinct categories: privacy, robustness, reliability, explainability,\nand fairness. Second, we review the existing literature and\ncategorize them according to the applications of medical image\nanalysis, including segmentation, clinical report generation,\nmedical question and answering (Q&A), and disease di-\nagnosis. Third, we remark on the trustworthiness issues in\nthe existing literature, noting the significant gaps in address-\ning these concerns. In addition, we propose future research\ndirections, emphasizing the need for innovative approaches\nto enhance model trustworthiness. This includes advancing\nprivacy techniques, improving model robustness, and fostering\nfairness and explainability in foundation models.\nRoadmap. The roadmap of this paper is shown in Figure 1.\nThe subsequent sections are organized as follows: In Section 2,\nwe introduce the background and usage of foundation models\nfor medical image analysis. In Section 3, we define five\ncategories of trustworthiness issues. In Section 4, we cate-\ngorize the existing literature of foundation models for medical\nimage analysis based on their application, and then by the\nreported trustworthiness concerns. Finally, we conclude with\nchallenges and future directions in Section 5 before presenting\nour conclusion in Section 6.\nTo provide a comprehensive overview, Figure 2 illustrates\nthe landscape of foundation models for medical image analysis\nstudied in this survey. We have covered 31 recent foundation\nmodels with applications in medical imaging from 72 research"}, {"title": "2. BACKGROUND AND TAXONOMY", "content": "2.1 Definition of Foundation Models\nFoundation models refer to massive machine learning mod-\nels trained on extensive volumes of diverse datasets, which\ncan later be fine-tuned for specific downstream tasks with rel-\natively minimal additional data (e.g., ViT [18]). These models\n(e.g., GPT [15], [16], SAM [13], CLIP [17]) serve as versatile\nplatforms that can be adapted to various specific tasks, ranging\nfrom language translation and content creation to complex\nproblem-solving in healthcare domains. The adaptability and\nefficiency of foundation models make them a cornerstone in\nthe development of cutting-edge technologies, providing a\nrobust basis for innovation and research in the field of medical\nimage analysis."}, {"title": "2.2 The Use of Foundation Models", "content": "There are four major approaches for training and using\nfoundation models in medical image analysis: training from\nscratch, fine-tuning, prompt-tuning, and using off-the-shelf\nmodels.\n1) Training From Scratch: Foundation models for medical\nimage analysis, often using the transformer architecture, are\ntrained on large, diverse datasets. Some, like the Segment Any-\nthing Model (SAM) [13], are pre-trained with labeled medical\nimages and paired segmentation masks. Others undergo self-\nsupervised learning (SSL), such as generative SSL (e.g., MAE\n[19]) and contrastive SSL (e.g., DINO [20], SimCLR [21],\nCLIP [17]), where they learn generic data patterns without\nlabeled examples. Masked autoencoders (MAEs) [19] involve\npre-text training to learn image representations by treating\nimages as sequences of patches, masking out certain patches,\nand predicting masked parts of the image, thereby teaching\nthe model to understand visual features such as shapes and\ntextures. Among the contrastive SSL strategies, a popular and\nadvanced approach is contrastive language-image pre-training\n(CLIP) [17], which uses the pre-training task of predicting\nwhich caption goes with which image. After learning a useful\ngeneralized representation of visual features, models are fine-\ntuned on specific downstream tasks, like tumor detection,\nusing a smaller labeled dataset, thereby tailoring their broad\ncapabilities to precise medical diagnosis.\n2) Fine-tuning: In medical image analysis, fine-tuning\nfoundation models on specific datasets significantly enhances\ntheir ability to detect diseases in medical images such as\nX-rays, MRI scans by adjusting model parameters to the\npeculiarities of medical images. Adopting full fine-tuning\nfor medical foundation models involves adjusting all the\nparameters of a pre-trained model to tailor it to a specific\ntask or dataset. However, this will lead to a computational\nburden. To address the expensive computation cost, parameter-\nefficient tuning, such as Lora [22] is proposed to adjust a\nsmall set of parameters in a pre-trained model to optimize\nit for new tasks with minimal computational overhead. This\napproach is particularly useful in medical image analysis,\nas most hospitals lack sufficient and powerful computational\nresources. Another objective for fine-tuning is to align with\nclinical insights, refine diagnostic accuracy and reliability.\nAs a popular alignment approach, Reinforcement Learning\nwith Human Feedback such as InstructGPT [23] can be\nleveraged to use medical experts' feedback for foundation\nmodel alignment. Similarly, AI-generated feedback (e.g., Self-\nRefine [24]) simulates expert reviews, allowing continuous\nimprovement in interpreting medical images and identifying\ncomplex conditions, even without direct human input.\n3) Prompt-Tuning: Prompt tuning, or prompt engineering,\nenhances foundation models for specific tasks by crafting input\nprompts while keeping the model backbone frozen [25]. The\nterms \"prompt tuning\" and \"prompt engineering\" are often\nused interchangeably, although their focuses are different.\nPrompt tuning focuses on updating continuous embeddings\noptimized for specific tasks, while prompt engineering tries\nout different prompts to find the best ones. Despite these\ndifferences, both approaches are commonly employed together\nin practice to direct the model's pre-trained knowledge to\nmeet the precise needs of the downstream task, sometimes\noutperforming full fine-tuning (e.g., VPT [26]). In medical\nimage analysis, well-designed prompts are specific inputs,\nsuch as text descriptions that guide the medical foundation\nmodel to focus on important features or regions within medical\nimages. For example, a well-designed textual prompt for a\nmedical foundation model analyzing chest X-rays might be,\n\"Focus on the lower lobe of the right lung for potential\ninfiltrates.\" This can significantly improve a model's diagnostic\naccuracy and provide a resource-efficient way to tailor models\nfor medical diagnostics.\n4) Off-the-Shelf: Due to the impressive generalization abil-\nities acquired from the huge amount of training data, many\ngeneral-purpose foundation models can be applied directly to\nthe medical image analysis domain without any modification.\nFor instance, SAM [13] can perform remarkably when applied\ndirectly to medical image analysis tasks [27]. For LLM-\nbased foundation models, off-the-shelf models are generally\nused to do in-context learning [15], which refers to providing\ndemonstrations or examples to the LLM at inference time to"}, {"title": "3. UNPACKING TRUSTWORTHINESS IN FOUNDATION\nMODELS FOR MEDICAL IMAGE ANALYSIS", "content": "3.1 Privacy (T1)\nPrivacy in medical image analysis is critical as patients\nexpect their healthcare service providers to follow necessary\nsafety measures to safeguard their inherent right to the privacy\nof their information, such as age and sex. This privacy concern\ncan also exist in medical foundation models due to the sensitiv-\nity of the health information involved in the extensive training\ndataset for medical foundation model training [28], [29]. With\nthe increase in data used for training and the complexity of\nfoundation models, there is a growing interest in exploring\nprivacy issues in medical foundation models in both academic\nand industrial fields.\n3.2 Robustness (T2)\nRobustness in medical image analysis refers to the ability\nto maintain performance when faced with various uncertain-\nties and adverse conditions. Similarly, foundation models for\nmedical image analysis also face challenges related to ro-\nbustness concerns. This includes handling errors or imbalance\nin input text prompts [10], being generalizable to data from\nvarious distributions [30], defending against adversarial attacks\ndesigned to mislead the model [31], and resisting attempts\nat data poisoning where malicious entities might attempt to\ninfluence the model's training data [32]. Thus, the emphasis\non robustness in foundation models for medical image analysis\nis crucial given the sensitive nature of medical diagnostics\nand the potential impact on patient care, requiring that these\nmodels have not only good performance but also resilient to\na wide range of potential vulnerabilities."}, {"title": "3.3 Reliability (T3)", "content": "Healthcare is commonly seen as a high-stakes field, where\nreliability is a foundational requirement. Firstly, it is common\nthat some foundation models such as LLMs can provide\nuntruthful answers or generate misleading information, which\nmay cause significant consequences in some scenarios such\nas Medical Q&A [33]. This is particularly concerning in\nhealthcare, where the accuracy of the information can directly\nimpact patient outcomes [34]. Secondly, hallucination can also\nhappen when LLM's confidence is miscalibrated. In a medical\ncontext, such overconfidence in wrong information can be dan-\ngerous. For instance, an LLM might generate a very confident\nbut incorrect interpretation of a patient's symptoms, leading\nhealthcare providers down the wrong diagnostic path [35].\n3.4 Explainability (T4)\nThe explainability of foundation models refers to the ability\nto understand and interpret how these models make decisions\nor generate outputs [36], [37]. This property becomes crucial\nin medical image analysis, especially when applying founda-\ntion models in healthcare, due to the demand for trustworthy\nand actionable decision-making in clinical settings. To be\nspecific, one common example of explainability in medical\nimage analysis is highlighting the regions of an image that\nthe model considers most indicative of a particular diagnosis.\nAnother example involves delineating the boundaries of dif-\nferent anatomical structures or pathological regions, allowing\npractitioners to understand why certain areas were identified\nas significant.\n3.5 Fairness (T5)\nFairness in foundation models refers to the equitable per-\nformance of the model across different demographic groups,\nensuring that no group experiences significantly lower perfor-\nmance. Similarly, it is also significant to eliminate fairness\nissues when adapting these foundation models to medical\nimage analysis fields [38]. Due to the common issue of\nunder-representation or imbalance in medical data, fairness in\nfoundation models for medical image analysis is particularly\ncritical. Therefore, it is essential to ensure that these medial\nfoundation models perform consistently well for all popula-\ntions, mitigating disparate impacts and reducing performance\ngaps between groups to maintain fairness."}, {"title": "4. TRUSTWORTHINESS IN FOUNDATION MODELS FOR\nMEDICAL IMAGE ANALYSIS", "content": "4.1 Overview of the Literature\nTable II lists the publications reviewed in this study on the\ndevelopment of medical imaging foundation models, specif-\nically highlighting those that include explicit discussions or\nevidence regarding their trustworthiness. If several foundation\nmodels are used in a paper, the two most representative ones\nare listed. In addition, the usage of the foundation model\nis categorized into the four medical imaging applications\nwe focus on, as well as the trustworthiness issues. All the\nmodalities, datasets, and body parts or organs covered in each\nliterature are also identified. Through this structured approach,\nreaders can easily navigate the landscape of current research\non medical foundation models and identify key trends and gaps\nin trustworthiness.\n4.2 Segmentation\nImage segmentation is a crucial task in medical image\nanalysis. A substantial stride in the development of a foun-\ndation model for medical image segmentation came with the\nintroduction of the Segment Anything Model (SAM) [13].\nSAM is a vision transformer (ViT) model developed by Meta\nAI which demonstrates great zero-shot performance on diverse\nsets of natural images. Inspired by the generalizability gained\nthrough prompt engineering of LLMs, SAM was designed\nto accept prompts as points, boxes, or text. For now, only\npoint and box prompts are publicly available. These prompts\nare encoded using a CLIP encoder. Based on SAM, several\nmodels, modifications and frameworks have been developed\nfor medical image analysis, including MedSAM [9], MedL-\nSAM [46], UR-SAM [10], UA-SAM [45], FedSAM [40],\nScribblePrompt [42], and GazeSAM [76].\n1) Privacy in Segmentation: Privacy concerns in foundation\nmodels for medical image segmentation represent a critical\nchallenge, given the sensitive nature of patient data processed\nby these models. Most foundation models for medical image\nsegmentation are based on SAM architecture [13], which\nasserts that the 11 M natural image dataset they use is \u201cprivacy\nrespecting\" - which they specify as meaning that faces and\nlicense plates are blurred out. Similarly, in MedSAM [9] the\nauthors leveraged the de-identified public medical images for\nmodel training. Federated learning is where model training is\ndone across multiple protected data sources without sharing\ndata [77]. As FedSAM [40] describes, this is a relevant\ntechnique for models trained on medical data, as it avoids the\npatient confidentiality concerns of storing and sharing privacy-\nsensitive medical data. Despite this, federated learning is rarely\nused for medical image segmentation models. FedSAM [40]\npresents a framework for federated SAM fine-tuing across\nmultiple clients, each of which has access to a fraction of the\ntotal dataset. Using prostate cancer MRI, brain tumour MRI,\nnuclei slide images, and fundus photograph sets for training,\nFedSAM shows comparable performance to SAM, except with\nthe nuclei slide images, where the dataset size is the smallest\n[40].\n2) Robustness in Segmentation: Robustness in foundation\nmodels for medical image segmentation is crucial for consis-\ntent and accurate results across various conditions. It ensures\nreliable outcomes, regardless of changes in data input or com-\nputational environment. This issue is particularly vulnerable\nin SAM-based segmentation models, where users can input\ncustomized prompts, potentially leading to inconsistent results\ndue to the variability and unpredictability of these user-defined\ninputs.\na) Robustness to different image modalities: SAMs still\nlack sufficient robustness on medical images to see widespread\nuse in the medical domain, which has a lower tolerance for"}, {"title": "MedLSAM", "content": "Literature in SAM-based medical image segmentation has\nalso explored other types of prompts, including scribble-\nbased [42] and text-based [79], though extensive user testing\nis not covered or neither across extensive modalities.\n3) Fairness in Segmentation: As with base SAM itself,\nlimitations are identified in the ability of MedSAM to perform\nreliably with underrepresented image modalities. In the context\nof MedSAM and medical image analysis, the concern comes\nfrom the fact that the training set largely consists of CT, MRI,\nor Endoscopy images. A similar issue is found in the vari-\nability of clinician needs in segmentation tasks; not all image\nmodalities should be divided the same way. This presents a\nconcern in the fairness of these segmentation models as they\nare used in rare clinical scenarios [9].\nMedSAM [9], a customization of the SAM fine-tuned on\nover 1M medical image-mask pairs covering 15 image modal-\nities, has become a popular medical image analysis foundation\nmodel for segmentation. Xu et al. [39] examines the perfor-\nmance of MedSAM on a kidney CT scan set and looks for\ndisparities in performance depending on patient characteristics.\nThey use Dice similarity scores as their performance metric."}, {"title": "4.3 Report Generation", "content": "The adoption of foundation models in the generation of\nmedical reports represents a pivotal advancement, facilitating\nthe creation of patient reports and radiology interpretations.\nHowever, this technological leap has also brought to light\ntrustworthiness concerns, including data privacy, the reliability\nof generated reports, and the necessity for transparent, explain-\nable models. This section explores potential trustworthiness\nissues and solutions presented in the literature.\n1) Privacy in Report Generation: Medical applications\ninherently come with significant concerns regarding patient\ndata privacy. To satisfy privacy requirements, Thawkar et\nal. [11] de-identified the MIMIC-CXR dataset by removing\npatient information when training their model. To preserve the\nutility of the model trained on de-identified data, the authors\nused GPT-3.5-turbo to remove the de-defined symbols \u201c____\u201d,\n66\n\\u201dwhile preserving the original meaning of the reports.\n2) Robustness in Report Generation: Medical report gen-\neration is a high-risk field with a low tolerance for errors,\nmaking it especially important to ensure the robustness of\nmedical foundation models. However, due to the complexity\nand variance across sources like different inspected body\nregions and institutions, ensuring the robustness in medial\nfoundation models still faces numerous challenges.\nTo improve the robustness of these models across different\nbody regions, Zhong et al. [47] propose ChatRadio-Valuer, a\nmodel based on LLMs that learns generalizable representa-\ntions from radiology reports of one institution and adapts to\nreport generation tasks across different body regions (chest,\nabdomen, muscle-skeleton, head, maxillofacial, and neck).\nAnother work on body-part robustness in report generation\nproposes a single-for-multiple (S4M) framework to make the\nmodel robust to six different body parts (chest, abdomen, knee,\nhip, wrist, and shoulder) while maintaining its performance\nor even outperforming other baseline methods [48]. This is\nachieved by incorporating general radiology knowledge with\nthe radiology-informed knowledge aggregation branch and\nenhancing the cross-modal alignment by the implicit prior\nguidance branch.\nAnother robustness issue comes from the heterogeneity of\ndifferent clinical institutions. To solve this problem, [47] starts\nwith training on radiology reports from one institution to\nacquire knowledge of particular patterns and representations,\nand then it undergoes supervised fine-tuning using data from\nother numerous institutions. This approach ensures that the\nmodel can effectively handle a wide range of clinical scenarios\nand diagnostic tasks, improving its generalization ability and\nrobustness to different institutions in real-world applications.\nIn addition, medical images with blurry boundaries or noise\ncan also lead the robustness concerns. To address the unstable"}, {"title": "AdaMatch", "content": "generalization issues, an innovative SAM-guided dual-encoder\narchitecture in MSMedCap is utilized to enable the capture\nof information with different granularities and overcome the\nunstable performance due to blurry boundaries, noise, and poor\ncontrast in medical images [49]. Inspired by NEFTune [80],\nRO-LLaMA also enhances the model's robustness and gen-\neralization abilities when faced with noisy inputs by adding\nrandom noise to the embedding vectors during training [61].\n3) Reliability in Report Generation: Inheriting the issue of\ngeneral foundation models, Vision Language Models (VLMs)\nbased report generation systems also suffer from reliability\nissues. First of all, medical foundation models can make\nhallucinated references to non-existent prior reports when\ngenerating medical reports. One reason for this is that these\nmodels are trained on datasets of real-world patient reports that\ninherently refer to prior reports and lead to the incompatibility\nwhen solely inputting a medical image [54], [51]. To address\nthis, [54] propose CXR-ReDonE to remove prior references\nfrom medical reports by using GPT-3 to rewrite the reports\nand using token classification to remove words referring to\npriors. Another study observe that when customizing general-\npurpose foundation models for medical report generation, they\ncan hallucinate unintended text [50]. They attribute this to\nthe use of cross-entropy loss for language modeling, which\nleads to the problem of exposure bias. One potential solution\nto address this issue is to leverage reinforcement learning\nwith human (AI) feedback. In addition, Lee et al. [53] find\nthat their proposed method for chest X-ray report generation\nmay hallucinate reports with nonexistent findings. To mitigate\nthis issue, they suggest some potential solutions, such as\nusing higher quality/quantity training data, larger LLMs, or to\nstrengthen the alignment between image and text modalities.\nAnother work [55] proposes to enhance the reliability of\nradiology report generation by injecting additional knowledge\nwith the current image in the generation process. Specifically,\nthey fuse the weighted medical concept knowledge and reports\nfor similar images with the current image features.\n4) Explainability in Report Generation: Explainability is\nalso a crucial concern in medical report generation. To improve\nthe interpretability of report generation systems, Danu et\nal. [56] propose a two-step approach for generating the find-\nings section of a radiology report from an automated interpre-\ntation of chest X-ray images: (1) detecting the abnormalities\nwithin the image using bounding-boxes with probabilities, and\n(2) harnessing the power of LLMs to translate the list of\nabnormalities into a Findings report. This two-step approach\nadds interpretability to the framework and aligns it with\nradiologists' systematic reasoning during the review of CXRs.\nUsing fine-grained vision-language models to provide align-\nment between image patches and texts which can match parts\nof the generated report with specific regions in the medical\nimage can also improve explainability. However, using image\npatches with fixed sizes may result in incomplete representa-\ntion of lesions which can occur in varying sizes. To address\nthis issue, Chen et al. [57] propose an Adaptive patch-word\nMatching (AdaMatch) model which matches texts from the"}, {"title": "4.4 Medical Q&A", "content": "With the power of language and vision foundation models,\nMedical Q&A systems leverage their ability for image and\ntext understanding to serve for a wide range of tasks [63],\n[33], [11], [81]. These Medical Q&A systems usually accept\na medical image and a text prompt as inputs and output text in\nresponse to the question. However, similar to medical report\ngeneration tasks, the appearance of LLMs in the medical\ndomain has also accentuated concerns regarding their trust-\nworthiness. Fortunately, the research community has begun\nidentifying and addressing these concerns as well as proposing\nmethods to mitigate the trustworthiness issues.\n1) Robustness in Medical Q&A: Robustness in medical\nfoundation models for Q&A tasks requires being able to\nhandle errors in the input text prompts as well as be resilient\nagainst adversarial and backdoor attacks in medical images.\nThough some medical foundation models achieve success in\ntheir model performance, they exhibit susceptibility to specific\ntext inputs. Sonsbeek et al. [60] proposes a prefix tuning\nmethod for open-ended medical VQA of LLMs for perfor-\nmance improvement, but the experimental results show that the\nmodel is heavily sensitive to the prompt structure, as swapping\nthe order of the question embeddings and the visual prefix\nyields a decrease in performance. To improve the robustness\nof LLM-based foundation models in medical Q&A, RO-\nLLaMA [61] incorporates consistency regularization, inspired\nby NEFTune [80], which adds random noise to the embedding\nvectors during training. This approach enhances the model's\nrobustness and generalization abilities when faced with noisy\ninputs. Additionally, robustness issues are also present in\nCLIP-based medical Q&A models such as MedCLIP [65]\nand PubMedCLIP [62], especially if the backbone CLIP-based\nmodels themselves have robustness problems. This concern\ncan be inferred by observations in [31], which show that using\na modest set of wrongly labeled data and introducing a \u201cBad-"}, {"title": "1) Privacy in Disease Diagnosis:", "content": "Distance\u201d between the embeddings of clean and poisoned data\ncan lead to successful backdoor attacks.\n2) Reliability in Medical Q&A: The reliability of founda-\ntion models is a significant concern in Q&A domains, particu-\nlarly because these models can produce untruthful answers or\nhallucinations. This challenge is further complicated by the\nfact that these models often inherit the same issues when\nadapted to other fields. Unlike general applications, where\nsome degree of unreliability may be tolerated, reliability in\nthe medical domain is crucial. Unreliable responses in medical\nQ&A can disastrously mislead decision-making, potentially\nresulting in harmful consequences [82].\nOne significant threat for reliability of medical foundation\nmodels is hallucinations, which is hard to evaluate and detect.\nAlthough existing works like CHAIR [83] are designed for\ngeneral foundation hallucination evaluation, they cannot be\ndirectly adapted to the medical domain. To address this, the\nfirst benchmark and a new evaluation metric named Med-\nHallMark and MediHall Score respectively, for hallucination\ndetection specific to medical foundation models is proposed\nto provide baselines for various foundation models in medical\nimage analysis [82]. Meanwhile, a hallucination detector for\nmedical foundation models is proposed for five types of\nhallucinations: catastrophic, critical, attribute, prompt-induced\nand minor hallucinations. Similarly, CARES is proposed to\ncomprehensively evaluate the hallucination of Medical Large\nVision Language Models (Med-LVLMs) [84]. To ensure relia-\nbility, Shaaban et al. [64] propose MedPromptX and leverage\nfew-shot prompting (FP) to eliminate hallucination in the\nfoundation model for chest X-ray Diagnosis by guiding the\noutput.\nAnother concern in reliability is truthfulness. A recent study\nhighlights that Medical Visual Question Answering (Med-\nVQA) systems may be unreliable for medical diagnosis ques-\ntions and could produce misleading information when existing\nstate-of-the-art foundation models, such as GPT-4, are directly\nadapted to medical image analysis domains [85]. A good\nexample of this is ChatCAD, which leverages LLMs' extensive\nand reliable medical knowledge to provide truthful interactive\nexplanations and advice [63]. However, the truthfulness of\nChatCAD is threatened by the restricted scope of applicable\nimaging domains and the lack of requisite depth in medical\nexpertise during interactive patient consultations. To address\nthese disadvantages of CADs, Zhao et al. [33] propose Chat-\nCAD+, which incorporates a domain identification module to\nwork with a variety of CAD models. Additionally, outdated\nmedical information can also contribute to incorrect answers.\nInstead of directly answering medical questions, ChatCAD+\nfirst retrieves knowledge from professional websites to obtain\nrelevant, up-to-date information, which further enhances the\ntruthfulness of its answers.\n3) Explainability in Medical Q&A: In high-risk medical\nenvironments, the use of black-box foundation models can lead\nto severe consequences, which may directly impact patient\nhealth and safety. Therefore, it is essential to ensure that the\ndecision-making processes of medical foundation models are"}, {"title": "4) Fairness in Medical Q&A:", "content": "transparent and explainable. By providing interactive explana-\ntions for the advice of the medical image, ChatCAD enhances\nits explainability for clinical decision-making [63]. With the\npopular use of GPT-4 in medical fields, providing appropriate\nprompts that ask the model to explain its answers during\nmedical Q&A can also enhance the explainability [35].\n4) Fairness in Medical Q&A: Under-representation of cer-\ntain groups in the training data or training data imbalance\ncan lead to significantly lower accuracy or effectiveness of\nthe model for certain populations, leading to disparate per-\nformance and fairness issues [86]. In medical Q&A systems,\nthese fairness issues can be caused by imbalanced training data\nin both the medical image and language domains, and can also\nuniquely arise in the query prompts.\nZhang et al. [59] propose PMC-VQA, a large-scale medical\nVQA dataset with 227k VQA pairs of 149k images cover-\ning various modalities and diseases. Despite their efforts to\nconstruct a comprehensive MedVQA dataset, the authors note\nthe potential presence of biases, which may arise from data\ncollection, annotation (inconsistencies or subjective interpreta-\ntions from human annotators), or the underlying distribution of\nthe medical images and questions. Figure 3 shows the word\nclouds for the questions and answers in the training set of\nthe PMC-VQA dataset. The imbalance in word frequencies\nmay be an indication of potential biases in the dataset. For\ninstance, for the word cloud of answers, \u201cX-ray\u201d, \u201cMRI\u201d,\nand \"CT scan\" are the most prevalent words, suggesting that\nmodels trained on this dataset may perform better on well-\nrepresented modalities like X-ray, CT, and MRI, and worse\non other imaging modalities.\nAnother popular dataset used for medical Q&A tasks is the\nSLAKE [87] dataset. The distribution of images for different\nbody parts is shown in Figure 4a. It can be observed that the\ndataset consists mostly of images of chest, abdomen, and head,\nwith pelvic and neck images only taking up a significantly\nsmaller portion. Thus, the under-representation of pelvic and\nneck images may lead to worse performance when tested\non images from these body regions. Another example is the\nCheXpert dataset, which is a large public dataset of chest\nradiographs of 65,240 patients. The age distribution of male\nand female patients is shown in Figure 4b, which reveals\na gender imbalance in the dataset, with a predominance of\nmale patients for most of the age groups, potentially leading\nto fairness issues in which the model performs better at\ndiagnosing radiographs from male patients."}, {"title": "4.5 Disease Diagnosis", "content": "Disease diagnosis", "Diagnosis": "Due to foundation mod-\nels for medical image analysis typically trained on large-\nscale datasets which always contain sensitive patient private\ninformation, privacy considerations in foundation models for\nmedical image analysis are increasingly paramount. As a\ngold standard framework for privacy-preserving, Differential\nprivacy (DP) can provide a privacy guarantee for machine"}]}]}