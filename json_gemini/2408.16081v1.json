{"title": "Logic-Enhanced Language Model Agents for Trustworthy Social Simulations", "authors": ["Agnieszka Mensfelt", "Kostas Stathis", "Vince Trencsenyi"], "abstract": "We introduce the Logic-Enhanced Language Model Agents (LELMA) framework, a novel approach to enhance the trustworthiness of social simulations that utilize large language models (LLMs). While LLMs have gained attention as agents for simulating human behaviour, their applicability in this role is limited by issues such as inherent hallucinations and logical inconsistencies. LELMA addresses these challenges by integrating LLMs with symbolic AI, enabling logical verification of the reasoning generated by LLMs. This verification process provides corrective feedback, refining the reasoning output. The framework consists of three main components: an LLM-Reasoner for producing strategic reasoning, an LLM-Translator for mapping natural language reasoning to logic queries, and a Solver for evaluating these queries. This study focuses on decision-making in game-theoretic scenarios as a model of human interaction. Experiments involving the Hawk-Dove game, Prisoner's Dilemma, and Stag Hunt highlight the limitations of state-of-the-art LLMs, GPT-4 Omni and Gemini 1.0 Pro, in producing correct reasoning in these contexts. LELMA demonstrates high accuracy in error detection and improves the reasoning correctness of LLMs via self-refinement, particularly in GPT-4 Omni.", "sections": [{"title": "Introduction", "content": "Experiments in the social sciences provide the knowledge necessary to make informed decisions in areas such as public policy, education, and healthcare. However, these experiments may face challenges such as high costs, issues with scalability, and ethical concerns (Kimmel 1988), making them impractical or unfeasible. A promising alternative is the application of Artificial Intelligence (AI) to conduct large-scale simulations of complex social systems (Conte and Paolucci 2014). Ever since its inception, AI has sparked hopes of simulating human reasoning (Newell, Shaw, and Simon 1959). Yet, despite decades of advancements, creating AI agents that accurately mimic human thought and decision-making remains a challenge.\nRecent advancements in large language models (LLMs), capable of producing elaborated natural language outputs, have garnered interest in their use for simulating human behaviour (Aher, Arriaga, and Kalai 2023). However, the reliability of simulations employing LLM-based agents is compromised by inherent hallucinations (Achiam et al. 2023), logical inconsistencies, and arithmetic errors (Imani, Du, and Shrivastava 2023). To address this challenge, we integrate LLMs, which enable the incorporation of human preferences, beliefs, and context, with symbolic AI that provides reliable logical reasoning. We introduce the Logic-Enhanced Language Model Agents (LELMA) framework, where reasoning from LLM-based agents is logically verified. This verification provides corrective feedback, enabling agents to reassess and refine their reasoning. We focus on decision-making in game-theoretic scenarios as a model of human interaction. By combining the strengths of LLMs and symbolic AI, LELMA aims to enhance the trustworthiness of LLMs-based social simulations, addressing the inconsistencies and errors found in current models.\nThe goals of this work are threefold: first, to assess the performance of state-of-the-art (SOTA) LLMs in social dilemmas in terms of the correctness of natural language reasoning; second, to propose a method for improving the performance of LLMs in this task; and third, to provide a robust method of evaluating the system. Our experiments demonstrate poor reasoning correctness of LLMs in three game-theoretic scenarios: Hawk-Dove game, Prisoner's Dilemma, and Stag Hunt. The introduced LELMA framework shows high accuracy in error detection for both GPT-4 Omni (OpenAI 2024) and Gemini 1.0 Pro (Rohan Anil et al. 2024) and significant improvement in correctness for GPT-4 Omni. The evaluation of reasoning correctness in this open-ended task proves to be challenging even for human evaluators.\nOur main contributions are: (a) the analysis of the performance of GPT-4 Omni and Gemini 1.0 Pro in social dilemmas reasoning tasks; (b) providing a framework for analysing the performance of language models in these tasks; (c) proposing a framework for logical verification of LLM agents' output and formal-evaluation-based feedback for the agents; and (d) providing evaluation protocol for the proposed framework. LELMA is modular by design, allowing the use of a custom solver. Therefore, it can be adapted for use outside the social simulations context, for instance, to detect and mitigate biases and ensure fairness."}, {"title": "Preliminaries", "content": ""}, {"title": "Large Language Models", "content": "The term Large Language Model (LLM) typically refers to a model based on the transformer architecture with billions of parameters. LLMs depend on pre-trained models due to their computationally and data-intensive training requirements (Zhao et al. 2023). Fine-tuning, which involves training the model on task-specific data, can further enhance LLM performance (Ziegler et al. 2020)."}, {"title": "GPT-4 Omni", "content": "OpenAI's GPT-4 Omni (OpenAI 2024) based on and successor of GPT-4 (Achiam et al. 2023) a current state-of-the-art LLM. Its multi-modal architecture supports text, image and audio inputs and outputs. It was pre-trained with publicly accessible data and third-party resources, and fine-tuned with human feedback. GPT-4 Omni demonstrates enhanced performance in reasoning tasks compared to its predecessor and is reported to have increased factuality. However, GPT-4 Omni still faces challenges with reliability due to hallucinations."}, {"title": "Gemini 1.0 Pro", "content": "Gemini 1.0 Pro is Google's previous largest model, with good general performance (Rohan Anil et al. 2024). The Gemini model family supports various forms of textual, visual and audio inputs and can produce interleaved textual and visual outputs. Gemini models are pre-trained on publicly available resources for each modality. Gemini models support external tool use and web search, increasing their performance in reasoning tasks."}, {"title": "Game Theory", "content": "Game theory provides the mathematical tools to analyze the strategic interaction between decision-makers (Osborne 2004). Game Theory applications range from analysing day-to-day social situations to complex political or economic problems.\nGame Following (Osborne and Rubinstein 1994) and (Rasmusen 2006), we can formally define games as:\n\u2022 N, a set of n players;\n\u2022 $A_i, \\forall i \\in N$ a non-empty set of actions;\n\u2022 $\\forall i \\in N$ a utility function $u_i: A \\rightarrow R$, producing i's payoff $\\pi_i$.\nWe assume that players are rational, intelligent, and act voluntarily, aiming to maximise their utility while fully understanding the game and its participants (Myerson 1984)."}, {"title": "Games in our experiments", "content": "We experiment with symmetric non-cooperative games presented in normal form, usually a matrix showing the players, strategies, and payoffs (Rasmusen 2006). We use a classical game, the Prisoner's Dilemma (PD), involving two players, each of whom can confess (cooperate) for mutual benefit or betray their partner (defect) for individual reward. The temptation to defect when the other player cooperates is denoted by payoff T, the reward for mutual cooperation is marked by payoff R, the punishment for mutual defection is payoff P, and the sucker's payoff for cooperating when the other defects is denoted by payoff S, giving rise to the general matrix of"}, {"title": "General game playing", "content": "General game playing (Genesereth, Love, and Pell 2005) aims at creating intelligent systems that understand the rules of arbitrary new games and learns to play them without human intervention. The Game Description Language (GDL) has been proposed as a formal, machine-processable language for describing the rules of arbitrary games (Love et al. 2006). GDL focused on information games only, so it was extended in GDL-II (Thielscher 2010) to cover n-player games with incomplete information and games in extensive normal form (Thielscher 2011). GDL-II is based on the standard syntax and semantics of logic programming and characterised by the special keywords"}, {"title": "Logic-Enhanced Language Model Agents", "content": "To improve the performance and trustworthiness of LLM agents in social dilemmas tasks, we introduce the Logic-Enhanced Language Model Agents (LELMA) framework. LELMA equips the agents capable of producing natural-language reasoning with an implementable knowledge representation framework for verification and re-evaluation of their reasoning. The framework is implemented in Python and Prolog."}, {"title": "Framework Overview", "content": "Our system comprises of the following components.\n\u2022 Reasoner: an LLM agent for analyzing a game scenarios in a human-like manner and making strategic decisions.\n\u2022 Translator: an LLM agent that maps statements from the Reasoner's response to query templates sent to the Solver.\n\u2022 Solver: a normal logic program implemented in Prolog.\n\u2022 a feedback loop: the mechanism for providing feedback if any of the query evaluations failed; each of the failed queries is translated back to natural language and forwarded to the The Reasoner using a feedback prompt (see Listing 1)."}, {"title": "Solver", "content": "Our solver consists of a game-independent part representing the rules of any game in extensive form, a game-dependent part defining the rules of a specific game using the predicates of the game-independent part, and a set of auxiliary predicates on top of these representations used to evaluate LLM reasoning for that specific game. We use the usual conventions to represent a game in Prolog: variables are denoted by uppercase letters, predicates and function symbols by lowercase letters, the symbol : - is read as if, and the symbol \\+ is read as not (negation by failure). An underscore '_'"}, {"title": "Game-independent part", "content": "We recursively specify all legal transitions of an extended form game from an initial situation S to a final situation F as follows:\ngame (F,F):-\nfinal(F).\ngame (S,F):-\n\\+ final (S),\nlegal (M,S),\ngame (do (M,S),F).\nThe first two lines return the final game situation F, when it is reached. Otherwise, in a non-final situation S, the game accepts a legal move M, and the game continues in the next do (M, S) situation, until the final situation F is reached. To reason about what holds in individual legal situations, we use Situation Calculus represented as:\nholds (F, S):-\ninitially (F, S).\nholds (F, do (M, S)):-\neffect (F, M, S).\nholds (F, do (M, S)):-\nholds (F, S),\n\\+ abnormal (F, M, S).\nA fluent F holds in the initial situation, a new fluent F is initiated by the effects of a move M executed in a situation S, and a fluent F persists after a move is made, provided it is not abnormal; abnormal fluents are terminated (do not persist).\nWe also use rules of the form:\nfinally (F, S):- Conditions.\nto return derived fluents F describing the result of the game, when the Conditions hold in the final situation S."}, {"title": "Game-dependent part", "content": "To represent a specific game we need to define game-dependent predicates for the initial state initial/1, the legal moves legal/2, what holds in the initial game situation via initially/2, the effects of a move on a situation via effect/3, what stops persisting in a situation after the execution of a move via abnormal/3, the final situation final/1, and the result of the game via finally/2 definitions. To exemplify these definitions, we show how to describe a PD game to our solver. The initial situation so is defined as:\ninitial(s0).\nWhat holds in this initial situation we specify it as:\ninitially (player(p1), s0).\ninitially (player(p2), s0).\ninitially (role (pl,row), s0).\ninitially (role (p2,col), s0).\ninitially (control(p1), s0).\ninitially (control(p2), s0).\nThese assertions define first the player names represented by unique identifiers (p1 and p2), their roles (p1 is the row player, while p2 is the column player), and then the fact that initially either of them can the game next (via the control/1 fluent, as in GDL). What holds in the initial situation changes as a result of move being made in the game. We represent moves as terms of the form choice(P, M), where P is a player, and M is a move. As it is possible for any player in a Prisoner's Dilemma game to choose defect ('D') or cooperate ('C'), we write this as:\npossible (choice(P,'D'), S):-\nholds (player(P), S).\npossible (choice (P,'C'), S):-\nholds (player(P), S).\nIt is then legal for a player to choose a possible move if they have the control to execute it in the current situation:\nlegal (choice(P,M), S):-\npossible (choice (P,M), S),\nholds (control (P), S).\nWhen a legal move M is made by a player P, the effect that this move is actually made is recorded in the next situation:\neffect (did (P, M), choice (P, M), S).\nOnce a legal move is executed, the player loses control, which we specify in our framework as:\nabnormal (control (P), choice (P, M), S).\nIn other words, after a choice is made by a player, the player loses control and therefore cannot play a move again from that situation onwards.\nMoves made in this way bring about the final situation, which we specify as a situation term with two choices made from the initial situation, one for each player.\nfinal (S):-\nground(S),\nS=do (choice(_,_), do (choice(_,_),I)),\ninitial (I).\nAssuming the payoff matrix of Table 1b defined as:\npayoff('D', 'D', 1, 1).\npayoff('C', 'D', 0,5).\npayoff('D', 'C', 5, 0).\npayoff('C', 'C', 3, 3).\nthe outcome of the game holds information about the actual moves made by the players, and their payoffs.\nfinally (outcome (P1,M1, U1,P2, M2,U2), S):-"}, {"title": "Queries for reasoning verification", "content": "The solver was extended with predicates developed based on the preliminary analysis of possible reasoning errors made by an LLM in the game-theoretic scenarios. The prompt used for translating natural language to predicates contains the list of the predicates templates and their natural language explanation . For any predicates that fail, the correct values of the variables are found, and the result is translated back into natural language and included in the feedback prompt."}, {"title": "Experiments", "content": "To assess the performance of LLMs in social dilemma tasks and evaluate the effectiveness of the LELMA framework in improving this performance, we conducted experiments using three previously introduced games. The primary criterion for correctness was the absence of errors related to the assignment and reasoning about payoffs. Additionally, we conducted a manual evaluation of all reasoning samples generated during the experiments."}, {"title": "Methods", "content": "Settings The rules of each game and the payoffs for each pair of actions were presented in an instruction prompt as a natural language description (e.g., \"If you both pick R, you each get $3.\"). The prompt included an instruction to \"perform reasoning as a human player.\u201d Preliminary experiments indicated better performance from GPT-4 Omni when the payoff matrix was presented in a standard format (i.e., with the typical order of actions and payoffs). However, performance declined when the payoff matrix was inverted, hence this order was used in the final experiments. To further obscure the task, the game's name was omitted from the prompt, and the action names were replaced with 'B' and 'R'. The experimental parameters are presented"}, {"title": "Correctness criteria", "content": "For assessing the correctness of the natural language reasoning, we assume that in the games considered, both actions are valid and can be justified based on factors such as risk aversion or cooperativeness. A reasoning sample is deemed incorrect only if it contains errors related to the assignment and reasoning about payoffs (errors that human participants typically do not make).\nOther types of errors, such as the improper use of game theory concepts like Nash equilibrium, do not result in a sample being considered incorrect. Therefore, the assessment focuses strictly on the accuracy of reasoning about payoffs based on the chosen actions, regardless of the knowledge of game theory."}, {"title": "Results", "content": "Attempts distribution  presents the distribution of reasoning attempts for GPT-4 Omni and Gemini 1.0 Pro. For GPT-4 Omni 78% samples required either one or two attempts, with the maximum number of attempts (5) being reached only in four cases. Further inspection showed that in these four cases, the last attempt was always correct. On the other hand, Gemini 1.0 Pro's performance was worse, with 27% of cases reaching 5 attempts, out of which 83% were incorrect at the last attempt.\nChoices distribution  presents the frequency of choice 'B' in the initial and final reasoning attempts for both LLMs. If the agent did not receive any feedback, the initial choice remained unchanged as the final choice. The choice 'B' corresponds to 'Dove' in Hawk-Dove, 'Confess' in the Prisoner's Dilemma, and 'Stag' in Stag Hunt.\nOverall, risk-averse choices were more frequent in the final attempts compared to the initial ones. The Hawk-Dove game resulted in the most consistent action choices, with 'Dove' being the prevalent action for both models. In the initial attempt, 'Dove' was chosen 100% of the time by GPT-4 Omni and 66.67% by Gemini 1.0 Pro, increasing to 100% and 70% in the final attempt, respectively. This is a risk-averse action that avoids the worst payoff of 0, even though it does not provide the highest possible payoff.\nIn the Prisoner's Dilemma, the most significant change between initial and final attempts was observed with GPT-4 Omni, where the frequency of choosing 'Confess' dropped from 96.67% to 23.33%. An analysis of the responses indicated that the initial cooperative choice was often due to incorrect reasoning about payoffs. Similarly, in the Stag Hunt, after receiving feedback from the solver, the frequency of choosing 'Stag', a less risk-averse option, decreased from 100% to 33% for GPT-4 Omni and from 80% to 60% for Gemini 1.0 Pro.\nCorrectness  shows the percentage of correct reasoning samples in the initial and final attempts for both models (the details of the correctness evaluation procedure are discussed in the subsequent section). Initially, both LLMs demonstrated low correctness levels. However, with corrective feedback, GPT-4 Omni significantly improved its cor-"}, {"title": "Computational cost", "content": "For experiments with GPT-4 Omni, on average, 1269.77 tokens per attempt were used in instructions to the model, and on average 1224.38 were generated by the model. The average time of one reasoning attempt was 16.53 seconds for GPT-4 Omni and 34.66 seconds for Gemini 1.0 Pro. The total time of experiments was 53 minutes and 173 minutes, respectively."}, {"title": "Evaluation", "content": "To assess the framework's ability to detect and correct reasoning errors, a ground truth is necessary. In the case of reasoning expressed in natural language, with both actions possibly valid there is no straightforward way to automatize creating a ground truth. Therefore, we conducted a manual evaluation of reasoning samples. To ensure objectivity and reproducibility, we developed an evaluation protocol and sheet (available at repository). Three independent evaluators were employed to minimize the impact of human error. Due to the high cognitive demand of the task, we acknowledge that not all evaluators may identify all errors. Additionally, it is more likely for an error to be missed than for a correct statement to be inaccurately classified as an error. Consequently, our aggregation method is conservative, classifying a sample as correct only if all evaluators agree on its correctness."}, {"title": "Confusion matrices and accuracy", "content": "Confusion matrices were calculated based on the manual evaluation (representing actual correctness) and the presence or absence of failed predicates (representing predicted correctness). Tables 8 and 9 present the confusion matrices for the initial reasoning attempt for both LLMs. Overall, error recognition was high, reaching 84% for GPT-4 Omni and 86% for Gemini 1.0 Pro. The game with the lowest error detection accuracy was Hawk-Dove, with 73% for GPT-4 Omni and 81% for Gemini 1.0 Pro. Among the detection errors, false positives were more common than false negatives. False positives affect trustworthiness, as they correspond to erroneous reasoning that went undetected. False negatives, while increasing computational cost, should have less impact on correctness, since feedback based on the solver's evaluation is always correct.\nInter-rater agreement To assess inter-rater agreement among three evaluators, Fleiss' Kappa (Fleiss 1971) was calculated. For GPT-4 Omni, the Kappa value was 0.31, indicating fair agreement, while for Gemini 1.0 Pro, it was 0.59, indicating moderate agreement. These results highlight the challenges of this evaluation process. The higher Kappa for Gemini 1.0 Pro can be attributed to the presence of more obvious and easier-to-detect errors compared to those in GPT-4 Omni."}, {"title": "Discussion", "content": "LLMs Performance in Social Dilemmas LLMs gained attention as potential agents within the framework of game theory (Akata et al. 2023; Fan et al. 2023; Guo 2023; Lor\u00e8 and Heydari 2023). This work evaluated the correctness of strategic reasoning produced by two LLMs, GPT-4 Omni and Gemini 1.0 Pro, across three one-shot games. The initial reasoning correctness, prior to any feedback, was generally low, ranging from 3.33% to 46.67%. This result underscores the limitations of current automated benchmarks for evaluating LLM performance in game theory and strategic decision-making contexts (such as (Duan et al. 2024)). Moreover, it highlights the challenges of using LLMs to develop agents for social simulations.\nLELMA Accuracy and Effectiveness Our framework demonstrated 84% accuracy in determining reasoning correctness for GPT-4 Omni and 86% for Gemini 1.0 Pro. While the accuracy of error detection was high for both models, the improvement due to corrective feedback was significant for GPT-4 Omni (ranging from 27 to 73 percentage points) but more moderate for Gemini 1.0 Pro (10 to 27 percentage points). Notably, after receiving verification-based feedback, the cooperation rate for the Prisoner's Dilemma (PD) aligned more closely with cooperation rates observed in human participant experiments (B\u00f3 2005). GPT-4 Omni exhibited a stronger ability to utilize feedback effectively. However, it's important to note that while Gemini 1.0 Pro often was not effective in using the feedback for error correction, in some cases, it exhibited errors related to incorrect calculations of expected payoffs errors not addressed by the developed predicates. This finding suggests that predicates need to be tailored not only to the problem domain but also to the specific model.\nDespite the high accuracy of incorrect reasoning detection, it was not ideal. The detection accuracy depended on the accuracy of translating natural language into predicates, a task performed by another instance of an LLM. While the translation to a formal representation, known as autoformalization (Wu et al. 2022), is a simpler task than generating correct reasoning, there remains room for improvement. One potential way to enhance translation accuracy, and thus error detection, is to fine-tune the LLM instance responsible for the translation.\nLELMA Evaluation The moderate and fair inter-rater agreement values indicate that this evaluation task is challenging even for humans. The lower agreement for the more advanced model suggests that in open-ended tasks like the considered one, more sophisticated outputs require greater cognitive effort to identify errors, making them harder to detect. Of course, while manual evaluation is a necessary first step in this case, it is not scalable. The recent proliferation of LLMs has led to the development of various automated and semi-automated evaluation approaches, including LLM-derived metrics, prompting LLMs, human-LLM collaborative evaluation, and fine-tuning (Gao et al. 2024a). As an alternative to human evaluation, we tested prompting LLMs using an adapted version of our evaluation protocol. However, neither GPT-4 Omni nor GPT-4 Turbo produced reliable results. Among the listed evaluation methods, fine-tuning may be a promising direction, provided that a sufficiently large training dataset can be collected.\nLELMA Applications We demonstrated the effectiveness of the proposed approach in social simulations in the context of game theory. However, the LELMA framework is modular and can be adapted to any domain where a solver and a set of predicates can be defined. Autoformalization using an LLM can assist in developing both the solver and predicates. The framework can then be employed for error detection and providing corrective feedback. One potential application of this approach is bias detection.\nLimitations Some limitations of this study include the need for manual assessment of reasoning correctness, which limited the size of the evaluation sample. Another limitation is the accuracy of translating natural language into a formal representation, which affects the accuracy of reasoning error detection. Inaccurate translations can result in false positives, false negatives, and unintended improvements due to feedback based on incorrectly translated predicates."}, {"title": "Related Work", "content": "LLMs have significantly propelled the progress of conversational AI and NLP (Achiam et al. 2023), (Jiang et al. 2024), (Rohan Anil et al. 2024). However, for tasks such as reasoning, even SOTA models may perform poorly (Rae et al. 2022), (Nye et al. 2021). Chain of thought prompting (Wei et al. 2023) has been shown to improve the performance of LLMs in reasoning tasks. When complex tasks are decomposed into detailed step-by-step prompts, the arithmetic, symbolic and common sense reasoning performance of LLMs improves. Automatic feedback loops such as the self-refine pipeline (Madaan et al. 2023) also improve model output. (Madaan et al. 2023) use the same LLM to generate a response, analyse the response and use the feedback from the analysis to refine the initial output. In contrast, we use different LLMs, as the verbal reinforcement learning approach in (Shinn et al. 2023), but we follow an idea akin to (Patil et al. 2023) to generate specific API calls.\nSelf-correction relying on calls to external domain-specific tools to derive critiques and improve output accuracy has been shown to improve response quality (Gou et al. 2024), (Gao et al. 2024b). Moreover, LLMs can be used as cooperating actors in a multi-agent framework to solve complex tasks, e.g. LLM agents taking different roles of a software development team (Qian et al. 2023), or agents acting as specialized domain experts (Hong et al. 2023), including inter-agent and human-in-the-loop conversational cooperation (Wu et al. 2023)."}, {"title": "Conclusions and Future Work", "content": "LELMA is a novel framework designed to enhance the trustworthiness of social simulations employing LLM-based agents. To the best of our knowledge, it is the first framework to use a game solver and feedback loop to verify and self-correct the strategic reasoning of LLM-based agents. The evaluation of the GPT-4 Omni and Gemini 1.0 Pro models in game-theoretic scenarios highlighted reasoning errors related to payoff attribution and comparison. Experimental results demonstrated improved reasoning correctness for both, particularly GPT-4 Omni, when used within the LELMA framework. This work lays the foundation for more reliable LLM-based agents by providing mechanisms for detection of errors in reasoning and decision-making and self-correction.\nFuture work will focus on enhancing the accuracy of translating natural language into corresponding queries through fine-tuning. Additionally, we plan to validate our approach across a broader range of language models. Given LELMA's modular design, which allows for the easy substitution of different LLM agents, query sets, and solvers, we also aim to extend its application to other domains."}]}