{"title": "BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical Applications", "authors": ["Guido Manni", "Clemente Lauretti", "Francesco Prata", "Rocco Papalia", "Loredana Zollo", "Paolo Soda"], "abstract": "Endoscopic surgery relies on two-dimensional views, posing challenges for surgeons in depth\nperception and instrument manipulation. While Simultaneous Localization and Mapping (SLAM)\nhas emerged as a promising solution to address these limitations, its implementation in endoscopic\nprocedures presents significant challenges due to hardware limitations, such as the use of a monocular\ncamera and the absence of odometry sensors. This study presents a robust deep learning-based\nSLAM approach that combines state-of-the-art and newly developed models. It consists of three\nmain parts: the Monocular Pose Estimation Module that introduces a novel unsupervised method\nbased on the CycleGAN architecture, the Monocular Depth Estimation Module that leverages the\nnovel Zoe architecture, and the 3D Reconstruction Module which uses information from the previous\nmodels to create a coherent surgical map. The procedure's performance was rigorously evaluated\nusing three publicly available datasets (Hamlyn, EndoSLAM, and SCARED) and benchmarked\nagainst two state-of-the-art methods, EndoSFMLearner and EndoDepth. The integration of Zoe in the\nMDEM demonstrated superior performance compared to state-of-the-art depth estimation algorithms\nin endoscopy, whereas the novel approach in the MPEM exhibited competitive performance and\nthe lowest inference time. The results showcase the robustness of our approach in laparoscopy,\ngastroscopy, and colonoscopy, three different scenarios in endoscopic surgery. The proposed SLAM\napproach has the potential to improve the accuracy and efficiency of endoscopic procedures by\nproviding surgeons with enhanced depth perception and 3D reconstruction capabilities.", "sections": [{"title": "1. Introduction", "content": "Laparoscopy has become the preferred surgical method\nfor various procedures across medical specialties [1]. How-\never, a significant limitation of this technique is its reliance\non two-dimensional views [2], and inferring depth from 2D\nimages can be challenging for surgeons and requires years\nof experience [3].\nTo address this challenge, there has been growing interest in\nintegrating 3D imaging technology into surgical procedures.\nStudies have shown that 3D imaging reduces strain on sur-\ngeons, enhances procedural performance, and increases the\naccuracy of instrument manipulation [4, 5]. These benefits\nlead to better procedural outcomes and shortened learning\ncurves for surgical trainees [6, 7, 8].\nIn recent years, camera-based tracking and mapping meth-\nods have gained popularity in surgical 3D imaging research\n[1]. Approaches such as mosaicking, Structure from Motion,\nand Shape from Template have been explored, but they\nface limitations in the surgical context. Mosaicking stitches\ntogether overlapping images to create a comprehensive\nview of the scene [9, 10, 11]: however, it struggles with\nillumination variations, camera motion, moving objects,\nand noise, which can lead to artifacts and reduced visual\nquality in surgical environments. Structure from Motion\nreconstructs 3D structures from 2D images taken from\ndifferent viewpoints [12, 13]: it faces challenges such as\ncomputational complexity, sensitivity to noise and outliers,\nlimited camera calibration, and difficulties handling com-\nplex surgical scenes with moving objects, occlusions, and\nreflections. Shape from Template estimates the 3D shape\nof a deformable object by aligning a known template to\nobserved images [14, 15], but, it is affected by template\nsize and complexity, image quality and noise, rotation and\nscaling issues, partial occlusion, and the presence of similar\nshapes or clutter in surgical scenes.\nIn contrast, Simultaneous Localization and Mapping (SLAM)\nhas emerged as a promising solution to address these lim-\nitations [16, 17, 18]. SLAM algorithms simultaneously\nestimate the camera pose and construct a map of the envi-\nronment in real-time, so that they can effectively handle the\ndynamic and complex nature of surgical scenes, including\nmoving objects, occlusions, and deformations. However,\nimplementing SLAM in endoscopic procedures presents sig-\nnificant challenges. Hardware limitations, such as the use of\na monocular camera due to size constraints and the absence\nof odometry sensors, led SLAM relying solely on visual\ninformation for its core processes, such as localization and\nmapping. Various environmental factors further complicate\nthe application of SLAM, including low-texture surfaces\n[19], lighting variability [20, 21], organ deformation [22],\nand the presence of blood, fluids, and smoke [23, 24].\nTo overcome these limitations in this paper we propose a\nrobust deep learning SLAM approach that effectively works\nin diverse surgical settings. Our framework combines state-\nof-the-art and newly developed models, consisting of three\nmain modules:\n1. The Monocular Depth Estimation Module (MDEM),\nwhich employs a novel deep learning technique that\ndemonstrates exceptional generalization abilities across\nvarious imaging contexts.\n2. The Monocular Pose Estimation Module (MPEM),\nusing a novel unsupervised method we developed that\nlearns to estimate the relative camera pose between\nconsecutive frames.\n3. The 3D Reconstruction Module (3DM), that intro-\nduces a multi-phase process that generates point clouds,\nrefines pose estimates, and converts the aligned point\nclouds into a volumetric representation. The module\naddresses incremental drift error and scale ambiguity\ninherent in monocular pose estimation.\nTo rigorously test our approach and demonstrate its gen-\neralization capabilities, we conducted comprehensive eval-\nuations in three surgical environments: laparoscopy, gas-\ntroscopy, and colonoscopy. To this end, we used the Hamlyn\n[17], EndoSlam [16], and SCARED [25] datasets, each\noffering unique features for validating our proposed method.\nWe also benchmarked our approach against two state-of-the\nart methods [17, 16], which are presented in section 5.\nThe remainder of this paper is organized as follows: section\n2 provides an overview of the related work. Section 3 de-\nscribes our proposed approach, detailing the three primary\nmodules of our framework. Section 4 presents the mate-\nrials used in this study, including the training, validation,\nand testing datasets. Section 5 outlines the experimental\nconfiguration, including the comparative analysis against\ntwo state-of-the-art approaches, performance metrics,\nand statistical analysis. Section 6 presents the experimental\nresults. Finally, section 7 provides concluding remarks."}, {"title": "2. Related Works", "content": null}, {"title": "2.1. Traditional Monocular Visual SLAM\nArchitecture", "content": "A classical Monocular Visual SLAM (MVSLAM) sys-\ntem typically consists of several integrated components, as\nillustrated in Figure 1A.\nThe framework begins with a feature extraction mod-\nule, which identifies salient features (like ORB, SURF, and\nSIFT [26, 27, 28]) within the images. A feature-matching\nalgorithm then establishes correspondences between these\nfeatures across consecutive frames. The matched features\nare passed into a motion estimation algorithm and a depth\nmap estimation algorithm to estimate the relative motion\nbetween consecutive frames and generate a sparse depth\nmap, respectively. These outputs are utilized by a map-\nbuilding algorithm to reconstruct the environmental model.\nAdditionally, the MVSLAM framework incorporates an op-\ntimization module, which executes procedures such as bun-\ndle adjustment [29] and pose graph optimization to refine the\nmap and pose estimates."}, {"title": "2.2. Challenges and Limitations of Traditional\nMVSLAM in Endoscopic Surgery", "content": "Traditional feature-based methodologies in MVSLAM\nhave encountered notable challenges in the context of endo-\nscopic surgery due to the unique nature of surgical images.\nEndoscopic images often lack discernible features and are\nsubject to lighting variations, posing substantial difficulties\nfor feature extraction and matching algorithms. For example,\na modified version of ORB-SLAM [30], which utilized a\nlimited number of keyframes for dense reconstruction, strug-\ngled with soft, texture-less tissue, a common characteristic\nin Minimally Invasive Surgery (MIS). This highlights the in-\nherent challenge of detecting robust features in environments\nwhere texture and contrast are minimal.\nEarly SLAM techniques, such as those relying on Struc-\nture from Motion [31], aimed to concurrently track cam-\nera motion and reconstruct the 3D environment. However,\nthese methods were severely limited by the low-texture\nenvironments typical of MIS. Structure from Motion relies\non robust wide-baseline feature matching, which becomes\nuntenable in texture-deficient surgical scenes. Moreover, the\ncomputational expense of Structure from Motion is another\ndrawback, as it requires processing all images collectively to\noptimize the 3D structures and camera poses, which is not\nalways feasible in real-time surgical applications."}, {"title": "2.3. Deep Learning-based MVSLAM Architecture", "content": "To address the limitations of traditional MVSLAM\nmethods in endoscopic surgery, researchers have turned\nto deep learning-based approaches. As shown in Figure\n1B, a fully deep learning-based MVSLAM framework re-\nplaces the conventional feature matching, feature extraction,\nmotion estimation, and depth estimation algorithms with\ndeep learning models. These models can learn to extract\nrelevant features and estimate depth and motion directly\nfrom the input images, even in the presence of low-texture\nenvironments and lighting variations.\nDeep learning models offer several advantages over tra-\nditional MVSLAM methods in the context of endoscopic\nsurgery. They can adapt to the dynamic, texture-sparse, and\nunpredictable nature of surgical environments by learning\nto extract relevant features and estimate depth and motion\ndirectly from the data. This eliminates the need for hand-\ncrafted feature detectors and matching algorithms, which\noften struggle in low-texture environments."}, {"title": "2.4. Advancements in Deep Learning-based\nMVSLAM for Endoscopic Surgery", "content": "Early applications of deep learning in MVSLAM for en-\ndoscopic surgery have shown promising results. For exam-\nple, Chen et al. [32] used Generative Adversarial Networks\n(GANs) for depth estimation, demonstrating the ability to\novercome challenges related to feature scarcity, tissue homo-\ngeneity, surface deformation, and highly variable specular\nappearances.\nHybrid approaches, such as the Endo-Depth-and-Motion\nframework [17], have attempted to integrate deep learn-\ning with traditional techniques. This framework employs\nMonoDepthV2 [33] for depth estimation and a photomet-\nric approach inspired by PTAM [34] for pose estimation.\nWhile this approach showed improvements over traditional\nmethods, it still encountered limitations in prolonged tasks,\nas pose estimation was prone to failure over time.\nFully deep learning-based SLAM systems, such as En-\ndoSLAM [16] for gastroscopy, have further advanced the\nfield by employing deep learning methods for both depth\nand pose estimation. These approaches have demonstrated\nthe potential of deep learning to provide more robust and\nadaptable solutions for MVSLAM in the surgical environ-\nment."}, {"title": "2.5. Challenges in Deep Learning-based\nMVSLAM for Endoscopic Surgery", "content": "Despite the advancements in deep learning-based MVS-\nLAM for endoscopic surgery, several challenges remain.\nOne major challenge is the need for large amounts of diverse\ntraining data to develop models that can generalize well\nacross different surgical scenarios. Collecting and annotat-\ning such datasets can be time-consuming and expensive.\nAnother challenge is the computational complexity of\ndeep learning models, which can hinder real-time perfor-\nmance in resource-constrained surgical environments. Bal-\nancing model accuracy and efficiency is crucial for the prac-\ntical deployment of deep learning-based MVSLAM systems\nin endoscopic surgery."}, {"title": "2.6. Distinctive Elements of the Proposed\nMVSLAM Framework", "content": "Our proposed MVSLAM framework introduces several\ndistinctive elements that set it apart from the existing litera-\nture and address the challenges mentioned above:\n1. Integration of the state-of-the-art Zoe model [35] for\nmonocular depth estimation. This model has demon-\nstrated superior generalization capabilities across di-\nverse datasets, enabling our framework to perform\nwell in various surgical scenarios without the need for\nextensive fine-tuning.\n2. A novel pose estimation module, named CyclePose,\nwhich leverages the power of Generative Adversarial\nNetworks (GANs) to estimate the relative camera pose\nbetween consecutive frames. CyclePose is specifically\ndesigned to enhance the accuracy and robustness of\npose estimation in the challenging surgical setting,\naddressing the limitations of previous approaches.\n3. A comprehensive 3D reconstruction module that inte-\ngrates the outputs from the depth and pose estimation\nmodules to generate detailed and coherent 3D models\nof the surgical scene. The module incorporates several\noptimization techniques, such as pose graph optimiza-\ntion and the Iterative Closest Point (ICP) algorithm, to\nrefine the pose estimates and enhance the alignment of\nthe reconstructed point clouds. This ensures the gen-\neration of accurate and visually appealing 3D models,\nwhich can be valuable for surgical planning, guidance,\nand post-operative analysis.\nBy introducing these distinctive elements, our proposed\nMVSLAM framework aims to address the limitations of cur-\nrent methods in endoscopic surgery, such as the inability to\nhandle feature-poor imagery, occlusions by surgical instru-\nments, uniform textures, and variable lighting conditions."}, {"title": "3. Methods", "content": "The proposed MVSLAM approach is composed of three\nkey modules: the Monocular Depth Estimation Module\n(MDEM), the Monocular Pose Estimation Module (MPEM),\nand the 3D Reconstruction Module (3DM), as shown in\nFigure 2. The structure of the framework is designed to\nensure that while the MDEM and MPEM operate inde-\npendently, their outputs synergistically contribute to the\neffective functioning of the 3DM. The workflow begins\nwith the MPEM (section 3.1), where the RGB input is\nprocessed using CyclePose to estimate the relative motion of\nthe camera between consecutive frames, outputting a motion\nmatrix M = [R, $t_{unscaled}$, 1,0], where R is the rotation\nmatrix and $t_{unscaled}$ is the unscaled translation vector due to\nthe scale ambiguity inherent in monocular pose estimation.\nSimultaneously, the MDEM (section 3.2) estimates the\ndepth map of the scene from the RGB input. Given that\nthe translation component ($t_{unscaled}$) of the relative motion\nmatrix is unscaled, a dedicated module, comprising a trans-\nlation estimation module and an Unscented Kalman Filter,\nis tasked with scale correction. The translation estimation\nmodule estimates a scaled translation vector $t_{scaled}$, which\nis then combined with $t_{unscaled}$ using the UKF to correct\nthe scale of the motion matrix. The 3DM (section 3.3) then\nemploys the outputs from both the MDEM and MPEM.\nIt utilizes the depth map from the MDEM and the pose\nestimations from the MPEM to generate an initial point\ncloud of the current field of view of the endoscopic camera.\nThis point cloud is carefully merged with the broader scene\npoint cloud, leveraging the pose information to ensure\nprecise alignment and integration."}, {"title": "3.1. Monocular Pose Estimation Module (MPEM)", "content": "The first step of the approach is based on CyclePose, a\nmodified version of CycleGAN, inspired by RetinaGAN [36]\nand InfoGAN [37]. The main idea is to learn the estimation\nof the relative camera pose between two consecutive frames,\ndenoted as $f_{i-1}$ and $f_i$, through the pre-task of image-\nto-image translation in a totally unsupervised manner. We\nconsider this pre-task as a function $G : f_{i-1} \\rightarrow f_i$, which\ngenerates the current frame $f_i$ from the previous frame $f_{i-1}$.\nSimilarly, we define the function $F : f_i \\rightarrow f_{i-1}$, which\ngenerates the previous frame $f_{i-1}$ from the current frame\n$f_i$. To improve the performance of this image-to-image\ntranslation pre-task, we draw inspiration from InfoGAN's\nmanipulation of the latent space through the maximization\nof mutual information. We propose to manipulate the latent\nspace z produced by the generator encoder E by concate-\nnating a prediction of the relative pose M between the two\nconsecutive frames to z. The underlying concept is that if the\npredicted pose M is correct, the image-to-image translation\ntask $G(f_{i-1}, M) \\approx f_i$ will also be successful. Conversely,\nif the estimated pose is incorrect, the pre-task will fail. To\naccommodate this, the architecture has been modified at the\nbottleneck level of the generator architecture (right panel of\nFigure 3). The modified generator now performs two tasks:\n\u2022 Predicting the pose M between two concatenated\nconsecutive frames ($f_{i-1}$ and $f_i$). This is done by pass-\ning the concatenated frame $f_c = [f_{i-1}, f_i]$ through\nthe generator encoder E, which then feeds into the\npose estimation tail P. The pose network P outputs\nthe relative motion matrix M in quaternion form to\navoid gimbal lock and improve numerical stability.\nThe quaternion is immediately converted to a matrix\nrepresentation M = [R, $t_{unscaled}$, 1,0], where R is the\nrotation matrix and $t_{unscaled}$ is the unscaled transla-\ntion vector, for better integration within the SLAM\npipeline. The final linear layer of the pose network is\nresponsible for outputting M.\n\u2022 Generating the next frame (image-to-image transla-\ntion pre-task). First, the previous frame $f_{i-1}$ is passed\nthrough the encoder E. The resulting latent space\nz = E($f_{i-1}$) is then concatenated with the predicted\npose M from the first task. This concatenated latent\nspace [z, M] is then fed into the generator G, which\nproduces the next frame $F_i = G(z, M)$.\nWe now discuss the objective loss function, which has been\noptimized for the task of pose estimation. The objective\nfunction consists of three terms:"}, {"title": "consistency, the term $L_{cycPose}(G, F)$ is defined as follows:", "content": "$L_{cycPose}(G, F) = L_{chordal}(R_{i-1,i}, \\hat{R}_{i-1,i}) + L_1(t_{i-1,i}, \\hat{t}_{i-1,i}) + L_{chordal}(R_{i,i-1}, \\hat{R}_{i,i-1}) + L_1(t_{i,i-1}, \\hat{t}_{i,i-1})$\nwhere $R_{i-1,i}$ and $\\hat{R}_{i-1,i}$ are the rotation components of the estimated pose between the real frames ($f_{(i-1)real}, f_{(i)real}$) and the generated frames ($f_{(i-1)recov}, F_{(i)real}$), respectively.\nSimilarly, $t_{i-1,i}$ and $\\hat{t}_{i-1,i}$ are the translation vectors of the es-\ntimated pose between the real frames ($f_{(i-1)real}, f_{(i)real}$) and the generated frames ($f_{(i-1)recov}, F_{(i)real}$), respectively. The terms $R_{ii-1}$, $\\hat{R}_{ii-1}$, $t_{i,i-1}$ and $\\hat{t}_{i,i-1}$ are defined analogously for the mapping $F$. The chordal loss $L_{chordal}$ is defined as:\n$L_{chordal}(R, \\hat{R}) = ||R - \\hat{R}||_F$\nwhere $||.||_F$ denotes the Frobenius norm. The $L_1$ loss is defined as:\n$L_1(t, \\hat{t}) = \\sum_{i=1}^n |t_i - \\hat{t}_i|$\nwhere n is the dimension of the translation vector. The chordal loss constrains the rotation component of the esti-mated relative pose, while the $L_1$ loss constrains the trans-lation component.\nThe overall workflow (left part of Figure 3) is as follows:"}, {"title": "3.2. Monocular Depth Estimation Module\n(MDEM)", "content": "The Monocular Depth Estimation Module is a crucial\ncomponent of our MVSLAM approach, responsible for\nestimating depth from the endoscopic camera frames. Ac-\ncurate depth estimation is essential for the subsequent 3D\nreconstruction process.\nTo address the challenges in monocular depth estimation,\nparticularly generalization across diverse datasets, our MDEM\nemploys the innovative architecture of Zoe [35]. Zoe model\nutilizes a distinctive two-stage framework:\n1. Pre-training on a wide array of datasets for relative\ndepth estimation, which fosters excellent generaliza-\ntion ability.\n2. Adding heads for metrics depth estimation and fine-\ntuning on metrics depth datasets.\nThis approach allows Zoe to maintain metrics scale\nwhile benefiting from the generalization capabilities ob-\ntained during the relative depth pre-training phase.\nZoe architecture features a novel metrics bins module, which\ntakes multi-scale features from the MiDaS [38] decoder and\npredicts bin centres crucial for metrics depth prediction. The\nmodule predicts all bin centres at the initial stage and adjusts\nthem at subsequent decoder layers using attractor layers,\nenabling more accurate and adaptable depth estimation.\nAnother innovative aspect of Zoe is its use of the log-\nbinomial method for final metrics depth prediction, instead\nof the conventional softmax approach. This method linearly\ncombines the bin centres, weighted by their probability\nscores, enhancing the model's ability to accurately predict\ndepth in a structured and ordered manner.\nIntegrating Zoe into our MVSLAM approach represents a\nsignificant advancement in-depth estimation for endoscopic\napplications. Its architecture, combining the strengths of\nrelative and metrics depth estimation and utilizing novel\ncomponents, makes it exceptionally suited for the challeng-\ning and variable conditions of endoscopic surgery.\nIn our approach, we chose to assess Zoe's out-of-the-box\nperformance to evaluate its generalization capabilities across\ndomains, without retraining or fine-tuning the model specif-\nically for the surgical context (section 6.2)."}, {"title": "3.3. 3D Reconstruction Module (3DM)", "content": "The 3D reconstruction module generates three-dimensional\nmodels from endoscopic images through a multi-phase\nprocess. The key steps involved are:\n1. Constructing a point cloud from the endoscopic cam-\nera perspective using pseudo-RGBD frames produced\nby the MDEM.\n2. Refining the pose estimate provided by the MPEM\nthrough multiple optimization steps.\n3. Aligning and integrating the newly acquired point\ncloud with the pre-existing reconstructed scene, guided\nby the refined pose estimate.\nOne of the critical aspects of this module is the optimization\nprocedures applied to enhance the pose estimate derived\nfrom the MPEM. The raw pose estimate cannot be directly\nemployed for point cloud alignment and merging due to scale\nambiguity inherent in monocular pose estimation.\nTo address this issue, a dedicated sub-module for scale\ncorrection has been implemented. This sub-module employs\na classical pose estimation algorithm that leverages features\nextracted (SIFT or ORB) from pseudo-RGBD frames to es-\ntimate a pose. Although less effective for pose estimation in\nlow-texture environments, it is viable for deriving a pseudo-\nscale.\nAn Unscented Kalman Filter (UKF) is utilized to fuse the un-\nscaled translation vector $t_{unscaled}$ obtained from the MPEM\nwith the scaled translation vector $t_{scaled}$ estimated by the\ntranslation estimation module. The state vector of the UKF\nis defined as:\nx = [$t_x, t_y, t_z$]^T\nwhere $t_x, t_y,$ and $t_z$ are the components of the translation\nvector. The measurement model of the UKF is given by:\nz = [$t_{scaled,x}, t_{scaled,y}, t_{scaled,z}$]^T\nwhere $t_{scaled,x}, t_{scaled,y},$ and $t_{scaled,z}$ are the components of the\nscaled translation vector obtained from the RGB-D odome-\ntry or the scaling factor computation.\nThe UKF operates in two main steps:\n1. prediction step: The UKF predicts the next state $\\hat{x}$\nusing the unscaled translation vector $t_{unscaled}$ obtained\nfrom the MPEM:\n$\\hat{x} = f(x, t_{unscaled})$\nwhere f is the state transition function that uses the\nMPEM to predict the next state.\n2. Update step: The UKF updates the predicted state\n$\\hat{x}$ using the scaled translation vector $t_{scaled}$ obtained\nfrom the RGB-D odometry or the scaling factor com-\nputation:\nx = $\\hat{x}$ + K(z - h($\\hat{x}$))\nwhere K is the Kalman gain, z is the measurement\nvector containing the scaled translation, and h is the\nmeasurement function that maps the predicted state to\nthe measurement space.\nThe corrected translation vector $t_{corrected}$ is then ob-\ntained from the updated state of the UKF:\nt_{corrected} = [$x_1, x_2, x_3$]^T\nwhere $x_1, x_2$, and $x_3$ are the scaled translation components\nof the updated state vector.\nThe corrected translation vector $t_{corrected}$ is used to up-\ndate the motion matrix M obtained from the MPEM, result-\ning in a scaled motion matrix $M_{scaled}$:\nM_{scaled} = [R, $t_{corrected}$, 1, 0]\nwhere R is the rotation matrix obtained from the MPEM.\nAdditionally, pose graph optimization is periodically em-ployed to mitigate the cumulative effect of drift by evenly\ndistributing the error across all estimated poses.\nUpon aligning the point cloud, the module utilizes the Trun-cated Signed Distance Function (TSDF) [39] technique toconvert the point cloud data into a comprehensive volumet-ric representation. The TSDF is a volumetric representationthat stores the signed distance to the nearest surface at eachvoxel. It allows for efficient fusion of depth informationfrom multiple viewpoints and enables the generation of asmooth and continuous surface reconstruction. The sceneis partitioned into voxels, each storing a cumulative signeddistance function. This voxel-based approach is continuallyupdated through sequential averaging, reflecting the mostcurrent information captured by the endoscopic camera."}, {"title": "4. Materials", "content": "In this study, we employed a comprehensive approach to\ntrain, validate, and test our proposed method, utilizing the\ndiverse datasets listed in table 1 to ensure robustness and\ngeneralizability in endoscopic applications."}, {"title": "4.1. Training Datasets for CyclePose", "content": "The training and validation processes were performed\non the CyclePose module because it was the only model\nwe trained in this study. Two distinct datasets were utilized\n- an extensive internal dataset for training and selected\nsubsets from the EndoSLAM dataset [16] for validation.\nFor training, we leveraged a large internal unlabeled dataset\ncomprising more than 300 hours of gastroscopy and prosta-\ntectomy videos, collected from 100 patients undergoing\nthese procedures at Fondazione Policlinico Universitario\nCampus Bio-Medico. In total, this training dataset contained\n2,250,900 frames, allowing us to effectively train our pose\nestimation network on a substantial amount of endoscopic\ndata. To prepare the training data, we processed the videos\nby extracting individual frames and performing a center crop\nof size 128x128 pixels on each frame. This center cropping\napproach enhances system robustness by training it to work\neffectively with a reduced receptive field, while avoiding\nany distortion that may arise from resizing the frames. By\nfocusing on the central region of each frame, we ensure\nthat the most relevant visual information is preserved while\nmaintaining the spatial integrity of the data. To validate\nCyclePose, we used the following subsets from the En-\ndoSLAM dataset, which are completely separate from the\ntraining data: HighCam Colon IV Tumor-Free Trajectory\n1, LowCam Colon IV Tumor-Free Trajectory 1, HighCam\nColon IV Tumor-Free Trajectory 5, and LowCam Colon IV\nTumor-Free Trajectory 5. These subsets account for four\nvideo sequences with a total of 3024 frames, providing a\ndiverse range of scenarios to assess CyclePose's inference\nperformance. Importantly, there is no overlap between the\ntraining and validation datasets, as the EndoSLAM subsets\nwere specifically chosen for validation purposes only and\nwere not used in any way during the training phase."}, {"title": "4.2. Testing Datasets for Depth and Pose\nEstimation Modules", "content": "To evaluate the performance of our depth estimation\nand pose estimation modules we utilized three datasets with\ndistinct characteristics, as summarized in Table 1. They are:\n1. Hamlyn Dataset [17]: We employed an enhanced ver-\nsion of the Hamlyn dataset, which contains stereo\nimages from endoscopic interventions on porcines.\nThis dataset, augmented with depth ground truth de-\nrived from stereo images using Libelas, consists of\n78,160 frames across 20 videos, making it suitable for\ntesting the depth estimation module of our method.\nHowever, it lacks pose ground truth, limiting its utility\nfor assessing the pose estimation module.\n2. EndoSLAM Dataset [16]: In addition to its use in\nvalidating CyclePose, the EndoSLAM dataset was\nalso employed for testing the pose estimation module\nof our method. The subsets used for validating Cycle-\nPose were excluded from the testing process to main-\ntain a clear separation between validation and testing\ndata. While the EndoSLAM dataset includes depth\ndata, we did not use it for testing depth estimation due\nto its synthetic nature. The dataset comprises both ex-\nvivo and synthetic porcine data, with 35 sub-datasets\ntotaling 76,837 frames spanning various gastrointesti-\nnal areas, providing pose data for a comprehensive\nevaluation of our method's performance in pose es-\ntimation.\n3. SCARED Dataset [25]: The SCARED dataset, recorded\nusing a da Vinci Xi surgical robot, consists of 7\ntraining and 2 test sets, each corresponding to a single\nporcine subject. Each set contains 4 to 5 keyframes\nrepresenting distinct scene views, with structured light\npatterns projected for dense stereo reconstruction.\nThe movement of the endoscope enabled the capture\nof camera poses relative to the robot base, and the\nprojection of 10-bit Gray code patterns onto the scene\nensured unique pixel encoding for efficient stereo\nmatching and depth recovery. The SCARED dataset\nprovides both depth maps and ground truth poses,\nmaking it suitable for testing both the depth and pose\nestimation modules of our method.\nBy leveraging these diverse datasets, we were able to thor-\noughly test our depth estimation and pose estimation mod-\nules across various scenarios and data types, ensuring their\nrobustness and generalizability in endoscopic applications."}, {"title": "5. Experimenental Configuration", "content": "To validate the effectiveness and generalization capabil-\nities of our proposed MVSLAM approach, we conducted a\ncomprehensive comparative analysis against two state-of-\nthe-art approaches [1] in the field of endoscopic surgery:\nEndoSLAM [16] and EndoDepth [17]. These methods were\nselected based on their proven performance and distinct ap-\nproaches to addressing the challenges inherent in endoscopic\nimagery. EndoSLAM, also known as EndoSfMLearner, is an\nunsupervised monocular visual odometry and depth estima-\ntion approach designed specifically for endoscopic videos.\nIt focuses on depth estimation, pose estimation, and 3D\nreconstruction. The depth estimation component, based on\n[40], predicts dense disparity maps from single images using\nspatial attention mechanisms and a hybrid loss function. The\npose estimation component calculates the relative 6-DoF\ncamera poses between consecutive image frames, leveraging\na spatial attention block (ESAB) to emphasize texture details\nand depth differences.\nEndoDepth presents a comprehensive approach for 3D re-\nconstruction and camera motion estimation in monocular\nendoscopic sequences. It utilizes self-supervised depth net-\nworks, photometric tracking, and volumetric fusion to gen-\nerate pseudo-RGBD frames, track the camera pose, and fuse\nregistered depth maps into a coherent 3D scene model. The\ndepth estimation component employs the Monodepth2 [33]\nnetwork architecture, which is self-supervised and operates\non a U-Net encoder-decoder structure with a ResNet18\nencoder. Both EndoSLAM and EndoDepth were utilized as\ndeveloped by their respective authors, without any modifi-\ncations or retraining. The training and validation contexts\nof these models are also significant. EndoDepth validation\non the Hamlyn Dataset and EndoSLAM on the EndoSLAM\ndataset provide a varied baseline for assessing the general-\nization capabilities of our proposed MVSLAM approach."}, {"title": "5.1. Performance Metrics and Statistical Analysis", "content": "To evaluate the performance of the SLAM approach,\nwe employed a set of well-established metrics for depth\nestimation and pose estimation. For depth estimation, we\nselected the Absolute Relative Difference (Abs, Rel. Diff),\n$Abs. Rel. Diff. = \\frac{1}{N} \\sum_{i=1}^N \\frac{|d_i^p - d_i^t|}{d_i^t}$  \n$Sq. Rel. = \\frac{1}{N} \\sum_{i=1}^N \\frac{(d_i^p - d_i^t)^2}{d_i^t}$ \n$RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N (d_i^t - d_i^p)^2}$ \n$RMSE-Log = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (log d_i^t - log d_i^p)^2}$  \n$Accuracy(\\delta) = \\frac{1}{N} \\sum_{i=1}^N \\delta_i$ with $\\delta_i = 1$ if $[\\frac{d_i^p}{d_i^t} \\, \\frac{d_i^t}{d_i^p"}]}