{"title": "The NPU-HWC System for the ISCSLP 2024 Inspirational and Convincing\nAudio Generation Challenge", "authors": ["Dake Guo", "Jixun Yao", "Xinfa Zhu", "Kangxiang Xia", "Zhao Guo", "Ziyu Zhang", "Yao Wang", "Jie Liu", "Lei Xie"], "abstract": "This paper presents the NPU-HWC system submitted to the\nISCSLP 2024 Inspirational and Convincing Audio Generation\nChallenge 2024 (ICAGC). Our system consists of two modules:\na speech generator for Track 1 and a background audio genera-\ntor for Track 2. In Track 1, we employ Single-Codec to tokenize\nthe speech into discrete tokens and use a language-model-based\napproach to achieve zero-shot speaking style cloning. The\nSingle-Codec effectively decouples timbre and speaking style\nat the token level, reducing the acoustic modeling burden on the\nautoregressive language model. Additionally, we use DSPGAN\nto upsample 16 kHz mel-spectrograms to high-fidelity 48 kHz\nwaveforms. In Track 2, we propose a background audio gen-\nerator based on large language models (LLMs). This system\nproduces scene-appropriate accompaniment descriptions, syn-\nthesizes background audio with Tango 2, and integrates it with\nthe speech generated by our Track 1 system. Our submission\nachieves the second place and the first place in Track 1 and\nTrack 2 respectively.", "sections": [{"title": "1. Introduction", "content": "Deep learning technologies have significantly advanced speech\nsynthesis, enabling the generation of high-quality and human- \nlike speech [1, 2, 3]. Recent breakthroughs, such as lan-\nguage models and diffusion models [4, 5], powered by massive\ndatasets and extensive model parameters, have pushed the ca-\npabilities of speech synthesis systems to new heights, making\nit possible to produce speech that matches human-level perfor- \nmance.\nTo generate highly realistic and synthetic speech close to\nnatural sound, it not only needs to generate high-quality vocals\nbut also needs to incorporate appropriate background sounds.\nThese background sounds serve to add layers of realism and\ncontext, significantly enhancing the overall auditory experience.\nExpressive speech ensures that the synthetic audio is engaging,\nconveying the intended styles and nuances of the speaker effec-\ntively. Additionally, background sounds also play a crucial role\nin improving the realism and context of the synthetic speech.\nThey provide an auditory environment that can place the speech\nin a specific setting, making it more realistic.\nTo promote the development of realistic speech synthe- \nsis, the International Conference on Chinese Spoken Language\nProcessing 2024 (ISCSLP 2024) has initiated the Inspirational\nand Convincing Audio Generation Challenge 2024 (ICAGC\n2024) [6]. This challenge focuses on generating high-quality\naudio close to real-world sound. The challenge is divided into\ntwo tracks. Track 1 involves a zero-shot speaker and style\ncloning, where participants use a speech sample from a tar-\nget speaker to clone the voice and modulate it to convey var-\nious themes, such as novel chapters and ancient Chinese po-\nems. Track 2 focuses on background audio generation. Parti-\ncipants are tasked with adding background sounds to the synthetic\nspeech from Track 1, accurately representing real-life scenarios,\nthereby enhancing the realism of the audio.\nThis paper describes our systems developed for both tracks\nin the ICAGC 2024 challenge. In Track 1, we implement a\nzero-shot speaker and style cloning speech generator. Specifi-\ncally, we use a Single-Codec [7] as our speech tokenizer, which\nexplicitly decouples timbre and speaking style at the token\nlevel, thereby reducing the acoustic modeling burden on the\nautoregressive model. Moreover, we adopt the DSPGAN [8]\nvocoder to upsample 16 kHz mel-spectrograms to high-fidelity\n48 kHz waveforms. In Track 2, we introduce a cascaded back-\nground audio generation system based on large language mod-\nels (LLMs), which generate scene-appropriate accompaniment\ndescriptions. Additionally, it synthesizes background audio\nwith Tango 2 and mixes it with the vocal speech produced by\nthe Track 1 system. Our proposed system achieves an average\nMean Opinion Score (MOS) of 3.63 in Track 1, ranking the sec- \nond place, and an average MOS of 3.67 in Track 2, securing the\nfirst place."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Zero-shot Speaker and Style Cloning", "content": "Zero-shot and style speaker cloning aims to generate speech in\nthe same style as an unseen speaker, given only a few seconds\nof acoustic reference. Speaker modeling-based approaches have\nbeen commonly used to achieve zero-shot speaker cloning. Sev-\neral works have focused on extracting a global speaker em-\nbedding from a given reference speech using a pre-trained\nspeaker verification (SV) model or a joint-trained speaker en-\ncoder [9, 10]. However, the limited information-capturing ca-\npability of a single vector in these global speaker modeling ap-\nproaches often results in less resemblance between synthetic\nspeech and the speaker's original voice. To address this, other\nworks have aimed to capture both global timbre information\nand local prosodic variations through multi-level speaker rep-\nresentation, thereby improving speaker similarity for zero-shot\nspeaker and style cloning [11, 12, 13, 14].\nRecent breakthroughs in speech synthesis have significantly\nenhanced speech realism by leveraging massive datasets and ex-\ntensive model parameters. NaturalSpeech 2 [4] introduced a"}, {"title": "3. Methodology", "content": "As illustrated in Figure 1, our system is structured into two\nmain components: the speech generator and the background au-\ndio generator. The speech generator is engineered to produce\nspeech that matches the timbre and style of the target speaker.\nMeanwhile, the background audio generator is designed to cre-\nate appropriate background sounds based on the speech tran-\nscripts. This dual-component system ensures that the synthetic\nspeech is not only realistic in terms of timbre similarity but also\nenriched with contextual sounds."}, {"title": "3.1. Zero-shot Speaker and Style Cloning", "content": "As shown in Figure 1 (a), the speech generator comprises four\nmain components: text and speech tokenizers for encoding dif-\nferent modal information, a language model for generating au-\ndio tokens, and a vocoder for reconstructing audio from mel-\nspectrograms."}, {"title": "3.1.1. Tokenizer", "content": "We adopt Byte Pair Encoding (BPE) as our text tokenizer rather\nthan traditional phonemes. BPE works by iteratively merging\nfrequently occurring characters or sequences of characters to\nform new encoding units, resulting in a more compact and effi-\ncient text representation. This approach helps not only to min-\nimize cumulative errors in text front-end processing but also to\ncapture the semantic information in the text better.\nFor speech tokenization, we utilize the Single-Codec.\nSingle-Codec targets the mel-spectrogram for modeling, effec-\ntively compressing speech information while preserving essen-\ntial details. It employs a decoupled Vector Quantized Varia-\ntional Autoencoder (VQ-VAE), segmenting the speech into tim-\nbre, acoustic environment-related time-invariant embeddings,\nand a discrete sequence related to pronunciation. Additionally,\nby integrating a BLSTM module, mixed sampling, and resam-"}, {"title": "3.1.2. Language Model", "content": "Given text-speech pairs {xi, si}N=0, where x represents the text\ntranscription and s denotes the corresponding audio sample, we\nprocess these pairs to generate speech tokens. The text token\nis obtained by the BPE tokenizer, while the speech token\ns is extracted by the Single-Codec from the speech s. Sim- \nilar to previous work, we employ a Language Model (LM)- \nbased approach for Text-to-Speech (TTS) generation. We use\na NanoGPT-architecture\u00b9 as a language model, parameterized\nby \u03b8, to predict speech tokens conditioned on both text and\nreference speech sref in an autoregressive manner. The ref- \nerence speech sref involves a randomly selected mel-spectrum\nclip from the target speech s and then encoded into a fixed-size\nembedding eref. The reference embedding eref, alongside the\ntext token and speech token s, are concatenated into a single\nsequence and employ a next-token prediction modeling by the\nGPT-based autoregressive model. To enhance the capability to\ndistinguish and accurately process the various types of input,\nwe employ distinct positional embeddings for text tokens t and\nspeech tokens s.\nAt the training stage, we employ the teacher-forcing\nscheme, in which the left-shifted sequence is employed as the\nmode inputs and the original sequence serves as the expected\noutputs. In order to retain textual information to guide prosody,\nthe cross-entropy loss is calculated on both text token and\nspeech token s:\nL = \u2212 \u2211 (log P (xn | \u3121<n, eref; 0))\nn=0\nM\n\u2211 (log P (Sm | X<N, S<m, Cref; 0))\nm=0\nWhere N is the length of the text sequence and M is the length\nof the speech sequence."}, {"title": "3.1.3. Vocoder", "content": "We implement frequency band expansion in the vocoder to\nenhance speech quality, particularly under conditions where\nthe primary training data comprises low-resolution 16 kHz au-\ndio. We utilize DSPGAN [8] to upsample the 16 kHz mel-\nspectrogram to a high-fidelity 48 kHz waveform which effec-\ntively bridges the gap between the inherent limitations of the\ntraining data and the demand for high-quality speech genera-\ntion."}, {"title": "3.2. Background Audio Generation", "content": "As shown in Figure 1 (b), we develop a LLM-based system\nto generate appropriate background audio for synthetic speech,\nconsisting of two main stages: first analyze the speech text\nto generate background sound description (Text-to-Description)\nand then synthesize background audio based on the generated\ndescription (Description-to-Audio)."}, {"title": "3.2.1. Text-to-Description", "content": "In the Text-to-Description stage, the goal is to generate descrip-\ntive texts of potential scenes corresponding to the speech tran-\nscripts using the semantic understanding capabilities of LLM.\nFor this purpose, we choose DeepSeek-V22, a robust Mixture-\nof-Experts (MoE) model known for its strength in language un-\nderstanding. However, we are facing two significant challenges.\nFirst, the LLM tends to produce complex and lengthy scene de-\nscriptions, which is problematic as TTA technology performs\nbetter with simpler descriptions. Second, the LLM could gener-\nate appropriate descriptions when the text involves concrete ob-\njects, such as ancient poems. However, in cases where the text\nlacks concrete elements or has very few (such as in philosophi-\ncal texts like the Analects), the LLM's generated scene descrip-\ntions tend to deviate from relevance, as texts without tangible\nobjects inherently lack a clear corresponding scene.\nTo address these challenges, we refine our prompting strat-\negy for the LLM, as shown in the right of figure 1. We limit the\ncomplexity of the descriptions in the prompts and provide the\nLLM with two options: to generate either a scene or a musical\ndescription. This ensures that the background audio generated\ncan effectively match and enhance both concrete and abstract\ntexts, making the synthetic output more versatile and contextu-\nally appropriate."}, {"title": "3.2.2. Description-to-Audio", "content": "We choose Tango 23 to generate background sounds. Tango\n2 is an advanced TTA model that combines a latent diffusion\nmodel, a textual-prompt encoder powered by the FLAN-T5 [26]\nlarge language model. It introduces unique features such as\nthe Audio-alpaca dataset, designed for preference alignment,\nand employs diffusion-DPO for training, which helps in pro-\nducing audio outputs that closely match human perceptual pref-\nerences. These methods make Tango 2 particularly effective\nin creating highly realistic and contextually aligned audio from\ntext prompts. We convert the scene or musical descriptions ob-\ntained from the Text-to-Description stage into corresponding\naudio. Furthermore, because the primary focus of the audio\nis vocal content, to ensure that the background audio does not\noverpower the main speech, we apply a 10dB gain reduction to\nthe generated background audio during mixing."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Dataset", "content": "We use over 80,000 hours of Mandarin speech data in our train-\ning set. This includes 10,000 hours of open-source data from re-\nsources like WenetSpeech4TTS [27] and DiDiSpeech [28]. The\nremaining 70,000 hours of data are sourced from Chinese pod-\ncasts and audiobooks available online. To obtain high-quality\naudio segments from single speakers, we employ the same data\nprocessing pipeline as it in WenetSpeech4TTS [27]."}, {"title": "4.2. Training Settings", "content": "We use r50k_base with tiktoken from OpenAI for text process-\ning, which features a vocabulary size of 50,257. Our Single-\nCodec, NanoGPT, and DSPGAN models are each trained inde-\npendently on distinct datasets tailored to their specific require-"}, {"title": "4.3. Evaluation metrics", "content": "We evaluate the performance of Track 1 on the reserved test set\nwith speaker embedding cosine similarity (SECS) and character\nerror rate (CER) in ASR. The test set comprised audio samples\nfrom 20 different speakers, with each speaker generating 430\nidentical sentences to maintain consistency across the evalua-\ntions. The SECS metric is computed by extracting speaker em-\nbeddings with Resemblyzer 5 and calculating the cosine sim-\nilarity. The CER is measured between transcripts of real and\nsynthesized utterances transcribed by Paraformer [29].\nIn the official competition, the evaluation metrics include\nQuality Mean Opinion Score (MOS), Similarity MOS, Emo-\ntion MOS, and Background Audio Matching Degree MOS. The\nQuality MOS measures the overall perceptual quality of the syn-\nthesized speech. The Similarity MOS assesses how closely the\nsynthesized speech mirrors the vocal characteristics of the tar-\nget speaker. The Emotion MOS evaluates the synthetic speech's\neffectiveness in conveying intended emotions that resonate and\ninspire listeners on a cognitive level, where native speakers\njudge the emotional impact and inspirational quality of the\nspeech. Finally, the Audio and Speech Convincing Matching\nDegree metric gauges how well audio enhancements like sound\neffects and background sounds integrate with the speech, en-\nhancing the overall realism."}, {"title": "4.4. Track 1", "content": "We conduct a comparative analysis between our speech gener-\nator and an open-source system, Chinese VALL-E 6 trained on\nWenetSpeech4TTS. As shown in Table 1, our system demon-\nstrates outstanding performance in both CER and SECS. It\nachieved a CER of 6.86%, significantly lower than VALL-E, in-\ndicating a substantial advantage in reducing character errors and\nenhancing speech accuracy. Additionally, our system achieves\nan SECS of 0.849, showcasing strong competitiveness, far sur-\npassing VALL-E. As shown in Table 1, our system outper-\nforms the open-source system in terms of CER. This superior-"}, {"title": "4.5. Track 2", "content": "As shown in Table 3, in the subjective tests for Track 2, our\nsystem outperforms another team in all aspects. A significant\nfactor contributing to our success is the use of a LLM to gener-\nate scene or music descriptions that better match the transcripts."}, {"title": "5. Conclusion", "content": "In this paper, we introduce background audio and speech gen-\neration system for IGAGC2 2024. For Track 1, we created a\nspeech generator capable of zero-shot speaking style cloning.\nWe employed Single-Codec as the speech tokenizer, which de-\ncouples timbre and speaking style at the token level, thus re-\nducing the acoustic modeling load on the autoregressive model.\nAdditionally, we used the DSPGAN vocoder to upsample 16\nkHz mel-spectrograms to high-fidelity 48 kHz waveforms. In\nTrack 2, we employ a cascading system to generate background\naudio using LLM. This process begins with the LLM creating\nscene-appropriate accompaniment descriptions. Next, these de-\nscriptions are used by Tango 2 to synthesize the background au-\ndios. Finally, the synthesized background audio is mixed with\nthe vocal audio produced in Track 1. Our system achieves an\naverage Mean Opinion Score (MOS) of 3.63 in Track 1, and an\neven higher average MOS of 3.67 in Track 2."}]}