{"title": "ParkingE2E: Camera-based End-to-end Parking Network, from Images to Planning", "authors": ["Changze Li", "Ziheng Ji", "Zhe Chen", "Tong Qin", "Ming Yang"], "abstract": "Autonomous parking is a crucial task in the intelligent driving field. Traditional parking algorithms are usually implemented using rule-based schemes. However, these methods are less effective in complex parking scenarios due to the intricate design of the algorithms. In contrast, neural-network-based methods tend to be more intuitive and versatile than the rule-based methods. By collecting a large number of expert parking trajectory data and emulating human strategy via learning-based methods, the parking task can be effectively addressed. In this paper, we employ imitation learning to perform end-to-end planning from RGB images to path planning by imitating human driving trajectories. The proposed end-to-end approach utilizes a target query encoder to fuse images and target features, and a transformer-based decoder to autoregressively predict future waypoints. We conduct extensive experiments in real-world scenarios, and the results demonstrate that the proposed method achieved an average parking success rate of 87.8% across four different real-world garages. Real-vehicle experiments further validate the feasibility and effectiveness of the method proposed in this paper. The code can be found at: https://github.com/qintonguav/ParkingE2E.", "sections": [{"title": "I. INTRODUCTION", "content": "Intelligent driving involves three main tasks: urban driving, highway driving, and parking maneuvers. Automated valet parking (AVP) and auto parking assist (APA) systems, crucial parking tasks within intelligent driving, offer significant improvements in parking safety and convenience. However, mainstream parking methods [1] are often rule-based, requiring the entire parking process to be decomposed into multiple stages such as environmental perception, mapping, slot detection, localization and path planning. Due to the intricate nature of these complex model architectures, they are more susceptible to encountering difficulties in tight parking spots or intricate scenarios.\nEnd-to-end (E2E) autonomous driving algorithms [3]\u2013[7] mitigate cumulative errors across modules by integrating perception, prediction, and planning components into a unified neural network for joint optimization. The application of end-to-end algorithms to parking scenarios helps decrease the dependence of parking systems on manually designed features and rules, providing a comprehensive, holistic, and user-friendly solution.\nWhile end-to-end autonomous driving has demonstrated significant advantages, most of the research has concentrated on simulation [8] without validating the algorithm's real-world effectiveness. In contrast to the complexity of urban environments and the hazards of highway driving, parking scenarios are characterized by low speeds, confined spaces, and high controllability. These features provide a feasible pathway for incrementally deploying end-to-end autonomous driving capabilities in vehicles. We develop an end-to-end parking neural network and validate the algorithm's feasibility in real-world parking situations.\nThis work extends our previous work E2E-Carla [2] by presenting an imitation-learning-based end-to-end parking algorithm, which has been successfully deployed and evaluated in real environments. The algorithm takes in surround-view images captured by on-board cameras, predicts future trajectory results, and executes control based on the predicted waypoints. Once the user designates a parking slot, the end-to-end parking network collaborates with the controller to automatically maneuver the vehicle into the parking slot until it is fully parked. The contributions of this paper are summarized as follows:\n\u2022\nWe designed an end-to-end network to perform parking task. The network converts the surround view images into Bird's Eye View (BEV) representation, and fuses it with the target parking slot features by employing the target features to query the image features. Due to the sequential nature of the trajectory points, we utilize an autoregressive approach based on transformer decoder to generate trajectory points.\n\u2022\nWe deployed the end-to-end model on real vehicles for testing and verified the feasibility and generalizability of the network model for parking across various real-world scenarios, offering an effective solution for end-to-end network deployment."}, {"title": "II. LITERATURE REVIEW", "content": "A. BEV Perception\nBEV representation offers at least two advantages over perspective representation. First, it easily integrates inputs from different modalities due to its clear physical interpretability. Secondly, the BEV view avoids perspective distortion issues, thereby reducing the complexity of downstream tasks such as planning. In recent years, the BEV representation has seen widespread adoption in perception systems. Unlike previous deep learning-based perception algorithms comprising a feature extraction module and a task head module, BEV perception incorporates an additional viewpoint conversion module alongside these two modules. This conversion module facilitates the transformation between the sensor view and the Bird's Eye View (BEV).\nLSS [23] utilizes BEV perception for detection and segmentation. This method acquires BEV features by estimating the depth distribution at each pixel of the feature map and projecting it onto the BEV plane. DETR3D [26] follows the basic paradigm of DETR [25] and employs sparse queries for 3D object detection. PETR [27] adds 3D positional embedding, which provides 3D positional information to 2D features, aiming to the neural network to implicitly learn depth. BEVFormer [28] adopts BEV queries for perception, and incorporates spatial cross-attention and temporal self-attention mechanisms to boost performance. BEVDepth [29] builds upon LSS and utilizes LiDAR points for depth supervision during training to enhance the depth estimation quality, thereby improving BEV perception performance. BEVFusion [30] extracts BEV features from both cameras and LiDAR data and fuses them in the BEV space.\nB. End-to-end Autonomous Driving\nIn contrast to the traditional module-based autonomous driving solutions, the end-to-end paradigm [9, 10] can mitigate accumulated errors, prevent information loss across module and minimize redundant computations. Consequently, it has emerged as a popular and prominent research topic in the field of autonomous driving tasks.\nThe research on end-to-end driving initially focused on autonomous urban driving tasks. ChauffeurNet [11], an imitation-learning-based end-to-end method, learned effective driving strategies from expert data. Many methods have adopted an encoder-decoder framework that extracted BEV features from sensors and then utilizes GRU (Gate Recurrent Unit) decoder to predict waypoints in an autoregressive manner, such as Transfuser [3, 12], Interfuser [13], and NEAT [14]. Besides, CIL [15] and CILRS [16] developed a neural network that directly maps front-view images, current measurements, and navigation commands into control signals without a separate PID controller. MP3 [17] and UniAD [7] propose a modular design but jointly optimize all components in an end-to-end manner.\nIn recent years, end-to-end networks have been developed for parking scenarios. Rathour et al. [18] proposed a two-stage learning framework to predict the steering angles and gears from images. In the first stage, the network predicts an initial estimate of a sequence of steering angles. In the second stage, an LSTM (Long Short-Term Memory) network are used to estimate the optimal steering angles and gears. Li et al. [19] trained a CNN (Convolutional Neural Network) on rear-view images to automatically control steering angle and velocity. ParkPredict [20] proposed a parking slot and waypoints prediction network based on a CNN-LSTM architecture. In the following work, ParkPredict+ [21] designed a transformer and CNN-based model to predict future vehicle trajectories based on intent, image, and historical trajectory.\nExisting end-to-end autonomous driving methods often demand substantial computational resources, pose training challenges, and face difficulties in real-vehicle deployment. On the other hand, parking approaches exemplified by ParkPredict primarily focus on prediction from aerial imagery, which differs from our task. Our method propose an end-to-end parking planning network that utilizes an autoregressive transformer decoder to predict future waypoints from BEV features extracted from RGB images and target slot."}, {"title": "III. METHODOLOGY", "content": "A. Preliminaries: Problem Definition\nWe use end-to-end neural network No to imitate expert trajectories for training, defining the dataset:\n$D = \\{(I_{i,k}, P_{i,j}, S_i)\\}$,\nwhere trajectory index $i \\in [1, M]$, trajectory points index $j \\in [1, N_i]$, camera index $k \\in [1, R]$, RGB image $I$, trajectory point $P$ and target slot $S$. Reorganize the dataset into:\n$T_{i,j} = \\{P_{i,min(j+b,N_i)}\\}_{b=1,2,...,Q}$,\nand\n$D' = \\{(I_{j,k}, T_{i,j}, S_i)\\}$,\nwhere $Q$ denotes the length of the predicted trajectory points and $R$ denotes the number of RGB cameras.\nThe optimization goals for the end-to-end network are as follows:\n$\\theta' = arg \\min_\\theta E_{(I,T,S)\\sim D'}[L(T,N_\\theta(I, S))],$\nwhere $L$ denotes the loss function.\nB. Camera-based End-to-end Neural Planner\n1) Overview: As shown in Fig. 2, we developed an end-to-end neural planner that takes RGB images and a target slot as input. The proposed neural network includes two main parts: an input encoder and an autoregressive trajectory decoder. With the input of RGB images and the target slot, the RGB images are transformed to BEV features. Then, the neural network fuses BEV features with the target slot and generates the next trajectory point in an autoregressive way using transformer decoder.\n2) Encoder: We encode the inputs in the BEV view. The BEV representation provides a top-down view of the vehicle's surrounding environment, allowing the ego-vehicle to detect parking slots, obstacles, and markings. At the same time, the BEV view provides a consistent viewpoint representation across various driving perspectives, thereby simplifying the complexity of trajectory prediction.\nCamera Encoder At the beginning of the BEV generation pipeline, we first utilize EfficientNet [22] to extract image features $F_{img} \\in \\mathbb{R}^{C\\times H_{img}\\times W_{img}}$ from RGB inputs. Inspired by LSS [23], We learn a depth distribution $d_{dep} \\in [\\mathbb{R}^{D\\times H_{img}\\times W_{img}}$ of image features and lift each pixel into 3D space. We then multiply the predicted depth distribution $d_{dep}$ and image feature $F_{img}$ to obtain the image feature with depth information. With the camera extrinsics and intrinsics, image features are projected into BEV voxel grid to generate camera features $F_{cam} \\in \\mathbb{R}^{C\\times H_{cam}\\times W_{cam}}$. The range of BEV features in the x-direction is denoted as $[-R_x, R_x]m$, where m denotes meters, and the range in the y-direction is denoted as $[-R_y, R_y]m$.\nTarget Encoder In order to align the target slot with the camera feature $F_{cam}$, we generate a target heat map in the BEV space as the input of target encoder based on the specified parking slot location. Subsequently, we extract the target slot features $F_{target}$ using a deep CNN neural network to obtain the same dimension with $F_{cam}$. During the training, the target parking slot is determined by the end points of the human driving trajectory.\nTarget Query By aligning the camera features $F_{cam}$ and the target encoding features $F_{target}$ in BEV space and using the target feature to query the camera feature via the cross-attention mechanism, we can effectively fuse the two modalities. The positional encoding ensures the spatial correspondence is maintained between camera features and target features when associating the features at the specific BEV location. Utilizing $F_{target}$ as the query, camera feature $F_{cam}$ as the key and the value and employing the attention mechanism, we obtain the fused feature $F_{fuse}$.\n3) Decoder: Many end-to-end planning studies [12]\u2013[14] have employed a GRU decoder to predict next points from a high-dimensional feature vectors in an autoregressive way. However, the high-dimensional vectors of features lack a global receptive field. Taking inspiration from Pix2seq [24], we approach trajectory planning as a sequence prediction problem using a transformer decoder. This involves autoregressive, step-by-step prediction of the trajectory points. Our approach effectively combines low-dimensional trajectory points with high-dimensional image features.\nTrajectory Serialization Trajectory serialization represents trajectory points as discrete tokens. By serializing the trajectory points, the position regression can be converted into token prediction. Subsequently, we can leverage a transformer decoder to predict the trajectory point $(P_{x}^{t}, P_{y}^{t})$ in the ego vehicle's coordinate system, we utilize the following serialization method:\n$Ser(P_{x}^{t}) = \\lfloor \\frac{P_{x}^{t} + R_{x}}{2R_{x}} \\rfloor \\times N_t,$\nand\n$Ser(P_{y}^{t}) = \\lfloor \\frac{P_{y}^{t} + R_{y}}{2R_{y}} \\rfloor \\times N_t,$\nwhere $N_t$ represents the maximum value that can be encoded by a token in the sequence and the symbol for serializing trajectory points is denoted as Ser(). $R_x$ and $R_y$ represent the maximum values of the predicted range in the x and y directions, respectively.\nAfter serialization, the i-th trajectory can be expressed as follow:\n[BOS, Ser(P1), Ser(P1), ..., Ser(PN\u2081), Ser(PN\u2081), EOS],\nwhere BOS represents the start flag and EOS represents the end flag.\nTrajectory Decoder The BEV features serve as the key and the value, while the serialization sequence is utilized as the query to generate trajectory points using a transformer decoder in an autoregressive manner. During training, we add positional embedding to the sequence points and implement parallelization by masking unknown information. During inference process, given the BOS token, then the transformer decoder predicts following points in sequence. Then we append the predicted point to the sequence for the next step repeating this process until encountering EOS or reaching the specified number of predicted points.\nC. Lateral and Longitudinal Control\nDuring the control process, the parking start moment denoted as $t_0$, is used as the starting time to predict the path $T_{t_0} = N_\\theta (I_{t_0}, S')$ based on the end-to-end neural planner and the relative pose from the initial moment $t_0$ to the current moment t can be obtained by the localization system, denoted as $ego_{t_0\\rightarrow t}$. The target steering angle $\\Delta_{tar}$ can be obtained using the RWF (Rear-wheel Feedback) method, which can be expressed as follows:\n$\\Delta_{tar} = RWF(T_{t_0}, ego_{t_0\\rightarrow t})$.\nAccording to the speed feedback $V_{feed}$ and steer feedback $A_{feed}$ from the chassis, as well as the target speed $V_{tar}$ from the setting and the target steer $\\Delta_{tar}$ from the calculation, cascade PID controller is utilized to achieve lateral and longitudinal control. After a new predicted trajectory is generated, $T_{t_0}$ and $ego_{t_0\\rightarrow t}$ are reset, eliminating the necessity of relying on global localization throughout the entire vehicle control process."}, {"title": "IV. EXPERIMENTS", "content": "A. Dataset Collection\nThe datasets are collected using vehicle-mounted devices. To facilitate comprehensive visual perception and trajectories, surround-view cameras are employed to capture RGB images. Concurrently, dead reckoning techniques are integrated, leveraging sensor data fusion algorithms to achieve robust and accurate vehicle localization. The layout of the experimental platform and the sensors used are shown in Fig. 4. The data is collected across various scenarios for parking, including underground and ground garages, as shown in Fig. 5. Collecting data from diverse environments helps enhance the generalization capability of neural networks.\nB. Implement Details\nDuring the training process, surround-view camera images (the number of cameras R is 4) are used as input, and the target parking space is determined by some points at the end of the parking. The trajectory sequence points are used to supervise the end-to-end prediction results.\nIn the inference process, the target parking slot is selected by using \"2D-Nav-Goal\" in the RViz interface software to obtain the target parking slot. The model takes in current images from surround-view cameras and the target slot to predict the locations of subsequent n trajectory points in an autoregressive manner. The controller steers the vehicle based on the path planning results, ego pose, and feedback signals to park the vehicle in the designated slot. It is worth noting that the coordinates of the target point and predicted trajectory points are represented in the vehicle coordinate frame, ensuring the trajectory sequence and BEV features are expressed in consistent coordinate bases. This design also makes the entire system independent of the global coordinate frame.\nRegarding the neural network details, the size of BEV features is 200 \u00d7 200, corresponding to the actual spatial range of x \u2208 [\u221210m, 10m], y \u2208 [\u221210m,10m] with a resolution of 0.1 meters. In the transformer decoder, the maximum value of the trajectory serialization Nt is 1200. The trajectory decoder generates a sequence of predictions with a length of 30, achieving the best balance of accuracy and speed in inference.\nWe implement our method using the PyTorch framework. The neural network is trained on one NVIDIA GeForce RTX 4090 GPU with a batch size of 16, and the total training time is approximately 8 hours with 40,000 frames. Test data consists of about 5,000 frames.\nC. Evaluation Metrics\n1) Model Trajectory Evaluation: To analyze the performance of a model before conducting a real scenario experiment, we design some evaluation metrics to evaluate the inference ability of the model.\nL2 Distance (L2 Dis.) L2 Distance refers to the average Euclidean distance between waypoints of the predicted and the ground-truth trajectories. This metric evaluates the precision and accuracy of model inference.\nHausdorff Distance (Haus. Dis.) Hausdorff Distance refers to the maximum value of minimum distances between two point sets. This metric evaluates how well the predicted trajectory matches the ground-truth trajectory from the perspective of the point set.\nFourier Descriptor Difference (Four. Diff.) Fourier Descriptor Difference can be used to measure the difference between trajectories. The lower value indicates the smaller difference between the trajectories. This metric uses a certain number of Fourier descriptors to represent both the actual and predicted trajectories as vectors.\n2) End-to-end Real-vehicle Evaluation: In real-vehicle experiments, we use the following metrics to evaluate end-to-end parking performance.\nParking Success Rate (PSR) The parking success rate describes the probability of the ego vehicle successfully parking in the target parking slot.\nNo Slot Rate (NSR) Rate of failure to park in designated parking spaces.\nParking Violation Rate (PVR) Parking violation rate refers to a situation where a vehicle slightly extends beyond the designated parking space without obstructing or impeding adjacent parking spaces.\nAverage Position Error (APE) The average position error is the average distance between the target parking position and the stopping position of the ego vehicle when parking successfully.\nAverage Orientation Error (AOE) The average orientation error is the average difference between the target parking orientation and the stopping orientation of the ego vehicle when parking successfully.\nAverage Parking Score (APS) The average parking score is calculated through a comprehensive evaluation that includes the position error, orientation error, and success rate during parking. Scores are distributed between 0 and 100.\nAverage Parking Time (APT) The average parking duration time across multiple parking maneuvers. The parking duration is measured from the moment the parking mode is initiated until the vehicle is successfully parked in the designated space, or the parking process is terminated due to an anomaly or failure.\nD. Quantitative Results\nUsing the proposed end-to-end parking system, we conducted closed-loop vehicle tests in four different parking garages to validate the performance of our proposed system. The results are shown in Table I.\nIn the experiment, we tested in four different garages. Garage I is an underground garage, and Garage II, III and IV are ground garages. For each garage, we conducted three different experimental scenarios. Scene A is parking with no obstacles on either side. Scene B is parking with vehicles on the left side or right side. Scene C is parking with obstacles or walls nearby. For each experimental scenario, we randomly selected three different parking slots. We conducted approximately three parking tests on both the left and right sides of each slot. Experimental results show that our proposed"}, {"title": "E. Ablation Study", "content": "We designed ablation experiments to analyze the impact of different network designs. In terms of network structure, we conducted ablation experiments on the feature fusion, as illustrated in Table III. We compare the results of the baseline (target query), feature concatenation, and feature element-wise addition. The target query approach utilizes attention and spatial alignment mechanisms to fully integrate the target feature and the BEV feature. It explicitly constrains the spatial relationship between the target slot and the BEV image to achieve the highest trajectory prediction accuracy."}, {"title": "F. Visualization", "content": "The parking processes in different scenarios are shown in Fig. 6, demonstrating the versatile adaptation capabilities across diverse scenarios of our algorithm."}, {"title": "G. Limitations", "content": "Although our proposed method demonstrates advantages in the parking task, there are still some limitations. First, our method has poor adaptability to moving targets due to the restriction of data scale and scenario diversity. The model's adaptability to moving objects can be subsequently enhanced by expanding the dataset. Secondly, due to the training process that utilizes expert trajectories, it is impossible to provide effective negative samples. Additionally, there is no robust corrective mechanism when a significant deviation occurs during the parking process, ultimately resulting in parking failure. Subsequently, an end-to-end model can be trained using deep reinforcement learning by constructing a simulator that closely resembles real-world conditions through the utilization of NeRF [31] (Neural Radiance Field) and 3DGS [32] (3D Gaussian Splatting). Lastly, although our end-to-end parking method has achieved favorable results, there remains a gap compared to traditional rule-based parking methods. However, we believe this problem will be solved as end-to-end technology continues to advance. We expect that end-to-end parking algorithms will demonstrate advantages in complex scenarios in the future."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a camera-based end-to-end parking model. The model inputs the target slot and the surround-view RGB images, obtains the fused features in BEV view by target query, and predicts the trajectory points using a transformer decoder in an autoregressive manner. The results of trajectory planning are subsequently utilized for control. We extensively evaluated the proposed method across various scenarios, and the results demonstrate its reliability and generalizability. Nevertheless, there still exists a performance gap between our end-to-end method and highly optimized rule-based parking methods. In our future work, we aim to further improve the performance of end-to-end parking algorithms, with the expectation that learning-based approaches will eventually outperform traditional methods. We believe that our research and practice will inspire and provoke thoughts among fellow researchers and engineers."}]}