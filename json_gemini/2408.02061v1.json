{"title": "ParkingE2E: Camera-based End-to-end Parking Network, from Images to Planning", "authors": ["Changze Li", "Ziheng Ji", "Zhe Chen", "Tong Qin", "Ming Yang"], "abstract": "Autonomous parking is a crucial task in the intelligent driving field. Traditional parking algorithms are usually implemented using rule-based schemes. However, these methods are less effective in complex parking scenarios due to the intricate design of the algorithms. In contrast, neural-network-based methods tend to be more intuitive and versatile than the rule-based methods. By collecting a large number of expert parking trajectory data and emulating human strategy via learning-based methods, the parking task can be effectively addressed. In this paper, we employ imitation learning to perform end-to-end planning from RGB images to path planning by imitating human driving trajectories. The proposed end-to-end approach utilizes a target query encoder to fuse images and target features, and a transformer-based decoder to autoregressively predict future waypoints. We conduct extensive experiments in real-world scenarios, and the results demonstrate that the proposed method achieved an average parking success rate of 87.8% across four different real-world garages. Real-vehicle experiments further validate the feasibility and effectiveness of the method proposed in this paper. The code can be found at: https://github.com/qintonguav/ParkingE2E.", "sections": [{"title": "I. INTRODUCTION", "content": "Intelligent driving involves three main tasks: urban driving, highway driving, and parking maneuvers. Automated valet parking (AVP) and auto parking assist (APA) systems, crucial parking tasks within intelligent driving, offer significant improvements in parking safety and convenience. However, mainstream parking methods [1] are often rule-based, requiring the entire parking process to be decomposed into multiple stages such as environmental perception, mapping, slot detection, localization and path planning. Due to the intricate nature of these complex model architectures, they are more susceptible to encountering difficulties in tight parking spots or intricate scenarios.\nEnd-to-end (E2E) autonomous driving algorithms [3]-[7] mitigate cumulative errors across modules by integrating perception, prediction, and planning components into a unified neural network for joint optimization. The application of end-to-end algorithms to parking scenarios helps decrease the dependence of parking systems on manually designed features and rules, providing a comprehensive, holistic, and user-friendly solution.\nWhile end-to-end autonomous driving has demonstrated significant advantages, most of the research has concentrated on simulation [8] without validating the algorithm's real-world effectiveness. In contrast to the complexity of urban environments and the hazards of highway driving, parking scenarios are characterized by low speeds, confined spaces, and high controllability. These features provide a feasible pathway for incrementally deploying end-to-end autonomous driving capabilities in vehicles. We develop an end-to-end parking neural network and validate the algorithm's feasibility in real-world parking situations.\nThis work extends our previous work E2E-Carla [2] by presenting an imitation-learning-based end-to-end parking algorithm, which has been successfully deployed and evaluated in real environments. The algorithm takes in surround-view images captured by on-board cameras, predicts future trajectory results, and executes control based on the predicted waypoints. Once the user designates a parking slot, the end-to-end parking network collaborates with the controller to automatically maneuver the vehicle into the parking slot until it is fully parked. The contributions of this paper are summarized as follows:\nWe designed an end-to-end network to perform parking task. The network converts the surround view images into Bird's Eye View (BEV) representation, and fuses it with the target parking slot features by employing the target features to query the image features. Due to the sequential nature of the trajectory points, we utilize an autoregressive approach based on transformer decoder to generate trajectory points.\nWe deployed the end-to-end model on real vehicles for testing and verified the feasibility and generalizability of the network model for parking across various real-world scenarios, offering an effective solution for end-to-end network deployment."}, {"title": "II. LITERATURE REVIEW", "content": "BEV representation offers at least two advantages over perspective representation. First, it easily integrates inputs from different modalities due to its clear physical interpretability. Secondly, the BEV view avoids perspective distortion issues, thereby reducing the complexity of downstream tasks such as planning. In recent years, the BEV representation has seen widespread adoption in perception systems. Unlike previous deep learning-based perception algorithms comprising a feature extraction module and a task head module, BEV perception incorporates an additional viewpoint conversion module alongside these two modules. This conversion module facilitates the transformation between the sensor view and the Bird's Eye View (BEV).\nLSS [23] utilizes BEV perception for detection and segmentation. This method acquires BEV features by estimating the depth distribution at each pixel of the feature map and projecting it onto the BEV plane. DETR3D [26] follows the basic paradigm of DETR [25] and employs sparse queries for 3D object detection. PETR [27] adds 3D positional embedding, which provides 3D positional information to 2D features, aiming to the neural network to implicitly learn depth. BEVFormer [28] adopts BEV queries for perception, and incorporates spatial cross-attention and temporal self-attention mechanisms to boost performance. BEVDepth [29] builds upon LSS and utilizes LiDAR points for depth supervision during training to enhance the depth estimation quality, thereby improving BEV perception performance. BEVFusion [30] extracts BEV features from both cameras and LiDAR data and fuses them in the BEV space.\nIn contrast to the traditional module-based autonomous driving solutions, the end-to-end paradigm [9, 10] can mitigate accumulated errors, prevent information loss across module and minimize redundant computations. Consequently, it has emerged as a popular and prominent research topic in the field of autonomous driving tasks.\nThe research on end-to-end driving initially focused on autonomous urban driving tasks. ChauffeurNet [11], an imitation-learning-based end-to-end method, learned effective driving strategies from expert data. Many methods have adopted an encoder-decoder framework that extracted BEV features from sensors and then utilizes GRU (Gate Recurrent Unit) decoder to predict waypoints in an autoregressive manner, such as Transfuser [3, 12], Interfuser [13], and NEAT [14]. Besides, CIL [15] and CILRS [16] developed a neural network that directly maps front-view images, current measurements, and navigation commands into control signals without a separate PID controller. MP3 [17] and UniAD [7] propose a modular design but jointly optimize all components in an end-to-end manner.\nIn recent years, end-to-end networks have been developed for parking scenarios. Rathour et al. [18] proposed a two-stage learning framework to predict the steering angles and gears from images. In the first stage, the network predicts an initial estimate of a sequence of steering angles. In the second stage, an LSTM (Long Short-Term Memory) network are used to estimate the optimal steering angles and gears. Li et al. [19] trained a CNN (Convolutional Neural Network) on rear-view images to automatically control steering angle and velocity. ParkPredict [20] proposed a parking slot and waypoints prediction network based on a CNN-LSTM architecture. In the following work, ParkPredict+ [21] designed a transformer and CNN-based model to predict future vehicle trajectories based on intent, image, and historical trajectory.\nExisting end-to-end autonomous driving methods often demand substantial computational resources, pose training challenges, and face difficulties in real-vehicle deployment. On the other hand, parking approaches exemplified by ParkPredict primarily focus on prediction from aerial imagery, which differs from our task. Our method propose an end-to-end parking planning network that utilizes an autoregressive transformer decoder to predict future waypoints from BEV features extracted from RGB images and target slot."}, {"title": "III. METHODOLOGY", "content": "We use end-to-end neural network $\\mathbb{N}_{\\theta}$ to imitate expert trajectories for training, defining the dataset:\n$D = \\{(I_{i,j}, P_{i,j}, S_{i})\\},$\nwhere trajectory index $i \\in [1, M]$, trajectory points index $j \\in [1, N_{i}]$, camera index $k \\in [1, R]$, RGB image $I$, trajectory point $P$ and target slot $S$. Reorganize the dataset into:\n$T_{i,j} = \\{P_{i,min(j+b,N_{i})}\\}_{b=1,2,...,Q},$\nand\n$D' = \\{(I_{j}, T_{i,j}, S_{i})\\},$\nwhere $Q$ denotes the length of the predicted trajectory points and $R$ denotes the number of RGB cameras.\nThe optimization goals for the end-to-end network are as follows:\n$\\theta' = arg \\min\\limits_{\\theta} \\mathbb{E}_{(I,T,S)\\sim D'}[\\mathcal{L}(T, \\mathbb{N}_{\\theta}(I, S))],$\nwhere $\\mathcal{L}$ denotes the loss function.\nAs shown in Fig. 2, we developed an end-to-end neural planner that takes RGB images and a target slot as input. The proposed neural network includes two main parts: an input encoder and an autoregressive trajectory decoder. With the input of RGB images and the target slot, the RGB images are transformed to BEV features. Then, the neural network fuses BEV features with the target slot and generates the next trajectory point in an autoregressive way using transformer decoder.\nWe encode the inputs in the BEV view. The BEV representation provides a top-down view of the vehicle's surrounding environment, allowing the ego-vehicle to detect parking slots, obstacles, and markings. At the same time, the BEV view provides a consistent viewpoint representation across various driving perspectives, thereby simplifying the complexity of trajectory prediction.\nAt the beginning of the BEV generation pipeline, we first utilize EfficientNet [22] to extract image features $F_{img} \\in \\mathbb{R}^{C\\times H_{img}\\times W_{img}}$ from RGB inputs. Inspired by LSS [23], We learn a depth distribution $d_{dep} \\in \\mathbb{R}^{D\\times H_{img}\\times W_{img}}$ of image features and lift each pixel into 3D space. We then multiply the predicted depth distribution $d_{dep}$ and image feature $F_{img}$ to obtain the image feature with depth information. With the camera extrinsics and intrinsics, image features are projected into BEV voxel grid to generate camera features $F_{cam} \\in \\mathbb{R}^{C\\times H_{cam}\\times W_{cam}}$. The range of BEV features in the x-direction is denoted as $[-R_{x}, R_{x}]$m, where m denotes meters, and the range in the y-direction is denoted as $[-R_{y}, R_{y}]$m.\nIn order to align the target slot with the camera feature $F_{cam}$, we generate a target heat map in the BEV space as the input of target encoder based on the specified parking slot location. Subsequently, we extract the target slot features $F_{target}$ using a deep CNN neural network to obtain the same dimension with $F_{cam}$. During the training, the target parking slot is determined by the end points of the human driving trajectory.\nBy aligning the camera features $F_{cam}$ and the target encoding features $F_{target}$ in BEV space and using the target feature to query the camera feature via the cross-attention mechanism, we can effectively fuse the two modalities. The positional encoding ensures the spatial correspondence is maintained between camera features and target features when associating the features at the specific BEV location. Utilizing $F_{target}$ as the query, camera feature $F_{cam}$ as the key and the value and employing the attention mechanism, we obtain the fused feature $F_{fuse}.\nMany end-to-end planning studies [12]-[14] have employed a GRU decoder to predict next points from a high-dimensional feature vectors in an autoregressive way. However, the high-dimensional vectors of features lack a global receptive field. Taking inspiration from Pix2seq [24], we approach trajectory planning as a sequence prediction problem using a transformer decoder. This involves autoregressive, step-by-step prediction of the trajectory points. Our approach effectively combines low-dimensional trajectory points with high-dimensional image features.\nTrajectory serialization represents trajectory points as discrete tokens. By serializing the trajectory points, the position regression can be converted into token prediction. Subsequently, we can leverage a transformer decoder to predict the trajectory point $(P_x, P_y)$ in the ego vehicle's coordinate system, we utilize the following serialization method:\n$Ser(P_x) = [\\frac{P_x + R_x}{2R_x}] \\times N_t,$\nand\n$Ser(P_y) = [\\frac{P_y + R_y}{2R_y}] \\times N_t,$\nwhere $N_t$ represents the maximum value that can be encoded by a token in the sequence and the symbol for serializing trajectory points is denoted as $Ser()$. $R_x$ and $R_y$ represent the maximum values of the predicted range in the $x$ and $y$ directions, respectively.\nAfter serialization, the i-th trajectory can be expressed as follow:\n$[BOS, Ser(P_1^x), Ser(P_1^y), ..., Ser(P_{N_i}^x), Ser(P_{N_i}^y), EOS],$\nwhere BOS represents the start flag and EOS represents the end flag.\nThe BEV features serve as the key and the value, while the serialization sequence is utilized as the query to generate trajectory points using a transformer decoder in an autoregressive manner. During training, we add positional embedding to the sequence points and implement parallelization by masking unknown information. During inference process, given the BOS token, then the transformer decoder predicts following points in sequence. Then we append the predicted point to the sequence for the next step repeating this process until encountering EOS or reaching the specified number of predicted points.\nDuring the control process, the parking start moment denoted as $t_0$, is used as the starting time to predict the path $T_{t_0} = \\mathbb{N}_{\\theta}(I_{t_0}, S)$ based on the end-to-end neural planner and the relative pose from the initial moment $t_0$ to the current moment $t$ can be obtained by the localization system, denoted as $ego_{t_0\\rightarrow t}$. The target steering angle $\\Delta_{tar}$ can be obtained using the RWF (Rear-wheel Feedback) method, which can be expressed as follows:\n$\\Delta_{tar} = RWF(T_{t_0}, ego_{t_0\\rightarrow t}).$\nAccording to the speed feedback $V_{feed}$ and steer feedback $A_{feed}$ from the chassis, as well as the target speed $V_{tar}$ from the setting and the target steer $\\Delta_{tar}$ from the calculation, cascade PID controller is utilized to achieve lateral and longitudinal control. After a new predicted trajectory is generated, $T_{t_0}$ and $ego_{t_0\\rightarrow t}$ are reset, eliminating the necessity of relying on global localization throughout the entire vehicle control process."}, {"title": "IV. EXPERIMENTS", "content": "The datasets are collected using vehicle-mounted devices. To facilitate comprehensive visual perception and trajectories, surround-view cameras are employed to capture RGB images. Concurrently, dead reckoning techniques are integrated, leveraging sensor data fusion algorithms to achieve robust and accurate vehicle localization. The layout of the experimental platform and the sensors used are shown in Fig. 4. The data is collected across various scenarios for parking, including underground and ground garages, as shown in Fig. 5. Collecting data from diverse environments helps enhance the generalization capability of neural networks."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a camera-based end-to-end parking model. The model inputs the target slot and the surround-view RGB images, obtains the fused features in BEV view by target query, and predicts the trajectory points using a transformer decoder in an autoregressive manner. The results of trajectory planning are subsequently utilized for control. We extensively evaluated the proposed method across various scenarios, and the results demonstrate its reliability and generalizability. Nevertheless, there still exists a performance gap between our end-to-end method and highly optimized rule-based parking methods. In our future work, we aim to further improve the performance of end-to-end parking algorithms, with the expectation that learning-based approaches will eventually outperform traditional methods. We believe that our research and practice will inspire and provoke thoughts among fellow researchers and engineers."}]}