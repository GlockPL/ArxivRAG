{"title": "RobustX: Robust Counterfactual Explanations Made Easy", "authors": ["Junqi Jiang", "Luca Marzari", "Aaryan Purohit", "Francesco Leofante"], "abstract": "The increasing use of Machine Learning (ML) models to aid decision-making in high-stakes industries demands explainability to facilitate trust. Counterfactual Explanations (CEs) are ideally suited for this, as they can offer insights into the predictions of an ML model by illustrating how changes in its input data may lead to different outcomes. However, for CEs to realise their explanatory potential, significant challenges remain in ensuring their robustness under slight changes in the scenario being explained. Despite the widespread recognition of CEs' robustness as a fundamental requirement, a lack of standardised tools and benchmarks hinders a comprehensive and effective comparison of robust CE generation methods. In this paper, we introduce RobustX, an open-source Python library implementing a collection of CE generation and evaluation methods, with a focus on the robustness property. RobustX provides interfaces to several existing methods from the literature, enabling streamlined access to state-of-the-art techniques. The library is also easily extensible, allowing fast prototyping of novel robust CE generation and evaluation methods.", "sections": [{"title": "1 Introduction", "content": "With the increasing use of Machine Learning (ML) models to aid decision-making in high-stakes fields such as healthcare [Shaheen, 2021] and finance [Cao, 2022], there is a growing need for better explainability of these models. Counterfactual Explanations (CEs) [Guidotti, 2024] are often leveraged in Explainable AI (XAI) to this end due to their intelligibility and alignment with human reasoning [Miller, 2019; Byrne, 2019]. In particular, CEs can offer insights into the predictions produced by an ML model by showing how small changes in its input may lead to different (often more desirable) outcomes. To see what benefits CEs can bring, consider an illustration of a loan application with features 27 years of age, low credit rating, and 15K loan amount. Assume a bank's ML model classifies the application as not creditworthy. A CE for this outcome could be an altered input where a medium credit rating (with the other features unchanged) would result in the application being classified as creditworthy, thus giving the applicant an idea of what is required to have their loan approved.\nDespite their potential, current approaches to generating CEs often fall short in generating robust explanations. Consequently, these methods may produce explanations whose validity is compromised by slight changes in the scenario being explained. For instance, recent work [Upadhyay et al., 2021; Jiang et al., 2023b; Hamman et al., 2023] has highlighted that even small alterations in the parameters of an ML model, e.g. following fine-tuning, may invalidate previously generated CEs. An example of this scenario is captured in Figure 1, where a lack of robustness is demonstrated on a model trained for binary classification tasks. In Figure 1a, an input (yellow circle) receives an initial classification (blue class), and two counterfactuals are generated for it: one (red cross) laying exactly on the decision boundary (full black line) and one deeper inside the counterfactual class (green cross). In Figure 1b, we observe that the decision boundary of the model undergoes slight changes, induced by fine-tuning on a slightly shifted input distribution. As a result, previously generated CEs may cease to be valid if no precautions are taken to ensure robustness. For instance, we observe that the CE corresponding to the red cross is now classified as belonging to the blue class and is thus invalid. Now consider the consequences of these behaviours in our loan example: after fine-tuning, the applicant changing their credit rating to medium no longer ensures the success of the loan application, as the CE was not robust. When this happens, the CE previously generated by the bank is invalidated, and the bank may be liable for inconsistent statements made to customers regarding loan terms.\nThis and many other forms of robustness of CEs have been the subject of intense research efforts recently, and numerous algorithms to evaluate the robustness of CEs have been proposed (a recent survey identified about 40 methods [Jiang et al., 2024c]). However, the current state of robust CE research is fragmented, with various methods developed independently and implemented in different, often incompatible, ways. This lack of standardisation has resulted in challenges for the broader research community, as comparing the effectiveness of robust CE generation methods is impractical.\nWe fill this gap in this paper and introduce RobustX, an open-source Python library to standardise and streamline the generation, evaluation, and benchmarking of robust CEs. RobustX provides flexible, extensible, and customisable tools to implement custom CE methods. Differently from existing frameworks, e.g. [Pawelczyk et al., 2021; Agarwal et al., 2022], our library focuses on providing a consistent framework for testing robustness and systematically comparing various methods, ensuring fair and reliable evaluations. RobustX addresses key limitations on library tools in the current landscape ([Keane et al., 2021; Jiang et al., 2024c]) by offering a standardised approach to robust CE development while also promoting extensibility, allowing users to integrate new datasets and explanation algorithms as needed. The library, including documentation and tutorials, is publicly available at the following link:\nhttps://github.com/RobustCounterfactualX/RobustX\nThe reminder of this paper is organised as follows. Section 2 presents the main components of the library, providing details about their functionalities. Section 3 demonstrates how easy it is to use RobustX to benchmark existing CE generation algorithms and compare them using different metrics. Finally, Section 4 offers some concluding remarks and pointers for future work."}, {"title": "2 Overview", "content": "RobustX implements a complete pipeline for robust CE generation and evaluation (Figure 2) with three major components: Task, CE generator, and CE evaluator. Each has an abstract class template for easy customisation. Users start by creating a task which defines the model and inputs to be explained. Then, the user can choose whether to use RobustX to generate CEs, or directly evaluate the robustness of previously (externally) generated CEs.\nTask objects, providing functionality for interactions between models and datasets, are the basic class passed into the CE generation and evaluation pipelines. Our current implementation assumes a ClassificationTask by default, as this is the most commonly considered use case in the literature; however, users can also implement customised Task objects for learning problems other than classification. RobustX natively supports models trained using sklearn [Pedregosa et al., 2011], Keras [Chollet and others, 2015] and PyTorch [Paszke, 2019]. Models trained using other frameworks can also be used by instantiating a BaseModel wrapper class. As far as datasets are concerned, RobustX offers a selection of pre-loaded example datasets that can be readily loaded using the DatasetLoader class. Additionally, this class also allows the uploading of custom datasets if needed via.csv files.\nRobustX currently implements nine robust CE generation methods across different robustness use cases reported in the yellow box in Figure 2: APAS [Marzari et al., 2024], ArgEnsembling [Jiang et al., 2024b], DiverseRobustCE [Leofante and Potyka, 2024], MCER [Jiang et al., 2023b], ModelMultiplicityMILP [Leofante et al., 2023], PROPLACE [Jiang et al., 2023a], RNCE [Jiang et al., 2024a], ROAR [Upadhyay et al., 2021], STCE [Dutta et al., 2022; Hamman et al., 2023]. It also provides four popular non-robust methods that can be used as baselines to crease new generation methods: BLS [Leofante and Potyka, 2024], MCE [Mohammadi et al., 2021], KDTreeNNCE [Brughmans et al., 2024], and the seminal work by [Wachter et al., 2017]. All methods inherit from the abstract class CEGenerator and implement the _generation_method() function, providing an easy interface to other components in the pipeline.\nCE evaluation methods can take in CEs, either generated within or outside RobustX, and benchmark their robustness along with other common properties identified in the literature. Currently, RobustX provides a CEEvaluator class to evaluate the validity and proximity of CEs [Wachter et al., 2017], as well as five classes specifically focusing on robustness evaluation metrics: VaRRobustnessEvaluator to assess the validity of CEs after retraining [Dutta et al., 2022], DeltaRobustnessEvaluator [Jiang et al., 2023b] to"}, {"title": "3 RobustX in Action", "content": "In this section we provide an example on how to use RobustX in practice; additional examples are available online. Figure 3 shows how to run and compare six methods supported by RobustX. In this example we focus on robustness against model changes [Upadhyay et al., 2021] and perform a comparison between four robust methods and two non-robust baselines. To this end, we first import the (pre-loaded) ionosphere dataset for binary classification (line 6) and apply standard pre-processing. We then create and train a simple three-layer neural network model (line 8). A task object is then created from the dataset and model. Then, we specify the CE generation and evaluation methods of interest (line 16 and 17) and run the benchmarking procedure (line 19). The default benchmark function in this example runs each method with its default hyperparameters, although customised hyperparameters can be configured. It then generates CEs for all instances in the dataset which are predicted with an undesirable class (here 102 points with neg_value=0), and runs the specified evaluation methods. In this example, we evaluate CEs along three metrics: validity, proximity [Wachter et al., 2017] and \u0394-robustness [Jiang et al., 2023b]. The results along the selected evaluation metrics are then printed in a structured table, so that we can easily compare how each method performs."}, {"title": "4 Conclusion", "content": "We presented RobustX, a Python framework to generate, evaluate and compare robust CE for ML models. Our library fills a major gap in the existing literature on robust CEs, providing an easy-to-use and extensible platform to benchmark existing algorithms for robust CEs. Building upon extensive research in the area, RobustX provides a unified platform to run and compare existing approaches, as well as implementing new ones, reducing the need to re-implement software from scratch. Work is underway to further expand the list of available generation algorithms, evaluation methods and additional software facilities for testing and validation to ensure the correctness of the implementations. We believe RobustX will streamline research efforts in robust CEs, accelerating the development of innovative solutions and fostering collaboration within this rapidly growing field."}]}