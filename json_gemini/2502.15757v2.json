{"title": "TLOB: A Novel Transformer Model with Dual Attention for Stock Price Trend Prediction with Limit Order Book Data", "authors": ["Leonardo Berti", "Gjergji Kasneci"], "abstract": "Stock Price Trend Prediction (SPTP) based on Limit Order Book\n(LOB) data is a fundamental challenge in financial markets. De-\nspite advances in deep learning, existing models fail to generalize\nacross different market conditions and struggle to reliably pre-\ndict short-term trends. Surprisingly, by adapting a simple MLP-\nbased architecture to LOB, we show that we surpass SoTA perfor-\nmance; thus, challenging the necessity of complex architectures.\nUnlike past work that shows robustness issues, we propose TLOB,\na transformer-based model that uses a dual attention mechanism\nto capture spatial and temporal dependencies in LOB data. This\nallows it to adaptively focus on the market microstructure, making\nit particularly effective for longer-horizon predictions and volatile\nmarket conditions. We also introduce a new labeling method that\nimproves on previous ones, removing the horizon bias. We evalu-\nate TLOB's effectiveness using the established FI-2010 benchmark,\nwhich exceeds the state-of-the-art by an average of 3.7 F1-score(%).\nAdditionally, TLOB shows improvements on Tesla and Intel with a\n1.3 and 7.7 increase in F1-score(%), respectively. Additionally, we\nempirically show how stock price predictability has declined over\ntime (-6.68 absolute points in F1-score(%)), highlighting the growing\nmarket efficiencies. Predictability must be considered in relation\nto transaction costs, so we experimented with defining trends us-\ning an average spread, reflecting the primary transaction cost. The\nresulting performance deterioration underscores the complexity\nof translating trend classification into profitable trading strategies.\nWe argue that our work provides new insights into the evolving\nlandscape of stock price trend prediction and sets a strong founda-\ntion for future advancements in financial AI. We release the code\nat https://github.com/LeonardoBerti00/TLOB.", "sections": [{"title": "1 Introduction", "content": "Over the past few decades, the global financial landscape has under-\ngone a profound transformation, transitioning from manual trading\noperations to sophisticated electronic platforms. This evolution has\nbeen so significant that by 2020, electronic trading accounted for\nover 99% of equity shares traded in the United States, a stark con-\ntrast to just 15% in 2000 [25]. At the heart of this revolution lies the\nelectronic Limit Order Book (LOB), a dynamic data structure that\nhas become the cornerstone of modern financial markets. In today's\ncompetitive financial world, the majority of the markets utilize elec-\ntronic LOBs to record trades. The continuous inflow of limit orders,\norganized by price levels, creates a dynamic structure that evolves\nover time, reflecting the real-time balance of supply and demand.\nHowever, this multidimensional structure, which spans price lev-\nels and volumes, presents complex challenges for understanding\nmarket behavior, forecasting stock price trends, and simulating\nrealistic market conditions. The non-stationary nature of LOB data,\ncharacterized by its stochastic behavior, makes modeling stock\nprice movements challenging. Traditional statistical methods fail to\ncapture these complexities, especially when attempting to predict\nshort-term price trends. However, recent advancements in deep\nlearning have opened new avenues for tackling these challenges,\noffering the ability to model the non-linear relationships and tem-\nporal dependencies inherent in LOB data.\nStock Price Trend Prediction (SPTP)\u00b9 remains one of the most\nchallenging and economically significant problems in financial mar-\nkets, attracting significant attention from academic researchers\nand industry practitioners. One prominent application of SPTP,\nparticularly utilizing Limit Order Book (LOB) data, lies within high-\nfrequency trading, where algorithms attempt to capitalize on short-\nterm price movements. Predicting future market movements is a\nhighly challenging task due to the complexity, non-stationarity, and\nvolatility of financial markets. However, with the growing avail-\nability of Limit Order Book (LOB) data and advancements in deep\nlearning, new opportunities have emerged to improve the accu-\nracy of these predictions. This paper explores the application of\ndeep learning models to SPTP using Limit Order Book (LOB) data,\nwhich provides the most granular and complete information on\nstock trades. Financial markets do not exist in a vacuum; they are\ncontinuously shaped by the actions and expectations of countless\nparticipants who, according to the Efficient Market Hypothesis\n(EMH), collectively incorporate all available information into asset\nprices. When models discover a predictive pattern and traders act\n\u00b9In the literature it is also referred to as mid-price movement prediction."}, {"title": "2 Background", "content": "In the contemporary, highly competitive financial landscape, the\npredominant mechanism for recording and managing market trans-\nactions is the electronic Limit Order Book (LOB). Within a limit\norder book market, traders can submit orders to buy or sell a speci-\nfied quantity of an asset at a predetermined price. Three primary\norder types are prevalent in such markets: (1) Market orders,\nwhich are executed immediately at the best available price with a\npredetermined quantity; (2) Limit orders, allows traders to decide\nthe maximum (in the case of a buy) or the minimum (in the case\nof a sell) price at which they want to complete the transaction.\nA quantity is always associated with the specified price; and (3)\nCancel orders (alternatively referred to as deletions), which serve\nto remove an active limit order.\nThe LOB is a data structure that maintains and matches active\nlimit orders and market orders in accordance with a predefined\nset of rules. This structure is transparently accessible to all market\nparticipants and is subject to continuous updates with each event,\nincluding order placement, modification, cancellation, and execu-\ntion. The most widely adopted mechanism for order matching is\nthe Continuous Double Auction (CDA) [3]. Under the CDA frame-\nwork, orders are executed whenever the best bid (the highest price\na buyer is willing to offer) and the best ask (the lowest price a seller\nis willing to accept) overlap. This mechanism facilitates continuous\nand competitive trading among market participants. The price of\na security is commonly defined as the mid-price, calculated as the\naverage of the best ask and best bid prices, with the difference\nbetween these prices representing the bid-ask spread.\nGiven that limit orders are organized into distinct depth levels,\neach comprising bid price, bid size, ask price, and ask size, based on\ntheir respective prices, the temporal evolution of a LOB constitutes\na complex, multidimensional temporal problem. Research on LOB\ndata can be broadly categorized into four primary types: empirical\nanalyses of LOB dynamics [5, 11], price and volatility forecasting\n[37, 48], stochastic modeling of LOB dynamics [12, 16], and LOB\nmarket simulation [7, 10, 26]."}, {"title": "3 Related Work", "content": "The challenge of modeling the complex data structures and vast\nquantities associated with LOBs has spurred the development of\ndeep learning algorithms for related modeling and forecasting tasks.\nIn this section, we will summarize the State-of-The-Art (SoTA) deep\nlearning models in the Stock Price Trend Prediction (SPTP) task,\nwhich consists of forecasting the direction of mid-price movements\nat a high-frequency resolution. Tsantekis et al. [42] (2017) utilize a\nRecurrent Neural Network (RNN) based on Long-Short Term Mem-\nory (LSTM) layers to predict mid-price movements. In the same\nyear, the authors presented another approach [41], introducing a\nCNN-based model (CNN). Subsequently, the same group proposed\ntwo additional architectures in [43] (2020). The first focuses on\ncapturing temporal dynamics from LOB data and correlating tem-\nporally distant features using convolutional layers. The second\narchitecture, CNNLSTM, merges the CNN with an LSTM. The CNN\ninitially extracts features from the LOB time series, which are then\npassed to the LSTM for classification. Tran et al. [39] (2018) pro-\nposed the Temporal Attention-Augmented Bilinear Layer (TABL)"}, {"title": "4 Task Definition", "content": "We represent the evolution of a LOB as a time series L, where each\n$L(t) \\in \\mathbb{R}^{4L}$ is called a LOB record, for $t = 1, . . . , N$, with N being\nthe number of LOB observations and L the number of levels. In\nparticular,\n$L(t) = (p^{ask}(t), v^{ask}(t), p^{bid}(t), V^{bid}(t)),$  (1)\nwhere $p^{ask}(t)$ and $p^{bid}(t) \\in \\mathbb{R}^L$ are the prices at levels 1 through\nL, and $V^{ask}(t)$ and $V^{bid}(t) \\in \\mathbb{R}^L$ are the corresponding volumes.\nTrend Definition We employ a ternary classification system for\nprice trends: U (\"upward\") denotes an increasing price trend, D\n(\"downward\") indicates a decreasing trend, and S (\u201cstable\u201d) repre-\nsents price movements with only minor variations.\nIn equity markets, mid-prices are generally considered the most\nreliable single-value indicator of actual stock prices. However, ow-\ning to inherent market fluctuations and exogenous shocks, mid-\nprices can exhibit considerable volatility. Consequently, labeling\nconsecutive mid-prices ($p_t, p_{t+1}$) often results in noisy labels.\nTo mitigate this, many labeling strategies employ smoother mid-\nprice functions, averaging prices over a chosen \"window length\"\nto reduce short-term noise and better reflect persistent directional\nmoves. An example of this approach appears in [31], detailed in\nSection 6.2.\nHowever, as shown by Zhang et al. [48] (Fig. 2), smoothing\nonly the future prices can lead to instability in trading signals.\nThis instability often causes redundant trading actions and higher\ntransaction costs. To address this, Tsantekidis et al. [41] proposed\nalso smoothing past prices. They define:\n$l(t,k) = \\frac{m_+(t,k) - m_-(t, k)}{m_-(t, k)},$   (2)\nwhere\n$m_+(t,k) = \\frac{1}{k+1}\\sum_{i=0}^k p(t+i)$ and (3)\n$m_-(t,k) = \\frac{1}{k} \\sum_{i=0}^k p(t-i),$ (4)\nnoting that i runs from 0 to k, so there are (k + 1) terms in the\nsum. A key drawback is that the window length k coincides with\nthe prediction horizon h. This can bias the labels: for instance, a\nhorizon of h = 2 may not provide enough smoothing, whereas a\nlarge horizon might over-smooth price moves.\nTo overcome this, we propose a more general labeling strategy\nthat dissociates k from h. Specifically, we define:\n$w_+(t,h,k) = \\frac{1}{k+1}\\sum_{i=0}^k p(t+h-i)$ (5)\n$w_-(t,h,k) = \\frac{1}{k + 1} \\sum_{i=0}^k p(t-i).$  (6)"}, {"title": "5 Models", "content": "We propose two novel deep learning models for Stock Price Trend\nPrediction (SPTP) using Limit Order Book (LOB) data. The first,\ncalled MLPLOB, is a simple MLP-based model. The second, TLOB,\nleverages a dual-attention Transformer-based approach. Both mod-\nels take as input a sequence of LOB time series consisting of the\nlast T LOB snapshots for 10 LOB levels.\n5.1\nMLPLOB\nA key finding from the benchmark study by Prata et al. [33] reveals\nthat, despite the proliferation of specialized deep learning archi-\ntectures for SPTP, their performance often converges toward low\nvalues when tested on diverse and complex datasets. Inspired by the\nwork of Tolstikhin et al. [38] and Zeng et al. [46], who demonstrated\nthat simple MLP-based models can perform as well as state-of-the-\nart (SoTA) methods in certain domains, we develop an MLP-based\narchitecture for SPTP with LOB data, called MLPLOB.\nArchitecture Overview. MLPLOB is composed of multiple\nblocks, each containing two types of MLP layers:\n(1) Feature-Mixing MLPs, which operate along the feature axis.\n(2) Temporal-Mixing MLPs, which operate along the time axis.\nThis design aims to capture both spatial and temporal relation-\nships in LOB data-characteristics that Sirignano and Cont [36, 37]\nidentified as fundamental to LOB dynamics and modeling.\nEach MLP layer consists of two fully connected layers, mirror-\ning the MLP component used in Transformer architectures [44].\nInitially, the input sequence is projected linearly into a tensor\n$X \\in \\mathbb{R}^{T\\times N}$, where N is a chosen hyperparameter.\nFeature-Mixing MLPs. We apply a feature-mixing MLP row\nby row (i.e., for each time step i). Formally,\n$U_{i,*} = \\sigma (LayerNorm(\\sigma(X_{i,*} W_1) W_2 + X_{i,*}))$  for $i = 1,..., T,$ (8)\nwhere \u03c3 is the GeLU activation function [17], and LayerNorm de-\nnotes layer normalization.\nTemporal-Mixing MLPs. Next, we transpose the resulting ten-\nsor U and apply a temporal-mixing MLP column by column (i.e.,\nfor each feature dimension j):\n$Z_{*,j} = \\sigma (LayerNorm(\\sigma(U_{*,j} W_3) W_4 + U_{*,j}))$ for $j = 1, ..., N.$ (9)\nModel Simplicity and Isotropic Design. The MLPLOB architec-\nture relies only on matrix multiplications, reshaping operations,\nand scalar nonlinearities. It also adopts an isotropic design, wherein\neach block (beyond the initial projection) has a constant dimen-\nsionality. This contrasts with the pyramidal layouts found in many\nCNNs (which reduce spatial resolution while increasing channel\ndepth). Notably, isotropic designs are also common in Transformers\nand Recurrent Neural Networks (RNNs).\nFinal Prediction. After several blocks of feature and temporal\nmixing, MLPLOB performs dimensionality reduction to collapse\nall features into a single vector, which then passes through several\nfully connected layers that gradually diminish the vector dimension\nand a final standard classification head. The network outputs the\ndirectional trend (up, down, or stable) for the final time step. Our\nprimary objective in devising MLPLOB is to show that a carefully\nstructured MLP-based model can match or exceed more complex\narchitectures in the SPTP task. The same method is also applied to\nTLOB."}, {"title": "5.2 TLOB", "content": "The Transformer architecture [44] has led to major breakthroughs\nin deep learning, notably in natural language processing [6, 23]"}, {"title": "6 Experiments", "content": "We conduct a comprehensive evaluation of MLPLOB and TLOB\nmodel training and testing on both the Benchmark FI-2010 dataset\nand the TSLA-INTC dataset, composed of Tesla and Intel. TLOB and\nMLPLOB surpass SoTA performances on every dataset and every\nhorizon. TLOB performs the best on larger horizons, while MLPLOB\nperforms the best on the shorter ones. Our experiments extend be-\nyond merely demonstrating the state-of-the-art performance of\nTLOB, aiming to address several critical research questions: (1) Are\nstock prices harder to forecast than in the past? (2) What if we\nchoose \u03b8 equal to the average spread? (3) Are temporal and spatial\nattention necessary? Through these investigations, we seek not\nonly to validate our models' predictive capabilities but also to con-\ntribute to the broader understanding of deep learning applications\nin financial forecasting.\n6.1\nTSLA-INTC Dataset\nIn the majority of state-of-the-art (SoTA) research within the do-\nmain of Deep Learning applied to LOB data, researchers typically\nemploy one, two, or three stocks [10, 21, 26, 29, 34, 35], predom-\ninantly from the technology sector. Adhering to this established\npractice, we construct a LOB dataset comprising two NASDAQ-\nlisted stocks \u2013 namely, Tesla and Intel \u2013 spanning the period from\nJanuary 2nd to January 30th, 2015. We posit that stylized facts\nand market microstructure characteristics exhibit independence\nfrom individual stock behaviors (as demonstrated in [4, 5, 13, 16]\u00b3),\nthereby rendering specific stock attributes non-critical to the anal-\nysis. The dataset encompasses 20 order book files for each stock,\ncorresponding to each trading day, resulting in a total of approxi-\nmately 24 million samples. Each order book sample is represented\nas a tuple ($p^{ask}(t), V^{ask}(t), p^{bid}(t), V^{bid}(t)$), where $p^{ask}(t)$ and\n$p^{bid}(t) \\in \\mathbb{R}^L$ denote the prices at levels 1 through L, and $V^{ask}(t)$\nand $V^{bid}(t) \\in \\mathbb{R}^L$ represent the corresponding volumes. The dataset\nis partitioned such that the initial 17 days are allocated for training,\nthe 18th day for validation, and the final two days for testing. In\nSampling. Limit Order Book data, especially for liquid stocks,\nis massive, every day, hundreds of thousands of orders are placed\nfor each stock. Furthermore, financial data are known to have a\nlow signal-to-noise ratio [28]. Accordingly, it is unnecessary to\nconsider every LOB update, so defining a valid sampling technique\nis essential. While time-based and event-based sampling methods\n5These seminal works in finance elucidate the universal statistical properties of LOBs,\ntranscending specific stocks and markets."}, {"title": "6.2 Benchmark dataset FI-2010", "content": "Our model will be evaluated against SoTA models utilizing the\nFI-2010 benchmark dataset [31]. The FI-2010 dataset [31] is the\nmost widely adopted LOB dataset within the field of deep learning"}, {"title": "6.3 Experimental settings", "content": "For each dataset, we trained and tested the performance of each\nmodel on different horizons, namely 10, 20, 50, 100. All the ex-\nperiments were carried out using an RTX 3090. Since the FI-2010\ndataset also contains 104 handcrafted features derived from the\nLOB, we used them in both our models. This choice improved the\nperformance of the F1-Score (%) by approximately 1. For Tesla\nand Intel, given the availability of message files containing the\norder information, we augmented the LOB snapshots by concate-\nnating them with the corresponding orders. This integration was\nundertaken to incorporate additional information not present in\nthe LOB. Consequently, this approach resulted in an approximate\nimprovement of 1.5 in the F1-score (%). We report the details on\nthe hyperparameters search in the Appendix (A).\nTrend Classification Threshold We remark that \u03b8 is the pa-\nrameter that determines if a percentage change $l_t$ is classified as\nan up, stable, or downtrend. For the TSLA-INTC dataset, to ensure\nbalanced class distribution, we set \u03b8 equal to the mean percentage\nchange. In Sec. 7.4 we explore an alternative approach to defining\n\u03b8 based on financial parameters rather than class balance opti-\nmization. For the FI-2010 dataset, we retained the original labels to\nmaintain consistency with existing benchmark studies and previous\nworks.\nMetric We selected the F1-score as our primary performance\nmetric because it captures both precision and recall in a single value.\nAccuracy is not a valid metric for our experiments because the\nclasses are not balanced for each horizon. The F1-Score is robust\nto the class imbalance problem, which detrimentally affects the"}, {"title": "7 Results", "content": "Table 2 presents the performance comparison across four predic-\ntion horizons for the FI-2010 benchmark dataset. In the Appendix\n(B) we report also the precision and recall curves for horizon 100.\nMLPLOB and TLOB exhibit very high precision, also at high recall\nvalues, consistently achieve higher precision at all recall levels com-\npared to the other models. The results for the baselines are extracted\nfrom the benchmark of Prata et al. [33] since the settings are equal\nfor the FI-2010 dataset. MLPLOB and TLOB outperform all the other\nmodels analyzed in [33], surpassing state-of-the-art performance.\nInterestingly, MLPLOB demonstrates the best performance in the\nfirst three horizons. Notably, the performance differential between\nMLPLOB and TLOB is minimal, which, as we will demonstrate\nin Section 7.2, can be attributed to the lower complexity of the\nFI-2010 dataset, which explains the uselessness of a more complex\narchitecture such as TLOB for this particular dataset.\n7.2\nTesla and Intel results\nIn Table 3 we show the results for Tesla and in Table 4 for Intel.\nFor each stock, we trained a different model. In the Appendix (B)\nwe report also the precision and recall curves for a horizon equal\nto 100. For INTC, they exhibit excellent precision at low recall\nThis experiment examines the challenges associated with market\nprediction over time and the self-destruction of predictable patterns\nin financial markets. Empirical evidence consistently demonstrates\nthat forecasting models effective in certain periods become obso-\nlete over time. Several studies indicate that previously observed\npredictability patterns disappeared after becoming widely known.\nDimson and Marsh [15] found this for the UK small-cap premium,\nwhile Bossaert and Hillion [2] noted a decline in international stock\nreturn predictability around 1990. Aiolfi and Favero [1] reported\nsimilar findings for US stocks in the 1990s. The market is increas-\ningly efficient and difficult to predict as time goes by. We extend\nthis investigation to our best-performing model TLOB. Specifically,\nwe tested on a day of Intel from 2012/06/217 and confronted the\ndifference in performance with 2015/01/30. We report the perfor-\nmance in Table 5. As expected the performance from 2012 is better\nthan that from 2015. We confirm the hypothesis and the empirical\nevidence from other works.\n7.4 Alternative Threshold Definition Using\nAverage Spread\nBased on the fact that predictability has to be considered in\nrelation to the transaction costs, we explore an alternative approach\nto define the trend classification parameter \u03b8, setting it equal to\nthe average spread as a percentage of the mid-price, reflecting the\nprimary transaction cost. This methodology could only be applied\nto Tesla data, as Intel's higher trading volume (approximately 10\ntimes greater in January 2015) and lower volatility relative to traded\nshares would result in 99.99% of trends classified as stationary. We\nset the horizons to 50, 100, and 200 because with shorter horizons\n99% of the mid-price movements would be classified as stationary.\nIn Table 6 we report the results. In general, performances show a\ndeterioration, which is probably caused by the classes' unbalance.\nThis experiment highlights the necessity for further refinements in\ntrend definition and method complexity when targeting profitability\nin practical applications.\n7.5 Ablation Study\nTo evaluate the contribution of each attention mechanism within\nthe TLOB architecture, we performed an ablation study on the\nFI-2010 dataset. Specifically, we compared the performance of the\ncomplete TLOB model against two ablated versions: one without\nspatial attention (TLOB w/o SA) and another without temporal\nattention (TLOB w/o TA). To avoid inconsistency, we maintain"}, {"title": "8 Conclusion", "content": "We proposed two new deep learning models MLPLOB: A simplified\nyet effective MLP-based architecture and TLOB, a Transformer-\nbased approach, for the task of stock price trend prediction on\nLimit Order Book (LOB) data. Both models demonstrated superior\nperformance compared to existing state-of-the-art approaches, with\nTLOB showing particular promise in handling high-frequency mar-\nket data. NASDAQ stocks (Tesla, Intel) proved significantly more\nchallenging to predict than Finnish stocks (FI-2010). Our research\nalso showed that prediction accuracy decreases as the forecasting\nhorizon increases, highlighting the inherent challenges of long-term\nprediction in financial markets.\nLimitations: When considering practical implementation, we\nfound that defining trend thresholds based on average spread (trans-\naction costs) significantly impacts model evaluation and potential\nprofitability. This finding underscores the critical gap between aca-\ndemic performance metrics and practical trading applicability.\nFuture works Looking ahead, several avenues for future re-\nsearch emerge. The investigation of scaling laws for financial deep\nlearning models remains an open question, as does the development\nof more robust approaches to handling increased market efficiency\nand complexity. Additionally, the exploration of alternative trend\ndefinition methodologies that better align with practical trading\nconstraints could prove fruitful.\nRisks: Firstly, it is important to acknowledge that the proposed\nmethodologies are not sufficiently mature for practical deployment\nin live trading environments. Furthermore, the application of deep\nlearning models to stock price prediction and subsequent utilization\nin trading carries significant inherent risks. A primary concern is\nthe limited explainability of such models. Furthermore, automated"}]}