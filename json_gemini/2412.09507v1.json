{"title": "Vision Transformers for Efficient Indoor Pathloss Radio Map Prediction", "authors": ["Edvard Ghukasyan", "Hrant Khachatrian", "Rafayel Mkrtchyan", "Theofanis P. Raptis"], "abstract": "Vision Transformers (ViTs) have demonstrated remarkable success in achieving state-of-the-art performance across various image-based tasks and beyond. In this study, we employ a ViT-based neural network to address the problem of indoor pathloss radio map prediction. The network's generalization ability is evaluated across diverse settings, including unseen buildings, frequencies, and antennas with varying radiation patterns. By leveraging extensive data augmentation techniques and pretrained DINOv2 weights, we achieve promising results, even under the most challenging scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "The accurate prediction of radio signal propagation in indoor environments is a fundamental problem in wireless communications, enabling efficient network planning, resource allocation, and performance optimization [1]. Indoor environments present unique challenges for radio propagation modeling due to their complex geometries, diverse materials, and dynamic obstacles, making traditional empirical and analytical methods insufficient for capturing intricate pathloss characteristics [2]. Data-driven machine learning approaches have emerged as a powerful alternative, offering the potential to learn complex patterns directly from data.\nFollowing our previous works on using such approaches for closely related problems, such as wireless positioning [3], [4] and environment map reconstruction [5], [6], we shift our methodological design from convolutional neural networks to vision transformers and from outdoor to indoor setting [7]. This shift is performed in order to attempt to address the problem of indoor pathloss radio map prediction, and specifically in the context of the First Indoor Pathloss Radio Map Prediction Challenge at ICASSP 2025 [8]."}, {"title": "II. DATA DESCRIPTION", "content": "The dataset used in this work consists of path loss (PL) radio maps generated using a ray-tracing algorithm. These maps simulate indoor wireless signal propagation under various conditions and provide a benchmark for evaluating the generalization capabilities of neural networks in indoor radio map reconstruction tasks.\nA. Dataset Overview\nThe dataset includes PL radio maps for:\n\u2022 25 indoor geometries (B1-B25): Each geometry represents a unique building layout.\n\u2022 3 frequency bands:\nf1 = 868 MHz,\nf2 = 1.8 GHz,\nf3 = 3.5 GHz.\n\u2022 5 antenna radiation patterns (Ant1\u2013Ant5):\nAnt1 represents an isotropic antenna.\nAnt2-Ant5 represent directional patterns with varying gain and random steering angles.\nThe spatial resolution of the radio maps is 0.25 meters, ensuring precise signal representation. For all simulations, the transmitting antenna (Tx) is positioned at a height of 1.5 meters above the floor, matching the receiving plane's height.\nB. Task Descriptions\nThe challenge consists of three tasks, each designed to evaluate model generalization under progressively challenging conditions:\n\u2022 Task 1: Includes data from isotropic antennas (Ant1) operating at 868 MHz (f\u2081) across all buildings (B1-B25). Participants train on the given geometries and are tested on unseen geometries.\n\u2022 Task 2: Extends Task 1 by including data for all three frequency bands (f1, f2, f3). Participants train on the given geometries and are tested on unseen geometries and frequencies.\n\u2022 Task 3: Builds upon Task 2 by incorporating all five antenna radiation patterns (Ant1\u2013Ant5). The task requires generalization to new geometries, frequencies, and radiation patterns.\nC. Data Format\nThe buildings and antenna locations are encoded in three-channel images. Each pixel corresponds to a grid cell of size 0.25 \u00d7 0.25 meters. The first channel contains absolute values for normal incidence reflectance of the walls (0 for air, fig. 1a). The second channel contains absolute values for normal incidence transmittance of the walls (0 for air, fig. 1b). The third channel encodes the distance from the antenna in meters (fig. 1c)."}, {"title": "III. MODEL DESIGN AND NEURAL NETWORK ARCHITECTURE", "content": "Our method is based on deep learning algorithms by utilizing minimal prior knowledge in the physics of radio signal propagation. We attempt to take a well-pretrained neural network and train it on the available training data by leveraging heavy data augmentation.\nA. Data Preprocessing\nTo ensure consistency, enhance feature representation, and improve the generalization capabilities of the model, a series of preprocessing steps were systematically applied to the dataset. These steps are designed to standardize the input data, address variations in geometry and dimensions, and facilitate the incorporation of additional context for specific tasks. The following techniques were employed:\n1) Data Preparation for Tasks 1, 2, and 3: To align with the specific requirements of Tasks 1, 2, and 3, we prepared the dataset as follows:\n\u2022 Task 1: The input data consists of three chan-nels-reflectance, transmittance, and distance. Each input image is padded to a square, and then resized to 518\u00d7518. Padding is performed with values of -1, as 0 holds meaningful significance in the dataset.\n\u2022 Task 2: For the second and third tasks we need to include frequency information as well. Additionally, for task 3 we also need to incorporate the information about the antenna type. We chose to encode this information by introducing additional input channels of the same dimensionality.\n\u2022 Task 3: To represent the antenna's radiation pattern and simulate signal propagation in Task 3, we visualized signal strength as triangular regions radiating from the antenna's location. The key steps are as follows:\nAntenna Location Identification: The antenna's posi-tion is determined by identifying the pixel with the minimum intensity in the third channel of the input image.\nSignal Data and Azimuthal Adjustment: The radi-ation pattern data contains signal strength values for each 360 angles (0\u2013359). An azimuth offset is applied to align the orientation of the antenna.\nTriangular Signal Representation: For each angle,the following actions are taken:\n* Two angles, offset by 0.5\u00b0, are calculated to form the edges of the current triangle.\n* The triangle is defined by three points: the an-tenna's location and two points determined by the angles and a maximum signal propagation length.\nSignal Strength Encoding: Each triangle is filled witha signal strength value from the radiation pattern data, with pixel intensity representing the signal strength in the respective direction.\nThe resulting radiation pattern visualizes the antenna's signal coverage, providing spatial context for signal propagation and station placement, as illustrated in fig. 1d.\n2) Normalization: We performed exploratory analysis on the provided data and determined the ranges of values for reflectance, transmittance and distance. We choose channel specific maximum values (25 for reflectance, 20 for transmittance and 200 for distance), divided each channel by those numbers. These normalized channels become an input for the models focused on the first task.", "\u2022 For encoding the frequency information, we simply create a channel with a constant value equal to the frequency in GHz,divided by 10 to ensure the values are less than 1.\nFor normalizing the channel representing the radiation pat-tern, we also choose channel specific maximum values (40),and divided the channel by this number.\n3) Data Augmentation: Data augmentation is a criticalstep, especially when dealing with limited training data. Byartificially increasing the diversity of the dataset, augmentationmethods simulate various real-world scenarios and improvethe generalization capability of the model. We applied thefollowing augmentation techniques:\na) MixUp Augmentation: MixUp is an augmentationtechnique that generates data by blending pairs of input samples and their associated labels to create more generalizedrepresentations of the data. We employed this method toimprove the model's generalization and robustness. In ourtraining pipeline, MixUp was applied to 75% of the trainingdata, while the remaining 25% of the samples were leftunaltered. This balanced approach ensures that the networkistrained on both augmented and original data, enabling it tolearn a diverse feature space while maintaining alignment withthe original data distribution.\nb) Rotation Augmentation: Rotation augmentation introduces variations in the orientation of input images, enhancingthe model's invariance to spatial transformations. During train-ing, each input sample is randomly rotated by 0\u00b0, 90\u00b0, 180\u00b0, or270\u00b0 (multiples of 90\u00b0) degrees, chosen with equal probability.It is clear that in the case of 0\u00b0 no change occurs, whichmeans that rotation is performed on approximately 75% of thetraining data. This ensures that the network can distinguishbetween augmented and unaugmented orientations, learningmore robust spatial features.\nc) Cropping and Resizing: Cropping is employed tosimulate partial observations of the input data. In this case,75% of the training data is also processed, leaving the restunchanged. The side length of the square is chosen randomlyfrom a range between half the size of the input image (259pixels) and the full size (518 pixels). After cropping, theresulting square is resized back to 518 \u00d7 518 to match theinput dimensions required by the model.\n4) Training, Validation, and Testing Splits: For our tasks,the dataset was partitioned into training, validation, and testingsets based on specific criteria. The splits for each task aredetailed below:\n\u2022 Dataset Structure:\nTraining data includes environments B1-19, withadditional augmentations such as MixUp, rotation,and cropping.\nValidation data includes environments B20-22, uti-lized without augmentation for task evaluation.\nTesting data includes environments B23-25, pre-served for final performance assessment.\n\u2022 For Task 2:\nValidation and Testing sets include frequency f2 for evaluation purposes.\n\u2022 For Task 3:\nTraining data includes Ant1-3. Ant4 is saved for validation. And Ant5 is saved for testing to assess performance based on different antenna configurations.\nSo, for our validation and test, we are picking unseen rooms for Task 1, also unseen frequency for Task 2, and also unseen antenna for Task 3. The input channels and the target for a training example, after applying the preprocessing steps and augmentations, can be seen in fig. 2.": ""}, {"title": "B. Neural Network Design", "content": "Vision transformers (ViT) [9] have been prominently proven to achieve state-of-the-art results in image-based tasks and beyond. Moreover, we have a successful experience in adopting a ViT-based approach to map reconstruction based only on radio information [5]. In the previous section, we described the way the input data is represented as an image. In this section, we will present our proposed neural network based on the DINOv2 vision transformer.\nOur neural network consists of three parts: DINOv2 vision transformer [10], used as an encoder, UPerNet convolutional decoder [11], and a neck responsible for connecting the Vit-based encoder and the convolutional decoder. We choose the ViT-B/14 version of DINOv2 with pre-trained weights. First, the input image is passed through a convolutional layer, which outputs a three-channel image. We do this for compatibility with the DINOv2 input, to be able to leverage the pre-trained weights of the network. The resulting image is passed to the encoder to obtain the embeddings of the image. Then the embeddings from all the 14 layers (the first layer, 12 hidden layers, and the output layer) are passed through a linear layer to decrease the embedding size from 768 to 256 for task 1, and 512 for tasks 2 and 3. The low-dimensional embeddings are then reshaped into 37*37 squares and fed to the neck, which consists of a convolutional layer with a ReLU activation function, a resize operation, and another convoultional layer with a ReLU activation function. The activations are then passed to the UPerNet decoder to obtain the output. To reinforce the room borders to the network, we also concatenate the reflectance and transmittance channels to the activations obtained from the neck, before feeding it to the decoder. Sigmoid function is then applied to the output, and the result is multiplied by 160 to get the final prediction."}, {"title": "IV. RESULTS", "content": "Table I summarizes the performance across the three tasks. We report the performance on the test set, that we created and on the test set provided by the First Indoor Pathloss Radio Map Prediction Challenge at ICASSP 2025 [8]. These results secured the 8th place in the challenge.\nOur analysis showed that the large difference between the scores on two test sets comes mostly from a distribution shift between building layouts. The layouts from the Challenge Test Set are significantly smaller and have denser walls.\nDevelopment of methods to increase robustness of the deep learning models on such shifts is left for future work.\nThe results also show that our training strategy is quite robust with respect to changes in frequencies and antenna types. The degradation of performance between Task 1 and the other two tasks is less than 3.4 in absolute values, or 30% in relative terms, in the Challenge Test Set. This ratio is better than the degradation we have seen on our test set, and is comparable to the winner solutions of the Challenge. Whether this robustness comes from the input representation or the augmentation technique requires more ablation studies that is left for future work."}]}