{"title": "Real-world Adversarial Defense against Patch\nAttacks based on Diffusion Model", "authors": ["Xingxing Wei", "Caixin Kang", "Yinpeng Dong", "Zhengyi Wang", "Shouwei Ruan", "Yubo Chen", "Hang Su"], "abstract": "Adversarial patches present significant challenges to the robustness of deep learning models, making the development of\neffective defenses become critical for real-world applications. This paper introduces DIFFender, a novel DIFfusion-based DeFender\nframework that leverages the power of a text-guided diffusion model to counter adversarial patch attacks. At the core of our approach is\nthe discovery of the Adversarial Anomaly Perception (AAP) phenomenon, which enables the diffusion model to accurately detect and\nlocate adversarial patches by analyzing distributional anomalies. DIFFender seamlessly integrates the tasks of patch localization and\nrestoration within a unified diffusion model framework, enhancing defense efficacy through their close interaction. Additionally,\nDIFFender employs an efficient few-shot prompt-tuning algorithm, facilitating the adaptation of the pre-trained diffusion model to\ndefense tasks without the need for extensive retraining. Our comprehensive evaluation, covering image classification and face\nrecognition tasks, as well as real-world scenarios, demonstrates DIFFender's robust performance against adversarial attacks. The\nframework's versatility and generalizability across various settings, classifiers, and attack methodologies mark a significant\nadvancement in adversarial patch defense strategies. Except for the popular visible domain, we have identified another advantage of\nDIFFender: its capability to easily expand into the infrared domain. Consequently, we demonstrate the good flexibility of DIFFender,\nwhich can defend against both infrared and visible adversarial patch attacks alternatively using a universal defense framework.", "sections": [{"title": "1 INTRODUCTION", "content": "EEP neural networks are susceptible to adversarial\nexamples [11], [38], where small, often imperceptible\nperturbations are deliberately introduced to natural images,\ncausing the model to make erroneous predictions with\nhigh confidence. The majority of adversarial attacks and\ndefenses have focused on lp-norm threat models [4], [8],\n[11], [26], which constrain adversarial perturbations within\nan lp-norm boundary to ensure they remain imperceptible.\nHowever, these conventional lp-based perturbations neces-\nsitate altering every pixel of an image, a method that is\ntypically impractical in physical environments. In contrast,\nadversarial patch attacks [3], [21], [22], [41], which focus\nperturbations on a specific region of the object, are more\nfeasible in real-world scenarios. These patch-based attacks\npose substantial threats to applications such as face recogni-\ntion [35], [48] and autonomous driving [7], [19].\nDespite the numerous adversarial defenses against patch\nattacks proposed in recent years, their performance remains\ninsufficient to ensure the safety and reliability required for\nreal-world applications. Some approaches rely on adver-\nsarial training [31], [46] and certified defenses [5], [12],\nwhich tend to be effective only against specific types of\nattacks and often fail to generalize well to other forms of\npatch attacks in practical scenarios [29]. Another category of\ndefenses involves pre-processing techniques [14], [24], [28],\n[51], which aim to neutralize adversarial patches through\nmethods like image completion or smoothing. However,\nthese techniques frequently struggle to preserve the high\nfidelity of the original images, resulting in visual artifacts in\nthe reconstructed images that can negatively affect recogni-\ntion performance. Moreover, these defenses are vulnerable\nto stronger adaptive attacks that exploit gradient obfusca-\ntion [2], further limiting their effectiveness.\nRecently, diffusion models [16], [36] have gained promi-\nnence as a powerful class of generative models, showing\nsuccess in enhancing adversarial robustness through the\npurification of input data [29], [40], [47]. Initially, we hy-\npothesize that diffusion purification might be effective in\ndefending against patch attacks. However, our experiments\nreveal that this approach falls short, as it fails to eliminate\nadversarial patches. Instead, we discover a phenomenon we\nterm Adversarial Anomaly Perception (AAP), illustrated\nin Fig. 1. This phenomenon demonstrates that by analyzing\nthe differences between multiple denoised versions of an\nimage, it is possible to localize adversarial patches. This\ninsight allows for the targeted restoration of the specific\nregions affected by the patch. The underlying reason for\nthis phenomenon may be that adversarial patches are of-\nten intricately designed perturbations or contextually in-\nappropriate elements that starkly contrast with the natural\nimage distributions on which the model was trained. This\ndiscovery advances our understanding of how diffusion\nmodels can differentially respond to adversarial patches,\nthereby addressing the challenge of balancing the removal\nof patches with the preservation of image semantics.\nBased on the AAP phenomenon, we further introduce\nDIFFender, a novel DIFfusion-based DeFender framework\nagainst adversarial patch attacks, utilizing the text-guided\ndiffusion models. DIFFender operates by first localizing\nthe adversarial patch through the analysis of discrepan-"}, {"title": "2 RELATED WORKS", "content": null}, {"title": "2.1 Adversarial Attacks", "content": "Deep neural networks (DNNs) can be deceived into pro-\nducing incorrect outputs by introducing small perturbations\nto the input data. Most adversarial attacks [8], [11], [26],\n[27] achieve this by subtly altering pixel values, leading to\nmisclassification errors. While these techniques are effective\nin generating adversarial examples in digital environments,\nthey often lack practicality in real-world applications.\nIn contrast, adversarial patch attacks mislead models\nby applying a visible pattern or sticker to a localized area\nof an object, a method that is more feasible in physical\nsettings. First introduced in [3], adversarial patches target\ndeep neural networks used in real-world scenarios, aiming\nto introduce unbounded perturbations within specific re-\ngions of clean images. Unlike lp-norm-based perturbations,\nwhich are designed to be imperceptible, adversarial patches\nare conspicuous yet resilient modifications, making them\nparticularly effective for physical attacks. These patches\nhave been widely employed across various visual tasks,\nposing significant threats to model deployment.\nPrevious research has explored various approaches to\ndeveloping more effective patches. For example, meaning-\nless patch attacks like LaVAN [21] randomly select patch\nlocations and generate perturbations, while GDPA [22] op-\ntimizes both patch placement and content to enhance attack\neffectiveness. Similarly, Wei et al. [42] introduced a rein-\nforcement learning framework to jointly optimize texture\nand position in a black-box setting. Additionally, RHDE [41]\nproposed a natural and practical patch attack method, using\nreal stickers and optimizing their placement for adversarial\npurposes. This approach not only achieves a high success\nrate but is also easy to implement, as it can utilize common\nmaterials like cartoon stickers as fixed patterns."}, {"title": "2.2 Adversarial Defenses", "content": "As adversarial attacks have evolved, numerous defense\nmechanisms have been proposed. However, most exist-\ning defenses predominantly address global perturbations\nconstrained by lp norms, including earlier diffusion-based\ndefenses [29], [40], [47], while defenses specifically targeting\npatch attacks have received less attention. Although adver-\nsarial training [31], [46] and certified defenses [5], [12] are\neffective against certain types of attacks, they often fail to\ngeneralize to other forms of patch attacks.\nAs a result, many studies have focused on pre-processing\ndefenses. For example, Digital Watermarking [14] employs\nsaliency maps to detect adversarial regions and uses erosion\noperations to eliminate small perturbations. Local Gradient\nSmoothing [28] targets regions with high gradient ampli-\ntudes, smoothing gradients to mitigate the high-frequency\nnoise introduced by patch attacks. Feature Normalization\nand Clipping [51] reduces informative class evidence by\nperforming gradient clipping, leveraging network structure\nknowledge. Jedi [39] uses entropy-based masking, while\nSAC [24] offers a general framework for detecting and\nremoving adversarial patches.\nWhile these methods offer some defense against patch\nattacks, they often struggle to accurately reconstruct the\noriginal image and can be circumvented by adaptive attacks"}, {"title": "2.3 Infrared Adversarial Attacks and Defenses", "content": "Adversarial examples are prevalent across various domains.\nRecently, researchers have begun to explore adversarial\nexamples in infrared imagery. Edwards and Rawat [10]\ninvestigated the performance of adversarial attacks in ship\ndetection under thermal infrared imaging. Osahor and\nNasrabadi [30] explored how to generate visually imper-\nceptible adversarial infrared examples that can evade detec-\ntion by deep neural network-based object detectors. These\nmethods generate perturbations by altering pixel values\nwithin infrared images, thus rendering them impractical for\nuse in the physical world. To address this issue, Zhu et\nal. [55] made the first attempt to create physical adversarial\nexamples using a set of small bulbs that modify the infrared\nradiation distribution of an object by simulating additional\nheat sources. Subsequently, Zhu et al. [54] proposed adver-\nsarial clothing designed to deceive infrared detectors from\nvarious angles by enveloping the entire body. Wei et al. [43]\nintroduced a method named Unified Adversarial Patch\n(UAP), which designs a unified adversarial patch capable\nof affecting detection systems across different modalities.\nSpecifically, the authors constructed a patch that produces\nadversarial effects in both visible and infrared images, fa-\ncilitated by the use of special materials and coatings for\nmulti-modal attacks. Furthermore, Wei et al. [45] proposed\nAdversarial Infrared Patches, focusing on designing the\nshape and location of patches rather than complex patterns,\nmaking them easy to implement in physical world.\nIn the defense domain, techniques such as PixelMask [1],\nBit squeezing [49], JPEG compression [9], Spatial Smooth-\ning [49], and Total variation minimization [13] are employed\nto defend against infrared patch attacks. However, these\nmethods were not specifically designed for infrared patch\nattacks and thus do not achieve satisfactory results. Cur-\nrent research on patch adversarial defense predominantly\nfocuses on the RGB modality, with little attention to others\nsuch as the infrared modality. Our work showcases the first\nto concurrently address both RGB and infrared modalities,\nfurther validating multi-modal attack defense."}, {"title": "3 METHODOLOGY", "content": "In this section, we present the proposed DIFFender frame-\nwork. The pipeline of DIFFender, illustrated in Fig. 2,\ncomprises three key modules: patch localization, patch\nrestoration, and prompt tuning. We begin by discussing"}, {"title": "3.1 Discovery of the AAP Phenomenon", "content": "iffPure [29] is a recent method that employs diffusion\nmodels to remove imperceptible perturbations by introduc-\ning Gaussian noise at a predetermined ratio t* (ranging\nfrom 0 to 1) to adversarial images, followed by a denoising\nprocess using the reverse dynamics of diffusion models.\nOur initial objective was to evaluate the effectiveness of\nDiffPure against patch attacks. However, as shown in Fig.\n3, our empirical results reveal that DiffPure is insufficient\nfor countering patch attacks. This shortcoming arises from\na fundamental trade-off: a larger t* is required to effectively\npurify adversarial perturbations, but this also risks compro-\nmising the image's semantic integrity, whereas a smaller t*\npreserves semantics but fails to eliminate the adversarial\npatches. This makes it impossible to identify an optimal\nnoise ratio that can effectively defend against patch attacks.\nIn contrast, we observed that at a critical noise ratio t*, a\nunique pattern emerged: while adversarial patches resisted\ndenoising and struggled to be restored, the rest of the image\nremained semantically intact. This observation suggests that\nby analyzing differences between various denoised images,\nit is possible to identify the regions containing adversarial\npatches. This finding, illustrated in Fig. 4, led to the iden-\ntification of the Adversarial Anomaly Perception (AAP)\nphenomenon.\nThe AAP phenomenon likely occurs because adversarial\npatches are often carefully engineered perturbations with\ncomplexity that far exceeds the natural noise found in real\nimage datasets. Alternatively, they may represent meaning-\nful stickers placed in contextually inappropriate locations,\nmaking them stand out as anomalies. Since diffusion models\nare trained to learn the probability distribution of natural\nimages, they struggle to adapt to the distribution of adver-\nsarial examples in their latent space, leading to noticeable\ndiscrepancies.\nThe discovery of AAP offers valuable insights into how\ndiffusion models can differentially respond to adversarial\npatches. It enables the diffusion model to detect and localize\nadversarial patches by analyzing distributional discrepan-\ncies, which in turn facilitates targeted restoration of the"}, {"title": "3.2 DIFFender", "content": "Patch Localization. DIFFender begins with precise patch\nlocalization, leveraging the AAP phenomenon observed in\ndiffusion models. For an adversarial image xadv, we first\nintroduce Gaussian noise to generate a noisy image xt with\na specific noise ratio t* (set to 0.5 in our experiments).\nNext, a text-guided diffusion model is applied to denoise\nxt, producing xp using a textual prompt prompt\u2081, and xe\nusing an empty prompt. The adversarial patch region M\nis then estimated by calculating the difference between the\ndenoised images xp and xe.\nHowever, diffusion models typically require a significant\nnumber of time steps T, leading to high computational\ncosts. To mitigate this, we directly predict the image xo\nfrom xt in a single step, reducing the processing time by a\nfactor of T. Although the one-step prediction may introduce\nsome blurriness and discrepancies, the differences between\none-step predictions still reflect the AAP phenomenon. In\npractice, we perform one-step denoising twice, yielding two\nresults: xa, guided by prompt \u2081, and x6, guided by an empty\nprompt. The difference is then binarized to estimate the\npatch region as follows:\nM = Binarize  (\\frac{1}{m} \\sum_{i=0}^{m} (x_a - x_b)) (1)\nwhere the difference is computed m times (set to 3 in our\nexperiments) to enhance stability and reduce randomness.\nThe prompt can be manually designed (e.g., \"adversarial\")\nor automatically tuned, as discussed in Sec. 3.3.\nSpecifically, we compute the difference between the la-\ntent denoising results for each pair of noisy inputs. The\nabsolute differences of the latent variables are summed\nacross channels, averaged, and normalized.\nMask Refinement. As shown in Fig. 5, the initial mask\nderived from the averaged difference may contain minor\ninaccuracies. To address this, we first binarize the difference"}, {"title": "3.3 Prompt Tuning", "content": "In line with the pipeline described", "53": "we\nintroduce the prompt-tuning algorithm to effectively adapt\nthese learned representations for adversarial defense tasks\nby only few-shot tuning.\nLearnable Prompts. We begin by replacing the textual\nvocabulary with learnable continuous vectors. Unlike text\nprompts", "follows": "nprompt\u2081 = [VL", "1[VL": "2...", "VL": "n; (2)\npromptr = [VR", "1[VR": "2...", "VR": "n"}, {"VL": "or [VR", "M": "nLCE(M", "x": "nL1 (Xr, x) = |x - X|. (4)\nFinally, to ensure the adversarial effects are fully mitigated,\nwe draw inspiration from [23", "52": "by aligning the high-"}]}