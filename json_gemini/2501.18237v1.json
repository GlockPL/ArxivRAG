{"title": "Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular Intervals with Vision Transformers", "authors": ["Malte T\u00f6lle", "Mohamad Scharaf", "Samantha Fischer", "Christoph Reich", "Silav Zeid", "Christoph Dieterich", "Benjamin Meder", "Norbert Frey", "Philipp Wild", "Sandy Engelhardt"], "abstract": "A patient undergoes multiple examinations in each hospital stay, where each provides different facets of the health status. These assessments include temporal data with varying sampling rates, discrete single-point measurements, therapeutic interventions such as medication administration, and images. While physicians are able to process and integrate diverse modalities intuitively, neural networks need specific modeling for each modality complicating the training procedure. We demonstrate that this complexity can be significantly reduced by visualizing all information as images along with unstructured text and subsequently training a conventional vision-text transformer. Our approach, Vision Transformer for irregular sampled Multi-modal Measurements (ViTiMM), not only simplifies data preprocessing and modeling but also outperforms current state-of-the-art methods in predicting in-hospital mortality and phenotyping, as evaluated on 6,175 patients from the MIMIC-IV dataset. The modalities include patient's clinical measurements, medications, X-ray images, and electrocardiography scans. We hope our work inspires advancements in multi-modal medical AI by reducing the training complexity to (visual) prompt engineering, thus lowering entry barriers and enabling no-code solutions for training. The source code will be made publicly available.", "sections": [{"title": "Introduction", "content": "During a hospital stay, a patient typically undergoes multiple examinations, each offering distinct insights into their health status. While physicians have learned to intuitively extract the different information and assemble them to an overall picture, neural networks need specific modeling of the different modalities and their interactions. Nevertheless, once these challenges are addressed, multi-modal models have demonstrated promising performance [1,2,8,17,19,21,38,47]. However, a significant challenge persists: How to integrate multi-modal data that is captured at irregularly sampled time intervals (Fig. 1)?\nPatients admitted to the intensive care unit (ICU) require close monitoring and extensive diagnostics to restore a positive health status. The extensive assessment of a patient's health leads to significant resources being spent on ICU patients, in the US alone they amount for up to 1% of GPD annually [13]."}, {"title": "Uni-modal Training", "content": "First, all methods are evaluated for single modality training. MeTra and MedFuse are designed to work with only two modalities. However, our method easily accommodates training on administered medications and ECG scans by converting their time-series data into visual plots, while utilizing the exact same training procedure as for clinical measurements and CXR images.\nFor clinical measurements (C) our method achieves the highest area under receiver operating curve (AUROC) of 0.837 (MeTra: 0.791, MedFuse: 0.812) and balanced accuracy (Bal.Acc. = (sensitivity + specificity) / 2) of 0.743 (MeTra: 0.609, MedFuse: 0.571). For area under the precision recall curve (AUPRC) MeTra achieves a value of 0.441, MedFuse 0.448, and ours 0.512. Classifying CXR images (X) mainly comes down to choosing the better feature extractor. Here, we can see the superiority of the Swin transformer [34] in our method (AUROC=0.826) compared to the conventional ViT [7] in MeTra (0.810) and a ResNet34 [15] in MedFuse (0.662). Training on administered medications (M) and ECG scans (E) yields lower classification accuracy. For medications our method achieves a AUROC of 0.741 (AUPRC=0.346, Bal.Acc.=0.680), for ECG the AUROC is 0.704 (AUPRC=0.297, Bal.Acc. 0.636).\nPhenotyping, a multi-label multi-class binary classification task for present diagnoses, is a more challenging task than predicting in-hospital mortality. However, our method, ViTiMM, outperforms MeTra and MedFuse in the single modality setting for both, clinical measurements (C) and CXR images (X) (see Table 1). By training on C we achieve an AUROC of 0.766 (MeTra: 0.691, MedFuse: 0.705), with training on X the scores are slightly lower: ViTiMM: 0.730, MeTra: 0.667, MedFuse: 0.644. Again, this is mostly determined by the employed architecture."}, {"title": "Training on Paired X-ray and Clinical Measurements Data", "content": "Having investigated single modality training we combine two modalities, clinical measurements (C) and CXR images (X) as was done by MedFuse and MeTra as well."}, {"title": "Extension to All Modalities", "content": "To the best of our knowledge no method in the literature has so far investigated training on the four modalities, clinical measurements (C), medications (M), X-ray images (X), and ECG scans (E). Thus, we do not have a direct comparison to our method.\nWhen feeding all modalities, clinical measurements, CXR images, medications, one electrocardiography scan, as well as the corresponding demographics, CXR reports, ECG machine measurements, and patient's diagnoses the predictive performance can significantly be improved. ViTiMM reaches an AUROC of 0.922 outperforming the two modality training across all methods. Also in terms of AUPRC (0.764) and balanced accuracy (0.847) training with all modalities represents the benchmark.\nFor phenotyping the predictive performance of our method cannot be enhanced to a similar extend as for In-hospital Mortality. Still, we achieve a new benchmark with an AUROC of 0.784 by combining clinical measurements, administered medications, CXR images, and ECG scans. The AUPRC is 0.549, balanced accuracy lies at 0.659. In contrast to the mortality prediction task we cannot feed the patient's diagnoses as they shall be predicted from the input data."}, {"title": "Interpretability with Attention Visualization", "content": "Transformer architectures use the attention mechanism for trading of local and global contexts in vision tasks [7]. When using the conventional ViT the importance of input features can be visualized by overlaying the attention weights of the classification. The same procedure can be conducted for the text input. The patient, whose input data is shown in Figure 3, has not survived his hospital stay. The model specifically focuses on specific features in the clinical measurements data, namely Respiratory Rate (from bottom left row 2, column 6), Non Invasive Blood Pressure Diastolic (2,4), Non Invasive Blood Pressure Diastolic (2,3), and Heart Rate (3,5) (Figure 3a with corresponding field names in Supplementary Figure 1). In the medication inputs the most weight is put on Vasopressors and Inotropes (1,2), more precisely Norephinephrine, Phenylephrine, and Dobutamine, but the model attends to all administered medications to a small degree (Figure 3b with corresponding field names in Supplementary Figure 2). The attention on the electrocardiography scan is diffuse, the focus rather seems to lie on the overall interaction of all leads. However, the largest attention lies on lead aVL (blue in Figure 3c). In the CXR image specific focus lies on the implanted device (Figure 3d). In the text input the model directs specific attention to the administered medications and the implanted pacemaker. General attention lies on the patient demographics and ECG findings. While no findings and impressions are present in the CXR report, the attention peaks in this part indicating general importance if information exists. Further attention visualizations for both classes in the In-hospital mortality task can be found in Supplementary Figures 3 and 4."}, {"title": "Discussion", "content": "We present a method that can learn from arbitrary modalities by employing vision transformer only by representing the respective data in image format as an extension to ViTST [29]. We demonstrate that by representing any modality as an image, in a manner broadly analogous to how humans interpret data, multi-modal training can be achieved with minimal effort. This is because vision transformers are inherently capable of capturing the context of data in a way that resembles human perception. This approach simplifies the training process significantly, it is broken down to visual prompt engineering, which can be performed with little coding knowledge. We hope this will lower the barrier of multi-modal training immensely in the future. Our model not only simplifies multi-modal training in medicine but also shows superior performance on the benchmark tasks of in-hospital mortality prediction and phenotyping [12] compared to approaches in the literature [14,27]. To the best of our knowledge we are the first to train one model across the modalities clinical measurements, CXR images, medications, and ECG scans as well as further patient metadata on the MIMIC-IV dataset [11,24,25,26]."}, {"title": "Visual Prompt Engineering Simplifies Modeling of Modalities", "content": "Humans are able to quickly grasp information and trends from visual prepared graphical data. Vision transformer seem to be able to capture information in a similar manner also observing interactions between different variables. The approach is especially beneficial for irregularly sampled time series data [29]. Most approaches need to explicitly model the time dimension by e.g. partitioning the data in fixed time intervals [31,35] or use dedicated architectures such as LSTMs or graph neural networks [3,36,51]. However, modeling the time dimension might lead to loss of information dependent on e.g. the sampling rate when partitioning observations. On the other side, when representing the data as line graphs observations can be explicitly marked and missing intervals interpolated in-between. The model is made aware of the missing information in this area, but is presented a hint for possible values in the interval. The advantage is emphasized when only comparing the results for clinical measurements (C) across the three methods (MeTra [27], MedFuse [14], and ViTiMM) in Table 1, where our method obtains the best performance across all metrics. In MeTra for each clinical parameter in question the mean over the time period is taken; in MedFuse all measurements must be discretized and harmonized to similar time points across variables."}, {"title": "Advantages of the Multi-Modal Approach", "content": "Our results confirm that multi-modal training is beneficial for all investigated methods for both tasks under consideration. The multi-modal approach enables the extraction of complementary information from the used modalities. Our approach, ViTiMM, outperforms the two comparative approaches (MeTra [27] and MedFuse [14]) with the same data sources, clinical measurements and CXR images. We attribute this to potential information loss in the time series data caused by averaging the variables over the respective timeframe (see above).\nThe key difference between ViTiMM and other approaches in the literature is its straightforward extensibility to other modalities by either representing the data as an image such as e.g. line graphs or providing the data as unstructured text. Thus, we are able to also feed the medications of each patient despite the high missing ratios (see Supplementary Table 3), the ECG scan, and diagnoses of the patients without altering our encoders. While our method already achieves the best results compared to MeTra and MedFuse with only clinical measurements and CXR images, the additional modalities lead to even better performances. Extension of MeTra and MedFuse to other modalities is not straightforward, as each modality must be separately modeled and, thus, receive a different encoder than used currently in their methods. This would alter the presented methods drastically, which might not align with the intentions of the authors. In general, especially medications would require much modeling due to high missing ratios and the different dosages of different solutions of the active ingredients.\nStill, careful consideration must be made regarding possible confounding variables in the input modalities. For example, certain of our considered medications, particularly opiods and sedatives, are commonly administered before death. To assess their impact on the accuracy of predictions, we analyzed the model outputs for those using medications as input (all modalities and medications only). We compared the performance across two subgroups: patients who received opiods and/or sedatives (group1) vs. those who did not (group2). Comparing AUROC values, we observed a slight increase in performance for group1 (0.942 vs. 0.930 in the all modality setting and 0.768 vs. 0.751 for medications only). However, a DeLong test found these differences not statistically significant (p=0.546, p=0.580) [5]."}, {"title": "Benchmark Tasks", "content": "The superiority of using multiple modalities for training due to their complementary information has been proven multiple times [28]. However, public dataset with multiple modalities are scarce. Especially in medicine the literature focuses on the MIMIC-IV dataset [14,25,27,42,49]. The MIMIC-IV is a role model for an open source multi-modal dataset covering several modalities namely clinical measurements, medications, CXR images, ECG scans, and recently also echocardiography (ECHO) data [10]. Benchmark tasks are defined with a extraction pipeline establishing comparability between studies [12]. However, some discrepancies still occur when paired or partial samples across modalities are used [14,27]. Further public multi-modal benchmark datasets are needed to examine the generalizability of our and other methods.\nIn future work we also plan to incorporate echocardiography data into our pipeline, which technically is straightforward as they are naturally represented as images. However, echocardiography data has a very large intra-patient variance due to multiple views, which are not explicitly labeled or identified in the dataset."}, {"title": "Methods", "content": "This manuscript's study and results adhere to all pertinent ethical guidelines and uphold ethical standards in both research conduct and manuscript preparation, in accordance with all relevant laws and regulations concerning human subject treatment. All models were trained on publicly available datasets described below and tested for their performance in predicting the survival of patients in intensive care."}, {"title": "Data and Benchmarks", "content": "The most frequently used dataset with matched time-resolved samples on a patient level across multiple modalities is the the Medical Information Mart for Intensive Care (MIMIC) [25]. MIMIC-IV is a freely accessible dataset that contains extensive clinical information from de-identified patients admitted to the emergency department or an intensive care unit at Beth Israel Deaconess Medical Center (BIDMC) in Boston, Massachusetts, between 2008 and 2019. It consists of two main modules. The hospital-wide EHR module includes patient and admission information, medication administrations, billed diagnoses, microbiological data, laboratory measurements and hospital performance-related information. The ICU-specific module comprises detailed records to ICU stays, including intravenous and fluid administrations, patient excretions, ventilation data, and clinical interventions. In total, the dataset contains 299,712 unique patient identifiers, 431,231 hospitalizations and 73,181 intensive care unit (ICU) stays. All data is publicly available via Physionet [9].\nFor subsets of the patients contained in the MIMIC-IV dataset further modalities are published, more precisely chest X-ray (CXR) images (MIMIC-CXR) [24,26] as well as electrocardiography scans (MIMIC-IV-ECG) [11]. The MIMIC-CXR dataset includes 377,110 CXR images (frontal and lateral views) corresponding to 227,835 radiological examinations collected between 2011 and 2016. The radiographs are in DICOM format and sourced from the hospital's Picture Archiving and Communication System (PACS). A free-text radiology report is assigned to each examination. MIMIC-IV-ECG provides electrocardiograms from 161,352 different patients and includes 800,035 diagnostic ECGs recorded be-"}, {"title": "Model Input Transformations", "content": "To enable training on time series data with vision transformers a transformation into images more precisely line graphs is necessary. All three time series modalities need different preprocessing to be afterwards processed in a similar way. Only X-ray scans need no further preprocessing as they are already in image format (Figure 2d).\nTo visualize the course of temporal data points most often line graphs are used, where each observation is marked by its time (x-axis) and value (y-axis). These observations are connected with straight lines in chronological order, which accounts for interpolating values in between. This approach is agnostic to the type of time series under consideration and generalizes as we show for different modalities. Essentially, this is a form of visual prompt engineering similar to language models, where users refine and optimize natural language prompts to improve model performance [29]. To distinguish measurements from interpolation we use a \"*\" symbol to indicate observations. We plot each variable in a separate plot with a distinct color for better differentiation as was found beneficial for model training (see Figure 2a) [29]. We omit tick labels as well as other graphical components as an observation's position already signals its relative time and magnitude."}]}