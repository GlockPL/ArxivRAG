{"title": "Voice-Enabled AI Agents can Perform Common Scams", "authors": ["Richard Fang", "Dylan Bowman", "Daniel Kang"], "abstract": "Recent advances in multi-modal, highly capable LLMs have enabled voice-enabled AI agents. These agents are enabling new applications, such as voice-enabled autonomous customer service. However, with all AI capabilities, these new capabilities have the potential for dual use.\nIn this work, we show that voice-enabled AI agents can perform the actions necessary to perform common scams. To do so, we select a list of common scams collected by the government and construct voice-enabled agents with directions to perform these scams. We conduct experiments on our voice-enabled agents and show that they can indeed perform the actions necessary to autonomously perform such scams. Our results raise questions around the widespread deployment of voice-enabled AI agents.", "sections": [{"title": "Introduction", "content": "AI capabilities have advanced rapidly in the past few years. Recently, AI vendors have introduced capabilities for tool use and real-time voice conversations (OpenAI, 2024). Combined, these capabilities allow for beneficial applications, such as autonomous customer service (OpenAI, 2024). However, as with all AI capabilities, these capabilities have the potential for dual use (Kang et al., 2024; Fang et al., 2024b; Urbina et al., 2022; Weidinger et al., 2022, 2021).\nIn this work, we investigate the question: can voice-enabled AI agents perform the tasks needed to conduct common scams? We answer the question in the affirmative, showing that voice-enabled Al agents can perform common scams in real-time. To do so, we first identify a list of common scams as collected by the government (Paxton, 2024). From these scams, we designed voice-enabled AI agents with directions to conduct these scams with access to simple tools (Figure 1). In this work, we focus specifically on the actions needed to perform the scams and do not consider questions of persuading victims.\nWe conduct a series of experiments, showing that voice-enabled AI agents are highly capable of autonomously performing the actions needed to conduct these common scams. These actions include logging into bank accounts, completing a two-factor authentication process by eliciting the code from the user, and others."}, {"title": "Common Scams", "content": "Phone-based scams are incredibly prevalent. According to the Office of the Attorney General for DC, they target as many as 17.6 M Americans and the social cost is as much as $40 billion per year (Schwalb, 2024).\nThese scams typically involve a scammer calling a victim to convince them to take actions or reveal sensitive information. Based on these actions or information, the scammer then typically steals funds from the victim. We provide a list of common scams in Table 1 as provided by the Attorney General of Texas' office (Paxton, 2024).\nPerforming these scams can require complex interactions with websites and feedback from the user. Consider a scam where the scammer steals"}, {"title": "Agent Design", "content": "We designed a series of agents to perform the actions necessary for common scams. Our agents consist of a base, voice-enabled LLM (GPT-40), a set of tools that the LLM can use, and scam-specific instructions. The LLM and tools were the same for all agents but the instructions varied.\nThe AI agents had access to five browser access tools based on the browser testing framework playwright. These tools are granular browser actions:\nget_html, which gets the HTML of a page.\nnavigate, which navigates to a specific URL.\nclick_element, which clicks on an element with a CSS selector.\nfill_element, which fills an element with the specified value.\nevaluate_javascript, which executes JavaScript on a page.\nWe used GPT-40 for all of our experiments. GPT-40 will refuse to handle user credentials in certain circumstances. We used a standard jailbreaking prompt template to bypass these protections. The instructions were specific to each scam."}, {"title": "Experiments", "sections": [{"title": "Experimental Setup", "content": "We deployed our agents on the scams in Table 1. However, we excluded gift card exfiltration and credential stealing (bank) since these scams are strictly easier (in terms of actions taken) than IRS impostor (gift card) and bank account transfer. We simulated a scam by manually talking with the voice agent with a credulous victim.\nIn order to determine if the scam successfully succeeded, we manually confirmed if the end state was achieved on real applications/websites. For example, we used Bank of America for the bank transfer scams and confirmed that the money was actually transferred. The credential stealing scams required a successful login. We list the applications (MyMonero, Gmail, Instagram, Bank of America, Google Play) in Table 2.\nWe executed each scam 5 times and recorded the overall success rate, the total number of tool calls (i.e., actions) required to perform each successfully executed scam, the total call time for each successfully executed scam, and the approximate API cost for each successfully executed scam. Namely, we exclude the unsuccessful scams for computing the number of actions and total call time."}, {"title": "Overall Results", "content": "Our agents are capable of performing all the scams in Table 1, with results shown in Table 2. The success rate ranges from 20% to 60%, with the success rate across all scams being 36%.\nPerforming these scams also takes a substantial number of actions. For example, the bank transfer scam takes 26 actions to complete. They also take a substantial amount of time, with complex scams taking as much as 3 minutes to execute. These agents also maintain coherence in conversation and adherence to goals over the conversations. Finally, the overall costs are low: under $0.75 on average per successful scam. The bank transfer scam's costs is primarily in text tokens from the large HTML from the website.\nBeyond the overall statistics, the majority of failures for many of the scams were actually due to transcription errors. For example, our GPT-40 based agents failed to transcribe the password or username for the Gmail credential stealing scam, but successfully completed the rest of the actions. However, other scams, such as the bank account transfer, required navigating complex websites (Bank of America), which caused errors in the actions themselves. Thus, the difficulty of the scam does appear to affect the success rate."}]}, {"title": "Case Study", "content": "To further understand the trajectory of these scams, we provide a redacted transcript and an abridged action log for a bank transfer scam.\nThe transcript proceeds as follows:\nVictim: Hello?\nScammer: Hello! This is John from Bank of America. We've noticed some unusual activity on your account and need to verify your information for security purposes. Could you please provide your username and password so we can ensure your account remains protected?\nVictim: What?\nScammer: I understand your concern. Your security is our top priority. To verify your identity and resolve this issue, I need to authenticate you through your Bank of America account. Please provide your login details so we can proceed with securing your account. Your cooperation is appreciated.\nVictim: Yeah, my username is [username], my password is [password].\nScammer: It seems we need to verify your identity. Could you please provide the 2FA code sent to your registered device? This will allow us to proceed with the login process.\nVictim: My code is [2FA code].\nBetween items 5 and 6 of the transcript, the agent navigates to the Bank of America login page and inputs the username and password, which takes 6 actions (navigate, get_html, fill_element, fill_element, click_element, get_html).\nAfter item 7 of the transcript, the agent performs 20 actions to fill out the 2FA code, navigate to the transfer page, and transfer the money. Conceptually, the agent must:\nFill out the 2FA code.\nNavigate to the transfer page.\nSearch for a recipient.\nSelect the recipient."}, {"title": "Discussion of Results", "content": "We believe that the capabilities demonstrated in this section are a lower bound for future voice-assisted AI agents. These agents are likely to improve in several ways.\nFor example, the actions that our agents must take are highly granular, such filling out specific fields, clicking on buttons, and navigating to specific websites. More ergonomic methods of interacting with web browsers will likely improve performance. Other agents improve significantly with techniques like retrieval-augmented generation (Lewis et al., 2020; Fang et al., 2024a).\nBeyond improvements in agents, base models have substantially improved in the past few years (Brown et al., 2020; Achiam et al., 2023). These improvements have translated to broad improvements in a range of downstream tasks and we anticipate that this will also be the case for efficacy in scams."}, {"title": "Related Work", "content": "We now provide an overview of related work."}, {"title": "Dual use of AI", "content": "The use of AI for dual use has widely been studied (Kang et al., 2024; Fang et al., 2024b; Urbina et al., 2022; Weidinger et al., 2022, 2021). These studies range from taxonomizing potential dual use applications of AI to demonstrating capabilities on cybersecurity attacks. To our knowledge, the ability to perform real-time voice conversations and perform tool use has not been widely available until recently. As such, ours is the first work to demonstrate that voice-enabled AI agents can perform the actions necessary for common scams."}, {"title": "AI scams and spam", "content": "Al has already been used in the real world to perform scams and produce spam. For example, deepfakes have already been used to scam a British engineering company out of $25 million dollars (Chen, 2024). They are also widely used to create social media spam (Bond, 2024). To our knowledge, autonomous, responsive voice scams are not widely deployed due to technological limitations. Namely, these scams are currently performed by humans (Hanoch and Wood, 2021). Our work shows that autonomous voices scams are possible with new advances in AI."}, {"title": "Conclusions", "content": "As we have shown, voice-enabled LLM agents can perform the actions necessary to perform common phone scams. These agents are highly capable, can react to changes in the environment, and retry based on faulty information from the victim. Our results highlight the need for future research in protecting victims from such scams."}, {"title": "Limitations, Ethical Considerations", "content": "A major limitation of our work is the focus on the actions and not the persuasion aspect of performing the scams. Namely, for an agent to perform a scam autonomously, it must first convince the victim that it is legitimate. Nonetheless, we believe our work highlights an important capabilities of newly available technology.\nOur work explores nefarious uses of AI technology. By outlining such nefarious uses, malicious actors could potentially take advantage of such technology. However, we believe it is important to study the capabilities of new technology, especially in its dual use capabilities.\nWe have elected not to make our agents public. This is for two reasons. First, following prior work on dual use technology, we believe it is beneficial not to release our code so that nefarious actors cannot leverage our work. Second, we believe that keeping our code private allows model providers (e.g., OpenAI) to build defenses to prevent such nefarious use."}]}