{"title": "IMPROVING UNCERTAINTY QUANTIFICATION IN LARGE LANGUAGE MODELS VIA SEMANTIC EMBEDDINGS", "authors": ["Yashvir S. Grewal", "Edwin V. Bonilla", "Thang D. Bui"], "abstract": "Accurately quantifying uncertainty in large language models (LLMs) is crucial for their reliable deployment, especially in high-stakes applications. Current state-of-the-art methods for measuring semantic uncertainty in LLMs rely on strict bidirectional entailment criteria between multiple generated responses and also depend on sequence likelihoods. While effective, these approaches often overestimate uncertainty due to their sensitivity to minor wording differences, additional correct information, and non-important words in the sequence. We propose a novel approach that leverages semantic embeddings to achieve smoother and more robust estimation of semantic uncertainty in LLMs. By capturing semantic similarities without depending on sequence likelihoods, our method inherently reduces any biases introduced by irrelevant words in the answers. Furthermore, we introduce an amortised version of our approach by explicitly modelling semantics as latent variables in a joint probabilistic model. This allows for uncertainty estimation in the embedding space with a single forward pass, significantly reducing computational overhead compared to existing multi-pass methods. Experiments across multiple question-answering datasets and frontier LLMs demonstrate that our embedding-based methods provide more accurate and nuanced uncertainty quantification than traditional approaches.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) have revolutionised natural language processing (see e.g. The Gemini Team,, 2023; Touvron et al., 2023; OpenAI, 2023; Brown et al., 2020), achieving state-of-the-art performance across a wide variety of tasks including question-answering. As these models are increasingly deployed in critical domains like healthcare (Singhal et al., 2023) and law (Weiser, 2023) ensuring their reliability and trustworthiness has become imperative. A significant challenge in this context is the phenomenon of \u201challucinations\u201d\u2014instances where LLMs generate fluent and coherent responses that are factually incorrect or misleading (Ji et al., 2023; Filippova, 2020; Maynez et al., 2020; Tian et al., 2024).\nUncertainty quantification (UQ) methods like Bayesian inference (Wilson & Izmailov, 2020), en-semble methods (Lakshminarayanan et al., 2017), and Monte Carlo dropout (Gal & Ghahramani, 2016) have been extensively studied in traditional neural networks to enhance model reliability by providing confidence measures in predictions. However, applying these traditional UQ methods to LLMs faces challenges due to the open-ended nature of free-form natural language generation (Kuhn et al., 2023). The core issue lies in the fundamental mismatch between traditional UQ approaches, which typically estimate uncertainty in output probabilities, and the semantics (meaning) space of language generation in LLMs.\nFor example, consider the following scenario of two responses generated for the same query: \"London is the biggest city in the UK\". \"The largest city in the UK is London\". In this scenario, the"}, {"title": "BACKGROUND", "content": "We first provide a concise summary of Semantic Entropy and detail its limitations which form the motivation of our proposed approaches."}, {"title": "SEMANTIC ENTROPY", "content": "Semantic Entropy (SE) is a measure of uncertainty in sequences generated by language models (Kuhn et al., 2023). The central idea is that if a language model is unsure about how to answer a specific question, it will produce responses that are different in wording and semantics across mul-tiple generations when the model is given the same input. SE groups semantically similar responses and calculates the entropy based on the variety of distinct meanings found in the output responses. Specifically, the key steps involved are: (i) sample M output sequences {$1,...,sM} from the lan-guage model's predictive distribution p(s|x) given an input x; (ii) cluster the sampled sequences into K semantic equivalence classes C = {c1, ..., cK } using a bidirectional entailment algorithm. Two sequences si and s; are considered semantically equivalent if and only if a natural language inference model classifies their mutual relationship as entailment in both directions; (iii) estimate the probability of each semantic cluster p(ck|x) = \u2211sec, p(s|x); and (iv) compute the entropy over\nsemantic clusters: SE(x) = \u03a3\u03ba=1P(Ckx) log p(ck|x)."}, {"title": "BIDIRECTIONAL ENTAILMENT AND ITS LIMITATIONS", "content": "We focus on step (ii), where bidirectional entailment is used to identify distinct semantic clusters among the M responses. This strict criterion, however, can be overly sensitive to minor variations in wording, additional correct information, or non-essential words. This issue is illustrated by several examples listed in Table 1.\nExample 1: Generality Mismatch in Responses Both responses correctly state that mitochondria produce energy. However, bidirectional entailment fails because the first response uses \"produce"}, {"title": "SEMANTIC EMBEDDING UNCERTAINTY", "content": "To overcome the limitations of bidirectional entailment in measuring semantic uncertainty, we pro-pose semantic embedding uncertainty (SEU), a novel approach based on the average pairwise co-sine similarity of the generated responses' embeddings. This method leverages continuous semantic representations to capture nuanced meanings more precisely, offering a robust measure of semantic uncertainty in language model outputs.\nSimilar to Semantic Entropy, given an input x, we generate M output sequences {S1, S2, ..., SM}\nfrom the language model's predictive distribution p(sx). We then obtain vector embeddings\n{\u04351, \u04352,..., \u0435\u043c} for each sequence using a pretrained embedding model $(s), such as a transformer-based sentence encoder (Reimers & Gurevych, 2019). The semantic uncertainty is\nquantified by computing the negative average pairwise cosine similarity between the embeddings:\nSEU(x) = 1 - \\frac{2}{M(M-1)} \\sum_{i=1}^{M-1} \\sum_{j=i+1}^{M} cos (e_i, e_j), (1)\nwhere cos (ei, ej) is the cosine similarity between embeddings e\u00bf and ej, cos (ei, ej) = \\frac{e_ie_j}{||e_i|| ||e_j||}' ||e|| denotes the Euclidean norm of vector e, and e\u00bf \u00b7 ej represents the dot product between vectors\ne\u017c and ej.\nThe proposed approach relies on high-quality embedding models to map semantically similar sen-tences to nearby points in the embedding space (e.g. Mikolov et al., 2013; Pennington et al., 2014;\nReimers & Gurevych, 2019). Intuitively, cosine similarity quantifies the angle between vectors in the\nhigh-dimensional embedding space, serving as a measure of their semantic alignment. By aggregat-ing pairwise similarities across all generated responses, we capture the overall semantic coherence\nof the model's outputs. If the language model is certain about the response to input x, the generated\nresponses will be semantically similar, leading to high cosine similarity scores and a low seman-\ntic uncertainty SEU(x). Conversely, if the model is uncertain, the responses will be more diverse\nsemantically, resulting in lower cosine similarity scores and a higher SEU(x).\nThe proposed approach offers two key advantages over bidirectional entailment. First, unlike the bi-nary outcome of bidirectional entailment\u2014which rigidly classifies responses as either semantically\nequivalent or not\u2014cosine similarity provides a continuous metric. This allows for a nuanced assess-\nment of semantic closeness between responses. While bidirectional entailment may fail to recognise\nnear-equivalent meanings due to minor differences (thereby assigning a value of zero similarity),\ncosine similarity captures the degree of similarity between responses. This continuous spectrum\nmore accurately reflects the gradations in human language understanding. Second, as shown above,\nbidirectional entailment is highly sensitive to syntactic variations, paraphrasing, and the inclusion of\nadditional relevant information, often resulting in false negatives when determining semantic equiv-\nalence. In contrast, cosine similarity focuses on the underlying semantic content rather than exact\nentailment. This makes it less sensitive to linguistic variability, such as differences in syntax or\nphrasing."}, {"title": "EMPIRICAL EVALUATION OF SEU", "content": "In this section, we empirically evaluate our proposed SEU method against existing uncertainty esti-mation techniques. Our goal is to demonstrate that SEU provides a more accurate and robust mea-sure of semantic uncertainty in language model outputs, particularly in the context of open-domain question answering."}, {"title": "EXPERIMENTAL SETUP", "content": "Models To align with modern practices of using instruction fine-tuned LLMs for chat purposes, we employ Llama-3.1-8B-Instruct (Dubey et al., 2024), Mistral-7B-Instruct (Jiang et al., 2023), and Phi-3.5-mini-instruct (Abdin et al., 2024) as our base models. For each model-dataset combination, we generate 5 responses per question (that is M = 5) at a temperature of 0.5. This temperature was recommended as the optimal temperature for SE in previous work (Kuhn et al., 2023). Additionally, we prompt Llama and Phi models with \"Answer the following question as briefly as possible\", and"}, {"title": "RESULTS", "content": "Our empirical evaluation demonstrates the effectiveness of the proposed Semantic Embedding Un-certainty (SEU) method across different models and datasets. We present our findings in two parts: a comparative analysis of uncertainty estimation methods and an in-depth examination of the trade-off between false positive rate (FPR) and true positive rate (TPR)."}, {"title": "COMPARATIVE ANALYSIS OF UNCERTAINTY ESTIMATION METHODS", "content": "Figure 1 presents the AUROC scores for different uncertainty estimation methods across three mod-els (Llama-3.1-8B-Instruct, Phi-3.5-Instruct, and Mistral-7B-Instruct) and three datasets (TriviaQA, NQ Open, and Flan QA). We note the proposed SEU method consistently outperforms or matches the performance of other uncertainty estimation methods across all model-dataset combinations. Specifically, while the relative performance of methods varies slightly across models, SEU main-tains its advantage, suggesting robustness to model architecture differences. The performance pat-terns differ across datasets, with all methods generally performing better on TriviaQA compared to NQ Open and Flan QA. Crucially, SEU consistently outperforms Semantic Entropy, supporting our hypothesis that the latter may overestimate uncertainty due to its sensitivity to minor linguistic variations."}, {"title": "ANALYSIS OF FALSE POSITIVE RATE AND TRUE POSITIVE RATE TRADE-OFF", "content": "To further investigate the performance difference between SEU and Semantic Entropy, we analyse the False Positive Rate (FPR) and True Positive Rate (TPR) at the optimal Youden's J statistic point for the NQ Open dataset. A \"positive\u201d case refers to an instance where the model's response is correct. Table 2 presents these results. Notably, SEU consistently achieves a higher TPR compared to Semantic Entropy across all models. This indicates that SEU is more effective at identifying the"}, {"title": "AMORTISED SEMANTIC EMBEDDING UNCERTAINTY", "content": "While our proposed Semantic Embedding Uncertainty (SEU) method demonstrates superior perfor-mance in uncertainty estimation across various models and datasets, it shares a significant limitation with Semantic Entropy: computational inefficiency. Both SEU and Semantic Entropy require multi-ple forward passes through the language model to generate a set of responses for each input, which can be prohibitively expensive, especially for large language models in production environments. To this end, we present amortised SEU (ASEU) to tackle the challenge of estimating semantic un-certainty in a single forward pass. The goal is to represent the semantics of a sequence as latent variables and spend a small effort to finetune and obtain an amortised approximate posterior over them to bypass the need for an external paragraph or sentence embedding model at test time."}, {"title": "LATENT SEMANTIC MODEL", "content": "Suppose we have a training set of N sequences and xn = (Xn,1, Xn,2, ..., Xn,T) is the n-th sequence that has T tokens. We assume there is a latent vector zn \u2208 RD that captures the semantic of the n-th sequence and that the embeddings en \u2208 RD of this sequence can be computed using an external, pre-trained embedding model. The joint distribution over the latent semantic and observed embeddings is defined as follows,\np({en, zn}=1/w) = \\prod_{n=1}^{N} p(z_n)p(e_n|z_n, w),"}, {"title": "APPROXIMATE INFERENCE", "content": "Readers familiar with latent variable modelling might have noted similarity between the model above and Gaussian latent variable models in Kingma & Welling (2014); Rezende et al. (2014). The most natural next step for inference would be to impose an approximate posterior over zn, q(znen, 4), that mirrors that of the exact posterior, p(zn|en, w). While this is arguably the most accurate approach, computing z for a new sequence at test time requires access to the embedding e, which we seek to avoid. To sidestep this, we posit the variational Gaussian distribution over z, q(zn|xn, \u03c8, 0) = N(zn; \u03bc\u03b7, \u03a3n), where \u00b5\u03b7 and En are outputs of a fully-connected neural network, parameterised by 4. This network takes as input the representation of xn provided by the language model, that is parameterised by 6. This parameterisation allows us to obtain a distribution over z using the same backbone as used for modelling x. Equipped with the model and variational distribu-tion specifications, we now wish to minimise the KL divergence between the approximate posterior and the true posterior, KL[q(zn|xn, \u03c8, 0) || p(zn|en, w)], or equivalently, minimising the negative lower bound to the log marginal likelihood log p({en}=1|w), L(\u03b8, \u03c9, \u03c8) = \u2211n Ln(\u03b8, \u03c9, \u03c8), where\nLn(0,\u03c9, \u03c8) = J\\prod_{Zn} q(znen, \u03c8, 0) [log q(zn|en, \u03c8, \u03b8) \u2013 log p(en, zn|W)]\n= KL[q(znen, \u03c8, \u03b8) || p(Zn)]J\\prod_{Zn} q(Zn Xn, \u03c8, \u03b8) log p(en zn, w).\nThe above objective is intuitive: we want to fine-tune the language model and optimise the model and variational parameters such that the language model's representation helps reconstruct the em-bedding of the full sequence. Additionally, 0 can be kept fixed if a pre-trained language model is already available or simultaneously fine-tuned using the negative log-likelihood of x as in conven-tional autoregressive language modelling."}, {"title": "UNCERTAINTY ESTIMATION AT TEST TIME", "content": "As the variational approximation is trained to approximately imitate the sequence embedding, it can be leveraged to estimate the semantic uncertainty similarly to the SEU method proposed earlier. At each step t during response generation, we draw K samples {zt,1, ..., zt,K} from the approximate posterior q(Ztxt, 4, 0), where xt are the prompt and the tokens generated so far. We then compute the average pairwise cosine similarity between these samples:\nSt = \\frac{2}{K(K-1)} \\sum_{j=1}^{K-1} \\sum_{k=j+1}^{K} COS(Zt_j, Zt_k)\nwhere cos(Zt,j, Zt,k) denotes the cosine similarity between samples zt,j and zt,k. After gen-erating the complete response of length T, we calculate the raw ASEU score: ASEUraw\n1\n= median{S1,..., ST}. The intuition behind this approach is that if the LLM is uncertain about the latent semantics of future tokens, the sampled embeddings at each step will have lower aver-age similarity compared to cases where the LLM is more certain. Taking the median across all steps yields a robust measure of the overall semantic uncertainty for the entire response. To account for the impact of response length on uncertainty, we apply length normalization, defining our final ASEU score, with higher values indicating greater uncertainty and lower values indicating greater certainty. This normalization step is crucial as it mitigates potential bias towards longer responses, which might accumulate more uncertainty simply due to their length. We also found the proposed approach is more robust than using the entropy of the variational approximation."}, {"title": "EMPIRICAL EVALUATION OF AMORTIZED SEU", "content": "In this section, we empirically evaluate our proposed amortized Semantic Embedding Uncertainty (ASEU) method. Unlike the previous multi-pass setting, we now focus on estimating uncertainty in a single forward pass, which is crucial for practical applications in production environments."}, {"title": "EXPERIMENTAL SETUP", "content": "Models: We use the same three models as in the previous experiments: Llama-3.1-8B-Instruct, Phi-3.5-Instruct, and Mistral-7B-Instruct. However, for this evaluation, we fine-tune these models to optimize the variational objective presented in section 5.2.\nFine-tuning: To learn the approximate posterior distribution q(z|x, 4, 0) and parameters \u03b8 and w, we fine-tune the LLMs on the TriviaQA dataset. We chose TriviaQA for fine-tuning due to its size and diverse coverage of question-answering tasks and its focus on short answers, which aligns with the paper's emphasis so far. This fine-tuning process allows the models to leverage the underlying language model to estimate semantic uncertainty in a single forward pass.\nBaselines: In the single forward pass setting, we compare our ASEU method against the length-normalized predictive entropy of a single forward pass response. This baseline is chosen as it is the most relevant uncertainty estimation method that can be computed in a single pass."}, {"title": "RESULTS", "content": "Figure 2 presents the AUROC scores for our ASEU method and the length-normalized predictive entropy baseline across the three models (Llama-3.1-8B-Instruct, Phi-3.5-Instruct, and Mistral-7B-Instruct) and two datasets (NQ Open and Flan QA). We also compare the armotised uncertainty esti-mates with SEU and other methods that require multiple generations in figure 3. We note that ASEU consistently outperforms the length-normalized predictive entropy baseline across all model-dataset combinations, suggesting it captures more meaningful uncertainty information. While ASEU gener-ally doesn't match the performance of multi-pass SEU method, it achieves comparable results for the Mistral model. The performance gap between ASEU and multi-pass SEU varies across models and datasets, but the computational efficiency gained through single-pass estimation makes ASEU more suitable for real-world applications, especially in production environments where multiple forward passes are infeasible."}, {"title": "ANALYSIS OF LEARNT LATENT EMBEDDINGS", "content": "To demonstrate that the latent embeddings of our amortized model carry meaningful semantic in-formation, we examined the cosine similarities between the means of the variational distributions given semantically related queries and presented the results in Table 3. The perfect similarity be-tween queries about England's capital and the UK's biggest city reflects their close relationship"}, {"title": "RELATED WORKS", "content": "Hallucinations in LLMs: The challenge of hallucination detection in LLMs has become increas-ingly important as these models are deployed in real-world applications. Various benchmarks have been developed to evaluate this phenomenon, including TruthfulQA (Lin et al., 2021), Factuali-tyPrompt (Lee et al., 2022), FActScore (Min et al., 2023), HaluEval (Li et al., 2023a), and FACTOR (Muhlgay et al., 2023). Early research on hallucinations primarily focused on issues in summariza-tion tasks, where models would generate content unfaithful to the source text (Maynez et al., 2020; Durmus et al., 2020; Wang et al., 2020). This work laid the foundation for understanding the broader challenge of hallucinations in LLMs.\nUncertainty Estimation Approaches: A significant body of work has explored methods to esti-mate uncertainty in LLM outputs. Many of these approaches rely on comparing multiple model generations or outputs by leveraging additional LLMs or by using the same LLM (Duan et al., 2023; Chen & Mueller, 2023; Manakul et al., 2023; M\u00fcndler et al., 2023). The field has seen a variety of innovative techniques, including those proposed by Kadavath et al. (2022), Mitchell et al. (2022), and Xu et al. (2022), which leverage different aspects of model behaviour to gauge uncertainty.\nKnowledge Integration Methods: Another line of research focuses on integrating external knowl-edge to verify and improve the factual accuracy of LLM outputs. The RARR framework (Gao et al., 2023) uses search engines for knowledge retrieval and correction. Similarly, the Verify-and-Edit approach (Zhao et al., 2023) leverages external information sources. However, these methods face challenges in resolving conflicts between model knowledge and retrieved information, as high-lighted by Shi et al. (2023). Additional work in this area includes efforts by Dziri et al. (2021), Peng et al. (2023), and Li et al. (2023c), who explore various techniques for grounding LLM outputs in external knowledge sources.\nGeneration and Fine-tuning Strategies: Researchers have also developed strategies to reduce hal-lucinations during the generation process or through model fine-tuning. Lee et al. (2022) introduced factual-nucleus sampling to balance output diversity and factual accuracy. Reinforcement learning from human feedback (RLHF) has been employed by Ouyang et al. (2022) and Touvron et al. (2023)"}, {"title": "CONCLUSION", "content": "This work introduces two novel approaches for uncertainty quantification in large language models: Semantic Embedding Uncertainty (SEU) and its amortized version (ASEU). Our methods leverage semantic embeddings to achieve more robust and nuanced estimations of semantic uncertainty com-pared to existing techniques. While SEU provides improved accuracy over traditional approaches, ASEU offers a significant computational advantage by enabling uncertainty estimation in a single forward pass. This efficiency is particularly crucial for real-time applications and when dealing with larger language models where multiple forward passes can be prohibitively expensive. Empir-ical evaluations across multiple datasets and frontier LLMs demonstrate that our embedding-based methods provide more accurate uncertainty quantification than traditional approaches, particularly in scenarios where minor linguistic variations or additional correct information might lead to overes-timation of uncertainty. Furthermore, ASEU's ability to maintain comparable performance to SEU while drastically reducing computational overhead represents a substantial step towards making un-certainty quantification more practical and accessible in production environments.\nWhile our results are promising, several limitations of this work should be acknowledged. Our experimental setup primarily focused on short-answer questions and responses, which may not fully capture the complexity and diversity of real-world LLM applications that often involve longer, more nuanced responses. The use of Rouge-L score as an automatic evaluation metric, while suitable for short answers, may not be appropriate for assessing longer or more complex responses. This limitation restricts the generalisability of our findings to broader LLM use cases. Additionally, while we used multiple datasets, they were all in the domain of question-answering. The effectiveness of our methods on other types of language tasks, such as code generation, remains to be explored. Our study also focused on a specific set of commonly used open source LLMs, and the performance and behaviour of our methods on larger models were not investigated."}, {"title": "APPENDIX", "content": "A.1 ROC CURVES\nWe provide the full ROC curves of the methods considered in the main text, across various models and evaluation datasets.\nA.2 MODEL PROMPTS AND EXAMPLE RESPONSES\nThis section presents the prompts used for each model and provides examples of their responses to demonstrate the brevity of the generated answers."}, {"title": "PROMPTS", "content": "For the initial experiments, we used the following prompts:\n\u2022 Llama and Phi models: \u201cAnswer the following question as briefly as possible\"\n\u2022 Mistral model: \u201cAnswer the following question briefly using a few words\"\nAfter fine-tuning, we used the same prompts except for the Llama model, where we changed the prompt to: \"Give a short reply to the following question\"."}, {"title": "EXAMPLE RESPONSES", "content": "Tables 4 and 5 include two examples from the NQ Open dataset to demonstrate that the models indeed generate short answers, adhering to the instruction for brevity."}]}