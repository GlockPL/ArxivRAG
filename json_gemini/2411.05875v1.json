{"title": "Towards Improved Preference Optimization Pipeline: from Data Generation to Budget-Controlled Regularization", "authors": ["Zhuotong Chen", "Fang Liu", "Jennifer Zhu", "Wanyu Du", "Yanjun Qi"], "abstract": "Direct Preference Optimization (DPO) and its variants have become the de facto standards for aligning large language models (LLMs) with human preferences or specific goals. However, DPO requires high-quality preference data and suffers from unstable preference optimization. In this work, we aim to improve the preference optimization pipeline by taking a closer look at preference data generation and training regularization techniques. For preference data generation, we demonstrate that existing scoring-based reward models produce unsatisfactory preference data and perform poorly on out-of-distribution tasks. This significantly impacts the LLM alignment performance when using these data for preference tuning. To ensure high-quality preference data generation, we propose an iterative pairwise ranking mechanism that derives preference ranking of completions using pairwise comparison signals. For training regularization, we observe that preference optimization tends to achieve better convergence when the LLM predicted likelihood of preferred samples gets slightly reduced. However, the widely used supervised next-word prediction regularization strictly prevents any likelihood reduction of preferred samples. This observation motivates our design of a budget-controlled regularization formulation. Empirically we show that combining the two designs leads to aligned models that surpass existing SOTA across two popular benchmarks.", "sections": [{"title": "1 Introduction", "content": "Recently, Direct Preference Optimization (DPO) (Rafailov et al., 2024) and its variants (Meng et al., 2024; Azar et al., 2024; Ethayarajh et al., 2024; Liu et al., 2024; Pal et al., 2024; Xu et al., 2024) have gained popularity over traditional reinforcement learning from human feedback (RLHF) (Ziegler et al., 2019), which involves training a reward model followed by reinforcement learning. DPO-based methods bypass the need for a reward model in optimization by directly optimizing the target model using preference data, leading to simpler and more efficient training.\n\nThe pipeline of DPO (and its variants) consists of two key stages: (1) collecting preference data by scoring various outputs generated by the target LLM model, and (2) performing direct optimization using the preference data.\n\nThe first stage of constructing preference data involves two steps: (1) the target model generates multiple completions for each input prompt; (2) then a reward model selects preferred and dispreferred completions from these candidates for each prompt (Xiong et al., 2024; Meng et al., 2024). Existing open-sourced reward models are mostly based on a classification architecture by modifying the last layer of a LLM (Liu and Zeng, 2024; Wang et al., 2024b,a). This scoring-based approach for evaluating the quality of a prompt-completion pair introduces considerable noise (Cui et al., 2023; Ganguli et al., 2022; Guo et al., 2024), and the issue becomes even more when the downstream task is out-of-distribution compared to the training data used to construct the reward model.\n\nAfter constructing high-quality preference data, standard preference optimization algorithms compute the relative probability of selecting one completion over another by using pairs of preferred and dispreferred completions (Rafailov et al., 2024; Meng et al., 2024; Azar et al., 2024). Optimizing towards this relative objective can potentially lead to a reduction of target model's predicted likelihood of the preferred completions, as long as the relative probability between the preferred and dispreferred completions increases with the preference optimization. This may cause training instability issue (Pal et al., 2024; Feng et al., 2024; Liu et al., 2024). To address the challenge, several regularization techniques have been proposed to utilize supervised next-word prediction of the preferred examples. While these techniques effectively improve"}, {"title": "2 Preference Dataset Generation", "content": "The quality of preference data is crucial to the performance of any preference optimization algorithm. This section first outlines existing preference data generation methods (Sec. 2.1), then introduces an iterative pairwise ranking approach (Sec. 2.2)."}, {"title": "2.1 Existing Data Generation Methods", "content": "A preference dataset consists of N tuples {(x^{i}, y_w^{i}, y_l^{i})}_{i=1}^{N}, where x^{i}, y_w^{i} and y_l^{i} represent input prompt, preferred and dispreferred completions, respectively. In this work, we assume that input prompts are provided. In an online setting, the target LLM generates multiple completions for each prompt, denoted as y^{i,1}, y^{i,2}, ..., y^{i,M}. Then preference data are constructed by selecting preferred and dispreferred completions from these candidates (Xiong et al., 2024).\n\nLet r^*(x, y) denote the ground-truth reward model that provides a reward score on a prompt-completion pair (x, y). The objective function for identifying the most preferred completion y_w^{i} can be formulated as follows,\ny_w^{i} = arg \\max_{y \\in {y^{i,m}}_{M=1}} r^*(x^{i}, y). (1)\n\nThe same methodology can be applied to search for y_l^{i} by considering the arg min over {y^{i,m}}_{M=1}. Typically, Eq. (1) is solved using an estimated reward model r(x, y) (Pal et al., 2024; Feng et al., 2024; Liu et al., 2024). Then preferred and dispreferred completions are selected based on these estimated reward scores. While these reward models demonstrate high accuracy on tasks closely aligned with their training datasets (Lambert et al., 2024), they generalize poorly on out-of-distribution tasks and require adaptation to new domains (Bai et al., 2022; Tang et al., 2024)."}, {"title": "2.2 Proposed: Iterative Pairwise Ranking via Dueling Bandits", "content": "We propose an Iterative Pairwise Ranking (IPR) approach motivated by the dueling bandits framework (Sui et al., 2018) to address Eq. (1). This"}, {"title": "3 Regularizing Preference Optimization", "content": "In this section, we first analyze the failure mode associated with preference optimization algorithms (Sec. 3.1). We then discuss regularization techniques aimed at improving training stability (Sec. 3.2). Lastly, we introduce a budget-controlled regularization (Sec. 3.3) that balances between training stability and model alignment performance."}, {"title": "3.1 Failure Mode of Preference Optimization", "content": "Given a pairwise preference dataset, DPO (and its variants) optimizes the LLM to increase the gap between the probabilities of generating preferred and dispreferred completions, subject to a KL-divergence constraint that prevents large deviation of the optimized model from the initial base model, this is formulated as a maximum likelihood optimization of the target distribution \\pi_{\\theta}(\\cdot|x), \n\nL_{DPO}(\\pi_{\\theta}, \\pi_{ref}) = -E_{(x, y_w, y_l) \\sim D} [log \\sigma(r(x, y_w) - r(x, y_l))], where r(x, y) = \\beta log (\\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)}); (2)\n\nwhere the reward function r(x, y) is parameterized by the ratio between target and reference models scaled by a hyper-parameter \\beta. The DPO loss is a function of the difference in the log-ratios, which means that we can achieve a low loss value even if the reward of preferred completion r(x, y_w) is lowered, as long as the reward of dispreferred completion r(x, y_l) is sufficiently lower. This implies that the log-likelihood of the preferred completions can be reduced even below the original log-likelihood from the reference model.\n\nWe empirically showcase the failure mode in preference optimization. Specifically, we apply DPO (Rafailov et al., 2024) to train the Llama-3.1-8B instruct model using the UltraFeedback Binarized dataset (details in Sec. 5.1). As shown in Fig. 2, while DPO effectively improves both the reward margin and reward accuracy, indicating that the model better learns the underlying preference data, there is a significant reduction in the log-likelihood of predicting preferred completions, leading to the failure mode. Extensive numerical evidences on the failure mode of DPO (and its variants) across different settings can be found in Appendix B.2."}, {"title": "3.2 Next-Word Prediction Regularization", "content": "Regularization for preference optimization has shown its effectiveness to prevent the failure mode. These regularization techniques generally focus on a supervised next-word prediction objective with a goal of increasing the log-likelihood of predicting the preferred completions during training. One notable algorithm is named DPO-Positive (DPOP) (Pal et al., 2024), \n\nL_{DPOP}(\\pi_{\\theta}, \\pi_{ref}) = -E_{(x, y_w, y_l) \\sim D} [log \\sigma(r(x, y_w) - r(x, y_l) - \\lambda \\cdot max \\big(0, log (\\frac{\\pi_{ref}(y_w|x)}{\\pi_{\\theta}(y_w|x)})\\big))], (3)\n\nwhere \\lambda is a hyper-parameter to balance between the reward difference of DPO objective and regu-"}, {"title": "3.3 Budget Controlled Regularization", "content": "Here we propose a Budget Controlled Regularization (BCR) that balances between training stability and model alignment performance. First, similar to Contrastive Preference Optimization (Xu et al., 2024), the proposed regularization acts as the supervised next-word prediction objective outside of the log-sigmoid function, which prevents the failure mode of DPO more effectively than DPOP by avoiding the overfitting issue. Moreover, the analyses in Fig. 2 reveal that the reduction in the log-likelihood of predicting preferred completions is necessary for the model to achieve a high reward margin and accuracy. Specifically, as the regularization effect of DPOP strengthens (with an increase in \\lambda), the resulting models underperform compared to those trained with DPO. Extensive empirical validations can be found in Sec. B.2.\n\nFig. 3 illustrates the trade-off between the average sum log-likelihood of preferred completions and model performance on the Alpaca-Eval 2.0 dataset (Dubois et al., 2024). Each data point represents the evaluation result of a model checkpoint trained on a particular set of hyperparameters. The sum log-likelihood is averaged across the samples in dev set, while model performance is measured as the win rate against a golden reference completion. As training progresses, the sum log-likelihood decreases, consistent with Fig 2(c). The model performance initially improves but later declines due to overfitting to the preference dataset. Thus, the regularization term should allow a certain reduction of the log-likelihood on preferred completion (defined as budget) for the decrease in sum log-likelihood but penalize the decrease beyond the budget. The training objective with the proposed budget controlled regularization is as follows: \n\nL_{DPOBCR}(\\pi_{\\theta}, \\pi_{ref}) = L_{DPO}(\\pi_{\\theta}, \\pi_{ref}) + \\mathbb{E}_{(x, y_w) \\sim D} [max (0, log(\\frac{\\pi_{ref}(y_w|x)}{\\pi_{\\theta}(y_w|x)}) - \\delta)], (4)\n\nwhere is an non-negative hyper-parameter. Specifically, when \\delta = 0, DPOBCR strictly penalizes any reduction of likelihood of predicting the preferred completion. A small positive \\delta allows the probability of predicting preferred completions to be slightly reduced, while maximizing the reward margin via L_{DPO}. Such regularization term enables the optimization process to achieve best trade-offs between the sum log-likelihood and policy performance."}, {"title": "4 Related Works", "content": "In this section, we outline preference optimization algorithms and existing regularization techniques to improve training stability. Extensive discussion is provided in Appendix D.\n\nDPO and Its Variants. Since the introduction of DPO (Rafailov et al., 2024), several algorithms have emerged to further refine preference optimization. SimPO (Simple Preference Optimization) introduces length regularization on the log-probabilities of both preferred and dispreferred completions, eliminating the need for a reference model (Meng et al., 2024). IPO (Identity Preference Optimization) addresses the shortcomings of BT preference modeling in cases where preference data are highly deterministic, when the preferred completion is almost always better to the dispreferred one. In such cases, the KL-divergence regularization becomes ineffective. IPO resolves this by replacing the logistic loss with a squared loss and incorporating a margin, providing a more theoretically sound approach (Azar et al., 2024). Other notable algorithms include RPO (Regularized preference optimization) that emphasizes the role of length regularization (Park et al., 2024), and iterative preference learning that iteratively refine the target LLM based on preference data (Xiong et al., 2024; Kim et al., 2024a)."}, {"title": "Supervised Next-Word Prediction Regularization Improves Training Stability.", "content": "To improve the training stability of preference optimization, various forms of supervised next-word prediction regularization have been proposed to improve training stability. SLIC (sequence likelihood calibration) adds a term to maximize log-likelihoods on certain reference completions (Zhao et al., 2023), CPO (Contrastive Preference Optimization) applies a behavior cloning regularizer (Hejna et al.; Xu et al., 2024). Additionally, DPOP introduces a hinge loss on the log-ratio between the reference and target models (Pal et al., 2024). Despite the improvements in training stability, our analysis indicates that regularized preference optimization often results in worse performance compared to non-regularized approaches."}, {"title": "5 Experimental Results", "content": "In this section, we showcase the improved model alignment performance achieved through the proposed designs (Sec. 5.2). Additionally, we provide a comprehensive ablation study to assess the quality of preference data generated by IPR and the effectiveness of BCR(Sec. 5.3)."}, {"title": "5.1 Experimental Setup", "content": "We discuss our design choices regarding base models, training details and evaluation metrics. Additional details are provided in Appendix A."}, {"title": "Base models.", "content": "We conduct all experiments using both Llama-3.1-8B instruct and Mistral-Instruct-7B. Both models have undergone extensive instruction-tuning."}, {"title": "Preference Data Construction.", "content": "To mitigate the distribution shift between base models and the preference optimization process, we generate the preference dataset using the base models (Tang et al., 2024; Meng et al., 2024; Xiong et al., 2024). This makes the training process closer to an on-policy setting. Specifically, we use prompts from the UltraFeedback dataset and regenerate the preferred and dispreferred completions with the base models. For each prompt, as a default setting, we generate 5 completions using the base model with a sampling temperature of 0.8. For re-ward model-based method, we consider ArmoRM-Llama3-8B-v0.1 (Wang et al., 2024b) to score all completions and select the highest-scoring one as Yw and the lowest-scoring one as y\u0131. In addition, we"}, {"title": "5.2 Main Results Summary", "content": "summarizes the alignment performance of\nall trained models.\nPreference optimization with IPR significantly outperforms existing methods. By comparing models trained using the reward model (Armo Llama3), using IPR method to construct preference data significantly improves model alignment performance across different preference optimization algorithms. In the Alpaca-Eval 2.0 evaluation, the Llama-3.1 models trained with DPO and SimPO show substantial performance gains, with winrate improvements of 15% and 20%, respectively, when trained with IPR preference data. Notably, models trained with regularized objectives like CPO exhibit an even greater winrate increase of 27%. This performance improvement can be seen for preference tuned Mistral-Instruct (7B). Furthermore, the effectiveness of the IPR method is influenced by the capability of the LLM used as the preference judge. Models trained with IPR data constructed\nTable 2"}, {"title": "BCR matches state-of-the-art performance with less optimization budget.", "content": "Recall in Sec. 3.1, as both reward margin and reward accuracy increase, the log-likelihood of predicting preferred completions decreases, indicating the failure mode of preference optimization. Here we define the optimization budget as the log-likelihood of predicting preferred samples. As shown, with models trained using IPR, while adding BCR for preference optimization does not significantly further improve model alignment performance, it allows the trained model to achieve the same level of performance using much less optimization budget. Specifically, for Llama-3.1-Instruct (8B), SimPOBCR_outper-"}, {"title": "5.3 Ablation Study", "content": "IPR results in high quality preference data. We perform a preference data quality analysis using three public reward models listed at top of the RewardBench (Lambert et al., 2024): Reward Gemma, and"}, {"title": "6 Conclusion", "content": "This work presents a comprehensive study of preference optimization algorithms, with a focus on improving preference data generation and regularization techniques. Our empirical results show that preference optimization can be more effective when the likelihood of both preferred and dispreferred completions is managed carefully, allowing for a more balanced optimization. By combining IPR for data generation and BCR for preference optimization, we demonstrate notable improvements. There has been evidence that online alignment algorithms generally outperform offline methods, we aim to extend the current pipeline to an online setting where the completions are generated during training by the target model. We believe that the proposed designs can benefit the online setting with higher preference data quality and training stability."}, {"title": "Ethical Considerations", "content": "While BCR and IPR build up an effective preference optimization workflow, aligning LLM with human preferences raises certain ethical concerns. One concern is that human preferences are complex, nuanced, and often contradictory. Attempting to codify human values into an AI system may oversimplify complex issues, for instance, it is difficult to decide whose preferences should be optimized for - the developers', users', or society's as a whole. Optimizing for any one group's preferences could lead to issues like bias and exclusion of minority viewpoints."}, {"title": "Limitations", "content": "The proposed IPR strategy for constructing preference data requires substantial computing resources. This is because it involves running multiple iterations of inferences with a large-scale LLM to select the preferred completion, and this process is repeated for all training data."}, {"title": "A Experimental Setup", "content": ""}, {"title": "A.1 Training Details", "content": "Training hyperparameters: Our findings highlight the critical role of hyperparameter tuning in achieving optimal performance for preference optimization methods. However, prior research may have underestimated its significance, potentially resulting in suboptimal baseline results. To ensure a fair comparison, we perform comprehensive hyperparameter tuning for all methods evaluated in our experiments. Table 5 summarizes all hyperparameters used for all preference optimization algorithms.\n\nFor general training hyperparameters, we fix a batch size of 128 for all training tasks, and a cosine learning rate schedule with 10% warmup steps for 1 epoch. Preference optimization algorithms are extremely sensitive to learning rates, especially for non-regularized implementations, such as DPO, IPO and SimPO. Therefore, we search for the optimal learning rate from 1e-6 to 8e-6 with an increment of 1e-6."}, {"title": "A.2 Evaluation Details", "content": "We primarily assess our models using two of the most popular open-ended instruction-following benchmarks: AlpacaEval 2.0 and Arena-Hard . AlpacaEval 2.0 consists of 805 questions from 5 datasets, Arena-Hard incorporats 500 well-defined technical problem-solving queries."}, {"title": "Model judge:", "content": "Due to the limited access to GPT-4, we consider Mixtral-8x7B-Instruct as the model judge. This is a powerful evaluator LLM that closely mirrors human and GPT-4 judgements. The following provides the input prompt used for model judge to compare two candidates."}, {"title": "B Extensive Experimental Results", "content": "In this section, we provide extensive numerical experimental results."}, {"title": "B.1 Preference Data Construction via IPR", "content": "We create a preference dataset by using completions generated from the base model, which helps reduce the gap between the base model's outputs and the preference optimization process. For each input prompt, we generate five candidate completions and use our proposed IPR method to select the most preferred one. Figure 5 shows the statistics for IPR(Llama70B) (using Llama-3.1-70B as the preference judge).\n\nEach comparison can result in one of three outcomes: Tie, Candidate, or Baseline. Since all candidate completions come from the same distribution (the base model), a large number of Ties occur in each iteration. In cases of a Tie, we always select the baseline completion as the winner. If all four iterations result in Ties, we choose the first candidate completion. This preferred completion is still of high quality because it is at least as good as the other candidates."}, {"title": "B.2 Preference Optimization Regularization", "content": "DPO versus DPOP results: Here we provide extensive results to showcase the failure mode in preference optimization. Figure 6 shows the training progresses for DPO and DPOP. In Figure 6 (a1), (b1) and (c1), as both reward margin and reward accuracy increases, DPO leads to a reduction on the log-likelihood of predicting preferred completions. When the supervised next-word prediction regularization is added by setting x = 0.5 in DPOP, in Figure 6 (a2), (b2) and (c2), the issue of reducing log-likelihood of predicting preferred completion is alleviated, however, the reward accuracy is lower compared to DPO in Figure 6 (a2). When the regularization effect is stronger with a larger x = 5, the log-likelihood of predicting preferred completion is non-decreasing through the whole training progress. However, the reward accuracy is considerably lower compared to DPO in Figure 6 (b1).\n\nSimPO Versus CPO results: Figure 7 illustrates the training progress of SimPO and CPO"}, {"title": "C Efficient Preference Data Generation", "content": "An early stopping criterion. Given consideration of computational efficiency, the goal is to explore the preferred completion while minimizing the number of comparison signals, which can be computationally expensive (such as using an LLm judge). The threshold-based stopping criterion aims to stop exploration when there is sufficient evidence that one completion is preferred over all others . We define this criterion using prior estimations for all possible pairwise comparisons. Recall that each comparison signal has 3 possible outcomes, baseline wins, candidate wins and a tie. In the exhaustive search process, we select the outcome from the first non-tie comparison as the overall preferred completion.\n\nThis approach is motivated by the online preference optimization setting, where candidate completions are generated by sampling from the same distribution in the target LLM and there is a high probability that many comparisons will result in ties. Therefore, by selecting the first non-tie outcome, the process can be stopped early, avoiding unnecessary comparisons."}, {"title": "D Related Works", "content": "In this section, we first outline DPO and its variants, then we discuss the training instability issue associated to these preference optimization algorithms and existing solutions.\n\nDPO and Its Variants. Since the introduction of DPO (Rafailov et al., 2024), several algorithms have emerged to further refine preference optimization. SimPO (Simple Preference Optimization) introduces length regularization on the log-probabilities of both preferred and dispreferred completions, eliminating the need for a reference"}, {"title": "Supervised Next-Word Prediction Regularization Improves Training Stability.", "content": "DPO models the relative probability of selecting one completion over another using pairs of preferred and non-preferred data. However, the standard DPO loss may inadvertently reduce the model's likelihood of producing the preferred completion, as long as the relative probability between the preferred and non-preferred completions increases . This can result in a failure mode during DPO training . To address this, various forms of supervised next-word prediction regularization have been proposed to improve training stability. For example, SLIC adds a term to maximize log-likelihoods on certain reference completions , while CPO (Contrastive Preference Optimization) applies a behavior cloning regular-izer that specifically optimizes the preferred com-pletions . Additionally, DPOP introduces a hinge loss on the log-ratio between the reference and target models . Despite the improvements in training stability, our analysis indicates that regularized preference optimization often results in worse performance compared to non-regularized approaches."}]}