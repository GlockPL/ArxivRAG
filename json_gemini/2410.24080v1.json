{"title": "Graph Learning for Numeric Planning", "authors": ["Dillon Z. Chen", "Sylvie Thi\u00e9baux"], "abstract": "Graph learning is naturally well suited for use in symbolic, object-centric planning\ndue to its ability to exploit relational structures exhibited in planning domains and\nto take as input planning instances with arbitrary numbers of objects. Numeric\nplanning is an extension of symbolic planning in which states may now also exhibit\nnumeric variables. In this work, we propose data-efficient and interpretable ma-\nchine learning models for learning to solve numeric planning tasks. This involves\nconstructing a new graph kernel for graphs with both continuous and categorical at-\ntributes, as well as new optimisation methods for learning heuristic functions for nu-\nmeric planning. Experiments show that our graph kernels are vastly more efficient\nand generalise better than graph neural networks for numeric planning, and also\nyield competitive coverage performance compared to domain-independent numeric\tplanners. Code is available at https://github.com/DillonZChen/goose", "sections": [{"title": "Introduction", "content": "Planning requires long range reasoning over combinatorially large state spaces. Numeric planning\nis an extension of classical planning in which states have numeric variables and the underlying\ntransition system is built from inequality conditions and assignments over arithmetic expressions of\nsuch variables. It was formalised in PDDL 2.1 [FL03] and is undecidable in the general case [Hel02]\nwhich makes it more difficult than classical planning which is PSPACE-complete [Byl94]. Numeric\nplanning is a well-established problem in the symbolic AI community and exhibits significant research\neffort [CCFL13, IM17, SHTR20, KSP+22, KSB23, SKB23], but this expressivity result implies that\nbuilding a general, scalable numeric planner is a challenging problem.\nLearning for Planning (L4P) is a research direction which focuses on learning to solve problems from\na specified domain in an automated supervised manner [TTTX20, STT20, FGT+22, KS21, SBG22,\nSBG23, MLTK23, CTT24a, SDS+24, RTG+24, APK24]. Planning tasks in L4P are assumed to\nexhibit a factored, symbolic representation, which allow us to generate training data in a matter of\nseconds from easy to solve tasks with a domain-independent planner. We can then learn domain\nknowledge in a supervised manner that scales planners to significantly larger planning tasks.\nThis is in contrast to Reinforcement Learning where agents do not require access to well-defined mod-\nels but spend significant amounts of time exploring and learning from rewards [SB98]. Regardless,\nseveral works have showed that encoding or learning symbolic models for sequential decision mak-\ning reasoning and embodied AI tasks [LCZ+21, ZYP+22, LSS+22, SCK+23, KVS+23, LPM23]\nprovided better performance and transparency over end-to-end reinforcement learning methods.\nFurthermore, it was shown recently that classical ML methods are much better suited for L4P than\ndeep learning methods for symbolic planning [CTT24b] as they (1) can generalise well from small\ntraining data, (2) are orders of magnitude more efficient to train and evaluate than deep learning\nmethods, which is important in time sensitive tasks such as planning, and (3) have interpretable\nfeatures to understand what is being learned.\nIn this paper we study whether this fact carries over to Learning for Numeric Planning (L4NP) [WT24]\nwhich now requires reasoning over logic and arithmetic. It is reasonable to think that because neural\nnetworks are function approximators, they may offer better reasoning capabilities over numbers than"}, {"title": "Background", "content": "Numeric Planning Task. A numeric planning task can be viewed as a compact representation\nof a deterministic, goal-conditioned Markov Decision Process with the use of predicate logic and\nrelational numeric variables. A majority of the remainder of this section formalises the necessary\ncomponents of a numeric planning task we use in the paper.\nA numeric planning task [FL03] is given by a tuple \u03a0 = (Xp, Xn, A, 50, G) where Xp is a finite\nset of propositional variables with domain {T, \u22a5} and Xn is a finite set of numeric variables with\ndomain R. Let X = Xp U Xn denote the set of state variables, where a state is a total assignment of\nvalues the propositional and numeric variables. The variables implicitly induce a possibly infinite set\nof states S, where so is the initial state.\nA propositional condition is a positive (resp. negative) literal x = T (resp. \u22a5) for some propositional\nvariable x \u2208 Xp, and a numeric condition has the form \u03be \u25b7 0 where \u03be is an arithmetic expression over\nnumeric variables and \u25b7 \u2208 {>, >, =}. We write [x]s (resp. [\u03be]s) for the value of a state variable x\n(resp. expression \u03be) in state s, and V (\u03be) for the set of numeric state variables in \u03be. A state s satisfies\na set of conditions (i.e. a set of propositional and numeric conditions) if each condition in the set\nevaluates to true given the values of the state variables in s. The goal G is a set of conditions and we\nwrite Gp (resp. Gn) for the subset of propositional (resp. numeric) goal conditions."}, {"title": "Relational features for numeric planning", "content": "In this section, we describe an automatic method for generating embeddings for numeric planning\ntasks that may be used for any downstream inference model. The method is an extension of the feature\ngeneration method for classical planning [CTT24b] and consists of two main steps: (1) generating a\ngraph representation of a planning task, and (2) running a variant of the WL-algorithm for generating\nfeatures for the graph [SSVL+11]. Extending the first step of the method is simple as it is easy to\nextend the graph encoding to capture numeric information of the task. This is done in Sec. 3.1 where\nwe introduce the Numeric Instance Learning Graph (vILG) representation for numeric planning tasks.\nThe second step is more difficult as we require constructing a WL-algorithm variant that can handle\nboth categorical and continuous nodes features in a meaningful way for numeric planning. This is\nwhere we introduce the CCWL algorithm in Sec. 3.2 that handle such node features. Thus, we can\ngenerate features for numeric planning tasks by first converting them into the vILG representation,\nand then running the CCWL algorithm on them."}, {"title": "Graph encoding of numeric planning tasks", "content": "We begin by describing our graph encoding of a planning task, namely the Numeric Instance Learning\nGraph (VILG). Similarly to the classical case, the graph representation does not encode the transition\nmodel of the planning task nor requires grounding all possible variables in the planning task. Thus,\nour encoding only requires a first-order representation of states, and therefore applies to problems\nwhose transition model is unknown such as in model-free reinforcement learning.\nWe begin with a descriptive definition of the graph with an example Fig. 2 illustrating a subgraph\nof the ILG representation of the example ccBlocksworld problem. In the figure, the nodes in the\ngraph represent the objects (light blue), propositional variables true in the state (green), numeric\nvariables (red), propositional goals (yellow) and numeric goals (not present in the example) of the\nproblem. Blue (resp. orange) edges connect object nodes to goal and variable nodes where the object\nis instantiated in the first (resp. second) argument of the corresponding node variable or condition.\nWe provide the formal definition below. Let Xp(s) denote the set of propositional variables that are\ntrue in s, Xn(s) the set of numeric variables, and X(s) = Xp(s) U Xn(s).\nDefinition 3.1 (Numeric Instance Learning Graph). The Numeric Instance Learning Graph (vILG)\nof a numeric planning task in the lifted representation \u03a0 = (\u039f, \u03a3\u03c1, \u03a3f, \u03a3\u03b1, A, So, G) is a graph\nG = (V, E, F cat, Fcon, L) with\n\u2022 nodes V = OUX (80) UG, where we assume w.l.o.g. that V(g) \u2264 X (so) for all g \u2208 Gn,\n\u2022 edges E = Up=5(01,...,0ng)\u2208X(50)UGp {{p, oz) | i\u2208 [no]} \u222a\u222a\u00a3\u22650\u2208Gn {{\u00a7, v) | v \u2208 V(g)},\n\u2022 categorical node features $F_{cat} : V \\rightarrow \\Sigma_y$ with $F_{cat}(u)$ =\n$\n\\begin{cases}\nOBJ(u) & \\text{if } u \\in O \\\\\nFUNC(u) & \\text{if } u \\in X_n(s_0) \\\\\n\\begin{aligned}\n&\\begin{cases}\n(PRED(u), \\text{achieved\\_propositional\\_goal}) & \\text{if } u \\in X_p (s_0) \\cap G_p \\\\\n(PRED(u), \\text{unachieved\\_propositional\\_goal}) & \\text{if } u \\in X_p (s_0) \\setminus G_p \\\\\n(PRED(u), \\text{achieved\\_propositional\\_nongoal}) & \\text{if } u \\in G_p \\setminus X_p(s_0)\n\\end{cases}\n\\end{aligned} & \\text{if } u \\in X_p(s_0) \\cup G_p \\\\\n(COMP(u), ACH(u)) & \\text{if } u \\in G_n\n\\end{cases}\n$\nwhere OBJ(u)  u if u is a constant object and object otherwise, PRED(u)/FUNC(u) re-\nturns the predicate/function symbol from which a proposition/fluent was instantiated from,\nCOMP(u) \u2208 {>, >, =} encodes the comparator type of the numeric goal condition u, and\nACH(u) \u2208 {unachieved_numeric_goal, achieved_numeric_goal} encodes whether so satisfies u,\n\u2022 continuous node features $F_{con} : V \\rightarrow \\mathbb{R}$ where Fcon(u) = [u]50 for nodes u \u2208 Xn(so), Fcon(u) =\n[\u00a7]50 for nodes u \u25b7 0 \u2208 Gn with [\u00a7]500, and Fcon(u) = 0 otherwise, and\n\u2022 edge labels L : E \u2192 \u03a3\u314c where for edges of the form e = (p, oi), we have L(e) = i, and otherwise\nfor edges e = (\u03be, \u03c5), we have L(e) = 0."}, {"title": "The CCWL algorithm for numeric planning", "content": "The WL algorithm [WL68] has been adapted to computing features for graphs with categorical\nnode attributes by [SSVL+11]. A variant of the WL algorithm for graphs with continuous node\nattributes has been proposed by [TGL+19] for the purpose of computing kernels with the Wasserstein\ndistance between graph embeddings. However, the graph embeddings themselves are not invariant\nto the order of graphs in the nodes. Furthermore, from [CTT24b], non-linear kernels result in\npoorer generalisation compared to linear models in the context of L4P due to overfitting to the range\nof training targets. Morris et al. [MKKM16] constructed kernels for continuous node attributes\nby hashing Euclidean embeddings into categorical features but such a method loses the semantic\nmeaning of numbers. Thus, we propose a new variant of the WL algorithm for graphs with both\ncategorical and continuous node attributes for generating graph embeddings (CCWL algorithm). This\nalgorithm is summarised in Alg. 1 and also depicted in Fig. 3.\nAlgorithm 1: CCWL algorithm\nData: A graph G = (V, E, Fcat, Fcon, L) with continuous and\ncategorical attributes, a deterministic and injective HASH\nfunction, allowed colours C = [|C|], a pooling function\nPOOL, and number of CCWL iterations L.\nResult: Feature vector of size R(1+d)|C|\n1 \u03ba0(v) \u2190 Fcat(v), v \u2208 V\n2 for j\u2208 [L] do for v \u2208 V do\n3 \u03ba\u00cd (v) \u2190 HASH (\u03ba\u00cd\u22121(v), UL\u2208LE {(\u03ba\u00cc\u22121 (u), \u03b9) | u \u2208 N, (v)})\n4 M \u2190 Uj\u2208 {0}U[L] {\u03ba' (v) | v \u2208 V}\n5 \u0e1b\u0e35cat \u2190 [COUNT(M, C\u2081), . . ., COUNT(M, C|C|)]\n6 S\u2081 = {v | v \u2208 V s.t. \u2203j \u2208 {0} U [L], \u03ba\u00cc (v) = ci} ,\u2200i \u2208 C\n7 \u0e1b\u0e35con \u2190 [con(1) || ... || con(|C|)], con(i) = POOLves; (Fcon (v))\n8 return cat || con\nLines 1-3 of Alg. 1 are the original steps of the WL algorithm for generating graph embeddings by\niteratively refining categorical node features, which we call colours, with two differences. Firstly, we\nreplaced the multi-set with a set in the input of the hashing function. This is because in planning,\nunseen colours arise from graphs with increasing degrees which occur for out-of-distribution testing\nproblems of increasing size. This problem is limited by relaxing the hash input with a set, which\ntrades expressivity for generalisation. Secondly, we make use of edge labels in the hashing function.\nLines 4-5 collect the counts of allowed colours C seen during the main loop of the algorithm to\ngenerate the categorical feature vector in the form of a histogram. We assume by relabelling colours\nthat C = [|C|]. Lines 6\u20137 generate features from pooling the continuous attributes from different"}, {"title": "Relational neural networks for numeric planning", "content": "Deep learning architectures such as graph neural networks (GNNs) [SGT+09, GSR+17] bene-\nfit in generating latent representations automatically with backpropagation when trained end-to-\nend [LBH15]. GNNs also benefit from being able to train and evaluate on arbitrary sized graphs.\nHowever, it is generally understood that the expressive power of GNNs is limited by the WL-algorithm\nand counting logics with two variables [XHLJ19, BKM+20]. This result translates to the impos-\nsibility result of GNNs not being able to learn features that can work well for arbitrary planning\ndomains [SBG22, CTT24a]. Nevertheless, their application to numeric planning tasks, in which both\nlogical and numeric reasoning is required, is less well understood. Thus, we still propose GNNs as\nan additional baseline for L4NP and empirically evaluate their performance for numeric planning in\nSec. 6.\nFor our GNN architecture, we perform a transformation on the node features of the vILG\nfrom 3.1 as input for GNNs that can handle edge labels. More specifically, given a vILG\nG = (V, E, Fcat : V \u2192 \u2211v, Fcon \u2192 R, L), we construct a new graph G' with continuous node\nattributes X : V \u2192 R|2v|+2 defined by X(u) = OH(Fcat(u)) ||[r1, r2], where OH(Fcat(u)) \u2208\n{0,1}|2v| CRI\u2211| denotes a one-hot encoding of the categorical node feature of u, and r\u2081 denotes\nthe numerical value of numeric variable nodes defined by r\u2081 = [u]\u00ba if u \u2208 Xn(so) and r\u2081 = 0\notherwise, and r2 denotes the goal error for numeric goal nodes defined by r2 = [u]\u00ba if u \u2208 Gn\nand r2 = 0 otherwise. We denote the vILG for GNNs by (V, E, X, L) with notation for categorical\nfeatures removed. Thus, we can use this graph encoding of numeric planning tasks as input into any\ndownstream GNN that can handle edge labels or features."}, {"title": "Optimisation formulations for learning heuristic functions", "content": "In this section, we describe two optimisation methods used for learning heuristic functions from\ntraining data, namely by minimising cost-to-go estimate error and ranking estimate error. Fig. 4\nillustrates examples of learned heuristic functions on states of a planning task when trained to zero\nloss with both the cost-to-go and ranking formulations. We assume that training data for our models\nconsist of a set of numeric planning tasks \u03a01,..., In with corresponding optimal plans \u03c01,..., \u03c0\u03b7\u00b7"}, {"title": "Experiments", "content": "We take 8 domains out of 10 domains from the International Planning Competition 2023 Learning\nTrack (IPC-LT) [SSA23] and either convert them to equivalent numeric formulations, or introduce\nnumeric variables to model extra features such as capacity constraints. The two domains from the\nIPC-LT that we do not convert into numeric domains are Floortile and Sokoban which do not have\nany benefit from compilation to a numeric representation nor exhibit any interesting features that can\nbe modelled with numeric variables. The domains we considered from the IPC-LT are summarised\nin Fig. 5 alongside the sizes of training and testing tasks, and time to generate training data. Each\ndomain consists of 90 testing problems and at most 99 small training problems for which the median\ntime for generating an optimal training plan is less than a second and a few outliers taking more than\na minute. We refer to the appendix for further details on the domains."}, {"title": "Experimental setup", "content": "Training. As discussed in Sec. 5, we only consider optimal plans from small problems as training\ndata. We compute them with the Numeric Fast Downward planner [AN17] using A* search and the\nadmissible hLMCUT heuristic [KSP+22], with a 30 minute timeout and 8GB main memory.\nWe consider 4 model configurations. Firstly, we use CCWL features from Sec. 3 with Support\nVector Regression and the linear dot product kernel to learn a linear model for cost-to-go estimation\n(hCWLF). Next, we use CCWL features in optimisation problem in (1) with CPLEX version 22.11\nand a timeout of 600 seconds for ranking estimation (hWLF). Both hoWLF and hoWLF models\nhave allowed colours C in Alg. 1 given by all the refined colours seen during training. We also\nhave cost-to-go (hGNN) and ranking (hGNN) estimation models using GNNs operating on vILG\nrepresentations of planning tasks and optimised with the MSE loss function and (2), respectively.\nFor the backbone GNN, we use a Relational Graph Convolution Network [SKB+18] but replacing\nthe mean aggregation function with the element-wise max operator in the message-passing update\nstep: $h_{u}^{(l+1)}=\\sigma(W_0 h_{u}^{(l)}+ \\sum_{i\\in \\Sigma_E} max_{v\\in N_i(u)} W_i h_{v}^{(l)} )$, where l denotes the GNN layer, o is\nimplemented with the leaky ReLU function, and Wo and W) are learnable weight matrices. Each\nGNN has a hidden dimension of 64, and is trained with the Adam optimiser [KB15] with an initial\nlearning rate of 10-3 and batch size of 16. A scheduler reduces the training loss by a factor of 10 if\nloss does not improve after 10 epochs. Training then terminates if the learning rate falls below 10-5.\nLet L denote the iterations hyperparameter for CCWL models and number of layers for GNN models.\nEvaluation. We consider several numeric planners as baselines for benchmarking the effectiveness\nof learning. We first include hLMcUT as the only optimal planner baseline as it is also the training data\ngenerator but solves a more difficult problem of optimal planning compared to satisficing planning.\nWe consider the Metric-FF planner (M-FF) [Hof03], and the hADD, MRP, hMRP+hj and M(3h||3n)\nconfigurations in the ENHSP planner [SHTR20, SSSG20, CT24]. We have that hADD and hMRP are\nplanners that perform GBFS with a single heuristic only, while hMRP+hj and M(3h||3n) use additional\ntechniques (macro actions, multiple queues, and novelty heuristics) to boost planning performance.\nOur CCWL and GNN models are all used in single-queue GBFS with the learned heuristic function,\nwith Numeric Fast Downward as the backend search implementation. All baselines and models are\nrun on a single Intel Xeon Platinum 8268 (2.90 GHz) core with a 5 minute timeout for search and"}, {"title": "Limitations", "content": "The setup of our work is limited to the assumption that the problems being solved can be explicitly\nrepresented in a symbolic language such as PDDL. The assumption of the existence of PDDL\nencodings of planning problems allows us to generate training data quickly with domain-independent\nnumeric planners for supervised training. Furthermore, experiments and theoretical insights also\nshow that our proposed techniques have room for improvement as there are still classes of numeric\nplanning tasks with which our models cannot learn and generalise well in."}, {"title": "Conclusion", "content": "We have proposed a new graph embedding algorithm, the CCWL algorithm, and optimisation criterion\nfor learning heuristic functions for numeric planning. Planning tasks are encoded as Numeric\nInstance Learning Graphs (VILG) on which we run our CCWL algorithm for generating features.\nOur numeric planning features are interpretable and efficient to generate. Experimental results show\nthe effectiveness of our approach by achieving competitive performance over both deep learning\narchitectures and domain-independent numeric planners. Furthermore, we have identified future\nwork by improving the expressivity of our algorithms for capturing more complex numeric domains.\nLastly, one can learn forms of domain knowledge different from heuristic functions with our new\nnumeric planning features and graph representations such as policies [WT24], portfolios [MFH+20]\nand detecting relevant objects [SCC+21]."}, {"title": "Related Work", "content": "Two related fields to Learning For Planning (L4P) and Learning For Numeric Planning (L4NP) are\nGeneralised Planning (GP) and Reinforcement Learning (RL). In the following subsections, we\noutline the main difference between L4P with the respective related fields as well as corresponding\nrelated work."}, {"title": "Generalised planning", "content": "GP consists of automatically characterising the solution of a (possibly infinite) set of planning\ntasks [Sri10, SIZ08]. The most common characterisations are action policies, but other character-\nisations also include finite state controllers [BPG09, BPG10, HG11, HG13, AJJ18], and programs\nwith branching and looping [AJJ21, ACSJ22]. Logic programming approaches involving decision\nlists [Kha99, GT04] and Datalog programs [GRH24, CH\u016024] have also been used to characterise\nsolutions for planning domains. We refer to articles [CAJ19] and [Sri23] for more detailed surveys of\nGP. The difference L4P and GP can be subtle given that there is a non-empty intersection between the\ntwo fields, and works in both fields generally aim to compute structures that solve problems from a\ngiven domain. The way we differentiate the two fields is that L4P follows generally follows traditional\nsupervised learning approaches, whereas GP can be likened to performing program synthesis.\nWith regards to numeric planning, Srivastava et al. [SZIG11] introduced Qualitative Numeric Planning\n(QNP) which is a subset of numeric planning where numeric variables exhibit non-negative domains,\nand actions increase or decrease the value of numeric variables by indeterminate amounts. A solution\nfor a QNP is a policy which can be used to represent solutions for sets of planning tasks. QNP has\nbeen shown to be equivalent to fully observable non-deterministic (FOND) planning [BG20] arising\nfrom the non-determinism of action effects, and the connection between FOND and GP has often\nshown itself when used to synthesise generalised policies [BG18, IM19]. Lin et al. [LCF+22] studies\nGP for a more expressive class of numeric planning, by allowing for integer numeric variables and\nemploying linear expressions in conditions and action effects. Their approach involves synthesising\nprograms that allow for branching and looping. Lastly, vASNets [WT24] extends ASNets [TTTX20]\nin order to learn policies with a neural network architecture for planning."}, {"title": "Reinforcement Learning", "content": "RL is a learning paradigm for decision making that does not have access to a model and instead\nlearns from rewards [SB98]. RL has achieved promising results in games when combined with deep\nlearning [MKS+15, SHM+16]. A major difference between RL and L4P is that the former requires\nreasoning over dense reward functions, whereas the latter requires reasoning over logic [Gef18].\nNevertheless, there has been some preliminary work looking at the intersection of RL and planning.\nReward machines [IKVM22] are a logical language used for specifying reward functions for RL\nproblems, inspired by the declarative nature of the planning as modelling paradigm. RL has also been\napplied directly into planning tasks, as done by [MV21] for temporal planning. Rewards are mostly\nsparse, with 1 being reward for achieved goals, minor 10-5 rewards for achieved goal propositions,\nand no reward otherwise. Gehring et al. [GAC+22] explored introducing denser reward functions\nto planning through domain-independent heuristics to allow for RL approaches. Supervised RL\nhas also been used for learning planning policies [SBG23]. Nevertheless, the use cases for RL and\nplanning are generally different, with RL being more suited for control tasks in continuous or dynamic\nenvironments such as in robotics, and planning being more suited for combinatorial tasks in discrete\nor abstract environments such as in logistics."}, {"title": "Description of Benchmark Domains", "content": "This domain was described in Sec. 2. A task from the domain consists of n blocks stacked on top\nof one another to form towers on top of b bases. Each base has a capacity of how many blocks it\ncan support. The goal is to stack and unstack blocks to achieve a target tower configuration. The\nnumeric component of this domain arises from modelling the capacity of bases. Training problems\nhave n \u2208 [2, 11] blocks while testing problems have n \u2208 [5, 488] blocks."}, {"title": "Numeric Childsnack", "content": "A task from the domain consists of feeding c children with sandwiches in I locations, of which\nsome are allergic to gluten. There are a finite amount of gluten-free (GF) and non-GF ingredients."}, {"title": "Numeric Ferry", "content": "A task from the domain consists of c cars spread across l locations. A ferry is able to transport up\nto a fixed amount of cars around to different locations. The goal of the domain is to transport the\ncars with the ferry to various target locations. The numeric component of the domain arises from\nmodelling the capacity of the ferry. Training problems have c \u2208 [1, 20] cars while testing problems\nhave c\u2208 [4, 974] cars."}, {"title": "Numeric Miconic", "content": "A task from the domain consists of p passengers with different weights spread across f floors. There is\na single elevator with a fixed load capacity that can transport passengers between floors. Furthermore,\nif the load of the elevator exceeds a secondary threshold, it takes twice as long to move between floors.\nThe goal of the domain is to move the passengers to their target floors. The numeric component of\nthe domain arises from modelling the weight of the passengers and load capacity of the elevator.\nTraining problems have p \u2208 [1, 10] passengers while testing problems have p \u2208 [1, 485] passengers."}, {"title": "Numeric Rovers", "content": "A task from the domain consists of r rovers some of which can sample rock and soil data, while\nothers have cameras that can take images of objectives. The goal of each problem is to sample rock\nand soil data as well as take images of objectives and communicate all g data to the lander. The rovers\ncan move around a map with w waypoints and the rover is only able to communicate data to the\nlander from a subset of waypoints. Furthermore, rovers have a limited energy supply that is consumed\nwith any action, but they can recharge with solar panels at certain waypoints. Thus, the problem has\ndeadends because rovers have limited energy and could exhaust them in waypoints where they cannot\nrecharge. The numeric component of the domain arises from modelling the energy supply of the\nrovers. Training problems have g \u2208 [1, 10] goals while testing problems have [2, 728] goals problems"}, {"title": "Numeric Satellite", "content": "A task from the domain consists of s satellites, each carrying a subset of i instruments that can take\npictures of space using a subset of m modes. Satellites can rotate to take pictures of d locations in\nspace. Each satellite has a fixed amount of fuel that is consumed when rotating, and a fixed amount\nof data capacity that is consumed when taking pictures. Thus, the problem has deadends because\nresources are finite and can be wasted. The goal of a Satellite problem is to take pictures of a set of\nlocations in space with specified modes while adhering to the fuel and data capacity constraints. The\nnumeric component of the domain arises from modelling the fuel and data capacity features. Training\nproblems have s \u2208 2, 10 satellites and testing problems have s \u2208 [4,98] satellites."}, {"title": "Numeric Spanner", "content": "A task from the domain consists of s spanners scattered along a one-way hallway with l locations, and\nn nuts at the end of the hallway that have to be fixed. Each spanner can only be used to fix a single\nnut before it breaks. The goal of the domain is to fix all the nuts. The problem has deadends if not\nenough spanners are picked up before reaching the end of the hallway. The numeric component of the\ndomain arises from modelling the number of spanners and nuts. Training problems have s \u2208 [1, 10]\nspanners while testing problems have s \u2208 [1487] spanners."}, {"title": "Numeric Transport", "content": "A task from the domain consists of p packages spread across l locations, with t number of trucks\nthat can transport pick up and transport packages on a map. Each truck has a limited capacity of\npackages it can carry. The goal of the problem is to transport all the packages to their target locations.\nThe numeric component of the domain arises from modelling the capacity of the trucks. Training\nproblems have p \u2208 [1, 7] packages while testing problems have p \u2208 [1, 194] packages."}, {"title": "More Details for the ccBlocksworld Example", "content": "We repeat the running ccBlocksworld example in Fig. 7. Listings 2 and 1 provide the explicit PDDL\ndomain and problem encodings for the running ccBlocksworld example. An optimal plan for the\nproblem is given as follows on the left, and an optimal plan without capacity constraints on the right.\n1.\t(unstack f dj)\t1.\t(unstack f dj)\n2.\t(stack fa i)\t2.\t(stack fek)\n3.\t(unstack d bj)\t3.\t(unstack d bj)\n4.\t(stack d f i)\t4.\t(stack d f k)\n5. (pickup bj)\t5.\t(pickup a i)\n6.\t(stack bek)\t6.\t(stack a dk)\n7.\t(unstack dfi)\t7.\t(pickup bj)\n8.\t(putdown d j)\t8.\t(putdown bi)\n9. (unstack fai)\t9.\t(unstack adk)\n10.\t(stack f d j)\t10.\t(stack a bi)\n11. (pickup a i)\n12.\t(stack a f j)\n13. (unstack bek)\n14.\t(putdown bi)\n15.\t(unstack a f j)\n16.\t(stack a bi)"}]}