{"title": "ColorEdit: Training-free Image-Guided Color editing with diffusion model", "authors": ["Xingxi Yin", "Zhi Li", "Jingfeng Zhang", "Chenglin Li", "Yin Zhang"], "abstract": "Text-to-image (T2I) diffusion models, with their impressive generative capabilities, have been adopted for image editing tasks, demonstrating remarkable efficacy. However, due to attention leakage and collision between the cross-attention map of the object and the new color attribute from the text prompt, text-guided image editing methods may fail to change the color of an object, resulting in a misalignment between the resulting image and the text prompt. In this paper, we conduct an in-depth analysis on the process of text-guided image synthesizing and what semantic information different cross-attention blocks have learned. We observe that the visual representation of an object is determined in the up-block of the diffusion model in the early stage of the denoising process, and color adjustment can be achieved through value matrices alignment in the cross-attention layer. Based on our findings, we propose a straightforward, yet stable, and effective image-guided method to modify the color of an object without requiring any additional fine-tuning or training. Lastly, we present a benchmark dataset called COLORBENCH, the first benchmark dataset to evaluate the performance of color change methods. Extensive experiments validate the effectiveness of our method in object-level color editing and surpass the performance of popular text-guided image editing approaches in both synthesized and real images.", "sections": [{"title": "1. Introduction", "content": "Color is one of the most important visual perceptions for humans. The color of an object significantly influences the emotional responses and perceptions of a person toward it, which makes color a key element in both functional and aesthetic design decisions across industries, particularly in the design field. With the development of diffusion models, some studies [18, 23, 47] have applied the stable diffusion (SD) model to the colorization task, which involves adding color to grayscale or black-and-white images. However, currently, no studies specifically investigate the task of color change. Although some methods [3, 9, 10, 13, 24, 42] demonstrate the capability to modify an object's color, those text-guided techniques often fail to change the color of an object as expected, as shown in Fig. 2.\nIn this paper, we conduct an in-depth exploration on the cross-attention layer, which aligns and transforms text information into synthesized images. Specifically, we visualize the cross-attention maps of objects in different blocks of the model through different stages of the denoising process to elucidate how textual information directs the generation of images. We identify the semantic information captured by various cross-attention blocks and demonstrate when and where an object's shape, contour, and texture are established. For the unsuccessful outcome of text-guided color editing, we argue that there are two main factors: (a) the imprecise distribution of color attribute attention weights on the spatial area, called cross-attention leakage. (b) The collision of information on attributes in the cross-attention map from the original object and the color term in the target prompt. We find that, compared to altering the Key matrices in the cross-attention layer, modifying the Value matrices of the target image results in a more stable color change effect. Based on our findings, we introduce a simplified yet stable and effective method called training-free Image-Guided Color Editing. Our method performs object color editing by aligning the Value matrices of the target image with the Value matrices extracted from a reference color image in specific cross-attention layers of the diffusion model in the early stages of the denoising process (see an example in Fig. 1).\nOur contributions are as follows: (1) We demonstrate that the shape, contour, and texture of an object are determined in the U-Net decoder in the early stage of the denoising process. (2) We propose a tuning-free image-guided method to edit the color of an object through Value matrices alignment in the cross-attention layer. (3) We introduce COLORBENCH, the first benchmark dataset to evaluate the color editing task. (4) Experimental results demonstrate that our Image-Guided Color Editing method surpasses current popular text-guided image editing methods on both synthe-"}, {"title": "2. Related work", "content": "Significant advances in diffusion-based text-guided image generation methods [12, 14, 26, 28, 32, 34, 35, 37] have led to the rapid adoption of these methods for various generative visual tasks, including image editing, which includes semantic editing, structural modifying, and stylistic editing. Semantic editing involves altering the fundamental meaning or conceptual representation of an image. This may include tasks such as adding [2, 3, 6, 9-11, 19, 25, 29, 44, 48] , removing [9, 10, 50], or replacing objects [2, 3, 6, 7, 9-11, 13, 19, 20, 24, 27, 29, 48, 51], and changing the background [2, 22]. Structural editing modifies the composition of an image, including repositioning [6, 8], changing the size and shape [8, 11, 22, 31], altering actions and poses [4, 16, 19, 22], and adjusting the perspective or viewpoint [4] of an object. Stylistic editing involves modifying the artistic style or aesthetic presentation of an image, including color change [3, 9, 10, 13, 19, 24, 27, 42], texture change [42], and style change [10, 19, 20, 27, 29, 39, 42, 46, 49, 53].\nFor color change task, according to their learning strategies, text-guided image editing methods can be classified into training-based approaches [3, 9, 10], testing-time fine-tuning approaches [19, 27], and training- and fine-tuning-free approaches [4, 13, 24, 42]. Training on a constructed image-text-image dataset, InstructPix2Pix [3], InstructDiffusion [10], and MGIE [9] train a new diffusion model, allowing users to edit images using natural language instructions. Imagic [19] approximates the input image's text embedding through tuning, then edits the image by interpolating between this embedding and the target text embedding. Null text inversion [27] employs an optimization method to reconstruct the image and utilizes P2P [13] for image editing. P2P [13] observes that cross-attention layers capture interactions between pixel structures and prompt words, enabling semantic image editing through cross-attention map manipulation. PnP [42] demonstrated that the structure of the image can be preserved by manipulating spatial features and self-attention maps. FreePromptEditing [24] explores cross-attention and self-attention mechanisms, showing that cross-attention maps often carry object attribution information, leading to editing failures, while self-attention maps are essential for preserving geometric and shape details during transformations. Although all of these methods are capable of editing the color of an object in an image, none are specifically designed to address the task of color change. In this paper, we introduce a training-free method which utilizes a reference color image to modify the color of an object. Unlike training-based or testing-time fine-tuning approaches [3, 10, 19, 27], our method operates without requiring any fine-tuning process. The most related work to ours is training and fine-tuning free approaches P2P [13], PnP [42], and FreePromptEditing [24]. However, due to attention leakage and attribution collision between the cross-attention map of the original object and the new attribute in the text prompt, those methods may fail on the color change task."}, {"title": "3. Preliminaries", "content": "Diffusion models [15, 38, 41] involve two Markov chain processes. The forward diffusion process is designed to transform a dataset's distribution into a specified distribution, such as the Gaussian distribution. The denoising process removes noise by sampling the data from a learned Gaussian distribution. The goal of the denoising process is to learn to reverse the forward diffusion process, ultimately reconstructing a distribution that closely mirrors the original. Our approach takes advantage of the advanced text-conditioned model, Stable Diffusion (SD) [35]. The denoising backbone is structured as a time-conditional U-Net [36]."}, {"title": "3.2 Attention Mechanism in LDM", "content": "The U-Net in the Stable Diffusion model is an Encoder-Decoder architecture comprising an encoder, a decoder, and a connection module that integrates these components. The encoder has three CrossAttnDownBlocks, while the decoder utilizes three CrossAttnUpBlocks, and the connection module is a CrossAttnMidBlock. Each Cross-Attention block in the U-Net consists of a series of basic blocks. Each basic block includes a residual block, a self-attention module, and a cross-attention module. The core of the self-attention module and the cross-attention module lies in the attention mechanism [1], which can be formulated as follows:\nAttention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V. (1)\nHere, Q represents the Query matrices projected from the spatial features, while K and V denote the Key and Value matrices, which are projected from either the spatial features (in the self-attention module) or the textual embeddings (in the cross-attention module). $d_k$ is the dimension of K."}, {"title": "4. Method", "content": "Given a source image $I_s$ or a text prompt $P$, an interest object $O$, and a reference color image $I^c$, our objective is to change the color of the object and output a target image $I_T$. In the target image, the object's color should align with the reference color image, while the shape, contour, and"}, {"title": "4.1 Cross-Attention Mechanism in LDM", "content": "In text-guided image generation, the text guiding process utilizes the mechanism of cross-attention [43] within a low-dimensional latent space. This mechanism aligns the image pixel with the semantic meaning of the prompt words. Let the text prompt be $P$, the textual embedding be $\\phi(P)$, the noisy vector in the latent space at the diffusion step $t$ be $z_t$, the deep spatial features of the noisy vector be $\\psi(z_t)$. The Query $Q$, Key $K$, and Value $V$ in the cross-attention layer are obtained through learned linear projections function $f_q$, $f_k$ and $f_v$ separately.\n$Q = f_q(\\psi(z_t)), K = f_k(\\phi(P)), V = f_v(\\phi(P)),$ (2)\nIt is obvious that textual information encoding is fed into the linear projection function $f_k$, $f_v$, which means that the text prompt affected the image synthesis process directly through the Key and Value matrices, while Q is derived from the previous layer. The query Q and the key K are then used to calculate the attention map, which determines the spatial layout and geometry of the generated image. In this paper, we investigate the interaction between pixel spatial structures and subjects in the prompts in different blocks of the diffusion model. As can be seen in Fig. 11, the shape, contour, and texture of an object in the generated image are determined in the U-Net decoder. Specifically, the shape, contour, and texture of the object are defined in the 1st, 2nd, and 3rd CrossAttnUpBlocks. More detailed results of the cross-attention map of the object within different cross-attention blocks can be seen in Section 7.2. In Fig. 5, we can observe that the shape is established first, followed by the contour and texture, which are refined until the end of the process. The main shape and contour of an object are established in the early stage of the denoising process.\nWhen changing color using text-guided training-free image editing methods [13, 24], the color editing task is performed by adding a color term to describe the object. During the denoising process, the cross-attention map of this color is injected into the corresponding part of the cross-"}, {"title": "4.2 Reference color image Value matrices extraction", "content": "To perform image-guided color editing, we first need to convert the reference color image $I^c$ into its latent code $z_c$. And then extract the Value matrices $V^c$ of the cross-attention layer when we reconstruct the reference color image $I^c$. In our paper, we primarily utilize Null-text Inversion [27] as the image inversion method. After inverting the reference color image $I^c$, we denoise the latent codes $z_c$ and utilize the PyTorch register forward hook method [30] to extract the value matrices $V^c$ from each layer of cross-attention of the denoising network in all timesteps. It is worth noting that for one reference color, the Value matrices $V^c$ and latent code $z_c$ need to be extracted only once and then can be applied repeatedly to different source images to change the color of the object."}, {"title": "4.3 Color Attribute Alignment and Object Structural Preservation", "content": "Based on our findings, we propose an image-guided method to modify the color of an object by using the Value matrices of a reference image $V^c$ to render the Value matrices of the target image $V_T$, which we call color attribute alignment. Those modified Value matrices are then used to calculate the spatial features for the next layer, thereby injecting color information into the denoising process. By aligning the Value matrices early in the generation process, when only the shape and contour of the object are established, we can resolve attribute information collisions between the cross-attention maps of the object and the color. Additionally, since no color term is introduced in the text prompt, the cross-attention leakage of the color attribute and the object is eliminated. To perform color attribute alignment, we normalize $V_T$ of the target image using $V^c$ of the reference color image with an adaptive normalization operation (AdaIN) [17]:\n$V^* = AdaIN(V_T, V^c),$ (3)\nwhere the AdaIN operation is given by:\n$AdaIN(x,y) = \\frac{\\sigma(y)}{\\sigma(x)}(x - \\mu(x)) + \\mu(y),$ (4)\nand $\\mu(x), \\sigma(x) \\in \\mathbb{R}^{d_k}$ are the mean and standard deviation of the value matrices (and the pipeline is given in Section 7.4). However, the aligned value matrices may also introduce additional information, potentially changing the structure of the object. To address this issue, we replace the self-attention map of the target image $M_{self}$ with the self-attention map of the source image $M^s_{self}$, since PnP [42], MasaCtrl [4], and FreePromptEditing [24] have shown that self-attention maps preserve spatial and structural information of an image. So, our denoising process is given by\n$\\begin{cases}\nz_{t-1} = \\text{VAlignDM}(z_t, P, t, V^c) \\{M_{self}^s \\leftarrow M_{self}\\} & \\text{if } t > T \\\\\nz_{t-1} = \\text{DM}(z_t, P, t) \\{M_{self}^s \\leftarrow M_{self}\\} & \\text{otherwise}\n\\end{cases}$\nwhere VAlignDM$(z_t, P, t, V^c)$ is the alignment of the color attribute AdaIN$(V_T, V^c)$ in the cross-attention layer in the"}, {"title": "4.4 Object Segment, Latent blending, and Background Preservation", "content": "The objective of this study is to alter the object's color while maintaining the background region unchanged. To achieve this aim, it is necessary to segment the object from the background in the source image. In P2P[13] and MasaCtrl [4], the binary mask was extracted directly from the cross-attention maps of the subject. However, the binary mask does not always precisely segment the object, especially around its edges. Observing that the cross-attention map from the 1st CrossAttnUpBlocks provides clear spatial location information of the object, we employ the state-of-the-art segmentation model SAM [21] to generate the mask of the object $Mask_{obj}$. More detailed information is given in Section 7.5. To enhance the effectiveness of editing, we integrate the information from the latent code $z_c$ of the reference color image into $z_t$ with the object mask $Mask_{obj}$ at the beginning of the denoising process and call it latent blending.\n$z_t^* = (1-Mask_{obj}) * z_t^+ + Mask_{obj} * (z_t^+ * (1 - R)+z_c^* * R)$\nA higher ratio R may enhance the color change, but could also cause the object to undergo more achromatic texture change (see the quantitative experiment result in Section 8.5). In practice, we observe that setting the ratio to 0.1 yields the best quantitative results, while setting it to 0.15 gives better color changes perceived by humans. In order to preserve the background information, we update $z_t^+$ with the latent $z_s$ of the source image $I_s$ in the final few steps of the denoising process.\n$z_t^* = (1-Mask_{obj}) * z_t^+ + Mask_{obj} * z_t^s$\nwhere $z_t^s$ is the latent variable of the target image which is calculated after alignment of the value matrices and the replacement of the self-attention map."}, {"title": "5. Experiments", "content": "We implement the proposed method on the text-to-image Stable Diffusion model [35] with publicly available checkpoints v1.4. During the sampling process, we used DDIM sampling [40] with 50 denoising steps and set the classifier-free guidance scale to 7.5. We mainly compare our tune-free method with current state-of-the-art text-based diffusion editing methods, including training-based methods InstructPix2Pix [3], Instructdiffusion [10] and MGIE [9], fine-tuning-free approaches P2P [13], PnP [42] and FreePromptEditing [24]. We did not compare our method with the testing time fine-tuning method Imagic [19], since it is not publicly available at this time. For our method, if not specifically indicated, the alignment of the cross-attention Value matrices is performed in the interval of $t \\in [0.8T, T]$ of the denoising process, the reference color image latent blending ratio be 0.1, and background preservation at the last 5 timesteps."}, {"title": "5.1. Qualitative Experiments", "content": "For generated images, as illustrated in the first row of Fig. 8, P2P and FreePromptEditing can preserve the background of the source image but fail to preserve the information around the object's contour (see the left ear of the pikachu). All of those training-free methods may fail to edit the color of objects, especially when the object's color is uncommon or unlikely to occur in the real world. For training-based methods, InstructPix2Pix, Instructdiffusion, and MGIE can alter the color of the object. InstructPix2Pix can preserve the visual structure of the object while Instructdiffusion and MGIE might change the structure of the object. However, these training-based methods may inadvertently alter the background color of the target image. Our method is capable of modifying the color of the object while preserving both the object's structure and the background information. For real images, as demonstrated in the second row of Fig. 8, compared with training-free methods P2P, PnP, and FreePromptEditing, our method can preserve the structure of the object and change the color of the object. Furthermore, our method achieves the same or even better results compared to the results of training-based methods. Moreover, as shown in Fig.9, our method is capable of modifying the color of the object in a more refined and controlled way, and Fig.10 indicates our method can edit different sizes of objects in the same image while keeping preview turn editing results. For qualitative experiments, more results can be found in Section 8.1."}, {"title": "5.2. Quantitative Experiments", "content": "In the absence of publicly available datasets to validate the effectiveness of color change editing methods, we developed a generated dataset and a real image dataset for quantitative assessment. For the generated dataset, we selected 160 most common subjects from various categories with ChatGPT. For each subject, we automatically generated 7 prompts, resulting in 1,120 image-prompt pairs. For each source image, we used the SAM method [21] to generate the object mask and manually selected the best one. We then modified the object to seven different colors\u2014white, gray, black, red, yellow, blue, and green\u2014resulting in a total of 7,840 pairs of source and target images. For the real dataset, we manually crafted 406 images from 100 subjects with Photoshop, resulting in a total of 2,842 pairs of source and target images, which we call COLORBENCH. More in-"}, {"title": "5.3. Human Evaluation", "content": "We performed a human evaluation to assess the quality of color modifications across various methods. We randomly sampled 100 source-target examples and let 10 annotators rate these images. Participants are asked to rate the following aspects on a scale from 0 to 5: 1) effectiveness of color editing on the object, 2) preservation of the object's structure, 3) background preservation, and 4) overall editing quality. The human evaluation results are listed in Table 3. As can be seen, our method performs the best in all aspects, and the results are consistent with the quantitative experiments. For more detailed information, see Section 8.6."}, {"title": "5.4. Ablation Study", "content": "We conducted an ablation study to evaluate the effectiveness of our image-guided color editing method, highlighting the significance of each component. These components are (1) self-attention map replacement; (2) alignment of the value matrices in the cross-attention layer of the U-net decoder at the early stage of the denoising process; and (3) blending the latent of the reference color image. The results are presented in Table 2, and it is clear that without self-attention map replacement, the structure of the image is broken, leading to the lowest figures of DS and SSIM. Without cross-attention alignment, the structure of the image is well preserved (DS and SSIM are highest) but resulting in the lowest change in the color of the object ($L1_{Hue}^{obj}$ and $L1_{HSV}^{obj}$ are the highest). Without the reference color image blending, the effect of color change improves a little, but with the scrubbing of the loss of structure preservation. Our method can achieve the best balance between the preservation of the structure and the change in the color of the object. A visualization ablation example is given in Section 8.7 of the Supplementary Material."}, {"title": "6. Conclusion", "content": "In this work, we visualized the intermediate process of text-guided image generation and discovered that the shape, contour, and texture of an object are established in the U-Net decoder during the early denoising stage. We identified that cross-attention leakage and attribute collision between the original object and color term are key factors to the failure of text-guided methods in the color change task. Based on those findings, we introduce a training-free image-guided method to edit the color of a subject through the color attribute alignment in the Value matrices in the cross-attention layer of the U-Net decoder in the early denoising stage. Furthermore, we introduce the COLORBENCH, the first benchmark to evaluate the color change task. Although simple in design, comprehensive qualitative and quantitative results demonstrate the effectiveness of our method. Limitation and Future Work. Our approach encounters certain limitations and will be addressed in our future work, which"}]}