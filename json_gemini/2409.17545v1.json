{"title": "Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult", "authors": ["Cheolhun Jang"], "abstract": "Preference optimization methods typically begin training with a well-trained SFT model as a reference model. In RLHF and DPO, a regularization term is used during the preference optimization process to prevent the policy model from devi- ating too far from the reference model's distribution, thereby avoiding the generation of anomalous responses. When the reference model is already well-aligned with the given data or only requires slight adjustments, this approach can pro- duce a well-aligned model. However, if the reference model is not aligned with the given data and requires significant de- viation from its current state, a regularization term may ac- tually hinder the model alignment. In this study, we propose Modulated Intervention Preference Optimization (MIPO) to address this issue. MIPO modulates the degree of interven- tion from the reference model based on how well the given data is aligned with it. If the data is well-aligned, the interven- tion is increased to prevent the policy model from diverging significantly from reference model. Conversely, if the align- ment is poor, the interference is reduced to facilitate more ex- tensive training. We compare the performance of MIPO and DPO using Mistral-7B and Llama3-8B in Alpaca Eval 2.0 and MT-Bench. The experimental results demonstrate that MIPO consistently outperforms DPO across various evaluation sce- narios.", "sections": [{"title": "Introduction", "content": "As the performance of Large Language Models (LLMs) trained with a large amount of data has been attracting atten- tion, methods (Chowdhery et al. 2023; Touvron et al. 2023; Brown et al. 2020) for training them have been actively stud- ied. The commonly used LLM training pipeline is to pre- train LLM using a large amount of data, and then use the instruction-tuning method (Wei et al. 2021) to allow LLM to follow the human-provided instruction.\nHowever, it is difficult to train LLM to produce the de- sired output (helpful, harmless) or to prevent LLM from pro- ducing the output that LLM should not produce (Bai et al. 2022a). Therefore, LLM alignment methods employing hu- man feedback have started to gain significant attention.\nAmong these methods, Reinforcement Learning from Hu- man Feedback (RLHF) (Christiano et al. 2017; Askell et al. 2021) received significant attention. Models trained with RLHF are well-aligned with human feedback, demonstrat- ing reliable performance as a result (Korbak et al. 2023;\nHavrilla et al. 2024; Dai et al. 2023). However, the RLHF approach involves a complex training process, including the training of a reward model, which has posed significant chal- lenges in the implementation and training (Casper et al. 2023; Peng et al. 2023).\nDirect Preference Optimization (DPO) (Rafailov et al. 2024) is a method designed to overcome these limitations. In DPO, the optimization problem of RLHF is modified to eliminate the reward model and train only the policy model. This makes it easier to train DPO compared to RLHF, and DPO also effectively learned human preferences, demon- strating strong performance.\nIn DPO and RLHF, the policy model is trained to align with the instance while ensuring its distribution does not move significantly away from the reference model's distri- bution to prevent it from generating anomalous responses (ex. hallucinations). Therefore, if the reference model is moderately aligned with the given preference pair, it could be possible to train a well-aligned policy model for the given data without significantly diverging from the reference model's distribution. However, if the reference model is not aligned with the given preference pair, it will be difficult for the policy model to align with the data through minor ad- justments, without moving far from the reference model's distribution. Therefore, it is crucial to adjust the training ob- jective based on how well the reference model is aligned.\nIn this paper, we propose a preference optimization algo- rithm, Modulated Intervention Preference Optimization (MIPO), to address this issue. As seen in Figure 2, MIPO utilizes the average log likelihood to measure how well the reference model is aligned with the given preference pair. Through this value, the MIPO objective is configured to modulate the intervention of the reference model, allowing more extensive training on pairs that are judged to be poorly aligned with the reference model. We use Alpaca Eval 2.0 and MT-Bench to compare the performance of MIPO with DPO and other preference optimization methods. Across diverse experimental settings, MIPO consistently achieves outstanding performance. To summarize, MIPO has the fol- lowing properties:\n\u2022 Modulate the intervention of the Reference Model: MIPO is a novel approach that modulates the intervention of the reference model for each instance. It determines the extent of the reference model's intervention based on the degree of alignment. MIPO maintains performance on pairs where the reference model already well-aligned, while simultaneously achieving substantial performance gains on pairs where the reference model previously un- derperformed (Section $4).\n\u2022 Outstanding Benchmark Performance: We conduct experiments using Llama3-8B-Instruct (AI@Meta 2024) and Mistral-7B-Base (Jiang et al. 2023) to verify the ef- fectiveness of MIPO in various models. On Alpaca Eval 2.0, our proposed method consistently outperforms DPO. As we can see in Figure 1, in Llama3-8B-Instruct, it outperforms DPO by approximately 9 points (+36.07%), and in Mistral-7B-Base, it outperforms about 8 points (+54.24%). In most cases, MIPO achieves the best per- formance not only compared to DPO but also when com- pared to other methods. On MT-Bench, MIPO also ex- hibits the best performance among the compared ap- proaches (Section $6.1).\n\u2022 Simple and Effective Training: The high-performance model can be found in MIPO by tuning only the hyper- parameter B. Moreover, consistently outstanding perfor- mance is achieved within a specific range of \u00df, indepen- dent of model architecture or dataset. Thus, unlike other methods that require extensive tuning, this approach al- lows for easy acquisition of a high-performance model with minimal tuning effort (Section $6.2)."}, {"title": "Related Works", "content": "After being pretrained on a large amount of data (Chowd- hery et al. 2023) and fine-tuned (Chung et al. 2024; Ra- mamurthy et al. 2022), LLMs have achieved notable per- formance across many tasks (Touvron et al. 2023; Brown et al. 2020; Thoppilan et al. 2022). However, LLMs that could generate responses that were even more helpful and harmless were needed, leading to the development of prefer- ence optimization methods (Christiano et al. 2017; Bai et al. 2022a,b) that fine-tune LLMs more closely to human feed- back.\nRLHF (Askell et al. 2021; Ouyang et al. 2022) is one such preference optimization method for LLM alignment. In RLHF, preference data is used to train a reward model, which is then utilized to optimize the policy model by Prox- imal Policy Optimization (Schulman et al. 2017). RLHF ef- fectively aligns models with human feedback, resulting in good performance (Korbak et al. 2023; Havrilla et al. 2024). However, there are challenges, such as the difficulty of ob- taining scored data, ensuring stable training, and the neces- sity of training a reward model (Casper et al. 2023; Peng et al. 2023; Wang et al. 2024).\nDPO is a preference optimization method that solves op- timization problem of RLHF in a more easier and efficient manner. (Rafailov et al. 2024) proposed DPO to eliminate the reward model in RLHF and train only the policy model with preference data. It is simple compared to RLHF, and the training phase is more stable. So it has become one of the widely used method for aligning language models. How- ever, DPO also has its drawbacks like dependency on the reference model and issues with length exploitation (Liu, Liu, and Cohan 2024; Gorbatovski et al. 2024; Xu et al. 2024). Therefore, new model alignment methods such as KTO (Ethayarajh et al. 2024), IPO (Azar et al. 2024) and ORPO (Hong, Lee, and Thorne 2024) continue to emerge.\nHowever, most methods including DPO does not take into account the differences in the degree of alignment of the reference model between preference pairs. As mentioned earlier, if the reference model is already well-aligned, only minimal training will be needed to achieve alignment. Con- versely, if the reference model is completely misaligned, ex- tensive training will be required. However, DPO does not account for these differences (Section $3.3).\nTo address this issue, we propose MIPO, which varies the learning weights among instances by modulates the degree of intervention from the reference model (Section $4)."}, {"title": "Background", "content": "In this section, we will review the DPO in Section $3.2, and analyze the ineffective aspects of DPO in Section $3.3."}, {"title": "Terminology", "content": "$D = \\{x^i, Y_\\omega^i, Y_\\iota^i\\}_{i=1}^N$ is for pairwise-preference dataset, where $x^i$ is prompt and $Y_\\omega^i$ is chosen (preferred) response and $Y_\\iota^i$ is rejected (dis-preferred) response for that prompt. $\\pi_{ref}$ is reference model, initial LLM that we start training from. $\\pi_\\theta$ is policy model, which is a model we train."}, {"title": "DPO", "content": "DPO employs the Bradley-Terry (BT) model (Bradley and Terry 1952) to represent the distribution of human prefer- ence. BT model represents human preference distribution for $y_\\omega, y_\\iota$ by the reward function as follows:"}, {"title": "Ineffective Aspects of DPO", "content": "DPO does not consider how well the preference pairs are aligned. Looking at the reward of DPO in Eq (2) without $Z(x)$. It can be seen that the reward is the difference between the log likelihood of the policy model and the log likelihood of the reference model. This implies that DPO al- lows for high rewards to be obtained solely by increasing the log likelihood of a response, without considering the de- gree to which the reference model already performs well on that response. Consequently, the training process proceeds without taking into account the extent to which the reference model is aligned with the give preference data.\nFor example, consider pair1, preference data where the reference model already well-aligned, and pair2, where it does not. Ideally, model will require to train slightly on pair\u2081 to maintain its current performance, while it will re- quire substantial training for pair2 compared to pair1.\nLet's assume that the policy model has been trained so that the log likelihood of the chosen response increases by \u03b1 compared to the reference model, while the log likeli- hood of the rejected response remains unchanged in both pairs ($log\\pi_\\theta(Y_\\omega|x) \u2013 log\\pi_{ref}(Y_\\omega|x) = \\alpha, log\\pi_\\theta(y_\\iota|x) \u2013 log\\pi_{ref}(y_\\iota|x) = 0$). In DPO, both pairs would yield the same loss by Eq (3). This implies that the improvement in log likelihood for pair\u2081 and pair2 holds equal significance in DPO.\nConsequently, DPO trains the model without discriminat- ing between instances of strong and weak alignment with the reference model. This uniform approach can result in in- sufficient training for pairs where the reference model needs improvement and excessive training for pairs where prefer- ences are already adequately captured. Therefore, this issue can negatively impact the performance of the trained model."}, {"title": "Methodology", "content": "In this section, we explain why we use average log like- lihood to determine how well reference model is aligned to data in Section $4.1. Then we introduce Modulated In- tervention Preference Optimization (MIPO), an algorithm that adjusts the degree of intervention from the reference model based on the level of alignment in Section $4.2."}, {"title": "Measuring the Alignment Degree", "content": "To solve the problem of DPO mentioned above Section $3.3, we first need to measure which pairs are well-aligned to ref- erence model and which pairs are poorly aligned.\nIn the context of preference learning, being \"well- aligned\" can be interpreted as the model being more likely to generate a chosen response $y_\\omega$ than a rejected response $y_\\iota$ for a given input $x$. However, using the difference in log likelihoods between chosen and rejected responses to mea- sure alignment is not feasible, as log likelihood is highly sen- sitivity to response length. If the lengths of the chosen and rejected responses differ significantly, the longer response's log likelihood will be disproportionately lower, regardless of individual token probabilities.\nTherefore, we decide to use of average log likelihood. It allows for a more fairer comparison of generation probabil- ities between chosen and rejected responses, mitigating the impact of length discrepancies. We have decided to use the difference in average log likelihood, $K$, as a metric to assess the alignment of the reference model with a given pair.\n$K = \\frac{log\\pi_{ref}(Y_\\omega|x)}{|Y_\\omega|} - \\frac{log\\pi_{ref}(y_\\iota|x)}{|y_\\iota|}$\nWe interpret a high $K$ value as indicative of strong align- ment in the reference model, whereas a low $K$ value suggest insufficient alignment. Based on this assumption, we pro- pose our objective as follows:"}, {"title": "Deriving the MIPO Objective", "content": "$L_{MIPO}(\\pi_\\theta; \\pi_{ref}) = E_{(x,y_\\omega,y_\\iota) \\sim D}\\left[-\\frac{log\\sigma(\\beta \\left(\\frac{log\\pi_\\theta(y_\\omega|x)}{|Y_\\omega|} - \\frac{log\\pi_\\theta(y_\\iota|x)}{|y_\\iota|}\\right))}{q(K)}\\right]$\nFor the reasons mentioned above, the MIPO objective is designed to enhance the alignment of the policy model by using average log likelihood, $f(0)$. Additionally, it is adjusted based on the degree of alignment through $q(K)$, which acts as a modulator for the degree of intervention from the reference model.\nLet's examine the MIPO objective in two cases:\nWhen reference model is well aligned for a given pair It means $K$ is large enough. Then, $q(K)$ converges to $K$ and the objective of MIPO can be expressed as follows.\n$L_{MIPO} = -log\\sigma(\\beta\\left(\\frac{log\\pi_\\theta(y_\\omega|x)}{|Y_\\omega|} - \\frac{log\\pi_\\theta(y_\\iota|x)}{|y_\\iota|}\\right))$ $\n\\  - \\beta(\\frac{log\\pi_{ref}(Y_\\omega|x)}{|Y_\\omega|} - \\frac{log\\pi_{ref}(y_\\iota|x)}{|y_\\iota|}))$\nThe objective is calculated based on the difference be- tween the policy model's average log likelihood difference, $f(0)$, and this values of reference model, $K$. Therefore, as $f(0)$ diverges further from $K$, the loss decreases, prevent- ing the policy model from significantly diverging from the reference model.\nWhen reference model is poorly aligned for a given pair It means $K$ is low. In this case, $q(K)$ approaches to 0 and objective can be expressed as follows.\n$L_{MIPO} = -log\\sigma(\\beta\\left(\\frac{log\\pi_\\theta(Y_\\omega|x)}{|Y_\\omega|} - \\frac{log\\pi_\\theta(y_\\iota|x)}{|y_\\iota|}\\right))$\nSince the MIPO objective does not include a term for the reference model, it only considers the $f(0)$ for align- ment, focusing solely on increasing this value. When com- pared to the case where $q(K) = K$, it is clear that the MIPO loss significantly greater because $f(0)$ is less than $f(0) \u2013 K(. : K < 0)$. Consequently, the policy model can be trained while diverging further from the distribution of the reference model.\nIn summary, the MIPO assesses how well the reference model is aligned with the given instance through the met- ric $K$. This metric is then used to calculate $q(K)$, which determines the extent to which the reference model's influ- ence on the policy model's learning. When $K$ is high, it in- dicates strong alignment with the given data. In this case, $q(K)$ takes on the value of $K$, thereby increasing the in- tervention of the reference model. Consequently, the policy model train without diverging significantly from the refer- ence model. Conversely, if $K$ is low, $q(K)$ becomes zero, allowing the policy model to train without intervention from the reference model.\nMore detailed explanations about objective are provided in the Section $6.4 and gradient analysis can be found in Appendix A."}, {"title": "Experimental Settings", "content": "In this section, we will provide more experimental setting\n5.1 Datasets\nBinarized UltraFeedback We train models with Bina- rized UltraFeedback Dataset (Cui et al. 2023). It consists of 64K preference pairs from diverse resources.\nLlama3 UltraFeedback Because there is a possibility that Binarized Ultrafeedback data was used in the training phase of Llama3-8B-instruct, (Meng, Xia, and Chen 2024) proposed new dataset. The data is created base on responses generated by Llama3-8B-Instruct by using the Binarized Ul- trafeedback prompts. Among these responses, the highest scoring response and the lowest scoring response, which are scored by reward model (Jiang, Ren, and Lin 2023), are used to form preference pairs. In this study, models trained using this dataset is labeled with the v0.1 tag.\n5.2 Evaluation\nThe trained models are evaluated on AlpacaEval2.0 and MT- Bench."}, {"title": "Benchmark Results", "content": "As shown in Table 1, MIPO consistently achieves higher scores compared to DPO and demonstrates outstanding per- formance relative to other methods in the most cases.\nComparative analysis using Alpaca Eval 2.0 reveals that MIPO consistently and significantly outperforms DPO"}, {"title": "Performance Based on \u03b2", "content": "One of the advantages of MIPO is the ease of hyperparam- eter tuning. MIPO objective contains only a single hyperpa- rameter, B, allowing for optimal model training by adjusting just this one. Figure 3 illustrates how the model's perfor- mance varies with different \u1e9e in Mistral-7B and Llama-8B.\nAs depicted in Figure 3, MIPO maintains exceptionally high performance across a similar beta range ([5,50]), demon- strating robustness across various models and datasets. The optimal model configuration is consistently identified within this range.\nIn conclusion, MIPO demonstrates a significant advan- tage: it consistently produces models that substantially outperform DPO and approach optimal performance lev- els, achieved through the tuning of a single hyperparam- eter, B, within a moderate range. This capability persists"}, {"title": "Analysis about Average Log Likelihood", "content": "Figure 4, represents the average log likelihood difference be- tween chosen and rejected responses for the model on the evaluation dataset, showing how this difference changes af- ter training. It specifically highlights how the values for in- stances in the top 20% and bottom 20% of average log like- lihood differences in reference model have evolved.\nAt this point, the top 20% are instances with a large aver- age log likelihood difference in reference model, indicating they are already well-aligned data, while the bottom 20% are poorly aligned and require more training.\nIn the bottom 20%, the average log likelihood difference for DPO actually decrease, whereas for MIPO, the average log likelihood clearly increase. Conversely, in the top 20%, the average log likelihood for DPO increase significantly, while for MIPO, it only increase slightly. This pattern is ob- served in both the Llama3-8B and Mistral-7B.\nThis indicates that in DPO, the data that is already well- aligned continued to be better aligned through further train- ing, while the data that is not well-aligned do not see signif- icant improvement. However, in MIPO, the training is oper- ated to maintain performance on well-aligned data while sig- nificantly improving the alignment of poorly aligned data. MIPO achieves the intended outcome described in Section $4.2, thereby effectively enhancing model alignment."}, {"title": "Analysis about MIPO objective function", "content": "As seen in Eq (5), the MIPO objective can be expressed as the difference between the average log likelihood of the cho- sen response and rejected response in policy model and mi- nus $q(K)$ consists of values calculated from the reference model.\nLet's examine how the MIPO objective behaves during the training process in two scenarios."}, {"title": "Conclusion", "content": "In DPO, rewards are calculated based on the reference model for all pair data without considering how well the reference model is aligned with the given pair data. Therefore, DPO does not distinguish between instances that require more training ant those that only need minimal training. In this paper, we proposed Modulated Intervention Preference Optimization (MIPO) as a method to address and improve upon this issue. MIPO adjusts the objective based on the de- gree of alignment of the reference model on the given in- stances. For pairs that require more learning, MIPO reduces the intervention of the reference model, allowing the policy model to diverge from it and find better weights. Conversely, for pairs that are better aligned, the intervention of the ref- erence model is maintained, ensuring that the policy model does not significantly diverge from the reference model.\nThrough experiments, we found that models trained us- ing MIPO demonstrated significantly improved performance compared to those trained using DPO. Moreover, we ob- served a notable increase in the average log likelihood dif- ference for instances with initially small differences from the reference model, aligning with our expectations compared to DPO."}, {"title": "Limitations & Future Work", "content": "While MIPO achieved sig- nificantly better performance than DPO, there are still sev- eral areas for improvement and further investigation\nAverage log likelihood is not an absolute measure of the degree of alignment The degree of preference between the chosen and rejected responses can vary for each prefer- ence pair. In some cases, the chosen and rejected responses might be decided by a very subtle difference, while in others, the difference could be significant. If a given preference pair has only a slight difference, the model may be well-aligned, but the average log probability difference (K) is unlikely to be large. Therefore, it is difficult to accurately assert that a large K indicates superiority on a particular preference pair. The K alone does not provide an absolute measure of per-formance across different preference pairs.\nAlthough MIPO does not account for the difficulty differ- ences between preference pairs, it is likely that pairs where the model was poorly aligned improved more, as higher av- erage log likelihoods typically indicate better performance for each pair.\nExperiments on Various Adaptation Terms While we used $ln(1 + e^K)$ to construct the loss for MIPO, there are various functions that could be used as adaptation term. To achieve the same effect as MIPO, the function $q$ should use an alternative function that converges to K when K is large and to 0 when K is small. However, experiments exploring all possible functions for this have not yet been conducted."}]}