{"title": "PretextTrans: Investigating Medical Factual Knowledge Mastery of LLMs with Predicate-text Dual Transformation", "authors": ["Yuxuan Zhou", "Xien Liu", "Chen Ning", "Ji Wu"], "abstract": "In the study, we aim to investigate current LLMs' mastery of medical factual knowledge with a dynamic evaluation schema, which can automatically generate multiple test samples for each medical factual knowledge point. Test samples produced directly by LLMs always introduce factual errors and lack diversity in the manner of knowledge expression. To overcome the drawbacks, here we propose a novel evaluation method, Predicate-text Dual Transformation (PretextTrans), by introducing predicate transformations into the dynamic evaluation schema. Specifically, each medical knowledge point is firstly transformed into a predicate expression; then, the predicate expression derives a series of variants through predicate transformations; lastly, the produced predicate variants are transformed back into textual expressions, resulting in a series of test samples with both factual reliability and expression diversity. Using the proposed PretextTrans method, we systematically investigate 12 well-known LLMs' mastery of medical factual knowledge based on two medical datasets. The comparison results show that current LLMs still have significant deficiencies in fully mastering medical knowledge, which may illustrate why current LLMS still perform unsatisfactorily in real-world medical scenarios despite having achieved considerable performance on public benchmarks. Our proposed method serves as an effective solution for evaluation of LLMs in medical domain and offers valuable insights for developing medical-specific LLMs.", "sections": [{"title": "1 Introduction", "content": "Recent years have witnessed the rapid advancement of large language models (LLMs), which have exhibited potential across various domains (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023; OpenAI, 2023; Madani et al., 2023; Boiko et al., 2023), including medicine. Solving medical problems requires LLMs to master medical factual knowledge comprehensively and in-depth. Recent studies (Singhal et al., 2023; Nori et al., 2023; Pal and Sankarasubbu, 2024) showed that some LLMs (e.g., GPT-4) encode substantial medical factual knowledge, significantly outperforming previous SOTAS across multiple medical benchmarks (e.g., MedQA (Jin et al., 2021)). However, these LLMs are found to perform unsatisfactorily on real-world medical tasks (Thirunavukarasu et al., 2023; Clusmann et al., 2023; Wornow et al., 2023), falling far short of their benchmark performance. This indicates that current benchmarks do not accurately and comprehensively reflect LLMs' mastery of medical factual knowledge. Therefore, we aim to develop a new evaluation method that more precisely and comprehensively investigates LLMs' mastery of medical factual knowledge.\nCurrent evaluations of LLMs' medical knowledge mastery primarily rely on medical benchmarks (Jin et al., 2019, 2021; Pal et al., 2022; Singhal et al., 2023; Sung et al., 2021; Meng et al., 2022), which are reliable but not comprehensive enough for LLM evaluation. Although some newer benchmarks (He et al., 2023; Cai et al., 2024) address this issue by collecting the latest data from diverse sources, constructing these benchmarks can be costly, and they will face problems such as becoming outdated or leaked to LLMs over time. Recently, several researchers have developed a series of methods (Zhu et al., 2023; Li et al., 2024; Zhu et al., 2024b) to dynamically generate test samples for LLM evaluation, effectively avoiding issues of outdated data and leakage. Therefore, dynamically generating multiple test samples based on each knowledge point in medical knowledge resources is a promising way to comprehensively evaluate LLMs' medical knowledge mastery. A straightforward method is to directly generate test samples using LLMs based on knowledge points. However, this method has two drawbacks as illustrated in Figure 1: (1) factual error introduction: factual errors (e.g., incorrect relations) may be introduced during sample generation, affecting the reliability of evaluation; and (2) low diverse expression: samples generated from the same knowledge point primarily differ in wording (e.g., synonym replacement) rather than in knowledge expression structure, compromising the diversity of evaluation.\nThe purpose of this study is to comprehensively investigate LLMs' mastery of medical factual knowledge using a dynamical evaluation method. Because medical factual knowledge primarily involves relationships between medical entities, it can be effectively expressed through predicates. Inspired by this, we propose a Predicate-text Dual Transformation method (PretextTrans) that dynamically generates multiple test samples based on the medical knowledge points being evaluated. Figure 2 presents the schema of our method. Specifically, we first express each knowledge point using a predicate expression. Then, we derive a series of structurally diverse variants from this predicate expression through logical implication. Finally, an LLM is employed to transform these variants back to the textual space for generating test samples. The logical implication process ensures the structural diversity of generated test samples and also effectively prevents the introduction of factual errors. Additionally, the LLM-based predicate-to-text transformation ensures that the generated samples are fluent and natural, while also enhancing their syntactic and lexical diversity.\nUsing the proposed method, we conduct a systematic medical knowledge evaluation of current LLMs based on two medical datasets. Experimental results show that the performance of current LLMs on the multi-sample datasets generated by our method, where each knowledge point is evaluated by multiple samples, is much lower than those on the original single-sample datasets. Furthermore, these LLMs exhibit inconsistency in handling test samples derived from the same knowledge point, failing to achieve the expected performance. These findings indicate that current LLMs have not comprehensively mastered medical factual knowledge, failing to perform satisfactorily in real-world medical scenarios. Our contributions are summarized as follows:\n\u2022 We introduce a dynamic evaluation method (PretextTrans) for comprehensively evaluating LLM medical factual knowledge mastery. Our method generates a series of diverse and reliable test samples for each knowledge point using predicate-text dual transformation.\n\u2022 Employing the proposed method, we systematically investigate the medical factual knowledge mastery of 12 well-known LLMs.\n\u2022 Furthermore, we compare LLMs' performance on samples derived from different types of logical implications, shedding light on developing medical foundation models."}, {"title": "2 Related Work", "content": "LLM Medical Evaluation Current medical evaluation benchmarks for LLMs can be divided into two categories: (1) QA datasets that evaluate LLMs' comprehensive medical capabilities with questions collected from medical literature (Jin et al., 2019), exams (Jin et al., 2021; Pal et al., 2022), or online websites (Singhal et al., 2023); (2) datasets for probing LLM medical knowledge mastery (Sung et al., 2021; Meng et al., 2022). These static benchmarks are meticulously created by medical experts and possess high reliability. However, they may face problems such as becoming outdated or leaked to LLMs, affecting the comprehensiveness of evaluation. While constructing new benchmarks can alleviate these problems, they will also become obsolete over time.\nDynamic Evaluation Schema Several studies have proposed dynamic evaluation methods that automatically generate new test samples, effectively avoiding data obsolescence and leakage issues. Some works leverage algorithms to dynamically generate test samples for specific tasks, such as mathematics (Zhu et al., 2024a) and SQL execution (Lei et al., 2023). Others (Zhu et al., 2023, 2024b) generate test samples by paraphrasing existing benchmarks. However, there is currently no related work utilizing dynamic evaluation methods to evaluate LLMs' factual knowledge mastery. To our knowledge, our proposed method is the first to apply the dynamic evaluation schema for evaluating LLMs' mastery of medical factual knowledge."}, {"title": "3 Method", "content": "3.1 Evaluation Schema\nIn this section, we introduce the schema of our PretextTrans method, which generates more diverse and reliable test samples for LLM factual knowledge evaluation. Given a knowledge point P, a straightforward idea is to directly generate a test sample using an LLM:\n$S = G_{LLM}(P)$ (1)\nHere, $G_{LLM}$ denotes the LLM generation process, and S refers to the generated test sample. As introduced above, $G_{LLM}$ may create samples that lack diversity and reliability. In contrast, our method first expresses the knowledge point using a predicate expression and then derives a series of variants via logical implication:\n$p = T_{text2pre}(P)$ (2)\n$[q_1, q_2,\\cdots, q_K] = T_{Imp}(P)$ (3)\nHere, $T_{text2pre}$ denotes a mapping that projects the original knowledge point P into the predicate expression p. $T_{Imp}$ refers to the logical implication, and ${q_i}_{1}^{K}$ are the variants derived from the original expression p. The property of logical implication ensures the reliability of these variants, provided that the original expression p is true:\n$(p = T) \\Rightarrow (q_i = T), 1 \\leq i \\leq K$ (4)\nFinally, we convert each predicate variant back to a textual test sample for evaluation:\n$S_i = T_{pre2text}(q_i), 1 \\leq i \\leq K$ (5)\nHere, $T_{pre2text}$ maps each predicate variant $q_i$ into a corresponding test sample (textual variant). Since these samples are derived from predicate variants with diverse structures, the predicate-text duality ensures they exhibit substantial diversity while maintaining reliability.\n3.2 Evaluation Framework\nBuilding on the proposed evaluation schema, we develop a novel evaluation framework to evaluate LLMs' mastery of medical factual knowledge comprehensively. Figure 3 presents an overview of this framework.\n3.2.1 Predicate Variant Generation\nA single knowledge point can be denoted as P = (A, R, B), where A, R, and B refer to the head entity, the relation, and the tail entity, respectively. In predicate logic, such a relation can be effectively presented by:\n$p = R(A, B)$ (6)\nHere, R(x, y) is a predicate derived from the relation R, representing the statement \"x has the relation R with y\". p represents its value at the point (A, B). Next, the framework employs three types of logical implications that are widely employed in practical medical applications."}, {"title": "3.2.2 Textual Sample Generation", "content": "A straightforward method to generate test samples from predicate variants is by directly prompting LLMs. However, this method may also introduce factual errors, affecting the reliability of the generated samples. To address this issue, we designed a prototype-based sample generation strategy, as depicted in Figure 4. Specifically, for each predicate variant $T_{imp}^{i}(R(A, B))$, we initially retrieve the corresponding prototype from a pre-constructed prototype pool based on the predicate $T_{imp}R$. For predicate variants obtained through double negation, we retrieve prototypes based on their negated form (i.e., single negation form) to generate negated samples for LLM evaluation. Subsequently, the prototype is instantiated by the arguments (A, B). The instantiated prototype precisely conveys the predicate variant in the textual space. Finally, the prototype is further rephrased by an LLM to obtain the final test sample $S_i$. Since current LLMs possess strong language capabilities and rarely make mistakes in sentence rephrasing, the proposed sample generation strategy can ensure the reliability and diversity of the generated samples.\n3.2.3 Evaluation Metrics\nIn our framework, we evaluate LLMs using statement verification tasks, asking them to determine whether a given statement is true or false:\n$Score(M, S_i) = \\mathbb{1}(M(S_i) = l_i), 1 \\leq i \\leq K$ (8)\nHere, M is the evaluated LLM, $S_i$ is the textual variant (statement) generated by our framework, and $M(S_i) \\in \\{T, F\\}$ denotes LLM's prediction for $S_i$. $l_i \\in \\{T, F\\}$ is the label of $S_i$, and the function $\\mathbb{1}(\\cdot)$ is a characteristic function that equals 1 when the enclosed expression is true, and 0 otherwise. For a dataset with N knowledge points ${P_j}_{j=1}^N$, we initially use the metric average accuracy to compute the accuracy across all test samples:\n$d_{avg} = \\frac{1}{N K} \\sum_{j=1}^N \\sum_{i=1}^K Score(M, S_{ij})$ (9)\nHere, $S_{ij}$ denotes the $i^{th}$ test sample derived from the $j^{th}$ knowledge point $P_j$. While this metric is widely applied in various benchmarks, it cannot evaluate the consistency of LLMs in predicting all test samples derived from the same knowledge point, which is crucial for high-risk applications in the medical domain. Therefore, we also utilize another metric, joint accuracy, which considers a knowledge point as mastered if all the related samples are predicted correctly:\n$A_{joint} = \\frac{1}{N} \\sum_{j=1}^N \\prod_{i=1}^K Score(M, S_{ij})$ (10)\nBy applying these metrics, we can achieve a comprehensive evaluation of LLMs' mastery of medical factual knowledge."}, {"title": "4 Experiments", "content": "4.1 Experiment Setup\nDatasets Introduction To investigate the mastery of medical factual knowledge in current LLMs, we applied the proposed framework to two datasets: a biomedical evaluation benchmark MedLAMA (Meng et al., 2022) and a clinical knowledge base DiseK (Zhou et al., 2024). MedLAMA is a large-scale biomedical evaluation benchmark containing 39,053 knowledge triplets across 19 relations, all manually selected from the UMLS Metathesaurus (Bodenreider, 2004) to ensure high quality. DiseK is a clinical knowledge base containing 24,413 triplets, covering 1,000 high-frequency diseases across four crucial relations related to disease diagnosis and treatment. Mastering this disease-related knowledge is essential for LLMs to be applicable in real medical scenarios.\nConsidering computational costs and dataset size, we select a subset from each dataset for evaluation. Specifically, we randomly select a single entity from the corresponding tail entities for each pair of a head entity and a relation. This approach aims to reduce the evaluation scale while maximizing the diversity of the evaluated knowledge. We also excluded two relations in MedLAMA, which are the inversion of the other two relations in MedLAMA. Furthermore, for each head-relation pair (A, R), we randomly sample a negative entity C that satisfies $\\neg R(A, C)$ to create a negative triplet (A, R, C). Test samples generated from this negative triplet possess a similar structure to those generated from the positive triplet but with opposite labels. By introducing negative triplets, we can further evaluate the ability of LLMs to discern non-knowledge, which is also essential for practical application. \nMethod Setting To ensure the diversity of evaluation, we combined the three types of logical implication and generated K = 8 expressions (variants) for each knowledge point, including the original expression. We crafted a prototype for each combination of relation and logical implication type to generate test samples. Moreover, we utilize Llama3-70B-Instruct (AI@Meta, 2024) to rephrase the instantiated prototypes since it exhibits strong performance on LLM leaderboards. \nFor LLM evaluation, we employ the popular 5-shot in-context learning setting (Brown et al., 2020), where five examples are presented before the test sample, guiding LLMs to produce answers in consistent format with the provided examples.\nBaselines We initially compare our method with the original datasets. For original datasets, we leverage the templates provided in the benchmarks to generate statements for evaluation. We also implemented a dynamic evaluation baseline (named as LLMEval) that directly generates test samples from triplets using an LLM. Specifically, we prompt Llama3-70B-Instruct\u00b9 to generate K = 8 statements, presenting the given triplet in different ways. We carefully crafted the prompt to ensure maximum diversity in generated samples.\nEvaluated LLMS In our study, we evaluate 12 well-known general and medical-specific LLMs: (1) general LLMs: ChatGLM3-6B (Du et al., 2022), Gemma-7B (Team et al., 2024), Llama2 (7B,70B) (Touvron et al., 2023), Llama3 (8B,70B) (AI@Meta, 2024), Vicuna (7B,13B) (Zheng et al., 2023), and GPT-3.5-turbo (Ouyang et al., 2022); (2) medical-specific LLMs: ClinicalCamel-70B (Toma et al., 2023), Meditron-70B (Chen et al., 2023) and Med42-70B (Christophe et al., 2023). We have not evaluate LLMs that are either too expensive (e.g., GPT-4 (OpenAI, 2023)) or not publicly available (e.g., MedPaLM (Singhal et al., 2023))."}, {"title": "4.2 Results", "content": "4.2.1 Comparison Study\nWe first conduct a comparison study across different evaluation methods and LLMs. Experimental results demonstrate that all evaluated LLMs achieve much lower performance on datasets generated by PretextTrans compared to the original datasets. This suggests that dynamically generating multiple samples for each knowledge point can significantly enhance the comprehensiveness of evaluation. Moreover, compared to datasets directly generated by an LLM (LLMEval), almost all LLMs achieve lower performance on datasets created by PretextTrans, with some models (e.g., ChatGLM3-6B and GPT-3.5-turbo) experiencing over 10% degradation. These findings indicate that PretextTrans is capable of generating test samples that are more comprehensive than those directly generated by LLMs.\nAmong all the evaluated LLMs, Llama3-70B outperforms the others across all datasets and evaluation methods, achieving accuracies of 76.9 and 70.9 evaluated by PretextTrans. Llama3-8B also performs best on PretextTrans-generated datasets among LLMs with around 10B parameters, even slightly surpassing the 10x larger Llama2-70B. These results indicate that Llama3 model series encodes significantly more medical knowledge than other evaluated LLMs. Additionally, while some medical-specific LLMs (ClinicalCamel, Med42) perform similarly to their backbone model (Llama2-70B) on original datasets, they notably outperform it by around 7% on PretextTrans-generated datasets. This suggests that training on medical corpora can notably improve the depth of medical knowledge mastery.\nWe also study the joint accuracies of LLMs evaluated by increasing numbers of expressions per knowledge point. The results of seven typical LLMs are illustrated in Figure 5, with the full results provided in Appendix E. To eliminate the impact of sample addition orders, we enumerate all possible orders and averaged the results. Therefore, the value at x = i corresponds to the expected joint accuracy evaluated with any i samples. We observe that the results from LLMEval and PretextTrans are quite close when using a single sample for evaluation. However, as the number of test samples increases, the difference between the results from the two methods grows notably larger. This phenomenon indicates that current LLMs generally exhibit significant lower consistency when confronted with structurally diverse test samples generated by our method compared to samples directly generated by LLMs. Moreover, as the number of expressions increases, Llama3-70B exhibits a slower decline in performance compared to other LLMs, indicating a more consistent understanding of diverse expression structures from the same knowledge points. Nevertheless, there is still room for improvement in current LLMs' mastery of medical knowledge."}, {"title": "4.2.2 Effectiveness Analysis", "content": "Effect of framework components First, we conduct an ablation study to analyze the contribution of each component to our proposed framework.  Here, we focus on the logical implication (LogImp) and the LLM rephrasing (LMReph) modules that are designed to increase the diversity of test samples. We observe that removing these two modules results in higher evaluation performance, especially when the logical implication module was removed (around 7%). These results indicate that the logical implication module contributes most to the evaluation diversity in the proposed framework.\nEffect of Implication Types We further conduct a fine-grained analysis of the logical implication types applied in our framework, with results presented."}, {"title": "4.2.3 Case Study", "content": "We also conduct a case study to examine the effectiveness of the proposed PretextTrans framework.  The case shows that Llama3-70B correctly answers LLMEval-generated samples that have the same knowledge expression structure. In contrast, the PretextTrans-generated samples possess distinct expression structures, and some of them cannot be correctly answered by Llama3-70B. These findings indicate that the proposed PretextTrans framework effectively increases the diversity of knowledge expression structures in generated samples, enabling a more comprehensive evaluation of LLMs' true mastery of medical knowledge."}, {"title": "5 Conclusion and Discussion", "content": "In this paper, we comprehensively investigate LLMs' mastery of medical factual knowledge by designing a dynamical evaluation method named PretextTrans. The proposed method leverages predicate-text dual transformation to dynamically generate multiple test samples for each knowledge point in medical knowledge resources, ensuring their reliability and structural diversity. The experimental results indicate that current LLMs lack comprehensive mastery of medical factual knowledge; thus, they are not yet competent for real-world medical tasks. Furthermore, these LLMs exhibit inconsistency in understanding diverse expressions derived from the same medical knowledge point, thus limiting their practical application in the medical domain. These findings demonstrate that our method can serve as an effective solution to comprehensively evaluate LLMs' medical knowledge mastery. Our study may also shed light on developing medical foundation models. For example, incorporating content that presents the same medical knowledge in diverse ways into the training data may improve LLMs' consistency and comprehensiveness in understanding medical concepts. In the future, we aim to integrate this method with other evaluation forms (e.g., question answering) and medical datasets to conduct a more comprehensive evaluation of LLM medical knowledge mastery."}, {"title": "Limitations", "content": "One limitation of our study is that, despite evaluating several well-known general and medical-domain-specific LLMs, we excluded some notable models like GPT-4 and MedPaLM. This was due to either their high costs (it would require $1200 to evaluate GPT-4 on MedLAMA) or their unavailability for public access (e.g., MedPaLM). We plan to evaluate other LLMs in the future if feasible.\nAdditionally, although our evaluation method has the potential to be applied in other domains, it was initially devised and validated for the medical domain. Applying it to other domains may require further validation."}]}