{"title": "Sub-SA: Strengthen In-context Learning via Submodular Selective Annotation", "authors": ["Jian Qian", "Miao Sun", "Sifan Zhou", "Ziyu Zhao", "Ruizhi Hun", "Patrick Chiang"], "abstract": "In-context learning (ICL) leverages in-context examples as prompts for the predictions of Large Language Models (LLMs). These prompts play a crucial role in achieving strong performance. However, the selection of suitable prompts from a large pool of labeled examples often entails significant annotation costs. To address this challenge, we propose Sub-SA (Submodular Selective Annotation), a submodule-based selective annotation method. The aim of Sub-SA is to reduce annotation costs while improving the quality of in-context examples and minimizing the time consumption of the selection pro- cess. In Sub-SA, we design a submodular function that facilitates effective subset selection for annotation and demonstrates the char- acteristics of monotonically and submodularity from the theoreti- cal perspective. Specifically, we propose RPR (Reward and Penalty Regularization) to better balance the diversity and representativeness of the unlabeled dataset attributed to a reward term and a penalty term, respectively. Consequently, the selection for annotations can be effectively addressed with a simple yet effective greedy search algorithm based on the submodular function. Finally, we apply the similarity prompt retrieval to get the examples for ICL. Compared to existing selective annotation approaches, Sub-SA offers two main advantages. (1.) Sub-SA operates in an end-to-end, unsupervised manner, and significantly reduces the time consumption of the se- lection process (from hours-level to millisecond-level). (2.) Sub-SA enables a better balance between data diversity and representativeness and obtains state-of-the-art performance. Meanwhile, the theoreti- cal support guarantees their reliability and scalability in practical scenarios. Extensive experiments conducted on diverse models and datasets demonstrate the superiority of Sub-SA over previous meth- ods, achieving millisecond(ms)-level time selection and remarkable performance gains. The efficiency and effectiveness of Sub-SA make it highly suitable for real-world ICL scenarios.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) can rapidly adapt to various down- stream tasks from just a few examples provided in the prompt without parameter update [42, 43, 1] while improving accuracy [14, 8]. This remarkable ability is called in-context learning (ICL). Unlike tradi- tional methods [9, 6, 7] that rely on task-specific data to update"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 In-context Learning", "content": "In-context learning (ICL) is a pivotal capability behind the astounding performance of Large Language Models (LLMs). The combination of test input with a few input-output demonstrations allows these models to undertake a variety of tasks without updating parameters. Formally, a k-shot prompt for ICL consists of k examples, given a test sample"}, {"title": "($X_{\\text{test}}$, $Y_{\\text{test}}$), LLMs predicts $\\hat{y}$ based on the in-context prompts and input $X_{\\text{test}}$:", "content": "$\\hat{y} = \\text{LLM}(e_k, ..., e_1, X_{\\text{test}})$ (1)\nwhere $e_i = (x_i, Y_i)_{i=1}^k$ represents an sequence consisting of input- output pairs, k denotes the shot number and is the operation of concatenation. It is paramount to optimize the in-context prompts by seeking the ideal example set {$e_k,..., e_1$} in A for $X_{\\text{test}}$, aiming to make the LLM\u2019s prediction $\\hat{y}$ match the ground truth $Y_{\\text{test}}$, where A = {($X_i, Y_i$)}^N_{i=1} is the annotated examples and T is the fixed budget."}, {"title": "2.2 Submodular Subset Selection", "content": "Considering a finite set V, given a set function $F: 2^V \\rightarrow R$ that maps any subset A\u2282V and B\u2282V to a real value. When F is a submodular function, its definition is as follows:"}, {"title": "Definition 1 (Submodular Function [45]).", "content": "For any set A \u2286 B \u2286 V. Given an element a, where a \u2208 V\\B. The set function F is a submodular function when it satisfies monotonically non-decreasing:\nF(A\u222a{a}) - F (A) \u2265 0 (2)\nAnd:\nF(A\u222a{a}) - F (A) > F (B\u222a {a}) \u2013 F (B). (3)"}, {"title": "2.3 Selective annotation Problem Setting", "content": "As show in Figure 3, given a pool of unlabeled samples $U = {x_i}^N_{i=1}$, where N represents the count of unlabeled instances. The goal of selective annotation is to select the most informative subset A\u2282U to make manual annotation, the size of A is the fixed budget T. After annotation, we can obtain the annotated dataset A = {(Xi, Yi)}^T_{i=1}. Then, ICL applies prompts retrieve to get the k-shot ICL examples $e_k$ from the selected subset A, which can perform on an undetected test set.\nIn this paper, we concentrate on the selective annotation process to construct a new prompt that achieves efficient and effective in-context"}, {"title": "learning. With a monotonically non-decreasing submodular function F: 2^V\\rightarrow R, the selective annotation problem can be viewed as maximizing the value F(A) with fixed budget T. Mathematically, the goal is to select a set A consisting of a limited number T examples through the submodular function F in the large-scale unlabeled data U:", "content": "$\\max_{A \\subset U, |A| < T} F(A)$ (4)"}, {"title": "3 Methodology", "content": "In this section, we formally introduce Sub-SA (Submodular Selective Annotation), an efficient and effective example selection approach for real-world in-context learning (ICL) scenarios. Figure 3 presents the overall pipeline of selection annotation for ICL in large language model inference. In Section 3.1, we introduce the process of Sub-SA, including the monotonous submodular function and a naive greedy algorithm to search the most informative subset with the maximum function. Figure 2 (c) shows the framework of our Sub-SA. In Sec- tion 3.2, we introduce the prompt retrieval method to get the k shot examples for LLM."}, {"title": "3.1 Selective Annotation", "content": "For ICL prompt examples selections, it is cost-effective to identify an information-dense subset from a large-scale unlabeled data pool. Inspired by submodularity [2], which naturally evaluates the represen- tative and diversity of the dataset. We propose Sub-SA, which adopts a submodular function that consists of a reward term for representa- tive scores and a penalty term for diverse scores to select the most informative examples within a fixed annotation budget for prompt retrieval."}, {"title": "3.1.1 Reward and Penalty Regularization", "content": "In order to quantify the influence of all elements in the large-scale unlabeled dataset, we apply Sentence-BERT [13] to calculate a vector representation emb. Then, we apply a non-negativity cosine similarity kernel s to evaluate the example relationship. Finally, we introduce a submodular gain function with reward term $S_{\\text{repres}}$. and penalty term $S_{\\text{diverse}}$. to choose the most informative examples to annotate.\nRepresentative Score. As previously mentioned, annotating large unlabeled data is cost-ineffective. Here we propose $S_{\\text{repres}}$. which characterizes the representation score of the large-scale dataset selec- tion.\n$S_{\\text{repres.}} = \\sum_{i \\in U} \\sum_{k \\in A} \\frac{\\text{emb}[i] \\cdot \\text{emb}[k]}{\\|\\text{emb}[i]\\\\cdot \\text{emb}[k]\\|}$ (5)\nwhere U is the large-scale unlabeled samples, A is the selected subset of U. i is the elements of ublabeled dataset U and k is the elements of dataset A, respectively.\nDiverse Score. For selective annotation, balancing diversity and rep- resentativeness is vital to improve the reliable and stable performance of LLM\u2019s prediction with ICL. Here we put forward $S_{\\text{diverse}}$. which calculates the diverse score between a selected subset of U.\n$S_{\\text{diverse}} = \\sum_{m \\in A} \\sum_{n \\in A} \\frac{\\text{emb}[m] \\cdot \\text{emb}[n]}{\\|\\text{emb}[m]\\\\cdot \\text{emb}[n]\\|}$ (6)\nwhere the A is the subset of large-scale unlabeled dataset U, m and n are the elements of the dataset A."}, {"title": "3.1.2 Submodular Function", "content": "We construct the objective function F(A) for selective annotation through a combination of the above representative score and diverse score.\nF(A) = $\u03bb_1 S_{\\text{repres.}} + \u03bb_2 S_{\\text{diverse.}}$\n= $\u03bb_1 \\sum_{i \\in U} \\sum_{k \\in A}  s_{ik} + \u03bb_2 \\sum_{m \\in A} \\sum_{n \\in A} s_{mn}$ (7)\nwhere \u03bb\u2081 and \u03bb\u2082 represent the weighting factors used to trade-off between representativeness and diversity, s is the similarity kernel. This time \u03bb\u2081 is set to 2 for representative dataset selection and \u03bb\u2082 is set to -1 for improving the diversity. We prove the submodularity and monotonically below.\nProof. This proof is inspired by [2]. We first prove the monotonically, then we prove submodularity based on the first derivation process. For monotonically proof, we have:\nF(A\u222a{a}) = $\u03bb_1 \\sum_{i \\in U} \\sum_{k \\in A\u222a{a}} s_{ik}+\u03bb_2 \\sum_{m \\in A\u222a{a}} \\sum_{n \\in A\u222a{a}} s_{mn}$  (8)\nThus:\nF(A\u222a{a}) - F (A) = $\u03bb\u03b9 \u03a3_{i\u2208U} S_{ai} + \u03bb_2 (2\u2211_{m\u2208A} S_{ma} + S_{aa})$ (9)\nSince \u03bb\u2081 = 2, A\u2082 = -1, and s is non-negativity cosine similarity kernel. Follow up:\nF(A\u222a{a}) - F (A) \u2265 0 (10)\nFor submodularity proof, We have:\nF(A\u222a {a}) - F (A) = $\u03bb\u03b9 \u03a3_{i\u2208U} S_{ai} + \u03bb_2 (2\u2211_{m\u2208A} S_{ma} + S_{aa})$ (11)\nF(B\u222a {a}) - F (B) = $\u03bb\u03b9 \u03a3_{i\u2208U} S_{ai} + \u03bb_2 (2\u2211_{m\u2208B} S_{ma} + S_{aa})$\nSince: A\u2286B, Therefore:\n$\u2211_{m\u2208a} S_{ma}$ \u2264 $\u2211_{m\u2208b} S_{ma}$ (12)\nWe can get:\nF(A\u222a{a}) \u2013 F(A) > F(B\u222a {a}) \u2013 F (B) (14)"}, {"title": "3.1.3 Greedy Search Algorithm", "content": "Algorithm 1 A greedy search based algorithm for selective annotation\nInput: The annotation budget T, Unlabeled data U with index.\nOutput: The set A that includes T examples to annotate.\nGiven a large-scale unlabeled dataset U, we can apply Equation 4 to search the diversity and representativeness subset by selecting T elements that maximize the value of the submodular Function 7. This problem can be proficiently addressed by implementing a naive greedy algorithm. Referring to related works, we apply Algorithm 1 to optimize the output of the submodular function F(A). Based on the Definition 1 we have proved that the Function 7 is a submodular function. According to the theory of [28], we have:"}, {"title": "Theorem 1.", "content": "A denotes the solution obtained by the greedy search approach, and $A*$ denotes the optimal solution. If F(\u00b7) is a submod- ular function, then the solution A has the following approximation guarantee:\nF(A) \u2265 $(1 - \\frac{1}{e})F(A*)$ (15)\nwhere e is the base of natural logarithm."}, {"title": "3.2 Prompt Retrieval", "content": "Following the above submodular selective annotation, we receive a set of annotated examples A. Then, we retrieve k shot examples from the annotated set A to concatenate with the test input. Finally, the prompts are given to the LLM for relevant task prediction. As previous research [3, 12], we apply Sentence-BERT [13] to evaluate embeddings for all annotated examples and pick out the instances of each test case derived from the similarity between examples and test cases."}, {"title": "4 Experiments", "content": "In this section, we perform experiments across eight different NLP datasets with diverse categories of tasks (including classification, commonsense reasoning, dialogue, and text generation). We first introduced the experimental setups and implementation details in Section 4.1 and Section 4.2. Then, in Section 4.3, we present the proposed method Sub-SA, which can effectively and efficiently ascer- tain better selective annotation compared with the baselines. Finally, we demonstrate the excellent performance of the proposed method in three different settings (alternative selective annotation methods, various retrieval approaches, and different models) for comprehensive comparison in Section 4.4."}, {"title": "4.1 Experimental Setups", "content": "Experimental Datasets. For extensive evaluations, we select eight diverse datasets from different NLP tasks, including classification, commonsense reasoning, dialogue, and generation. Following previous work [12, 3], we split the \u201ctrain/dev/test\u201d dataset from the Transformer library [4], and apply test data for evaluation in SST-5, SST-2 [33], MWoZ [35] datasets. For the rest, we use the development dataset to evaluate the performance as former research [12, 3]. We evaluate the method by accuracy for classifications and multiple choices tasks; for dialogue task MWoz [35], we apply joint accuracy [35]; And for generation task GeoQuery [36], we adopt the test suite accuracy [5].\nBase Models. In our study, we present the main empirical results by using the GPT-J with 6B parameters [10], except the GeoQuery and MWoZ datasets, which we apply GPT-3.5-Turbo with 175B parame- ters [1] to measure the performance. We also adopt GPT-Neo with 2.7B parameters [11] to estimate the robustness of proposed methods. Unless otherwise stated, all experiments are conducted with GPT-J (6B) model."}, {"title": "4.2 Implementation Details", "content": "Details of Getting Unlabeled Data. As the previous state-of-the-art selective annotation methods [12, 3], we simulate the realistic setting to obtain unlabeled examples. Specifically, we conduct selective an- notation from 3K instances that are randomly taken from the original training data for each task. For each experiment, we repeat this sub-sampling procedure three times, and average the results over the three trials. Table 1 and 3 present the mean/maximum/minimum evaluation results from these three trials. We will find that Sub-SA demonstrates empirical success across previous state-of-the-art selective annotation methods.\nExperimental Conditions. We apply PyTorch [29] to implement our method and the baselines. For GPT-3.5-Turbo, we perform the experiments by calling the OpenAI API using a MacBook. The GPT-J 6B and GPT-Neo 2.7B models are from the Huggingface transformer library [4]. We experiment with three different random seeds and present the mean, maximum, and minimum outcomes. All our exper- iments of GPT-J 6B and GPT-Neo 2.7B are conducted on a single NVIDIA A40 (48GB) GPU."}, {"title": "4.3 Main Results", "content": "Effectiveness. We present the performance on eight datasets, cover- ing classification, commonsense reasoning, dialogue, and generation tasks, with Random, Vote-k, IDEAL, and our method Sub-SA. Note that for the sake of fairness, we select 18 and 100 as the annotation budget respectively. Following the experimental conditions setting, the result is reported in Table 1 and Table 3. From the results, we have the following findings: First, our method outperforms than the baselines in the evaluation tasks on 13 out of 16, especially in classi- fication tasks. This improvement is evidence the Sub-SA effectively selects the annotation across most NLP datasets; Second, in-context learning with 18 examples selected by the Sub-SA achieves higher performance than the one with 100 randomly selected constructors on 6 out of 8 tasks. The deterministic selective annotation method Sub-SA shows stability and capability of better in-context learner; Last, as the OpenAI deprecated the Codex-davinci-002 model, we apply the GPT-3.5-Turbo API to evaluate the dialogue and generation tasks. For the MWoZ task, the Vote-k shows better performance than others, these results present the robust ability of the GPT-3.5-Turbo model on prompt selection for dialogue task.\nEfficiency. We compare the time consumption of subset selection in our method Sub-SA against Vote-k and IDEAL on all tasks with the same hardware condition. Random selection does not use an opti- mization analysis, letting this approach randomly select the examples without delay. Therefore, we did not compare Random selection with other methods. As shown in Table 4, With respect to the LLM\u2019s feed- back, Vote-k selects the examples based on the diversity confidence score of the prediction for most unlabeled data. This process ensures that different annotation budgets (e.g., 18/100) have the same selec- tion time. Constructing from the influence-driven mechanism, the selection time of IDEAL increases continuously as the annotation budget increases. Based on the table, the selection time on the minute or even hourly level does not align with real-world use cases. In our work, Sub-SA achieves millisecond-level time consumption during subset selection, which is efficient in realistic scenarios. In Figure 4, we compare the time cost of subset selection with log presentation under millisecond scale, our SubSA achieves the lowest selection time for In-context learning. Therefore, we recommend that researchers and practitioners use the practical selective annotation method (e.g., Sub-SA) for ICL."}, {"title": "4.4 Analysis", "content": "Comparisons With Alternative Methods. For in-depth comparison, We also explore three alternative selective annotation methods from large-scale unlabeled data: (1) Maximizing facility location (MFL), which aims at optimizing the representativeness of selected subset. (2) Fast Vote-k, which picks T samples with the largest Vote-k scores, and avoids using the pre-trained language model to the computer confidence score for each instance, resulting in a significantly faster process. (3) Diversity, which focuses on maximizing the diversity of the embeddings for selected examples. We present the results on MRPC, SST-5, and SST-2 tasks, the results shown in Table 5. Performance is also averaged over three random trials. As can be seen, Sub-SA consistently outperforms all the other methods, demonstrating its predominance in selective annotation.\nEvaluation With Different Retrieval Methods. Up to this point, we have conducted similarity-based prompt retrieval methods. In this part, we use experiments to quantify the effect of the random baseline"}, {"title": "5 Related Work", "content": ""}, {"title": "5.1 In-Context Learning", "content": "In-context learning (ICL) [1, 14, 48], which engages with no pa- rameter updates or fine-tuning for downstream tasks, has exhibited competitiveness across multiple Natural Language Understanding (NLU) and Natural Language Generation (NLG) tasks [47, 23, 46]. Generally, prompts are crafted as task/test instructions, which are re- trieved from a large-scale annotated dataset with paired input-output examples. Although ICL led a striking property of LLMs in many fields, recent research [15, 24] indicates the performance is profoundly dependent on the construct prompts. Taking this into account, select- ing the optimal in-context learning examples has been crucial to improving the capability of ICL. Previous works can be mainly classi- fied into the following two categories: Test example-based methods: these methods [14, 22] aim to retrieve analogous examples for every single test case based on similarity. Task-based methods: these meth- ods [19, 20, 21] focus on obtaining a set of examples that are suitable for all queries on the same tasks.\nIn contrast to selected examples from the annotated dataset, selec- tion annotation commits to construct prompts from unlabeled large- scale datasets, which is exceptional cost efficiency and more in line with real-world scenarios. Prior approaches like Votk-k [12] and IDEAL [3] proposed LLM estimation and influence-driven mecha- nism to select prompt examples, respectively. In this study, we propose Sub-SA (Submodular Selective Annotation) for ICL, Experimental results show that Sub-SA presents better performance than existing"}, {"title": "selective annotation methods across diverse tasks (including classifi- cation, commonsense reasoning, dialogue, and text generation) in an end-to-end manner. Given its millisecond-level processing time for subset selection, Sub-SA proves to be exceptionally practical for ICL scenarios in real settings.", "content": ""}, {"title": "5.2 Coreset Selection", "content": "With growing volumes of data in artificial intelligence, the challenges of how to organize and analyze large data let us design efficient approaches to distill the dataset. Coreset selection aims to select such a small subset of the most informative samples from the given large dataset, which dramatically reduces the memory and computational cost for the subsequent action [18, 25, 17, 2]. Various strategies have been explored to achieve this, ranging from geometry-based methods to optimization-based methods and submodularity-based methods. Geometry-based methods assumed data points close to each other in the feature space with similar properties. So removing these redundant data points can certainly boost efficiency. Related works include Herding [37], k-Center-Greedy [38] and Moderate Coreset [39]. For optimization-based methods, these approaches are modern the coreset selection as a bilevel optimization problem. Existing works including Glister [40] and Retrieve [41]. Lastly, submodularity-based methods, which design a function with submodularity to measure the diversity and information of a large dataset. The works included in this method are Log Determinant, Facility Location, and Graph Cut [2]. In this work, we propose Sub-SA, which is a submodularity-based method, to select a diverse and representative subset from a large-scale unlabeled data pool that strengthens ICL for LLM to be better learners."}, {"title": "6 Conclusions", "content": "In this paper, we propose Sub-SA, a submodular-based selective annotation method for LLM to be better learners with well-constructed demonstrations. The design of submodular function and RPR in Sub-SA facilitates efficient and effective subset selection from the large unlabeled dataset across diverse tasks (covering classification, commonsense reasoning, dialogue, and text generation), meanwhile exponentially reducing the time consumption (from hour-level to millisecond-level). Theoretically, we demonstrate that our Sub-SA possesses the characteristics of monotonically and submodularity. Empirically, Sub-SA can improve the performance on a series of benchmarks in an end-to-end manner. The effectiveness and efficiency of Sub-SA offer practical implications for in-context learning in real- world contexts, facilitating more effective language-based tasks. We hope our Sub-SA can give more inspiration to the community in ICL."}]}