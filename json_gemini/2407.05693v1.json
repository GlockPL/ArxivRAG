{"title": "Sub-SA: Strengthen In-context Learning via Submodular Selective Annotation", "authors": ["Jian Qian", "Miao Sun", "Sifan Zhou", "Ziyu Zhao", "Ruizhi Hun", "Patrick Chiang"], "abstract": "In-context learning (ICL) leverages in-context examples as prompts for the predictions of Large Language Models (LLMs). These prompts play a crucial role in achieving strong performance. However, the selection of suitable prompts from a large pool of labeled examples often entails significant annotation costs. To address this challenge, we propose Sub-SA (Submodular Selective Annotation), a submodule-based selective annotation method. The aim of Sub-SA is to reduce annotation costs while improving the quality of in-context examples and minimizing the time consumption of the selection process. In Sub-SA, we design a submodular function that facilitates effective subset selection for annotation and demonstrates the characteristics of monotonically and submodularity from the theoretical perspective. Specifically, we propose RPR (Reward and Penalty Regularization) to better balance the diversity and representativeness of the unlabeled dataset attributed to a reward term and a penalty term, respectively. Consequently, the selection for annotations can be effectively addressed with a simple yet effective greedy search algorithm based on the submodular function. Finally, we apply the similarity prompt retrieval to get the examples for ICL. Compared to existing selective annotation approaches, Sub-SA offers two main advantages. (1.) Sub-SA operates in an end-to-end, unsupervised manner, and significantly reduces the time consumption of the selection process (from hours-level to millisecond-level). (2.) Sub-SA enables a better balance between data diversity and representativeness and obtains state-of-the-art performance. Meanwhile, the theoretical support guarantees their reliability and scalability in practical scenarios. Extensive experiments conducted on diverse models and datasets demonstrate the superiority of Sub-SA over previous methods, achieving millisecond(ms)-level time selection and remarkable performance gains. The efficiency and effectiveness of Sub-SA make it highly suitable for real-world ICL scenarios. Our codes are available at https://github.com/JamesQian11/SubSA", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) can rapidly adapt to various down-stream tasks from just a few examples provided in the prompt without parameter update [42, 43, 1] while improving accuracy [14, 8]. This remarkable ability is called in-context learning (ICL). Unlike tradi-tional methods [9, 6, 7] that rely on task-specific data to update model parameters, ICL harnesses the powerful few-shot learning abil-ity of LLMs effectively, making it a promising application. Recent works [14, 15, 44] show that collecting prompts from a broad selection of annotated examples is vital to achieving high-quality performance. Specifically, these papers have indicated that the performance sub-stantially improves when similar examples (with some embedding criteria) are retrieved as in-context examples for the single test case. Given the varying requirements of different test cases for tailored in-context examples, the availability of a comprehensive collection of annotated examples assumes utmost importance in boosting the performance of LLMs.\nHowever, compiling and annotating extensive datasets to obtain corresponding examples for ICL demands substantial manpower and financial costs. For example, when an annotator labels a question and answer pair, they are also required to provide a rational, step-by-step thinking approach on how to obtain the final output [16], which significantly increases the annotation costs.\nTo address the challenges of annotation cost, prior researches en-deavor Vote-k and IDEAL [12, 3]. Vote-k [12] selects a diverse and representative subset from a large-scale unlabeled data pool for annotation based on the LLM's feedback to avoid retrieving all the examples. However, as shown in Figure 2 (a) Vote-k is subject to three drawbacks that limit its practical application. (1.) Vote-k places excessive emphasis on diversity in data selection, resulting in the neglect of representative data and leading to the selection of outliers [3]. (2.) Vote-k is not an end-to-end pipeline, leading to increased processing complexity and additional inference costs. (3.) Vote-k lacks theoretical support to guarantee their reliability and scalability in practical scenarios. Following that, IDEAL [3] utilizes an influence-driven mechanism to select annotation from a directed graph, which is built from the unlabeled dataset. Particularly, as shown in Figure 2 (b), IDEAL sets every unlabeled data as a vertex and constructs the directed graph by considering the similarities between every example as the edges. After that, the influence of each candidate unlabeled subset is evaluated through an information diffusion process. Finally, a subset with high influence is iteratively selected based on a simple greedy algorithm. However, despite its performance well in empir-ical evaluations, IDEAL still lacks effectiveness in real-world sce-narios with the following limitations. (1.) IDEAL applies a classic independent-cascade diffusion model to select a subset, thereby ig-noring the diversity and representativeness of unlabeled examples, resulting in the unsatisfactory performance of the method. (2.) The process of examples selection from the graph, which involves repeat-ing multiple steps of the diffusion process ten times, significantly increases computational complexity and renders the method highly inefficient (Table 4).\nIn this paper, to minimize annotation costs for ICL and overcome the above issues of existing studies, an unsupervised, end-to-end submodule-based selective annotation method is proposed, called Sub-SA (Submodular Selective Annotation). In general, Sub-SA aims to identify a suitable subset of data from a large unlabeled dataset to serve as a proxy and a viable substitute for large annotated examples in subsequent ICL tasks. In detail, our Sub-SA has two core designs, which are submodular function and RPR (Reward and Penalty Regularization). Submodular function aim to facilitate effi-cient and effective subset selection from a large unlabeled dataset.\nNotably, due to the design attributes to the design of the submodu-lar function, our selection process does not involve the LLMs [12] or iterative selection [3], thereby exponentially reducing the time-consuming of the selection process. RPR aims to better balance the diversity and representativeness of the unlabeled dataset and can be seamlessly integrated into the submodule selection function. Specifi-cally, in RPR, the reward term encourages the selection of candidate elements with higher representative scores based on their similarity to the overall elements. On the other hand, the penalty term reduces the selection of candidate elements that are too similar to the elements already selected within the set. Subsequently, based on the results of the submodular function, a simple yet effective greedy search algo-rithm is utilized to obtain a subset of annotated examples. Finally, we apply similarity prompt retrieval to get the examples for the ICL task. Theoretically, we demonstrate that our Sub-SA possesses the char-acteristics of monotonicity and submodularity, which provide the corresponding lower bound of the subset selection. Empirically, we validate the performance of our proposed approach over 8 datasets across diverse tasks (covering classification, commonsense reasoning, dialogue, and generation). Various LLMs (parameters from 2.7B to 175B) and prompt retrieval technologies are also included in evalu-ations. Comprehensive experiments show that the proposed method achieves further performance improvement compared to the state-of-the-art methods in 13 out of 16 cases. Meanwhile, it is worth noting that our method significantly reduces the time consumption of the selection process, from hours-level to millisecond-level, achieving a speedup of magnitudes around 10,000\u00d7 (Table 4). In particular, with an annotation budget of either 18 or 100 from a 3K unlabeled dataset, the selective annotation cost decreases to 30-150\u00d7. Figure 1 presents the comparison on performance and time consumption, our Sub-SA outperforms the Vote-k and IDEAL methods, which makes it more suitable for real-world ICL scenarios. Furthermore, we compare different prompt retrieval methods for ICL tasks, our method consis-tently achieves better results than reference methods. This creates a strong baseline of selective annotations for follow-up research.\nIn summary, the main contributions of this paper are as follows:\n\u2022 We propose a novel submodule-based selective annotation method named Sub-SA, which is an unsupervised, end-to-end subset selec-tion technique for ICL. Furthermore, we theoretically demonstrate that our Sub-SA possesses the characteristics of monotonically and submodularity, which provide the corresponding lower bound of the subset selection.\n\u2022 We design RPR (Reward and Penalty Regularization) for Sub-SA, which consists of a reward term and a penalty term, to balance the diversity and representativeness of the unlabeled dataset selection.\n\u2022 Extensive experiments demonstrate that Sub-SA shows better per-formance than existing selective annotation methods across diverse tasks and models. More importantly, Sub-SA significantly reduces the time consumption during subset selection, from hours-level to millisecond-level, making it highly suitable for practical ICL scenarios."}, {"title": "2 Preliminaries", "content": "2.1 In-context Learning\nIn-context learning (ICL) is a pivotal capability behind the astounding performance of Large Language Models (LLMs). The combination of test input with a few input-output demonstrations allows these models to undertake a variety of tasks without updating parameters. Formally, a k-shot prompt for ICL consists of k examples, given a test sample (Xtest, Ytest), LLMs predicts \u0177 based on the in-context prompts and input Xtest:\n$\\\u0177 = LLM (e_k,..., e_1; X_{test})$\nwhere $e_i = (x_i, Y_i)_{i=1}^{k}$ represents an sequence consisting of input-output pairs, k denotes the shot number and ; is the operation of concatenation. It is paramount to optimize the in-context prompts by seeking the ideal example set $\\{e_k,..., e_1\\}$ in A for $X_{test}$, aiming to make the LLM's prediction \u0177 match the ground truth $Y_{test}$, where A = $\\{(X_i, Y_i)\\}_{i=1}^{N}$ is the annotated examples and T is the fixed budget.\n2.2 Submodular Subset Selection\nConsidering a finite set V, given a set function $F: 2^V \\rightarrow R$ that maps any subset $A \\subseteq V$ and $B \\subseteq V$ to a real value. When F is a submodular function, its definition is as follows:\nDefinition 1 (Submodular Function [45]). For any set $A \\subseteq B \\subseteq V$. Given an element a, where $a = V\\B$. The set function F is a submodular function when it satisfies monotonically non-decreasing:\n$F(A \\cup \\{a\\}) - F (A) \\geq 0$\nAnd:\n$F(A \\cup \\{a\\}) - F (A) > F (B\\cup \\{a\\}) - F (B).$\nThis definition indicates that the submodular function possesses a diminishing returns characteristic, allowing the subset selection prob-lem to be framed as either minimizing or maximizing a submodular function [2]. It naturally measures the diversity and information of select set A, which ensures that each additional element contributes the maximum possible value to the overall objective.\n2.3 Selective annotation Problem Setting\nAs show in Figure 3, given a pool of unlabeled samples $U = \\{x_i\\}_{i=1}^{N}$, where N represents the count of unlabeled instances. The goal of selective annotation is to select the most informative subset $A \\subset U$ to make manual annotation, the size of A is the fixed budget T. After annotation, we can obtain the annotated dataset $A = \\{(X_i, Y_i)\\}_{i=1}^{T}$. Then, ICL applies prompts retrieve to get the k-shot ICL examples $e_k$ from the selected subset A, which can perform on an undetected test set.\nIn this paper, we concentrate on the selective annotation process to construct a new prompt that achieves efficient and effective in-context learning. With a monotonically non-decreasing submodular function F : 2V\u2192 R, the selective annotation problem can be viewed as maximizing the value F(A) with fixed budget T. Mathematically, the goal is to select a set A consisting of a limited number T examples through the submodular function F in the large-scale unlabeled data U:\n$\\max_{A \\subset U, |A| < T} F(A)$\n3 Methodology\nIn this section, we formally introduce Sub-SA (Submodular Selective Annotation), an efficient and effective example selection approach for real-world in-context learning (ICL) scenarios. Figure 3 presents the overall pipeline of selection annotation for ICL in large language model inference. In Section 3.1, we introduce the process of Sub-SA, including the monotonous submodular function and a naive greedy algorithm to search the most informative subset with the maximum function. Figure 2 (c) shows the framework of our Sub-SA. In Sec-tion 3.2, we introduce the prompt retrieval method to get the k shot examples for LLM."}, {"title": "3.1 Selective Annotation", "content": "For ICL prompt examples selections, it is cost-effective to identify an information-dense subset from a large-scale unlabeled data pool. Inspired by submodularity [2], which naturally evaluates the represen-tative and diversity of the dataset. We propose Sub-SA, which adopts a submodular function that consists of a reward term for representa-tive scores and a penalty term for diverse scores to select the most informative examples within a fixed annotation budget for prompt retrieval.\n3.1.1 Reward and Penalty Regularization\nIn order to quantify the influence of all elements in the large-scale unlabeled dataset, we apply Sentence-BERT [13] to calculate a vector representation emb. Then, we apply a non-negativity cosine similarity kernel s to evaluate the example relationship. Finally, we introduce a submodular gain function with reward term $S_{repres}$. and penalty term $S_{diverse}$. to choose the most informative examples to annotate.\nRepresentative Score. As previously mentioned, annotating large unlabeled data is cost-ineffective. Here we propose $S_{repres}$., which characterizes the representation score of the large-scale dataset selec-tion.\n$S_{repres.} = \\sum_{i \\in U} \\sum_{k \\in A} \\frac{emb[i] \\cdot emb[k]}{||emb[i]|| ||emb[k]||}$\nwhere U is the large-scale unlabeled samples, A is the selected subset of U. i is the elements of ublabeled dataset U and k is the elements of dataset A, respectively.\nDiverse Score. For selective annotation, balancing diversity and rep-resentativeness is vital to improve the reliable and stable performance of LLM's prediction with ICL. Here we put forward $S_{diverse}$., which calculates the diverse score between a selected subset of U.\n$S_{diverse.} = \\sum_{m \\in A} \\sum_{n \\in A} \\frac{emb[m] \\cdot emb[n]}{||emb[m]|| \\cdot ||emb[n]||}$\nwhere the A is the subset of large-scale unlabeled dataset U, m and n are the elements of the dataset A."}, {"title": "3.1.2 Submodular Function", "content": "We construct the objective function F(A) for selective annotation through a combination of the above representative score and diverse score.\n$F(A) = \\lambda_1 S_{repres.} + \\lambda_2 S_{diverse.}$\\n$= \\lambda_1 \\sum_{i \\in U} \\sum_{k \\in A} S_{ik} + \\lambda_2 \\sum_{m \\in A} \\sum_{n \\in A} S_{mn}$\nwhere $\\lambda_1$ and $\\lambda_2$ represent the weighting factors used to trade-off between representativeness and diversity, s is the similarity kernel. This time $\\lambda_1$ is set to 2 for representative dataset selection and $\\lambda_2$ is set to -1 for improving the diversity. We prove the submodularity and monotonically below.\nProof. This proof is inspired by [2]. We first prove the monotonically, then we prove submodularity based on the first derivation process. For monotonically proof, we have:\n$F(A\\cup\\{a\\}) = \\lambda_1 \\sum_{i \\in U} \\sum_{k \\in A \\cup \\{a\\}} S_{ik} + \\lambda_2 \\sum_{m \\in A\\cup\\{a\\}} \\sum_{n \\in A\\cup\\{a\\}} S_{mn}$\nThus:\n$F(A\\cup\\{a\\}) - F (A) = \\lambda_1 \\sum_{i \\in U} S_{ai} + \\lambda_2 (2\\sum_{m \\in A} S_{ma} + S_{aa})$\nSince $\\lambda_1 = 2, \\lambda_2 = -1$, and s is non-negativity cosine similarity kernel. Follow up:\n$F(A\\cup\\{a\\}) - F (A) \\geq 0$\nFor submodularity proof, We have:\n$F(A\\cup \\{a\\}) - F (A) = \\lambda_1 \\sum_{i \\in U} S_{ai} + \\lambda_2 (2\\sum_{m \\in A} S_{ma} + S_{aa})$\n$F(B\\cup \\{a\\}) - F (B) = \\lambda_1 \\sum_{i \\in U} S_{ai} + \\lambda_2 (2\\sum_{m \\in B} S_{ma} + S_{aa})$\nSince: A \u2286 B, Therefore:\n$\\sum_{m \\in A}S_{ma} \\leq \\sum_{m \\in B} S_{ma}$\nWe can get:\n$F(A\\cup\\{a\\}) - F(A) > F(B\\cup \\{a\\}) - F(B)$\n3.1.3 Greedy Search Algorithm\nAlgorithm 1 A greedy search based algorithm for selective annotation\nInput: The annotation budget T, Unlabeled data U with index.\nOutput: The set A that includes T examples to annotate.\n1: Initialize $A_0 \\leftarrow \\emptyset$, i = 0\n2: while i < T do\n3: $a_i \\leftarrow arg \\max_{a_i \\in U \\backslash A_i} F(A_i \\cup \\{a_i\\})$\n4: $A_{i+1} \\leftarrow A_i \\cup a_i$\n5: i\u2190i+1\n6: end while\n7: return A\nGiven a large-scale unlabeled dataset U, we can apply Equation 4 to search the diversity and representativeness subset by selecting T elements that maximize the value of the submodular Function 7. This problem can be proficiently addressed by implementing a naive greedy algorithm. Referring to related works, we apply Algorithm 1 to optimize the output of the submodular function F(A). Based on the Definition 1 we have proved that the Function 7 is a submodular function. According to the theory of [28], we have:\nTheorem 1. A denotes the solution obtained by the greedy search approach, and $A^*$ denotes the optimal solution. If F(\u00b7) is a submod-ular function, then the solution A has the following approximation guarantee:\n$F(A) \\geq (1 - \\frac{1}{e})F(A^*)$\nwhere e is the base of natural logarithm."}, {"title": "3.2 Prompt Retrieval", "content": "Following the above submodular selective annotation, we receive a set of annotated examples A. Then, we retrieve k shot examples from the annotated set A to concatenate with the test input. Finally, the prompts are given to the LLM for relevant task prediction. As previous research [3, 12], we apply Sentence-BERT [13] to evaluate embeddings for all annotated examples and pick out the instances of each test case derived from the similarity between examples and test cases."}, {"title": "4 Experiments", "content": "In this section, we perform experiments across eight different NLP datasets with diverse categories of tasks (including classification, commonsense reasoning, dialogue, and text generation). We first introduced the experimental setups and implementation details in Section 4.1 and Section 4.2. Then, in Section 4.3, we present the proposed method Sub-SA, which can effectively and efficiently ascer-tain better selective annotation compared with the baselines. Finally, we demonstrate the excellent performance of the proposed method in three different settings (alternative selective annotation methods, various retrieval approaches, and different models) for comprehensive comparison in Section 4.4.\n4.1 Experimental Setups\nExperimental Datasets. For extensive evaluations, we select eight diverse datasets from different NLP tasks, including classification, commonsense reasoning, dialogue, and generation. Table 2 describes details of the datasets. Following previous work [12, 3], we split the \"train/dev/test\" dataset from the Transformer library [4], and apply test data for evaluation in SST-5, SST-2 [33], MWoZ [35] datasets. For the rest, we use the development dataset to evaluate the performance as former research [12, 3]. We evaluate the method by accuracy for classifications and multiple choices tasks; for dialogue task MWoz [35], we apply joint accuracy [35]; And for generation task GeoQuery [36], we adopt the test suite accuracy [5].\nBase Models. In our study, we present the main empirical results by using the GPT-J with 6B parameters [10], except the GeoQuery and MWoZ datasets, which we apply GPT-3.5-Turbo with 175B parame-ters [1] to measure the performance. We also adopt GPT-Neo with 2.7B parameters [11] to estimate the robustness of proposed methods. Unless otherwise stated, all experiments are conducted with GPT-J (6B) model.\nBaseline Methods. We compare the proposed Sub-SA with three baselines: Random selection, Vote-k [12], and IDEAL [3]. A sim-ple baseline method is the random selection, which does not involve any analysis. This approach randomly selects a series of specified examples from the unlabeled data pool. Vote-k [12] first selects small"}, {"title": "4.3 Main Results", "content": "Effectiveness. We present the performance on eight datasets, cover-ing classification, commonsense reasoning, dialogue, and generation tasks, with Random, Vote-k, IDEAL, and our method Sub-SA. Note that for the sake of fairness, we select 18 and 100 as the annotation budget respectively. Following the experimental conditions setting, the result is reported in Table 1 and Table 3. From the results, we have the following findings: First, our method outperforms than the baselines in the evaluation tasks on 13 out of 16, especially in classi-fication tasks. This improvement is evidence the Sub-SA effectively selects the annotation across most NLP datasets; Second, in-context learning with 18 examples selected by the Sub-SA achieves higher performance than the one with 100 randomly selected constructors on 6 out of 8 tasks. The deterministic selective annotation method Sub-SA shows stability and capability of better in-context learner; Last, as the OpenAI deprecated the Codex-davinci-002 model, we apply the GPT-3.5-Turbo API to evaluate the dialogue and generation tasks. For the MWoZ task, the Vote-k shows better performance than others, these results present the robust ability of the GPT-3.5-Turbo model on prompt selection for dialogue task.\nEfficiency. We compare the time consumption of subset selection in our method Sub-SA against Vote-k and IDEAL on all tasks with the same hardware condition. Random selection does not use an opti-mization analysis, letting this approach randomly select the examples without delay. Therefore, we did not compare Random selection with other methods. As shown in Table 4, With respect to the LLM's feed-back, Vote-k selects the examples based on the diversity confidence score of the prediction for most unlabeled data. This process ensures that different annotation budgets (e.g., 18/100) have the same selec-tion time. Constructing from the influence-driven mechanism, the selection time of IDEAL increases continuously as the annotation budget increases. Based on the table, the selection time on the minute or even hourly level does not align with real-world use cases. In our work, Sub-SA achieves millisecond-level time consumption during subset selection, which is efficient in realistic scenarios. In Figure 4, we compare the time cost of subset selection with log presentation under millisecond scale, our SubSA achieves the lowest selection time for In-context learning. Therefore, we recommend that researchers and practitioners use the practical selective annotation method (e.g., Sub-SA) for ICL."}, {"title": "4.4 Analysis", "content": "Comparisons With Alternative Methods. For in-depth comparison, We also explore three alternative selective annotation methods from large-scale unlabeled data: (1) Maximizing facility location (MFL), which aims at optimizing the representativeness of selected subset. (2) Fast Vote-k, which picks T samples with the largest Vote-k scores, and avoids using the pre-trained language model to the computer confidence score for each instance, resulting in a significantly faster process. (3) Diversity, which focuses on maximizing the diversity of the embeddings for selected examples. We present the results on MRPC, SST-5, and SST-2 tasks, the results shown in Table 5. Performance is also averaged over three random trials. As can be seen, Sub-SA consistently outperforms all the other methods, demonstrating its predominance in selective annotation.\nEvaluation With Different Retrieval Methods. Up to this point, we have conducted similarity-based prompt retrieval methods. In this part, we use experiments to quantify the effect of the random baseline for prompt retrieval with a 100 annotation budget. Table 6 presents the results. Based on the table, we observe that Vote-k, IDEAL, and Sub-SA suffer from inferior performance when the prompt retrieval method shifts from similarity-based to random selection. Notice that Sub-SA consistently maintains better performance, whether under random selection or similarity-based retrieval methods. Therefore, Sub-SA can create a more stable training subset for ICL.\nMeasurement with Other Model. Here we evaluate Sub-SA on GPT-Neo 2.7B model. The evaluation results are presented in Figure 5"}, {"title": "5 Related Work", "content": "5.1 In-Context Learning\nIn-context learning (ICL) [1, 14, 48], which engages with no pa-rameter updates or fine-tuning for downstream tasks, has exhibited competitiveness across multiple Natural Language Understanding (NLU) and Natural Language Generation (NLG) tasks [47, 23, 46]. Generally, prompts are crafted as task/test instructions, which are re-trieved from a large-scale annotated dataset with paired input-output examples. Although ICL led a striking property of LLMs in many fields, recent research [15, 24] indicates the performance is profoundly dependent on the construct prompts. Taking this into account, select-ing the optimal in-context learning examples has been crucial to improving the capability of ICL. Previous works can be mainly classi-fied into the following two categories: Test example-based methods: these methods [14, 22] aim to retrieve analogous examples for every single test case based on similarity. Task-based methods: these meth-ods [19, 20, 21] focus on obtaining a set of examples that are suitable for all queries on the same tasks.\nIn contrast to selected examples from the annotated dataset, selec-tion annotation commits to construct prompts from unlabeled large-scale datasets, which is exceptional cost efficiency and more in line with real-world scenarios. Prior approaches like Votk-k [12] and IDEAL [3] proposed LLM estimation and influence-driven mecha-nism to select prompt examples, respectively. In this study, we propose Sub-SA (Submodular Selective Annotation) for ICL, Experimental results show that Sub-SA presents better performance than existing selective annotation methods across diverse tasks (including classifi-cation, commonsense reasoning, dialogue, and text generation) in an end-to-end manner. Given its millisecond-level processing time for subset selection, Sub-SA proves to be exceptionally practical for ICL scenarios in real settings.\n5.2 Coreset Selection\nWith growing volumes of data in artificial intelligence, the challenges of how to organize and analyze large data let us design efficient approaches to distill the dataset. Coreset selection aims to select such a small subset of the most informative samples from the given large dataset, which dramatically reduces the memory and computational cost for the subsequent action [18, 25, 17, 2]. Various strategies have been explored to achieve this, ranging from geometry-based methods to optimization-based methods and submodularity-based methods. Geometry-based methods assumed data points close to each other in the feature space with similar properties. So removing these redundant data points can certainly boost efficiency. Related works include Herding [37], k-Center-Greedy [38] and Moderate Coreset [39]. For optimization-based methods, these approaches are modern the coreset selection as a bilevel optimization problem. Existing works including Glister [40] and Retrieve [41]. Lastly, submodularity-based methods, which design a function with submodularity to measure the diversity and information of a large dataset. The works included in this method are Log Determinant, Facility Location, and Graph Cut [2]. In this work, we propose Sub-SA, which is a submodularity-based method, to select a diverse and representative subset from a large-scale unlabeled data pool that strengthens ICL for LLM to be better learners."}, {"title": "6 Conclusions", "content": "In this paper, we propose Sub-SA, a submodular-based selective annotation method for LLM to be better learners with well-constructed demonstrations. The design of submodular function and RPR in Sub-SA facilitates efficient and effective subset selection from the large unlabeled dataset across diverse tasks (covering classification, commonsense reasoning, dialogue, and text generation), meanwhile exponentially reducing the time consumption (from hour-level to millisecond-level). Theoretically, we demonstrate that our Sub-SA possesses the characteristics of monotonically and submodularity. Empirically, Sub-SA can improve the performance on a series of benchmarks in an end-to-end manner. The effectiveness and efficiency of Sub-SA offer practical implications for in-context learning in real-world contexts, facilitating more effective language-based tasks. We hope our Sub-SA can give more inspiration to the community in ICL."}]}