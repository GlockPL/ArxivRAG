{"title": "ENHANCING TRUST IN LARGE LANGUAGE MODELS\nWITH UNCERTAINTY-AWARE FINE-TUNING", "authors": ["Ranganath Krishnan", "Piyush Khanna", "Omesh Tickoo"], "abstract": "Large language models (LLMs) have revolutionized the field of natural language\nprocessing with their impressive reasoning and question-answering capabilities.\nHowever, these models are sometimes prone to generating credible-sounding but\nincorrect information, a phenomenon known as LLM hallucinations. Reliable\nuncertainty estimation in LLMs is essential for fostering trust in their generated\nresponses and serves as a critical tool for the detection and prevention of erro-\nneous or hallucinated outputs. To achieve reliable and well-calibrated uncertainty\nquantification in open-ended and free-form natural language generation, we pro-\npose an uncertainty-aware fine-tuning approach for LLMs. This approach en-\nhances the model's ability to provide reliable uncertainty estimates without com-\npromising accuracy, thereby guiding them to produce more trustworthy responses.\nWe introduce a novel uncertainty-aware causal language modeling loss function,\ngrounded in the principles of decision theory. Through rigorous evaluation on\nmultiple free-form question-answering datasets and models, we demonstrate that\nour uncertainty-aware fine-tuning approach yields better calibrated uncertainty es-\ntimates in natural language generation tasks than fine-tuning with the standard\ncausal language modeling loss. Furthermore, the experimental results show that\nthe proposed method significantly improves the model's ability to detect halluci-\nnations and identify out-of-domain prompts.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) have shown remarkable success in various natural language pro-\ncessing tasks (Touvron et al., 2023; Gemma et al., 2024; Achiam et al., 2023) and are increas-\ningly becoming ubiquitous in a variety of domains for their decision-making and reasoning abili-\nties (Eigner & H\u00e4ndler, 2024). However, their real-world deployment, particularly in high-stakes\nand safety-critical applications, is hindered by challenges such as hallucinations and out-of-domain\nprompts, which can lead to the generation of erroneous or nonsensical outputs. Hallucinations, of-\nten described as plausible-sounding but incorrect or unfaithful model generations (Ji et al., 2023),\npresent a crucial challenge in developing trustworthy systems especially in critical domains such as\nmedical (Ahmad et al., 2023) and legal (Magesh et al., 2024). The ability to recognize out-of-domain\nprompts and to acknowledge the limits of a model's knowledge base paves the way for building safe\nAI systems (Amodei et al., 2016).\nUncertainty quantification (UQ) in LLMs plays a pivotal role in understanding what the model\nknows and does not know, which is an active area of research for free-form natural language gener-\nation (NLG) (Kadavath et al., 2022; Kuhn et al., 2023; Lin et al., 2024). UQ methods has emerged\nas a step towards determining the trustworthiness of responses generated by LLMs (Fadeeva et al.,\n2023; Plaut et al., 2024; Kadavath et al., 2022). Uncertainty estimation techniques such as semantic\nentropy (Kuhn et al., 2023) have shown to be effective indicators in detecting 'confabulations' (Far-\nquhar et al., 2024), a subcategory of hallucinations characterized by the generation of arbitrary and\nincorrect responses.\nThe calibration of uncertainty estimates is crucial for the reliability of LLMs; a well-calibrated\nmodel should correlate low uncertainty with accurate responses and high uncertainty with likely\nincorrect responses. However, recent studies (Xiong et al., 2024; Yang et al., 2024) have revealed\nthat LLM predictions are often poorly calibrated, leading to overconfidence in incorrect outputs."}, {"title": "BACKGROUND AND RELATED WORKS", "content": null}, {"title": "UNCERTAINTY ESTIMATION IN NATURAL LANGUAGE GENERATION", "content": "We refer to Abdar et al. (2021); Gawlikowski et al. (2023) for surveys on uncertainty quantification\n(UQ) in deep learning. In machine learning models, predictive uncertainty is composed of two\nprimary sources: epistemic uncertainty, associated with model's lack of knowledge and aleatoric\nuncertainty - the inherent noise in the data or observation. As LLMs continue to evolve rapidly,\nthere is an increasing interest in enhancing our understanding of the uncertainty associated with\nLLM responses for developing trustworthy and reliable systems (Fadeeva et al., 2023). UQ methods\nfrom deep learning can be effectively applied to structured natural language processing tasks, such\nas text classification (Xiao & Wang, 2019) and multiple-choice question answering (Kumar et al.,\n2023). However, the application of these methods to free-form natural language generation presents\ndistinct challenges."}, {"title": "MODEL CALIBRATION", "content": "Calibration is important in applications where decision-making relies on not just the model's pre-\ndictions, but also on the trustworthiness of its uncertainty scores. It is important to capture well-\ncalibrated uncertainty estimates of a model for creating reliable and trustworthy systems. Model\ncalibration is a well-explored area of research in deep learning, with a variety of strategies proposed\nto enhance the calibration of deep neural networks for classification and regression tasks. These\nstrategies include post-hoc rescaling techniques (Guo et al., 2017; Kull et al., 2017), which adjust\nthe model's predictions to better align with true event likelihoods; data augmentation, which en-\nriches the training dataset to promote generalization (Thulasidasan et al., 2019; Hendrycks et al.,\n2020); and probabilistic modeling approaches that integrate uncertainty directly into the model's\narchitecture (Blundell et al., 2015; Lakshminarayanan et al., 2017). Other line of works utilize ex-\nplicit calibration loss functions during training to directly optimize for calibration (Kumar et al.,\n2018; Krishnan & Tickoo, 2020; Mukhoti et al., 2020; Karandikar et al., 2021), that has resulted in\nbetter calibrated models.\nCalibration of LLMs for natural language processing tasks is an ongoing area of research. Prior\nworks have largely focused on refining LLMs for structured tasks like text classification (Kong\net al., 2020) or multiple-choice question answering (Desai & Durrett, 2020; Jiang et al., 2021). Stud-\nies like the one by Xiong et al. (2024) have highlighted that despite the impressive performance of\nfoundational LLMs on a wide array of tasks, these models often exhibit poor calibration, particularly\nexhibiting overconfidence in their predictions. As LLMs are increasingly used in natural language\ngeneration tasks, new calibration techniques (Geng et al., 2024) are emerging to enhance the relia-\nbility of the generated text. More recently, Liu et al. (2024b) introduced a calibration technique for\nLLMs that trains a single linear layer over the model's last hidden layer representations to predict\na bias term, which is then used to adjust the model's logits and alter the generation confidence for\nshort-form and long-form responses. Band et al. (2024) propose a training objective for linguistic\ncalibration, utilizing reinforcement learning to optimize and calibrate long-form text generations.\nKapoor et al. (2024) proposed a calibration tuning method designed for LLMs in multiple-choice\nquestion-answering settings. Prior work by Liu et al. (2024b) has shown that standard fine-tuning\nof LLMs can lead to poorer calibration. The calibration of LLMs for free-form text generation, as\nwell as uncertainty-aware fine-tuning, represents a significant open area of research. Our work ad-\ndresses this gap by developing an uncertainty-aware fine-tuning method for LLMs, fine-tuning less\nthan 1% of the model parameters, to achieve well-calibrated models for free-form natural language\ngeneration."}, {"title": "FINE-TUNING LARGE LANGUAGE MODELS", "content": "With the emergence of foundation models, fine-tuning have become a common practice in the field\nof natural language processing, enabling the adaptation of general-purpose pre-trained models to\nspecialized tasks and domains. As fine-tuning a LLM with billions of parameters can be resource-\nintensive, parameter-efficient fine-tuning (Mangrulkar et al., 2022) strategies have been proposed.\nThese parameter-efficient fine-tuning techniques also mitigate catastrophic forgetting more effec-\ntively in comparison to full fine-tuning (Wang et al., 2022). One approach is to update only a subset\nof the model's parameters, such as adapter modules (Houlsby et al., 2019) or Low-Rank Adaptation"}, {"title": "UNCERTAINTY-AWARE CAUSAL LANGUAGE MODELING", "content": "Motivated by the need to overcome the challenges of uncertainty miscalibration (Xiong et al., 2024)\nin Large Language Models (LLMs) and the increasing trend of fine-tuning pre-trained foundational\nmodels for domain-specific adaptation where fine-tuned LLMs often exhibit overconfidence in\ntheir predictions (Kong et al., 2020) we propose a novel uncertainty calibration fine-tuning ap-\nproach for natural language generation settings. We introduce a novel uncertainty-aware causal\nlanguage modeling loss based on the principles of decision theory (Murphy, 2012). Our fine-tuning\napproach emphasizes increasing the uncertainty for wrong token predictions, while optimizing for\naccuracy and certainty for correct token predictions. Decision theory offers a mathematical and\ntheoretical framework that guides to achieve optimal predictions by employing a task-specific util-\nity function. Within the decision theory framework, our task is to generate natural language text\naccompanied by reliable uncertainty estimates. The utility function in this scenario is represented\nby the uncertainty-aware optimization objective function that is aimed at producing well-calibrated\nuncertainty estimates for causal language modeling. We design a differentiable loss function that\nincentivizes the model to yield low uncertainty when it generates correct tokens, and encourages the\nmodel to exhibit high uncertainty when it is at risk of predicting the next token incorrectly.\nIn causal language modeling, the goal is to predict the next token in a sequence given the previous\ntokens. Given a sequence of tokens [W1, W2, ..., wr], where T is the length of the sequence and each\ntoken wi is an element from a fixed vocabulary of size V, the model aims to learn the conditional\nprobability distribution Po(wi|W0:i-1) for each token w\u2081 given the preceding set of tokens Wo:i-1;\nwhere, 0 represents the parameters of the LLM. The loss function for standard causal language\nmodeling (CLM) is typically the negative log-likelihood as defined in Equation 1 below.\nLCLM :=1T\u03a3log Po(Wi|Wo:i-1)\ni=0\nDesideratum: The desired and ideal outcome in causal language modeling is to achieve a state\nwhere every correctly generated token is assigned low predictive uncertainty, and high predictive\nprobability, reflecting the model's high confidence in its accuracy. Conversely, for every token that\nis generated incorrectly, the model should assign high uncertainty, and low predictive probability,\ndenoting low confidence in these instances. This ensures that the model's confidence levels and\nuncertainty estimates are perfectly calibrated with the actual correctness of its predictions.\nWe define the uncertainty-aware causal language modeling (UA-CLM) loss in Equation 2 based on\nthe above desideratum. The loss function captures the trade-off between predictive accuracy and\nuncertainty calibration, and is composed of two terms: one that deals with incorrectly generated\ntokens and one that deals with correctly generated tokens.\nLUA-CLM :=1|C|\u03a3\u00a1E\u010cPo(Wi|Wo:i-1)log(tanh(Hi))\nUtility function for incorrect tokens\u22121|C|\u03a3i EC(1 \u2013 Po(wi|Wo:i-1))log(1 \u2212 tanh(Hi))\nUtility function for correct tokens\nwhere,\nHi := \u2212 \u03a3Po(w? |Wo:i-1)log Po(w? |Wo:i-1)\nj=1"}, {"title": "EXPERIMENTS AND RESULTS", "content": "We perform extensive empirical evaluation to compare our proposed uncertainty-aware causal lan-\nguage modeling (UA-CLM) fine-tuning method to the standard causal language modeling (CLM)\nfine-tuning, pre-trained baseline, unlikelihood training (ULT) (Welleck et al., 2020), and calibration\ntuning (CT) (Kapoor et al., 2024) methods. We evaluate on open-ended, and free-form natural lan-\nguage generation tasks. Our comprehensive evaluation rigorously assesses the quality of uncertainty\nestimates and the quality of the generated text. This includes an analysis of broadly four aspects:\nhallucination detection, uncertainty-guided selective generation, out-of-domain prompt detection,\nand calibration analysis based on the inverse correlation between the uncertainty estimates and the\nquality of generated text."}, {"title": "EXPERIMENTAL SETTINGS", "content": "Datasets We utilize free-form question-answering (QA) datasets to evaluate the proposed methods\non LLMs: CoQA (Reddy et al., 2019), an open-book conversational QA dataset; TriviaQA (Joshi\net al., 2017), a reading comprehension QA dataset. These datasets are frequently utilized bench-\nmarks for evaluating uncertainty quantification (UQ) in LLMs for natural language generation, as\nevidenced by prior works from Kuhn et al. (2023), Lin et al. (2024), and Farquhar et al. (2024).\nWe employ the OK-VQA dataset (Marino et al., 2019), an open-ended visual question-answering\n(VQA) dataset to extend our evaluation to large vision language models (LVLMs), thereby pro-\nviding a comprehensive analysis of our approach across diverse open-ended free-form QA tasks.\nAdditionally, we use BioASQ (Krithara et al., 2023), a biomedical question-answering dataset for\nthe evaluation of out-of-domain prompt detection. In our experiments, we utilize the development\nsplit of the CoQA dataset, which contains approximately 8,000 question-answer pairs, the validation\nsplit of TriviaQA with around 10,000 question-answer pairs, and the validation split of OK-VQA,\ncomprising roughly 5,000 question-answer pairs along with their corresponding images. For each\ndataset, we allocate 20% of the data for fine-tuning purposes, while the remaining 80% serve as the\ntest sets for evaluation. We use a standard prompt across all datasets, more details on the datasets\nand the prompt are provided in Appendix A.1.1 and A.1.2, respectively.\nModels We use the Llama-2 models with 7B and 13B parameters (Touvron et al., 2023) and the\nGemma model with 2B parameters (Gemma et al., 2024) for the free-form QA experiments. Addi-\ntionally, we utilize the LLaVA-1.5 model with 7B parameters (Liu et al., 2024a) for the open-ended\nvisual question-answering task.\nFine-tuning We perform parameter-efficient fine-tuning using the Low-rank Adaptation (LoRA)\nframework (Hu et al., 2022), where less than 1% of model parameters are trainable. The models\nundergo fine-tuning as described in Section 3 for the uncertainty-aware causal language modeling\nmethod. For comparative purposes, we also fine-tune models using the standard cross-entropy loss to\nevaluate against the standard causal language modeling fine-tuning method. For all our experiments,\nthe models are fine-tuned for a concise duration of 3 epochs, utilizing only 20% of the data split.\nThe optimization is carried out using the AdamW optimizer (Loshchilov & Hutter, 2019), with an\ninitial learning rate of 1e-4, a weight decay of 0.001, and a warm-up ratio of 0.03. We follow the\nsame setup for both CLM and UA-CLM methods for a fair comparison. During the fine-tuning\nprocess, only the LoRA parameters are updated, while all other model parameters remain frozen.\nWe provide more details on the hyperparameters and implementation in Appendix A.1.3, to facilitate\nthe reproducibility of the results."}, {"title": "DISCUSSION", "content": "We proposed a novel fine-tuning approach to improve uncertainty calibration in Large Language\nModels (LLMs) devised for natural language generation. Our method incorporates a differentiable\nuncertainty-aware causal language modeling loss, which is grounded in the principles of decision\ntheory. This loss function is designed to enhance the model's ability to provide well-calibrated uncer-\ntainty estimates without compromising the quality of text generation, a crucial aspect of trustworthy\nAl models.\nOur extensive empirical evaluations on open-ended and free-form question-answering tasks has\nshown that the uncertainty-aware causal language modeling approach yield better-calibrated uncer-\ntainty quantification, which in turn significantly enhances the model's ability to detect hallucinations,\nidentify out-of-domain prompts, and selective generation decisions. We demonstrated the general-\nizability of the proposed fine-tuning method to different text generation tasks. We also introduced a\nnovel application of correlation analysis to the evaluation of sentence-level uncertainty calibration in\nfree-form text generation, accounting for the varying sentence lengths between generated responses\nand ground-truth references.\nLimitations and Future work: Currently, the proposed method is tailored to white-box model\nsettings, where the model internals are accessible for calibration fine-tuning. However, there is a\npotential to extend uncertainty-aware fine-tuning to black-box models by calibrating an auxiliary\nmodel, or prompt tuning for calibrated uncertainty quantification. Additionally, the focus of this\nwork has been on calibrating token-level uncertainty, which sets the stage for the exploration of\ncalibrating sentence-level uncertainty. We plan to explore these two avenues in our future work. We\nhope this work opens new avenues for the research community to enhance the uncertainty calibration\nin LLMs for free-form natural language generation.\nIn conclusion, this work contributes towards the broader goal of developing trustworthy LLMs. The\nability to recognize out-of-domain prompts and to acknowledge the limits of a model's knowledge\nbase through reliable uncertainty quantification paves the way for reducing hallucinations and en-\nhancing decision-making in AI systems."}, {"title": "APPENDIX", "content": null}, {"title": "EXPERIMENTAL DETAILS", "content": null}, {"title": "DATASETS", "content": "CoQA Conversational Question Answering (CoQA) (Reddy et al., 2019) dataset was developed to\nevaluate models' ability to respond to natural, dialogue-based questions, with free-form text answers\nsupported by highlighted evidence from the passage. The full dataset comprises of 127k question-\nanswer pairs derived from 8k conversations based on text passages across 7 distinct domains. For all\nour experiments, we utilize the development subset of CoQA, which consists of 8k question-answer\npairs. Figure 4 shows the color-coded co-reference chains in CoQA as illustrated in the (Reddy\net al., 2019).\nTriviaQA TriviaQA (Joshi et al., 2017) is a reading comprehension dataset consisting of over\n650k question-answer-evidence triplets. It includes 95,000 question-answer pairs authored by trivia\nenthusiasts, along with an average of six independently gathered evidence documents per question,\nproviding high-quality distant supervision for answering the questions. In our experiment, we used\nthe validation split of the dataset with around 10,000 question-answer pairs. Table 5 shows some of\nthe samples from the dataset.\nOK-VQA Outside Knowledge-Visual Question Answering benchmarks (Marino et al., 2019) con-\nsists of visual queries where the image content alone is not sufficient to answer the questions. Thus,\nit requires models to incorporate external knowledge to generate accurate answers. The dataset con-\nsists of 14k questions across 10 knowledge categories. In our experiment, we used the validation\nsplit of the dataset with around 5k question-answer pairs. Figure 5 shows a few samples from the\ndataset across different knowledge categories."}, {"title": "PROMPT TEMPLATE", "content": "Open-book QA Prompt:\nAnswer the following question as briefly as possible.\nContext: [Provided context paragraph]\nQuestion: [Associated Question]\nAnswer:\nBiography generation Prompt:\nYou are an Al assistant. You use a tone that is technical and scientific.\nUSER: Write a paragraph for [name]'s biography.\nASSISTANT:"}, {"title": "FINETUNING HYPERPARAMETERS AND IMPLEMENTATION", "content": "We fine-tune the models for generic causal language modeling (CAUSAL_LM) task in autore-\ngressive manner that predict the next token in a sequence based on the preceding tokens. In the\nCAUSAL LM task, labels are created directly from the prompt itself by using the subsequent to-\nkens in the sequence as the target labels for prediction. For each position in the prompt sequence,"}, {"title": "TEXT GENERATION QUALITY METRICS", "content": "Recall-Oriented Understudy for Gisting Evaluation\n(ROUGE) is a widely-used evaluation metric for assessing the quality of text generated\nbased on n-gram matching. We use the Rouge-L variant which uses the longest common\nsubsequence between the generated answer and the ground truth answer.\nExact Match (EM) metric is a stringent evaluation criterion used\nto assess the performance of models on tasks such as question answering (QA), where a\ngenerated response is compared to a reference answer. It is a widely used metric for open-\nbook QA, this metric evaluates a model's ability to extract the precise text span from the\ncontext to answer a question.\nAccuracy: The generated answer is considered as accurate if it achieves Rouge-L(y, \u0177) >\n0.3, for a given reference answer y and a model generation \u0177. We follow this criterion for\nquantifying accuracy in free-form text generation based on the findings from (Kuhn et al.,\n2023) that demonstrated this criterion closely matches the human evaluation accuracy on\nCOQA and TriviaQA datasets, both of which are utilized in our experiments.\nBERTScore utilizes word embeddings to compute a\nsimilarity score between the tokens in the prediction and ground truth and has shown to\nwell correlate with human judgement. We report Precision, Recall and F1 BERTScores for\nall our experiments."}]}