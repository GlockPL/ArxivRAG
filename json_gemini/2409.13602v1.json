{"title": "MeLIAD: Interpretable Few-Shot Anomaly Detection with Metric Learning and Entropy-based Scoring", "authors": ["Eirini Cholopoulou", "Dimitris K. Iakovidis"], "abstract": "Anomaly detection (AD) plays a pivotal role in multimedia applications for detecting defective products and automating quality inspection. Deep learning (DL) models typically require large-scale annotated data, which are often highly imbalanced since anomalies are usually scarce. The black box nature of these models prohibits them from being trusted by users. To address these challenges, we propose MeLIAD, a novel methodology for interpretable anomaly detection, which unlike the previous methods is based on metric learning and achieves interpretability by design without relying on any prior distribution assumptions of true anomalies. MeLIAD requires only a few samples of anomalies for training, without employing any augmentation techniques, and is inherently interpretable, providing visualizations that offer insights into why an image is identified as anomalous. This is achieved by introducing a novel trainable entropy-based scoring component for the identification and localization of anomalous instances, and a novel loss function that jointly optimizes the anomaly scoring component with a metric learning objective. Experiments on five public benchmark datasets, including quantitative and qualitative evaluation of interpretability, demonstrate that MeLIAD achieves improved anomaly detection and localization performance compared to state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "NOMALY detection (AD) refers to the process of recognizing patterns in data that deviate from normal, a process that can be of high practical significance in diverse multimedia applications, including video surveillance and defect detection [1]. AD is considered a key component in multimedia-based quality inspection that helps assure high production quality and cost efficiency in identifying and discarding defective products. Also, it is particularly challenging considering that in real-world applications, anomalous instances are scarce, heterogenous, and not readily available compared to normal data.\nConsidering the data availability and heterogeneity of anomalies, various AD methods have been developed requiring different levels of supervision. Fully supervised methods assume all possible defect classes to be known beforehand; therefore, they are unsuitable in real-world scenarios where not all defect classes are known a priori. To address this limitation supervised one-class classification AD methods have been proposed, capable of learning from normal samples to discriminative samples that deviate from normal [2]. Also, various unsupervised AD methods have been proposed [3], [4]. However, these approaches often incur a high false positive rate [5]. Few-shot learning has been proposed as a less demanding supervised approach for AD, capable of learning to discriminate anomalies from only a few annotated training samples that do not necessarily belong to one class [6], [7] . Interestingly, it has been recently shown that some of the few shot AD methods are capable of generalizing well, even to new, previously unknown, anomaly classes [8]. However, such methods often impose prior distribution assumptions that may deviate from that of the true anomalies, limiting their generalization capacity to unseen classes. Although this is a promising direction towards real- world AD, the works tackling this challenge are still limited.\nFurthermore, in AD it is often required to have some cues explaining why a sample is characterized as anomalous. This means that the model should not only identify anomalies but also provide visual explanations that highlight the specific features or regions of the data that contributed to the anomaly detection. This can be particularly useful in AD systems, e.g., to adhere to fairness requirements or to investigate possible bias effects [9]. Methods with such a capacity are characterized as interpretable. In the context of AD only a limited number of such methods have been proposed [10], [11]. However, most current approaches address interpretability not inherently, but using simplified post-hoc models. These models do not consider the computations of the original model, and their use may lead to unreliable interpretations [12].\nIn this study, we propose a novel Metric Learning and Entropy-based methodology for Interpretable Anomaly Detection (MeLIAD) in images. This methodology is motivated by the need of trustworthy methods, inherently able to reveal the rationale behind the anomaly detection and scoring, without any prior assumptions on the distribution of the true anomalies. It is based on Convolutional Neural Networks (CNNs) and it requires only a few samples from a subset of anomaly classes for training. MeLIAD derives anomaly scores by using an entropy-based measure to identify the most informative feature maps, by assessing their activation probability to identify the level of abnormality in images. The regions in the images with the highest entropy scores correspond to the most anomalous regions, which are highlighted in the form of interpretation"}, {"title": "II. RELATED WORK", "content": "Traditional AD methods usually leverage statistical measures and information theory [13]. These methods can be effective; however, they struggle to adapt in complex and high- dimensional data representations. More recently, deep learning methods with automatic feature extraction capabilities, have been proposed to offer enhanced adaptivity in solving AD problems [14]. Some of these approaches usually leverage pre- trained deep learning models only for the task of feature extraction, and handle the anomaly scoring process as a separate task [15]. Recently, the joint optimization of feature extraction with anomaly scoring has been proved a promising direction providing improved AD performance; however, there is still only a limited work, with the current methods being fully supervised [16]."}, {"title": "A. Few-shot Anomaly Detection", "content": "Few-Shot AD (FSAD) methods are becoming increasingly popular in anomaly detection multimedia applications due to their fewer training requirements [17]. Some FSAD approaches leverage energy-based generative models to synthesize defective samples based on the available ones [18], and hierarchical generative models (HGM) for this purpose [19]. A recent approach, called FastRecon [20], utilizes a few normal samples to reconstruct the normal versions of the anomalies, and then AD is achieved by sample alignment. There is a limited number of FSAD methods that do not require all classes to be represented in the training set. Among these, Registration- based few-shot Anomaly Detection (RegAD) [21] employs a registration-based scheme to train a category-agnostic learning model, and Deviation Networks (DevNet) [22] is based on deviation learning that leverages prior probabilities. These methods have demonstrated a remarkable performance outperforming the most recent approaches."}, {"title": "B. Interpretable Anomaly Detection", "content": "In AD interpretability refers to the extraction of useful insights from a model into why specific data are identified as anomalous [23]. The demand for interpretation of decisions made by black box systems, such as artificial neural networks, has led to the use of model-agnostic tools that offer insights about deep learning-based AD in a post-hoc manner i.e., applied after the training process is complete [24]. Examples of interpretable AD (IAD) methods using post hoc techniques include attention-based models [25] and reconstruction-based methodologies [11], that provide saliency maps for individual predictions.\nMethodologies that reveal the rationale behind the anomaly scoring mechanism, thus incorporating interpretability directly into the model architecture, are characterized as inherently interpretable or interpretable by design [12]. In AD, such approaches can ensure interpretations that are more coherent with how the anomaly scores drive the decisions of the machine learning models [26]. Inherently IAD approaches include Fully Convolutional Data Description (FCDD) [27], which employs a gradient-based mechanism to generate anomaly heatmaps. Similarly, multiresolution knowledge distillation (MKD) [28] utilizes a cloner network to distill feature information and produce anomaly interpretation heatmaps. DevNet [22], learns prior-driven anomaly scores and provides interpretations by attributing them to the inputs through gradient backpropagation.\nMeLIAD is inspired by the most recent methods targeting real-world applications, such as DevNet, which do not require training samples from all possible anomaly classes. Most AD methods, incorporating anomaly scoring components, e.g., [29], are based on specific distance measures to identify anomalies, and they are not considered inherently interpretable, since the anomaly scores are not directly optimized by the model. Unlike the previous methods, MeLIAD introduces an entropy-based anomaly scoring mechanism, that is trained jointly with metric learning to optimize the entire AD process in an end-to-end inherently interpretable manner. This scoring component is used to generate visual interpretations that indicate why an image was identified as anomalous, providing insights into the decision-making process of the model."}, {"title": "III. METHODOLOGY", "content": "An overview of MeLIAD is schematically illustrated in Fig. 1. It is a few-shot learning methodology that consists of three components, namely the feature extraction, the anomaly scoring, and the anomaly interpretation component. During its training phase (dotted lines), the feature extraction is optimized by a metric learning objective, the anomaly scoring component is optimized by a novel entropy-based loss function, and both components are adapted using another loss function designed to jointly optimizing the AD performance in an end-to-end manner.\nThe first component implementing feature extraction includes a pre-trained network f and a feature reduction block \u03c6. Network f receives an image x as input and outputs a set of D feature maps with size wxh from its last convolutional layer. Block o aims to reduce the computational load of the next processing steps by reducing the number of feature maps to D' < D with the application of a series of convolutions, while maintaining their size.\nThe second component n, implements anomaly scoring, i.e., it assigns a score quantifying the degree to which the input image can be characterized as anomalous. This is performed in two stages: a) the feature maps $z_i$, i = 1, ..., D' are transformed to embeddings $e_i^c$, i = 1, ..., D', c = 0,1, where c represents the class of the input image (0 normal / 1 abnormal). These embeddings aim to provide representations that are more focused on the anomalies. They are computed by a linear transformation of the feature maps $a_i$, i = 1, ..., D', which are class-wise reweighted instances of the feature maps $z_i$, highlighting the features that discriminate their contents upon the class they belong to; b) the embeddings $e_i^c$ are then reduced by applying a linear transformation along the depth of the tensor, to derive a set of anomaly scores $s^c = (s_1^c, ..., s_{w \\times h}^c)$, c = 0,1. Specifically, the output vectors s combine the information from the D' embeddings into a single output value for each spatial location within the embeddings to indicate the likelihood of an anomaly being present. The final anomaly score $\\hat{s}^c$ is derived by applying a global max pooling operation on the derived anomaly scores, so that the highest anomaly score is selected. If the highest"}, {"title": "B. Problem Statement", "content": "This study addresses the challenge of detecting anomalies by leveraging a limited set of labelled images, that includes only a subset of a few known anomalous instances. Let us consider a training set $X = \\{x_1, x_2, ..., x_N, x_{N+1}, ..., x_{N+A}\\}$, where the majority of the labeled images are normal, $X_n = \\{x_1, x_2, ..., x_N \\}$, and only a limited set of them are abnormal, $X_a = \\{x_{N+1}, x_{N+2}, ..., x_{N+A}\\}$, A << N. The objective is to learn a function $\u03c4(\u00b7): X \u2192 R$ that assigns an anomaly score $\\hat{s}^c = \u03c4(x)$ to a sample x, where c represents the class (0 normal / 1 abnormal), such that $\\hat{s}^1 > \\hat{s}^0$."}, {"title": "C. Feature Extraction and Metric Learning", "content": "Feature extraction can be implemented by any given neural network f that learns to map input images (x) to lower- dimensional representations f(x). The network is followed by a feature reduction block \u03c6(\u00b7; $\u0398_\u03b3$), with $\u0398_\u03b3$ = {$W^l$|$l$ = 1, ..., $l'$} represent its trainable parameters, where $W^l$ is the weight matrix corresponding to the convolutional layer l. This block consists of four convolutional layers using Rectified Linear Units (ReLU) activation functions, each followed by batch normalization. The final layer of the reduction block is a 1\u00d71 convolutional layer that reduces the dimensionality to the desired D' output dimension.\nGenerally, in deep metric learning the objective is to learn a highly separable embedding space, in which the distance between feature representations of the same class is minimized and maximized for representations of different classes. Given two feature maps $z_i = \u03c6(f(x_i))$ and $z_j = \u03c6(f(x_j))$ the distance metric in the embedding space can be calculated as:\n$d_{ij} = ||z_i - z_j||$ (1)\nwhere || || represents the cosine distance.\nIn order to optimize the metric learning objective to effectively separate normal data from anomalies, the adaptive margin-based loss [30] is adopted to facilitate the learning process of the distance metric defined in (1). This loss was chosen because it is well-recognized for its effectiveness in learning pairwise relationships, by leveraging distance weighted sampling to select more informative datapoints than traditional approaches. It is defined as:\n$l_{margin} := max(0, (\u03bc + y_{ij}(d_{ij} \u2212 \u03b2))$ (2)\nwhere $y_{ij}$ \u2208 {0,1} indicates if a pair of samples is positive ($y_{ij}$ = 1), or negative ($y_{ij}$ = 0), \u03bc is the hyperparameter that defines the minimum margin for the separation between positive and negative pairs and \u1e9e determines a shift of the margin boundary. During training, the minimization of (2) encourages the network to adjust its trainable parameters to learn the feature representations $z_i$ by assigning smaller distance values between positive pairs and larger distance values between negative pairs by a margin, ultimately enhancing the capacity of the model to identify anomalies."}, {"title": "D. Anomaly Scoring", "content": "The purpose of the anomaly scoring component is to facilitate inherently interpretable anomaly learning by encouraging the model to identify anomalous feature representations and derive visual interpretations. The anomaly scoring process in the proposed methodology is defined as a mapping function $\u03b7(\u00b7): R^{D'} \u2192 R$ that is performed in two stages. The first one includes the transformation of the feature representations $z_i$, i = 1, ..., D' to embeddings $e_i^c$, i = 1, ..., D', c = 0,1. These embeddings can be formulated as a linear transformation:\n$e_i^c = W^{l'+1}\u03b1_i + b^c$ (3)\nwith trainable parameters $\u0398_u$ = {$W^{l'+1},b^c$ }, where $W^{l'+1} \u2208 R^{D' \\times D'}$ is a weight matrix, and b = $(b_1^c, ..., b_{D'}^c)$ is a vector composed of bias terms. The embeddings $\u03b1_i^c$, i = 1, ..., D' are computed as the elementwise product between the feature representations $z_i$, i = 1, ..., D' and the probability distribution of the weights $W^{l'+1}$ that indicate the relevance of specific patterns to each respective class:\n$\u03b1_i^c = z_i \\odot \u03c3(\\frac{W^{l'+1}}{||W^{l'+1}||_F} t )$ (4)\nwhere a represents the SoftMax operation, $ \\odot$ denotes the elementwise product, $||\u00b7||_F$ denotes the Frobenius norm, and t is a regularization temperature parameter. The output of the SoftMax is expanded along the spatial dimensions to $R^{w \\times h \\times D'}$ to match the dimensionality of $z_i$, in order to perform the element-wise operation. The use of the SoftMax activation function aims to highlight the most relevant patterns to a class, by amplifying the higher weight values and diminishing the lower values. Thus, the values of the reweighted feature maps $\u03b1_i^c$ signify the abnormality degree of each feature map, with higher values (t\u21920) indicating increased involvement of anomalous patterns in an image.\nThe next step involves the computation the anomaly scores $s^c$, by reducing the embeddings $e_i^c$ along the depth dimension, to compress their information and focus on the most relevant features for anomaly detection. Each element_in_$s^c$ = $(s_1^c,..., s_{w \\times h}^c)$, is obtained by a linear combination of the corresponding elements along the depth of the tensor $e_i^c$, expressed as:\n$s^c = W^{l'+2}e_i^c + b$ (5)\nwith trainable parameters $\u0398_s$ = {$W^{l'+2},b$}, where $W^{l'+2} \u2208 R^{1 \\times D'}$ is the weight matrix and b \u2208 R the bias parameter. Then the final anomaly score $\\hat{s}^c$ is derived by selecting the maximum value across the depth of the anomaly scores tensor.\nTherefore, the overall process of mapping an anomaly score to an input image x can be addressed in an end-to-end manner, expressed as an anomaly score learning function $\u03c4(\u00b7; \u0398): X \u2192 R$, that can be represented as:\n$\u03c4(\u00b7; \u0398) = \u03b7((\u03c6; \u0398_\u03b3); \u0398_u, \u0398_s)$, (6)\nwhere $\u0398$ = {$\u0398_\u03b3, \u0398_\u03b7, \u0398_s$} are the respective trainable parameters of MeLIAD."}, {"title": "E. Loss Function for Joint Optimization", "content": "A loss function is proposed to facilitate the construction of a feature representation space by learning the distance metric defined in (1), while simultaneously refine the anomaly scores to align with the objective of the anomaly scoring function \u03c4. This loss function aims to enhance the robustness of the model in tasks characterized by imbalanced class distributions.\nTo this end, the hypersphere classification (HSC) loss [31], is adapted so that it leverages the entropy to optimize the derived anomaly scores, instead of optimizing the Euclidean distance of the mapped feature representations as anomaly scores. Given the anomaly scores $\\hat{s}^c$ (predicting class c) and target labels $y_i$ \u2208 {0,1}, where $y_i$ = 1 denotes an anomalous sample and $y_i$ = 0 denotes a normal sample, the loss function can be expressed as:\n$l_{entr} = \\frac{1}{N+A} \\sum_{i=1}^{N+A} (1-y_i) \\hat{s}^c - y_i log(1 - e^{-\\hat{s}^c})$ (7)\nThis loss function penalizes higher anomaly scores for normal samples ($y_i$ = 0) and low anomaly scores for anomalous samples ($y_i$ = 1), which is consistent with the condition $\\hat{s}^1 > \\hat{s}^0$ of the anomaly score learning function \u03c4.\nThe joint optimization of both objectives can be achieved in an end-to-end manner, by the minimization of the following loss function formulation:\n$L_{total} = l_{entr} + \u03bbl_{margin}$ (8)\nwhere \u03bb is a positive hyperparameter that controls the impact of the $l_{margin}$ objective. The first component of the loss function, $l_{entr}$, refines the anomaly scores to receive higher values for images with anomalies, and lower values for normal images, by minimizing the discrepancy between the predicted and the true anomaly scores. The second component, $l_{margin}$, learns a distance metric to effectively map the feature representations of the input data. The combined effect of these two losses contributes to the holistic optimization of the model. Higher values of lambda place more emphasis on the margin loss and encourage the model to learn decision boundaries that are more discriminative, whereas lower values of lambda place more emphasis on the $l_{entr}$ loss. Thus, $l_{entr}$ encourages the model to use the most informative feature representations towards assigning anomaly scores that predict the degree to which data samples deviate from normal."}, {"title": "F. Anomaly Interpretation", "content": "The inherent interpretability of the model, involves using the anomaly scoring mechanism to derive interpretations, by selecting the most informative feature maps of an image. The regions in the feature maps with the highest entropy-based anomaly scores, correspond to the most anomalous regions, which are highlighted in the form of interpretation heatmaps. The generation of an interpretation heatmap S highlights the most active regions of the input image that contribute to its classification as anomalous. More specifically, the anomaly interpretation process of MeLIAD involves the backward propagation of the loss through the network with respect to the feature map activations $z_i$, to receive $M_i$, i = 1, ..., D' activation maps which are formulated as:\n$M_i = \\sum_{n=1}^{W} \\sum_{m=1}^{h} \\frac{\u2202\\hat{s}^c}{\u2202z_{n,m}}$ (9)\nwhere $z_{n,m}$ is the ith feature map with indices n = 1, ..., w and m = 1, ..., h denoting the spatial positions of width and height respectively and $\\hat{s}^c$ refers to the score for the class c, with the gradients of the abnormal class (target class) set to 1 and for the normal class set to zero.\nThe interpretation heatmap is then derived as $S = G_\u03c3(\\sum(M))$ by aggregating the most informative activation maps M. These maps correspond to the top H entropy scores of the respective activations $M_i$, computed by the Shannon entropy measure. $G_\u03c3(\u00b7)$ denotes a 2D Gaussian filter with standard deviation \u03c3, that is utilized for noise reduction and smoothing. To visualize the interpretation heatmap, S is upsampled with bilinear interpolation to match the size of the original input image x."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "The proposed methodology was thoroughly evaluated using 5 public AD datasets presented in Table I. The MVTec AD [32] dataset, is a large publicly available benchmark dataset widely acknowledged as a reference for industrial anomaly detection. It consists of 5,354 images of 15 substantially different object and texture classes, comprising a set of 3,629 normal images and a set of 1,725 images with various kinds of anomalies, including bottles, cables, capsules, hazelnuts, metalnuts, pills, screws, toothbrushes, transistors, zippers, and images of carpet, grid, leather, tile and wood textures. Each category is divided into a training set, that contains only normal images, and a test set that contains both normal images and images of various types of anomalies. The images vary in resolution, ranging from 700\u00d7700 to 1024\u00d71024 pixels. The recent GoodsAD dataset [33], consists of six categories of supermarket goods, including boxed cigarettes, bottled drinks, canned drinks, bottled foods, boxed foods, and packaged foods. Each category has several types of anomalies. GoodsAD includes 6,124 images, with 4,464 images of normal goods and 1,660 images of anomalous goods. The Mobile phone screen Surface Defect (MSD) dataset [34], consists of 1,200 images that depict three types of defects: oil, stain and scratch. The Kolektor Surface-Defect Dataset (KolektorSDD) [35] consists of 399 images from which 52 have surface defects and 347 images are normal. Mastcam [36] is a dataset of 9,728 hyperspectral images of geologic observations on Mars, from which 426 images were classified into 8 types of novel classes that include meteorite, float rock, bedrock, vein, broken rock, dump pile, drill hole, and dust removal tool. All datasets were used as provided from their sources, without employing any data augmentation technique. Pixel-level ground truth annotations are provided for images with anomalies. In our experiments, for each category the training set of normal images and a very small sample of k randomly selected images with anomalies from the test set (see Section IV.B), were used for training. The rest of the test set was used for testing, i.e. the test set images without the k anomalous samples.\nTo quantify the AD performance of the proposed methodology the Area Under the Receiver Operating Characteristic (AUROC) was adopted as the most widely used metric in the context of AD. AUROC was used both for the detection of images (at image-level) with anomalies and for anomaly localization (at pixel-level).\nTo quantitatively assess the interpretability, the well- recognized Remove and Debias (ROAD) metric was used [37]. ROAD measures the consistency, debias and robustness of an attribution method in its capacity to provide heatmap explanations. This is achieved by evaluating the shift in the classification confidence of the model, when modifying the regions of the image indicated by the heatmap. Methods with higher ROAD scores are considered more effective in providing interpretations. The proposed methodology was implemented in Python 3.6 and Pytorch 1.10. Network f was implemented using VGG-11 CNN (without fully connected layers) pre- trained on a generic dataset (ImageNet) not designed specifically for anomaly detection \u2013 from a wide variety of domains and contexts (e.g., animals, objects, scenes). The Adam optimization algorithm was used for training, with an initial learning rate set to le-3. The batch size for training was set to 32. The early stopping technique was used for training and the maximum number of epochs was set to 200. All images were resized to 224\u00d7224 pixels, considering the expected input size of the VGG-11 network. For the margin-based loss the parameters \u03bc = 0.2 and \u03b2 = 1.2 were used as suggested in [30]. A grid search strategy with values in the range of [0, 2] and step of 0.5, was employed for the selection of the hyperparameter \u03bb, that represents the trade-off term of (8), with the objective to jointly maximize the detection and interpretability of the model. For \u03bb < 1 and \u03bb > 1, the AD performance in terms of AUROC was deteriorating by 0.02 to 0.04, as compared with the performance achieved for \u03bb = 1, which was the best. A 2D Gaussian filter G, was applied on the derived interpretation heatmap, with a standard deviation o = 4."}, {"title": "B. Comparative Evaluation", "content": "The performance of MeLIAD was compared with several state-of-the-art FSAD methods, including PatchCore [38], RegAD [21], FSFA [18], FastRecon [20] and HGM [19], as well as inherently IAD methods including MKD [28], DevNet [22], and FCDD [27]. The quantitative and qualitative results were obtained by using the official source code and hyperparameters of each reported method."}, {"title": "C. Interpretability Evaluation", "content": "The interpretability evaluation of MeLIAD was assessed both qualitatively (Fig. 2), presenting visual interpretation heatmaps, and quantitatively (Table V), reporting the results of the ROAD interpretability quantification metric. The qualitative evaluation involves the comparison with the state-of-the-art inherently IAD methods, by visualizing their output heatmaps. Only IAD methods are considered for this evaluation because the heatmap generation process is an inherent part of their architecture similarly to MeLIAD. To this end, Fig. 2 displays representative anomalous images per image category, along with their respective ground truth (GT) mask. It can be observed that MeLIAD provides precise interpretation heatmaps of the detected anomalies that are more consistent with the GT masks, than those produced by the other interpretable methods. This observation is consistent with the pixel-level AUROC localization results of Table III, that are computed between the predictions and the GT masks, confirming the precision of the interpretation heatmaps generated by MeLIAD. The heatmaps of MKD and FCDD generally highlight larger regions as anomalous, that exceed the boundaries of true anomalies; DevNet offers more precise heatmaps compared to MKD and FCDD, however it still highlights parts of the image that are considered normal.\nThe quantitative interpretability evaluation results are reported in Table V. These indicate the average ROAD [37] score for all classes, compared among the inherently IAD methods. As it can be observed, the highest score, that indicates higher effectiveness in providing interpretations, is achieved by the proposed method of generating explanations, which further substantiates the interpretability of MeLIAD."}, {"title": "D. Metric Learning Effectiveness", "content": "To demonstrate the effect of the metric learning objective to learn the embedding space as part of the proposed methodology, the t-Distributed Stochastic Neighbor Embedding (t-SNE) [39] was used, which is a well-known dimensionality reduction technique that visually represents the spread and clustering of data points in the feature representation space. In this case, the test images of the dataset are used to generate the t-SNE plots presented in Fig. 3, indicative of five categories. Each plot corresponds to a different image category, depicting both anomalous (purple, red) and normal (green, blue) data points, before (crosses) and after (circles) the optimization of the metric learning objective. As it can be observed, prior to the optimization process, the t- SNE plots, represented by the purple and green crosses showcase dispersed points in the embedding space, indicating low discriminative representations of features. After the optimization process, the t-SNE plots represented by the red"}, {"title": "V. DISCUSSION", "content": "The results obtained from the experimental evaluation indicate that MeLIAD offers improved AD performance over other relevant methods on different benchmark datasets. However, this study also revealed some issues which may be considered as limitations. For example MeLIAD exhibits a sensitivity to textural variations, which could be considered normal for some image categories. This affects the overall pixel-level AUC. For example in the first row of Fig. 4, MeLIAD detects a slight variation in the carpet's texture located below the region that has been characterized as anomaly by the experts. However, there is also a possibility that this is indeed an anomaly that has been missed by the expert during the annotation process. Therefore, considering the ambiguity of whether this is an anomaly or not, such a sensitivity may not necessarily be a limitation. A similar issue appears in the classes representing objects, such as the pill image in the second row of Fig. 4. MeLIAD highlights the letters on the lower left side of the capsule as abnormal, which can be expected as these letters are absent in most of the normal images of the respective class. On the other hand, the number on the right ('500') exists in all normal images; therefore, it was not considered as an anomaly by MeLIAD.\nAnother issue that could be considered as a limitation, is that the number of anomalies k to be included in the training set is manually determined, e.g., empirically or after an ablation study, such as the one performed in section IV. Also, the selection of the most representative anomalies to be included in the training set is a challenge. However, these constitute limitations not only for MeLIAD, but also for most current few- shot visual AD methods."}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "In this paper, MeLIAD, a novel AD methodology was presented. It requires only a few known anomalies for training and provides inherently interpretable results in the form of visual explanations, by proposing a trainable anomaly scoring entropy-based component that is jointly optimized with a metric learning objective in a unified methodology. Unlike other inherently interpretable methods MeLIAD: a) does not rely on imposing prior knowledge over the anomaly scores, that assumes the distribution of true anomalies, and b) features a trainable entropy-based anomaly scoring component. The conclusions of this study can be summarized as follows:\n\u2022 The results obtained from the experimental study validate the robustness of MeLIAD, and its effectiveness, even using a very small number of images with anomalies in the training set.\n\u2022 MeLIAD can achieve a higher detection and localization performance on visual quality inspection tasks, in terms of AUROC scores, when compared to state-of-the-art FSAD and IAD methodologies.\n\u2022 The improved interpretability offered by MeLIAD was demonstrated both quantitatively and qualitatively, by the average ROAD scores and the interpretation heatmap comparisons, respectively.\nFuture work includes exploring adaptive strategies for the selection of k, and investigation of image mining techniques [42] for selecting representative samples to improve model performance in multimedia applications."}]}