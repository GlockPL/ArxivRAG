{"title": "Model Developmental Safety: A Safety-Centric Method and Applications in Vision-Language Models", "authors": ["Gang Li", "Wendi Yu", "Yao Yao", "Wei Tong", "Yingbin Liang", "Qihang Lin", "Tianbao Yang"], "abstract": "In the real world, a learning-enabled system usually undergoes multiple cycles of model development to enhance the system's ability to handle difficult or emerging tasks, which involve collecting new data, training a new model and validating the model. This continual model development process raises a significant issue that the model development for acquiring new or improving existing capabilities may inadvertently lose capabilities of the old model, also known as catastrophic forgetting. Existing continual learning studies focus on mitigating catastrophic forgetting by trading off performance on previous tasks and new tasks to ensure good average performance. However, they are inadequate for many applications especially in safety-critical domains, as failure to strictly preserve the performance of the old model not only introduces safety risks and uncertainties but also imposes substantial expenses in the re-improving and re-validation of existing properties. To address this issue, we introduce model developmental safety as a guarantee of a learning system such that in the model development process the new model should strictly preserve the existing protected capabilities of the old model while improving its performance on target tasks. To ensure the model developmental safety, we present a safety-centric framework by formulating the model developmental safety as data-dependent constraints. Under this framework, we study how to develop a pretrained vision-language model (aka the CLIP model) for acquiring new capabilities or improving existing capabilities of image classification. We propose an efficient constrained optimization algorithm with theoretical guarantee and use its insights to finetune a CLIP model with task-dependent heads for promoting the model developmental safety. Our experiments on improving vision perception capabilities on autonomous driving and scene recognition datasets demonstrate the efficacy of the proposed approach.", "sections": [{"title": "1 Introduction", "content": "Learning-enabled systems are rapidly transforming various sectors, with applications in autonomous vehicles, medical diagnosis, and financial prediction. These systems often rely on ML models that are"}, {"title": "2 Related Work", "content": "Continual learning. This work is closely related to Continual learning (CL), also known as lifelong learning, yet it exhibits nuanced differences. Continual learning usually refers to learning a sequence of tasks one by one and accumulating knowledge like human instead of substituting knowledge [74, 53]. There is a vast literature of CL of deep neural networks (DNNs) [4, 37, 19, 30, 21, 45]. The core issue in CL is known as catastrophic forgetting [41], i.e., the learning of the later tasks may significantly degrade the performance of the models learned for the earlier tasks. Different approaches have been investigated to mitigate catastrophic forgetting, including regularization- based approaches [24, 80, 32], expansion-based approaches [82, 31, 60, 70], and memory-based approaches [59, 65, 21, 37, 14]. The framework proposed in this work is similar to memory-based approaches in the sense that both use examples of existing tasks to regulate learning. However, the key difference is that most existing continual learning focuses on the trade-off between learning plasticity and memory stability and aims to find a proper balance between performance on previous tasks and new tasks [74]. Hence, they do not provide a guarantee for MDS. A recent work [48] has proposed an ideal continual learner that never forgets by assuming that all tasks share the same optimal solution. However, it is not practical and not implementable for deep learning problems. Besides, existing continual learning studies usually highlight resource efficiency when accumulating knowledge by reducing the number of samples of previous tasks. In contrast, this work tends to utilize more examples to construct developmental safety constraints for protected tasks to facilitate MDS.\nAI Safety. Our notion of model developmental safety should not be confused with model safety (aka AI safety). The latter is a field concerned with mitigating risks associated with AI, whose surge in attention stems from the growing capabilities of AI systems, particularly large foundation models [25, 76, 10, 54]. As these models become more adept at complex tasks, concerns around potential misuse, bias, and unintended consequences rise proportionally. [5] presents several practical research problems related to AI safety, including avoiding side effects, avoiding reward hacking, scalable oversight, safe exploration, and robustness to distributional shift. More recently, [73] elaborate on eight different perspectives to evaluate the trustworthiness of LLMs, including toxicity,"}, {"title": "3 Notations and Preliminaries", "content": "Notations. We consider developing a model w to improve its capabilities on a target task \\(T_0\\) while preserving its performance on a set of protected tasks denoted by \\(T_1,...,T_m\\). A task can be as simple as predicting a class for multi-class classification or as complicated as coding ability of LLMs. In the paper, we focus on classification using CLIP models and each task refers to one"}, {"title": "4 A Safety-Centric Framework", "content": "4.1 Model developmental safety\nTo measure the model developmental safety, it is necessary to evaluate how the performance of the model changes in protected tasks from the old model \\(w_{old}\\) to a new model \\(w_{new}\\). We introduce the formal definition of model developmental safety (MDS) in Definition 1, which ensures the new model strictly preserves performance on each individual protected task.\nDefinition 1 (Model Developmental Safety (MDS)). In model development process, the model de- velopmental safety is satisfied if \\(L_k(w_{new}, D_k) \\leq L_k(w_{old}, D_k),\\forall k \\in \\{1, ...,m\\}\\), where \\(L_k(w, D_k) = E_{x,y\\sim D_k}l_k(w,x,y)\\).\nIn practice, the developmental safety will be measured using a set of examples \\(S_j \\sim D_j\\) for each protected task. Hence, we define the empirical developmental safety metric, corresponding to"}, {"title": "4.2 A Safety-centric Approach for Model developmental safety", "content": "The key to our safety-centric framework is to utilize examples of protected tasks to define empirical safety constraints when updating the model on a target task. In order to develop the model for improving the performance on a target task \\(T_0\\), we assume that a set of data D for \\(T_0\\) is constructed and a proper objective is given based on application, denoted by \\(F(w,D)\\). Then, our safety-centric approach for model development is imposed by solving the following problem:\n\\[\\begin{aligned}\nw_{new} = \\arg \\min_w F(w,D) \\\\\ns.t.\\ L_k(w,\\mathcal{D}_k) - L_k(w_{old},\\mathcal{D}_k) \\leq 0,\\ k=1,\\ldots,m.\n\\end{aligned}\\]\nWe will propose an algorithm to directly solve this data-dependent constrained optimization problem with a contrastive objective in the context of developing a CLIP model in next section.\nGeneralization Analysis. Since we can only use empirical data \\(D_1,..., D_m\\) in (3), there exist generalization errors between the safety constraints in (3) and the safety we want to ensure in Definition 1. The lemma below uses a standard tool of statistical error analysis to bound the safety generalization error. For simplicity, we assume each protected task is associated with the same loss function, namely, \\(l_k = l\\) for \\(k = 1, ..., m\\). In the analysis, we use the Rademacher complexity of the loss class \\(\\mathcal{H} = \\{l(w,\\cdot, \\cdot) : \\mathcal{X} \\times \\mathcal{Y} \\rightarrow [0,1]|w\\in \\mathbb{R}^d\\}\\) induced by the model w on n data points, which is denoted by \\(R_n(\\mathcal{H})\\). We assume that \\(R_n(\\mathcal{H}) \\leq Cn^{-\\alpha}\\) for some \\(C>0\\) and \\(\\alpha \\leq 0.5\\). We note that \\(\\alpha = 0.5\\) in the vast majority of model and loss families, including linear models [23], deep neural networks [7], and model families with bounded VC dimension [7].\nLemma 2 (Safety Generalization Error). Suppose that \\(R_n(\\mathcal{H}) \\leq Cn^{-\\alpha}\\) for some \\(C \\geq 0\\) and \\(\\alpha \\leq 0.5\\). Then, with probability at least \\(1 - \\delta\\), it holds that\n\\[L_k(w_{new}, D_k) - L_k(w_{old}, D_k) \\leq L_k(w_{new}, \\mathcal{D}_k) - L_k(w_{old}, \\mathcal{D}_k) + \\frac{4C}{\\sqrt{n^{\\alpha}}} + 2\\sqrt{\\frac{\\ln(2m/\\delta)}{2n_k}} \\forall k.\\]\nRemark: The lemma indicates that as long as the empirical safety constraints are satisfied, i.e., \\(L_k(w_{new}, D_k) - L_k(w_{old}, D_k) \\leq 0\\), the model developmental safety is ensured up to a statistical error in the order of \\(O(n^{-\\alpha})\\), where \\(n = \\min_k n_k\\). Hence, the more examples used to construct the safety constraints, the more likely the new model is safe. The proof is given in Appendix B.1."}, {"title": "5 Safe Development of CLIP Models", "content": "Based on the proposed framework above, in this section, we present an efficient algorithm for improving a pretrained CLIP model on a target task while ensuring MDS on a set of protected tasks. The CLIP model is of particular interest because (i) it is a foundation model that has been used extensively in many applications; and (ii) can adapt to the open-world for handling new classes using languages. However, existing studies have shown that directly applying a pretrained CLIP"}, {"title": "5.1 Efficient Optimization and Convergence Analysis", "content": "The optimization problem in (4) is challenging for multiple reasons. First, this problem involves a non-convex objective and non-convex constraints, so finding a global optimal solution is intractable in general. Second, the objective and constraint functions are formulated using a large dataset, so we need to sample from the dataset in order to construct stochastic gradients of the functions to update the solution. Lastly, (4) may contain a large number of constraints, so updating the solutions using the gradients of all constraints may be prohibited. Given these challenges, we need to develop a stochastic optimization for (4) based on advanced techniques and constraint sampling. Our method is motivated by the stochastic quadratic penalty method in [3], which first converts (4) into an unconstrained problem by adding a quadratic penalty on the constraints violation to the objective function and then solves the unconstrained problem using a variance-reduced stochastic gradient method. Unfortunately, their method can not be directly applied to (4) because (i) they only consider equality constraints while (4) involves inequality constraints and (ii) they require an unbiased stochastic gradients for each update while the stochastic gradients for (4) will be biased due to the compositional structure. Note that an augmented Lagrangian algorithm (ALA) is also studied by [3], which has the same issue as their penalty method. We only consider quadratic penalty method for (4) because it has the same complexity as the ALA but is more intuitive and easier to implement.\nA quadratic penalty method converts (4) into the following unconstrained problem:\n\\[\\min_w \\Phi(w) := F(w,\\mathcal{D}) + \\frac{\\beta}{m}\\sum_{k=1}^m ([h_k(w)]_+)^2\\]\nwhere \\([\\]_+ = \\max\\{\\cdot,0\\}\\) and \\(\\beta \\geq 0\\) is the penalty parameter. Under mild conditions, a large enough \\(\\beta\\) will ensure the optimal solution to (5) is also an optimal solution to (4).\nIn the following, we introduce an efficient stochastic algorithm to solve (5). It is notable that both terms are of finite-sum coupled compositional structure [72], i.e., \\(\\sum_i f(g_i(w))\\), where f is non-linear. We discuss how to approximate the gradient of two terms of the objective using mini-batch samples below. Define \\(g_{1i}(w) = E_{\\tau_j\\in T_i^-} \\exp (E_1(w,x_i)E_2(w, t_j) - E_1(w,x_i) E_2(w,t_i)/\\tau)\\) and \\(g_{2i}(w) = E_{x_j \\in I_i^-} \\exp (E_2(w,t_i) E_1(w, x_j) - E_2(w,t_i)E_1(w,x_i)/\\tau)\\). Then, \\(F(w,\\mathcal{D}) = \\frac{1}{n_0}\\sum_{(x_i, t_i)\\in \\mathcal{D}}"}, {"title": "5.2 Promoting developmental safety via Task-dependent heads", "content": "Below, we present a way to design the text encoder of the CLIP model such that the value of \\(\\delta\\) could be larger. Without causing confusion, we denote by w the parameter of the text encoder, which consists of two components u and W such that the text embedding \\(E_2(w, t) \\in \\mathbb{R}^{d_2}\\) can be represented as \\(E_2(w,t) = W\\cdot\\bar{E}_2(u, t)\\), where \\(\\bar{E}_2(u, \\cdot) \\in \\mathbb{R}^{d_1}\\) is a backbone encoder while \\(W \\in [\\mathbb{R}^{d_2\\times d_1}\\) is called the head. The idea of task-dependent heads is to let each task k have its own head \\(W_k = W+U_kV\\) using low rank matrices \\(U_k \\in [\\mathbb{R}^{d_2\\times r}\\) and \\(V_k \\in [\\mathbb{R}^{d_1\\times r}\\), where \\(r 0 such that \\(||\u2207h(w^t)[h(w^t)]^+|| \\geq \\delta||[h(w^t)]^+||\\) for"}, {"title": "6 Experiments", "content": "In this section, we conduct extensive experiments to understand our proposed method comprehen- sively. We start by presenting a visualization of the learning process of the proposed method in Section 6.1 to provide an overview of how our approach works. In Section 6.2, we demonstrate the effectiveness of the proposed method in achieving model developmental safety compared with other strong baselines. We present a detailed ablation study to help understand each design choice of the proposed method in Section 6.3, including the effect of using external data for contrastive learning"}, {"title": "6.1 Visualization of Learning Process", "content": "To provide a direct understanding of why and how the proposed algorithm works, we present the learning trajectory of the algorithm in Figure 2. Each dot in this figure represents a solution during the learning processing, with lighter colors indicating earlier stages and darker colors representing later stages. From the top four figures for training sets, we can observe a common trend that"}, {"title": "6.2 Comparison with Baselines for Model developmental safety", "content": "In this part, we compare the proposed method with other baselines to demonstrate the superiority. Specifically, we focus on two metrics, i.e., safety ratio for measuring the possibility of strictly preserving the performance on all protected tasks and accuracy on the target task. On autonomous driving BDD100K dataset, we conduct experiments with different numbers of data for constraints, i,e., 100, 1k, 2k, 4k from each task. The comparison results are presented in Figure 3. The figure illustrates that improving the base model on the target tasks is not challenging, as nearly all methods accomplish this effortlessly. However, all baselines, including the strong continual learning baseline GEM, exhibit a zero safety ratio across almost all settings, showing the insufficiency of existing methods for ensuring zero forgetting on protected tasks. In contrast, our method begins to ensure developmental safety with 1k samples per protected task and even 100 samples for the target class tunnel. Besides, the safety ratio increases when using more data for constraints, consistent with the result obtained in Lemma 2 (Refer to Table 1 for more results). Notably, our method achieves a 100% safety ratio with 4k samples per protected task in all three settings, while improving accuracy on the target class. We also see that the target class overcast is most difficult to improve as the base model already has 73.6% accuracy.\nFrom Figure 3, we notice that the baseline RM fails to achieve MDS, even though it has a tunable weight parameter \\(\\alpha\\) for protected tasks. Comparison with RM can directly verify the advantage of our method as the only difference between the two methods is how to handle the protected tasks. From Eqn. (8), we can see that our algorithm has an effective weight \\(\\beta[u]_+\\), for each protected task. It is adaptively adjusted during learning, and depends on the degree of violation of constraints, i.e., the larger the violation, the larger the weight. Figure 4 (left) shows that these effective weights gradually decrease to zero during the learning of our algorithm, which allows the model to learn from the target task while satisfying constraints. This mechanism plays a big role in not only achieving MDS but also improving the performance on the target task. In contrast, RM uses a constant weight \\(\\alpha\\) for every protected task. Simply increasing \\(\\alpha\\) may not ensure MDS, due to varied learning difficulty between protected tasks. Besides, too large \\(\\alpha\\) will also harm the performance of the target task. We further investigate the phenomenon in Appendix A.4 and find that, with a"}, {"title": "6.3 Ablation studies", "content": "6.3.1 Importance of the external data from LAION400M\nWe conduct experiments on targeting foggy to investigate the benefits of the external data retrieved from LAION400M dataset. In detail, we vary the number of retrieved target-related image-text pairs utilized in the objective function, i.e., {0, 2k, 5k, 11k}, with 1k samples from each protected task as constraints. From Tab. 2, we can see that, with only 57 foggy samples from BDD100k dataset (i.e., 0 samples from the external data), the model does not improve the target accuracy at all. However, with more and more retrieved image-text pairs utilized to augment the dataset D, the improvement on the targeted task appears and becomes significant, showing the advantages of incorporating the retrieved target-related image-text pairs for boosting target task accuracy. Regarding safety ratios,"}, {"title": "6.3.2 Importance of Task-dependent Heads", "content": "As introduced in Section 5.2, to reduce the total complexity of our algorithm, we propose task- dependent heads to increase the parameter \\(\\delta\\) in Assumption 4, avoiding getting trapped at a flat location where \\(w^t\\) is infeasible but \\(\\nabla H(w^t) = 0\\). To verify the effectiveness of the design, we experiment on targeting overcast and foggy tasks with varying numbers of data for constraints. The results are presented in Figure 5. The results show that models equipped with task-dependent heads almost consistently exhibit both higher safety ratio and higher accuracy on the target task. Besides, without task-dependent heads, models may have trouble achieving 100% developmental safety, such as targeting Overcast, demonstrating the importance of task-dependent heads for promoting developmental safety."}, {"title": "6.3.3 Verification of Lemma 4", "content": "To verify the theoretical result in Lemma 4, we empirically calculate \\(\\nabla \\hat{h}(w)\\) and \\(\\nabla h(w)\\) with CLIP models. Specifically, for targeting overcast, we compute the minimal singular values of \\(\\nabla h(w)\\) and \\(\\nabla \\hat{h}(w)\\) on the base model and two trained models, with 1k samples for each protected task. The initial value of \\(U_k\\) is set to zero so \\(U_kV = 0\\). From the results presented in Table 3, we can observe that, on the initial model, the minimal singular value of \\(\\nabla \\hat{h}(w)\\) is slightly larger than that of \\(\\nabla h(w)\\) and the gap become much significant after training, which is consistent with the theoretical result in Lemma 4 and also provides some insight on the empirical results in Figure 5."}, {"title": "6.3.4 Constant \u03b2 vs Increasing \u03b2", "content": "In theory, an increasing penalty parameter \\(\\beta\\) may help reduce the complexity of constrained problems as shown in [3], but in our empirical experiments, we find that using a constant \\(\\beta\\) is generally behave better than using an increasing \\(\\beta\\). As shown in Fig. 6 for target task foggy, models with a constant \\(\\beta\\) are able to achieve 100% safety ratio with 2k or 4k sampler per constraint. On the contrary, models using a cosine increasing \\(\\beta\\) obtain both lower safety ratio and lower accuracy on the target task compared to models with constant \\(\\beta\\). We conjecture that this is because models with an increasing \\(\\beta\\) might leave the feasible developmental safety region too far in the initial stages"}, {"title": "6.4 Performance with Multiple Rounds of Model Development", "content": "Finally, to demonstrate the effectiveness of the proposed safety-centric framework in iterative model development process, we conduct two consecutive rounds of development on recognizing weather conditions. Specifically, we first target at overcast task, taking all the other five weather conditions as protected tasks, then with one selected improved model, we successively improve the model, targeting at improving the performance of the foggy task. As shown in Fig. 1, our method notably improves the performance of the overcast task in the first round while ensuring the performance of other tasks does not decrease. In the second round, it continues to enhance the performance of the foggy task. Simultaneously, it preserves the performance, if not boosts it, across other tasks, with only a slight decrease on the snowy task, showing the effectiveness of the proposed framework for maintaining the model developmental safety."}, {"title": "7 Conclusion", "content": "In this paper, we introduced the concept of \"model developmental safety\" to ensure that model development not only acquires new capabilities but also strictly preserves those already present in the old model, addressing the critical developmental safety oversight in existing ML/AI stud- ies. To ensure developmental safety, we proposed a safety-centric framework by formulating the model developmental safety as data-dependent constraints. We proposed an efficient constrained optimization algorithm with theoretical guarantees to develop a pretrained vision-language model (CLIP model) for improving existing image classification capabilities. Comprehensive experiments demonstrate the effectiveness of the algorithm in improving vision-based perception capabilities in autonomous driving and scene recognition, showing its practical value in real-world scenarios."}, {"title": "A More Experimental Details and Results", "content": "All experiments in our paper are run on two High Performance Research Computing platforms. One contains 117 GPU nodes, each with two A100 40GB GPUs. Another contains 100 GPU nodes, each with four A40 48GB GPUs.\nWe present the statistics for BDD100K Dataset in our experiments in Table 4."}, {"title": "A.1 Details about Baselines", "content": "FLYP. In the original FLYP paper [20], the author presents extensive experiments demonstrating the superiority of employing the contrastive loss used during pre-training instead of the typical cross-entropy for finetuning image-text models for zero-shot vision classification. As the local contrastive loss, defined over the mini-batch samples, utilized in their paper requires a very large mini-batch size to converge, we follow [79] to employ a global constrastive loss (GCL) as indicated in Eqn. 10 to address this issue:\n\\[\\min_W \\frac{1}{N_{all}} \\sum_{(x_i,t_i) \\in D_{all}} L_{ctr}(w; x_i, t_i, D_{all}, D_{all})\\]\nwhere \\(D_{all} = D \\cup D_-\\cup D_1\\cup\\ldots\\cup D_m\\), \\(n_{all} = n_0 + 10 + n_0 + n_1 + \\ldots + n_m\\), \\(D_-\\) is the negative data collected form LAION400M as discussed in AppendixA.2. All available data, including those used in our objective and constraints, are utilized for fine-tuning. The simple text prompts for the labeled BDD100k dataset are the same as those used for our method, i.e., \"the weather is [Weather]\" and \"the scene is a [Scene]\".\nWCCL. Weighted Combination of Contrastive Losses(WCCL) is a straightforward baseline that utilizes a weight to combine GCL losses on protected tasks and the target task to balance protected tasks and the target task and achieve model developmental safety. Specifically, the objective can be formulated as:\n\\[\\min_W (\\alpha \\frac{1}{n_0} \\sum_{(x_i,t_i) \\in D}  L_{ctr}(w; x_i, t_i, T_{io}, I_{io}))\\]\n\\[ + (1-\\alpha) (\\frac{1}{nm} \\sum_{k=1}^m \\frac{1}{n_k} \\sum_{(x_i,t_i) \\in D_k} L_{ctr}(w; x_i, t_i, T_{ik}, I_{ik}))\\]"}, {"title": "A.2 Retrieving external data from LIAON400M", "content": "As mentioned in the main paper, for each target task, we retrieve task-related image-text pairs from Laion400M [62] to improve target performance, by going through the dataset and retrieving the image-text pairs with text containing the specific target task names, e.g., 'foggy', 'overcast', 'tunnel', 'dressing room'.\nMoreover, we refine the retrieved datasets. Let's take the task 'tunnel' as an example. For task 'tunnel', the retrieved data contained excessive noise, including numerous image-text pairs unrelated to tunnels, but contained 'tunnel' in the text. Therefore, we employed the GPT-40 API to filter the retrieved data with prompt \"Determine whether the following caption mentions a tunnel or related context. First provide reasoning for your answer, and then respond with 'True' if it mentions a tunnel, or 'False' if it does not.\", thereby decreasing the noise of our retrieved data. The statistics of obtained task-related image-text pairs are presented in the Table 5. Additionally, for each target class, we randomly sample a set of image-text pairs from LAION400M that is 10 times larger than the positive set as negative data for contrasting."}, {"title": "A.3 Visualization of Models' Learning Curves", "content": "Along with the learning trajectory in the main paper, we present the training and validation curves in Fig. 7 to further illustrate the learning process of the algorithm. From the figure, we can see that the DevSafety(acc) fluctuates along the safety line while \\(\\Delta Acc(Target)\\) continues to increase, which shows the model is striving to improve the model's performance while satisfying the safety requirements."}, {"title": "A.4 Deficiency of Weighting Methods", "content": "As observed in Figure 3, the naive weighting approach RM fail to achieve model developmental safety, even though they tradeoff the performance on the target task and protected tasks with weight parameter \\(\\alpha\\). To have a close look at why this happens, we show the detailed performance RM when targeting foggy with 4k samples for each protected task in Table 6. We find that, with a uniform weight for all the protected tasks, the method might preserve previous performance on some of the protected tasks but fail to achieve MDS for all the protected tasks, even with a very high \\(\\alpha\\). Moreover, with the weight \\(\\alpha\\) getting larger, the performance on the target task drops dramatically while the decrease gap goes smaller, e.g., Clear tasks for RM. In contrast, our proposed method is able to preserve all the protected tasks' performance and improve the target task, as the mechanism of our algorithm is very different from using the uniform weight. In our method, weights for constraints depend on the loss of those tasks, i.e., the larger the violation, the larger the weight. As shown in Figure 4, the weight for each protected task is adaptively adjusted during learning and once one protected task constraint is satisfied, it will not be penalized (weight becomes zero). This mechanism plays a big role in enabling the model to find feasible solutions to ensure zero-forgetting on all the protected tasks.\nTo further demonstrate the deficiency of the weighting method, we compare RM with our method on the Place365 dataset, targeting Dressing room class and protecting the other 364 tasks in Figure 8. With \\(\\alpha\\) = 1,10, 100, 1000, 10000, RM causes performance drops in 50, 35, 33, 32, and 35 classes, respectively. Although larger weights reduce the number of classes where performance drops, RM still cannot ensure MDS for all protected tasks and excessively high weights lead to performance decrease on the target task instead of improvement. In contrast, we can see that even with hundreds of protected tasks, our method is still effective in preserving their performance whiling improving the target task."}]}