{"title": "Confidence intervals uncovered:\nAre we ready for real-world medical imaging AI?", "authors": ["Evangelia Christodoulou", "Annika Reinke", "Rola Houhou", "Piotr Kalinowski", "Selen Erkan", "Carole H. Sudre", "Ninon Burgos", "Sofi\u00e8ne Boutaj", "Sophie Loizillon", "Ma\u00eblys Solal", "Nicola Rieke", "Veronika Cheplygina", "Michela Antonelli", "Leon D. Mayer", "Minu D. Tizabi", "M. Jorge Cardoso", "Amber Simpson", "Paul F. J\u00e4ger", "Annette Kopp-Schneider", "Ga\u00ebl Varoquaux", "Olivier Colliot", "Lena Maier-Hein"], "abstract": "Medical imaging is spearheading the AI transformation of healthcare. Performance reporting is key to determine which methods should be translated into clinical practice. Frequently, broad conclusions are simply derived from mean performance values. In this paper, we argue that this common practice is often a misleading simplification as it ignores performance variability. Our contribution is threefold. (1) Analyzing all MICCAI segmentation papers (n = 221) published in 2023, we first observe that more than 50% of papers do not assess performance variability at all. Moreover, only one (0.5%) paper reported confidence intervals (CIs)", "sections": [{"title": "1 Introduction", "content": "As demonstrated by the fact that more than 530 of the first 692 AI in healthcare products approved by the U.S. Food and Drug Administration (FDA) fall within the application domain of medical imaging [16], medical imaging is spearheading the AI-powered transformation of healthcare. Performance reporting and compar- isons of medical imaging models are key to determining their potential for clinical translation. Clinical translation requires approval by regulatory agencies such as the U.S. FDA, whose recommendations insist on the importance of characterizing variability and reporting confidence intervals (CIs); for instance in [15,5,14,4]). A recent paper [6] written by FDA staff describes regulatory science principles on performance assessment of AI algorithms in imaging and emphasizes that \"The statistical analysis plays a critical role in the assessment of machine learning (ML) performance but may be under-appreciated by many ML developers\". Cur- rent practice in reporting results (including that of the authors!) often does not fulfill these requirements and thus far from lends itself to determining whether a medical imaging model is suited for clinical translation. The underlying question is: Can we really trust performance claims made in publications? The purpose of this paper was to address this important question:\n1. Based on a comprehensive analysis of all MICCAI 2023 segmentation papers, we show that performance variability is rarely accounted for in the medical image analysis community.\n2. To demonstrate the implications of the reporting bottleneck, we propose a work-around to approximate variability parameters from the information provided in publications. Specifically, we show that the unreported standard deviation (SD) in segmentation papers can be approximated using a second- order polynomial function of the mean Dice similarity coefficient (DSC)."}, {"title": "2 Methods", "content": "Assessment of AI model performance variability is crucial as it directly impacts the model's reliability in clinical practice. While variability reporting guidelines- in particular regarding the inclusion of CIs are available in the clinical prediction modeling domain [8], such practices are still unfamiliar in the medical imaging domain.\nIn this paper, we focus on two statistical concepts capturing performance variability: SD is a measure of the dispersion or spread of data points from the mean value. For example, given a set of performance metric values (e.g., DSC values of a model on multiple images) the SD states how much these values vary from the average performance. A small SD indicates that the values are close to the mean, while a large one suggests that the values are more dispersed. A CI can be used to estimate the range within which a population parameter (such as the mean) is expected to lie with a certain level of confidence. For example, a 95% CI for the mean suggests that if we were to take many samples and calculate the CI for each, about 95% of these intervals would contain the true population mean. CIs provide a measure of the precision of an estimate. Their widths approach 0 for infinite sample sizes.\nTo identify current practices in performance variability reporting and further raise awareness on this matter in the medical imaging community, our work addresses research questions (RQs) depicted in Figure 2."}, {"title": "2.1 Systematic review of MICCAI 2023 segmentation papers", "content": "Given that segmentation is a key focus of MICCAI and DSC is the community's primary metric [10], our study concentrates on segmentation papers. From each of the identified segmentation papers we extracted information on the claims of the paper, method performance, its variability, and validation practices. To reduce the bias in extracting information from the papers, each paper was screened independently by two researchers. Subsequently, three additional researchers, distinct from those involved in data extraction, addressed data extraction conflicts.\nWith the inclusion criteria being use of a test set for validation and mention of the exact test set size and mean DSC values, we identified all segmentation papers for which we could approximate the SD and CI. We excluded papers that solely used a random train/test split with no validation set (because there is a risk that the test set was used for validation, e.g., for model selection, leading to overoptimistic performance estimates) or only provided performance information graphically."}, {"title": "2.2 Approximation of missing variability parameters", "content": "To develop a method for approximating missing SD and CI from data present in publications, we used data from the Medical Segmentation Decathlon challenge"}, {"title": "3 Experiments and Results", "content": "RQ1: Common practice with respect to variability reporting\nFrom all 730 papers published in the scope of MICCAI 2023, we identified 221 (30.3%) segmentation papers. As shown in Figure 3, more than half of the papers (54.8%) did not report any kind of variability. CIs were reported in only one paper (0.5%). Of those that did report variability, only 47% reported SD (21% of all"}, {"title": "4 Discussion", "content": "Our work is the first to systematically analyze common practice with respect to model performance variability reporting in the field of medical image analysis.\nOur study clearly shows that reporting of performance variability, in particular reporting of CIs, is a rare exception. These reporting practices are at odds with the MICCAI reproducibility checklist guidelines [11] which include the following item: \"A description of results with central tendency (e.g., mean) & variation (e.g., error bars)\u201d. Even when variability is reported, for instance in the form of the SD, conclusions are commonly drawn based on mean performance values without taking variability into account. This reporting practice can be very misleading and is highly unhelpful in reaching the ultimate goal of clinical translation of imaging models. A model exhibiting a high mean metric score but large variability in performance may not be suitable for safety-critical real-world applications, in which low performance on even some images may have dramatic consequences for patients.\nA limitation of our study could be seen in the fact that we had to approximate the SDs and subsequently CIs based on the data available. However, our external validation of our SD approximation indicates high reliability (Figure 4(a)).\nRelated work on the topic of variability analysis is sparse. In an analysis of biomedical image analysis challenge reporting, [18] showed that claims are often solely drawn from aggregated results in tables, which supports our hypothesis. [9] likewise emphasize the lack of reporting CIs in medical image segmentation.\nReporting on the Conference on Neural Information Processing Systems (NeurIPS) reproducibility program, [12] state that \"it seems surprising to have 87% of papers that see value in clearly defining the metrics and statistics used, yet 36% of papers judge that error bars are not applicable to their results\u201d. While our current analysis focuses on the variability of the trained model (i.e., accounting from variance coming from the test set), [3] analyzed the variability of the learning procedure (i.e., accounting for other sources of variances such as random seeds or hyperparameters) during initial method development. Additionally, [10] investigated rankings in biomedical challenges and found that these are often not stable. Similarly, [17] found that rankings between private and public leaderboards in Kaggle competitions were not stable.\nIn this paper, we focused on the question: Can we trust the reported mean performance results? Note that this is conceptually different from asking whether one method truly outperforms another, as investigated in [10]. In fact, the confidence in reported mean performance values, as measured by CIs, is necessary (but, of course, not sufficient) for deciding on whether a proposed algorithm is ready for clinical translation. Our study revealed that claims of scientific progress are typically based on small differences (around 0.01) in the mean DSC, suggesting that these were considered clinically relevant by the authors.\nIn contrast, CIs are on average much wider. We consider this contradictory because if a difference of 0.01 matters, then, shouldn't a CI with a much larger width be concerning (and at least be reported), as it means that the true mean may be substantially smaller than the reported one?"}]}