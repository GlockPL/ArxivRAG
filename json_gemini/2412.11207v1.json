{"title": "ProFe: Communication-Efficient Decentralized Federated Learning via Distillation and Prototypes", "authors": ["Pedro Miguel S\u00e1nchez S\u00e1nchez", "Enrique Tom\u00e1s Mart\u00ednez Beltr\u00e1n", "Miguel Fern\u00e1ndez Llamas", "G\u00e9r\u00f4me Bovet", "Gregorio Mart\u00ednez P\u00e9rez", "Alberto Huertas Celdr\u00e1n"], "abstract": "Decentralized Federated Learning (DFL) trains models in a collaborative and privacy-preserving manner while removing model centralization risks and improving communication bottlenecks. However, DFL faces challenges in efficient communication management and model aggregation within decentralized environments, especially with heterogeneous data distributions. Thus, this paper introduces ProFe, a novel communication optimization algorithm for DFL that combines knowledge distillation, prototype learning, and quantization techniques. ProFe utilizes knowledge from large local models to train smaller ones for aggregation, incorporates prototypes to better learn unseen classes, and applies quantization to reduce data transmitted during communication rounds. The performance of ProFe has been validated and compared to the literature by using benchmark datasets like MNIST, CIFAR10, and CIFAR100. Results showed that the proposed algorithm reduces communication costs by up to \u224840-50% while maintaining or improving model performance. In addition, it adds ~20% training time due to increased complexity, generating a trade-off.", "sections": [{"title": "I. INTRODUCTION", "content": "Over the past few years, Federated Learning (FL) has emerged as a pivotal methodology in Artificial Intelligence, primarily due to its ability to train models without centralizing data [1]. This capability is especially critical in scenarios where data privacy and security are paramount. Building upon the foundations of FL, Decentralized Federated Learning (DFL) has been proposed as a natural progression of FL, facilitating collaboration among distributed nodes without a central server [2]. This evolution aims to address inherent challenges associated with traditional FL, such as communication bottlenecks and risks linked to model centralization.\nThe interest in DFL is driven by the need for models that can operate efficiently in distributed environments. However, this approach introduces new challenges, including the efficient management of communication between nodes and the aggregation of models in the absence of a central coordinator [3]. To mitigate these challenges, various techniques have been proposed to optimize communication in DFL by reducing the model size, decreasing the data transmitted between nodes [4]. Recent literature approaches have applied techniques such as Knowledge Distillation (KD), Prototype Learning, quantization, and pruning to minimize communication overhead without significantly compromising model performance. KD enables the creation of smaller, efficient models by transferring knowledge from larger models, reducing both computational and communication demands [5]. Prototype learning simplifies data representation by modeling essential data characteristics that are common between classes, sharing these representations instead of the complete model [6]. Finally, quantization reduces the precision of model parameters, while pruning reduces the number of parameters in the network, both leading to smaller model sizes [7].\nDespite these advancements, several open issues concerning communication optimization in DFL persist. (i) A significant challenge is to maintain model performance while reducing size; techniques like KD, prototype learning, and quantization can lead to loss of critical information, adversely affecting model accuracy. (ii) The heterogeneity of data distributions (non-IID) and computational capabilities across different nodes further complicates the implementation of these techniques, as most existing methods assume a level of uniformity that is often unrealistic in practical scenarios. Therefore, reducing communications while maintaining model performance in DFL is an open challenge that requires additional efforts.\nTo tackle these issues, the main contribution of this work is ProFe, a novel communication optimization algorithm for DFL that combines KD, prototypes, and quantization techniques. ProFe uses the knowledge acquired by a local large model during each training round to transfer it to a smaller model for aggregation. Then, prototypes are used for the improved learning of unseen classes. Finally, quantization provides an extra optimization in the number of bytes sent during each round. The communication and model performance of ProFe have been evaluated with MNIST, CIFAR10, and CIFAR100 datasets. Additionally, the most representative literature solutions are compared with the proposed approach. The results showed that ProFe effectively reduces communication costs of \u224850-60% while maintaining or even improving model performance, particularly in non-independent and identically distributed settings. Despite introducing an acceptable training time overhead of \u224820%, ProFe outperforms existing methods by balancing communication efficiency and model accuracy across various scenarios."}, {"title": "II. RELATED WORK", "content": "Several works in the literature have explored the communication optimization problem in FL from various perspectives. In terms of KD, FedKD [4] proposes an FL framework where each client stores a larger teacher model and a shared smaller student model, while a central server coordinates the collaborative learning. Federated Mutual Learning (FML) [8] is a framework to address data, model, and objective heterogeneity in FL. A \"meme model\u201d serves as an intermediary between personalized and global models, enabling knowledge transfer via Deep Mutual Learning on local data. For objective heterogeneity, FML uses a shared global model with selective components, keeping the personalized model task-specific. FML achieves higher accuracy and faster convergence than other FL methods like FedAvg and FedProx, though communication usage is not reported.\nFrom the prototype perspective, FedProto [9] uses prototype aggregation in a heterogeneous FL framework, transmitting only prototypes between nodes to improve model accuracy and reduce communication costs. This enables varied model architectures across nodes, as long as prototypes remain consistent. However, nodes are limited to learning only labels present in their local data, as models align closely with known label prototypes. Combining prototypes and KD, FedGPD [10] tackles data heterogeneity in FL by using global class prototypes to guide local training, aligning local objectives with the global optimum. This approach reduces data distribution issues, improving global model performance and achieving 0.22% to 1.28% higher accuracy than previous methods.\nUsing pruning and quantization, FL-PQSU [11] introduces techniques that significantly reduce computational and communication costs in FL models with minimal impact on performance. By applying initial model pruning to eliminate unnecessary connections and transmitting only updates that meet specific conditions, unnecessary data transfer is reduced. Additionally, all communication is performed using quantized data types. This combination reduces the model size from 112MB to just 2.83MB (approximately 2.5% of the original size) with a minimal accuracy loss of 0-1.9%, and quadruples the communication speed with the server.\ndo not consider the particularities of DFL. Prototype-based methods reduce communication costs by sharing class prototypes, but struggle with non-overlapping label distributions, preventing clients from learning unseen classes. Therefore, there is a need for a decentralized approach that combines KD and prototype learning to enable efficient communication, support heterogeneous data and models, and allow clients to learn unseen classes without a central server."}, {"title": "III. PROFE ALGORITHM DESIGN", "content": "This section presents ProFe, the proposed algorithm for model compression in DFL. Fig. 1 describes the steps taken in each federation node to perform the learning process. Next, the details of KD and prototype learning are given, explaining later the proposed joint approach for model training."}, {"title": "A. Knowledge Distillation", "content": "The implementation of KD between the teacher and student models follows the response-based distillation approach. It begins by calculating $p_s$ and $p_t$, the smoothed logarithmic probability distributions for the student and teacher models. For the student model, a Log-Softmax function is used, while for the teacher model, a standard Softmax function is applied.\nLet $n$ be the number of output layers in the neural network, and let $p_s = (p_{s,1}, p_{s,2},...,p_{s,n})$ and $p_t = (p_{t,1}, p_{t,2},...,p_{t,n})$ represent the smoothed probability distributions for the student and teacher models, respectively. Then,\n$p_{s,j} = \\frac{e^{y_{s,j}/T}}{\\sum_{k=1}^{n} e^{y_{s,k}/T}}$ and $p_{t,j} = \\frac{e^{y_{t,j}/T}}{\\sum_{k=1}^{n} e^{y_{t,k}/T}}$\nwhere $(y_{s,1}, y_{s,2},\u2026\u2026\u2026, y_{s,n})$ and $(y_{t,1}, y_{t,2},..., y_{t,n})$ are the outputs of the student and teacher networks, respectively, and $T$ is the temperature parameter in KD that smooths the probability distributions. The Kullback-Leibler divergence is then used to quantify how the probability distribution $p_t$ deviates from $p_s$ as:\n$KL(p_t||p_s) = \\sum_{j=1}^{n} p_{t,j} ln(\\frac{p_{t,j}}{p_{s,j}})$\nThe total KD loss with temperature is calculated as:\n$L_{KD}(y_s, y_t) = KL(p_t||p_s) \\times T^2$.\nIn addition, a cross-entropy loss function is incorporated. Let $y_s = (y_{s,1}, y_{s,2},..., y_{s,n})$ be the Logits from the student model for $n$ and $y = (y_1,y_2,...,y_n)$ be the vector of true labels in one-hot format. The Softmax function is applied to convert the model outputs into a probability distribution:\n$q_s = (q_{s,1}, q_{s,2},...,q_{s,n})$ where $q_{s,j} = \\frac{e^{y_{s,j}}}{\\sum_{k=1}^{n} e^{y_{s,k}}}$\nThe cross-entropy loss is then calculated as:\n$L_{CE}(y_s, y) = \\sum_{j=1}^{n} y_j ln(q_{s,j})$\nThe final loss function of the model when KD is applied is computed as:\n$L = L_{CE} + \\beta L_{KD}$,\nwhere $\\beta$ is a parameter in the interval [0, 1] that determines the weight of the distillation loss during training.\n1) Professor Importance Decay: In this implementation, $\\beta$ is decremented to half of its initial value after each federated round. A new variable, $\\beta_{limit}$, is introduced so that when $\\beta$ falls below this threshold, it is set to zero. This adjustment allows the student to bypass training the teacher model, thereby saving computational time, energy, and memory. Moreover, the use of the previously mentioned functions enhances the versatility of the models and simplifies the implementation of communication changes."}, {"title": "B. Prototype Generation", "content": "Implementing the prototype basis in the FL environment will follow the approach presented in FedProto [9], but using cross-entropy loss ($L_{CE}$) instead of negative-log likehood loss (LNLL). This approach simplifies the classification process and improves results, particularly when data is extremely limited. Moreover, it only shares class prototypes, significantly reducing communication costs.\nLet $C$ be the set of all existing labels, and $C_i$ the subset of labels known by node $i$. The prototype $C_j^{(i)}$, representing the j-th class in $C_i$, is defined as the average of the vector representations of elements in class j. These representations are the output from the model inference process, taken prior to the final predictions. As shown in Fig. 1, prototypes are calculated using the output of the model first linear layer. The inference function at node i is expressed as $f_i = f_{i,2} \\circ f_{i,1}$, where $f_{i,1}$ provides the vector representations used to calculate prototypes, and $f_{i,2}$ computes the final model predictions using the representations in a standard manner. The prototype $C_j^{(i)}$ for node i is computed as:\n$C_j^{(i)} = \\frac{1}{|D_{i,j}|} \\sum_{(x,y) \\in D_{i,j}} f_{i,1}(x)$,\nwhere $D_{i,j}$ is the subset of the local dataset $D_i$, and all data points belong to class j.\nAfter each round, prototypes are calculated and sent to other nodes. Each client receives the prototypes from other nodes, allowing them to compute global prototypes $\\Bar{C_j^{(i)}}$ for each label. These global prototypes are defined as:\n$\\Bar{C_j^{(j)}} = \\frac{1}{|N_j|} \\sum_{i \\in N_j} C_j^{(i)}$,\nwhere $C_j^{(i)}$ denotes the prototype for class $j$ at node $i$, $N_j$ represents the set of clients that know class $j$, and $|N_j|$ is the number of instances of class j across all nodes.\nOnce the global prototypes are defined, the inference function at node i no longer uses $f_{i,2}$. Instead, the predicted label $\\hat{y}$ is determined by:\n$\\hat{y} = arg \\min_j || f_{i,1}(x) - \\Bar{C_j^{(j)}} ||_2$,\nwhere $|| ||_2$ is the Euclidean distance. The predicted class will be the label of the prototype closest to the input vector representation.\nThe loss function of the models is computed as the weighted sum of two loss components. First, cross-entropy ($L_{CE}$) is used, as defined in the previous section (see Equation 1). The second component is the mean squared error loss ($L_{MSE}$), calculated between the current output's vector representations ($f_{i,1}(x)$) and the global prototype of class j, which corresponds to the true label of the input $x$. This is given by:\n$L_{MSE}(f_{i,1}(x), \\Bar{C_j^{(j)}}) = \\sum_{k=1}^{n} (f_{i,1,k}(x) - \\Bar{C_{j,k}^{(j)}})^2$,\nwhere $n$ is the number of elements in the vector representations, $f_{i,1,k}(x)$ is the k-th element of the vector representation $f_{i,1}(x)$, and $\\Bar{C_{j,k}^{(j)}}$ is the k-th element of the global prototype for class j. All these elements are tensors of the same dimension, ensuring the feasibility of the operation. Thus, the final loss function is calculated as:\n$L = L_{CE} + \\beta L_{MSE}$,\nwhere in this case $\\beta$ is set to 1 [9]."}, {"title": "C. Knowledge Distillation and Prototype Integration", "content": "Once the separate processes of KD and Prototype generation have been described separately, they are integrated together in the proposed approach (see Fig. 1). This perspectives ensures achieving the benefits of both techniques, improving the performance of applying them independently.\nIn this approach, student model is trained using the cross-entropy loss function ($L_{CE}$), the loss between global prototypes and the student's intermediate vector representations ($L_{MSE}$), the KD loss between the logits of the teacher and student models ($L_{KD}$), and an additional loss between the intermediate vector representations of the student and teacher models ($L_{MSE}$). This last loss employs the same function as the loss between the global prototypes and the student vector representations but is applied with different parameters, thus requiring no further development.\nThe teacher model is trained using the cross-entropy loss function ($L_{CE}$), the loss between global prototypes, the teacher's intermediate vector representations ($L_{MSE}$).\nThe student model overall loss is expressed as:\n$L_s = L_{CE}(y_s, y) + \\beta_s L_{MSE}(f_{s,1}(x), \\Bar{C_j^{(i)}}) + \\alpha_s[L_{KD}(y_s, y_t) + L_{MSE}(f_{s,1}(x), f_{t,1}(x))]$.\nWhile the teacher model loss is given by:\n$L_t = L_{CE}(y_t, y) + \\beta_t L_{MSE}(f_{t,1}(x), \\Bar{C_j^{(i)}})$.\nWhere j is the true label corresponding to input x. The notation is the same used throughout this section, where y represents the one-hot encoded vector, where the correct class at position j is set to one. $y_s$ and $y_t$ denote the logits of the student and teacher models. $f_{s,1}(x)$ and $f_{t,1}(x)$ are the intermediate vector representations of the student and teacher models, and $\\Bar{C_j^{(i)}}$ is the global prototype for label j.\nTo compute all these loss functions, the inference methods for student and teacher models must return the logits, intermediate vector representations, and convolutional layers. To achieve this, a dedicated inference function is defined for training purposes in both the student and teacher models."}, {"title": "D. Quantization", "content": "To reduce communication costs in model and prototype exchanges, 16-bit quantization is applied, halving the data size transferred. The quantization function is defined as:\n$Q(x) = \\frac{x}{\\Delta} + 0.5 \\cdot \\Delta$,\nwhere x is the model parameter, Q(x) is the quantized value, and $\\Delta$ is the quantization step. In this case, $\\Delta$ is chosen to fit the values into the 16-bit range.\nWhen models and prototypes from other nodes arrive for aggregation, inverse quantization approximates the original values using:\n$x' = Q(x) \\cdot \\Delta$.\nAfter inverse quantization, model training continues at 32-bit precision, preserving performance while reducing communication costs by half. The precision loss from 16-bit quantization is minimal, ensuring accuracy during training and aggregation."}, {"title": "IV. EXPERIMENTS", "content": "A pool of experiments has evaluated the communication cost, model performance, and training time of ProFe. In addition it has been compared to FedAvg, FedGPD [10], FML [8], and FedProto [9], the most relevant works of the literature. For the experiments, MNIST, CIFAR10, and CIFAR100 datasets (three well-known benchmark datasets focused on image classification) have been considered. To test data diversity, five different configurations are tested: IID, non-IID with 60% classes present in each client, non-IID with 40% classes, non-IID with 20% classes, and non-IID Dirichlet with $\\alpha = 0.5$. Before data splitting, 10% of each dataset is selected as a global test dataset for evaluation. Then, in each node, the local dataset is divided into 80%/20% for training and tests.\nThe experiments comprise 20 nodes following a fully connected topology. For MNIST experiments, 10 rounds of one epoch are executed in the training configuration; for CIFAR10, 20 rounds of three epochs are executed; and for CIFAR100, due to the more complex task, 80 rounds of three epochs are executed. Regarding model setup, for MNIST, a two-layer CNN is chosen as the teacher network, having half of the channels in the student network. For CIFAR10, Resnet18 is chosen as teacher and Resnet8 as student. Finally, for CIFAR100, Resnet32 is chosen as teacher and Resnet18 as student. None of these networks are pre-trained. All the experiments are run on a server with an Intel i7-10700F CPU, two RTX 3080 GPUs, and 98GB RAM. While running the experiments, no other tasks were executed to avoid performance conflicts. For validation, the Nebula framework [12] was used, which includes real communications between nodes using Dockers. The implementation and experiments code is available at [13]."}, {"title": "A. Performance", "content": "The first critical point to validate regarding the proposed approach is the performance comparison with the baseline (FedAvg) and the literature methods. Fig. 2 illustrates the average node F1-Score in each training round for the different datasets tested. ProFe performance is aligned with the baseline in all the scenarios, even improving it is many non-IID ones. It achieves +95% F1-Score for MNIST in IID and most non-IID setups, only degrading to +80% in non-IID with 20% classes known. Similarly, it achieves ~ 60% on CIFAR10, only degraded to +30% on non-IID 20%. Finally, it achieves \u2248 50% on CIFAR100, degraded to \u2248 45% on non-IID 20%.\nFedProto works well on the IID case for MNIST. However, when there is a non-IID distribution or the classification task becomes more complex (CIFAR10/100), its performance greatly degrades. A similar situation is observed for FML, which performs close to FedProto in most experiments. ProFe improves both FedProto and FML in all the conducted experiments. FedGPD maintains a performance closer to the baseline and ProFe in most of the experiments, specially in CIFAR100 classification. In contrast, FedGPD performance degrades in the non-IID cases for the rest of the datasets, only achieving the best performance on non-IID Dirichlet for CIFAR10/100, and being surpassed by ProFe in the rest of the cases.\nFrom these results, it can be considered that ProFe is a viable option for DFL scenarios, having a performance aligned with the baseline and improving other methods in most of the cases, specially in non-IID setups. Then, the next vital step is to verify and compare the communication improvement achieved by the proposed solution."}, {"title": "B. Communication and Training Time Costs", "content": "Another key aspect to consider and optimize is the network cost of all the communications, which exponentially explodes in DFL scenarios. TABLE II shows the bytes sent and received on average by each node in the experiments. Only the network usage in the IID configuration is shown for space purposes, as the local node data distribution does not vary the network usage proportion between methods.\nIt can be seen how FedProto [9] is the best solution in all cases in terms of communications. In the second place, ProFe reduces \u224840-50% of the communication cost from the baseline FedAvg and the other solutions compared, which have even more communication cost than the baseline. This improvement is particularly relevant in DFL, where communication overhead is a critical bottleneck affecting the scalability and practicality of deployment in real-world applications.\nIn terms of training time, TABLE III shows the average time required by the nodes to complete the experiments using each dataset. It can be seen how FedProto is the clear winner in all the datasets. However, it has been shown that its performance is greatly degraded when the classification tasks get more complex. Other literature methods (FedGPD and FML) are close to the base FedAvg, adding between 5-11% to the base time. In contrast, the proposed method has a larger training time due to its increased complexity, adding around 18-20% in the case of CIFAR10 and CIFAR100. In contrast, the time is almost identical to FedAvg in the case of MNIST, where simpler networks are used. This increase in the training time is natural due to the combination of KD and prototypes in the proposed technique, being acceptable in scenarios where optimizing communications is a priority in exchange for sacrificing a proportion of training time.\nIn conclusion, ProFe offers a \u224840-50% communication reduction and an increase of \u224820% in the training time. While this trade-off is reasonable for most scenarios, it is a matter of each use case to decide which approach is the most suitable."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "This paper introduced ProFe, a novel communication optimization algorithm for DFL that integrates KD, prototype learning, and quantization techniques to reduce communication overhead without sacrificing model accuracy. Experiments with benchmark datasets such as MNIST, CIFAR10, and CIFAR100 demonstrated that ProFe effectively reduces communication costs by approximately 40-50% compared to the baseline and other leading methods. Importantly, ProFe maintained or even improved model performance in most cases, particularly in non-IID data setups where data heterogeneity among nodes is significant. In contrast, the integration of these techniques increases training time by approximately 18-20%. However, this trade-off is acceptable in scenarios where communication bandwidth is a critical constraint. Future work aims to reduce ProFe computational complexity through techniques like model pruning and adaptive optimization algorithms. Incorporating methods such as transfer learning may enable ProFe to quickly adapt to new tasks."}]}