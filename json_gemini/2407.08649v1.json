{"title": "Confidence-Based Estimators for Predictive Performance in Model Monitoring", "authors": ["Juhani Kivim\u00e4ki", "Jakub Bia\u0142ek", "Jukka K. Nurminen", "Wojtek Kuberski"], "abstract": "After a machine learning model has been deployed into production, its predictive performance needs to be monitored. Ideally, such monitoring can be carried out by comparing the model's predictions against ground truth labels. For this to be possible, the ground truth labels must be available relatively soon after inference. However, there are many use cases where ground truth labels are available only after a significant delay, or in the worst case, not at all. In such cases, directly monitoring the model's predictive performance is impossible.\nRecently, novel methods for estimating the predictive performance of a model when ground truth is unavailable have been developed. Many of these methods leverage model confidence or other uncertainty estimates and are experimentally compared against a naive baseline method, namely Average Confidence (AC), which estimates model accuracy as the average of confidence scores for a given set of predictions. However, until now the theoretical properties of the AC method have not been properly explored. In this paper, we try to fill this gap by reviewing the AC method and show that under certain general assumptions, it is an unbiased and consistent estimator of model accuracy with many desirable properties. We also compare this baseline estimator against some more complex estimators empirically and show that in many cases the AC method is able to beat the others, although the comparative quality of the different estimators is heavily case-dependent.", "sections": [{"title": "Introduction", "content": "The last decade has seen a considerable improvement in the predictive capability of machine learning (ML) models. However, as techniques in designing and implementing these models have matured, their use in real-life scenarios is still in its infancy. There are many reasons for the slow adoption of ML models in industrial practices. The real-life use cases require the models to be integrated with other services and data pipelines, forming a complex Machine Learning system (Schr\u00f6der and Schulz, 2022). Furthermore, questions of reliability need to be addressed.\nIt is widely accepted that ML models are rarely perfect in their predictions, which always contain some amount of uncertainty. However, in many cases, especially in safety-critical ones, one would like to be able to quantify these uncertainties reliably. Often it is not sufficient to know what the average predictive performance for a held-out test set was in the design phase of the model. One also needs to monitor the uncertainties and model performance after deployment. This is ideally done on a batch or even instance level."}, {"title": "Background", "content": "In this section, we present the necessary background theory for confidence-based model monitoring. We begin with definitions for different distributional shifts, then shortly describe a standard statistical approach to model monitoring when GT is available, and finally discuss confidence scores and model calibration."}, {"title": "Unsupervised accuracy estimation under dataset shift", "content": "Let us assume that we have trained a supervised ML model $f$ using data from a source distribution $p_s(x, y)$ available to us during development. Once the model is deployed, it will make predictions using data from a target distribution"}, {"title": "Traditional process monitoring", "content": "Statistical Process Control (SPC) was pioneered by Shewhart at Bell Labs in the 1920s for systematic monitoring and quality control of industrial production processes. This approach has since been applied to all kinds of processes outside manufacturing with great success. In this wider context, a process can be defined as \"everything required to turn an input into an output for a customer\" (Stapenhurst, 2005). As such, the predictions of an ML model fit this pattern as well.\nIn SPC it is assumed that there is always some variation present in the process being monitored. Furthermore, it is assumed that the variation can be divided into two types, namely common cause variation and special cause variation. Ideally, a process is only affected by common cause variation. If this is the case, the process is said to be under statistical control, meaning that the variance observed in the process conforms to expectations.\nControl charts are central tools used to monitor processes in SPC. A control chart is essentially a run chart of some statistic calculated from consecutive samples. It tracks whether the variation of the statistic from the mean stays within preset control limits. There are various established sets of decision rules (such as the Western Electric rules, see e.g. (Montgomery, 2009)) for recognizing when the data shown on a control chart implies that the process has likely strayed outside statistical control. If this is the case, an alarm is raised for the user. In an ML monitoring setting, widely-used algorithms such as Drift Detection Method (DDM) and its numerous variants (Bayram et al., 2022) are implementations of SPC. However, these methods can only be applied with access to GT.\nThough there are many variants of control charts for monitoring different processes, in this work we are only interested in two variants, namely np-charts and p-charts, which are used to monitor the number and fraction of non-conforming units in a sample respectively. Both of these divide each unit in a sample into binary categories of either conforming (pass) or non-conforming (fail). Naturally, the user needs to establish criteria for treating each unit accordingly. These charts are used in monitoring by gathering samples from the process and tracking the number or fraction of non-conforming units in each sample. The use of these charts requires three assumptions to be made:\n1. The probability of nonconformity $p$ is the same for each unit.\n2. Each unit is independent of its predecessors or successors.\n3. The inspection procedure is the same for each sample and is carried out consistently from sample to sample.\nIn essence, under these assumptions, the nonconformity of each unit is considered to be an i.i.d. random variable following a Bernoulli distribution with parameter $p$. Accordingly, the number of non-conforming units in a sample of $n$ units follows the binomial distribution $B(n, p)$.\nSince the sample mean is an unbiased and consistent estimator of the population mean of the process, which is the quantity we ultimately care about, all common cause variation can be attributed to sampling effects. According to the Central Limit Theorem, the sampling distribution is approximately normal, making the control limits interpretable. A typical control limit is to raise an alarm if the number or fraction of non-conforming units is three standard deviations above the mean, corresponding to 99.7% probability that an observed sample mean outside the control limits is an indicator of special cause variation. The statistics of the sampling distribution are determined with some in-control reference data."}, {"title": "Confidence calibration", "content": "In addition to predicting a certain class, most classifier models also output (or can be modified to output) a vector of soft scores with one value for each class. Usually, the values of the vectors are normalized on the interval [0, 1], summing to one, which means they form a categorical probability distribution over the label space. The maximum value of the normalized vector can be interpreted as a confidence score (Guo et al., 2017) for the prediction, where the value 1 signals maximal confidence.\nIn addition to classifiers, confidence scores can be derived for many other model types. Recently, one increasingly popular approach with non-classifier models has been to train an auxiliary model to assign a confidence score for the predictions of the base model (Chen et al., 2019; Corbi\u00e8re et al., 2019; DeVries and Taylor, 2018; Kivim\u00e4ki et al., 2023; Shao et al., 2020). Using such techniques, one can attach a confidence score to practically any type of model as long as criteria for treating each prediction of the base model as either conforming or non-conforming exists.\nIn the context of classifier models, it is tempting to interpret a confidence score as the probability that a given instance was predicted correctly. However, it is rather common that the predicted probabilities do not align with empirical probabilities. That is, if one monitors all the predictions that were assigned a certain confidence score, the fraction of correct predictions among those predictions would deviate from the confidence score significantly (Guo et al., 2017; Zadrozny and Elkan, 2001). If the confidence scores produced by a model align with empirical probabilities, they are said to be calibrated. We will next provide a more formal definition of calibration.\nLet us denote the feature space of our model by $X$, the output space by $Y$, and the interval of confidence scores [0,1] by $S$. In this work, we are interested in estimating the predictive performance of a given supervised model $f: X \\rightarrow Y \\times S$, with $f = (f_Y, f_S)$, where $f_Y: X \\rightarrow Y$ and $f_S: X \\rightarrow S$. We assume that there exist some fixed criteria $g: Y \\times \\hat{Y} \\rightarrow \\{0, 1\\}$ for deciding if a prediction $\\hat{y} = f_Y(x)$ should be treated as non-conforming or conforming. We let the random variables $X$ and $Y$ denote respectively the features and label drawn from a joint data distribution $p(x, y)$, where $x \\in X$ and $y \\in Y$. We further define derived random variables $\\hat{Y} = f_Y(X)$, $S = f_S(X)$, and $C = g(Y, \\hat{Y})$.\nIf $f_Y$ is a classifier model, a natural choice would be to treat a prediction as conforming if and only if it is correct. Other types of models might require less straightforward ways to assess the (binary) quality of their predictions. For example, regressor models might include tolerance in the form of some acceptable margin of error. This very general formulation enables us to reduce a wide variety of supervised learning settings as binary classification. In this binary classification setting, we can now define what it means for the confidence scores of a model (or the model itself) to be perfectly calibrated.\nDefinition 1. Model $f: X \\rightarrow Y \\times S$ is perfectly calibrated within distribution $p(x, y)$ iff.\n$\\mathbb{P}_{p(x,y)}(C = 1 | S = s) = s \\forall s \\in S$.\nFor the sake of brevity, in the rest of this paper, we will refer to the conforming predictions as correct and to the non-conforming as incorrect. In practice, no model can achieve perfect calibration, so for the concept of calibration to be meaningful, we must be able to quantify calibration error. How this should be done is an active discussion within the scientific community (see e.g. (Guo et al., 2017; Nixon et al., 2019; Posocco and Bonnefoy, 2021; Vaicenavicius et al., 2019)). One of the currently most used metrics is Expected Calibration Error (ECE), which requires the predictions to be binned into $M$ bins according to their confidence scores. The hyperparameter $M$ is set according to user preferences and affects the estimated calibration error. ECE is defined as\n$\\text{ECE} = \\sum_{m=1}^M \\frac{|B_m|}{n} |A_m - C_m|,$ (1)\nwhere $n$ is the total number of predictions, $B_m$ the set of predictions in bin $m$, and $A_m$ and $C_m$ the average accuracy and confidence of predictions in bin $m$ respectively. In the standard version, the bins are assumed to be equiwidth. In this work, however, we choose to use equisize binning, where bin boundaries are adapted so that each bin holds the same number of predictions. This adaptive binning scheme gives rise to Adaptive Expected Calibration Error (ACE), which is shown to be more robust with respect to the number of bins hyperparameter (Nixon et al., 2019). We use $M = 20$ in our experiments. Other than the choice of binning, ACE is calculated exactly the same way as ECE.\nIf the confidence scores are not calibrated, a post hoc calibration mapping can be applied to reduce the calibration error and make the confidence scores match empirical probabilities more accurately (e.g. (Alexandari et al., 2020; Guo et al., 2017; Kull et al., 2017, 2019; Kumar et al., 2019; Naeini et al., 2015; Platt, 1999; Zadrozny and Elkan, 2001, 2002)). There are also ways in training models inducing an implicit calibration effect on the confidence scores (e.g. (Kumar et al., 2018; Mukhoti et al., 2020; M\u00fcller et al., 2019; Ovadia et al., 2019; Seo et al., 2019; Zhang et al., 2020)), although most of these methods were not originally designed with calibration in mind."}, {"title": "Related Work", "content": "In recent years, there have been many propositions on how to perform unsupervised accuracy estimation. Some of these methods leverage model confidence expressed as the maximum of an output vector from the final softmax layer of a neural network (Guillory et al., 2021; Garg et al., 2022; Deng et al., 2023). Alternatively, the prediction entropy has been used as a measure of uncertainty. Others train several models and try to assess the predictive accuracy by tracking discrepancies in the predictions of those models (Baek et al., 2022; Chen et al., 2021a; Jiang et al., 2022). One approach is to use importance weighting (IW) (Lu et al., 2022) to account for the differences between the source and target distributions (Chen et al., 2021b). The methods presented by Guillory et al. (2021) and Garg et al. (2022) are most similar to AC, so we will explain them in more detail below and focus on them in our experiments.\nIn the Difference of Confidences (DoC) method (Guillory et al., 2021), one first measures the difference of the average confidences of a model on a training set and on multiple shifted datasets, which are created from the training dataset by adding different kinds of synthetic noise. Then, an auxiliary regressor model is trained to estimate the base model's accuracy on the shifted datasets using the difference in confidence scores between the training set and the shifted set as the only feature. The authors show experimentally that this approach leads to better generalization on both synthetic and natural shifts when compared to using common distributional distances such as Fr\u00e9chet distance (Fr\u00e9chet, 1957) and Maximum Mean Discrepancy (Muandet et al., 2017) as features (Guillory et al., 2021). They also experiment with using the difference of average entropies (DoE) in place of average confidences arriving at similar results. Furthermore, and most interestingly for our current pursuit, they compare DoC against three simple regression-free baselines: Average Confidence (AC), Average Confidence after calibration with Temperature Scaling (AC TempScaling) (Guo et al., 2017), and subtracting the Difference of Confidences feature value from training accuracy (DoC-Feat). They show that AC is able to beat all of the regression models trained using distributional distances but loses the comparison against DoC, DoE, and Doc-Feat. Most interestingly, they claim that AC performs better than AC TempScaling, implying that calibration might be harmful.\nOne shortcoming of the DoC approach is that it requires the creation of several shifted datasets. This might be infeasible in many cases. Furthermore, no guarantees can be given that the regression function learned with the synthetic shifted datasets would work in all cases of real-world shifts. In fact, Garg et al. (2022) experimentally show the existence of cases where the DoC approach would fail. Instead of using the difference of confidence, they propose to use Average Thresholded Confidence (ATC), where one first finds a confidence threshold such that the fraction of training samples below that threshold equals the prediction error rate on the training set. Then, for a shifted dataset the estimated accuracy is the fraction of samples above the threshold. This method is shown to beat AC, DoC-Feat, IW and the GDE method described by Jiang et al. (2022). No direct comparison between ATC and DoC is made, with the rationale that ATC is regression-free and DoC is not. Contrary to Guillory et al. (2021), the authors found that calibration enhances the effectiveness of their method. In fact, for all methods they tested against (including AC and DoC-Feat), calibration improved the estimates in almost all of the test cases.\nOn a more general note, all of the methods mentioned in the first paragraph of this section are tested on the dataset level. That is, a model is first trained on one dataset and its accuracy is evaluated on a shifted dataset, where the shifted dataset consists of thousands or even tens of thousands of samples. For the estimates to be useful in most practical monitoring settings, they should be relatively accurate for much smaller sets of samples, in the order of hundreds of samples. None of the methods are tested on such smaller sets of samples. To make matters worse, the variance of estimation error is not explored for any of the methods. Thus, even though the methods might give good estimates on a dataset level, the estimates on smaller batches of samples might express large variance, which could seriously hamper their usability in estimating accuracy in a monitoring setting. Furthermore, no theoretical guarantees for their methods are given by Baek et al. (2022); Deng et al. (2023); Guillory et al. (2021), and even where such guarantees are given, they depend on some highly specific assumptions. This raises the question of how well would the experimental results described in these papers hold in other settings.\nThe methods are mainly tested in a multiclass image classification setting (although (Garg et al., 2022; Chen et al., 2021a; Lu et al., 2023) also include some tests with textual data) with neural networks and the confidence scores are taken to be the maximum of the softmax output. This typically results in uncalibrated confidence scores (Guo et al., 2017) and although all of the methods utilizing these confidence scores also report results for calibrated confidence scores, the calibration procedure is equated with temperature scaling (Guo et al., 2017) and the calibration errors for neither the original nor shifted datasets are reported. This makes using AC as a comparative baseline somewhat problematic since the performance of AC might be seriously hampered under miscalibration (as we shall soon see).\nAlthough it is taken as common knowledge that calibration error tends to increase under covariate shift (Ovadia et al., 2019), this is only shown to hold for softmax classifiers calibrated with temperature scaling (Guo et al., 2017). In fact, methods to ensure low calibration error under covariate shift are being developed (Bia\u0142ek et al., 2024). Interestingly, AC"}, {"title": "Average Confidence", "content": "In this section, we present some theoretical results for the AC method, which was originally proposed as a baseline method for detecting misclassified and out-of-distribution instances in neural networks (Hendrycks and Gimpel, 2017). When used in unsupervised accuracy estimation, the predictive accuracy is estimated to be the average of confidence scores, hence the name.\nOur analysis hinges on the assumption that the monitored model produces calibrated confidence scores in addition to its predictions. If this calibration assumption is met, the correctness of each prediction can be regarded as a Bernoulli trial where the confidence score $S$ for the prediction acts as the parameter. Assuming that these trials are independent of each other, the number of correct predictions in $n$ predictions is the sum of the outcomes of the $n$ trials, which follows a Poisson binomial distribution, with the confidence scores $S_i$ as parameters. The probability mass function of a random variable $K$ following this distribution is\n$\\mathbb{P}(K = k) = \\sum_{A \\in F_k} \\prod_{i \\in A} S_i \\prod_{j \\notin A} (1 - S_j),$ (2)\nwhere $F_k$ is the set of all subsets of $\\{1, 2, ..., n\\}$ with $k$ members.\nSince $|F_k| = \\binom{n}{k}$, calculating the required probabilities using the definition directly is infeasible even for relatively small values of $n$. Fortunately, there are several ways to avoid this combinatorial explosion. In this work, we leverage the algorithm presented in Hong (2013), which is based on the Fast Fourier Transform and implemented in the poibin\u00b9 python library. It allows us to derive the Poisson binomial distributions exactly and almost instantly.\nStrictly speaking, our calibration assumption requires the model being monitored to be perfectly calibrated, which (as already stated) is not possible in practice. We will omit this detail in the following theoretical considerations and assume that these results are approximately applicable when the calibration error is small enough. We will return to this issue in Section 5 for more insight on practical applicability."}, {"title": "Estimating predictive accuracy", "content": "The key benefit of leveraging the Poisson binomial distribution to monitor a deployed ML model is that it allows us to waive the binomial assumption used in p-charts and np-charts, where the non-conformity of each unit is considered equally probable. Instead, we can assign these probabilities for each unit individually and still perform rigorous statistical inference in this more general and complex setting.\nIn what follows, we assume for simplicity that the monitor is run in a batch fashion where predictions are gathered into non-overlapping fixed-size windows. The window size $n$ can be set by the user according to their needs. Furthermore, we assume that the model being monitored produces calibrated confidence scores and that we do not have access to GT. After each batch of $n$ predictions, the observed confidence scores are used to form a Poisson binomial distribution for the number of correct predictions within that batch. Next, we will show how one can produce unbiased and consistent estimates for predictive accuracy under these assumptions.\nLet $Z = \\{(X, y)\\}$ be a sample of size $n$, drawn from some target distribution $p_t(x, y)$. We feed the features $X_i$ within the sample $Z$ through our calibrated model $f$, which results in predictions $\\hat{Y}_i$ and calibrated confidence scores $S_i$ with realizations $s_i \\in [0, 1]$. Let the distribution of confidence scores induced by model $f$ operating on $p_t(x, y)$ be denoted as $p_t(s)$. Now, the number of correct predictions $K_Z$ within the sample $Z$ follows a Poisson binomial distribution with the known expectation $\\mathbb{E}[K_Z] = \\sum_{i=1}^n S_i$ and the sample accuracy of model $f$ over the sample $Z$ is $Acc_f(Z) = \\frac{K_Z}{n}$. This implies that the expected sample accuracy is the sample average of the confidence scores, formally\n$\\mathbb{E}_{p_t(x,y)}[Acc_f(Z)] = \\mathbb{E}_{p_t(x,y)}[\\frac{K_Z}{n}] = \\frac{1}{n} \\sum_{i=1}^n S_i = \\bar{S_Z}.$\nOn the other hand, if we sample a single instance $(X, Y)$ from the target distribution $p_t(x, y)$, we can show the expected value of the corresponding confidence score $S$ to be equal to $Acc_f(p_t(x, y))$, the accuracy of model $f$ over the whole target distribution."}, {"title": "Uncertainty in the AC estimates", "content": "We have shown that under the calibration assumption, AC is an unbiased and consistent estimator of model accuracy, which is the best we can hope for in the sense that even if we could directly measure the model accuracy from a given sample of predictions and their corresponding labels, this would still be only an unbiased and consistent estimate for the model performance on the whole underlying data distribution. However, in the latter case, we would only need to account for the (common cause) variance caused by sampling effects, which in SPC is dealt with control limits. With the AC estimator, there is also uncertainty in the estimates themselves, which adds another layer of variance that one might want to take into account.\nAlthough we might not have access to GT labels to measure sample accuracy, we have already seen that sample accuracy can be modeled as a random variable following a Poisson binomial distribution with its expected value equal to the sample average of confidence scores. Thus, one can choose to use the CDF of the Poisson binomial distribution to estimate confidence intervals (CI) for the sample accuracy. For example, one can estimate the (central) 95% CI for the sample accuracy $Acc_f(Z)$ in a given sample $Z$ by first deriving the CDF for the Poisson binomial distribution of $z$ and then finding the range where $0.025 \\leq F(z) \\leq 0.975$. We will empirically demonstrate this choice's effectiveness in Chapter 5. An illustration of the AC method with CIs is given in Fig. 1"}, {"title": "Estimating the confusion matrix for failure prediction", "content": "Sometimes confidence scores are used as a tool for failure prediction (Hendrycks and Gimpel, 2017; Kivim\u00e4ki et al., 2023). A failure prediction mechanism based on confidence scores is a mapping $h: S \\rightarrow \\{0, 1\\}$. It sets a confidence threshold $t \\in [0, 1]$ and treats all predictions with a confidence score equal to or above the threshold as correct and below the threshold as incorrect. Predictions with sub-threshold confidence can be altogether discarded, or sent to further inspection by some downstream system or human-in-the-loop. This might be useful, especially in situations where different types of errors induce different costs. It is worthwhile to stress that this treatment creates second-order predictions (predictions about predictions) and is not to be confused with the true underlying discriminator $g$."}, {"title": "Experiments", "content": "We conduct two experiments with synthetic data. In the first experiment, we examine the quality of point predictions and confidence intervals of the estimates derived using the AC method when the confidence scores are known to be calibrated. In the second experiment, we compare AC, DoC-Feat, and ATC under covariate shift when used to estimate predictive accuracy in a monitoring setting."}, {"title": "Experimenting with calibrated data", "content": "Since with any real-life data, the inevitable calibration error will affect the outcome of our experiments, we will first create a simulated dataset, which will be calibrated by construction. We will begin by describing the data creation process."}, {"title": "Data description", "content": "As stated in Section 2, calibration error is caused by the confidence scores not aligning with empirical probabilities. We can try to minimize this discrepancy by first creating a set of confidence scores. Then, for each element in the set of confidence scores, we can sample a binary label from a Bernoulli distribution using the confidence score as the parameter. This way of sampling ensures that the confidence scores will align with empirical probabilities, albeit some small amount of calibration error might persist due to sampling effects. However, it will suffice to validate the properties discussed in Section 4 empirically.\nWe chose to create our simulated set of confidence scores by drawing samples from a mixture of three Beta distributions. The first component was biased to generate high-confidence predictions, the second to generate average ones, and the third to create low-confidence predictions. By adjusting the mixture weights, we could effectively emulate distributions of confidence scores outputted by models operating on real-life data. Then, by changing the mixture weights, we could easily simulate shifts in model performance to see how the estimator under scrutiny managed to keep track of them. The original dataset was trying to emulate a typical distribution of confidence scores where most predictions are made with high confidence (Guo et al., 2017). For the shifted distribution, the fraction of high-confidence predictions was slightly decreased. The exact parameters and weights of the mixture components used in the experiment are given in Table 2."}, {"title": "Quality of point estimates under shift", "content": "We tested the quality of point estimates for predictive accuracy derived using AC by simulating gradual data shifts. We used two different monitoring window sizes, namely 100 and 500 predictions. We gradually increased the fraction of instances sampled from the shifted distributions in increments of 5%, starting from 0% and ending in 100%. For each degree of shift, we conducted 1,000 trials, where we measured the error of the estimate in each of the trials and plotted the mean of the errors in Fig. 2, along with one standard deviation (shaded area). We contrasted the results of AC by also similarly plotting estimates derived by using either ATC (Garg et al., 2022) or a binomial assumption (using the overall accuracy of the original distribution as the parameter). The binomial assumption comparison was included to give a sense of how fast the estimation error would increase if the predictive performance was assumed not to change.\nWe excluded DoC-Feat from this comparison, since under the calibration assumption, DoC-Feat is equal to AC. In fact, it is straightforward to verify that Doc-Feat can improve on AC only, if the model is either over- or underconfident to begin with and the predictions become even more over- or underconfident. In cases where an underconfident model becomes overconfident after the shift, or vice versa, or if the discrepancy between the AC estimate and true accuracy gets smaller, DoC-Feat will necessarily yield worse estimates than AC.\nThe results show that AC yields point estimates, which on average align with the true sample accuracy. In contrast, ATC underestimates the true accuracy systematically with increasing shifts. Unsurprisingly, the binomial assumption of no change in the predictive performance leads to overestimating the true accuracy. In both cases, the AC method also yields smaller variances in the estimates, with the variance decreasing with a larger window size."}, {"title": "Quality of confidence intervals", "content": "We tested the quality of the estimated confidence intervals of the AC method by first estimating the 95% CI as described in Section 4 and checking how often the actual sample accuracy fits inside the estimated interval. We did this for both the original and shifted data and for window sizes of $\\{100, 200, 300, 400, 500\\}$. In each case, we conducted 10,000 trials and calculated the fraction of times the actual accuracy was within the estimated CI. For good quality estimates this fraction should be close to 95%. This generally desired property of CIs is referred to as validity. Another desired property is optimality, which states that the method used in constructing the CIs should use as much of the information in the dataset as possible.\nSince none of the other methods discussed in Section 3 yield confidence intervals for their estimates, we contrasted the CIs of AC derived using the properties of the Poisson binomial distribution against the method used in NannyML2, which is an open-source library for model monitoring. NannyML implements a performance estimation algorithm, namely Confidence-based Performance Estimator (CBPE), which uses calibrated confidence scores to estimate the confusion matrix similarly to what was described in Section 4.3. Estimates for all typical classification metrics such as precision, recall, F1, AUROC, and so on can then be derived from this matrix. Notably, when CBPE is used to estimate model accuracy, it results in point estimates identical to AC. However, it has a different way of estimating confidence bands. In CPBE, the confidence band is defined as Sampling Error, which is set to be $\\pm$3 Standard Errors of the Mean (SEM) $\\sigma_\\pi$, which is calculated as\n$\\sigma_\\pi = \\frac{\\sigma}{\\sqrt{n}}$ (3)\nwhere $\\sigma$ is an estimation of the population variance as the variance of the true labels in the reference data set (where GT is known) and $n$ is the monitoring window size (NannyML uses the term \"chunk size\"). We replaced the $\\pm$3 standard errors with $\\pm$1.96 standard errors in our tests to get the lower and upper 95% confidence limits for a fair comparison against the CIs derived using the Poisson binomial approach. The results of this experiment are presented in Fig. 3, clearly showing the superiority of the Poisson binomial approach, which produces (roughly) valid CIs. In contrast, the confidence bands produced by the SEM approach are extremely conservative.\nUsing SEM to construct confidence bands only accounts for the uncertainty caused by the sample accuracies of the chunks not aligning with the (potentially shifted) distribution accuracy due to sampling effects. The uncertainty in the estimates for the sample accuracy is omitted, which implicitly equates to treating the mean of confidence scores in a batch as if it matched the sample accuracy exactly. In fact, the only information used from each batch is the chunk size, which results in fixed confidence bands for each batch. Furthermore, the SEM itself can only be estimated from the reference data since the population variance is not known, and since the required i.i.d. assumption gets broken under distributional shift, the estimate becomes an approximation.\nWe argue that it is conceptually clearer to account for the variance due to sampling effects with control limits and form the confidence bands for the estimated accuracy solely on the basis of uncertainty in the estimates themselves using the Poisson binomial approach. In this way, all available information from each batch gets utilized and the confidence bands are allowed to vary from batch to batch. We are currently working to implement this improved approach to forming confidence bands into the NannyML library."}, {"title": "Experimenting under covariate shift", "content": "Next, we compared the confidence-based accuracy estimators AC, DoC-feat, and ACT with both uncalibrated and calibrated confidence scores. Earlier studies (Guillory et al., 2021; Garg et al., 2022) have had conflicting results on whether calibration has a beneficial effect in estimating predictive accuracy or not. In these studies, accuracy estimation has been performed on the dataset level (with sample size in the order of thousands or tens of thousands), whereas we are interested in estimates derived for sample sizes generally used in model monitoring (in the order of hundreds). Furthermore, they have emphasized neural networks used in image classification, with the nature of shifts in the shifted datasets not fully controlled. We expanded this scope by comparing the estimation methods using seven non-neural classifier models, namely Logistic Regression (LR), (Gaussian) Na\u00efve Bayes (NB), K-Nearest Neighbors (KNN), Support Vector Machine (SVM) with RBF kernel, and Random Forest (RF), using implementations provided by the scikit-learn library 3, as well as XGBoost (XGB) 4, and Light-GBM (LGBM) 5 to see whether the earlier results would generalize to these models as well. All models were trained with the default parameter settings of their respective implementations."}, {"title": "Data and model training descriptions", "content": "We used simulated data in order to have full control over the nature of the shift. Since all confidence-based accuracy estimators will eventually fail under concept shift (Garg et al., 2022), we restricted the shift as covariate shift. We experimented with two scenarios, where in both scenarios our data consisted of two features and a binary label (1 or 0).\nIn both scenarios, we drew samples from a mixture of two-dimensional Gaussians, where the modes would form a symmetrical pattern. In the first scenario, we assigned label values for each datapoint stochastically, according to their distance from the line $y = x$. Points falling exactly on the line had a 50% probability for either label. For data points sampled from modes further from the line $y = x$, the majority of samples would be labeled as either 0 or 1 depending on which side of the line they fell, making them easy to predict. Points sampled from modes closer to the line would have higher label entropy, making them hard to predict. In the second scenario, the label values would depend on their distance from the circle $x^2 + y^2 = 5$, where points falling on the circle had a 100% probability of being assigned label 1. This probability decreased rapidly as points moved further from the circle (either inside or outside). This time, points falling close to the circle or far from it could be considered easy and points falling somewhere in between could be considered hard. A more detailed"}]}