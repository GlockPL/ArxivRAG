{"title": "Dual-CBA: Improving Online Continual Learning via Dual Continual Bias Adaptors from a Bi-level Optimization Perspective", "authors": ["Quanziang Wang", "Renzhen Wang", "Yichen Wu", "Xixi Jia", "Minghao Zhou", "Deyu Meng"], "abstract": "In online continual learning (CL), models trained on changing distributions easily forget previously learned knowledge and bias toward newly received tasks. To address this issue, we present Continual Bias Adaptor (CBA), a bi-level framework that augments the classification network to adapt to catastrophic distribution shifts during training, enabling the network to achieve a stable consolidation of all seen tasks. However, the CBA module adjusts distribution shifts in a class-specific manner, exacerbating the stability gap issue and, to some extent, fails to meet the need for continual testing in online CL. To mitigate this challenge, we further propose a novel class-agnostic CBA module that separately aggregates the posterior probabilities of classes from new and old tasks, and applies a stable adjustment to the resulting posterior probabilities. We combine the two kinds of CBA modules into a unified Dual-CBA module, which thus is capable of adapting to catastrophic distribution shifts and simultaneously meets the real-time testing requirements of online CL. Besides, we propose Incremental Batch Normalization (IBN), a tailored BN module to re-estimate its population statistics for alleviating the feature bias arising from the inner loop optimization problem of our bi-level framework. To validate the effectiveness of the proposed method, we theoretically provide some insights into how it mitigates catastrophic distribution shifts, and empirically demonstrate its superiority through extensive experiments based on four rehearsal-based baselines and three public continual learning benchmarks.", "sections": [{"title": "I. INTRODUCTION", "content": "Continual learning (CL) [1], [2] aims to develop models that can accumulate new knowledge while consolidating previously learned knowledge from streaming data. In the context of CL, the data distribution of streaming tasks is in general non-stationary and changes over time, which violates the independent and identically distributed (i.i.d) assumption commonly adopted in traditional machine learning. Therefore, continual learning suffers from catastrophic forgetting problem [3], where the model severely forgets the previously learned knowledge after being trained on a new task.\nTraditional offline CL stores all training batches of the current task and the model is trained on these samples for multiple epochs to achieve relatively superior performance. However, the availability of previously learned batches might be restricted due to privacy concerns [4] or memory limitations. In this paper, we mainly focus on online CL [5], a more challenging and realistic setting. In online CL, samples from each task can be trained in only a single-pass (i.e., one epoch), and previous batches are not accessible in the future.\nUnlike traditional CL settings, the training data distribution in online CL continuously changes throughout the entire training process. Consequently, online CL often leads to more severe distribution shifts, further exacerbating catastrophic forgetting. To alleviate this problem, rehearsal-based methods [2], [6]\u2013[8] employed a small memory buffer to store the examples of previous tasks, aiming at approximating the entire data distribution of all seen tasks. Even though these rehearsal-based methods have achieved sound performance in online CL, most of them often suffer from task-recency bias [9], i.e., the classifiers tend to classify samples into the classes that are currently being trained. Consequently, some previous works aim to improve the original linear classifier [10]\u2013[12] or replace it directly with the nearest classifier [6], [9] to mitigate the negative effects of class imbalance between currently received classes and replayed classes. Despite the promising performance, almost all of these methods implicitly view task-recency bias as a label distribution shift and tackle it from the perspective of class imbalance, which makes these methods sub-optimal in practice [13].\nFrom the Bayes viewpoint, the target of online CL is to accomplish a stable consolidation of knowledge for all tasks by fitting the posterior probability $P(Y|X)$, where $X$ and $Y$ represent stochastic variables of the input data and the corresponding label, respectively. According to Bayes's rule $P(Y|X) \\propto P(X|Y)P(Y)$, any shift from the prior probability $P(Y)$ or the likelihood $P(X|Y)$ will lead to distribution change in $P(Y|X)$. For example, as previous works point out [10]\u2013[13], incoming new tasks can change the label distribution $P(Y)$, leading to severe forgetting. On the other hand, $P(X|Y)$ can also suffer from catastrophic shifts (dubbed feature distribution shift for simplicity) due to the time-varying data streams, especially in online continual learning (CL). To illustrate the feature distribution shift, we conducted a toy experiment using Experience Replay (ER) [14] on CIFAR-10 [15]. As shown in Fig. 1, we exchange the incoming classes of the 4th and 5th tasks while maintaining the label distribution $P(Y)$ unchanged, as plotted in red and blue lines, respectively. By continuously tracking the accuracy of the 1st task, we observe a significant difference in its final accuracy. This validates the existence of feature distribution shifts (i.e., changes in $P(X|Y)$) and highlights a challenging problem for online continual learning (CL): how to achieve stable consolidation of past knowledge amidst these distribution shifts.\nTo tackle these challenges, we introduce the Continual Bias Adaptor (CBA) module to directly adapt the posterior distribution shift online in our conference version [16]. This module aids the original classification network in aligning with the evolving posterior distribution, thereby facilitating stable knowledge consolidation across all seen tasks. To jointly optimize the classification network and the CBA module, we propose a bi-level optimization framework, in which the inner loop problem optimizes the classification network by rehearsal-based methods with the help of CBA, and the outer loop optimizes CBA to assimilate the training bias in continual learning. Specifically, the CBA module in [16] is designed as a lightweight neural network such as a multi-layer perceptron (MLP), which maps the posterior distribution predicted by the classification network to the corresponding adapted posterior probability in a class-specific manner. For clarity, we refer to the CBA module proposed in the conference version as the class-specific CBA in the subsequent sections.\nAlbeit fast adaptation to drastic posterior distribution change, the class-specific CBA module suffers from a serious stability gap problem [17], where the performance of the previously learned tasks significantly drops upon starting to learn a new task, followed by a fast recovery phase. To illustrate this phenomenon, we compare the accuracy curve of the baseline model ER with that of the class-specific CBA (ER-CBA) on CIFAR-100 in Fig. 2. It can be observed that ER-CBA achieves higher performance at the end of each task training, but it suffers a more substantial degradation than ER upon starting to train a new task. This indicates that the class-specific CBA cannot immediately adapt to the catastrophic posterior distribution change caused by incoming new tasks, thus compromising the need for continuous performance evaluation at any time in online CL. This paper reveals that this serious stability gap arises from the class-specific CBA's input, specifically the posterior distribution predicted by the classification network, which changes dramatically upon starting to learn a new task or at the task transition timestamp. Thus, the class-specific CBA module may produce incorrect adjustments for the classification network, resulting in a substantial stability gap.\nTo mitigate the stability gap problem, we propose a novel class-agnostic CBA designed to capture the stable relationship between the new task and all previously learned tasks in the continual learning process. The proposed class-agnostic CBA aggregates the overall posterior probabilities of the new task and those of all the old tasks separately, subsequently yielding stable corrections for the resulting posterior distribution. As a robust and transferable bias adaptor, the class-agnostic CBA module can effectively adapt to sudden changes in posterior distributions and thus can mitigate the stability gap problem. To further leverage the advantages of both the class-agnostic CBA and the class-specific CBA, we integrate these two modules into a unified Dual-CBA. This integration allows the classification network to effectively fit the implicit posterior distribution while maintaining stable knowledge consolidation from all seen tasks throughout the entire continual learning process, as shown in Fig. 2.\nFurthermore, we found that the bi-level framework proposed in [16] encounters a feature bias challenge. This challenge arises from the commonly used batch normalization (BN) [18], which induces feature deviations during the testing stage. Specifically, the population statistics of BN are only updated in the inner loop together with the classification network parameter according to the samples from the new task and those saved in the memory buffer. This causes the exponential moving average (EMA) of BN used for testing to be gradually dominated by the new task because most of the training data in the inner loop comes from the new tasks, leading to biased population statistics. Unfortunately, this bias is not fed back into the posterior distribution, making it challenging for the proposed Dual-CBA to adapt accordingly.\nTo address this problem, we propose Incremental BN (IBN), a simple yet effective method to mitigate the aforementioned feature bias. IBN stops the updating of EMA population statistics of BN in the inner loop and instead replaces them with statistics calculated from the outer loop. This strategy yields a balanced population estimation for the classification model and effectively assimilates the feature bias for the testing stage. For a simple implementation, we can only update the population statistics before the testing stage with the memory buffer data, which shows significant performance improvement for the proposed bi-level learning algorithm.\nIn a nutshell, building on the class-specific CBA published in our conference version [16], this paper extends it from several perspectives. The main contributions are summarized as follows,\n\u2022  We shed light on certain limitations of our previously designed class-specific CBA module. On this basis, we propose the class-agnostic CBA and combine it with the class-specific CBA to form a novel Dual-CBA, which can effectively assimilate the training bias and alleviate the stability gap issue in continual learning.\n\u2022  Based on the proposed bi-level learning framework, we propose Incremental BN to estimate more accurate BN statistics with a simple implementation, assimilating the feature bias during testing.\n\u2022 Theoretically, we explain how the proposed method adapts to distribution shifts and alleviates forgetting in CL from the perspective of gradient alignment. Additionally, we provide insights into the optimization process of the bi-level learning framework from a linear formulation.\n\u2022  We conduct extensive experiments to demonstrate that our method consistently improves upon various rehearsal baselines across multiple continual learning (CL) settings. Specifically, we extend Dual-CBA to a semi-supervised continual learning setting and further validate its generalization ability.\n\u2022 We highlight the strong transferability of the proposed class-agnostic CBA module to unseen tasks or datasets when pre-trained on a limited number of tasks. Concretely, we demonstrate that a pre-trained module can be directly applied to various intra-dataset and inter-dataset scenarios.\nThe paper is organized as follows. Sec. II discusses related works. Sec. III introduces the proposed Dual-CBA method in detail. Sec. V present extensive experiments to evaluate our method, and the conclusions are summarized in Sec. VI."}, {"title": "II. RELATED WORKS", "content": "Continual learning settings. Based on different task construction manners, continual learning (CL) mainly falls into three categories [1], [2], [19]: Task-incremental learning (Task-IL), Domain-incremental learning (Domain-IL), and Class-incremental learning (Class-IL). Specifically, Task IL necessitates prior knowledge of the task index in both the training and testing stages. Domain-IL mainly focuses on concept drift, where the domain of each task changes while the label space remains unchanged [20]\u2013[22]. This paper concentrates on the more challenging Class-IL, where the task index is unavailable during testing [6], [7], [23], [24]. Additionally, from the training perspective, CL can be divided into offline and online CL. Offline CL involves preserving all samples of the current task and training the model on them across multiple epochs [6], [7], [25]\u2013[27]. As for online CL, samples of the current task arrive sequentially, which cannot be stored entirely, and each sample is typically seen once, except when stored in the memory buffer [5], [28]. In this paper, we mainly focus on online CL, which represents a more demanding and realistic setting compared to offline CL. Furthermore, since it is expensive to obtain a large amount of labeled data, many works focus on certain weakly supervised scenarios, such as few-shot [29]\u2013[31], semi-supervised [32]\u2013[34], and imbalance [35], [36], etc. To demonstrate the flexibility of our approach, we also extend our method to the semi-supervised continual learning setting.\nRehearsal-based methods in online CL. In online CL, the main objective is to make the model quickly acquire new knowledge from a new task while retaining previously learned knowledge from old tasks [13], [28], [37]\u2013[41]. A commonly used baseline, Experience replay (ER), trains the incoming new samples along with old samples from the memory buffer together. The variants of ER attempt to employ different techniques in the replay strategy, such as knowledge distillation and random augmentation. For example, DER++ [7] utilized a stronger distillation method to further replay logits of the memory buffer data and Mnemonics [42] applied bi-level optimization to distillate global information of all seen examples into a few learnable ones. RAR [8] adopted random augmentation to alleviate overfitting of the memory buffer, while CLSER [25] constructed plastic and stable models to consolidate recent and structural knowledge distillation. Different from these methods, some studies emphasize maximizing the utility and benefits of the memory buffer samples [42]\u2013[47]. For example, instead of randomly sampling, GSS [43] selected the samples stored in the memory buffer according to the cosine similarity of gradients, MIR [44] chose maximally interfering samples whose prediction will be most negatively impacted by the foreseen parameters update, and OCS [45] picked the most representative data of the current task while minimizing interference with previous tasks. Unlike these methods, our method focuses on alleviating distribution shifts and can plug in most current rehearsal-based approaches.\nTask-recency bias in CL. Task-recency bias [2], [11] in online CL refers to the tendency of classifiers to mistakenly classify examples belonging to previously learned classes as newly received ones. Typically, the linear classifier is susceptible to task-recency bias. To address this, iCaRL [6] proposed to replace the linear classifier with nearest class mean (NCM) classifiers. Similarly, SCR [9] and Co\u00b2L [48] employed the NCM classifier, where the feature extractor was trained using a contrastive learning paradigm. A wide range of works tackles the task-recency bias as a class imbalance problem [10]\u2013[12], [26], [49]. For example, LUCIR [11] introduced weight normalization to the linear classifier, BiC [12] proposed a bias correction layer turned on a held-out validation set, and SS-IL [10] modified the softmax to mitigate the imbalanced penalization for the outputs of old classes. On the flip side, ER-ACE [13] pointed out that task-recency bias can also arise from feature interference and designed an asymmetric loss to address this problem. Different from these methods, our proposed method relaxes the assumption on label/feature distribution shift by directly modeling the posterior distribution shift. Besides, some methods addressing class imbalance may potentially be explored for tackling task-recency bias in online CL [50]\u2013[53]. However, many of these approaches face challenges in generalization to CL due to unstable data distribution.\nNormalization Layer in CL. The normalization layer is a crucial component of deep neural networks. Batch Normalization (BN) [18] has shown a strong performance and become the most commonly used normalization strategy in single-task scenarios with fixed data distribution [54], [55]. However, BN may hinder the continual learning performance because of evolving data distributions [56]\u2013[58]. To address this, Continual Normalization (CN) [56] reduced the cross-task differences by Group Normalization [59] and subsequently utilized BN to normalize the input features. BNT [60] constructed a balanced batch to update BN statistics. TBBN [57] addressed the imbalance problem in BN by employing reshape and repeat operations to construct a task-balanced batch during training. Task-Specific BN [61] focused on task-IL and applied specific BN statistics for each task. Besides, AdaB2N [58] leveraged a regularization term to optimize a modified momentum to balance BN statistics. Different from these works, our proposed IBN is designed to address the feature bias that arises from the bi-level optimization used in this paper."}, {"title": "III. METHOD", "content": "In continual learning, the model is trained on a stream of tasks with evolving data distribution. Considering $N$ sequential tasks {$T_1, T_2,\u2026\u2026, T_N$}, each task can be represented as $T_t = {(x, y)}_{i=0}^{N_t}$, where $N_t$ is the total number of training samples in task $T_t$. For the online CL setting, the classification model $f_\\theta$ trains each new sample only once, and the previously seen batches are almost inaccessible. Let $T_t$ represent the current learning task, and $M$ denotes a small memory buffer that stores the samples of previous tasks {$T_1,\u2026, T_{t\u22121}$}. Here we take a widely used rehearsal-based method, Experience Replay (ER) [14], [62], as an example. ER jointly trains current task $T_t$ with examples sampled from the buffer $M$. The training objective function $\\mathcal{L}$is\n$\\mathcal{L}_{trn} (\\mathcal{B}_{trn}; f_\\theta) = \\frac{1}{|\\mathcal{B}_{trn}|} \\sum_{x,y \\in \\mathcal{B}_{trn}} \\mathcal{L}(f_\\theta(x), y),$\\nwhere the training batch $\\mathcal{B}_{trn} = \\mathcal{B}_t \\cup \\mathcal{B}_{buf}$ consists of a batch of incoming new samples $\\mathcal{B}_t \\subset T_t$ and a batch of samples from the memory buffer $\\mathcal{B}_{buf} \\subset M$, and $\\mathcal{L}$ denotes the cross-entropy loss. Note that the memory buffer $M$ is updated by reservoir sampling after training each batch $\\mathcal{B}_t$, which is a relatively balanced set containing samples of all seen tasks.\nIn this study, we focus on mitigating the issue of catastrophic forgetting of rehearsal-based models with an online CL scenario. As previously mentioned, existing rehearsal-based models often struggle with catastrophic distribution changes caused by dynamic data streams over time. To tackle this challenge, unlike prior methods that regard shifts in label or feature distributions, we suggest directly modeling the catastrophic distribution change for posterior probability $P(Y|X)$, enabling the original classification model $f_\\theta$ to learn a stable knowledge consolidation for all previous tasks.\nThe main methodology involves the design of a Continual Bias Adaptor (CBA) denoted as $g_\\phi$, which serves two key purposes: 1) Dynamically augmenting the classification network $f_\\theta$ to produce more diverse posterior distributions by adjusting the parameters of $g_\\phi$ (where $\\phi$ can be regarded as hyper-parameters), aiming to address the catastrophic posterior change. 2) Guiding the original classification model $f_\\theta$ to fit an implicit posterior that tends to achieve a stable consolidation of knowledge from previously learned tasks. In summary, during the training stage, for a given training example $x_{trn}$, its posterior probability is modified by the augmented classification network $F_{\\theta,\\phi} = g_\\phi \\circ f_\\theta$ in an online CL manner, which can be formulated as\n$\\tilde{y}_{trn} = F_{\\theta,\\phi}(x_{trn}) = g_\\phi(f_\\theta(x_{trn})),$\\nwhere $\\circ$ is the function composition operator and $g_\\phi$ is a lightweight network. The augmented classification network $F_{\\theta,\\phi}$ is firstly updated to learn new knowledge from the training data $\\mathcal{B}_{trn} = \\mathcal{B}_t \\cup \\mathcal{B}_{buf}$ that minimizes the rehearsal-based empirical risk, i.e.,\n$\\theta^* (\\phi) = arg \\min_{\\theta} \\mathcal{L}_{trn} (\\mathcal{B}_{trn}; F_{\\theta,\\phi}),$\\nwhere $\\eta$ is a hyper-parameter of the optimal $\\theta^*$. Note that different rehearsal-based loss functions can be used as the training loss $\\mathcal{L}_{trn}$. Here we only take ER as an example for simplicity and more details can be found in Appendix D.\nThe ultimate objective of our method is to protect the original classification network $f_\\theta$ from catastrophic distribution shift while achieving a stable knowledge consolidation across different tasks. To this end, we further keep tracking the performance of the classification network to prevent catastrophic forgetting, which requires that $f_{\\theta^*({\\phi})}$, obtained by minimizing the rehearsal-based empirical risk Eq. (3), maximizes the performance of all previously seen data. However, accessing all of this historical data is unfeasible in CL, we approximate it by the empirical risk over the memory buffer data, i.e.,\n$\\phi^* = arg \\min_{\\Phi} \\mathcal{L}_{buf} (\\mathcal{B}_{buf}; f_{\\theta^* (\\phi)}),\n = arg \\min_{\\Phi} \\frac{1}{|\\mathcal{B}_{buf}|} \\sum_{x,y \\in \\mathcal{B}_{buf}} \\mathcal{L}( f_{\\theta^* (\\phi)}(x), y) .$\\nThis objective function aims to find the optimal CBA such that the optimized classification network performs well on the memory buffer data, which acts as a stable consolidation of knowledge from the learned tasks.\nIndeed, Eq. (3) and Eq. (4) formulate a bi-level learning framework and the main flowchart is illustrated in Fig. 3. In the inner loop Eq. (3), the classification network is updated to learn new knowledge and rehearse old knowledge from"}, {"title": "C. Design of Dual-CBA", "content": "In this subsection, we delve into the architectural design of the CBA module $g_\\phi$, which plays a vital role in adapting to catastrophic posterior changes in continual learning.\nClass-specific CBA. An intuitive design for the CBA module is to element-wisely adjust the posterior distribution $\\hat{y} \\in \\mathbb{R}^{|C_t|}$ output by the original classification network $f_\\theta$, where $C_t$ denotes the set of all seen classes. This adjustment essentially defines a mapping from $\\hat{y}$ to a corresponding adapted posterior distribution $\\tilde{y} \\in \\mathbb{R}^{|C_t|}$. To achieve this, we use a multi-layer perceptron (MLP) network $g^{spc}_{\\omega}$ parameterized by $\\omega$ as a part of the CBA module, which can be formulated as:\n$\\tilde{y}^{spc} = g^{spc}(\\hat{y}) : \\mathbb{R}^{C_t} \\rightarrow \\mathbb{R}^{C_t|},$\\nwhere $\\hat{y} = [p(y = 1|x),p(y = 2|x),\u2026,p(y = {|C_t|}|x)]^T$ and $\\tilde{y}^{spc} = [p(y = 1|\\hat{y}),p(y = 2|\\hat{y}),\u2026\u2026,p(y = {|C_t|}|\\hat{y})]^T$. Such a class-specific CBA is known as a universal approximator, capable of fitting almost any continuous function [69] and thus can adapt to various posterior distribution changes. Importantly, our conference version [16] has validated the effectiveness of the class-specific CBA on assimilating task-recency bias upon the rehearsal-based CL methods.\nClass-agnostic CBA. Albeit fast adaptation to drastic posterior distribution changes, the proposed class-specific CBA module suffers from a substantial stability gap, where performance significantly drops upon starting to learn new tasks, followed by a fast recovery phase. This to some extent does not meet the need for online CL to conduct performance evaluations at any time. Thus, a natural question is: what causes the class-specific CBA to aggregate the stability gap problem, and how can this challenge be effectively addressed?\nThe primary reason is that the class-specific CBA cannot adapt to the abrupt change in the posterior distribution upon starting to learn a new task. To investigate this, we visualize the difference between the posterior probability $\\hat{y}$ predicted by the classification network before and after the task transition timestamp in Fig. 4. We can see that the classification network $f_\\theta$ produces a relatively high posterior probability for the 85th class, a new class introduced in task 9. However, when task 10 comes in, the probability of the 85th class is significantly lower than before. Since the posterior probability predicted by the classification network is the input of the class-specific CBA, the corruption of these inputs for task 9 hinders the class-specific CBA from appropriately adjusting its posterior probabilities.\nThe second reason is that the class-specific CBA cannot immediately adjust the posterior probability of the classes in the new task. When a new task comes in, the class-specific CBA fails to accurately accommodate the posterior probability of the new task because it has not yet encountered the incoming new classes. As shown in Fig. 4, the arrival of task 10 leads to a high posterior probability for its own classes, such as the 95th class. This indicates that the class-specific CBA fails to control the increasing probability of the new task.\nTo address the aforementioned challenges, instead of directly adjusting the posterior probability in a class-specific manner, we resort to modeling a stable relationship between new and old tasks to adapt to the posterior distribution shift while avoiding a significant stability gap upon starting to learn new tasks. Let $C_{new}$ and $C_{old}$ denote the class sets of the new task and old tasks, respectively. Our main idea is to adjust the posterior probability of the new task $\\hat{y}_{new} = \\sum_{c \\in C_{new}} P(y = c|x)$ and that of all the old tasks $\\hat{y}_{old} = \\sum_{c \\in C_{old}} P(y = c|x)$, given their relatively stable numerical relationship throughout the continual learning process as shown in Fig. 5. To achieve this, we design a novel class-agnostic CBA $g_{\\nu}$ parameterized by $\\nu$ as follows:\n$(\\tilde{Y}_{old}, \\tilde{Y}_{new}) = g^{agn}(Y_{old}, \\hat{Y}_{new}) : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2.$\nNote that the class-agnostic CBA treats the posterior probabilities of the classes in the new task as a whole, and does the same for the old task. Then, for a specific class, the adjustment of its posterior probability is formulated by averaging $\\tilde{Y}_{old}$ or $\\tilde{Y}_{new}$, and it can be formulated as\np(y = c|\\hat{y}) = \n\\frac{\\frac{1}{|C_{old}|} \\tilde{Y}_{old}}{\\frac{1}{|C|} \\tilde{Y}} c \\in C_{old}\n\\frac{\\frac{1}{|C_{new}|} \\tilde{Y}_{new}}{\\frac{1}{|C|} \\tilde{Y}} c \\in C_{new}\nand the final adapted posterior probabilities are given by $\\tilde{y}^{agn} = [p(y = 1|\\hat{y}), p(y = 2|\\hat{y}),\u2026\u2026,p(y = {|C_t|}|\\hat{y})]^T$.\nThe proposed class-agnostic CBA module serves as a robust and transferable bias adaptor, effectively adapting to sudden changes in posterior distributions after training on a few pairs of new and old tasks. Specifically, we can analyze its adaptation mechanisms for both the preceding old task and the incoming new task. We assume that the task transition timestamp is $t + 1$, and we have\n\u2022 For the incoming $(t + 1)$-th new task, the class-agnostic CBA module has processed $t$ pairs of old and new tasks, and its current weights can produce a valuable initialization or experience learned from previous data. Therefore, the class-agnostic CBA can quickly adapt to new tasks."}, {"title": "D. Incremental Batch Normalization", "content": "In this subsection, we further address the feature bias intro-duced during the training stage by the commonly used BN [18] of our proposed bi-level framework. Specifically, BN calculates batch statistics of each feature map for normalization during training, while using population statistics estimated by an exponential moving average (EMA) during testing. In our proposed bi-level optimization framework in the conference version, the population statistics of BN are only updated in the inner loop together with the classification network parameters $\\theta$. Unfortunately, these population statistics estimated by EMA in the inner loop are seriously biased toward the current task because the training data are dominated by new tasks, leading to a biased feature extraction during testing. For simplicity, we take the population mean used for testing at task $t$ as an example. Specifically, we denote it as $\\mu_{test}^{t,k}$, and it is updated using an EMA scheme as follows:\n$\\mu_{test}^{t,k} = (1 \u2013 \\eta) \\mu_{test}^{t,k-1} + \\eta \u00b7 \\mu_{train}^{t,k},$\\nwhere $\\mu_{train}^{t,k}$ is the feature mean of the $k$-th batch of training data $\\mathcal{B}_{trn}$ at task $t$. Then, we have\n$\\mu_{test}^{t,k} = (1 \u2013 \\eta)^k \\mu_{test}^{t,0} + \\sum_{l=1}^k (1- \\eta)^{k-l} \\eta \u00b7 \\mu_{train}^{t,l},$\\nwhere $\\mu_{test}^{t,0}$ is initialized as the population mean calculated at the final of the last task $t \u2212 1$. It can be observed that $\\mu_{test}^{t,0}$ encodes the feature mean of previous tasks and the corresponding weight is exponentially decreasing as current task training progresses. Additionally, the batch mean $\\mu_{train}^{t,k}$ is dominated by the current new task data because the number of the memory buffer samples is much smaller than that of incoming new samples. Consequently, the population mean updated by EMA in the inner loop is seriously biased towards new tasks, where the population variance has the same tendency.\nNote that this feature bias is not reflected in the posterior distribution during training, where the proposed Dual-CBA module cannot correct the biased feature. To achieve unbiased estimations of these population statistics, we should estimate them on the seen data from all tasks, which is unrealistic in continual learning. Therefore, from the bi-level optimization perspective, an intuitive idea is to update these population statistics by the balanced buffer data set used in the outer loop. Intrinsically, the memory buffer obtained via reservoir sampling is an excellent approximation of the entire data distribution for all seen tasks in an ideal case. Based on this"}, {"title": "IV. THEORETICAL ANALYSIS", "content": "In this section, we provide a theoretical analysis of our method. Firstly, we explain how our bi-level optimization method effectively prevents forgetting from the perspective of gradient alignment. Then we illustrate the general intuition by a linear formulation of the bi-level optimization framework.\n\n\nThe following theorem reveals that the proposed bi-level optimization inherently establishes gradient alignment between the loss on the training set $\\mathcal{B}_{trn}$ and the memory buffer $\\mathcal{B}_{buf}$.\n$\\mathcal{L}_{trn} (\\mathcal{B}_{trn}; F_{\\theta,\\phi})$ denote the gradients of the outer-loop and inner-loop losses with respect to the classification model parameter $\\theta$, respectively. If the outer-loop loss $\\mathcal{L}_{buf} (\u00b7; f_{\\theta})$ is $\\eta$ gradient Lipschitz, continuous, then the bi-level optimization Eq. (3) and (4) potentially guarantees an alignment between $\\mathcal{G}_{buf}$ and $\\mathcal{G}_{trn}$, that is\n$\\langle \\mathcal{G}_{buf}, \\mathcal{G}_{trn} \\rangle \\geq \\frac{\\alpha \\eta}{2}||\\mathcal{G}_{trn}||^2,$\\nwhere $\\alpha > 0$ is the inner-loop learning rate and $\\eta > 0$ is the Lipschitz constant.\nThe detailed proof of Theorem 1 is shown in Appendix B. This theorem demonstrates that optimizing our proposed bi-level framework Eq. (3) and (4) encourages the angle between the gradients $\\mathcal{G}_{buf}$ and $\\mathcal{G}_{trn}$ to be as small as possible, which reveals two insights into our algorithm. On the one hand, this theorem indicates that our approach will push the gradients of the classification network on the training data (with the Dual-CBA module) to those on the buffer data (without the Dual-CBA module). This property potentially encourages the Dual-CBA module to mitigate the training bias present in the training data. That is why our algorithm can effectively mitigate the task-recency bias in the CL process. On the other hand, this theorem shows an alignment between the gradients of the classification network on the training data and buffer data, which regularizes the updating of the classification network on the new task and ensures it does not deviate from the previous updating direction too much. This gradient alignment can effectively alleviate forgetting in CL, which has been demonstrated in many previous gradient-alignment-based methods [70], [71]. However, these works calculate the classification network gradient on training data without accounting for task-recency bias, where the network may seriously bias to new tasks, resulting in an imprecise gradient alignment. In contrast, our proposed Dual-CBA method mitigates this bias, ensuring more accurate gradient alignment and improved performance.\nWe herein consider a convex model to delve into the insight of our proposed Dual-CBA model. Specifically, we reformulate Dual-CBA as a linear model where the origin and augmented classification networks are represented as:\n$Original: \\hat{Y}_t = (X_t)^T \\Theta + e_t$\n$Augmented: \\tilde{Y}_t = \\hat{Y}_t \\Phi^* + e_t,$\\nwhere $X_t \\in \\mathbb{R}^{p \\times n_t}$ is the input data and $\\hat{Y}_t, \\tilde{Y}_t \\in \\mathbb{R}^{n_t \\times c_t}$ denote outputs of the original and augmented classification networks, respectively. The parameters of the classification model are $\\Theta \\in \\mathbb{R}^{p \\times c_t}$ and the Dual-CBA module is represented as $\\Phi^* \\in \\mathbb{R}^{C_t \\times c_t}$, where we omit the notation of $\\nu$ for clarity. Additionally, we assume the Gaussian feature and noise of this linear model following [72] with $e_t \\sim \\mathcal{N}(0, \\sigma^2)$ denoting the noise vector.\nWe consider the mean square error (MSE) loss as the convex objective function. In this case, the proposed bi-level optimization framework can be represented as follows:\n$\\min_{\\Phi} \\mathcal{L}_{trn} = \\frac{1}{n_t} \\min_{\\theta_t} || (X_{trn})^T \\Theta - Y_{trn} ||^2$\ns.t.\n$\\theta_t (\\phi) = arg \\min_{\\theta_t} \\frac{1}{n_{trn}} \\min_{\\theta_{trn}} || (X_{trn})^T \\Theta_t - Y_{trn} ||^2.$\nNote that $\\theta_t \\in \\mathbb{R}^{p \\times c_t}$ is the weight matrix and $\\theta_t(\\phi)$ represents a function of $\\Phi \\in \\mathbb{R}^{C_t \\times C_t}$. Thanks to the excellent properties of convex optimization, we can get the closed-form solution of this bi-level optimization framework, which can be summarized as follows:\n$\\\\=(X_{trn}(X_{trn})^T)^{-1}X_{trn}$ and \n$\\mathbb{A}=(X_{buf})^T(X_{trn}) = (X_{buf})^T (X_{trn}(X_{trn})^T)^{-1} X_{trn}$. If the parameter of CBA $\\Phi$ is not singular, then the closed-form"}, {"title": "APPENDIX", "content": "In this section, we first review the proposed bi-level learning framework to optimize the parameters of the classifier network and the Dual-Continual Bias Adaptor (Dual-CBA) module. Formally"}]}