{"title": "Dual-CBA: Improving Online Continual Learning via Dual Continual Bias Adaptors from a Bi-level Optimization Perspective", "authors": ["Quanziang Wang", "Renzhen Wang", "Yichen Wu", "Xixi Jia", "Minghao Zhou", "Deyu Meng"], "abstract": "In online continual learning (CL), models trained on changing distributions easily forget previously learned knowledge and bias toward newly received tasks. To address this issue, we present Continual Bias Adaptor (CBA), a bi-level framework that augments the classification network to adapt to catastrophic distribution shifts during training, enabling the network to achieve a stable consolidation of all seen tasks. However, the CBA module adjusts distribution shifts in a class-specific manner, exacerbating the stability gap issue and, to some extent, fails to meet the need for continual testing in online CL. To mitigate this challenge, we further propose a novel class-agnostic CBA module that separately aggregates the posterior probabilities of classes from new and old tasks, and applies a stable adjustment to the resulting posterior probabilities. We combine the two kinds of CBA modules into a unified Dual-CBA module, which thus is capable of adapting to catastrophic distribution shifts and simultaneously meets the real-time testing requirements of online CL. Besides, we propose Incremental Batch Normalization (IBN), a tailored BN module to re-estimate its population statistics for alleviating the feature bias arising from the inner loop optimization problem of our bi-level framework. To validate the effectiveness of the proposed method, we theoretically provide some insights into how it mitigates catastrophic distribution shifts, and empirically demonstrate its superiority through extensive experiments based on four rehearsal-based baselines and three public continual learning benchmarks.", "sections": [{"title": "I. INTRODUCTION", "content": "Continual learning (CL) [1], [2] aims to develop models that can accumulate new knowledge while consolidating previously learned knowledge from streaming data. In the context of CL, the data distribution of streaming tasks is in general non-stationary and changes over time, which violates the independent and identically distributed (i.i.d) assumption commonly adopted in traditional machine learning. Therefore, continual learning suffers from catastrophic forgetting problem [3], where the model severely forgets the previously learned knowledge after being trained on a new task.\nTraditional offline CL stores all training batches of the current task and the model is trained on these samples for multiple epochs to achieve relatively superior performance. However, the availability of previously learned batches might be restricted due to privacy concerns [4] or memory limitations. In this paper, we mainly focus on online CL [5], a more challenging and realistic setting. In online CL, samples from each task can be trained in only a single-pass (i.e., one epoch), and previous batches are not accessible in the future.\nUnlike traditional CL settings, the training data distribution in online CL continuously changes throughout the entire training process. Consequently, online CL often leads to more severe distribution shifts, further exacerbating catastrophic forgetting. To alleviate this problem, rehearsal-based methods [2], [6]\u2013[8] employed a small memory buffer to store the examples of previous tasks, aiming at approximating the entire data distribution of all seen tasks. Even though these rehearsal-based methods have achieved sound performance in online CL, most of them often suffer from task-recency bias [9], i.e., the classifiers tend to classify samples into the classes that are currently being trained. Consequently, some previous works aim to improve the original linear classifier [10]\u2013[12] or replace it directly with the nearest classifier [6], [9] to mitigate the negative effects of class imbalance between currently received classes and replayed classes. Despite the promising performance, almost all of these methods implicitly view task-recency bias as a label distribution shift and tackle it from the perspective of class imbalance, which makes these methods sub-optimal in practice [13].\nFrom the Bayes viewpoint, the target of online CL is to"}, {"title": "II. RELATED WORKS", "content": "Continual learning settings. Based on different task construction manners, continual learning (CL) mainly falls into three categories [1], [2], [19]: Task-incremental learning (Task-IL), Domain-incremental learning (Domain-IL), and Class-incremental learning (Class-IL). Specifically, Task IL necessitates prior knowledge of the task index in both the training and testing stages. Domain-IL mainly focuses on concept drift, where the domain of each task changes while the label space remains unchanged [20]\u2013[22]. This paper concentrates on the more challenging Class-IL, where the task index is unavailable during testing [6], [7], [23], [24]. Additionally, from the training perspective, CL can be divided into offline and online CL. Offline CL involves preserving all samples of the current task and training the model on them across multiple epochs [6], [7], [25]\u2013[27]. As for online CL, samples of the current task arrive sequentially, which cannot be stored entirely, and each sample is typically seen once, except when stored in the memory buffer [5], [28]. In this paper, we mainly focus on online CL, which represents a more demanding and realistic setting compared to offline CL. Furthermore, since it is expensive to obtain a large amount of labeled data, many works focus on certain weakly supervised scenarios, such as few-shot [29]\u2013[31], semi-supervised [32]\u2013[34], and imbalance [35], [36], etc. To demonstrate the flexibility of our approach, we also extend our method to the semi-supervised continual learning setting.\nRehearsal-based methods in online CL. In online CL, the main objective is to make the model quickly acquire new knowledge from a new task while retaining previously learned knowledge from old tasks [13], [28], [37]\u2013[41]. A commonly used baseline, Experience replay (ER), trains the incoming new samples along with old samples from the memory buffer together. The variants of ER attempt to employ different techniques in the replay strategy, such as knowledge distillation and random augmentation. For example, DER++ [7] utilized a stronger distillation method to further replay logits of the memory buffer data and Mnemonics [42] applied bi-level optimization to distillate global information of all seen examples into a few learnable ones. RAR [8] adopted random augmentation to alleviate overfitting of the memory buffer, while CLSER [25] constructed plastic and stable models to consolidate recent and structural knowledge distillation. Different from these methods, some studies emphasize maximizing the utility and benefits of the memory buffer samples [42]\u2013[47]. For example, instead of randomly sampling, GSS [43] selected the samples stored in the memory buffer according to the cosine similarity of gradients, MIR [44] chose maximally interfering samples whose prediction will be most negatively impacted by the foreseen parameters update, and OCS [45] picked the most representative data of the current task while minimizing interference with previous tasks. Unlike these methods, our method focuses on alleviating distribution shifts and can plug in most current rehearsal-based approaches.\nTask-recency bias in CL. Task-recency bias [2], [11] in online CL refers to the tendency of classifiers to mistakenly classify examples belonging to previously learned classes as newly received ones. Typically, the linear classifier is susceptible to task-recency bias. To address this, iCaRL [6] proposed to replace the linear classifier with nearest class mean (NCM) classifiers. Similarly, SCR [9] and Co\u00b2L [48] employed the NCM classifier, where the feature extractor was trained using a contrastive learning paradigm. A wide range of works tackles the task-recency bias as a class imbalance problem [10]\u2013[12], [26], [49]. For example, LUCIR [11] introduced weight normalization to the linear classifier, BiC [12] proposed a bias correction layer turned on a held-out validation set, and SS-IL [10] modified the softmax to mitigate the imbalanced penalization for the outputs of old classes. On the flip side, ER-ACE [13] pointed out that task-recency bias can also arise from feature interference and designed an asymmetric"}, {"title": "III. METHOD", "content": "In this study, we focus on mitigating the issue of catastrophic forgetting of rehearsal-based models with an online CL scenario. As previously mentioned, existing rehearsal-based models often struggle with catastrophic distribution changes caused by dynamic data streams over time. To tackle this challenge, unlike prior methods that regard shifts in label or feature distributions, we suggest directly modeling the catastrophic distribution change for posterior probability $P(Y|X)$, enabling the original classification model $f_\\theta$ to learn a stable knowledge consolidation for all previous tasks.\nThe main methodology involves the design of a Continual Bias Adaptor (CBA) denoted as $g_\\phi$, which serves two key purposes: 1) Dynamically augmenting the classification network $f_\\theta$ to produce more diverse posterior distributions by adjusting the parameters of $g_\\phi$ (where $\\phi$ can be regarded as hyper-parameters), aiming to address the catastrophic posterior change. 2) Guiding the original classification model $f_\\theta$ to fit an implicit posterior that tends to achieve a stable consolidation of knowledge from previously learned tasks. In summary, during the training stage, for a given training example $x_{trn}$, its posterior probability is modified by the augmented classification network $F_{\\theta,\\phi} = g_\\phi \\circ f_\\theta$ in an online CL manner, which can be formulated as\n$\\tilde{y}_{trn} = F_{\\theta,\\phi}(x_{trn}) = g_\\phi (f_\\theta (x_{trn})),                                                                 (2)$\nwhere $\\circ$ is the function composition operator and $g_\\phi$ is a lightweight network. The augmented classification network $F_{\\theta,\\phi}$ is firstly updated to learn new knowledge from the training data $B_{trn} = B_t \\cup B_{buf}$ that minimizes the rehearsal-based empirical risk, i.e.,\n$\\theta^* (\\phi) = \\arg \\min_{\\theta} L_{trn} (B_{trn}; F_{\\theta,\\phi}),                                                      (3)$\nwhere $\\alpha$ is a hyper-parameter of the optimal $\\theta^*$. Note that different rehearsal-based loss functions can be used as the training loss $L_{trn}$. Here we only take ER as an example for simplicity and more details can be found in Appendix D.\nThe ultimate objective of our method is to protect the original classification network $f_\\theta$ from catastrophic distribution shift while achieving a stable knowledge consolidation across different tasks. To this end, we further keep tracking the performance of the classification network to prevent catastrophic forgetting, which requires that $f_{\\theta^* (\\phi)}$, obtained by minimizing the rehearsal-based empirical risk Eq. (3), maximizes the performance of all previously seen data. However, accessing all of this historical data is unfeasible in CL, we approximate it by the empirical risk over the memory buffer data, i.e.,\n$\\phi^* = \\arg \\min_{\\Phi} L_{buf} (B_{buf}; f_{\\theta^* (\\phi)}),                                                  \\newline\\newline     = \\arg \\min_{\\Phi}   \\frac{1}{|B_{buf}|} \\sum_{x,y \\in B_{bu f}} L (f_{\\theta^* (\\phi)} (x), y).                                          (4)$\nThis objective function aims to find the optimal CBA such that the optimized classification network performs well on the memory buffer data, which acts as a stable consolidation of knowledge from the learned tasks.\nIndeed, Eq. (3) and Eq. (4) formulate a bi-level learning framework and the main flowchart is illustrated in Fig. 3. In the inner loop Eq. (3), the classification network is updated to learn new knowledge and rehearse old knowledge from"}, {"title": "C. Design of Dual-CBA", "content": "In this subsection, we delve into the architectural design of the CBA module $g_\\phi$, which plays a vital role in adapting to catastrophic posterior changes in continual learning.\nClass-specific CBA. An intuitive design for the CBA module is to element-wisely adjust the posterior distribution $\\hat{y} \\in R^{|C_t|}$ output by the original classification network $f_\\theta$, where $C_t$ denotes the set of all seen classes. This adjustment essentially defines a mapping from $\\hat{y}$ to a corresponding adapted posterior"}, {"title": "D. Incremental Batch Normalization", "content": "In this subsection, we further address the feature bias introduced during the training stage by the commonly used BN [18] of our proposed bi-level framework. Specifically, BN calculates batch statistics of each feature map for normalization during training, while using population statistics estimated by an exponential moving average (EMA) during testing. In our proposed bi-level optimization framework in the conference version, the population statistics of BN are only updated in the inner loop together with the classification network parameters $\\theta$. Unfortunately, these population statistics estimated by EMA in the inner loop are seriously biased toward the current task because the training data are dominated by new tasks, leading to a biased feature extraction during testing. For simplicity, we take the population mean used for testing at task t as an example. Specifically, we denote it as $\\mu_{test}^t$, and it is updated using an EMA scheme as follows:\n$\\mu_{test}^{t,k} = (1 \u2013 \\eta) \u00b7 \\mu_{test}^{t,k-1} + \\eta \u00b7 \\mu_{train}^{t,k},                                                                              (11)$\nwhere $\\mu_{train}^{t,k}$ is the feature mean of the k-th batch of training data $B_{trn}$ at task t. Then, we have\n$\\mu_{test}^{t,k} = (1 - \\eta)^k \\mu_{test}^{t,0} + \\sum_{l=1}^{k} (1-\\eta)^{k-l} \\eta \\cdot \\mu_{train}^{t,l}                                                        (12)$\nwhere $\\mu_{test}^{t,0}$ is initialized as the population mean calculated at the final of the last task t - 1. It can be observed that $\\mu_{test}^{t,0}$ encodes the feature mean of previous tasks and the corresponding weight is exponentially decreasing as current task training progresses. Additionally, the batch mean $\\mu_{train}$ is dominated by the current new task data because the number of the memory buffer samples is much smaller than that of incoming new samples. Consequently, the population mean $\\mu_{test}^{t,k}$ updated by EMA in the inner loop is seriously biased towards new tasks, where the population variance has the same tendency."}, {"title": "IV. THEORETICAL ANALYSIS", "content": "In this section, we provide a theoretical analysis of our method. Firstly, we explain how our bi-level optimization method effectively prevents forgetting from the perspective of gradient alignment. Then we illustrate the general intuition by a linear formulation of the bi-level optimization framework."}, {"title": "A. Gradient Alignment in Dual-CBA", "content": "The following theorem reveals that the proposed bi-level optimization inherently establishes gradient alignment between the loss on the training set $B_{trn}$ and the memory buffer $B_{buf}$.\nTheorem 1: Let $\\mathcal{G}_{buf} \uc2a4 \\nabla_{\\theta} \\mathcal{L}_{buf} (B_{buf}; f_{\\theta^* (\\phi)})$ and $\\mathcal{G}_{trn} \uc2a4 \\nabla_{\\theta} \\mathcal{L}_{trn} (B_{trn}; F_{\\theta \\kappa,\\phi})$ denote the gradients of the outer-loop and inner-loop losses with respect to the classification model parameter $\\theta$, respectively. If the outer-loop loss $\\mathcal{L}_{buf} (\u00b7; f_{\\theta})$ is $\\eta$ gradient Lipschitz, continuous, then the bi-level optimization Eq. (3) and (4) potentially guarantees an alignment between $\\mathcal{G}_{buf}$ and $\\mathcal{G}_{trn}$, that is\n$\\langle \\mathcal{G}_{buf}, \\mathcal{G}_{trn} \\rangle \\ge \\frac{\\alpha \\eta}{2} ||\\mathcal{G}_{trn}||^2,                                                                           (14)$\nwhere $\\alpha > 0$ is the inner-loop learning rate and $\\eta > 0$ is the Lipschitz constant."}, {"title": "B. Closed-form Solution of Linear Dual-CBA", "content": "We herein consider a convex model to delve into the insight of our proposed Dual-CBA model. Specifically, we reformulate Dual-CBA as a linear model where the origin and augmented classification networks are represented as:\nOriginal: $\\widehat{Y}_{t} = (X_{t})^T\\theta + e_{t}$\nAugmented: $Y_{t} = \\widehat{Y}_{t}\\Phi^* + e_{t},                                                                                (15)$\nwhere $X_{t} \\in R^{p\\times n_t}$ is the input data and $\\widehat{Y}_{t}$, $Y_t \\in R^{n_t \\times c_t}$ denote outputs of the original and augmented classification networks, respectively. The parameters of the classification model are $\\theta \\in R^{p\\times c_t}$ and the Dual-CBA module is represented as $\\Phi^* \\in R^{C_t \\times c_t}$, where we omit the notation of $\\nu$ for clarity. Additionally, we assume the Gaussian feature and noise of this linear model following [72] with $e_{t} \\sim N(0, \\sigma^2)$ denoting the noise vector.\nWe consider the mean square error (MSE) loss as the convex objective function. In this case, the proposed bi-level optimization framework can be represented as follows:\n$\\phi^* = \\arg \\min_{\\Phi} \\frac{1}{n_{t}} || (X_{buf})^T\\theta(\\Phi) - Y_{buf} ||^2                                                                          \\newline\\newline   s.t. \\ \\theta(\\phi) = \\arg \\min_{\\theta} \\frac{1}{n_{trn}} || (X_{trn})^T\\theta - Y_{trn} ||^2.                                                                 (16)$\nNote that $\\theta_t \\in R^{p\\times c_t}$ is the weight matrix and $\\theta(\\Phi)$ represents a function of $\\Phi \\in R^{C_t \\times C_t}$. Thanks to the excellent properties of convex optimization, we can get the closed-form solution of this bi-level optimization framework, which can be summarized as follows:\nTheorem 2: Let $(X)^\u2020= (XX^T)^{-1}X$ and A = $(X_{buf})^T(X_{trn}) = (X_{buf})^T (X_{trn} (X_{trn})^T)^{-1} X_{trn}$. If the parameter of CBA $\\Phi$ is not singular, then the closed-form"}, {"title": "V. EXPERIMENTS", "content": "To validate the effectiveness of the proposed method, we compare our Dual-CBA to multiple approaches on various datasets under online CL, semi-supervised CL, blurry tasks, and offline CL settings. We also conduct extensive ablation experiments to analyze different components of our approach."}, {"title": "A. Experiment Settings", "content": "Experimental datasets. Following [7], we experiment with three widely-used datasets: Split CIFAR-10 [15], Split CIFAR-100 and Split Tiny-ImageNet [73]. Concretely, Split CIFAR-10 contains five binary classification tasks, which are constructed by evenly splitting ten classes of CIFAR-10. Split CIFAR-100 and Split Tiny-ImageNet both have longer task sequences with each comprising ten disjoint tasks. Specifically, Split CIFAR-100 includes ten tasks with 10 classes each, while Split Tiny-ImageNet includes ten tasks with 20 classes each (see Appendix D for details of the three datasets).\nEvaluation metrics. To comprehensively evaluate all comparison methods, we consider the following metrics:\n\u2022 Average Accuracy (ACC \u2191): This metric calculates the average accuracy of the model trained on all tasks, i.e. $ACC = 1/T \\sum_{j=1}^{T} \\sum_{t=1}^{j} a_{t,T}$, where $a_{i,j}$ represents the accuracy of the task i after training on the task j. \u2191 indicates that a higher ACC value corresponds to better performance.\n\u2022 Forgetting Measure (FM \u2193): This metric averages the differences between the best accuracy and the final accuracy, i.e. $FM = 1/T \\sum_{t=1}^{T} a_t^* \u2212 a_{t,T}$, where the $a_t^*$ is the best accuracy of task t in the whole training process. \u2193 indicates that a lower FM value corresponds to better performance.\n\u2022 Area Under the Curve of Accuracy (ACCAUC\u2191): This metric is the area under the curve of the accuracy [28], i.e., $ACCAUC = \\sum_{i} \\overline{a(i\u22c5 \u0394n)} \u22c5 \u0394n$, where $\\overline{a(i)}$ represent the average accuracy when the model training at step i, and $\u0394n$ is the interval training step which is 5 for faster evaluation in our experiments. \u2191 indicates that a higher ACCAUC value corresponds to better performance.\nImplementation details. We adopt the commonly used ResNet-18 [74] as our backbone [7], [12], [25], and train all methods using the Stochastic Gradient Descent (SGD) optimizer. We use Adam [75] to optimize the proposed Dual-CBA module and set the learning rate as 0.001 for Split CIFAR-10, and 0.01 for Split CIFAR-100 and Split Tiny-ImageNet. To reduce the variability in experimental results, each reported result in the online CL setting is averaged over 10 repeated runs, and each result in the offline CL is averaged over 5 runs. More details about the baselines and implementations are listed in Appendix D."}, {"title": "B. Comparison on Disjoint Scenario", "content": "Dual-CBA can enhance current rehearsal-based methods. We first investigate the performance of the proposed method Dual-CBA across different datasets and various memory buffer sizes. Our proposed Dual-CBA can be easily plugged into multiple rehearsal continual learning baselines. To demonstrate it, we choose four commonly used rehearsal-based methods (i.e., ER, DER, RAR, and CLSER) as baselines in our experiments, and the results are summarized in Table I. Note that the suffixes \u2018CBA' and 'Dual-CBA' represent the class-specific CBA in conference version [16] and the proposed Dual-CBA in this paper applied to the corresponding baselines, respectively. It can be observed that: 1) Our Dual-C\u0412\u0410 can consistently improve the ACC of all four baselines and significantly reduce their FM across various settings. This indicates that our method can generalize to multiple rehearsal baselines and help them mitigate the task-recency bias by fitting the implicit posterior distribution during CL training. 2) Dual-CBA further shows significant improvements over the previously proposed CBA in [16] across all four baselines, which highlights the effectiveness of our proposed method.\nAnalysis of other baselines. LUCIR employs a weight normalization prior, and BiC designs an additional linear layer, with both methods aimed at addressing the distribution shift from the class imbalance perspective. However, these approaches oversimplify the issue of distribution shift, leading to suboptimal performance across these three benchmarks. iCaRL struggles with larger datasets such as Split CIFAR-100/Tiny-ImageNet, suggesting that the NCM classifier relies"}, {"title": "C. Comparison under Semi-Supervised Scenario", "content": "In semi-supervised continual learning, only a small portion of samples from each task is labeled with the proportion of these labeled data referred to as label ratio. We apply the FixMatch [76] to two rehearsal-based CL methods, namely ER and DER++. Similar to the fully supervised setting, our method Dual-CBA can also be easily applied to these baselines, and the comparison results across different datasets and various label ratios are shown in Table II. It can be observed that CBA only marginally improves the baselines in some cases, with overall performance showing modest gains. For example, when the label ratio is 0.2, ER-CBA even decreases the ACC and increases the FM of baseline ER across all three datasets. In contrast, our Dual-CBA consistently improves the performance of both baselines and surpasses CBA by a large margin, especially in terms of the FM metric. This is likely because the class-specific CBA requires more labeled data to adapt to changed posterior distribution for each new task, making it prone to overfitting in a semi-supervised CL setting with limited labeled samples. However, the strong transferability of the proposed class-agnostic CBA reduces the need for labeled data and enhances the performance of our Dual-CBA. Additionally, these results indicate that the proposed IBN is effective in a semi-supervised CL setting."}, {"title": "D. Comparison on Blurry Scenario", "content": "Following [43], [77], we adopt the Blurry-K online CL setting. It simulates a practical situation where task boundaries are unclear, characterized by an overlap across all tasks. Specifically, a fraction (K%) of the training data from one task may appear in other tasks. Here we take K = 10 and 30 as an illustration, and the comparison results on Split CIFAR-100 are summarized in Table III.\nIt can be observed that our Dual-CBA substantially improves the performance of the corresponding baselines and the original CBA module. For example, CBA only improves the ACC of ER by about 1.76% with 500 buffer samples under the Blurry-10 setting, while the proposed Dual-CBA can further enhance the performance of ER-CBA by about 2.91%. Additionally, our method significantly reduces the forgetting of the two baselines as shown in Table III. These results demonstrate that the proposed Dual-CBA can be flexibly applied to various rehearsal baseline models and help them adapt to the posterior distribution shift without being perturbed by unclear task boundaries, further verifying the effectiveness of the proposed improvements on CBA."}, {"title": "E. Comparison under the Offline CL", "content": "To further validate the generalization ability of the proposed Dual-CBA, we extend our method Dual-CBA to offline continual learning, where each task can be trained over multiple epochs to achieve more stable convergence. As shown in Table IV, all four baselines achieve better optimum solutions compared to the online context. Obviously, the proposed Dual-CBA significantly improves the performance of corresponding baselines and consistently outperforms the CBA module. These results verify the negative impact of the training bias on CL models and the strength of our method, which can also help the baseline models to adapt to the shifting posterior distribution in the offline setting, indicating the strong generalization ability of the proposed Dual-CBA."}, {"title": "F. Discussion and Ablation Study.", "content": "Dual-CBA helps the model adapt to distribution shifts. To ascertain this, we exhibit the accuracy of each task after the final task training, i.e., $a_{t,T}$ (where t = 1,...,T) in Table V. It can be observed that the baseline ER focuses on the new task too much and easily forgets the previously learned knowledge, that is the so-called task-recency bias. However, the proposed Dual-CBA significantly improves the accuracy of previous tasks and prevents the model from paying much more attention to the new task, indicating that our method can help the baseline model adapt to the dramatic distribution shift and absorb the training bias during the continual learning process. Additionally, the Dual-CBA module shows more powerful ability to mitigate task-recency bias than CBA across all datasets, further demonstrating the effectiveness of the proposed enhancement in Dual-CBA.\nDual-CBA meets the need for real-time evaluation in Online CL. In online continual learning, a challenge is that the model should be evaluated at any time throughout the CL process [28], [78]. To explore the effectiveness of our method for any time inference, we display the ACC of the baseline ER, ER-CBA, the proposed ER-Dual-CBA, and ER-Dual-C\u0412\u0410 without IBN in Fig. 2 and Fig. 6. It can be observed that especially on the larger datasets Split CIFAR-100 and Tiny-ImageNet, ER-CBA (black line) requires a few iterations to adapt the incoming data distribution and then improve the performance of ER (blue line) quickly. However, ER-Dual-CBA without IBN (red line) can surpass the baseline ER and ER-CBA at the beginning of each task and keep the higher performance during the entire CL process, which demonstrates that the proposed Dual-CBA can be evaluated in real-time and mitigate the stability-gap problem. Additionally, ER-Dual-CBA equipped with IBN (yellow line) further illustrates the effectiveness of the proposed IBN, which can significantly assimilate the feature extraction bias during inference. Furthermore, we also calculate ACCAUC in the last column of Table V. Our method can significantly improve the ACCAUC of baseline ER and ER-CBA across different datasets, also indicating that our method can improve the performance of baseline in real-time.\nIBN generalizes well to alleviate feature bias. To demonstrate the effectiveness of the proposed IBN, we compare it with the traditional batch normalization [18], and two recent normalization methods aiming to adjust the bias introduced by"}, {"title": "VI. CONCLUSION", "content": "In this paper, we address the challenges of online continual learning, where the classification network commonly exhibits dramatically changing posterior distributions, which yields performance bias toward the arriving new tasks. To tackle this, we propose a novel bi-level optimization framework that enables the classification network to adapt to these catastrophic distribution shifts. Specifically, we introduce a Dual-CBA module that includes a class-specific CBA module, a class-agnostic module, and an Incremental BN module. This combined module is capable of adapting to catastrophic distribution shifts and mitigating the stability gap issue arising from continual training. This ensures the classification network meets the requirements for real-time inference in online CL. The effectiveness of our method is demonstrated through both theoretical and empirical evidence. Theoretically, we provide an explanation of how our approach addresses task-recency bias and alleviates catastrophic forgetting, and we offer insights into the bi-level optimization framework through a linear formulation. Empirically, our algorithm consistently improves performance across various rehearsal-based baselines, showcasing its ability to assimilate training bias and effectively consolidate learned knowledge in online CL. Currently, our method is limited to being a plug-in for rehearsal-based methods. In the future, we will further investigate the applicability of our approach to rehearsal-free models."}, {"title": "APPENDIX", "content": "In this section, we first review the proposed bi-level learning framework to optimize the parameters of the classifier network and the Dual-Continual Bias Adaptor (Dual-CBA) module. Formally, the proposed bi-level optimization problem can be formulated as\n$\\min_{\\Phi} L_{buf} (B_{buf}; f_{\\theta(\\Phi)}),                                                                                    \\newline\\newline   s.t. \\ \\theta(\\Phi) = arg \\min_{\\theta} L_{trn} (B_{trn}; F_{\\theta,\\Phi}).                                                    (18)$\nWe use a nested gradient-optimization-based method to update the classifier network parameter $\\theta$ and the Dual-CBA parameter $\\phi$. Specifically, in the inner loop, the updating formulation of $\\theta$ at iteration step k can be expressed as\n$\\theta^{k+1}(\\phi) = \\theta^{k} \u2013 \u03b1 \u00b7 \\nabla_{\\theta} L_{trn} (B_{trn}; F_{\\theta^{k},\\phi^{k}}),                                      (19)$\nwhere $\\alpha > 0$ is the learning rate of the inner loop.\nThen we present details of the updating formulation of the Dual-CBA parameter $\\phi$ in the outer loop. The updating of $\\phi$ can be represented as\n$\\phi^{k+1} = \\phi^{k} \u2013 \u03b2 \u00b7 \\nabla_{\\phi} L_{buf} (B_{buf}; f_{\\theta^{k+1}(\\phi)}),                                        (20)$\nwhere $\\beta > 0$ is the learning rate. In Eq. (20), the derivation term can be represented as\n$\\nabla_{\\phi} L_{buf} (B_{buf}; f_{\\theta^{k+1}(\\phi)}) = \\frac{\\partial L_{buf} (B_{buf}; f_{\\theta^{k+1}(\\phi)})}{\\partial \\theta^{k+1}(\\phi)} \\frac{\\partial \\theta^{k+1}(\\phi)}{\\partial \\phi}   = -\u03b1 \\frac{\\partial L_{buf} (B_{buf}; f_{\\theta^{k+1}(\\phi)})}{\\partial \\theta^{k+1}(\\phi)} \\frac{\\partial \\mathcal{L}_{trn} (B_{trn}; F_{\\theta^k ,\\phi^k})}{\\partial \\phi},                                          (21)$\nObviously, the update of $\\phi$ introduces a second-order derivation that can be easily implemented by PyTorch [66]. To alleviate the calculation burden of this second-order derivation in Eq. (21), we assume that this derivation only depends on the parameter of the last linear classification layer rather than the whole classifier network like ResNet-18. The linear classification layer only introduces a small number of parameters which can significantly speed up the computation and make our bi-level optimization efficient and suitable for online CL.\nProof: The bi-level learning framework is represented as Eq. (18). If the inner optimization problem is approximated by"}, {"title": "C. Implementation Details of Dual-CBA", "content": "This section provides implementation details of the proposed Dual-CBA module. We use two-layer MLP as the class-specific CBA and the class-agnostic CBA. As we aforementioned in Sec. III-C, the class-specific CBA in continual learning may aggregate the stability gap, we reinitialize the class-specific CBA modules to avoid incorrect adjustments when a new task arrives, and then we train it together with the class-agnostic CBA.\nIn practice, to further simplify the optimization of Dual-CBA, we introduce a skip-layer connection between the outputs of classification network $f_\\theta$ and the outputs of Dual-CBA $g_\\phi$, i.e., the adapted posterior distribution $\\tilde{y} = \\frac{(g_{\\phi}(\\hat{y}) + \\hat{y})}{Z}$ where Z is a normalization constant. As verified in previous works [74], the skip-layer connection can aid the model convergence and facilitate the gradient backward propagation.\nOptional: In our experiments, we empirically found that performance can be further improved by using two individual class-specific CBA modules: one for the posterior probabilities of the classes from the new task and another for the classes from all old tasks. This approach is more effective in avoiding the interference between the new and old tasks than using a single class-specific CBA for the posterior probabilities of all new and old classes. To demonstrate this, we conduct ablation studies in Table X.\nIn Table X, we compare the effectiveness of each component of the Dual-CBA module, including the two individual class-specific CBA, a single class-specific CBA, and the proposed class-agnostic CBA. ER-Dual-CBA-1 shows the effectiveness of the proposed class-agnostic CBA, which can significantly assimilate the training bias in CL and achieve comparable results to ER-Dual-CBA. Additionally, we set two individual class-specific CBA modules to adapt to the posterior probability of the old and new classes, respectively. Comparing ER-Dual-CBA-2 and 3 in Table X, the performance of two individual class-specific CBA modules is significantly higher than that of only using one class-specific CBA for all classes, indicating that the individual class-specific CBA can adjust the posterior probability more accurate and assimilate the negative influence between new and old tasks. Finally, combining individual class-specific CBA and class-agnostic CBA, our ER-Dual-CBA achieves the best performance and the results demonstrate the effectiveness of each component of our method."}, {"title": "D. Details of Experiments", "content": "In this section, we detail the specific hyper-parameters setting of different methods and datasets in our experiment.\nTo further demonstrate the effectiveness of our proposed Dual-CBA module, we illustrate the old-task posterior probabilities $\\hat{y}_{"}]}