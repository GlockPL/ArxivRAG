{"title": "APEX2: Adaptive and Extreme Summarization for Personalized Knowledge Graphs", "authors": ["Zihao Li", "Dongqi Fu", "Mengting Ai", "Jingrui He"], "abstract": "Knowledge graphs (KGs), which store an extensive number of relational facts, serve various applications. Recently, personalized knowledge graphs (PKGs) have emerged as a solution to optimize storage costs by customizing their content to align with users' specific interests within particular domains. In the real world, on one hand, user queries and their underlying interests are inherently evolving, requiring PKGs to adapt continuously; on the other hand, the summarization is constantly expected to be as small as possible in terms of storage cost. However, the existing PKG summarization methods implicitly assume that the user's interests are constant and do not shift. Furthermore, when the size constraint of PKG is extremely small, the existing methods cannot distinguish which facts are more of immediate interest and guarantee the utility of the summarized PKG. To address these limitations, we propose APEX2, a highly scalable PKG summarization framework designed with robust theoretical guarantees to excel in adaptive summarization tasks with extremely small size constraints. To be specific, after constructing an initial PKG, APEX\u00b2 continuously tracks the interest shift and adjusts the previous summary. We evaluate APEX2 under an evolving query setting on benchmark KGs containing up to 12 million triples, summarizing with compression ratios \u2264 0.1%. The experiments show that APEX outperforms state-of-the-art baselines in terms of both query-answering accuracy and efficiency.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) have been proven an effective tool for constructing solutions in many application domains, such as healthcare, finance, cyber security, education, question answering, and social network analysis [11, 23, 33, 38-41, 53, 78]. Due to the ever-growing amount of data, encyclopedic knowledge graphs are becoming increasingly large and complex [12, 13], such as DBpedia [2], Freebase [5], Wikidata [62], and YAGO [57]. In contrast, KG users (e.g., individual people, systems, software packages) usually do not have very general interests but only care about a small portion of the whole KG for certain topics. Therefore, personalized knowledge graphs (PKGs) have recently attracted much research attention for balancing storage cost and query-answering accuracy [27, 55, 61]. In brief, a personalized knowledge graph (PKG) is extracted (summarized, compressed, or distilled) from a larger comprehensive knowledge graph. For KG and an individual user, a PKG has a limited size, but contains many entities/triples in the KG that the user is interested in, and can answer their personal queries appropriately. Furthermore, on the application side, each user will have a PKG. Since there might be many individual users, each PKG is expected to store as few facts as possible, to minimize the total storage cost. As the individual KG's size has been very large, re-summarizing PKG from scratch requires many computational resources, but in the real world, users' personal query interests may shift over time. As shown in Figure 1, in the morning (9am), the user works as a programmer and wonders about software engineering. During the afternoon coffee break (3pm), the same user cares about the UEFA Champions League (soccer games). In the evening (8pm), the same user likes painting and is curious about art. Under these circumstances, on one hand, an outdated summarized PKG might be sub-optimal, since outdated information agglomerates and adversely affects both storage cost and search performance [12]. On the other hand, re-summarizing PKG from scratch at each timestamp causes unaffordable computational complexity. Given the fact that the KG is massively large [2, 5, 57, 62], to make the PKG acceptably small, the compression ratio has to be extremely small. For example, the entire YAGO3 is about 300GB. Even with a 1% compression ratio, the PKG is 3GB, which is still larger than most mobile applications. For the KGs measured in TB, more extreme compression is needed.\nMotivated by the above use case, the previous work [12] informally introduced the problem of adaptive PKG summarization, which seeks to find a compact summary given the knowledge graph and query history. Take Figure 1 as an example. We expect the adaptive PKG could, during the afternoon coffee break, quickly adapt from software engineering to more sports-related topics, decaying but not eliminating topics on software engineering. Then, at night, the PKG evolves with the user's interests in art topics. Moreover, in this paper, we study how to adaptively summarize the PKG under extremely small storage constraints. Theoretically, in Appendix B, we show that the existing PKG summarization methods [27, 55], even if re-run from scratch for the new interested topics, could not incorporate the new interests into the previously summarized PKG when query topics change, or corrupt under extremely small storage constraints.\nTo address these limitations, we propose APEX2 (Adaptive and Extreme Summarization for Personalized Knowledge Graphs), which enables summarization to incrementally evolve with user interests over time while satisfying extremely small storage constraints. To the best of our knowledge, this work presents the first adaptive PKG summarization framework tailored for evolving query topics. In brief, given a bunch of queries with different interested topics at different timestamps, APEX2 works by modeling user interests through a heat diffusion process [9] and maintaining dynamic data structures that allow incremental updates. Under the extremely small storage limitation, APEX2 incrementally infers the interest scores of the facts and picks the ones with the highest scores (i.e., immediately more interested by the user). Our contributions are summarized as:\n\u2022 Problem Formulation and Theoretical Analysis. We formally formulate the problem of adaptive PKG summarization with storage limitation. We provide theoretical analysis of the adaptability of existing PKG summarization methods. We also prove the efficiency and topic adaptability of our methods.\n\u2022 Algorithm. We propose the adaptive and extreme PKG summarization solution APEX2 and its variant APEX2-N to address different circumstances with theoretical guarantees.\n\u2022 Experiment Evaluation. We design extensive experiments under real-world query-answering scenarios on real KGs to show the effectiveness and efficiency of our proposed methods."}, {"title": "2 Problem Definition", "content": "We use calligraphic letters (e.g., A) for sets, bold capital letters for matrices (e.g., A), parenthesized superscript to denote the temporal index (e.g., A(t)), unparenthesized superscript to denote the power (e.g., Ak). For matrix indices, we use Ai,j to denote the entry in the ith row and the jth column. The notation used in our proposed APEX is summarized in Table 1.\nKnowledge Graph. A knowledge graph G = (E, R, T) is defined by an entity set &, a relation set R and a triple set T. A triple xijk = (ei, rk, ej) \u2208 T is defined by entities ei, ej and their relationship rk. The undirected adjacency matrix A is defined as\n$A_{i,j} = 1 \\Leftrightarrow i \\ne j \\land \\exists k \\text{ s.t. } x_{ijk} \\text{ or } x_{jik} \\in T$ (1)\nPersonalized Knowledge Graph. A PKG P of KG G is defined by an entity set Ep \u2286 &, a relation set Rp \u2286 R and a triple set Tp \u2286 T. In our problem setting, a PKG is summarized from a KG according to the user's query log Q.\nQuery Log. Since most real queries to KGs consist of only one or two triples [6], assuming we have the full access to the whole KG, we study simple queries with known answers. A query log Q consists of a number of queries. Each query q consists of a query entity e, query relation r, and a set of answer entities A. A triple xijk = (ei, rk, ej) in the knowledge graph G is said to be an answer triple to a query q, if e\u00a1 = e \u2227 rk = r \u2227 ej \u2208 A. A query may have multiple answer triples. For example, for the query \"What movie did Christopher Nolan direct\", the query entity is \"Christopher Nolan\" (\"Nolan\" for short), and the query relation is"}, {"title": "3 Adaptive PKG Summarization", "content": "In the adaptive PKG summarization scenario, compared to previous static PKG summarization [27, 55, 61], the major difference is that the user's interest may be shifting, manifested by the user's evolving query history. The adapting may be simply achieved by re-applying summarization methods from scratch every time the user's query log has evolved. But such a from-scratch solution lacks real-time efficiency. A natural follow-up is: can we reuse the results from the previous summarization and further develop a real-time framework that can evolve incrementally with the user's interest? Moreover, when the storage constraint is extremely small, can we incrementally maintain a descending-order rank of the user's interests in each entity/relation/triple? To address these questions, we propose our solution APEX2.\nThe core idea of APEX2 is to maintain a real-time sparse heat structure that stores the user's interest and incrementally updates its content, then greedily chooses the triples with the highest heat to construct the summarization, even under extremely small storage limitations. To adapt to the user's interests, we introduce a decay factor y into our framework. This factor is crucial to both adapting effectiveness and efficiency: (i) decaying previous interests gives higher priority to recent queries, which are more likely to represent the user's current interest; (ii) for a set of elements, decaying all of them does not affect their order and will not introduce additional computations to the heat ranking process. y controls the trade-off between adapting to new interests and retaining relevant information from past queries.\nSystematically, our APEX2 consists of three components for adaptive PKG summarization, i.e., Dynamic Model of User Interests, Incremental Updating, and Incremental Sorting. In brief, Dynamic Model of User Interests is proposed to model the user interest dynamically. Then based on the evolving interests modeled, Incremental Updating tracks the new interests. Incremental Sorting is necessary to construct a high-quality newly summarized PKG, under an extremely small storage constraint. Details of the three components are introduced through Subsections 3.1 - 3.3. Our end-to-end APEX2 is summarized in Algorithm 4."}, {"title": "3.1 Dynamic Model of User Interests", "content": "In order to formulate more conveniently, we start by freezing at a specific timestamp T (T as a constant), and define $q_{total}$ to store the number of times each entity is accessed as query entity or answer entity in the query log. If accessed as the answer to a query, the marginal value will be weighted by # of answers We provide a simple example here. Suppose we have 5 entities (indexed 0, 1, 2, 3, 4). The first query is (entity 0, some relation, {entity 1, entity 3}), the second query is (entity 2, some relation, {entity 0, entity 3}), then Itotal will be a column vector (1+0.5, 0.5+0,0+1, 0.5+0.5, 0 + 0) = (1.5, 0.5, 1, 1, 0). Assuming the user's temporal query log is Q(t), we use $Q^{(t)} \\backslash Q^{(t-1)}$ to denote new queries arriving at time t. Additionally, Q-1 = \u2205. Then,\n$I^{(T)}_{total} = \\sum_{t=0}^{T} \\gamma^{T-t} q^{(t)} = \\sum_{t=0}^{T} \\gamma^{T-t} \\sum_{i \\in Q^{(t)}\\\\Q^{(t-1)}} q_i$ (3)\nwhere for each query i \u2208 $Q^{(t)} \\backslash Q^{(t-1)}$ with answer set Ai, qi is a vector with dimension (|8|\u00d71) whose entries are\n$q_i[e] = \\begin{cases} 1 & \\text{e is the query entity of query i} \\\\ \\frac{1}{|A_i|} & \\text{e is an answer entity to query i} \\\\ 0 & \\text{otherwise (e is unrelated to query i)} \\end{cases}$ (4)\nwhere e \u2208 {1, ..., |8|} is the index of entities.\nWe model user's interest on an entity e in a heat-diffusing style. Define Ni(e) to be the l-hop neighbors of entity e. Additionally N0 (e) = e. A topic is a sub-area in the KG that has implicit inner connections. Such connections carry semantic meanings for humans (e.g., artistic, physical entities) and are modeled topologically by entities\u00b9. With the straightforward inspiration that entities near the searched ones are likely to be in the user's interested topics, we model the user's static preference for entities as\n$Pr(e|Q) = \\sum_{l=0}^{d} \\alpha^l \\sum_{e_0 \\in N_l(e)} \\sum_{q \\in Q^{(T)}} 1(e_0 \\in q)$ (5)"}, {"title": "3.2 Incremental Updating", "content": "An advantage of our model is that the user's preference on entities and relations at time T can be incrementally updated from the previous timestamp T \u2013 1. Denoting $q^{(T)} = \\sum_{i \\in Q^{(T)}\\\\Q^{(T-1)}} q_i$, We derive the incremental updating equations for $I^{(T)}_{total}$, $e^{(T)}$, and $r^{(T)}$ as follows.\n$I^{(T)}_{total} = \\sum_{t=0}^{T} \\gamma^{T-t} q^{(t)} = \\sum_{t=0}^{T-1} \\gamma^{T-t} q^{(t)} + q^{(T)}$ (16)\n$e^{(T)} = \\sum_{l=0}^{d} \\alpha^l A^l q^{(T)}_{total} = \\sum_{l=0}^{d} \\alpha^l A^l (\\gamma I^{(T-1)}_{total} + q^{(T)})$ (17)\n$r^{(T)} = \\sum_{t=0}^{T} \\gamma^{T-t} \\tilde{q}^{(t)} = \\sum_{t=0}^{T-1} \\gamma^{T-t} \\tilde{q}^{(t)} + \\tilde{q}^{(T)} = \\gamma r^{(T-1)} + \\tilde{q}^{(T)}$ (18)\nTo model the user's interest in triples, we define H as a sparse 3-dimensional temporal array, implemented using a dictionary,\n$H^{(T)}[i][j][k] = e^{(T)}[i] r^{(T)}[j] e^{(T)}[k]$ (19)\nThe updating of H(T) from H(T\u22121) is per-entry conditional. Assume at timestamp T, compared to timestamp T \u2013 1, if entries i,k in e(T) and entry j in r(T) are not updated (excluding decay), then $H^{(T)}[i][j][k] = \\gamma^3 H^{(T-1)}[i][j][k]$. Only entries in H(T) [i][j][k] with any of those being updated need to be recalculated. The number of updated entries is small as we assign d much less than the diameter of G. Therefore, for each timestamp, we multiply H by y\u00b3 and then do a small-scale update."}, {"title": "3.3 Incremental Sorting", "content": "After the user's preference is updated in real-time, we can directly sort the triples by their preferences and then pick the ones with the highest heat until the size budget is reached. However, the sorting algorithms usually cost O(n log2 n) complexity [25].\nFor every new timestamp, excluding decay (which does not change the order), only part of the triples' heat will be updated and recalculated. This indicates that it is possible to reuse the previous order and accelerate the process to get the up-to-date order. For the following problem, we propose an intuitive solution named incremental binary insertion sort, as shown in Algorithm 3."}, {"title": "4 Algorithms", "content": "Our APEX2 combines user preference modeling, incremental heat updating and incremental sorting as both solutions and optimizations. As shown in Algorithm 4, in Steps 1-7, APEX2 first initializes the data structures for both heat updating and incremental sorting, then performs pre-computing. Then, in Steps 8-14, for each later timestamp a user inputs new queries, APEX2 performs macroscopic decaying, incrementally updating and necessary recalculating. After that, the heat of triples gets incrementally sorted and a new PKG is constructed by picking the ones with the highest heat.\nThe adaptability of APEX2 is ensured by the decaying operations, and we prove the effectiveness of APEX2 in Theorem 4.1. As for efficiency, intuitively, though the whole KG is available, APEX2 only accesses the entities, relations and triples of the user's interest. Moreover, when the heat of a triple is decayed to a small enough value, APEX2 would switch it to zero and such an out-of-interest triple does not require any further computational resource. These facts show that APEX\u00b2 is highly scalable for large databases. We prove that the incremental time complexity of APEX2 is unrelated to the size of KG in Theorem 4.2. Here, incremental time complexity means \u201ctime complexity per adapting phase\u201d, which is the total time in the adapting phase amortized by the number of for-loop iterations in Steps 8-14. In the following theorems, connectivity of an area V = (Ev, Ru, To) is defined as $c_V = \\frac{\\sum_{v \\in E_v} degree(v)}{|E_v|}$\nTHEOREM 4.1 (EFFECTIVENESS OF APEX2). Assume two areas (topics) U and V with connectivity cu and co are sub-KGs of G, and the user initially queries U for a times and starts to query V, then APEX2 takes $\\log_\\gamma \\frac{1}{\\frac{1}{(1-\\gamma^a)}+1}$ queries to adapt from U to V, where A = $\\frac{E_u+2 T_u}{E_u}$ and B = $\\frac{E_v+2 T_v}{E_v}$. (Proof in Appendix E.6)\nFor each query q = (e, r, A), we can decompose it into a set of sub-queries $q_{sub} = \\{(e, r, \\{a\\}) \\forall a \\in A\\}$. Similarly, we can decompose a query log Q into a sub-query log. Theorem 4.2 shows the time complexity of APEX2 is only related to the connectivity of KG and the number of sub-queries the user performed.\nTHEOREM 4.2 (TIME COMPLEXITY OF APEX2). The incremental time complexity for APEX\u00b2 's adapting phase is $O(c \\cdot |Q|^2 \\cdot \\log_2(c \\cdot |Q|))$, where Q is the query log decomposed into sub-queries (each query in Q has only one query entity, one query relation and one answer). c = $\\frac{nnz((\\sum_{l=0}^{d} (\\alpha A)^l))}{E}$, where nnz is the operator outputting the number of non-zero elements in a matrix, A \u2208 $R^{n \\times n}$ is the adjacency matrix of G, & is the set of entities of G. (Proof in Appendix E.2)\nIf set a threshold Eths to eliminate small-enough entries to 0, then after logy\u20acths timestamps, entries introduced by previous queries will be decayed to 0. In this case, the effective number of queries |Q| above can be bounded by a constant $\\log_\\gamma \\epsilon_{ths}$. By this operation, the incremental time complexity is further optimized to $O(c \\log_2(c))$, where c = $\\frac{nnz(\\sum_{l=0}^{d} (\\alpha A)^l)}{E}$ is the average number of neighbors within d-hops. In other words, the time complexity of APEX2 updating is only related to the connectivity of KG."}, {"title": "4.2 APEX2 Variant: APEX2-N", "content": "In APEX2, we model the user's interest in triples in Eq. 14, where we assign equal weights to entities and relations. However, the user may put more attention on entities than relations when performing queries. For example, a user who searched \"what pieces of music did Taylor Swift create\" might be more likely to search \"what's Taylor Swift's music style\" than \"what pieces of music did Bill Evens create\" later on. In this case, we propose APEX2-N, a variant of APEX2 that gives higher weights to entities than relations.\nAPEX2-N only incrementally tracks and sorts the heat of entities but not for relations. In other words, APEX2-N gives weight 1 to entities and 0 to relations. APEX2-N is designed mainly for adaptive solutions of PKG summarization, and we leave the trade-off between weights on entities and relations to future work. Since APEX2-N is a variation of APEX2, we summarize the detailed operations of APEX2-N in Algorithm 5 in the Appendix. We also give proof of the effectiveness and efficiency of APEX2-N as follows.\nTHEOREM 4.3 (EFFECTIVENESS OF APEX-N). Assume two areas (topics) U and V with connectivity cu and co are sub-KGs of G. If a user initially queries U for a times and starts to query V, then APEX2-N takes $\\log_\\gamma \\frac{1}{\\frac{1}{(1-\\gamma^a)}+1}$ queries to adapt from U to V, where A = $\\frac{1-(\\alpha c_u)^{d+1}}{1-\\alpha c_u}$ and B = $\\frac{1-(\\alpha c_v)^{d+1}}{1-\\alpha c_v}$. (Proof in Appendix E.7)\nTHEOREM 4.4 (TIME COMPLEXITY OF APEX2-N). The incremental time complexity for APEX2-N's adapting phase is $O(c \\log_2(c|Q|))$, where Q is the query log decomposed into sub-queries (each query in Q has only one query entity, one query relation and one answer). And c = $\\frac{nnz(\\sum_{l=0}^{d} (\\alpha A)^l)}{E}$, where A \u2208 $R^{n \\times n}$ is the adjacency matrix of G, & is the set of entity of G, and nnz is the operator outputting the number of non-zero elements in a matrix. (Proof in Appendix E.3)\nLike APEX2, the time complexity of APEX2-N can also be optimized to O(clog2 c) by setting a threshold value. In the future, Both APEX2 and APEX2-N may be extended to the fully dynamic setting, where the KG itself can evolve. New entities can be reserved as dummy nodes. When a new entity, relation, or triple is added, the initial heat is zero, therefore we only need to update the adjacency matrix and start tracking their heat from the next timestamp. When a triple is deleted, we clear its heat to zero. When an entity or relation is removed, it means there is no triple with that entity, and we can safely clear its heat to zero."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Setup", "content": "5.1.1 Datasets. We use YAGO 3 [57], DBPedia 3.5.1 [2], MetaQA [69] and Freebase [5] as knowledge graph datasets in our experiments. The basic information of knowledge graphs is summarized in Table 2. For YAGO, DBPedia and Freebase, we use synthetic queries that follow the logic and structure of Linked SPARQL Queries' DB- pedia data dump\u00b2. The same format has been used in previous work [55]. For MetaQA, we use the queries provided in the dataset. More details about the datasets can be found in Appendix C.2, and we validate the high quality of our synthetic queries in Appendix C.3."}, {"title": "5.1.2 Baselines", "content": "We choose several state-of-the-art methods as the baselines. We compare sampling-based knowledge graph summarization algorithm (GLIMPSE [55]), merging-based graph summarization algorithm (PEGASUS [27]), workload-based knowledge graph summarization algorithm (iSummary [61]), random walk with restart on knowledge graph (Personalized PageRank [34]), together with our APEX2 and APEX2-N. Details of GLIMPSE, PE- GASUS and iSummary are provided in section B. The PPR baseline calculates the PageRank vector personalized to qtotal and constructs the summarization by continuously adding the most relevant entity."}, {"title": "5.1.3 Re-summarization Interval", "content": "By design, the baselines cannot take temporal query logs as inputs. To enable the baselines to handle adaptive PKG summarization problem, we let them output new PKGs after a certain amount R of timestamps. The choice of R affects the performances of baseline methods. If R is small (i.e., R = 1 means re-summarize every timestamp), then the re-summarization happens frequently, and the baselines will become very slow. If R is large, then the baseline summaries are outdated for most timestamps. To pick a good R for fair comparisons, we conduct pre-experiments in Appendix C.5 and find that R = 9 is a good effectiveness-efficiency trade-off for baselines.\nIn our design, APEX2 and APEX2-N can also take multiple queries at one time by masking the summary updating phase (line 13-14 in Algorithm 4 and line 13-17 in Algorithm 5) for non- summary timestamps. We use RAPEX to denote that they update the PKG every RAPEX timestamp. By default, RAPEX = 1, and we provide a comprehensive study on RAPEX in Section 5.6."}, {"title": "5.1.4 Metrics", "content": "Same as previous research works [8, 55, 58], we use F1 score [20] on the very next query as the metric for searching effectiveness. More details of this can be found in Appendix C.1."}, {"title": "5.2 Experimental Settings", "content": "We show the outperformance of APEX\u00b2 and APEX2-N through autoregressive\u00b3 style experiments. We set the default hyperparameters \u03b3 = 0.5, \u03b1 = 0.3, d = 1 and PageRank restart probability to be 0.85. We set the compression ratio to be 0.000001 = 0.0001% (one in a million) for YAGO and DBPedia, 0.0001 = 0.01% (one in ten thousand) for MetaQA, 0.0005 = 0.05% for Freebase.\nGenerate user queries. To calculate the average and standard deviation, we simulate 10 users to query the KGs. Following the norm set by Freebase discussed in section 3.1, we model the abstract concept of topic by \"queries with the same query entity\"4. To simulate a real-world querying scenario with interest shift, for each KG, we generate 200 queries on 20 topics for each user. Each group of 10 consecutive queries are in the same topic. We associate each query t with timestamp t \u2013 1. For MetaQA, we first categorize the provided queries into different topics by query entity, then ran- domly sample 20 distinct query entities. After that, we randomly choose 10 queries on each of the 20 topics. For DBPedia, YAGO and Freebase, we synthetically generate queries by randomly choosing 20 query entities in the KG. Then for each query, we randomly choose 10 relations (with possible multiplicity) that the query en- tity has. Finally, we include all entities e satisfying (query entity, chosen relation, e) \u2208 T into the answer set. We study 1-hop simple queries with known answers because in real life there is only a small portion of complex queries [6], which can be decomposed into simple queries.\nQuery Answering Evaluation. After loading KG and the queries of a user, we adaptively summarize the KG. (i) For GLIMPSE, PEGA- SUS, iSummary, PageRank, we construct an initial summary using the first query, then re-summarize after each R = 9 timestamps (i.e., queries). (ii) For APEX2 and APEX2-N, they both evolve every timestamp whenever the user performs a new query. We calculate the F1 score of the very next query after re-summarization for all the methods. For example, if at timestamp t the summarization method performs re-summarization or evolving using query t + 1, then at timestamp t + 1 we search the query t + 2 in the new PKG and calculate the F1 score."}, {"title": "5.3 Comparisons", "content": "Effectiveness Comparison. We measure the F1 score of each PKG summarization method in the auto-regressive querying scenario for 10 users. We report the mean and standard deviation of sampled timestamps in Figure 2. First, both APEX2 and APEX2-N outperform the existing baseline methods on the F1 score in all cases. Second, APEX2 and APEX2-N remain highly effective even if the knowledge graph is large and the compression rate is extremely small, showing the scalability of our methods. Third, APEX2-N outperforms all methods, showing the necessity of considering users' attention more on entities than relations. The baseline PEGASUS performs worst, possibly because this method is not designed specifically for knowledge graphs. This provides evidence that traditional graph algorithms should be reconsidered before being applied to the KG domain. In later comparisons, we only compare with GLIMPSE and PageRank baselines because they have acceptable sub-optimal performances.\nEfficiency Comparison. We measure the time consumption of PKG summarization methods to produce one summary under the R = RAPEX = 1 setting. We report the mean and standard deviation in Table 3. PEGASUS and iSummary are not coded in Python. We add an extremely strong baseline in terms of efficiency: the parallel implementation of PageRank \"ParallelPR\" with walk-length 1 and record its execution time, which is almost the optimal time cost that any diffusion-based algorithm can achieve."}, {"title": "5.4 Ablation Study", "content": "5.4.1 Decay Ablation. In our design, decaying (i.e., a forgetting mechanism), is the key to the adaptive and extreme summarization. Here, we compare the effectiveness under different levels of decaying to further show the importance of decaying. We run the user querying scenario in MetaQA with varying y values. Other hyperparameters are set to be the same as Section 5.2. We use the same query generated for MetaQA and report both APEX2 and APEX2-N's average F1 score of 10 users in Figure 3. Larger y means lower decay level (less extent of decay). When y = 1, the decaying is completely eliminated, and the PKG is full of outdated interests, resulting in low F1 scores of both APEX2 and APEX2-N. In general, starting from y = 0.5, the effectiveness does not change much until y = 0.9, and then the performance gets worse massively from y = 0.9 to y = 1. It turns out that when the storage space is very limited, decaying the previous interests can pave the way for personalized summarization.\n5.4.2 Component Ablation. Aiming to accelerate the computation, after the necessary dynamic modeling of user interests, we designed incremental updating and incremental sorting. These two components serve to accelerate the computation and do not affect the computational results; therefore, to show that all three components of the APEX22 framework contribute to the overall efficiency performance, we can compare the mean execution time in seconds shown as follows."}, {"title": "5.5 Hyperparameter Study", "content": "We study the parameter sensitivity to show our models' robustness using MetaQA dataset. From the result in Figure 4, larger compression ratio leads to better F1 score, but after a certain value F1 score does not change much. This is intuitive as the searching accuracy increases with more triples stored in the PKG. In general, our methods are robust with damping factor and diffusing parameter. A larger diffusing diameter takes more time because more items get non-zero heat. Time per adapting phase increases quadratically with diffusing parameter, because the interested area grows with d."}, {"title": "5.6 Handling Multiple Queries at One Time", "content": "As mentioned in Section 5.1.3, APEX2 and APEX2-N can re-summarize the KG every RAPEX timestamp to increase overall efficiency. We conduct comprehensive experimental analysis on varying RAPEX and report the results in Appendix C.6 due to page limitation. In short, on the effectiveness side, our methods, with varying RAPEX of 2, 3, 6, achieved competitive query accuracy (i.e., F1 score) and still outperformed baseline methods. On the efficiency side, the time consumed by the varying RAPEX methods is similar to that of R = 1. These results suggest that our heat tracking method has a robust performance over timestamps."}, {"title": "5.7 Case Study", "content": "We provide a comparative case study to illustrate how our methods work and that they can summarize high-quality PKGs.\n5.7.1 Dataset and Query. We do a case study on MetaQA dataset using the queries of user 0 to show in which way our methods are able to adaptively summarize highly interested items into PKG. We use the first 33 queries from the user. Most of the interested part of the KG is shown in Figure 11. In the first 3 groups of 10 queries", "Stevan Riley\", the movie \"The Disappearance of Haruhi Suzumiya\" and the movie \"LOL\".\n5.7.2 Settings. We aim to study how APEX2, APEX2-N, GLIMPSE and PageRank deal with topic shift. Starting from the 31st query, the user asks (Chad Michael Murray, movie_to_actor, ?), i.e., \"which movies did Chad Michael Murray act in\" three times. The correct answer entities are \"A Cinderella Story\", \"House of Wax\", and \"Left Behind\". For APEX2 and APEX2-N, we use the same setting as in the main experiments": "both of them evolve every timestamp from the beginning. For GLIMPSE and PageRank, we give them higher privileges that they can re-summarize each new timestamp, compared to per 9 timestamps in the main experiments. We use the same hyperparameters as the main experiments.\n5.7.3 Results. The PKG results are shown in Figure 12, 13, 14, 15. The summarized PKG at timestamp t 1 have been fed query t. (i) The sub-figure (a) (i.e., the PKGs after the first three groups of queries) of GLIMPSE and PageRank still contains all information about the out-of-interest topic \"Blue Blood\" and \"Stevan Riley\", which were accessed in the first 10 queries. (ii) For both APEX2 and APEX2-N, after the first query on the new topic, the three answer triples (Chad Michael Murray, movie_to_actor, A Cinderella Story), (Chad Michael Murray, movie_to_actor, House of Wax), and (Chad Michael Murray"}]}