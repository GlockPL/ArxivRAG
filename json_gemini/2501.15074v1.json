{"title": "PATENTLMM: Large Multimodal Model for Generating Descriptions for Patent Figures", "authors": ["Shreya Shukla", "Nakul Sharma", "Manish Gupta", "Anand Mishra"], "abstract": "Writing comprehensive and accurate descriptions of technical drawings in patent documents is crucial to effective knowledge sharing and enabling the replication and protection of intellectual property. However, automation of this task has been largely overlooked by the research community. To this end, we introduce PATENTDESC-355K, a novel large-scale dataset containing ~355K patent figures along with their brief and detailed textual descriptions extracted from 60K+ US patent documents. In addition, we propose PATENTLMM \u2013 a novel multimodal large language model specifically tailored to generate high-quality descriptions of patent figures. Our proposed PATENTLMM comprises two key components: (i) PATENTMME, a specialized multimodal vision encoder that captures the unique structural elements of patent figures, and (ii) PATENTLLAMA, a domain-adapted version of LLaMA fine-tuned on a large collection of patents. Extensive experiments demonstrate that training a vision encoder specifically designed for patent figures significantly boosts the performance, generating coherent descriptions compared to fine-tuning similar-sized off-the-shelf multimodal models. PATENTDESC-355K and PATENTLMM pave the way for automating the understanding of patent figures, enabling efficient knowledge sharing and faster drafting of patent documents. We make the code and data publicly available\u00b9.", "sections": [{"title": "1 Introduction", "content": "Patents are a cornerstone of intellectual property protection, granting inventors exclusive rights to their creations. Effective communication of these inventions is crucial for patent examiners, courts, and the technical community to appreciate the inventiveness of these inventions and assess their novelty. Patent documents rely heavily on figures and their corresponding textual descriptions to present technical details. Writing accurate descriptions of these figures is essential for an unambiguous understanding of the invention and its components and facilitates knowledge sharing within the technical community. Comprehensive descriptions also ensure that the invention is adequately protected against potential infringements by others. However, manually crafting such descriptions is time-consuming and laborious, hindering the efficiency of patent processing and analysis.\nOne of the major challenges for generating patent figure descriptions in an automated way is the lack of large-scale labeled datasets. Existing datasets, while invaluable for advancing research in natural and scientific figure captioning, do not adequately capture the nuances and complexities inherent to patent illustrations. To address this gap, we curate PATENTDESC-355K, a novel large-scale dataset containing ~355K patent figures and their brief and detailed textual descriptions extracted from 60K+ patent documents. This dataset offers a rich and diverse collection of patent figures that span various technical domains, along with their corresponding descriptions, enabling the development and evaluation of models specifically tailored for this task.\nTypically, patent figures are associated with brief and detailed descriptions. In our proposed PATENTDESC-355K dataset, we found that they span an average of ~34 and ~1680 tokens, respectively. Thus, unlike existing image captioning benchmarks, for example COCO (Lin et al. 2014), TextCaps (Sidorov et al. 2020) and NoCaps (Agrawal et al. 2019) where captions span an average of ~12 tokens, the descriptive captioning of patent figures in our dataset is much more challenging. Moreover, unlike the natural scene images of the existing captioning datasets, patent figures are structured technical illustrations that adhere to a more standardized visual style for technical and legal documentation.\nThe emergence of Large Language Models (LLMs) and Large Multimodal Models (LMMs) has revolutionized almost every vision and language task. These models exhibit a remarkable ability to understand and generate coherent language across diverse domains. However, applying these models to the generation of patent descriptions presents unique challenges. The length of descriptions and the complexity inherent to patent diagrams underscore the need to focus on various elements of the figure, such as arrows, nodes, and text annotations. Further, contrary to dense document images, patent figures are sparse and comprise several elements like text, nodes, node labels (a number associated with nodes in the patent figure), figure numbers, and arrows in different styles, i.e., uni-direction and bidirectional, solid, and dotted, among others."}, {"title": "2 Related Work", "content": "Image Captioning in Pre-LMMs era: The patent figure description task is broadly similar to the image caption-ing task, which has been an active research area in the last decade. Some representative early work on image captioning includes the combination of a CNN encoder with an LSTM decoder (Vinyals et al. 2015), a multimodal RNN architec-ture that uses local and global image features (Andreas et al. 2016), an adaptive attention model (Lu et al. 2017), and a bottom-up and top-down attention model (Anderson et al. 2018). Recent works have also focused on improving cap-tion diversity (Shetty, Roumeliotis, and Laaksonen 2017), novel object captioning (Lu et al. 2018), and incorporating external knowledge (Gu et al. 2019). As discussed in the pre-vious section, our task differs significantly from these previ-ous efforts on image captioning in terms of the length of descriptions and the structure of patent figures.\nDescribing Scientific Figures: Patent figures are a specific form of scientific illustrations. Although previous work on generating descriptions of patent figures has been sparse, ample research has been done to caption scientific fig-ures. Chen et al. (2019, 2020) create and leverage FigCAP and adapt an LSTM-based model (Hochreiter and Schmid-huber 1997) for captioning. Recently, Hsu, Giles, and Huang (2021) collected the SciCap dataset from articles published on arXivIn (Yang et al. 2023), the authors augment the SciCap dataset with additional information such as OCR text from figures and referring sentences from the text to curate SciCap+, and demonstrate the performance boost achieved by incorporating extra information. Kantharaj et al. (2022) and Tang, Boggust, and Satyanarayan (2023) address the problem of captioning various visualization charts"}, {"title": "3 PATENTDESC-355K: A Novel Dataset of Patent Figures with Descriptions", "content": "We introduce PATENTDESC-355K a novel large-scale dataset tailored for generating descriptions for patent fig-ures. Our proposed dataset comprises 355K patent figures sourced from Google Patents\u00b2, with each image accompa-nied by its brief and detailed descriptions extracted from the corresponding patent documents. The dataset is available for download on our project website: https://vl2g.github. io/projects/PatentLMM/. Fig. 1 visualizes a patent fig-ure, brief description, detailed description) triplet from our dataset. With our primary focus on US patents published af-ter 2004, our dataset spans over 60K patents from assignees like Amazon, Microsoft, LinkedIn, Google, Yahoo, etc. To assess the quality of the dataset, we manually evaluated a random set of 100 patent figures with their brief and detailed"}, {"title": "4 Methodology", "content": "Our approach is inspired by the recent success of large multimodal models like MiniGPT-4 (Zhu et al. 2024) and LLaVA (Liu et al. 2023b, 2024a), which have demonstrated state-of-the-art performance on several benchmarks by ef-fectively aligning visual and textual modalities. We intro-duce PATENTLMM, which combines our domain-adapted version of the LLaMA language model, namely PATENTL-LAMA, with our novel visual encoder specialized for patent figures, namely PATENTMME. In this section, we describe the architectures of PATENTMME and PATENTLLAMA, and the overall framework of PATENTLMM."}, {"title": "4.1 PATENTMME: Encoder for Patent Figures", "content": "The Vision Transformer (ViT) (Dosovitskiy et al. 2021), commonly used as a vision encoder in existing image cap-tioning frameworks, is typically pre-trained on natural scene images, which are fundamentally different from patent fig-ures. A better suited encoder is perhaps LayoutLM (Xu et al. 2020, 2021; Huang et al. 2022) which has shown impressive performance in document image understanding tasks. How-ever, patent figures have a sparse layout compared to dense document images and are characterized by specific struc-tured visual syntax. Unlike document images, patent figures comprise labeled nodes interconnected with arrows and ac-companied by textual elements. The semantic relationship"}, {"title": "4.2 PATENTLLAMA: Description Generator", "content": "PATENTLLAMA is a domain-adapted version of the LLaMA-2 7B model for the patent domain. We continue to pre-train the LLaMA-2 7B model using LoRA (Hu et al. 2022) adapters, on the descriptions from HUPD patent dataset (Suzgun et al. 2024), to bias the model to gener-ate the language inherent to patent documents. To avoid any train-test leakage, we ensure that we use the HUPD dataset after removing patent documents corresponding to the validation and test splits of our PATENTDESC-355K dataset."}, {"title": "4.3 PATENTLMM", "content": "Inspired by recent multimodal LLM studies like MiniGPT-4 (Zhu et al. 2024) and LLaVA (Liu et al. 2023b, 2024a), we integrate PATENTMME and PATENTLLAMA through a single MLP network to exploit their pre-trained represen-tations. The detailed architecture for PATENTLMM is il-lustrated in Fig. 3. Given a patent figure, we first obtain its layout-aware text and visual representations from frozen PATENTMME. These representations are projected into the input embedding space of PATENTLLAMA using a projec-tion MLP, and the PATENTLLAMA is finetuned to maxi-mize the likelihood of the corresponding description condi-tioned on these projected representations."}, {"title": "5 Experiments", "content": "PATENTMME: PATENTMME is initialized with LayoutLMv3-Large to inherit its document understanding capabilities. For each of the three losses discussed in Section 4, the text and image embeddings obtained from PATENTMME are projected through separate MLPs (loss heads) before the loss is calculated. Since the network weights already have a good initialization, to prevent major changes in weights of the multimodal transformer, we adopt two-step training. During Step-1, the weights of the multimodal transformer remain frozen and only the loss heads are trained for 1 epoch with a higher learning rate of 1e-3 and 1K warm-up steps to learn good initialization. During Step 2, the entire model is trained end-to-end for 8 epochs with a lower learning rate of 5e-5 and with 10K warm-up steps. The PATENTMME model is trained on 8\u00d7V100 GPUs, with an effective batch size of 64 and Adam (Kingma and Ba 2014) optimizer.\nPATENTLMM: Following the standard practice (Liu et al. 2024a), we train our PATENTLMM model in two stages. To align the patent figure representations ob-tained from PatentMME with the input latent space of PATENTLLAMA, we train only the projection layer in the first stage, keeping all other parameters frozen. Dur-ing stage 2, we add LoRA adapters to all the linear lay-ers of the PatentLLaMA module, except for the language modeling head, whose weights remain frozen. The weights of PATENTMME are kept frozen throughout. We train our PATENTLMM with an effective batch size of 192 on 3\u00d7A100 GPUs (40 GB). Stage 1 training progresses at a higher learning rate of 1e-3, and stage 2 training takes place"}, {"title": "5.3 Results and Discussion", "content": "The quantitative performance comparison for the brief and detailed description generation task is reported in Table 2. In the zero-shot setting, GPT-4V demonstrated superior per-formance among baselines across all metrics, significantly outperforming other baselines owing to its large scale and the diverse data it has seen during its pre-training. The poor zero-shot performance of other baselines highlights the gap in their pre-training data and the nature of patent figures and descriptions. The fine-tuned models outperform their zero-shot counterparts, highlighting the importance of task-specific training for these models. MiniGPT-4 and LLaVA-1.5 utilize a frozen pre-trained ViT trained on web-scale natural images, which results in suboptimal representation of patent figures. Similarly, OFA also enforces these priors by utilizing a pre-trained discrete image tokenizer. On the other hand, PATENTLMM gives a boost of ~ 8% across all metrics, signifying the importance of better domain knowl-edge embedded in it through the proposed PATENTMME pretraining and PatentLLaMA.\nSimilar to brief description generation, GPT-4V outper-formed all other baselines for the detailed description gener-ation task. We observe that majority of the baselines struggle with performance in the zero-shot setup. In the fine-tuned setting, our PATENTLMM main-tained its superior performance, achieving the highest scores across all metrics. This consistent top performance for both brief and detailed descriptions suggests the efficacy of our proposed approach for the task of generating descriptions from patent figures. The overall lower scores for detailed descriptions can be attributed to their comprehensiveness, complexity, and length, requiring models to capture and generate more nuanced and detailed information.\nAblations: We perform the following three ablation stud-ies to quantify the impact of different components of our proposed PATENTLMM model:\n(i) PATENTMME Pre-training objectives: Table 3 shows the ablation results with combinations of pre-training objec-tives for the brief description generation. We observe that using a combination of MLM and LAMIM leads to better re-sults compared to the pre-trained LayoutLMv3. Further, the PC loss also improves the performance of the model, when pre-trained with HUPD images data. A similar ablation for"}, {"title": "C Additional Experiments and Details", "content": "C.1 Evaluation Metrics\nTo measure the description generation performance, we use the standard image captioning metrics such as BLEU (Pap-ineni et al. 2002), ROUGE (Lin 2004) and METEOR (Banerjee and Lavie 2005). Higher values for all the scores are desired.\nBLEU-n (Papineni et al. 2002) calculates the n-gram over-lap between the generated and reference texts, taking into account precision with which captions are generated, and a brevity penalty for shorter texts. We report BLEU-2, BLEU-4 and Avg. BLEU for all our experiments. The Avg. Blue score averages BLEU-1, BLEU-2, BLEU-3 and BLEU-4 metric.\nROUGE (Lin 2004) also measures the overlap between the generated and reference texts. ROUGE-N measures N-gram overlap while ROUGE-L measures the overlap based on the longest common subsequence between the generated and reference texts. We report ROUGE-1, ROUGE-2 and ROUGE-L for our experiments.\nMETEOR (Banerjee and Lavie 2005) is computed based on the explicit word-to-word matches between the generated and reference texts. It considers not only exact word matches but also stem, synonym, and paraphrase matches, as well as applies weighted penalties for incorrect word order.\nC.2 Additional Ablations\nPATENTLMM Training: Table 6 shows the ablation re-sults with different design choices for training of the de-coder LLM of PATENTLMM for the brief description gen-eration task. We show results without stage 2 training of the PATENTLMM (rows 1 and 3). We also show results when the decoder LLM is initialized using LLaMA-2 versus PATENTLLAMA. We observe that stage-1 training is clearly not enough and the decoder LLM needs to be finetuned for the patent description task to generate reasonable descrip-"}, {"title": "E Future directions to mitigate failures", "content": "In this section, we describe potential approaches to address the observed failure cases in our model's generated descrip-tions. Specifically, we focus on reducing hallucinations and inaccuracies arising from missing contextual information, including figure references and technical details.\nDocument-level Reasoning for better Cross-Figure Ref-erences Our analysis reveals that the model occasionally hallucinates cross-figure references (as illustrated in our case studies (Appendix D.1)), particularly when an inven-tion's component is illustrated across multiple diagrams. To mitigate this issue, we propose enhancing document-level reasoning by linking interdependent figures throughout the patent document. By enabling the model to track and recon-cile components and their relations across multiple figures, we can ensure that figure references are more accurate and context-aware.\nIncorporation of External Knowledge Bases When key textual cues are absent or insufficient within a given figure, the model may hallucinate technical details. To address this limitation, we suggest integrating external technical knowl-edge sources\u2014such as domain-specific knowledge bases or authoritative patent databases\u2014into the generation process. By drawing on these external resources, the model can re-trieve and incorporate accurate relevant information rather than hallucinating it. To this end, techniques inspired by Retrieval-Augmented Generation (RAG) can be employed to query large, domain-specific repositories and return the most relevant knowledge snippets. This retrieval step pro-vides a verifiable grounding for the generated descriptions, significantly reducing the likelihood of hallucinated techni-cal content.\nGrounded Description Generation Combining the above strategies can lead to a unified approach for grounded de-scription generation, where figure content is consistently anchored to both internal textual references and external knowledge bases. By ensuring that each descriptive element corresponds to verifiable information, we can produce more reliable, less hallucinated, and overall higher-quality figure descriptions. This combination of document-level reasoning and external resource integration is a crucial step towards generating robust and trustworthy descriptions of patent fig-ures."}]}