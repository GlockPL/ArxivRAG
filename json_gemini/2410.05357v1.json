{"title": "Model-GLUE: Democratized LLM Scaling for A Large\nModel Zoo in the Wild", "authors": ["Xinyu Zhao", "Guoheng Sun", "Ruisi Cai", "Yukun Zhou", "Pingzhi Li", "Peihao Wang", "Bowen Tan", "Yexiao He", "Li Chen", "Yi Liang", "Beidi Chen", "Binhang Yuan", "Hongyi Wang", "Ang Li", "Zhangyang Wang", "Tianlong Chen"], "abstract": "As Large Language Models (LLMs) excel across tasks and specialized domains,\nscaling LLMs based on existing models has garnered significant attention, which\nfaces the challenge of decreasing performance when combining disparate models.\nVarious techniques have been proposed for the aggregation of pre-trained LLMs,\nincluding model merging, Mixture-of-Experts, and stacking. Despite their merits, a\ncomprehensive comparison and synergistic application of them to a diverse model\nzoo is yet to be adequately addressed. In light of this research gap, this paper\nintroduces Model-GLUE, a holistic LLM scaling guideline. First, our work starts\nwith a benchmarking of existing LLM scaling techniques, especially selective\nmerging, and variants of mixture. Utilizing the insights from the benchmark results,\nwe formulate an strategy for the selection and aggregation of a heterogeneous model\nzoo characterizing different architectures and initialization. Our methodology\ninvolves the clustering of mergeable models and optimal merging strategy selection,\nand the integration of clusters through a model mixture. Finally, evidenced by\nour experiments on a diverse Llama-2-based model zoo, Model-GLUE shows an\naverage performance enhancement of 5.61%, achieved without additional training.\nCodes are available at: https://github.com/Model-GLUE/Model-GLUE.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated unparalleled capability in a diverse array of\nnatural language tasks, encompassing commonsense reasoning, question answering, and even spe-\ncialized domains such as mathematics and programming [39, 43, 52]. The effectiveness of LLMs is\nbased on the scaling law, which posits that increased model and training data sizes correlate with\nenhanced model performance [27]. Nevertheless, the computation overhead and data requirement\nsurge as LLM continues to scale. With the widespread of open-sourced general or specialized LLMs,\naggregating existing models to construct a more versatile LLM emerges as an economical alternative\nto training a larger LLM from scratch [13, 16, 54]. This not only mitigates the computation cost but\nalso leverages the collective advancements of previous efforts in building LLMs.\nWithin different methods to combine existing LLMs, a major class is merging [2, 4, 22, 24, 35,\n59, 63, 64]. Model merging combines multiple models into a single one of the same size through\nweight-space transformation. Wortsman et al. [59] first propose to merge a few fine-tuned models as a\ntraining trick for the flat loss-landscape, and Ilharco et al. [22] extends it to multi-task scenario, both\nof which employ the simple averaging. Other works propose more complicated merging methods,\nleveraging weight sparsity [63, 64] and non-uniform coefficient [4, 35]. However, they assume that\nall candidate models are \u201cuseful\u201d when merging. While this may hold true for small-sized designed\nmodel collections, it may not be the case in real-world scenarios given a large and divergent model\nzoo. How to ensure the benefits of merging om different model zoo sizes and similarities, and how to\nexclude \"harmful\" candidates, remains underexplored."}, {"title": "Related Works", "content": "Model Merging. Merging methods can be divided into zero-shot merging and merge-then-train\napproaches. Early zero-shot merging methods are weight averaging and Linear Mode Connectiv-\nity [38, 59]. Later popular methods include Task Arithmetic [22] manipulating task vectors, and\nTIES [63] addressing parameter interference through trimming and conflict resolution. DARE [64]\noptimizes parameters selectively to enhance merging without extra training. Others focus on geo-\nmetric properties of weights for merging [49, 24]. Recent Evolutionary Model Merge [4] improves\nweight configuration and data token pathways during inference. For the merge-then-train approach,\nFisher merging [35] uses the Fisher information matrix to weigh model parameters to maximize\ntheir joint likelihood. RegMean [26] adapts the linear merging to each linear layer while averaging\nembeddings and biases. However, both zero-shot and merge-then-train approaches are less effective\nfor models initialized differently. [2, 23, 53, 62] exploit the permutation symmetry inherent in neural\nnetworks on small to large models. To boost merging efficiency, our focus on merging lies in the\nzero-shot merging of models with the same architecture and initialization.\nModel Mixture. Mixture-of-Experts (MoE) [47] scales up neural networks by utilizing router\nnetworks to activate different parts of the model for different input tokens. Its integration with Large"}, {"title": "Methodology", "content": "In this study, we consider a collection of n existing Large Language Models (LLMs), denoted\nas {M1,..., Mn}, which have been fine-tuned on diverse corpora. Our objective is to outline a\nsystematic approach towards producing one stronger aggregated model across all knowledge domains.\nSpecifically, the unified LLM incorporates single LLMs mainly through merging and mixture."}, {"title": "Model Merging", "content": "The concept of Model Merging Model merg-\ning is integrating multiple models into one uni-\nfied model in the weight space, compatible with\nLLMs of the same initialization [16]. Popular\nmerging methods can be divided into two types:\n\u2022 Merging entire model weights represented\nby Model Soup [59] (Linear), SLERP [49], and\nModel Stock [24]; \u2192 Task-vector based merging\nrepresented by Task Arithmetic [22], TIES [63],\nand DARE [64]. The former method directly\ninterpolates model weights, while the latter sub-\ntracts the pre-trained model from the fine-tuned\nmodel to obtain task vectors and utilizes spar-\nsity and consistency of parameters for refined\nmerging. The basic Linear interpolation merging is defined as $w_{u} \\sum_{i=1} s_{i} \\cdot w_{i}$, where $w_{i}$ and $s_{i}$\nare the corresponding model weights and merging coefficient of Mi \u2208 {M\u2081,...Mn}.\nSelective Merging Pipeline Merging can be easily applied to models with the same architecture,\nbut does not guarantee better results. Therefore, before searching for the merging coefficient, we first\npre-process the models by clustering all the models using cosine similarity and then searching for the\noptimal merging coefficient and method within each cluster. The details of clustering are explained\nin Appendix B.8."}, {"title": "Heuristic and Evolutionary Strategies", "content": "The heuristic strategy is for searching and filtering potential\nharmful models for merging. It is based on greedy search, involving three variants: Heuristic-\nAverage retain the candidate if there is an improvement on the proxy dataset in each round of\nmerging. Heuristic-Coefficient builds upon Heuristic-Average, by combining the previously\nmerged model with a new candidate using different coefficients in each round. Heuristic-Similarity\nselects the candidate model with the highest or lowest similarity and conducts a coefficient search to\ncombine it with the previously merged model. Detailed heuristic strategy algorithms can be found\nin Appendix B.4 Heuristic strategies perform pairwise merging of models, while many methods\nallow for merging multiple models at once. Therefore, we also consider jointly optimizing all model\ncoefficients using the Evolutionary Strategy."}, {"title": "Model Mixture", "content": "The concept of Model Mixture.\nModel mixture resembles Mixture-\nof-Experts(MoE). It scales a LLM\nwith multiple pre-trained LLM ex-\nperts and further extends beyond tradi-\ntional token-dependent Feed-Forward-\nNetwork (FFN) MoE designs [47]. A\nmixture model is composed of MoE\nmodules and the rest shared param-\neters. A MoE module consists of\na router $G(\u00b7)$ and n expert networks\n{E1,\u2026, En}. $G(\u00b7)$ takes a router in-\nput \u00e6g and generate expert assign-\nment for each token input \u00e6. Then\nMoE outputs a weighted sum of experts' outputs as $MoE(x, x_{g}) = \\sum_{i=1}G(x_{g})_{i} \\cdot E_{i}(x)$. We experi-\nment with several variations of Model Mixture, classified as follows:\nMixture levels. Traditional Mixture-of-expert models replace the dense FFN layer at each Trans-\nformer block with an MoE module, which is only compatible with LLMs that share the same\narchitecture. Besides this \u2192 FFN level mixture, we also experiment with two coarse-grained mixtures.\nBlock level mixture create MoE module by aggregating Transformer blocks with the same index\nfrom each LLM as experts and add a block-wise router. Block level mixture is applicable to models\nwith different architecture but the same embedding space, layer amounts, and intermediate dimension.\nModel level mixture take each LLM as an expert and use a router at mixture model input. Model\nlevel mixture covers any LLM groups not compatible with FFN and block level mixture. In particular,\nthe model level mixture is similar but not identical to the model ensemble, as the former can be sparse\nand focus more on efficiency and exploit single LLM expertise, while the latter produces general\nresults by averaging or majority voting overall model outputs. Details can be found in Appendix B.6\nRouter design. The router network of many MoE studies adheres to a linear router [47]. We\nexperiment with another more complex MLP router to examine whether this router design leads\nto better performance. It is implemented by two sequential FFN and a ReLU function in between,\ninspired by [48, 32]. For the routing method, we employ Top-K selection to all routers, which\nactivates the K experts corresponding to the K largest softmaxed router output [47, 48].\nRouter input. We adopt two types of router input for different levels of model mixture: 0 Token\ninput for FFN level mixture, where router input is the same as model input; Sample input for\nblock and model level mixture, where we calculate the average embedding as the sample input\n$x = \\Sigma_{n=1} X_{n}$, and route tokens of a sample to the same expert based on sample routing. The\nsample routing avoids inconsistency in attention operation.\nHybrid mixture. To explore LLM scaling in between model merging and model mixture, we\npropose the hybrid mixture as an intermediate solution. In a hybrid mixture, the bottom few layers of\nall single LLMs are merged, and then the rest layers follow any of the mixture level designs."}, {"title": "Model Merging and Model Mixture for LLMs", "content": "Model Zoo. 5 groups of Llama-2-based 7B LLMs are employed in experiments where the num-\nber of models \u2208 [2, 4, 8, 12, 16]. Specific model candidates and performances can be found in the\nAppendix B.2. For merging benchmarks, we experiment with a larger model zoo, namely Which4,\nWhich8, and Which12 with model filtered from Which16. For model mixture with higher compu-\ntational cost, we experiment with Which2 and Which4. Since we are trying to include different\nmodels trained for chatting, writing, mathematical reasoning, and coding in our collections, we also\nexperiment with closer models for comparison, coming up with Which4 (Chat), while Which is\nWhich4 (Domain) by default.\nBenchmarks We assess all models on three categories of tasks: (i) Commonsense reasoning\nusing ARC [10], WinoGrande [45], and MMLU [20]; (ii) Mathematics ability on GSM8K [11]; (iii)\nCoding ability on MBPP [6] and HumanEval [9]. The evaluation scripts are based on Im-eval 2 for\ncommonsense and mathematical reasoning and bigcode-eval 3 for coding datasets."}, {"title": "Implementation Details for Merging", "content": "Proxy Dataset. Since the performance of merging model is not necessarily positive, we need a\nproxy dataset to determine whether to reject a particular round of merging in the Heuristic Strategy,\nor to compute the model fitness in the Evolutionary Strategy. Details are in Appendix B.3.\nModel Zoo and Clustering. The Merging Bench considers 3 model zoos: Which4, Which8, and\nWhich16. We first cluster the model zoos based on cosine similarity with a threshold of 0.95. Due to\nWhich16 contains models that cannot be merged, we choose the mergable family obtained through\nclustering which is referred to as Which12."}, {"title": "Details of Heuristic Strategy and Evolutionary Strategy", "content": "For Heuristic Strategy, to reduce\nthe search space, we only evaluated Linear interpolation and the range of coefficient search is\n{0.1, 0.2...0.9}. In Heuristic-Similarity, we use the average similarity of all weights as the criterion\nfor selecting models in each round. For Evolutionary Strategy, we refer to the setting of Evolutionary\nModel Merge [4], which utilizes the CMA-ES [19] algorithm implemented by Optuna [3]. In contrast,\nall parameters are randomly initialized, and the fitness values are defined as the accuracy of the proxy\ndataset. The optimization was conducted for 200 trials in all scenarios."}, {"title": "Model Merging Benchmark Result", "content": "We start our discussion by examining the effectiveness of existing approaches in depth. Despite\nexisting merging methods focus on improving the merging techniques, their effectiveness is usually"}, {"title": "Does handcrafted rules apply to automated model selection and which one performs best?", "content": "A: Yes, by a greedy search approach. In this section, we explore three potential heuristics for\nmodel selection and report the results in Figure 4(a). We include the performance of the \u201cbest single\nmodel\" (the model participant before merging that achieves the best averaged performance). We\nadditionally validate the performance of heuristic-based merging technique, which are detailed in\nSection 3.2. As indicated by the results, the merging technique based on Heuristic-Coefficient yields\nconsistently superior performance when the model zoo is large. For Which4, Heuristic-Average\nachieved better performance, while Heuristic-Coefficient performed poorly. This is primarily because\nthe domain-specific models in Which4 exhibit similar performances and are indispensable."}, {"title": "How to utilize Evolutionary Strategy for coefficient optimization in model merging?", "content": "We divide the problem into the following sub-questions: (i) Which merging method is most compatible\nwith Evolutionary Strategy? (ii) Can finer-grained optimization lead to a better merged model? (iii)\nHow to efficiently merge in a large model zoo? For (i), A: simpler methods such as Linear and Task\nArithmetic are more competitive. We compared four methods: Linear, Task Arithmetic, DARE,\nand TIES. As shown in Figure 4(b), Linear merging consistently achieves great results. However,\nwhen the parameters to be optimized are small, Task Arithmetic performs slightly better than Linear.\nUnder a fixed computational budget, due to the doubling of parameters to be optimized, DARE and\nTIES exhibit slightly lower performance compared to other methods. For (ii), A: Yes, but we need\na larger computational budget. We group adjacent n decoder layers together, where they share\nthe same coefficients. The group size n \u2208 [32,8,4,1]. When n = 8, better results were achieved\ncompared to n = 32, as shown in Table 16. However, as we further decreased the group size, the\nperformance slightly declined. This could be attributed to our relatively small budget. For (iii), A:\nUse Heuristic Strategy to roughly search for coefficients and then fine-tune the coefficients\nusing Evolutionary Strategy. As shown in Table 17, the combination of the two strategies resulted\nin better results with fewer trials. For implementation details, please refer to Appendix B.5."}, {"title": "Implementation Details for Mixture", "content": "Model Zoo and Router Initialization. In Mixture Bench, we experiment with Which2 and Which4\nmodel settings. For router design, we mainly adopt a training-free linear layer router initialized from\nthe prompt vector, as previous studies have demonstrated its effectiveness in the zero-shot MoE\nmodel [16]. For specific prompt settings, we refer to the Beyonder model series 4. For the routing\nalgorithm, we use Top-1 routing for Which2 and Block level mixture and Model-level mixture for\nWhich4, and Top-2 for Which4 FFN level mixture."}, {"title": "At which level does the model mixture manifest its utmost effectiveness?", "content": "A: Model level mixture is\nconsistently better. Our\ncomparative analysis of the\n{FFN, block, model} level\nmixture, all employing the\nlinear router and the sam-\nple routing strategy as pre-\nsented in Table 2, consis-\ntently demonstrates the su-\nperiority of the Model level\nmixture under Which2 and\nWhich4 setting. This could\nbe attributed to the design\nthat Model Level Mixture route each sample to one expert model, thereby avoiding the conflicts\nbetween different expert models and maximizing the expertise of the most appropriate experts. Since\nthe experts are not derived from the same pre-training process, directly merging their inconsistent\nrepresentation spaces will affect the performance of the mixture model, with more expert parameters\nleading to worse results. This is especially evident for Block-level Mixture, as the routing is performed\nat each transformer layer and the representation is fed into different expert blocks in series, causing\nconfusion when switching between different expert knowledge."}, {"title": "Does more complex router design brings better results?", "content": "A: Not necessary, as the lin-\near router outperforms the MLP\nrouter. From Table 3, the perfor-\nmances of the linear router with-\nout additional training slightly sur-\npass MLP router models, i.e., F-L-\nT over F-M-T, B-L-S over B-M-S.\nSpecifically, linear router models\nare better at math and coding datasets, validating prompt vector is effective in assorting samples from\ndifferent domains, which is otherwise too implicit to learn via direct language modeling."}, {"title": "Does model mixture directly works on unmergeable models?", "content": "A: No. We directly ap-\nply the setting of Which2\nModel level mixture to\nLlama-2-7b-chat and Crys-\ntalChat, an unmergeable\nmodel pair with different ar-\nchitectures and initialization. As shown in Table 4, the performance is slightly behind the best single\nmodel. This may be due to simple prompts and direct mixture, as it fails to coordinate the divergence\nbetween drastically different models. We evaluate more complex prompts for the same model pair\nand the mixture model outperforms, see Table 19 for more information."}, {"title": "Which router input is better, token-level or sample-level?", "content": "A: Not quite different. To-\nken input suits a mixture\nof the same domain mod-\nels. Table 5 shows the per-\nformance token-based and\nsample-based routing are\npretty close. In particu-\nlar, for Which2 and Which4\n(Chat) where models are\nall trained for general chat-\nting purposes, token rout-\ning outperforms, whereas\nsample routing is better for\ndefault Which4 (Domain)\nwith differently specialized\nmodels. This may result from divergence of model knowledge and representation spaces will cause\nconflicts in fine-grained token routing."}, {"title": "Is it feasible for hybrid mixtures to provide enhancements?", "content": "A: Yes. Our experiments on F-\nL-T with v.s. without the hybrid\nmixture, as detailed in Table 6,\ndemonstrate that the hybrid mix-\nture significantly improves per-\nformance on average and simul-\ntaneously reduces the memory\noverhead during inference. This\nimprovement may be attributed\nto the higher sensitivity of the ini-\ntial transformer blocks. Avoiding\nusing MoE for these blocks can\nyield performance gains, as suggested by a few previous works as well [12, 41]. Surprisingly, our re-\nsults show that the hybrid F-L-T model consistently outperforms the standard F-L-T on math and code\ntasks. Our further analysis indicates that this improvement might be because of the conversational\nnature of the content in GSM8K, MBPP, and HumanEval datasets, which appears to challenge the\nrouting mechanisms within the initial transformer blocks, leading to ineffective expert specialization."}, {"title": "Superior Recipes to Aggregate LLM Knowledge", "content": "We select Full\nMerging as the representative\nmodel for the Llama-2 family\nand combined it with other mod-\nels that could not be merged\nby model mixture. On average,\nthe Model-GLUE demonstrates a\n5.61% improvement over the Best Single Model. More details are presented in Appendix B.7."}, {"title": "Model Merging v.s. Mixture", "content": "Q1: For a mergeable model zoo, how should we choose between merging and mixture? When\ncomputational resources are limited or the models are relatively homogeneous, merging is almost\nalways a simple and effective method. For the domain-specific models, mixture can bring greater\nimprovements."}, {"title": "How to combine models with greater differences in an extensive and varied model zoo?", "content": "Q2: How to combine models with greater differences in an extensive and varied model zoo?\nIn Which16, a larger and more diverse model zoo, some models cannot be merged due to structural\ndifferences and models that would degrade in performance when merged with other models. Therefore,\nwe first cluster the models based on cosine similarity. Within each mergeable family, we perform\neither merging or mixture. We initially employ heuristic strategies of merging and report the best\nresults (i.e., Full Merging) in Table 8. The Llama-2 family (i.e., Which12) consists of up to 12\nmodels, so directly combining them through the mixture is inefficient. Thus, we only consider models\nselected by merging and report the results of F-L-T Mixture. From Table 8, we can observe that\nFull Merging outperforms F-L-T Mixture."}, {"title": "Discussion with Other LLM Aggregation Techniques", "content": "Thus far, we have been mainly focusing on two LLM aggregation techniques: model merging and\nmixture. In this section, we discuss other potential techniques that could help models work together.\nModel Stacking. Research has demonstrated that stacking a model's parameters on top of itself\ncan accelerate training convergence as opposed to training a model of double the size from scratch\n[17, 18, 56, 60, 28]. This concept can be extended naturally to stack multiple models as one larger\nmodel. Our experimental results indicate that model stacking with lightweight fine-tuning can yield\nsuperior performance compared to various merging and mixture models. For instance, stacking\nLlama 7B and Vicuna 7B can achieve \u2265 55% on the MMLU benchmark. When compared to model\nmixture, model stacking offers less flexibility in terms of model choices. Although the resulting\narchitecture is more standardized than MoE, increasing the model depth through stacking also results\nin higher latency than mixture models where subnetworks are inferenced in parallel. Additionally,\nmodel stacking does not simplify the design space, such as determining whether, which, and how\nmany layers should be dropped when stacking two heterogeneous models. We refer interested readers\nto Appendix C.3 for fine-grained empirical analysis.\nModel Communication. Model communication [61, 31, 33] is a framework that enables the\ndevelopment of LLM applications through the use of multiple conversable agents that collaborate to\ncomplete tasks. This approach allows developers to design complex LLM application workflows as\nmulti-agent conversations, where agents with various roles and capabilities, driven by LLMs, tools,\nor human inputs, interact with each other. Unlike model merging, mixture, and stacking techniques,\nLLM communication is orthogonal to the primary focus of this paper because it does not modify the\nmodel weights; instead, it leverages the in-context learning and conversational capabilities of LLMs\nto coordinate agents. An empirical comparison with this class of methods is beyond the scope of this\nstudy and will be explored in future research."}, {"title": "Conclusion", "content": "In this paper, we explore the scaling LLM based on a model zoo of pre-trained LLMs within the real\nworld. We first benchmark state-of-the-art LLM merging, mixture, and discuss model stacking. Based\non previous findings, we then propose a novel LLM scaling framework, Model-GLUE. Specifically,\nwe scale up the model zoo closely examine the existing model merging techniques, and conclude the\nselective merging techniques based on heuristics and learnable algorithms. Further, we investigate\nvariants of Mixture-of-Experts for combining LLMs and suggest it can serve as an alternative\nto merging failure cases. Finally, we integrate selective merging strategies with model mixture\ntechniques, presenting this as a comprehensive solution for scaling a diverse array of LLM collections.\nFuture works will include model stacking and communication to our Model-GLUE framework."}, {"title": "Limitation", "content": "Our work has the same general limitations as existing studies on LLM scaling. First, a limited\ntheoretical foundation. While empirical evidence suggests that increasing model size, data volume,\nand computational complexity leads to better performance, there is little theoretical clarity on the\nexact mechanisms behind these improvements. Second, performance saturation. Although scaling\nlaws suggest that performance continues to improve as models get larger, recent evidence suggests\nthat scaling may lead to diminishing returns beyond a certain point. In addition, our work focuses on\nbenchmarking results, while the reasons for model merging, mixing improves performance could be\nfurther improved by post-hoc analysis, such as investigating parameter distribution and similarity\nduring model operations."}, {"title": "Implementation Details", "content": "For all the router training experiments, we apply the batch size of 128, a cosine learning rate scheduler,\nthe learning rate of 5e \u2013 5, and the epochs of 1. The training script we used can be found here."}, {"title": "Model Zoo Configuration", "content": "Table 9 provides an overview of the Model Zoo. For Section 5.1, we first compare merging and\nmixture in a mergeable family. We adopt the Which4 model zoo from Section 4, referred to as\nWhich4 (domain). It comprises four models from different domains that can be merged. We also\nintroduce Which4 (chat), consisting of four mergeable chat models, indicating that no single\nmodel has a dominant advantage in a specific domain. For Section 5.2, we use Which16. Besides\nthe 12 Llama-2-based models that can be merged in Which12, Which16 additionally includes four\nhighly performant domain-specific models that cannot be merged. Specifically, it includes three\nCodeLlama-based models, two of which are code models and one is a math model, as well as\nLLM360/CrystalChat. It is important to note that although CodeLlama is initialized from Llama-2,\nmerging CodeLlama-based models with Llama-2-base models leads to performance degradation.\nLLM360/CrystalChat differs from Llama-2 in terms of architecture, initialization, and training data,\nand therefore cannot be merged with other models."}, {"title": "Employed Datasets and Associated Licences", "content": "Benchmark datasets. We use the following datasets for benchmarks in the paper with the follow-\ning licenses. ARC [10], WinoGrande [45], and MBPP [6] are under Creative Commons License.\nMMLU [20], GSM8K [11], and HumanEval [9] are under MIT License.\nProxy datasets for model merging. We select a smaller proxy dataset for each task to evaluate\nthe merged model: (i) For MBPP, we select its validation set. (ii) For HumanEval, due to the\nunavailability of a validation set and its smaller size, we select 20% of the JavaScript version of\nHumanEvalPack [36] under MIT License. (iii) For other tasks, we chose the small-scale datasets\nreleased by tinybenchmarks [40] under MIT License."}, {"title": "Detailed Algorithms of Heuristic Strategy of Model Merging", "content": "Heuristic (Average). We present the implementation details in Algorithm 1. The algorithm takes a\nmergable model family as input and generate a merged model as output. For each candidate model in\ninput model family, we compute the accuracy of the temporary merged model, generated by the union\nof this candidate model and the previously selected model, on the proxy dataset, and the candidate\nthat brings no harm to the accuracy will be selected for the final merged model. Each weight of the\nmerged model is generated by averaging the corresponding weights of all the selected models."}, {"title": "Heuristic (Coefficient)", "content": "We present the implementation details in Algorithm 2. Heuristic (Co-\nefficient) builds upon Heuristic (Average) by combining the previously merged model with a new\ncandidate using different coefficients in each round. To reduce the search space, we set the range of\ncoefficient as 0.1, 0.2...0.9."}, {"title": "Heuristic (Similarity)", "content": "We present the implementation details in Algorithm 3. We use the average\nsimilarity of all weights as the criterion for selecting models in each round. This algorithm selects the\ncandidate model with the highest or lowest similarity and conducts a conefficient search to combine\nit with the previously merged model."}, {"title": "Detailed about Evolutionary Strategy of Model Merging", "content": "For the experiments of Q2 - (i) in Section 4.3, we constrain all parameter values to be within the\nrange of [0, 1]. TIES and DARE require to optimize 2 * k parameters, while other methods require to\noptimize k parameters, where k represents the number of models included in the model zoo.\nFor the experiments of Q2 - (ii) in Section 4.3, we choose the Linear method for experimentation,\nand we constrain all parameter values to be within the range of [0, 1]. For finer-grained merging, we\ngroup adjacent n decoder layers together, where they share the same coefficient. For the remaining\nparameters, we make them share the same coefficient. Hence, the number of parameters that need to\nbe fine-tuned is given by: k * (num_hidden_layers + 1), where k represents the number of models and\nn represents the size of groups. For the case of n n = 32, we utilized the previous results, thus the\nnumber of parameters to be optimized is k.\nFor the experiments of Q2 - (iii) in Section 4.3, we control the variation of coefficients obtained\nthrough heuristic strategy to not exceed 0.1, and when it is negative, we set it to 0. We also only\nevaluate the Linear method."}, {"title": "Detailed Algorithms of Model Mixture", "content": "Model Level Mixture. We present the implementation details in Algorithm 4. The mixed model\nconsists of a router, which determines the expert to execute inference, and all the input models as\nexperts. All the weights of input model's components, including embedding layers (embd_layer),\ndecoder layers (layers) and language model head (lm_head), will be integrated into the mixed model."}, {"title": "Block Level Mixture", "content": "We present the implementation details in Algorithm 5. Different from\nmodel-level mixture, block-level mixture utilizes the embd_layer and Im_head of an additional model\nwithin a model family to handle input and output. Meanwhile, the transformer blocks of other models\nwithin the model family act as experts, connected by a router."}, {"title": "FFN Level Mixture", "content": "We present the implementation details in Algorithm 6. FFN level mixture\nis similar to block level with only difference on inner-block component sharing. Each layer of the"}, {"title": "Hybrid Mixture", "content": "We present the implementation details in Algorithm 7. The hybrid mixture\ncombines both merging and mixture methods. Specifically, the first k layers of the mixed model\nare obtained by merging multiple models, while the rest of the layers use an FFN-level mixture\narchitecture."}, {"title": "Details of Model-Glue", "content": "The models selected by the heuristic strategy include:\nmigtissera/Synthia-7B-v1.2,\nneuralmagic/Llama-2-7b-evolcodealpaca, teknium/OpenHermes-7B, meta-llama/Llama-2-7b-chat-hf,\nmeta-math/MetaMath-7B-V1.0, Imsys/vicuna-7b-v1.5. Since merging ise-uiuc/Magicoder-S-CL-7B\nand codellama/CodeLlama-7b-Instruct-hf does not lead to improvement in the Codellama's\nmergeable family, we select ise-uiuc/Magicoder-S-CL-7B as the representative model.\nThe final models used for Model-level Mixture are: LLM360/CrystalChat, ise-uiuc/Magicoder-S-CL-\n7B, meta-math/MetaMath-Llemma-7B and the representative model of the Llama-2 family obtained\nthrough the Heuristic (Coefficient). Please refer to our repository for specific configurations."}, {"title": "Details of clustering in selective merging pipeline", "content": "Motivation for using cosine similarity as a model selection criterion Previous merging study [64"}]}