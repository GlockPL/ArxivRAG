{"title": "IPDN: Image-enhanced Prompt Decoding Network for 3D Referring Expression Segmentation", "authors": ["Qi Chen", "Changli Wu", "Jiayi Ji", "Yiwei Ma", "Danni Yang", "Xiaoshuai Sun"], "abstract": "3D Referring Expression Segmentation (3D-RES) aims to segment point cloud scenes based on a given expression. However, existing 3D-RES approaches face two major challenges: feature ambiguity and intent ambiguity. Feature ambiguity arises from information loss or distortion during point cloud acquisition due to limitations such as lighting and viewpoint. Intent ambiguity refers to the model's equal treatment of all queries during the decoding process, lacking top-down task-specific guidance. In this paper, we introduce an Image-enhanced Prompt Decoding Network (IPDN), which leverages multi-view images and task-driven information to enhance the model's reasoning capabilities. To address feature ambiguity, we propose the Multi-view Semantic Embedding (MSE) module, which injects multi-view 2D image information into the 3D scene and compensates for potential spatial information loss. To tackle intent ambiguity, we designed a Prompt-Aware Decoder (PAD) that guides the decoding process by deriving task-driven signals from the interaction between the expression and visual features. Comprehensive experiments demonstrate that IPDN outperforms the state-of-the-art by 1.9 and 4.2 points in mIoU metrics on the 3D-RES and 3D-GRES tasks, respectively.", "sections": [{"title": "1 Introduction", "content": "3D Referring Expression Segmentation (3D-RES) presents significant potential applications in areas such as virtual reality, augmented reality, robotics navigation, and human-computer interaction. The goal of this task is to segment the object pointed to by a given textual description from a point cloud scene (Huang et al. 2021; Wu et al. 2024b).\nThe earliest approaches (Huang et al. 2021) to 3D-RES employed a two-stage paradigm: first, they used an instance segmentation network to generate proposals, and subsequently matched these proposals with the text to compute matching scores, leading to the final segmentation result. However, this methodology was found lacking in both efficiency and effectiveness (Wu et al. 2024b). Consequently,\nthey are treated with equal importance, similar to purely visual 3D segmentation (Kolodiazhnyi et al. 2024; Sun et al. 2023; Lai et al. 2023; Schult et al. 2023; Lu et al. 2023a).\nHowever, in 3D-RES, only the target object described in the text needs to be segmented. Ideally, queries relevant to the text should be prioritized. Yet, current methods (Wu et al. 2024b,a; He and Ding 2024) do not highlight these relevant queries, leading to the model having to implicitly learn the distinction between relevant and irrelevant queries, significantly increasing the difficulty of the learning process.\nTo address the above issues, we introduce the Image-enhanced Prompt Decoding Network (IPDN), which leverages multi-view images and task-driven information in a top-down approach to unleash the model's reasoning capabilities. As shown in Fig. 1, to tackle the feature ambiguity issue, we propose the Multi-view Semantic Embedding (MSE) strategy. MSE employs CLIP (Radford et al. 2021) to extract 2D image features, which are then fused with 3D point cloud features to significantly enhance visual representation. Additionally, Spatial-aware Attention is incorporated to address the absence of spatial positional relationships in 2D features. This approach results in visual features with superior representational power, enriched with text prior knowledge from CLIP, facilitating better alignment with textual features. To address the intent ambiguity issue, we designed a Prompt-aware Decoder (PAD) that guides the decoding process using task-driven signals. Through the Task-driven Prompt module, we generate prompts that emphasize the relevance of each query to the text, effectively injecting task-specific information into the model and significantly reducing the learning complexity. Extensive qualitative and quantitative experiments on the ScanRefer (Chen, Chang, and Nie\u00dfner 2020) and Multi3DRefer (Zhang, Gong, and Chang 2023) datasets validate the superior performance of IPDN, surpassing the current state-of-the-art (SOTA) by 1.9 and 4.2 points in mIoU metrics on the 3D-RES and Generalized 3D Referring Expression Segmentation (3D-GRES) tasks, respectively.\nTo sum up, our main contributions are as follows:\n\u2022 We identify two critical challenges in the 3D-RES task, i.e., feature ambiguity and intent ambiguity, and propose a novel method, IPDN, to effectively address them.\n\u2022 IPDN comprises two essential modules, i.e., MSE and PAD. The MSE integrates multi-view image information into 3D representations while restoring spatial information lost. The PAD pre-processes task-related signals to guide the decoding process with greater precision.\n\u2022 Extensive experiments show that our IPDN outperforms existing state-of-the-art methods, delivering significant improvements in both 3D-RES and 3D-GRES tasks."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 3D Referring Expression Comprehension", "content": "3D Referring Expression Comprehension (3D-REC) task is to predict a bounding box for objects indicated by text. Existing approaches to the 3D-REC task can largely be categorized into two types: two-stage (Chen et al. 2022; Feng et al."}, {"title": "2.2 3D Referring Expression Segmentation", "content": "Unlike the relatively mature studies in 3D-REC and 2D-RES (Shah, VS, and Patel 2024; Yang et al. 2024a; Ding et al. 2021; Yang et al. 2022; Wang et al. 2022a; Liu, Ding, and Jiang 2023; Lai et al. 2024; Chng et al. 2024), 3D-RES (Qian et al. 2024a; He and Ding 2024; He et al. 2024; Lin et al. 2023; Xu et al. 2024) is still in its infancy. As the pioneering work in this domain, TGNN (Huang et al. 2021) adopted a two-stage strategy, leveraging Graph Neural Networks for matching candidate instances with textual descriptions. 3D-STMN (Wu et al. 2024b) harnessed a one-stage method, significantly enhancing both inference speed and performance. Other approaches, such as MCLN (Qian et al. 2024b), capitalized on the similarity between 3D-RES and 3D-REC tasks to facilitate multitask joint learning.\nTo liberate the 3D-RES task from its constraint of having one and only one target object per sentence, the 3D-GRES task was introduced (Wu et al. 2024a). A distinctive feature of 3D-GRES is that the object referenced by the text may not exist or could be multiple objects, no longer restricted to a single object."}, {"title": "2.3 Prompt Learning", "content": "Prompt learning generally refers to the augmentation of models with specific prompt information. These prompts can be hand-crafted or automatically learned during the training process. Initially applied in the field of Natural Language Processing (NLP) (Lester, Al-Rfou, and Constant 2021; Li and Liang 2021; Liu et al. 2021), prompt learning has since been adapted for use in visual (Jia et al. 2022; Wang et al. 2022b; Zhang, Zhou, and Liu 2022) and vision-language (Zhou et al. 2022a,b; Zhu et al. 2023) models as well. In our model, we utilize a set of prompts generated under textual guidance to instruct the model in differentiating between relevant queries and irrelevant ones."}, {"title": "3 Method", "content": "In this section, we first introduce the inputs to the decoder, namely how visual features, textual features, and queries are obtained (Sec. 3.1). Secondly, we detail the Multi-view Semantic Embedding (MSE) strategy (Sec. 3.2). Then we describe the Prompt-aware Decoder (Sec. 3.3). Finally, we outline the loss function for the entire model (Sec. 3.4). An overview of our framework is shown in Fig. 2."}, {"title": "3.1 Feature Extraction", "content": "Textual Feature Given a textual description for the target object, we utilize a pre-trained ROBERTa (Liu et al. 2019) to extract word-level embeddings $E \\in \\mathbb{R}^{N_t \\times C_t}$, where $N_t$\ndenotes the number of tokens, and $C_t$ indicates the $C_t$-dimensionality of each embedding. In order to have a unified feature dimension $d$ in the decoder, we transform $E$ into textual features $T \\in \\mathbb{R}^{N_t \\times d}$ via a linear projection:\n$T = EW_t$,\nwhere $W_t \\in \\mathbb{R}^{C_t \\times d}$ are learnable parameters.\nVisual Feature Given a point cloud scene $P \\in \\mathbb{R}^{N_p \\times (3+f)}$, where $N_p$ denotes the number of points. Each point carries a 3D coordinate as well as an auxiliary feature vector of $f$ dimensions, such as RGB values and normal vectors. We first employ a Sparse 3D U-Net (Graham, Engelcke, and Van Der Maaten 2018) to extract point-wise features $F_{3d} \\in \\mathbb{R}^{N_p \\times C_p}$, where $C_p$ represents the feature dimensionality. Subsequently, following the approach of (Sun et al. 2023), we generate $N_s$ superpoints ${SP_i}^N {i=1}$ (Landrieu and Simonovsky 2018) from the original point cloud and perform superpoint pooling on $F_{3d}$ to obtain 3D superpoint features $S_{3d} \\in \\mathbb{R}^{N_s \\times C_p}$. Then, a multi-layer perceptron (MLP) is utilized to transform the dimensionality to $d$, yielding the 3D visual features $V_{3d} \\in \\mathbb{R}^{N_s \\times d}$.\n$V_{3d} = MLP(SPPool(F_{3d}))$,\nwhere $MLP(\\cdot)$ is a learnable multi-layer perceptron, and $SPPool(\\cdot)$ is superpoint pooling operation. The final visual feature $V \\in \\mathbb{R}^{N_s \\times d}$ is obtained by the sum of $V_{3d}$ and $V_{2d}$ (introduced in Sec. 3.2).\nSparse Query Generation After obtaining the visual features $V$ and textual features $T$, our next step is to utilize both to generate the queries that will be used in the decoder. Specifically, We first perform farthest point sampling (Moenning and Dodgson 2003) on the superpoints (correspond one-to-one with the visual features), followed by spatial-aware attention (introduced in Sec. 3.2), then resample the results using the sampling module from MDIN (Wu\net al. 2024a), and generate the queries through an MLP:\n$Q_{seed} = V[FPS(p_{sp})]$,\n$Q_o = MLP(Sample(SPA(Q_{seed}), T))$,\nwhere $FPS(\\cdot)$, $Sample(\\cdot)$ and $SPA(\\cdot)$ denote the Farthest Point Sampling algorithm, the sampling module in MDIN and the spatial-aware attention respectively, $[\\cdot]$ denotes accessing elements by the index within it, $p_{sp} \\in \\mathbb{R}^{N_s \\times 3}$ represents the coordinates of the superpoints, $Q_{seed} \\in \\mathbb{R}^{2m \\times d}$ is the seed query, and $Q_o \\in \\mathbb{R}^{m \\times d}$ ($m << N_s$) is initial query."}, {"title": "3.2 Multi-view Semantic Embedding", "content": "Multi-view Feature Extraction The 3D features extracted solely from point cloud data are limited in representational capacity due to the information loss of point clouds and insufficient alignment with the language modality. To address this issue, we propose a Multi-View Semantic Embedding (MSE) strategy. This approach enhances the visual features by extracting well-aligned multi-view semantics and injecting them back into the original 3D features through 2D-3D projection.\nSpecifically, given $N_I$ images ${I_i}^N {i=1}$ of the point cloud scene from different perspectives, we first extract patch-level 2D features using the CLIP (Radford et al. 2021) visual encoder, which is pre-aligned with visual-language tasks. To accommodate camera parameters, we upsample these features to the original image resolution via interpolation, resulting in pixel-level 2D features ${F_{img}^i \\in \\mathbb{R}^{H \\times W \\times C_1}} {i=1}^{N_I}$, where $C_1$ denotes the feature dimension, and $H$ and $W$ represent the height and width of the image, respectively. Next, we project the 2D pixel coordinates into the 3D point cloud space using the camera parameters. Similar to previous works (Wang et al. 2024; Yu et al. 2024; Peng et al. 2023; Zhang, Dong, and Ma 2023), for a pixel coordinate (u, v), given the intrinsic camera parameters $K \\in \\mathbb{R}^{3 \\times 3}$,\nextrinsic parameters $R \\in \\mathbb{R}^{3 \\times 3}$ and $T \\in \\mathbb{R}^{3 \\times 1}$, and depth $D \\in \\mathbb{R}$, we obtain the corresponding 3D coordinates (x, y, z) through 2D-3D projection:\n$\\begin{bmatrix}\nx \\\\\ny \\\\\n1\n\\end{bmatrix} = Project(u, v) = R(K^{-1}\\begin{bmatrix}\nu \\\\v\\\\1\\end{bmatrix} \\cdot D) + T$.\nAfter the projection, all 2D pixel features are assigned 3D coordinates $p_{3d} \\in \\mathbb{R}^{HWN_I \\times 3}$. To inject these multi-view features into the point cloud, we apply spherical querying to $p_{3d}$ in the point cloud scene. This technique assigns each pixel feature to the points within a sphere centered at its 3D coordinate, thus embedding multi-view semantic information. For points residing in multiple spheres, the final multi-view feature is computed as the average of the pixel features associated with that point. In this way, we obtain the multi-view semantic features $F_{2d} \\in \\mathbb{R}^{N_p \\times C_1}$ for all points, which are then processed similarly to $F_{3d}$ (Eq. 2) to derive the 2D visual features $V_{2d} \\in \\mathbb{R}^{N_s \\times d}$. Finally, we get the visual feature $V \\in \\mathbb{R}^{N_s \\times d}$ by summing the $V_{3d}$ and $V_{2d}$.\nSpatial-aware Attention While incorporating multi-view semantics improves visual representation and visual-language alignment, it also introduces limitations inherent to 2D images, such as the absence of spatial positional information and potential multi-view conflicts. Specifically, each image has a restricted field of view and lacks depth information, complicating the determination of 3D object positions and inter-object distances. To mitigate these issues, we use a spatial-aware attention mechanism to incorporate explicit 3D spatial relationships, enhancing spatial positioning.\nAdditionally, due to the high computational cost and inefficiency of operating directly at the superpoint level, we implement efficient spatial-aware attention on the sparse seed query $Q_{seed}$, which is more manageable on our GPU.\nFirst, we construct a k-nearest neighbor matrix $M \\in \\mathbb{R}^{2m \\times 2m}$, where the element $M_{ij}$ in the $i^{th}$ row and $j^{th}$ column indicates whether the $j^{th}$ query is among the $k$ nearest queries to the $i^{th}$ query. If the $j^{th}$ query is within the $k$ nearest neighbors of the $i^{th}$ query, $M_{ij}$ is set to True; otherwise, it is set to False. The coordinates of the queries are obtained from the corresponding superpoint coordinates. Then, we use $M$ as a mask to perform self-attention on the seed queries $Q_{seed}$, producing the output $Q \\in \\mathbb{R}^{2m \\times d}$ as the input to the sample module:\n$Q = SPA(Q_{seed}) = Masked \\_Self(Q_{seed}, M)$,\nwhere $Masked \\_Self(\\cdot)$ denotes the masked self-attention."}, {"title": "3.3 Prompt-aware Decoder", "content": "Previous query-based methods (Wu et al. 2024b; He and Ding 2024; Qian et al. 2024b) inherit the instance segmentation approach (Sun et al. 2023; Lai et al. 2023; Schult et al. 2023) to handling queries, which does not distinguish the importance of different queries. However, this approach is not well-suited for the 3D-RES task, which aims to segment objects indicated by text rather than all objects. This means that queries related to the text should be prioritized. To help the model better differentiate the importance of queries and reduce the learning difficulty, we introduce task-driven prompt learning in the decoder. By dynamically generating a set of text-relevant prompts, these prompts guide the model during the decoding process to identify which queries are more important and more likely to correspond to the target object.\nTask-driven Prompt To design reliable prompts tailored for 3D-RES task, we first measure the relevance between the text and queries using cross-attention scores. Specifically, we perform a cross-attention operation by using the text features $T$ as the query and the Sparse Queries $Q_l$ from the $l^{th}$ layer as the keys and values within the attention mechanism:\n$T_l, A_l = Cross(T, Q_l, Q_l)$,\nwhere $T_l \\in \\mathbb{R}^{N_t \\times d}$, $A_l \\in \\mathbb{R}^{N_t \\times m}$, and $Q_l \\in \\mathbb{R}^{m \\times d}$ denote the text features, attention scores, and sparse queries at the $l^{th}$ layer, respectively. $Cross(\\cdot)$ denotes the cross-attention operation (Vaswani et al. 2017). Then, by summing the attention scores $A_l$ across the first dimension, we initially obtain the relevance scores $Sc_l \\in \\mathbb{R}^{m}$ indicating how closely each query is associated with the given textual description. After obtaining the scores $Sc_l$, a intuitive approach would be to directly apply the Softmax function to determine the desired relevance. However, most queries are irrelevant to the text description, and their scores essentially act as noise, which should be minimized. To address this, we introduce a threshold filtering operation to filter out irrelevant queries as much as possible, making the prompts more reliable.\nSpecifically, we utilize the probability $Prob_{l-1} \\in \\mathbb{R}^{m}$, generated by the prediction head of the upper layer queries, to filter out queries. This probability represents the likelihood that a query corresponds to the target instance. For queries with probabilities below the threshold $r$, their relevance scores are set to negative infinity, meaning their values will be 0 after applying the Softmax function. Finally, the Softmax function is applied to the relevance scores, and the results are multiplied by the queries, producing prompts that guide the model in distinguishing between relevant and irrelevant queries. The process can be formulated as follows:\n$Sc^{j} {l} = \\begin{cases}\n-\\infty, Prob^{j} {l-1} < r \\\\\nS^{j} {cl}, Prob^{j} {l-1} \\geq r\n\\end{cases}$,\n$Pt_l = Q_l \\cdot Softmax(\\hat{Sc}_l)$,\n$Q_l = Concat(Q_l, Pt_l)$,\nwhere $r$ is a hyperparameter, $j$ denotes the $j^{th}$ element, $Pt_l \\in \\mathbb{R}^{m \\times d}$ represents the prompts, $Concat(\\cdot)$ denotes the Concatenation operation, $Q_l \\in \\mathbb{R}^{2m \\times d}$ stands for the queries with the prompts attached, and all subscripts $l$ indicate the $l^{th}$ layer.\nFeature Fusion & Prediction Head We follow the feature fusion method outlined in MDIN (Wu et al. 2024a), utilizing the query $Q_l$ to integrate the textual features $T_l$ and visual features $V$, thereby updating the queries under the guidance"}, {"title": "of the prompts. The specific formula is presented as follows:", "content": "$Q_l = Abandon(Cross(Q_l, T_l, T_l)\n+ Self(Q_l)\n+ Cross(Q_l, V, V))$,\nwhere $Self(.)$ denotes the self-attention operation, $Abandon(.)$, denotes the discarding of the prompts, and $Q_l \\in \\mathbb{R}^{m \\times d}$ represents the updated queries. Considering the difference in scale between the queries and superpoints, we apply Spatial-aware Attention once again at the end of each layer for feature enhancement and to generate the query $Q_{l+1}$ for the next layer:\n$Q_{l+1} = SPA(Q_l)$.\nBefore going to the next layer, $Q_{l+1}$ will pass through a prediction head to generate $Mask_l$ and $Prob_l$ in $l^{th}$ layer:\n$Mask_l = Q_{l+1}(VW_{mask})^T$,\n$Prob_l = Q_{l+1}W_{prob}$,\nwhere $W_{mask} \\in \\mathbb{R}^{d \\times d}$ and $W_{prob} \\in \\mathbb{R}^{d \\times 1}$ are learnable parameters, superscript $T$ indicates matrix transpose, $Mask_l \\in \\mathbb{R}^{m \\times N_s}$ represents the predicted masks for every query and $Prob_l \\in \\mathbb{R}^{m}$ represents the likelihood that a query corresponds to the target instance.\nFollowing MDIN (Wu et al. 2024a), we select the query with the highest Prob value and binarize its corresponding mask to generate the prediction during inference in the 3D-RES task. In the 3D-GRES task, we merge the binary masks of all queries with Prob values greater than 0.5 to produce the final prediction."}, {"title": "3.4 Loss", "content": "The loss of our method primarily consists of three components. The first component is the basic loss $L_b$, which is applied only on the queries corresponding to the target instance (Wu et al. 2024a):\n$L_b = BCE(M^+, M^{tgt}) + DICE(M^+, M^{tgt})$,\nwhere $M^+$ represents the mask output by the prediction head for the query corresponding to the target instance, $M^{tgt}$ is the ground truth mask for the target instance, $BCE(\\cdot)$ denotes the Binary Cross-Entropy loss, and $DICE(\\cdot)$ refers to the Dice loss (Milletari, Navab, and Ahmadi 2016).\nThe second part is the probability loss $L_p$, which is used to supervise $Prob$, that is, the probability associated with the query corresponding to the target instance:\n$L_p = BCE(Prob, L^{tgt})$,\nwhere the label $L^{tgt} \\in \\{0,1\\}^m$ indicates whether the query corresponds to the target instance, with 1 representing a positive match and 0 representing a negative match, and Prob is the probability output by the prediction head.\nThe third part is the contrastive learning loss $L_c$, which is used to align the text features with their corresponding queries. Here, we adopt the approach used in EDA (Wu et al. 2023).\nThe final loss $L$ is calculated as the weighted sum of $L_b$, $L_p$ and $L_c$:\n$L = \\lambda_b L_b + \\lambda_p L_p + \\lambda_c L_c$,\nwhere $\\lambda_b$, $\\lambda_p$ and $\\lambda_c$ are hyperparameters."}, {"title": "4 Expriments", "content": "In our experiments, we apply the PolyRL strategy to adjust the learning rate starting from 0.0001, with a decay power of 4.0. The batch size is set to 16. The number of queries $m$ is set to 128. The decoder consists of 6 layers. The hyperparameter k in sec.3.2 is set to 8, and the hyperparameter r in sec. 3.3 is 0.75. In the loss function, the weights $\\lambda_b$, $\\lambda_p$, and $\\lambda_c$ are set to 1.0, 0.1, and 0.1 respectively. All experiments are conducted using the PyTorch framework on an NVIDIA GeForce RTX 3090 GPU."}, {"title": "4.2 Dataset and Evaluation Metrics", "content": "ScanRefer We utilize the ScanRefer dataset (Chen, Chang, and Nie\u00dfner 2020) to evaluate our method, which consists of 51,583 natural language expressions, encompassing 11,046 objects across 800 ScanNet (Dai et al. 2017) scenes. The evaluation metrics include mean Intersection over Union (mIoU), Acc@0.25, and Acc@0.5.\nMulti3DRefer We use the Multi3DRefer (Zhang, Gong, and Chang 2023) dataset to evaluate our model's performance on the 3D-GRES task, which differs from 3D-RES in that the number of targets referenced by the text can be arbitrary. The dataset consists of a total of 61926 language descriptions, of which 51583 are directly obtained from ScanRefer. Among these, 6688 descriptions match zero targets, 13178 match multiple targets, and the rest match a single target. The evaluation metric is the same as that used in ScanRefer. When the text refers to zero object, the sample's mIoU is 1 if the model correctly identifies this, otherwise, it is 0."}, {"title": "4.3 Quantitative Comparison", "content": "As shown in Tab. 1, our model significantly outperforms the existing SOTA methods on the 3D-GRES task, achieving an improvement of 4.2 points in mIoU and even 5.3 points in Acc@0.5. It can be observed that, apart from the zero target scenario with distractors where our model performs below, it significantly surpasses the MDIN (Wu et al. 2024a) in all other cases. Especially in single-target scenarios, without distractors, our model outperforms the MDIN by nearly eight points on the Acc@0.5 metric. This demonstrates that our task-driven prompt effectively guides the model to focus on more significant queries, thereby enabling more accurate localization of key targets.\nWe also conducted experiments on the traditional 3D-RES task, as shown in Table 2. On the ScanRefer dataset, our proposed IPDN model achieved state-of-the-art performance overall. Specifically, our model outperformed the previous best model, MDIN (Wu et al. 2024a), by 2.6, 1.8, and 1.9 points in terms of Acc@0.25, Acc@0.5, and mIoU, respectively. Notably, we observed more significant improvements in challenging scenes with multiple distracting objects. This indicates that our model benefits from more robust multi-view semantic integration and the task-driven prompt, which effectively guides the model to focus on more critical information, thereby enhancing its discriminative ability to accurately identify the target object among multiple instances of the same category.\nThanks to the integration of well-established large-scale pre-trained models from the 2D domain within the MSE module, the visual representations in our model are more robust, enabling it to perform reliably even on rarely seen classes in the training set. To validate this, inspired by (Rozenberszki, Litany, and Dai 2022; Yan et al. 2024; Lu et al. 2023b; Takmaz et al. 2023), We categorized object classes based on their frequency of appearance in the training set and conducted testing accordingly, as shown in Tab. 3. Specifically, we categorized all target classes in ScanRefer into three groups. The first group, labeled \"High\", consists of classes that make up more than 1% of the training set, accounting for approximately 75% of the total samples. The second group, labeled \u201cMid\u201d, includes classes that comprise less than 1% but more than 0.1% of the training set, representing about 20%. The remaining classes, labeled \"Low\", make up less than 0.1% of the training set and account for about 5% of the samples.\nAs shown, the performance of 3D-STMN (Wu et al. 2024b) and MDIN (Wu et al. 2024a) significantly drops for the \"Low\" frequency categories, decreasing by 17.5 and 14.9 points, respectively, compared to the \u201cHigh\u201d group. In contrast, our model shows a decrease of only 6.3 points. When comparing across models, our model outperforms MDIN by more than 10 points in the \"Low\" group. This substantial improvement highlights the enhanced robustness of our model, attributed to the multi-view semantic integration, enabling it to handle infrequent, long-tail samples effectively."}, {"title": "4.4 Ablation Study", "content": "All of our ablation experiments were conducted on ScanRefer dataset (Chen, Chang, and Nie\u00dfner 2020).\nComponent Ablation In our proposed IPDN, the main components include MSE and PAD. To assess the impact of these two components, we conducted an ablation study, as shown in Tab. 4. The results indicate that omitting both components results in a 2.1-point decrease in mIoU. Introducing MSE improves mIoU by 1.1 points, demonstrating its effectiveness in enhancing visual features. Further inclusion of PAD leads to an additional 1.0-point increase in mIoU, indicating that task-driven prompts effectively guide the model to focus on more important queries, thereby improving segmentation accuracy.\nSpatial-aware Attention Ablation We conducted an ablation study on the hyperparameter k in the Spatial-aware"}, {"title": "4.5 Qualitative Comparison", "content": "In this section, we visualized a set of representative examples in ScanRefer dataset, as shown in Fig. 3. It can be seen\nfrom the figure that our model demonstrates stronger reasoning capabilities compared to MDIN (Wu et al. 2024a). Specifically, in case (a), there is no distracting object present in the scene, only a vending machine, but MDIN still fails to identify it. This is because, within the ScanRefer training dataset of over 30,000 samples, there are only 10 samples where the target is a vending machine, which is insufficient for the model to recognize such an object. However, models with large-scale 2D pre-training do not suffer from this issue and can well identify vending machines, allowing our model to accurately locate the target. In case (b), the concept of \"left\" is involved, which is perspective-dependent. Since three-dimensional space theoretically contains an infinite number of perspectives, purely 3D models have difficulty distinguishing left from right. In contrast, the perspective in 2D images is fixed, which provides significant assistance in handling such cases. Finally, in case (c), thanks to the powerful prompting ability of our task-driven prompts, even when there are nearly ten distractor objects present, our model can still accurately locate the target object."}, {"title": "5 Conclusion", "content": "In this paper, we focus on addressing feature ambiguity and intent ambiguity by introducing the Image-enhanced Prompt Decoding Network (IPDN). To overcome feature ambiguity, we propose the Multi-view Semantic Embedding (MSE) module, which incorporates multi-view 2D image information into the 3D scene, compensating for any potential spatial information loss. To resolve intent ambiguity, we developed the Prompt-Aware Decoder (PAD), which guides the decoding process by generating task-driven signals from the interaction between the expression and visual features. Extensive experiments demonstrate the superiority of IPDN."}]}