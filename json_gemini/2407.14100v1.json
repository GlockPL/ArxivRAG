{"title": "ParamsDrag: Interactive Parameter Space Exploration via Image-Space Dragging", "authors": ["Guan Li", "Yang Liu", "Guihua Shan", "Shiyu Cheng", "Weiqun Cao", "Junpeng Wang", "Ko-Chih Wang"], "abstract": "Numerical simulation serves as a cornerstone in scientific modeling, yet the process of fine-tuning simulation parameters poses significant challenges. Conventionally, parameter adjustment relies on extensive numerical simulations, data analysis, and expert insights, resulting in substantial computational costs and low efficiency. The emergence of deep learning in recent years has provided promising avenues for more efficient exploration of parameter spaces. However, existing approaches often lack intuitive methods for precise parameter adjustment and optimization. To tackle these challenges, we introduce ParamsDrag, a model that facilitates parameter space exploration through direct interaction with visualizations. Inspired by DragGAN, our ParamsDrag model operates in three steps. First, the generative component of ParamsDrag generates visualizations based on the input simulation parameters. Second, by directly dragging structure-related features in the visualizations, users can intuitively understand the controlling effect of different parameters. Third, with the understanding from the earlier step, users can steer ParamsDrag to produce dynamic visual outcomes. Through experiments conducted on real-world simulations and comparisons with state-of-the-art deep learning-based approaches, we demonstrate the efficacy of our solution.", "sections": [{"title": "1 INTRODUCTION", "content": "Numerical simulations are integral to modern scientific inquiry, bridging theoretical models and real-world phenomena. These simulations enable scientists to investigate complex physical processes, verify scientific hypotheses, and refine the underlying physical models. In scientific research, the iterative process of conducting multiple simulation runs with varying simulation parameter conditions is essential for exploring the uncertainties inherent in physical parameters. For instance, in cosmological simulations, by performing a multitude of simulations and analyzing under a wide range of initial conditions, scientists can gain insights into the formation and evolution of the universe and the impact of different initial conditions on structural formation.\nHowever, optimizing simulation parameters is a complex and computationally resource-intensive task. Each parameter adjustment requires rerunning the simulation and evaluating its effects, a process that demands significant computational power and substantial storage for the resulting data. Furthermore, exploring valuable insights from these extensive datasets necessitates visualization and in-depth analysis of each simulation run's outcomes. Scientists refine simulation parameters by analyzing the key features and structures revealed in these results, moving closer to achieving their research goals. For example, scientists often employ simulations to repeat the formation of galaxy clusters or dark matter halos in studying the universe's large-scale structures. They may need to repeatedly adjust initial conditions and other relevant parameters to produce structures and physical properties corresponding to observational data. This process includes not only fine-tuning the parameters but also continuously monitoring the simulation outcomes. Scientists usually require this process to adjust simulation parameters to facilitate the formation of the anticipated structures.\nThe advent of deep learning models has introduced novel approaches to parameter exploration in numerical simulations. Recent research has introduced surrogate models, such as InSituNet [13], GNN-Surrogate [36], and VDL-Surrogate [35], leveraging deep neural networks to boost the efficiency of this exploration. These models employ deep learning to establish a mapping from initial simulation parameters to the ultimate outcomes, enabling direct result prediction and circumventing the need for time-consuming numerical simulations. However, these methods have yet to address a critical issue: intuitively interacting with predicted outcomes and retrieving the input simulation parameters corresponding to the outcomes. Existing surrogate models primarily aim to predict outcomes based on parameter adjustments. If the predicted results fall short of expectations, experts must infer the direction of parameter adjustments by drawing on their expertise. This situation leaves the parameter adjustment process needing a more precise and controllable method, thereby suggesting that the efficiency of exploring the parameter space needs enhancement.\nTo support the intuitive simulation parameter exploration, we propose ParamsDrag, a model that facilitates parameter space exploration through direct interaction with visualizations. Our work has two main goals: (1) to enable scientists to drag a feature of interest to the desired location on the visualization image and generate the corresponding image; (2) to retrieve the corresponding simulation parameter conditions of the generated images. Inspired by DragGAN [29], a technique to support interactive manipulation on classic image generation applications, we propose an approach that aims to properly edit the latent vector of our deep-learning-based surrogate model to support scientists dragging a feature on the visualization and then generate the corresponding image. However, a fundamental difference between applications of classic image generation and scientific data visualization generation leads to a few technical challenges. The difference is that the scientific visualization generation focuses on the accuracy of the generated visualization corresponding to the provided simulation parameter condition. Conversely, classic image generation applications focus on producing natural and sharp images, making it challenging to abstract comprehensive and quantifiable labels. Our study shows that the valid latent vectors of a well-trained scientific visualization generation model are distributed more discretely than classic image generation models. This phenomenon prohibits directly editing latent vectors as the classic image generation model in the latent space. In our work, we develop a deep-learning-based surrogate model that takes simulation parameters as input and generates the corresponding scientific visualization. Furthermore, we propose a gradient descent-based parameter search algorithm for editing the latent vector, enabling stepwise movement only on valid points in the discrete latent space. This algorithm ensures that only valid images are generated and also guarantees smooth transitions between images as scientists manipulate features on the image. Lastly, a function is provided to retrieve the corresponding simulation parameters of any generated image as scientists adjust features on the visualization. Overall, this work makes the following four contributions:\n\u2022 We elucidate the differences in latent vector distributions and the variations in editing methodologies between classic image generation and scientific visualization image generation.\n\u2022 We design a deep-learning-based surrogate model that inputs simulation parameters to generate corresponding scientific data visualizations efficiently.\n\u2022 We define the structure-based patch and implement direct interaction with visualization through feature supervision and tracking.\n\u2022 We implement a gradient descent-based parameter search method to support the generation of dynamic images and the inversion of corresponding parameters for visualization images."}, {"title": "2 RELATED WORK", "content": "This section reviews the relevant work on applying deep learning in scientific visualization and analyzing the parameter space in numerical simulations."}, {"title": "2.1 Deep Learning for Scientific Visualization", "content": "Deep learning techniques are increasingly demonstrating their significant potential in scientific visualization. Much research has utilized deep learning to address challenges within scientific visualization, achieving good results. The tasks related to our study primarily fall into prediction and visualization generation [43].\nPrediction primarily involves using deep learning models to learn from existing data distributions and forecast new data instances. For instance, in data prediction, CECAV-DNN [12] employed a DNN model to predict ensemble similarity, facilitating the analysis of ensemble data. Tkachev et al. [41] utilized neural networks to predict voxel values and identify anomalous data. Both Hong et al. [16] and Han et al. [9] applied deep learning models for predicting particle-related data. Regarding visualization prediction, the works of Yang et al. [50] and Shi et al. [34] leveraged neural networks to predict visualization images under new parameters, supporting the evaluation of viewpoint parameters. Engel and Ropinski [4] introduced a deep-learning approach for predicting per-voxel ambient occlusion in volumetric datasets within direct volume rendering.\nThe task of visualization generation involves using neural networks to learn the mapping from input parameters to visualization outputs, thereby enabling the generation of visualization results under new parameters. Its objective is to replace traditional, potentially cumbersome processes, thus enhancing the efficiency of exploration and analysis. This process encompasses different types of input parameters, such as simulation parameters, viewpoint parameters, and visual mapping parameters. For example, InSituNet [13], GNN-Surrogate [36], and VDL-Surrogate [35] have all replaced simulation programs with deep learning models to improve the efficiency of parameter space exploration, focusing primarily on simulation parameters. In viewpoint parameters, DNN-VolVis [15] achieved rapid rendering under new viewpoint parameters, while Berger et al. [3] utilized GANs to acquire visualization images from new viewpoints quickly. Regarding visual mapping parameters, DeepDVR [46] generated similar direct volume rendering results from examples in image space, eliminating the need for explicit feature design and manual transfer function specification. Additionally, Weiss et al. [47] employed FRVSR-Net to transform low-resolution isosurface rendering images into high-resolution ones, and in [48], they explored the relationship between data and image generation using an end-to-end neural rendering framework.\nThe first goal of our work is to utilize deep neural networks to predict the visualization results from parameters. These related studies provide a solid theoretical foundation for our work, offering valuable insights for model design and loss function construction."}, {"title": "2.2 Parameter Space Analysis", "content": "Visualization techniques are increasingly playing a pivotal role in the analysis of Parameter Spaces [44]. Existing related work can be technically categorized into two types: the first is traditional parameter analysis methods, and the second is surrogate model approaches [35].\nIn traditional analysis methods, the process typically starts with running simulation programs multiple times and then collecting the simulation data for subsequent visual analysis. In this paradigm, visualization focuses on exploring better design or data processing algorithms to support uncertainty visualization. For instance, employing curve-related visual forms to analyze spatiotemporal uncertainty in ensemble data [6, 7, 24, 49], modeling data uncertainty using probability density functions for further visual analysis [2, 10, 22, 28, 30, 32, 40], and some studies explore visualizing high-dimensional data to support uncertainty analysis, such as scatter plots [23, 27, 38], parallel coordinate plots [26, 45], and matrix charts [17, 31, 33]. However, a significant drawback of traditional methods was their substantial computational resource requirements to support the running of simulation programs, leading to inefficient parameter analysis and optimization.\nMachine learning methods provided new approaches to exploring parameter spaces, with the use of machine learning models to replace simulation programs significantly enhancing the efficiency of parameter exploration. For instance, NNVA [11] utilized a neural network-based surrogate model to facilitate the interactive exploration of high-dimensional input parameter spaces in yeast cell polarization simulations. Alden et al. [1] employed surrogate models to efficiently analyze biological systems simulations, enabling advanced statistical analyses and optimizations that were previously infeasible due to computational limitations. Similarly, Erdal et al. [5] applied Gaussian Process Emulators to enhance the efficiency of sampling in ensemble-based sensitivity analysis of environmental models, demonstrating a substantial improvement in identifying behavioral samples and local sensitivities. Furthermore, research closely aligned with our work includes InSituNet [13], GNN-Surrogate [36], and VDL-Surrogate [35], all of which utilize neural networks to substitute for numerical simulation programs. These related studies focused on adjusting parameters to modify the final outcomes, yet they often required multiple modifications and iterative analyses when the predicted results failed to meet expectations. This process still largely depended on expert intuition for parameter adjustments, indicating that there is still room for improvement in the efficiency of exploring the parameter space."}, {"title": "3 BACKGROUND", "content": "Generative models constitute a distinct class within the area of deep learning, fundamentally focused on learning data distributions. These models are capable of generating new data instances by learning the data distribution. Image generation is one of the tasks for generative models, which aims to create high-quality and lifelike images. Among the models frequently used for this purpose are Autoencoders (AE) [14], Variational Autoencoders (VAE) [21], and Generative Adversarial Networks (GANs) [8]. The task of generating scientific visualization images differs from classic generative tasks.\nIn computer vision, classic generative tasks primarily aim to produce images from the real world. However, it is difficult to assign descriptions with precise labels to these images, leading to the reliance on ambiguous labels or introducing noise as input. For instance, within GANs, random noise is employed as the initial input to the generator. This noise undergoes several transformations across the network's layers to yield samples that mimic the appearance of images from the actual dataset. Furthermore, in the case of StyleGAN [19] and Style-GAN2 [20], noise is not merely an input for the generator but also plays a crucial role in refining the latent space vectors. This application of noise serves to inject random details into the images, thus enhancing the model's ability to generate diverse data. Although incorporating noise in the latent space can enrich the model's generative, an input label can potentially map and generate multiple distinct images."}, {"title": "4 OVERVIEW", "content": "Our objective is to enable experts to interact with predicted images and get the corresponding simulation parameters, thus improving the efficiency of optimizing simulation parameters. Therefore, a generator is required to predict visualization outcomes by input simulation parameters. Based on the generator, we also propose an algorithm that enables users to intuitively drag the feature of interest on the image to generate the corresponding visualization and derive simulation parameters.\nFigure 1 illustrates the workflow of ParamsDrag, which integrates three main components: the construction of the dataset, the training of the generator, and interaction with the visualization image. During the phase of constructing the dataset, we visualize simulation data and store both the simulation parameters and visualization parameters associated with each image to build the dataset. In the subsequent phase, we refine the generator's architecture through a state-of-the-art image generation model to fit our scientific visualization purpose. The generator is trained to learn the mapping between the simulation parameters and visualization images using the above dataset. The trained generator can predict the image of given simulation parameters as input. The final phase is interactive image prediction and simulation parameter derivation through user interactions. We define the concept of the structure-based patch, which enables users to select features of interest within the visualizations. Upon interaction, feature supervision and feature tracking guide the generator in producing an image sequence, which illustrates the transition before and after user interactions. A gradient-based search technique is also proposed to derive the parameters associated with each image. By integrating these features, ParamsDrag model implements a novel approach to parameter space exploration that enables direct interaction with the predicted visualization images.\nAdditionally, we also demonstrate the differences in the generative models' latent vector distributions under applications of classic image generation and scientific visualization generation. We show how these differences impact the strategies for editing latent space vectors. This exploration reveals the rationale behind our model's structural design, showcasing how distinct approaches to handling latent space can impact model performance and editing capabilities, particularly in scenarios with scientific simulation parameter space exploration."}, {"title": "5 ParamsDrag MODEL", "content": "This section introduces the technical details of ParamsDrag model."}, {"title": "5.1 Dataset Construction", "content": "The process of constructing the training set commences with the identification of simulation parameters based on expert analysis objectives. After this, these parameters are sampled, followed by the execution of numerical simulations for each parameter set, with the data being recorded. The final step involves employing visualization algorithms on the simulation data to convert this data into images. The data to be stored throughout this procedure includes:\nSimulation parameters, formatted as one-dimensional vectors, encompassing physical parameters, time, and other variables pertinent to the simulation program.\nVisualization parameters, also in one-dimensional vectors, include aspects such as viewpoint, threshold, and other parameters relevant to the visualization algorithm.\nVisualization images, formatted as portable network graphics (PNG), which are generated from the simulation data through visualization algorithms and correspond to the simulation and visualization parameters."}, {"title": "5.2 Generator", "content": "In this subsection, we introduce our generator's architecture and loss function, which is designed to support latent vector editing and accurate visualization image prediction based on parameters."}, {"title": "5.2.1 Generator Architecture", "content": "Figure 2 displays the architecture of the generator, where the input consists of simulation parameters and visualization parameters, and the output is a visualization image. It comprises three sub-networks: Param Subnet, Mapping Subnet, and Synthesis Subnet. We reference multiple state-of-the-art models, such as StyleGAN2 and InSituNet, to guide our network design in the model's design process.\nThe Param Subnet primarily focuses on extracting features from the input parameters and transforming them into a format usable for the generation process. It comprises two branches: one for handling simulation parameters and the other for dealing with visualization parameters. Each branch features two fully connected layers (FC), designated as FC(m,512) and FC(n,512), where m and n represent the dimensions of different types of parameters. We use ReLU [25] as the activation function in all FC layers. The outputs of these two branches are then merged into a single vector through a concatenation operation, which is further processed by another fully connected layer, FC(1024,512). The output of this subnet is subsequently provided to the Mapping Subnet.\nThe Mapping Subnet serves to transform the output of the Param Subnet into a latent space w, which often possesses better attributes than the original parameter space, such as being easier for subsequent networks to learn and disentangle. The Mapping Subnet typically consists of multiple fully connected layers, with each layer in our model being FC(512,512), and multiple such layers can be stacked together. The purpose of each layer is to transform the input features, enhancing their distribution characteristics gradually. This sequence of FC and ReLU layers enables the network to learn complex nonlinear mappings, ultimately producing the latent vector w.\nThe Synthesis Subnet generates the final image from the latent vector w, which is output by the Mapping Subnet. In developing the Synthesis Subnet, we drew inspiration from the architectural framework of StyleGAN2 and made custom modifications, notably omitting noise injection components. This design change is necessary to ensure that one given parameter input does not produce two distinct visualizations. The details are discussed in Section 6. The Synthesis Subnet commences with a constant tensor, Const(4x4), as the initial feature map, which then undergoes progressive transformations via multiple Synthesis blocks to incrementally enhance the image resolution. Within each Synthesis block, an upsampling procedure doubles the feature map's dimensions, followed by convolutional operations refining the features. Moreover, the Synthesis blocks incorporate Modulation (Mod) and Demodulation (Demod) operations. Modulation dynamically adjusts the convolutional kernels using the latent vector w, enabling the network to control the representation of image features at various levels. Demodulation normalizes the modulated weights by computing a standardization coefficient, thus mitigating the risk of excessive activation values resulting from the modulation process. The definitions and implementations of Mod and Demod align with those delineated in StyleGAN2 [20]. These collaborative operations harness the information within w to direct the image synthesis process, culminating in the generating of images with high-quality details."}, {"title": "5.2.2 Loss Function", "content": "The objective of the loss function constraints is to quantify the differences between the images produced by the generator and the target images, thereby guiding the generator's training. In this work, we employ three loss functions: content loss (Lcontent), perceptual loss [18] (Lfeature), and edge loss (Ledge), as is shown in Equation 1.\n$L = \\alpha L_{content} + \\beta L_{feature} + \\gamma L_{edge}$ (1)\nwhere \\alpha, \\beta, and \\gamma are the coefficients.\nThe content loss measures the pixel-wise difference between the generated and target images. We employ the L1 loss function to fulfill this purpose, as depicted in Equation 2.\n$L_{content} = \\frac{1}{N} \\sum_{i=1}^{N} |I_i - \\hat{I}_i|$ (2)"}, {"title": "5.3 Interaction with Visualization Images", "content": "In the interaction part, we draw inspiration from DragGAN [29] to implement the interactive functionality of ParamsDrag. Direct application of DragGAN to our generative models for scientific visualization images is impracticable due to the discrete distribution of the model's latent vectors in the latent space, a consequence of explicit label constraints. To address this issue, we improve the method of latent vector editing in DragGAN, enabling our model to perform stepwise changes in the latent space, thereby generating accurate visualization images. The theoretical foundation of our approach is discussed in Section 6.\nTherefore, a single iteration comprises three critical steps: Feature Supervision, Parameter Inversion, and Feature Tracking. This process repeats until the termination conditions are met. Our method includes two termination conditions: the interactive object reaching the target position and the structural disappearance of the interactive object."}, {"title": "5.3.1 Structure-based Patch", "content": "We introduce the concept of the structure-based patch, with the primary goal of assisting users in identifying and selecting specific structures within scientific visualization images. Scientific visualization involves converting data attributes into different colors to represent attribute ranges, where multiple pixels of similar colors form a small region. Pixels in this region, having similar attribute values, can be defined as a structure in many domains. For example, a dark halo can be represented as a highlighted local area within the visualization image in cosmological simulations. Therefore, we utilize the structure-based patch to help users locate structures within visualization images and support further editing of these structures.\nThe structure-based patch is implemented through a breadth-first search algorithm based on the click location by calculating the pixel similarity d within a radius r of the user's click position. Our method also supports simultaneous selection and editing of multiple feature structures. For any given click position pi, the search algorithm is used to construct the pixel set Patch(pi). Additionally, a moving target position gi is set for each click position pi to guide the direction of the structure's movement. In our implementation, the calculation of structural similarity is conducted under the HSV color model."}, {"title": "5.3.2 Feature Supervision", "content": "The objective of feature supervision is to help the movement of selected structures within the visualization image. The essence of feature supervision is to minimize structural changes within the visualization image after movement, ensuring continuous local feature changes in the image. Therefore, if the feature's corresponding set of pixels Patch(pi) is to be moved to a target point gi, this can be achieved by supervising the patch to move a small step rm towards gi each time, and then through multiple iterations to accomplish the feature movement. During this process, we employ the loss function for feature supervision from DragGAN, as shown in Equation 5.\n$L_{MS} = \\sum_{i=0}^{n} \\sum_{q_i \\in Patch(p_i)} ||F(q_i) - F(q_i + v_i)||_1$ (5)"}, {"title": "5.3.3 Parameter Inversion", "content": "DragGAN utilizes the first six latent vectors of the W space from Style-GAN2 for gradient search to achieve image editing. The distribution of latent vectors in the latent space for ParamsDrag's generator differs from that of StyleGAN2's generator, and using a latent vector search approach would result in erroneous images. To address this issue, we opt for modifications in the parameter space to ensure the validity of the generated images. Specifically, we use Equation 5 as the loss function to quantify the differences before and after the patch movement, and then perform a gradient search on the input parameters to achieve editing of the latent vectors. The theoretical foundation for this approach is discussed in detail in Section 6.\nWe implement image editing through gradient search on input parameters, allowing the corresponding parameters of newly generated images to be acquired in real time. Furthermore, based on the diverse requirements of experts, we can conduct gradient searches on multiple input parameters or a single parameter to explore the parameter space under various analytical objectives."}, {"title": "5.3.4 Feature Tracking", "content": "The objective of feature tracking is to address the issue of inaccurately tracking control points after the target has moved while also determining whether the feature structure has disappeared. Feature tracking is accomplished through the nearest neighbor search of feature maps, as illustrated in Equation 7.\n$p_i = \\underset{q_i \\in SSquare(p_i, r_m)}{min} ||F'(q_i) - F_0(p)||_1$ (7)\nwhere Fo() represents the initial feature map, $p_0$ denotes the initial position selected by the user, and F'() is the current feature map. Therefore, the position pi is updated to the point within the square region with side length rm around pi in the current feature map that is most similar to the initial point po. After getting the new control points pi, we calculate the feature disappearance using Equation 8.\n$D = ||I_i(p_i) - I_0(p_i)||$ (8)\nwhere Io() represents the initial image, I\u00a1() represents current image. We terminate the iteration when the D exceeds the threshold."}, {"title": "6 THEORETICAL FOUNDATION", "content": "Our work references the image editing method employed by DragGAN, but directly applying this approach to our generator encountered issues. DragGAN generates images by manipulating the generator's latent space vector W. The generator used in DragGAN is derived from StyleGAN2, which not only utilizes noise as input but also introduces noise into the latent space for diversity. Applying DragGAN's latent space editing technique to our generator resulting in generated images collapsing after only a few iterative modifications to the latent vectors, as demonstrated in Figure 4. In this section, we discuss and provide evidence for the differences in the latent vector distributions and latent vector editing between two models on the same dataset."}, {"title": "6.1 Differences in Latent Vector Distributions", "content": "ParamsDrag exhibits a distinct distribution of latent vectors compared to StyleGAN2, necessitating different approaches for latent space manipulation. ParamsDrag employs explicit input parameters to generate visualization images, establishing a one-to-one correspondence between input and output. Consequently, each individual training data point is associated with a unique vector in the latent space. In contrast, StyleGAN2 employs noise augmentation within the latent vectors, resulting in multiple latent vectors corresponding to a single input parameter, thereby complicating the mapping from input to latent representation. Figure 5 provides a schematic representation of the latent vector distributions for both models, where each dot within the figure represents a sample from the training data. Dots of the same color correspond to the same data sample, with a total of three samples depicted. It is evident from the figure that each of the three samples from ParamsDrag correlates to a unique latent vector within the latent space. Conversely, StyleGAN2's approach results in each of the three samples correlating to clusters of latent vectors within the latent space. Moreover, methodologies such as InSituNet, GNN-Surrogate, and VDL-Surrogate similarly adopt explicit labeling strategies to generate images, aligning their latent space vector distributions closely with that of our model."}, {"title": "6.2 Distinctions in Editing Latent Vectors", "content": "The DragGAN approach operates by modifying the latent space vector W to generate new images, utilizing the generator from StyleGAN2. As illustrated in Figure 5, the latent vectors of StyleGAN2 exhibit a tendency toward continuity within the latent space, whereas the latent vectors from ParamsDrag are discrete. The original DragGAN method proposes a continuous transition from a starting latent vector to a target point; however, such a process under ParamsDrag's discrete latent space may lead to transitions to invalid points. Figure 6 demonstrates the differences in pass-through of the valid points for continuous editing of latent vectors between the two models. It reveals that continuous modifications in a model characterized by a discrete distribution of latent vectors result in transitions to invalid points, producing erroneous images. Conversely, StyleGAN2 features a continuous distribution of latent vectors, enabling seamless and coherent changes within the latent space.\nOverall, generative models under explicit parameter constraints do not continuously edit and move the vector in the latent space. The latent space of ParamsDrag exhibits a discrete distribution of valid points. Consequently, a straightforward approach is to develop stepwise transitions between these discrete points within the latent space. Thus, a straightforward strategy is to facilitate stepwise transitions between these valid points, enabling the latent vectors to change in a leapfrog manner. This approach ensures the validity of the generated images. After training, the ParamsDrag can accurately predict visual images based on parameters and provides precise predictions for any input parameters within a reasonable range. This demonstrates that the model can effectively learn the mapping relationship from parameters to visual images, with each input corresponding to a point in the latent space with generalization capabilities. Therefore, we can modify the input parameters to induce leapfrog changes in the latent vectors. By implementing a gradient descent search algorithm for input parameters under the constraints of feature supervision and motion supervision, we can effectively solve the challenge of editing latent vectors in a discrete latent space. The theory above guided us in designing the method for editing latent vectors in ParamsDrag."}, {"title": "6.3 Validation on Real Datasets", "content": "To substantiate the validity of the aforementioned theory, we present a visualization of the latent space distribution on a real dataset, as depicted in Figure 7. This figure displays the distribution of hidden vectors for 10 data samples under two different models. The scatter points in the figure represent the latent vectors, and we employ the Multidimensional Scaling (MDS) algorithm to reduce dimensionality and visualize the distribution. In Figure 7, each point on the left graph represents a latent vector corresponding to an input in ParamsDrag model, whereas the right graph displays the latent vectors for the same inputs within StyleGAN2 model. An observation from the figure is that ParamsDrag model's latent vectors are distributed discretely, in contrast to StyleGAN2 model, where latent vectors are clustered. Moreover, as the number of data increases, the latent vectors in StyleGAN2 model tend to form a continuum.\nWe also visualize the distribution of latent vectors corresponding to the same training data, test data, in-range input sampling data, and out-of-range input sampling data for both ParamsDrag and InSituNet, as shown in Figure 8. The algorithm used for visualizing latent vectors in Figure 8 employs the MDS algorithm. In numerical simulations, physical parameters have specific meanings and ranges, and inputs within these correct parameter ranges are defined as valid inputs. Through experimentation, it is observed that images generated from valid inputs are accurate. Sampling points that exceed the range are obtained by sampling far beyond the reasonable parameter limits, and their outputs are typically blurry or chaotic images. It is evident from the figure that the latent vector distributions of both ParamsDrag and InSituNet models are discrete. The latent vectors corresponding to the test dataset are distributed close to those of the training dataset, and similarly, the latent vectors for the in-range input sampling points are also situated near the training dataset's latent vectors. The distribution of the out-of-range input sampling points is more widespread.\nIn summary, the valid inputs are clustered around the latent vectors of the training data, yet not all points near these latent vectors are effective. Slight changes in the latent vectors can easily turn them into invalid points. Moreover, if the range of parameters in the training set is much smaller than the actual range of those parameters, this can cause inputs within the normal range of the parameter to become invalid. Therefore, ensuring that the training dataset covers the actual range of the physical parameters is also necessary."}, {"title": "7 EXPERIMENTS", "content": "In this section, we first introduce the experimental setup and then show the effectiveness of our approach."}, {"title": "7.1 Experimental Setup", "content": ""}, {"title": "7.1.1 Simulation Datasets and Parameters", "content": "In the experiment, we utilized two datasets. The first dataset is the Red Sea simulation, derived from the MIT Ocean general circulation model [42]. The second dataset involves the simulation of cosmic large-scale structures sourced from the Gadget-2 simulation program [39]. The parameter configurations are described below.\nRed Sea Simulation: This simulation simulates the circulation dynamics of the Red Sea area. The spatial and temporal resolutions are 500 \u00d7 500 \u00d7 50 and 60 time steps, respectively. We selected three parameters to conduct the experiments: Viscosity \u2208 [1.0,4.0], Time \u2208 [0,60], and Depth \u2208 [0, 50]. Furthermore, we applied the contour line algorithm to visualize the temperature of each layer to produce images with 256 \u00d7 256 resolution, where the visualization parameter is fixed throughout the experiment. We uniformly sampled 4, 30, and 50 values from viscosity, time, and depth parameters to produce 6000 simulation parameter combinations and render the corresponding images. For these data, we randomly selected 5000 as the training dataset and 1000 as the test dataset. During the interaction, the structural similarity parameter d is set to 95%, the structural radius parameter r is set to 3 pixels, the structural displacement parameter rm is set to 2 pixels, and the disappearance similarity parameter D is set to 95%.\nCosmological Simulation: The cosmological simulation employs particles to simulate the evolution of large-scale structures over 13.7 billion years in the universe. The parameters and corresponding ranges are Omega \u2208 [0.05,1.0], representing matter proportion; OmegaLambda \u2208 [0,0.95], denoting dark energy proportion; and Sigma8 \u2208 [0.6, 1.0], indicating the power spectrum normalization. We applied a visualization algorithm to convert the particle data into a grid format and output volumetric data with a resolution of 500 \u00d7 500\u00d7500. Then, we used volume rendering to generate global visualizations at a resolution of 512 \u00d7 512 and surface rendering to generate regional visualizations with the same resolution. We uniformly sampled 475 simulation parameter combinations from the parameter space created by the three physical parameters to produce corresponding images. Subsequently, we randomly selected 380 for the training dataset and 95 for the test dataset. For the interaction parameters, the structural similarity parameter d is set to 95%, the structural radius parameter r is set to 5 pixels, the structural displacement parameter rm is set to 8 pixels, and the disappearance similarity parameter D is set to 95%."}, {"title": "7.1.2 Experimental Environment and Computational Performance", "content": "In the experiments, we utilized a server equipped with two Intel(R) Xeon(R) Gold 6248 processors, two NVIDIA V100 graphics cards, and 128GB of system memory as the environment for model training and prediction. The neural network model implementation is based on the PyTorch Library. Our experiments employ the Red Sea dataset and the cosmological simulation dataset. For the Red Sea dataset, our model requires 16 hours for training. For the cosmological simulation dataset, our model training takes 7 hours. The interaction with visualization images is an iterative process, where the speed of each iteration is solely dependent on the image resolution. For images with a resolution of 256 x 256 from the Red Sea dataset, an iteration takes 185ms on average, whereas for images with a resolution of 512 \u00d7 512 from the cosmological simulation dataset, an iteration requires 390ms on average. The number of iterations needed for one interaction is related to the moving distance of the structure. In our cases, at least 10 iterations are necessary, with a maximum of up to 130 iterations. During the iterative process, the ability to display intermediate images in real-time ensures that users do not perceive any visual delays."}, {"title": "7.2 Evaluation of Loss Functions", "content": "Our approach incorporates three distinct loss functions. Table 1 presents the mean squared error (MSE) for different combinations of these loss functions across two datasets. The data indicate that the integration of all three loss functions achieves optimal performance on the Red Sea dataset, whereas the combination of content loss and edge loss is most effective for the cosmological simulation data. However, the performance difference from using all three loss functions on the cosmological simulation data is minimal, with error margins very close to the best result, making these subtle differences hard to detect in the visual outputs. Therefore, the combination of the three loss functions is suitable for most applications, and we employed this combination in our experiment. Regarding the weights \u03b1, \u03b2, and y for the combined loss functions, experimental results reveal that the error is minimized at a = 1, \u03b2 = 1, and y = 0.01."}, {"title": "7.3 Evaluation of Prediction Performance", "content": "In this section, we qualitatively and quantitatively compared the visualization generation ability of our model with InSituNet and VDL-Surrogate. Figure 9 shows generated images from three models and the ground truth for qualitative comparison. In the Red Sea simulation, the detail generation of our model outperforms the other two approaches. The close-up views reveal that the contour of the isotherms generated by our model is more similar to the ground truth image. Although InSituNet can also correctly generate these details, the image is blurry. The VDL-Surrogate cannot generate the correct isotherms. In the cosmological simulation, the overall visual differences among the three methods are not significant. Only local features present the difference. For instance, the colors of the halos are in red circles. Our method is closer to the ground truth, InSituNet still shows blurred local features, and the VDL-Surrogate result is sharper but loses correct details.\nTable 2 is the quantitative comparison of image generation quality using PSNR (Peak"}]}