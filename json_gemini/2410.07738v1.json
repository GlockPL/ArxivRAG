{"title": "ENHANCING FEDERATED DOMAIN ADAPTATION WITH\nMULTI-DOMAIN PROTOTYPE-BASED FEDERATED FINE-\nTUNING", "authors": ["Jingyuan Zhang", "Yiyang Duan", "Shuaicheng Niu", "YANG CAO", "Wei Yang Bryan Lim"], "abstract": "Federated Domain Adaptation (FDA) is a Federated Learning (FL) scenario where\nmodels are trained across multiple clients with unique data domains but a shared\ncategory space, without transmitting private data. The primary challenge in FDA is\ndata heterogeneity, which causes significant divergences in gradient updates when\nusing conventional averaging-based aggregation methods, reducing the efficacy\nof the global model. This further undermines both in-domain and out-of-domain\nperformance (within the same federated system but outside the local client). To\naddress this, we propose a novel framework called Multi-domain Prototype-based\nFederated Fine-Tuning (MPFT). MPFT fine-tunes a pre-trained model using multi-\ndomain prototypes, i.e., pretrained representations enriched with domain-specific\ninformation from category-specific local data. This enables supervised learning on\nthe server to derive a globally optimized adapter that is subsequently distributed\nto local clients, without the intrusion of data privacy. Empirical results show\nthat MPFT significantly improves both in-domain and out-of-domain accuracy\nover conventional methods, enhancing knowledge preservation and adaptation in\nFDA. Notably, MPFT achieves convergence within a single communication round,\ngreatly reducing computation and communication costs. To ensure privacy, MPFT\napplies differential privacy to protect the prototypes. Additionally, we develop a\nprototype-based feature space hijacking attack to evaluate robustness, confirming\nthat raw data samples remain unrecoverable even after extensive training epochs.\nThe complete implementation of MPFL is available at https://anonymous.\n4open.science/r/DomainFL/.", "sections": [{"title": "1 INTRODUCTION", "content": "Federated Learning (FL) is a privacy-preserving distributed machine learning paradigm designed to\nprotect the data of participating clients (McMahan et al., 2017b). In FL, only models trained on local\ndata are shared between clients and servers, rather than the raw data itself, mitigating the risk of data\nleaks. Mainstream FL research primarily focuses on optimizing each client's performance within\nits local data domain (in-domain performance). However, in Federated Domain Adaptation (FDA)\nscenarios, clients need to perform well on the collective data domains shared by all participants to\nmeet certain business requirements. For instance, consider a consortium of banks training a model\nto detect fraudulent transactions. Each bank has distinct customer bases and transaction patterns,\nleading to variations in their data (domains). A good out-of-domain performance is essential for\nanticipating new risks that a bank may not be exposed to yet. The challenge is to ensure the global\nmodel performs well across all banks, achieving good in-domain accuracy (within each bank's typical\nuse cases) and out-of-domain accuracy (generalizing across the federated financial system).\nStudies have shown that FL achieves results similar to centralized training only when the datasets\namong clients are independently and identically distributed (i.i.d) and share similar domain char-\nacteristics (Liu et al., 2023; Seol & Kim, 2023; Shaheen et al., 2022). In typical scenarios where\nall clients' models are trained on local data from the same domains, update directions are aligned,\nmaking it feasible to use averaging-based aggregation algorithms such as FedAvg (McMahan et al.,"}, {"title": "2 RELATED WORK", "content": "Efforts to enhance performance in FL across heterogeneous datasets (FDA) have been extensive.\nRegularization methods during training, such as FedProx (Li et al., 2020) and FedDyn (Jin et al., 2023),\nor knowledge distillation techniques like FedGen (Venkateswaran et al., 2023) and FedNTD (Lee\net al., 2022), aim to align local models more closely with the global model. Such approaches help\nmitigate the divergence among local models but rely on the assumption that the averaged global\nmodel is well-suited for the entire data distribution which often does not hold in FDA.\nUpdating only parts of the model during aggregation is another strategy, exemplified by personalized\nFL (PFL) (Tan et al., 2022a), which aims to customize local models to enhance in-domain perfor-\nmance. Some methods involve using a personalized aggregation base to select specific portions of\nglobal information for model aggregation, or updating only parts of the model during the aggrega-\ntion phase, e.g., APFL (Deng et al., 2020), FedFomo (Zhang et al., 2020), FedAMP (Huang et al.,"}, {"title": "3 PROBLEM STATEMENT", "content": "Scenario assumption. Consider a scenario involving N clients, where each client possesses a\nprivate training dataset $D_1,..., D_N$, each drawn from a unique data domain and shared category\nspace\u00b9. Our goal is to train a model that balances domain knowledge preservation and domain\nknowledge adaptation. Domain knowledge preservation refers to the ability of the model to retain\neach client's unique domain insights. Domain knowledge adaptation is defined as the model's ability\nto indirectly extract and transfer knowledge from each participating client's domain to others, even if\na client i cannot directly train on another client j's data.\nOptimization goal. Building on this, we define the Domain Knowledge Preservation loss $L^P$ and\nthe Domain Knowledge Adaptation loss $L^A$ as follows:\n$L^P = \\sum_{i=1}^{N} L_i(\\Theta_i^P; D_i; \\Theta^G)$,\n$L^A = \\sum_{i=1}^{N} \\sum_{j=1,j\\neq i}^{N} L_i(\\Theta_i^P; D_j; \\Theta^G)$,\n(1)\nwhere $L(\\cdot)$ denotes the loss function, $\\Theta_i^P$ denotes the local (personalized) model parameters for client\ni, and $\\Theta^G$ denotes the global model parameters. In conventional FL, the local models $\\Theta_1^P,..., \\Theta_N^P$\nare periodically synchronized with the global model $\\Theta^G$.\nThe optimization goal is to decrease both the $L^P$ and $L^A$. Thus, we formalize the FDA optimization\ngoal as follows:\n$\\lbrace \\Theta, \\Theta_i; \\Theta^G \\rbrace = \\arg \\min(\\alpha L^P + \\alpha^\\prime L^A)$,\n(2)\nwhere $\\alpha$ and $\\alpha^\\prime$ are client-defined weight parameters which balance the trade-off between domain\nknowledge preservation and domain knowledge adaptation, in line with the \"no free lunch\" theorem.\nEvaluation metrics. To quantify the effectiveness of the optimization, we propose two metrics:\nin-domain accuracy (ind acc) and out-of-domain accuracy (ood acc). Denoting $ACC_{ij}$ to be the\naccuracy for a client i when tested on domain j, ind acc and ood acc are defined as follows:\nind acc = $\\frac{\\sum_{i=1}^{N} \\frac{N_i}{\\sum_{i=1}^{N_i}} ACC_{ii}}{\\sum_{i=1}^{N_i}}$,\nood acc = $\\frac{\\sum_{i=1}^{N} \\sum_{j\\neq i}^{N} \\frac{N_i}{\\sum_{i=1}^{N_i}} ACC_{ij}}{\\sum_{i=1}^{N} \\sum_{j\\neq i}^{N_j}}$\n(3)\nThese domains may be largely isolated with minimal overlap, but MPFT also generalizes to scenarios where\neach client may have data from multiple domains, as demonstrated in Section 5."}, {"title": "4 METHODOLOGY", "content": "Overview. We introduce Multi-domain Prototype-based Federated Fine-Tuning (MPFT), which\nconsists of three main components: Prototype Generation, Global Adapter Initialization, and Few-shot\nLocal Adaptation, as depicted in Figure 2. During the Prototype Generation phase, we generate\ndomain-specific prototypes for each client based on specific sampling methods and ratios. The clients\nsubsequently transmit their local prototypes to the server. In the Global Adapter Initialization phase,\nwe utilize these prototypes to train a global adapter designed to handle the multi-domain distribution\nof all clients, thereby improving the ood performance of the global adapter across clients. The global\nadapter is then sent back to the clients for local inference. While the global adapter performs well in\nood accuracy, some clients may require better ind performance. In such cases, they can proceed to\nthe Few-shot Local Adaptation phase, where a few-shot dataset is sampled locally to further fine-tune\nthe local adapter. Knowledge distillation is employed to mitigate catastrophic forgetting of global\nknowledge during this phase.\nPrototype Generation. A prototype is a compact representation of a specific class feature that\nis unique to each domain. To synchronize the consistency of the prototype's embedding space\nacross domains, each client utilizes the same pretrained image encoder to generate the prototype\nset $\\lbrace P_i^{(1)}, ..., P_i^{(K)} \\rbrace$, where $K$ represents the total number of classes. Although all clients share the\nsame label space, each label manifests uniquely within its respective feature domain.\nTo generate these prototypes, different sampling methods can be applied, including Mean Sampling,\nCluster Sampling, and Random Sampling, as described in Algorithm 1. The choice of sampling\nmethod depends on the desired trade-off between computational efficiency and prototype representa-\ntional robustness. In Mean Sampling, each client i generates a prototype for class k by calculating\nthe mean of the pretrained embeddings for that class. In Cluster Sampling, clustering (e.g., k-means)\nis performed on pretrained embeddings of each class, and a certain number of cluster centers are then\nselected based on a predefined sampling rate to form the prototype set. In Random Sampling, a fixed\nnumber of pretrained embeddings of each class are randomly selected according to the sampling rate,\nand these selected embeddings constitute the prototype set.\nOnce each client has generated their prototypes, they transmit these to the server. Consequently, the\nserver accumulates N domain-specific representations subset (prototypes subset) for each class k,\nwhich are collectively represented as $\\mathcal{D}^P = \\lbrace \\cup_{i=1}^{N} \\lbrace P_i^{(1)},...,P_i^{(K)} \\rbrace \\rbrace$."}, {"title": "Global adapter initialization.", "content": "Algorithm 2 outlines the global adapter initialization process. Note\nthat we avoid averaging-based aggregation within each class across different clients, as this would\ndistort the global distribution. Utilizing DP, we train the global adapter $A^G$ to adapt to the entire\nsystem's data distribution with cross-entropy loss L:\n$\\lbrace\\Theta^G, A^G\\rbrace = \\arg \\min L(\\mathcal{D}^P; \\Theta^G; A^G)$.\n(4)\nUpon successful training, the global adapter $A^G$ is sent to the clients, replacing their local adapters."}, {"title": "Few-shot local adaptation.", "content": "While global adapter $A^G$ performs well in ood accuracy, it may not be\nsufficient for ind accuracy. To address this, clients can use their local few-shot data $D^F$ to further\nfine-tune $A^G$, adapting it to their local domain and improving ind accuracy, as shown in Algorithm 3."}, {"title": "5 EXPERIMENT", "content": "Datasets and Models. To simulate a FDA environment, We use the DomainNet (Peng et al., 2019)\nand PACS (Li et al., 2017) datasets which are widely used in multi-domain data adaptation. For\nthese datasets, we employ pretrained CLIP models from OpenCLIP (Cherti et al., 2023; Radford\net al., 2021; Schuhmann et al., 2022). The image encoder of CLIP for DomainNet is a ViT-B-32\npretrained on the LAION-2B dataset, while for PACS, we use a ConvNeXT-Base pretrained on the\nLAION-400M dataset as the image encoder.\nImplementation details. We implement various representative FL algorithms as baselines, includ-\ning FedAvg (McMahan et al., 2017a), FedProx (Li et al., 2020), Ditto (Li et al., 2021b), MOON (Li\net al., 2021a), FedProto (Tan et al., 2022b), and DBE (Zhang et al., 2024), using the PyTorch library"}, {"title": "5.1 PERFORMANCE ON MULTI-DOMAIN", "content": "We evaluate our method alongside other FL approaches in Table 1, including local training (i.e.,\neach client fine-tunes the pretrained model separately). Empirical results show that local training\nexcels in ind accuracy but performs poorly in ood accuracy. A reason is that local fine-tuning results\nin catastrophic forgetting (Luo et al., 2023). Personalized FL methods such as FedProto and DBE,\nwhich generally maintain a personalized local model for each client, have higher ind accuracy but\ncompromise ood accuracy. In contrast, methods like FedAvg, MOON, and Ditto demonstrate more\nbalanced improvements in both ind and ood accuracies. FedProx, which introduces a regularization\nterm between the global and local models, improves ood accuracy at the expense of ind accuracy. In\ncomparison, our method consistently achieves superior performance in both ood and ind accuracy\nacross all DomainNet subsets. As the subset size of DomainNet increases, we observe variable\nconvergence stability across methods such as FedAvg, FedProx, and Ditto, while DBE demonstrates\naccelerated convergence. In contrast, our method requires only one global communication round,\nwhich significantly reduces both computational and communication costs. This benefit is further\nelaborated in section 5.5."}, {"title": "5.2 DETAILS ABOUT RESULTS ON EACH DOMAIN", "content": "To further explain why MPFT achieves better ind and ood accuracy compared to the baselines, we\nvisualize the performance of each domain in Figure 3. Each axis of the radar chart represents a specific\ndata domain (e.g., Real or Painting), with the shape and coverage area of the curves illustrating the\nglobal model's performance across these domains. Empirically, the roundness of the curve could\nreflect the fairness of the model across different clients (domains)\u2014the rounder the curve, the more\nfair the method is in the global distribution, leading to better ood accuracy.\nCompared to other FL baselines across different DomainNet subset sizes, MPFT with average\nsampling method performs exceptionally well in the Quickdraw domain, with a more balanced\ncurve shape. Additionally, MPFT maintains strong performance across other domains relative to the\nbaselines, thereby achieving better overall fairness. For more details about the effects of random and\ncluster sampling compared to average sampling on each domain, please refer to Appendix E."}, {"title": "5.3 IMPACT OF MULTI-DOMAIN DIFFERENCES ON PERFORMANCE", "content": "In real-world scenarios, a client may contain data from multiple domains rather than a single specific\ndomain\u00b2. We simulate a situation where each client contains 1 $m_r$ percent of data from its original\ndomain, mixed with $m_r$ percent of data from another domain. Here, $m_r$ represents the mixed ratio,\nindicating the level of domain diversity on the client side. We evaluate our method alongside other FL\napproaches under this scenario, as shown in Table 2, where DomainNet subset-50 is used. We observe\na reduction in the performance advantage of our method compared to others, as the mixed ratio\nincreases. This decline is due to the reduced heterogeneity within the FL system, which diminishes\nthe strengths of our approach. However, it is still evident that our method outperforms most FL\nalgorithms, particularly when using Random and Cluster sampling strategies."}, {"title": "5.4 PERFORMANCE WITH LOCAL ADAPTATION", "content": "We compare the few-shot performance of local adaptation with different knowledge distillation (KD)\nweights in Figure 4. As the KD weight increases, there is less out-of-domain knowledge forgetting but\nworse in-domain knowledge alignment. With an increase in the number of few-shot samples, the ood\nand ind accuracy show a similar trend. We provide more details about the experiment implementation\nof KD in local adaptation in Appendix C.4."}, {"title": "5.5 COMPUTATION COST AND COMMUNICATION COST", "content": "We further analyze the computation cost and communication cost in Table 3. Computation cost\nis the total training time of FL, which is related to the number of communication rounds and the\ncomputational complexity within each round. Among the methods compared, Ditto is the most\ntime-consuming due to the additional local training epoch it requires. In contrast, the MPFT, which\nconverges in just one global round, significantly reduces training time, particularly when the sampling\nmethod is set to average. For other sampling methods, such as Random and Cluster, our approach\ntrades a modest increase in training time for substantial improvements in both ind and ood accuracy.\nCommunication cost is the number of parameters transmitted, which is theoretically influenced by\nthe number of communication rounds R, the model (adapter) parameters $\\Sigma$, and the prototypes\n$\\Pi$. The number of communication rounds R directly contributes to a linear increase in the overall\ncommunication cost. Furthermore, the model adapter parameters $\\Sigma$ and the size of the prototypes\n$\\Pi$ determine the communication cost per round in these specific algorithms. In our empirical\nresults, MPFT with average sampling achieved the lowest communication cost across all experiments.\nHowever, while random or cluster sampling slightly increases communication overhead, it also\nsignificantly improves MPFT's performance (see Table 1)."}, {"title": "5.6 PRIVACY PRESERVATION ANALYSIS", "content": "Following DBE (Zhang et al., 2024), we add Gaussian noise $N$ to client prototypes $p_1, ..., p_n$ with a\nperturbation coefficient $q$ for the noise and a scale parameter $s$ for the noise distribution, the perturbed\nprototype $p_i^\\prime$ of client i can be defined as $p_i^\\prime = p_i + q \\cdot N(0, s^2)$, where $p_i$ is the original prototype of\nclient i.\nTable 4 shows the results of applying this differential privacy method on the DomainNet subset-50,\nwith the sampling method set to average under various noise parameter combinations. This approach\neffectively mitigates attackers from inferring individual data points even when they possess the\npretrained model and most of prototypes. Furthermore, we observe that specific noise configurations\ncan reduce bias across heterogeneous datasets, enhancing the robustness of prototype data. In some\ncases, this even leads to improved performance compared to models without noise. For instance,\nthe combinations of (q = 0.5, s = 0.1), (q = 0.1, s = 0.05), and (q = 0.5, s = 0.05) exhibit such\neffects. According to DBE, setting q = 0.2 and s = 0.05 is sufficient to ensure privacy protection.\nHowever, excessively large noise can degrade model performance."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "We propose an adaptive and lightweight FDA framework, MPFT, designed to align a global model\nwith heterogeneous domains by fitting prototypes from different domains. Extensive experiments\ndemonstrate the effectiveness, low cost, and robustness of MPFT. This study may inspire further\nresearch in FDA that focuses on generalizing across heterogeneous domain prototypes, rather than\nrelying on model parameter averaging for aggregation.\nWhile MPFT achieves strong performance, it has some limitations. First, the quality of the prototypes\nis highly dependent on the pretrained model's ability to extract meaningful features. Second, although\nattackers cannot reconstruct specific raw data from the prototypes, they may still be able to perform\nmembership inference attacks (Shokri et al., 2017) or attribution inference attacks (Fredrikson\net al., 2015) by exploiting statistical information contained within the prototypes. Addressing the\naforementioned limitations could further enhance the viability and effectiveness of this approach in\npractical FL applications, making this method a viable alternative of averaging-based FL methods."}, {"title": "A VISUALIZATION OF PROTOTYPE", "content": "In order to allow readers to intuitively understand the relationship between prototypes and original\ndata, we visualize the data embeddings (all prototypes) and averaged prototypes in two-dimensional\ncoordinates using the t-SNE (Van der Maaten & Hinton, 2008) algorithm, as shown in Figure 5.\nDifferent colors represent different domains: for example, blue indicates the \"Painting\" domain,\nwhile orange signifies the \"Real\" domain. Different markers represent various categories of data\nsamples. The darker markers located within each sample cluster represent the prototypes of the\n\"domain-class\". It is evident that each prototype effectively reflects the distribution of its specific\ndomain-class information. However, the mean prototype, represented by the large red star, is distant\nfrom each individual prototype. This observation underscores why we do not average the prototypes,\nas it fails to accurately reflect the overall data distribution, particularly in FDA scenarios.\nThe Universal Approximation Theorem (Hornik et al., 1989) suggests that neural networks act as\n\"universal\" data distribution fitters, effectively fitting the distribution of given data samples. However,"}, {"title": "B CONVERGENCE ANALYSIS", "content": "Setup: The global loss function for fine-tuning using prototypes is:\n$\\mathcal{L}(\\mathcal{D}^P; \\Theta^G, A^G) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}_i(\\mathcal{D}^P; \\Theta^G, A^G)$,\nwhere $\\mathcal{D}^P$ is the set of prototypes aggregated from all clients.\nAssumptions: The standard assumptions follow those of Li et al. (2020); Tan et al. (2022b): 1)\nThe loss function $\\mathcal{L}$ is non-convex but smooth, 2) The gradient of the loss function is Lipschitz\ncontinuous with constant $L$. 3) The divergence between the prototypes of different clients for a given\nclass is bounded. Let $\\Delta_i^{(k)}$ be the divergence between client i's prototype for class k and the average\nprototype across all clients:\n$\\Delta_i^{(k)} = || p_i^{(k)} - \\bar{p}^{(k)} ||, \\bar{p}^{(k)} = \\frac{1}{N} \\sum_{i=1}^{N} p_i^{(k)}$,\nand assume that $\\Delta_i^{(k)} < \\Delta$ for all i and k. Since the prototypes are derived from pretrained image\nembeddings, this assumption likely holds due to the consistency provided by the pretrained encoder\nin synchronizing the embedding space across domains.\nProof: Given the update rule $A_{t+1}^G = A_t^G - \\eta \\nabla_{A^G} \\mathcal{L} (\\mathcal{D}^P; \\Theta^G, A_t^G)$, for a smooth, non-convex loss\nfunction $\\mathcal{L}$, the Lipschitz continuity implies:\n$\\mathcal{L}(\\mathcal{D}^P; \\Theta^G, A_{t+1}^G) < \\mathcal{L}(\\mathcal{D}^P; \\Theta^G, A_t^G) - \\frac{\\eta}{2} || \\nabla_{A^G} \\mathcal{L} (\\mathcal{D}^P; \\Theta^G, A_t^G) ||^2 + \\frac{\\eta L^2}{2} \\Delta^2 \\frac{\\Delta}{N}$"}, {"title": "B.1 COROLLARY 1: ON CONVERGENCE TO STATIONARY POINT AND CONVERGENCE RATE", "content": "To analyze the convergence rate, we sum both sides of the inequality from t=1 to T:\n$\\sum_{t=1}^{T} \\mathcal{L}(\\mathcal{D}^P; \\Theta^G, A_{t+1}^G) < \\sum_{t=1}^{T} \\mathcal{L}(\\mathcal{D}^P; \\Theta^G, A_{t}^G) - c \\sum_{t=1}^{T} || \\nabla_{A^G} \\mathcal{L} (\\mathcal{D}^P; \\Theta^G, A_t^G) ||^2 + \\frac{T \\Delta}{N}$.\nThis simplifies to:\n$\\mathcal{L}(\\mathcal{D}^P; \\Theta^G, A_{t}^G) - \\mathcal{L}(\\mathcal{D}^P; \\Theta^G, A_1^G) > c \\sum_{t=1}^{T} || \\nabla_{A^G} \\mathcal{L} (\\mathcal{D}^P; \\Theta^G, A_t^G) ||^2 - \\frac{T \\Delta}{N}$.\nRearranging the terms and dividing by $cT$, we obtain the convergence rate:\n$\\frac{1}{T} \\sum_{t=1}^{T} || \\nabla_{A^G} \\mathcal{L} (\\mathcal{D}^P; \\Theta^G, A_t^G) ||^2 < \\frac{\\mathcal{L}(\\mathcal{D}^P; \\Theta^G, A_1^G) - \\mathcal{L}(\\mathcal{D}^P; \\Theta^G, A_T^G)}{cT} + \\frac{\\Delta}{cN}$\nAs T increases, the average gradient norm decreases, indicating convergence to a stationary point, ad-\njusted for prototype divergence. Note that the size of the divergence $\\Delta$ affects the rate of convergence.\nA smaller $\\Delta$ implies that the prototypes across clients are more similar, leading to faster convergence.\nConversely, a larger $\\Delta$ suggests greater variability between client data, which may slow down the\nconvergence rate."}, {"title": "B.2 REMARK: ON ERROR BOUNDS", "content": "By incorporating the average prototype divergence, the error bound for client i is:\n$e^{ind} < e_{local} + \\alpha \\Delta^{avg}$,\nwhere $e_{local}$ is the local model error without federation, $\\alpha$ is a constant that measures the sensitivity of\nthe error to the prototype divergence, and $\\Delta^{avg} = \\frac{1}{K} \\sum_{k=1}^{K} (\\bar{p_i}^{(k)} - \\bar{p}^{(k)})$ is the average prototype divergence\nfor client i. Thus, the in-domain error bound can be written as:\n$e^{ind} < \\frac{\\sum_{i=1}^{N} \\frac{N_i}{\\sum_{i=1}^{N_i}}(e_{local} + \\alpha \\Delta^{avg})n_i}{\\sum_{i=1}^{N_i}}$,\nFor out-of-domain accuracy (ood acc), we are interested in how well the global model generalizes\nacross different client domains. The maximum divergence of prototypes captures the worst-case\ndivergence between any two domains.\nGiven the metric:\n$ood acc = \\frac{\\sum_{i=1}^{N} \\sum_{j\\neq i}^{N} \\frac{N_i}{\\sum_{i=1}^{N_i}} ACC_{ij} n_j}{\\sum_{i=1}^{N} \\sum_{j\\neq i}^{N_j}}$"}, {"title": "C MORE DETAILS ABOUT EXPERIMENT IMPLEMENTATION", "content": "C.1 BASIC SETUP\nGlobal classification head generation. We generate the global classification head by utilizing the\nmanually designed prompts and class names of the dataset, calculated by the pretrained text encoder\nin the CLIP model used in our experiments. The global classification head is obtained by averaging\nthe 'prompt+label' embeddings from all different domains. Following common fine-tune settings, we\nonly train the adapter, while freezing the entire image encoder and global classification head.\nTraining. Our simulations are conducted on a Google\u00ae Compute Platform (GCP) equipped with\n47 Intel\u00ae Xeon\u00ae CPUs and 4 NVIDIA\u00ae L4 GPUs. For global adapter training, we employ cross-\nentropy loss with an AdamW optimizer, setting the learning rate to 0.001. We set the maximum\nglobal rounds to 200 and implement an early stopping strategy to evaluate the convergence rounds. It\nis important to note that our method achieves convergence in just one global round, rendering the\nearly stopping strategy primarily applicable to other FL methods. For simplicity, we assume that all\nclients can participate in every communication round in all experiments.\nC.2 DETAILS ABOUT IMPLEMENTATION OF BASELINES\nFor the baseline models used in the experiments, we identified the best parameters for our dataset\nwithin the recommended parameter ranges provided in their original texts. For FedAvg, we followed\nthe settings in the original article and used the dataset sizes of different clients as the basis for the\nweighted average. In FedProx, we set the regularization coefficient to 5, which is lower than the usual\nsettings of 10, 100, or 1000. This adjustment was made because a higher regularization coefficient\nmade it difficult for the model to converge to the global equilibrium point due to data heterogeneity.\nIn Ditto, we used a local round number of 1 and set the regularization term to 2. For MOON, we"}, {"title": "C.3 DETAILS ABOUT EARLY STOPPING STRATEGY", "content": "Each client completes one local epoch per global round. We set the total number of global rounds to\n200 and implement an early stopping strategy to evaluate the convergence rounds of each algorithm.\nThe criterion for early stopping is based on validation loss; specifically, we select the results from the\nround that achieves the best validation loss as the final outcome. The patience parameter is set to 10\nrounds, meaning that if the validation loss does not decrease below the best recorded loss within a\nspan of 10 consecutive rounds, the training process is terminated. By implementing the early stopping\nstrategy, we can more easily test the convergence round of each method and use this strategy to find\nthe round with the best result."}, {"title": "C.4 DETAILS ABOUT KNOWLEDGE DISTILLATION IN LOCAL ADAPTATION", "content": "We use the most basic form of knowledge distillation strategy in our framework (Hinton et al., 2015),\nwhich is response-based knowledge distillation:\n$A_i^F = \\arg \\min L_{KD} (D_i^F; \\Theta^G; A_i^F; A^G)$.\n(6)\nHere, $A_i^F$ represents the local adapter for client i, $D_i^F$ is the local dataset, $\\Theta^G$ are the global model\nparameters, and $A^G$ represents the global adapter. Then, we have:\n$I^G = \\lbrace A^G (f(\\phi; x_1)), ..., A^G (f(\\phi; x_n)) \\rbrace, \\lbrace (x_1,y_1),..., (x_n, y_n) \\rbrace \\in D_i^F$.\n(7)\nHere, $I^G$ denotes the set of outputs from the global adapter for the local dataset $D_i^F$.\n$I_i^F = \\lbrace A_i^F(f(\\phi; x_1)),..., A_i^F(f(\\phi; x_n)) \\rbrace, \\lbrace (x_1,y_1),..., (x_n, y_n) \\rbrace \\in D_i^F$.\n(8)\nSimilarly, $I_i^F$ denotes the set of outputs from the local adapter $A_i^F$ for the local dataset $D_i^F$.\n$L_{KD} = KL(I^G || I_i^F)$\n(9)\nThe knowledge distillation loss $L_{KD}$ is computed as the Kullback-Leibler (KL) divergence between\nthe outputs of the global adapter and the local adapter.\n$KL(p || q) = \\sum p_i log (\\frac{p_i}{q_i})$\n(10)\nHere, p and q represent the probability distributions output by the global and local adapters, respec-\ntively, and KL(p || q) denotes the KL divergence."}, {"title": "D SENSITIVITY ANALYSIS OF GLOBAL CONVERGENCE THRESHOLD", "content": "During the global adapter initialization phase, we set a threshold to ensure the model stops training\nthe prototypes when the variance of loss over multiple rounds decreases to a low value, indicating\nthat the global adapter has converged. To test the sensitivity of the experimental results to the\nthreshold, we test different thresholds for the global adapter initialization process, specifically 0.1,\n0.01, 0.001, and 0.0001. Figure 6 shows the impact of these thresholds\u00b3. As the threshold decreases,\nthe out-of-distribution (ood) and in-distribution (ind) performance initially increase and then decrease.\nIn contrast, the convergence epochs and training time consistently increase. This trend is intuitive\nbecause a lower threshold requires more rounds for the model to converge. Overall, we recommend\nusing a threshold of 0.01 or 0.001 to minimize training time and reduce the risk of overfitting."}, {"title": "E MORE DETAILS ABOUT RESULTS ON EACH DOMAIN", "content": "Figure 7 and Figure 8 further illustrate the effects of random sampling and cluster sampling compared\nto average sampling within the MPFT framework across different sizes of the DomainNet subset. They\nreveal similar trends: random sampling and cluster sampling achieve more balanced performance\nin the Quickdraw domain, with curves approaching circular shapes and covering larger areas. This\nsuggests that these improved sampling methods enhance the model's ability to handle balance and\ndiversity across various data domains.\nWe also observe that as the sampling rate increases in the random or cluster sampling methods, the\nmodel's performance in the Quickdraw domain improves, leading to more globally optimized results.\nHowever, the increase in sampling results in more training convergence time consumption and higher\ndata transmission between the server and clients, which raises both computation and communication\ncosts, as shown in Table 3. This trade-off needs to be considered in practical applications.\n3Note that the rounds in the figure represent the convergence epochs for the global prototype training in the\nserver, not the communication rounds (global rounds)."}, {"title": "F RESULTS OF MULTIPLE CLIENTS WITH SAME DOMAIN", "content": "In this section, we present the results of experiments where multiple clients belong to the same data\ndomain, a scenario commonly encountered in FL. Specifically, we partition the data from the same\ndomain into multiple subsets, each maintaining"}]}