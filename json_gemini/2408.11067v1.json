{"title": "Toward End-to-End Bearing Fault Diagnosis for Industrial Scenarios with Spiking Neural Networks", "authors": ["Yongqi Ding", "Lin Zuo", "Kunshan Yang", "Biao Chen", "Mengmeng Jing", "Yunqian Yu"], "abstract": "Spiking neural networks (SNNs) transmit information via low- power binary spikes and have received widespread attention in areas such as computer vision and reinforcement learning. However, there have been very few explorations of SNNs in more practical industrial scenarios. In this paper, we focus on the application of SNNs in bearing fault diagnosis to facilitate the integration of high- performance AI algorithms and real-world industries. In particular, we identify two key limitations of existing SNN fault diagnosis methods: inadequate encoding capacity that necessitates cumber- some data preprocessing, and non-spike-oriented architectures that constrain the performance of SNNs. To alleviate these problems, we propose a Multi-scale Residual Attention SNN (MRA-SNN) to simultaneously improve the efficiency, performance, and robust- ness of SNN methods. By incorporating a lightweight attention mechanism, we have designed a multi-scale attention encoding module to extract multiscale fault features from vibration signals and encode them as spatio-temporal spikes, eliminating the need for complicated preprocessing. Then, the spike residual attention block extracts high-dimensional fault features and enhances the expressiveness of sparse spikes with the attention mechanism for end-to-end diagnosis. In addition, the performance and robustness of MRA-SNN is further enhanced by introducing the lightweight attention mechanism within the spiking neurons to simulate the biological dendritic filtering effect. Extensive experiments on MFPT and JNU benchmark datasets demonstrate that MRA-SNN signifi- cantly outperforms existing methods in terms of accuracy, energy consumption and noise robustness, and is more feasible for deploy- ment in real-world industrial scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "Spiking neural networks (SNNs), which mimic the information transmission mechanism of biological neural systems, have at- tracted considerable attention for their low-energy paradigm [23, 32]. Specifically, SNNs transmit information via discrete 0-1 spikes. Spiking neurons are silenced for 0-valued input spikes and only need to perform accumulation (AC) operations for 1-valued spikes (event-driven) [23]. In contrast, the current widely used artificial neural networks (ANNs) have intensive multiply-accumulate (MAC) operations. In the typical case of a 32-bit floating-point implemen- tation in 45nm technology [8], the AC operation consumes 0.9pJ of power, while the MAC operation requires 4.6pJ, more than five times that of the AC operation. Therefore, SNNs are expected to be a low-power alternative to ANNs, and are more favorable for deploy- ment in energy- and latency-sensitive edge devices. For instance, Kim et al. [13] deployed Spiking-YOLO on the TrueNorth [19] neu- romorphic chip, which achieves 98% of the performance of Tiny YOLO while consuming 280 times less energy.\nBenefiting from the advantages of low energy consumption and high bionicity, SNNs have been used widely in computer vision, reinforcement learning and other fields [3, 10, 22]. However, SNNs are rarely explored for industrial scenarios related to real-world applications. Industrial tasks have a huge impact on the normal operation of equipment and even the safety of personnel, such as typical bearing fault diagnosis, which often requires fast and robust algorithmic support [34]. Existing fault diagnosis methods typically use ANNs, which provide decent results but still struggle with high latency and energy consumption [30, 38]. To overcome this energy-performance dilemma, SNNs have been introduced into bearing fault diagnosis with promising results [28, 35, 41]. Unfortu- nately, these SNN methods are either constrained to shallow fully connected forms [40, 41] or residual network architectures [28, 35] like ANNs, which do not consider spike properties and suffer from limited diagnostic performance. Therefore, it remains necessary to further explore efficient and high-performance SNN fault di- agnosis methods for real industrial environments to facilitate the deployment of next-generation AI algorithms.\nIn this paper, we first identify two key factors that limit the diagnostic performance of SNNs: (1) inadequate spike encoding capacity requires additional data preprocessing, and (2) network architectures that do not account for spike characteristics, resulting in suboptimal performance. To this end, we propose a Multi-scale Residual-Attention SNN (MRA-SNN): extracting multi-scale fea- tures in the data and adaptively fusing them for spike encoding with the attention mechanism, thus eliminating tedious data pre- processing and dramatically improving the diagnostic efficiency; and rectifying the high-dimensional residual features with the at- tention mechanism for the increasingly sparse spikes to improve the representation performance of the SNN. Moreover, inspired by the filtering of input currents by dendrites in biological neu- rons [18, 25], we introduce the attention mechanism in spiking neurons to mimic this dendritic filtering effect. This enhances the discriminative ability of the spiking neurons [4], which further improves the overall performance and robustness of the MRA-SNN for accurate fault diagnosis under noisy interference. The attention mechanism we use is lightweight and involves only single-channel 1D convolutions for channel-space attention, making MRA-SNN concise and effective. Extensive experiments on the challenging MFPT and JNU benchmarks demonstrate the superior performance of MRA-SNN. Compared to other existing SNN fault diagnosis meth- ods, the lightweight MRA-SNN shows better performance in both normal and noisy environments, even surpassing ANN methods. In summary, the main contributions of this paper are as follows:\n(1) We propose MRA-SNN for bearing fault diagnosis, with a multi-scale attention encoding module to convert vibration signals into spikes, thus eliminating cumbersome data pre- processing, and a spike residual attention block to enhance the representational capability of the network.\n(2) We introduces the lightweight attention mechanism in spik- ing neurons to simulate the filtering behavior of biological dendrites. This enhances the bionic and discriminative prop- erties of the spiking neurons, as well as the fault diagnosis performance and noise robustness of the MRA-SNN.\n(3) Extensive experiments on the MFPT and JNU benchmarks confirm the effectiveness of our method, which achieves su- perior performance in both normal and noisy environments with significantly lower energy consumption compared to existing methods."}, {"title": "2 RELATED WORK AND BACKGROUND", "content": "2.1 Spiking Neural Network\nAs the third generation of neural networks, SNNs have been widely used in various fields. In computer vision, for example, SNNs are used for object recognition [5, 33], detection [13, 26], and track- ing [17]. For reinforcement learning, SNNs have been able to per- form game and motion control [22, 27]. With the advent of the large model era, the spiking Large Language Model has also achieved impressive results [1, 39]. In this paper, we aim to push the SNN to the industry to better utilize its low power consumption and high efficiency to advance the task of mechanical bearing fault diagnosis.\n2.2 Fault Diagnosis\nFault diagnosis aims to detect device faults according to the one- dimensional time series vibration signals collected by the device side sensors. Early methods built sophisticated device-dependent mathematical-physical models, but were not applicable to increas- ingly complex mechanical systems [16]. Currently, data-driven ANN-based methods capable of adaptively learning and diagnosing from large amounts of historical data are the most popular methods. For example, Zhao et al. [38] proposed deep residual shrinkage network (DRSN) for robust fault diagnosis based on convolutional neural networks (CNNs). Chen et al. [2] combined CNN and long short-term memory (LSTM) to extract fault-related features from raw vibration signals. However, high-performance ANNs demand huge energy consumption [6], which makes these methods hardly feasible for practical edge devices. Therefore, exploring fault di- agnosis methods that balance low energy consumption and high performance has received widespread attention [12].\n2.3 SNNs in Fault Diagnosis\nPrevious work has introduced SNNs to the field of fault diagnosis with quite impressive effects. Zuo et al. [41] used Local Mean De- composition (LMD) to extract features from vibration signals and then a single-layer SNN for bearing fault diagnosis. Wang et al. [29] proposed an improved SNN for intershaft bearing fault diagnosis using short-time Fourier transform (STFT)-Norm-LIF coding and simplifying the backpropagation process of spiking neurons. Based on the probabilistic transmission mechanism, Zuo et al. [40] use a multilayer SNN, which outperforms multilayer ANNs and has great transparency. Xu et al. [35] proposed deep spiking residual shrinkage network (DSRSN), which achieves robust fault diagnosis under noise interference by using the attention mechanism and soft thresholding. SNN fault diagnosis methods have also been ex- tended to the fault diagnosis of devices other than bearings. Wang et al. [28] proposed membrane learnable residual SNN (MLR-SNN) for fault diagnosis of sensors in autonomous vehicles. These works confirm the potential of SNNs for fault diagnosis, but still suffer from several serious challenges:\n\u2022 Heavy data preprocessing. It is difficult to extract fault fea- tures from non-smooth and non-linear vibration signals by directly using SNNs, so the existing methods use LMD [40, 41] or STFT [29] to extract time-frequency features before using SNNs for fault diagnosis. The pre-processing of vi- bration signals limits the diagnostic efficiency and makes it almost impossible to diagnose faults on-line in real time.\n\u2022 Non-spike oriented architecture. Existing methods directly use fully connected or ResNet architectures for ANNs and lack the exploration of architectures that incorporate spike characteristics. Effective architectures that can extract more"}, {"title": "3 PRELIMINARY", "content": "This section describes the preliminaries of SNNs, including the dynamics of spiking neurons and the SNN training method used in this work.\n3.1 Spiking Neuron\nSpiking neurons distinguish SNNs from ANNs. Unlike neurons in ANNs such as Rectified Linear Unit (ReLU), spiking neurons model the information transmission mechanism of biological neurons with complicated internal dynamics. Spiking neurons iteratively experienced the process of charging, firing spikes, and resetting membrane potential over time.\nAt timestep t, the spiking neuron receives the input current I transmitted from the previous layer of neurons and charges the membrane potential H by incorporating it. For the most commonly used leaky integrate-and-fire (LIF) [33] neurons, whose membrane potential leaks over timestep:\n\\begin{equation}\\label{eq1} H_{i}^{l}(t)=(1-\\tau)H_{i}^{l}(t-1)+I_{i}^{l}(t), \\end{equation}\nwhere U is the membrane potential after resetting at the previous timestep; superscript I and subscript i denote the i-th neuron in layer 1. t is the membrane potential constant that controls the leakage rate.\nAfter charging the membrane potential, a spike is generated once the membrane potential reaches the firing threshold 9:\n\\begin{equation}\\label{eq2} S(t) = \\Theta(H(t) - \\vartheta), \\end{equation}\nwhere $\\Theta()$ denotes the Heaviside step function:\n\\begin{equation}\\label{eq3} \\Theta(x) = \\begin{cases} 1, x \\geq 0 \\\\ 0, x < 0 \\end{cases} \\end{equation}\nAfter the spike is fired, the spiking neuron resets the membrane potential U. This paper uses the soft reset to reduce the membrane potential by a magnitude of the threshold:\n\\begin{equation}\\label{eq4} U(t) = r(H(t), S(t)) = H(t) - S_{i}^{l}(t)\\vartheta. \\end{equation}\n3.2 Surrogate Gradient Training\nThe spike activity is discontinuous and non-differentiable due to the Heaviside step function, which prevents the back-propagation (BP) algorithm from being used directly to optimize SNNs. To obtain high performance SNNs, the surrogate gradient-based method gen- erates spikes during forward propagation using the Heaviside step function, and replaces the Heaviside step function during backward propagation with a predefined surrogate function h(.) to calcu- late the gradient. The smooth surrogate functions enable feasible optimization of parameters in SNNs based on the BP algorithm. Specifically, the gradient of the spike w.r.t. the membrane potential"}, {"title": "4 METHODOLOGY", "content": "can be calculated as:\n\\begin{equation}\\label{eq5} \\frac{\\partial S(t)}{\\partial H(t)} = \\frac{dh(H(t), \\vartheta)}{\\partial H(t)} \\frac{\\partial \\Theta}{\\partial H_{i}^{l} (t)} \\end{equation}\nThis work uses the rectangular surrogate function [33]:\n\\begin{equation}\\label{eq6} h(H_{i}^{l}(t), \\vartheta) = \\text{sign} \\left( H_{i}^{l}(t) - \\vartheta \\right), \\end{equation}\nwhere a = 1 is a hyperparameter that controls the shape of the rectangular function.\n4 METHODOLOGY\nThe overall schematic of the MRA-SNN is shown in Fig. 1. The multi-scale attention encoding module encodes the raw vibration signals directly into spikes without the need for heavy data pre- processing to extract time-frequency domain features. The sub- sequent two spike residual attention blocks extract fault-related high-dimensional features taking advantage of residual learning and rectify the sparse spike residual information through the atten- tion mechanism. Finally, the fully connected layer is used to classify fault types for end-to-end bearing fault diagnosis. Note that since SNNs run over multiple timesteps (denoted as T), the raw vibration signal temporally extends to T identical signals that are input to the MRA-SNN at each timestep. This temporal extension does not affect the efficiency because it does not involve data computation or time-frequency domain feature extraction. The details of the multi- scale attention encoding module, the spike residual attention block, and the attention spiking neuron are described in detail below.\n4.1 Multi-Scale Attention Encoding Module\nFor bearing fault diagnosis, it is crucial to extract critical informa- tion from non-smooth, non-linear vibration signals and encode it as spikes. Previous methods use LMD [40, 41] or STFT [29] to preprocess the vibration signals and then a simple SNN to classify the faults, which greatly affects the diagnostic efficiency. In order to avoid the heavy preprocessing, the multi-scale attention encoding module was specially designed in this paper to extract key features from the raw vibration signals and encode them into spikes.\nThe schematic of the multi-scale attention encoding module is shown in the bottom left of Fig. 1. Three convolution pathways with 1\u00d73, 1\u00d75, and 1\u00d77 convolutional kernels are available for extracting fault features at different scales. The features extracted from multi- ple scales are more comprehensive than vanilla single-scale SNNs and model the multi-level structure of the biological cortex [24], pro- viding a basis for accurate fault diagnosis. The convolved features are converted into input current I through the Batch Normalization (BN) [11] layer to be transmitted into the spiking neuron, which consequently generates spike sequences. This couples convolution and spiking together, preserving the energy efficiency benefits of SNNs and enabling deployment on neuromorphic chips [9].\nFor the second BN layer in each convolution pathway, the cur- rent I it generates is not passed directly to the spiking neuron. This is because if all three pathways generate spikes, the fused output becomes an analog value (spikes are added directly or weighted), thus losing the low-energy characteristic of 0-1 spikes. Instead, we first used channel attention to selectively focus on the currents of the three pathways on a channel-wise basis to distinguish the"}, {"title": "4.2 Spike Residual Attention Block", "content": "importance of different scales of information. The additive fusion of the filtered currents is then fed to the spiking neurons to accumu- late membrane potential and fire spikes. In this way, the efficient 0-1 spike output is maintained, while effective fusion of multi-scale information is achieved. The visualization of the multi-scale path- ways and fused spikes is shown in Appendix A to more clearly illustrate the extracted multi-scale feature information.\nLet X denote the input raw vibration signal, the process of gen- erating multi-scale currents can be formulated as:\n\\begin{equation}\\label{eq7} I_{3} = bn(conv_{1 \\times 3}(pool(sn(bn(conv_{1 \\times 3}(X)))))), \\end{equation}\n\\begin{equation}\\label{eq8} I_{5} = bn(conv_{1x5}(pool(sn(bn(conv_{1 \\times 5}(X)))))), \\end{equation}\n\\begin{equation}\\label{eq9} I_{7} = bn(conv_{1\\times 7}(pool(sn(bn(conv_{1 \\times 7}(X)))))), \\end{equation}\nwhere conv() denotes the convolution layer, bn() represents the BN layer, and sn() is the spiking neuron layer. To reduce the size of the features as well as the computational overhead, the spike maps generated by the first spiking neuron layer were downsampled using average pooling, denoted by pool(\u00b7), with stride set to 2.\nThe fusion of multi-scale currents to accumulate membrane potential and generate spikes can be formulated as:\n\\begin{equation}\\label{eq10} S = sn(ca(I_{3}; I_{5}; I_{7}) \\times (I_{3}; I_{5}; I_{7})), \\end{equation}\nwhere (a; b; c) denotes the concatenation operation along the chan- nel dimension and ca() is the channel attention, which will be detailed in Section 4.3."}, {"title": "4.3 Attention Spiking Neuron", "content": "Residual learning [7] effectively mitigates the information and gra- dient vanishing problem in deep neural networks, preventing per- formance degradation. Based on this, we construct spike residual attention blocks for extracting abstract fault features in MRA-SNN and preventing information vanishing. Considering the large length of the bearing vibration signal, we need to continuously reduce the feature map to decrease the computational cost. This prevents the identity connections commonly used in ANNs from being used in the spike residual attention block. To do this, each block downsam- ples the input feature map through the first convolutional layer on the residual and the shortcut branch, and accumulates the sum of the two pathways. For the implementation, the stride of the first convolution layer on the residual and shortcut pathways is set to 2, as shown in the bottom right of Fig. 1.\nOn the other hand, the spikes in SNNs become sparser as the layer deepens, so it is necessary to improve the expressiveness of sparse spikes. Therefore, we refine the features extracted from the residual pathways using joint channel-spatial attention to am- plify/suppress critical/redundant features. Both channel and spatial attention are implemented by one-dimensional convolution and the sigmoid function, described in detail in Section 4.3, with only negligible computational overhead. This feature refinement is used for the output of the BN layer, which can be regarded as a modu- lation of the input current to the spiking neurons, to some extent modeling the information filtering mechanism of the biological nervous system [18, 25]. This practice is somewhat similar to [36], but we do not adjust the membrane potential of the spiking neuron, thus eliminating the need to couple attention to the neuron model, and is more conducive to deployment on neuromorphic chips [9].\nNote that both the residual and shortcut pathways generate analog value outputs. Similar to the encoding module, the sum of the outputs of these two pathways is used as the input current to the spiking neuron, which then fires the spike. Therefore, the spike residual attention block outputs discrete 0-1 spikes, maintaining the low energy consumption characteristic of SNNs.\nWithout loss of generality, let the input to the spike residual block be x, the residual pathway can be formulated as:\n\\begin{equation}\\label{eq11} I_{residual} = bn(conv(sn(bn(conv_{s2}(x))))), \\end{equation}\nwhere s2 is the convolution stride of 2 for downsampling. The shortcut pathway can be formulated as:\n\\begin{equation}\\label{eq12} I_{shortcut} = bn(conv_{1\\times 1}(x)), \\end{equation}\nwhere 1 \u00d7 1 is the convolution kernel size. The output spikes y of the spike residual block can be calculated as:\n\\begin{equation}\\label{eq13} y = sn(sa(ca(I_{residual})) + I_{shortcut}), \\end{equation}\nwhere ca() and sa() are channel and spatial attention, respectively, as detailed in Section 4.3."}, {"title": "5 EXPERIMENTS", "content": "Spiking neurons simulate the information transmission mechanism and internal dynamics of biological neurons. Theoretically, the higher the bionicity of the spiking neuron, the more ingenious the internal dynamics and the greater the performance [37]. However, highly bio-characteristic neurons are challenging to implement in computing platforms. Majority of existing SNNs employ sim- ple LIF [33, 41] neurons or their parameterized variants [5, 28], which limits the performance of SNNs. Inspired by the filtering of information by dendrites in biological neurons [18, 25], this work proposes the attention spiking neuron to model the dendrite with a lightweight channel-spatial attention mechanism. This significantly improves the bionicity and discrimination of spiking neurons with negligible parameter overhead.\nSpecifically, the attention mechanism is used in the process of charging the membrane potential of a spiking neuron to discrimi- nate information in the input current. The charging process of a spiking neuron can be reformulated as:\n\\begin{equation}\\label{eq14} H_{i}^{l}(t) = f(U_{i}^{l} (t - 1), \\hat{I}_{i}^{l} (t)), \\end{equation}\nwhere $\\hat{I}_{i}^{l} (t)$ is the input current filtered by the attention mechanism, expressed as:\n\\begin{equation}\\label{eq15} \\hat{I}(t) = f_{att} (I(t)), \\end{equation}\nwhere $f_{att}()$ denotes the attention mechanism. This is similar in form to the attention discrimination mechanism (ADM) in [4]. However, ADM uses a vanilla convolution layer and a sigmoid function as its attention mechanism. This work, on the other hand, employs a lightweight channel-spatial attention mechanism with less parameter overhead and superior performance.\nAs shown in Fig. 2, the attention mechanism in the proposed attention spiking neuron is composed of two elements: channel attention and spatial attention. For channel attention, the input current is globally averaged in the spatial dimension, and then the channel-wise attention scores are calculated adaptively in the channel dimension using a one-dimensional convolution. This was inspired by [31], and the number of additional parameters required is only the convolution kernel size. The spatial attention is sim- ilar to the channel attention where the input current is globally averaged in the channel dimension, and then the attention scores are adaptively calculated in the spatial dimension using another one-dimensional convolution. The element-wise attention weights for filtering the input current are obtained by the product of the channel attention score and the spatial attention score and the sig- moid function. Assuming a one-dimensional convolution of size k, channel attention and spatial attention need only 2 \u00d7 k additional parameters. In this case, the ADM [4] with vanilla convolution requires c \u00d7 c \u00d7 k additional parameters, where c is the numbers of the channel.\nIn this paper, attention filtering is coupled with the internal dy- namics of LIF neurons. Let $I^{l}(t) \\in \\mathbb{R}^{b \\times c \\times s}$ be the input current, where b denotes the batch size, c is the number of channels, and s in- dicates the length of the spatial dimension. Global average pooling of $I^{l} (t)$ in spatial and channel dimensions yields $\\text{AVG}(t) \\in \\mathbb{R}^{b \\times c \\times 1}$ and $\\text{AVG}(t) \\in \\mathbb{R}^{b \\times 1 \\times s}$. To enable the one-dimensional convolu- tion operation, $\\text{AVG}(t)$ is transposed to $\\text{AVG}(t) \\in \\mathbb{R}^{b \\times 1 \\times c}$. Then one-dimensional convolution is applied to obtain the channel at- tention score $w_{ca}$ and the spatial attention score $w_{sa}$:\n\\begin{equation}\\label{eq16} w_{ca} = conv_{1\\times ke} (\\text{AVG}(t)), \\end{equation}"}, {"title": "5.1 Dataset Description", "content": "\\begin{equation}\\label{eq17} w_{sa} = conv_{1\\times ks} (\\text{AVG}(t)), \\end{equation}\nwhere $k_{c}$ and $k_{s}$ denote the size of the convolution kernel for chan- nel attention and spatial attention, respectively. In this work, $k_{s}$ is set to 7 and $k_{e}$ follows [31]: $k_{c} = |\\text{log}_{2}(c) + 1|_{odd}$.\nThen, $w_{ca}$ is transposed to $\\hat{w}_{ca} \\in \\mathbb{R}^{b \\times c \\times 1}$ to obtain the channel- wise attention score. The element-wise attention weights $w \\in \\mathbb{R}^{b \\times c \\times s}$ are calculated as:\n\\begin{equation}\\label{eq18} w = \\sigma(\\hat{w}_{ca} \\otimes w_{sa}), \\end{equation}\nwhere $\\sigma(\\cdot)$ is the sigmoid function and $\\otimes$ denotes the product with the broadcast mechanism. The filtered input current $\\hat{I}^{l} (t)$ is:\n\\begin{equation}\\label{eq19} \\hat{I}^{l}(t) = f_{att} (I^{l}(t)) = w \\cdot I^{l}(t). \\end{equation}\nThe filtered current $\\hat{I}^{l} (t)$ replaces the original current $I^{l} (t)$, accumu- lating membrane potential and firing spikes based on the dynamics of the LIF neurons.\n5 EXPERIMENTS\n5.1 Dataset Description\n5.1.1 MFPT. The MFPT [20] Bearing Fault Dataset is a benchmark dataset for validating bearing fault diagnosis algorithms. The MFPT dataset includes normal, multiple loads outer race, inner race fault bearing data from a bearing test rig, and fault data from three real- world environments. In the experiments, we used data from one baseline condition, seven outer race fault conditions, and seven inner race fault conditions. As a result, a total of 1 normal class and 14 fault classes were generated. Each class contains 140 samples, and each sample vibration signal has a length of 1024, obtained from the raw data using non-overlapping sampling. For evaluating the fault diagnosis model, 70% of the samples were randomly divided for training the model, and the remaining 30% of the samples were used for performance evaluation.\n5.1.2 JNU. The JiangNan University (JNU) [15] bearing fault dataset was collected by Jiangnan University, China. The JNU dataset con- tains data of four health conditions: (1) normal; (2) outer-race de- fects; (3) inner-race defects; and (4) roller element defects. Vibration signals with a sampling frequency of 50 kHz were obtained at three rotating speeds, yielding a total of 12 classes. Each class contains 150 samples of length 1024, with 50% each for training and evaluation."}, {"title": "5.2 Experimental Details", "content": "All experiments were conducted with the PyTorch package. All models were trained for 100 epochs using the Adam optimizer. The initial learning rate was 0.01, scaled down to 0.1 times the previous rate every 30 epochs. The batch size is 64. For spiking neurons, \u03c4 = 2.0 and threshold 9 = 1.0, and timestep of 4 if not specified. All experiments were repeated five times with different random seeds, and the average accuracy and standard deviation were reported.\nFor performance comparison, some existing methods are repro- duced (For a fair comparison, all methods follow the same training strategy as described above):\n\u2022 ResNet: The ResNet used for comparison has the same structure as ResNet-18 in the original paper [7]. To preserve the information in the vibration signal, the first layer uses a 1 \u00d7 3 convolution to replace convolution kernel 7 and max pooling, as shown in Table 1."}, {"title": "5.3 Evaluation and Comparison", "content": "The comparative results are shown in Table 2. MRA-SNN achieved diagnostic accuracies of 94.57% and 94.18%, respectively, with only 1.75M parameters. In the comparative methods, the accuracies of DRSN [38] with 5.24M parametric quantities were only 88.95% and 93.06%. As an SNN model, DSRSN [35] has the same structure as DRSN, but the accuracy reaches only 86.13% and 92.15%, which is 8.44% and 2.03% lower than MRA-SNN. MS-ResNet [10] using the membrane potential shortcut is able to prevent performance degra- dation by achieving identity mapping in SNNs, whose diagnostic"}, {"title": "5.4 Ablation Study", "content": "We perform ablation studies to investigate the effects of each com- ponent of the proposed method. We construct the following com- parative models for ablation studies:\n\u2022 MRA-LIF: MRA-LIF adopts the MRA-SNN framework but with vanilla LIF neurons.\n\u2022 MRA-ADM: MRA-ADM employs the MRA-SNN framework and filters the input current using ADM [4] within LIF neurons.\n\u2022 MA-ResNet: MA-ResNet replaces the spike residual attention blocks in MRA-SNN with vanilla spike residual blocks, leaving the other components unchanged.\n\u2022 MR-SNN: MR-SNN removes the channel attention mechanism from the multi-scale attention encoding module, leaving the other components unchanged."}, {"title": "5.5 Robustness Evaluation and Comparison", "content": "Due to the harshness of the actual working conditions, the vibration signals are inevitably affected by noise. The noise robustness of fault diagnosis algorithms is extremely critical for deployment in real-world scenarios. To evaluate the noise robustness of the pro- posed method, we add noise to the raw vibration signal to obtain different signal-to-noise ratios (SNR). For specific details on adding noise, see Appendix D. The noise robustness evaluation and com- parative results on the MFPT and JNU datasets are shown in Table 4 and Table 5, respectively. When the noise influence is weak (SNR greater than 20 dB), the performance of the different fault diagnosis models is slightly affected. As the noise increases, the performance of the diagnosis models degrades dramatically, especially for ResNet on MFPT. Compared to other comparative models, the proposed MRA-SNN has consistently higher diagnostic accuracy at any SNR. Additional visualizations and accuracy change curves during noise interference are provided in Appendix E, which more visually illustrates the noise robustness of the MRA-SNN. The robust diag- nostic performance under noise interference demonstrates that our MRA-SNN can be better applied in real-world scenarios, opening up further opportunities for deployment."}, {"title": "5.6 Output Visualization", "content": "To make the biological plausibility of MRA-SNN more intuitive, we visualized its output in four timesteps, as shown in Fig. 4. For samples belonging to target classes 0 and 3, the output neurons at the corresponding positions all generated larger predictive values. For the other non-target classes, MRA-SNN generated significantly smaller predictions, which exhibited excellent distinguishability. It is worth noting that the predicted values output by MRA-SNN at the first timestep (indexed at 0) are not discriminative. The expla- nation for this is that the SNN is just accumulating the membrane potential at the first timestep and therefore produces few and un- stable spikes. As the timestep continues, the SNN is able to make accurate predictions (the last three timesteps in Fig. 4)."}, {"title": "5.7 Influence of Timestep", "content": "We evaluate the performance of the MRA-SNN with different timesteps to validate its performance efficiency trade-off. The results are shown in Fig. 5. As the timestep increases, the overall performance of MRA-SNN gradually improves. When the timestep is 4, the per- formance of MRA-SNN saturates and degrades as the timestep increases. It is worth noting that even with a timestep of 1, the av- erage accuracy of MRA-SNN on MFPT is still above 91.17%, which exceeds the other comparative models in Table 2. In this case, the MRA-SNN provides an excellent balance of performance and effi- ciency. For JNU, the influence of the timestep on MRA-SNN is more"}, {"title": "6 CONCLUSION", "content": "In this paper, we facilitate the application of SNNs to industrial scenarios by proposing MRA-SNN for end-to-end bearing fault di- agnosis. Our MRA-SNN offers a lightweight architecture, superior spike encoding and feature extraction capabilities for efficient and effective fault diagnosis without pre-processing vibration signals. In addition, a lightweight attention spiking neuron that mimics biological synaptic filtering through a separated channel-spatial attention mechanism and enhances the performance and robust- ness of the MRA-SNN was presented. Extensive experiments on the challenging MFPT and JNU datasets show that our MRA-SNN outperforms existing SNN fault diagnosis methods, and even ANN models, in terms of performance and noise robustness. We expect that this will facilitate the application and deployment of SNNs in more real-world scenarios. In addition, given the inherent similari- ties between vibration signals and time series data, we also expect this work to contribute to time series classification, and we leave this as future work."}, {"title": "APPENDICES", "content": "A MULTI-SCALE FEATURE VISUALIZATION\nTo clearly illustrate the effect of the multi-scale attention encod- ing module, Fig. 6 visualizes the outputs of the three convolution pathways and the spiking neuron layer at the first two timesteps. Each of the three convolution pathways extracted different features related to the fault, and the spiking neurons incorporating the fused input currents generated discrete 0-1 spikes. These 0-1 spikes are passed as input to the follow-on spike residual block for further fault diagnosis.\nB ENERGY ANALYSIS\nIn neural networks, the number of floating-point operations (FLOPs) is a typical metric used to evaluate the computational burden. For ANNs, their floating point operations are all MAC operations. As for the SNN, all are binary spike features, i.e., AC operations, except for the first layer, where the inputs are floating point values that introduce MAC operations. Similar to [14", "21": "."}]}