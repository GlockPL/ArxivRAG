{"title": "JOINT FINE-TUNING AND CONVERSION OF PRE-TRAINED SPEECH AND LANGUAGE MODELS TOWARDS LINEAR COMPLEXITY", "authors": ["Mutian He", "Philip N. Garner"], "abstract": "Architectures such as Linformer and Mamba have recently emerged as competitive linear time replacements for transformers. However, corresponding large pre-trained models are often unavailable, especially in non-text domains. To remedy this, we present a Cross-Architecture Layerwise Distillation (CALD) approach that jointly converts a transformer model to a linear time substitute and fine-tunes it to a target task. We also compare several means to guide the fine-tuning to optimally retain the desired inference capability from the original model. The methods differ in their use of the target model and the trajectory of the parameters. In a series of empirical studies on language processing, language modeling, and speech processing, we show that CALD can effectively recover the result of the original model, and that the guiding strategy contributes to the result. Some reasons for the variation are suggested.", "sections": [{"title": "INTRODUCTION", "content": "Recent progress in language modeling from BERT (Devlin et al., 2019) to Llama3 (Dubey et al., 2024) has witnessed the rise of the attention mechanism and transformer architecture. Such models are able to scale up to very large model capacity and pretraining data for natural language processing (NLP), and have lead to breakthroughs in a broad range of downstream applications under either fine-tuning or zero-shot/in-context scenarios. This is surely not restricted to the text domain: Off-the-shelf speech and audio models like Wav2Vec2 (Baevski et al., 2020), HuBERT (Hsu et al., 2021), and WavLM (Chen et al., 2022) are pretrained on extensive audio data; they can be fine-tuned to obtain good performance on various tasks from automatic speech recognition (ASR) to spoken language understanding (SLU). However, such large-scale models are computationally expensive; the main reason is that in the standard attention mechanism, each token interacts with all other tokens in the input sequence, forming a complete graph. The computation time therefore increases quadratically with the input length, and a linear-growing KV-cache is required. As a result, the computation requirements can become prohibitive, especially for long contexts. A large cohort of architectures and algorithms have been proposed to mitigate the problem and to reach linear complexity, using techniques such as low-rank projection, hashing, and sparse attention (Tay et al., 2023). More recently, there has been a revival of recurrent networks such as state space models (Gu et al., 2022; Gu & Dao, 2023), RWKV (Peng et al., 2023), and RetNet (Sun et al., 2023). By recurrently updating states, those models allow linear time and constant space inference while retaining the transformer's parallel training capability. It has been demonstrated that the latest recurrent models, notably Mamba2 (Dao & Gu, 2024), are capable of reaching performance comparative with the most well-trained transformer, even in large-scale models.\nIn spite of such transformer substitutes, there remain obstacles to putting them into practical use. These new models are not simply plug-and-play upgrades for existing pretrained models, but are generally pretrained from scratch again. The pretrained weights of these models are often not publicly released; the data and computational cost of repeating such pretraining for every new model can be prohibitive for users and researchers of downstream applications. In particular, most models are primarily developed for processing language or text. These models are typically general-purpose"}, {"title": "RELATED WORK", "content": ""}, {"title": "LINEAR-COMPLEXITY AND RECURRENT MODELS", "content": ""}, {"title": "Linear-complexity models.", "content": "There is a large body of work on reducing the $O(N^2)$ time complexity of transformer owing to the attention computation $A(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d}})V$. The $N \\times d$ matrices Q, K, and V represent the sequence of N query, key, and value feature vectors, each with constant d dimensions. A straightforward approach is to limit the attention to several pre-specified positions in the sequence, e.g., neighboring tokens, several special global tokens, or a sparse subset of the sequence (Child et al., 2019; Beltagy et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020). Alternatively, the attended positions can be determined on-the-fly by hashing and clustering (Kitaev et al., 2020; Roy et al., 2021). With Linformer, Wang et al. (2020) further found that the attention"}, {"title": "Recurrent models.", "content": "By applying a kernel trick in a reverse direction, Katharopoulos et al. (2020) reduce transformers to RNNs of infinite dimension. In this RNN, hidden states $z_t = \\sum_{i<t} \\phi(K_{i,:})$ are the sum of an infinite-dimensional feature $\\phi(K_{i,:})$ mapped from each key vector (more specifically, along with their product with values). Using various finite dimension approximations of $\\phi$ (Peng et al., 2021; Choromanski et al., 2021; Zhai et al., 2021), the RNN can be implemented as an approximation of the standard attention, which paves the way for the later revival of recurrent models. Such models are nevertheless different from classical RNNs in that the training can be parallel as long as there is no non-linear transform in the transition from $z_t$ to $z_{t+1}$. In addition to simply summing up features as in $z_t$ above, RWKV (Peng et al., 2023) and RetNet (Sun et al., 2023) use exponential time decay of past features. Gating (Yang et al., 2024) or linear transform (Orvieto et al., 2023) have also been introduced. Similarly, a state-space model (SSM) is defined as a transformation of a 1-D input sequence x(t) to output y(t) via an N-D hidden state h(t), following\n$h'(t) = Ah(t) + Bx(t)$\n$y(t) = Ch(t)$"}, {"title": "DISTILLATION AND MODEL CONVERSION", "content": ""}, {"title": "Knowledge distillation.", "content": "Distilling knowledge from teacher models to a student one by matching the output probabilities is a standard approach to compress ML models, including pretrained language models (LMs) (Hinton et al., 2015; Sanh et al., 2019). Intermediate hidden states of the teacher and student can be further aligned by layerwise distillation (Sun et al., 2019; Aguilar et al., 2020). In addition, Brown et al. (2023) have attempted to distill and compress existing pretrained linear-complexity LMs. In these works, parameters from the teacher are also transferred to the student when possible; an example of the direct parameter transfer approach."}, {"title": "Model conversion.", "content": "Distinct from model compression, there is also a series of works aimed at converting (a.k.a. uptraining) existing models into new, more efficient ones. Parameter transfer is generally applied, e.g., in the conversion from standard attention to sparse attention (Zaheer et al., 2020), RNN (Kasai et al., 2021), linear attention (Mao, 2022; Mercat et al., 2024), and multi/grouped-query attention (Ainslie et al., 2023). Distillation, or behavior transfer, from the standard transformer as the teacher has been also introduced, starting from early attempts to convert transformers to RNN using teacher outputs or generations (Senellart et al., 2018; Tang et al., 2019). Zhang et al. (2024a) further tried to reproduce the attention matrix of standard attention in linear attention with a loss to align the teacher and student attention weights. Bhati et al. (2024) also used output distillation for pretraining Mamba-based speech models, but without parameter transfer. A concurrent work then combined the distillation on attention weights, outputs, and hidden states (Bick et al., 2024), and managed to convert large-scale LMs into Mamba with limited computation costs and performance"}, {"title": "METHODS", "content": "We aim at building a broadly applicable framework to convert an existing pretrained model into a task-specific linear-complexity one. To achieve this goal, we combine parameter transfer and distillation to best reproduce the performance of the target teacher model, a standard transformer model fine-tuned on the target task from the pretrained source teacher model. For parameter transfer, we replace only the attention layers in the teacher with the more efficient sequence-mixing modules we intend to use, forming the student model as a result; most other layers including embeddings, feed-forward, and layernorm will be preserved. Following this approach, the teacher parameters can be used to initialize the student model as much as possible. This constitutes our baseline unguided approach, in which we directly fine-tune the student model with transferred parameters. Instead, by posing loss on the difference of hidden states, we enforce constraints on the deviation of the student's hidden states from the teacher. In this way, we directly guide each layer in the student to reproduce the behavior of the corresponding teacher layer. Different possible modes of such guidance are illustrated in Figure 3 and introduced below:\n\u2022 Target Guided. We can directly transfer the parameters from the fine-tuned teacher target model and distill from it. More specifically, given the model inputs for training on a target classification task, we denote the hidden states from the student and the teacher target as $H^{(s)}$ and $H^{(t)}$, each with m vectors; outputs (class probabilities) of the student and the teacher target as $y^{(s)}$ and $y^{(t)}$; and one-hot labels as y. Then the loss terms are written as\n$L_{CE}(y^{(s)}, y) = \\sum_{i} Yi log(y_i^{(s)})$\n$L_{KD}(y^{(s)}, y^{(t)}) = \\sum_i (\\frac{y_i^{(t)}}{\\sum_j y_j^{(t)}}) log \\frac{y_i^{(s)}}{\\sum_j y_j^{(s)}}$ \n$L_{LD}(H^{(s)}, H^{(t)}) = \\frac{1}{m} \\sum_{i=1}^m ||H_i^{(s)} - H_i^{(t)}||^2$\n$L = \\alpha_{CE}L_{CE} + \\alpha_{KD}L_{KD} + \\alpha_{LD}L_{LD}$\nWe set the temperature \u03b2 = 2 following Hinton et al. (2015). The final loss L is a combination of the cross entropy $L_{CE}$, the KL divergence $L_{KD}$ between teacher and student outputs in standard KD, and an MSE loss $L_{LD}$ between hidden states for layerwise distillation. The three terms are weighted by a set of hyperparameters a. Instead, the unguided mode uses $L_{CE}$ only. More specifically, as the target sequence-mixing module (e.g., RNN) is nevertheless distinct from standard attention, it can be more difficult to reproduce the hidden states right after the sequence-mixing layer. Instead, we extract the hidden states after the processing of the following feed-forward layer as H for layerwise distillation.\n\u2022 Trajectory guided. Models initialized from pretrained parameters perform much better than random initialization when trained on downstream tasks. This can be explained by the rich knowledge internalized into the pretrained model, which may be lost during fine-tuning. It has been found that preserving the knowledge during fine-tuning can be helpful for downstream performance (Li et al., 2018; He & Garner, 2023). Therefore, it can be sub-optimal to only reproduce the hidden states produced by the fine-tuned teacher target model, which has already lost certain knowledge. We are further inspired by Li et al. (2019)"}, {"title": "EMPIRICAL STUDIES", "content": ""}, {"title": "MODEL CONFIGURATION", "content": "We carry out experiments on several scenarios that are representative to demonstrate the advantages of our approach, including\n\u2022 Converting RoBERTa to Linformer, fine-tuned on language processing tasks, and\n\u2022 Converting Wav2Vec2 to the latest Mamba2, fine-tuned on speech tasks.\nAlthough not our focus, we also carry out an extra experiment to convert a widely-used open-source LM of various sizes, namely Pythia (Biderman et al., 2023), into Mamba for language modeling by retraining on a small subset of the pretraining corpus.\nAs mentioned above, we build the target architecture that allows parameter transfer as much as possible. In this way, the architecture may not be identical to the standard Linformer, Mamba, or Mamba2, as specifically described below:\n\u2022 Language processing by RoBERTa \u2192 Linformer: We only add the E and F matrices to project the hidden features, which are randomly initialized by $\\mathcal{N}(0, 1)$ to preserve the scale of features after projection. All other parts of the model are left unchanged and initialized with pretrained RoBERTa parameters. We follow the reported best Linformer configuration (KV or layer-shared E and F, with rank k = 256).\n\u2022 Language modeling by Pythia \u2192 Mamba: We only replace each attention layer with a Mamba mixer of the same hidden size, initialized under the Mamba scheme.\n\u2022 Speech processing by Wav2Vec2 \u2192 Mamba2: Wav2Vec2 is a bidirectional model, while Mamba2 is unidirectional. Hence we build a bidirectional Mamba2 similar to the one by Zhang et al. (2024b). Each attention layer is replaced by two Mamba2 mixers of the same hidden size, but with expand factor $\\epsilon = 1$, as shown in Figure 4.1.\nIn this way, all the models have roughly the same number of parameters after conversion. Classification tasks are carried out by adding a linear prediction head after mean pooling. The modeling and training configuration under each scenario is further introduced below, and more details including hyperparameters are provided in Appendix A."}, {"title": "LANGUAGE PROCESSING", "content": "We follow the settings in the Linformer paper to fine-tune the models and report the validation accuracy on a set of widely-used benchmarks: QNLI (Rajpurkar et al., 2016) for natural language inference, QQP (Iyer et al., 2017) for text similarity, as well as SST2 (Socher et al., 2013) and IMDB (Maas et al., 2011) for sentiment classification; the former three being part of the GLUE benchmark (Wang et al., 2019). The models are converted from and compared with pretrained ROBERTa-base."}, {"title": "LANGUAGE MODELING", "content": "We then present the results of experiments on converting Pythia to Mamba for language modeling. This is nevertheless not our focus as recent pretrained LMs are typically directly evaluated by zero or few-shot performance in downstream tasks without fine-tuning. Therefore, we will essentially need to do the re-pretraining under this scenario. Yet we try to convert the model by training only on a tiny (0.5%, 1%, or 2%) subset of the pretraining corpus, i.e. the deduplicated Pile (Gao et al., 2021; Biderman et al., 2023). Under this scenario, we can only compare the unguided, target guided, and hybrid approaches. Models are trained for 2 epochs. Due to limitations in computational resources, we perform experiments only on Pythia-1B.\nWe evaluate the models using a set of benchmarks based on those reported in the Pythia paper as shown in Table 2. Compared with the standard Pythia-1B, there remains a performance gap. Nevertheless, we can find that the gap gets limited using 2% data. More importantly, the CALD models consistently get better results than the unguided ones, especially with the hybrid method. We also observe that the unguided training is rather unstable with frequent NaNs. All the results support the effectiveness of our CALD approach."}, {"title": "SPEECH PROCESSING", "content": "We then convert the Wav2Vec2-large models into the latest Mamba2, using a bidirectional architecture specified above. We evaluate them on three common speech tasks with corresponding common benchmarks: ASR with TED-LIUMv3 (Hernandez et al., 2018), intent classification, a standard SLU task, with SLURP (Bastianelli et al., 2020), along with speaker ID with VoxCeleb1 (Nagrani et al., 2020), following the SUPERB configuration (Yang et al., 2021). Specifically, ASR training is performed by CTC loss instead of cross entropy as $L_{CE}$. The converted models are compared with standard fine-tuned Wav2Vec2 models. We take a waypoint every 10,000 steps during fine-tuning. Test word error rate (WER) and accuracy are reported respectively.\nResults are given in Table 3, which are consistent with previous findings. There is a significant gap between the performance of standard Wav2Vec2 and the unguided conversion. This can be largely alleviated by CALD using the target guided approach, reaching results close to or even better than standard Wav2Vec2. The hybrid approach further brings slight improvements on ASR and IC.\nHowever, the waypoint guided approach is found to be not helpful or even harmful; we choose not to proceed with the more precise trajectory guided approach. Our assumption is that the trajectory guided approach helps retain the knowledge in the hidden states of the pretrained model. Hence We hypothesize that the hidden states of the Wav2Vec2 model undergo significant shift during fine-tuning. As a result, the hidden states of the original pretrained model or those in the early phase of fine-tuning are less useful for the target task. To examine the hypothesis, for each model, we"}, {"title": "HIDDEN STATE TRAJECTORY", "content": "We further visualize the trajectory of hidden states during fine-tuning on the QNLI dataset using different guidance modes by applying t-SNE to concatenated hidden states of 20 random samples in the training set. Hidden states produced by every half-epoch checkpoint are included, hence a total of 20 points during training are plotted for each model. As shown in Figure 4.4, the actual trajectory matches our expectation conceptualized in Figure 3 that the target guided model approaches the fine-tuned target teacher model (the end of the green trajectory), while the waypoint or trajectory guided models further mimic the trajectory, which is found to be beneficial to the final results. In sharp contrast, the unguided model will completely deviate from the teacher, which leads to training instability and failure to reproduce the original performance."}, {"title": "CONCLUSION", "content": "We investigate the task of converting various existing pretrained transformer models into efficient linear-complexity architectures without the need to redo the whole pretraining. For this goal, we propose a novel and broadly compatible Cross Architecture Layerwise Distillation (CALD) approach, and further enhance CALD with a trajectory-based guidance or a hybrid approach. Through our empirical experiments, we confirm that CALD can effectively convert the transformer into an efficient linear-complexity model with performance close to or on par with the standard transformer, much better than directly converted models. We also verify the versatility of CALD by experiments on multiple tasks with various speech and language models. We further identify the tasks where the enhancement approaches are effective in improving the results."}, {"title": "IMPLEMENTATION DETAILS", "content": "We build the models as mentioned above, and train the models using the AdamW optimizer. On Mamba/Mamba2 models, we also find it important to apply layer normalization on hidden states prior to computing the layerwise distillation loss, i.e. we take the hidden states after the layernorm operation at the following layer. Otherwise the hidden states will have a rather wide range and bring training instability. We only use output distillation by setting $\\alpha_{KD} = 1$ in ASR experiments. On other experiments, we find that the distillation on output probabilities actually leads to suboptimal results, and thus set $\\alpha_{KD} = 0$. We always use $\\alpha_{CE} = 1$. As for the hybrid mode of distillation, we start with the optimal target-guided configuration, but switch to unguided mode when the loss is close to converge, generally at around 30% of the total training steps.\nFor each target task, we train the models based on its common training recipe for standard transformers, and further search and determine hyperparameters and the model checkpoint to use according to the validation performance. Details for each task are listed below, and more specific details can be accessed from our source code."}, {"title": "Language processing:", "content": "We train the model using a learning rate (LR) schedule of linear decay with 6% warmup steps. A total batch size of 32 is used. Other hyperparameters are listed in Table 4."}, {"title": "Language modeling:", "content": "We train the model using a learning rate of 3e-4 and a schedule of cosine decay with 3% warmup steps and a 3e-5 minimum learning rate. We use a total batch size of 128, each with 2048 tokens. We also apply the weight decay of scale 0.1 on Mamba parameters following the Mamba training scheme, along with gradient norm clipped to 1. We set $\\alpha_{LD} = 15$ in the experiments. FP16 mixed precision training is adopted."}, {"title": "Speech processing:", "content": "We train the model using a learning rate schedule of exponential decay with 5000 warmup steps and 1.5e-6 minimum learning rate. We use a dynamic batch strategy that leads to a batch size of approximately 64 in average. We also apply the weight decay of scale 5e-3, along with gradient norm clipped to 1. For ASR, IC, and SID, we set $\\alpha_{LD}$ to 15, 15, and 30, respectively. BF16 mixed precision training is adopted. We find that these hyperparameters work well across different modes of distillation."}]}