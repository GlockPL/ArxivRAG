{"title": "Autonomous Robot for Disaster Mapping and Victim\nLocalization", "authors": ["Michael Potter", "Rahil Bhowal", "Richard Zhao", "Anuj Patel", "Jingming Cheng"], "abstract": "In response to the critical need for effective\nreconnaissance in disaster scenarios, this research article\npresents the design and implementation of a complete\nautonomous robot system using the Turtlebot3 Burger (TB3)\nwith Robot Operating System (ROS) Noetic. Upon deployment\nin closed, initially unknown environments, the system aims to\ngenerate a comprehensive map and identify any present 'victims'\nusing AprilTags as stand-ins. We discuss our solution for search\nand rescue missions, while additionally exploring more advanced\nalgorithms to improve search and rescue functionalities. We\nintroduce a Cubature Kalman Filter (CKF) to help reduce\nthe Mean Squared Error (MSE) [m] for AprilTag localization\nand an information-theoretic exploration algorithm to expedite\nexploration in unknown environments. Just like turtles, our\nsystem takes it slow and steady, but when it's time to save the\nday, it moves at ninja-like speed! Despite Donatello's shell, he's\nno slowpoke he zips through obstacles with the agility of a\nteenage mutant ninja turtle. So, hang on tight to your shells\nand get ready for a whirlwind of reconnaissance!", "sections": [{"title": "I. PROBLEM STATEMENT", "content": "In the wake of disasters, rapid and efficient reconnaissance is\ncritical for successful rescue operations. Traditionally, human\nresponders perform this task, but they often face considerable\nrisks and limitations in accessibility. The development of\nautonomous robotic systems presents a promising alternative,\ncapable of navigating and mapping disaster environments\nquickly and safely. However, existing autonomous systems\nprimarily focus on mapping and fail to effectively integrate\nvictim identification within their operational protocols.\nThis project we addresses the need for an autonomous robotic\nsystem that can both map unknown, closed environments and\nidentify \"victims(AprilTags)\" in these settings. Our goal is\nto design and implement a robust system using the Robot\nOperating System (ROS) Noetic, which can:\n\u2022\n\u2022\nGenerate a complete occupancy grid map of an unknown\nenvironment, providing essential data for navigation and\nstrategic planning.\nLocate and accurately estimate the poses of \"victims,\"\nrepresented by AprilTags, to simulate the identification\nand location of individuals in need of rescue.\nThe system must operate entirely autonomously, adapting\nto the dynamic and unpredictable nature of disaster envi-\nronments. The key challenges include integrating advanced\nmapping techniques with efficient victim detection algorithms,\nensuring the system's reliability in diverse conditions, and\nmanaging computational constraints potentially imposed by\nthe hardware used."}, {"title": "II. SETUP", "content": "The PC setup, Single Board Computer setup, and OpenCR\nsetup were completed by adhering to the Turtlebot3 Burger\n(TB3) setup instructions provided on the Robotis website [1].\nSubsequently, we progressed to more advanced software and\nhardware configurations after the initial setup.\nA. TurtleBot3 Hardware Setup\nThe TB3 is a widely used, compact, and customizable robotic\nplatform suitable for a variety of robotic applications, par-"}, {"title": "III. ENVIRONMENT", "content": "A. Gazebo World\nFor the experimental setup, we created two Gazebo [4] en-\nvironments to simulate a search and rescue mission with\nAprilTags serving as victims. A total of 12 AprilTags were set\nup, each measuring around 100mm x 100mm. Each tag has\na unique ID between 0-11 and belongs to the family of April\ntag 36h_11 [5]. The reference worlds were turtlebot3_house as\nshown in Fig. 1(a) and turtlebot3_world as shown in Fig. 1(b).\nA third Gazebo environment (Fig. 1(c)) was created to com-\npare explore_lite package and our exploration implementation.\nB. Real World\nFor the experimental setup in the real world, we attempted\nto set up the arena using cardboard and placed 8 AprilTags\nbelonging to the April tag 36h_11 family, each with a unique\nID between 0-7. The maze setup is shown in Fig. 2."}, {"title": "IV. METHODOLOGY", "content": "A. Proposed Solution\nAs outlined in Section II, our initial setup utilized existing\npackages such as explore_lite [6] for frontier explo-"}, {"title": "B. Exploration", "content": "The explore_lite package is used for real world demo.\nIt's a simple frontier-based exploration algorithm that picks\nthe lowest score frontier centroid as the exploration goal. A\nfrontier's cost is computed as a weighted difference between\nthe robot's distance to its closest frontier cell and the frontier's\nsize (number of frontier cells).\nIt is able to achieve exploration, but has some limitations:\n\u2022\n\u2022\nThe chosen exploration goal's orientation is left as zero,\nwhich is not optimal for discovering unknown areas,\nespecially if the sensor has limited FOV sensor, such as\ncamera.\nor\n\u2022\nThe chosen exploration goal might be unsafe\nuntraversable: A frontier's centroid may be located\nin unknown or occupied space. This can cause\nexplore_lite to be stuck until a lack of progress\ntimeout is reached, then the node blacklists that goal.\nThe frontier detection is not efficient: Every iteration\nstarts anew and performs an entire map search for fron-\ntier cells, starting from the closest free cell. This isn't\nnoticeable in small to medium sized maps as the planner\nfrequency is 0.33 Hz, but it's expected to be slow for\nlarger maps.\nWe implemented a more efficient and effective exploration\nalgorithm that combines ideas from both frontiers and next\nbest view exploration methods.\nThe exploration algorithm consist of two parts: a) frontier\ndetection and b) computation of exploration goal. The frontier\ndetection is responsible for detecting new frontier cells and\nforming them into frontiers. This runs continuously in the\nbackground. The computation of exploration goal is responsi-\nble to return an exploration goal based on the detected fron-\ntiers. This runs whenever the associated service is requested.\nFrontier Detection: The frontier detection implements the\nalgorithm Expanding Wavefront Frontier Detection (pseudo-\ncode in [10]). It is a BFS algorithm that traverses free cells\non the map to detect new frontier cells, as shown in Fig. 4.\nEach free cell inserted into the queue is marked as visited\nand evaluated to ascertain its status as a frontier cell. If it's a"}, {"title": "Computation of Exploration Goal:", "content": "As discussed, choosing a\nposition among frontier centroids can have several limitations.\nFor this reason, we used ideas from next best view planners\nto sample around free space for an optimal exploration goal.\nThe algorithm for choosing an exploration goal is as follows:\n1) Sample M points around each frontier's centroid in the\nfree space (Fig. 5).\n2) Choose an optimal orientation for each sampled point that\nmaximizes the information gain.\n3) Compute the potential gain for each pose.\n4) Choose the pose with highest potential gain as the explo-\nration goal (Fig. 5).\nWe compute the total unknown area that the robot would\ndiscover as the information gain, similar to [12]. The infor-\nmation gain and the orientation optimization are implemented\naccording to the next algorithm [13]:\n1) Discretize 360\u00b0 into rays.\n2) For each ray i, count the number of unknowns cells $N_i$.\nThe information gain for this ray is computed as the\nlength of unknown cells: $I_i = N_i \\text{ map resolution}$\n3) For each orientation $\\theta$, the information gain is computed\nby summing up each ray's information gain in the sen-\nsor's field of view:\n$I_{\\theta} = \\sum_{i \\in \\text{sensor fov}} I_i$ (1)\n4) Choose orientation $\\theta^*$ that maximizes information gain.\nThe potential gain G(X) for each pose X = {p,0} is\ncomputed using information gain and time cost. Let the robot's\ncurrent pose be denoted as $X_r = {p_r, \\theta_r}$ and its maximum\nvelocities be {$\\omega_{max}, V_{max}$}, the potential cost is given by:\n$G(X) = \\frac{I(X)}{T(X)}$ (2)\n$T(X) = \\max( \\frac{\\omega - \\theta_r}{\\omega_{max}} , \\frac{||p-p_r||}{V_{max}} ) $ (3)\nThe time cost T(X) is a simple estimate that assumes there is\na straight, collision-free path toward the given pose, and the\nrobot is moving with maximum velocities. The information\ngain I(X) is computed as mentioned. When scoring sampled\npoints, we use their optimal orientation."}, {"title": "C. AprilTag Pose Estimation", "content": "Qualitatively, we observed that the apriltag_ros package\ndisplayed estimator bias in estimating the AprilTag position."}, {"title": "D. Search and Rescue", "content": "To ensure all AprilTags are detected in the environ-\nment, our custom node search_and_rescue waits for the\nexplore_lite package to finish exploring the environment.\nOnce the exploration_flag is received, indicating the\ncompletion of the exploration phase and the start of the\nsearch and rescue operation, the approach utilizes a grid-\nbased decomposition of the environment represented by the\noccupancy grid map.\nThe path planning process includes the following key steps:\n1) Inflated Occupancy Grid: The occupancy grid map un-\ndergoes inflation to accommodate the robot's physical\ndimensions, ensuring obstacles are buffered by a safety\nmargin during path planning. Free Space Identification:\nThe inflated map is analyzed to identify unblocked areas,\nwhich constitute the free space accessible to the robot."}, {"title": "V. RESULTS", "content": "A. Survivor Localization\nDue to the challenge of acquiring actual ground truth positions\nof AprilTags in real-life arenas, we utilize Gazebo to simulate\narenas, deliberately placing 11 AprilTags across the map. The\nground truth positions of AprilTags can be extracted from the\nlaunch files of the Gazebo simulated arenas. We specifically\nexamine two simulated arenas: the TB3 World and the House\nWorld.\n1) TurtleBot3 World: The tb3 World features numerous pillars\nlocated in the center of the arena, resulting in occlusion of the\nAprilTags within the camera's field of view. We hypothesize\nthat this occlusion adversely affects the perspective-n-point\ntransformations used to estimate the relative positions of April-\nTags from the raspicam. Consequently, the final measurements\nfrom the april_tag ROS node may result in inaccurate\nposition estimates of the AprilTags. This issue is exacerbated\nas the distance between the AprilTag and the tb3 increases.\nA visual of the apriltag_ros estimator, CKF estimator,\nand ground truth AprilTag position is shown in Fig. 9\n2) House: The House contains open, spacious rooms with\nminimal obstacles, resulting in little occlusion of the AprilTags\nwithin the camera's field of view. However, the absence\nof occlusion allows the apriltag_ros package to detect\nAprilTags from greater distances, leading to more erroneous\nmeasurements compared to the tb3 World arena. We have\nobserved a direct bias estimation error that is proportional\nto the distance. A visual of the apriltag_ros estimator\n, CKF estimator, and ground truth AprilTag position is shown\nin Fig. 10\nWe observed that the CKF mitigated most of the issues\ndescribed above. The CKF estimate, which uses many mea-\nsurements of the same AprilTag, resulted in an improved\nMean Squared Error (MSE) for the AprilTag position estimates\n(Table I)."}, {"title": "B. Survivors Detected", "content": "We demonstrated that our search and rescue algorithm almost\nalways detects and localizes the AprilTags. However, there\nwere cases where the tb3 failed to detect a single AprilTag\n(out of 8 AprilTags)."}, {"title": "C. Time to Explore Map", "content": "The explore_lite package and custom exploration package\nare compared in two gazebo maps: House(video 1) and\nMaze(video 2). The simulated LIDAR has a range 10m and a\n360\u00b0 FOV."}, {"title": "VI. LESSONS LEARNED", "content": "Through our experiences, we have gained insights from the\nchallenges encountered. Let us delve into the four notable\nissues we encountered along the way.\nA. Drift Issue\nDrift in mobile robots refers to the unintentional deviation\nfrom a planned path or orientation. This can stem from\nsensor inaccuracies, uneven floor surfaces, or imbalances in\nthe drive mechanism. It's crucial for precision tasks and affects\nthe reliability of autonomous operations. When the OpenCR\nboard, housing the IMU, was not securely fastened to the TB3,\nit resulted in odometry readings drifting to infinity, causing the\nTB3 to believe it strayed beyond the map boundaries.\nB. Carpet Issue\nOperating on carpeted surfaces presents unique challenges\nfor robots, the thickness and texture of carpets can impede\nmovement and operational efficiency. We discovered that the\nwheel encoder inaccurately recorded movement for the TB3\nwhen the wheels encountered resistance in the carpet, resulting\nin suboptimal motion control and incorrect odometry.\nC. Incorrect Size of April Tag\nWe initially set the length of the April Tag to 100 mm, but\nupon printing, we discovered it was actually 70 mm (refer to\nSection VI-C(a), where the estimated position is inaccurate).\nTherefore, we needed to re-measure the actual length to\navoid any distortion of the grey box of the QR code during\nscanning. Figure Section VI-C((b) shows accurate estimation\nafter measuring length of April Tag and camera calibration.\nD. Frame Rate Drop\nThis issue affects the visual processing capabilities of robots,\nwhere the frame rate of camera feeds unexpectedly drops. It\ncan result in delayed image processing and reduced image\nquality, crucially impacting tasks that require real-time data\ninterpretation. In real scenario, for the default resolution we\ndesire 15 frames per second. Sometimes it suddenly drops. To"}, {"title": "E. Hardware Problems", "content": "Robotics systems are susceptible to hardware failures, ranging\nfrom motor failures to sensor malfunctions, which can disrupt\nentire operations. For instance, our OpenCR board experienced\na short circuit, necessitating its replacement. Similarly, our\nLIDAR encountered issues with room scanning, sometimes\nfailing to receive any scans, prompting us to replace it. These\nsteps involved disassembling the TB3 and reassembling it (and\nrewiring it) with the new installations."}, {"title": "VII. FUTURE WORK", "content": "Our primary focus is on integrating and optimizing our ex-\nploration and search and rescue algorithms. Presently, our\ncustom exploration algorithm has only been tested in sim-\nulation (Gazebo) and remains disconnected with the search\nand rescue pipeline. However, we acknowledge the imperative\nneed for these algorithms to collaborate during missions,\nenabling immediate responses upon detecting victims during\nexploration. This integration ensures a more efficient and\ncohesive operation of our robotic system.\nFurthermore, we emphasize the importance of extensive testing\nin both simulated and real-world environments. While our\nexploration methods have undergone simulation testing with\nvarious Gazebo world configurations, real-world scenarios\ndemand further evaluation. By experimenting with different\nlayouts in physical arenas, we can validate the efficacy of our\nalgorithms in practical settings. This holistic approach ensures\nthe robustness and reliability of our system across diverse\nenvironments. Additionally, addressing the AprilTag position\nestimation, the measurement model should either leverage the\nperspective-n-point model or incorporate the measurement bias\nerror as a corrective function."}]}