{"title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training", "authors": ["Siyu Yuan", "Zehui Chen", "Zhiheng Xi", "Junjie Ye", "Zhengyin Du", "Jiecao Chen"], "abstract": "Large Language Models (LLMs) agents are increasingly pivotal for addressing complex tasks in interactive and agentic environments. Existing work primarily focuses on enhancing performance through behavior cloning from stronger experts, yet such approaches often falter in real-world applications, mainly due to the inability to recover from errors. However, step-level critique data is notoriously difficult and expensive to collect. Automating and dynamically constructing self-critique datasets is thus crucial to empowering models with intelligent agent capabilities. In this work, we propose an iterative self-training framework, Agent-R, that enables language Agent to Reflect on the fly. Unlike traditional methods that reward or penalize actions solely based on correctness, our approach leverages Monte Carlo Tree Search (MCTS) to construct training samples that recover correct trajectories from erroneous ones. A key challenge of agent task reflection lies in the necessity for timely revision rather than waiting until the end of a rollout to revise errors. To address this, we introduce a model-guided critique construction mechanism: the actor model identifies the first error step (within its current capability) in a failed trajectory. Starting from it, we splice it with the adjacent correct path, which shares the same parent node in the tree. This strategy enables the model to learn reflection based on its current policy, therefore yielding better learning efficiency. To further explore the scalability of this self-improvement paradigm, we investigate iterative refinement of both error correction capabilities and dataset construction. Our findings demonstrate that this approach continuously improves the model's ability to recover from errors and enables earlier/timely error correction. Extensive experiments on three representative interactive and agentic environments show that the proposed framework effectively equips agents to identify and correct erroneous actions while avoiding loops, achieving superior performance compared to baseline methods (+5.59%). Code is available at https://github.com/bytedance/Agent-R.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have become foundational tools in solving complex tasks across inter- active and agentic environments (Wang et al., 2022; Yao et al., 2022; Deng et al., 2023; Zhang et al., 2024b; Chen et al., 2024c). These LLM-based agents are increasingly employed in scenarios requiring capabilities such as autonomous decision-making, error correction, and task optimization (Prasad et al., 2024; Liu et al., 2024b). Despite the impressive performance of the existing methods, their reliance on behavior cloning from stronger experts poses significant limitations: due to the adoption of all-correct trajectories for training, it struggles to proactively self-correct errors, leading to cascading failures and suboptimal task performance (Zhang et al., 2024a; Xie et al., 2024). This limitation arises from an inability to effectively detect errors or revise trajectories dynamically once errors occur, highlighting the need for methods emphasizing timely revision capabilities.\nPrevious work has proposed methods relying on explicit error signals or reward functions for self- correction. However, these methods mainly focus on single-turn scenarios, such as code repair (Kim et al., 2024; Chen et al., 2024b), tool use (Olausson et al., 2023), and mathematical reasoning (Kumar et al., 2024; Havrilla et al., 2024). In contrast, tasks in interactive and agentic environments usually involve multi-turn interactions and do not reveal explicit error signals until reaching the terminal state. Additionally, unlike mathematical reasoning (Xi et al., 2024b), designing high-quality reward functions to critique intermediate actions in long interactive trajectories remains difficult."}, {"title": "2. Preliminary", "content": ""}, {"title": "2.1. Task Formulation", "content": "In this paper, we focus on tasks with partial observations in interactive environments. Following prior work (Song et al., 2024b; Qiao et al., 2024), these tasks can be formulated as a Partially Observable Markov Decision Process (POMDP): (U, S, A, O, T, R). The instruction space U provides task descriptions and their corresponding requirements. S represents the state space, A is the action space, and O is the observation space. The transition function T : S \u00d7 A \u2192 S is determined by the environment, while the reward function R : S \u00d7 A \u2192 [0, 1] specifies the reward. For language agent, U, S, A, and O are represented in natural language. At each time step t, the historical trajectory $T_t$ is defined as:\n$T_t = (a_1, o_1, ..., a_t, o_t) \\sim \\pi_\\theta(T_t|u)$ (1)\nwhere $(a_1, o_1,..., a_t, o_t)$ is a sequence of actions and observations, and $o_t$ is the observation obtained after executing action $a_t$.\nThe language agent with parameter $\\theta$ is the actor model $\\pi_\\theta$. At each time step t + 1, the actor model should generate $a_{t+1}$ based on $t_\\tau$ and instruction u, i.e., $a_{t+1} \\sim \\pi_\\theta(\\cdot|T_t, u)$. We adopt the ReAct approach (Yao et al., 2023b), generating rationales before each action to enhance performance. The task ends when it is successfully completed, or the maximum number of rounds is reached. Then, the final reward r(t) \u2208 [0, 1] is then given by the interactive environments. Thus, at the terminal time step T, the probability distribution of the entire trajectory $\\tau_T$ is represented as:\n$\\pi_\\theta(\\tau_T|u) = \\prod_{t=1}^T \\pi_\\theta(a_t|T_{t-1}, u)$. (2)"}, {"title": "2.2. Monte Carlo Tree Search", "content": "Monte Carlo Tree Search (MCTS) is a decision-making algorithm widely used in complex decision processes (Kocsis and Szepesv\u00e1ri, 2006; Browne et al., 2012). It builds a search tree and simulates"}, {"title": "3. Method", "content": "In this section, we present Agent-R in detail, which consists of two core phases, as illustrated in Figure 2. Phase I: Model-Guided Reflection Trajectory Generation (\u00a7 3.1), which utilizes Monte Carlo Tree Search (MCTS) to dynamically generate revision trajectories, transforming erroneous trajectories into corrected ones by identifying the most suitable reflection step (for current policy model). Phase II: Iterative Self-Training with Revision Trajectories, where agents are iteratively trained on the dynamically constructed revision trajectories. Such an iterative training manner enables us to explore the scalability of Agent-R, where agents progressively learn harder (earlier) revision steps based on its current policy and improve their decision-making process, avoiding cascading errors and loops."}, {"title": "3.1. Phase I: Model-Guided Reflection Trajectory Generation", "content": "Reflection Trajectory Definition We first define four types of trajectories that play a central role in Agent-R: initial trajectories, bad trajectories, good trajectories, and revision trajectories. These are described as follows\u00b9:\n\u2022 Initial Trajectory, denoted as $\\tau^i = (a_1, o_1, ..., a_t, o_t)$. An initial trajectory represents the initial sequence of actions and observations.\n\u2022 Bad Trajectory, denoted as $\\tau^b = (\\tau^i, a_{t'+1}, o_{t'+1}, ..., a_T, o_T)$. A bad trajectory extends the initial trajectory $\\tau^i$ with a sequence of suboptimal actions and observations, leading to an erroneous or less-rewarding outcome.\n\u2022 Good Trajectory, denoted as $\\tau^g = (\\tau^i, a_{t'+1}, o_{t'+1}, ..., a_{T'}, o_{T'})$. A good trajectory builds upon the initial trajectory $\\tau^i$, involving optimal or high-reward actions and observations.\n\u2022 Revision Trajectory, denoted as $\\tau^r$. A revision trajectory is constructed by correcting a bad trajectory with a good trajectory. Formally, it is defined as:\n$\\tau^r = (\\tau^i, a_{t'+1}, o_{t'+1}, ..., a_{t'}, o_{t'}, r_s, a_{t'+1}, o_{t'+1}, ..., a_{T'}, o_{T'})$,\nwhere t' represents the transition point between the bad and good trajectory segments, and $r_s$ is the revision signal marking the transition. The revision signal rs is a brief communication such as: Assistant: [reflection on the error] \\n Human: OK., which facilitates the agent's reflection on its past actions.\nThe interactive environment will provide the final reward after the trajectory has terminated. Based on the final reward, we define constraints to ensure that bad trajectories can be effectively corrected into good ones and that the quality of the revision trajectory matches that of the good trajectory. The reward conditions are as follows:\nr(\\tau^b) < \u03b2 < r(\\tau^g) \u2264 1, a < r(\\tau^g) = r(\\tau^r), (4)\nwhere \u03b2 is a threshold separating the rewards of bad and good trajectories, and a represents a lower bound for high-quality trajectories, encouraging revisions that are of consistently high quality.\nr(7g) = 1 means that this good trajectory is a Optimal Trajectory.\nTrajectory Collection with MCTS To efficiently explore and reflect errors within the trajectory space, we employ Monte Carlo Tree Search (MCTS) to collect revision trajectories. This method systematically searches through possible action paths, ensuring diversity in the generated trajectories while balancing exploration and exploitation.\nStarting from the initial root node s0, which corresponds to the user-provided instruction u, MCTS iteratively performs four key stages: selection, expansion, simulation, and backpropagation. As shown in Figure 2, during the simulation stage, a default rollout policy is used to sample future actions. To improve reward estimation and ensure diversity in collected trajectories, multiple rollouts are performed for each simulation. In the selection phase, we balance exploration and exploitation using the UCT criterion:\n$UCT(s) = Q(s) + C_{uct} \\sqrt{\\frac{log N_p(s)}{N(s)}}$, (5)\nwhere Q(s) is the average reward of state s, N(s) the number of visits to state s, Np(s) is the total visit count of the parent node of s, and Cuct is a constant that controls the exploration-exploitation trade-off.\nWhen a terminal node is reached, either due to a terminal state or exceeding a predefined maximum depth, MCTS generates a trajectory from the root node to the terminal node and obtains the final reward for the trajectory from the environment. By performing multiple rollouts, we collect a diverse set of trajectories. Good and bad trajectories share the same initial trajectory and begin to diverge"}, {"title": "3.2. Phase II: Iterative Self-Training with Revision Trajectories", "content": "In this phase, the goal is to train language agents using self-generated revision trajectories collected through MCTS. The agent uses its own rollouts to identify and correct errors dynamically through self-reflection, progressively improving its policy based on its own experiences during these rollouts.\nAlthough training on revision trajectories enables the agent to develop self-reflection capabilities, relying solely on these trajectories may initially hinder the agent's ability to identify the correct trajectory. A common solution in previous works is to train the model using both optimal and revision trajectories. However, this still suffers from the cold-start problem, where the agent begins with little to no knowledge of the environment and must depend on trial and error. This results in a limited number of optimal trajectories discovered early in training.\nTo address this issue, we propose mixing revision trajectories with good trajectories during training. Over the course of self-training, we gradually increase the value of a in Equation 4, which encourages the good trajectories to progressively converge toward optimal trajectories. This enables the agent to improve both its error reflection capabilities and its ability to identify the correct trajectory over time.\nAdditionally, following the strategy from AgentTuning (Zeng et al., 2024a), we combine agent"}, {"title": "4. Experiment", "content": "In this section, we conduct extensive experiments on three interactive environments to demonstrate the effectiveness of Agent-R."}, {"title": "4.1. Interactive and Agentic Environments", "content": "Following previous work (Xi et al., 2024a; Prasad et al., 2024), we conduct experiments on three types of representative interactive environments:\n\u2022 WebShop (Yao et al., 2022), which is an interactive web environment for online shopping. It contains 12k instructions and offers over one million real products from amazon.com. Agents can click buttons on the webpage or perform searches using the search engine.\n\u2022 ScienceWorld (Wang et al., 2022), which is a scientific, text-based environment designed to evaluate agents' scientific reasoning abilities. It includes 30 types of scientific tasks at the standard elementary science curriculum level.\n\u2022 TextCraft (Prasad et al., 2024), which is a text-based environment for crafting Minecraft items. It constructs a crafting tree based on Minecraft's recipes. Each task provides a target item and"}, {"title": "4.2. Experiment Setting", "content": "We conduct our experiments on AgentGym (Xi et al., 2024a), an interactive platform that includes diverse agent environments.\nData Split The statistical details of these three interactive environments are shown in Table 1. To create the revision trajectories, we randomly sample 300 simulations from WebShop, 200 from SciWorld, and 200 from TextCraft to conduct MCTS. We set the distinguishable gap \u03b2 = 0.2 between bad and good trajectories in Equation 4. As mentioned in \u00a7 3.1, considering the cold-start problem for agents in these challenging environments, the initial threshold for good trajectories is set relatively low and gradually increases in later iterations. Specifically, we conduct Agent-R for three iterations. We set a = 0.5 for iteration 1, a = 0.7 for iteration 2, and a = 1.0 for iteration 3. By iteration 3, the good trajectories converge toward optimal trajectories. Following AgentTuning (Zeng et al., 2024a), we use ShareGPT as the general dataset Dgeneral and set \u03b7 = 0.2 in Equation 6. Following Xi et al. (2024a), we select 200 tasks for the WebShop test set, 200 for the SciWorld test set, and 100 for the TextCraft test set.\nMCTS Settings For all simulations in the three interactive environments, during the trajectory self-generation stage, we sample k = 8 rollouts for each Monte Carlo estimation. The depth d is set to 20 for all tasks. In the expansion phase, the temperature of the LLMs is set to 1. At each depth, the LLMs generate 4 candidate actions as new child nodes. In the selection phase, to balance exploration and exploitation, we set Cuct = 0.25 for UCT in Equation 5.\nTraining Settings We perform Agent-R for three iterations and present the results of the third iteration in Table 2. We set the epoch number to 3 for the first iteration and 1 for subsequent iterations to avoid over-fitting. Detailed training settings are provided in Appendix C.1. Our main backbone model is the instruct version of LLama-3.1-8B, i.e., Llama-3.1-8B-Instruction (Dubey et al., 2024).\nBaselines Following Xi et al. (2024a), we select closed-source models, i.e., GPT-3.5-Turbo (OpenAI, 2022), GPT-4-Turbo (OpenAI, 2023), GPT-40 (gpt-40-2024-08-06) (OpenAI, 2023), Claude 3 (Anthropic, 2024), and DeepSeek-Chat (Liu et al., 2024a). We also select open-source models like Llama2-Chat (Touvron et al., 2023) and agents trained on expert trajectories, i.e., AgentLM (Zeng et al., 2024a) and Agent-Flan (Chen et al., 2024d). Additionally, we compare ETO (Song et al., 2024b), which first applies SFT to a base agent using behavioral cloning and then uses DPO (Rafailov et al., 2023) to fine-tune the model with contrastive pairs of good and bad trajectories. We also report the results of Llama-3.1-8B-Instruct trained on direct-revision trajectories mixed with good trajectories over three iterations for comparison (w/ Direct-Revision).\nEvaluation Metrics Following Xi et al. (2024a), for SciWorld and WebShop, we use the average final reward as the evaluation metric. For TextCraft, we use the success rate as the evaluation metric. In all three environments, the maximum number of rounds is set to 100. Detailed evaluation settings are provided in Appendix C.1."}, {"title": "4.3. Main Result", "content": "The overall results for the three interactive environments are shown in Table 2. We find that:\n1. Agent-R significantly improves the performance of language agents in interactive environments. This approach outperforms both advanced closed-source models (e.g., GPT-40) and agents trained on expert trajectories (e.g., AgentLM and Agent-Flan). The results highlight the critical role of revising erroneous trajectories during the learning process rather than solely relying on expert data.\n2. Trajectories constructed by Agent-R, through dynamic self-reflection and early error detection, lead to faster recovery and more stable learning. This early intervention prevents the propagation of errors, resulting in higher-quality models and better performance than direct-revision methods.\n3. Although adopting contrastive learning methods (e.g., ETO) improves language agent performance in interactive environments, these methods do not equip agents with self-reflection capabilities. This limitation is analyzed in \u00a7 4.4."}, {"title": "4.4. Findings with Analysis", "content": "Finding 1: Training with trajectories from Agent-R can outperform using optimal trajectories. To further investigate the significance of revision trajectories constructed by Agent-R, we compare our approach with Llama-3.1-8B-Instruct trained on direct-revision trajectories combined with good trajectories (w/ Direct-Revision Trajectory) for each iteration. Additionally, we include two other ablated variants: w/ Optimal Trajectory: Llama-3.1-8B-Instruct trained only on self-generated optimal trajectories (r(\u03c4) = 1). w/ Optimal + Good Trajectory: Llama-3.1-8B-Instruct trained on optimal trajectories combined with the good trajectories from iteration 1 in Agent-R. The results in Figure 3 show that:"}, {"title": "Finding 2: Agent-R can effectively provide language agents with self-reflection capabilities.", "content": "To further explore the self-reflection capabilities of language agents trained with Agent-R, we first collect"}, {"title": "Finding 3: Training with revision trajectories helps agents more easily identify and correct erroneous actions.", "content": "As mentioned in \u00a7 3.1, to construct revision trajectories, we ask language agents to identify the first erroneous action in the bad trajectories and then concatenate the correct trajectories after this point. To deter- mine if agents trained with iterative SFT can more effectively and quickly detect the first er- ror, we measure the revision length. This is the number of actions from the start of the bad tra- jectory to the first identified error. We report the average revision length for different iterations across the three interactive environments."}, {"title": "Finding 4: Training with revision trajectories helps agents avoid getting stuck in loops.", "content": "To understand why revision trajectories are more effective than self-generated optimal ones, we measure the length of repeat action sequences in test set trajectories under different training settings. We report the average count, which reflects how often agents repeat the same action sequence when stuck in a loop. For example, a sequence length of 2 with an average count of 5 means the agent repeats the same sequence of two actions 5 times on average.\nThe results in Figure 4 show that although agents can obtain optimal trajectories through MCTS, these trajectories may contain repeated or noisy middle actions, causing the agent to get stuck in dead loops, which negatively impacts performance. Compared to training with optimal trajectories, training with trajectories from Agent-R significantly reduces the occurrence of dead loops. This indicates that agents trained with revision trajectories are more likely to explore new actions and avoid getting trapped in local loops. These findings emphasize the importance of revision trajectories in enhancing exploration and overall performance."}, {"title": "Finding 5: Multi-task training is a more effective strategy for Agent-R.", "content": "We compare our multi-task training approach with single-task training, where good and revised trajectories are collected separately for each task using Agent-R. The results in Figure 6 show that multi- task training is a more effective way to train lan- guage agents in interactive environments. Ad- ditionally, the trajectories collected by Agent-R are better suited for multi-task training, further enhancing performance. The detailed results of three environments with further analysis are shown in Appendix C.2."}, {"title": "5. Related Work", "content": "Agent Learning in Interactive Environments Previous approaches to agent learning in interactive environments can be categorized into three strategies: 1) Prompt-based Strategy, which uses human- written prompts to guide LLMs in summarizing experiences (e.g., constructing and refining sets of transferable skills (Nottingham et al., 2024; Sarch et al., 2024) or helpful hints (Chen et al., 2024a; Majumder et al., 2023; Zhao et al., 2024; Yuan et al., 2024)) during exploration. These summarized experiences are incorporated into the model's memory to enhance its knowledge and improve performance. 2) Inference-time Search Strategy, which employs various search algorithms, such as Tree-of-Thought (Yao et al., 2023a; Light et al., 2024; Koh et al., 2024; Hao et al., 2023) and Monte Carlo Tree Search (MCTS) (Zhao et al., 2023), to identify optimal trajectories during inference. This strategy leverages prior knowledge in LLMs to enable more efficient and effective"}, {"title": "6. Conclusion", "content": "In this paper, we address the critical challenge of error correction for language agents operating in interactive environments, a limitation that remains unresolved by existing approaches relying on expert trajectories. To tackle this issue, we introduce Agent-R, an iterative self-training framework that empowers agents to dynamically reflect and correct their actions in interactive and agentic envi- ronments. By leveraging Monte Carlo Tree Search (MCTS) and model-guided reflection construction, Agent-R allows for timely revision of erroneous trajectories, significantly improving agents' ability to recover from errors in real time. Experiments across three interactive environments show that Agent-R not only enhances error correction capabilities but also prevents looping and suboptimal behaviors, leading to superior performance compared to baseline methods. Furthermore, integrating self-generated revision trajectories into training improves the agent's overall performance and supports the development of more intelligent, self-reflective agents. These findings open exciting avenues for future work, particularly in refining the role of self-correction as a critical function in agent-based systems."}, {"title": "A. Prompt Template of Revision Trajectory", "content": "Table 4 shows the prompt of Agent-R to determine the transition point.\nTen different revision thoughts are shown in Table 5."}, {"title": "B. Trajectory Definition", "content": "We define four types of trajectories: initial trajectories, bad trajectories, good trajectories, and revision trajectories. Below, we provide formal definitions and mathematical formulations for each type of trajectory:\n\u2022 Initial Trajectory (7\u00b2): An initial trajectory represents the initial sequence of actions and observa- tions given the user-provided instruction u. Formally, it is expressed as:\n$\\tau^i = (a_1, o_1, ..., a_t, o_t)$, (7)\nwhere $a_k$ and $o_k$ denote the k-th action and the corresponding observation in the trajectory.\n\u2022 Bad Trajectory (7b): A bad trajectory extends the initial trajectory \u03c4i with a sequence of suboptimal actions and observations. It is represented as:\n$\\tau^b = (\\tau^i, a_{t'+1}, o_{t'+1}, ..., a_T, o_T)$, (8)\nwhere Tb denotes the terminal step of the bad trajectory.\n\u2022 Good Trajectory (7g): A good trajectory builds upon the initial trajectory \u03c4i but involves optimal or high-reward actions and observations. It is given by:\n$\\tau^g = (\\tau^i, a_{t'+1}, o_{t'+1}, ..., a_{T'}, o_{T'})$, (9)\nwhere Tg denotes the terminal step of the good trajectory.\n\u2022 Revision Trajectory (\u03c4r): A revision trajectory synthesizes components from both a bad trajectory Th and a good trajectory 79. Specifically, it starts with the bad trajectory up to step t' and transitions to the good trajectory thereafter. Formally, it is defined as:\n$\\tau^r = (\\tau^i, a_{t'+1}, o_{t'+1}, ..., a_{t'}, o_{t'}, r_s, a_{t'+1}, o_{t'+1}, ..., a_{T'}, o_{T'})$,\nwhere t' < To denotes the transition point between the bad and good trajectories. rs is the revision signal, which does not involve any environment-altering action but serves as a marker for the revision process. In our settings, rs is a single-turn conversation: Assistant: [revision thought] \\n Human: OK.\""}, {"title": "C. Experiment Details", "content": ""}, {"title": "C.1. Training and Evaluation Details", "content": "All experiments are conducted using eight A100-80GB GPUs. Services for different environments are deployed on separate ports of the same server.\nThe training process employed an iterative SFT approach. For the first iteration, the number of epochs is set to 3 to ensure sufficient learning. In subsequent iterations, the number of epochs is reduced to 1 to mitigate overfitting. The learning rate for iterative SFT is 2e\u22125, with 3% warm-up and a"}, {"title": "C.2. Multi-task training v.s. Single-task Training", "content": "We compare our multi-task training approach to single-task training, where good and revised trajec- tories are collected separately for each task using Agent-R. The results in Table 6 demonstrate that multi-task training outperforms single-task training when using trajectories generated by Agent-R. Multi-task training leverages cross-task learning to enhance performance across datasets, especially in later iterations, highlighting its scalability and effectiveness with additional training cycles. Trajec- tories from Agent-R appear particularly beneficial for multi-task training, potentially providing richer"}, {"title": "C.3. Case Study", "content": "Error Correction in Trajectory Generation As mentioned in \u00a7 4, training with trajectories from Agent-R significantly enhances the performance of language agents in interactive environments. Table 7 presents examples of historical records before and after the adaptive transition point. These examples demonstrate that a language agent trained on our trajectories can effectively recover from incorrect trajectories and identify the correct action.\nAdaptive Transition Point Evaluation As shown in Figure 5, we find that iterative SFT allows language agents to more easily recognize when they are on a bad trajectory with erroneous actions. We further analyze the results of adaptive transition point evaluation to examine errors in trajectory actions. Examples in Table 8 categorize these erroneous actions into three types:\n\u2022 Invalid Actions: Actions that are not logically permissible or executable in the current environ- ment or context, often violating the task or system constraints.\n\u2022 Mismatch Observation: Actions that contradict the observed evidence or fail to align with the current state of the environment or task requirements.\n\u2022 Irrelevant Actions: Actions that do not contribute to achieving the task goal are often unrelated or misaligned with the task's specified objectives.\nThe examples demonstrate that language agents trained with trajectories from Agent-R are better equipped to identify and avoid erroneous actions. These findings indicate that Agent-R enhances agents' ability to differentiate between permissible, aligned, and task-relevant actions, resulting in improved decision-making and trajectory optimization in complex environments."}]}