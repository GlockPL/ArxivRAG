{"title": "STABLE OFFLINE VALUE FUNCTION LEARNING WITH BISIMULATION-BASED REPRESENTATIONS", "authors": ["Brahma S. Pavse", "Yudong Chen", "Qiaomin Xie", "Josiah P. Hanna"], "abstract": "In reinforcement learning, offline value function learning is the procedure of using an offline dataset to estimate the expected discounted return from each state when taking actions according to a fixed target policy. The stability of this procedure, i.e., whether it converges to its fixed-point, critically depends on the representations of the state-action pairs. Poorly learned representations can make value function learning unstable, or even divergent. Therefore, it is critical to stabilize value function learning by explicitly shaping the state-action representations. Recently, the class of bisimulation-based algorithms have shown promise in shaping representations for control. However, it is still unclear if this class of methods can stabilize value function learning. In this work, we investigate this question and answer it affirmatively. We introduce a bisimulation-based algorithm called kernel representations for offline policy evaluation (KROPE). KROPE uses a kernel to shape state-action representations such that state-action pairs that have similar immediate rewards and lead to similar next state-action pairs under the target policy also have similar representations. We show that KROPE: 1) learns stable representations and 2) leads to lower value error than baselines. Our analysis provides new theoretical insight into the stability properties of bisimulation-based methods and suggests that practitioners can use these methods for stable and accurate evaluation of offline reinforcement learning agents.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning the value function of a policy is a critical component of many reinforcement learning (RL) algorithms (Sutton & Barto, 2018). While value function learning algorithms such as temporal-difference learning (TD) have been successful, they can be unreliable. In particular, the deadly triad, i.e., the combination of off-policy updates, function approximation, and bootstrapping, can make TD-based methods diverge (Sutton & Barto, 2018; Tsitsiklis & Van Roy, 1997; Baird, 1995). Function approximation is a critical component of value function learning since it determines the representations of state-action pairs, which in turn defines the space of expressible value functions. Depending on how this value function space is represented, value function learning algorithms may diverge (Ghosh & Bellemare, 2020). That is, the value function learning algorithm may not converge to its fixed-point, or may even diverge away from it. In this work, we investigate how to explicitly learn state-action representations to stabilize value function learning.\nIn seeking such representations, we turn to \u03c0-bisimulation algorithms. These algorithms define a metric to capture behavioral similarity between state-action pairs such that similarity is based on immediate rewards received and the similarity of next state-action pairs visited by \u03c0 (Castro, 2019). The algorithms then use this metric to learn representations such that state-action pairs that are similar under this metric have similar representations. Ultimately, the goal of \u3160-bisimulation methods is to learn representations such that state-actions pairs with similar values under \u03c0 also have similar representations (see Figure 1). While these algorithms have shown promise in improving the expected return of RL algorithms, it remains unclear whether they contribute to stability (Castro et al., 2023; Zhang et al., 2021; Castro et al., 2022). In this paper, we aim to understand whether \u03c0-bisimulation-based representations stabilize value function learning."}, {"title": "2 BACKGROUND", "content": "In this section, we present our problem setup and discuss prior work."}, {"title": "2.1 PROBLEM SETUP AND NOTATION", "content": "We consider the infinite-horizon Markov decision process (MDP) framework (Puterman, 2014), M = (S, A, r, P, \u03b3, do), where S is the state space, A is the action space, r : S \u00d7 A \u2192 [\u22121, 1] is the deterministic reward function, P : S \u00d7 A \u2192 \u2206(S) is the transition dynamics function, \u03b3\u2208 [0, 1) is the discount factor, and do \u2208 \u2206(S) is the initial state distribution, where \u2206(X) represents the set of all probability distributions over a set X. We refer to the joint state-action space as X := S \u00d7 A. The agent acting according to policy \u03c0 : S \u2192 \u0394(A) in the MDP generates a trajectory: So, A0, Ro, S1, A1, R1, ..., where So ~ do, At ~ \u03c0(\u00b7|St), Rt := r(St, At), and St+1 ~ P(. St, At) for t > 0.\nWe define the action-value function of a policy for a given state-action pair as q\" (s,a) := \u0395\u03c0 [\u03a3\u03c4\u03bf \u03b3^t r(St, At)|So = s, Ao = a], i.e., the expected discounted return when starting from state"}, {"title": "2.2 OFFLINE POLICY EVALUATION AND VALUE FUNCTION LEARNING", "content": "In offline policy evaluation (OPE), the goal is to evaluate a fixed target policy, \u03c0e, using a fixed dataset of m transition tuples D := {(Si, ai, Si, ri)}1. In this work, we evaluate \u03c0e by estimating the action-value function qte using D. Crucially, D may have been generated by a set of unknown behavior policies that are different from \u03c0e, which means that simply averaging the discounted returns in D will produce an inconsistent estimate of qte. In our theoretical results, we make the standard coverage assumption that \u2200s \u2208 S, da \u2208 A if \u03c0e(a|s) > 0, then the state-action pair (s, a) has non-zero probability of appearing in D (Sutton & Barto, 2018; Precup et al., 2000).\nWe measure the accuracy of the value function estimate with the mean squared value error (MSVE). Let qte be the estimate returned by a value function learning method using D. The MSVE of this estimate is defined as MSVE[\u011d\u02dc\u20ac] := E(S,A)~D[(\u011d\u02dc\u20ac(S, A) \u2013 q\u02dc\u20ac (S, A))^2]. In environments with continuous state-action spaces, where it is impossible to compute qte for all state-actions, we adopt a common evaluation procedure from the OPE literature of measuring the MSE across only the initial state-action distribution, i.e., MSE[\u011de] := Eso~do,Ao~\u3160e [(\u011d\u2122\u20ac (So, Ao) \u2013 q\u02dc\u20ac (So, Ao))^2]. For this procedure, we assume access to do (Voloshin et al., 2021; Fu et al., 2021). While in practice qte is unknown, it is standard for the sake of empirical analysis to estimate qte by executing unbiased Monte Carlo rollouts of \u03c0e or computing qte exactly using dynamic programming in tabular environments (Voloshin et al., 2021; Fu et al., 2021).\nLeast-Squares Policy Evaluation Least-squares policy evaluation (LSPE) is a value function learning algorithm, which models the action-value function as a linear function: qe(s,a) := $(s, \u03b1)\u03b8, where 0 \u2208 Rd (Nedic & Bertsekas, 2003). LSPE iteratively learns @ with the following updates per iteration step t:\n\nOt+1 \u2190 (ED[\u03a6\u03a6])\u00af\u00b9ED,\u03c0\u03b5 [\u03a6\u00af (r + \u03b3\u03a1\u03b5\u03a6\u03b8t)],\n\nwhere the expectations are taken with respect to the randomness of the dataset D and \u03c0e. Note that E[\u03a6\u03a6] is the feature covariance matrix. Assuming LSPE converges, it will converge to the same fixed-point as TD(0) (Szepesvari, 2010), which we denote as OLSPE. In this work, we follow a two-stage approach to applying LSPE: we first obtain the encoder & either through representation learning or using the native features of the MDP, and then feed the obtained & along with D and \u03c0\u03b5 as input to LSPE, which outputs que (Nedic & Bertsekas, 2003; Chang et al., 2022). This two-stage approach of learning a linear function on top of fixed representations is called the linear evaluation protocol (Chang et al., 2022; Farebrother et al., 2023; 2024; Grill et al., 2020; He et al., 2020). This protocol enables us to cleanly analyze the nature of the learned representations within the context of well-understood value function learning algorithms such as LSPE. In Appendix A, we include the pseudo-code for LSPE."}, {"title": "2.3 STABLE, REALIZABLE, AND GENERALIZABLE REPRESENTATIONS", "content": "We define stability of LSPE and related TD-methods following Ghosh & Bellemare (2020):\nDefinition 1 (Stability). LSPE is said to be stable if for any initial \u03b8\u03bf \u2208 Rd, limt\u2192\u221e \u03b8t = OLSPE when Ot is updated according to Equation (1).\nWhen determining the stability of LSPE, we have following proposition from prior work:\nProposition 1 (Asadi et al. (2024); Wang et al. (2021a)). LSPE is stable if and only if the spectral radius of (E[\u03a6\u03a6])\u2212\u00b9(\u03b3\u0395[\u03a6\u0422\u0420\u2208\u03a6]), i.e., its maximum absolute eigenvalue, is less than 1.\nTherefore, the stability of LSPE largely depends on the representations \u03a6 and the distribution shift between the data distribution of D and \u03c0e. In this work, we study the stability of LSPE for a fixed distribution of D and learn \u03a6. If a given I stabilizes LSPE, we say \u0424 is a stable representation.\nIn addition to stability, we also care about the realizability and generalizability of \u0424. We say I is a realizable representation if qe \u2208 Span(\u03a6), where Span(\u03a6) is the subspace of all expressible action-value functions with \u03a6. Note that even if I is a realizable and stable representation, LSPE may not recover the qe solution (Sutton & Barto, 2018). While generalization can have multiple interpretations, we say I generalizes well if the state-action features that are close in the representation space also have similar qe values (Lyle et al., 2022; Lan et al., 2021)."}, {"title": "2.4 RELATED WORKS", "content": "In this section, we discuss the most relevant prior literature on OPE and representation learning.\nRepresentations for Offline RL and OPE. There are several works that have shown shaping representations can be effective for offline RL (Yang & Nachum, 2021; Islam et al., 2023; Nachum & Yang, 2024; Zang et al., 2023a; Arora et al., 2020; Uehara et al., 2021; Chen & Jiang, 2019; Pavse & Hanna, 2023b). Ghosh & Bellemare (2020) presented a theoretical understanding of how various representations can stabilize TD learning. However, they did not discuss bisimulation-based representations. Kumar et al. (2021); Ma et al. (2024); He et al. (2024) promote the stability of TD-based methods by increasing the rank of the representations to prevent representation collapse. However, as we show in Section 4, these types of representations can still lead to inaccurate OPE. On the other hand, KROPE mitigates representation collapse and leads to accurate OPE. Chang et al. (2022) introduced BCRL to learn Bellman complete representations for stable OPE. While in theory, BC representations are desirable, we found that BCRL is sensitive to hyperparameter tuning. In contrast, we show that KROPE is more robust to hyperparameter tuning. Pavse & Hanna (2023a) showed that bisimulation-based representations mitigate the divergence of FQE; however, they did not provide an explanation for divergence mitigation. Our work provides theoretical insight into the stability properties of bisimulation-based algorithms.\nBisimulation-based Representation Learning. Recently, there has been lot of interest in \u03c0-bisimulation algorithms for better generalization (Ferns et al., 2004; 2011; Ferns & Precup, 2014; Castro, 2019; Zang et al., 2023b). These algorithms measure similarity between two state-action pairs based on immediate rewards received and the similarity of next state-action pairs visited by \u03c0. These algorithms first define a distance metric that captures this \u03c0-bisimilarity, and then use this metric to learn representations such that \u03c0-bisimular states have similar representations (Castro et al., 2022; Castro, 2019; Zhang et al., 2021; Castro et al., 2023; Chen & Pan, 2022; Kemertas & Jepson, 2022). Zhang et al. (2021); Castro (2019) introduced a \u03c0-bisimulation learning algorithm but assume that the transition dynamics are either deterministic or Gaussian. Gelada et al. (2019) introduced a method closely related to bisimulation methods but required a reconstruction loss to work in practice. Castro et al. (2022) introduced MICO which allows for stochastic transition dynamics and no reconstruction loss, but was difficult to theoretically analyze. To overcome this difficulty, Castro et al. (2023) took a kernel perspective of \u03c0-bisimulation methods, which made their algorithm amenable to theoretical analysis. To the best of our knowledge, no works have studied the stability properties of \u03c0-bisimulation algorithms. In our work, we address this gap in the literature. We first extend Castro et al. (2023)'s kernel-based formulation from states to state-actions, and then show that this formulation stabilizes offline value function learning. The proofs for KROPE's basic theoretical properties (Section 3.1) follow those by Castro et al. (2023). Our stability-related theoretical results (Sections 3.2 and 3.3) and empirical analysis (Section 4) are novel to this work."}, {"title": "3 KERNEL REPRESENTATIONS FOR OFFLINE POLICY EVALUATION", "content": "We now present our bisimulation-based representation learning algorithm, kernel representations for OPE (KROPE). We present the desired KROPE kernel, define the KROPE operator, present its theoretical properties, prove stability properties of KROPE representations, and present a practical learning algorithm to learn them. We defer the proofs to Appendix B."}, {"title": "3.1 KROPE KERNEL AND OPERATOR", "content": "Prior \u03c0-bisimulation works define similarity between states in terms of the immediate rewards received and similarity of next states under \u03c0 (see Figure 1) (Castro, 2019). In this work, we follow Castro et al. (2023) and define a kernel ke : X \u00d7 X \u2192 R that captures this notion of similarity under \u03c0e, but for pairs of state-actions. We refer to ke as the KROPE kernel.\n\nke (s1, a1; S2, A2) = k1(s1, a1; S2, a2) + yk2(k\u02dc\u20ac)(P\u03c0\u03b5(\u00b7|81, a1), P\u00f1e (\u00b7|82, A2)].\n\n\nwhere k1(81,01; 82,02) := 1 - \\frac{|r($1,01)-r($2,02)|}{rmax-rmin} and k2(ke)(Pe(\u00b7|81, A1), P\u00ae\u00ab(\u00b7|82, A2)) :=\nEs\u2081,a1~Pe (181,01), 82, a\u2082~Pe (182,a2) [ke (s1, a'1; 82, a2)]. Here, k\u2081 measures short-term similarity based on rewards received and k2 measures long-term similarity between probability distributions by measuring similarity between samples of the distributions according to ke (Castro et al., 2023).\nGiven this definition of the KROPE kernel, we now present an operator that converges to ke:\nDefinition 2 (KROPE operator). Given a target policy \u03c0e, the KROPE operator Fe : RX\u00d7X \u2192 Rxxx is defined as follows: for each kernel k : X \u00d7 X \u2192 R, \u2200(s1, a1; 82, a2) \u2208 X \u00d7 X,\n\nFe (k)(s1, a1; S2, A2) := k1(s1, a1; S2, a2) + Es1,s2~P,a1,a2~\u03c0\u03b5 [\u03ba(81, \u03b11; 82, \u03b1\u2082)]\n\n~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ \nwhere s'\u2081 ~ P(8181, a1), 82 ~ P(82/82, a2), a1 ~ \u03c0\u03b5(181), \u03b12 ~ \u03c0\u03b5(82), and k1(81, a1; 82, a\u2082) :=\n1- \\frac{|r(s1,a1)-r(s2,a2)|}{rmax-rmin} is a positive semidefinite kernel.\nProposition 2, proved in Appendix B.1, tells us that for some initial kernel k, repeatedly applying Fre to it will result in convergence ke. Ultimately, ke outputs a high (or low) similarity measure for two state-action pairs if their action-values under \u03c0e are similar (or dissimilar). This intuition is formalized in Lemma 3, which states that the absolute action-value difference between any two state-action pairs under \u03c0e is upper-bounded by the distance function induced by ke plus an additive constant. Since KROPE's contraction, metric space completeness, and fixed-point uniqueness properties are similar to Castro et al. (2023)'s kernel, we defer the details to Appendix B.1."}, {"title": "3.2 STABILITY OF KROPE REPRESENTATIONS", "content": "In the previous section, we defined the KROPE kernel. Ultimately, however, we are interested in representations that satisfy the relationship in Equation (2). We modify Equation (2) accordingly by giving ke some functional form in terms of state-action representations. We do so with the dot product: (u, v) = u\u00afv, \u2200u, v \u2208 Rd, i.e., ke (s1, a1; 82, a2) = $(s1, a1)(82, a2). With this setup, we write Equation (2) in matrix notation and define the KROPE representations as follows:\nDefinition 3 (KROPE Representations). Consider state-action representations \u03a6 \u2208 R|x|\u00d7d that are embedded in Rd with ke (81,01; 82, a2) = $(s1, a1)$(s2, a2). We say \u00de is a KROPE representation if it satisfies the following:\n\n\u0415\u0414[\u0424\u0424] = ED[K1] + YED,\u03c0\u03b5[\u03a1\u20ac\u03a6(\u03a1\u03c0\u03b5\u03a6)\n\nwhere each entry of K\u2081 \u2208 R|x|\u00d7|x| represents the short-term similarity, k\u2081, between every pair of state-actions, i.e., K1(81, a1; 82, a\u2082) := 1-\\frac{|r(s1,a1)-r(82,a2)|}{rmax-rmin}.\nGiven this definition, we present our novel result proving the stability of KROPE representations:\nTheorem 1. If I is a KROPE representation as defined in Definition 3, then the spectral radius of (E[\u03a6\u03a6]))\u00af\u00b9E[\u03b3\u03a6\u00afPe\u03a6] is less than 1. That is, \u03a6 stabilizes LSPE."}, {"title": "3.3 CONNECTION TO BELLMAN COMPLETENESS", "content": "In this section, we draw a novel connection between KROPE representations and Bellman completeness. We say a function class F is Bellman complete if it is complete under the Bellman operator: T\u2122ef \u2286 F,\u2200f \u2208 F. For instance, suppose F is the class of linear functions spanned by \u0424, F := {f \u2208 R\u0142 : f := \u03a6w},w \u2208 Rd. Then if Te f,\u2200f \u2208 F is also a linear function within the span of I, we say I is a Bellman complete representation. Bellman completeness is an alternative condition for stability and is typically assumed to ensure to data-efficient policy evaluation (Wang et al., 2021b; Szepesv\u00e1ri & Munos, 2005; Chang et al., 2022). We now present our second main result. It states that KROPE representations are Bellman complete:\nTheorem 2. Let \u00a2 : X \u2192 X$ be the state-action abstraction induced by grouping state-actions x,y \u2208 X such that if dKrope(x,y) = 0, then $(x) = $(y),\u2200x,y \u2208 X. Then \u0444 is Bellman complete if the abstract reward function r\u00a2 : X \u2192 (\u22121, 1) is injective (i.e., distinct abstract rewards)."}, {"title": "Takeaway #1: Stability of Bisimulation-based Representations", "content": "KROPE representations induce non-expansive value function updates and are Bellman complete. They avoid divergence of offline value function learning."}, {"title": "3.4 KROPE LEARNING ALGORITHM", "content": "In this section, we present an algorithm that learns the KROPE representations from data. We include the pseudo-code of KROPE in Appendix A. The KROPE learning algorithm uses an encoder \u03c6\u03c9 : S\u00d7A \u2192 Rd, which is parameterized by weights w of a function approximator. It then parameterizes the kernel with the dot product, i.e, kw (81,a1; 82, a2) := \u03c6\u03c9(81,a1)\u03a4\u03c6\u03c9(82, a2) (see Equation (4)). Finally, the algorithm then minimizes the following loss function, which is similar to how the value function is learned in deep RL (Mnih et al., 2015):\nLKROPE (W) := ED 1\\Bigg (\\frac{|r(s1, a\u2081) - r(s2, A2)|}{rmax-rmin} + \u03b3\u0395\u03c0\u03bf [\u039a\u03c9 (81, \u03b11; 82, \u03b1\u2082)] \u2013 \u039a\u03c9 (81, A1; S2, A2) \\Bigg )^2\nwhere the state-action pairs are sampled from D, and are weights of the target network that are periodically copied from w (Mnih et al., 2015). In this work, we use KROPE as an auxiliary task, which introduces only a learning rate as an additional hyperparameter. We note that this fixed-point optimization procedure is similar to how the action-value function is learned in other RL fixed-point algorithms such as fitted q-evaluation (FQE) (Le et al., 2019)."}, {"title": "4 EMPIRICAL RESULTS", "content": "In this section, we present our empirical study designed to answer the following questions.\n1. Does KROPE lead to stable representations with good realization and generalization?\n2. Do KROPE representations lead to stable MSVE and low MSVE?"}, {"title": "4.1 EMPIRICAL SETUP", "content": "In this section, we describe the main details of our empirical setup. For further details such as datasets, policies, hyperparameters, and evaluation protocol please refer to Appendix C."}, {"title": "Takeaway #2: Practical Stable and Accurate Offline Policy Evaluation", "content": "OPE practitioners can use KROPE for stable and accurate evaluation of offline RL agents."}, {"title": "5 LIMITATIONS AND FUTURE WORK", "content": "In this section, we discuss limitations and future work. A shortcoming of our work is that KROPE'S learning algorithm is susceptible to instability since it is a semi-gradient method (Sutton & Barto, 2018). Moreover, its fixed-point optimization means it does not solve any objective function (Feng et al., 2019). In our work, we employed commonly-used techniques such as layernorm and wide neural networks to mitigate instability (Ota et al., 2021; Gallici et al., 2024). While these techniques potentially side-step the issue, the consequences of a semi-gradient method may still exist. In Appendix C.3.1, we present an empirical analysis to gauge when KROPE's learning algorithm may be unstable. We find that while individual off-policy transitions can determine the instability of fixed-point and semi-gradient algorithms such as FQE, pairs of off-policy transitions can determine KROPE'S instability. Since we are unlikely to have control over the distribution over pairs of transitions in practice, we need to resort to fundamental changes to the algorithm. One potential change is based on that by Feng et al. (2019). Their key insight is to leverage the Legendre-Fenchel transformation from optimization theory and replace the fixed-point loss function of semi-gradient methods with an equivalent expression that avoids semi-gradient learning (Rockafellar & Wets, 1998). However, a drawback with this approach is that the new learning objective is a minimax procedure, which can be challenging to optimize in practice. In future work, we will explore the viability of this approach to design a provably convergent version of KROPE."}, {"title": "6 CONCLUSION", "content": "In this work, we tackled the problem of stabilizing offline value function learning in reinforcement learning. We introduced a bisimulation-based representation learning algorithm, kernel representations for OPE (KROPE), that shapes the state-action representations to stabilize this procedure. Theoretically, we showed that KROPE representations are stable from two perspectives: 1) non-expansiveness, i.e., they lead to value function learning updates that enable convergence to a fixed-point and 2) Bellman completeness, i.e., they satisfy a condition for data-efficient policy evaluation. Empirically, we showed that KROPE leads to more stable and accurate offline value function learning than baselines. Our work showed that bisimulation-based representation learning effectively stabilizes long-term performance evaluations of offline reinforcement learning agents."}]}