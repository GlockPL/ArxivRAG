{"title": "Similarity-based Context Aware Continual Learning for Spiking Neural Networks", "authors": ["Bing Han", "Feifei Zhao", "Yang Li", "Qingqun Kong", "Xianqi Li", "Yi Zeng"], "abstract": "Biological brains have the capability to adaptively coordinate relevant neuronal populations based on the task context to learn continuously changing tasks in real-world environments. However, existing spiking neural network-based continual learning algorithms treat each task equally, ignoring the guiding role of different task similarity associations for network learning, which limits knowledge utilization efficiency. Inspired by the context-dependent plasticity mechanism of the brain, we propose a Similarity-based Context Aware Spiking Neural Network (SCA-SNN) continual learning algorithm to efficiently accomplish task incremental learning and class incremental learning. Based on contextual similarity across tasks, the SCA-SNN model can adaptively reuse neurons from previous tasks that are beneficial for new tasks (the more similar, the more neurons are reused) and flexibly expand new neurons for the new task (the more similar, the fewer neurons are expanded). Selective reuse and discriminative expansion significantly improve the utilization of previous knowledge and reduce energy consumption. Extensive experimental results on CIFAR100, ImageNet generalized datasets, and FMNIST-MNIST, SVHN-CIFAR100 mixed datasets show that our SCA-SNN model achieves superior performance compared to both SNN-based and DNN-based continual learning algorithms. Additionally, our algorithm has the capability to adaptively select similar groups of neurons for related tasks, offering a promising approach to enhancing the biological interpretability of efficient continual learning.", "sections": [{"title": "1. Introduction", "content": "Lifelong learning is the prominent capability of biological intelligence and the significant challenge of artificial intelligence. In the process of continuously encountering new environments, the brain effectively identifies the connections between new and old knowledge through task contexts. It reshapes the neural network by associating new tasks with similar prior knowledge to adapt to new information, while strengthening the old knowledge Bar (2007, 2004). However, there is still much room for existing continual learning research to improve context-based efficient and flexible learning.\nSpiking Neural Networks (SNNs) Maass (1997) have been extensively researched due to their high efficiency and bio-interpretability, incorporating SNNs with continual learning mechanisms of the brain provides natural advances. Nevertheless, to the best of our knowledge, there is little continual learning for SNNs. Only ASP Panda et al. (2017) and HMN Zhao et al. (2022) use STDP-based regularization and neuronal activity-based subnetwork selection to overcome catastrophic forgetting, but they are only suitable for shallow networks. DSD-SNN Han et al. (2023b) and SOR-SNN Han et al. (2023a) apply brain-inspired continual learning algorithms to deep SNNs, ignoring the effect of task-to-task associations.\nThe brain adaptively modulates neuronal generation, allocation, extinction, and reuse for continual learning."}, {"title": "2. Related Works", "content": "Structure expansion continual learning. Continual learning algorithms based on structural expansion separate the features of old and new tasks and prepare a unique set of parameters for each task. The most straightforward approach is to construct a separate network for each task. To integrate knowledge, progressive neural network (PNN) Rusu et al. (2016); Siddiqui and Park (2021) adds lateral connections between old and new task networks and freezes the old task network. DER Yan et al. (2021) adds an attention layer to synthesize the convolutional features of all network outputs and combines with iCaRL Rebuffi et al. (2017) algorithm achieving class incremental learning. However, as the number of tasks increases, the parameters of the PNN expand rapidly. To address this issue, subnetwork selection algorithms have been proposed to identify a sub-network for each task within a fixed-size network.\nSimilarity contexts in continual learning. The brain is adept at utilizing old knowledge to enhance learning new tasks. Hence, recent research is concerned with recognizing the similarity between tasks to guide learning new tasks."}, {"title": "3. Prerequisites", "content": "In continual learning, the model sequentially learns a series of non-smooth tasks {T\u2081, ..., T\u2081, ..., T\u0272 }. Each task T has a training set Dtrain = {(xi, yi), i = 1, ..., ntrain } where xi is the picture data, yi is the true lable and ntrain n is the number of training samples. Similarly, T, has a testing set Dtest. When the task T, comes, the model already has the structure and function of the past learned tasks {T1, ..., T\u2081_1}. During training, we continually train based on the existing model using Dtrain. The {Dtrain, ..., Dtrain} are overwhelmingly invisible. Only a few samples can be saved in the memory store. In testing, task incremental learning (TIL) assumes that the task identifiers of the current samples are known, i.e., separately test {Dtest,..., Dtest} as follow:\nSpiking Neural Network\nThe spiking neural network, as a third generation neural network, has the advantages of high biological interpretabil-ity, efficient spatio-temporal joint information transfer, and low energy consumption. The most essential difference between the spiking neural network (SNN) and deep neural network (DNN) is that SNN uses spiking neurons instead of traditional artificial neurons. Spiking neurons receive and transmit discrete spikes, aligning with the information transmission patterns of the biological brain and reducing energy consumption. We use the common leaky integrate-and-fire (LIF) spiking neuron Abbott (1999). The output spike O of LIF neuron i in layer I at step t is calculated as follows:"}, {"title": "3.1. Continual Learning Problem Setting", "content": "In continual learning, the model sequentially learns a series of non-smooth tasks {T\u2081, ..., T\u2081, ..., T\u0272 }. Each task T has a training set Dtrain = {(xi, yi), i = 1, ..., ntrain } where xi is the picture data, yi is the true lable and ntrain n is the number of training samples. Similarly, T, has a testing set Dtest. When the task T, comes, the model already has the structure and function of the past learned tasks {T1, ..., T\u2081_1}. During training, we continually train based on the existing model using Dtrain. The {Dtrain, ..., Dtrain} are overwhelmingly invisible. Only a few samples can be saved in the memory store. In testing, task incremental learning (TIL) assumes that the task identifiers of the current samples are known, i.e., separately test {Dtest,..., Dtest} as follow:\n$max \\underset{\\theta}{ } E_{(x_j,y_j)\\sim T_i}[logp_{\\theta}(y_j|x_j,T_i)]$\nwhere $\\theta$ is the network parameters. However, in the real world, task identifiers are not always available. Therefore, class incremental learning (CIL) requires testing the mixed dataset Dtest of all samples in all learned tasks {T\u2081, ..., T\u2081}:\n$max \\underset{\\theta}{ } E_{(x_j,y_j)\\sim\\{T_1,...,T_i\\}}[logp_{\\theta}(y_j|x_j)]$"}, {"title": "3.2. Spiking Neural Network", "content": "The spiking neural network, as a third generation neural network, has the advantages of high biological interpretabil-ity, efficient spatio-temporal joint information transfer, and low energy consumption. The most essential difference between the spiking neural network (SNN) and deep neural network (DNN) is that SNN uses spiking neurons instead of traditional artificial neurons. Spiking neurons receive and transmit discrete spikes, aligning with the information transmission patterns of the biological brain and reducing energy consumption. We use the common leaky integrate-and-fire (LIF) spiking neuron Abbott (1999). The output spike O of LIF neuron i in layer I at step t is calculated as follows:\n$U_i^l = \\tau (1 - U_i^{l, t-1}) + \\sum_{j=1}^{M^{l-1}} W_{ij}^{l} O_j^{l-1}$ \n$O_i^l = \\begin{cases}1, & U_i^l \\geq V_{th} \\\\0, & U_i^l < V_{th}\\end{cases}$\nwhere $\\tau = 0.2$ is the time constant, $U_i^l$ is the membrane potential, and $V_{th}$ is the spiking threshold. Since discrete spike information hinders the gradient-based backpropagation algorithm, deep SNN uses surrogate gradients to optimize the network. Specifically, a derivable continuous function is used to approximate the derivative of the spike. The proposed algorithm uses Qgategrad Qin et al. (2020) surrogate gradients algorithm:\n$\\dot O_i^l = \\begin{cases} 0, & |O_i^l| > \\frac {1} {2} \\\\ 1-4|O_i^l|^2 , & |O_i^l| \\leq \\frac {1} {2} \\end{cases}$"}, {"title": "4. Method", "content": "We elucidate the SCA-SNN algorithm as follows. An overview flow in Section 4.1, the computational details in Section 4.2, including the similarity evaluation, discriminative expansion and selective reuse."}, {"title": "4.1. SCA-SNN Architecture", "content": "Similarity-based context assessment. As shown in Fig. 1, context assessment serves as the first step of the proposed algorithm. When a new task arrives, SCA-SNN first evaluates the similarity St,p, p = 1,..., t 1 between the new task T, and each of the previous tasks T\u2081, ..., T7-1 to guide subsequent expansion and reuse. This evaluation combines data similarity with the current network state, and does not require an additional evaluation network. A smaller value of St,p indicates higher similarity between the two tasks. When learning the first task, SCA-SNN is trained as SNN routine in a predefined initial network.\nNeuronal discriminative expansion. Based on the above similarity, to balance the cost of network expansion while considering the learning capacity of the new task, we compute the number of neuron expansions for the new task, which is proportional to S\u2081\u2081p. When two tasks are similar, the network expansion scale dynamically decreases. Neuron population Ne is added to the layer 1, and Ne is initially fully connected to all the neurons in the previous layer 1-1. which contains all the existing neurons of the old task {Ne-1, ..., Ne\u22121} and the newly expanded neurons Ne\u22121. Existing neurons of the old task can not add input synapses and change the weights of the input synapses but can add new learnable output synapses.\nNeuronal selective reuse. During the training of the new task, SCA-SNN uses a gradient-based neuron relevance assessment method, combined with the similarity to select"}, {"title": "4.2. SCA-SNN Computational Details", "content": "In this section, we introduce the detailed scheme we use throughout this paper."}, {"title": "4.2.1. Similarity-based context assessment", "content": "When a new task arrives, SCA-SNN does not modify the network structure immediately. Instead, it first evaluates the relationship between the new task and each of the previous tasks. By alternating between similarity assessment and learning the new task within a unified network, SCA-SNN integrates data and the current network state without requiring additional similarity assessment networks.\nIn the continual learning scenario, we can only obtain the features of the current task. In order to judge the similarity between the current task and the previous task, we save the mean value of the sample features M\u2081\u2081 of each task category t as the feature anchors after each task for subsequent task calling. The feature anchor size is the product of the number of categories per task and the feature dimension, which does not cause additional memory burden. Specifically, we input the new task samples into each of the existing old task neuron populations to extract features Ft,p, p = 1, ..., t 1 and compute the feature means M\u2081\u2081p for each category c\u2208 C\u2081. To ensure the scatter stability in SNN, we introduce the coefficient y < 1. It only guarantees that the scatter is greater than 0, and does not affect the relative magnitude of the similarity. In summary, according to the independent sampling KL divergence calculation method proposed by"}, {"title": "4.2.2. Neuronal discriminative expansion", "content": "Depending on the difference in similarity-based context association between the current and previous tasks, SCA-SNN discriminatively expands different scale neuron populations joining the network for each new task to learn new features. The association magnitude A of the new task with the previous task is defined as the minimum value of the similarity between the new task and each old task as follow:\n$KL_{t,p} = \\sum_{c \\in C} log \\frac{||F_{t,p} - M_{p,p}||_2}{\\gamma||F_{t,p} - M_{t,p}||_2}$ \nThen we map the KL scatter to the similarity ranging from [0, 1] as the following formula:\n$S_{t,p} = min\\{KL_{t,p}, 1 - e^{2KL_{t,p}}\\}$\nWe obtain the similarity between the new task and each old task {St.1,..., St.t-1}.\nA smaller value of A indicates a stronger association, implying that many features already learned in the old tasks are similar to the new task and can be reused (see the next subsection). Therefore, the number of newly expanded neurons can be reduced without affecting the learning ability. To ensure that the number of expanded neurons remains within a reasonable range, we design the following adaptive calculation method to determine the number of neurons to be expanded in layer 1:\n$A = min\\{S_{t,1}, ..., S_{t,t-1}\\}$\n$NUM = M(1 - e^{-aA})$\nwhere M is the maximum number of neuron expansions, and the minimum value is 0."}, {"title": "4.2.3. Neuronal selective reuse", "content": "In the developmental habituation mechanism of the brain, neurons of learned habituation require repeated stimulation to re-fire, while irrelevant ones are disconnected Grissom and Bhatnagar (2009). Inspired by this, we propose a gradient-based neuronal relatedness assessment to determine which neurons from the previous tasks are beneficial for the new task. At the beginning of the new task learning, the newly expanded neurons are fully connected to all the neurons of the old task. We learn the weights of the newly expanded neurons while keeping the old task neurons' input synaptic weights unchanged and calculating their gradients. Larger gradients indicate that the synaptic weights would require significant changes to be effective for the new task, suggesting that these synapses are not relevant to the new task and may hinder learning if left unchanged. In our SCA-SNN model, we evaluate relatedness at the neuron level by summing the input synaptic gradients G of neurons from the old task.\nAs habituation activation in the brain requires repeated stimulation, the re-selection of neurons by the new task in our SCA-SNN also requires multiple judgments. We use a neuron relatedness function R to judge that when the function of the old task neuron to the new task relatedness stays high multiple times, that neuron is re-selected by the new task network. Conversely, when the neuronal relatedness function is low for many times, the neuron will be judged as an irrelevant neuron excluded from the new task network. The relatedness function formula is as follow:\n$R_i^{t,p} = 0.99R_i^{t-1,p} - e^{-epoch/2} (2Norm(G_i^p) - p_p)$\nWhere the negative exponent $e^{-epoch/2}$ make the pruning speed decreases with the epoch. This promotes the gradual stabilization of the network structure while knowledge learning is maturing, in line with the developmental process of biological brain networks that first rapidly decay and then gradually stabilize Huttenlocher et al. (1979).\nWe disconnect the old task neurons with $R_i^{t,p}$ less than 0 from the new task neurons. Here, $p_p$ is determined by the similarity $S_{t,p}$ between the new and old tasks to which the current neuron belongs. The larger the $S_{t,p}$, the smaller $p_p$ is, and the more neurons are disconnected. $p_p$ is calculated as follow:\n$p_p = \\beta - S_{t,p} + bias$"}, {"title": "5. Experimental Results", "content": "To validate the effectiveness of the SCA-SNN model, we not only conduct extensive experiments on the generalized CIFAR100 and ImageNet (Mini-ImageNet and Tiny-ImageNet) datasets but also test the proposed similarity-based context aware algorithm in the mix-task datasets FMNIST-MNIST and SVHN-CIFAR100. The specific experimental datasets are as follows:"}, {"title": "5.1. Datasets and Models", "content": "To validate the effectiveness of the SCA-SNN model, we not only conduct extensive experiments on the generalized CIFAR100 and ImageNet (Mini-ImageNet and Tiny-ImageNet) datasets but also test the proposed similarity-based context aware algorithm in the mix-task datasets FMNIST-MNIST and SVHN-CIFAR100. The specific experimental datasets are as follows:\nSplit CIFAR100: We train the natural image dataset CIFAR100 in 10 steps split (10 new classes per step) and 20 steps split (5 new classes per step) without pre-training phase.\nSplit ImageNet: We randomly select 100 classes and 200 classes to form the Mini-ImageNet and Tiny-ImageNet datasets, and split them into 10 steps.\nPermuted FMNIST - MNIST: We respectively permute the FMNIST and MNIST datasets to five tasks via random permutations of the pixels. Each task contains ten classes, divided into 60,000 training and 10,000 test samples. We alternately learn FMNIST and MNIST for a total of ten tasks.\nRotated SVHN - Split CIFAR100: The SVHN dataset is arranged into five tasks by rotational transformation at different angles. We alternate between learning five Rotated SVHM tasks and five Spilt CIFAR100 tasks. Each task consists of 10 classes."}, {"title": "5.2. Comparison with SNN-based continual learning", "content": "First, we compare the few existing deep SNN-based structure-expanded continual learning algorithms DSD-SNN Han et al. (2023b) and SOR-SNN Han et al. (2023a). In addition, since SNN-based continual learning algorithms belong to the preliminary exploration with less existing work, we reproduce some DNN-based continual learning in SNNs for comparison to show the superiority of our algorithms in SNNs. As shown in Fig. 2, our SCA-SNN consistently demonstrates superior performance throughout the learning process for all tasks in both general and mixed datasets. Particularly in generalized high-resolution Mini-ImageNet, the SCA-SNN model achieves a significant improvement in the accuracy. Our average accuracy is 75.7%, which is 20.7% higher than SOR-SNN, with the second highest accuracy. Moreover, compared with the decreasing accuracy trend of other methods, the accuracy of our SCA-SNN is stable at around 75%, which indicates that proper selective reuse not only efficiently utilizes the past learned knowledge to help learn new tasks but also eliminates the interference of irrelevant knowledge on new tasks.\nIn the mixed dataset, the accuracy of our SCA-SNN remains stable across tasks of different types. However, the EWC, MAS, LSTM_NET, and SOR-SNN methods all exhibit task-related regularity fluctuations during the alternating learning process of mixed datasets. These methods perform well only on a set of similar tasks, with a significant decrease in accuracy on the other set of tasks. In particular, two regularization methods, EWC and MAS, fluctuate dramatically. For example, in the SVHN-CIFAR100 dataset, when the first task is from the SVHN dataset, these two methods are effective only for SVHN tasks, while their accuracy on CIFAR100 tasks approaches random selection. HNET completely loses its learning ability when the task type changes. This indicates that the flexibility of these algorithms in responding to different tasks in SNNs requires enhancement. Continual learning algorithms specifically designed for SNNs that can adapt to dynamically changing environments are urgently needed. Unlike the above methods, our SOR-SNN achieves an average accuracy of 90.73% when the first task comes from SVHN, which is higher than the second most accurate one, DSD-SNN, by 13.25%. When the first task is from the CIFAR100 dataset, SCA-SNN achieves an average accuracy of 90.31%, which is not significantly different from the performance before the exchange of tasks. This is attributed to our similarity-based context aware algorithm, which effectively recognizes the similarity between tasks and performs targeted neuron expansion and reuse, allowing the network to adapt effectively to different tasks."}, {"title": "5.3. Comparison with DNN-based continual learning", "content": "For extensive comparison and analysis, we compare our method with mature DNN-based continual learning methods on task incremental and class incremental learning. For the CIFAR100 dataset as Tab. 1, our method achieves an accuracy of 85.61\u00b10.24% in TIL and 57.06.\u00b10.29% in CIL of 10steps. Moreover, in the 20steps scenario with more tasks, SCA-SNN achieves superior accuracy of 86.45\u00b10.35% in TIL and 50.19\u00b10.45% in CIL, respectively. These results outperform the structural expansion algorithms such as ERK Yildirim et al. (2024), with improvements of 8.98% and 8.64% in TIL for the 10steps and 20steps scenarios, respectively, and outperform HAT Serra et al. (2018) by 15.96% and 24.19% in CIL. For the more complex Tiny-ImageNet dataset in 10steps scenario, as shown in Tab. 2, our algorithm achieves the highest performance among a series of DNN-based continual learning algorithms. The average accuracy of SCA-SNN reaches 71.92\u00b10.52%, which improves accuracy by 3.10% compared to the RDER Wang et al. (2024) algorithm, whose accuracy is the next highest. In summary, although our SCA-SNN algorithm is implemented based on SNN, we achieve the same favorable performance compared to DNN-based algorithms."}, {"title": "5.4. Energy consumption", "content": "We compare the network energy consumption of the proposed methods in terms of number of connections, number of neurons, floating point operations per second (FLOPs), and computational energy, as Tab. 3 shows. Among them, the number of connections and neurons is the average of the number of weights and neurons activated by each task. The computational energy follows the widely used Chakraborty et al. (2021), for DNN:\n$E_{SNN} = FLOPP_{SNN} * E_{MAC}$\nwhere $E_{MAC} = 4.6pJ$ is the energy consumption of multiply-accumulate (MAC) operations. For SNN:\n$E_{SNN} = FLOPP_{SNN} * E_{AC} *T$\nwhere $E_{AC} = 0.9pJ$ is the energy consumption of accumulate (AC) operations.\nThe results show that after completing the learning of all tasks, the parameter pruning rate of our network is 42.08%, and the average number of connections and neurons used in testing are respectively 8.4\u00d7106 and 3111, which are significantly lower than the other methods. In this case, the required FLOPs are also smaller. This is because when learning similar tasks, our SCA-SNN model reuses more existing neurons to reduce the expansion of new neurons. When learning dissimilar tasks, the existing irrelevant neurons are heavily pruned. Therefore, compared to regularization and replay methods, our actual number of activated connections and neurons is less than the standard structure of ResNet18. In comparison to structure expansion algorithms, our similarity-based discriminative expansion and selective reuse increase the sparsity of reused neurons while reducing the number of newly expanded neurons.\nIn addition, compared to DNN, our SNN-based method replaces the multiply-accumulate operation with the accumulate operation. Under the same FLOPs, SNN-based methods require lower computational energy. Since our method has lower FLOPs and is based on SNNs, it achieves the lowest computational energy 3.3\u00d7109 pJ. In summary, with the dual effect of the sparsity of the proposed SCA method and the high efficiency of SNN, our SCA-SNN method significantly reduces the network energy consumption."}, {"title": "5.5. Ablation Studies", "content": "We validate the effectiveness of the proposed similarity assessment method and the parameter robustness of the proposed SCA-SNN, including: similarity parameter y, expansion parameter a, and reuse parameter \u03b2. For similarity assessment, we compare the proposed KL-based algorithm with other commonly used methods for similarity distance assessment, including Jensen-Shannon divergence, Jeffreys"}, {"title": "5.6. Similarity-based neuronal population assignment", "content": "In the biological brain, the nervous system activates similar neuron populations when learning similar tasks to efficiently coordinate multiple cognitive tasks. Our SCA-SNN learning process involves reusing neurons from past similar tasks and heavily pruning irrelevant neurons from past dissimilar tasks. The magnitude of newly expanded neurons is proportional to the similarity distance. Thus, when learning a new task t similar to the previous task t 1, the neurons activated by both are more similar; when learning a new task unrelated to the previous task, the overlap of the neuron populations activated by both is less. Fig. 4 a visualizes the expansion and reuse of the fourth convolutional layer of ResNet18 during the learning of Task 1 to Task 4 for the CIFAR100 10steps TIL. Task 1 initializes a dense network. As an example the distances of Task 3 from Tasks 1 and 2 are 0.74 and 0.29 respectively, indicating that Task 3 is more similar to Task 2 and less similar to Task 1. Thus task 3 heavily reuses neurons from task 2 and extends fewer neurons, while pruning more neurons already in task 1 that are irrelevant to task 3.\nIn addition, we statistics the relationship between similarity and the number of selectively reused neurons, the relationship between the distance magnitude among tasks and the pruning rate of task-related neurons. In Fig. 4 b, it is observed that in the ResNet18 SNNs pathway for tasks 6, 7, 9, and 10, a larger distance indicates less similarity between tasks and leads to a higher pruning rate of neuron population belonging to the old task. For example, when learning the new task 7, the distance between task 7 and task 3 was evaluated to be 0.12, and the task 7 network pruned only 18.57% of the neurons extended from task 3. The distance between task 7 and task 6 was evaluated to be a larger 0.74. Thus, the task 7 pathway pruned 45.46% of the neurons belonging to task 6. This demonstrates that our SCA-SNN model can adaptively coordinate similar neuron populations for similar tasks through similarity-based discriminative expansion and selective reuse, aligning with the multi-task continual learning mechanism of the biological brain and improving learning efficiency."}, {"title": "6. Discussion", "content": "In the lifelong learning process of humans, the brain gradually acquires proficiency in various tasks and adapts to ever-changing environments. Among these processes, the task context recognition mechanism plays a crucial role by associating new tasks with similar previously learned tasks and selecting useful knowledge from the old tasks to facilitate the learning of new ones. Inspired by this, we propose the similarity-based context aware continual learning algorithm called SCA-SNN, which is based on the brain-like SNN infrastructure. When a new task arrives, SCA-SNN first uses its own network to identify the similarity relationships between the new task and each of the learned tasks. This information guides subsequent neuron expansion and reuse, ultimately leading to the adaptive allocation of similar neuron groups for similar tasks.\nUnlike other continual learning algorithms, when faced with non-smooth continual learning tasks with large knowledge spans, the proposed algorithm associates tasks containing similar knowledge with similar neuron groups, assigning different tasks with their own neuron groups. Our method enables stable learning for different tasks while reducing interference and facilitating appropriate beneficial knowledge transfer. Thus, during the cross-continual learning process of different tasks, the accuracy of proposed algorithm continues to rise steadily. In contrast, regularization-based methods such as EWC SNN and MAC SNN suffer from incompatibility between tasks due to significant differences in important synapses for different tasks, leading to a loss of continual learning ability after significant task-related fluctuations. Structure-based methods like SOR-SNN and LSTM_NET SNN, which independently select subnetworks for each task without considering the similarity relationships between tasks, also result in task-related performance fluctuations and a decline in final performance. Furthermore, by expanding and reusing neuron groups based on similarity relationships, our method avoids redundant neurons between similar tasks and interference between neurons of dissimilar tasks, significantly reducing energy consumption while effectively enhancing performance. Combined with the energy-saving attributes of discrete SNN networks, the proposed method reduces energy consumption by 1.21, 1.55, and 8.52 times, respectively, compared to the DNN-based regularization algorithm MAS, the replay algorithm iCaRL, and the structural expansion algorithm DER.\nTo verify the effectiveness of the proposed algorithm, we conducted extensive experiments not only on general datasets such as CIFAR100 and Tiny-ImageNet but also validated the performance of SCA-SNN in more realistic non-smooth cross-task learning scenarios. The experimental results demonstrate that the proposed algorithm exhibits stable and efficient learning capabilities for different tasks, with enhanced performance robustness, structural adaptability, and biological plausibility. However, SCA-SNN focuses on completing different image classification tasks and does not yet possess the comprehensive recognition and continual learning abilities for various cognitive tasks such as perception, decision-making, and reasoning. In the future, we will further enhance the degree of freedom in neuron allocation to achieve adaptive continual learning for dynamic tasks in real-world environments within a single efficient non-traditional structural network."}, {"title": "7. Conclusion", "content": "Inspired by brain contextual association mechanism, we propose a similarity-based context aware spiking neural network continual learning algorithm applied in both task incremental learning and class incremental learning. The proposed similarity assessment algorithm can effectively identify the association of different task contexts, guiding deep SNNs to perform neuron discriminative expansion and selective reuse, reducing task interference, and improving knowledge utilization efficiency. Our SCA-SNN algorithm achieves superior performance and reduces network energy consumption compared to both SNN-based and DNN-based continual learning for generalized and mixed datasets. Meanwhile, experiments demonstrate that our algorithm enables adaptive allocation of similar neuron populations for similar tasks like the biological brain."}]}