{"title": "EUR/USD Exchange Rate Forecasting incorporating Text Mining Based on Pre-trained Language Models and Deep Learning Methods", "authors": ["Xiangyu Shi", "Hongcheng Ding", "Salaar Faroog", "Deshinta Arrova Dewi", "Shamsul Nahar Abdullah", "Bahiah A Malek"], "abstract": "This study introduces a novel approach for EUR/USD exchange rate forecasting that integrates deep learning, textual analysis, and particle swarm optimization (PSO). By incorporating online news and analysis texts as qualitative data, the proposed PSO-LSTM model demonstrates superior performance compared to traditional econometric and machine learning models. The research employs advanced text mining techniques, including sentiment analysis using the RoBERTa-Large model and topic modeling with LDA. Empirical findings underscore the significant advantage of incorporating textual data, with the PSO-LSTM model outperforming benchmark models such as SVM, SVR, ARIMA, and GARCH. Ablation experiments reveal the contribution of each textual data category to the overall forecasting performance. The study highlights the transformative potential of artificial intelligence in finance and paves the way for future research in real-time forecasting and the integration of alternative data sources.", "sections": [{"title": "I. INTRODUCTION", "content": "In the intricate tapestry of the global financial market, the exchange rate between the Euro and the US Dollar (EUR/USD) stands as a pivotal thread, weaving together the economic narratives of two powerhouse currencies. As a barometer of the economic relations between the Eurozone and the United States, the EUR/USD exchange rate holds profound implications for international trade, cross-border investments, and the overall health of the global economy. Accurately forecasting this exchange rate has become a holy grail for financial market participants, as it can provide a strategic edge in navigating the complex and ever-shifting currents of the foreign exchange market [1], [2].\nRecent advancements in the field of EUR/USD exchange rate forecasting have been propelled by the advent of machine learning and the proliferation of big data. State-of-the-art methodologies have harnessed the power of deep learning architectures, such as Long Short-Term Memory (LSTM) networks, to capture the non-linear and temporal dependencies inherent in financial time series data [3], [4]. These models have shown promising results in predicting exchange rate movements by learning from vast amounts of historical data and uncovering hidden patterns that traditional econometric models might overlook [5].\nHowever, despite the significant strides made by these cutting-edge approaches, there remain notable limitations. One critical drawback is the reliance on structured, quantitative data, such as historical prices and economic indicators [6]. While these data points provide valuable insights, they fail to fully capture the rich tapestry of qualitative information that shapes market sentiment and drives currency fluctuations. News articles, financial reports, and social media discussions often contain nuanced and real-time insights that can signif-icantly influence exchange rate dynamics. Neglecting these qualitative data sources can lead to an incomplete understand-ing of the market and suboptimal forecasting performance.\nTo address this challenge, we propose a novel approach that seamlessly integrates both quantitative and qualitative data to enhance the accuracy and robustness of EUR/USD exchange rate forecasting [7], [8]. Our methodology leverages the power of LLMs, specifically GPT-4, to process and extract relevant information from vast amounts of textual data. By employing advanced techniques such as prompt engineering and sen-timent analysis, we enrich the predictive power of LSTM networks, enabling them to consider both historical price pat-terns and real-time market sentiments [9], [10]. Furthermore, we incorporate PPSO to fine-tune the hyperparameters of our model, ensuring optimal performance and adaptability to changing market conditions [11].\nThis research introduces several innovative contributions to the field of financial forecasting, enhancing existing method-ologies and introducing novel approaches:\n\u2022 We demonstrate how to effectively integrate state-of-the-art LLMs for preprocessing and annotating financial data, significantly improving the quality and relevance of the dataset for forecasting purposes. This involves utilizing LLMs and prompt engineering techniques to filter noise from high-noise news sources, ensuring cleaner and more accurate data for analysis.\n\u2022 Our study showcases the combined use of twitter-ROBERTa-Large-topic-sentiment-latest and ROBERTa-"}, {"title": "II. LITERATURE REVIEW", "content": "The landscape of exchange rate forecasting undergoes a significant transformation, transitioning from a reliance on traditional econometric models to the adoption of advanced machine learning and deep learning techniques. In the early stages, this field relies heavily on conventional statistical mod-els such as Autoregressive Moving Average (ARMA), Autore-gressive Integrated Moving Average (ARIMA), and General-ized Autoregressive Conditional Heteroskedasticity (GARCH) [12], [13]. These models aim to predict exchange rate move-ments by analyzing the inherent features of historical time series data. However, they often struggle to capture the com-plex, non-linear dynamics and non-stationary characteristics of exchange rate time series [14], [15]. This limitation of traditional econometric models highlights the need for more sophisticated and adaptive methodologies capable of handling the intricate nature of financial data.\nThe advent of machine learning techniques marks a signif-icant milestone in the evolution of exchange rate forecasting. Algorithms such as Support Vector Machines (SVM) and Ran-dom Forests gain prominence due to their ability to uncover hidden patterns and trends in complex datasets [16], [17]. These methods offer a level of flexibility and accuracy that often surpasses that of traditional statistical models. For in-stance, Yu et al. [18] demonstrate the superior performance of SVM in forecasting exchange rates compared to conventional ARIMA models. Similarly, other researchers [19] showcase the effectiveness of Random Forests in capturing the non-linear relationships in exchange rate dynamics. The success of these early machine learning applications paves the way for more advanced techniques and sets the stage for the integration of deep learning in exchange rate forecasting.\nDespite the promising results of machine learning methods, the prediction of EUR/USD exchange rates continues to face challenges due to the limitations of relying solely on historical data. Traditional econometric models and early machine learn-ing approaches primarily focus on analyzing past exchange rate trends, which often fail to capture the real-time dynamics and sentiment-driven fluctuations of the market. However, the emergence of big data, especially in the form of unstructured textual data, provides a new source of information that could potentially revolutionize exchange rate forecasting [20]. On-line text mining and sentiment analysis become prominent areas of research, with studies demonstrating the value of incorporating market sentiment derived from various sources such as news articles, social media, and financial reports.\nOne notable example is the work of Das and Chen [21], who develop a method to extract investor sentiments from online forums and social media to predict stock market trends. Their approach showcases the potential of leveraging user-generated content to gauge market sentiment and improve prediction accuracy. Similarly, Tetlock [22] quantifies media sentiment by analyzing content from The Wall Street Journal and explores its impact on the stock market and EUR/USD exchange rate fluctuations. These studies highlight the importance of considering sentiment trends and topic-specific discussions in understanding and predicting exchange rate movements.\nThe introduction of deep learning technologies marks a significant leap forward in the field of exchange rate fore-casting. Models such as LSTMnetworks and Gated Recurrent Units (GRU) demonstrate remarkable abilities in capturing intricate patterns and long-term dependencies in time series data [23], [24]. The multi-layered, non-linear architecture of these models allows them to learn complex representations from vast amounts of financial data, making them particularly well-suited for analyzing high-dimensional datasets [25]. The application of deep learning in exchange rate forecasting opens up new avenues for research and showcases the potential for more accurate and robust predictions.\nHowever, the incorporation of textual information into deep learning models for exchange rate prediction remains relatively unexplored. While studies like Smales [26] and Beckmann et al. [27] make early attempts to integrate news sentiment into financial market forecasting, the full potential of leveraging large-scale textual data through advanced language models has yet to be realized. The development of transformer-based models, such as BERT [28], opens up new possibilities for mining textual information and extracting valuable insights for exchange rate forecasting. These models, pre-trained on vast amounts of unlabeled text data, capture intricate semantic relationships and contextual information, making them ideally suited for processing financial news and sentiment analysis.\nRecent advancements in natural language processing (NLP) further expand the horizons of exchange rate forecasting by enabling the fusion of heterogeneous data sources. Researchers begin to explore the integration of structured data, such as macroeconomic indicators, with unstructured data, including news articles and social media posts. For instance, Serrano-Cinca et al. [29] propose a framework that combines traditional financial data with sentiment extracted from Twitter to predict"}, {"title": "III. METHODOLOGY", "content": "Our research design seamlessly integrates advanced text analysis techniques with robust financial modeling to forecast exchange rate movements accurately, shown in Figure 1. The process begins with the collection and preprocessing of relevant data, including exchange rates, financial news, and market data. The preprocessing stage involves normalization and sentiment annotation using a fine-tuned BERT model, ensuring data quality and consistency.\nFollowing the preprocessing, we conduct sentiment analysis to extract market sentiments from news texts. These sentiment features are then combined with key financial indicators to cre-ate a comprehensive feature set that will inform our forecasting models. The careful selection and integration of these features are critical to capturing the complex dynamics of the foreign exchange market.\nIn the final phase, the curated features are fed into a Vector Autoregression (VAR) model to predict EUR/USD exchange rate movements. The VAR model, a mainstay in economet-ric modeling, is well-suited to capture the interdependencies among multiple time series variables. By incorporating both quantitative financial data and qualitative sentiment features, our VAR model aims to provide a holistic view of the factors driving exchange rate fluctuations.\nThe effectiveness of our forecasting model is evaluated by comparing its predictions against actual market behavior. Particular emphasis is placed on assessing the added value of textual sentiment analysis in capturing market dynamics. This evaluation process provides insights into the model's performance and guides further refinements to enhance its predictive accuracy."}, {"title": "B. Data Processing for Textual Data", "content": "Our methodology for processing textual data involves a multi-stage approach, as illustrated in Figure 2. The process begins with the original data, which undergoes a series of transformations to extract valuable insights.\nThe first stage is data cleaning, where the raw data is pro-cessed using Python scripts to remove noise, inconsistencies, and irrelevant information. The cleaned data is then passed through an API, which serves as an interface between the cleaning stage and the subsequent filtering stage.\nIn the filtering stage, the cleaned data is further refined using an LLM. The LLM applies advanced natural language processing techniques to filter out unwanted elements and retain only the most pertinent information. This filtered data is then passed to the extracting stage via another API.\nThe extracting stage employs the same LLM to extract key features, patterns, and insights from the filtered data. The LLM's ability to understand context and semantics enables it to identify the most salient aspects of the text. The extracted information is then channeled through an API to the final annotating stage.\nIn the annotating stage, the LLM is utilized once more to annotate the extracted data with meaningful labels, tags, or categories. This annotation process enriches the data by adding structured metadata, making it more accessible and interpretable for further analysis.\nThroughout the data processing pipeline, prompt engineer-ing plays a crucial role in guiding the LLM's behavior and outputs. Carefully designed prompts are used to elicit precise, contextually relevant, and coherent responses from the model at each stage. This targeted approach ensures that the LLM focuses on the specific aspects of the task at hand, yielding results that align closely with the desired objectives.\nBy leveraging the power of LLMs and prompt engineering, our methodology enables the efficient and effective processing of textual data, transforming raw information into valuable insights that can inform decision-making and analysis in various domains."}, {"title": "C. LLMs", "content": "LLMs like CHATGPT-4.0 have revolutionized the field of Natural Language Processing (NLP). These models, built upon deep learning architectures and trained on vast amounts of data, possess remarkable capabilities in understanding and generating human language [39].\nThe core strength of LLMs lies in their ability to capture the intricate structures and semantic patterns of language. By learning from extensive text corpora, LLMs develop a deep understanding of vocabulary, grammar, context, and the nuances of language use [40]. This enables them to generate fluent, coherent, and contextually relevant text, making them highly adaptable to various linguistic settings.\nOne of the key advantages of LLMs is their ability to understand context and infer implicit meanings. Rather than merely processing surface-level information, LLMs can grasp the underlying intent, emotional tone, and contextual connec-tions within the text [41]. This depth of understanding stems from their vast parameter space and complex neural network architectures, enabling them to handle ambiguous or context-dependent language with greater accuracy.\nMoreover, the continuous training process of LLMs on di-verse datasets equips them with broad cross-cultural and cross-domain knowledge. This makes them particularly powerful in applications that span multiple languages, cultures, and subject areas. The versatility of LLMs has opened up new possibilities in NLP, pushing the boundaries of what artificial intelligence can achieve in language understanding and generation.\nThe CHATGPT-4.0 and BERT models, both based on the transformer architecture shown in Figure 3, exemplify the state-of-the-art in LLMs. In their encoder components, they process input text through stacked multi-head self-attention mechanisms and fully connected networks [42]. The input text is represented as a sum of token embeddings, segment embeddings, and positional embeddings, capturing both the semantic and positional information.\nThe encoder consists of multiple layers, each containing a multi-head self-attention sublayer and a feed-forward network (FFN) sublayer. Residual connections and layer normalization are applied between the sublayers to facilitate gradient flow and stabilize training.\nThe attention mechanism, a core component of transform-ers, enables the model to weigh the relevance of different input tokens dynamically. By computing attention scores between"}, {"title": "D. Sentiment Analysis with RoBERTa-Large", "content": "For sentiment analysis, we employ the RoBERTa-Large model, a variant of the ROBERTa (Robustly Optimized BERT Pretraining Approach) model [43]. RoBERTa builds upon the BERT architecture, introducing several key modifications to improve performance and robustness.\nThe ROBERTa-Large model is pre-trained on a vast corpus of unlabeled text, allowing it to develop a deep understanding of language structure and semantics. We then fine-tune the model on our specific sentiment analysis task, leveraging its pre-trained knowledge to achieve high accuracy in understand-ing the nuances of financial language.\nOne of the strengths of the RoBERTa-Large model is its bidirectional architecture, which enables it to capture context from both past and future tokens in the input sequence. This bidirectional understanding is particularly crucial in sentiment analysis, where the sentiment of a word or phrase can be heavily influenced by its surrounding context.\nIn our study, we fine-tune the ROBERTa-Large model to classify the sentiment of financial news articles based on the movement of the EUR/USD exchange rate within a single trading day. The model is trained to predict the sentiment label $M_t$ based on the day's closing exchange rate $P_t$:\n$M_t = \\begin{cases} 0, & P_t < P_{t-1} \\\\ 1, & P_t \\geq P_{t-1} \\end{cases}$ (4)\nBy learning to associate patterns in the text with the corresponding exchange rate movements, the RoBERTa-Large model can uncover latent relationships between sentiment and market dynamics.\nThe fine-tuning process involves careful selection of hyper-parameters such as learning rate, batch size, and the number of training epochs. We employ a systematic approach to hy-perparameter optimization, evaluating different configurations to identify the most effective settings. The details of our hyperparameter search are provided in Appendix B."}, {"title": "E. Time Series Forecasting with LSTM", "content": "To capture the temporal dependencies in the EUR/USD exchange rate time series, we employ the LSTMmodel [44]. LSTM is a variant of Recurrent Neural Networks (RNNs) designed to overcome the limitations of traditional RNNs, such as the vanishing and exploding gradient problems.\nThe key innovation of the LSTM architecture is the in-troduction of memory cells and gating mechanisms, shown in Figure 4. Each LSTM unit consists of an input gate, a forget gate, and an output gate, which regulate the flow of information into and out of the memory cell. This allows the model to selectively retain or forget information over long sequences, enabling it to capture both short-term and long-term dependencies.\nThe input gate $i_t$ controls the amount of new information added to the memory cell, while the forget gate $f_t$ determines what information should be discarded. The output gate $o_t$ regulates the exposure of the memory cell to the next layer. The memory cell state $C_t$ represents the internal state of the LSTM unit, which is updated based on the gating mechanisms. The computations within an LSTM unit can be summarized as follows:\n$f_t = \\sigma(W_f. [h_{t-1},x_t] + b_f)$ (5)\n$i_t = \\sigma(W_i\\cdot [h_{t-1},x_t] + b_i)$ (6)\n$\\tilde{C_t} = tanh(W_c\\cdot [h_{t-1},x_t] +b_c)$ (7)\n$C_t = f_t\\cdot C_{t-1} + i_t \\cdot \\tilde{C_t}$ (8)\n$o_t = \\sigma(W_o\\cdot [h_{t-1},x_t] + b_o)$ (9)\n$h_t = o_t tanh(C_t)$ (10)\nwhere $W_f, W_i, W_c,$ and $W_o$ are weight matrices; $b_f, b_i, b_o,$ and $b_c$ are bias terms; $\\sigma$ is the sigmoid activation function; and tanh is the hyperbolic tangent activation function.\nThe LSTM model takes as input a sequence of historical exchange rates and outputs a prediction for the future exchange rate. The length of the input sequence, known as the time step length, determines the amount of historical data used to make each prediction. This allows the model to learn both short-term and long-term patterns in the exchange rate dynamics [45].\nThe number of LSTM units in the hidden layer determines the model's capacity to learn complex representations of the input data. These units are fully connected, processing the input through weighted summation and applying activation functions to produce the output.\nBy capturing the temporal dependencies and learning robust representations of the exchange rate time series, the LSTM model enables accurate forecasting of future EUR/USD ex-change rate movements."}, {"title": "F. Hyperparameter Optimization with PPSO", "content": "To optimize the hyperparameters of our LSTM model, we employ the PPSO algorithm [46]. PSO is a metaheuristic optimization technique inspired by the social behavior of bird flocks or fish schools.\nIn the context of hyperparameter optimization, each particle in the swarm represents a candidate solution, i.e., a set of hyperparameter values. The particles move through the search space, adjusting their positions based on their own best-known position (personal best) and the best position discovered by the entire swarm (global best).\nThe movement of each particle i at time step t is governed by the following equations:\n$V_i^{t+1} = w_1 V_i^t + C_1 r_1 (P_{ibest} - X_i^t) + C_2 r_2 (P_{gbest} - X_i^t)$ (11)\n$X_i^{t+1} = X_i^t + V_i^{t+1}$ (12)\nwhere $V_{it}$ is the velocity of particle i at time t, $X_i^t$ is the position of particle i at time t, $w_1$ is the inertia weight, $c_1$ and $c_2$ are acceleration coefficients, $r_1$ and $r_2$ are random numbers between 0 and 1, $P_{ibest}$ is the personal best position of particle i, and $P_{gbest}$ is the global best position of the swarm.\nThe objective of the optimization is to find the set of hyperparameters that minimizes the forecasting error of the LSTM model. We use the root mean square error (RMSE) as the fitness function to evaluate the quality of each candidate solution. The PSO algorithm iteratively updates the particles' positions until a predefined stopping criterion is met, such as a maximum number of iterations or a convergence threshold.\nBy leveraging the global search capabilities of PSO, we can efficiently explore the hyperparameter space and identify the optimal configuration for our LSTM model, leading to improved forecasting accuracy."}, {"title": "G. Evaluation Metrics", "content": "To assess the performance of our proposed PSO-LSTM model, we compare it against benchmark models such as GARCH and SVM. We employ two widely used evaluation metrics to quantify forecasting accuracy: MAE and RMSE [47].\nThe MAE measures the average absolute difference between the predicted and actual values:\n$MAE = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y_i}|$ (13)\nThe RMSE is the square root of the mean squared error and provides a measure of the average deviation of the predictions from the actual values:\n$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 }$ (14)\nwhere $y_i$ is the actual value, $\\hat{y_i}$ is the predicted value, and n is the number of samples.\nBy evaluating our model using these metrics, we can com-prehensively assess its forecasting performance and compare it against benchmark models. This evaluation process allows us to validate the effectiveness of our proposed approach and demonstrate its potential for accurate EUR/USD exchange rate prediction."}, {"title": "IV. PROCEDURE OF EXPERIMENT", "content": "We employ web scraping techniques to gather comprehensive datasets from investing.com and forexempire.com, covering the period from February 11, 2016, to January 19, 2024. The collected data undergoes preprocessing steps, including filtering, normalization, and summarization, to ensure its readiness for advanced text mining tasks. To handle the inherent noise in the original news data, the GPT-4.0 TURBO API [48] is employed for data cleansing, leveraging its capabilities to retain essential information while removing irrelevant content. This process facilitates accurate subsequent analyses, such as Latent Dirichlet Allocation (LDA) [49], Dynamic Topic Modeling (DTM) [50], and sentiment analysis, by focusing the data on relevant content and enhancing the precision of thematic and sentiment extraction.\nAll financial indicators used in this study can be found in the Table I. These indicators are collected from the same online platform within the same period."}, {"title": "B. Sentiment and Content Categorization", "content": "Our research distinguishes the collected textual data into two primary categories: news and sentiment analysis. News texts encompass a broad range of topics, such as political events, economic data releases, and central bank decisions, all of which can influence currency trends. Sentiment-focused texts, however, are primarily analytical, often featuring technical analysis, market predictions, and investment strategies, thus exhibiting a more objective and rational tone. This catego-rization enables tailored approaches in data processing and prediction tasks based on the distinct characteristics of each text type."}, {"title": "C. Text Preprocessing and Annotation", "content": "Further preprocessing of the raw textual data involves tech-niques such as tokenization, where elements like stop words, punctuation, and non-alphanumeric characters are eliminated. Commonly used stop words, like 'the', 'is', and 'at', are removed as they do not contribute meaningful information. This cleansing step primes the data for thematic language analysis, ensuring that only significant information remains. In the realm of large language models, effective data processing requires specific prompt words to guide tasks such as text cleansing and annotation. Utilizing the API, we introduce prompt engineering to refine and optimize text processing for sentiment and content annotation. This approach removes irrelevant material, particularly unrelated currency data, to focus exclusively on EUR/USD content. A detailed description of prompt words and techniques used is available in Appendix A and B."}, {"title": "D. Sentiment Scoring and Fine-tuning RoBERTa-Large", "content": "Following the cleansing stage, sentiment intensity within the textual data is assessed using the gpt-4-1106-preview API. Each text is assigned an emotional intensity score ranging from [-1.0, 1.0], where scores at or below -1.0 indicate negative sentiment, and scores at or above 1.0 indicate positive sentiment, with values around zero representing neutrality. This scoring provides a quantitative basis for analyzing the emotional tone's impact on exchange rate movements.\nTo adapt ROBERTa-Large for our currency-focused senti-ment analysis and pattern detection, we fine-tune the open-source model from Hugging Face on our annotated dataset. This fine-tuning involves multiple training iterations, during which the model's internal parameters are adjusted to better capture the linguistic nuances of exchange rate discourse and discern unique patterns associated with financial language. This customization of RoBERTa-Large allows it to accurately analyze sentiment and identify underlying trends within the specific context of EUR/USD exchange rate texts, enhancing the model's predictive capacity compared to traditional CNN-based approaches."}, {"title": "E. Sentiment Index Construction", "content": "Recognizing the impact of news on market sentiment over time, we develop a sentiment index (SI) inspired by the work of Xu and Berkely (2014). This SI model assumes that the influence of news decays exponentially, with the most significant impact occurring within the first seven days post-publication. The sentiment index SI e-m/7 models this decay, where m represents the number of days since the news release. The sentiment intensity on a given day t, denoted as $SI_t$, is calculated by summing the Sentiment Value (SV) of that day with the SVs from preceding days, weighted by an exponential decay factor $e^{-(t-i)/7}$ to account for the influence of past sentiments:\n$SI_t = \\sum_{i=1}^{t-1} e^{-(t-i)/7} \\cdot SV_i + SV_t,$ (15)\nThis approach more accurately reflects the persistence of news impact on the EUR/USD exchange rate, enhancing the predictive power of our sentiment analysis."}, {"title": "F. Topic Scores for Polarity, Subjectivity, and RoBERTa Analysis", "content": "We calculate topic scores for polarity, subjectivity, and ROBERTa analysis as follows:\n$P_{k,t} = \\frac{1}{n_{k,t}}\\sum P_i,$ (16)\nwhere $P_{k,t}$ denotes the polarity-topic score of topic k on day t, and $n_{k,t}$ is the number of polarity instances for that topic on that day.\n$S_{k,t} = \\frac{1}{n_{k,t}}\\sum S_i$ (17)\nwhere $S_{k,t}$ represents the subjectivity-topic score of topic k on day t.\n$C_{k,t} = \\frac{1}{n_{k,t}}\\sum C_i$ (18)\nwhere $C_{k,t}$ denotes the RoBERTa-topic score of topic k on day t."}, {"title": "G. Lag Order Selection and Feature Selection", "content": "The selection of lags in our study employs the Vector Autoregression (VAR) method. This approach is crucial for determining the optimal number of lags to be used within the model, as an appropriate lag length can more accurately predict exchange rate movements. Initially, we assess the fea-sibility of different lag lengths. For each potential lag length, a separate VAR model is estimated, and its effectiveness is evaluated using criteria such as the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These evaluation standards help us identify the lag length that best captures the temporal dynamics in the data while avoiding overfitting. Through this method, we determine the most suitable lag length for our dataset, providing a solid foundation for the subsequent exchange rate prediction model.\nFor feature selection, our study utilizes the Recursive Fea-ture Elimination (RFE) method, based on the Random Forest regression approach. The RFE method starts with the full set of features and iteratively removes the least important feature at each step. In each iteration, the model is retrained, and the significance of each feature is assessed, typically based on the feature's importance scores in decision trees or its contribution to model performance. This process continues until the predetermined number of features is reached or further removal of features leads to a significant decrease in model performance. The RFE method enables us to effectively identify and select those features that have the most substan-tial impact on EUR/USD exchange rate prediction, thereby enhancing the model's accuracy and interpretability."}, {"title": "V. EMPIRICAL RESULT", "content": "The comparison experiment outlined in the Table VI presents a comprehensive evaluation of various forecasting models, categorized into three primary methodologies: Deep Learning with optimization, standalone Deep Learning, and traditional Machine Learning with and without optimization, as well as classical Statistical Methods.\nIn the realm of Deep Learning enhanced by optimization techniques, the PSO-LSTM model demonstrates superior per-formance, achieving the lowest MAE and Root Mean Square Error (RMSE) across all models tested, justifying its first-place ranking and a weighted rank of 1. The PSO-GRU model, however, lagged significantly behind, ranking ninth in MAE and eleventh in RMSE, culminating in a weighted rank of 10, which indicates a need for further parameter tuning or a reevaluation of its optimization process.\nWhen examining standalone Deep Learning models, the LSTM and GRU variants performed commendably, securing third and fourth places in RMSE, respectively. Their perfor-mance highlights the efficacy of deep learning architectures in capturing the sequential dependencies present in financial time series data without the need for external optimization.\nWithin the Machine Learning domain, the addition of op-timization, as seen in the PSO-SVM model, resulted in a notable improvement in MAE, earning it a third-place ranking. However, the PSO-SVR model's performance was less opti-mal, underscoring the varying effectiveness of optimization depending on the underlying model structure.\nThe performance of traditional Machine Learning models, SVM and SVR, illustrates their robustness with the SVM model outperforming its optimized counterpart in RMSE and securing a sixth-place rank, emphasizing the importance of model selection before optimization.\nThe Statistical Methods showcased a mix of results with the VAR model performing the weakest, likely due to its inability to capture the non-linear patterns present in the financial data, as reflected in its twelfth-place ranking for both MAE and RMSE. In contrast, the ECM model's second-place rank for MAE showcases the potential of multi-series methods in capturing complex market relationships.\nIn the single-series statistical approach, the ARIMA model presented a middling performance, whereas the GRACH model, despite its fifth-place rank for MAE, suggests its relevance in capturing volatility, a critical aspect of exchange rate movements.\nThe weighted rank, a composite measure based on MAE and RMSE rankings, provides an overarching assessment of model performance, guiding stakeholders in model selection based on a balance of accuracy and predictive reliability.\nThis comparative experiment elucidates the trade-offs be-tween model complexity, optimization, and forecasting effi-cacy, offering valuable insights for future model development and selection in the field of financial forecasting."}, {"title": "B. Experiment with and without Textual Data (Ablation Study)", "content": "Using the MAE and Root Mean Square Error (RMSE) as metrics, the study assesses the performance of models like PSO-LSTM, LSTM, VAR, and linear regression with three types of data inputs: text features, financial futures, and a combination of both.\nFrom the textual analysis, we observe the following for-mulas used for calculating the percentage improvement when incorporating both text and financial futures data, compared to using only financial futures:\n$Improvement\\ Rate\\ (MAE) = \\frac{MAE_{Financial} - MAE_{Combined}}{MAE_{Financial}}$ (22)\nand\n$Improvement\\ Rate\\ (RMSE) = \\frac{RMSE_{Financial} - RMSE_{Combined}}{RMSE_{Financial}}$ (23)\nThese formulas quantify the performance change when adding text features. Positive improvement rates for PSO-LSTM and LSTM indicate enhanced model performance with text features, with PSO-LSTM achieving around a 17.3% increase in MAE and a 46.5% increase in RMSE. In contrast,"}, {"title": "C. Ablation Experiment Based on PSO-LSTM", "content": "In this part, we have three kinds of textual data. Kind 1: news sentiment score and analysis sentiment score; Kind 2: classification-news and classification analysis; Kind 3: LDA1 and LDA2."}, {"title": "D. DM Test with Different Optimization Algorithms", "content": "The optimization algorithms featured in this study include Cuckoo Search (CS) [100], Whale Optimization Algorithm (WOA) [101], Genetic Algorithm (GA) [102], and Bat Al-gorithm (BAT) [103], each applied to LSTM models, hence the notations CS-LSTM, WOA-LSTM, GA-LSTM, and BAT-LSTM. These algorithms are chosen for their ability to nav-igate complex solution spaces efficiently and are evaluated against models optimized with PPSO [104] applied to LSTM and GRU networks, as well as Support Vector Machines (SVMs) and Support Vector Regression (SVRs) [105].\nThe DM test results are intriguing, revealing the PSO-LSTM model as the most accurate, earning the top rank for its forecasting prowess. This highlights the potency of PSO as an optimization technique when combined with the LSTM architecture."}, {"title": "VI. CONCLUSION", "content": "In conclusion, this research advances EUR/USD exchange rate forecasting by combining deep learning, textual analysis, and PSO. Our approach integrates interdisciplinary techniques, drawing from computer science, finance, and economics to address financial forecasting challenges.\nEmpirical results highlight the significant advantage of incorporating textual data. By leveraging online news and analysis, the PSO-LSTM model achieved notable accuracy improvements, showcasing the potential of qualitative data to enhance predictive analytics.\nComparative analysis reveals the superior performance of the PSO-LSTM network over classical models, including SVM, SVR, and statistical methods like ARIMA and GARCH. This is supported by the model's top performance across metrics such as MAE and RMSE, further validated by the Diebold-Mariano test.\nThrough ablation studies, we examined the impact of various textual data categories, including sentiment scores, classification outputs, and LDA topics. Results indicate that comprehensive integration of these data types leads to the most effective forecasting performance."}, {"title": "VII. FUTURE RESEARCH", "content": "This study's reliance on data from limited online sources highlights an area for future improvement. Expanding to include a broader array of platforms, social media, and poten-tially non-English content would provide a more comprehen-sive understanding of market sentiment and reveal additional predictors of exchange rate fluctuations. Exploring other opti-mization algorithms, such as GA or ACO, could further refine the PSO-LSTM model's performance. Additionally, ensemble methods combining forecasts from multiple models may offer more robust and resilient predictions.\nEnhanced sentiment analysis is another promising direction. Future research could examine a broader range of emotions beyond polarity, such as trust and anticipation, which may hold predictive value for market trends. Adapting newer ma-chine learning models, like Transformer-based architectures, for time-series forecasting is another rich area for exploration. These models have shown great success in natural language processing and may offer improvements in financial forecast-ing. Finally, real-time forecasting systems capable of process-ing live data streams could provide valuable insights for high-frequency trading. Addressing challenges of data velocity and accuracy in real-time systems remains a promising avenue for future research.\nOverall, these directions hold potential to further revolu-tionize financial forecasting, contributing to both academic literature and practical applications in market analysis."}, {"title": "APPENDIX", "content": "The Figure 8 shows the correlation analysis of each index within the indicator system. This is essential for establishing the predictive validity and reliability of the model, which allows for the identification of statistically significant predic-tors, the reduction of multicollinearity, and the enhancement of model robustness, thus ensuring that the forecast of the EUR/USD exchange rate is grounded in empirically verifiable relationships."}]}