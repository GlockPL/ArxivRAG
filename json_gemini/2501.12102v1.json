{"title": "Proxies for Distortion and Consistency with Applications for Real-World Image Restoration", "authors": ["Sean Man", "Guy Ohayon", "Ron Raphaeli", "Michael Elad"], "abstract": "Real-world image restoration deals with the recovery of images suffering from an unknown degradation. This task is typically addressed while being given only degraded images, without their corresponding ground-truth versions. In this hard setting, designing and evaluating restoration algorithms becomes highly challenging. This paper offers a suite of tools that can serve both the design and assessment of real-world image restoration algorithms. Our work starts by proposing a trained model that predicts the chain of degradations a given real-world measured input has gone through. We show how this estimator can be used to approximate the consistency \u2013 the match between the measurements and any proposed recovered image. We also use this estimator as a guiding force for the design of a simple and highly-effective plug-and-play real-world image restoration algorithm, leveraging a pre-trained diffusion-based image prior. Furthermore, this work proposes no-reference proxy measures of MSE and LPIPS, which, without access to the ground-truth images, allow ranking of real-world image restoration algorithms according to their (approximate) MSE and LPIPS. The proposed suite provides a versatile, first of its kind framework for evaluating and comparing blind image restoration algorithms in real-world scenarios.", "sections": [{"title": "1. Introduction", "content": "Image restoration \u2013 the task of reconstructing high-quality images from their degraded measurements (e.g., noisy, blurry) \u2013 is one of the most extensively studied problems in imaging sciences. The typical foundation for designing and testing image restoration algorithms is the assumption that the degradation process (forward model) is known and follows a specific mathematical form. In image denoising, for example, the noise contamination is commonly assumed to be additive, white, and Gaussian with a known variance [7, 10, 12, 13, 15, 17, 27, 30, 44, 70, 71]. This enables synthesizing corresponding pairs of clean images and degraded measurements, which can then be used for supervised learning (e.g., minimizing the MSE [64]) and/or evaluation (e.g., computing distortion measures such as"}, {"title": "2. Preliminaries", "content": "As in [3], we consider a natural image x to be a realization of a random vector X with probability density function $p_X$. A degraded measurement y is also a realization of a random vector Y, which is related to X via the conditional probability density function $p_{Y|X}$. Generally speaking, an image restoration algorithm is some estimator $\\hat{X}$ that generates reconstructions according to $p_{\\hat{X}|Y}$, where $X \\rightarrow Y \\rightarrow \\hat{X}$ ($\\hat{X}$ and X are statistically independent given Y).\n\nThe degradation process is typically unknown in real-world scenarios. Namely, a given real-world measurement can result from many different variations over the possible degradation processes. We denote by A the random vector that represents all the possible degradation processes that y could have gone through, and by $p_A$ its density. Thus, we can generally say that the degraded measurement y is sampled from the conditional density $P_{Y|X,A}(\u00b7|x,a)$, where x and a are realizations of the random vectors X and A, respectively, which are sampled jointly and independently. In blind face image restoration (BFR), the degradation process is commonly modeled [16, 31, 61, 69, 75] as\n\n$Y = \\text{JPEG}_Q((K *x)\\downarrow_s + N),$\\n\nwhere $A = (K,S,N,Q)$ is a random vector: K is a Gaussian blur kernel of width $\\sigma_K$, $\\downarrow_s$ denotes the bilinear down-sampling operator with scale-factor S, $N \\sim \\mathcal{N}(0,\\sigma_N^2I)$ is a white Gaussian noise with standard deviation $\\sigma_N$, and $\\text{JPEG}_Q$ is JPEG compression-decompression algorithm with quality-factor Q. Throughout the paper, we consider the BFR task for demonstrating our proposed tools, where $\\sigma_K, S, \\sigma_N$ and Q in Eq. (1) are sampled independently and uniformly from [0.1, 15], [1, 32], [0, 20/255], [30, 100], respectively (as in DifFace [69]), unless mentioned otherwise."}, {"title": "3. Empirical likelihood approximation (ELA)", "content": "The conditional probability density function $p_{Y|X}$ leads to the definition of the log-likelihood function $l(y,x) = \\log p_{Y|X} (y|x)$, where a larger value of l(y, x) implies that the image x is more consistent with the measurement y. Such a likelihood function has several practical benefits in image restoration. For instance, it facilitates plug-and-play image restoration, where the outputs generated by a pre-trained diffusion model are enforced to be consistent with the given measurement [8, 27, 28, 40, 48, 59, 63, 76]. However, $p_{Y|X}$ is typically unknown in real-world scenarios, so l(y, x) cannot be used directly. To overcome such a limitation, we introduce Empirical Likelihood Approximation (ELA), an approach to approximate l(y, x) based on a novel degradation estimator. Utilizing the proposed likelihood function, we present a blind consistency measure for real-world problems, followed by a plug-and-play restoration method for blind face restoration that harnesses a pre-trained diffusion prior."}, {"title": "3.1. Approximate log-likelihood", "content": "Many of the algorithms that operate in non-blind settings (where $p_{Y|X}$ is known) focus on deterministic degradation operators with additive Gaussian noise. This leads to the familiar log-likelihood expression\n\n$l(y, x) \\propto - ||y \u2013 h(x)||^2,$\\n\nwhere h is some deterministic degradation operator (e.g., bi-cubic down-sampling). Suppose that Y = y is coupled with an appropriate A = a, namely y is the degraded measurement that resulted from the degradation corresponding to a. We approximate the log-likelihood function corresponding to such a degradation by\n\n$l(y, x) \\approx -||y \u2013 \\mu_Y (x, a)||^2,$\\n\nwhere $\\mu_Y (x, a) := E[Y|X = x, A = a]$, and the expectation is taken w.r.t. the remaining stochastic portions of the degradation A = a (such as noise). To design an appropriate log-likelihood function for an unknown degradation, we train a model to predict A from Y, and then use the result in Eq. (3). Namely,\n\n$l(y, x) \\approx -||y \u2013 \\mu_Y(x, a_{\\theta}(y))||^2$\\n\nis our final approximation of the log-likelihood, where $a_{\\theta}$ is a trained model which estimates A from Y."}, {"title": "3.2. Degradation estimator", "content": "Given a measurement y, the task of the estimator $a_{\\theta}(y)$ is to predict the degradation that produced y, i.e. $a_{\\theta}(y) \\approx a$. For the task of blind face restoration, we train a regression model to estimate the degradation parameters described in Eq. (1). To train our model, we incorporate the standard squared error regression loss $L_{\\text{Main}} = ||a \u2013 a_{\\theta}(y)||^2$. Moreover, to ensure that the predicted solution aligns with $\\mu_Y(x,a)$, we also add the regularization term $L_{\\text{Reg.}} = ||\\mu_Y (x, a)-\\mu_Y (x, a_{\\theta}(y))||^2$. Our final training loss is given by 0.25$L_{\\text{Main}}$ + $L_{\\text{Reg..}}$. Figure 2 demonstrates the high accuracy of the trained estimator. We defer the architecture and optimization details to Appendix E.\n\nWhile in this paper we focus on the degradation estimator's benefits for likelihood approximation, one may leverage this estimator for additional applications. As an example, we estimate the degradations' parameters in the real-world datasets LFW-Test [22], WebPhoto-Test [61], and WIDER-Test [68, 75] and approximate their distribution using Kernel Density Estimation (KDE). Then, we synthesize degraded measurements from CelebA-Test [25, 33, 75], a dataset of clean images, by sampling A according to the predicted distribution corresponding to each real-world dataset. The resulting synthetic datasets mimic the real-world ones, and therefore enable a more appropriate evaluation of real-world image restoration algorithms. This stands in contrast with prior works that sample the parameters of A uniformly [16, 31, 61, 66, 69, 75]. Hence, we utilize our synthetic datasets (as well as uniform sampling) to evaluate the"}, {"title": "3.3. Blind measure of consistency", "content": "An immediate application of Eq. (3) is a blind consistency measure. In the case where the true degradation process a is known, we define\n\n$\\text{CMSE}(X) := E_{(x,y)\\sim p_{X,Y}} [||Y - \\mu_Y (x, a) ||^2].$\\n\nThis is a direct generalization of the commonly used consistency measure, as practiced in [1, 27, 36-39, 44, 45]. In real-world scenarios where a is unknown, we use the approximation of l(y, x) in Eq. (4) and define\n\n$\\text{ProxCMSE}(X) := E_{(x,y)\\sim p_{X,Y}} [||Y - \\mu_Y (\\hat{x}, a_{\\theta}(y))||^2].$\\n\nNamely, if $a_{\\theta}(y)$ is trained appropriately, ProxCMSE(X) would be an approximation of CMSE(X). In Figure 4 we"}, {"title": "3.4. Plug-and-play real-world image restoration", "content": "Another application of our degradation estimator and the derived likelihood approximation is the ability to construct plug-and-play blind restoration algorithms that harness pre-trained generative image priors, such as diffusion models. To achieve valid and perceptually pleasing restorations in blind face restoration, prior plug-and-play diffusion methods use different types of heuristics. PGDiff [67] uses the guidance term $\\nabla ||\\hat{x} - f(y) ||$ at each step, where f(y) is an MMSE predictor of X from Y = y. DifFace [69] starts from an intermediate diffusion timestep, initialized by f(y) with no guidance term. As seen in Figure 5, these heuristics tend to produce inconsistent outputs.\n\nHere, we present a new approach: Empirical Likeli-hood Approximation with Diffusion prior (ELAD) (Algorithm 1). Our algorithm extends DifFace by guiding the diffusion process to take steps in the direction of the approximated log-likelihood score. Thus, our approach is similar to DPS [8], where we use our approximation of the likelihood (Eq. (4)) instead of the true one, $\\log p_{Y|X} (y|x)$ (which"}, {"title": "4. Proxy distortion measures", "content": "This section introduces the Proxy Mean Squared Error (ProxMSE), a distortion measure that allows to practically rank image restoration algorithms according to their MSE without access to the ground-truth images. Using the same derivations, we present ProxLPIPS as a no-reference perceptual distortion measure. We start by defining distortion measures in general and MSE and LPIPS in particular. We continue by defining ProxMSE and ProxLPIPS, showing why they produce rankings that are faithful to the true MSE"}, {"title": "4.1. Background", "content": "Non-blind image restoration algorithms are typically evaluated by full-reference distortion measures, which quantify the discrepancy between the reconstructed images and the ground-truth ones. Formally, the average distortion of an estimator X is defined by\n\n$E_{(x,\\hat{x})\\sim p_{X,\\hat{X}}} [\\Delta(x, \\hat{x})],$\\n\nwhere $\\Delta(x, \\hat{x})$ is some distortion measure (e.g., the squared error). Thus, measuring the average distortion in practice requires access to pairs of samples from $p_{X,\\hat{X}}$. The most common way to obtain such pairs is to degrade the given samples from $p_{X}$ according to $p_{Y|X}$ (e.g., add noise), and then reconstruct the results using $\\hat{X}$. Yet, in real-world scenarios, there is no access to $p_{Y|X}$, so evaluating the distortion in such cases is impossible.\n\nTwo prominent examples of distortion measures are (1) the squared error $\\Delta_{SE}(x,\\hat{x}) = ||x - \\hat{x}||^2$; and (2) LPIPS [73], a full-reference (perceptual) distortion measure that compares weighted features extracted from neural networks such as VGG [53]. In Appendix D we show that LPIPS can be interpreted as a squared error in latent space. Denoting by z the feature vector corresponding to x, we have $\\Delta_{LPIPS} (x, \\hat{x}) = \\Delta_{SE}(z, \\hat{z}).$"}, {"title": "4.2. Derivation", "content": "Let $\\hat{X}$ be some estimator, and let $X^* = E[X|Y]$ be the posterior mean (the MMSE estimator). We define\n\n$\\text{ProxMSE}(X) := E_{(x,x^*)\\sim p_{X,X^*}} [||\\hat{x} - x^*||^2],$\\n\nwhere the right hand side is the MSE between $\\hat{X}$ and $X^*$. Interestingly, ProxMSE satisfies the following appealing property (the proof is deferred to Appendix B):\n\nProposition 1. The ProxMSE and the MSE of an estimator $\\hat{X}$ are equal up to a constant which does not depend on $\\hat{X}$,\n\n$\\text{ProxMSE}(X) = \\text{MSE}(\\hat{X}, X^*) \u2013 d^*$.\\n\nNamely, the ranking order of estimators according to their ProxMSE is equivalent to that according to their MSE.\n\nNote that Eq. (9) was originally proved by Freirich et al. [14] (Lemma 2, Appendix B.1). Ohayon et al. [46] used ProxMSE(X) as a no-reference distortion measure, but did not assess its practical validity in experiments.\n\nIn practice, we do not have access to the true MMSE estimator, but rather to an approximation of it (typically a neural network trained to minimize the MSE). In Appendix C we develop a bound on this approximation. Lastly, since LPIPS is just a MSE in latent space, we similarly define\n\n$\\text{ProxLPIPS}(X) := E_{(z,z^*)\\sim p_{Z,Z^*}} [||\\hat{z} - z^* ||^2],$\\n\nwhere $Z^*$ is the MMSE estimator in the latent space, which again is approximated using a neural network trained to minimize the LPIPS loss."}, {"title": "5. Related work", "content": "Degradation estimation and its application. The idea of identifying the degradations that a given image has gone through was previously explored in the literature in different contexts. In [5], a classifier was trained to determine which degradation is present in a given image, out of 4 possibilities (rain, haze, noise, low-light). An image captioner was trained in [24] to produce the list of degradations from an image, and a coarse description of their severity (e.g., high amount of noise, low amount of blur). These methods neglect the order and the exact parameters' values of each degradation. Another line of work [42, 49, 57] learns the latent representation of degradations in an unsupervised manner. In all of the above, the identified degradation is used as an additional input condition for the restoration algorithm. This is fundamentally different than our approach, as we do not train an algorithm conditioned on the predicted degradations, but rather use the estimated degradations to approximate the likelihood. In [4], camera sensor noise was modeled and fitted as a heteroscedastic Gaussian distribution to mimic a real-world noisy dataset, and [23, 52] optimized a down-sampling kernel per-image. Both are not applicable to a more complex chain of degradations.\n\nConsistency. Measuring the consistency of the reconstructions with the inputs is common for non-blind image restoration tasks, such as noiseless super-resolution and JPEG decoding [1, 2, 36-39]. The work in [27, 44] considered the consistency in noisy problems to be testing the residual image between the degraded input and the restored output for normality. Ohayon et al. [45] generalized this notion as a requirement for the similarity between the conditional distributions $p_{Y|X}$ and $P_{Y|\\hat{X}}$. However, $p_{Y|X}$ is unknown in blind restoration tasks."}, {"title": "6. Conclusion and limitations", "content": "This work aimed to provide practical tools to help foster progress in a highly challenging task: Real-world image restoration. We proposed ELA, a new approach to approximate the consistency of a reconstructed candidate with a given degraded image. Our method relies on a novel degradation estimator, which we train to predict the chain of degradations that a given degraded image has gone through. Using ELA, we directly define measures of consistency for real-world image restoration algorithms (Prox)CMSE, and develop ELAD: a new plug-and-play real-world image restoration algorithm that beats its predecessors. Moreover, our proposed ProxMSE and ProxLPIPS measures offer a new way to indicate the distortion of real-world image restoration methods, without any access to the ground-truth images. While our work provides new effective tools for real-world image restoration, it also has several limitations. First, the parametrization of the degradation family may limit the effectiveness of the degradation estimator. For example, some images in WebPhoto seem to contain haze (see, e.g., Appendix F.2), which is not accounted in Eq. (1). Second, ELA utilizes a degradation predictor to approximate the mean of $p_{Y|X}$, yet it is possible that many different degradations correspond to a given degraded image. A more faithful approximation could be one that averages over such space of possible degradations. Third, our proxy measures assumes access to accurate approximations of MMSE estimators. While in Appendix C we provide an upper bound for the error of such a measure, the tightness of this bound remains unclear in practice. Apart from addressing the above limitations, follow-up work could explore a multitude of different directions, such as (i) testing our tools on other real-world tasks such as blind super-resolution (BSR),"}, {"title": "A. Supplementary results", "content": "A.1. Proxy performance of real-world end-to-end methods\nFor completeness, we evaluate real-world blind face restoration end-to-end methods in Table 2 using the proxy measures we defined in this paper. To complement our distortion measures, we measure the perceptual quality of the reconstructions with FID [20]. It is important to note that the performance of ELAD, despite being a plug-and-play method, is not far from that of the leading end-to-end methods, as evident from Fig. 6.\nA.2. CelebA-Test synthetic results\nIn Figures 4 and 7 in the main paper, we demonstrate the alignment between each proxy measure and its true counterpart measure on synthetic CelebA-Test datasets, using various plug-and-play and end-to-end methods. We also provide the same data in Table 3. As we noted in Sec. 4.3, a key takeaway from these results is the dominance of ELAD over the other plug-and-play methods in all measures. This is consistent with the results in Table 1.\nA.3. Additional visual results\nIn Figures 8 to 11, we present additional restoration examples produced by various methods, including ELAD, on CelebA-Test, LFW-Test, WebPhoto-Test, and WIDER-test.\nB. Proof of proposition 1\nProposition 1. The ProxMSE and the MSE of an estimator X are equal up to a constant which does not depend on X,\n$\\text{ProxMSE}(X) = \\text{MSE}(X, Y) \u2013 d^*.$\nNamely, the ranking order of estimators according to their ProxMSE is equivalent to that according to their MSE.\nProof. The proof follows directly from Lemma 2 in [14] (Appendix B.1). Namely, for any estimator we can write\n$E[||X \u2013 X ||^2] = E[||(X \u2013 X^*) \u2013 (X \u2013 X^*)||^2]$\n= E[||X \u2013 X^*||^2] + E[||X \u2013 X^*||^2] \u2013 2E[(X \u2013 X^*)^T (X \u2013 X^*)]."}, {"title": "C. MMSE estimator approximation effect", "content": "In practice, we only have an approximation of the true MMSE estimator $X^*$ (e.g., a neural network trained to minimize the MSE loss). Denoting by $X^*$ such an approximation of $X^*$, we are interested in the effect of the approximation error $R = X^* \u2013 X^*$ on the proposed ProxMSE measure. To this end, we present the following bound on the absolute error of ProxMSE.\nProposition 2. The absolute error of ProxMSE when using $X^*$ instead of $X^*$ is bounded by\n$|E[||X \u2013 X^*||^2] \u2013 E [||X \u2013 X^*||^2] | \\le E[||R||^2 + 4||R||_1],$\nwhere $R = X^* \u2013 X^*$ and we assume that image pixels are taking values in [0, 1].\nProof. We start by decomposing $||X \u2013 X^*||^2$ and $||X \u2013 X^*||^2$. Since $X^* = X^* - R$, it holds that\n$||X \u2013 X^* ||^2 = || X ||^2 + ||X^*||^2 \u2013 2(X, X^*)$\n= $|| X||^2 + ||X^* ||^2 + ||R||^2 \u2013 2(X^*, R) \u2013 2(X, X^*)$\n= $|| X||^2 + ||X^*|| + ||R||^2 \u2013 2(X^*, R) \u2013 2(X, X^*) + 2(X, R),$\nwhere we expanded $||X^* ||^2$ in Eq. (23) and $(X, X^*)$ in Eq. (24). Moreover, we have\n$|| X \u2013 X^* ||^2 = || X ||^2 + ||X^*||^2 \u2013 2(X, X^*).$\nTaking the absolute difference between the expected values in Eqs. (22) and (25), we get\n$|E[||X \u2013 X^*||^2] \u2013 E [||X \u2013 X^*||^2] | = |E[||R||^2 + 2(X, R) \u2013 2(X^*, R)]|$\n$\\le E[||R||^2] + 2|E[(X, R)]| +2|E[(X^*, R)]|,$"}, {"title": "D. LPIPS is MSE in latent space", "content": "Denote by $f_l \u2208 R^{H_l\u00d7W_l\u00d7C_l}$ the channel-wise normalized feature of $x$ from the $l$'th layer and by $w_l \u2208 R^{C_l}$ a per-channel weight vector. To compute LPIPS [73], we average spatially and sum channel-wise a weighted $l_2$ distance per element,\n$\\Delta_{LPIPS} (x, \\hat{x}) = \\sum_l \\frac{1}{H_lW_l} \\sum_{h,w} w_l(f^l_{h,w} - \\hat{f}^l_{h,w})||^2.$\nThis is equivalent to an MSE between flattened feature vectors. Denote by $z = [vec(z_1), . . . , vec(z_l)]$, where $z_l =\n\\frac{1}{\\sqrt{H_lW_l}}f_l$, then\n$\\Delta_{LPIPS} (x, \\hat{x}) = ||z - \\hat{z}||^2 = \\Delta_{SE}(z, \\hat{z}).$"}, {"title": "E. Implementation details", "content": "E.1. Degradation estimator\nOur degradation estimator consists of a ConvNext-Large [34] for feature extraction, two convolutional layers to sub-sample the extracted features, followed by an MLP head to transform the feature into four parameter values. The ConvNext is initialized from a robust ImageNet [11] checkpoint [32]. The architecture is illustrated in Fig. 12. We train the estimator on the FFHQ dataset [26] at 512\u00d7512 resolution for 300K steps and batch size 16 using the AdamW [35] optimizer $(\u03b2_1=0.9,\u03b2_2=0.999, \u03f5=10^{\u22128})$ with a constant learning rate of 2.5 \u00d7 $10^{\u22125}$, weight decay of 0.05, linear warmup for 1K steps, and linear cooldown for 20% steps [18] (for a total of 360K steps).\nE.2. ELAD\nWe detail in Algorithm 2 Empirical Likelihood Approximation with Diffusion prior (ELAD). We start with a concise summary of diffusion model notations used in the algorithm, based on DDIM [54], followed by the design choices taken in ELAD.\nNotations. Denote the probability density function of clean images by $q(x_0)$. We construct a forward noising process as a Markov chain,\n$q(x_t|x_{t\u22121}) = N(\\sqrt{1 \u2212 \u03b2_t}x_{t\u22121}, \u03b2_tI),$"}, {"title": "F. Datasets analysis & synthesis", "content": "F.1. Real-world datasets analysis\nPrior work [75] considered the degradations in LFW simpler than those in WebPhoto and WIDER. However, they could not justify those claims quantitatively as the degradations in those datasets are unknown. Using our degradation estimator, we presented in Figure 3 the estimated distribution of degradation parameters in BFR real-world datasets. The estimation confirms that images in the LFW dataset have undergone blur and downsampling operations sampled from a narrower distribution centered on lower values than WebPhoto and WIDER distributions, leading to a simpler restoration task. Yet, we also reveal that LFW images were compressed more aggressively compared to other datasets, an observation that might not have been easily made before."}, {"title": "F.2. Synthetic datasets examples", "content": "Throughout the paper we use the real datasets LFW-Test, WebPhotos-Test, and WIDER-Test to the proposed tools. In Sec. 3.2 we additionally demonstrate how we can leverage the degradation estimator to analyze those datasets.\nTo demonstrate the effectiveness of our degradation estimator, we present in Figure 13 examples of degraded images from each dataset, and we predict the degradation of each. Using each predicted degradation we synthesize a synthetic degraded image from a clean CelebA-Test image.\nAs described in Sec. 3.2, we approximate the distribution of A for each dataset by predicting the degradations of each image using our estimator and fitting a KDE over the predictions. In Appendix F.2 we present examples from the synthetic datasets alongside images from the real datasets. Note that in this figures the image are not paired, as each degraded image is created by sampling randomly from the estimated distribution of A."}]}