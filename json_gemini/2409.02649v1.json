{"title": "OpenFact at CheckThat! 2024: Combining Multiple Attack Methods for Effective Adversarial Text Generation", "authors": ["W\u0142odzimierz Lewoniewski", "Piotr Stolarski", "Milena Str\u00f3\u017cyna", "Elzbieta Lewa\u0144ska", "Aleksandra Wojewoda", "Ewelina Ksi\u0119\u017cniak", "Marcin Sawi\u0144ski"], "abstract": "This paper presents the experiments and results for the CheckThat! Lab at CLEF 2024 Task 6: Robustness of Credibility Assessment with Adversarial Examples (InCrediblAE). The primary objective of this task was to generate adversarial examples in five problem domains in order to evaluate the robustness of widely used text classification methods (fine-tuned BERT, BILSTM, and ROBERTa) when applied to credibility assessment issues. This study explores the application of ensemble learning to enhance adversarial attacks on natural language processing (NLP) models. We systematically tested and refined several adversarial attack methods, including BERT-Attack, Genetic algorithms, TextFooler, and CLARE, on five datasets across various misinformation tasks. By developing modified versions of BERT-Attack and hybrid methods, we achieved significant improvements in attack effectiveness. Our results demonstrate the potential of modification and combining multiple methods to create more sophisticated and effective adversarial attack strategies, contributing to the development of more robust and secure systems.", "sections": [{"title": "1. Introduction", "content": "Robustness of credibility assessment is an interesting area of research that helps to understand the limitations of automatic classification methods used in variety of applications. Therefore we began our study with a comprehensive literature review to understand the current state of adversarial attack methods. We systematically tested various adversarial attack techniques on BERT and BiLSTM classifiers [1], focusing on four misinformation tasks: Fake News Classification (FC), Hate Speech Detection (HN), Propaganda Recognition (PR), and Rumor Detection (RD). Previous work in this area demonstrated that BERT-Attack [2] and Genetic algorithms [3] often yield the best results depending on the task and victim model [1]. Other methods, including DeepWordBug [4] and SCPN [5], also showed high effectiveness in specific scenarios.\nInspired by findings from previous works and the variety of plausible techniques, we decided to test and modify them in order to combine the most effective. We utilized our experiments on five datasets: C19, FC, HN, PR2, and RD. Utilizing the OpenAttack Python library [6], we implemented and tested various adversarial attack methods. Our goal was to refine these methods and develop an optimized ensemble approach to adversarial attacks."}, {"title": "2. Background", "content": "The paper presents the experiments and results for the CheckThat! Lab at CLEF 2024 Task 6: Robustness of Credibility Assessment with Adversarial Examples (InCrediblAE) [18, 19]. The objective of this task was to evaluate the robustness of widely used text classification methods when applied to credibility assessment issues. There were three trained victim classifiers provided by task's organizers: fine-tuned BERT, BILSTM, and ROBERTa. The first two classifiers were made available as soon as the task was announced, so they were the foundation for our approaches and conducted experiments. The latter was made available only in the test phase and used to evaluate our methods. Moreover, five problem domains were defined, each being a binary classification task, and for each problem, a separate dataset was provided:\n\u2022 Style-based news bias assessment (HN dataset),\n\u2022 Propaganda detection (PR2 dataset),\n\u2022 Fact checking (FC dataset),\n\u2022 Rumour detection (RD dataset),\n\u2022 COVID-19 misinformation detection (C19 dataset).\nOur task was to create adversarial examples for each dataset. The adversarial examples should include small modifications in each dataset's record that, on the one hand, would preserve the meaning (semantics) of the original text and, on the other hand, would change the victim classifier's decision.\nTo complete the task, we started with an in-depth analysis of the relevant literature on adversarial attacks on NLP classification and the BODEGA framework, as described in section 3. In parallel, we analyzed the datasets provided by the task's organizers (see below). In the next step, we applied the existing attack methods to the provided datasets and treated the received results as a baseline for our approach. The baseline attack methods encompassed those used by [1], namely BAE, BERT-ATTACK,\nDeepWordBug, Genetic, SemesePSO, PWWS, SCPN, TextFooler, implemented in the OpenAttack library (see section 3). Finally, we developed and tested own approaches to create adversarial examples for each dataset, which are described in detail in the following section. The received adversarial examples for each dataset were evaluated using the BODEGA score, consisting of three components: confusion score, semantic score and character score. Our aim was to maximize BODEGA. In our experiments, we worked solely on the attack datasets that were provided for the task (the train and dev datasets were not used). Table 1 presents the basic statistics on the datasets used.\nThe detailed description of datasets (apart from C19) is provided in [1]. For each datasets, we modified the following elements to get the adversarial examples:\n\u2022 C19 dataset - a pair of texts \u2013 target claim and/or relevant evidence. The output label indicates whether the evidence supports the claim or refutes it."}, {"title": "3. Related Work", "content": "Adversarial attacks are manipulations of input text designed to deceive machine learning models and cause them to make errors. Adversarial attacks often involve making subtle, human-imperceptible changes to the input, which can lead the model to produce incorrect or unexpected outputs. Those changes might include one or many of the following, exemplary scenarios: character-level (adding typos in the text, adding extra characters at the beginning/end of the sentence), word-level (replacing word with its synonym, adding semantically neutral words), sentence-level (paraprasing) [8, 9]. Adversarial attacks are used for ML-models tuning in order to increase their robustness [10, 9, 8, 11]\nThe goal of the presented research was to create adversarial examples targeted at victim models that classify text instances in five different domains (style-based news bias assessment, propaganda detection, fact checking, rumour detection, and COVID-19 misinformation detection) described further in section 2.\nThe methods presented in this paper have been evaluated within a framework provided in InCrediblAE, called BODEGA (Benchmark for aDversarial Example Generation in credibility Assessment) [1] \u2013 a framework for testing adversarial examples generation solutions. In general, there are two goal functions in the adversarial attack: (1) to maximise difference between the classes predicted by the classifier for the original and alternated instances; and (2) to maximise similarity between the original and alternated instances. The BODEGA measure ranges from 0 to 1, where 0 means that the original text has been completely altered, including a change in its meaning, and 1 means that the altered text has retained the original semantics, while having only a minimal edit distance from the original text.\nThe BODEGA is defined as follows:\n$BODEGA\\_score(x_i, x^*) = Con\\_score(x_i, x^*) \\times Sem\\_score(x_i, x^*) \\times Char\\_score(x_i, x^*),(1)$"}, {"title": "4. Methodology", "content": "In general our approach can be characterized as a special case of ensemble learning. We took the inspiration from this machine learning group of techniques. In case of ensemble learning it is assumed that employment of multiple learning algorithms will outperform any single algorithm used alone. Although the idea of an ensemble is relatively simple it is more problematic on an implementational level. Especially it is hard to be deployed with respect to new application. In our case this new application is the adversarial attack domain. There are a number of difficulties to solve:\n\u2022 Firstly, there is a variety of approaches that can be taken when it comes to the realization of the ensemble learning. It has to be decided which specific approach will fit best to the projected task.\n\u2022 Secondly, there is an abundant set of methods already used for the execution for the adversarial attack task. The decision has to be made how to create a subset for methods that should be used as one of the ensemble algorithms.\nIn our approach we have started from a literature review and then systematically tested some of the adversarial attack methods. One of the important works in this area [1] tested different adversarial attacks on the BERT and BiLSTM classifiers (victims) in four misinformation tasks: FC, HN, PR, RD. The result showed, that generally depending on the task and victim model the best results can be obtained using BERT-ATTACK [2] and Genetic algorithm [3]. In some specific cases and measures the highest scores can be obtained using such approaches as TextFooler [17], DeepWordBug [4] and SCPN [5]. Therefore we decided to test proposed adversarial attacks on our five attack datasets: C19, FC, HN, PR, RD. Similarly to the mentioned work, we also used implementations of selected adversarial attack methods in the OpenAttack Python library [20] in our first stages of experiments.\nNext we decided to experiment with different settings and provided own modifications to the BERT-attacker implemented in OpenAttack library for better results. Due to the fact, that balance between semantic preservation and attack success rate can be regulated throughout a threshold of semantic similarity score of substituted word, we modified \"threshold_pred_score\u201d parameter from 0.3 to 0.2. Also we decided to check how twice-increased number of substitutes (from 36 to 72) will affect results. Those modificated version of the BERT-Attack we called BAm. The table 2 presents a comparison of BERT-Attack with default parameters in OpenAttack library ('BA') and modified version ('BAm') on five datasets and three victims in BODEGA, success, semantic, character scores and queries number per example.\nAs we can see, BAm improves the BODEGA score in many cases due to a higher success score. However, the semantic score has been slightly reduced because this model additionally selects substitute words that are less related with the meaning of the original. Also we can see that BAm model performs more queries.\u00b9\nIn order to improve the results, we decided to provide different method of selecting important words for replacement. Comparison from initial version to modified version is shown below:\n\u2022 In initial approach for each word in the sentence (text), the word is replaced with a \"[UNK]\" token (unknown token), creating multiple masked versions of the sentence. So, each masked version has a different word replaced by \"[UNK]\u201d. The victim model is used to compute the probabilities for each of these masked sentences. For each word, an importance score is calculated based on the change in probabilities to an opposite class (label) caused by masking that word. The words are ranked based on their importance scores in descending order. This ranking indicates the significance of each word in contributing to the classification decision of the victim model."}, {"title": "5. Results", "content": "The main purpose of the experiment was to test the BODEGA rate with the datasets and to check how the research methods compare with the baseline of a semantic evaluation of the robustness. We compared the results with the baseline presented in [1]. The results of HN, FC and RD datasets are from the experiments presented in [1]. In order to compare the results, it was necessary to repeat an experiment on PR2 for establishing baseline. The analogous problem occurred with C19 \u2013 the dataset needed to be run on our side, as there was no baseline established yet.\nTable 3 includes results of the attack on BERT and BiLSTM classifiers."}, {"title": "5.1. Qualitative Analysis of Manual Evaluation Results", "content": "The CheckThat! Lab Task 6 is founded on the assumption that adversarial examples can alter classifica-tion results without changing the message's underlying meaning. To support automated verification, a manual annotation procedure was implemented. Adversarial examples were manually classified into three categories:\n\u2022 (a) Preserve the Semantic Meaning,\n\u2022 (b) Change the Semantic Meaning,\n\u2022 (c) No Sense.\nA significant discrepancy was observed between automatically generated semantic scores and manual verification results. Automatic scores ranged from 0.68 to 0.86, with CLARE consistently yielding higher results. In contrast, manual scoring indicated that only 0.11 of adversarial examples preserved the semantic meaning. We believe this discrepancy arises from two main sources.\nFirstly, while BLEURT aligns well with human judgment, its sensitivity may be lower compared to human annotators, despite its ability to handle multi-word modifications and contradictions.\nSecondly, BLEURT processes long text fragments by splitting them into sentences and averaging the semantic similarities between sentence pairs. This approach differs significantly from manual scoring, which evaluates adversarial examples as a whole, resulting in much lower scores.\nUpon receiving guidelines for manual annotation, we conducted a qualitative analysis of a subset of adversarial examples generated by our methods. We found that approximately half of these examples introduced incorrect information not present in the original text (e.g., 'Teck was a professional athlete.' changed to 'Teck was a professional maid.'). About a quarter of the changes were negations of the original text (e.g., 'Grease did not have a soundtrack.' changed to 'Grease did include a soundtrack.')."}, {"title": "6. Conclusions and Future Works", "content": "In this study, we explored the application of ensemble learning to the domain of adversarial attacks on text classifiers. Our approach was motivated by the principle that leveraging multiple adversarial attack methods can yield better performance compared to any single method. Through systematic experimentation and modification of existing attack algorithms, we aimed to improve the effectiveness and efficiency of adversarial attacks against various victim models.\nWe began with a literature review and replicated some of the prominent adversarial attack methods, using implementations from the OpenAttack library. Initial experiments indicated that BERT-Attack and Genetic Algorithm were generally effective, though other methods like TextFooler, DeepWordBug, and SCPN also showed promise in specific scenarios. This led us to test these methods on our five datasets: C19, FC, HN, PR2, and RD, initially targeting two victim models: BERT and BiLSTM.\nOur first set of modifications to BERT-Attack (resulting in the BAm variant) involved adjusting parameters to enhance the balance between semantic preservation and attack success rate. These adjustments improved performance across several metrics, as shown in our comparative analysis.\nFurther, we introduced a novel method for ranking word importance, which led to the BAm2 variant. This method evaluates the impact of potential word substitutions on model predictions, thereby allowing more informed and effective attacks. Additionally, for cases where BAm2 failed, we integrated the Genetic Algorithm to bolster the attack success rate. This combination, BAm2&Genetic, yielded significant improvements, particularly in the BODEGA and success scores.\nWe then explored an alternative approach, GSWSE, which employs word embeddings for synonym replacement. This method, coupled with TextFooler for the challenging cases, offered further enhance-ments in BODEGA score, albeit with a slight reduction in the success rate.\nIn the final stage of our experiments, we used the CLARE method. CLARE's innovative mask-then-infill procedure, leveraging a pre-trained masked language model, demonstrated superior performance across all metrics. It provided a high attack success rate while maintaining textual similarity and fluency, making it the most effective method in our ensemble.\nFinally, the developed solutions were tested on the third victim model: RoBERTa. The evaluation confirmed the effectiveness of our approach in generating adversarial text in five problem domains.\nOur findings underscore the potential of ensemble learning in the adversarial attack domain. By combining multiple attack strategies and continuously refining our methods, we achieved notable improvements in attack effectiveness. Future work could explore additional ensemble configurations and further optimizations, aiming to develop even more robust adversarial attack frameworks.\nIn future works we plan to extend the methods by using various data sources and large language models (LLMs). For example, as a comprehensive text corpus, Wikipedia can be used to train or fine-tune language models for better contextual understanding and generation. Using Wikipedia articles, the quality of adversarial examples can be further enhanced by providing more natural and contextually appropriate substitutions, particularly when we can consider quality differences between Wikipedia articles in various language versions [22]. Especially, we plan to use publicly available services with measures related to information quality and reliability of sources, such as BestRef [23], WikiRank [24] and various revscoring models [25]. Future research can leverage the analysis of Wikipedia references to enhance adversarial attacks by incorporating high-quality scientific sources into training datasets, ensuring more credible and contextually relevant adversarial examples [26]."}]}