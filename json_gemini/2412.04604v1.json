{"title": "ARC Prize 2024: Technical Report", "authors": ["Fran\u00e7ois Chollet", "Mike Knoop", "Gregory Kamradt", "Bryan Landers"], "abstract": "As of December 2024, the ARC-AGI benchmark is five years old and remains unbeaten. We believe\nit is currently the most important unsolved AI benchmark in the world because it seeks to measure\ngeneralization on novel tasks the essence of intelligence as opposed to skill at tasks that can be\nprepared for in advance. This year, we launched ARC Prize, a global competition to inspire new ideas\nand drive open progress towards AGI by reaching a target benchmark score of 85%. As a result, the\nstate-of-the-art score on the ARC-AGI private evaluation set increased from 33% to 55.5%, propelled by\nseveral frontier AGI reasoning techniques including deep learning-guided program synthesis and test-time\ntraining. In this paper, we survey top approaches, review new open-source implementations, discuss the\nlimitations of the ARC-AGI-1 dataset, and share key insights gained from the competition.", "sections": [{"title": "1 Introduction: ARC-AGI", "content": "Fran\u00e7ois Chollet first wrote about the limitations of deep learning in 2017 (5). In 2019, he formalized these\nobservations into a new definition of artificial general intelligence (AGI), characterizing it as a system capable\nof efficiently acquiring new skills and solving novel problems for which it was neither explicitly designed nor\ntrained. (7)\nAlongside this definition, Chollet published the Abstraction and Reasoning Corpus (ARC) benchmark (6)\n(later renamed ARC-AGI to avoid name collisions with other AI benchmarks), as a first concrete attempt to\nmeasure this definition of intelligence. We will refer to this dataset as ARC-AGI-1. It is a set of independent\n\"tasks\" (see figure 1), each consisting of a number of \"demonstration pairs\" (two or more, with a median\ncount of three) and one or more \u201ctest inputs\". A test pair consists of an \"input grid\u201d, a rectangular grid of\nvariable size (up to a maximum size of 30 rows by 30 columns) where each cell can have one of ten distinct\n\"values\", and an output grid which should be fully inferable from the characteristics of the input grid. The\ngoal is to use the demonstration pairs to understand the nature of the task, and use this understanding to\nconstruct the output grid corresponding to each test input. The test taker is allowed two attempts per test\ninput.\nThe defining characteristic of the benchmark is that it should not be possible to prepare for any of the tasks\nin advance. Every task in the dataset follows a different logic. All tasks were created by humans to ensure\na high degree of novelty and diversity.\nARC-AGI tasks do not require specialized world knowledge (e.g., historical facts) nor language to solve.\nThe only assumed prior knowledge is Core Knowledge (7) - concepts such as objectness, basic topology,\nelementary integer arithmetic, etc. Human Core Knowledge has been investigated by Spelke et al. (22).\nThese knowledge priors are acquired by children very early (typically before age four) and are universally\nshared by all humans. The ARC-AGI-1 public training tasks are designed to expose test-takers to all the\nCore Knowledge priors needed to solve ARC-AGI tasks."}, {"title": "1.1 Dataset Composition", "content": "ARC-AGI-1 consists of 1,000 tasks split into four subsets:\n\u2022 Public training tasks (400, easy) - Intended to demonstrate the task format and allow for learning the\nCore Knowledge priors.\n\u2022 Public evaluation tasks (400, hard) - Intended to let researchers locally evaluate their performance.\n\u2022 Semi-private evaluation tasks (100, hard) - Intended to let us evaluate third-party approaches that\nrely on publicly-available commercial APIs. It is \"semi-private\" because while it hasn't been publicly\nreleased, it has been exposed to commercial APIs and thus suffers from a risk of leakage.\n\u2022 Private evaluation tasks (100, hard) - Intended to let us evaluate standalone approaches. It is fully\nprivate and theoretically free of leakage."}, {"title": "1.2 Pre-2024 Progress", "content": "ARC-AGI has been the target of three public competitions before ARC Prize 2024:\n\u2022 2020: First ARC-AGI Kaggle competition ($20,000 USD in prizes) (9)\n\u2022 2022: ARCathon 1 ($100,000 in prizes) (16)\n\u2022 2023: ARCathon 2 ($100,000 in prizes) (17)\nIn the years following the original release of ARC-AGI-1, pure deep learning approaches turned out to perform\npoorly on ARC-AGI, as the classic deep learning paradigm works by relating new situations to situations\nseen at training time, with no adaptation or knowledge recombination at test time, making it impossible\nfor such models to apprehend entirely novel tasks at test time. In the first Kaggle competition (2020), no\ndeep-learning based approach scored above 1%. The original GPT-3 model from OpenAI scored 0% on the\npublic evaluation via direct prompting.\nThis is why, despite being created before large language models (LLMs), ARC-AGI has resisted the rise of\nLLMs in the 2022-2024 period.\nThe first ARC-AGI competition ran on Kaggle in 2020 (9) with a top score of 20%. Four years later, the top\nscore had only increased to 33%. This lack of progress on ARC-AGI can be attributed to lack of progress\ntowards AGI. From 2020 to early 2024, the field of AI research was dominated by the scaling up of deep\nlearning systems, which increased task-specific skills but did not improve the ability to tackle tasks without\navailable training data at training time (i.e., general intelligence). Our view is that progress towards AGI\nhad stalled during this period - AI systems had been getting bigger and memorizing ever more training data,\nbut generality in frontier AI systems had been roughly static."}, {"title": "2 ARC Prize 2024 Results", "content": "The poor performance of frontier AI systems on ARC-AGI at the start of 2024 was clear evidence of the\nconceptual limitations hindering AGI progress. In response, we launched ARC Prize (15) to inspire AI\nresearchers to work on new ideas and share them openly. Most frontier AI research is no longer being\npublished by industry labs, which is why ARC Prize incentivizes and promotes open sharing.\nARC Prize 2024 launched on June 11, 2024, and ended November 10, 2024. The competition ran both on\nkaggle.com and arcprize.org. Prizes included a Grand Prize of $600,000 USD for the first team to reach 85%\non the private evaluation set, $50,000 in progress prizes tied to the Kaggle leaderboard, and $75,000 in prizes\nfor the best paper submissions. The Grand Prize was not claimed.\nThe 2024 winners are are shown in Table 1. All scores are open source and reproducible on arcprize.org.\nThe winners competed on Kaggle, where their solutions attempted to solve the 100 tasks of the private\nevaluation set on a virtual machine that featured a single P100 GPU in under 12 hours with no internet\naccess. Only those who open-sourced their solution could be named as a winner and claim prizes. MindsAI\nachieved the highest score of 55.5% on the private evaluation set during the competition but chose not to\nopen source their solution, and was therefore ineligible for a prize."}, {"title": "2.1 Kaggle Leaderboard", "content": "The poor performance of frontier AI systems on ARC-AGI at the start of 2024 was clear evidence of the\nconceptual limitations hindering AGI progress. In response, we launched ARC Prize (15) to inspire AI\nresearchers to work on new ideas and share them openly. Most frontier AI research is no longer being\npublished by industry labs, which is why ARC Prize incentivizes and promotes open sharing.\nARC Prize 2024 launched on June 11, 2024, and ended November 10, 2024. The competition ran both on\nkaggle.com and arcprize.org. Prizes included a Grand Prize of $600,000 USD for the first team to reach 85%\non the private evaluation set, $50,000 in progress prizes tied to the Kaggle leaderboard, and $75,000 in prizes\nfor the best paper submissions. The Grand Prize was not claimed.\nThe 2024 winners are are shown in Table 1. All scores are open source and reproducible on arcprize.org.\nThe winners competed on Kaggle, where their solutions attempted to solve the 100 tasks of the private\nevaluation set on a virtual machine that featured a single P100 GPU in under 12 hours with no internet\naccess. Only those who open-sourced their solution could be named as a winner and claim prizes. MindsAI\nachieved the highest score of 55.5% on the private evaluation set during the competition but chose not to\nopen source their solution, and was therefore ineligible for a prize."}, {"title": "2.2 Public Leaderboard", "content": "In addition to the Kaggle leaderboard, ARC Prize also featured a secondary leaderboard, ARC-AGI-Pub,\nwhich allowed internet access and relaxed compute constraints, in order to evaluate the performance achiev-\nable with closed-source frontier models. Due to the risk of data leakage, submissions were not verified on\nthe private evaluation set, but rather on the \"semi-private\" evaluation set (100 tasks). We report results\nalongside the public evaluation set (400 tasks) to guard against overfitting. We consider scores overfit if\nsemi-private and public evaluation set scores exceed \u00b110% absolute difference. All scores are open source\nand reproducible on arcprize.org."}, {"title": "2.3 Paper Awards", "content": "ARC Prize 2024 also featured \"Paper Awards\" to reward novel concepts regardless of how their solutions\nscored. Prizes were awarded to the following papers. All papers are shared alongside open source code on\narcprize.org."}, {"title": "3 Top Approaches", "content": "Until 2024, all top ARC-AGI approaches relied on discrete program search, starting with the 2020 winning\nentry by icecuber (9), which exclusively leveraged brute-force program search to achieve 20% on the private\nevaluation set.\nOver the next four years, progress was slow. Despite the advent of LLMs (e.g., GPT-3, 3.5, 4), attempts\nto use these systems to beat ARC-AGI were unsuccessful. Advancement primarily came in the form of\nimproved domain-specific languages (DSLs), notably one created by Michael Hodel (12) which improved the\nperformance of the program search process.\nProgress re-accelerated, however, during ARC Prize 2024, catalyzed by three major categories of approaches:\n\u2022 Deep learning-guided program synthesis: Leveraging deep learning models, particularly special-\nized code LLMs, to generate task-solving programs or guide the program search process beyond blind\nbrute-force methods.\n\u2022 Test-time training (TTT) for transductive models\u00b9: Fine-tuning an LLM at training time on\na given ARC-AGI task specification in order to recombine the prior knowledge of the LLM into a new\nmodel adapted to the task at hand.\n\u2022 Combining program synthesis together with transductive models: Merging together the two\napproaches above into a single super-approach, based on the observation that each approach tends to\nsolve different kinds of tasks."}, {"title": "3.1 Deep Learning-Guided Program Synthesis", "content": "Upon releasing ARC-AGI in 2019, Chollet suggested that it could be understood as a program synthesis\nbenchmark, and that it could be solved by leveraging deep learning models to guide a discrete program search\nprocess thereby solving the bottleneck of program synthesis, combinatorial explosion. Such an approach\nwas described in detail in Chollet's AAAI Fall Symposium talk in November 2020 (8). The 2020 competition\nwas entirely dominated by brute-force program search technique, and the rise of LLMs capable of generating\ncode from 2023 onwards led to more efficient program synthesis solutions that used LLMs to write candidate\nprograms that would then be evaluated by a code interpreter.\nProgram synthesis for ARC-AGI has so far come in the following flavors:\n\u2022 Brute-force search over a Domain Specific Language (DSL): This approach involves exhaus-\ntively searching the space of possible programs within a predefined DSL. While theoretically complete,\nit cannot scale on its own to complex programs as it suffers from combinatorial explosion as the size of\nthe DSL and the size of the desired program grow. This is the first approach to have yielded positive\nresults on ARC-AGI, and as early as 2020 we had a proof of existence of a very simple, relatively\nlow-compute brute-force strategy that could achieve 49% on the private evaluation set by ensembling\nall 2020 competition entries together. The highest score seen on any single Kaggle submission with\nthis approach has been 40% on the private evaluation set (by Agnis Liukis, team name alijs.)"}, {"title": "3.2 Test-Time Training", "content": "The classical deep learning paradigm, exemplified by LLMs from the 2022-2023 period, involves first training\na model on a large dataset and then performing inference with a frozen version of the model. However, solving\nARC-AGI requires going beyond simple fetching and application of memorized patterns - it necessitates the\nability to adapt to the specific task at hand at test time. This has led to the emergence of test-time training\n(TTT), also known as test-time fine-tuning (TTFT), as a dominant approach in LLM-based solutions for\nARC-AGI. Today, all top LLM-based transduction approaches for ARC-AGI leverage TTT, and there does\nnot exist any static inference-style transduction solution that scores above 10%. This stark gap highlights\nthe inability of the classical deep learning paradigm to generalize to novel tasks.\nTTT, in this context, involves fine-tuning a pretrained LLM on the demonstration pairs of each task instance\nseen at test time, effectively creating a different variant of the base model for each task. The model is then\nprompted to directly predict the output grid (transduction)."}, {"title": "3.3 Combining Program Synthesis with Transduction", "content": "There are broadly two ways to approach ARC-AGI:\n\u2022 Program synthesis, or \u201cinduction\u201d: Based on the demonstration pairs of a test task, find a program\nor function that appears to turn the input grids into their corresponding output grids, then apply the\nprogram to the test input grid(s).\n\u2022 Transduction: Based on the demonstration pairs of a test task and an input grid, directly predict\nthe corresponding output, for instance, by prompting an LLM with the task description and the test\ninput.\nAs soon as transduction-based approaches started scoring above zero (in late 2023, pioneered by Jack Cole)\nresearchers noticed that program search and transduction were able to solve significantly distinct sets of\ntasks. This issue was later investigated in detail by Li et al. in \"Combining Induction and Transduction for\nAbstract Reasoning\" (19).\nToday, all top scores (e.g., Aky\u00fcrek and Berman on the public leaderboard, the ARChitects, Barbadillo,\nand MindsAI on the Kaggle leaderboard) use a combination of transduction and induction. The best\ntransduction-only and induction-only single submissions score around 40%, so only an ensemble of both\ncan compete for the state of the art."}, {"title": "4 Future", "content": "We have committed to running ARC Prize annually until the ARC-AGI benchmark is defeated and a public\nreference solution is shared. ARC Prize 2024 was a large-scale experiment that we consider highly successful,\nand we aspire to grow ARC Prize from its experimental origins into a durable north star for AGI. We are\napplying lessons learned during ARC Prize 2024 to inform future versions of both the competition and the\nbenchmark."}, {"title": "4.1 ARC Prize: 2025 and Beyond", "content": "We're excited that ARC Prize catalyzed significant attention towards new ideas for AGI. We had expected\nARC Prize to drive academics, independent researchers, and big labs alike to give renewed attention towards\nARC-AGI. But we were surprised to the degree that well-funded AI research startups would change their\nroadmaps to prioritize beating the benchmark. We are now aware of at least seven distinct efforts to solve\nARC-AGI by organizations that have greater than $1M in funding, including Basis AI (basis.ai), Tufa\nLabs (tufalabs.ai), Agemo (agemo.ai), and Symbolica (symbolica.ai).\nAll of these groups do not share the same incentives (e.g., progress prizes would not be sufficient to incentivize\nsharing for venture-funded startups and large corporate labs), and we plan to redesign the 2025 edition of\nthe competition to account for this. Our goal is to provide the best directional compass towards AGI across\nthe full spectrum of AI research actors, from academic labs to startups to big labs."}, {"title": "4.2 ARC-AGI-2", "content": "The ARC-AGI-1 private evaluation set has been unchanged since 2019, and it is known to suffer from a\nnumber of flaws. First of all, the private evaluation set is limited to only 100 tasks. These 100 tasks have\nbeen used by all four ARC-AGI competitions for reporting intermediate leaderboard scores, and, as a result,\non the order of 10,000 private evaluation set scores have been reported to participants so far. This presents\na significant risk of overfitting, since each score has the potential to extract a tiny but non-zero amount\nof information about the content of the hidden tasks. The benchmark's reliability can be improved by\nincreasing the sample size and using two separate datasets: one for intermediate leaderboard scores (a larger"}, {"title": "5 Conclusion", "content": "ARC Prize 2024 was a highly successful experiment awareness of the benchmark was raised significantly\nand several new approaches have emerged, bringing the state of the art from 33% to 55.5%. However, ARC-\nAGI remains undefeated - still by a considerable margin - especially considering that a score of 49% was\ntechnically achievable with basic brute-force program search as early as 2020. New ideas are still needed to\nbuild AGI. The fact that ARC-AGI survived five months of intense scrutiny with an outstanding $600,000\ngrand prize and hundreds of thousands of dollars in additional prizes is strong evidence that the solution does\nnot yet exist. We're inspired, proud, and hopeful that ARC-AGI has played an important role in shifting\nattention towards new research ideas. Our belief is the team that will eventually build AGI is thinking about\nARC-AGI today, and we're committed to stewarding this attention as a north star towards AGI."}, {"title": "6 Appendix", "content": "ARC Prize has inspired a community of researchers and developers who have contributed valuable tools,\ndatasets, and repositories to support both competition participants and the greater AI community."}, {"title": "6.1 The ARC-AGI Ecosystem", "content": "ARC Prize has inspired a community of researchers and developers who have contributed valuable tools,\ndatasets, and repositories to support both competition participants and the greater AI community.\n\u2022 ARC-DSL - A domain-specific language for working with ARC-AGI tasks: GitHub Repository\n\u2022 Concept ARC - Benchmark in the ARC-AGI domain that systematically assesses abstraction and\ngeneralization abilities on a number of basic spatial and semantic \u201cconcept groups\": GitHub Repository\n\u2022 RE-ARC - A repository to procedurally generate examples for the ARC-AGI training tasks: GitHub\nRepository\n\u2022 BARC - Tools for generating synthetic ARC-AGI problems: GitHub Repository\n\u2022 arcsolver - A Python library for automatically solving ARC challenges using Claude and object-centric\nmodeling: GitHub Repository\n\u2022 ARC Interactive - An interactive web-based tool for engaging with ARC-AGI tasks: Web Tool\n\u2022 Arckit - Python and command-line tools for easily working with the Abstraction & Reasoning Corpus:\nGitHub Repository"}, {"title": "6.2 Acknowledgments", "content": "ARC Prize builds on the legacy of earlier ARC-AGI competitions: the 2020 competition on Kaggle as well as\nthe 2022 and 2023 \"ARCathons\", which were a collaboration between Fran\u00e7ois Chollet and the Davos-based\nnon-profit AI lab, Lab42. We are grateful to the Lab42 team Rolf Pfister, Oliver Schmid, and Hansueli\nJud for their expertise and support in facilitating a smooth transition for the ARC-AGI community.\nTheir contributions were significant in facilitating a bigger, bolder competition and advancing initiatives like\nARC-AGI-2.\nWe would also like to recognize the dedication of past ARCathon participants, who not only championed\nthe benchmark but whose prior work brought new members of the community quickly up to speed. In\nparticular, we thank Michael Hodel, Jack Cole, Mohamed Osman, and Simon Ouellette for their ongoing\nefforts to develop ARC-AGI solutions. Special recognition is due to Simon Strandgaard for his exceptional\nrole as a community ambassador and prolific open-source contributor.\nFinally, we extend our deepest gratitude to all participants in ARC Prize 2024, especially those who shared\ntheir work with the community. Your dedication advances the broader field of AI, bringing us closer to\nrealizing the transformative potential of AGI for humanity."}]}