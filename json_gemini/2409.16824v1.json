{"title": "Uncertainty Representations in State-Space Layers for Deep Reinforcement Learning under Partial Observability", "authors": ["Carlos E. Luis", "Alessandro G. Bottero", "Julia Vinogradska", "Felix Berkenkamp", "Jan Peters"], "abstract": "Optimal decision-making under partial observability requires reasoning about the uncertainty of the environment's hidden state. However, most reinforcement learning architectures handle partial observability with sequence models that have no internal mechanism to incorporate uncertainty in their hidden state representation, such as recurrent neural networks, deterministic state-space models and transformers. Inspired by advances in probabilistic world models for reinforcement learning, we propose a standalone Kalman filter layer that performs closed-form Gaussian inference in linear state-space models and train it end-to-end within a model-free architecture to maximize returns. Similar to efficient linear recurrent layers, the Kalman filter layer processes sequential data using a parallel scan, which scales logarithmically with the sequence length. By design, Kalman filter layers are a drop-in replacement for other recurrent layers in standard model-free architectures, but importantly they include an explicit mechanism for probabilistic filtering of the latent state representation. Experiments in a wide variety of tasks with partial observability show that Kalman filter layers excel in problems where uncertainty reasoning is key for decision-making, outperforming other stateful models.", "sections": [{"title": "1 Introduction", "content": "The classical reinforcement learning (RL) formulation tackles optimal decision-making in a fully observable Markov Decision Process (MDP) (Sutton and Barto, 2018). However, many real-world problems are partially observable, since we only have access to observations that hide information about the state, e.g., due to noisy measurements. Learning in partially observable MDPs (POMDPs) is statistically and computationally intractable in general (Papadimitriou and Tsitsiklis, 1987), but in many practical scenarios it is theoretically viable (Liu et al., 2022) and has lead to successful applications in complex domains like robotics (Zhu et al., 2017), poker (Brown and Sandholm, 2019), real-time strategy games (Vinyals et al., 2019) and recommendation systems (Li et al., 2010)."}, {"title": "2 Related Work", "content": "RL architectures for POMDPs. In order to deal with partial observability, RL agents are typically equipped with some form of memory system, e.g., based on human psycology (Fortunato et al., 2019) or context-dependent retrieval (Oh et al., 2016). The most widespread memory system in the RL literature is through sequence models, also known as history encoders (Ni et al., 2024), whose purpose is to encode the past history into a state representation useful for RL. These sequence models can augment policies (Wierstra et al., 2007), value functions (Schmidhuber, 1990; Bakker, 2001) and/or world models (Schmidhuber, 1991; Becker et al., 2019; Shaj et al., 2021a;b; 2023), which allows handling of partial observability in previous RL algorithms such as DQN (Hausknecht and Stone, 2015), SAC (Ni et al., 2022), PPO (Kostrikov, 2018; Ni"}, {"title": "3 Background", "content": "In this section, we provide the relevant background and introduce core notation used throughout the paper. We use bold upper case letters (A) to denote matrices and calligraphic letters (X) to denote sets. The notation P(X) referes to the space of probability distributions over X."}, {"title": "3.1 Reinforcement Learning in Partially Observable Markov Decision Processes", "content": "We consider an agent that acts in a finite-horizon partially observable Markov decision process (POMDP) M = {S, A, O,T, p, O, r, \u03b3} with state space S, action space A, observation space O, horizon T \u2208 N, transition function p: S \u00d7 A \u2192 P(S) that maps states and actions to a probability distribution over S, an emission function O: S \u2192 P(O) that maps states to a probability distribution over observations, a reward function r: S \u00d7 A \u2192 R, and a discount factor \u03b3\u2208 [0,1).\nAt time step t of an episode in M, the agent observes $o_t \\sim O(\\cdot | s_t)$ and selects an action $a_t \\in A$ based on the observed history $h_{:t} = (o_{:t}, a_{:t-1}) \\in H_t$, then receives a reward $r_t = r(s_t, a_t)$ and the next observation $o_{t+1} \\sim O(\\cdot | s_{t+1})$ with $s_{t+1} \\sim p(\\cdot | s_t, a_t)$.\nWe adopt the general setting by Ni et al. (2023; 2024), where the RL agent is equipped with: (i) a stochastic policy $\u03c0 : H_t \u2192 P(A)$ that maps from observed history to distribution over actions, and (ii) a value function $Q: H_t \u00d7 A \u2192 R$ that maps from history and action to the expected return under the policy, defined as $Q^\u03c0(h_{:t}, a_t) = \u0395_\u03c0[\\sum_{\u03c4=t}^{T-1} \u03b3^{\u03c4-t}r_\u03c4 | h_{:t}, a_t]$. The objective of the agent is to find the optimal policy that maximizes the value starting from some initial state $s_0$, $\u03c0^*= argmax_\u03c0 E[\\sum_{t=0}^{T-1} \u03b3^tr_t | s_0]$."}, {"title": "3.2 History Representations", "content": "A weakness of the general formulation of RL in POMDPs is the dependence of both the policy and the value function on the ever-growing history. Instead, practical algorithms fight this curse of dimensionality by compressing the history into a compact representation. Ni et al. (2024) propose to learn such representations via history encoders, defined by a mapping $\u00a2 : H_t \u2192 Z$ from observed history to some latent representation $z_t := \u03c6(h_{:t}) \u2208 Z$. With slight abuse of notation, we denote $\u03c0(a_t | z_t)$ and $Q^\u03c0 (z_t, a_t)$ as the policy and values under this latent representation, respectively. In this paper, we propose a history encoder implemented via Kalman filtering layers that perform simple probabilistic inference on the latent state."}, {"title": "3.3 Structured State Space Models", "content": "We consider time-varying linear SSMs defined by\n$x(t) = A_tx(t) + B_tu(t), y(t) = Ctz(t) + D_tu(t),$ (1)\nwhere $t > 0 \u2208 R$, $x(t) \u2208 R^N$ is the hidden or latent state, $u(t) \u2208 R^P$ is the input, $y(t) \u2208 R^M$ is the output and $(A_t, B_t, C_t, D_t)$ are matrices of appropriate size. Such a continuous-time system can be discretized (e.g.,"}, {"title": "3.4 Probabilistic Inference on Linear SSMS", "content": "To introduce uncertainty into state-space models, we consider a standard linear-Gaussian SSM\n$x_k = A_kx_{k-1} + B_ku_{k-1} + \u03b5_k, y_k = C_kx_k + v_k,$ (3)\nwhere $\u03b5_k \u223c N(0, \u03a3_\u03b5)$ and $v_k \u223c N(0, \u03a3_v)$ are zero-mean process and observation noise variables with their covariance matrices $\u03a3_\u03b5$ and $\u03a3_v$, respectively. The latent state probabilistic model is then $p(x_k | x_{k-1}, u_{k-1}) = N(A_kx_{k-1} + B_ku_{k-1}, \u03a3_\u03b5)$ and the observation model is $p(y_k | x_k) = N(\u0108_kx_k, \u03a3_v)$. Inference in such a model has a closed-form solution, which is equivalent to the well-studied Kalman filter (Kalman, 1960).\nPredict. The first stage of the Kalman filter propagates forward the posterior belief of the latent state at step k 1, given by $N(x_{k-1}^+, \u03a3_{k-1}^+)$, to obtain a prior belief at step k, $N(x_k, \u03a3_k)$, given by\n$x_k = A_kx_{k-1}^+ + B_ku_{k-1}, \u03a3_k = A_k\u03a3_{k-1}^+A_k^T + \u03a3_\u03b5$ (4)\nUpdate. The second stage updates the prior belief at step k given some observation $w_k$, to obtain the posterior $p(x_k | x_{k-1}, w_k) = N(x_k^+, \u03a3_k^+)$ given by\n$x_k^+ = x_k + K_k(w_k \u2013 C_kx_k), \u03a3_k^+ = (I \u2212 K_kC_k)\u03a3_k,$ (5)\nwhere $K_k = \u03a3_kC_k^T(C_k\u03a3_kC_k^T+\u03a3_v)^{-1}$ is known as the Kalman gain. The predict and update steps are interleaved to process sequences of input and observations ${u_k, w_k}_{k=1}^K$ of length K, starting from some initial belief $N(x_{-1}^+, \u03a3_{-1}^+)$. While (4) and (5) include expensive matrix operations, they simplify to cheap element-wise operations under structured SSMs (e.g., diagonal), as done in prior work (Becker et al., 2019; Becker and Neumann, 2022)."}, {"title": "3.5 Parallel Scans", "content": "Efficient implementation of state-space models and Kalman filters employ parallel scans to achieve logarithmic runtime scaling with the sequence length (Smith et al., 2023; Sarkka and Garcia-Fernandez, 2021). Given a sequence of elements $(a_0, a_1, . . . , a_{K\u22121})$ and an associative\u00b9 binary operator \u2299, the parallel scan algorithm outputs all the prefix-sums $(a_0, a_0 \u2299 a_1, . . . , a_0 \u2299 \u00b7 \u00b7 \u00b7 \u2299 a_{K\u22121})$ in O(log K) runtime, given sufficient parallel processors."}, {"title": "4 Method: Off-Policy Recurrent Actor-Critic with Kalman filter Layers", "content": "In this section, we describe our method that implements Kalman filtering as a recurrent layer within a standard actor-critic architecture."}, {"title": "4.1 General Architecture", "content": "In Figure 1 we present our Recurrent Actor-Critic (RAC) architecture inspired by Ni et al. (2022), where we replace the RNN blocks with general history encoders. We will use this architecture in the following to test the capabilities of different history encoders in various POMDPS."}, {"title": "4.2 Kalman Filter Layers", "content": "Our main hypothesis is that principled probabilistic filtering within history encoders boosts performance in POMDPs, especially those where reasoning about uncertainty is key for decision-making. To test this hypothesis, we introduce KF layers, as shown in Figure 2. The layer receives as input a history embedding sequence $h_{:t}$ which is then projected into the input $u_{:t}$, observation $w_{:t}$ and observation noise $\u03a3_v$ sequences. These three signals serve as input to the standard KF predict-update equations (4) and (5), which output a posterior (filtered) latent state $x_{:t}^+$. Finally, the posterior sequence is projected back to the history embedding space to produce the compressed history representation $z_{:t}$."}, {"title": "4.3 Masked Associative Operators for Variable Sequence Lengths", "content": "In off-policy RAC architectures, the agent is typically trained with batches of (sub-)trajectories of possibly different length, sampled from an experience replay buffer. Thus, history encoders must be able to process batches of variable sequence length during training.\nA common approach is to right-pad the batch of sequences up to a common length and ensure the model's output is independent of the padding values. For transformer models, this can be achieved by using the padding mask as a self-attention mask. For stateful models like RNNs and SSMs, it is imperative to also output the correct final latent state for each sequence in the batch. This typically requires a post-processing step that individually selects for each sequence in the batch the last state before padding. It turns out that for any recurrent model expressed with an associative operator (e.g., SSMs and KFs), we can obtain the correct final state from a batch of padded sequences without additional post-processing by using a parallel scan routine with a Masked Associative Operator (MAO).\nDefinition 1 (Masked Associative Operator). Let \u2299 be an associative operator acting on elements $e \u2208 E$, such that for any a,b,c \u2208 E, it holds that $(a \u2299 b) \u2299 c = a \u2299 (b \u2299 c)$. Then, the MAO associated with \u2299, denoted \u02dc\u2299, acts on elements \u1ebd \u2208 E \u00d7 {0, 1} = (e, m), where $m \u2208 {0, 1}$ is a binary mask. Then, for \u00e3 = (a, ma) and b = (b, m\u044c), we have:\n$\\tilde{a} \\tilde{\\circledcirc} \\tilde{b} = \\begin{cases} (a \\circledcirc b, m_a) & \\text{if } m_b = 1 \\\\ \\tilde{a} & \\text{if } m_b = 0 \\end{cases}$ (6)"}, {"title": "5 Experiments", "content": "In this section, we evaluate the RAC architecture under different history encoders in various POMDPS. Implementation details and hyperparameters are included in Appendices B and C, respectively."}, {"title": "5.1 Baselines", "content": "We consider the following implementation of history encoders within the RAC architecture.\nVSSM. Vanilla, real-valued SSM with diagonal matrices. It is equivalent to a KF layer with infinite observation noise, i.e., the update step has no influence on the output. It can also be seen as a simplification of the S4D model (Gu et al., 2022b), where states are real-valued rather than complex (as in Mega (Ma et al., 2023), such that the recurrence can be interpreted as an exponential moving average) and the recurrence is implemented with a parallel scan rather than a convolution (as in (Smith et al., 2023)).\nvSSM+KF. Probabilistic SSM via the KF layers described in Figure 2, which is equivalent as adding Kalman filtering on top of VSSM.\nvSSM+KF-u. Equivalent to vSSM+KF without the input signal $u_{:t}$. It maintains the uncertainty-based gating from the KF layer, but looses flexibility in the KF predict step to influence the prior belief via the input.\nMamba (Gu and Dao, 2023). Selective state-space model with input-dependent state transition matrices.\nGRU (Cho et al., 2014). Stateful model with a gating mechanism and non-linear state transitions.\nvTransformer (Vaswani et al., 2017). Vanilla encoder-only transformer model with sinusoidal positional encoding and causal self-attention.\nAll SSM-based approaches are implemented using MAOs and parallel scans. Besides these memory-based agents, we include two additional memoryless agents that implement the same RAC architecure but without embedders or history encoders.\nOracle. It has access to the underlying state of the environment, effectively removing the partial observability aspect of the problem. This method should upper-bound the performance of history encoders.\nMemoryless. Unlike Oracle, it does not have access to the underlying state of the environment. This method should lower-bound the performance of history encoders."}, {"title": "5.2 Probabilistic Reasoning - Adaptation and Generalization", "content": "We evaluate probabilistic reasoning capabilities with a carefully designed POMDP that simplifies our running example from Section 1, where an AI chatbot probes a user in order to recommend a restaurant. Given noisy observations (user's answers) sampled from a bandit with distribution $N(\u03bc_\u03b5, \u03c3_\u03b5)$ (user's preference), the task is to infer whether the mean $\u03bc_\u03b5$ lies above or below zero (binary decision between two restaurants A and B). At the start of each episode, $\u03bc_\u03b5$ and $\u03c3_\u03b5$ (the latent parameters) are sampled from some given distribution (i.e., we have a different user every episode). Then, at each step of an episode, the RL agent has three choices: (1) request a new observation from the bandit (ask a new question to the user), which incurs a cost p, (2) decide the arm has mean above zero (recommend restaurant A) or (3) decide the arm has mean below zero (recommend restaurant B), both of which immediately end the episode and provide a positive reward if the decision was correct, or a negative reward if the decision was incorrect (i.e., reward is based on whether the recommendation matches the user's preference). We set a maximum episode length of 1000 steps; if the agent does not issue a decision by then, it receives the negative reward. Example rollouts for this environment are provided in Figure 3. Given the Bayesian state from Figure 3, an optimal agent must strike a balance between requesting new information (which reduces uncertainty about the estimated mean) and minimizing costs. Effective history encoders for this problem should similarly produce a state representation that encodes uncertainty about the latent parameters.\nWe evaluate two core capabilities: adaptation and generalization. Intuitively, an optimal policy for this problem must be adaptive depending on the latent parameters. For example, if $\u00b5_\u03b5$ is close to zero the agent might need many observations to make an informed decision, whereas with a large || the correct decision can be made with few observations. Moreover, we can also evaluate generalization of the learned policy by testing on latent parameters not seen during training. Our hypothesis is that an agent that learns proper probabilistic reasoning (e.g., Bayes' rule) should generalize reasonably well in this task.\nWe conduct experiments for all baselines under increasing cost p. Instead of providing the latent parameters directly to the Oracle baseline, we provide the Bayesian posterior mean and standard deviation around the latent parameter $\u03bc_\u03b5$, as shown in Figure 3. The agents are trained under the latent parameter distribution given by $\u03bc_\u03b5 \u223c Unif(-0.5,0.5)$ and $\u03c3_\u03b5 \u223c Unif(0.0, 2.0)$. We additionally evaluate out-of-distribution generalization by using GOOD ~ Unif(2.0, 3.0), i.e., we test how the agent generalizes to bandits with higher variance."}, {"title": "5.3 Probabilistic Filtering - Continuous Control under Observation Noise", "content": "In this experiment, we evaluate the ability to learn control policies subject to observation noise. Effective history encoders must learn to aggregate observations over multiple time steps to produce a filtered state"}, {"title": "5.4 General Memory Capabilities", "content": "So far the evaluations were conducted in tasks where probabilistic filtering was intuitively expected to excel. In this experiment, we evaluate performance in a wider variety of POMDPs from the POPGym (Morad et al., 2023) benchmark. We select a subset of 12 tasks that probe models for long-term memory, compression, recall, control under noise and reasoning. The aggregated results are shown in Figure 9 and full training curves are also included in Appendix H. Below we discuss the main insights.\nKF layers can be generally helpful in POMDPs. From the performance profile in Figure 9 we observe a statistically significant gap between vSSM and vSSM+KF. Interestingly, the largest improvements in sample-efficiency (RepeatPreviousEasy) and final performance (MineSweeperEasy) correspond to tasks that probe for memory duration and recall, respectively. The parameter count difference between vSSM and VSSM+KF in these problems is less than 6%, so we believe model capacity is unlikely the reason behind the large performance difference. We hypothesize that, while probabilistic filtering is not required to solve these tasks, the KF layer has extra flexibility via the latent observation and noise signals to accelerate the learning process. We also highlight that vSSM+KF and vSSM+KF-u show comparable performance in this benchmark, suggesting the input signal to be less critical in general memory tasks.\nVSSM+KF is less sample-efficient in pure-memory tasks. In particular, we observe that Mamba's input-selectivity is the best-suited mechanism for SSM agents to solve long-term memory problems, matching the performace of GRU and vTransformer. This is an expected result based on the associative recall performance of Mamba reported in its original paper (Gu and Dao, 2023)."}, {"title": "5.5 Ablation", "content": "We conduct an ablation on vSSM+KF where we vary two hyperparameters: the latent state size N and the number of stacked KF layers L. We select four representative tasks from POPGym that test different memory capabilities. The final scores are presented in Figure 10 and the full training curves are included in Appendix J. Performance is most sensitive to these hyperparameters in the RepeatFirstMedium task, where the agent must recall information from the first observation over several steps. The general trend is that using more than one layer improves final performance and increases sample-efficiency (see the training curves in Figure 18). Our results are aligned with the good performance of stacked S5 layers reported by Lu et al. (2023), but differ from the observations in (Ni et al., 2023), where both LSTM and transformer models performed best with a single layer in a similar long-term memory task (T-maze passive). From these observations, we believe an interesting avenue for future work is to study what mechanisms enable effective stacking and combination of multiple recurrent layers."}, {"title": "6 Conclusion", "content": "We investigated the use of Kalman filter (KF) layers as sequence models in a recurrent actor-critic architecture. These layers perform closed-form Gaussian inference in latent space and output a filtered state representation for downstream RL components, such as value functions and policies. Thanks to the associative nature of the Kalman filter equations, the KF layers process sequential data efficiently via parallel scans, whose runtime scales logarithmically with the sequence length. To handle trajectories with variable length in off-policy RL, we introduced Masked Associative Operators (MAOs), a general-purpose method that augments any associative operator to recover the correct hidden state when processing padded input data. The KF layers are used as a drop-in replacement for RNNs and SSMs in recurrent architectures, and thus can be trained similarly in an end-to-end, model-free fashion for return maximization.\nWe evaluated and analysed the strengths and weaknesses of several sequence models in a wide range of POMDPS. KF layers excel in tasks where uncertainty reasoning is key for decision-making, such as the Best Arm Identification task and control under observation noise, significantly improving performance over stateful models like RNNs and deterministic SSMs. In more general tasks, including long-term memory and associative recall, KF layers typically match the performance of transformers and other stateful sequence models, albeit with a lower sample-efficiency."}, {"title": "Limitations and Future Work", "content": "We highlight notable limitations of our methodology and suggest avenues for future work. First, we investigated two design decisions in KF layers related to time-varying process noise and posterior covariance as output features. While they resulted in worse performance (see Appendix E), in principle they generalize KF layers and may bring benefits in other tasks or contexts, so we believe it is worth further investigation. Second, we use models with relatively low parameter count (<1M) which is standard in RL but not on other supervised learning tasks. It may be possible that deeper models with larger parameter counts enable new capabilities, e.g., probabilistic reasoning, without explicit probabilistic filtering mechanisms. Third, vSSM+KF uses KF layers as standalone history encoders, but more complex architectures may be needed to stabilize training at larger parameter counts. Typical strategies found in models like Mamba include residual connections, layer normalization, convolutions and non-linearities. Fourth, our evaluations were limited to POMDPS with relatively low-dimensional observation and action spaces, where small models have enough capacity for learning. Future work could further evaluate performance in more complex POMDPs and compare with our findings."}]}