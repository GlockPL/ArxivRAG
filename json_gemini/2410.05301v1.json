{"title": "Diffusion-based Unsupervised Audio-visual Speech Enhancement", "authors": ["Jean-Eudes Ayilo", "Mostafa Sadeghi", "Romain Serizel", "Xavier Alameda-Pineda"], "abstract": "This paper proposes a new unsupervised audio-visual speech enhancement (AVSE) approach that combines a diffusion-based audio-visual speech generative model with a non-negative matrix factorization (NMF) noise model. First, the diffusion model is pre-trained on clean speech conditioned on corresponding video data to simulate the speech generative distribution. This pre-trained model is then paired with the NMF-based noise model to iteratively estimate clean speech. Specifically, a diffusion-based posterior sampling approach is implemented within the reverse diffusion process, where after each iteration, a speech estimate is obtained and used to update the noise parameters. Experimental results confirm that the proposed AVSE approach not only outperforms its audio-only counterpart but also generalizes better than a recent supervised-generative AVSE method. Additionally, the new inference algorithm offers a better balance between inference speed and performance compared to the previous diffusion-based method.", "sections": [{"title": "I. INTRODUCTION", "content": "Speech enhancement (SE) refers to the problem of extracting a clean speech signal from a noisy recording. Early algorithms implemented for solving this task relied solely on acoustic features. However, speech production is inherently multimodal, involving movements of the lips and tongue, for example. Research in speech perception has demonstrated a crucial impact of visual cues on the ability of humans to focus their auditory attention on a speech signal [1]\u2013[3]. Consequently, audio-visual speech enhancement (AVSE) has emerged as a new research trend. In this approach, lip movements are primarily used as complementary information to acoustic features to improve the performance, particularly in low signal-to-noise ratio (SNR) environments [1], [4].\nExisting deep neural network (DNN)-based AVSE frameworks (and SE in general) are primarily divided into two learning approaches: supervised [5]\u2013[12] and unsupervised [13]\u2013[15]. Supervised methods, whether predictive or generative, involve training a DNN on pairs of clean and noisy speech, and possibly corresponding visual data. Specifically, predictive methods focus on mapping noisy speech directly to clean speech or to a time-frequency mask. In contrast, generative methods aim to generate clean speech at inference by learning the distribution of clean speech conditioned on the noisy input, rather than directly mapping from noisy to clean speech.\nSome recent supervised-generative methodologies for SE [11], [16]\u2013[19] leverage diffusion models [20]. A diffusion model learns data distribution by first pushing the clean data distribution towards a prior Gaussian distribution via the progressive corruption of the clean data with Gaussian noise. A DNN is trained to progressively transform samples drawn from the prior Gaussian distribution into clean data. In the diffusion-based supervised-generative SE context, the diffusion model which aims to learn clean speech distribution is conditioned on noisy speech. Lips video features can also be incorporated in such SE models as shown by [9]. As diffusion model inference requires many iterations, a faster alternative to [9], called FlowAVSE, was proposed in [10]. It is a two-stage method that uses in its first stage a supervised-predictive network that outputs an estimate of clean speech given the noisy speech and the lips video. Then, a conditional generative flow matching algorithm generates the final enhanced speech in just one sampling step, conditionally to the lips video and the output of the first stage. Despite their generative nature, these methods still require pairs of clean and noisy speech for training. Although all these supervised methods can generalize to unseen noise conditions at test time, a significant amount of paired data is needed [21], [22]. This is because it is impossible to train these models across all potential noise types and acoustic scenarios [13].\nTo enhance the robustness of SE models to unseen noises during training, some unsupervised methods have been developed that do not require a noise dataset. Here, the training stage involves learning the prior distribution of clean speech using models like variational auto-encoder (VAE) [14], [23] or dynamical VAE [15]. This learned prior is then combined with a noise model based on non-negative matrix factorization (NMF) [24] to estimate clean speech using expectation-maximization (EM). In this vein, Nortier et al. [25] recently introduced UDiffSE, an unsupervised audio-only SE framework utilizing diffusion models. In this approach, an unconditional diffusion model is trained on clean speech data. This diffusion model, serving as a prior for clean speech, is then combined with an NMF-based noise model to enhance speech. This method has been shown to outperform VAE-based SE approaches on some metrics and provides better generalization than its supervised counterpart, i.e., [11].\nIn this paper, we propose a diffusion-based AVSE framework that learns a clean speech distribution from audio-visual"}, {"title": "II. DIFFUSION-BASED UNSUPERVISED SE", "content": "In this section, we review the diffusion-based unsupervised (audio-only) SE framework [25]. The modeling is done in the complex-valued short-time Fourier transform (STFT) domain. The observation model is x = s + n, where x, s, and n denote STFT arrays of noisy (mixture) speech, clean speech, and background noise, respectively. For notational simplicity, 2D STFT arrays (F frequency bins and T time frames) are represented by flattened 1D arrays, e.g., s \u2208 CFT."}, {"title": "A. Speech generative modeling", "content": "Learning a speech generative prior using diffusion models involves smoothly injecting noise into training samples, via a diffusion process {st}t\u2208[0,1], which transforms clean training data s0 = s into Gaussian noise over time t. This can be described by the following forward stochastic differential equation (SDE) [20], [25]\n$$dst = f(st)dt + g(t)dw,$$ (1)\nwhere w denotes a standard Wiener process, the vector-valued f is the drift coefficient term, the scalar function g is the diffusion coefficient, and dt is an infinitesimal time-step. Here, f(st) = -\u03b3st, where \u03b3 is a constant parameter, and g(t) controls the variance of the stochastic noise. The SDE in (1) has the perturbation kernel defined below, which allows one to directly sample st given s\n$$Pot(St|s) = Nc(\u03b4ts, \u03c3(t)^2I),$$ (2)\nwhere \u03b4t = e-\u03b3t, and the variance \u03c3(t)^2 is determined from the SDE. Under some light regularity conditions [27], the noising process can be reverted through a reverse SDE:\n$$dst = [-f(st) + g(t)^2\u2207st log pt(st)]dt + g(t)dw,$$ (3)\nwhere w is a standard Wiener process running backward in time. The term \u2207st log pt (st), known as the score function, is intractable to compute directly. It is thus approximated by a time-dependent DNN-based score model, denoted as S\u03b8(st,t). To learn \u03b8, the following problem is solved [28]:\n$$\u03b8* = argmin E_{t,s,\u03b5,st} [|g(t)S\u03b8(st,t) + \u03b5|^2],$$ (4)\nwhere \u03b5 ~ Nc(0, I) is a zero-mean complex-valued Gaussian noise. The reverse SDE can then be numerically solved, e.g., using the Predictor-Corrector (PC) sampler [20], to sample from the data distribution."}, {"title": "B. Unsupervised speech enhancement", "content": "The additive noise is modeled as n ~ Nc(0, diag(v)), where v = vec(WH), with W, H being low-rank matrices with non-negative entries, and vec(.) denoting the vectorization operator. Given the pre-trained speech prior with frozen parameters \u03b8*, an iterative EM method is performed to update \u03a6, where the M-step writes:\n$$\\max_\\Phi E_{p(s|x)} {\\log p_\\Phi(x|s)} .$$ (5)\nThe above expectation is approximated using a Monte Carlo estimate, which involves sampling from the intractable posterior p\u03a6(s|x) \u221d p(x|s)p(s) during the E-step. This approximation is implemented by substituting \u2207st log pt(st) in (3) with the following posterior-based score function:\n$$\u2207st log p(st|x) = \u2207st log p(x|st) + \u2207st log pt(St),$$ (6)\nwhere the time-dependent likelihood p\u03a6(x|st) is approximated with a noise-perturbed pseudo-likelihood [25] and the score function is replaced with S\u03b8*(st,t). Plugging the obtained clean speech estimate in (5), W, H are learned with multiplicative update rules. The EM steps are iteratively performed until convergence, typically requiring around five EM iterations for sufficient performance [25]."}, {"title": "III. DIFFUSION-BASED UNSUPERVISED AVSE", "content": "This section presents our proposed unsupervised AVSE framework using diffusion models. We first develop an audio-visual speech prior model. Then, we propose a fast inference algorithm to estimate the clean speech."}, {"title": "A. Audio-visual speech generative model", "content": "We model the conditional speech generative distribution p(s|v), where v denotes a visual embedding associated with s. Following the diffusion-based framework discussed in Section II-A, a conditional score network S\u03b8(st,v,t) is learned over clean AV speech data. The visual data denoted as v \u2208 RT\u00d7p, where T represents the number of video frames and p indicates the embedding dimension, is incorporated into the score network as illustrated in Fig. 1.\nTo integrate audio and visual features within the score network, a cross-attention mechanism is employed at each downsampling and upsampling stage of the U-Net-like architecture. Here, audio features serve as queries, while visual features are used as keys and values. More specifically, we denote the acoustic embedding features at the ith layer of the score network as ea,i \u2208 R^{Ci\u00d7Fi\u00d7Ti}, where Ci represents the number of channels, and Fi and Ti indicate the embedding dimensions. Initially, the audio and visual features are projected into di-dimensional spaces for queries, keys, and values. This is followed by the computation of dot-product attention to produce a feature map of dimensions Ci \u00d7 di \u00d7 Ti. This feature map is then projected into an Fi-dimensional space, resulting in an intermediate audio-visual representation, eav,i \u2208 R^{Ci\u00d7Fi\u00d7Ti}. The final audio-visual representation at layer i, denoted eav,i, is achieved by adding the original"}, {"title": "B. Fast inference algorithm", "content": "The UDiffSE framework requires multiple rounds of an iterative reverse diffusion process as the E-step to obtain an estimation of the clean speech by sampling from the posterior p(s|x). This method is computationally intensive. We introduce a significantly more efficient methodology named UDiffSE+, which requires only one round of reverse diffusion.\nThis new framework leverages a clean speech estimate at each iteration of the reverse diffusion process, thereby eliminating the need for a complete reverse cycle. In this methodology, the noise parameters, i.e., W and H, are updated following each reverse iteration based on the clean speech estimate obtained. This approach effectively employs an alternating maximization strategy, aimed at solving\n$$\\max_{\\Phi,\u03c2} \\log p(x|s) + \\log p(s),$$ (7)\nby performing the following iterations\n$$S_{0,k+1} = argmax_s \\log p_{\\Phi k}(x|s) + \\log p(s),$$ (8a)\n$$\\Phi_{k+1} = argmax_\\Phi \\log p(x|S_{0,k+1}).$$ (8b)\nProblem (8a), which is maximum a posteriori (MAP) estimation, can be solved by performing one reverse iteration, as done in UDiffSE, in which case, the iteration index k is replaced with a time discretization, denoted \u03c4. To obtain an estimate of s at iteration \u03c4 of the reverse diffusion process, denoted \u015d0,\u03c4, we leverage Tweedie's formula [29], that is\n$$\u015d_{0,\u03c4} = E_{p_{\u03c4|0}(s_0|s_\u03c4)}[s_0] \\approx s_\u03c4 + \\frac{\u03c3^2S_\u03b8^*(s_\u03c4,v,\u03c4)}{\u03b4\u03c4}$$, (9)\nPlugging this into (8b), the parameters are updated by a single multiplicative update iteration. The overall (audio-only/AV) UDiffSE+ algorithm is described in Algorithm 1. By excluding lines 12 and 13, the algorithm simplifies to the E-step of UDiffSE. The highlighted box corresponds to the operations that ensure observation (x) consistency, without which the algorithm reduces to prior sampling."}, {"title": "IV. EXPERIMENTS", "content": "Baselines. We compare the performance of our proposed method against two frameworks: the audio-only UDiffSE [25] and the supervised-generative FlowAVSE model [10].\nDataset. The TCD-TIMIT corpus [30] was employed for training. It comprises AV speech data from 56 English-speaking individuals with Irish accents, distributed among 39 for training, 8 for validation, and 9 for testing. The dataset features 98 distinct sentences, each approximately 5 seconds in duration and sampled at 16 kHz, totaling around 8 hours of data. Additionally, each spoken utterance is accompanied by a corresponding video, recording the speaker from a frontal perspective at a frame rate of 30 fps. We downsampled the videos to 25 fps and the lip regions of interest are extracted as 88\u00d788 grayscale following [31], [32]. Training the supervised baseline model requires noisy speech counterparts. As such, we consider mixing TCD-TIMIT clean speech with DKITCHEN, OMEETING, PRESTO, PSTATION, NPARK noises from the DEMAND dataset [33] at {-10, 0, 10}dB.\nTo evaluate SE performance, we define two scenarios: matched and mismatched, based on the origin of the test clean speech signals or noises. In the matched scenario, noisy speech is constructed by mixing clean speech from the TCD-TIMIT test set with noises from the DEMAND dataset (TMETRO, OOFFICE, TBUS, STRAFFIC, SPSQUARE). For each type of noise and SNR level, we randomly selected 10 utterances"}, {"title": "V. CONCLUSION", "content": "In this paper, we present a diffusion-based, unsupervised audio-visual speech enhancement (AVSE) framework, which leverages a diffusion model to simulate clean speech distribution, conditioned on visual cues from lip movements. The pre-trained diffusion model is integrated with an NMF-based noise model through an iterative process of reverse diffusion steps to estimate speech. Our experiments show that the proposed AVSE framework consistently outperforms its audio-only counterpart and offers better generalization than a recent supervised-generative approach [10]. Moreover, compared to the previous inference method [25], our algorithm achieves a better trade-off between performance and runtime. Future work includes a comprehensive subjective performance assessment."}]}