{"title": "BitNet b1.58 Reloaded: State-of-the-art\nPerformance Also on Smaller Networks", "authors": ["Jacob Nielsen", "Peter Schneider-Kamp"], "abstract": "Recently proposed methods for 1-bit and 1.58-bit quantiza-\ntion aware training investigate the performance and behavior of these\nmethods in the context of large language models, finding state-of-the-art\nperformance for models with more than 3B parameters. In this work, we\ninvestigate 1.58-bit quantization for small language and vision models\nranging from 100K to 48M parameters. We introduce a variant of BitNet\nb1.58, which allows to rely on the median rather than the mean in the\nquantization process. Through extensive experiments we investigate the\nperformance of 1.58-bit models obtained through quantization aware\ntraining. We further investigate the robustness of 1.58-bit quantization-\naware training to changes in the learning rate and regularization through\nweight decay, finding different patterns for small language and vision\nmodels than previously reported for large language models. Our results\nshowcase that 1.58-bit quantization-aware training provides state-of-the-\nart performance for small language models when doubling hidden layer\nsizes and reaches or even surpasses state-of-the-art performance for small\nvision models of identical size. Ultimately, we demonstrate that 1.58-bit\nquantization-aware training is a viable and promising approach also for\ntraining smaller deep learning networks, facilitating deployment of such\nmodels in low-resource use-cases and encouraging future research.", "sections": [{"title": "1 Introduction", "content": "The recent years of development of natural language processing (NLP) have\nbeen dominated by the capabilities offered by Large Language Models (LLMs).\nHowever, due to the size of these models, they pose a challenge in deployment and\nraise concerns regarding the environmental impact. Post-training quantisation\nmethods transform the 16-bit weights to a lower bit-representation, which both\nreduces the memory and computational needs. The idea is to take the trained\nweights and find a good way of mapping them to fewer bits, enabling more\nefficient inference."}, {"title": "2 Method", "content": "In this section we present our quantization aware training architecture as a gener-\nalization of the BitNet b1.58 architecture [11]. First, we present our quantization\nmethod. Then, we document our experimental setup."}, {"title": "2.1 b1.58 Quantization", "content": "Our BitLinear layer functions as a drop-in replacement for PyTorch's torch.nn. Linear\nlayer. Figure 1 illustrates BitLinear's 5-step computation flow:\n1. The activations are normalized.\n2. The normalized activations are quantized to k-bit precision.\n3. The 16-bit shadow weights are quantized to 1.58-bit weights."}, {"title": null, "content": "In the following, we details the mathematics behind this computation flow. We\ndenote the Layer normalization [4] of input I, as \u00ce. We then define the quantified\nactivation-bits as $X_{scale}$, constituting the AbsMax:\n$X_{scale} = \\frac{Q_b}{max(|\\hat{I}|) + \\epsilon}$ (1)\nwhere $Q_b = 2^{k-1}$ is the range of the k bits used for the quantized activation. e is\ns small value preventing zero-division. This means all activations can be scaled\nto integer values {-$Q_b$ \u2212 1, ..., $Q_b$}. We define the AbsMax Quantization for the\nactivations as follows:\n$X_{quant} = max(-Q_B, min(Q_B \u2013 1, round(\\hat{I} \\cdot X_{scale}))$ (2)\nFurthermore, we quantize the 16-bit weights W \u2208 $R^{n\u00d7m}$ to a ternary system of\ninteger values {\u22121,0,1} as follows. We define the scaling of W as:\n$W_{scale} = \\frac{1}{Measure(|W|) + \\epsilon}$ (3)\nWhere Measure denotes either the mean or median function, constituting the\nAbsMeasure Quantization.\nWe define the quantized weights $W_{quant}$ (denoted as 1.58-Bit Weights in\nFigure 1) as:\n$W_{quant} = max(-1, min(1, round(W \\cdot w_{scale}))$ (4)"}, {"title": null, "content": "Having quantized both the activations and the weights, we can apply a kernel\nwith $X_{quant}$ and $W_{quant}$ as inputs:\n$Y_{quant} = X_{quant} \\cdot W_{quant} + b$ (5)\nwhere b is optional bias. We detach both $X_{quant}$ and $W_{quant}$ from the computation\ngraph to achieve a straight-through estimation of the gradients. The gradients\nupdate the \"shadow weights\", i.e., the 16-Bit Weights that are quantized by\nAbsMeasure Quantization.\nFinally, we rescale the output y during the Dequantization process:\n$y = \\frac{Y_{quant}}{W_{scale} X_{scale}}$ (6)\nComparing to the original BitNet b1.58, there are a number of differences:\nWe chose to use a standard layer normalization (LayerNorm) rather than RMS\nnormalization, as the computational overhead is minimal and we observed\nslightly better performance with the standard layer norm in preliminary\nexperiments.\nWe allow the use of both the median and the mean for quantizing weights.\nPrior works [13,11] solely employ the mean. We investigate the impact of this\nchoice in Section 3.\nWe actually quantize weights and activations to integer values. This means\nthe matrix multiplications are performed between the 1.58-bit weights with\ninteger values {\u22121,0,1} and the 8-bit quantized activations with integer\nvalues -128,..., 127. This allows to develop multiplication-free kernels, as\nmultiplication with -1 corresponds to the subtraction of an 8-bit integer\nvalue, multiplication with 0 to the disregard of a value, and multiplication\nwith 1 to the addition of an 8-bit integer value.\nThis is in contract to previous work [11], where the quantized weights have\nfloating point values {$\\frac{1}{W_{scale}}$,0, -$\\frac{1}{W_{scale}}$} while quantized activations have float-\ning point values {-$\\frac{128}{X_{scale}}$, ...,$\\frac{127}{X_{scale}}$} according to the published information\nabout the implementation[1]. Consequently, our BitNet 1.58 Bits Reloaded ar-\nchitecture is more directly amenable to custom software kernels and hardware\nimplementations."}, {"title": "2.2 Experimental setup", "content": "We conduct all experiments with standard networks in small configurations\nwith the torch.nn.Linear layers replaced by our BitLinear layers. The Adam[6]\noptimizer and a batch-size of 128 are employed. The number of model parameters\nis slightly higher in the BitLinear setting, as we both have 1.58-bit weights as\nwell as the 16-bit shadow weights. However, this fact does not change the number\nof trainable/optimized parameters in practice.\nFor SLMs, we train small Mistral-like models with 4 layers and hidden sizes\nof 32, 64, 128, and 256. The number of attention head and key-value cache heads\nis set to the ceiling of the hidden size divided by 64, i.e., 1 head for 32 and 64\nhidden sizes and 2 and 4 heads for 128 and 256, respectively. The resulting models\nsizes are 6M, 12M, 24M, and 48M parameters. We use a text corpus of 135M\ntokens and train from scratch for 10 epochs unless otherwise noted, corresponding\nto a total of 1.35B tokens for each training. We trained a Byte Pair Encoding\ntokenizer with a vocabulary size of 8,000. The experiments are conducted with\nthe standard trainer from the Hugging Face transformers library\u00b3.\nFor vision models, we consider a standard serial implementation of classifier for\nMNIST and standard CNN-based implementations for CIFAR-10, and CIFAR-100.\nThe model for MNIST is the smallest in this paper with only 100K parameters. The\nCIFAR-10 and CIFAR-100 models represent the but smallest models with 2.1M\nand 2.2M, respectively. The difference in model size is explained by CIFAR-10\nhaving 10 classes and CIFAR-100 having 100 classes. The experiments are based\non Pytorch Lightning and use torchvision's versions of the datasets.\nThe MNIST [2] dataset consists of 60.000 train and 10.000 test samples. The\nCIFAR10 [7] and CIFAR100 [7] datasets both contains 50.000 train and 10.000\ntest samples. All models are trained from scratch. We calculate the accuracy as\nthe mean of the percentage of correct batches across the test set."}, {"title": "3 Results", "content": "In this section, we present a comparison of our BitLinear implementation with 16-\nbit floating point torch.nn. Layer, showing close-to-state-of-the-art performance\non SLMs and better-than-state-of-the-art performance for vision models. We also\nperform ablation studies on the learning rate and weight decay hyperparameters,\nas well as the choice of mean vs median for the quantization of the weights."}, {"title": "3.1 Small Language Models", "content": "The first experiment for SLMs is a scaling experiment, where we perform 16-\nbit and 1.58-bit training on all four model sizes. The second experiment is a\nhyperparameter tuning for the learning rate and weight decay in a 12M SLM, with\na fixed hidden size of 64. We show the results of the first and second experiment\nin Tables 1 and 2, respectively.\nBoth tables show the different configurations and perplexities after 10 epochs.\nFor most configurations, the training has converged or is close to convergence\nat the end of the experiment. It is important to keep in mind that the reported\nperplexity is the exponentiation of the entropy, i.e., here the exponentiation of\nthe loss defined via cross-entropy. Thus, minor changes in the loss result in quite\ndiscernible changes to the perplexity.\nThe first two columns give the hidden layer size and the number of parameters.\nThe third column provides the bit-depth and implementation: \"16\" stands for"}, {"title": "3.2 Small Vision Models", "content": "The implementation of Bitnet b1.58 [1] adopts from [11] the strategy of employing\nsignificantly higher learning rates, arguing that this is crucial for optimising the\n1.58-bit weights. They also state, that this does not carry to the full 16-bit\nprecision, suggesting this might be because of prior fine-tuning. We show in the\ngraphs presented in Figure 5b that larger learning rates are sub-optimal for both\n1.58- and 16-bit weights in small classification models, despite being trained from\nscratch. We use the mean-based benchmark for comparability, but observe similar\nresults for the median-based counterpart. For 1.58 bits, we see in Figure 5a that\nperformance gradually declines as the learning grows from 0.0001 to 0.1, with the\nsmallest learning rate providing the best performance. We observe a similar trend\nin Figure 5b, where learning with a rate of 0.05 or above distorts the training,\npreventing the network from learning at all, as evident in the evaluation."}, {"title": null, "content": "In Figure 4 we document the impact of weight decays of 0%, 1%, 5%, and\n10% across the two learning rates 0.001 and 0.0001. Figure 4a showcases the\neffect of weight decay when using higher learning rates, whereas in Figure 4b, we\nsee more continuity over the first epochs and that a training with a weight decay\nof 1% appears superior.\nAs described in Section 2, we conducted experiments with both AbsMean and\nwith AbsMedian quantization, all of which are shown in Table 3. The mean-based\nquantization is superior on MNIST and CIFAR10, with 0.15 and 1.22 difference\nin percentage points test accuracy, respectively. On CIFAR100 the median-based\nquantization is superior with a percentage point difference of 0.7. From our\nexperiments in Table 3, no clear conclusion can be drawn as to which is preferable\nin general. Similarly, in Figure 4b, we do not see a clear distinction between the\ntwo in neither the evolving performance nor the resulting one. Therefore, we\npropose the choice of AbsMean vs AbsMedian quantization for the weights as a\nhyperparameter for 1.58-bit training."}, {"title": "4 Discussion", "content": "Our results demonstrate that 1.58-bit training provides competitive performance\non both small language and vision models. We hope our work encourages the\ncommunity to work on 1.58-bit based architectures to facilitate efficient and fast\ninference independent of model scale. Overall, this enables both more environmen-\ntally friendly inference for many applications, and the deployment of deep neural\nnetworks in various low-resource uses-cases, with the potential of increased energy\nefficiency through multiplication-free kernels and even specialised hardware.\nAs reported in Section 3, using our approach to 1.58-bit training generally\nyields a small performance penalty in SLMs. In the small vision models, we see\n1.58-bit outperforming the CNN-based networks for the CIFAR10 and CIFAR100\ndatasets, while being within a percentage point difference of 0.85 for a sequential-\nbased network trained on MNIST. This difference between training on text and\nimage data is not entirely unexpected due to the nature of complexity difference\nin the SLM-data and the simpler vision-classification datasets. While 1.58-bit\ntraining still relies on the full precision 16-bit weights as shadow weights for the\nquantization in computing the 1.58-bit weights, we are reducing the capacity of\neach weight in the linear layers and, hence, of the overall network.\nThis implies that, in some case, there might be a need for creating networks\nrelying on an increased number of parameters to re-introduce some capacity.\nEvidently in our SLM-results shown in Figure 2c, we see the need to utilize\nhidden layers of size 64 in the 1.58-mean case to gain the same performance\nas the 16-bit on using hidden layers of size 32. This is also holds for hidden\nlayer sizes of 128 for 1.58 bits and corresponding 64 for 16-bit. Prior works have\nshown that LLMs do not utilize all parameters effectively [3] and even consist of\nredundant layers [12]. Therefore, we would expect the need for for this to decrease\nas model-size grows, i.e., we would expect 1.58-bit, to work well in networks from\na certain size without increasing the number of parameters. This is in line with\nprior work [11].\nThe need for increasing parameters seems to depend on the complexity of the\ndownstream task, evident from our results on SLMs in Section 3.2 compared to\nour results on small vision models in Section 3.2, where the same architecture\n(with an adjusted size of prediction head) outperforms full precision 16-bit models\non both CIFAR10 and CIFAR100. For SLMs, we do not consider the increased size\nof hidden layers to be an obstruction for profiting from 1.58-bit architectures, as\nthe models can still be expected to run significantly more efficient in inference\nsettings when implemented using custom kernels.\nIn Sections 3.1 and 3.2 we conducted extensive experiments employing either\none of the two quantization schemes: \"1.58-mean\" or \"1.58-median\". Changing\nbetween the two changes the factor with which 16-bit weights are scaled before\nrounding to integers in the quantization process. The median will, in some cases,\nbe resilient to weight-updates, allowing higher variance without notable effect\non the scaling factor. The mean will be more directly affected, particularly by\nlarge weight changes of few weights. Therefore, one provides flexibility of weights-\nupdates whereas the other provides more constrained feedback affecting the\ngradient-magnitude on the shadow weights.\nIn Figures 3d, 3e, and 3f, we report the robustness in SLMs over different\nlearning rates across 16-bit and both mean and median 1.58-bit schemes. Contrary,\nfrom what is reported on the 1.58-bit LLMs in [1], the large learning rate 0.01\n(1e-1) produces instability to such a degree that it distorts the training, effectively\nrendering it unable to optimize the performance of the network. This also happens\nfor 16 bits even though we are training from scratch. The fact that we are training\nsmall networks might explain this behaviour. The learning rate of 0.01 (1e-2) in\nFigure 3f shows the effectiveness of the median quantization, as it converges faster\nthan the mean quantization proposed in [13,11] and actually yields a convergent\nprocess similar to the one for 16 bits. This supports our claims of the behavior\nexplained above, i.e., that the flexibility allowed in median quantization can\naid faster convergence in some situations. Interestingly, we see that employing\nmedian quantization yields a significant difference in convergence when using a\nlearning rate of 0.01 and 0.001 (1e-3), contrary to the same learning rates with\nmean quantization as displayed in Figure 3e, making the network more sensitive\nto the learning rate. SMLs exhibt some but not the same level of learning-rate\nrobustness as LLMs [11] when being trained using larger learning rates."}, {"title": "5 Conclusion", "content": "In this paper, we introduced a variant of the BitNet b1.58-bit precision quantization-\naware training demonstrating state-of-the-art performance on core downstream\ntasks for SLMs and vision classification models. To the best of our knowledge,\nthis is the first work studying the characteristics and behaviour of the particular\n1.58-bit quantization approach from [13,11] on small networks. The investigations\nprovided in this work underline the potential of employing 1.58 bits more gener-\nally in small networks, mitigating prior arguments that these weight-resolutions\nonly exhibits potential on large networks with billions of parameters. This opens\nup for the efficient deployment of SLMs and small vision models, particularly\nin low-resource use-cases. We encourage future work to investigate 1.58-bit\nquantization-aware training on other networks such as object-detection networks\nin the vision domain and language models with encoders, investigating the degree\nto which our conclusions hold for such types of networks.\nOur results suggest a scaling law for small SLMs, with a 1.58-bit network\nneeding approximately the double size of hidden layers to achieve performance\ncomparable to 16-bit versions. The learning rate for SLMs and small vision models\nemploying the 1.58-bit does not follow the findings in prior work [11] to employ\nsignificantly larger learning-rates, even when trained from scratch. Weight decay\ndistorts the training when employed in training with a high learning rate, but\nto the contrary helps when applied with smaller learning rates. Our results on\nemploying AbsMean vs AbsMedian quantization of the 16-bit shadow weights do\nnot yield distinctive and conclusive results, leaving it as a hyperparameter for\nnow and opening avenues for future work on the most advantageous quantization\nschemes from 16 to 1.58 bits in the context of quantization-aware training."}]}