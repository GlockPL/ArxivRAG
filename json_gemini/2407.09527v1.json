{"title": "BitNet b1.58 Reloaded: State-of-the-art Performance Also on Smaller Networks", "authors": ["Jacob Nielsen", "Peter Schneider-Kamp"], "abstract": "Recently proposed methods for 1-bit and 1.58-bit quantization aware training investigate the performance and behavior of these methods in the context of large language models, finding state-of-the-art performance for models with more than 3B parameters. In this work, we investigate 1.58-bit quantization for small language and vision models ranging from 100K to 48M parameters. We introduce a variant of BitNet b1.58, which allows to rely on the median rather than the mean in the quantization process. Through extensive experiments we investigate the performance of 1.58-bit models obtained through quantization aware training. We further investigate the robustness of 1.58-bit quantization-aware training to changes in the learning rate and regularization through weight decay, finding different patterns for small language and vision models than previously reported for large language models. Our results showcase that 1.58-bit quantization-aware training provides state-of-the-art performance for small language models when doubling hidden layer sizes and reaches or even surpasses state-of-the-art performance for small vision models of identical size. Ultimately, we demonstrate that 1.58-bit quantization-aware training is a viable and promising approach also for training smaller deep learning networks, facilitating deployment of such models in low-resource use-cases and encouraging future research.", "sections": [{"title": "1 Introduction", "content": "The recent years of development of natural language processing (NLP) have been dominated by the capabilities offered by Large Language Models (LLMs). However, due to the size of these models, they pose a challenge in deployment and raise concerns regarding the environmental impact. Post-training quantisation methods transform the 16-bit weights to a lower bit-representation, which both reduces the memory and computational needs. The idea is to take the trained weights and find a good way of mapping them to fewer bits, enabling more efficient inference."}, {"title": "2 Method", "content": "In this section we present our quantization aware training architecture as a generalization of the BitNet b1.58 architecture [11]. First, we present our quantization method. Then, we document our experimental setup."}, {"title": "2.1 b1.58 Quantization", "content": "Our BitLinear layer functions as a drop-in replacement for PyTorch's torch.nn.Linear layer. Figure 1 illustrates BitLinear's 5-step computation flow:\n1. The activations are normalized.\n2. The normalized activations are quantized to k-bit precision.\n3. The 16-bit shadow weights are quantized to 1.58-bit weights."}, {"title": "3 Results", "content": "In this section, we present a comparison of our BitLinear implementation with 16-bit floating point torch.nn. Layer, showing close-to-state-of-the-art performance on SLMs and better-than-state-of-the-art performance for vision models. We also perform ablation studies on the learning rate and weight decay hyperparameters, as well as the choice of mean vs median for the quantization of the weights."}, {"title": "3.1 Small Language Models", "content": "The first experiment for SLMs is a scaling experiment, where we perform 16-bit and 1.58-bit training on all four model sizes. The second experiment is a hyperparameter tuning for the learning rate and weight decay in a 12M SLM, with a fixed hidden size of 64. We show the results of the first and second experiment in Tables 1 and 2, respectively.\nBoth tables show the different configurations and perplexities after 10 epochs. For most configurations, the training has converged or is close to convergence at the end of the experiment. It is important to keep in mind that the reported perplexity is the exponentiation of the entropy, i.e., here the exponentiation of the loss defined via cross-entropy. Thus, minor changes in the loss result in quite discernible changes to the perplexity.\nThe first two columns give the hidden layer size and the number of parameters. The third column provides the bit-depth and implementation: \"16\" stands for"}, {"title": "3.2 Small Vision Models", "content": "The implementation of Bitnet b1.58 [1] adopts from [11] the strategy of employing significantly higher learning rates, arguing that this is crucial for optimising the 1.58-bit weights. They also state, that this does not carry to the full 16-bit precision, suggesting this might be because of prior fine-tuning. We show in the graphs presented in Figure 5b that larger learning rates are sub-optimal for both 1.58- and 16-bit weights in small classification models, despite being trained from scratch. We use the mean-based benchmark for comparability, but observe similar results for the median-based counterpart. For 1.58 bits, we see in Figure 5a that performance gradually declines as the learning grows from 0.0001 to 0.1, with the smallest learning rate providing the best performance. We observe a similar trend in Figure 5b, where learning with a rate of 0.05 or above distorts the training, preventing the network from learning at all, as evident in the evaluation."}, {"title": "4 Discussion", "content": "Our results demonstrate that 1.58-bit training provides competitive performance on both small language and vision models. We hope our work encourages the community to work on 1.58-bit based architectures to facilitate efficient and fast inference independent of model scale. Overall, this enables both more environmentally friendly inference for many applications, and the deployment of deep neural networks in various low-resource uses-cases, with the potential of increased energy efficiency through multiplication-free kernels and even specialised hardware.\nAs reported in Section 3, using our approach to 1.58-bit training generally yields a small performance penalty in SLMs. In the small vision models, we see 1.58-bit outperforming the CNN-based networks for the CIFAR10 and CIFAR100 datasets, while being within a percentage point difference of 0.85 for a sequential-based network trained on MNIST. This difference between training on text and image data is not entirely unexpected due to the nature of complexity difference in the SLM-data and the simpler vision-classification datasets. While 1.58-bit training still relies on the full precision 16-bit weights as shadow weights for the quantization in computing the 1.58-bit weights, we are reducing the capacity of each weight in the linear layers and, hence, of the overall network.\nThis implies that, in some case, there might be a need for creating networks relying on an increased number of parameters to re-introduce some capacity. Evidently in our SLM-results shown in Figure 2c, we see the need to utilize"}, {"title": "5 Conclusion", "content": "In this paper, we introduced a variant of the BitNet b1.58-bit precision quantization-aware training demonstrating state-of-the-art performance on core downstream tasks for SLMs and vision classification models. To the best of our knowledge, this is the first work studying the characteristics and behaviour of the particular 1.58-bit quantization approach from [13,11] on small networks. The investigations provided in this work underline the potential of employing 1.58 bits more generally in small networks, mitigating prior arguments that these weight-resolutions only exhibits potential on large networks with billions of parameters. This opens up for the efficient deployment of SLMs and small vision models, particularly in low-resource use-cases. We encourage future work to investigate 1.58-bit quantization-aware training on other networks such as object-detection networks in the vision domain and language models with encoders, investigating the degree to which our conclusions hold for such types of networks.\nOur results suggest a scaling law for small SLMs, with a 1.58-bit network needing approximately the double size of hidden layers to achieve performance comparable to 16-bit versions. The learning rate for SLMs and small vision models employing the 1.58-bit does not follow the findings in prior work [11] to employ significantly larger learning-rates, even when trained from scratch. Weight decay distorts the training when employed in training with a high learning rate, but to the contrary helps when applied with smaller learning rates. Our results on employing AbsMean vs AbsMedian quantization of the 16-bit shadow weights do not yield distinctive and conclusive results, leaving it as a hyperparameter for now and opening avenues for future work on the most advantageous quantization schemes from 16 to 1.58 bits in the context of quantization-aware training."}, {"title": "2.1 b1.58 Quantization", "content": "$\nx_{scale} = \\frac{Q_b}{max(|\\hat{I}|) + \\epsilon}\n$"}, {"title": "2.1 b1.58 Quantization", "content": "$\nx_{quant} = max(-Q_B, min(Q_B - 1, round(\\hat{I} \\cdot x_{scale}))\n$"}, {"title": "2.1 b1.58 Quantization", "content": "$\nw_{scale} = \\frac{1}{Measure(|W|) + \\epsilon}\n$"}, {"title": "2.1 b1.58 Quantization", "content": "$\nW_{quant} = max(-1, min(1, round(W \\cdot w_{scale})\n$"}, {"title": "2.1 b1.58 Quantization", "content": "$\nY_{quant} = X_{quant} \\cdot W_{quant} + b\n$"}, {"title": "2.1 b1.58 Quantization", "content": "$\ny = \\frac{Y_{quant}}{W_{scale} X_{scale}}\n$"}]}