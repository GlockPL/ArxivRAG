{"title": "Ensemble based approach to quantifying uncertainty of LLM based classifications", "authors": ["Srijith Rajamohan", "Ahmed Salhin", "Rohit Kumar", "Yu-Cheng Tsai", "Josh Frazier", "Todd Cook"], "abstract": "The output of Large Language Models (LLMs) are a function of the internal model's parameters and the input provided into the context window. The hypothesis presented here is that under a greedy sampling strategy the variance in the LLM's output is a function of the conceptual certainty embedded in the model's parametric knowledge, as well as the lexical variance in the input. Finetuning the model results in reducing the sensitivity of the model output to the lexical input variations. This is then applied to a classification problem and a probabilistic method is proposed for estimating the certainties of the predicted classes.", "sections": [{"title": "1 Introduction", "content": "LLMs tend to hallucinate [1] as a result of a lack of knowledge, and in this work we propose a method to measure the certainty of an LLM-generated output for a classification problem. This is crucial for key areas where trustworthy decision-making is required such as in healthcare, finance, accounting or law. There have been various efforts to solve this particular problem.\nThere are white-box methods and black-box methods [2] for assessing the truthfulness of LLMs. White-box techniques allow for inspection of the generation process whereas in black-box methods the model's internal states are unavailable during response generation. White-box methods rely on measuring the entropy of the generated tokens such as semantic entropy [3]. Token probability-based metrics are notable for their computational efficiency since they only a single generation is required here. The Mean Token Entropy metric described by [4] computes the average uncertainty over all the tokens in a generated response. The TokenSAR metric introduced by [5] proposes a heuristic that measures the importance of different tokens in a sequence.\nSome examples of black-box methods are provided by [4], [3], [6] that incorporates techniques such as lexical similarity, number of semantic sets, sum of eigenvalues of the graph laplacian"}, {"title": "2 Methodology", "content": "LLMs can be considered to be state machines where the model state is represented by the model's parametric knowledge or the model weights. The outputs produced by a model are then a function of the parametric knowledge and the model input provided through the context. For a classification problem, we can write the following equation, where the classification can be parsed from the LLM output tokens generated until timestep ttermination:\nLLMclassification = F(LLMweights, LLMinput)\nNow, we posit that the variance in the LLM-predicted class can be written as\nvar(LLMclassification classification) = F(conceptual_certainty, var(LLMinput))\nwhere the term \u201cconceptual_certainty\" represents the soundness of the parametric knowledge residing within the LLM. Ideally, fine-tuning or updating a model's weights should result in reducing the sensitivity of the predicted class in response to the lexical variance of the input, i.e. better conceptual certainty as a result of fine-tuning leads to less sensitivity to the input.\nNow consider question Q\u00bf for i \u2208 {0,1, ..., k}, where k is the space of all possible unique questions that can be asked. Consider Q\u2081 to represent the 'latent intent' which is defined as the underlying intent behind a natural language question. Now each such question can be phrased in n different ways, where the same intent is expressed through a different sequence/combination of words.\nQi = {Q10, Qil, ..., Qin}\nQij for i \u2208 {0, 1, ..., k}, j \u2208 {0, 1, ..., n}\nConsider an example intent question Q\u2081 and its variants where n 15.\nIntent (Q_i) = \"Who are the suppliers that I need to pay?\"\nExample variants of Q\u00bf are shown below as Qij\nQ_ij = {'Who are the suppliers that I need to pay?',\n\"Can you tell me the suppliers I'm currently indebted to?\",\n'Which suppliers have outstanding payments from me?',\n'To which suppliers do I have financial obligations?',\n\"I'd like to know the suppliers to whom I owe money.\",\n'Could you list the suppliers that are awaiting payment from me?',"}, {"title": "2.1 How does it help with training?", "content": "From a concept graph perspective, it identifies the need to strengthen disambiguation either through definition or examples. Responses fall into the following three categories:\n1. Confident (i.e., more certainty) and correct\n2. Confident and incorrect\n\u2022 Makes a classification with high certainty but this class is wrong\n\u2022 Hard to detect\n\u2022 We should target this for improvement with training data, adding more disambiguation data\n3. Not confident and incorrect\n\u2022 We can possibly detect this by building a distribution of uncertainties\n\u2022 The question could be ambiguous"}, {"title": "2.2 What is the benefit of this approach vs. the sampling approach at inference?", "content": "It allows us to measure uncertainty of an answer relative to question intent Q\u2081 as opposed to uncertainty relative to phrasing of a question intent, i.e. Qij.\nBy setting do_sample = True or a variation of it to perform non-greedy sampling during the inference stage of an LLM, we can sample from the distribution of answers. With sufficient samples, we can build up the distribution of predictions and thereby compute a margin or entropy as an uncertainty measure for the predictions [9]. However, this once again computes the uncertainty for the variant Qij and not the intent Q\u00bf, and is therefore sensitive to the lexical formulation of Qij"}, {"title": "2.3 Operation of the detectors", "content": "The system under discussion is responsible for identifying the REST endpoints and the parameters necessary to form a REST route to extract the appropriate data requested by a user. Given the question Which products are due for a reorder?, the a fine-tuned LLM identifies the endpoints and the parameters necessary to extract the data needed to answer the question. The candidates are provided through the prompt and the LLM is asked to make a selection from the list of candidates, any results that are not present in the list are discarded. An example of the parsed dictionary from the LLM for endpoint selection is shown here:\n`{'endpoint': '/stock_items', 'reason': 'Use the /stock_items endpoint since this contains the purchase order reorder information <|end|>'}`\nThe response contains two keys, endpoint and reason. For the parameter detector the key would be a list of strings along with the reason key. The experiment is then conducted for the following LLMS\n1. base LLM\n2. trained LLM\nand the distributions of predicted classes are noted and compared. If the fine-tuned LLM has learned the correct endpoint/parameters they should have a higher probability of selection relative to the base LLM."}, {"title": "3 Evaluation experiments", "content": "A few questions (179) from the space of Q\u2081 are sampled and for each question a set of n=15 variations are generated {Q10, ...Qi15} using an LLM to rephrase the original question Q\u2081. The larger the value of n the better we approximate the distribution, but it is possible to have duplicates with augmentation techniques and a larger n may not necessarily mean a larger number of unique samples. Another potential issue with this approach is the possibility of a variant deviating from the original intent when generated automatically. It must be noted that these are labeled samples."}, {"title": "3.1 Evaluating the detectors", "content": "The charts below represent the distribution of parameter or endpoint predictions for the trained model and the base model. Note that the 'accuracy' listed in the chart titles below refers to the 'ensemble_true_label_accuracy' defined above."}, {"title": "3.1.1 Evaluating the endpoint detector", "content": "Inference is performed using the base and fine-tuned models utilizing the same set of prompts. Each model is posed every variant j from the set (n=15) and the response is noted. The frequency of the predicted endpoints are plotted as a distribution for the trained model and the base model. The true label is depicted in blue while everything else is in red."}, {"title": "3.1.2 Evaluating the parameter detector", "content": "Inference is performed using the base and fine-tuned models utilizing the same set of prompts. Here the correct endpoint is known apriori and the candidate pool of parameters for the endpoint are presented to the LLMs for selection. However, unlike endpoint detection multiple parameters can be predicted for a given variant question, thereby making this a more challenging exercise since sufficiency is a stricter condition to satisfy compared to 'most likely'. Each model is posed every variant j from the set (n=15) and the response is noted."}, {"title": "4 How do we quantify uncertainty?", "content": "An experiment was conducted over 179 data questions for endpoint detection. These questions are labeled and the classes span the same set of classes as in the experiments above. The prediction accuracy of endpoint detection was 92.7%. The detectors are defined such that prediction is the class which has the highest frequency from the ensemble of predictions and therefore the prediction certainty can be measured from this relative frequency or ensemble accuracy as defined above. See ensemble accuracy as shown in [Ensemble_accuracy = the highest frequency (of prediction from above)]\nThese certainties are curated for each of the 179 questions in our experiment and it is noted whether the prediction is correct or not based on the ground truth labels. This is divided into two sets of data ensemble_certainty_correct when the prediction is correct and ensemble_certainty_incorrect when the prediction is incorrect. This gives us two distributions for certainties (correct and incorrect predictions). The distributions of these certainties are plotted in Figure 2 below."}, {"title": "5 Conclusion and Future work", "content": "There is some evidence to suggest that fine-tuning LLMs results induces greated conceptual certainty in the parametric knowledge of the LLMs making them more immune to the lexical variations of the inputs. Examples were provided to test this hypothesis using the classification problem presented here. We use an ensemble approach to estimate an 'ensemble_accuracy' to measure the prediction uncertainty and demonstrate improvement. Distributions of 'ensemble_accuracy' scores for correct and incorrect predictions are created and these prior distributions are used to compute measures that indicate the likelihood of correctness of a new prediction. While the need for more representative samples is acknowledged here, a notable area of improvement is the need for class-specific distributions. In this work distributions were built over the space of all classes but class-specific distributions are more likely to be accurate, however this requires extensive class-representative sample curation and labeling and is left for future exploration."}]}