{"title": "Reduce, Reuse, Recycle: Categories for Compositional Reinforcement Learning", "authors": ["Georgios Bakirtzis", "Michail Savvas", "Ruihan Zhao", "Sandeep Chinchali", "Ufuk Topcu"], "abstract": "In reinforcement learning, conducting task composition by forming cohesive, executable sequences from multiple tasks remains challenging. However, the ability to (de)compose tasks is a linchpin in developing robotic systems capable of learning complex behaviors. Yet, compositional reinforcement learning is beset with difficulties, including the high dimensionality of the problem space, scarcity of rewards, and absence of system robustness after task composition. To surmount these challenges, we view task composition through the prism of category theory-a mathematical discipline exploring structures and their compositional relationships. The categorical properties of Markov decision processes untangle complex tasks into manageable sub-tasks, allowing for strategical reduction of dimensionality, facilitating more tractable reward structures, and bolstering system robustness. Experimental results support the categorical theory of reinforcement learning by enabling skill reduction, reuse, and recycling when learning complex robotic arm tasks.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) is a powerful tool for sequential decision-making, which is crucial in training robots to execute complex tasks. Nevertheless, the challenge of composing multiple tasks a prerequisite for creating adaptable, interpretable, and versatile robotic systems-persists. Central to this challenge are issues such as high dimensionality problem spaces and task complexity. These issues manifest in several ways: sparse rewards [17], where positive feedback is infrequent, making learning slower and more complex; a lack of robustness [37], making systems susceptible to variations in the environment or task; and complexities in sequencing and coordinating sub-tasks [41], particularly when tasks have interdependencies or must perform in a specific order.\nA compositional perspective alleviates these challenges inherent in multi-task RL by decomposing complex tasks into simpler sub-tasks [26]. First, principled decomposition reduces the dimensionality of the problem space. Each sub-task can be learned and optimized separately, resulting in more manageable reward structures and quicker learning times. For instance, sparse rewards become less problematic as each sub-task can be associated with its reward structure, providing more frequent feedback to the learning algorithm. Second, by segregating tasks, failures or variations in one component have a lesser chance of disrupting the entire system, improving overall stability and robustness. If a particular sub-task is performing poorly due to environmental variations, it can be isolated, analyzed, and improved without affecting the performance of other tasks. Third, when tasks are modular, the interactions and dependencies between different tasks can be systematically mapped and managed, simplifying coordinating and sequencing sub-tasks.\nDespite progress in compositional RL, the challenge of modular task composition remains a barrier to creating versatile robotic systems. Deciding how to break down a complex task into manageable subtasks relies on domain-specific knowledge. The categorical interpretation of RL problem formulations offers an abstraction that systematically captures tasks' interdependencies and compositional structures, leading to a principled way of discovering optimal task decompositions. Additionally, combining policies from different modules to form a coherent overall policy can lead to conflicts or inefficiencies, especially if the components were developed independently [30]. The categorical semantics of RL specifies how policies are combined, ensuring consistency and coherence in the resulting composite policy and enhancing the robustness of integrated policies. Additionally, proof of the existence of categorical structures equips compositional RL with a symbolic knowledge representation for interfacing and gluing sub-task constructions. In this paper, we validate our theoretical results [8] through computational evidence using increasingly complex robotic tasks.\nMerits of category-theoretic RL:\n\u2022 Reduce: By representing the dynamics between tasks, such as picking up an object and placing it in a box, the categorical formalism streamlines coordination and sequencing, improving the ability to learn increasingly complex tasks through reduction.\n\u2022 Reuse: Through abstraction within the categorical framework, the robot can learn a task, like \"picking up an object,\" and then reuse this knowledge across related tasks modularly.\n\u2022 Recycle: The categorical formalism extends this reusability further, allowing for the recycling of learned skills across different contexts. For example, a skill developed for lifting a block can be fine-tuned to lift a soda can."}, {"title": "2 Preliminaries", "content": "Here we give informal definitions of the category-theoretic structures we use. Consult Lawvere and Schanuel [24], Leinster [25], or Mac Lane [27] for an in-depth treatment of category theory.\nDefinition 1 (Category). A category C consists of the following data\nobjects a collection of objects, denoted obj C\nmorphisms for each pair of objects A, B, a collection of morphisms from A to B, denoted Hom c [A, B]\u00b9\nidentity for each object A, a morphism $A \\xrightarrow{id_A} A$\ncomposition for each A, B, C objects, a composition operation\n$o_{A,B,C}: Hom_c[B, C] \\times Hom_c[A, B] \\rightarrow Hom_c[A, C]$.\nTo represent a category this data has to also respect the following relations for each $A \\xrightarrow{f} B$, $B \\xrightarrow{g} C$, and $C \\xrightarrow{h} D$:\n$id_B \\circ f = f$\n$f \\circ id_A = f$\n$(h \\circ g) \\circ f = h \\circ (g \\circ f)$,\nmeaning that composing with the identity morphism either from the left or the right recovers the morphism itself (unitality) and the order of operations when composing morphisms does not matter as long as the order of operands is unchanged (associativity).\nWe will work within Set, the category whose objects are sets and morphisms are functions between them. For each set A, $id_A$ is the identity function from A to itself, and composition is function composition. Rather than treating sets as collections of items, the categorical interpretation emphasizes the relationships between sets.\nDefinition 2 (Commutative diagrams). A standard diagrammatic way to express composites is $X \\xrightarrow{f} Y \\xrightarrow{g} Z$ and equations via commutative diagrams of the following form\n$ \\begin{matrix}\nX &\\xrightarrow{f} & Y \\\\\n& & \\downarrow^g\\\\\n& & Z\\end{matrix}  $, which stands for $g \\circ f = h$.\nA commutative square is, instead,\n$\\begin{matrix}\nX &\\xrightarrow{f} & Y \\\\\n\\downarrow^g & & \\downarrow^{f'}\\\\\nZ &\\xrightarrow{g'} & W\\end{matrix}  $, which stands for $g' \\circ f = f' \\circ g$.\nA diagram commutes when the result of a composition is the same regardless of the path we take from one object to another.\nDefinition 3 (Pushout). A pushout for morphisms $f: C \\rightarrow A$ and $g: C\\rightarrow B$ is an object W together with morphisms $a: A \\rightarrow W$ and $b: B\\rightarrow W$ such that the square\n$\\begin{matrix}\nC &\\xrightarrow{f} & A \\\\\n\\downarrow^g & & \\downarrow^{a}\\\\\nB &\\xrightarrow{b} & W\n\\end{matrix}$\ncommutes, where the morphisms a, b are thought of as \"inclusions,\" such that $a \\circ f = b \\circ g$, and is universal: for any object $W_0$ with morphisms $a_0: A \\rightarrow W_0$ and $b_0: B \\rightarrow W_0$ such that $a_0 \\circ f = b_0 \\circ g$, there exist a unique morphism $w: W \\rightarrow W_0$ such that $w \\circ a = a_0$ and $w \\circ b = b_0$.\nAnother way to state the above is that W is the colimit (see Leinster [25, chapter 5]) of the diagram\n$\\begin{matrix}\nC &\\xrightarrow{f} & A \\\\\n\\downarrow^g & & \\\\\nB\n\\end{matrix}$\nWe write $W = A \\cup_C B$ for the pushout, with the morphisms f, g implied in the notation.\nA simple example of a pushout is the disjoint union $A \\amalg B$ of two sets, which is the pushout of the morphisms $ \\emptyset \\rightarrow A$ and $ \\emptyset \\rightarrow B$."}, {"title": "3 Compositional Reinforcement Learning via Morphisms and Subprocesses", "content": "In this section, we restate some definitions and results from our theoretical work for completeness [8]. Task interactivity often relies on the Markov decision process (MDP) structure [32]. We start by developing a definition for MDPs that is congruent with the traditional MDP representation while containing some subtle generalizations, such as added flexibility of action spaces. Our definition is designed to accommodate uniform action spaces across the entire state space. However, we also cater to applications where varying action spaces in different state space regions are more appropriate, each with distinct semantic meanings.\nFor instance, the environment remains relatively stable in a robotic arm example. Conversely, consider a drone maintaining a safe altitude; the action space dynamically adjusts when entering an area where this altitude threshold changes. Traditionally, the literature focuses on surjective morphisms on state and action spaces, aiming to reduce these spaces for efficient learning. Our approach seamlessly incorporates this aspect. However, to emphasize the compositionality feature, it is required also to allow for the expansion of state spaces to encompass task components and incorporate morphisms with mixed characteristics (neither purely injective nor surjective).\nDefinition 4 (MDP). An MDP M = (S, A, \u03c8, T, R) consists of:\n\u2022 The state space S, a measurable space with a fixed \u03c3-algebra.\n\u2022 The state-action space A, the total set of actions available at all different states, i.e. elements of S.\n\u2022 A function \u03c8: A\u2192 S that maps an action a \u2208 A to its associated state s \u2208 S. Every action in the action space a \u2208 A can be taken at a specific state s \u2208 S, and \u03c8 maps the action to that state so that \u03c8(a) = s. Equivalently, the set of actions available at s is the pre-image \u03c8\u207b\u00b9(s) \u2282 A.\n\u2022 The information of the transition probabilities is given as a function T: A \u2192 $P_S$, where $P_S$ denotes the space of probability measures on S.\n\u2022 The reward function R: A \u2192 \u211d.\nThe above definition of MDP is congruent with the usual definition seen in, for example, Sutton and Barto [39]. The extra information is necessary to formally examine the compositionality feature, for example, to answer which settings composing two MDPs generate a holistic optimal policy. The resulting theorems apply to the usual definition for any practical RL application. A concrete example of MDPs in use is learning for robotics (section 4).\nDefinition 5 (Category of MDPs). MDPs form a category (definition 1) MDP whose morphisms are as follows. Let $M_i = (S_i, A_i, \\psi_i, T_i, R_i)$, with i = 1,2, be two MDPs.\nA morphism m = (f,g): M\u2081 \u2192 M2 is the data of a measurable function f: S1 \u2192 S2 and a function g: A1 \u2192 A2 satisfying the following compatibility conditions:\n1. The diagram\n$\\begin{matrix}\nA_1 & \\xrightarrow{g} & A_2\\\\\n\\downarrow^{\\psi_1} & & \\downarrow^{\\psi_2}\\\\\nS_1 & \\xrightarrow{f} & S_2\n\\end{matrix}$\nis commutative (definition 2).\n2. The diagram\n$\\begin{matrix}\nA_1 & \\xrightarrow{g} & A_2\\\\\nT_1\\downarrow & & \\downarrow T_2\\\\\nP_{S_1} & \\xrightarrow{f_*} & P_{S_2}\n\\end{matrix}$\nis commutative, where $f_*$ maps a probability measure $\u03bc_1 \u2208 P_{S_1}$ to its pushforward, meaning $\u03bc_2 = f_*\u03bc_1 \u2208 P_{S_2}$ under f.\n3. $R_1 = R_2 \\circ g: A_1 \\rightarrow \\mathbb{R}$.\nThe constant MDP pt is the MDP pt whose state space and action spaces are the one-point set. Every MDP M admits a unique, natural morphism M \u2192 pt and pt is the terminal object in MDP.\nThe two commutative diagrams above show us when two MDPS are compatible in that their interfaces agree. Namely, diagram (1) guarantees that if an action $a_1$ in MDP M1 is associated to a state $s_1 \u2208 S_1$, then its image action $a_2 = g(a_1)$ under m is associated to the image state $s_2 = f(s_1)$. Similarly, diagram (2) ensures that the transition probability from any state $s_1$ to another state $s'_1 \u2208 f\u207b\u00b9(s_2)$ under taking action $a_1$ in M1 is equal to the transition probability from the state $s_2 = f(s_1)$ to $s'_2 = f(s'_1)$ under action $a_2 = g(a_1)$ in M2. The third compatibility condition accounts for the reward in our categorical formulation.\nIntuitively, the category of MDPs represents a way to relate different MDPs to each other through morphisms. A morphism between two MDPs is a pair of functions that consistently map states and actions from one MDP to another. The two commutative diagrams presented above define what it means for these mappings to be consistent. The first diagram ensures that if an action is associated with a specific state in the first MDP, the action in the second MDP must be related to the corresponding state. The second diagram ensures that transition probabilities between states are preserved under the mapping. In other words, how actions transition from one state to another in the first MDP must correspond to how actions transition between the mapped states in the second MDP. When augmented with the reward function, two MDPs must preserve the relationships between states and actions and the rewards associated with those actions.\nSubprocesses The definition of morphism correctly captures the notion of a subprocess of an MDP.\nDefinition 6 (Subprocess of MDP). We say that M\u2081 is a subprocess of the MDP M2 if there exists a morphism (f, g): M1 \u2192 M2 such that f and g are injective. We say that M1 is a full subprocess if diagram (1) is cartesian.\nSince f is injective, we may consider the state space S\u2081 as a subset of S2. Moreover, the condition that diagram (1) is cartesian means that the only available actions on S\u2081 come from MDP M1. Thus, M1 being a full subprocess of M2 implies that an agent following the MDP M2 who finds themself at a state s1 \u2208 S\u2081 will remain within S\u2081 no matter which action a1 \u2208 A1 they elect to apply.\nConversely, for an MDP M2 and any subset $S_1 \\subseteq S_2$ there is a canonical subprocess M\u2081 with state space S1, whose action space A1 is defined by\n$A_1 := \\psi_2^{-1}(S_1) \\cap T_2^{-1}(f_*(P_{S_1})).$\nIn fact, M\u2081 is uniquely characterized as the maximal such subprocess.\nProposition 1. Any subprocess M\u2081 \u2192 M2 with state space S1 factors uniquely through the subprocess M1 \u2192 M2.\nThis concept of factoring reflects the idea that certain morphisms or relationships break down into simpler or more fundamental parts, and uniqueness ensures that this breakdown is done in one specific way. In particular, for MDPs, the above proposition establishes a unique intermediate structure or relationship that connects subprocesses.\nPushouts: a gluing construction The categorical notion of pushout models the gluing of two objects along a third object with morphisms to each. Interesting categorical properties usually are universal. Universal properties represent specific ideals of behavior within a defined category [38, 2]. A pushout's universal property is determined by its being minimal in an appropriate, universal sense. As mentioned above, the simplest example is given in the category of sets by the disjoint union S1 II S2, which can be viewed as the pushout of the two morphisms \u00d8 \u2192 S1 and \u00d8 \u2192 S2.\nSuppose that we have two MDPS M1 and M2 together with a third M3, which is expressed as a component of both through morphisms m1: M3 \u2192 M1 and m2: M3 \u2192 M2. The existence of a pushout operation in the category MDP allows us to model the composite task obtained by putting together M1 and M2 and capture the internal behavior of their common component in a maximally efficient way without introducing extra cost in resources or dimensionality or sacrificing accuracy of the representation. The universal property of pushouts guarantees this as minimal gluings along an overlap. Moreover, if M3 is a subprocess, then both M\u2081 and M2 form subprocesses of the composite M, as desired.\nTheorem 1. There exists an MDP $M = M_1 \\cup_{M_3} M_2$ which is the pushout (definition 3) of the diagram in MDP:\n$\\begin{matrix}\nM_3 & \\xrightarrow{m_1} & M_1\\\\\n\\downarrow^{m_2} & & \\\\\nM_2\n\\end{matrix}$\nGluing behaves well with respect to subprocesses. Theorem 1 is the foundation of MDP compositionality. Pushouts of two MDPs are minimal, universal ways of gluing two MDPs along an overlap in full generality. Their existence is the most desired property of the category MDP in that it guarantees that one can always systematically glue MDPs whenever possible and most efficiently. Without Theorem 1, introducing the category MDP would be of limited value, as one would have to resort to ad hoc constructions, which, after proving Theorem 1, would necessarily have to reduce to the pushout construction anyway given its universality.\nProposition 2. Suppose that M3 is a subprocess of M\u2081 and M2. Then M\u2081 and M2 are subprocesses of $M_1 \\cup_{M_3} M_2$.\nThe above theoretical framework enables us to make compositionality explicit within RL by providing tools to analyze and represent complex agent relationships through universal constructions-constructions that do not depend on a particular problem or definition of compositional RL but apply to any formulation that uses MDPs."}, {"title": "4 Compositional Task Completion", "content": "We illustrate the implications of the constructions above in the context of compositional task completion, but this is one possible application. In particular, we derive a denotational language for compositional RL based on the properties of subprocess and pushout.\nWe employ denotational semantics to provide a rigorous mathematical foundation for modeling RL tasks. Denotational semantics precisely define the meanings of constructs without ambiguity, using mathematical objects. This approach is crucial in RL as it allows us to define the components of learning tasks such as states, actions, rewards, and transitions in a clear, consistent, and universally applicable way across different scenarios. By using denotational semantics, we ensure that each component of an RL task is described in terms of its effects and interactions, which facilitates an interpretable composition of complex behaviors. This method contrasts with more operational approaches focusing on the computation process itself. The benefit of denotational semantics in our context is its ability to abstract and generalize problem-solving strategies, making it potentially more manageable to apply them to various tasks. This abstraction is advantageous when dealing with complex decision-making environments, where clarity and consistency in task formulation are crucial to developing robust and scalable solutions."}, {"title": "4.1 Zig-zag Diagrams", "content": "In this subsection, we restate results from our theoretical work [8]. For designing compositional tasks, we desire to operationalize using the categorical semantics of RL, which involve accomplishing tasks sequentially. In a general setting, we consider the setup given by, what we term a zig-zag diagram of MDPs\n$\\begin{matrix}\nN_0 & & N_1 & & & & & N_{n-1}\\\\\n & \\searrow & & \\searrow & & & & \\searrow\\\\\nM_0 & & M_1 & & M_2 & & M_{n-1} & & M_n\n\\end{matrix}$\nwhere for each i = 0, . . ., n \u2212 1, Ni is a subprocess of Mi (definition 6).\nThe composite MDP associated with the above diagram is the MDP Cn defined by the inductive rule\n$C_0 := M_0$,\n$C_1: C_0 \\cup_{N_0} M_1$,\n...\n$C_n: C_{n-1} \\cup_{N_{n-1}} M_n$.\nEach subprocess Ni \u2192 Mi models the completion of a task in the sense that an agent's goal is to find themselves at a state of Ni eventually. Once the i-th goal is accomplished inside the environment given by Mi, we allow for the possibility of a changing environment and more options for states and actions to achieve the next goal modeled by the subprocess Ni+1 \u2192 Mi+1.\nThe composite MDP Cn is a single environment capturing all the tasks simultaneously.\n(?) Suppose an agent learned an optimal policy for each MDP Mi given the reward function Ri for achieving the i-th goal for each i = 0,...,n. Under what conditions do these optimal policies determine optimality for the joint reward on the composite MDP Cn?\nOne scenario in which optimality is preserved is when the zig-zag diagram is forward-moving, meaning that Ni is a full subprocess of Mi and the optimal value function $v_i^*(s)$ for any state s in the state space of a component Mi, considered as a state of Cn, is monotonic for subsequent subprocesses $M_{i+1},..., M_n$. Monotonicity here means that the expressions\n$\\sum_{s' \\in S_i} T(a)(s')(R_i(a) + \\gamma \\cdot v_i^*(s'))$\n$\\sum_{s' \\in S_i} T(a)(s')(R_i(a) + \\gamma \\cdot v_{C[i,n]}(s'))$\nare maximized by the same action a \u2208 (Ai)s. Here C[i,n] denotes the composite MDP of the truncated zig-zag diagram\nMonotonicity is a strong assumption that helps make a formal argument but can be relaxed. It is related to the notion that myopic solutions to the above maximization problems are globally optimal. For a non-example, one can consider a moving agent on a grid having to come within a certain distance of two locations, which are an equal distance away from the agent's starting point.\nIn practice, a zig-zag diagram can always be made forward-moving by removing the actions of Ni that can potentially move the agent off Ni back into Mi. This can be formalized as puncturing Mi along the complement of Ni and intersecting the result with Ni. The details are of general interest but not immediately relevant to the present paper, so we skip a further discussion.\nTheorem 2. Suppose that the zig-zag diagram (4) is forward-moving and the optimal value function of Cn is monotonic. Then, following the individually calculated policies \u03c0\u1d62 on each component Mi gives an optimal policy on the composite MDP Cn."}, {"title": "4.2 Experiments", "content": "To support the categorical formalism through experiments, we tasked RL agents with four distinct manipulation challenges (figure 1) using robosuite [48, 4].\nTask 1 Lift a block The robot lifts a single block:\nThe experiments within the robosuite simulator demonstrate a compositional approach to RL by integrating category theory. This integration emphasizes the use of zig-zag diagrams as an efficient denotational tool. The resulting compositional RL technique provides a direct method for sequential decision-making and introduces modularity in robotic tasks, adding precision to learning.\nThe practical strengths of mapping computational tasks to mathematical objects manifest in the following properties.\n\u2022 Compositionality: When tasks break down into sub-tasks, the semantics guarantee that the entirety's meaning is an aggregation of its components, streamlining the synthesis of intricate tasks from foundational ones.\n\u2022 Scalability: Scaling up and integrating new computational tasks becomes unambiguous.\n\u2022 Interoperability: Employing a common mathematical framework ensures consistent and modular understanding across varied system compositions.\nIn the context of the zig-zag diagrams and the categorical formalism, each manipulation task corresponds to an environment Mi, and its series of sub-tasks align with the subprocesses Ni. We have a defined dense reward signal $r_{dense}$ and a set success criterion for every such sub-task. Meeting this success criterion implies the agent has reached a state within the subprocess N\u2081 and receives a task reward $r_{task}$. However, if the agent meets the failure criterion, it signifies a deviation from the intended subprocess path, resetting the agent to the beginning of the task or environment Mo.\nEach sub-task within an environment Mi is associated with a subprocess Ni, and we dedicate an individual RL agent to each such subprocess. During training, the agent corresponding to the active subprocess or sub-task samples an action records the subsequent experience, and refines its policy. From the perspective of the categorical structure, this approach resembles traversing through the zig-zag diagrams sequentially. Initially, with all agents set to random policies, the training effectively starts with the environment Mo and its associated subprocess No. Only after achieving success in this initial sub-task does the robot begin accumulating experiences in the subsequent subprocesses or sub-tasks, moving through the diagram.\nHere are the settings for each sub-task MDP mapping to the zig-zag diagrams above.\n\u2022 $M_r[r, o]$ models the reaching task, where the robot r must reach its grippers around an object o of interest. The 6-dimensional state space consists of the position of the robot hand and the object: s = [$p_r$, $p_o$]. At each step, the robot controls the displacement of its hand, moving at most 10 cm in each direction: a = $\u0394p_r$. When the MDP transitions from state s to s', the robot receives a shaped reward r = $||p_r - p_o||^2 - ||p'_r - p'_o||^2$.\n\u2022 $M_l[r, o]$ models the lifting task, assuming the robot hand is aligned with the object. The robot is only allowed up-and-down movements plus gripper controls. The state space contains the robot hand height, object height, and gripper width: s = [$z_r$, $z_o$, w], and a = [$\u0394z_r$, \u0394\u03c9]. The robot is rewarded for staying close to the object and lifting it off the table: r = ($z_r-z_o-z^*-z_{o1}^*) + (z^*-z_r)$.\n\u2022 $M_t[r, o]$ is the transporting task. The robot must transport a held object to the other side of the table. The robot controls the hand movement but cannot open the gripper. Because the robot is holding the object, they share the same position. So s = $p_r$, and a = $\u0394p_r$. The robot is rewarded for moving in the +Y direction: r = $y'_r - y_r$.\n\u2022 $M_p[r, o, g]$ is the placing task. Starting with the robot holding the object, it must place the object at the desired goal location g. The state space contains the robot position, object position, goal position, and gripper width: s = [$p_r$, $p_o$, $p_g$, W]. The robot controls the hand movement and gripper opening: a = [$\u0394p_r$, \u0394\u03c9].\nNext, we define the subprocesses corresponding to the sub-task MDPs.\n\u2022 $N_r[r, o]$ contains the states where the robot position is at most 1 cm away from the object: $||p_r - p_o||^2 < 0.01$.\n\u2022 $N_l[r, o]$ is entered when the object's height is above some threshold h: $y_o > h$.\n\u2022 $N_t[r, o]$ contains the states where the robot has crossed the center line of the table: $y_r > 0.03$.\nWe use the same reward structure for the baseline. In particular, the baseline to compare against trains a single RL agent to complete the total task, and the reward given to the agent is the summation of the rewards given to the zig-zag diagram. Additionally, robosuite incorporates built-in stochasticity regarding objects' initial locations for all experiments. The success rate is computed from 20 evaluation episodes with stochastic object placements. We perform evaluations every 2000 training steps."}, {"title": "4.2.2 Performance Evaluation of Subprocess Composition", "content": "Decomposing complex, long-horizon manipulation tasks into smaller subprocesses enhances learning performance, as evidenced by success rate and convergence speed improvements. This improvement is captured through the formalism of zig-zag diagrams, which provide a structured approach to understanding the relationship between sub-tasks within the larger task. In this framework, the zig-zag diagrams represent the sub-tasks composition, connecting states and transitions within the MDP. By systematically breaking down complex tasks, we enable more efficient learning and synthesis of solutions, leading to observed performance improvements.\nWe designed a robosuite environment wrapper using categorical constructs to oversee and transition between sub-tasks. This wrapper filters state vectors at each step, modifies actions per the defined sub-task MDPs, and allocates dense rewards $r'_{dense}$. Completing a sub-task triggers a $r_{task}$ = 10 reward and transitions to the subsequent sub-task. Upon task completion or environment termination, it reverts to the initial sub-task. This approach of breaking a long-horizon control task into shorter segments facilitates training individual RL agents for each segment. For comparison, we train using a direct robosuite environment with a similarly constructed dense reward. Both methods use soft actor-critic [18], an advanced model-free RL algorithm.\nWe conduct experiments using 5 random seeds for each setting.\nThe category-theoretic compositional RL performs well in training sample efficiency and final model performance (figure 2). In the block-lifting task, our method converges to a 100% success rate after 150k training steps, whereas the baseline method converges at around 225k steps. In a more challenging task like block-stacking, our method converges to over 90% success rate while the baseline method struggles to reach even a 50% success rate. The trend continues to the nut-assembly and can-moving tasks where our method consistently learns a better policy with fewer training steps.\nTask composition enables the reuse or recycling of existing trained sub-task policies. Because all four robosuite tasks involve the reach and lift sub-tasks, repetitive training can be avoided after training the block-lifting task. Aside from direct reuse, where the trained policies are directly used in the sub-task of a different environment, recycling could prove beneficial: the interactions and reward signals from the new sub-task are used to fine-tune the policy parameters. Reusing the reach and lift skills allows our method to start training directly from the place sub-task, significantly improving sample efficiency (figure 3). However, the final performance is lower than when compositional RL is trained from scratch. When a second block is present, the policies trained to lift a single block occasionally fail due to the robot hand getting stuck on the other block. When further fine-tuning is performed, the lifting policies initially trained on the block quickly adapt to the soda can with a different size.\nWe achieve higher success rates and faster convergence by breaking down complex, long-horizon tasks into systematic sub-tasks, as represented through zig-zag diagrams. Moreover, the reuse and recycling of trained sub-task policies highlight the adaptability of zig-zag diagrams, avoiding redundancy and further boosting efficiency. The experimental results reinforce that categorical formalism is a robust foundation for composition and structure in RL."}, {"title": "4.2.3 Limitations", "content": "A comprehensive compositional generalization benchmark is still lacking [28, 17]. This absence hinders the ability to systematically evaluate and compare the performance of different compositional RL algorithms across a standardized set of tasks. If such benchmarks existed, it would be straightforward to determine which algorithms are more effective at generalizing from their training environments to unseen scenarios. Furthermore, the need for standardized evaluation metrics for compositional generalization in RL adds another layer of complexity. Metrics that can accurately reflect the ability of an algorithm to leverage compositional structures in learning and decision-making processes are essential for advancing the field. This methodological gap means that current algorithm comparisons rely on inconsistent criteria or non-comparable tasks. We attempted to provide a common baseline that does not compare sparse reward structures with unfair dense reward structures. Future directions will attempt to derive a fair benchmark for compositional RL algorithms and compare compositional RL algorithms with our proposal based on categorical structures."}, {"title": "5 Related Work", "content": "Our work builds upon and extends a rich tradition in RL that explores the structure and abstraction in decision processes [23, 35, 36] and recent developments in applied category theory [1, 5, 6, 7, 11, 19, 20, 45, 46, 47]. Seminal contributions on minimalization have laid foundational concepts for understanding state and action abstractions in MDPs [16]. Similarly, the work on factored and propositional representations has been pivotal in advancing structured solution techniques and symbolic dynamic programming [9, 10]. Additionally, work on the theoretical underpinnings of structured solution techniques in RL provides a way to synthesize behaviors [43].\nIn hierarchical reinforcement learning (HRL), significant formal models have been developed that go beyond the heuristic layering of policies. These models provide structured approaches to defining subtasks and subgoals, facilitating multi-task learning and systematic problem decomposition [33, 13, 34, 31]. Our categorical approach aims to integrate these hierarchical structures within a unified mathematical framework, offering a complementary perspective on task composition and policy integration.\nBuilding on foundational principles, compositionality in RL has traditionally focused on temporal and state abstractions to execute complex behaviors and enhance learning efficiency through mechanisms such as skill chaining [40, 42, 22, 21, 28]. In contrast, our categorical formalism introduces a shift towards functional composition in RL. This approach leverages the denotational nature of category theory to decompose tasks into distinct behavioral functions, providing a more structured and mathematically rigorous framework for task decomposition than previously available.\nThis functional approach offers granularity and aligns with contemporary efforts in robotics and policy modularization. For instance, it complements methods used in robotics tasks [12], where decomposing complex behaviors into simpler, manageable units is crucial. Similarly, it supports the development of modular neural architectures for policy learning [29], where each module can be understood and optimized independently. Our framework enriches these efforts by providing a formal language of zig-zag diagrams, which aids in the systematic composition and decomposition of tasks and policies. This diagrammatic language not only enhances the interpretability of complex decision-making structures but also ensures that these structures can be rigorously analyzed and validated within a coherent theoretical framework.\nOur categorical formalism addresses the need for a unified and rigorous mathematical framework that can encapsulate and generalize the concepts of MDP homomorphisms and task composition. Unlike previous works, our approach leverages the semantics of category theory to provide a systematic structure for decomposing and recomposing tasks and policies. This is achieved through the introduction of categorical operations such as pushouts, which we prove exist for MDPs and serve as a novel method for task integration.\nWe extend the work on probabilistic representations in category theory, moving beyond the stochastic process descriptions [15, 44] and recent categorical treatments of MDPs [3, 14]. Our framework not only models the dynamics of decision processes but also provides a compositional toolset for functional decomposition in RL, which has been less explored in the existing literature.\nIn this paper, we examine how the categorical approach can enhance RL systems' modularity, scalability, and interoperability. By providing a rigorous mathematical structure for task and policy composition, our approach offers potential improvements in learning efficiency and adaptability in complex environments. However, we also acknowledge the challenges and limitations of applying abstract mathematical frameworks in practical RL scenarios."}, {"title": "6 Conclusion", "content": "In this work", "RL": "task synthesis, generalization, and interpretability, to name a few."}]}