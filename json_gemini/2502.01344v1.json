{"title": "PSSD: Making Large Language Models Self-denial\nvia Human Psyche Structure", "authors": ["Jinzhi Liao", "Zenghua Liao", "Xiang Zhao"], "abstract": "The enhance of accuracy in reasoning results of LLMs arouses\nthe community's interests, wherein pioneering studies investigate\npost-hoc strategies to rectify potential mistakes. Despite extensive\nefforts, they are all stuck in a state of resource competition demand-\ning significant time and computing expenses. The cause of the\nsituation lies in the failure of identifying the fundamental feature of\nthe solutions in this line, coined as the self-denial of LLMs. In other\nwords, LLMs should confidently determine the potential existence\nof mistakes and carefully execute the targeted correction. As the\nwhole procedure conducts within LLMs, supporting and persuasive\nreferences are hard to acquire, while the absence of specific steps\ntowards refining hidden mistakes persists even when errors are ac-\nknowledged. In response to the challenges, we present PSSD, which\nrefers to and implements the human psyche structure such that\nthree distinct and interconnected roles contribute to human reason-\ning. Specifically, PSSD leverages the recent multi-agent paradigm,\nand is further enhanced with three innovatively conceived roles:\n(1) the intuition-based id role that provides initial attempts based\non benign LLMs; (2) the rule-driven superego role that summarizes\nrules to regulate the above attempts, and returns specific key points\nas guidance; and (3) the script-centric ego role that absorbs all\nprocedural information to generate executable script for the final\nanswer prediction. Extensive experiments demonstrate that the\nproposed design not only better enhance reasoning capabilities, but\nalso seamlessly integrate with current models, leading to superior\nperformance.", "sections": [{"title": "1 Introduction", "content": "Recently, the NLP community has witnessed the booming devel-\nopment of LLMs, where many downstream tasks manifested new\nmilestones, especially in reasoning tasks [15, 18]. Albeit superior,\nLLMs still exhibit weakness in guaranteeing the reasoning cor-\nrectness. That is, LLMs tend to make up facts and details, hence\nmisleading the direction of inference and generating the errorneous\nresults [16, 31, 33].\nAmidst this backdrop, pioneering studies have investigated the\npost-hoc strategy, which emphasizes refining generated results, pri-\nmarily influenced by the concept of correcting based on mistakes [11].\nAccording to the achievement, these methods mainly can be put\ninto three categories: (1) Fine-tuning LLMs aims to proactively pre-\nvent the recurrence of mistakes by fine-tuning on previous correct\nand incorrect samples [1, 5, 19, 27]; (2) Leveraging tools iteratively\npolishes results through interaction with external assistance to cor-\nrect the mistakes in each step [17, 23, 25, 30]; (3) Multi-agent debate\ndefines multiple roles of LLMs, with each individually generating a\nresponse and engaging in multi-round debates to reach a consistent\nanswer [4, 6, 12, 29]. The training phase in first class does not meet\nthe timely demands in practical applications. Though methods in\nsecond class can provide immediate results, their performance re-\nlies heavily on availability of high-quality external resources. In\norder to ensure the timeliness and avoid the mentioned dependency,\nour research focuses on exploiting the inherent potential of LLMs,\nwhich belongs to last stream (vividly shown in Figure 3).\nAs aforementioned, current studies in multi-agent debate try to\nallocate several roles for agents initializing discussions, where the\nround of interaction depends on reach of the consistency and num-\nber of roles relies on the empirical study. These intuitive designs,\ninstead of approaching the fundamental features of learning from\nmistakes, are stuck in the resource competition (ref. Figure 5), in\nwhich satisfactory results are highly dependent on the times of in-\nvoking LLMs. To avoid the dilemma, we first identify the feature in\nthis category as: facilitating agents' self-denial, where the reasoning\ndirection of LLMs is guided and refined via their experience. These\nenhanced agents consciously exert control over the generation by\nappropriate interventions, corrections, or completions in reason-\ning, based on confidential determination. The process theoretically\nprevents a stack of resources and obtains gold results."}, {"title": "2 Related Work", "content": "This section briefs relevant efforts from three perspectives."}, {"title": "2.1 Fine-tuning LLMs", "content": "Methods in this approach treat LLMs as a supervisor to obtain\nfeedback for a provided mistake to fine-tune LLMs. STAR [28] em-\nphasizes the rationales leading to the correct results. With the gold\nannotations, an iterative generation strategy is employed to ob-\ntain the ideal reasoning path for each question to fine-tune LLMs.\nLEMA [1] focuses on inaccurate reasoning paths, and employs GPT-\n4 to generate details, reasons, and final answer for them. These\nsamples then perform as annotations for fine-tuning LLMs. Simi-\nlar to LEMA, GWFS [2] directly guides LLMs to generate harmful\nresponses and informs LLMs to evaluate its output with specific cri-\ntique. This mistake analysis data is then used for model fine-tuning.\nSelf-rethinking [19]pre-defines some error classes to provide LLMs\nwith typical correction examples to prepare fine-tuning samples.\nSALAM [20] designs an assistant LLM to interact with the main\nLLM and leverages the resulting conversations to fine-tune assistant\nLLM, thereby enhancing the supervisor's flexibility compared with\naforementioned approaches. The utilization of domain fine-tuning\ncan enhance the performance of LLMs; however, a new training\nphase encounters delayed response issue. Additionally, a substan-\ntial amount of computational powers are required to execute the\nlearning process."}, {"title": "2.2 Leveraging Tools", "content": "The main idea in this line is to leverage the feedback to verify\nLLMs' outputs. TRAN [23] accumulates rules from incorrect cases\nsummarized by LLMs to form a rule base. When encountering\nnew queries, it extracts relevant rules as external clues for the\nreasoning process. VE [30] transforms the attention to leverage\nexternal knowledge, e.g., Wikipedia and Google. The information\nperforms as the facts to verify the results and supports the re-\nanswer procedure. To put the thoughts deeply, ReAct [25] proposes\nan interaction framework to enforce LLMs to execute the thought-\naction-observation circle based on goolge engine. In each phrase,\nthe model will make a feedback-dependent decision and prepare\nfor the next state. Reflexion [17] tries reinforcement learning for\nreflection to further explore decision-making capability. External\nassistance can help users to obtain immediate results; however, the\nperformance is heavily reliant on the quality of knowledge source\nor effective tools. Once the assistance is inaccessible, the essential\ncomponents of these methods are compromised."}, {"title": "2.3 Multi-agent Debate", "content": "Several agents are defined to invoke discussion within the same\ntopic. LM vs LM [4] facilitates a multi-turn conversation between\nthe claim-generating LM and an examining LM, which introduces\nquestions to discover inconsistencies. Multiagent Debate [6] makes\nmultiple agents propose and debate their individual responses and\nreasoning processes over multiple rounds to reach a common final\nanswer. MAD [12] facilitates the expression of arguments among\nmultiple agents in a \"tit for tat\" manner, while a judge oversees\nthe debate process to obtain a final solution. Self-Contrast [29]\nemploys diverse agents to offer distinct perspectives on a query.\nSubsequently, a new agent compares and summarizes discrepancies\nto generate the final answer. The multi-agent debate framework\neffectively ensures timeliness and independence, but the omission of"}, {"title": "3 Proposed Method", "content": "This section presents an overview of the proposed method and\ndetailed design of each role."}, {"title": "3.1 Framework", "content": "As response to identified challenges, we follow the idea of human\npsyche and design three tailored roles. The overall framework of\nPSSD is illustrated in Figure 1.\nFor a given question, the intuition-based id role relies on LLMs'\n(e.g., GPT-4) inherent abilities to generate multiple reasoning paths\n(ref. Section 3.2). Subsequently, these intuitive attempts serve as\nfundamental materials for the rule-driven superego to judge and\ncriticize. To enhance reliability of LLMs' self-supervision determi-\nation, we introduce general persuasive rules summarization based\non the training samples from a methodological perspective. Con-\nsidering that even for human beings it is challenging to definitively\nassert the correctness of the original answer, these rules mainly\nhelp the agent in abstracting guiding key points based on the spe-\ncific question and its associated attempts (ref. Section 3.3). Third,\nthe script-centric ego role comprehensively synthesizes question,\noriginal results, and key points to construct a script. The script\nprovides step-by-step guidance for refining the potential mistakes\nin the reasoning process, thus facilitating the generation of the\nfinal answer (ref. Section 3.4).\u00b9 Last but not least, according to the\ntheory, the functions of above three roles possess sequential rele-\nvance. Consequently, we apply open-source LLMs (e.g., LLaMa and\nMistral) to fine-tune these roles into an integration (ref. Section 3.5)."}, {"title": "3.2 The Intuition-based Id Role", "content": "The intuition-based id role performs as an initial reasoning attempt\non a given question by leveraging the LLM's inherent abilities, as de-\nfined by Freud [7], to capture its intuitive responses as a foundation\nfor subsequent analysis.\nSpecifically, given a question q', we provide the LLM with a id\nrole prompt (i.e., Mid) with several examples as below:\nE = {Ei}=1 = {(q,a)}=1\nwhere z denotes the number of selected examples, q and a, used for\nfew-shot learning, denote the i-th question along with its labelled\nanswer from the training dataset.\nWe set the number of returned responses to l, and then Mid\ngenerates the initial attempts, formally,\nA = {aj}}j=1 = Mid(E, q').\n(1)"}, {"title": "3.3 The Rule-driven Superego Role", "content": "The rule-driven superego, representing internalized values and\nnorms [7], is responsible for evaluating and regulating reasoning\npaths A to ensure that specific key points are accurately acquired.\nTo strengthen the reliability of this process, we abstract a set of\nhighly relevant rules as a guideline to support generating key points.\nThese rules guide LLMs in identifying key points across various\nquestion types, thus providing a systematic framework that tran-\nscends individual responses. This framework helps LLMs overcome\ntheir limitations, especially when reasoning falls outside their fac-\ntual knowledge base."}, {"title": "3.3.1 Development of Rules.", "content": "A well-constructed set of rules is crit-\nical for effective key point generation. To develop these rules, we\nemploy a contrastive approach using GPT-4. For each question\nqf in the training set, we instruct GPT-4 to generate high-quality\nkey points K, which are then manually reviewed for accuracy.\nIn parallel, we instruct LLaMA-2-7B-Chat to generate suboptimal\nkey points K. The comparison between these two sets of outputs\nprovides a gradient that allows GPT-4 to learn what constitutes\na high-quality key point. This process is divided into two stages:\npattern extraction and rule summary."}, {"title": "Pattern Extraction.", "content": "For each question qf, we instruct GPT-4 to\ncontrast high-quality key points Kh with suboptimal ones K to\nidentify meaningful patterns P\u2081. These patterns explain the distinc-\ntions between the two key points, mathematically,\nPb = arg max P (T, KK)\nb\n= {t1, t2,..., tm},\n(2)\nwhere T represents all possible patterns, and the top m patterns\nare selected. We extract patterns for each question in the training\nset, and this process results in a pattern set P = {P1, P2, ..., P|P|}."}, {"title": "Rule Summary.", "content": "Based on extracted pattern set P, we instruct\nGPT-4 to categorize all questions Qt = {q1, q2, \u2026, qq} into sev-\neral types and generate corresponding rules for each type. These\nrules describe the key insights derived from patterns across similar\nquestions, which can guide LLMs from a methodological perspec-\ntive to better generate key points, formally,\nn\nU = arg max P (U | Q\u00b2, P)\n= {U1, U2,..., Un},\n(3)\nwhere U represents the rule set for all possible question types.\nThe rules summarized by this method are used to guide LLMs in\ngenerating key points.\u00b2"}, {"title": "3.3.2 Key Point Generation.", "content": "Using a predefined superego role prompt,\ni.e., MSuperego, we require the LLM to analyze reasoning attempts\nA and generate key points K for a given question q' under the\nguidance of rule U as below:\nK = Msuperego (U, q', A).\n(4)\nAs illustrated in Figure 1, the rule-driven superego role evaluates\ninitial reasoning attempts, identifying incorrect logic (e.g., key point\n2 in attempt 1) and guiding corrections in subsequent steps."}, {"title": "3.4 The Script-centric Ego Role", "content": "The script-centric ego role tries to mediate conflicts between the\nintuition-based id role and rule-driven superego role by striking a\nbalance between the original attempts and the obtained key points.\nThis process embodies the methodological guidance into a specific\nexecutable script that illustrates detailed steps for refining previous\nresults, which briefly involves three main steps."}, {"title": "3.4.1 Script Generation.", "content": "In detail, we provide a LLM with the pre-\ndefined ego role prompt (i.e., MEgo) to analyze the initial attempt\nA and key points K. The step-wise script S is generated as below:\nS = MEgo (q', A, K).\n(5)\nThe script synthesizes LLMs' reasoning capabilities with the\nsummarized rules, distinguishing from key points in terms of levels.\nAs depicted in Figure 1, key points emphasize what to do for en-\nhancing the precise of the answer, such as \"Ensure that every step in\nthe reasoning is based on verified information\", The script focuses\non how to achieve these targets like \"Verify that the adaptation of\nthe film 'Snatch' into a television series indeed occurred\"."}, {"title": "3.4.2 Script Execution.", "content": "In this step, the script-centric ego role ex-\necutes the script step-by-step, ensuring that LLMs follow the rea-\nsoning path outlined in the generated script. In order to keep the\nexecution process smoothly, we integrate the script with the key\npoints as input to guide MEgo, formally,\nS' = MEgo (q', K, S),\n(6)\nwhere S' denotes the answered script as illustrated in Figure 1.\nAfter the execution, the script further confirms details relevant\nto the question and provides supplementary descriptions. These\nreferences contribute to the accuracy of the final decision."}, {"title": "3.4.3 Answering.", "content": "Finally, the script-centric ego role formulates the\nfinal answer based on all relevant process information generated in\nthe previous steps. Specifically, we instruct the LLM to synthesize\nthe question q', key points K, script S, and script execution S' to\ngenerate the final result R. This process is formalized as:\nR = Mego (q', K, S, S')."}, {"title": "3.5 The Merge of Three Roles", "content": "To mitigate resource competition in multi-agent debate, we further\nexplore effective management strategy for frequency of invoking\nLLMs. In accordance with the internal unity of three roles, they\nare merged into a whole through the utilization of open-source\nLLMs. As fine-tuning process performs as an adhesive, combining\ndistinct roles, the modified method is called PSSD-SFT to differenti-\nate it from PSSD. Subsequently, we discuss the construction of the"}, {"title": "3.5.1 Construction of Training Dataset.", "content": "First of all, the fine-tuning\nprocess is essential a supervised learning procedure, thereby neces-\nsitating annotated samples as training labels. To construct required\ndataset, we use publicly available reasoning datasets as the data\nsource. e. For each question q in training set, we apply GPT-4 as\nthe fundamental model of PSSD framework to generate reasoning\nrecords, which include the initial attempts, key points, script, script\nexecution, and final answer.\n\u00b3\nTo ensure accuracy, these reasoning records undergo a rigorous\nannotation process:\n\u2022 Initial Annotation. Three graduate students initially annotate\nthese reasoning records and carefully review and improve\nthem based on the groundtruth.\n\u2022 Review and Consensus. Each record is thoroughly reviewed\nby an additional annotator. Any discrepancies are thoroughly\ndiscussed until a consensus is reached on the final annota-\ntion.\n\u2022 Structured Combination. Each annotated reasoning record\nis integrated into a predefined training sample template.\nFinally, This process can be formally defined as follows:\nd\u2081 = Template[q \u2295 A\u266d \u2295 K\u2081 \u2295 S\u2081 \u2295 S\u2081 \u2295 R\u266d],\nwhere q, Ab, K\u266d, St, S\u2081 and R\u2081 denote the given question, corre-\nsponding initial attempts, key points, script, script execution and\nfinal answer respectively. \u2295 denotes the operation of concatenation."}, {"title": "3.5.2 Fine-Tuning LLMs.", "content": "To further conserve computational re-\nsources, we employ the LoRA [10] method for fine-tuning the LLMs.\nOn one hand, considering the intuition-based id role purely relies\non the capability of the fundamental model, its enhancement em-\nphasizes the improvement of LLMs. Therefore, we fine-tune the\nfirst LoRA model using question-reasoning path pairs as training\ndata. This process can be formulated as:\nW1: L1 =\n11\nL\n\u2211 log P(xiq, a\u00b9),\ni=1\n(8)\nwhere W\u2081 represents weights of the first LoRA model, L\u2081 represents\nits loss function, L is the token length of the sequence ab, xi is the\ncurrently predicted response token, qf and afi are the question\nand the response tokens before xi.\nOn the other hand, acquired structured D = {d1, d2, ..., d|D| }\naims to provide supervised signals for training other more com-\nplicated roles (i.e., rule-driven superego and script-centric ego).\nThus, it is employed to fine-tune another LoRA model focusing the\ngeneration of elements in d, formally,\nL\nW2: L2 = -log P(xilqf, Ab, di),\ni=1\n(9)\nwhere As and di are the initial attempt with 5 reasoning paths\nand the response tokens before xi."}, {"title": "4 Experiments", "content": "This section provides a detailed presentation of the experiments,\nalong with an in-depth analysis of the results."}, {"title": "4.1 Experiment Setup", "content": "4.1.1 Datasets. We conduct experiments on both textual and math-\nematical reasoning tasks using four datasets: ADVHOTPOTQA, 2WIKI-\nMULTIHOPQA, GSM8K, and MATH.\nFor textual reasoning, we utilize the following datasets: (1) AD-\nVHOTPOTQA\u2074 [26] is a challenging subset derived from the multi-\nhop question answering dataset HOTPOTQA [24], where the correct\nand incorrect predictions are balanced; (2) 2WIKIMULTIHOPQA\u2075 [9]\nis a multi-hop question answering dataset that leverages the struc-\ntured format of Wikidata and applies logical rules.\nFor mathematical reasoning, we use: (1) GSM8K\u2076 [3], a bench-\nmark dataset of grade-school-level math word problems, designed\nto evaluate mathematical reasoning. Each question is accompanied\nby a detailed step-by-step solution. (2) MATH\u2077 [8] is a collection of\nchallenging high school-level math problems, aimed at evaluating\nadvanced mathematical reasoning and problem-solving skills."}, {"title": "4.1.2 Competitors.", "content": "To put our results in perspective, we apply two\nclasses of competitive baselines for demonstration.\nIn the first class, we select representative methods from leveraging-\ntools and multi-agent debate categories to evaluate PSSD, as these\nmethods commonly employ closed-source LLMs (e.g., GPT-4) as the\nfundamental model: (1) Standard Prediction (Stand) directly predicts\nthe answer for input using manually provided examples; (2) Original\nCoT (CoT-Ori) [22] generates a reasoning path before predicting the\nfinal answer; (3) CoT with Self-Consistency (CoT-SC) [21] samples\nfive reasoning paths and selects the final answer based on consis-\ntency values; (4) ReAct [25] enhances CoT reasoning by integrating\nWikipedia API to improve factual accuracy; (5) VE [30] is a post-\nediting framework that leverages external knowledge to increase"}, {"title": "4.1.3 Metrics.", "content": "We evaluate the performance of PSSD via three met-\nrics: (1) Exact Match (EM) measures the percentage of predictions\nthat exactly match the ground truth, which evaluates the answering\nability of models; (2) Potential Match (PM) measures the conditional\nprobability of accurately selecting the correct answer from original\nattempts wherein the gold answer already exists, which evaluates\nthe ability to distinguish between correct and incorrect results; (3)\nRectified Match (RM) measures the conditional probability of origi-\nnal incorrect predictions that is finally corrected, which evaluates\nthe capability to correct."}, {"title": "4.1.4 Implementations.", "content": "To evaluate the effectiveness of PSSD and\nPSSD-SFT, we select both leading closed-source and powerful open-\nsource LLMs as baselines. For closed-source LLMs, we use GPT-4-\nturbo\u2078 [14], accessed via its APIs. For open-source LLMs, select the\npowerful open-source LLMs Mistral-7B-Instruct-v0.2\u2079 and LLaMA-\n3-8B-Instruct\u00b9\u2070 as baseline models. In order to ensure fairness\nin our experiments, the same foundation is utilized to support\ncompetitors for the comparison. The experiments are performed\nwith 2* RTX 3090. During the training phase of PSSD-SFT, we\nutilize the AdamW optimizer [13] with \u03b2\u2081 = 0.9 and \u1e9e2 = 0.999.\nThe learning rate is set to 1e-6, with a 0.1 ratio of warm-up steps\nand linear decay. We configure the maximum input length to 4,096\ntokens and establish a training batch size of 4. The entire training\nprocess is completed within 4.5 hours, and we employ the final\ncheckpoint for subsequent evaluations."}, {"title": "4.2 PSSD Results", "content": "As shown in Table 1, PSSD consistently achieves state-of-the-art\nresults across nearly all tasks in all metrics, demonstrating the su-\nperiority and generalizability of our design. The methods, such as"}, {"title": "4.3 PSSD-SFT Results", "content": "As shown in Table 2, PSSD-SFT achieved competitive performance\non all benchmarks, particularly excelling in mathematical reasoning,\nwhere it delivered state-of-the-art results.\nIn comparison, the results of AugGPT and LEC indicate that ad-\nditional LLM-generated training data does not always yield positive\noutcomes. For instance, on 2WIKIMULTIHOPQA and GSM8K, this\napproach even underperformed compared to the dataset's built-in\nCoT training set. The result might be attributed to biases introduced\nby LLM-generated data, which can impair the fine-tuned LLMs'\nspecific reasoning capabilities. Mistake Tuning shows promise by\nenhancing LLMs' ability to distinguish between correct and incor-\nrect responses. However, the contrastive-style fails to approach\nLLMs' self-denial mechanism. Therefore, the model merely learns\nsuperficial features of mistakes."}, {"title": "4.4 Ablation Study", "content": "To analyze effects of different roles in PSSD, we perform the ablation\nstudy and results is presented in Table 3. Due to the sequential\nfeature of PSSD, in each setting, we instruct LLMs to derive a final\nanswer based on the information generated by remaining roles.\nIn comparison with the complete model, the removal of specific\nroles results in performance drop. Notably, the whole rule-driven\nsuperego role provides the increase of 1.92% in ADVHOTPOTQA and\n2.40% in GSM8K, which supports the significance of first identified\nchallenge. Once the model is able to confidentially and convincingly\nsupervise its generation, it starts to engage in self-denial. The fact\nthat the introduction of summarized rules improve performance by\n0.64% in ADVHOTPOTQA and 0.60% in GSM8K indicates a strongly\npositive correlation between persuasiveness of references and the\naccuracy of results. Moreover, the equipment of the script-centric\nego role contributes to a 1.63% improvement in ADVHOTPOTQA and\na 1.20% improvement in GSM8K, illustrating the value of handling"}, {"title": "4.5 Compatibility Study", "content": "To demonstrate that PSSD is seamlessly orthogonal to other cat-\negory, we evaluate it by incorporating leveraging-tools methods,\nsuch as VE and ReAct. During the final answer generation step\nin PSSD, we set the number of returned candidate texts to 5 and\napply above methods to assist in answering questions when its\nconsistency values fall below 3. The result is provided in Table 4.\nThe results demonstrate that when combined with VE or ReAct,\nPSSD can significantly enhance the reasoning capabilities of LLMs.\nThis illustrates strong compatibility or PSSD with external retrieval\nsystems, thereby highlighting its adaptability and effectiveness\nin leveraging additional knowledge. The fact suggests a future\ndirection for further optimizing the proposed methodology. Besides,\nthe observed increase (i.e., 3.58%) in 2WIKIMULTIHOPQA combined\nwith ReAct and the highest result substantiate our hypothesis that\nthe leverage of Wikipedia determines the best result of ReAct in\n2WIKIMULTIHOPQA (ref. Section 4.2)."}, {"title": "4.6 Consistency Analysis", "content": "To explore the confidence of PSSD in its determination, we use\nkernel density estimation [21] to analyze the confidence distribution\nof different methods in generating outputs. The results, visualized\nin Figure 2, show distinct distribution patterns for each method.\nFrom a global perspective, CoT-SC, ReAct, and VE present a bi-\nmodal distribution, while PSSD displays a right-skewed distribution.\nThe distribution illustrates that PSSD instills greater confidence in\nits determinations, which might be derived from the rule-driven\nsuperego role and the script-centric ego role. More specifically,\nPSSD exhibits a predominantly right-skewed distribution, with the\nhighest peak value in correct samples compared to others. In incor-\nrect samples, however, the distribution presents a bimodal feature,\nindicating that PSSD still make wrong prediction with higher con-\nfidence. The situation is mitigated when integrated with ReAct and\nVE, and we leave the further improvement in this direction as the\nfuture work."}, {"title": "5 Conclusion", "content": "In this paper, we propose PSSD a novel and comprehensive ap-\nproach to enhance reasoning abilities of LLMs via acquiring self-\ndenial. By identifying the challenges, our method builds on the idea\nof human psyche structure and introduces three tailored roles. The\nintuition-based id role tries to provide initial attempts based on\nLLMs. Subsequently, the rule-driven superego role aims to increase\nthe precision of judgement via summarized rules. The key points of\na specific question are generated as clues for the next phase. Finally,\nthe script-centric ego role focuses on executable scripts to guide the\nfinal refinement, wherein it synthesizes attempts and key points to\ncomplete detailed execution. Besides, we merge three roles into an\nintegration by proposing two-stage fine-tuning strategy to evaluate\nthe resource-friendliness of PSSD. Comprehensive experiments\ndemonstrate that PSSD and PSSD-SFT outperform competing mod-\nels in all primary category, and can be fused to retrieve systems for\nfurther enhancement in reasoning accuracy."}, {"title": "A.1 Resource Analysis", "content": "PSSD inspires LLMs to engage in human psyche by adopting a\nmulti-agent debate paradigm that is based on the distinct roles\nof intuition-based id, rule-driven superego, and script-centric ego,\nrather than simply stacking numerous roles or iterating. The under-\nlying assumption is that PSSD surpasses Self-Contrast in terms of\nboth accuracy and stability. To validate this assumption, we conduct\nan experiment using 200 samples from the ADVHOTPOTQA dataset,\ncomparing the results of PSSD and Self-Contrast on each sample."}, {"title": "A.3 Answering Ability Analysis", "content": "The focus lies in harnessing the potential of LLMs through learn-\ning from mistakes, while it remains crucial to accurately discern\nthe original questions that can be correctly answered. Hence, ex-\ntensive experiments are conducted to closely observe the situation,\nas depicted in Figure 5.\nEvidently, PSSD consistently achieves accurate predictions across\nall answer types and maintains a high level of consistency to the\noriginal correct determination (as shown in the right bar). For the\nquestions in missing answer type, PSSD provides more correct pre-\ndictions than the incorrect ones while minimizing the number of\nunresolved cases. This indicates that the proposed method enables"}, {"title": "B Detailed Statistics for Data", "content": ""}]}