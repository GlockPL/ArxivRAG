{"title": "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "authors": ["Yanjun Chen", "Dawei Zhu", "Yirong Sun", "Xinghao Chen", "Wei Zhang", "Xiaoyu Shen"], "abstract": "Reinforcement Learning from Human Feedback significantly enhances Natural Language Processing by aligning language models with human expectations. A critical factor in this alignment is the strength of reward models used during training. This study explores whether stronger reward models invariably lead to better language models. In this paper, through experiments on relevance, factuality, and completeness tasks using the QA-FEEDBACK dataset and reward models based on Longformer, we uncover a surprising paradox: language models trained with moderately accurate reward models outperform those guided by highly accurate ones. This challenges the widely held belief that stronger reward models always lead to better language models, and opens up new avenues for future research into the key factors driving model performance and how to choose the most suitable reward models. Code and additional details are available at https://github.com/EIT-NLP/AccuracyParadox-RLHF.", "sections": [{"title": "1 Introduction", "content": "Language models (LMs) have made remarkable progress, achieving close-to-human capabilities in a wide range of tasks. While traditional fine-tuning has been effective, it often suffers from exposure bias, where models are trained on ground truth data rather than their own predictions, leading to inconsistencies during generation. Additionally, fine-tuning lacks the ability to optimize for sequence-level rewards, limiting its effectiveness in capturing complex, human-like preferences. RLHF addresses these limitations by incorporating feedback from humans, allowing models to generate more contextually relevant and aligned outputs."}, {"title": "2 Motivation and Problem Settings", "content": "Motivation. Findings indicate that the strength of reward models in RLHF does not consistently correlate with improved language model performance, challenging the assumption that stronger reward models always lead to better outcomes. Understanding the dynamic relationship between reward model accuracy and language model performance is essential for optimizing RLHF in complex NLP tasks. This study posits that there exists an optimal range of reward model accuracy that maximizes language model performance. Therefore, the primary aim of this research is to identify this optimal range and examine its implications for various NLP applications.\nProblem Settings. This study investigates the effect of reward model strength on language model performance in RLHF, focusing on tasks that evaluate the factuality, relevance, and completeness of generated text. Specifically, reward model strength is defined by binary classification accuracy on test sets, and language model performance is measured using high-accuracy, independent reward models.\nFormally, for a language model trained with RLHF, this study analyzes how the reward model's classification accuracy (SRM) and the number of training steps ($\\tau$) affect language model performance ($P_{LM}$). This relationship is mathematically represented by:\n$P_{LM} = f(S_{RM}, \\tau)$                                                                                                                      (1)\nThe objective is to determine the optimal conditions that maximize language model performance across various tasks, providing insights for the development of more effective RLHF strategies in NLP."}, {"title": "3 Experiment and Results", "content": "3.1 Basic Experimental Setup\nModels. We examine three models from the T5 language model family: T5-small, T5-base, and T5-large. Each model underwent supervised fine-tuning (SFT). Reward models were based on Longformer-base-4096, suitable for processing long sequences, necessary for tasks requiring extensive context. These models were trained for tasks involving factuality, relevance, and completeness, with training steps and accuracy ranges summarized in Table 1.\nDatasets. The QA-FEEDBACK dataset, derived from the ASQA dataset, is used for this study. This dataset focuses on generating long-form answers to ambiguous factual questions in an open-domain setting. The data is split into 3,853/500/948 for training, validation, and testing, requiring the generation of detailed answers from multiple knowledge passages.\nHyperparameter Settings. We follow the hyperparameter settings recommended by Wu et al., whose configuration has been specifically designed and empirically validated for RLHF tasks involving QA-feedback. These settings are selected to ensure an optimal trade-off between model performance and training stability, based on prior experimental findings. For a detailed description of all hyperparameters used in the experiments, please refer to Appendix D.\nTraining and Evaluation Paradigm. Following common practice, we begin by fine-tuning LMs, followed by applying RLHF using Proximal Policy Optimization (PPO). In addition, a separate instance of the T5-base model was specifically initialized as the value model for"}, {"title": "3.2 Are High-Accuracy and Deeply Trained Reward Models Always the Best?", "content": "Setup. Building on the Basic Experimental Setup, reward models for relevance, factuality, and completeness from the QA-FEEDBACK dataset were used in PPO training. Performance was assessed at regular intervals, and top-performing instances were identified and visualized in three-dimensional plots.\nResults. Figures 1 to 3 show that optimal language model performance is achieved using reward models with moderate accuracy and an appropriate number of trained steps. For the relevance task, the T5-small model performed best with moderately accurate reward models, effectively mitigating the risk of overfitting. Similarly, the results for factuality emphasized the importance of maintaining balanced reward model accuracy to prevent overfitting and ensure reliable outcomes. These findings suggest that overly accurate reward models can result in overfitting, which impairs the generalization ability of LMs. These trends were consistent across the T5-base and T5-large models, further supporting the conclusion that moderate accuracy in reward models strikes the best balance between training stability and performance."}, {"title": "3.3 How Do Best and Most Accurate Reward Models Differ?", "content": "Setup. This evaluation utilized three models: T5-small, T5-base, and T5-large, to compare the best-performing and most accurate reward models across relevance, factuality, and completeness tasks. The analysis focused on understanding the differences in reward behavior during training for each model. While the primary analysis in this section is based on the T5-small model, similar trends were observed with the T5-base and T5-large models, whose results are provided in the appendix for reference.\nResults. Figures 4, 5, and 6 illustrate the distinct strategies of the best-performing reward models compared to the most accurate models using the T5-small model. For the relevance task, the best-performing reward model provided higher and more variable rewards (Figure 4), indicating an aggressive approach that likely stimulated the generation of more relevant outputs. In the factuality task, this model maintained higher mean rewards with less variability (Figure 5), promoting factual accuracy. Conversely, for the completeness task, it employed a conservative strategy with lower average rewards but greater variability (Figure 6).\nAnalysis. Moderately accurate best-performing reward models typically align rewards with task requirements. In both relevance and factuality tasks, these models provide higher and more varied rewards, thus encouraging the generation of more relevant and accurate outputs. This variability allows LMs to explore a broader range of responses, improving the quality of the generated text. Conversely, in completeness tasks, a conservative strategy with lower average rewards but greater variability helps ensure thorough and comprehensive text evaluation. The trends observed in T5-small models are consistent with those seen in T5-base and T5-large models, further supporting the conclusion that moderate accuracy in reward models effectively balances overfitting and underfitting. Detailed results for T5-base and T5-large can be found in the Appendix B."}, {"title": "3.4 How Do Best and Most Accurate Rewards Impact Models?", "content": "Setup. This section evaluates the impact of reward models on the training dynamics of T5-small, T5-base, and T5-large models in relevance, factuality, and completeness tasks, with a focus on KL divergence trends to assess stability and adaptability. While the results presented here focus on the T5-small model, similar trends were observed for the T5-base and T5-large models, whose results are provided in the appendix.\nKL Divergence and Its Role in RLHF KL divergence (Kullback-Leibler divergence) is a measure of how one probability distribution P diverges from a second, expected probability distribution Q. It is commonly used in reinforcement learning to constrain the difference between the current policy and a reference policy during training. Mathematically, KL divergence is defined as:\n$D_{KL}(P || Q) = \\sum_i P(i) \\log \\frac{P(i)}{Q(i)}$                                                                                                                         (2)\nIn the context of RLHF, KL divergence serves as a regularization term to prevent the trained policy from deviating excessively from the reference policy. This constraint helps to stabilize the training process by reducing the chance of reward hacking or reward gaming, where the model could exploit the reward system without truly improving performance.\nResults. Comparing KL divergence trends revealed significant differences in how LMs aligned with the training data. For the relevance task, the best reward model resulted in consistently lower KL divergence and variance, indicating stable alignment (Figure 7). In the factuality task, the best reward model exhibited higher mean KL divergence but lower variance, suggesting a consistent yet varied alignment process (Figure 8). For the completeness task, the best reward model showed higher mean and variance in KL divergence, indicating a flexible approach suitable for evaluating complex texts (Figure 9).\nAnalysis. Best-performing reward models, which are typically of moderate accuracy, create a balanced training environment that facilitates both stability and adaptability. In relevance and factuality tasks, these models encourage stable learning, enhancing the relevance and accuracy of outputs. For the completeness task, the flexibility in handling complex texts is demonstrated by higher variance in KL divergence. The observed trends in T5-small models were consistent with those seen in T5-base and T5-large models, further validating the conclusion that moderate accuracy in reward models effectively balances overfitting and underfitting. Detailed results for T5-base and T5-large models can be found in the Appendix C."}, {"title": "4 Conclusion and Future Work", "content": "This study demonstrates that LMs trained with moderately accurate reward models in RLHF achieve optimal performance, challenging the conventional belief that higher accuracy is always more beneficial. The results show that moderately accurate reward models offer more task-aligned feedback and foster a balanced, stable training process, promoting better generalization. This research highlights the limitations of relying exclusively on highly accurate reward models, as excessive focus on accuracy may lead to suboptimal outcomes. In future work, it will be crucial to further explore the potential overfitting of reward models, particularly in their ability to generalize to out-of-distribution (OOD) tasks. Techniques such as regularization, data augmentation, and explicit OOD evaluation will be key areas of investigation to enhance the robustness of reward models across diverse scenarios and ensure their effectiveness in guiding LMs in broader, more complex NLP tasks."}, {"title": "Limitations", "content": "Dataset Constraints. The conclusions are drawn from the QA-FEEDBACK dataset, which is specialized in generating long-form responses to factual inquiries. This focus may limit the generalizability of the results, necessitating validation across various datasets, including those pertaining to conversational and question-answering contexts.\nModel Scope. The evaluation utilized T5 models of different scales for initial validation. Future investigations should incorporate more complex models, such as Llama2, to gain deeper insights and verify the robustness of the proposed methodologies across a broader range of model architectures.\nReward Model Variations. This study did not explore the impact of different reward model sizes and architectures on RLHF performance. The reward models used were based on a single architecture, which may limit the applicability of the findings. Future research should systematically investigate how variations in reward model size, capacity, and design affect the learning process, generalization, and overall RLHF performance, particularly in diverse NLP tasks. Understanding the influence of these factors will be crucial for developing more robust and scalable reward models that can generalize across a wider range of applications."}]}