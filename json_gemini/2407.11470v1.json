{"title": "Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models", "authors": ["Jiasheng Zheng", "Boxi Cao", "Zhengzhao Ma", "Ruotong Pan", "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Le Sun"], "abstract": "In recent years, researchers have proposed numerous benchmarks to evaluate the impressive coding capabilities of large language models (LLMs). However, existing benchmarks primarily focus on assessing the correctness of code generated by LLMs, while neglecting other critical dimensions that also significantly impact code quality. Therefore, this paper proposes the RACE benchmark, which comprehensively evaluates the quality of code generated by LLMs across 4 dimensions: Readability, maintainability, Correctness, and Efficiency. Specifically, considering the demand-dependent nature of dimensions beyond correctness, we design various types of user requirements for each dimension to assess the model's ability to generate correct code that also meets user demands. We evaluate 18 representative LLMs on RACE and find that: 1) the current LLMs' ability to generate high-quality code on demand does not yet meet the requirements of software development; 2) readability serves as a critical indicator of the overall quality of generated code; 3) most LLMs exhibit an inherent preference for specific coding style. These findings can help researchers gain a deeper understanding of the coding capabilities of current LLMs and shed light on future directions for model improvement.", "sections": [{"title": "1 Introduction", "content": "The impressive coding capabilities demonstrated by Large Language Models (LLMs) are reshaping the landscape of software development (Zheng et al., 2023c,b; Fan et al., 2023), attracting significant attention from researchers. To accurately measure and compare the coding capabilities of various large models, numerous benchmarks have been proposed to evaluate the code generation (Chen et al.,"}, {"title": "2 Related Work", "content": "The outstanding code generation capabilities exhibited by LLMs have attracted considerable attention from researchers (Wang et al., 2021; Li et al., 2022; Fried et al., 2022; Xu et al., 2022; Roziere et al., 2023; Zheng et al., 2023a). Some representative Code LLMs, such as CodeX (Chen et al., 2021), CodeGen (Nijkamp et al., 2022), and AlphaCode (Li et al., 2022), have achieved notable performance in code generation, program repair and code translation. Currently, research on LLMs for code primarily focuses on data and pretraining methods. For training data collection, WizardCoder (Luo et al., 2024) introduce code instructionfollowing training constructed by Evol-Instruct to enhance the capabilities of Code LLMs. For pretraining methods, StarCoder (Li et al., 2023a) and DeepSeek-Coder (Guo et al., 2024) incorporate fill-in-the-middle training task to enhance the model's capability to handle various structural arrangements in code. With the rapid advancement of Code LLM capabilities, there is an increasing demand for reliable and comprehensive code evaluation benchmarks."}, {"title": "3 RACE Benchmark Construction", "content": "The philosophy of our framework design comes from the demands for code quality in software engineering (B\u00f6rstler et al., 2023). Firstly, we summarize multiple representative factors for each dimension based on their respective quality definitions (Curtis et al., 2022; Nistala et al., 2019; Sadeghzadeh Hemayati and Rashidi, 2017). Secondly, we design several reasonable customized requirements for each factor and integrate them into task descriptions, requiring the model to generate code that is both correct and meets these requirements. Information on the detailed evaluation data is presented in Table 1. Finally, leveraging static analysis and runtime monitoring techniques, we develop evaluation metrics tailored to each factor to achieve accurate and efficient evaluation. The specific designs of each instruction refer to Appendix A."}, {"title": "3.1 Correctness", "content": "To investigate the impact of incorporating customized instructions on code correctness, we evaluate the accuracy of the LLM-generated code on the original benchmark tasks and also calculate the accuracy when provided with instructions containing customization requirements.\nTo thoroughly investigate the impact of customized instructions across various tasks, we select the following datasets: HumanEval+ and"}, {"title": "3.2 Readability", "content": "In real-world development scenarios, code is required to adhere to a consistent style to ensure comprehensibility and reduce the time cost of maintaining the code, which refers to code readability (B\u00f6rstler et al., 2023). Specifically, the code length is the most straightforward aspect of style; excessively long lines of code can lead to incomplete screen display, severely affecting readability. Meanwhile, good and consistent naming styles help developers quickly understand the functionality of interfaces, and comments assist in rapidly comprehending the implementation logic of the code. Therefore, we summarize the code readability into three representative factors: Length, Naming Convention, and Comment. Based on the real-world development requirements, we collect corresponding customizable options for different factors.\nFor the Length factor, the readability requirements for code length vary due to the differences in display scales across different user scenarios. Therefore, we refer to the PEP8 style for Python, and define the following user requirements concerning code length: (60, 20), (70, 30), and (79, 40), with the parentheses corresponding to the maximum line length and the maximum lines of functions, respectively. For the Naming Convention"}, {"title": "3.3 Maintainability", "content": "The maintainability of code significantly impacts the long-term health of software and the efficiency of development teams. Many quality models propose empirical quantitative equations for maintainability. Simultaneously, the Single Responsibility Principle (SRP) is a crucial part of code design principles to avoid excessive functional coupling."}, {"title": "3.4 Efficiency", "content": "In most applications, code efficiency is directly linked to user experience or business process efficiency. Generally, efficiency is assessed using time complexity and space complexity. Based on this principle, we define them as factors. Due to varying user-side hardware conditions, achieving a balance between execution time and memory usage, or optimizing one of these aspects to the extreme, is a common practice to ensure code efficiency. Recognizing these scenarios, we gather 101 cases from LeetCode programming problems designed to simulate such conditions. These cases are customized with specific time complexity requirements, space complexity requirements, or both, to evaluate the extent to which the LLM-generated code meets the efficiency requirements.\nTo measure code efficiency, we propose the Normalized Index (NI), i.e., to measure the degree to which the generated code satisfies the complexity requirement. Given two solution codes with time and space complexity $C_T, C_S^1$ and $C_T, C_S^2$, respectively, where $C_T^1$ and $C_S^1$ are better, and given their total running time $T_1, T_2$ ($T_1 < T_2$) and memory usage $S_1, S_2$ ($S_1 > S_2$) on all test cases. Now there is a code $\\hat{C}$ to be evaluated, which has a running time $\\hat{T}$ and memory usage $\\hat{S}$, with requirements $C_T^*, C_S^*$, then the normalized index is:\n$NI_T = 100 \\cdot Clip(1 - \\frac{\\hat{T}-T_1}{T_2-T_1}, 0, 1)$\n$NI_S = 100 \\cdot Clip(1 - \\frac{S_2 - \\hat{S}}{S_2-S_1}, 0, 1)$\nNIT indicates the degree of time complexity toward $C_T^*$, and NIS indicates the degree of space complexity toward $C_S^*$."}, {"title": "4 Experiments", "content": "In this section, we conduct a detailed evaluation of 18 Code LLMs and obtain several valuable findings. We first introduce the input formats and inference configurations for code generation tasks, along with the selection of LLMs. Subsequently, we present the overall experimental findings and conduct further analysis of the results to derive meaningful conclusions. The detailed experimental results are shown in Appendix B."}, {"title": "4.1 Settings", "content": "Task formats We construct the different prompts based on the completion style and chat style, to better induce the LLMs to accomplish the corresponding tasks. In the inference process, we use a greedy strategy and set the temperature to 0.\nModels We select several state-of-the-art Code LLMs ranging in different sizes, both open and closed source, including DeepSeek-Coder (Guo et al., 2024), CodeLlama (Roziere et al., 2023), WizardCoder-Python (Luo et al., 2024),"}, {"title": "4.2 Correlation Analysis Across Dimensions", "content": "To conduct a more in-depth analysis of how various factors across different dimensions influence overall code quality, we analyze the correlations between different factors in each model. Specifically, we first compute the proportion of the generated code that is both correct and follows customized instructions across 8 factors for 18 Code LLMs. Subsequently, we calculate Pearson correlation coefficients between these factors."}, {"title": "4.3 Preference Bias of Code LLMS", "content": "To investigate whether the model's inherent preferences affect its ability to follow user instructions, we conduct a more fine-grained comparison of various incorporated user requirements. Specifically, we focus on naming conventions, requiring LLMs to consistently use either camel-case or snake-case for both function names and variable names. Additionally, regarding code length, LLMs are tasked with generating single-line lengths not exceeding 60, 70, and 79 characters, as well as ensuring that individual method lengths do not exceed 20, 30, and 40 lines. For loop structures, the requirement is for LLMs to use only either for or while statements to implement necessary loop constructs. Finally, we calculate the proportion of LLM-generated code that follows these customized requirements, i.e. the rate of instruction-following (IF).\nFigure 6 demonstrates the IF rates of 18 LLMs across all the customized requirements above. We find that the majority of LLMs exhibit an inherent preference bias towards generating code in specific styles. This bias often results in these LLMs being unable to follow user instructions effectively if the requested style differs from that prevalent in their training data. Specifically, for naming conventions, Python conventionally employs snake-case for function and variable names. When LLMs are requested to use camel-case, most LLMs, such as CodeLlama and CodeQwen, almost fail to comprehend and fulfill this requirement, with instruction-following rates below 20%. For code length, a 79-character single-line length limit is a common style in Python. When dealing with more stringent requirements, the IF rates of most instruct-type LLMs drop by nearly 20%, while DeepSeek-Coder-V2-236B maintains the best instruction-following rates. For loop structures, all LLMs except GPT series and DeepSeekCoder-V2-236B exhibit a pronounced tendency to use \"for\" statements. When tasked with using \"while\" statements, LLMs struggle to transform between different statements, reflected in the IF rates generally below 70%. These findings imply that most LLMs may simply learn the inherent patterns of the next token from examples, without a clear understanding of the logic of code comprehension. However, GPT-3.5, GPT-4o, and DeepSeek-CoderV2-236B perform well in this aspect, with an IF rate above 90%. Such preference bias can lead to the ossification of code style in Code LLMs, thereby hindering their ability to meet specific realworld project requirements and consequently affecting the adaptability and scalability of generated code. This issue may be more pronounced in programming languages like Perl, JavaScript, and PHP, where there is no strict, widely accepted standard code style."}, {"title": "5 Conclusion", "content": "We present the RACE benchmark, a multidimensional evaluation framework for code generation, including correctness, readability, maintainability, and efficiency. The RACE benchmark evaluates whether LLMs can generate code that is both correct and meets customized requirements, based on the selection of factors within each dimension and the customized requirements designed for each factor. Based on further experiments in 18 representative LLMs, we find that the present capabilities of LLMs in generating code of high quality as needed still fall short of the demands in software development. Additionally, code readability serves as a pivotal indicator of the overall quality of generated code. Our research highlights the critical importance of improving the multidimensional quality of generated code. Future efforts should focus on improving the ability of LLMs to meet real-world requirements."}, {"title": "Limitations", "content": "Currently, the RACE benchmark consists of four dimensions, each comprising two to three factors. However, there are additional dimensions worthy of consideration in defining code quality and meeting practical development needs, such as security, testability, and dynamic behavior. Additionally, our experiments have only been conducted on Python code data thus far. Future plans include expanding to multilingual code to explore differences in model preferences across languages and their impact on meeting real-world scenario requirements. Additionally, future efforts will focus on further analyzing the Code LLMs' ability to meet customized requirements, exploring deeper factors influencing generated code quality, and investigating how code placement in longer code affects compliance with requirements."}]}