{"title": "Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models", "authors": ["Jiasheng Zheng", "Boxi Cao", "Zhengzhao Ma", "Ruotong Pan", "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Le Sun"], "abstract": "In recent years, researchers have proposed numerous benchmarks to evaluate the impressive coding capabilities of large language models (LLMs). However, existing benchmarks primarily focus on assessing the correctness of code generated by LLMs, while neglecting other critical dimensions that also significantly impact code quality. Therefore, this paper proposes the RACE benchmark, which comprehensively evaluates the quality of code generated by LLMs across 4 dimensions: Readability, maintainability, Correctness, and Efficiency. Specifically, considering the demand-dependent nature of dimensions beyond correctness, we design various types of user requirements for each dimension to assess the model's ability to generate correct code that also meets user demands. We evaluate 18 representative LLMs on RACE and find that: 1) the current LLMs' ability to generate high-quality code on demand does not yet meet the requirements of software development; 2) readability serves as a critical indicator of the overall quality of generated code; 3) most LLMs exhibit an inherent preference for specific coding style. These findings can help researchers gain a deeper understanding of the coding capabilities of current LLMs and shed light on future directions for model improvement.", "sections": [{"title": "1 Introduction", "content": "The impressive coding capabilities demonstrated by Large Language Models (LLMs) are reshaping the landscape of software development (Zheng et al., 2023c,b; Fan et al., 2023), attracting significant attention from researchers. To accurately measure and compare the coding capabilities of various large models, numerous benchmarks have been proposed to evaluate the code generation (Chen et al.,"}, {"title": "2 Related Work", "content": "The outstanding code generation capabilities exhibited by LLMs have attracted considerable attention from researchers (Wang et al., 2021; Li et al., 2022; Fried et al., 2022; Xu et al., 2022; Roziere et al., 2023; Zheng et al., 2023a). Some representative Code LLMs, such as CodeX (Chen et al., 2021), CodeGen (Nijkamp et al., 2022), and AlphaCode (Li et al., 2022), have achieved notable performance in code generation, program repair and code translation. Currently, research on LLMs for code primarily focuses on data and pretraining methods. For training data collection, WizardCoder (Luo et al., 2024) introduce code instructionfollowing training constructed by Evol-Instruct to enhance the capabilities of Code LLMs. For pretraining methods, StarCoder (Li et al., 2023a) and DeepSeek-Coder (Guo et al., 2024) incorporate fill-in-the-middle training task to enhance the model's capability to handle various structural arrangements in code. With the rapid advancement of Code LLM capabilities, there is an increasing demand for reliable and comprehensive code evaluation benchmarks."}, {"title": "2.1 Code LLMs", "content": "The outstanding code generation capabilities exhibited by LLMs have attracted considerable attention from researchers (Wang et al., 2021; Li et al., 2022; Fried et al., 2022; Xu et al., 2022; Roziere et al., 2023; Zheng et al., 2023a). Some representative Code LLMs, such as CodeX (Chen et al., 2021), CodeGen (Nijkamp et al., 2022), and AlphaCode (Li et al., 2022), have achieved notable performance in code generation, program repair and code translation. Currently, research on LLMs for code primarily focuses on data and pretraining methods. For training data collection, WizardCoder (Luo et al., 2024) introduce code instructionfollowing training constructed by Evol-Instruct to enhance the capabilities of Code LLMs. For pretraining methods, StarCoder (Li et al., 2023a) and DeepSeek-Coder (Guo et al., 2024) incorporate fill-in-the-middle training task to enhance the model's capability to handle various structural arrangements in code. With the rapid advancement of Code LLM capabilities, there is an increasing demand for reliable and comprehensive code evaluation benchmarks."}, {"title": "2.2 Coding benchmark for LLMs", "content": "The existing benchmarks for LLM-based code (Ni et al., 2023), such as HumanEval (Chen et al., 2021), APPS (Hendrycks et al., 2021), MBPP (Austin et al., 2021), CodeContests (Li et al., 2022), and DS-1000 (Lai et al., 2023), focusing on the correctness of generated code in scenarios such as code exercises, data science, and competitions (Yan et al., 2023; Li et al., 2023b; Shinn et al., 2024). However, these efforts only focus on the correctness of the generated code, using the pass rate of test cases as the sole evaluation metric. Meanwhile, there has been a recent trend in considering other dimensions (Li et al., 2024; Jain et al., 2024b; Tian et al., 2024); for example, Huang et al. (2024) evaluate the efficiency of the generated code, while Dillmann et al. (2024) bridge the connection between cross-entropy and logical lines of code (LLOC). Nevertheless, these studies neither account for the demand-dependent nature of these dimensions nor systematically evaluate the LLM's code capabilities across multiple dimensions."}, {"title": "3 RACE Benchmark Construction", "content": "The philosophy of our framework design comes from the demands for code quality in software engineering (B\u00f6rstler et al., 2023). Firstly, we summarize multiple representative factors for each dimension based on their respective quality definitions (Curtis et al., 2022; Nistala et al., 2019; Sadeghzadeh Hemayati and Rashidi, 2017). Secondly, we design several reasonable customized requirements for each factor and integrate them into task descriptions, requiring the model to generate code that is both correct and meets these requirements. Information on the detailed evaluation data is presented in Table 1. Finally, leveraging static analysis and runtime monitoring techniques, we develop evaluation metrics tailored to each factor to achieve accurate and efficient evaluation. The specific designs of each instruction refer to Appendix A."}, {"title": "3.1 Correctness", "content": "To investigate the impact of incorporating customized instructions on code correctness, we evaluate the accuracy of the LLM-generated code on the original benchmark tasks and also calculate the accuracy when provided with instructions containing customization requirements.\nTo thoroughly investigate the impact of customized instructions across various tasks, we select the following datasets: HumanEval+ and MBPP+ (Liu et al., 2024) for code exercise problems, ClassEval (Du et al., 2023) for class-level code generation, and LeetCode (Guo et al., 2024) for coding competition problems. To mitigate the bias introduced by additional information in the original dataset on the customized requirements, we remove such information from the datasets.\nTo measure code correctness, we calculate the macro accuracy at the dataset level, which is the proportion of generated code that passes all test cases for the corresponding problems."}, {"title": "3.2 Readability", "content": "In real-world development scenarios, code is required to adhere to a consistent style to ensure comprehensibility and reduce the time cost of maintaining the code, which refers to code readability (B\u00f6rstler et al., 2023). Specifically, the code length is the most straightforward aspect of style; excessively long lines of code can lead to incomplete screen display, severely affecting readability. Meanwhile, good and consistent naming styles help developers quickly understand the functionality of interfaces, and comments assist in rapidly comprehending the implementation logic of the code. Therefore, we summarize the code readability into three representative factors: Length, Naming Convention, and Comment. Based on the real-world development requirements, we collect corresponding customizable options for different factors.\nFor the Length factor, the readability requirements for code length vary due to the differences in display scales across different user scenarios. Therefore, we refer to the PEP8 style for Python, and define the following user requirements concerning code length: (60, 20), (70, 30), and (79, 40), with the parentheses corresponding to the maximum line length and the maximum lines of functions, respectively. For the Naming Convention factor, camel-case and snake-case are commonly used naming methods in computer programming, with varying preferences across different projects for naming functions and variables. Consequently, we offer the choice between camel-case and snake-case based on the naming convention used for functions or variables as customization options. For the Comment factor, different levels of granularity serve varying purposes and needs. Line-level comments aid in understanding the implementation details of the code, while function-level comments assist in comprehending the functionality and usage of functions. Additionally, line-level comments are particularly beneficial for novice programmers. Consequently, we have defined two customization options: function-level comments and line-level comments.\nCode exercise tasks are derived from snippets of real-world development tasks, encapsulating scenarios encountered in actual development environments. Subsequently, leveraging these code exercise scenarios, we evaluate the generated code against customized readability requirements on HumanEval+ datasets to assess its alignment with specific criteria.\nFurthermore, to measure code readability, we employed abstract syntax tree analysis and heuristic methods to assess code length, examine naming conventions, and distinguish between different levels of comment granularity."}, {"title": "3.3 Maintainability", "content": "The maintainability of code significantly impacts the long-term health of software and the efficiency of development teams. Many quality models propose empirical quantitative equations for maintainability. Simultaneously, the Single Responsibility Principle (SRP) is a crucial part of code design principles to avoid excessive functional coupling. Therefore, based on these principles, we summarize two factors for code maintainability: Maintainability Metric and Modularity.\nFor the Maintainability Metric factor, we use the Maintainability Index (MI) (Coleman et al., 1994) to measure how maintainable the code is, which is widely used in the Microsoft Visual Studio 2010 development environment. This index is a four-metric polynomial equation, resulting in a value between 0 and 100, with higher values indicating greater maintainability. The formulation is as follows:\n$MI = max\\left[0, 100\\cdot\\left[171 - \\frac{171}{6.2\\ln V} - 0.23G\\frac{16.2\\ln L + 50 \\sin(\\sqrt{2.4C}) }{171}\\right]\\right]$\nwhere V is Halstead Volume to identify measurable properties of the code, G is Cyclomatic Complexity corresponding to the number of decisions a block of code contains plus 1, L is the number of source lines of code, and C is the percent of comment lines.\nTo comprehensively assess the maintainability requirements satisfaction of LLM-generated code, we employ ClassEval (Du et al., 2023) dataset, to ensure the complexity of the code problems.\nFor the Modularity factor, different application requirements dictate varying levels of modularization. Achieving compactness often necessitates implementing functionality using a single function, whereas maximizing code reusability demands the use of multiple functions. Accordingly, we define the following customization options: implementing functionality using 1, 2, or 3 functions. We choose to measure the modularity of generated code on LeetCode (Guo et al., 2024) dataset, which is more challenging to ensure better discriminative capability. In addition, we design corresponding rule-based methods to check the degree of modularity in the generated code."}, {"title": "3.4 Efficiency", "content": "In most applications, code efficiency is directly linked to user experience or business process efficiency. Generally, efficiency is assessed using time complexity and space complexity. Based on this principle, we define them as factors. Due to varying user-side hardware conditions, achieving a balance between execution time and memory usage, or optimizing one of these aspects to the extreme, is a common practice to ensure code efficiency. Recognizing these scenarios, we gather 101 cases from LeetCode programming problems designed to simulate such conditions. These cases are customized with specific time complexity requirements, space complexity requirements, or both, to evaluate the extent to which the LLM-generated code meets the efficiency requirements.\nTo measure code efficiency, we propose the Normalized Index (NI), i.e., to measure the degree to which the generated code satisfies the complexity requirement. Given two solution codes with time and space complexity $C_T, C_S^1$ and $C_T^2, C_S^2$, respectively, where $C_T^1$ and $C_S^1$ are better, and given their total running time $T_1, T_2$ ($T_1 < T_2$) and memory usage $S_1, S_2$ ($S_1 > S_2$) on all test cases. Now there is a code $\u0108$ to be evaluated, which has a running time $\\hat{T}$ and memory usage $\\hat{S}$, with requirements $C_T^\\star$, $C_S^\\star$, then the normalized index is:\n$NI_T = 100 \\cdot Clip\\left(1 - \\frac{\\hat{T} - T_1}{T_2 - T_1}, 0, 1\\right)$,\n$NI_S = 100 \\cdot Clip\\left(1 - \\frac{S_2 - \\hat{S}}{S_2 - S_1}, 0, 1\\right)$(2)\n$NI_T$ indicates the degree of time complexity toward $C_T^\\star$, and $NI_S$ indicates the degree of space complexity toward $C_S^\\star$."}, {"title": "4 Experiments", "content": "In this section, we conduct a detailed evaluation of 18 Code LLMs and obtain several valuable findings. We first introduce the input formats and inference configurations for code generation tasks, along with the selection of LLMs. Subsequently, we present the overall experimental findings and conduct further analysis of the results to derive meaningful conclusions. The detailed experimental results are shown in Appendix B."}, {"title": "4.1 Settings", "content": "Task formats We construct the different prompts based on the completion style and chat style, to better induce the LLMs to accomplish the corresponding tasks. In the inference process, we use a greedy strategy and set the temperature to 0.\nModels We select several state-of-the-art Code LLMs ranging in different sizes, both open and closed source, including DeepSeek-Coder (Guo et al., 2024), CodeLlama (Roziere et al., 2023), WizardCoder-Python (Luo et al., 2024), CodeQwen1.5-7B-Chat (Bai et al., 2023), gpt3.5-turbo-0125 (OpenAI, 2022), and gpt-402024-05-13. We use 6.7B/7B/33B/V2-LiteInstruct(16B)/V2-Instruct(236B) for DeepSeekCoder, use 7B/13B/34B for both CodeLlamaInstruct and CodeLlama-Python, and use 15B/33B for WizardCoder with 7B/13B for WizardCoderPython."}, {"title": "4.2 Correlation Analysis Across Dimensions", "content": "To conduct a more in-depth analysis of how various factors across different dimensions influence overall code quality, we analyze the correlations between different factors in each model. Specifically, we first compute the proportion of the generated code that is both correct and follows customized instructions across 8 factors for 18 Code LLMs. Subsequently, we calculate Pearson correlation coefficients between these factors."}, {"title": "4.3 Preference Bias of Code LLMS", "content": "To investigate whether the model's inherent preferences affect its ability to follow user instructions, we conduct a more fine-grained comparison of various incorporated user requirements. Specifically, we focus on naming conventions, requiring LLMs to consistently use either camel-case or snake-case for both function names and variable names. Additionally, regarding code length, LLMs are tasked with generating single-line lengths not exceeding 60, 70, and 79 characters, as well as ensuring that individual method lengths do not exceed 20, 30, and 40 lines. For loop structures, the requirement is for LLMs to use only either for or while statements to implement necessary loop constructs. Finally, we calculate the proportion of LLM-generated code that follows these customized requirements, i.e. the rate of instruction-following (IF).\nFigure 6 demonstrates the IF rates of 18 LLMs across all the customized requirements above. We find that the majority of LLMs exhibit an inherent preference bias towards generating code in specific styles. This bias often results in these LLMs being unable to follow user instructions effectively if the requested style differs from that prevalent in their training data. Specifically, for naming conventions, Python conventionally employs snake-case for function and variable names. When LLMs are requested to use camel-case, most LLMs, such as CodeLlama and CodeQwen, almost fail to comprehend and fulfill this requirement, with instruction-following rates below 20%. For code length, a 79-character single-line length limit is a common style in Python. When dealing with more stringent requirements, the IF rates of most instruct-type LLMs drop by nearly 20%, while DeepSeek-Coder-V2-236B maintains the best instruction-following rates. For loop structures, all LLMs except GPT series and DeepSeekCoder-V2-236B exhibit a pronounced tendency to use \"for\" statements. When tasked with using \"while\" statements, LLMs struggle to transform between different statements, reflected in the IF rates generally below 70%. These findings imply that most LLMs may simply learn the inherent patterns of the next token from examples, without a clear understanding of the logic of code comprehension. However, GPT-3.5, GPT-4o, and DeepSeek-CoderV2-236B perform well in this aspect, with an IF rate above 90%. Such preference bias can lead to the ossification of code style in Code LLMs, thereby hindering their ability to meet specific realworld project requirements and consequently affecting the adaptability and scalability of generated code. This issue may be more pronounced in programming languages like Perl, JavaScript, and PHP, where there is no strict, widely accepted standard code style."}, {"title": "5 Conclusion", "content": "We present the RACE benchmark, a multidimensional evaluation framework for code generation, including correctness, readability, maintainability, and efficiency. The RACE benchmark evaluates whether LLMs can generate code that is both correct and meets customized requirements, based on the selection of factors within each dimension and the customized requirements designed for each factor. Based on further experiments in 18 representative LLMs, we find that the present capabilities of LLMs in generating code of high quality as needed still fall short of the demands in software development. Additionally, code readability serves as a pivotal indicator of the overall quality of generated code. Our research highlights the critical importance of improving the multidimensional quality of generated code. Future efforts should focus on improving the ability of LLMs to meet real-world requirements."}, {"title": "Limitations", "content": "Currently, the RACE benchmark consists of four dimensions, each comprising two to three factors. However, there are additional dimensions worthy of consideration in defining code quality and meeting practical development needs, such as security, testability, and dynamic behavior. Additionally, our experiments have only been conducted on Python code data thus far. Future plans include expanding to multilingual code to explore differences in model preferences across languages and their impact on meeting real-world scenario requirements. Additionally, future efforts will focus on further analyzing the Code LLMs' ability to meet customized requirements, exploring deeper factors influencing generated code quality, and investigating how code placement in longer code affects compliance with requirements."}, {"title": "A Evaluation Data and Customized Instructions", "content": "Based on existing data, we design customized requirements that are both reasonable and closely aligned with real-world application scenarios, incorporating these requirements into the task description to obtain evaluation data for our RACE benchmark. Detailed customization instructions for each factor are shown in Figure 7 and Figure 8. For code correctness, we utilize HumanEval+, MBPP+, ClassEval, and LeetCode data. For code readability, we use HumanEval+ data. For code maintainability, we use ClassEval and LeetCode data. For code efficiency, we use self-constructed data derived from LeetCode. We adhere to the task settings defined in the original data while incorporating our designed customization requirements. Simultaneously, for the HumanEval+ and MBPP+ datasets, we discard the original prompt format and extract the primary task descriptions from the original prompts to serve as the final prompts. This approach helps avoid conflicts between function template information included in the original prompts and requirements related to code readability, thus providing a better assessment of code-related abilities. Additionally, it mitigates the impact of potential data leakage, thereby increasing the difficulty of the benchmark."}, {"title": "B Detailed Experiment Results", "content": "The detailed experimental results on the RACE benchmark are shown in Table 3 and Table 4. For the Naming Convention factor, we design 6 settings that require the function names (function_camel, function_snake), variable names (var_camel, var_snake), or both (camel, snake) in the generated code, to follow specified naming conventions. We can see that the majority of models struggle to adhere to the camel-case naming convention. Furthermore, the variance in capabilities among different models primarily manifests in scenarios requiring function names to use camel-case (function_camel). For the Length factor, we can see that as the constraints became progressively stringent, ranging from maximum single-line length of 79 and maximum method line count of 40 (L_79_40), to maximum single-line length of 60 and maximum method line count of 20 (L_60_20), most models exhibit a significant decline in their ability to meet requirements. For the Comment factor, different models respond variably to related requirements. However, we find that most models can improve code correctness by meeting the code comment requirements, such as models deepseek-coder-33b-instruct, DeepSeek-Coder-V2-236B, and WizardCoder-Python-13B-V1.0."}]}