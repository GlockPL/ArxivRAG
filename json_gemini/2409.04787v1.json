{"title": "Selective Self-Rehearsal: A Fine-Tuning Approach to Improve Generalization in Large Language Models", "authors": ["Sonam Gupta", "Yatin Nandwani", "Asaf Yehudai", "Mayank Mishra", "Gaurav Pandey", "Dinesh Raghu", "Sachindra Joshi"], "abstract": "Fine-tuning Large Language Models (LLMS) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task or the characteristics of the training data, resulting in a loss of generalization. This paper introduces Selective Self-Rehearsal (SSR), a fine-tuning approach that achieves performance comparable to the standard supervised fine-tuning (SFT) while improving generalization. SSR leverages the fact that there can be multiple valid responses to a query. By utilizing the model's correct responses, SSR reduces model specialization during the fine-tuning stage. SSR first identifies the correct model responses from the training set by deploying an appropriate LLM as a judge. Then, it fine-tunes the model using the correct model responses and the gold response for the remaining samples. The effectiveness of SSR is demonstrated through experiments on the task of identifying unanswerable queries across various datasets. The results show that standard SFT can lead to an average performance drop of up to 16.7% on multiple benchmarks, such as MMLU and TruthfulQA. In contrast, SSR results in close to 2% drop on average, indicating better generalization capabilities compared to standard SFT.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have made remarkable progress in recent years, demonstrating impressive capabilities across a wide range of tasks, including question-answering (Rajpurkar et al., 2016), summarization (Nallapati et al., 2016), and more (Brown et al., 2020). This advancement has led to the adoption of LLMs in various real-life applications, such as customer support (Xu et al., 2017) and code assistance (Chen et al., 2021). However, adapting these models to specialized domains and tasks often requires adjustments to meet the specific unique needs of model designers. For example, a designer of a customer support agent may want the model to abstain from answering questions that are unanswerable, off-topic, or potentially unsafe.\nCurrent approaches to address this challenge include prompt engineering and fine-tuning with task-specific data. Prompt engineering involves guiding the model's behavior through instructions and few-shot in-context examples without altering its weights, allowing it to retain its original capabilities. However, this method may lead to suboptimal performance on the target task (Stiennon et al., 2020). Fine-tuning, on the other hand, can better align the model with the desired behavior (Peters et al., 2019), but may reduce the model's generality. Our work aims to follow the fine-tuning approach while aiming to maintain the model's general capabilities.\nSupervised Fine-Tuning (SFT) typically relies on gold responses for training. However, for instruction-tuned models, we observe two key issues: 1) many model responses, while differing from gold responses, are still satisfactory, and 2) the distribution of gold responses often diverges"}, {"title": "2 Proposed Method", "content": "Let \\(M_{\\theta}\\) parameterized by \\(\\theta\\) be a given large language model. Let \\(\\theta = \\theta_0\\) be the given model weights obtained after pre-training and instruction tuning the model. We refer to \\(M_{\\theta_0}\\) as the base model. Let us assume that we cannot access the pre-training and instruction fine-tuning datasets. Further, let \\(T\\) be the new task that we wish to teach the model \\(M_{\\theta}\\), and let \\(D = \\{(x_i, y_i) | i = 1...N\\}\\) be the corresponding dataset that we may use to teach the new task to the model. In standard Supervised Fine-Tuning (SFT), we backpropagate through the standard Cross Entropy loss over the training dataset, computed as:\n\\[L_{SFT}(D) = \\sum_{i=1}^N \\log P_{\\theta}(y_i|x_i) \\qquad (1)\\]\nHere, \\(P_{\\theta}\\) is the conditional probability assigned by the model \\(M_{\\theta}\\).\nNow, let us assume that for an input \\(x_i\\), \\(\\hat{y}_i = M_{\\theta_0}(x_i)\\) is the base model's prediction. We know that in many applications of NLP, e.g. machine translation, content-grounded conversations, summarization, reading comprehension etc., an input \\(x\\) can have multiple correct answers, and it may suffice to generate any one of them. Given that the model has already been instruction-tuned on a variety of tasks, it is quite likely that the prediction"}, {"title": "3 Experimental Setup", "content": "Task: Our experiments aim to compare the proposed SSR method with the standard SFT for teaching a new task to the LLM. We focus on teaching the task of content-grounded QA/conversation. In this task, the LLM must respond based on the information present in the provided document. If the document doesn't contain the information necessary to respond, then the LLM must refrain from responding and inform the user that it can't find the information in the provided document. We observe that the base LLM generates acceptable responses when the document contains the answer. However, it hallucinates when the question can't be answered from the provided document. This perfectly fits the premise for SSR method \u2013 the base LLM is good at answering the 'answerable' questions, while it needs to learn to refrain from answering for the \u2018unanswerable\u2019 queries.\nDatasets: To fine-tune a base LLM, we use two publicly available content-grounded QA/conversation datasets: (1) natural questions (NQ) (Kwiatkowski et al., 2019), and (2) Mul-"}, {"title": "4 Results and Discussion", "content": "Our experiments evaluate three research questions.\n1. In-Domain Performance: How does SSR perform compared to baselines when fine-tuned and evaluated on the same dataset?\n2. Out-Domain Performance: How does SSR perform compared to baselines when evaluated on the datasets unseen during train?\n3. Generalization: How well does SSR retain the inherent capabilities of the base model post fine-tuning?\n4.1 In-Domain Performance\nTable 3 reports our modified recall and classification accuracy of Mistral-Instruct-v2-7b finetuned over MD2D and NQ datasets. Here, we evaluate the base, SFT, and SSR models over the test set corresponding to the training dataset. To assess the base model's capability to generate responses for answerable queries, we also report token-recall (T.Recall(AA)) only for those answerable queries where the model generates a response instead of refraining from answering (Answerable queries classified as Answerable). We first observe that the base model is good at answering the answerable questions (achieves good T.Recall(AA)) but struggles to identify when not to respond (poor classification accuracy). Hence, this is the skill we would like the model to learn by fine-tuning without forgetting its ability to generate good answers. We observe that both SFT and SSR techniques for fine-tuning result in a model that is able to identify unanswerable queries equally well (similar accuracy). However, we observe that SSR retains the original model's ability to answer the questions, whereas token recall for the SFT model drops drastically compared to the base and SSR models. As a result, SSR achieves the best overall performance as quantified by our modified recall metric.\nHuman Evaluation: Table 4 reports the human evaluation results on test samples from MD2D using the model fine-tuned on MD2D. We see that both SFT and SSR have been able to surpass the score of the simple prompting approach. We also see that the SFT approach is a bit better than SSR on the in-domain setup. We get moderate inter-annotator agreement (r = 0.34) using Kendall's Tau. The agreement is moderate as MD2D is conversational, and there are many possible ways to respond to the user. Some annotators prefer one style of response over others, e.g., some like short answers and others prefer a more detailed answer.\n4.2 Out-domain Performance\nThis experiment aims to demonstrate that SSR achieves better generalization than SFT. To do so, we train the model on one dataset and evaluate its performance on the other datasets. Specifically, we finetune the base model using MD2D (NQ) and evaluate them on MuSiQue and NQ (MD2D). Table 5 reports the performance using Mistral-Instruct-v2-7B as the base model.\nWe first observe that even for a multi-hop reasoning dataset (MuSiQue), prompting the base model archives the best token-recall over the answerable queries classified as answerable. It demonstrates that the base model often answers correctly when it chooses to respond. We would like to retain this capability of the model upon finetuning. In addition, the base model achieves 69.8% classification accuracy as well. While SFT on MD2D improves the classification accuracy, it takes a big hit in the token-recall, resulting in a huge drop in overall modified recall (drops to 48.8). We hypothesize that this is due to the model forgetting its multi-hop reasoning capability when using SFT. This phenomenon is more prominent when we do SFT on NQ, resulting in a significant drop in both classification accuracy and token recall.\nOn the other hand, using SSR always improve the classification accuracy on MuSiQue while re-"}, {"title": "4.3 Generalization", "content": "One of the major issues with SFT is that the model forgets the skills that it learnt during pre-training and instruction tuning. Here, we show that SSR alleviates this issue. To do so, we compare the SFT and SSR models against the base model on a diverse set of publically available benchmarks. Specifically, we evaluate them on MMLU (Hendrycks et al., 2020), Truthful-QA (Lin et al., 2022), GSM8k (Cobbe et al., 2021) and Hellaswag (Zellers et al., 2019). Table 6 reports our findings. We compare the performance of the two SSR models (trained on MultiDoc2Dial and NQ) with the corresponding SFT models and the base model. We observe that irrespective of the dataset used for training, there is a significant drop in the performance of SFT models across all benchmarks. On average, SFT on Mistral-7B results in a drop of 16.7% and 12.7% when trained using MultiDoc2Dial and NQ, respectively. On the other hand, SSR results in an average drop of only 2.3% and 2.0% when trained on MD2D and NQ, respectively, with most of the drop (5.8 and 6.4) coming from GSM8k. In contrast, the corresponding drop in SFT on GSM8k is 31.0% and 23.9%. This clearly demonstrates that our proposed SSR technique for finetuning preserves the base model's capabilities. On the other hand, standard SFT results in overfitting to the training dataset, resulting in catastrophic forgetting of the skills acquired by the base model during pre-training and instruction tuning.\n4.4 Subjective Analysis\nIn Table 7, we present three examples that illustrate the generalizability of SSR on out-domain testsets. The first example, Q1, is from MD2D and SFT/SRR are fine-tuned using NQ. NQ mostly contains factoid QA pairs and its answers are typically phrases from the grounded documents. We see that SFT model is overfit to this style of answering, and hence the model response is extractive and not even a complete sentence. This over-fitting has forced"}, {"title": "5 Related Work", "content": "Unanswerability: Previous research has used unanswerable questions to evaluate reasoning abilities (Rajpurkar et al., 2018; Ferguson and Ture, 2020; Kwiatkowski et al., 2019). SQUAD v2 (Rajpurkar et al., 2018) was the first dataset to include unanswerable questions, followed by the NATURAL QUESTIONS (NQ) dataset (Kwiatkowski et al., 2019). Trivedi et al. (2022) introduced MuSiQue, a challenging multi-hop QA benchmark featuring unanswerable questions with key information intentionally removed. Our experiments leverage these datasets to evaluate the SSR abilities and demonstrate our approach's capability to identify unanswerability.\nThe unanswerability capabilities of large language models (LLMs) have largely been studied using few-shot prompting (Kandpal et al., 2022; Weller et al., 2023). Recent research shows that as LLMs grow larger (Mishra et al., 2022b; Kandpal et al., 2022; Carlini et al., 2023) or train on more instruction tuning data (Mishra et al., 2022a; Chung et al., 2022; Wan et al., 2023), they become easier to steer with natural language prompts. In our work, we compare prompting and SFT with SSR.\nContinual Learning in Language Models: Continual learning for language models faces the challenge of fine-tuning over-fitting and loss of generalization (Yogatama et al., 2019; Zhang et al., 2021). Rehearsal-based methods, such as experience replay (Rolnick et al., 2019) and representation consolidation (Bhat et al., 2022), have shown promise by storing and replaying a subset of data from previous tasks. However, these approaches often rely on the availability of real data, which may be limited or unavailable in real-world scenarios. To overcome this hurdle, utilizing model-generated responses has been proposed. Techniques such as self-training (He et al., 2020; Xie et al., 2020) and self-supervised learning (Devlin et al., 2019; Lewis et al., 2020) leverage model-generated outputs to create additional training data. However, the effectiveness of using model-generated responses in continual learning for language models has not been extensively explored.\nExisting approaches often focus on using real"}, {"title": "6 Conclusion", "content": "In this paper, we introduced Selective Self-Rehearsal (SSR) as a fine-tuning approach that not only matches the performance of standard supervised fine-tuning (SFT) but also significantly improves generalization across different datasets for the same task. Our results on the task of identifying unanswerable questions demonstrate that fine-tuning a pre-trained model using SSR enables it to learn a new task without compromising its performance on a wide range of other tasks, as evidenced by evaluations on standard benchmarks such as MMLU and GSM8K.\nThe proposed method exploits the observation that multiple correct outputs may exist for a given input, and forcing the model to fine-tune the ground truth output even when it already produces a correct response can unnecessarily alter its current state. During fine-tuning, our method uses the ground truth outputs only in instances where the pre-trained model generates an incorrect response. In future work, we plan to investigate techniques for sampling correct outputs across all data and then use them for fine-tuning to achieve minimal changes in the pre-trained model's weights. Upon acceptance, we will release the augmented datasets and our code."}, {"title": "7 Limitations", "content": "The SSR method involves performing model inference on the entire training dataset to identify instances where the pre-trained model produces correct and incorrect responses, which is computationally intensive. Additionally, evaluating these inference outputs to determine the correctness of the model's responses can be laborious and may require significant manual effort. We address this challenge by using a large language model (LLM) as a judge to assess the accuracy of responses. However, this approach is not without its limitations, as the LLM's judgments can be prone to errors."}, {"title": "A Appendix", "content": "A.1 Prompt for Generating the Response\nWe list the prompts used with mistral-instruct-v2 to generate the base model responses. For the sake of consistency and fair comparison, the same prompts are used for fine-tuning using SFT and SSR techniques.\nA.2 Answerable vs Unanswerable\nClassification Prompt\nA.3 Human Judges\nFigure 7 outline the specific instructions given to the human annotators so that they can clearly understand the evaluation criteria. We further show a screenshot of the user-interface that the annotator used for annotation in Figure 8.\nA.4 Training Details\nWe use a learning rate of \\(1 \u00d7 e^{-5}\\). We train all the models for 5000 steps, validate after every 500 steps, and select the best checkpoint based on the classification accuracy over the validation set.\nTraining for all the experiments was carried out on a single A100 (80 GB) GPU. None of the experiments took more than 12 hours to train. The generation of base model's responses for training followed by the LLM-as-a-judge was a bottleneck. 2 A100 (80GBs) were used for evaluation. In all the entire cycle of inferencing using base model, took at most 48 hours."}]}