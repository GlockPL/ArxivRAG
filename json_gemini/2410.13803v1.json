{"title": "A Pattern to Align Them All: Integrating Different Modalities to Define Multi-Modal Entities", "authors": ["Gianluca Apriceno", "Valentina Tamma", "Tania Bailoni", "Jacopo de Berardinis", "Mauro Dragoni"], "abstract": "The ability to reason with and integrate different sensory inputs is the foundation underpinning human intelligence and it is the reason for the growing interest in modelling multi-modal information within Knowledge Graphs. Multi-Modal Knowledge Graphs extend traditional Knowledge Graphs by associating an entity with its possible modal representations, including text, images, audio, and videos, all of which are used to convey the semantics of the entity. Despite the increasing attention that Multi-Modal Knowledge Graphs have received, there is a lack of consensus about the definitions and modelling of modalities, whose definition is often determined by application domains. In this paper, we propose a novel ontology design pattern that captures the separation of concerns between an entity (and the information it conveys), whose semantics can have different manifestations across different media, and its realisation in terms of a physical information entity. By introducing this abstract model, we aim to facilitate the harmonisation and integration of different existing multi-modal ontologies which is crucial for many intelligent applications across different domains spanning from medicine to digital humanities.", "sections": [{"title": "1 Introduction", "content": "There is an increasing demand for modelling multi-modal aspects of knowledge to break through the bottleneck of real-world applications [43,33] or, more recently, to be exploited either for manipulating or customising Large Foundational Models [10]. Indeed, as stated by Zhu and colleagues [54], the \u201cmulti-modalization\" of knowledge is a key step towards achieving human-level machine intelligence.\nKnowledge Graphs (KGs) have had a fast growth in the last decade and the benefits deriving from their adoption have been perceived in a wide range of real-world applications, including text understanding, recommendation systems, and natural language question answering. The landscape of KGs includes resources covering common sense"}, {"title": "2 Background", "content": "Autonomous intelligent behaviour relies on the ability to perceive multi-modal information from the environment, and reason with this information, possibly aided by some form of background or common-sense knowledge to support decision-making and distil new knowledge [23]. Multi-modality has been defined as \u201cthe coexistence of more than one semiotic mode within a given context\" [13], something that living beings experience in everyday life through sight, sound, and movement. A similar coexistence of modalities has also been adopted by many robotics and autonomous systems, whose decision-making is based on data generated by a plethora of different sensor types, e.g. vision, proximity, and tactile sensors [8]. This has resulted in the proliferation of multi-modal information in the form of images, sounds, or sensor-based location information, to name a few [37], typically represented using different data formats.\nIntelligent decision-making [54], therefore, relies on processing multi-modal knowledge such that symbols are grounded in their corresponding modalities and mapped to their referents together with their meaning. Indeed, without the ability to ground symbols to the modality they refer to, and to map them to their corresponding referents with meaning (therefore associating specific data elements to their real-world counterparts in a manner that preserves their semantic relevance or meaning) a machine has only a limited understanding of its environment.\nKGs are gaining prominence as a formalism to model and process multi-modal knowledge, and therefore provide an explicit representation of the association between data elements and their semantic referents. MMKGs extend traditional KGs by integrating information from diverse modalities, thus representing different modal aspects that contribute to the definition of the same entity. In MMKGs, some of the symbols used to denote nodes and edges together with their corresponding data items, are associated with diverse modalities such as text, image, sound, or video, that could embody the knowledge [54]. The inclusion of different modalities provides a more comprehensive, and contextually aware view of entities modelling a domain, therefore better capturing the nuances and complexity of real-world data, and providing a richer understanding of the relationships connecting them [42,9]. Indeed, these modalities rarely exist in isolation but are often intertwined with structured semantic information, in an application-dependent fashion. Numerous efforts in the literature leverage this interplay across a range of tasks, such as image retrieval [49,21,39], Image Captioning [27,18,52,53] and Visual Storytelling [17,50,24]\nDespite this growing interest in modelling multi-modal data, the term modality does not have a widely accepted definition. In particular, differences in semantics arise when multi-modal information is encoded in KGs. In most MMKGs, the concept of modality remains tightly intertwined with the corresponding (digital) resource being modelled. A notable example is schema.org, where the nature of both MediaObject and its more specific subclasses, such as ImageObject and AudioObject, is closely linked to its content, as indicated by the contentURL property.\nIn the context of this work, we argue the separation of concerns between the resource being modelled, and how this is modelled, which can manifest itself through different formats: images, text, etc, each contributing some of its meaning to a resource. We, therefore, consider a resource as a type of Information Entity (IE) [41], i.e. the content"}, {"title": "3 Motivation", "content": "Despite the increased popularity of MMKGs, an analysis of the literature identified three main problems:\n1. the term modality does not have a well-accepted definition and consequent ontologi-cal formalisation (except for the work in [3]). The use in the literature can encompass both typical examples of modalities, e.g. images, sounds, or less typical ones, e.g. linguistic features of co-occurring terms [22,29,12,28,51];\n2. typical MMKGs tend to model homogeneous knowledge, i.e. either images, movies, or sound, which is inherently siloed [54]. Some recent state-of-the-art studies, e.g. [25], focus primarily on textual and image information individually and do not consider the possible interaction between modalities. This homogeneity affects also representational learning [7], where many multi-modal KG embedding approaches map all multi-modal information onto one high-dimensional embedding [38]; and\n3. the scope and the type of the knowledge modelled typically depends on the task for which it is designed, and therefore is not easily reused in or aligned to other applications [36].\nTo overcome these limitations, we propose a novel Multi-modality pattern, that is general in scope and supports the coexistence of different modalities. Ontology design patterns (ODPs) can be seen as a lightweight module or fragment of a foundational on-tology, therefore providing an abstraction of sound modelling choices that can be shared and reused across ontologies or KG schemata [19]. The proposed pattern establishes the ontological commitment to the notion of modality and thus separates the manifestation of the modality from its semantics.\nThe design of this pattern [15,6] is based on a set of requirements that MMKGs should satisfy, and that were identified by considering two main activity types in the KG engineering life-cycle, MMKG creation and MMKG alignment.\nMMKG creation refers to the instantiation of a MMKG. In an MMKG, the schema for a particular task or application is a specialisation of the proposed Multi-Modality pattern. The pattern provides also the blueprint for defining how to instantiate the schema, in the creation and evolution of a KG of interconnected multi-modal entities, where each entity may be described by more than one modality. We explore this further in Subsection 5.1. By MMKG alignment we refer to those cases in which it is necessary to align and integrate several existing MMKGs that were developed for some downstream application to represent a more exhaustive view of some multi-modal entity, and to support interoperability between multi-modal representations. For example, we could align the entity representing \u201cHey Jude\u201d in KGRec-sound, a dataset of songs with song's textual descriptions, tags and user's listening habits extracted from Songfacts.com and Last.fm, respectively [34] with the corresponding DBpedia [2] entity that links the images depicting the song, e.g. the picture of Julian Lennon, for whom the song was written. We explore this further in Subsection 5.2.\nWhile these activity types are not meant to be exhaustive they allow us to show the applicability, generality, and robustness of the pattern for different situations.\nWe use these two activities to identify requirements for the design of the Multi-modal pattern we propose:\n1. the pattern should support the integration of different modalities that model holis-tically the same entity;\n2. the pattern should promote semantic agreement on the definition of modality;\n3. the pattern should represent modalities at the highest level of abstraction, therefore ensuring a separation of concerns between the specific downstream applications supported and the content of the pattern;\n4. the pattern should be domain agnostic, task-independent and easy to use;"}, {"title": "4 The Multi-Modal Ontology Design Pattern", "content": "The Multi-modal ODP's goal is to represent information entities with their associ-ated modalities, of diverse nature, and the corresponding relationships between them (AIM\u2081) while providing flexibility and extensibility to seamlessly incorporate new modalities and adapt to changes in existing ones (AIM2). These two aims determine the requirements underlying the pattern's design: AIM1 identifies the functional re-quirements that the pattern should satisfy and that we represent in the four competency questions (CQs) outlined in Table 1. AIM2 relates to the set of (non-functional) re-quirements identified in Section 3. Overall, our proposition extends and combines the expressivity of the Information Realization ODP [11] with the flexibility of the modality separation approach advocated by [3]."}, {"title": "5 Applications and Alignments", "content": "This section demonstrates the application of the ODP pattern within an existing KG and illustrates how current multi-modal ontologies are aligned through its utilisation."}, {"title": "5.1 Multi-Modal Pattern Application: FuS-KG", "content": "The Functional Status Knowledge Graph (FuS-KG) is a multi-modal knowledge graph that provides a complete and structured way to represent all the information about the function status information (FSI) of a person. This is done with the aim of building and deploying more effective coaching solutions that, by exploiting the FuS-KG, can support fragile individuals during their daily lives in achieving a healthy lifestyle. Currently, FuS-KG comprises ten distinct modules: Core, Food, Recipes, Diseases, Activity, Barrier Temporal, User, Guideline, and Multi-modal. Notably, the multi-modal module, which contains multi-modal recipe resources (e.g., images and videos of recipes), has been implemented in accordance with the ODP of Section 4. We present two illustrative examples that show how the ODP is used within the module.\nImage modality: The recipe shown in Figure 2 refers to the individual named Recipe1m-60 (namely, Crock Pot Caramelized Onion). This recipe is associated with two individ-uals of type ModalDescriptor, i.e., Recipe1m-60-image_jpeg480x320-1 and Recipe1m-60-image_jpeg 550x826-1 18, each one pointing to different ImageModality modalities. It is important to note that the naming convention used for ModalDescriptor instances includes the recipe name, the modality metadata (e.g., image, video, and format), and a progressive number. This number aims to disambiguate multiple instances associated with the same modality within a recipe. Indeed, a recipe may have multiple images that share image modalities. For example, Recipe1m-60 may have multiple images of the same format and size, leading to sequentially numbered modal descriptors (e.g., Recipe1m-60-image jpeg 480x320-2 sharing the image-jpeg 480x320 modality. As can be seen, each ModalDescriptor instance is provided with the URL data properties point-ing to the corresponding (image) resource.\nVideo modality: The recipe shown in Figure 3 refers to the individual named Tasty-2834 (namely, Pizza Margherita). This recipe is associated with one individual of type ModalDescriptor (i.e., Tasty-2834-video_mp4_30.0_720x720-1) pointing to a Video-Modality. Differently from Figure 2, the naming convention for ModalDescriptor in-stances, in this case, also contains the frames-per-second (fps) that is used as a data property to represent the frames per second of a specific video instance. Furthermore, this recipe instance, differently from the one shown in Figure 2, represents an example of an instance where the resource (i.e, the video of the recipe), is contained in a specific path inside an (external) zip archive as denoted by the data properties, hasPathInArtifact, and resourceURL, respectively."}, {"title": "5.2 Multi-Modal Pattern Alignment: Aligning Existing Ontologies", "content": "In this subsection, we show how existing ontologies covering multi-modality, can be aligned with our ODP. We select and show the alignment for the following ontologies:\n\u2013 The Multimodal Description of Social Concepts Ontology (MUSCO) [35]\n\u2013 The Ontology for Multimodal Knowledge Graphs for Data Spaces [47]\n\u2013 The Polifonia Ontology Network (PON) [4]\nMUSCO: Automatic detection of social concepts evoked by images presents a signif-icant challenge. This is primarily due to the complexity of social concepts, which lack unique physical features and rely on more abstract characteristics compared to concrete\nconcepts. The Multimodal Description of Social Concept (MUSCO) ontology has been proposed to investigate, model, and experiment with how and why social concepts (e.g., peace and violence) are represented and detected by both humans and machines in images. To achieve this, the ontology integrates multisensory data and is grounded in cognitive theories about social concepts, mapping them to multimodal frames.\nThe upper part of Figure 4 illustrates the relationship between the ImageObject and the ImageData concepts within MUSCO ontology. The definition of ImageObject designates it as a type of information object with a location in a pixel box and indicates its use in evoking a social concept (i.e., SCMultiModalFrame within MUSCO). In contrast, ImageData is treated as one of its possible (information) realisations.\nThe lower part of Figure 4 shows the alignment of these concepts within the ODP shown in Figure 1. The ImageData has been aligned by specifying it as a subclass of the ModalDescriptor class. Additionally, the content of the ImageData has been distinguished from its modality through the use of the ImageModality modality. The ImageObject encompasses the role of the Multi-ModalEntity, presented in Figure 1. Consequently, this class inherits the hasModalDescriptor property, which connects the ImageObject with its associated image resource(s).\nOntology for Multimodal Knowledge Graphs for Data Spaces: Data spaces [14] offer flexible and scalable solutions for integrating heterogeneous data from multiple, multi-modal sources. By combining structured and unstructured data, data spaces enable detailed insights into complex patterns hidden within the data. Moreover, integrating external background knowledge can enhance the reasoning process by inferring new facts not immediately deducible from the data. In this context, MMKGs have the potential\nto enrich data spaces by providing a unified framework for reasoning over structured multi-modal data from diverse sources. To address this, [47] proposes a foundational multi-modal ontology to represent, query, and analyse multi-modal data in data spaces.\nThe upper part of Figure 5 illustrates an excerpt of the ontology, depicting the relationship between the primary concepts involving multi-modality: mmkg:Scene and mmkg:Source. The Scene class represents a scene captured by a camera, characterised by two data properties: dc:identifier and time:instant, denoting the timestamp of the scene captured. The Source class represents unstructured data files received from the camera, such as images. This class contains properties describing a source, including dc:sizeOrDuration, dc:title, dc:format, dc:created (indicating the file creation time), time:instant, and mmkg:filePath, to denote the file's location. The Source class aligns closely with ModalDescriptor, with the distinction that the nature of the Source is intrinsically linked to its content.\nThe lower part of Figure 5 shows the alignment of Source and Scene concepts within the ODP. As can be seen from the figure, the Source class is specified as a subclass of ModalDescriptor, making the mmkg:filePath data property redundant, as it is inherited from ModalDescriptor. Additionally, dc:format is moved from Scene to the Modality class, alongside dc:sizeOrDuration, which is further broken down into width, height, and duration based on the type of modality. The Scene encompasses the role of the Multi-\nModalEntity, and, consequently, this class inherits the hasModalDescriptor property, which connects the Scene with its associated resource(s).\nPolifonia Ontology Network (PON) In the domain of music, several ontologies have been proposed to annotate both symbolic and audio data, aiming to create comprehensive Music Knowledge Graphs. However, existing models suffer from a lack of interoper-ability and fail to adequately capture the historical and cultural contexts surrounding music creation. This limitation might also propagate bias toward recent trends and cul-tural preferences in downstream applications. As a response to these potential issues, [4] introduced the Polifonia Ontology Network (PON), offering a modular framework of music ontologies to address both cultural heritage preservation and broader queries within the music domain. Currently, PON is composed of 15 modules, each address-ing various aspects of the music domain. Notably, the CoMeta module, an extension of Music Meta [5], focuses on describing the metadata of music collections, corpora, containers, or music datasets, with a particular emphasis on multi-modal concepts.\nThe upper part of Figure 6 illustrates an excerpt of the CoMeta ontology, depicting the relationship between the primary concepts involving multi-modality: com:DatasetContent, com:RawDataDocument and com:Modality. The DatasetContent describes the content of a dataset from a summative perspective (e.g. the audio content of a music collection, the audio features it provides, etc.) and its production process (provenance). The Raw-DataContent class, a subclass of DatasetContent, is characterised as a partition of the dataset containing raw data of a specific modality, whether structured (e.g., tabular data) or unstructured (e.g., audio files). For example, a dataset partition containing images of music albums could be described as RawDataContent. Finally, the RawDataCon-tent class is linked, via the hasModality object property, to the DataModality class, which describes the modality of a dataset partition, such as audio, video, or image. The connection between RawDataContent and DataModality strongly aligns with that of ModalDescriptor and Modality in the Multi-modal pattern in Figure 1. The main difference is that instances of single modalities are not as specific as those in the ODP.\nThe alignment of CoMeta within the ODP is illustrated in Figure 6 (bottom). As can be seen from the figure, RawDataContent class is now connected to the ModalDescriptor by the object property skos:relatedMatch7, which is used to state an associative map-ping link between the two concepts, and the hasModalitySpecification relation which originates from the ModalDescriptor class. As a consequence, single modalities are no longer just instances but conceptual entities serving the objectives outlined in Section 4. As we find no Multi-ModalEntity in CoMeta, this example demonstrates the partial reuse of the Multi-modal pattern when the focus is on the modality specifications \u2013 to describe the various realisations in a dataset, rather than on the information objects."}, {"title": "6 Conclusion and Future Works", "content": "This article proposes a novel multi-modal ODP that facilitates the integration of different modalities to define multi-modal entities. Our design is based on a comprehensive analysis of how multi-modal knowledge has been represented in the context of knowledge representation (Section 2). Specifically, we focused our analysis on the construction and use of MMKGs, and we have identified three main limitations of current approaches (Section 3) that helped us scope the requirements that a multi-modal ODP should satisfy (Section 4).\nThe proposed pattern aims to foster interoperability among diverse systems, lay-ing the foundation for more cohesive and integrated (multi-modal) knowledge graphs. Through the application of our pattern to existing knowledge graphs and its alignments with other ontologies found in the literature (Section 5), we have shown both its usability and generalisability.\nIt is worth mentioning that during the alignment of external ontologies with our pattern, we encountered difficulties in accessing human-readable documentation detail-ing the concepts used within these ontologies. As in many types of ontological reuse, the accurate alignment of these resources depends on the availability of detailed docu-mentation that describes the intent of the ontology developers and examples of how the ontology can be used to define certain notions [46,1]. This highlights the importance of both human-readable documentation and concrete examples in facilitating a clear understanding of an ontology's concepts, thereby mitigating the risk of design errors resulting from misinterpreting concepts. Therefore, the alignments shown in Section 5, represent our best effort given the available resources. However, we do not exclude\nthat with more detailed documentation and examples for the aforementioned ontologies, these alignments could be potentially refined further.\nMoving forward, there are several directions for further refinement and exploration. First, extending the validation process described in Section 5 to include a wider range of external ontologies, will enhance the robustness and applicability of our pattern. Second, refining the multi-modal ODP by introducing additional modalities and their associated data properties will broaden its applicability across various domains. Lastly, modelling the relationships between the different modalities, such as text-to-audio conversion, as well as including user preferences and constraints over modalities, will enhance the adaptability and the user-centric nature of multi-modal knowledge representation systems."}]}