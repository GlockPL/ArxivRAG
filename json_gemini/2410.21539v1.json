{"title": "Bayesian Regression for Predicting Subscription to\nBank Term Deposits in Direct Marketing Campaigns", "authors": ["Muhammad Farhan Tanvir", "Md Maruf Hossain", "Md Asifuzzaman Jishan"], "abstract": "Abstract-In the highly competitive environment of the bank-\ning industry, it is essential to precisely forecast the behavior of\ncustomers in order to maximize the effectiveness of marketing\ninitiatives and improve financial consequences. The purpose of\nthis research is to examine the efficacy of logit and probit models\nin predicting term deposit subscriptions using a Portuguese\nbank's direct marketing data. There are several demographic,\neconomic, and behavioral characteristics in the dataset that affect\nthe probability of subscribing. To increase model performance\nand provide an unbiased evaluation, the target variable was\nbalanced, considering the inherent imbalance in the dataset. The\ntwo model's prediction abilities were evaluated using Bayesian\ntechniques and Leave-One-Out Cross-Validation (LOO-CV). The\nlogit model performed better than the probit model in handling\nthis classification problem. The results highlight the relevance of\nmodel selection when dealing with complicated decision-making\nprocesses in the financial services industry and imbalanced\ndatasets. Findings from this study shed light on how banks can\noptimize their decision-making processes, improve their client\nsegmentation, and boost their marketing campaigns by utilizing\nmachine learning models.", "sections": [{"title": "I. INTRODUCTION", "content": "The banking sector, particularly in the field of marketing\nand customer engagement, faces the ongoing challenge of\npredicting customer behavior, such as term deposit subscrip-\ntions. In addition, accurate forecasting of such behavior is\nvery crucial for improving marketing efficiency, enhancing\ncustomer segmentation, and optimizing decision-making. This\nresearch investigates whether Bayesian Data Analysis can\nenhance the predictive precision of term deposit subscriptions\nin direct marketing campaigns.\nPrevious studies have shown various machine learning and\nstatistical techniques implemented to predict customer behav-\nior in the banking industry [1]. While these approaches have\nshown promise, they cannot often account for data imbalances\nand cannot incorporate prior knowledge into the modeling\nprocess [2]. For instance, traditional logistic regression treats\ndata as stationary and has limited efficacy in reflecting the un-\ncertainty inherent in financial data even if it offers reasonable\npredictive ability [3]. On the other hand, machine learning\nmodels may have problems with being hard to understand and\nnot being able to incorporate expert knowledge or adapt to\nreal-time data dynamically [4]. Bayesian methods are very\ngood for financial applications [5] because they are flexible\nand can use previous information to make constantly updated\npredictions based on new data. Even though they have benefits,\nBayesian methods for predicting customer term deposit sub-\nscriptions have not been studied much [6]. The study aims to\nfill that gap by using Bayesian models to make more accurate\nand easy-to-understand predictions.\nThe comparison of two Bayesian models, Bayesian Logis-\ntic Regression and Bayesian Probit Regression, which were\napplied to a real-world direct marketing dataset from a Por-\ntuguese bank, is the primary contribution that this research\nstudy makes. With a total of 41,188 observations and 21\nvariables, the dataset was used to select a balanced subset of\n10,000 records for the purpose of developing a model. Both\nmodels were able to achieve convergence successfully, and\ntheir predictive accuracy was thoroughly evaluated by em-\nploying techniques such as Leave-One-Out Cross-Validation\n(LOO-CV) and Out-of-Sample Prediction.\nThe structure of this research paper is as follows: Section 2\npresents the literature review, Section 3 covers the dataset and\ndata quality, Section 4 describes the methodology, and Section\n5 presents the results and discussion. The conclusion outlines\nthe main conclusions and makes recommendations for further\nstudy."}, {"title": "II. LITERATURE REVIEW", "content": "Several studies have explored predictive techniques to en-\nhance customer behavior forecasting in bank marketing cam-\npaigns. One study [7] analyzed customer patterns to improve\na Portuguese bank's direct marketing strategy using logistic\nregression and decision tree models, finding logistic regression\nto be more accurate. Another study [8] applied oversam-\npling techniques like SMOTE, ADASYN, ROS, and others\nto predict term deposit subscriptions, with the Naive Bayes\nclassifier performing best, while AHC outperformed other\nmethods. Similarly, [9] identified \"duration\" as the key factor\ninfluencing term deposit decisions, with Logistic Regression\nand Bayesian Logistic Regression achieving 89% and 90%\naccuracy, respectively. In [10], XGBoost delivered the highest\naccuracy (91.73%), followed by logistic regression (88.79%)\nand decision trees (91.2%).\nIn this study [11], clustering methods like K-Means, K-\nMedoids, and DBSCAN were combined with classification\nalgorithms such as Decision Tree, Na\u00efve Bayes, and Random"}, {"title": "III. DATASET", "content": "The dataset contains detailed direct marketing campaign\ndata carried out by a Portuguese banking institution, consisting\nof 41,188 samples and 21 variables [19]. The dataset is com-\nplete without any missing values, ensuring a comprehensive\nand reliable source for analysis. It includes a combination\nof categorical and integer input variables representing various\nelements of marketing campaigns and client demographics.\nThe factors consist of characteristics such as age, occupa-\ntion, marital status, educational attainment, housing situation,\nand contact information, among others. The target variable\nrepresents customer subscription behavior to a term deposit,\nindicating whether a client subscribed yes or not. A signif-\nicant imbalance exists in the dataset, with a majority of 'no'\nresponses. To address this issue, a balanced subsample was\ncreated using oversampling techniques, where the minority\nclass 'yes' was oversampled to match the majority class. By\nincreasing minority representation, oversampling corrects class\ninequality. Replicating or synthesizing minority class samples\nuntil equitable representation is achieved. Oversampling min-\nimizes model bias toward the majority class, improving its\nability to anticipate both classes. From the original 41,188\nsamples, 10,000 samples were randomly chosen, and the\nsubsample was further balanced using oversampling approach\nwhich included 5,000 positive responses and 5,000 negative\nresponses. Bayesian models are computationally intensive,\nespecially for large datasets. Hence, a subsample of the main\ndata was chosen for modeling. Complex models and lots\nof data make Bayesian inference computationally costly. It\ncompensates with computational resources by picking a repre-\nsentative subsample, allowing rapid model evaluation without\ncompromising analysis integrity. This balancing technique en-\nables a more robust evaluation of model performance, ensuring\nthat class imbalance does not skew the analysis or lead to\nbiased predictions."}, {"title": "IV. METHODOLOGY", "content": "In this section, the study briefly looks at the setup of the\nmodel and the criteria selected for the model."}, {"title": "A. Logit model", "content": "Logistic regression, also known as the logit model, is\nused to explain the probability of a binary outcome with a\ndichotomous dependent variable (0 or 1). It transforms the\nlinear combination of predictor variables into a probability\nusing the logistic (sigmoid) function. The formula of the\nlogistic function can be defined as follows [20]:\n$\\pi = h(\\eta) = \\frac{exp(\\eta)}{1 + exp(\\eta)}$\nor (equivalently) the logit link function\n$g(\\pi) = log(\\frac{\\pi}{1 - \\pi}) = \\eta = \\beta_0 + \\beta_1x_1 + ... + \\beta_kx_k$\nwhere is the probability of success, $B_0$ is the intercept\nterm and $B_1$, $B_2$, ..., $B_k$ are the coefficients associated with the\npredictor variables $x_1$, $x_2$,..., $x_k$ respectively."}, {"title": "B. Probit model", "content": "Probit regression is an alternative technique for estimating\nthe probability of a binary outcome. It uses the probit function,\na cumulative distribution function (CDF) of the standard nor-\nmal distribution, to link predictor variables with the probability\nof a binary outcome. The formulation of the probit model is\nas mentioned below [20]:\n$\\pi = \\Phi(\\eta) = \\Phi(x'\\beta)$\nAccording to the conventional normal distribution, the CDF\nis denoted by \u03a6. One can determine the chance that a standard\nnormal random variable is either less than or equal to a\ngiven value by calculating the CDF of the standard normal\ndistribution \u03a6."}, {"title": "C. Priors and Posterior Inference", "content": "The Bayesian method uses priors based on previous research\nor expertise, which influence posterior distributions after data\nis observed. In Bayesian logit and probit models, weakly infor-\nmative normal priors centered around zero were implemented\nto allow flexibility in predictor variable effects. Using Markov\nChain Monte Carlo (MCMC), posterior distributions were\nestimated, which provide probabilistic support for parameter\nvalues and predictions, unlike point estimates in frequentist\nmethods [21]."}, {"title": "D. Priors Assumptions", "content": "1) Prior Selection for Logit Model: The logit model im-\nplemented more informative priors to reflect problem-specific\nknowledge. The intercept had a normal prior with a mean\nof 3.5 and a standard deviation of 1, indicating a strong\nbelief about the event's baseline probability with moderate\nfluctuation. The regression coefficients had a normal prior with\na mean of 0 and a standard deviation of 0.5, suggesting modest\nbut meaningful predictor effects, making the model conserva-\ntive for scenarios where predictors have limited influence [22].\n2) Prior Selection for Probit Model: The intercept had a\nnormal prior with a mean of 0 and a standard deviation of\n5, allowing for significant fluctuations in the baseline event\nprobability. The regression coefficients had normal priors with\na mean of 0 and a standard deviation of 2, suggesting a\npotentially greater but uncertain influence on the outcome.\nThis approach is suitable when predictor effects are less known\nor expected to deviate more from zero [22]."}, {"title": "V. RESULT AND DISCUSSION", "content": "This section represents the research's outcomes. This study\nselected 10,000 observations from the entire dataset for the\nfinal model implementation."}, {"title": "A. Bayesian Logit Regression Model Implementation", "content": "The results of implementing the Bayesian logit regression\nmodel, taking into account twenty-one features, are presented\nfor consideration.\n1) Summary of the model: Table 1 shows the estimated\ncoefficients and diagnostic measures for the Bayesian Logit\nRegression model, highlighting predictor-outcome relation-\nships. Positive effects are observed for the \"intercept\" (2.34),\n\"marital\" (0.19), and \"education\" (0.08), while negative effects\nare seen for \"default\" (-0.44) and \"contact\" (-0.45). Credible\nintervals reflect the uncertainty of these estimates, with nar-\nrower intervals indicating greater precision.\nThe Rhat values of 1.00 for all variables indicate that\nthe Bayesian model has effectively converged. This shows\nthat the MCMC algorithm has mixed well and the posterior\ndistributions have stabilized. Additionally, high ESS Bulk and\nESS Tail values, such as for \"education\" and \"campaign,\"\nconfirm adequate sampling and reliable posterior estimates,\nensuring credible and well-converged results for the model."}, {"title": "B. Bayesian Probit Regression Model Implementation", "content": "The results of implementing the Bayesian probit regression\nmodel, taking into account twenty-one features, are presented\nfor consideration.\n1) Summary of the model: Table 2 presents the estimated\ncoefficients and diagnostics for the Bayesian Probit Regression\nmodel, showing the effects of predictor variables on the\noutcome. The \"intercept\" has a strong negative estimate (-\n3.62) with a narrow credible interval (-3.79 to -3.46). Small\nnegative effects are seen for \"default\" (-0.04), \"housing\" (-\n0.00), and \"contact\" (-0.05), while \"education\" (0.01) and\n\"marital\" (0.01) show small positive impacts. The estimates,\nalong with standard errors and credible intervals, provide\ninsights into the direction, magnitude, and uncertainty of these\neffects. The Rhat values of 1.00 for all variables confirm\nsuccessful convergence, indicating that the MCMC simulations\nhave mixed well and the posterior distributions are stable,\nensuring reliable parameter estimates.\nThe table reports high ESS Bulk values, such as for \"age\"\n(3528), \"job\" (4797), and \"education\" (5327), indicating pre-\ncise central tendency estimates from the posterior distribution.\nESS Tail values, robust for variables like \"housing\" (2799) and\n\"euribor3m\" (2832), confirm well-sampled extreme values.\nThese large ESS values indicate minimal autocorrelation in\nthe MCMC chains, ensuring the reliability of the posterior\nestimates.\nThe diagnostic metrics in Table 2 confirm the robustness and\nstability of the Bayesian Probit Regression model. Credible\nintervals reflect the uncertainty of each estimate. In contrast,\nthe coefficient estimates, Rhat, and ESS values together high-\nlight the reliability of the Bayesian estimation process and the\neffects of the predictor variables."}, {"title": "C. Model Comparison", "content": "Table 3 summarizes the results of a Leave-One-Out Cross-\nValidation (LOO-CV) comparison between the Bayesian Logit\nRegression model logit_model and the Bayesian Pro-\nbit Regression model probit_model. The table presents\nthe Expected Log Pointwise Predictive Density Difference\nelpd_diff and its associated standard error of the difference\nse_diff, allowing for an evaluation of these models pre-\ndictive performance. In this comparison, the logit_model\nserves as the reference model with an elpd_diff of\n0.0 and no associated uncertainty (se_diff = 0.0). The\nprobit_model, however, exhibits a significantly negative\nelpd_diff value of -1860.9, indicating that its predictive\nperformance is substantially worse than the logit_model.\nThe se_diff for the probit_model is 49.7, indicating a\ndegree of uncertainty in this performance difference. Despite\nthis uncertainty, the large negative elpd_diff suggests\nthat the logit_model outperforms the probit_model in\nterms of predictive accuracy based on LOO-CV results.\nThe comparison highlights that the logit_model is\nbetter suited for predicting the outcome in this context, as\nevidenced by its higher expected log pointwise predictive\ndensity. Although the probit_model shows some degree"}, {"title": "D. Out of Sample Prediction", "content": "Table 4 presents prediction results for the Bayesian Logit\nModel, summarizing key statistics for three instances. The first\ninstance has an estimated prediction of 0.257 with a standard\nerror of 0.437 and a 95% CI ranging from 0 to 1, indicating"}, {"title": "E. Discussion", "content": "The research findings demonstrate that both the Bayesian\nLogit and Probit regression models applied to 10,000 ob-\nservations provide reliable estimates for predicting outcomes,\nthough their predictive performance varies. In both models,\npredictors like \"marital status\", \"education\", and \"contact\"\nshow significant effects. As shown in Table 1, the Bayesian\nLogit Regression model converges effectively with Rhat values\nof 1.00 and sufficiently large ESS values, ensuring reliable\nparameter estimation. The Probit model on Table 2 also\nconverged with similar stability. Moreover, Table 3 compares\nthe two models using Leave-One-Out Cross-Validation (LOO-\nCV). The Logit model outperforms the Probit model, with\nan elpd_diff of 0.0 and -1860.9 for the Probit model,\nindicating better predictive accuracy for the Logit model\ndespite some uncertainty se_diff. On the other hand, Out-\nof-sample predictions in Tables 4 and 5 reflect similar patterns\nof uncertainty in both models, though the Logit model con-\nsistently provides better predictive performance. Overall, the\nBayesian Logit Regression model proves to be more accurate\nand reliable in forecasting outcomes."}, {"title": "VI. CONCLUSION", "content": "This study used a dataset from a Portuguese bank's direct\nmarketing campaign to assess the performance of logit and\nprobit models in predicting client subscriptions to bank term\ndeposits. To ensure fair and thorough testing of the models,\nthis research employed Bayesian modeling techniques and\nbalanced the dataset. Using Leave-One-Out Cross-Validation\n(LOO-CV) as a measure of prediction accuracy, this research\nresults showed that the logit model routinely beat the probit\nmodel. Based on these results, it seems like the logit model\nmight work better here for classifying customer subscription\nbehavior into two categories. In complicated fields like finan-\ncial marketing, the study shows how important it is to choose\nthe right models for imbalanced datasets. Financial institutions\ncan improve their decision-making processes, fine-tune their\nmarketing tactics, and better target their customers with the\nknowledge gathered from this research. To further enhance\nprediction accuracy in comparable applications, future studies\ncould investigate other modeling techniques, such as ensemble\nmethods or non-linear classifiers."}]}