{"title": "CLAP. I. Resolving miscalibration for deep learning-based galaxy photometric redshift estimation", "authors": ["Qiufan Lin (\u6797\u79cb\u5e06)", "Hengxin Ruan (\u962e\u6052\u5fc3)", "Dominique Fouchez", "Shupei Chen (\u9648\u6811\u6c9b)", "Rui Li (\u674e\u745e)", "Paulo Montero-Camacho", "Nicola R. Napolitano", "Yuan-Sen Ting (\u4e01\u6e90\u68ee)", "Wei Zhang (\u5f20\u4f1f)"], "abstract": "Obtaining well-calibrated photometric redshift probability densities for galaxies without a spectroscopic measurement remains a challenge. Deep learning discriminative models, typically fed with multi-band galaxy images, can produce outputs that mimic probability densities and achieve state-of-the-art accuracy. However, several previous studies have found that such models may be affected by miscalibration, an issue that would result in discrepancies between the model outputs and the actual distributions of true redshifts. Our work develops a novel method called the Contrastive Learning and Adaptive KNN for Photometric Redshift (CLAP) that resolves this issue. It leverages supervised contrastive learning (SCL) and k-nearest neighbours (KNN) to construct and calibrate raw probability density estimates, and implements a refitting procedure to resume end-to-end discriminative models ready to produce final estimates for large-scale imaging data, bypassing the intensive computation required for KNN. The harmonic mean is adopted to combine an ensemble of estimates from multiple realisations for improving accuracy. Our experiments demonstrate that CLAP takes advantage of both deep learning and KNN, outperforming benchmark methods on the calibration of probability density estimates and retaining high accuracy and computational efficiency. With reference to CLAP, a deeper investigation on miscalibration for conventional deep learning is presented. We point out that miscalibration is particularly sensitive to the method-induced excessive correlations among data instances in addition to the unaccounted-for epistemic uncertainties. Reducing the uncertainties may not guarantee the removal of miscalibration due to the presence of such excessive correlations, yet this is a problem for conventional methods rather than CLAP. These discussions underscore the robustness of CLAP for obtaining photometric redshift probability densities required by astrophysical and cosmological applications. This is the first paper in our series on CLAP.", "sections": [{"title": "1. Introduction", "content": "In cosmological and extragalactic studies, the cosmological redshift z characterises the cosmic distance, and it is a crucial quantity for probing the properties of galaxies and the evolution of the Universe. The most direct and accurate way to measure redshift is through spectroscopy. However, spectroscopic redshifts (spec-z) are highly time intensive to obtain. There have been multiple ongoing or planned galaxy imaging surveys in recent years, such as the Kilo-Degree Survey (KiDS; de Jong et al. 2013), the Dark Energy Survey (DES; Dark Energy Survey Collaboration et al. 2016), the Euclid survey (Laureijs et al. 2011), the Hyper Suprime-Cam (HSC; Aihara et al. 2018), the Nancy Grace Roman Space Telescope (Spergel et al. 2015), the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST; Ivezi\u0107 et al. 2019), and the China Space Station Telescope (CSST; Zhan 2018). They require redshift estimates for hundreds of millions or billions of galaxies for which spectroscopic measurements are infeasible. Given such extremely rich datasets, photometric redshifts (photo-z), measured typically using photometry, have become an alternative in order to meet the needs for large imaging surveys.\nThe idea behind photometric redshift estimation for individual galaxies lies in the mapping between the observed galaxy photometric or morphological properties and the redshift. Broadly speaking, two categories of methods are commonly leveraged for deriving individual redshift estimates (see Salvato et al. (2019) and Newman & Gruen (2022)). Template-fitting methods (e.g. Arnouts et al. 1999; Ben\u00edtez 2000; Feldmann et al. 2006; Ilbert et al. 2006; Greisel et al. 2015; Leistedt et al. 2019) determine redshifts by finding the best fit between the galaxy spectral energy distribution (SED) and a library of SED templates covering different physical and morphological properties such as galaxy types. Data driven or empirical methods determine redshifts by empirically learning the mapping from photometric data to redshift estimates without any theoretical modelling. These include unsupervised learning approaches in which known redshifts are given as external information, such as k-nearest neighbours (KNN; e.g. Zhang et al. 2013; Beck et al. 2016; De Vicente et al. 2016; Speagle et al. 2019; Han et al. 2021; Luken et al. 2022), self-organising maps (SOMs; e.g. Way & Klose 2012; Carrasco Kind & Brunner 2014; Speagle & Eisenstein 2017; Buchs et al. 2019; Wilson et al. 2020), and supervised learning approaches in which known redshifts are used as labels, such as artificial neural networks (ANNs; e.g. Collister & Lahav 2004; Brescia et al. 2014; Bonnett 2015; Cavuoti et al. 2015; Hoyle 2016; Sadeh et al. 2016; Cavuoti et al. 2017; Bilicki et al. 2018; D'Isanto & Polsterer 2018; Pasquet et al. 2019; Mu et al. 2020; Schuldt et al. 2021; Dey et al. 2022a; Ait Ouahmed, R. et al. 2024; Treyer et al. 2024), decision trees and random forest (e.g. Carliles et al. 2010; Carrasco Kind & Brunner 2013; Luken et al. 2022), boosted decision trees (e.g. Gerdes et al. 2010), support vector machines (SVMs; e.g. Jones & Singal 2017), and Gaussian mixture models (GMMs; e.g. D'Isanto & Polsterer 2018; Jones & Heavens 2019; Hatfield et al. 2020; Ansari et al. 2021).\nThanks to the recent advances in deep learning, a few studies (e.g. D'Isanto & Polsterer 2018; Pasquet et al. 2019; Schuldt et al. 2021; Dey et al. 2022a; Ait Ouahmed, R. et al. 2024; Treyer et al. 2024) have developed deep neural networks (DNNs) to predict photometric redshifts and achieve state-of-the-art predictive accuracy. These discriminative models, developed in a supervised end-to-end manner and usually having a large number of trainable weights, directly take multi-band stamp images of galaxies as inputs and produce redshift estimates. They are trained iteratively with mini-batches of training data, minimising a loss function via gradient descent in which spectroscopic redshifts are used as labels. In this way, the models can automatically establish a mapping from input images to the target redshifts. Studies such as Henghes et al. (2022), Li et al. (2022a), Zhou et al. (2022a), and Zhou et al. (2022b) explored hybrid networks that are fed with both images and multi-band photometry or fluxes. The outperformance of such image-based methods in accuracy over photometry-only methods suggests that other than photometry there may be extra information such as galaxy morphology encoded in images that helps improve redshift estimation (Soo et al. 2018).\nPhotometric redshift estimates for individual galaxies are usually considered in two forms: (a) the point estimate Zphoto derived from photometric data d (i.e. photometry or images) and (b) the (posterior) probability density estimate p(z|d) over possible values given photometric data d, accounting for imperfect knowledge in determining redshifts. This work concentrates on probability density estimation via deep learning.\nObtaining well-calibrated redshift probability density estimates is a challenging task, as reported for a large array of redshift estimation methods (e.g. Wittman et al. 2016; Tanaka et al. 2018; Amaro et al. 2019; Euclid Collaboration et al. 2020; Schmidt et al. 2020; Kodra et al. 2023). There have been several techniques coupled with deep learning models used to express redshift probability densities, including the softmax function (Pasquet et al. 2019; Treyer et al. 2024), GMMs (D'Isanto & Polsterer 2018), and Bayesian neural networks (BNNs; Zhou et al. 2022b), outputting vectors covering a redshift range and normalised to unity. Despite their high accuracy, the calibration of such estimates produced by neural networks remains an open issue. From a statistical viewpoint, a well-calibrated probability density estimate should fulfil the frequentist definition: for a given sample of galaxies, the integrated probability within any arbitrary redshift range must describe the relative frequency of true redshifts falling in this range (Dey et al. 2021, 2022b). While there are proofs that ANNs are capable of providing reliable posterior probability estimates (Richard & Lippmann 1991; Rojas 1996), several studies have shown evidence that the probability density outputs from discriminative models especially those with high capacity may suffer from miscalibration, that is, a model's confidence is not consistent with its accuracy (e.g. Guo et al. 2017; Thulasidasan et al. 2019; Minderer et al. 2021; Wen et al. 2021). In other words, miscalibration would lead to a deviation between the estimated p'(z|d) and the empirically correct p(z|d). Guo et al. (2017) pointed out that the extent of miscalibration generally grows with the model capacity for typical neural networks. As we show, neural networks similar to the one developed by Treyer et al. (2024) (with ~60 layers and ~7 million trainable parameters) already exhibit clear miscalibration behaviours. Furthermore, the variability stemming from the iterative training procedure would contribute additional uncertainties and increase the potential risk of miscalibration (Huang et al. 2021, 2023).\nUnlike point estimates that only give the collapsed values, well-calibrated probability density estimates are capable of describing the intrinsic dispersions due to the complex many-to-many mapping from photometric data to redshift, and describing the uncertainties due to the method implemented for redshift estimation and errors in photometric data. They are thus preferred for precision cosmology, in which one needs to understand and incorporate the full uncertainties in photometric redshift estimation into analysis (e.g. Mandelbaum et al. 2008; Myers et al. 2009; Bonnett et al. 2016; Abruzzo & Haiman 2019; Ruiz-Zapatero et al. 2023; Zhang et al. 2023). On the other hand, this leads to a strong requirement for the calibration of probability density estimates. The potential miscalibration issue associated with conventional deep learning would result in unreliable characterisation of uncertainties and would bias the photometric redshift estimation, consequently degrading the cosmological inference. In particular, miscalibrated probability densities for individual galaxies may induce biases on the mean redshifts estimated in tomographic bins and lead to poorly reconstructed redshift distribution over a galaxy sample, which are severe for weak lensing tomography (e.g. Ma et al. 2006; Huterer et al. 2006; Laureijs et al. 2011; Joudaki et al. 2020; Hildebrandt et al. 2020; Euclid Collaboration et al. 2021).\nIn our previous work (Lin et al. 2022) we developed a set of consecutive steps to correct biases in photometric redshift estimation with deep learning, potentially overcoming the impact of miscalibration on the mean redshifts in tomographic bins. However, this approach is limited to point estimate analysis, and suffers from a trade-off between constraining estimation errors and correcting biases, which originates from retraining a fraction of a network using a subset of training data and soft labels that enlarges estimation errors. We thus attempt to find a better solution to tackle the miscalibration issue for probability density estimation.\nIn this work we propose the Contrastive Learning and Adaptive KNN for Photometric Redshift (CLAP), a novel method that resolves the miscalibration encountered by conventional deep learning in the context of photometric redshift probability density estimation. CLAP leverages the gains from KNN and retains the advantages of deep learning simultaneously. It turns a discriminative model into a contrastive learning framework that projects galaxy images and additional input data to redshift-sensitive latent vectors, followed by an adaptive KNN algorithm and a KNN-enabled recalibration procedure to produce locally calibrated probability density estimates, which leverage diagnostics with the probability integral transform (PIT; Gass & Harris 2001). The contrastive learning is coupled with supervised learning in which spectroscopic redshifts are used as labels. We give a solution for alleviating the reliance on the expensive computation required for KNN, and suggest a proper way to combine an ensemble of probability density estimates for reducing uncertainties. We demonstrate CLAP using data separately from the Sloan Digital Sky Survey (SDSS; Eisenstein et al. 2011), the Canada-France-Hawaii Telescope Legacy Survey (CFHTLS; Gwyn 2012), and the Kilo-Degree Survey (KiDS; de Jong et al. 2013) As we show, the main advantages of CLAP include the following:\n- a substantial improvement on the calibration of probability density estimates thanks to KNN;\n- a high accuracy comparable to that obtained by conventional image-based deep learning methods, better than photometry-only KNN approaches such as Beck et al. (2016);\n- computational efficiency of end-to-end deep learning models retained by bypassing the intensive computation required for KNN.\nThis is in line with other works that demonstrate the merits of combining deep learning with KNN (e.g. Papernot & McDaniel 2018; Chen et al. 2020; Dwibedi et al. 2021; Sun et al. 2022; Liao et al. 2023). CLAP is analogous to likelihood-free inference approaches for parameter estimation and inference (e.g. Charnock et al. 2018; Fluri et al. 2021; Livet et al. 2021), but instead of exploiting simulated data for inference, we perform KNN on real data and labels to ensure the probability density estimates to be locally calibrated.\nWith CLAP as a reference, we delve into the miscalibration issue and showcase that it is a common shortcoming of conventional deep learning methods for probability density estimation. In particular, we present an illustration of miscalibration over different representative discriminative models, and reveal its association with uncertainties and correlations between data instances. In light of these findings, suggestions are given on circumventing miscalibration in deep learning in order to obtain reliable photometric redshift estimates applicable to actual astrophysical and cosmological analysis.\nAs a further note, another common issue for conventional methods is the mismatch between the spectroscopic sample used to train a model and the target (or test) sample to which the model is applied. As the learned distribution p(z, d) is fixed in the model once the model is trained, applying it to a target sample with a different distribution would induce biases. In contrast, CLAP has the flexibility to match the target distribution by assigning different weights to the nearest neighbours for each data instance, offering a possible way to circumvent mismatches. This is another advantage of CLAP over conventional methods. We note that a mismatch may also lead to inconsistency between a model's confidence and accuracy for the target sample, though we use 'miscalibration' to exclusively refer to the issue internal to the method implementation irrespective of mismatches. We will discuss the approaches built on CLAP for resolving mismatches in the next paper of our CLAP series, while in the current work we present the details of CLAP and solely focus on miscalibration.\nIn addition, we note that probability density estimates are denoted with probability density functions (PDFs) in most studies. From a rigorous statistical point, a PDF should characterise the dispersions intrinsic to a physical phenomenon, free from model assumptions, implementations, and noise in data. In actual applications, however, probability density estimates inevitably depend on the certain estimation method and the data quality. It is challenging to get rid of all those extra uncertainties, even after resolving miscalibration. We thus do not refer to probability density estimates as PDFs in order to avoid misuse of the terminology. For brevity, we adopt the expression p(z|d) without adding extra terms to the condition unless otherwise noted.\nThis paper organised is as follows. Section 2 describes the data used in this work, primarily galaxy images and spectroscopic redshifts. Section 3 introduces our method CLAP. The information about the network architectures is provided in Appendix A. In Sect. 4 we show our results on probability density estimation using CLAP. More results and discussions are presented in Appendices B, C, D, and E. An investigation on miscalibration is presented in Sect. 5. Finally, we summarise our results and give concluding remarks in Sect. 6. In the section dedicated to Data and Code Availability, we provide the photometric redshift catalogues produced by CLAP and the code used in this work."}, {"title": "2. Data", "content": "We took data from three imaging surveys, the Sloan Digital Sky Survey (SDSS), the Canada-France-Hawaii Telescope Legacy Survey (CFHTLS), and the Kilo-Degree Survey (KiDS), each of which covers a different redshift and magnitude range. The distributions of spectroscopic redshift and r-band magnitude for each dataset are shown in Fig. 1. The redshift coverage and sample division are detailed in Table 1."}, {"title": "2.1. Sloan Digital Sky Survey (SDSS)", "content": "The SDSS dataset used in this work contains 516524 galaxies from the Main Galaxy Sample available from SDSS Data Release 12 (Alam et al. 2015). These data were retrieved by Pasquet et al. (2019) and also used in Lin et al. (2022), with detailed information provided in Pasquet et al. (2019). This dataset has a low-redshift coverage z < 0.4 and a cut at 17.8 on dereddened r-band petrosian magnitude, providing a rich reservoir of data in a restricted parameter space. The galactic reddening E(B \u2013 V) for each galaxy along the line of sight is obtained using the dust map from Schlegel et al. (1998). Each galaxy is associated with a spectroscopically measured redshift. Five stamp images that cover five optical bands u, g, r, i, z were made to have the galaxy located at the centre and encompass 64 \u00d7 64 pixels in spatial dimensions, with each pixel covering 0.396 arcsec. The five images together with the galactic reddening E(B \u2013 V) are regarded as a data instance and used as input data for CLAP. While previous work noticed that inputting the information of point spread functions (PSFs) to deep learning models would improve the estimation of galaxy properties (e.g. Umayahara et al. 2020; Li et al. 2022b), we did not find a strong impact of PSFs on photometric redshift estimation for the data used in our work. Therefore, we did not include the PSF information in the input data for CLAP.\nVia complete random sampling, we selected 393 219 galaxies as a training sample, 20000 galaxies as a validation sample, and 103 305 galaxies as a target sample, similar to the sample division by Pasquet et al. (2019). Such division imposes that all the samples follow the same parent redshift-data distribution in spite of different sample sizes. This is the assumption we hold in this work without considering mismatches."}, {"title": "2.2. Canada-France-Hawaii Telescope Legacy Survey (CFHTLS)", "content": "We took the CFHTLS dataset that was described in detail in Treyer et al. (in prep.) and also used in Lin et al. (2022). It contains 158 193 galaxies observed in either the CFHTLS-Deep survey or the CFHTLS-Wide survey (Hudelot et al. 2012), with spectroscopic redshift up to z ~ 4.0 and dereddened r-band petrosian magnitude up to r ~ 27.0.\nSince CFHTLS had no spectroscopic observations, the spectroscopic redshifts were retrieved from a few other spectroscopic surveys. The majority is a collection of high-quality redshifts measured with high S/N spectra or multiple spectral features, obtained from the COSMOS Lyman-Alpha Mapping And Mapping Observations survey (CLAMATO; Data Release 1; Lee et al. 2018), the DEEP2 Galaxy Redshift Survey (Data Release 4; Newman et al. 2013), the Galaxy And Mass Assembly survey (GAMA; Data Release 3; Baldry et al. 2018), the SDSS survey (Data Release 12), the UKIDSS Ultra-Deep Survey (UDS; McLure et al. 2013; Bradshaw et al. 2013), the VANDELS ESO public spectroscopic survey (Data Release 4; Garilli et al. 2021), the VIMOS Public Extragalactic Redshift Survey (VIPERS; Data Release 2; Scodeggio et al. 2018), the VIMOS Ultra-Deep Survey (VUDS; Le F\u00e8vre et al. 2015), the VIMOS VLT Deep Survey (VVDS; Le F\u00e8vre et al. 2013), the WiggleZ Dark Energy Survey (Final Release; Drinkwater et al. 2018), and the zCOSMOS survey (Lilly et al. 2007). There is also a collection of low-resolution redshifts, acquired from the secure low-resolution prism redshift measurements from the PRIsm MUltiobject Survey (PRIMUS; Data Release 1; Coil et al. 2011; Cool et al. 2013) and the grism redshift measurements from the 3D-HST survey (Data Release v4.1.5; Skelton et al. 2014; Momcheva et al. 2016). In the 158 193 CFHTLS galaxies, 134759 have high-quality spectroscopic redshifts and 23 434 have low-resolution redshifts. Similar to the SDSS images, the stamp images of each CFHTLS galaxy cover five optical bands u, g, r, i, z and have 64 \u00d7 64 pixels with a pixel scale of 0.187 arcsec. The galactic reddening E(B-V) is also included as a part of the data.\nWe randomly selected 14759 galaxies as a validation sample and 20000 galaxies as a target sample, both from the high-quality collection. The remaining 100 000 galaxies and the low-resolution collection of 23 434 galaxies form a training sample. That is, for testing the results we only use the high-quality redshifts that are assumed to be secure, while the low-resolution redshifts are just used to increase statistics in training. In addition, we do not treat the CFHTLS-DEEP sample and the CFHTLS-WIDE sample separately as in Lin et al. (2022)."}, {"title": "2.3. Kilo-Degree Survey (KiDS)", "content": "We used the KiDS dataset retrieved by Li et al. (2022a) from the KiDS Data Release 4 (Kuijken et al. 2019). There are 134 147 galaxies in this dataset, with spectroscopic redshift up to z ~ 3.0 and dereddened r-band 'Gaussian Aperture and Point spread function (GAaP)' magnitude up to r ~ 24.5. The detailed description can be found in Li et al. (2022a). Similar to the CFHTLS dataset, the spectroscopic redshifts of the KiDS galaxies were acquired from other surveys including the Chandra Deep Field South (CDFS; Szokoly et al. 2004), the DEEP2 Galaxy Redshift Survey, the Galaxy And Mass Assembly survey (GAMA), and the zCOSMOS survey. The stamp images from KiDS cover four optical bands u, g, r, i and have a pixel scale of 0.2 arcsec. Again, we set the spatial dimensions to be 64 \u00d7 64.\nIn addition to the KiDS images, this dataset includes the dereddened GAaP magnitudes in the five near-infrared (NIR) bands Z, Y, J, H, Ks from the VISTA Kilo-degree Infrared Galaxy Survey (VIKING; Edge et al. 2014), as well as the galactic reddening E(B - V). Nonetheless, we refer to this dataset as 'the KiDS dataset' for brevity throughout the paper.\nSimilar to Li et al. (2022a), we randomly selected 100000 galaxies as a training sample, 14147 galaxies as a validation sample, and 20000 galaxies as a target sample."}, {"title": "3. Our method: CLAP", "content": "Algorithm 1 CLAP\nSupervised contrastive learning: Compress high-dimensional input data to low-dimensional redshift-sensitive latent vectors so that the subsequent KNN can be performed.\nAdaptive KNN: Obtain the optimal number of nearest neighbours for each data instance via local PIT diagnostics, whose known spectroscopic redshifts are used to construct an initial probability density estimate.\nRecalibration: Ensure that the obtained probability density estimates are locally calibrated.\nRefitting: Resume an end-to-end model in order to bypass the expensive computation of KNN and regain the computational efficiency of deep learning.\nCombining an ensemble of estimates: use the harmonic mean to combine the probability density estimates from an ensemble of models developed following the procedures above."}, {"title": "3.1. Overview", "content": "This section introduces our method CLAP. As illustrated in Fig. 2 and summarised in Algorithm 1, a CLAP model is developed via supervised contrastive learning (SCL), adaptive KNN, reconstruction, and refitting, based on deep learning neural networks. For the model inputs, we prefer multi-band galaxy images rather than magnitudes or colours alone, because the images contain more information than photometry and potentially enable better accuracy to be achieved in photometric redshift estimation. On the other hand, as we demonstrate, the KNN algorithm is a key component of CLAP in resolving miscalibration, while the multi-band images have a large number of dimensions such that directly implementing the KNN on the images is infeasible. Therefore, CLAP first leverages contrastive learning to project the multi-band images and additional input data (i.e. galactic reddening E(B \u2013 V), magnitudes) to low-dimensional redshift-sensitive latent vectors that form a latent space with a defined distance metric, enabling the subsequent KNN to be performed. It is essentially a deep learning-based compression of complex high-dimensional data. The contrastive learning is coupled with supervised learning using the spectroscopic redshift labels for better extraction of redshift information.\nThe adaptive KNN is implemented on the obtained latent space. For each data instance from the target sample or the validation sample, the optimal k value (i.e. the number of the nearest neighbours), which defines its neighbourhood, is determined via diagnostics based on the local PIT distributions. The PIT values are computed using the known spectroscopic redshifts of the selected nearest neighbours from the training sample. Once the k value is determined, the known redshifts of the k nearest neighbours within the neighbourhood are used to construct a probability density estimate, which is then recalibrated via local PIT diagnostics on the nearest neighbours again. The PIT values used for recalibration may be computed using the known redshifts from the validation sample. In essence, with the adaptive KNN and the KNN-enabled recalibration, the joint distribution p(z, d) for the given dataset is modelled locally by leveraging the neighbourhood of the projection \u03a6(d) in the latent space, where \u03a6 stands for the implemented model for projecting data in SCL that has been omitted in the expression p(z|d) for brevity.\nThe computational complexity of KNN precludes its use for processing large amounts of data envisioned by future imaging surveys. In order to avoid running computationally expensive KNN for every data instance from a potentially large target sample, a refitting procedure is implemented to resume an end-to-end model ready to directly produce the desired probability density estimates given input images, which regains the efficiency of deep learning.\nFinally, for reducing uncertainties and improving accuracy, we develop an ensemble of CLAP models following the procedures above and combine the estimates from the ensemble. We propose that using the harmonic mean is a proper way for performing such combination.\nThe details of CLAP are elaborated in the following subsections."}, {"title": "3.2. Supervised contrastive learning", "content": "We adopted a contrastive learning technique to establish a latent space that encodes redshift information (see Huertas-Company et al. 2023 for a review of contrastive learning in astrophysics). The essence of contrastive learning is to have positive pairs and negative pairs simultaneously, minimising the contrast between positive pairs and maximising the contrast between negative pairs. A naive choice of generating positive pairs would be to modify the images on the pixel level such as colour jittering, resizing, smoothing, and adding noise, such that contrastive learning is performed in an unsupervised manner (e.g. Hayat et al. 2021; Wei et al. 2022). However, our initial experiments showed that unsupervised contrastive learning with such pixel-level operations failed to produce a good representation for redshift, presumably because redshift is complex high-order information in a multi-dimensional spectroscopic and photometric parameter space, and may have exquisite dependence on pixel intensities (Campagne 2020). Therefore, we propose to incorporate supervised learning in the contrastive learning technique using spectroscopic redshift labels, so that meaningful redshift information can propagate from input data to the latent space. We also consider implementing a deep learning-based reconstruction of redshift-informed images to provide positive counterparts for the original images to avoid performing pixel-level operations.\nIn addition, we do not perturb input data to account for the noise in data (or 'aleatoric' uncertainties) like that applied in MLPQNA (Brescia et al. 2014; Cavuoti et al. 2015, 2017), because, again, this may destruct redshift information. In fact, the errors in the input data from the training sample can be encoded in the latent space in the training process, and then encapsulated in the obtained probability density estimates by KNN for the target sample. Assuming that the errors in the training data are representative of those in the target data, these errors can be well accounted for without any particular treatment. While we assume that the spectroscopic redshifts used in this work are noiseless (except the low-resolution redshifts from the CFHTLS dataset), the possible errors in the spectroscopic redshifts from the training sample can also be encapsulated in the same way.\nWith these considerations, we developed a supervised contrastive learning (SCL) framework customised for the given problem. It consists of an encoder network, an estimator network, and a decoder network. The encoder is fed with multi-band galaxy images (denoted with I) and additional input data, and outputs two low-dimensional vectors va and UB. The vector va is used to encode redshift information, which we refer to as the 'latent vector' that is taken to form a latent space, while the vector UB is used to encode other information extracted from the input data needed for reconstructing images. The vector VA is inputted to the estimator that produces a redshift output having the form of a probability density (denoted with q(z|d)), which is expressed in a series of bins given by the softmax function applied on the last fully connected layer and trained in a classification setting (i.e. considering each redshift bin as a class). The vectors UA and UB are concatenated and inputted to the decoder, reconstructing images that resemble the original inputs I. The reconstructed images (denoted with I,) together with the original additional input data are re-inputted to the encoder, producing vectors ' and 'g, and subsequently a redshift output q'(z|d) and images I. In this second pass, the encoder, the estimator, and the decoder all use shared weights. Furthermore, we inputted the images I* that are original but reshaped with random flipping and rotation by 90 deg steps, yet the redshift is not modified under the assumption of spatial invariance. Correspondingly, A, B, q*(z/d), and I were obtained.\nIn this framework, we regarded the images I, and I (or equivalently, the latent vectors va and va) for the same galaxy as a positive pair on condition that redshift information has been passed to va, va, and Ir. I* and I (or v and v\u0104) for the same galaxy were regarded as another positive pair, though this would not extract meaningful redshift information but only disentangle spatially variant information such as galaxy morphology. Lastly, the original images I\u2081 and I2 for any two different galaxies (or the corresponding latent vectors UA1 and UA2) were regarded as a negative pair.\nWe adopted the Euclidean distance as a metric to characterise the contrast between two latent vectors UA1 and UA2,\n$D_{Euclidean}(u_{A1}, u_{A2}) = \\sqrt{\\sum_{l=1}^L \\mid u_{A1, l} - u_{A2, l} \\mid^2},$ (1)\nwhere the index l runs over the L dimensions of the latent vectors. For a mini-batch of n data instances from the training sample, we obtained the latent vectors {a,i}, {0,}, and {0,}, where the index i runs over all the data instances. Both (UA, UA,i) and (UA, UA,i) for each data instance were used as positive pairs, having 2n pairs in total. The similarity function for all the positive pairs was defined as\n$Sim_p = exp \\left( - \\frac{1}{2n} \\sum_{i=1}^{n} (D_{Euclidean}(u_{Ai}, u_{A_i}) + D_{Euclidean}(u'_{Ai}, u_{A_i}))) \\right).$ (2)\nWe generated n/2 negative pairs from {va,i} each having two different galaxies (denoted with (UA,i,, UA,i,)). The similarity function for all the negative pairs was defined as\n$Sim_n = \\frac{2}{n} exp \\left( \\sum_{(i_p, i_p) \\in \\{i\\}} D_{Euclidean}(u_{Ai_p}, u_{Ai_p}) \\right).$ (3)\nWe then defined the contrastive loss function as\n$L_{Contrastive} = -log \\frac{Sim_p}{Sim_p + Sim_n}.$ (4)\nWe converted the spectroscopic redshifts into one-hot labels using the same binning as the softmax output, assuming no spectroscopic errors beyond the bin size. The same one-hot label (denoted with y) was used to supervise both q(z|d) and q'(z|d) via the cross-entropy loss function\n$L_{CE,y} = - \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{m} y_{ij} (log q_{ij} + log q'_{ij}),$ (5)\nwhere the index i runs over the n data instances in the mini-batch; the index j runs over the m redshift bins for each data instance. We also used the cross-entropy to impose mutual consistency between q(z|d), q'(z|d), and q*(z|d),\n$L_{CE,q} = -\\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^m (\\hat{q_{ij}} log q_{ij} + \\hat{q'_{ij}} log q'_{ij})$\n$-\\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^m (\\hat{q_{ij}} log q^*_{ij} + \\hat{q^*_{ij}} log q_{ij}),$ (6)"}]}