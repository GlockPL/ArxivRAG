{"title": "AI ALIGNMENT AT YOUR DISCRETION", "authors": ["Maarten Buyl", "Hadi Khalaf", "Claudio Mayrink Verdun", "Lucas Monteiro Paes", "Caio C. Vieira Machado", "Flavio du Pin Calmon"], "abstract": "In AI alignment, extensive latitude must be granted to annotators, either human or algorithmic, to judge which model outputs are 'better' or 'safer.' We refer to this latitude as alignment discretion. Such discretion remains largely unexamined, posing two risks: (i) annotators may use their power of discretion arbitrarily, and (ii) models may fail to mimic this discretion. To study this phenomenon, we draw on legal concepts of discretion that structure how decision-making authority is conferred and exercised, particularly in cases where principles conflict or their application is unclear or ir- relevant. Extended to AI alignment, discretion is required when alignment principles and rules are (inevitably) conflicting or indecisive. We present a set of metrics to systematically analyze when and how discretion in AI alignment is exercised, such that both risks (i) and (ii) can be observed. Moreover, we distinguish between human and algorithmic discretion and analyze the discrepancy between them. By measuring both human and algorithmic discretion over safety alignment datasets, we reveal layers of discretion in the alignment process that were previously unaccounted for. Furthermore, we demonstrate how algorithms trained on these datasets develop their own forms of discretion in interpreting and applying these principles, which challenges the purpose of having any principles at all. Our paper presents the first step towards formalizing this core gap in current align- ment processes, and we call on the community to further scrutinize and control alignment discretion.", "sections": [{"title": "1 Introduction", "content": "AI alignment aims to ensure that artificial intelligence (AI), like large language models (LLMs), \u2018behaves'\u00b9 in accor- dance with human intentions and social, legal, and ethical principles [55]. Particular interest has gone to aligning LLMs through learning from human feedback, which involves (i) collecting examples of possible AI outputs, (ii) an- notating these examples by asking human annotators \u201cwhich output is better\" (typically with limited instructions), and (iii) training the model to follow these human preferences [80]. This example-based approach to alignment is widely deployed in practice [80, 55, 36, 72]. Nevertheless, a gap remains between translating human intentions and social, legal, and ethical principles into a simplistic decision of \u201cwhich output is better.\u201d This gap gives annotators extensive discretion in defining what alignment means in practice, hindering the interpretability of the alignment process."}, {"title": "2 Related work", "content": "To our knowledge, we are the first to study discretion in AI alignment empirically. Next, we review threads of related work that inform our analysis.\nAlignment from human feedback aims to ensure that model outputs are in accordance with user expectations using human feedback [18]. The popular approach is Reinforcement Learning from Human Feedback (RLHF), which is discussed in Sec. 4.1. Researchers have developed many algorithms to perform RLHF like [114, 80]. At the same time, multiple vulnerabilities were found in this process, leading to a diverse set of jailbreak attacks [85]. Our work differs from previous contributions by identifying a fundamental limitation that is independent of the algorithm used to perform RLHF - the excessive discretionary power inherent in annotation processes, which persists even in recent variants like DPO [90] and KTO [37].\nAlignment from AI feedback aims to ensure that model outputs are in accordance with user expectations without using human feedback. The main example of such alignment from AI feedback is Constitutional AI [6], which defines an explicit list of principles to align to. Language models are then used to generate and annotate examples to follow these principles [6]. However, as noted in Anthropic's public discussion of Claude's constitution [2], principles are applied stochastically during training: \"The model pulls one of these principles each time it critiques and revises its re- sponses during the supervised learning phase, and when it is evaluating which output is superior in the reinforcement learning phase. It does not look at every principle every time, but it sees each principle many times during training.\" This stochastic approach leaves open questions about principle prioritization and conflict resolution. Collective Con- stitutional AI developed a framework to learn principles from users instead of arbitrarily defining them [48]. Recent work has expanded these foundations through various frameworks like [74, 29]. In general, these approaches never require direct human supervision, removing the power of discretion from users and deferring it to the models. Our work learns how principles are encoded and prioritized in human preference data, analyzing the human discretion contained in these datasets, and the discretion of models trained in these datasets.\nPrinciples and human preferences. Recent papers have tried to learn a set of principles encoded in preference datasets. This is an instance of the problem of bridging principles to practice [25]. The Value Imprint [77] established a framework for auditing human values embedded within preference datasets by developing a taxonomy of human values. Moreover, [60] propose an approach inspired by moral philosophy to determine and reconcile relevant values from diverse human inputs. Drawing inspiration from constitutional design, [41] proposes a framework to distill individual and group-level preferences into a set of principles to guide model behavior. Our work builds upon these by analyzing how (i) humans prioritize different principles by analyzing discretion and (ii) how these principles are learned by models by analyzing algorithmic discretion. Ultimately, we argue that there is currently an excessive amount of discretion in the hands of model developers and annotators.\nPluralistic alignment expanded alignment approaches by embracing diverse human values and perspectives [100]. Key developments include the Value Kaleidoscope taxonomy of values and rights [99], the PRISM dataset for mul- ticultural feedback [58], frameworks for leveraging community-specific LMs [40], and approaches that consider the temporal aspects of pluralistic alignment with multiple stakeholders [59]. While these works focus on gathering diverse perspectives and defining principles, our research specifically analyzes how to weigh and resolve conflicts between different principles. This goes in line with the recent push for a social-choice approach to alignment to ag- gregate and reconcile preferences of diverse annotators and principles [21]. Our work differs from this literature by offering transparency over what we are aligning to, complementary to the question of who we are aligning to, and ultimately indicating whether aligning to a list of rules produces AI-adherence to a system of values, as a rule-based system would. We hope that future work analyzes how different communities exercise their power of discretion.\nAnnotator disagreement is a well-studied problem in natural language tasks [94, 106, 14] as it impacts all stages of the usual ML pipeline [88]. [112] shows that how these disagreements are often rooted in personal biases rather than annotation errors. With existing alignment methods typically depending on a single ground-truth label, we risk privileging certain views at the expense of others, thereby ushering in a tyranny of the majority [39]. For this reason, scholars proposed approaches to better aggregate conflicting annotations beyond majority voting by using Bayesian approaches [83] and proposing model architectures that handle multiple annotations [73]. The challenge of annotation disagreement becomes particularly relevant with the increasing use of LLM evaluators, leading recent work to focus on measuring and reducing biases in their evaluations [68, 110] and improving their reliability and interpretability [66]. Discretion in AI alignment reveals the principles influencing annotator decisions and how annotators prioritize conflicting principles, explaining why annotators disagree as a function of their principles.\nAI Alignment and Law. Recent legal literature has argued that AI alignment operates in a similar fashion to the legal system [15, 1, 76]. The authors emphasize the role of interpretation and application of normative principles to guide AI behavior. For instance, [15] argues that AI alignment faces similar challenges in accommodating diverse"}, {"title": "3 From judicial discretion to alignment discretion", "content": "Discretion lies at the heart of AI alignment, as annotators are necessary to label which model outputs are \"better\" or \"safer.\" Such discretion manifests when annotators assess outputs where principles conflict or provide insufficient guidance. We here remark that employing humans as annotators is expensive and carries ethical risks [33, 47]. As Al models become increasingly powerful, it has become popular to instead use algorithms as a cheaper source of preference annotations [6, 64, 22]. Alignment discretion can thus involve both human and algorihmic discretion."}, {"title": "3.1 Why analyze alignment discretion?", "content": "The hypotheses in Fig. 1 exemplify that the underlying exercise of discretion can fundamentally alter how outputs are judged, thus determining whether the AI should refer to a medical doctor or suggest medication. Moreover, we cannot effectively ensure that AI systems properly learn from and respect the legitimate diversity of human judgments.\nMore broadly, parallels can be drawn with judicial discretion. Indeed, legal theorists have long recognized that dis- cretion in judicial systems \u2013 arbitrium judicis \u2013 requires careful structuring to ensure transparency, accountability, and legitimacy [26, 63] \u2013 discretion in AI alignment demands similar scrutiny. Without understanding and structuring this discretion, we cannot know what we are aligning to or whether we are successful, and we risk embedding unexamined value judgments into AI systems that become resistant to auditing or revision once deployed."}, {"title": "3.2 How do judicial discretion and alignment discretion relate?", "content": "As Caputo [15] observed, jurisprudence and AI alignment share fundamental challenges in translating abstract prin- ciples into concrete decisions while maintaining consistency and legitimacy. In both contexts, decision-makers must navigate what Dworkin [32] terms the \u201cdimension of weight.\" Unlike rules, principles do not have an \"all-or-nothing\" application but must be weighted against each other. Judicial discretion and alignment discretion thus appear to share strong similarities, potentially making the judicial process a rich source of inspiration for better-interpreted alignment. To understand how far this inspiration can take us, we discuss key parallels and differences."}, {"title": "3.2.1 Parallels", "content": "Principle Application and Generalization. Both must apply broad and abstract principles to specific situations that may not have been anticipated when those rules were created. Judges interpret laws for novel situations; AI systems must apply alignment principles to unforeseen prompts.\nConsistency vs. Flexibility. Both must balance maintaining consistent application of principles with flexibility in adapting to nuanced contexts. Legal systems strive for predictability while allowing for case-specific considerations [46] - AI systems must also provide consistent responses while appropriately handling context-dependent ethical considerations. This balance relies on building precedents and a consistent understanding of many cases; judges through years of legal practice and life experience, annotators through their lived experience, and AI systems through exposure to vast amounts of text that captures human decision-making patterns.\nManaging Conflicts. Both must ponder and balance competing principles. Courts often resolve competing rights or interests; AI must weigh a range of alignment principles that may suggest different courses of action."}, {"title": "3.2.2 Differences", "content": "Scale and Granularity. Judges make high-stakes decisions about consequential real-world actions. Conversely, alignment annotators exercise discretion over countless seemingly minor choices about model outputs. However, the discretion exercised in these goes unnoticed and unaccounted for, producing impacts that can be both impactful in specific cases and/or accumulate to create inscrutable interpretations of principles in the long run.\nHuman-Algorithm Translation. Unlike judicial systems where discretion is exercised by human judges within es- tablished frameworks, AI alignment involves a complex interplay between annotators humans or algorithms \u2013 and"}, {"title": "4 Formalizing pairwise preferences", "content": "In this section, we define preference functions that express which candidate AI output is preferred by an annotator \u2013 human or algorithmic. We also define principle-specific preference functions for a particular alignment principle (e.g., \"don't help with illegal activities\u201d) and assess which model output better adheres to the principle. First, however, we give a brief background on aligning an LLM's outputs to pairwise preferences."}, {"title": "4.1 A brief background", "content": "The most prominent form of alignment employs pairwise preferences to perform Reinforcement Learning from Human Feedback (RLHF) [18, 109, 102]. For a query $x \\in X$, we denote the set of possible answers to the query as $Y$. A (pairwise) preference over a pair of responses $y_0 \\in Y$ and $y_1 \\in Y$ is denoted as $y_1 \\succ y_0$ if $y_1$ is preferred over $y_0$.\nIn RLHF, it is commonly assumed that these pairwise preferences follow the Bradley-Terry-Luce (BTL) model [13] (see also [69, 23, 87, 24, 49, 38, 45]). For a pair of items ($y_0$, $y_1$), it expresses the probability of preferring $y_1$ over $y_0$ by assuming each response has an latent 'quality'. Estimating this quality with a reward model $r_\\phi$, the BTL model is\n$P(y_1 \\succ y_0 | x) = \\sigma(r_\\phi(x,y_1) - r_\\phi(x, y_0)) $\nwhere $\\sigma$ is the logistic sigmoid function. The reward model $r_\\phi$ is trained by minimizing the cross-entropy between the prediction of $y_1 \\succ y_0$ according to (1) and ground truth preference labels. An LLM with policy $\\pi_\\theta$, i.e. the function that computes the probability that an output y should follow a context x, can then be aligned by training it to maximize the reward $r_\\phi$ while staying close to a reference model $\\pi_{ref}$:\n$L_{RLHF}(\\pi_\\theta, \\pi_{ref}, r_\\phi, \\lambda) = E_{x\\sim D} E_{y \\sim \\pi_\\theta(\\cdot|x)} [r_\\phi(x, y)] + \\lambda \\cdot KL(\\pi_\\theta||\\Pi_{ref}).$\nwhere D is a distribution over prompts. The reference policy $\\pi_{ref}$ is typically a pre-trained language model and ensures the model retains its general language capabilities and knowledge, with $\\lambda$ controlling the strength of this constraint."}, {"title": "4.2 Human vs algorithmic annotators", "content": "The alignment approach in Sec. 4.1 makes no distinction about who prefers $y_1$ over $y_0$. Yet, to compare discretion between annotators in Sec. 5.3, we will distinguish between human and algorithmic annotators. To simplify notation, we first introduce preference functions that, given a query x and two candidate responses $y_0$ and $y_1$, outputs 1 if the annotator prefers response $y_1$, -1 if $y_0$ is preferred, and 0 if the annotator is indifferent."}, {"title": "Definition 1 (preference functions)", "content": "A preference function denoted by $Pref_a(y_1 \\succ y_0 | x) \\in [-1,0,1]$ is a ternary- valued function that expresses the preference of annotator a over $(y_0, y_1)$ for the context x. It is defined as\n$Pref_a (y_1 \\succ y_0 | x) = \\begin{cases} 1, & \\text{if a prefers } y_1, \\text{ i.e. } Y_1 \\succ Y_0 \\\\ -1 & \\text{if a prefers } y_0, \\text{ i.e. } Y_0 \\succ Y_1 \\\\ 0, & \\text{if a is indifferent towards } y_0 \\text{ and } y_1, \\text{ i.e. } (Y_1 \\nsucc Y_0) \\land (Y_0 \\nsucc Y_1). \\end{cases}$\nWe simply use $Pref_a$ if the argument tuple $(y_0, Y_1, x)$ is clear from the context."}, {"title": "Definition 2 (reward model preference functions)", "content": "We set the preference function $Prefr$ of a reward model $r_\\phi$ as\n$Prefr (y_1 \\succ y_0 | x) \\equiv sign(r_\\phi(x, y_1) \u2013 r_\\phi(x, y_0)).$\nIntuitively, reward model preferences $Pref = 1$ prefer $y_1$ iff $r_\\phi$ assigns a higher \u2018reward' to $y_1$ than to $y_0$.\nTo instantiate the 'preference' of an LLM, we make use of its policy $\\pi_\\theta$, e.g. as optimized in (2), which outputs the probability $\\pi_\\theta(y|x)$ that y ought to be generated in response to context x. We could mirror (4) by setting $Pref_{\\pi_\\theta} = sign(\\pi_\\theta(Y_1|x) \u2013 \\pi_\\theta(Y_0 | x))$. However, we opt to instead show all of (x, $y_0$, $y_1$) to the LLM at once in a prompt where we 'ask' whether it prefers $y_0$ or $y_1$, just as we would ask a human annotator. Indeed, the latter has become the norm when using LLMs to judge fixed pairs of responses ($y_0$, $y_1$) [70] because the actual scores $\\pi_\\theta(y_0|x)$ and $\\pi_\\theta(y_1|x)$ may be poorly calibrated for responses that the model is unlikely to output itself [103]."}, {"title": "Definition 3 (LLM preference functions)", "content": "We set the preference function $Pref$ of an LLM with policy $\\pi_\\theta$ as\n$Pref_{\\pi_\\theta}(Y_1 \\succ Y_0 | x) = \\begin{cases} 1 & \\text{if \u201cResponse 1\u201d = } arg \\underset{z}{max} \\pi_{\\theta}(z | T(x, Y_0, Y_1)) \\\\ -1 & \\text{if \u201cResponse 0\u201d = } arg \\underset{z}{max} \\pi_{\\theta}(z | T(x, Y_0, Y_1)) \\\\ 0 & else \\end{cases}$\nwith T a composition of (x, $y_0$, $y_1$) into a textual prompt that \u2018asks' an LLM with policy $\\pi_\\theta$ whether \u201cResponse 0\" or \u201cResponse 1\" (representing $y_0$ and $y_1$ respectively) is \u201cbetter\", while optionally specifying that the LLM is allowed to choose neither if none are clearly better. The exact template is provided in Appendix B.5."}, {"title": "4.3 Principle-specific preferences", "content": "As the final ingredient to characterize discretion in Sec. 5, we formalize what it means for a preference $y_1 \\succ y_0$ to 'adhere' to a principle c. For this, we introduce principle-specific preferences $Y_1 \\succ_c Y_0$."}, {"title": "Definition 4 (principle-specific preferences)", "content": "For a principle c \u2208 C, the principle-specific preference $Y_1 \\succ_c Y_0$ ex- presses that $y_1$ better adheres to the principle c than $y_0$ does.\nA key property of principle-specific preferences is that they can be far more objective than generic preferences. For example, we could define a principle c = \u201cmaximize output length\" and verify $y_1 \\succ_c y_0$ by simply counting characters in $y_0$ and $y_1$. More importantly, we argue less discretion is required to verify whether $Y_1 \\succ_c Y_0$ holds for a principle like c = \"don't help with illegal activity\u201d than for abstractly assessing which response is \u201cmore harmless\u201d or \u201csafer\u201d.\nTo compute our discretion metrics, we will therefore assume an oracle is available that can perfectly judge principle- specific preferences > for each c \u2208 C. In the preference function notation of Def. 1, we denote this judgment as $Pref_{oracle} (Y_1 \\succ c Y_0 | x)$. We then (slightly) overload this notation to define principle-specific preference functions that, given a query x and two answers for the input $y_0$ and $y_1$, outputs 1 if the annotator believes response $y_1$ is more aligned with principle c, -1 if $y_0$ is more aligned with principle c, and 0 if the annotator is indifferent."}, {"title": "Definition 5 (principle-specific preference functions)", "content": "Assuming the availability of an oracle to judge principle- specific preferences >c, we denote principle-specific preference functions $Pref_c$ for principle c \u2208 C as\n$Pref_c(y_1 \\succ y_0 | x) \\doteq Pref_{oracle}(Y_1 \\succ_c Y_0 | x).$\nPrinciple-specific preference functions $Pref_c$ allow us to assess the (dis)agreement between a preference $Pref_a$ and a principle c. For example, $Pref_a \\times Pref_c = 1$ holds if they both prefer the same y, and $Pref_a \\times Pref_c = -1$ holds if they disagree. If either is indifferent, we will have $Pref_a \\times Pref_c = 0$. We remark that the oracle assumption in Def. 5 clearly poses limitations, as many principles are too vague to be assessed without requiring its own discretion (which is out of scope for this work). In our experiments, we will use an LLM as the oracle, computing its principle-specific preferences > similarly to (5). We discuss this in detail in Sec. B.3."}, {"title": "5 Alignment Discretion", "content": "We define alignment discretion as the latitude afforded to annotators to operationalize alignment principles. Building upon parallels with legal theory (Sec. 3) and preference functions (Sec. 4), we now formalize when and how discretion is exercised. The resulting metrics will allow us to measure the discrepancy between human and algorithmic discretion."}, {"title": "5.1 When is discretion required?", "content": "Intuitively, if a response $y_1$ is preferred over $y_0$ by all principles c \u2208 C, then no discretion is required; preferring $y_0$ is irrational according to the principles. Principles may also conflict, however, or they may all be indifferent. We then cannot determine the best output with principles alone, and require an annotator to exercise discretion by stating their preference, thereby prioritizing certain principles. When assessing the agreement among principles based on their preference function Pref (see Def. 5), we distinguish three fundamental cases: consensus, conflict, and indifference."}, {"title": "Definition 6 (consensus, conflict, & indifference)", "content": "Given principle-specific preferences $Pref = Pref_c(y_1 \\succ y_0 | x)$ for all c \u2208 C, exactly one of the following holds for candidate responses ($y_0$, $y_1$) in context x:\nA. principle consensus (Consensusc): At least one principle prefers one response and no principles disagree:\n$Consensus_c = (\\forall c_1, c_2 \\in C : Pref_{c1} \\times Pref_{c2} \\neq \u22121) \\land (\\exists c \\in C : Pref_c \\neq 0).$\nB. principle conflict (Conflictc): At least two principles disagree on the preferred output:\n$Conflict_c = \\exists c_1, c_2 \\in C: Pref_{c1} \\times Pref_{c2} = -1.$\nC. principle indifference (Indifferencec): All principles are indifferent:\n$Indifference_c = \\forall c \\in C : Pref_c = 0.$\nThe three cases of principle agreement in Def. 6, illustrated in Fig. 2, are determined solely by examining the principle- specific preferences Pref. Their classification requires no additional human annotations or model outputs beyond the initial principle-specific assessments provided by the oracle model for each principle c \u2208 C.\nPrinciple consensus allows for no discretion, as it fully determines the best output according to the set of principles. From this perspective, any annotation or behavior that disagrees with the consensus could be considered \u2018arbitrariness', which we formally measure in Def. 7.\nPrinciple conflicts create legitimate tension between competing objectives. Hence, principle conflicts call for mean- ingful discretion to be exercised by an annotator. The annotator is empowered to choose which response they prefer, and thus which principle ought to win out. Such supremacy of principles is characterized in Def. 8.\nPrinciple indifferences provide no meaningful guidance and thus only allow for unconstrained discretion, meaning annotators are still free to prefer either response, but they cannot be explained through any (known) principle. Such lack of constraint limits the legitimacy of the annotation, as it may be irrational, idiosyncratic, or meaningless.\nAmong the three cases, only principle consensus eliminates the need for annotators. Hence, to increase the power of principles over annotators, conflict and indifference may be reduced by intervening on the (i) principle set C or (ii) the dataset of response pairs ($y_0$, $y_1$). Unfortunately, this may prove challenging. Adding new principles to C or making response pairs ($y_0$, $y_1$) more distinct may reduce indifference, in turn leading to more consensus but likely also to more conflict. Similarly, conflict can be reduced by removing controversial principles from C or by making principles more specific, but doing so may only increase the frequency of consensus at the cost of increased indifference."}, {"title": "5.2 How is discretion exercised?", "content": "We now characterize how discretion is used by an annotator denoted by a. First, we measure how often their discretion is arbitrary. Second, we model how much they prioritize each principle. For both, we work with empirical probabilities Pr() computed over a dataset D consisting of tuples (x, $y_0$, $y_1$), treating the dataset as our sample space.\nWe say that discretion is arbitrary when the annotator disagrees with a principle consensus. As argued in Sec. 5.1, we may want to avoid such disagreement entirely for a desirable set of principles. Hence, we measure how often it occurs."}, {"title": "Definition 7 (Discretion Arbitrariness)", "content": "Given principle-specific preferences $Pref = Pref_c(y_1 \\succ y_0 | x)$ for all c \u2208 C, an annotator a's discretion arbitrariness (DA) is empirically measured as\n$DA_C(a) \\doteq Pr (\\exists c \\in C : Pref_a \\times Pref_c = -1 | Consensus_c \\land (Pref_a \\neq 0)).$\nFor example, an annotator preferring \"I don't know\" over \"911\" in Fig. 2 would be counted as arbitrary discretion.\nDepending on the principles, consensus may be rare. Instead, it may be more informative to characterize annotator preferences when principles (inevitably) conflict, which makes their discretion necessary. We thus propose to infer how annotators prioritize principles. For example, the prohibition of torture is absolute in the European Union [96]. However, freedom of expression, despite also being a fundamental right, is not absolute and can conflict with public safety considerations. Hence, we measure the supremacy over principle pairs according to the annotator."}, {"title": "Definition 8 (Principle Supremacy)", "content": "Given principle-specific preferences $Pref = Pref_c(y_1 \\succ y_0 | x)$ for all c \u2208 \u0421, an annotator a's principle supremacy (PS) of principle c over c' \u2208 C with c \u2260 c' is empirically measured as\n$PS_{c>c'}(a) \\doteq Pr (Pref_a \\times Pref_c = 1 | (Pref_{c'} \\times Pref_{c} = \u22121) \\land (Pref_a \\neq 0)).$\nIn other words, $PS_{c>c'}(a)$ measures how often c \u2018wins out' by agreeing with annotator a while disagreeing with c'.\nWhen principles c and c' conflict, $PS_{c>c'}(a)$ can be interpreted as the probability that annotator a sides with principle c over c', described by a Bernoulli distribution. This interpretation is supported by the antisymmetric relationship $PS_{c>c'}(a) = 1 \u2013 PS_{c'>c}(a)$, which ensures the probabilities of siding with either conflicting principle sum to 1. Furthermore, we say a principle c is absolute if $PS_{c>c'}(a) = 1$ for all other principles c' \u2208 C \\ {c}, meaning the annotator consistently gives it supremacy over other principles when they conflict."}, {"title": "Definition 9 (Principle Priority)", "content": "Let \u010c \u2286 C denote the principles that are not always indifferent or absolute:\n$\u00d1\u00ba {c\\in C | (\\exists c' \\in C : PS_{c>c'}(a) > 0) \\land (\\exists c' \\in C : PS_{c>c'}(a) < 1)} .$"}, {"title": "5.3 How does discretion differ across annotators?", "content": "Both human and algorithmic annotators may be used to exercise discretion, but the nature of this discretion differs fundamentally. Work in pluralistic AI alignment has demonstrated that human annotators exhibit diverse social and political backgrounds [53, 59, 58], enabling meaningful variation in how they exercise discretion. In contrast, algo- rithmic discretion is inherently less diverse, and the 'values' exhibited in its decisions are determined by particular choices of their dataset, design, and optimization [97, 50].\nTaking human discretion as the baseline, we can then measure: do models exercise discretion similarly as human anno- tators? To answer this, we compare our characterizations of discretion from Sec. 5.2 across annotators by introducing discretion discrepancy between annotators' principle priority weights w."}, {"title": "Definition 10 (Discretion Discrepancy)", "content": "The discretion discrepancy (DD) between annotators a and a' measures the difference between the ranking of their principle priorities for principles c \u2208 C:\n$DD_c(a, a') \\equiv d_k ({(w_c^*(a), w_c^*(a')) | c \\in C'}).$\nwith $d_k$ the normalized Kendall tau rank distance [62]. Infinitely high (low) priorities are ranked highest (lowest).\nIntuitively, the Kendall tau distance counts how many pairs of principles (C1, C2) are ordered differently by two an- notators. For example, if annotator a considers \u201cavoid harm\u201d to be more important than \u201cbe helpful\u201d ($w_{harm}^*(a) > w_{help}^*(a)$) but annotator a' has the opposite ordering ($w_{harm}^*(a') < w_{help}^*(a')$), this contributes to their discretion discrep- ancy. The distance is normalized to [0, 1], where 0 indicates identical principle rankings and 1 indicates completely reversed rankings. The DD metric can reveal whether models have learned to prioritize principles similarly to humans when exercising discretion. A high DD suggests the model may be making decisions based on principle orderings that diverge significantly from what guides human preferences. We leave the metric's direct minimization for future work."}, {"title": "6 Experiments", "content": "To explore alignment discretion 'in the wild', we compute the metrics proposed in Sec. 5 over two popular alignment datasets: the harmlessness partition of Anthropic's HH-RLHF dataset [5] (referred as HH), and PKU's Safe-RLHF [54], (referred as PKU). A brief overview of our setup is given below and we expand more on it in Appendix B."}, {"title": "6.1 Setup", "content": "Datasets. HH and PKU are annotated by humans with generic preferences over text completion pairs. PKU also provides annotations that split the generic preference into two: \u2018more helpful' and 'safer'. For all metrics, we use the test split of these datasets. When training is necessary (i.e., the reward models and LLMs), we use the training splits.\nPrinciples. Neither dataset provides a complete list of principles that guided its creation, though PKU does include specific harm categories like \u201cWhite-collar crime.\u201d A rigorous methodology to compile such a list of principles is outside the scope of this work and our definitions are principle-agnostic. Hence, we resort to using the 21 seed principles from Collective Constitutional AI [48], as these cover a range of principles one may want to align their chatbots to, including helpfulness- and harmless-oriented principles. We use GPT-4o as the oracle to evaluate each principle independently (see Def. 5), allowing it to prefer either response or indicate no preference if neither response clearly adhered more to the principle.\nAlgorithmic Annotators. For each dataset, we report results for the most downloaded reward model\u00b3 for this dataset, as well as a Llama-3 8B that was previously supervised fine-tuned (SFT) [28] and a Mistral-7B reward model that we"}, {"title": "6.2 Results", "content": "What is the extent of discretion? Figure 3 lists the share of principle agreement (Def. 6) in the test set of both datasets. Despite the multitude of principles, the vast majority (\u2248 80-85%) of response pairs either have a principle consensus or indifference. The 15-20% of pairs where conflict does occur allow us to measure how annotators prioritize principles. Some examples of response preferences, including principle-specific preferences, are shown in Appendix D.\nWhen is discretion arbitrary? When principles are in consensus, they unambiguously determine the 'best' output. We measure how often annotators nevertheless disagree with consensus as discretion arbitrariness (see Def. 7). Our results in Tab. 1 show that arbitrariness for human annotators is relatively high in both datasets: 28.9% on HH, and 15% to 20% on PKU. The algorithmic annotators diverge significantly: reward models have an arbitrariness close to the human annotators, while Llama-3 and Mistral disagree with the consensus over half the time. Remarkably, GPT-4o's arbitrariness is very low (< 1%), while arbitrariness is higher (but still quite low) for DeepSeek-V3 and Claude 3.5 Sonnet. This can be explained by noting that GPT-4o is also the model we use as the oracle for principle preferences; it is thus heavily biased towards agreeing with the consensus of its own preferences.\nHow do human annotators prioritize principles? For response pairs where principles conflict, we characterize discretion by first computing their principle supremacies according to Def. 8, which we report for the human annotator in HH in Fig. 4 (and for PKU in Fig. 14). The derived principle priorities for both human and algorithmic annotators"}, {"title": "7 Interpreting our Results Based on the Legal Literature on Discretion", "content": "Our experiments in Sec. 6.2 revealed concerning patterns: (i) principles are often in conflict or indifferent, frequently requiring discretionary judgment from annotators; (ii) even when principles reach consensus, human annotators often disagree with this consensus, indicating a level of arbitrary discretion rather than principled decision-making; (iii) we found divergences in how human and algorithmic annotators balance and prioritize different principles, raising questions about whether RLHF can effectively capture and reproduce ethical and legal value systems.\nThese findings raise concerns about how alignment discretion is being exercised, and whether aligning to principles effectively produces stability, predictability, and consistency, which discretion should promote and not decrease. To structure these"}]}