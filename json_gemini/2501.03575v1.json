{"title": "Cosmos World Foundation Model Platform for Physical AI", "authors": ["NVIDIA"], "abstract": "Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via NVIDIA Cosmos.", "sections": [{"title": "1. Introduction", "content": "Physical AI is an Al system equipped with sensors and actuators: the sensors allow it to observe the world, and the actuators allow it to interact with and modify the world. It holds the promise of freeing human workers from physical tasks that are dangerous, laborious, or tedious. While several fields of AI have advanced significantly thanks to data and compute scaling in the recent decade, Physical AI only inches forward. This is largely because scaling training data for Physical AI is much more challenging, as the desired data must contain sequences of interleaved observations and actions. These actions perturb the physical world and may cause severe damage to the system and the world. This is especially true when the Al is still in its infancy when exploratory actions are essential. A World Foundation Model (WFM), a digital twin of the physical world that a Physical AI can safely interact with, has been a long-sought remedy to the data scaling problem.\nIn this paper, we introduce the Cosmos World Foundation Model (WFM) Platform for building Physical AI. We are mainly concerned with the visual world foundation model, where the observations are presented as videos, and the perturbations can exist in various forms. As illustrated in Fig. 2, we present a pre-training- and-then-post-training paradigm, where we divide WFMs into pre-trained and post-trained WFMs. To build a pre-trained WFM, we leverage a large-scale video training dataset to expose the model to a diverse set of visual experiences so it can become a generalist. To build a post-trained WFM, we fine-tune the pre-trained WFM to arrive at a specialized WFM using a dataset collected from a particular Physical Al environment for the targeted, specialized Physical Al setup. Fig. 1 shows example results from our pre-trained and post-trained WFMs.\nData determines the ceiling of an AI model. To build a high-ceiling pre-trained WFM, we develop a video data curation pipeline. We use it to locate portions of videos with rich dynamics and high visual quality that facilitate learning of physics encoded in visual content. We use the pipeline to extract about 100M clips of videos ranging from 2 to 60 seconds from a 20M hour-long video collection. For each clip, we use a visual language model (VLM) to provide a video caption per 256 frames. Video processing is computationally intensive. We leverage hardware implementations of the H.264 video encoder and decoder available in modern GPUs for decoding and transcoding. Our video data curation pipeline leverages many pre-trained image/video understanding models. These models have different throughputs. To maximize the overall throughput for generating trainable video data, we build a Ray-based orchestration pipeline (Moritz et al., 2017). The details are described in Sec. 3.\nWe explore two scalable approaches for building pre-trained WFMs discussed in Sec. 5. These approaches are"}, {"title": "2. World Foundation Model Platform", "content": "Let xo:t be a sequence of visual observations of the real world from time 0 to t. Let ct be the perturbation to the world. As illustrated in Fig. 3, a WFM is a model W that predicts the future observation at time t + 1, 2t+1, based on the past observation xo:t and the current perturbation ct. In our case, xo:t is an RGB video, while ct is a perturbation that can take many forms. It can be an action taken by the Physical AI, a random perturbation, a text description of the perturbation, etc."}, {"title": "2.1. Future Cosmos", "content": "We believe a WFM is useful to Physical AI builders in many ways, including (but not limited to)\n\u2022 Policy evaluation. This refers to evaluating the quality of a policy model in a Physical AI system. Instead of evaluating a trained policy by deploying it to a Physical AI system operating in the real world, one could instead let the digital copy of the Physical AI system interact with the world foundation model. The WFM-based evaluation is more cost-effective and time-efficient. With the WFM, builders can deploy the policy model in unseen environments that are otherwise unavailable. WFMs can help developers rule out incapable policies quickly and focus the physical resources on a few promising ones.\n\u2022 Policy initialization. A policy model generates actions to be taken by the Physical AI system based on the current observations and the given task. A well-trained WFM, which models the dynamic patterns of the world based on the input perturbations, can serve as a good initialization of the policy model. This helps address the data scarcity problem in Physical AI.\n\u2022 Policy training. A WFM paired with a reward model can be a proxy for the physical world to provide feedback to the policy model in a reinforcement learning setup. The agent can gain proficiency in solving tasks by interacting with the WFM.\n\u2022 Planning or model-predictive control. A WFM can be used to simulate different future states following different action sequences taken by a Physical AI system. A cost/reward module can then be used to quantify the performance of these different action sequences based on the outcomes. The Physical AI can then execute the best action sequence based on the simulation results as a whole, as in planning"}, {"title": "2.2. Current Cosmos", "content": "Fig. 4 visualizes what is available in the Cosmos WFM platform that is included in this paper, which includes video curator, video tokenization, world foundation model pre-training, world foundation model post-training, and guardrail.\nVideo curator. We develop a scalable video data curation pipeline. Each video is split into individual shots without scene changes. A sequence of filtering steps is then applied to the clips to locate high-quality and dynamic information-rich subsets for training. These high-quality shots are then annotated using a VLM. We then perform semantic de-duplication to construct a diverse but compact dataset.\nVideo tokenization. We develop a family of video tokenizers of different compression ratios. These tokenizers are causal. The token computation for the current frames is not based on future observation. This causal design has several benefits. On the training side, it makes joint image and video training possible since a causal video tokenizer is also an image tokenizer when the input is a single image. This is important for the video model to leverage image datasets for training, which contain rich appearance information of the worlds and tend to be more diverse. On the application side, causal video tokenizers are better aligned with Physical AI systems that live in the causal world.\nWFM pre-training. We explore two scalable approaches for building pre-trained world foundation models\u2014the diffusion model and the autoregressive model. We use the transformer architecture for its scalability.\nFor the diffusion-based WFM, the pre-training consists of two steps: 1) Text2World generation pre-training and 2) Video2World generation pre-training. Specifically, we train the model to generate a video world based on the input text prompt. We then fine-tune it to generate a future video world based on the past video and an input text prompt, which we refer to as the Video2World generation task.\nFor the autoregressive-based WFM, the pre-training consists of two steps: 1) vanilla next token generation and 2) text-conditioned Video2World generation. We first train the model to generate a future video world based on the input of past video-foresight generation. We then fine-tune it to generate a future video world based on the past video and a text prompt.\nThe video2world generation model is a pre-trained world model that generates the future based on the current"}, {"title": "3. Data Curation", "content": "We describe our video curation pipeline, which produces high-quality training datasets for both tokenizers and WFMs. As shown in Fig. 5, our pipeline consists of 5 main steps: 1) splitting, 2) filtering, 3) annotation, 4) deduplication, and 5) sharding. Every step is tailored to improve the data quality and accommodate the requirements of model training. We first present our raw dataset and then describe each step in detail."}, {"title": "3.1. Dataset", "content": "We use both proprietary video datasets and publicly available open-domain Internet videos to train our models. Our goal is to enable Physical AI developers. To this end, we curate the video training dataset to cover various Physical AI applications and target the following video categories:\n1. Driving (11%),\n2. Hand motion and object manipulation (16%),\n3. Human motion and activity (10%),\n4. Spatial awareness and navigation (16%),\n5. First person point-of-view (8%),\n6. Nature dynamics (20%),\n7. Dynamic camera movements (8%),\n8. Synthetically rendered (4%), and\n9. Others (7%)."}, {"title": "3.2. Splitting", "content": "Our videos have arbitrary lengths, and modern deep-learning models cannot directly consume very long videos. Also, many videos contain shot transitions. They can start from one scene and then transition to a different scene where the two scenes can be disconnected entirely, e.g., from two people talking in a modern kitchen in New York City to a scene of lions chasing zebra in an African savanna. It is important to segment each video based on its shot changes and generate visually consistent video clips so that the model can learn visual content transitions that are physically plausible instead of artificially edited."}, {"title": "3.2.1. Shot Detection", "content": "Splitting aims to temporally segment raw videos of arbitrary lengths into clips without shot changes. It takes the raw videos as input and generates each shot's start and end frame indices. Clips shorter than 2s are discarded, as they could be shot transitions or visual effects. Clips longer than 60s are further split to have a maximal length of 60s. The subsequent filtering steps can then determine whether a clip contains useful information for learning the physics of the world.\nShot boundary detection is a classical computer vision problem. Existing methods detect shot boundaries based on changes in the visual feature space, but they differ in how to learn visual features from video frames. We evaluate several algorithms for the task in Tab. 1: PySceneDetect (Castellano, 2024), Panda70M (Chen et al., 2024), TransNetV2 (Soucek and Lokoc, 2024), and AutoShot (Zhu et al., 2023).\nPySceneDetect is a popular library that detects shot changes by thresholding the temporal change of color histogram in HSV space. Note that it is also adopted by the recent MovieGen work (Polyak et al., 2024). Panda70M augments PySceneDetect with CLIP-embedding-based stitching and filtering. TransNetV2 and AutoShot, on the other hand, are neural network-based, predicting a probability of each frame being a transition frame given a 100-frame rolling input window.\nIt is critical to select an algorithm that can handle heavily edited videos well, as they often have complex shot changes compounded with various visual effects. This motivates us to build a dedicated benchmark to evaluate whether the method can generate clips with clean shot cuts from videos. Our benchmark (named ShotBench\u00b2) includes existing datasets, such as RAI, BBC Planet Earth (AI Image Lab, University of Modena, 2016), ClipShots (Tang et al., 2018) and SHOT (Zhu et al., 2023). For ClipShots, we define the transition frame as the midpoint of the start and end of each shot annotation to be consistent with other datasets."}, {"title": "3.2.2. Transcoding", "content": "Our videos use many different codecs with various settings, which poses challenges to data curation. We re-encode each video clip from shot detection into a consistent, high-quality mp4 format. This simplifies the subsequent data curation process. With a unified video codec, the stability and efficiency of our dataloader for model training are also greatly improved. We use the h264_nvenc codec with a high bitrate and stress test our setting using videos with fast motion and high-frequency texture to ensure no perceptible visual degradation.\nWe thoroughly evaluate different hardware and software configurations for transcoding to maximize the throughput in Tab. 2. Modern GPUs provide hardware-accelerated video encoding and decoding capabilities. NVIDIA L40S has hardware accelerators for both decoding (NVDEC) and encoding (NVENC), whereas NVIDIA H100 only has NVDEC. We compensate H100 with the maximum available CPU cores (28 instead of 1) for a fair comparison with L40S in Tab. 2. L40S has about 17% higher throughput than H100 (0.0674 vs. 0.0574). For software configurations, switching from libx264 to h264_nvenc and transcoding multiple clips from the same video in batches significantly boost the throughput. We observe issues with ffmpeg fully utilizing NVDEC/NVENC accelerators, especially on multi-GPU nodes. Replacing ffmpeg with PyNvideoCodec for video stream transcoding leads to much higher accelerator utilization and the biggest throughput improvement (0.3702 vs. 0.1026). We only keep ffmpeg for audio remixing and use PyNvideoCodec to better leverage the computing power in the GPUs. We achieve a ~ 6.5\u00d7 increase in throughput when combining all the improvements together."}, {"title": "3.3. Filtering", "content": "The video clips produced from the splitting step are noisy, with vastly different qualities covering various topics. We design the filtering step to 1) remove video clips whose visual quality fails to meet our minimal requirements, 2) select high-quality video clips suitable for fine-tuning, and 3) tailor the data distribution for building WFMs. We achieve the above goal by doing motion filtering, visual quality filtering, text filtering, and"}, {"title": "3.3.1. Motion Filtering", "content": "We have two main goals in motion filtering: 1) remove videos that are static or with random abrupt camera motion (usually from hand-held cameras) and 2) tag videos with different types of camera motion (e.g., pan, zoom, tilt, etc.), which can provide additional information to guide model training.\nWe build a lightweight classifier for motion filtering. The input to the classifier is a sequence of motion vectors or optical flow extracted from a video clip. The classifier is based on the ViT architecture and is trained with labeled videos. We experiment with motion vectors from h264 codec, the Farneback optical flow algorithm (Farneb\u00e4ck, 2003), and an NVIDIA TensorRT-accelerated optical flow estimation network. We find that the classifier built on top of the NVIDIA TensorRT-accelerated optical flow estimation works the best, producing high classification accuracy for motion filtering."}, {"title": "3.3.2. Visual Quality Filtering", "content": "We consider two criteria, distortion and appearance quality, for visual quality-based filtering. First, we remove video clips with distortions, such as artifacts, noise, blur, low sharpness, overexposure, underexposure, etc. We use a video quality assessment model trained on human-rated videos based on DOVER (Wu et al., 2023). This gives a perceptual quality score per clip, and we use the scores to remove clips that are in the bottom 15%. Second, we filter out video clips with low appearance quality. We apply an image aesthetic model (Schuhmann, 2022) on sampled frames from an input clip. We set a conservative threshold, i.e., 3.5, since aesthetics are less important for Physical AI."}, {"title": "3.3.3. Text Overlay Filtering", "content": "Some of our videos are post-processed to add text to include additional information for the viewer. We also find that text tends to co-occur with different visual effects. Our goal is to learn the physics of the world. It is crucial to remove videos with such excessive text. Note that we focus on text added in post-processing instead of text in the original scene from which the video is created, such as the street names in driving videos.\nWe train an MLP-based binary classifier to detect such videos. The input to the classifier is a video embedding extracted using InternVideo2 (Wang et al., 2025). We use a proprietary VLM to build the training set to label positive and negative videos. Our trained model achieves high prediction accuracy in the validation set."}, {"title": "3.3.4. Video Type Filtering", "content": "To adjust the training data distribution and filter out unwanted video types, we design a comprehensive taxonomy that categorizes videos based on their content type and visual style. We train a classifier to label each video clip with categories from the taxonomy. We refine our data by excluding specific video types that could lead to poor generation quality or unrealistic dynamics, such as abstract visual patterns, video game footage, animated content, etc. We further adjust the data distribution by upsampling from categories that are more relevant to WFMs (e.g., human action, human and object interaction, etc.) and downsampling on categories that are less important (e.g., nature or landscape videos).\nGiven the absence of pre-existing labeled datasets matching our taxonomy, we leverage a proprietary VLM to create training and evaluation data for the classifier. For each video clip, we prompt the VLM with eight"}, {"title": "3.4. Annotation", "content": "Text descriptions are usually paired with image and video data to provide supervision and conditions for world model training. We use a VLM to generate high-quality and consistent captions for each video clip. We configure the VLM in a way such that it focuses on the material facts and details in the videos. Using this approach to provide descriptions of videos instead of relying on Alt text also eases the burden of learning for world models as we do not need to adapt to different text styles or formats during training.\nWe test several SOTA methods (i.e., VFC (Ge et al., 2024), Qwen2-VL (Wang et al., 2024), VILA (Lin et al., 2024; Xue et al., 2024)) for caption generation on our videos, and find VILA generates more accurate descriptions based on a small-scale human evaluation. We use an internal VILA model with 13B parameters, fine-tuned for video captioning. It has an enlarged context window suitable for processing long, multi-frame contexts, with a max input and output token length of 5904 and 256, respectively. To improve the inference efficiency, we use an FP8-quantized TensorRT-LLM engine, resulting in a 10 \u00d7 speed-up in throughput compared to a PyTorch half-precision baseline, as shown in Tab. 3. We prompt VILA with \u201cElaborate on the visual and narrative elements of the video in detail\u201d and feed it 8 uniformly sampled frames from the input clip. The average length of captions is 559 characters or 97 words."}, {"title": "3.5. Deduplication", "content": "Given the sheer volume of our videos, there could be duplicated or near-duplicated samples in the training set. It is critical to deduplicate the data to create a more balanced and diverse data distribution. It also improves the efficiency of training and reduces the chance of memorizing specific training samples.\nWe adopt the approach from SemDeDup (Abbas et al., 2023) and DataComp (Gadre et al., 2024) for scalable semantic deduplication. We reuse the InternVideo2 embeddings computed during filtering and cluster the embeddings using a multi-node GPU-accelerated implementation of k-means (RAPIDS, 2023) with k = 10,000. We compute the pairwise distances within each cluster of embeddings to identify duplicates. When duplicated videos are detected, we choose the video with the highest resolution to ensure no quality is lost due to deduplication. To avoid storing the entire pairwise distance matrix in GPU memory, we calculate on-the-fly the necessary upper-triangular matrix and argmax reduction in blocks of 256. We remove about 30% of training data during deduplication.\nWe also leverage the extracted InternVideo2 embeddings and clustering results to build a visual search engine that supports querying the whole training dataset with free-form text and videos. The search engine is useful for debugging issues in our data and understanding the gap between the pre-training dataset and downstream applications."}, {"title": "3.6. Sharding", "content": "This step aims to package the processed video clips into webdatasets that our model trainer can directly consume for training. We shard the videos based on their resolution, aspect ratio, and length to align with our"}, {"title": "3.7. Infrastructure", "content": "Our data processing infrastructure uses AnyScale Ray (Moritz et al., 2017) to implement a streaming pipeline system for geographically distributed clusters, addressing two key challenges in large-scale ML workflows: efficient resource utilization across homogeneous nodes and robust operation over high-latency connections to data sources. By decoupling data transfer from computation, pipelines operate efficiently with remote data storage while maintaining memory requirements that scale with pipeline complexity rather than dataset size, enabling unbounded stream processing.\nOur architecture enables concurrent utilization of complementary hardware resources through parallel pipeline stages, for instance, simultaneously using network bandwidth for data ingestion, NVDEC units for video decoding, and GPUs for compute-intensive transformations. We extend the Fragmentation Gradient Descent algorithm (Weng et al., 2023) to optimize this multi-resource allocation, with our scheduler automatically scaling individual stages to maintain balanced throughput across specialized hardware accelerators."}, {"title": "4. Tokenizer", "content": "Tokenizers are fundamental building blocks of modern large-scale models. They transform raw data into more efficient representations by learning a bottle-necked latent space discovered in an unsupervised manner. Specifically, visual tokenizers map raw and redundant visual data such as images and videos\u2014into compact semantic tokens, making them crucial for handling high-dimensional visual data. This ability not only enables efficient training of large-scale transformer models but also democratizes their inference on limited computational resources. Fig. 6 schematically illustrates the tokenization training pipeline where the goal is to train the encoder and decoder so that the bottleneck token representation maximally preserves visual information in the input.\nTokenizers come in two types: continuous and discrete (see Fig. 7 for illustrations). Continuous tokenizers encode visual data into continuous latent embeddings, as in latent diffusion models like Stable Diffusion (Rombach et al., 2022) or VideoLDM (Blattmann et al., 2023). These embeddings are suitable for models that generate data by sampling from continuous distributions. Discrete tokenizers encode visual data into discrete latent codes, mapping them into quantized indices, as seen in autoregressive transformers such as VideoPoet (Kondratyuk et al., 2024). This discrete representation is necessary for models such as GPT that are trained with the cross-entropy loss. Fig. 7 illustrates the two types of tokens.\nThe success of tokenizers largely relies on their ability to deliver high compression rates without compromising their subsequent visual reconstruction quality. On one hand, high compression reduces storage and computa-"}, {"title": "4.1. Architecture", "content": "Cosmos Tokenizer is designed as an encoder-decoder architecture. Given an input video $x_{0:T} \\in \\mathbb{R}^{(1+T)\\times H \\times W \\times 3}$, with H, W, T being the height, width, and number of frames, the encoder (E) tokenizes the inputs into a token video $z_{0:T'} \\in \\mathbb{R}^{(1+T') \\times H' \\times W' \\times C'}$, with a spatial compression factor of $s_{HW} = \\frac{HW}{H'W'}$  and a temporal compression factor of $s_T = \\frac{T}{T'}$. The decoder (D) then reconstructs the input video from these tokens, resulting in the reconstructed video $\\hat{x}_{0:T} \\in \\mathbb{R}^{(1+T)\\times H \\times W \\times 3}$, mathematically given by:\n$\\hat{x}_{0:T} = D(E(x_{0:T})).$  (1)\nOur architecture employs a temporally causal design, ensuring that each stage processes only current and past frames, independent of future frames. Unlike common approaches, our tokenizer operates in the wavelet space, where inputs are first processed by a 2-level wavelet transform. Specifically, the wavelet transform maps the input video $x_{0:T}$ in a group-wise manner to downsample the inputs by a factor of four along x, y, and t. The groups are formed as: ${x_0, x_{1:4}, x_{5:8}, ..., x_{(T-3):T}} \\rightarrow {g_0, g_1, g_2, ..., g_{T/4}}$. Subsequent encoder stages process the frames in a temporally causal manner as ${g_0, g_{0:1}, g_{0:2}, ...} \\rightarrow {\\xi_0, \\xi_1, \\xi_2, ...}$. Successive encoder stages follow a similar scheme, finally outputting the tokens $z_{0:T'}$. The causal design helps adapt models built on top of the tokenizer to downstream Physical AI applications that often operate on the temporal causal setting. The wavelet transform allows us to operate on a more compact video representation that eliminates redundancies in pixel information, allowing the remaining layers to focus on more semantic compression."}, {"title": "4.2. Training Strategy", "content": "We employ a joint training strategy by alternating mini-batches of images and videos at a preset frequency. We only supervise the final output of our tokenizer's decoder. We do not use auxiliary losses tapped into the latent spaces, such as commitment or KL prior losses. For example, if a VAE (Kingma, 2013) formulation were used for continuous tokenizers instead of the vanilla AE, one would need to have the KL prior loss. If a VQ-VAE (van den Oord et al., 2017) were used for discrete quantization instead of the FSQ, one would need to have the commitment loss.\nWe employ a two-stage training scheme. In the first stage, we optimize with the L1 loss that minimizes the pixel-wise RGB difference between the input and reconstructed video (10:T), given by\n$L_1=||x_{0:T} - \\hat{x}_{0:T}||_1,$ (2)"}, {"title": "5. World Foundation Model Pre-training", "content": "Pre-trained WFMs are generalists that capture general knowledge of real-world physics and natural behaviors. We exploit two different scalable deep learning paradigms, diffusion models and autoregressive models, to build two families of WFMs. Both diffusion models and autoregressive models break a difficult generation problem into a sequence of easier sub-problems and have been turbo-charging the development of generative models. In the case of diffusion models, the difficult generation problem is divided into a sequence of denoising problems. In the case of autoregressive models, the difficult generation problem is divided into a sequence of next-token prediction problems. We discuss how we scale these deep learning paradigms using various parallelization techniques tailored for modern GPUs in our endeavor of building pre-trained WFMs. We train all of the WFM models reported in the paper using a cluster of 10,000 NVIDIA H100 GPUs in a time span of three months."}, {"title": "5.1. Diffusion-based World Foundation Model", "content": "Our diffusion-based WFMs are latent diffusion models that operate within a learned latent space of a tokenizer, enabling a compact, reduced-dimensional representation of videos. This design choice offers several advantages: it reduces computational costs during both training and inference while simplifying the denoising task (Hoogeboom et al., 2024; Rombach et al., 2022). To tokenize videos into latent representations, we employ Cosmos-1.0-Tokenizer-CV8x8x8."}, {"title": "5.1.1. Formulation", "content": "To train our diffusion WFMs, we adopt the approach outlined in EDM (Karras et al., 2022, 2024). The denoising score matching loss for the denoiser De, evaluated at a noise level o, is defined as\n$L(D_{\\theta}, \\sigma) = \\mathbb{E}_{x_0, n} [||D_{\\theta}(x_0 + n; \\sigma) - x_0||^2],$ (5)\nwhere x0 ~ Pdata is a clean image or video sampled from the training set, n ~ N(0, \u03c3\u00b2I) is i.i.d. Gaussian noise, and De is a noise-conditioned neural network tasked with denoising the corrupted sample x0 + n. We adhere to the preconditioning design introduced in EDM for parameterizing De. The overall training loss is defined as a weighted expectation of L(Do; \u03c3) over the noise levels:\n$L(D_{\\theta}) = \\mathbb{E}_{\\sigma} [\\lambda(\\sigma) \\frac{\\rho_u(\\sigma)}{\\int \\rho_u(\\sigma)} L(D_{\\theta}, \\sigma) + u(\\sigma)],$ (6)\n$\\lambda(\\sigma) = (\\sigma^2 + \\sigma_{data}^2) / (\\sigma.\\sigma_{data})^2,$ (7)\n$ln(\\sigma) \\sim \\mathcal{N}(p_{mean}, p_{std}).$ (8)\nwhere the distribution of noise levels o is controlled by hyperparameters Pmean and Pstd. Odata is the standard deviation of the training data, and the weighting function \u03bb(\u03c3) ensures equal contribution of each noise level at the beginning of the training. However, as training progresses, this balance may deteriorate. To mitigate this issue, we treat the optimization over various noise levels as a form of multi-task learning. We utilize the uncertainty-based weighting approach by introducing u(\u03c3) as a continuous uncertainty function quantifying the uncertainty for the denoising objective L(De, \u03c3) at noise level \u03c3. We use a simple MLP to parameterize u(\u03c3) and minimize the overall loss L(De) during training. Intuitively, the contribution of loss at noise level o is weighted down if the model is uncertain about the task, i.e., if u(\u03c3) is high. At the same time, the model is penalized for this uncertainty, encouraging u(\u03c3) to be as low as possible.\nCompared to recent video generative models that adopt the Gaussian flow matching formulation (Kong et al., 2024; Polyak et al., 2024), our work is derived from the diffusion score matching perspective (Ho et al., 2020; Song et al., 2020). However, as shown by Gao et al. (2024), these frameworks are theoretically equivalent, sharing fundamental similarities in their objectives and training procedures. Our EDM-based formulation aligns with these insights, mainly differing in the choice of preconditioning designs and hyperparameters. In practice, we have not encountered any performance limitations with the EDM formulation."}, {"title": "5.1.2. Architecture", "content": "In this section, we describe the design of our denoiser network De that builds upon DiT (Peebles and Xie, 2023), which was originally designed for label-conditioned image generation. We adapt its architecture to better suit our goal of controllable video generation. We visualize the overall network design in Fig. 11."}, {"title": "5.1.3. Training Strategy", "content": "This section outlines the methodologies employed to train our models on datasets spanning multiple modalities, resolutions, aspect ratios, and conditioning inputs.\nJoint image and video training. To leverage the vast abundance of high-quality, diverse image datasets in model training, we implement an alternating optimization strategy that interleaves batches of image and video data. To facilitate cross-modal knowledge transfer between image and video domains, we adopt a domain-specific normalization scheme that aligns the latent distributions using sufficient statistics estimated independently for image and video data. This approach is motivated by the observation that reducing the distributional shift between image and video latent representations improves generation quality. Furthermore, we observe non-stationary statistics across temporal and channel dimensions in video latent representations. To address this heterogeneity, we employ a normalization strategy that applies frame-wise and channel-wise"}, {"title": "5.1.4. Scaling Up", "content": "Here, we outline the techniques that enable efficient scaling of our diffusion WFMs. We analyze the memory requirements of our models, discuss parallelism strategies, and compare our training setup against other video diffusion models and state-of-the-art LLMs.\nMemory requirements. The four major components that consume the GPU memory are:\n\u2022 Model parameters: 10 bytes per parameter. Our mixed precision training stores model parameters in both FP32 and BF16, alongside Exponential Moving Average (EMA) weights in FP32.\n\u2022 Gradients: 2 bytes per parameter. We store the gradients in BF16.\n\u2022 Optimizer states: 8 bytes per parameter. We use AdamW (Loshchilov and Hutter, 2019) as our optimizer and store the optimizer states (i.e., first and second moments) in FP32.\n\u2022 Activations: (2 \u00d7 number_of_layers \u00d7 15 \u00d7 seq_len \u00d7 batch_size \u00d7 d_model) bytes. We store the activations in BF16. Tab. 13 provides details of the stored activations for major operations within the network. To optimize memory usage, we implement selective activation checkpointing (Chen et al., 2016; Korthikanti et al., 2023), recomputing activations for memory-limited layers such as normalization functions.\nFor instance, our 14B model (Cosmos-1.0-Diffusion-14B-Text2World) requires approximately 280 GB for model parameters, gradients, and optimizer states, alongside 310 GB for activations during high-resolution pre-training. Given the 80GB HBM3 limit of NVIDIA H100 GPUs, we employ Fully Sharded Data Parallelism (FSDP) and Context Parallelism (CP) to distribute memory demands across multiple GPUs.\nFully Sharded Data Parallelism (FSDP). FSDP improves memory efficiency by sharding model parameters, gradients, and optimizer states across devices. It gathers parameters only when needed during computation and releases them afterward. Unlike standard data parallelism, which duplicates parameters across devices, FSDP distributes parameters, gradients, and optimizer states, with each device managing only its shard. This approach minimizes memory usage to the largest temporarily unsharded parameter set alongside its shard of parameters, gradients, and optimizer states. For our implementation, we utilize a sharding factor of 32 for the 7B model and 64 for the 14B model to balance memory and communication latency."}, {"title": "5.1.5. Prompt Upsampler", "content": "During training", "are": "n\u2022 Fidelity to the input prompts: The upsampled prompt must"}]}