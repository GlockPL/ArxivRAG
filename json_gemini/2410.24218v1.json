{"title": "Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use", "authors": ["Jiajun Xi", "Yinong He", "Jianing Yang", "Yinpei Dai", "Joyce Chai"], "abstract": "In real-world scenarios, it is desirable for embodied agents to have the ability to leverage human language to gain explicit or implicit knowledge for learning tasks. Despite recent progress, most previous approaches adopt simple low-level instructions as language inputs, which may not reflect natural human communication. It's not clear how to incorporate rich language use to facilitate task learning. To address this question, this paper studies different types of language inputs in facilitating reinforcement learning (RL) embodied agents. More specifically, we examine how different levels of language informativeness (i.e., feedback on past behaviors and future guidance) and diversity (i.e., variation of language expressions) impact agent learning and inference. Our empirical results based on four RL benchmarks demonstrate that agents trained with diverse and informative language feedback can achieve enhanced generalization and fast adaptation to new tasks. These findings highlight the pivotal role of language use in teaching embodied agents new tasks in an open world.", "sections": [{"title": "1 Introduction", "content": "Developing embodied agents that can understand and communicate with humans in natural language to learn and accomplish tasks is a long-standing goal in artificial intelligence. In recent years, the integration of human language and reinforcement learning (RL) has seen significant advancements. Unlike traditional RL methods that typically rely on numerical reward signals to guide agent learning, recent works (Cheng et al., 2023; Lin et al., 2023) explore using language as an intuitive and useful signal to shape an agent's behaviors. For example, when the agent is making mistakes during the task completion, providing language feedback can largely improve the instantaneous performance thus enhancing the overall agent learning efficiency and effectiveness (McCallum et al., 2023).\nHowever, existing methods generally employ simple instructions, such as \"turn left\" and \"put the apple to the table\" to teach/control an agent (Hanjie et al., 2021; Zhang and Chai, 2021; Lin et al., 2023; McCallum et al., 2023; Shridhar et al., 2021). While useful, these instructions may not fully reflect the flexibility of language use in task learning and collaboration (Chai et al., 2018, 2019; Zhang et al., 2022, 2023; Dai et al., 2024a). In the real world, humans often express complex language instructions that are more informative. For instance, when a student makes a mistake, a teacher may help them to retrospect on what went wrong (i.e., hindsight instructions) and then guide them on what should be done next to finish the goal (i.e., foresight instructions). In addition, humans are likely to engage in conversations with more diverse language patterns, describing the same goal with different expressions and styles. Therefore, we ask the following question:\nHow do the informativeness and diversity of natural language used during RL training affect an agent's ability to learn tasks?\nWe take a popular offline RL model - decision transformer (DT) (Chen et al., 2021) - as a backbone architecture and conduct a comprehensive study to examine how informativeness and diversity of language use may impact agents' learning ability. To control informativeness, we leverage expert agents' actions as a reference to generate hindsight reflection and foresight guidance, using hand-crafted language templates. To increase diversity, we construct a GPT-augmented language pool, where GPT-4 (OpenAI, 2024) is used to augment hand-crafted templates into much more natural and richer expressions. We further extended DT into a multi-modal Language-Teachable DT"}, {"title": "2 Related Work", "content": "Offline Reinforcement Learning Offline reinforcement learning (RL) has become a focal point of research due to its ability to utilize pre-existing datasets for training agents without real-time interactions. Several algorithms address the unique challenges of offline RL, such as mitigating extrapolation errors and ensuring robust policy evaluation. A survey by Prudencio et al. (2023) outlines the field's taxonomy and open problems. Benchmarking efforts by Fujimoto et al. (2019) assess various batch deep RL algorithms. Key approaches include Conservative Q-Learning (CQL) (Kumar et al., 2020), Implicit Q-Learning (IQL) (Kostrikov et al., 2021), and the Decision Transformer (DT) (Chen et al., 2021), which treats RL as a sequence modeling problem (Janner et al., 2021). Recent work also explores generalization across tasks (Lee et al., 2022; Reed et al., 2022; Schubert et al., 2023), the use of exploratory data (Yarats et al., 2022), and integrating large language models (LLMs) (Mirchandani et al., 2023). Efficient online RL leveraging offline data is also a focus (Ball et al., 2023; Modhe et al., 2023). Our research builds on the Decision Transformer (DT) by integrating language feedback, creating the Language-Teachable Decision Transformer (LTDT). This novel approach incorporates rich, human-like language instructions, improving agent learning through enhanced informativeness and diversity of language inputs.\nLanguage in Reinforcement Learning The intersection of natural language and RL offers new ways to develop intuitive and effective learning paradigms for embodied agents. Initial works utilized language for feedback and task instructions (She and Chai, 2017; Nguyen et al., 2017; Shridhar et al., 2020). Recent studies have explored various methods for incorporating language feedback in RL, such as the LTC paradigm (Wang et al., 2023), lifelong robot learning with human-assisted language planners (Parakh et al., 2023), and frameworks for rich information requests (Dai et al., 2020; Tseng et al., 2021; Nguyen et al., 2022). Language for corrections (Sharma et al., 2022; Liu et al., 2023) and as reward signals (Xie et al., 2023; Goyal et al., 2019; Yu et al., 2023) has shown to enhance agent performance. Vision-language joint training approaches, like CLIP (Radford et al., 2021), BLIP-2 (Li et al., 2023), and InstructBLIP (Dai et al., 2023), demonstrate the potential of combining visual and language modalities for RL tasks (Ma et al., 2023; Nguyen et al., 2019; Khandelwal et al., 2022). Further, multimodal prompts for robotic manipulation (Jiang et al., 2023; Fan et al., 2022) and LLMs for planning in robotics (Ahn et al., 2022; Huang et al., 2022; Singh et al., 2023; Yao et al., 2022; Dai et al., 2024b) highlight the evolving role of language in RL. Other works, like (Mehta et al., 2023), focus on generating problem-specific language feedback templates. In contrast, our work focuses on the informativeness and diversity of language instructions, two problem-agnostic yet easy-to-implement properties. By using both hindsight and foresight language templates and enhancing diversity through GPT-4, we demonstrate notable improvements in agent performance and generalizability, showcasing the impact of complex language inputs in offline RL training."}, {"title": "3 Problem Setting", "content": "In this section, we outline the problem setting by defining the offline reinforcement learning problem"}, {"title": "3.1 Offline Reinforcement Learning", "content": "To support a systematic study of language use, we formulate the problem in the offline reinforcement learning (RL) setting. At each time step t, the agent receives an observation $o_t$, a reward $r_t$, and a language feedback $l_t$ for its previous action. The agent then executes an action $a_t$ according to a policy $\\pi$, which is conditioned on the entire interaction history $h_t$ up to time t, i.e., $\\pi(a_t | h_t)$, where $h_t = {o_{<t}, r_{<t}, l_{<t}, a_{<t}}$ represents the history of observations, rewards, language feedback, and past actions up to time t. The agent's goal is to complete the task by maximizing the expected discounted sum of rewards $E[\\sum_{t=1}^T \\gamma^t r_t]$ where T is the episode length, and $\\gamma$ is the discount factor. In offline RL, the training trajectories are pre-collected with an expert agent (a well-trained agent or a planner-based expert with privileged information). The trained agents are evaluated interactively with the environment."}, {"title": "3.2 Language Feedback: Informativeness and Diversity", "content": "We aim to investigate how the informativeness and diversity of language instructions used during the training of an offline RL agent affect the agent's"}, {"title": "3.2.1 Informativeness", "content": "Informativeness refers to the richness of information content in language feedback. Following Cheng et al. (2023), we categorize feedback into two types: hindsight and foresight. Hindsight feedback involves comments or critiques about the agent's past actions. For example, \"Excellent, you are moving towards the goal!\" encourages the agent to continue its current path, while \"You are getting too close to the enemy.\" alerts the agent about a mistake. Hindsight feedback reflects on incorrect actions taken in previous steps, which can guide agents toward success by narrowing down the search space for correct actions (See Appendix E for more analysis). Conversely, foresight feedback guides potential future actions. For instance, \"You should go right to get closer to the target.\" directs the agent towards the goal, and \"You should go left to avoid the enemy on the right.\" helps the agent make strategic decisions to avoid threats. Language feedback is considered most informative when it includes both hindsight and foresight elements, and least informative when neither is present."}, {"title": "3.2.2 Diversity", "content": "Diversity in language feedback refers to the variety of ways the same information is conveyed. If feedback is provided using only one template, it"}, {"title": "3.3 Environments", "content": "As shown in Figure 1, we conduct experiments across four environments-HomeGrid, ALFWorld, Messenger, and MetaWorld\u2014each featuring discrete action spaces, with hand-crafted hindsight and foresight language instructions. More information and examples of languages for each environment can be found in Appendix A.\nHomeGrid (Lin et al., 2023) is a multitask grid world designed to evaluate how well agents can understand and use various types of language to complete tasks. It includes five task types (FIND, GET, CLEAN UP, REARRANGE, OPEN), involving interaction with objects and trash bins with a total of 38 tasks. The agent receives a reward of 1 when the task is completed and receives a reward of 0.5 if a subgoal is completed.\nALFWorld (Shridhar et al., 2021) is a text-game environment that aligns with the embodied ALFRED benchmark (Shridhar et al., 2020) and provides simulation for household tasks. It includes six types of tasks which require the agent to navigate and interact with household objects by following language instructions. The agent gets a reward of 1 when the task is completed. We adopt the hindsight and foresight language templates from LLF-ALFWorld introduced in (Cheng et al., 2023), which adds an extra language wrapper to the original ALFWorld environment.\nMessenger (Hanjie et al., 2021) is a grid world with several entities. The agent's task is to retrieve a message from one entity and deliver it to another goal entity, while avoiding enemies. At the start of each episode, the agent is provided with a manual describing the randomized roles of the entities and their movement dynamics. The agent receives a reward of 1 when the task is completed.\nMetaWorld (Yu et al., 2019) is a benchmark that consists of a variety of manipulation tasks performed by a simulated Sawyer robot arm. It includes 50 types of common robot manipulation tasks. We select two of them in our experiments: ASSEMBLY and HAMMER. The agent receives a reward of 1 when completing a task."}, {"title": "4 Data Generation", "content": "To train an agent that can understand language feedback in an offline reinforcement learning manner, we construct an offline dataset D consisting of two parts:\n\u2022 Agent trajectory consisting of task description Td and the tuples $(R_t, s_t, a_t)$, where $R_t$ represents the reward, $s_t$ is the state, and $a_t$ is the action.\n\u2022 language feedback $l_t$ conveying hindsight and foresight information at each time step.\nAlgorithm 1 outlines the data generation process, and we explain the algorithm in detail in the following sections."}, {"title": "4.1 Trajectory Generation", "content": "To improve model generalization and avoid overfitting, it is essential to train on diverse, sub-optimal trajectories rather than relying solely on optimal ones generated by an expert agent (Kumar et al., 2020; Chen et al., 2021). We achieve this by introducing perturbations to an expert planner (see Appendix B), allowing the non-expert agent to produce sub-optimal trajectories. This promotes broader exploration of the state-action space, enhancing the model's ability to generalize to unseen scenarios (Kumar et al., 2020; Chen et al., 2021).\nDuring data collection, we begin by appending the task description Td to the trajectory sequence and initializing the environment with a fixed seed."}, {"title": "4.2 Language Feedback Generation", "content": "For the second part of the dataset D, we collect the language feedback along the non-expert agent's trajectory. As shown in Figure 2, we follow a structured process to generate diverse and informative language feedback. For the state at time step t, the expert agent $\\pi^*$ proposes an expert action $a_t$ (e.g. \"down\") at this state, which is further transformed into a foresight template $I_{fore}$ (e.g. \"Turn back.\") by the environment simulator, guiding the agent on what should be done at this state. After the non-expert agent $\\pi$ steps the environment (into time step t + 1) with its generated action $a_t$ (e.g. \"down\"), the environment simulator generates a hindsight template $I_{hind}$ (e.g. \"You are doing well so far.\") based on the comparison between agent action $a_t$ and expert agent action $a_t$ at the last time step t, reflecting on whether the agent is on the right track.\nFor each foresight/hindsight template, we use GPT-4 to augment it into more natural and varied expressions. (e.g. We can augment \"You are doing well so far.\" into \"Up until now, you're doing wonderfully.\" or \"So far, so good, you're doing great!\") We compile all the rewritten sentences into a set called the GPT-augmented language pool. At each step of the non-expert agent, we randomly select one candidate from the pool as the language instruction. This process ensures the feedback provided to the agent has high level of diversity and enriches the learning experience.\nThe level of informativeness and diversity of the language feedback depends on the inclusion of hindsight and foresight (e.g. concatenated when both are required) and the use of GPT-augmented language pool. The language feedback at each time step will finally get concatenated with the trajectory sequence into $(T_d, R_1, s_1, a_1, l_1, . . . R_t, s_t, a_t, l_t)$. Algorithm 1 summarizes the data collection process."}, {"title": "5 Model", "content": "Architecture. We extend the Decision Transformer (DT) architecture (Chen et al., 2021) to create the Language-Teachable Decision Transformer (LTDT) by augmenting the input to include language feedback. This architecture is a decoder-only transformer, similar to GPT-2 (Rad-"}, {"title": "6 Experiment", "content": "In this section, we design experiments to answer the following two research questions (RQs):\n\u2022 RQ 1: How do the informativeness and diversity of language affect agents' performance on seen tasks?\n\u2022 RQ 2: How does the informativeness of the language feedback affect pre-trained agents' adaptability on unseen tasks?\nFor RQ1, we control agents trained with hindsight information, foresight information, or both to investigate the function of informativeness. We compare agents trained with language from both hand-crafted templates and the GPT-augmented language pool to examine the function of language diversity.\nFor RQ2, agents are taught in languages from the GPT-augmented language pool and tested on"}, {"title": "6.1 Experimental Setup", "content": "Setup for RQ 1. We compare performance on seen tasks between agents trained with varying levels of language informativeness and diversity: 1) the No Language agent is trained without any language instructions; 2) the Template Foresight agent is trained with hand-crafted foresight language templates; 3) the Template Hindsight agent is trained with hand-crafted hindsight language templates; 4) the Template Hindsight + Foresight agent is trained with hand-crafted foresight and hindsight language templates; and 5) the GPT-augmented Hindsight + Foresight agent is trained with hindsight and foresight languages from the GPT-augmented language pool. We train on 100, 1,000, 20,000, and 10,000 trajectories for HomeGrid, ALFWorld, Messenger, and MetaWorld environments, respectively. Evaluation is performed over 5 runs, with 100 random seeds for each run.\nSetup for RQ 2. We pre-train different agents on seen tasks and then compare adaptability (how well an agent performs after few-shot learning) on unseen tasks: 1) the No Language pre-trained agent is pre-trained without any language instructions; 2) the GPT-augmented hindsight pre-trained agent is pre-trained with hindsight language from the GPT-augmented language pool; 3) the GPT-augmented foresight pre-trained agent is pre-trained with foresight language from the GPT-augmented language pool; 4) the GPT-augmented hindsight + foresight pre-trained agent is pre-trained with both hindsight and foresight language from the GPT-augmented language pool. During the few-shot adaptation stage, we choose to fine-tune the pretrained agents with both hindsight + foresight language from the GPT-augmented language pool for all settings, since this mimics a real-world few-shot learning scenario, where humans likely provide diverse feedback, including both hindsight and foresight, to guide the agent in new tasks. We pretrain on 6,432, 1,000, 20,000, and 10,000 trajectories for HomeGrid, ALFWorld, Messenger, and MetaWorld, respectively. For all environments, we adapt on 5, 10, and 20 trajectories to 1 new task. Evaluation is performed over 5 runs, with 100 seeds per run.\nFurther details on task setup of RQ 1 and RQ"}, {"title": "6.2 Experimental Results", "content": "Results for RQ 1. As we can see in Figure 4, agents trained with both diverse and informative language feedback (GPT-augmented Hindsight + Foresight) consistently achieve the highest performance across all environments. The varied and paraphrased instructions generated from GPT provide a richer set of linguistic inputs, enabling the agents to develop a more robust language understanding for task execution during evaluation.\nWhen examining the impact of informativeness, we observe that agents trained with both hindsight and foresight information (Template Hindsight + Foresight) consistently achieve higher performance across all environments compared to those trained with only hindsight or foresight information. This indicates that integrating both types of feedback enhances the informativeness of the language, enabling the agents to develop a more comprehensive understanding and leading to better decision-making and overall performance. The only exception is in the Messenger environment, where the no-language agent exhibits a surprisingly strong performance. However, upon further investigation of this exception, we find that if the hindsight-only or foresight-only feedback is from the GPT-augmented pool, the agent can still outperform the No Language agent (refer to Appendix F).\nIn terms of diversity, the results show that agents trained with diverse language feedback, as indi-"}, {"title": "6.3 Ablation Study", "content": "Efficiency Gain vs. Task Difficulty. Can language feedback help the agent to achieve more"}, {"title": "7 Conclusion", "content": "In this paper, we investigate how the informativeness and diversity of language feedback affect"}, {"title": "Limitations", "content": "Our study has several limitations. First, the investigated environments are primarily game-based and do not test the agents' ability to incorporate real-life visual inputs. Future work will focus on evaluating agents in more realistic and complex environments that involve real-world visual inputs and challenges. Second, while GPT language outputs can produce diverse and contextually relevant language, they may not fully cover all human language styles and nuances. Specifically, GPT models might miss certain idioms, dialects, or culturally specific references that are prevalent in human communication. Future work will aim to incorporate a broader spectrum of language variations and test agents in scenarios involving more diverse linguistic inputs."}, {"title": "Ethical Impacts", "content": "Our study, conducted entirely within simulated environments, does not present immediate ethical concerns. The teachable nature of our Language-Teachable Decision Transformer (LTDT) method is designed to make AI agents more controllable and better aligned with human values, promoting safer and more ethical interactions. By enhancing agent performance through informative and diverse language instructions, we aim to foster AI systems that are more transparent and responsive to human guidance, addressing ethical considerations in the deployment of artificial intelligence. As AI becomes more mainstream, these considerations are increasingly pertinent, and our work strives to advance AI technology responsibly."}, {"title": "A.1 Environments Overview", "content": "The Appendix Table 1 lists the information that is inherently available within the environment. All models, regardless of whether they are trained with language input or not, will have access to this environmental information."}, {"title": "A.2 Language Feedback for Different Environments", "content": "For each environment, we design multiple templates conveying different meanings, and then applied GPT-4 to augment the languages into a GPT-augmented language pool. The number of templates and the corresponding GPT-augmented sentences for each template are shown in Appendix Table 2."}, {"title": "A.2.1 HomeGrid", "content": "HomeGrid is a multitask grid world designed to evaluate how well agents can understand and use various types of language to complete tasks. Agents will receive both task specifications and language hints, providing prior knowledge about world dynamics, information about world states, or corrections to assist the agents. We adopt the language hints in HomeGrid as foresight and further extend the environment to provide hindsight that provides comments on agents' past performance. Agents are expected to ground both hindsight and foresight to the environment to achieve higher performance. It includes five task types involving interaction with objects and bins (find, get, clean up, rearrange, open), with a total of 38 tasks. Object"}, {"title": "A.2.2 ALFWorld", "content": "ALFWorld is a text-game environment that aligns with the embodied ALFRED benchmark (Shridhar et al., 2020) and provides simulation for household tasks. It includes six types of tasks where agents need to navigate and interact with household objects through text actions. The location of the task objects is randomly located among 50 locations in each episode, making the task challenging for the agent to plan and for the subgoals. For the experiment, we adopt LLF-ALFWorld (Cheng et al., 2023), which provides an extra language wrapper for hindsight and foresight language generation over the original ALFWorld. The languages are generated based on both agents' past actions and the optimal trajectory for the current episode. Agent gets a reward of 1 when the task is completed. Each template is augmented to 200 sentences in"}, {"title": "A.2.3 Messenger", "content": "Messenger is a grid world with several entities. The agent's primary task is to retrieve a message from one entity and deliver it to another goal entity, all while avoiding enemies. At the start of each episode, the agent is provided with a manual describing the randomized roles of the entities and their movement dynamics. The challenge lies in the fact that the agent does not have access to the true identity of each entity and must ground the text manual to the dynamics, necessitating multi-hop reasoning. (For example, grounding the \"an approaching queen is a deadly enemy\" to the observations of dynamics.) (Lin et al., 2023) The agent receives a sparse reward of 1 when the task is completed. Each template language is augmented to 80 sentences in the GPT template pool. Examples of hindsight and foresight languages are as follows:"}, {"title": "A.2.4 Meta World", "content": "MetaWorld is a simulated benchmark that includes a variety of manipulation tasks performed using a Sawyer robot arm. It includes 50 types of robot manipulation tasks common in daily life. Since our main goal is not meta-learning, we select the \"assembly\" and \"hammer\" tasks for pretraining and adaptation in our experiments. This requires the agent to pick up the tool and aim at the specific target with high precision. To increase the challenge of the tasks, we introduce random disturbances at random steps. This requires the robot to actively recover and return to its normal trajectory whenever it deviates. The agent receives a sparse reward of 1 when completing the task. Each template language is augmented to 180 template languages in the GPT template pool. Examples of hindsight and foresight languages are shown in the following:"}, {"title": "B Agent for Offline Data Collection and Language Feedback Generation", "content": "We use an expert agent and a non-expert agent with sub-optimal policies during the data collection. The sub-optimal policy is used for introducing some errors or perturbations in the training data, and letting the expert policy continue to recover. This helps agents learn to recover from potential failures using hindsight reflections and foresight instructions. In our experiments, we introduced 10-20% random noise in each trajectory as a sub-optimal policy. We found that this level of perturbation aids learning, but excessive disturbance (e.g., >50% per trajectory) significantly degrades performance as agents start learning suboptimal behaviors."}, {"title": "B.1\nHomeGrid", "content": "For the HomeGrid environment, we design an expert planer to work as the expert agent. We first divide the task into several sub-tasks (i.e. divide \"open the recycling bin\" into 1. \"navigate to the bin\", 2. \"open the bin\"). For navigation (move to some place) sub-tasks, we implement breadth-first search to find the optimal path; for interaction sub-task (interact with object), we output the corresponding action. We implement the non-expert agent by adding \"perturbation\" into the expert planer. For example, we randomly reverse the"}, {"title": "B.2 ALFWorld", "content": "For the ALFWorld environment, we use a pre-built expert planer from LLF-Bench (Cheng et al., 2023) to work as both the expert agent and the agent for the data collection."}, {"title": "B.3 Messenger", "content": "As for the Messenger environment, we implement an expert agent using the A* algorithm (Hart et al., 1968). We define the cost by the distance to the target and the distance to the nearest enemies, and then heuristically search in the grid environment. The non-expert agent in the data collection is implemented by adding random disturbance to the expert agent."}, {"title": "B.4 Meta World", "content": "We build the expert agent on the pre-defined policy from the original MetaWorld codebase (Yu et al., 2019) and adapt the policy to random disturbance so that the expert planner can recover to a normal trajectory in any situation."}, {"title": "C Task Settings for RQ 1 and 2", "content": "Task Setting for RQ 1. We evaluate the agents' performance using the same tasks as in the training phase (but with different initialization of the agents and object layout for different episodes). Concretely, 1) in HomeGrid, we train and evaluate on multi-tasks, including FIND, GET, REARRANGE and OPEN; 2) in ALFWorld, we train and evaluate on multi-tasks including PICK&PLACE, CLEAN&PLACE and HEAT&PLACE tasks; 3) in Messenger, we train and evaluate on the task goal \u201cfirst retrieve the message and then deliver to target entity\"; and 4) in MetaWorld, we train and evaluate on the ASSEMBLY task, in which the robot arm needs to pick up the wrench and put it on the peg.\nTask Setting for RQ 2. We evaluate agents' performance on unseen tasks by first pre-training agents on certain tasks and then adapting agents to unseen tasks with few-shot episodes. Specifically, 1) in HomeGrid, we take FIND, GET, REARRANGE, OPEN tasks for pre-training and the CLEAN-UP task for adaptation and evaluation; 2) in ALFWorld, we take PICK&PLACE and CLEAN&PLACE for pretraining and HEAT&PLACE tasks for adaptation and evaluation; 3) in Messenger, we take \"first re-"}, {"title": "D Performance under aligned language type with training.", "content": "As stated in Section 6.1, we use online GPT for all evaluations in RQ 1 and 2 to mimic real-life human language environments. In this section, we align the evaluation language type (and adaptation language type in RQ 2) with each agent's corresponding training language type for further investigation (e.g. No Language Agent is evaluated with empty language; Template Hindsight Agent is evaluated with Template Hindsight). Experiments on RQ 1 and 2 are conducted on HomeGrid and Messenger respectively, with the results presented in Table 3."}, {"title": "E Impact of hindsight on future steps", "content": "Compared to foresight feedback, which provides instructions for the correct action in the next step, hindsight feedback reflects on incorrect actions taken in previous steps. This retrospective analysis can still guide agents toward success by narrowing down the search space for corrective actions. To demonstrate the effectiveness of hindsight feedback, we conduct a quick comparative study between the No Language agent and the Template Hindsight agent in HomeGrid. The study was designed as follows:\n1. Both agents are driven to the same state using an expert policy.\n2. A deliberate mistake is introduced for both agents. Three types of mistakes are designed:\n\u2022 Navigation Mistake: The agent moves in the opposite direction compared to the expert action.\n\u2022 Object Pick/Drop Mistake: The agent picks or drops an object when the expert action is to drop or pick, respectively.\n\u2022 Bin Manipulation Mistake: The agent chooses the wrong action among pedal/lift/grasp to open a specific trash bin.\n3. We use expert actions as the ground truth (GT) actions and compare the performance of both agents over 500 runs.\nThe results are shown in Appendix Table 4: The"}, {"title": "F More results on the Messenger environment", "content": "In the Messenger environment, models trained with only template foresight or hindsight languages struggle to generalize to diverse languages during testing. Without exposure to diverse languages during training, these models fail to extract the learned hindsight or foresight information from mixed and diverse languages. However, Figure 9 demonstrates that models trained with more diverse hindsight or foresight languages can overcome the generalization problem, and outperform those trained without language feedback, showcasing the importance of diversity in the training languages. Furthermore, the agents trained with both hindsight and foresight information still perform the best, aligning with results in other environments."}, {"title": "G Models and Training", "content": "We build our Language-Teachable Decision Transformer based on the code of the original Decision Transformer (Chen et al., 2021). In this section, we will show our training setup and model hyperparameters for each environment.\nWhen selecting the data size, we prioritize the efficient use of a small-scale dataset and examine the impact of language feedback within the constraints of a limited budget and scarce data, as is common in the field of robotics."}, {"title": "G.1 HomeGrid", "content": "Estimated parameter size of the models: 12.191 MB. For research question 1, we train the model with 100 trajectories. For research question 2, the pretraining stages use 6432 trajectories. The models are trained on one Nvidia RTX A6000. For research question 1, training takes 3 GPU hours. For research question 2, pretraining takes 4 GPU hours and adaptation takes 3 GPU hours. Hyperparameters shown in Appendix Table 5."}, {"title": "G.2 ALFWorld", "content": "Estimated parameter size of the models: 6.5 MB. For research question 1, we train the model with 1000 trajectories. For research question 2, the pretraining stages use 10000 trajectories. The models are trained in one Nvidia RTX A6000. For research question 1, training takes 3 GPU hours. For research question 2, pretraining takes 4 GPU hours and adaptation takes 3 GPU hours. Hyperparameters shown in Appendix Table 6."}, {"title": "G.3 Messenger", "content": "Estimated parameters size of the models: 289.681 MB. We train the models with 10000 data trajectories during the pretraining stage for seen tasks. The pretraining stage for seen tasks takes 5 GPU hours on one Nvidia RTX A6000. The adaptation stage for unseen tasks takes 1 GPU hour. Hyperparameters are shown in Appendix Table 7."}, {"title": "G.4 Meta World", "content": "Estimated parameters size of the models: 289.681 MB. We train the models with 20000 data trajectories during the pretraining stage for seen tasks. The pretraining stage for seen tasks takes 2.5 GPU hours on one Nvidia RTX A6000. The adaptation stage for unseen tasks takes 1 GPU hour. Hyperparameters are shown in Appendix Table 8."}, {"title": "H Examples for Language Feedback in Evaluation", "content": "As discussed in section 6.1, we feed template hindsight ($l_{hind}$) and template foresight ($l_{fore}$) into an online GPT to generate language feedback as a proxy for real-world human feedback, which can be further extended into multi-turn human-machine dialogue systems in task-oriented settings (He et al., 2022a,b,c). In Figure 10, we demonstrate three examples of the GPT outcome. In example 1, we find GPT can concatenate both hindsight and foresight"}]}