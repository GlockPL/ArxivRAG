{"title": "Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning", "authors": ["Rui Zhao", "Qirui Yuan", "Jinyu Li", "Haofeng Hu", "Yun Li", "Chengyuan Zheng", "Fei Gao"], "abstract": "End-to-end autonomous driving, which directly maps raw sensor inputs to low-level vehicle controls, is an important part of Embodied AI. Despite successes in applying Multimodal Large Language Models (MLLMs) for high-level traffic scene semantic understanding, it remains challenging to effectively translate these conceptual semantics understandings into low-level motion control commands and achieve generalization and consensus in cross-scene driving. We introduce Sce2DriveX, a human-like driving chain-of-thought (CoT) reasoning MLLM framework. Sce2DriveX utilizes multimodal joint learning from local scene videos and global BEV maps to deeply understand long-range spatiotemporal relationships and road topology, enhancing its comprehensive perception and reasoning capabilities in 3D dynamic/static scenes and achieving driving generalization across scenes. Building on this, it reconstructs the implicit cognitive chain inherent in human driving, covering scene understanding, meta-action reasoning, behavior interpretation analysis, motion planning and control, thereby further bridging the gap between autonomous driving and human thought processes. To elevate model performance, we have developed the first extensive Visual Question Answering (VQA) driving instruction dataset tailored for 3D spatial understanding and long-axis task reasoning. Extensive experiments demonstrate that Sce2DriveX achieves state-of-the-art performance from scene understanding to end-to-end driving, as well as robust generalization on the CARLA Bench2Drive benchmark.", "sections": [{"title": "1. Introduction", "content": "Embodied AI equips intelligent agents like autonomous driving (AD) models with the ability to perceive, reason, and interact with the real world in real-time. However, a core challenge lies in the generalization and consensus of AD model frameworks. On one hand, AD learning frameworks may struggle with generalizing complex, dynamic traffic scenes like varied weather conditions, road layouts, traffic semantics, and the behavioral preferences of surrounding participants. On the other hand, AD systems' decision-making strategies often lack alignment with human drivers' cognitive processes, complicating humans' understanding of system behavior. These challenges arise from the gap between high-level scene semantic understanding and low-level motion control commands. Developing a human-like framework capable of all-weather, all-scene perception and reasoning has thus become a widely discussed topic.\nCurrent AD research often employs small-model learning frameworks (Zeng et al., 2019; Hu et al., 2022; 2023). Due to the limited reasoning capabilities of small models, these systems produce rigid responses to predefined problems, making it difficult to deliver satisfactory results when faced with new or unexpected queries.\nRecently, the rapid development of MLLMs (Li et al., 2023; Liu et al., 2024; Driess et al., 2023) has demonstrated significant advantages in various vision-language tasks. By leveraging MLLMs as a bridge between high-level scene semantic understanding and low-level motion control commands, we can address the challenges of generalization and consensus in AD models. Benefiting from pretraining on extensive cross-modal and cross-disciplinary data, MLLMs offer strong reasoning and generalization capabilities, enabling them to manage diverse scenarios and enhance adaptable cross-scene driving. Additionally, MLLMs' robust text query and cognitive abilities allow them to align driving thoughts with human consensus and translate complex reasoning into understandable natural language, providing a unified explanatory layer for AD. However, AD is a complex task characterized by spatiotemporal continuity, dynamic scenarios, and global coordination. Current MLLM-based AD research primarily uses single-frame front-view scene images (Sima et al., 2025) as perceptual input, leading to a lack of deep understanding of spatiotemporal relationships and road features, along with inadequate traffic scene comprehension. Moreover, when generating driving commands, current studies often only map scene factors to low-level control signals (Xu et al., 2024), overlooking the reasoning behind future vehicle behaviors, failing to utilize MLLMs' capabilities for generalized cognitive reasoning and diverging from human driving thought."}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Multimodal Large Language Models", "content": "Recent advancements have seen the creation of MLLMs. Flamingo (Alayrac et al., 2022) and BLIP2 (Li et al., 2023) align visual features with the embedding space of LLM through gated attention and Q-Former, while LLaVA (Liu et al., 2024) and MiniGPT4 (Zhu et al., 2023b) use Multilayer Perceptrons (MLPs) to combine pretrained vision module with LLM backbone. Additionally, some studies have attempted to extend modality interaction to video and audio. Video-LLaVA (Lin et al., 2023) employs a Language-Bind encoder to pre-align different visual features to the text space, facilitating joint training for images and videos. Video-Llama (Zhang et al., 2023) achieves joint processing of visual and auditory signals in video data by integrating pretrained visual and audio encoder into the LLM."}, {"title": "2.2. Autonomous Driving With Multimodal Large Language Models", "content": "MLLMs have demonstrated the potential to understand traffic scenes, optimize driving decisions, and fundamen-"}, {"title": "2.3. Visual Question Answering Datasets", "content": "To support the efficient training of MLLMs, the design of large-scale VQA datasets has become a research hotspot. Currently, various VQA datasets exist, including image-based datasets such as CLEVR (Johnson et al., 2017), VQA2.0 (Goyal et al., 2017), and EQA (Das et al., 2018), as well as video-based datasets like TVQA (Lei et al., 2018), TGIF-QA (Jang et al., 2017), and ActivityNet-QA (Yu et al., 2019). For ImageQA, early studies (Johnson et al., 2017; Fukui et al., 2016) attempted to fuse image features extracted by Convolutional Neural Networks (CNNs) with question encodings, which were then fed into decoders for answer generation. Recently, Transformer-based models (Tan & Bansal, 2019; Zhang et al., 2021) have achieved state-of-the-art performance in ImageQA tasks. Through attention networks, some studies have effectively captured the intrinsic relationships between temporal context and spatial features in video frames. 3D QA is a novel task in the VQA domain, focusing on answering questions about 3D-view scenes, requiring models to understand the geometric structures and spatial relationships of objects. Recently, many 3D QA datasets have been constructed, such as 3DQA (Ye et al., 2022), ScanQA (Azuma et al., 2022), and SQA3D (Ma et al., 2022). Despite significant progress in the VQA community, challenges remain when dealing with complex traffic scenes involving multimodal, multi-view, and multi-frame contexts. Moreover, the AD field currently lacks comprehensive VQA driving datasets."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Sce2DriveX Framework", "content": "This paper aims to develop a human-like CoT reasoning MLLM framework, enabling progressive reasoning learning from multi-view long-range scene understanding to behavior analysis, motion planning, and vehicle control driving process. As shown in Figure 2, See2DriveX consists of four components: 1) Modal Encoder, including a video encoder $f_{V\\_E}$ and an image encoder $f_{IE}$, initialized from OpenCLIP; 2) Shared Projection $f_p$, using two fully connected layers with GeLU activation; 3) LLM Backbone $f_{LLM}$, employing Vicuna-v1.5-7b; 4) Text Encoder $f_{T\\_E}$ and Text Decoder $f_{T\\_D}$, provided by LLaMA."}, {"title": "3.1.1. MULTIMODAL JOINT LEARNING", "content": "Given text instruction $X_T$, we first use a Byte Pair Encoding (BPE) tokenizer to segment the words into relatively common subwords, each corresponding to a unique logit. Then, these logits are encoded using the text encoder $f_{TE}$: \n$H_T = f_{TE}(BPE(X_T))$"}, {"title": "3.1.2. UNIFIED PROCESSING BY THE LLM BACKBONE", "content": "Our goal is to map multimodal tokens into the text embedding space, providing a unified visual representation for the LLM, which is then combined with tokenized text queries and fed into the LLM backbone to generate responses. Specifically, we first use the shared projection $f_p$ to map the video tokens $H_V$ and image tokens $H_I$: \n$H_L = f_P(H_V, H_I)$"}, {"title": "3.2. VQA Driving Instruction Dataset", "content": "To train Sce2DriveX, this paper constructs the first comprehensive VQA driving instruction dataset for 3D spatial understanding and long-axis task reasoning. Based on the open-source nuScenes (Caesar et al., 2020), it integrates structured data from three modalities: multi-view scene videos (local), BEV map images (global), and multi-round QA annotations. As shown in Figure 3, the dataset includes two subsets: 1) Hierarchical Scene Understanding Dataset; 2) Interpretable End-to-End Driving Dataset."}, {"title": "3.2.1. HIERARCHICAL SCENE UNDERSTANDING", "content": "The hierarchical scene understanding dataset is generated through a scalable automated pipeline, providing a hierarchical, structured description of traffic scenes. It covers weather, roads, facilities, and traffic participants (3D), challenging the model's 3D spatial understanding. Figure 3 shows its construction process, including hierarchical scene development, question set & answer template design, multi-round QA generation, and manual post-processing."}, {"title": "3.2.2. INTERPRETABLE END-TO-END DRIVING", "content": "The interpretable end-to-end driving dataset is automatically integrated through intent-cognition-oriented algorithmic rules, achieving a sequential and transparent description of the driving process. It covers meta-action, behavior justification, planned trajectory, and control signals (multi-type), challenging the model's long-axis task reasoning capabilities. Figure 3 (left) illustrates its construction process, including motion control signal processing, meta-action rule formulation, and behavior justification text generation."}, {"title": "3.3. Training Pipeline", "content": "To further enhance the perception-reasoning performance of Sce2DriveX, this paper introduces a task-oriented three-stage training pipeline, including: 1) Mixed Alignment Pre-training; 2) Scene Understanding Fine-tuning; and 3) End-to-End Driving Fine-tuning."}, {"title": "4. Experiment", "content": ""}, {"title": "4.1. Experimental Setup", "content": "Training Details: We crop each image to a size of 224 \u00d7 224. We sample 8 frames evenly from each video and perform image preprocessing on each frame. Each batch of data includes a combination of images and videos. During the pre-training stage, the model is trained for 1 epoch with a batch size of 128, distributed across 6 A100 (80GB) GPUs. During the fine-tuning stage, AdamW optimizer and cosine learning rate scheduler are used, with an initial learning rate set to 2e-5, a warm-up ratio of 0.03, and a gradient accumulation step of 2. Specifically, scene understanding fine-tuning stage trains the model for 1 epoch, while end-to-end driving fine-tuning stage trains the model for 3 epochs. The entire process is completed on 8 L20 (48GB) GPUs, with a batch size of 4 per GPU."}, {"title": "4.2. Hierarchical Scene Understanding", "content": "We evaluate the performance of Sce2DriveX in hierarchical scene understanding task. Evaluation metrics include NLP metrics, such as BLEU4 (B4), ROUGE (R), METEOR (M), CIDEr (C) and accuracy (Acc). NLP metrics assess text generation quality, while accuracy measures consistency with GT labels. Notably, We focus only on the accuracy of traffic participant types. As no baselines exist for this task, Table 1 reports only Sce2DriveX's results, aiming to inspire future research on hierarchical scene understanding in AD."}, {"title": "4.3. Interpretable End-to-End Driving", "content": "We report all evaluation metrics for interpretable end-to-end driving task. For motion planning (3s planned trajectory), the L2 error (m) is used. For meta-action reasoning, the weighted accuracy Acc(%) is adopted, assigning certain weights to different parts of the meta-action, where the"}, {"title": "4.4. Ablation Study", "content": "We conduct ablation study to evaluate the contributions of each module in the Sce2DriveX framework. The specific settings include: 1) using only the in-context learning strat-"}, {"title": "4.5. Generalization Evaluation", "content": "To further assess the generalization capability of Sce2DriveX, we conduct cross-dataset testing. Specifically, we obtain various styles of corner cases from the driving simulation dataset Bench2Drive (Jia et al., 2024). For concise visualization, only full-view images of the current frame are shown. As shown in Figure 4, we observe that Sce2DriveX is capable of generating satisfactory responses in a zero-shot transfer manner, demonstrating exceptional generalization."}, {"title": "4.6. Qualitative Demonstration", "content": "Figure 5 presents a visualized example of Sce2DriveX's reasoning in complex outdoor driving scenarios, showcasing its ability to perform progressive reasoning from hierarchical scene understanding to human-consensus end-to-end driving. The appendix provides additional qualitative comparisons and visual results of the comprehensive VQA dataset."}, {"title": "5. Conclusion", "content": "In this paper, we propose Sce2DriveX, enabling progressive reasoning from hierarchical scene understanding to interpretable end-to-end driving. Through multimodal learning of local scenes and global maps, it gains a deep understanding of long-range spatiotemporal relationships and road topology, enhancing generalization and consensus in cross-scene driving. We construct the first comprehensive VQA driving dataset for 3D spatial understanding and long-axis task reasoning and introduce a task-oriented three-stage supervised fine-tuning. Experimental results show Sce2DriveX excels in scene understanding, meta-action reasoning, behavior justification, motion planning, and control signal generation. We hope this work provides insights into MLLM applications in autonomous driving."}]}