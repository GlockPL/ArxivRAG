{"title": "On the Role of Model Prior in Real-World Inductive Reasoning", "authors": ["Zhuo Liu", "Ding Yu", "Hangfeng He"], "abstract": "Large Language Models (LLMs) show impressive inductive reasoning capabilities, enabling them to generate hypotheses that could generalize effectively to new instances when guided by in-context demonstrations. However, in real-world applications, LLMs' hypothesis generation is not solely determined by these demonstrations but is significantly shaped by task-specific model priors. Despite their critical influence, the distinct contributions of model priors versus demonstrations to hypothesis generation have been underexplored. This study bridges this gap by systematically evaluating three inductive reasoning strategies across five real-world tasks with three LLMs. Our empirical findings reveal that, hypothesis generation is primarily driven by the model's inherent priors; removing demonstrations results in minimal loss of hypothesis quality and downstream usage. Further analysis shows the result is consistent across various label formats with different label configurations, and prior is hard to override, even under flipped labeling. These insights advance our understanding of the dynamics of hypothesis generation in LLMs and highlight the potential for better utilizing model priors in real-world inductive reasoning tasks.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have drawn significant interests due to their performance on a diverse range of reasoning tasks (Kojima et al., 2022), such as mathematical reasoning, common-sense reasoning and symbolic reasoning. Inductive reasoning- an important component of reasoning (Yang et al., 2022; Heit, 2000), as a way to derive abstract hypothesis from limited specific observations, is widely regarded as a core aspect of human intelligence.\nExisting studies primarily assess the inductive reasoning capabilities of LLMs (Wang et al., 2023; Qiu et al., 2023; Cheng et al., 2024) by evaluating their ability to generate textual hypotheses based on in-context input-output pairs and subsequently test these hypotheses on unseen examples, thereby evaluating their generalization abilities. These studies demonstrated that LLMs can propose high-quality hypotheses, establishing them as exceptional hypothesis generators (Qiu et al., 2023; Cheng et al., 2024; Li et al., 2024).\nLLMs employ various approaches to generate hypotheses depending on the nature of the task. For symbolic tasks, such as mathematical function discovery (Shojaee et al., 2024), LLMs rely primarily on input-output mappings in demonstrations, often with minimal prior knowledge about the mathematical functions. In contrast, research by Qi et al. (2023) demonstrated that LLMs can formulate hypotheses solely from provided background information, leveraging the extensive and diverse knowledge gained during pre-training. In real-world applications, hypothesis generation tends to be data-driven, such as generating hypotheses for trending Twitter headline patterns (Zhou et al., 2024), where both prior knowledge and demonstrations are utilized. In these cases, the interaction between the model's task-specific priors and provided examples is mixed.\nIn empirical science, data-driven hypothesis generation serves as the foundational step toward scientific discovery (Majumder et al., 2024a,b). When employing LLMs for hypothesis generation, the goal is to uncover novel hypotheses that contribute fresh insights and ideas to the existing literature (Zhou et al., 2024). However, due to the combined influence of the model's prior knowledge and the provided examples, the origin of generated hypotheses often remains unclear. For certain tasks, where LLMs are pre-trained on extensive knowledge bases, a strong model prior may even overshadow the potential for generating genuinely novel insights from the provided examples. This raises a critical question: What is the role of model prior in real-world inductive reasoning?"}, {"title": "Related Work", "content": "Inductive Reasoning with LLMs. Primary studies on inductive reasoning mainly focus on evaluating their inductive reasoning capabilities. Qiu et al. (2023) evaluate LLMs by inducting rules from examples, demonstrated that LLMs are good hypothesis proposers. Wang et al. (2023) uses Python programs to select better hypothesis, thus improving the inductive reasoning performance. Besides these evaluations on symbolic tasks, Yang et al. (2022) propose to induce natural language rules from natural language facts while Hypotheses-to-Theories (Zhu et al., 2023) learns rules from deduction. Similarly, Honovich et al. (2022) also show LLMs are able to infer a natural task description by provided demonstrations. Recently, some works employ LLMs to generate hypothesis that can describe the difference or shift between two distributions in different modalities, such as text (Zhong et al., 2022, 2023; Singh et al., 2022), and image (Dunlap et al., 2024; Kim et al., 2024). Distinct from these studies, our work delves into understanding how LLMs perform inductive reasoning for real-world tasks, offering insights into their underlying mechanisms.\nHypothesis Generation with LLMs. Yang et al. (2023b) uses raw web corpus as observations to generate scientific hypothesis, and Pham et al. (2023) generates hypothesis to uncover latent topics in a text collection. In Qi et al. (2023), it shows LLMs are good hypothesis proposers with only background knowledge. Majumder et al. (2024a) provides initial evidence for LLMs to do data-driven discovery, where both search and verification of hypotheses may be carried out using a dataset alone. HypoGeniC (Zhou et al., 2024) also uses LLMs to generate hypothesis from real-world labeled examples. Si et al. (2024) and Baek et al. (2024) further explore the potential to generate hypothesis in research with LLMs to provide insights and ideas for the literature. Additionally, Liu et al. (2024) combines theory-based generation and data-driven generation to get better hypothesis. However, these works do not clearly distinguish whether the hypotheses originate from hidden knowledge or provided examples\u2014a distinction that is the central focus of our work."}, {"title": "Natural Language Hypothesis Generation", "content": "Let $Z = D_p \\cup D_N$ represent the labeled data for a real-world classification task $T$, where $D_p$ and $D_N$ correspond to demonstrations of the positive (P) and negative (N) classes, respectively. Each sample in Z is a pair $(x, y)$, where x denotes the example and $y \\in {P, N}$ represents the label. A valid natural language hypothesis h, as introduced by Zhong et al. (2022), is expressed as a natural language string. For any example x, h is capable of determining whether x belongs to the positive or negative class.\nNatural language hypothesis generation involves prompting LLMs to produce a set of valid hypotheses $H = {h_1, h_2, ..., h_m}$ using in-context demonstrations tailored to task T. In this paper, we consider the setting where the input to LLMs can be divided into two parts, as shown in Figure 1: (1) Task-Specific Instructions: a set of natural lan-"}, {"title": "Experimental Settings", "content": "In this paper, we evaluate three commonly-used hypothesis generation baselines.\nInput-Output Prompting. Input-output prompting (IO-Prompting) represents the most common approach to prompting LLMs (Qiu et al., 2023). In this standard IO-Prompting framework, we directly provide the LLMs with a set of in-context demonstrations within the prompt context. The objective is to generate m hypotheses that effectively captures the patterns of positive class P. This approach is a single-step method, utilizing the in-context demonstrations once to guide the model's hypothesis generation.\nIterative Refinement with Ranking. Standard IO-prompting utilizes in-context demonstrations only once, potentially under utilizing their full capacity. To address this limitation, various methods have been proposed to iteratively refine hypotheses, thereby enhancing model performance (Wang et al., 2023; Qiu et al., 2023; Shojaee et al., 2024; Xiao et al., 2024). In our approach, we iteratively refine hypotheses using ranking information as a feedback signal.\nThe refinement process begins with an initial set of m hypotheses generated via IO-prompting. At each iteration, hypotheses in the bank are ranked based on their performance on a validation set. The top-ranked m hypotheses are then fed back to the model, along with in-context demonstrations, guiding it to generate hypotheses with improved performance. In cases where no demonstrations are available, only the ranked hypotheses with their accuracies are provided in the iterative refinement process. This approach thus augments data utilization by continuously leveraging feedback to generate higher-quality hypotheses.\nUpdate from Mistakes: HypoGeniC. The previous methods leverage data within one single prompt to generate hypotheses, yet using all demonstrations in a single prompt may not be optimal for performance. Therefore, we also evaluate a strategy that updates hypotheses from mistakes made by current hypothesis. We largely follow an established approach, HypoGeniC (Zhou et al., 2024; Liu et al., 2024), which iteratively generate new hypotheses from incorrect prediction examples.\nIn our evaluation, we initialize the hypothesis bank using standard IO-prompting as well as the reward scores as in Zhou et al. (2024); Liu et al. (2024). During the update phase, if the number of incorrect examples for each group reaches a predefined number, these incorrect examples are employed to guide the generation of new hypotheses. In each update, m hypotheses with highest reward scores are kept in the hypothesis bank. This iterative updating approach enables the model to adapt hypotheses progressively, making better use of feedback from misclassifications. For a fair comparison, when demonstrations are absent, we update the hypothesis by iterative refinement, using reward scores for ranking.\nAll the implementation details are in the Appendix B."}, {"title": "Evaluation of Hypothesis", "content": "After generating a set of hypotheses $H = {h_1, h_2, ..., h_m}$, it is crucial to evaluate their quality to ensure that the generated hypotheses are both functional and interpretable. We perform this evaluation from three perspectives: hypothesis-based classification, LLM-based evaluation and human evaluation. These complementary methods allow for a robust assessment, combining quantitative performance metrics with qualitative assessments from domain experts.\nHypothesis-based Inference. In hypothesis-based inference (Liu et al., 2024; Zhou et al., 2024), the goal is to assess how well the generated hypotheses support downstream decision-making tasks. We measure the predictive performance of the hypothesis on a test dataset $D_{test} = {(x_j, y_j)}_{j\\in test}$. The hypothesis is evaluated based on how accurately it assigns the correct label to each input $x_j$. Predictions are made by comparing test examples $x_j$ with learned patterns, which can consist of a single hypothesis or multiple hypotheses. If a test example satisfies the pattern, it is assigned the corresponding class. Unless otherwise stated, the results reported in this work are based on patterns formed from single hypothesis. To remove the influence of prior in the inference, we also do hypothesis-based inference without knowledge, which can be found in Appendix C.1. See Appendix F for evaluation prompts.\nLLM-based Evaluation. In addition to assessing the effectiveness of hypotheses in downstream task usage, we also evaluate their helpfulness (Liu et al., 2024) and novelty (Liu et al., 2024; Si et al., 2024) through LLM-based metrics. Specifically: (1) Helpfulness measures the extent to which a hypothesis accurately captures the underlying patterns of the data and generalizes effectively to unseen samples. (2) Novelty assesses whether the hypothesis introduces new insights or unique perspectives relevant to the task.\nOur LLM-based evaluation incorporates both scoring and pairwise comparison assessments. For scoring, LLMs assign a rating on a 5-point scale to reflect each hypothesis's quality. For pairwise comparison, we randomly pair hypotheses generated with and without demonstrations, and prompt the LLMs to select the better hypothesis in each pair. This pairwise evaluation provides insights into relative performance, while scoring offers an absolute measure of quality."}, {"title": "Other Settings", "content": "Models. We conduct experiments with GPT-40, Qwen2-VL-72B\u00b9 and gemini-1.5-pro-002, leveraging both open-source models and API-accessible models to ensure diverse evaluation. Unless otherwise stated, we use GPT-402 in experiments."}, {"title": "Task-Specific Model Prior Dominates Hypothesis Generation", "content": "To see the impact of the model prior in hypothesis generation, we compare the hypothesis generation in the following two settings.\nModel Prior Only is a typical zero-shot hypothesis generation scenario without the use of demonstrations, relying primarily on prior for generation.\nDemos with Ground Truth Labels is used in a typical real-world inductive reasoning tasks, with demonstrations as a specific guidance.\nResults for single hypothesis-based and multiple hypotheses-based classification are shown in Table 1 and Table 3. From the results, We find that removing in-context demonstrations cause little degradation for the downstream task performance. The trend is consistent across five different datasets on three baselines. In some cases, LLMs can even generate better hypothesis using only model prior. Additionally, iterative refinement outperforms the other two baselines, showing that data still helps for hypothesis selection, but not as in-context demonstrations for hypothesis generation.\nResutls with Qwen2-VL and Gemini-1.5-pro. The results for single hypothesis-based classification on Qwen2-VL and Gemini-1.5-pro-002, with IO-prompting, are provided in Table 2 and Appendix C.3. These results similarly show a negligible performance drop without demonstrations, underscoring the universality of our findings across different models.\nThese results indicates LLMs are good zero-shot hypothesis proposers under strong prior, and in-context demonstrations with ground truth labels are not necessary to achieve acceptable hypothesis."}, {"title": "Input-Label Mappings in Demonstrations Cannot Override Strong Model Prior", "content": "To further explore the interaction between model prior and input-label mappings in demonstrations in hypothesis generation, we use in-context demonstrations with different label settings:\n(1) Demos with ground truth (correct) labels.\n(2) Demos with flipped labels.\n(3) Demos with random labels.\n(4) Only positive group demos.\n(5) Only negative group demos.\nFigure 2 illustrates the relative accuracy difference between various label settings and without demonstrations. From the result, there is quite limited difference (mostly smaller than 3%) of performance among different settings, with the flipped label setting in truthful review as an exception, which has a performance degradation about 4.5%.\nThese findings suggest that while demonstrations can provide some guidance, the models' hypothesis generation abilities are ultimately shaped more by its pre-trained priors than by any superficial label configurations. Furthermore, the prior"}, {"title": "Is the result consistent with different in-context demonstration label formats?", "content": "To evaluate the consistency of results across different label formats, we compare two label formats:\nLabel Format 1: Demonstrations are provided as examples for positive and negative classes as in Figure 1. Label Format 2: Demonstrations are presented in the format of (Example, Label).\nThe average accuracy across all datasets for the correct and flipped label settings is presented in Figure 5. (Results for each dataset of Label Fomat 2 can be found in Appendix C.2). With correct labels, the performance of the two label formats is very similar. However, in the flipped label settings, Label Format 2 shows almost no performance drop, which differs slightly from Label Format 1. Notably, neither label format outperforms the hypotheses generated without demonstrations. This finding highlights the dominant role of the strong model"}, {"title": "What's the difference between correct label and flipped label settings?", "content": "To get an deep understanding for the impact of flipping labels and provide a more fine-grained evaluation, we adopt two additional metrics introduced by Wu et al. (2024), Adverse Correction Rate (ACR) and Beneficial Correction Rate (BCR):\n$ACR = \\frac{\\sum_{i=1}^{n} \\mathbb{I} (Y_{correct} (X_i) = Y_i \\land Y_{flipped} (X_i) \\neq Y_i)}{\\sum_{i=1}^{n} \\mathbb{I} (Y_{correct} (X_i) = Y_i)}$\n$BCR = \\frac{\\sum_{i=1}^{n} \\mathbb{I} (Y_{correct} (X_i) \\neq Y_i \\land Y_{flipped} (X_i) = Y_i)}{\\sum_{i=1}^{n} \\mathbb{I} (Y_{correct} (X_i) \\neq Y_i)}$\nwhere $Y_{correct}(x_i)$ and $Y_{flipped}(x_i)$ represents the prediction results using the hypothesis generated with ground truth label and flipped label demonstrations, $x_i$, $y_i$ are input and ground truth label, respectively. These metrics offer a comprehensive evaluation of how flipping labels of the demonstrations influence the prediction results in downstream tasks.\nThe results indicate that flipping the labels of in-context demonstrations does lead to some shifts in prediction outcomes, particularly notable in the"}, {"title": "A Case Study: Hypothesis Generation for Positive Sentiment Pattern", "content": "This case study highlights that large language models (LLMs) heavily rely on prior knowledge when generating hypotheses, often ignoring patterns introduced in demonstrations. As shown in Figure 6, we replace true positive demonstrations with flipped label demonstrations (negative examples) to test whether the model adjusts its hypothesis or adheres to its prior.\nUsing IO-prompting, we provide six demonstrations, varying the number of flipped label demos from 0 to 5, and prompt the model to generate a hypothesis and corresponding supporting demonstrations. Repeating the experiment across 50 random seeds, we track the distribution of true positive and negative examples within the model's supported demonstrations for its hypothesis.\nThe results, shown in Figure 7, reveal notable patterns. The distribution of positive examples in the supported demonstrations begins to shift when three flipped label demonstrations are introduced. When five flipped demonstrations are provided, the mean number of positive examples converges to one. However, the model consistently avoids using flipped label demonstrations in its hypothesis generation, even when five demonstrations are flipped. This indicates that the model's hypotheses are predominantly influenced by prior knowledge rather than the provided demonstrations."}, {"title": "Conclusion", "content": "In this paper, we explore the role of task-specific priors in a real-world inductive reasoning scenario-hypothesis generation from labeled data. Experiments reveal that LLMs rely heavily on strong priors, which are difficult to override with demonstrations, offering insights into hypothesis generation mechanisms and future research directions."}, {"title": "Limitations", "content": "Beyond Classification Problems. Our experiments are limited to classification problems. Extensions to multi-choice or other tasks requires better representation of the hypothesis. We leave extensions to non-classification tasks for future work.\nBetter Application of Generated Hypotheses. We think future can explore better application of generated hypotheses. For instance, this paper uses hypotheses to construct patterns for classification problems. Better application of hypotheses can improve downstream task performance, which we leave for future work."}]}