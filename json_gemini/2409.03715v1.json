{"title": "Applications and Advances of Artificial Intelligence in Music Generation:A Review", "authors": ["Yanxu Chen", "Linshu Huang", "Tian Gou"], "abstract": "In recent years, artificial intelligence (AI) has made significant progress in the field of music generation, driving innovation in music creation and applications. This paper provides a systematic review of the latest research advancements in Al music generation, covering key technologies, models, datasets, evaluation methods, and their practical applications across various fields. The main contributions of this review include: (1) presenting a comprehensive summary framework that systematically categorizes and compares different technological approaches, including symbolic generation, audio generation, and hybrid models, helping readers better understand the full spectrum of technologies in the field; (2) offering an extensive survey of current literature, covering emerging topics such as multimodal datasets and emotion expression evaluation, providing a broad reference for related research; (3) conducting a detailed analysis of the practical impact of AI music generation in various application domains, particularly in real-time interaction and interdisciplinary applications, offering new perspectives and insights; (4) summarizing the existing challenges and limitations of music quality evaluation methods and proposing potential future research directions, aiming to promote the standardization and broader adoption of evaluation techniques. Through these innovative summaries and analyses, this paper serves as a comprehensive reference tool for researchers and practitioners in Al music generation, while also outlining future directions for the field.", "sections": [{"title": "Introduction", "content": "Music, as a universal and profound art form, transcends cultural and geographical boundaries, playing an unparalleled role in emotional expression(Juslin and Sloboda 2011). With the rapid advancement of technology, music creation has evolved from the manual operations of the early 20th century, relying on analog devices and tape recordings, to today's fully digital production environment(Katz 2010; Pinch and Bijsterveld 2012; Deruty et al. 2022; Oliver and Lalchev 2022). In this evolution, the introduction of Artificial Intelligence (AI) has injected new vitality into music creation, driving the rapid development of automatic music generation technologies and bringing unprecedented opportunities for innovation(Briot, Hadjeres, and Pachet 2020; Zhang, Yan, and Briot 2023).\nResearch Background and Current Status:The research on automatic music generation dates back more than 60 years, with the earliest attempts primarily based on grammatical rules and probabilistic models(Hiller and Isaacson 1979; Dash and Agres 2023). However, with the rise of deep learning technologies, the field of AI music generation has entered an unprecedented period of prosperity(Goodfellow 2016; Moysis et al. 2023). Modern AI technologies can not only handle symbolic music data but also generate high-fidelity audio content directly, with applications ranging from traditional instrument simulation to entirely new sound design(Oord et al. 2016; Lei et al. 2024). Symbolic music generation relies on representations such as piano rolls and MIDI, enabling the creation of complex structured musical compositions; meanwhile, audio generation models deal directly with continuous audio signals, producing realistic and layered sounds(Dong et al. 2018; Ji, Yang, and Luo 2023).\nIn recent years, AI music generation technologies have made remarkable progress, especially in the areas of model architecture and generation quality(Huang et al. 2018a; Agostinelli et al. 2023). The application of Generative Adversarial Networks (GANs), Transformer architectures, and the latest diffusion models has provided strong support for the diversity, structure, and expressiveness of generated music(Goodfellow et al. 2014; Vaswani 2017; Ho, Jain, and Abbeel 2020; Kong et al. 2020b; Shahriar 2022). Additionally, new hybrid model frameworks that combine the strengths of symbolic and audio generation further enhance the structural integrity and timbral expressiveness of generated music(Huang et al. 2018a; Wang, Min, and Xia 2024; Qian et al. 2024). These advancements have not only expanded the technical boundaries of AI music generation but also opened up new possibilities for music creation(Wang et al. 2024).\nResearch Motivation:Despite significant advances in AI music generation, numerous challenges remain. Enhancing the originality and diversity of generated music, capturing long-term dependencies and complex structures in music, and developing more standardized evaluation methods are core issues that the field urgently needs to address. Furthermore, as the application areas of AI-generated music continue to expand-such as healthcare, content creation, and education-the demands for quality and control of generated music are also increasing. These challenges provide a broad space for future research and technological innovation.\nResearch Objectives:This paper aims to systematically"}, {"title": "History of Music Production", "content": "Early Stages of Music Production\nIn the early 20th century, music production mainly relied on analog equipment and tape recording technology. Sound engineers and producers used large analog consoles for recording, mixing, and mastering. This period emphasized the craftsmanship and artistry of live performances, with the constraints of recording technology and equipment making the process of capturing each note filled with uncertainty and randomness.(Zak III 2001; Horning 2013) The introduction of synthesizers brought revolutionary changes to music creation, particularly in electronic music. In the 1970s, synthesizers became increasingly popular, with brands like Moog and Roland symbolizing the era of electronic music. Synthesizers generated various sounds by modulating waveforms (such as sine and triangle waves), allowing music producers to create a wide range of tones and effects on a single instrument, thereby greatly expanding the possibilities for musical expression(Pinch and Trocco 2004; Holmes 2012).\nThe Rise of Digital Audio Workstations (DAWS)\nWith advances in digital technology, Digital Audio Workstations (DAWs) began to rise in the late 1980s and early 1990s. The advent of DAWs marked the transition of music production into the digital era, integrating recording, mixing, editing, and composition into a single software platform, making the music production process more efficient and convenient(Hracs, Seman, and Virani 2016; Danielsen 2018; Th\u00e9berge 2021; Cross 2023). The widespread application of MIDI (Musical Instrument Digital Interface) further propelled the development of digital music production. MIDI facilitated communication between digital instruments and computers, becoming a critical tool in modern music production. Renowned DAWs like Logic Pro, Ableton Live, and FL Studio provided producers with integrated working environments, streamlining the music creation process and democratizing music production(D'Errico 2016; Reuter 2022).\nExpansion of Plugins and Virtual Instruments\nThe popularity of DAWs fueled the development of plugins and virtual instruments. Plugins, as software extensions, added new functionalities or sound effects to DAWS, vastly expanding the creative potential of music production. Platforms like Kontakt offered various high-quality virtual instruments, while synthesizer plugins such as Serum and Phase Plant, utilizing advanced wavetable synthesis, provided producers with extensive sound design possibilities. The diversity and flexibility of plugins greatly broadened"}, {"title": "Music Representation", "content": "The representation of music data is a core component of AI music generation systems, directly influencing the quality and diversity of the generated results. Different music representation methods capture distinct characteristics of music, significantly affecting the input and output of AI models. Below are some commonly used music representation methods and their application scenarios:\n3.1 Piano Roll\nA piano roll is a two-dimensional matrix that visually represents the notes and timing of music, making it particularly suitable for capturing melody and chord structures. The rows of the matrix represent pitch, columns represent time, and the values indicate whether a particular pitch is activated at a given time point. This representation is widely used in deep learning models as it directly maps to the input and output layers of neural networks, facilitating the processing and generation of complex musical structures. For example, MuseGAN(Dong et al. 2018) uses piano roll representation for multi-part music, generating harmonically rich compositions through Generative Adversarial Networks (GANs).\n3.2 MIDI (Musical Instrument Digital Interface)\nMIDI is a digital protocol used to describe various musical parameters such as notes, pitch, velocity, tempo, and chords. MIDI files do not record actual audio data but rather instructions that control audio, making them highly flexible and allowing playback in various styles on different synthesizers and virtual instruments. MIDI is extensively used in music creation, arrangement, and Al music generation, especially in symbolic music generation, where it serves as a crucial format for input and output data. Its advantages lie in cross-platform and cross-device compatibility and the precise control of musical parameters. Music-VAE(Brunner et al. 2018) utilizes MIDI to represent symbolic music, where notes and timing are discrete, enabling the model to better capture structural features and generate music with complex harmony and melody.\n3.3 Mel Frequency Cepstral Coefficients (MFCCs)\nMFCCs are a compact representation of the spectral characteristics of audio signals, widely used in speech and music processing, particularly effective in capturing subtle differences in music. By decomposing audio signals into short-time frames and applying the Mel frequency scale, MFCCs capture audio features perceived by the human ear. Although primarily used in speech recognition, MFCCs also find extensive applications in music emotion analysis, style classification, and audio signal processing. For example, Google's NSynth project uses MFCCs(Engel et al. 2017) for generating and classifying different timbres.\n3.4 Sheet Music\nSheet music is a traditional form of music representation that records musical information through staff notation and various musical symbols. It includes not only pitch and rhythm but also dynamics, expressive marks, and other performance instructions. In AI music generation, sheet music representation is also employed, particularly for generating readable compositions that adhere to music theory. Models using sheet music as input, such as Music Transformer(Huang et al. 2018b), can generate compositions with complex structure and coherence.\n3.5 Audio Waveform\nThe audio waveform directly represents the time-domain waveform of audio signals, suitable for generating and processing actual audio data. Although waveform representation involves large data volumes and complex processing, it provides the most raw and detailed audio information, crucial in audio synthesis and sound design. For instance, the WaveNet(van den Oord et al. 2016) model uses waveforms directly to generate highly realistic speech and music.\n3.6 Spectrogram\nA spectrogram converts audio signals into a frequency domain representation, showing how the spectrum of frequencies evolves over time. Common spectrograms include Short-Time Fourier Transform (STFT) spectrograms, Mel spectrograms, and Constant-Q transform spectrograms. Spectrograms are highly useful in music analysis, classification, and generation, as they capture both the frequency"}, {"title": "Generative Models", "content": "The field of AI music generation can be divided into two main directions: symbolic music generation and audio music generation. These two approaches correspond to different levels and forms of music creation.\n4.1 Symbolic Music Generation\nSymbolic music generation uses AI technologies to create symbolic representations of music, such as MIDI files, sheet music, or piano rolls. The core of this approach lies in learning the structures of music, chord progressions, melodies, and rhythmic patterns to generate compositions with logical and structured music. These models typically handle discrete note data, and the generated results can be directly played or further converted into audio. In symbolic music generation, LSTM models have shown strong capabilities. For instance, DeepBach(Hadjeres, Pachet, and Nielsen 2017a) uses LSTMs to generate Bach-style harmonies, producing harmonious chord progressions based on given musical fragments. However, symbolic music generation faces challenges in capturing long-term dependencies and complex structures, particularly when generating music on the scale of entire movements or songs, where maintaining long-range musical dependencies can be difficult.\nRecently, Transformer-based symbolic music generation models have demonstrated more efficient capabilities in capturing long-term dependencies. For example, the Pop Music Transformer(Huang and Yang 2020) combines self-attention mechanisms and Transformer architecture to achieve significant improvements in generating pop music. Additionally, MuseGAN, a GAN-based multi-track symbolic music generation system, can generate multi-part music suitable for creating compositions with rich layers and complex harmonies. The MuseCoco model(Lu et al. 2023) combines natural language processing with music creation, generating symbolic music from text descriptions and allowing precise control over musical elements, making it ideal for creating complex symbolic music works. However, symbolic music generation mainly focuses on notes and structure, with limited control over timbre and expressiveness, highlighting its limitations.\n4.2 Audio Music Generation\nAudio music generation directly generates the audio signal of music, including waveforms and spectrograms, handling continuous audio signals that can be played back directly or used for audio processing. This approach is closer to the recording and mixing stages in music production, capable of producing music content with complex timbres and realism.\nWaveNet(van den Oord et al. 2016), a deep learning-based generative model, captures subtle variations in audio signals to generate expressive music audio, widely used in speech synthesis and music generation. Jukebox(Dhariwal et al. 2020), developed by OpenAI, combines VQ-VAE and autoregressive models to generate complete songs with lyrics and complex structures, with sound quality and expressiveness approaching real recordings. However, audio music generation typically requires substantial computational resources, especially when handling large amounts of audio data. Additionally, audio generation models face challenges in controlling the structure and logic of music over extended durations.\nRecent research on diffusion models has made significant progress, initially used for image generation but now extended to audio. For example, DiffWave(Kong et al. 2020b) and WaveGrad(Chen et al. 2020b) are two representative audio generation models; the former generates high-fidelity audio through a progressive denoising process, and the latter produces detailed audio through a similar diffusion process. The MeLoDy model(Stefani 1987) combines language models (LMs) and diffusion probability models (DPMs), reducing the number of forward passes while maintaining high audio quality, addressing computational efficiency issues. Noise2Music(Huang et al. 2023a), based on diffusion models, focuses on the correlation between text prompts and generated music, demonstrating the ability to generate music closely related to input text descriptions.\nOverall, symbolic music generation and audio music generation represent the two primary directions of Al music generation. Symbolic music generation is suited for handling and generating structured, interpretable music, while audio music generation focuses more on the details and expressiveness of audio signals. Future research could combine these two methods to enhance the expressiveness and practicality of AI music generation, achieving seamless transitions from symbolic to audio, and providing more comprehensive technical support for music creation.\n4.3 Current Major Types of Generative Models"}, {"title": "Hybrid Model Framework: Integrating Symbolic and Audio Music Generation", "content": "Recently, researchers have recognized that combining the strengths of symbolic and audio music generation can significantly enhance the overall quality of generated music. Symbolic music generation models (e.g., MIDI or sheet music generation models) excel at capturing musical structure and logic, while audio generation models (e.g., WaveNet(Oord et al. 2016) or Jukebox(Dhariwal et al. 2020)) focus on generating high-fidelity and complex timbre audio signals. However, each model type has distinct limitations: symbolic generation models often lack expressiveness in timbre, and audio generation models struggle with long-range structural modeling. To address these challenges, recent studies have proposed hybrid model frameworks that combine the advantages of symbolic and audio generation. A common strategy is to use methods that jointly employ Variational Autoencoders (VAE) and Transformers. For example, in models like MuseNet(Topirceanu, Barina, and Udrescu 2014) and MusicVAE(Yang et al. 2019), symbolic music is first generated by a Transformer and then converted into audio signals. These models typically use VAE to capture latent representations of music and employ Transformers to generate sequential symbolic representations. Self-supervised learning methods have gained increasing attention in symbolic music generation. These approaches often involve pre-training models to capture structural information in music, which are then applied to downstream tasks. Models like Jukebox(Dhariwal et al. 2020) use self-supervised learning to enhance the generalization and robustness of generative models.\nAdditionally, combining hierarchical symbolic music generation with cascaded diffusion models has proven effective(Wang, Min, and Xia 2024). This approach defines a hierarchical music language to capture semantic and contextual dependencies at different levels. The high-level language handles the overall structure of a song, such as paragraphs and phrases, while the low-level language focuses on"}, {"title": "Datasets", "content": "In the field of Al music generation, the choice and use of datasets profoundly impact model performance and the quality of generated results. Datasets not only provide the foundation for model training but also play a key role in enhancing the diversity, style, and expressiveness of generated music. This section introduces commonly used datasets in AI music generation and discusses their characteristics and application scenarios.\n5.1 Commonly Used Open-Source Datasets for Music Generation\nIn the music generation domain, the following datasets are widely used resources that cover various research directions, from emotion recognition to audio synthesis. This section introduces these datasets, including their developers or owners, and briefly describes their specific applications.\n\u2022 CAL500 (2007)\nThe CAL500 dataset(Turnbull et al. 2007), developed by Gert Lanckriet and his team at the University of California, San Diego, contains 500 MP3 songs, each with detailed emotion tags. These tags are collected through subjective evaluations by listeners, covering various emotional categories. The dataset is highly valuable for static emotion recognition and emotion analysis research.\n\u2022 MagnaTagATune (MTAT) (2008)\nDeveloped by Edith Law, Kris West, Michael Mandel, Mert Bay, and J. Stephen Downie, the MagnaTagATune dataset(Law et al. 2009) uses an online game called \"TagATune\" to collect data. It contains approximately 25,863 audio clips, each 29 seconds long, sourced from Magnatune.com songs. Each clip is associated with a binary vector of 188 tags, independently annotated by multiple players. This dataset is widely used in automatic music annotation, emotion recognition, and instrument classification research.\n\u2022 Nottingham Music Dataset (2009)\nThe Nottingham Music Dataset(Boulanger-Lewandowski, Bengio, and Vincent 2012) was originally developed by Eric Foxley at the University of Nottingham and released on SourceForge. It includes over 1,000 traditional folk tunes suitable for ABC notation. The dataset has been widely used in traditional music generation, music style analysis, and symbolic music research.\n\u2022 Million Song Dataset (MSD) (2011)\nThe Million Song Dataset(Bertin-Mahieux et al. 2011) is a benchmark dataset designed for large-scale music information retrieval research, providing a wealth of processed music features without including original audio or lyrics. It is commonly used in music recommendation systems and feature extraction algorithms.\n\u2022 MediaEval Emotion in Music (2013)\nThe MediaEval Emotion in Music dataset(Soleymani et al. 2013) contains 1,000 MP3 songs specifically for music emotion recognition research. The emotion tags were obtained through subjective evaluations by a group of annotators, making it useful for developing and validating music emotion recognition models.\n\u2022 AMG1608 (2015)\nThe AMG1608 dataset (Penha and Cozman 2015), developed by Carmen Penha, Fabio G. Cozman, and researchers from the University of S\u00e3o Paulo, contains 1,608 music clips, each 30 seconds long, annotated for emotions by 665 subjects. The dataset is particularly suitable for personalized music emotion recognition research due to its detailed emotional annotations, especially those provided by 46 subjects who annotated over 150 songs.\n\u2022 VCTK Corpus (2016)\nDeveloped by the CSTR laboratory at the University of Edinburgh, the VCTK Corpus(Christophe Veaux 2017) contains speech data recorded by 110 native English speakers with different accents. Each speaker read about 400 sentences, including texts from news articles, rainbow passages, and accent archives. This dataset is widely used in Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) model development.\n\u2022 Lakh MIDI (2017)\nThe Lakh MIDI dataset(Raffel 2016) is a collection of 176,581 unique MIDI files, with 45,129 files matched and aligned with entries from the Million Song Dataset. It is designed to facilitate large-scale music information retrieval, including symbolic (using MIDI files only) and audio-based (using information extracted from MIDI files as annotations for matching audio files) research.\n\u2022 NSynth (2017)\nNSynth(Engel et al. 2017), developed by Google's Magenta team, is a large-scale audio dataset containing over 300,000 monophonic sound samples generated using instruments from commercial sample libraries. Each note has unique pitch, timbre, and envelope characteristics, sampled at 16 kHz and lasting 4 seconds. The dataset includes notes from various instruments sampled at different pitches and velocities.\n\u2022 DEAM (2017)\nThe DEAM dataset(Aljanaki, Yang, and Soleymani 2017), developed by a research team at the University of Geneva, is specifically designed for dynamic emotion recognition in music. It contains 1,802 musical pieces, including 1,744 45-second music clips and 58 full songs, covering genres such as rock, pop, electronic, country, and jazz. The songs are annotated with dynamic valence and arousal values over time, providing insights into the dynamic changes in musical emotion."}, {"title": "Evaluation Methods", "content": "Evaluating the quality of AI-generated music has always been a focus of researchers. Since the early days of computer-generated music, assessing the quality of these works has been a key issue. Below are the significant research achievements at different stages.\n6.1 Overview of Evaluation Methods\nIn terms of subjective evaluation, early research relied heavily on auditory judgments by human experts, a tradition dating back to the 1970s to 1990s. For example, (Loy and Abbott 1985)evaluated computer-generated music clips through listening tests. By the 2000s, subjective evaluation methods became more systematic. (Cuthbert and Ariza 2010) proposed a survey-based evaluation framework to study the emotional and aesthetic values of AI-generated music. With the advancement of deep learning technologies, the complexity of subjective evaluation further increased. (Papadopoulos, Roy, and Pachet 2016) and (Yang, Chou, and Yang 2017) introduced multidimensional emotional rating systems and evaluation models combining user experience, marking a milestone in subjective evaluation research. Recently, (Agarwal and Om 2021) proposed a multi-level evaluation framework based on emotion recognition, and (Chu et al. 2022) developed a user satisfaction measurement tool, which more accurately captures complex emotional responses and cultural relevance, making subjective evaluation methods more systematic and detailed.\nObjective evaluation dates back to the 1980s when the quality of computer-generated music was assessed mainly through a combination of audio analysis and music theory. Cope (Cope 1996)pioneered the use of music theory rules for structured evaluation. Subsequently, Huron (Huron 2008) introduced a statistical analysis-based model for evaluating musical complexity and innovation, quantifying structural and harmonic features of music, thus providing important tools for objective evaluation. With the advent of machine learning, Conklin (Conklin 2003) and Briot et al. (Briot, Hadjeres, and Pachet 2017) developed more sophisticated objective evaluation systems using probabilistic models and deep learning techniques to analyze musical innovation and emotional expression.\n6.2 Evaluation of Originality and Emotional Expression\nThe evaluation of originality became an important research direction in the 1990s. (Miranda 1995) and (Toiviainen and Eerola 2006) introduced early mechanisms for originality scoring through genetic algorithms and computational models. As AI technology advanced, researchers such as (Herremans, Chuan, and Chew 2017) combined Markov chains and style transfer techniques, further enhancing the systematic and diverse evaluation of originality. The evaluation of emotional expression began with audio signal processing. (Sloboda 1991) and (Picard 2000) laid the foundation for assessing emotional expression in music through the analysis of pitch, rhythm, and physiological signals. With the rise of multimodal analysis, (Kim et al. 2010) and (Yang and Chen 2012) developed emotion analysis models that combine audio and visual signals, significantly improving the accuracy and diversity of emotional expression evaluation.\n6.3 Implementation Strategies of Evaluation Frameworks\nThe implementation strategies of evaluation frameworks have evolved from simple to complex. The combined use of qualitative and quantitative analysis was first proposed by Reimer(Reimer 1991) in the field of music education and"}, {"title": "Application Areas", "content": "Al music generation technology has broad and diverse applications, from healthcare to the creative industries, gradually permeating various sectors and demonstrating immense potential. Based on its development history, the following is a detailed description of various application areas and the historical development of relevant research.\n7.1 Healthcare\nAl music generation technology has gained widespread attention in healthcare, particularly in emotional regulation and rehabilitation therapy. In the 1990s, music therapy was widely used to alleviate stress and anxiety. (Standley 1986) studied the effect of music on anxiety symptoms and highlighted the potential of music as a non-pharmacological treatment method. Although the focus was mainly on natural music at the time, (Sacks 2008), in his book Musicophilia, further explored the impact of music on the nervous system, indirectly pointing to the potential of customized music in neurological rehabilitation. With advancements in AI technology, generated music began to be applied in specific therapeutic scenarios. (Aalbers et al. 2017) demonstrated the positive impact of music therapy on emotional regulation and proposed personalized therapy through Al-generated music.\n7.2 Content Creation\nContent creation is one of the earliest fields where AI music generation technology was applied, evolving from experimental uses to mainstream creative tools. In the 1990s, David Cope's Experiments(Cope 1996) in Musical Intelligence (EMI) (1996) was an early attempt at using AI-generated music for content creation. EMI could simulate various compositional styles, and its generated music was used in experimental works. Although the technology was still relatively basic, this pioneering research laid the foundation for future applications. In the 2000s, AI-generated music began to be widely used in creative industries like film and advertising. Startups such as Jukedeck developed music generation platforms using Generative Adversarial Networks (GANs) and Recurrent Neural Networks (RNNs) to create customized background music for short videos and ads. Briot et al. found that Al-generated music had approached human-created music in quality and complexity, highlighting AI's potential to improve content creation efficiency(Briot, Hadjeres, and Pachet 2020). Recently, AI music generation technology has been applied even more widely in content creation. OpenAI's MuseNet(Payne 2019) and Google's Magenta project(Magenta Team 2023) demonstrated the ability to generate complex, multi-style music, providing highly context-appropriate background music for films, games, and advertisements.\n7.3 Education\nAl music generation technology has revolutionized music education, becoming an important tool for understanding music theory and practical composition. In the early 21st century, AI began to be applied in music education. Pachet explored the potential of automatic composition software in education, generating simple exercises to help students understand music structures and harmonies(Pachet 2003). These early systems aimed to assist rather than replace traditional teaching methods. As technology advanced, AI music generation systems became more intelligent and interactive. Platforms such as MusEDLab's AI Duet and Soundtrap's AI Music Tutor(MusedLab Team 2023) provide interactive educational experiences, listening to users' performances, interpreting inputs, and offering instant feedback or real-time performance to help improve skills and understand musical nuances.\n7.4 Social Media and Personalized Content\nAI-generated music significantly enriches user experiences in social media and personalized content, with personalized recommendations and automated content generation becoming key trends. In the 2000s, social platforms like MySpace first introduced simple music generation algorithms to create background music for user profiles. Although technically basic, these early attempts laid the groundwork for personalized content generation. As social media platforms diversified, personalized content generation became mainstream. Music streaming platforms like Spotify and Pandora use AI to generate personalized playlists by analyzing user listening history and preferences, providing highly customized music experiences. AI-generated music is also used on short video platforms to enhance content appeal. Recently, AI-generated music has become an essential part of social media, with platforms like TikTok using AI to"}, {"title": "Challenges and Future Directions", "content": "Despite significant progress in AI music generation technology, multiple challenges remain, providing rich avenues for future exploration. The current technological bottlenecks are primarily centered on the following key issues:\nFirstly, the diversity and originality of generated music remain major concerns for researchers. Early generative systems, such as David Cope's Experiments in Musical Intelligence (EMI)(Computer History Museum 2023), were successful at mimicking existing styles but often produced music that was stylistically similar and lacked innovation. This limitation in diversity has persisted in later deep learning models. Although the introduction of Generative Adversarial Networks (GANs) and Recurrent Neural Networks (RNNs) improved diversity, the results still often suffer from \"mode collapse\"\u2014where generated pieces across samples are too similar in style, lacking true originality. This phenomenon was extensively discussed in Briot et al., highlighting the potential limitations of deep learning models in music creation(Briot, Hadjeres, and Pachet 2020).\nSecondly, effectively capturing long-term dependencies and complex structures in music is a critical challenge in Al music generation(Briot, Hadjeres, and Pachet 2020). As a time-based art form, music's structure and emotional expression often rely on complex temporal spans and hierarchies(Hawthorne et al. 2018). Current AI models struggle with this complexity, and although some studies have attempted to address this by increasing the number of layers in the model or introducing new architectures (such as Transformer models), results show that models still find it difficult to generate music with deep structural coherence and long-term dependencies. The core issue is how to enable models to maintain overall macro coherence while showcasing rich details and diversity at the micro level during music generation.\nThe standardization of evaluation methods has also been a persistent challenge in assessing the quality of AI-generated music. Traditional evaluation methods mainly rely on subjective assessments by human listeners, but these methods often lack consistency and objectivity(Yang and Chen 2012). With the expanding applications of AI-generated music, the need for more objective and consistent evaluation standards has grown. Researchers have begun exploring quantitative evaluation methods based on statistical analysis and music theory(Herremans, Chew et al. 2016) however, effectively integrating these methods with subjective assessments remains an area needing further exploration(Engel et al. 2017). The refinement of such evaluation systems is crucial for advancing the practical applications of AI music generation technology.\nFacing these challenges, future research directions can focus on the following areas:\nExploring New Music Representations and Generation Methods: Introducing more flexible and diverse music representation forms can help generative models better capture the complexity and diversity of music. Research in this area can draw on recent findings in cognitive science and music theory to develop generation mechanisms that better reflect the human creative process.\nEnhancing Control Capabilities of Hybrid Models: By incorporating more contextual information (such as emotion tags or style markers), AI-generated music can achieve greater progress in personalization and diversity. The control capabilities of hybrid models directly affect the expressiveness and user experience of generated music, making this a critical direction for future research.\nApplying Interdisciplinary Approaches: Combining music theory, cognitive science, and deep learning will be key to advancing AI music generation. This approach can enhance the ability of generative models to capture com-"}, {"title": "Conclusion", "content": "This paper provides a comprehensive review of the key technologies, models, datasets, evaluation methods, and application scenarios in the field of AI music generation, offering a series of summaries and future directions based on the latest research findings. By reviewing and analyzing existing studies, this paper presents a new summarization framework that systematically categorizes and compares different technological approaches, including symbolic generation, audio generation, and hybrid models, thereby offering researchers a clear overview of the field. Through extensive research and analysis, this paper covers emerging topics such as multimodal datasets and emotional expression evaluation and reveals the potential impact of AI music generation across various application areas, including healthcare, education, and entertainment.\nHowever, despite significant advances in the diversity, originality, and standardization of evaluation methods, AI music generation technology still faces numerous challenges. In particular, capturing complex musical structures, handling long-term dependencies, and ensuring the innovation of generated music remain pressing issues. Future research should focus more on the diversity and quality of datasets, explore new generation methods, and promote interdisciplinary collaboration to overcome the current limitations of the technology.\nOverall, this paper provides a comprehensive knowledge framework for the field of AI music generation through systematic summaries and analyses, offering valuable references for future research directions and priorities. This not only contributes to the advancement of AI music generation technology but also lays the foundation for the intelligent and diverse development of music creation. As technology continues to evolve, the application prospects of AI in the music domain will become even broader. Future researchers can build upon this work to further expand the field, bringing more innovation and breakthroughs to music generation."}]}