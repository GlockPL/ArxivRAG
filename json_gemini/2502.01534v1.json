{"title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge", "authors": ["Dawei Li", "Renliang Sun", "Yue Huang", "Ming Zhong", "Bohan Jiang", "Jiawei Han", "Xiangliang Zhang", "Wei Wang", "Huan Liu"], "abstract": "Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive issue that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge. We release all codes and data at:\nhttps://github.com/David-Li0406/\nPreference-Leakage\u00b9.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in Large Language Models (LLMs) (Achiam et al., 2023; Jaech et al., 2024; Tong et al., 2024; Zhang et al., 2024a) have empowered various downstream tasks and applications. However, this also poses substantial challenges to the automatic evaluation of these models. Representatively, LLM-based AI agents' focus transfer from traditional natural language processing tasks (Yang et al., 2023; Zhang et al., 2023) to real-world (Liu et al., 2023b; Huang et al., 2023), open-ended response generation (Wu et al., 2024), which greatly limits the applicability of traditional n-gram matching methods (e.g., BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)) (Liu et al., 2016; Reiter, 2018) or model-based evaluators (Zhang et al., 2020; Zhong et al., 2022) for evaluation.\nTo address these challenges, the paradigm of LLM-as-a-judge (Zheng et al., 2023; Li et al., 2024a; Jiang et al., 2024a; Zhong et al., 2024; Li et al., 2025) has been proposed, designed to leverage LLM as evaluators to assess response quality. By combining powerful LLMs with well-designed prompting strategies, LLM-as-a-judge enables human-like evaluation of long-form and open-ended generation in a more cost-efficient and scalable manner. However, recent studies point out some weaknesses of such assessment. For instance, Ye et al. (2024) explores various biases and vulnerabilities of LLM-as-a-judge, highlighting the importance of developing a reliable and fair LLM-based evaluation system.\nIn this work, we aim to introduce another concern in LLM-as-a-Judge-Preference Leakage. This issue arises when the LLMs used for data generation and evaluation are closely related, as illustrated in Figure 1. Synthetic data generated by LLMs (Gan et al., 2023; Tan et al., 2024; Li et al., 2024b;c) has become a cornerstone of model training (Lee et al., 2025). When combined with LLM-as-a-Judge, they offer significant efficiency gains in model development. However, limited attention has been given to the potential contamination that occurs when the generator and evaluator LLMS share a close relationship. During our preliminary study, we find this issue is particularly pervasive in popular LLM-as-a-judge benchmarks (e.g., AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024e)) and LLM-relevant studies (more details can be found in Appendix A), due to the common reliance on the most advanced LLMs, such as GPT-4 (Achiam et al., 2023), for both data synthesis and evaluation to ensure the highest quality outputs. In our work, we reveal this relatedness-akin to the overlap between training data and evaluation sets in traditional data contamination\u2014would introduce a systematic bias of judge LLMs towards their related student models (i.e., the model distilled by the data generator which is related to the judge). Compared to other biases in LLM-as-a-Judge, such as length bias or egocentric bias (Ye et al., 2024; Panickssery et al., 2024), preference leakage is subtler and more challenging to detect, especially given that most LLMs do not disclose their training data.\nTo investigate and reveal the preference leakage problem, we first define three relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Each of these scenarios is commonly encountered in real-world applications. Then, we pose and answer three core research questions about preference leakage:\n\u2022 RQ1: Does preference leakage introduce systematic biases in LLM-based evaluation? To answer it, we conduct experiments with various LLM baselines in two widely recognized LLM-as-a-judge benchmarks, also introduce the preference leakage score to quantify the bias caused by preference leakage. The analysis results suggest an obvious bias of judging LLMs toward their related student models.\n\u2022 RQ2: What is the severity of preference leakage under various scenarios? We conduct experiments under various relatedness settings, tuning techniques, and data mixing strategies to address it, finding that preference leakage consistently affects judge LLMs. Moreover, the severity of preference leakage correlates with the degree of relatedness between the data generator and LLM judges, as well as the proportion of synthetic data.\n\u2022 RQ3: What are the underlying mechanisms causing preference leakage? For this question, we analyze LLMs' recognition capabilities on their related student models' generation as well as the distribution of bias across different question types and judgment dimensions. The analysis reveals that preference leakage is a subtle, hard-to-detect issue, particularly affecting subjective questions and judgment dimensions.\nTo summarize, our contributions in this work are as follows:\n\u2022 We introduce preference leakage, a contamination issue arising from the relatedness between the data generator and judge LLMs.\n\u2022 We conduct extensive experiments across various LLMs and benchmarks to study how and to what extent the potential bias brought by preference leakage influences judgment.\n\u2022 Our further analysis reveals that preference leakage is prevalent in diverse scenarios and difficult for judge LLMs to detect, providing valuable insights for future research on this challenging issue."}, {"title": "2. Related Work", "content": "LLM-as-a-Judge, introduced by Zheng et al. (2023), leverages LLMs to automatically evaluate responses and assign rewards. This approach has gained widespread adoption in areas such as model alignment (Zhang et al., 2024d) and benchmarking (Liu et al., 2023a; Zhang et al., 2024b; Gao et al., 2023; Zhong et al., 2024), driving significant progress in the field. Building on this concept, Zhuge et al. (2024) proposed Agent-as-a-Judge, where agentic systems are employed to evaluate other agentic systems. Additionally, Prometheus, a series of open-source LLMs tailored for LLM-as-a-Judge (Kim et al., 2023; 2024), addresses the prohibitive costs associated with proprietary models, further democratizing the technology.\nDespite its promising potential, recent studies have highlighted the vulnerabilities and limitations of LLM-as-a-Judge. Notable concerns include biases during evaluation. For example, Zheng et al. (2023) identify position bias, where LLMs may favor responses based on their order in the input, thereby compromising fairness. Other studies (Ye et al., 2024; Koo et al., 2023; Chen et al., 2024; Zheng et al., 2023; Huang et al., 2024) further emphasize the risks of evaluation biases. Thakur et al. (2024) assessed the judgment capabilities of LLM judges, finding that only the most advanced models align reasonably well with human evaluators. Moreover, a recent study (Shi et al., 2024) revealed the susceptibility of LLM-as-a-Judge to adversarial attacks, leading to incorrect judgments. In this paper, we explore another critical vulnerability of LLM-as-a-Judge-preference leakage-which poses additional risks to the reliability of this evaluation paradigm.\nThe possible overlap between training data and evaluation benchmarks has become a central issue, since LLMs are usually trained on extensive web corpora (Dodge et al., 2021). This phenomenon, known as data leakage, can artificially improve the performance of LLMs and undermine the reliability of the assessment (Deng et al., 2024a; Jiang et al., 2024b).\nSeveral researchers have proposed methods to detect and mitigate data contamination. Deng et al. (2024b) proposed a retrieval-based approach to assess the degree of overlap between pre-training text and benchmark data. Golchin & Surdeanu (2023) have developed \u201cguided instruction\" to flag contaminated instances. Dong et al. (2024b) proposed the CDD method to identify peaks in the output distribution to detect data contamination. Several studies analyze data leakage for specific LLMs (Balloccu et al., 2024) and report contamination such as cross-language contamination (Yao"}, {"title": "3. Preference Leakage", "content": "In this section, we first provide the formal definition of data contamination as the preliminary (Section 3.1). Based on the concept, we demonstrate how LLM-based data synthesis and evaluation can lead to the evolving preference leakage problem (Section 3.2).\nData leakage, also known as data contamination, refers to the inadvertent inclusion of information from the evaluation benchmarks into the training corpus thus creating an overlap between training and testing sets (Kaufman et al., 2012). This overlap would significantly influence the evaluation fairness by inflating the models' performance since the model has prior exposure to and memorized information that it's expected to generalize during testing (Elangovan et al., 2021).\nFormally, let $T$ represent the training corpus and $E$ be the evaluation set during test time. Data contamination occurs if:\n$T \\cap E \\neq \\emptyset$,\nwhere $\\cap$ denotes the intersection between the two sets. Such overlap violates the fundamental assumption that training and testing datasets should be disjoint to ensure an unbiased assessment of the model's generalization ability.\nWith the advent of LLMs, synthetic data generated by these models (Tan et al., 2024) has been widely adopted in various stages of model training, including pre-training, reinforcement learning with AI feedback (RLAIF) and supervised fine-tuning. Concurrently, the concept of LLM-as-a-judge has emerged, where LLMs are employed to automate the evaluation process. While these LLM-as-an-oracle approaches reduce human effort in data annotation, significantly enhancing the efficiency and scalability of model training and evaluation, they also blur the lines between models and data, introducing evolving challenges (Shumailov et al., 2024; Dai et al., 2024).\nIn this work, we examine the evolving contamination problem brought by LLM-as-a-oracle and formally propose the concept of preference leakage. This refers to a situation in which the LLMs used for synthetic data generation and evaluation are related. Formally, we define this as:\n$LL_{MG} \\cap LL_{MJ} \\neq \\emptyset$,\nwhere $LL_{MG}$ and $LLM_{J}$ denote the LLMs used for training data generation and evaluation. $\\cap$ represents the relatedness between the two (sets of) LLMs. This relatedness may involve:\n\u2022 Being the same model: the data generator and evaluator are the same model:\n$LL_{MG} = LL_{MJ}$.\n\u2022 Inheritance relationship: one model is trained on synthetic data generated by the other:\n$LL_{MG} = Inherit(LL_{MJ})$,\n$LL_{MJ} = Inherit(LL_{MG})$."}, {"title": "4. Main Experiment", "content": "Models. We choose three powerful LLMs as data generator/ judge models. They are GPT-40-2024-11-20 (Achiam et al., 2023), Gemini-1.5-flash (Team et al., 2024), and LLaMA-3.3-70B-Instruct-turbo (Dubey et al., 2024). For the student model, we choose Mistral-7B-v0.1 (Jiang et al., 2023) and Qwen-2.5-14B (Yang et al., 2024). To avoid potential preference leakage due to distilling data from other LLMs during the instruction-tuning process, we choose to use the -PRETRAINED version rather than the -INSTRUCT version of these student models.\nEvaluation Datasets. We choose two representative pairwise evaluation datasets, Arena-Hard (Li et al., 2024e) and AlpacaEval 2.0 (Dubois et al., 2024), to evaluate the trained student models. Arena-Hard includes 500 challenging questions in English. Additionally, the evaluation agreement between Arena-Hard and Chatbot Arena (Zheng et al., 2023)'s hard prompts achieved a 96.7% Spearman correlation, demonstrating the consistency of Arena-Hard with human preferences (Li et al., 2024e). AlpacaEval 2.0 is an improved evaluation method based on AlpacaEval (Li et al., 2023) and contains 805 questions. Compared to version 1.0, AlpacaEval 2.0 significantly reduces the effect of text length on the evaluation results.\nImplementation Details. In our main experiment, we examine the preference leakage introduced by using the same data generator and evaluator in supervised fine-tuning (SFT). We will discuss other relatedness and learning methods in Section 5. To obtain synthetic datasets, We first randomly sample 30,000 prompts from the Ultrafeedback dataset (Cui et al., 2024). The Ultrafeedback dataset includes instructions from several publicly available high-quality datasets such as TruthfulQA (Lin et al., 2022), FalseQA (Hu et al., 2023), and Evol-Instruct (Xu et al., 2023). For each data generator model, we provide these prompts for them to produce synthetic responses, resulting in three synthetic instruction datasets. We then use each dataset to supervised fine-tune the student model, obtaining three different versions for each baseline: Mistral/ Qwen-GPT-40, Mistral/ Qwen-Gemini-1.5 and Mistral/ Qwen-LLaMA-3.3. After that, we pair each two student models and obtain three model pairs. For each model pair, we perform the pairwise comparison using the three judge models respectively.\nMetrics & Annotation Based on our hypothesis, preference leakage would lead to bias of judge LLMs towards their related student models. Following this principle, we design the preference leakage score PLS(i, j) to measure the bias in model pair (i, j) caused by preference leakage:\n$PLS(i, j) = (WR(i,i)-AVG(i,j))/(AVG(i,j)) + (WR(j,j)-AVG(j,i))/(AVG(j,i)) / 2$,\n$AVG(i, j) = (WR(i, i) + WR(i, j))/2$,\nHere WR(i, j) represents the win-rate score from judge model i to student model j. Intuitively, a large preference leakage score indicates that the two judge models demonstrate strong bias toward their related student models, suggesting a significant preference leakage phenomenon.\nWhile our proposed preference leakage score quantifies the degree of preference leakage in each model pair, we also perform manual annotation to assess the preference leakage in each individual model. We randomly select 100 questions from AlpacaEval 2.0 and have three well-trained annotators perform pairwise comparisons independently for each response pair. After the annotation, the majority voting is applied to each response pair to get the final annotation results."}, {"title": "4.2. Main Results", "content": "In our main experiment, we aim to provide insights into RQ1.\nPreference leakage exists in most model pairs. The original judgment results from Arena-Hard and AlpacaEval 2.0, along with the calculated preference leakage scores, are shown in Figure 2, Figure 3, and Table 1. As the results demonstrate, in most model pairs (except Mistral-GPT-40 vsMistral-LLaMA-3.3 and Qwen-GPT-40 vs Qwen-LLaMA-3.3), the judge LLMs exhibit a strong preference toward their related student models, leading to large positive values in the preference leakage scores. This finding suggests that preference leakage, along with the resulting bias, is widespread in SFT when the data generator and evaluator are the same.\nEvaluators' bias towards certain LLMs can be inherited by its student models. From Figure 2 and Figure 3, we find an obvious preference of GPT-40 towards Mistral/ Qwen-LLaMA-3.3 and this leads to the low preference leakage score in the Mistral-GPT-40 vs Mistral-LLaMA-3.3 and Qwen-GPT-40 vs Qwen-LLaMA-3.3 pairs. To investigate the source of this preference, we examine whether the GPT-4 evaluator has a bias toward LLaMA series models. Using the MTBench (Zheng et al., 2023) dataset, which includes pairwise comparison judgments from both humans and GPT-4, we compare GPT-4's and human evaluators' judgments on LLaMA-2 vs other models (including Vicuna, Alpaca, GPT-3.5, and GPT-4, which are preferred by GPT-4 due to preference leakage or egocentric bias) and LLaMA-2 vs Claude. The results, shown in Figure 4, reveal a clear preference for LLaMA-2 by GPT-4. Consequently, we conclude that evaluators' bias can be inherited. In this case, GPT-4's bias toward LLaMA has been passed on to LLaMA's student models. This inheritance, combined with the opaque training data of LLMs, makes preference leakage a more complex and challenging problem.\nModel pairs with similar performance tend to have more obvious preference leakage. As shown in Table 1, we observe that the preference leakage scores for Mistral-GPT-40 vs Mistral-Gemini-1.5 and Qwen-GPT-40 vs Qwen-Gemini-1.5 (23.6% and 27.9% respectively) are consistently higher than that for Mistral-LLaMA-3.3 vs Mistral-Gemini-1.5 and Qwen-LLaMA-3.3 vs Qwen-Gemini-1.5 (16.4% and 21.9% respectively). We think that this is largely due to the more comparable performance between the student models of GPT-40 and Gemini-1.5. Intuitively, when the quality of the two responses is similar, the evaluator may rely more heavily on its inherent preferences to make a judgment, thereby exacerbating the preference leakage issue.\nLarger student models cause more bias from judge LLMs. Another observation from Table 1 is that the overall preference leakage score for Qwen-2.5-14B is higher than that for Mistral-7B. Drawing on insights from previous studies on data leakage, which suggest that larger and more powerful LLMs are more capable of memorizing extensive information and are thus more susceptible to data contamination (Bordt et al., 2024; Duan et al., 2024), we attribute this difference in preference leakage to the size and capabilities of the student LLMs. We assume that larger student models, due to their better performance and generalization abilities, are more capable of learning and memorizing the hidden preference pattern from the synthetic data, thus leading to a more serious preference leakage.\nDifferent data generator/ judge LLMs result in varying degrees of bias under preference leakage. While we have concluded that student model pairs with similar performance or more powerful student models tend to exhibit greater preference leakage, we also examine whether different data generator and judge LLMs contribute to varying degrees of preference leakage. Analyzing the manual annotation results presented in Table 5, we observe that Gemini-1.5 shows a strong bias toward its students, followed by GPT-40, with LLaMA-3.3 displaying the least bias. This variation in preference leakage may stem from differences in the level of leaked preference in the synthetic responses generated by the data generator LLMs. For instance, an LLM with a distinctive style or format in its responses offers more opportunities for student models to learn these characteristics, potentially leading to more pronounced preference leakage during evaluation. Future work could further quantify the extent of leaked preference for each data generator model."}, {"title": "5. Further Analysis", "content": "In this section, we conduct relatedness analysis, learning method analysis and data mixing analysis (Section 5.1 - 5.3) to answer RQ2. Due to the cost consideration, we conduct these analyses on Mistral-GPT-40 vs Mistral-Gemini-1.5. Moreover, we perform recognition analysis and category analysis to answer RQ3.\nWe demonstrate the impact of different relatedness conditions between the data generator and the judge LLM on the preference leakage problem, as shown in Table 2.\nPreference leakage under inheritance settings causes obvious bias of judges towards their related students. For the inheritance relationship, we consider the situation where the data generator is inherited from the judge model. We conducted the following two experiments: (1). we give the same instructions again as in the SFT stage (Inheritance w/ same ins.), or (2). we sample the same number of different instructions from the Ultrafeedback (Inherence w/ different ins.). Then, we let the fine-tuned Mistral model generate the answers and use these generated data to fine-tune a new Mistral student model. From the results, with the same instructions, the average preference leakage score is 19.3%. In comparison, the score with different instructions is 22.3%.\nFirstly, in an inheritance setting, data generators can inherit judges' preferences, which are then passed on to new student models, thereby compromising the fairness of their evaluation. Second, even when different instructions are used, judges' preferences leaked to data generators can still be transferred to the new student model through synthetic data, leading to a high preference leakage score.\nModels within the same series tend to cause more significant bias. For two models within the same family, we consider two settings: (1) Same series, where training data is generated by GPT-40 and Gemini-1.5-flash, and judged by GPT-4-turbo and Gemini-1.5-pro; (2) Different series, where training data is still generated by GPT-40 and Gemini-1.5-flash, but judged by GPT-3.5-turbo and Gemini-1.0-pro. In the same series setting, the average preference leakage score is 8.9%, indicating that despite using different models for data generation and judgment, their relatedness in terms of model family leads to some preference leakage. In contrast, the different series setting yields a significantly lower leakage score of 2.8%, likely due to differences in architecture, training data, and other factors, reducing the influence of model-related biases in evaluation.\nIn real-world applications, synthetic data from a single LLM is often mixed with manually-written data or other multi-source synthetic data to train student models. To mimic these scenarios and explore how much synthetic data could lead to preference leakage, we conduct a data mixing analysis. Specifically, we randomly sample 10%, 30%, 50%, and 70% from the original synthetic dataset and mix it with manually-written data and multi-source synthetic data, respectively, in order to maintain a consistent total volume of training data (30,000). For the manually-written data, we sample from the data pool collected in Section 5.2. For the multi-source synthetic data, we use the original synthetic data from Ultrafeedback, which includes responses generated by various LLMs (e.g., WizardLM, Flcon, etc.). After obtaining the mixing training data, we train the student models using SFT and calculate their preference leakage scores based on the judgment results.\nThe degree of preference leakage is directly proportional to the amount of synthetic data. We observe a strong correlation between the proportion of synthetic data in the mixture and the preference leakage score, with no clear threshold separating cases with preference leakage from those without. This suggests that preference leakage can occur even with a small amount of leaked synthetic data, posing significant challenges for its detection.\nPrevious studies demonstrate the LLM judges can recognize and thus prefer their own generation (Panickssery et al., 2024). In this work, we pose a similar question: Does preference leakage also source from the LLM judges' recognition"}, {"title": "5.5. Impact on Question Type & Judgment Dimension", "content": "In this section, we explore the impact of preference leakage across various question types and judgment dimensions. For the question type analysis, we first propose several general question types based on the question clusters introduced by Arena-Hard. Then, we prompt GPT-40 to map each question in Arena-Hard and AlpacaEval to one of the question types and calculate the preference leakage score for each question category. For the judgment dimension analysis, we follow the judgment dimensions introduced by Liu et al. (2023a) and also utilize GPT-40 to map the rationale generated by judge LLMs to one or multiple judgment dimensions.\nSubjective question and judgment dimension tend to lead to more bias. For question type analysis, we find objective questions with a definitive answer, like mathematical ones, demonstrate the least preference leakage. By contrast, subjective questions that have more than one standard answer, such as programming and writing, usually lead to a more obvious preference leakage. This observation is also applied to judgment dimension analysis, as objective dimensions (like completeness) have an overall lower leakage degree compared with subjective ones (like fairness). This suggests that preference leakage tends to be more significant in objective questions and dimensions, where the contaminated model is more likely to receive biased preference."}, {"title": "6. Conclusion", "content": "In this work, we formally highlight the preference leakage problem in LLM-as-a-judge systems. The results of our main experiment, measured using the proposed preference leakage score, reveal a clear bias in each judge toward its respective student model. We also observe that this bias is more pronounced in comparable model pairs and larger student models. Furthermore, we conduct additional analysis on various factors, including the relationship between the data generator and judge LLMs, model tuning techniques, and data mixing strategies. Our findings suggest that preference leakage can cause significant bias across diverse scenarios. Finally, through recognition and category analyses, we investigate the underlying mechanisms of preference leakage, demonstrating that it is a challenging and hard-to-detect issue, especially in subjective questions and judgment dimensions. In the future, we aim to explore methods for detecting, preventing, and mitigating this evolving challenge in LLM-as-a-judge systems."}, {"title": "Impact Statements", "content": "By revealing preference leakage, this work could help build more trustworthy and ethically grounded AI systems. The relatedness between data generators and evaluators can systematically bias evaluations, potentially compromising the fairness and reliability of the automatic evaluation paradigm. These biased evaluations may indirectly affect downstream tasks such as AI alignment and decision-making systems, leading to unintended ethical risks. To mitigate preference leakage, we hope that researchers will propose more reliable evaluation methods, diversify training data sources, and develop contamination-resistant benchmarks in the future."}]}