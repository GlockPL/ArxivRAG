{"title": "An Agile Adaptation Method for Multi-mode Vehicle Communication Networks", "authors": ["Shiwen He", "Kanghong Chen", "Shiyue Huang", "Wei Huang", "Zhenyu An"], "abstract": "This paper focuses on discovering the impact of communication mode allocation on communication efficiency in the vehicle communication networks. To be specific, Markov decision process and reinforcement learning are applied to establish an agile adaptation mechanism for multi-mode communication devices according to the driving scenarios and business requirements. Then, Q-learning is used to train the agile adaptation reinforcement learning model and output the trained model. By learning the best actions to take in different states to maximize the cumulative reward, and avoiding the problem of poor adaptation effect caused by inaccurate delay measurement in unstable communication scenarios. The experiments show that the proposed scheme can quickly adapt to dynamic vehicle networking environment, while achieving high concurrency and communication efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the continuous evolution of technological de-velopment and industrial transformation, the automotive industry is accelerating its development towards intelli-gence, networking, and high-end [1]. Cooperative ConnectedAutomated Driving (CCAD) achieves collaborative sensing,decision-making, and control of intelligent networked vehiclesthrough information exchange among vehicles, roadside units(RSUs), and the cloud, which enables them to advance fromsingle vehicle intelligence to group intelligence [2].\nThe real-time requirement of information exchanges inCCAD poses a significant challenge to wireless communication systems [3]. High-rate, reliable, bidirectional andintegrated intelligent network technology with multiple com-munication modes is important for the realization of intelligentvehicle-road cooperative system, and provides correspondingcommunication guarantee according to different levels ofrequirement. In the context of vehicle communication net-works (VCNs), the commonly used communication mode ismainly the device-to-device communication (proximity com-munication, PC5) which coexists with the cellular and Wi-Fi communication systems [4]. This implies that vehicles candirectly choose the RSUs or cellular networks to exchange theinformation. In fact, the same message in the VCNs may needto be simultaneously sent to multiple target nodes, includingother vehicles, RSUs, and non-vehicle nodes. Furthermore, thetransmission content and the received terminals depend on thecommunication environment, and the intent of driving [5]. Thiscommunication method expands the coverage area, preventsnode overload by dispersing the communication load, andenhances the reliability of data interaction and the adaptabilityof driving intentions in different communication environments.\nIn the current multi-mode communication systems, thedevices usually select one communication mode to performthe specific services, which cannot meet the actual applicationrequirements in the case with large number of transmissionservices [6]. To address this limitation, the authors of [7]proposed an unsupervised deep unrolling framework basedon projection gradient descent for solving constrained op-timization problems in wireless networks. The authors of[8] introduced a neural network-based communication selec-tion mechanism tailored for VCNs. More recently, multi-path transmission and multi-link transmission based on onecommunication protocol have attracted extensively attentionin the field of academia and industry.\nMulti-link transmission is commonly employed to ensurethe reliable transmission of data with low-latency requirements[9]. This technology uses multiple links of different frequen-cies that contain multiple base service sets to expand the avail-able bandwidth and further reduce latency [10]. Multi-pathtransmission typically involves the use of multiple multi-hoppaths, with the primary objective of balancing resource utiliza-tion among network nodes to alleviate congestion and decreasedelay [11]. However, when vehicles or RSUs encounter a surgein transmission tasks due to sudden changes in road conditions,the vehicles must communicate with multiple devices, makingit challenging to further reduce transmission delay throughparallel multi-link transmission. Multi-path transmission fo-cuses on ensuring data transmission reliability and congestioncontrol [12]. Concurrent multi-user transmissions not onlyrisk node congestion but also require frequent status inquiriesfrom other nodes during the transmission process. Moreover,the flexible scheduling of multiple communication modes, asan enhancement to multi-link and multi-path strategies, canfurther decrease latency and simplify the process of acquiringcongestion information from other nodes, thereby addressingthe challenges posed by the need for reliable and low-latencycommunication in VCNS.\nTherefore, in order to solve the above problems, this paper"}, {"title": "II. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "This paper considers a multi-mode VCNs, where the ve-hicles and RSUs have the ability to simultaneously supportmultiple communication modes (protocols), which usuallyprovide at least two communication mode in the (C-V2X) [4],i.e,\n\u2022 A short distance direct communication mode proximitycommunication between vehicles, pedestrians and RSUs.\n\u2022 Communication mode between terminals and base sta-tions, which can achieve reliable communication overlong distances and over a larger range.\nIn addition, the dedicated short range communications, Wi-Fi, long range radio are also commonly used communicationprotocols for vehicles, as illustrated in Fig. 1.\nFor the sake of convenience in description, let $V ={1,2,..., N}$ be the set of terminals in the multi-mode VCNs,where V denotes a collection of multi-mode communication\n$\\min\\limits_{\\{a(i,j,c)\\}} \\max\\limits_{j\\in V} \\sum\\limits_{c\\in C} a(i, j, c) \\frac{(F(i, c) + S(i, j, c)M(i, j))}{B(i, j, c)},$\n(1a)\ns.t. $a(i, j, c) \\in {0, 1}, \\forall j \\in V, \\forall c \\in C,$\n(1b)\n$\\sum\\limits_{c\\in C}a(i, j, c) = 1, \\forall j \\in V,$ \n(1c)\n$\\sum\\limits_{c\\in C}a(i, j, c) \\frac{(F(i, c) + S(i, j, c)M(i, j))}{B(i, j, c)} < D(i, j), \\forall j \\in V.$\n(1d)\nvehicle nodes, RSUs and base stations. Moreover this paper assumes that the multi-mode vehicle node i sends theinformation-bearing symbols to the other terminals denotedas $V = V\\{i\\}$.\nThis paper focuses on the decision-making policy of multi-mode node i, aiming to minimize the total transmissionlatency, as shown in problem (1). Specifically, in formula (1),$a(i, j, c)$ denotes the decision variable of multi-mode node ifor multi-mode node $j, j \\in V$ and $c\\in C$. Here, $a(i, j, c) = 1$indicates that the communication protocol c is selected fortransmitting data from node i to node $j, j \\in V$, whereas$a(i, j, c) = 0$ indicates otherwise. $B(i, j, c)$ is the bandwidthallocated by multi-mode node i for communication with multi-mode node j in mode c. $M(i, j)$ and $D(i, j)$ represent the datasize that multi-mode node i transmits to multi-mode node jand the maximum tolerable delay between node i and nodej respectively. Among them, the data size $M(i,j)$ can beunequal between node i and different nodes. $F(i, c)$ indicates the buffer data at node i for different communication modesc. Let $S(i, j,c)$ be the indicator of whether communicationprotocol c is supported between node i and node $j, i \\in V,j \\in V, c \\in C = \\{1,2,\u2026\u2026,C'\\}$ with $C'$ being the number ofcommunication protocols supporting by the multi-mode nodesin the multi-mode VCNs, i.e.,\n$S(i, j, c) \\in \\{0, 1\\}, \\forall i \\in V, j \\in V, \\forall c \\in C.$\n(2)\nIn equation (2), $S(i,j,c) = 1$ signifies that communicationprotocol c is supported between multi-mode node i and nodej; conversely, $S(i, j, c) = 0$ indicates the absence of supportfor protocol e between these two nodes. Unless otherwisespecified, every node within the multi-mode VCN is assumedto be a multi-mode node, meaning that there are more thanone communication protocols available (C' > 1) 1. Constraint(1a) means that only one communication mode can be selected"}, {"title": "III. REINFORCEMENT LEARNING MODEL FOR AGILE ADAPTATION PROBLEMS", "content": "To achieve low latency, this paper adopts Q-learning [13] totrain and optimize the vehicle communication decision model.In this section, the agile adaptation scheme is obtained byusing the Q-learning method. It is vital to first establish thethree fundamental elements of Q-learning: action space, statespace and reward function. Subsequently, a tailored modelnetwork is designed to cater to the specific requirements of theconsidered agile adaptation problem. In order to facilitate thedesign of the model, problem (1) is converted to problem (3)to obtain the minimum delay \u03c4, and constraint (3b) indicatesthat \u03c4 must be greater than the transmission delay betweenmulti-mode node i and any multi-mode node j in V.\n$\\min \\tau,$\n(3a)\ns.t. $\\sum\\limits_{c\\in C} a(i, j, c) \\frac{(F(i, c) + S(i, j, c) M(i, j))}{B(i, j, c)} < \\tau, \\forall j \\in V,$\n(3b)\n$\\sum\\limits_{c\\in C}a(i, j, c) = 1, \\forall j \\in V,$\n(3c)\n$a(i, j, c) \\in \\{0, 1\\}, \\forall j \\in V, \\forall c \\in C,$\n(3d)\n$\\sum\\limits_{c\\in C}a(i, j, c) \\frac{(F(i, c) + S(i, j, c)M(i, j))}{B(i, j, c)}  < D(i, j), \\forall j \\in V.$\n(3e)\nA. Basic Elements of Reinforcement Learning Model\nIn the agile adaptation problem (3), the communication net-work, comprised of multiple multi-mode nodes, is used as theenvironment of the AARLM, and the multi-mode nodes serveas agents in the AARLM to interact with the environment. Toensure the model's responsiveness to environmental shifts andoutput the optimal decision-making action according to thecurrent environment, the action space, state space and rewardfunction of the AARLM are described as follows:\nState space $S_i$ is the set of states $s_k$ that are observed bymulti-mode node i in the environment, where k is the indexof states. The status $s_k$ includes whether the transmission taskbetween multi-mode nodes is complete and the amount ofdata in the buffer for each communication mode. The taskis considered complete if it does not exceed D(i, j).\nAction space $A_i(s_k)$ is the set of actions that can betaken by multi-mode node i in state $s_k$. The action $a(i, j, c)$represents not only the decision of the multi-mode node i,but also the action. For ease of description, a will be used asshorthand for a(i, j, c).\nReward function $R_i(s_k, a)$ quantifies the reward associatedwith action $a \\in A_i(s_k)$ in state $s_k \\in S_i$. The reward function$R_i(s_k, a)$ is formulated as (4) for $\\forall j \\in V, \\forall c \\in C$.\n$R_i(s_k, a) = T_{max(s_k, a)} \u2013 T_{max(S_{k+1},a)},$\n(4)\nwhere $T_{max(s_k,a)}$ and $T_{max(S_{k+1},a)}$ are respectively themaximum delay before and after action $a(i, j, c)$ is taken ineach mode.\n$T_{max(S_k, a)} = \\max\\limits_{j\\in V} \\sum\\limits_{c\\in C} a(i, j, c) \\frac{(F(i, c) + S(i, j, c)M(i, j))}{B(i, j, c)},$\n(5)\nThe cumulative reward $R_i$ is calculated as\n$R_i = \\gamma\\overline{R_i(s_k, a)} + R_i(s_k,a)$\n(6)\nwhere \u03b3 is the discount factor, $\\overline{R_i(s_k, a)}$ represents the rewardthat multi-mode node i has already received.\n$\\pi(ask)$ is a mapping policy from the observed states to theactions that will be taken by an agent in those states. For easeof description, \u03c0in the following content stands for \u03c0(ask).\nB. Framework of proposed Reinforcement Learning Model\nThe framework of AARLM illustrated in Fig. 2 consists oftwo key steps: policy evaluation and policy optimization. Theformer module assesses the rewards associated with the chosenactions based on a given policy. The policy optimizationmodule identifies the optimal policy $\u03c0\u2217(ask)$ by tuning theselection of actions. To evaluate the policy \u03c0(ask), the multi-mode node i first selects the action a(i, j, c) according to theinitial policy \u03c0(ask) to make the state $s_k$ change, and thencalculates the state value. The state value function $V (s_k)$ isdefined as\n$V(s_k) = \\sum\\limits_{\\alpha\\in A_i(s_k)}\\pi(\\alpha|s_k)\\cdot$\n$[R_i(s_k, a) + \\gamma\\sum\\limits_{s_k\\in S_i} P(s_k|s_{k\u22121},a)V_i(s_k)]$\n(7)"}, {"title": "Algorithm 1 Policy Iteration of AARLM", "content": "Require: Initializes the state space $S_i$, state-value function$V(s_0)$, threshold for delay $D(i, j)$, the set of terminalsV, the optimal action set $A^\u03c0(c) = \u00d8$, the number of modeliterations $E_1$, the number of task moves $E_2$.\n1: // Initial assignment of transmission tasks\n2: Sort tasks in descending order by $M(i, j)$;\n3: Sort modes in descending order by $B(i, j, c)$;\n4: Calculate $T_{ave}$ according to (10);\n5: while\n6: Select the mode $c \u2208 C$ with the largest bandwidth, and$T_c < T_{ave}$;\n7: if $S(i, j, c) = 1$ then\n8: Select the multi-mode node j with the largest$M(i, j)$, then $a(i, j, c) = 1$ and $A^\u03c0(c) = A^\u03c0(c) \u222a a(i, j, c)$;\n9: end if\n10: until All the tasks have been arranged;\n11: Update Q table;\n12: // Policy Improvement\n13: for episode 1 to $E_1$ do\n14: Traverse the Q table and select the state $s_k$ with thelowest delay $T_{max}(s_k, a)$ as the initial state;\n15: for episode 1 to $E_2$ do\n16: Traverse $C$ and select the mode $C_{tm}$ with the maximize $T_c$;\n17: Iterate over all tasks in mode $C_{tm}$, selecting multi-mode node j with minimizes $\\frac{M(i,j)}{B(i,j,c)} (T_c - T_{ave})$;\n18: Traverse $C$ and select the mode $C_{rm}$ with the minimizes $T_c$ and $S(i, j, C_{rm}) = 1$;\n19: $a(i, j, C_{tm}) = 0, a(i, j, C_{rm}) = 1$;\n20: calculate $R_i(s_k, a)$ according to (4);\n21: if $R_i(s_k, a) > 0$ then\n22: Update Q table and $A^\u03c0(c)$;\n23: end if\n24: end for\n25: end for\n26: return min($T_c$), $A^\u03c0(c)$\nwhere $P(s_k \\vert s_{k\u22121}, a)$ represents the probability that multi-mode node i moves to state $s_k$ after selecting action $a(i, j, c)$in state $s_k$, and $V_i(s_k)$ is one of the state values of next state$S_k$. In order to evaluate the policy \u03c0(a|sk), it is necessary toselect actions based on the \u03c0(ask) and update the state untilthe termination state is reached.\nThrough policy evaluation, the value of each state can bedetermined, thereby providing insight into the quality of eachstate. However, the existing policy might not be optimal. Tofurther enhance the policy, it's crucial to assess the impactof actions that deviate from the policy in each state. So thefunction of the state-action value $Q(s_k, a)$ is designed asfollows:\n$Q^\u03c0(s_k, a) = R_i(s_k,a) +$\n$\\gamma\\sum\\limits_{S_k \\in S_i} P(S_k \\vert S_{k\u22121}, a) \\sum\\limits_{\\alpha \\in A_i(s_k)}\\pi(\\alpha|s_k)Q^\u03c0(S_k, a).$\n(8)\nThe state-action value $Q^\u03c0$, defined as the long-term re-ward, represents the expected cumulative discounted reward$\\sum\\limits_{s_k\\in S_i} P(s_k \\vert s_{k\u22121}, a) \\sum\\limits_{\\alpha \\in A_i(s_k)}\\pi(\\alpha|s_k)Q^\u03c0(s_k, a)$ for theaction a(i, j, c) that is taken by multi-mode node i in thestate $s_k$ under policy \u03c0. The objective of the AARLM is toselect the optimal policy $\u03c0\u2217(a|sk)$ for every state sk aimingto maximize $Q^\u03c0(s_k, \u03b1)$. The optimal policy for each state isidentified through equation (9).\n$\u03c0\u2217(ask) = arg \\max\\limits_{\\alpha \\in A_i(s_k)} Q^\u03c0(s_k, \u03b1)$\n(9)\nAlgorithm 1 outlines the procedure for policy update. Theinitial segment, lines 1 through 11, updates the selectedmode based on the initial policy \u03c0(ask). Specifically, line6 identifies the mode c subject to the maximum bandwidthcondition $T_c < T_{ave}$, $T_{ave}$ is defined as\n$T_{ave} = \\sum\\limits_{j \\in V} M (i,j) / \\sum\\limits_{j \\in V} B(i, j, c)$\n(10)\nFollowing that, on line 8, all tasks that support mode care selected, sorted by data volume from largest to smallest,and transmitted using mode c until $T_c > T_{ave}$. This iterationprocess repeats until all tasks have completed mode selection,yielding the initial task assignment plan and the Q-table.\nLines 13 to 25 of Algorithm 1 optimize the mode selectionpolicy to $\u03c0\u2217(ask)$. Line 14 starts by locating the state skwith the lowest delay from the Q-table as the starting state.Subsequently, line 15 identifies the mode Ctm with the highestdelay in state sk. and selects the task with a size closest to$T_c-T_{ave}$ from those transmitted in mode Ctm. Line 17 selectsmode Crm, which is supported by the task and has the smallestdelay. The task is then moved to mode Crm in line 19, andthe reward $R_i(s_k, a)$ is calculated in line 20. If $R_i(s_k, a)$ ispositive, the Q-table and $A^\u03c0(c)$ are updated until the iterationcount reaches $E_1$ and the task move count reaches $E_2$"}, {"title": "IV. NUMERICAL RESULTS", "content": "In this paper, the effectiveness of AARLM is verified bysimulation. The configuration of simulation parameters suchas the number of multi-mode nodes and the size of transmitteddata are shown in Table I. The effects of the number of multi-mode nodes and the number of modes on the overall delayas well as task completion rate are analyzed respectively, andthe existing mode adaptation schemes (random selection) andtraditional algorithms (annealing algorithms) are compared.\nachieving the lowest latency. When the bandwidth of theadditional modes is 40 MHz or 80 MHz, the reduction inlatency is most pronounced initially but then experiencesa deceleration. The random algorithm exhibits the highestlatency among the evaluated schemes. The delay of AARLMis slightly lower than that of annealing algorithm, and with theincrease of communication mode, AARLM has maintained acertain delay advantage over annealing algorithm.\nIn the case of 5 communication modes, Fig. 4 illustratesthe trend in delay changes for various algorithms as thenumber of multi-mode nodes increases. All algorithms exhibita general upward trend in delay, and is significantly impactedby small-bandwidth communication modes, leading to thesteepest delay increase. Besides, the time delay experiencesminor fluctuations due to the inherent randomness in taskassignment. As the task volume grows, the complexity of thetask allocation scheme intensifies, leading to a widening delaygap between the annealing algorithm and AARLM.\nFig. 5 illustrates the general increase trend in delay changesfor various algorithms as the number of multi-mode nodesincreases. The random algorithm, in particular, is significantlyimpacted by small-bandwidth communication modes, leadingto the fastest increase in delay. Initially, both the annealingalgorithm and AARLM demonstrate satisfactory task comple-tion rates. Nevertheless, when the volume of tasks becomesconsiderable and exceeds the maximum delay threshold, thetask completion rates for both the annealing algorithm andAARLM are observed to decrease sharply. Despite this,AARLM manages to retain a certain degree of superiority.\nFig. 6 shows the variation in task completion rate as thenumber of communication modes increases, with the taskcount remains constant at 100. Across all algorithms, thetask completion rate rose with more modes, a trend thatis accentuated by the higher bandwidth associated with theadditional modes, leading to a more rapid enhancement intask completion rates. Notably, AARLM stands out not onlyin maintaining the highest task completion rate but also inexhibiting the most rapid increase in this crucial metric."}, {"title": "V. CONCLUSION", "content": "This paper discussed an RL-based agile adaptation schemefor multi-mode communication in vehicle networks. The pre-liminary findings have been compelling, revealing that the pro-posed method has significantly reduced communication delayacross all scenarios, outperforming existing schemes. This hasproven that the RL-based agile adaptation scheme for multi-mode communication can effectively optimize the communi-cation process and reduce communication delay. Future workwill focus on optimizing the framework and incorporatinginterference factors as well as reliability indicators into multi-mode communication schemes."}]}