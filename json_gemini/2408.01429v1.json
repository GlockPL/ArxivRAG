{"title": "An Agile Adaptation Method for Multi-mode Vehicle Communication Networks", "authors": ["Shiwen He", "Kanghong Chen", "Shiyue Huang", "Wei Huang", "Zhenyu An"], "abstract": "This paper focuses on discovering the impact of communication mode allocation on communication efficiency in the vehicle communication networks. To be specific, Markov decision process and reinforcement learning are applied to establish an agile adaptation mechanism for multi-mode communication devices according to the driving scenarios and business requirements. Then, Q-learning is used to train the agile adaptation reinforcement learning model and output the trained model. By learning the best actions to take in different states to maximize the cumulative reward, and avoiding the problem of poor adaptation effect caused by inaccurate delay measurement in unstable communication scenarios. The experiments show that the proposed scheme can quickly adapt to dynamic vehicle networking environment, while achieving high concurrency and communication efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the continuous evolution of technological development and industrial transformation, the automotive industry is accelerating its development towards intelligence, networking, and high-end [1]. Cooperative Connected Automated Driving (CCAD) achieves collaborative sensing, decision-making, and control of intelligent networked vehicles through information exchange among vehicles, roadside units (RSUs), and the cloud, which enables them to advance from single vehicle intelligence to group intelligence [2].\n\nThe real-time requirement of information exchanges in CCAD poses a significant challenge to wireless communication systems [3]. High-rate, reliable, bidirectional and integrated intelligent network technology with multiple communication modes is important for the realization of intelligent vehicle-road cooperative system, and provides corresponding communication guarantee according to different levels of requirement. In the context of vehicle communication networks (VCNs), the commonly used communication mode is mainly the device-to-device communication (proximity communication, PC5) which coexists with the cellular and Wi-Fi communication systems [4]. This implies that vehicles can directly choose the RSUs or cellular networks to exchange the information. In fact, the same message in the VCNs may need to be simultaneously sent to multiple target nodes, including other vehicles, RSUs, and non-vehicle nodes. Furthermore, the transmission content and the received terminals depend on the communication environment, and the intent of driving [5]. This communication method expands the coverage area, prevents node overload by dispersing the communication load, and enhances the reliability of data interaction and the adaptability of driving intentions in different communication environments.\n\nIn the current multi-mode communication systems, the devices usually select one communication mode to perform the specific services, which cannot meet the actual application requirements in the case with large number of transmission services [6]. To address this limitation, the authors of [7] proposed an unsupervised deep unrolling framework based on projection gradient descent for solving constrained optimization problems in wireless networks. The authors of [8] introduced a neural network-based communication selection mechanism tailored for VCNs. More recently, multi-path transmission and multi-link transmission based on one communication protocol have attracted extensively attention in the field of academia and industry.\n\nMulti-link transmission is commonly employed to ensure the reliable transmission of data with low-latency requirements [9]. This technology uses multiple links of different frequencies that contain multiple base service sets to expand the available bandwidth and further reduce latency [10]. Multi-path transmission typically involves the use of multiple multi-hop paths, with the primary objective of balancing resource utilization among network nodes to alleviate congestion and decrease delay [11]. However, when vehicles or RSUs encounter a surge in transmission tasks due to sudden changes in road conditions, the vehicles must communicate with multiple devices, making it challenging to further reduce transmission delay through parallel multi-link transmission. Multi-path transmission focuses on ensuring data transmission reliability and congestion control [12]. Concurrent multi-user transmissions not only risk node congestion but also require frequent status inquiries from other nodes during the transmission process. Moreover, the flexible scheduling of multiple communication modes, as an enhancement to multi-link and multi-path strategies, can further decrease latency and simplify the process of acquiring congestion information from other nodes, thereby addressing the challenges posed by the need for reliable and low-latency communication in VCNS.\n\nTherefore, in order to solve the above problems, this paper"}, {"title": "II. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "This paper considers a multi-mode VCNs, where the vehicles and RSUs have the ability to simultaneously support multiple communication modes (protocols), which usually provide at least two communication mode in the (C-V2X) [4], i.e,\n\n\u2022 A short distance direct communication mode proximity communication between vehicles, pedestrians and RSUs.\n\n\u2022 Communication mode between terminals and base stations, which can achieve reliable communication over long distances and over a larger range.\n\nIn addition, the dedicated short range communications, Wi-Fi, long range radio are also commonly used communication protocols for vehicles, as illustrated in Fig. 1.\n\nFor the sake of convenience in description, let $V = {1,2,..., N}$ be the set of terminals in the multi-mode VCNs, where V denotes a collection of multi-mode communication\n\n$\n\\min_{\\{a(i,j,c)\\}} \\max_{j \\in V} \\sum_{c \\in C} a(i, j, c) \\frac{(F(i, c) + S(i, j, c)M(i, j))}{B(i, j, c)}, \n$\n(1a)\n\ns.t. $\\sum_{c \\in C} a(i, j, c) = 1, \\forall j \\in \\tilde{V},$\n(1b)\n\n$a(i, j, c) \\in \\{0, 1\\}, \\forall j \\in V, \\forall c \\in C,$\n(1c)\n\n$\\sum_{c \\in C} \\frac{a(i, j, c) (F(i, c) + S(i, j, c)M(i, j))}{B(i, j, c)} < D(i, j), \\forall j \\in V.$\n(1d)\n\nvehicle nodes, RSUs and base stations. Moreover this paper assumes that the multi-mode vehicle node i sends the information-bearing symbols to the other terminals denoted as $\\tilde{V} = V\\{i\\}$.\n\nThis paper focuses on the decision-making policy of multi-mode node i, aiming to minimize the total transmission latency, as shown in problem (1). Specifically, in formula (1), a(i, j, c) denotes the decision variable of multi-mode node i for multi-mode node j, $j \\in V$ and $c \\in C$. Here, a(i, j, c) = 1 indicates that the communication protocol c is selected for transmitting data from node i to node j, $j \\in V$, whereas a(i, j, c) = 0 indicates otherwise. B(i, j, c) is the bandwidth allocated by multi-mode node i for communication with multi-mode node j in mode c. M(i, j) and D(i, j) represent the data size that multi-mode node i transmits to multi-mode node j and the maximum tolerable delay between node i and node j respectively. Among them, the data size M(i,j) can be unequal between node i and different nodes. F(i, c) indicates the buffer data at node i for different communication modes c. Let S(i, j,c) be the indicator of whether communication protocol c is supported between node i and node j, $i \\in V$, $j \\in V$, $c \\in C = \\{1,2,\\ldots,C'\\}$ with C being the number of communication protocols supporting by the multi-mode nodes in the multi-mode VCNs, i.e.,\n\n$S(i, j, c) \\in \\{0, 1\\}, \\forall i \\in V, j \\in V, \\forall c \\in C$.\n(2)\n\nIn equation (2), S(i,j,c) = 1 signifies that communication protocol c is supported between multi-mode node i and node j; conversely, S(i, j, c) = 0 indicates the absence of support for protocol e between these two nodes. Unless otherwise specified, every node within the multi-mode VCN is assumed to be a multi-mode node, meaning that there are more than one communication protocols available (C' > 1) 1. Constraint (1a) means that only one communication mode can be selected"}, {"title": "III. REINFORCEMENT LEARNING MODEL FOR AGILE ADAPTATION PROBLEMS", "content": "To achieve low latency, this paper adopts Q-learning [13] to train and optimize the vehicle communication decision model. In this section, the agile adaptation scheme is obtained by using the Q-learning method. It is vital to first establish the three fundamental elements of Q-learning: action space, state space and reward function. Subsequently, a tailored model network is designed to cater to the specific requirements of the considered agile adaptation problem. In order to facilitate the design of the model, problem (1) is converted to problem (3) to obtain the minimum delay $\\tau$, and constraint (3b) indicates that $\\tau$ must be greater than the transmission delay between multi-mode node i and any multi-mode node j in V.\n\n$\\min \\tau,$\n(3a)\n\ns.t. $\\sum_{c \\in C} \\frac{a(i, j, c) (F(i, c) + S(i, j, c)M(i, j))}{B(i, j, c)} < \\tau, \\forall j \\in \\tilde{V},$\n(3b)\n\n$\\sum_{c \\in C} a(i, j, c) = 1, \\forall j \\in \\tilde{V},$\n(3c)\n\n$a(i, j, c) \\in \\{0, 1\\}, \\forall j \\in V, \\forall c \\in C,$\n(3d)\n\n$\\sum_{c \\in C} \\frac{a(i, j, c) (F(i, c) + S(i, j, c)M(i, j))}{B(i, j, c)} < D(i, j), \\forall j \\in V.$\n(3e)", "sections": [{"title": "A. Basic Elements of Reinforcement Learning Model", "content": "In the agile adaptation problem (3), the communication network, comprised of multiple multi-mode nodes, is used as the environment of the AARLM, and the multi-mode nodes serve as agents in the AARLM to interact with the environment. To ensure the model's responsiveness to environmental shifts and output the optimal decision-making action according to the current environment, the action space, state space and reward function of the AARLM are described as follows:\n\nState space Si is the set of states $s_k$ that are observed by multi-mode node i in the environment, where k is the index of states. The status $s_k$ includes whether the transmission task between multi-mode nodes is complete and the amount of data in the buffer for each communication mode. The task is considered complete if it does not exceed D(i, j).\n\nAction space Ai($s_k$) is the set of actions that can be taken by multi-mode node i in state $s_k$. The action a(i, j, c) represents not only the decision of the multi-mode node i, but also the action. For ease of description, a will be used as shorthand for a(i, j, c).\n\nReward function $R_i(s_k, a)$ quantifies the reward associated with action a $\\in$ Ai($s_k$) in state $s_k \\in$ Si. The reward function $R_i(s_k, a)$ is formulated as (4) for $\\forall j \\in V, \\forall c \\in C$.\n\n$R_i(s_k, a) = T_{max}(s_k, a) - T_{max}(s_{k+1}, a)$\n(4)\n\nwhere $T_{max}(s_k, a)$ and $T_{max}(s_{k+1}, a)$ are respectively the maximum delay before and after action a(i, j, c) is taken in each mode.\n\n$T_{max}(s_k, a) = max_{j \\in V} \\sum_{c \\in C} \\frac{a(i, j, c) (F(i, c) + S(i, j, c)M(i, j))}{B(i, j, c)}$\n(5)\n\nThe cumulative reward $R_i$ is calculated as\n\n$R_i = \\gamma \\tilde{R}_i(s_k, a) + R_i(s_k, a)$\n(6)\n\nwhere $\\gamma$ is the discount factor, $\\tilde{R}_i(s_k, a)$ represents the reward that multi-mode node i has already received.\n\n$\\pi(a|s_k)$ is a mapping policy from the observed states to the actions that will be taken by an agent in those states. For ease of description, $\\pi$ in the following content stands for $\\pi(a|s_k)$."}, {"title": "B. Framework of proposed Reinforcement Learning Model", "content": "The framework of AARLM illustrated in Fig. 2 consists of two key steps: policy evaluation and policy optimization. The former module assesses the rewards associated with the chosen actions based on a given policy. The policy optimization module identifies the optimal policy $\\pi*(a|s_k)$ by tuning the selection of actions. To evaluate the policy $\\pi(a|s_k)$, the multi-mode node i first selects the action a(i, j, c) according to the initial policy $\\pi(a|s_k)$ to make the state $s_k$ change, and then calculates the state value. The state value function V ($s_k$) is defined as\n\n$V(s_k) = \\sum_{a \\in A_i(s_k)} \\pi(a|s_k) \\cdot [R_i(s_k, a) + \\gamma \\sum_{s_k \\in S_i} P(s_k|s_{k-1}, a)V_i(s_k)]$\n(7)"}]}, {"title": "Algorithm 1 Policy Iteration of AARLM", "content": "ward, represents the expected cumulative discounted reward $\\sum_{s_k \\in S_i} P(s_k|s_{k-1}, a) \\sum_{a \\in A_i(s_k)} \\pi(a|s_k)Q_{\\pi}(s_k, a)$ for the action a(i, j, c) that is taken by multi-mode node i in the state $s_k$ under policy $\\pi$. The objective of the AARLM is to select the optimal policy $\\pi*(a|s_k)$ for every state $s_k$ aiming to maximize $Q_{\\pi}(s_k, a)$. The optimal policy for each state is identified through equation (9).\n\n$\\pi*(a|s_k) = arg \\max_{a \\in A_i(s_k)} Q_{\\pi}(s_k, a)$\n(9)\n\nAlgorithm 1 outlines the procedure for policy update. The initial segment, lines 1 through 11, updates the selected mode based on the initial policy $\\pi(a|s_k)$. Specifically, line 6 identifies the mode c subject to the maximum bandwidth condition $T_c < T_{ave}$, $T_{ave}$ is defined as\n\n$T_{ave} = \\sum_{j \\in V} M(i, j) / (i, j)$\n(10)\n\nFollowing that, on line 8, all tasks that support mode c are selected, sorted by data volume from largest to smallest, and transmitted using mode c until $T_c > T_{ave}$. This iteration process repeats until all tasks have completed mode selection, yielding the initial task assignment plan and the Q-table.\n\nLines 13 to 25 of Algorithm 1 optimize the mode selection policy to $\\pi*(a|s_k)$. Line 14 starts by locating the state $s_k$ with the lowest delay from the Q-table as the starting state. Subsequently, line 15 identifies the mode $C_{tm}$ with the highest delay in state $s_k$. and selects the task with a size closest to $T_c-T_{ave}$ from those transmitted in mode $C_{tm}$. Line 17 selects mode $C_{rm}$, which is supported by the task and has the smallest delay. The task is then moved to mode $C_{rm}$ in line 19, and the reward $R_i(s_k, a)$ is calculated in line 20. If $R_i(s_k, a)$ is positive, the Q-table and A (c) are updated until the iteration count reaches $E_1$ and the task move count reaches $E_2$."}, {"title": "IV. NUMERICAL RESULTS", "content": "In this paper, the effectiveness of AARLM is verified by simulation. The configuration of simulation parameters such as the number of multi-mode nodes and the size of transmitted data are shown in Table I. The effects of the number of multi-mode nodes and the number of modes on the overall delay as well as task completion rate are analyzed respectively, and the existing mode adaptation schemes (random selection) and traditional algorithms (annealing algorithms) are compared."}, {"title": "V. CONCLUSION", "content": "This paper discussed an RL-based agile adaptation scheme for multi-mode communication in vehicle networks. The preliminary findings have been compelling, revealing that the proposed method has significantly reduced communication delay across all scenarios, outperforming existing schemes. This has proven that the RL-based agile adaptation scheme for multi-mode communication can effectively optimize the communication process and reduce communication delay. Future work will focus on optimizing the framework and incorporating interference factors as well as reliability indicators into multi-mode communication schemes."}]}