{"title": "Software Performance Engineering for Foundation Model-Powered Software (FMware)", "authors": ["Haoxiang Zhang", "Shi Chang", "Arthur Leung", "Kishanthan Thangarajah", "Boyuan Chen", "Hanan Lutfiyya", "Ahmed E. Hassan"], "abstract": "The rise of Foundation Models (FMs) like Large Language Models (LLMs) is revolutionizing software development. Despite the impressive prototypes, transforming FMware into production-ready products demands complex engineering across various domains. A critical but overlooked aspect is performance engineering, which aims at ensuring FMware meets performance goals such as throughput and latency to avoid user dissatisfaction and financial loss. Often, performance considerations are an afterthought, leading to costly optimization efforts post-deployment. FMware's high computational resource demands highlight the need for efficient hardware use. Continuous performance engineering is essential to prevent degradation. This paper highlights the significance of Software Performance Engineering (SPE) in FMware, identifying four key challenges: cognitive architecture design, communication protocols, tuning and optimization, and deployment. These challenges are based on literature surveys and experiences from developing an in-house FMware system. We discuss problems, current practices, and innovative paths for the software engineering community.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid emergence of Foundation Models (FMs), particularly Large Language Models (LLMs), is reshaping software development, with market value expected to reach $36.1 billion by 2030 [1]. FMs empower the creation of intelligent software, defined as FMware by Hassan et al. [2], where applications rely on one or more building blocks that are FMs. Many cool demos built with FMware have emerged recently [3], [4]. However, developing FMware from prototypes into production-ready products is a complex engineering pro- cess, requiring collaborations across AI, software engineering, systems, and hardware domains throughout the lifetime of such software [5], [6]. Performance engineering, one of the key aspects in such an engineering process, has not been thoroughly discussed. That is, how to proactively ensure that the developed FMware meets the pre-defined performance goals, e.g., throughput or latency. These goals are sometimes also referred to as Service Level Agreements (SLAs) or Service Level Objectives (SLOs). Failing to meet these goals will result in unsatisfactory user experiences. However, in practice, we observed that performance con- cerns are often considered afterthoughts during the lifecycle of FMware, causing inefficient and costly performance opti- mization efforts after the FMware is deployed in production when SLAs are not met. In addition, due to the intensive computation resources that are needed for deploying FMware, it can become prohibitively expensive to serve FMware re- quests. Efforts to improve the overall efficiency of hardware utilization are needed to avoid the wastage of scarce com- puting resources, such as costly GPUs sitting idle. Lastly, as FMware is live software that keeps evolving autonomously, it is necessary to apply continuous performance tuning practices to avoid performance degradation over time. To summarize, Software Performance Engineering (SPE) practices are crucial in bringing FMware from prototype to production. Although the awareness of performance-oriented FMware production is growing [5], [6], systematic studies focusing on SPE for FMware (SPE4FMware) are still lacking. In this paper, we present a comprehensive analysis of SPE challenges in FMware development, deriving from four authoritative sources: (i) an extensive survey of both academic and grey literature, (ii) in-depth discussions with industrial stakeholders and active academicians during SEMLA 2023 & 2024 [7], FM+SE Vision 2030 [8], FM+SE Summit 2024 [9], and SE 2030 workshop - FSE 2024 [10] events, (iii) close collaboration with our customers and our internal FMware application development teams to understand their pain points with performance issues, and (iv) our hands-on experience de- signing and implementing an in-house FMware serving system (FMware Runtime). We identify four key SPE challenges that span across the lifecycle of FMware development: the design of cognitive architectures, defining communication pro- tocols, tuning and optimization approaches, and deployment options. For each challenge, we describe its aspects in detail, discuss state-of-practices, and share our vision of innovation paths that call for contributions from the software engineering research community. This paper is organized as follows: Section II outlines the background of our study. Section III delves into the SPE challenges that are associated with FMware. Section IV describes the vision of our serving system. Finally, Section V summarizes our insights and conclusions."}, {"title": "II. BACKGROUND", "content": "In this section, we first review SPE research for traditional software (Section II-A). Then we explain the inference process of FM (Section II-B). We also provide an overview of SPE for FM (Section II-C). At last, we present the background of FMware (Section II-D).", "subsections": [{"title": "A. Software Performance Engineering (SPE)", "content": "SPE involves modeling and analyzing software systems to understand performance characteristics and uncover optimiza- tion opportunities [11]. SPE encompasses various engineering practices aimed at meeting performance requirements such as latency, throughput, and resource utilization. As software complexity escalates, addressing performance issues as af- terthoughts becomes increasingly challenging and costly [12]. Hence, proactively applying SPE practices and embedding them throughout the development lifecycle is advantageous. Traditionally, most software components are considered deterministic, allowing developers to recreate issues when diagnosing performance degradation. However, as software systems evolve with increasingly complex interactions, non- deterministic behaviours have emerged [13]. For example, in real-time embedded systems, interrupt-driven interactions with environments introduce unpredictability, as interrupts occur randomly and are handled by priority levels, making overall system behaviour non-deterministic. This non-deterministic behaviour complicates performance engineering, making it dif- ficult to reproduce performance issues that occur in production. This challenge is amplified with FMs due to the probability- based token sampling process in FM inference. Consequently, the rise of FMware necessitates rethinking methods to accu- rately predict and optimize system performance."}, {"title": "B. Foundation Models (FMS) & Inference Process", "content": "FMs have transformed software by providing unparalleled abilities in comprehending and generating diverse data types. Trained on extensive unlabeled datasets, these models exhibit extraordinary versatility across various tasks, ranging from natural language processing to image generation [14]. Particu- larly notable are Large Language Models (LLMs), recognized for their sophisticated capabilities in text generation, lan- guage comprehension, and multilingual processing. A notable FM architecture is the Generative Pre-trained Transformer (GPT), which employs a decoder-only model architecture, excelling in language understanding and generation. As the number of parameters scales into the billions, the model's capabilities expand to handle general tasks due to emergent behaviours [15]. A prime example is the renowned GPT-4 model by OpenAI [16]. The inference process of an FM comprises two phases: prefill and decode [17]. In the prefill phase, the user-provided input, or prompt, is fed into the model, initiating the first forward pass to generate the initial output token. This phase is computation-intensive, involving substantial parallel matrix multiplication operations. During the decode phase, models sequentially generate tokens iteratively, where each new token is created based on all previously generated tokens. This token- by-token generation process requires storing the previously computed tokens' keys and values (known as KV cache) [18] to speed up inference by avoiding redundant computations. The decode phase is memory-bound and cannot be fully parallelized due to data dependency and the sequential nature of token generation."}, {"title": "C. SPE for FM Inference", "content": "The FM inference process exhibits two notable character- istics that impact its performance [19]. First, the queries sent to FM show a diverse length range due to workload hetero- geneity. The same task can be articulated through concise instructions or elaborate descriptions, leading to variations in both first token generation latency and KV cache memory consumption. Second, the generated tokens from FM show a diverse length range due to execution unpredictability, re- sulting in inference completion latency ranging from seconds to minutes and memory consumption varying from megabytes to gigabytes. These characteristics significantly impact how different performance requirements can be satisfied in practice. There are commonly three types of inference tasks: long input and short output (e.g., summarizing an essay), long input and long output (e.g., editing an essay), and short input and long output (e.g., generating an essay). Some tasks mainly demand low latency for the first output token, as subsequent token generation only needs to match human reading speed. In contrast, other tasks require minimal overall latency. These dif- ferent requirements highlight the importance of performance engineering based on specific use cases. As FMs continue to expand in size and capability, following the scaling laws [15], [20], optimizing inference becomes essential for efficient FMware deployment. Techniques such as model compression, quantization, and efficient hardware utilization are employed to balance performance with com- putational demands [21], [22]. A thorough understanding of the FM inference process is critical for advancing SPE for FMware, as FMs serve as the fundamental components. While numerous surveys exist on inference optimization of FMs [21]\u2013[28], our paper focuses specifically on the application-level (i.e., FMware) SPE challenges from the per- spective of application developers rather than AI engineers \u2013 in turn complementing existing efforts for model-level inference optimization."}, {"title": "D. FM-Powered Software (FMware)", "content": "FMs have become pivotal in AI-driven software applica- tions, revolutionizing software engineering and serving as the backbone for a new category of software known as FM- powered software, or FMware [2]. FMware can be classified into two categories: Promptware and Agentware. Promptware involves the direct utilization of FMs through one or many prompts. A notable example is Retrieval-Augmented Generation (RAG)-based software, which enhances output quality by combining FMs with ex- ternal knowledge sources and documents to avoid issues like"}]}, {"title": "III. SOFTWARE PERFORMANCE ENGINEERING CHALLENGES FOR FMWARE", "content": "In this section, we describe four challenges in SPE4FMware. For each challenge, we describe the characteristics unique to FMware and introduce a detailed breakdown of the challenge into several dimensions. For each dimension, we present the state of the practices that attempt to tackle the challenge and then discuss the innovation path for future research directions. In particular, the following four challenges are discussed: (1) How to create a high-performance cognitive architecture for FMware (Section III-A)? (2) How to develop a token-efficient communication language among the AI components of an FMware (Section III-B)? (3) How to continuously conduct performance tuning and optimization of FMware (Section III-C)? and (4) How to decide the deployment options for FMware (Section III-D)?"}, {"title": "A. Challenge 1: The complexity of creating high- performance cognitive architectures", "content": "The first step in developing FMware is to create an appro- priate cognitive architecture. A cognitive architecture defines how different AI components interact and reason together to achieve desired outcomes. This architecture complements the classical software architecture, detailing how AI reasoning and results are delivered through traditional software components like regular and/or vector databases. Choices made at the cognitive architecture level can significantly impact FMware performance, either directly or through their influence on the classical software architecture. Below are some critical considerations: Picking more powerful FMs within a simple cognitive architecture versus simpler FMs within a more complex cognitive architecture: FMware designers face a unique performance dilemma \u2013 they must choose between a simple cognitive architecture with fewer, larger, and more capable FMs (incurring high inference costs per request) versus a more complex cognitive architecture that combines multiple FMs (lowering inference costs per request, but involving many more inferences). The composition of multiple FMs introduces latency and performance challenges that go beyond those encountered in traditional software. The inference costs of FMs vary significantly, with some FM inferences being 10 times more expensive than others [33]. Additionally, the cost of each token generated from a single prompt is not constant. The first token incurs a much higher cost than subsequent tokens due to the need for a KV cache fill, while following tokens reuse this cache to respond faster [17], [34]. These cost dynamics are further complicated by the introduction of the new OpenAI 01 FM, which requires more reasoning time before responding, dramatically increasing the first token's cost [35]. Cognitive architecture choices range from leveraging a single FM for basic interactions to complex architectures proposed in multi-agent systems [36], [37]. Studies and our experiences indicate that smaller FMs within a more complex cognitive architecture can achieve similar, if not better, im- provements in FMware quality [2], [5]. However, increasing cognitive architecture complexity may result in higher latency for end-users (e.g., agents powered by weaker FMs debating each other versus a single prompt to a larger FM [38]). Chen et al. [39] demonstrated a balanced approach to FM algorithm design, considering both error reduction and cost minimization metrics. They tuned the parallel decomposition granularity as a hyperparameter, systematically balancing com- peting error and performance objectives. While complex cognitive architectures often aim to improve FMware accuracy, this may lead to suboptimal performance. Future research should explore techniques to help architects balance complex cognitive architectures with performance and cost considerations, mitigating performance overheads systematically."}, {"title": "Pipelining the execution of cognitive code as it is being generated versus waiting for the full generation and ver- ification of such code", "content": "FMware often generates a significant portion of their source code on the fly, either by prompting an FM or through interactions with one or more AI agents. For instance, an FM might be queried to define the necessary steps (i.e., create a plan), which are then executed using FM-powered components or traditional software components. Developers can either wait for the entire set of auto-generated instructions to be completed and verified before executing them [40], [41], or start pipelining the execution, risking the need to undo steps if the overall plan is later found to be inappropriate [42]. Pipelining cognitive architecture in FMware, whose code is generated on the fly, shows unique characteristics compared to classic software (Codeware). While waiting for complete plan generation and verification ensures correctness, it in- troduces substantial delays (aka user-observed latencies), as post-planning execution starts only when the entire plan is generated. Pipelining execution offers better responsiveness but risks costly and complex rollbacks. Currently, advanced mechanisms for integrating pipelining and rollbacks are implemented on a case-by-case basis without framework support, making it difficult for architects to system- atically reason about such crucial and complex FMware design choices."}, {"title": "The addition of semantic caching throughout the cognitive architecture", "content": "Semantic caching minimizes FM or AI compo- nent inference calls by identifying similar requests or those likely to generate previously produced content. These caches are vital in optimizing the performance of FMware by reducing redundant processing and lowering latency. However, design- ing caching mechanisms for FMware components remains ad-hoc, lacking best practices or techniques to help architects assess the ROI of adding such caches. Typically, semantic caches utilize FMs to determine request similarity, rather than relying solely on basic text similarity metrics. This sophisticated approach enables more accurate identification of repeated or similar queries, ensuring that only necessary computations are performed. Despite their potential, the implementation of semantic caching is still in its infancy, with a need for standardized methods and frameworks to guide their development and integration into FMware. Moreover, the effectiveness of semantic caching depends on the architecture's ability to efficiently store and retrieve cached results. This introduces challenges related to memory management and data retrieval speed, which must be addressed to realize the full benefits of semantic caching. Future re- search should focus on developing robust frameworks and best practices for semantic caching, ensuring that FMware can leverage these techniques to enhance performance and reduce computational overhead."}, {"title": "B. Challenge 2: The complexity of creating token-efficient communication language between the Al components of FMware", "content": "Traditional software systems assume consistent communica- tion costs between components, typically achieved through function calls or message passing via Remote Procedure Calls (RPC). For example, in a banking system, a function call might calculate interest on a savings account, taking the account balance and interest rate as inputs and returning the calculated amount. This process incurs minimal overhead due to deterministic encoding defined by the RPC interface. However, communicating with an FM requires using natural language, which is inherently more complex. Instead of a simple function call, we must instruct the FM in natural language, e.g., \"Calculate the simple yearly interest for $200 at an interest rate of 3.5%.\" This approach is more verbose and inefficient, with variability in verbosity across different languages. Parsing natural language inputs is resource-intensive com- pared to interpreting function calls, their parameters and return values, requiring sophisticated parsing and processing that incurs higher computational costs and latencies. Just as traditional systems use simple wire protocols for interactions, AI components need optimized communication protocols to manage their complex cognitive interactions effectively. These protocols significantly impact FMware performance. In summary, the shift from function calls to natural language communication introduces complexity and cost, necessitating the development of specialized protocols for efficient interac- tion management. Below, we discuss four dimensions of this challenge in detail."}, {"title": "Deciding the communication language", "content": "Different natural languages require varying amounts of tokens to express the same information semantically (language efficiency and density). This disparity in word-to-token ratios across lan- guages can significantly impact meeting performance require- ments [43]. For instance, Hindi requires eight times as many tokens as English to convey the same information [44]. This discrepancy results in longer processing times and varying per- formance based on the communication language used across the AI components of an FMware. API-based hosted models suffer from increased costs and longer response times with more tokens, while self-hosted models allow for language- specific fine-tuning to mitigate performance impacts. Prior studies have sought to address the impacts of the varying word-to-token ratios. Nag et al. [45] found that low-resource languages (LRLs) cost more than high-resource languages (HRLs) due to producing more tokens for the same content. They proposed using translation to reduce the token count processed by LRLs. However, adding translation as an intermediate step introduces drawbacks, such as increased processing time, which can affect FMware's ability to meet SLA requirements. In a prior multilingual FMware project [2], we translated requests to English, used English for internal cognitive communication, then translated responses back. This approach improved performance despite the additional trans- lation costs and aided developers who were not fluent in all supported languages in debugging the FMware. Further research is needed to design multilingual applica- tions that maintain consistent end-to-end SLAs despite to- ken count disparities. Possible approaches include assigning powerful GPUs for LRLs to speed up processing, adopting Nag et al.'s [45] translation step, and exploring prompt- compression techniques [46] to reduce token counts while retaining essential information. Fine-tuning FMs can also help them better understand and process the unique characteristics of specific domains, mitigating performance impacts due to token disparities."}, {"title": "Defining the communication format", "content": "Once the communica- tion language is decided, defining the communication format becomes crucial. JSON is a popular format, fine-tuned by many FMs for its structured, readable, and easily parsable"}, {"title": "Correcting communication messages", "content": "Once the communi- cation language and format structure have been defined, it is essential to ensure that the communication follows these rules. For example, if you are communicating in English and using JSON format, your messages need to be structured correctly to ensure FMs can parse and respond correctly. But if the output format is invalid or partially correct, then the downstream components of FMware will not work as expected as they may fail to understand the input. However, adhering to these rules often requires additional tokens in the prompts. For instance, to minimize error in output format, you might need to include a few-shot learning exam- ples in the prompt to help the FM understand the format. These expanded prompts ensure that the communication format is well-defined, but they also increase the number of tokens used (token-overhead), which can be costly in terms of processing time and resources. To mitigate these costs, some solutions integrate classical robust-parsing techniques on the communication channels. Instead of spending too many tokens to ensure the quality of the communication protocol, these techniques can help parse FM responses more efficiently. A practical example of this is documented by Bottaro and Ramgopal [49], where they used a classic, CPU-powered robust YAML parser to detect errors in communication. This method helps maintain a low error rate (0.01%), while also saving GPU jobs for more intensive tasks. By offloading the parsing to a CPU, they reduce the need for additional tokens in the prompt, leading to more efficient processing. Another example from Strong [54] proposes a multi step pipeline approach to mitigate the correctness of the output structure where the output structuring step is separated out from actual model reasoning step to produce the correct structured output finally. But the proposed approach uses two inference calls which would increase both cost as well as latency. Offloading the output parsing and structure formatting to less costly CPU based solutions is a first step towards ad- dressing this challenge. For instance, the output structuring step from Strong's work [54] can be offloaded to a CPU before sending the result downstream. On the other hand, innovative decoding approaches (such as the one proposed by Beurer- Kellne et al. [51]) which minimizes the performance overhead introduced with output structured generation, is another direc- tion."}, {"title": "Optimizing communication messages", "content": "Recent approaches have identified ways to optimize communication by skip generating parts of the message that are already known. This allows one to avoid generating each token individually, especially when the structure of the response is predictable. For example, suppose we know that a response should have a format <NAME=\"XXX\">. For a query like \"what is the name of the Nobel prize winner for peace in 2023\", the FM generates \u201c<NAME=Narges Mohammadi>.\" Instead of asking the FM to generate the entire response, we only ask the FM to generate the variable part (Narges Mohammadi). By using this approach, we can reduce the number of tokens that an FM needs to generate, leading to faster response times. A practical implementation of this concept is seen in the work of dottxt team [53]. They proposed the Coalescence framework to speed up the inference by five times with their structured generation that skips unnecessarily calls to FMs leveraging the known structure of the responses, only generating the variable parts that change. This work proposes an efficient guided text generation technique using finite- state machines and regular expressions to enforce structural constraints, significantly reducing computational costs and enhancing output quality while being model-agnostic."}, {"title": "C. Challenge 3: The complexity of performance tuning and optimization of FMware", "content": "Performance tuning and optimization in FMware requires a deep understanding of performance bottlenecks. The core of FMware is the inference of FMs. While many techniques focus on optimizing models [21], efficiently serving FMs is only the beginning. FMware involves interactions among multiple FMs and software components within a cognitive architecture, similar to classical software architecture, where each compo- nent has distinct resource demands. This leads to numerous configuration knobs, further complicated by heterogeneous hardware. Additionally, FMware might evolve continuously by itself as its agents perform self-exploration, compared to regular software which is static. Optimizing live FMware is akin to hitting a moving target. Hence, we categorize the challenges into three dimensions as described below:"}, {"title": "Complex model-level optimization", "content": "Techniques in FMware focus on enhancing hardware utilization, reducing latency, and maximizing throughput during the inference process. Existing FMs mostly rely on decoder-only transformer-based architectures. The inference process for these models has been described in detail in Section II. Many optimization techniques have been proposed, including model architecture redesigns (e.g., multi-query attention) and model compression strategies (e.g., knowledge distillation, quantization) [22]. For a more in-depth understanding, the reader can refer to existing surveys [21], [22]. In this section, we focus on the techniques that directly impact developers of FMware, where they interact with models through prompting. In FMware, developers invest significant effort in crafting effective prompts for the FM, also known as prompt engi- neering. Techniques like breaking down a complex prompt into multiple simpler prompts and adding explainability in-structions can enhance model output quality and reliability. However, they may increase the number of model inference calls or output tokens, raising end-to-end latency. When chain- ing multiple FM innovations, prior tokens cannot be used by downstream FMs, causing waiting times between calls. In pro- duction, developers need to carefully balance these prompting techniques with their impact on overall performance. Currently, prompt tuning relies heavily on manual and empirical methods. Developers frequently engage in trial-and- error approaches to refine prompts and find the optimal param- eters. For instance, Chen et al. [39] reasoned about the pros and cons of task decomposition for LLM-based applications, where each task formats a prompt based on its input and feeds it into an LLM. They studied parallel decomposition to guide developers in achieving the expected accuracy or efficiency. To boost model inference performance, Kurt [53] leveraged finite state machines and regular expressions to represent deterministic structures in the output, allowing the model to skip over predictable parts of the structure, thus substantially reducing generation latency. Streaming techniques have also been proposed to enhance performance by overlapping the output generation and input for the next model. For example, Bottaro and Ramgopal [49] proposed streaming the application pipeline so that downstream calls can be invoked as soon as they are ready, without waiting for the complete response. Additionally, Santhanam et al. [55] introduced ALTO, an FM serving system for streaming AI pipelines, demonstrating im- proved throughput and tail latency by streaming intermediate outputs to downstream tasks. While these methods are effective, the process is still manual and hard to extend to multiple objectives, making it hard to scale for more complex FMware. Future research might explore automating the prompt optimization process to minimize manual efforts. Through searching for multiple prompting goals such as output quality as well as performance requirements, the automated process enables developers to test and refine prompts rapidly. Additionally, real-world data anal- ysis through matching the pairs of prompt templates and the outputs, can help with effective prompt designs for developers, providing fast turnaround times during prototyping."}, {"title": "Excessive amount of performance configuration knobs", "content": "When dealing with FMware such as those illustrated by OPEA [56] in Figure 1, the configuration landscape becomes significantly complex. As shown, a performance engineer must consider multiple optimization opportunities, includ- ing different cognitive architectures, prompt designs, base model selections, model quantization decisions, fine-tuning processes, and communication protocol adjustments. These various aspects, from data ingestion to LLM inference and retrieval, illustrate the intricate array of configuration knobs required for optimizing the application-level performance. We describe three most prominent aspects as follows: Firstly, model selection involves a wide range of options, each performing differently in both functional and non-functional aspects such as generation speed, memory usage, and quality. Developers must not only aim for a model that produces good output quality but also one that satisfies pre-defined SLAs. Secondly, the choice of inference engine must align with the hardware setup to either maximize throughput by leveraging hardware capabilities or minimize costs with CPU-based alter- natives. Developers should recognize that different inference engines perform optimally under specific conditions. Finally, the complexity increases when taking a holistic view of FMware's entire software stack. Optimizations must account for the costs of loading and unloading large FMs due to limited accelerator availability and high operational costs. When different teams work on separate parts of the same FMware and use different models, careful orchestration is required. Decisions such as workload splitting between CPU and GPU resources, selecting an appropriate model with an ap-propriate inference engine, and finding an optimal combination of these elements significantly affect system performance. For example, splitting workloads involves deciding which tasks are better suited for FM agents versus traditional software. However, the impact of these choices is not well-studied and rigorous engineering guidelines are lacking. Many solutions have been proposed to address these chal- lenges, each tackling a specific aspect of FMware optimiza- tion. Maurya et al. [57] proposed SelectLLM, a framework that analyzes user prompts and selects the most appropri- ate models at runtime. This approach enables developers to maintain response quality while reducing computational costs, thereby improving the efficiency of model selection. Similarly, Shekhar et al. [58] introduced QC-Opt, a Quality-"}, {"title": "aware Cost Optimized LLM routing engine and framework", "content": "QC-Opt optimizes both the choice of LLM and input token count at runtime to minimize costs while maintaining output quality. This helps developers navigate trade-offs between quality and cost, providing flexibility in selecting models that best fit their requirements. Gong et al. [59] developed a bench- marking toolkit to evaluate various quantization strategies and parameter configurations, providing insights that can guide developers in making decisions on pruning and optimizing models across different deployment scenarios. To find suitable acceleration inference engines, while some articles provide high-level discussions and benchmarks for dif- ferent engines [60], there is still a lack of clear guidelines and standards to make informed selections. Further research and benchmarking are needed for specific scenarios. For instance, Xiao et al. [61] investigated the pros and cons of MLC-LLM and Llama.cpp in mobile environments, using mobile-sensitive metrics such as battery power consumption, latency, and mem- ory bottleneck. These insights are essential for tailoring LLM deployments to mobile devices where resource constraints are more stringent. To track the complexities of FMware tuning in a holis- tic perspective, Sun et al. [62] provided a multi-objective benchmarking toolkit, CEBench, that focuses on balancing ex- penditure and effectiveness for LLM deployments. By allow- ing easy modifications through configuration files, CEBench supports holistic decision-making across the entire software stack, enabling developers to optimize resource allocation, cost, and performance in an integrated manner. Papaioannou et al. [63] proposed a holistic approach to tuning LLM applications by addressing the complexities introduced by diverse workloads and real-world conditions. They noticed that most LLM applications, which often rely on synthetic datasets, may not account for the variability in input sizes and task demands found in practical applications. Their analysis, which includes different workload types and memory configurations, helps identify key performance bottlenecks and optimization opportunities. By providing a framework that considers a wide range of use cases, their research guides developers in making more informed decisions to enhance FMware efficiency across various scenarios. Addressing the complexity of performance optimization in FMware requires systematic studies, tools, and guidelines to support informed decision-making. Developing best prac- tices, patterns, and anti-patterns can help developers deter- mine which workloads are best suited to FMs versus tra- ditional software approaches. Additionally, creating bench- marking tools or simulation platforms for comparing different models, parameters, and deployment environments would al- low developers to test configurations quickly and assess their impact on performance and cost. In the future, automated techniques, such as search-based multi-objective optimization, could further enhance productivity by autonomously tuning configurations to balance accuracy, latency, and cost. This approach reduces the need for time-consuming manual tuning while efficiently identifying optimal configurations, ultimately improving FMware performance."}, {"title": "Evolving and moving target", "content": "Unlike traditional software, FMware is live software that keeps evolving. Each round of execution of an agent might lead to an adjustment of the whole system. To make things worse, agents can self-evolve, making benchmarking much harder than traditional software. As a result, performance issues might be difficult to reproduce due to: (a) the probabilistic nature of the model inference process with token sampling, and (b) the evolving nature of FMware through Data Flywheel [5] or self-exploration. To address the challenges of reproducibility, a common trick, also suggested by OpenAI [64], is using settings like a low temperature parameter value to ensure more consistent inference outputs. Additionally, setting seeds beforehand, as suggested by PyTorch's reproducibility guidelines [65], can further help achieve consistent behaviour across repeated in-"}, {"title": "ferences", "content": "These methods help standardize model behaviour, simplifying the identification of performance bottlenecks in FMware. However, such reproducibility measures restrict the model's ability to autonomously explore and optimize. The optimization process of FM and FM-powered agents should evolve from manually-tuning to a Data Flywheel- driven continuous self improving system. Firstly, we must continuously monitor and test if the accuracy of FMware drops through online feedback. Machmouchi and Gupta [66] proposed a comprehensive framework for evaluating LLMs, emphasizing the need for continuous testing with real-time user feedback. They highlighted the importance of segmenting user data to better capture the output quality of FMware. Such a framework would help us understand whether FMs need to be evolved. The evolution of FMware requires developers to make deci- sions about how frequently to update the models. One possible solution is fine-tuning the models based on newly generated datasets. Alternatively, developers can update prompting and post-processing techniques to enhance output quality and save costs by not retraining the model. Developers must weigh the cost of fine-tuning as a significant investment against the use of efficient prompting and post-processing techniques applied at the individual request level. Xia et al. [67] proposed a profiling tool to help developers estimate the cost of LLM fine-tuning on GPU clusters, which aids in planning the frequency of fine- tuning based on cost considerations. The decision between these strategies often hinges on factors like user request volume and the desired performance level. In low-volume scenarios, investing in advanced prompting techniques can be more cost-effective because each request can afford additional computational time. Conversely, in high request volume en- vironments, model fine-tuning becomes more beneficial as it allows simpler prompts, thereby reducing the computational overhead for each request. For controlling the self-exploration behaviour, one approach is to first record the self-exploration and replay to reproduce the performance issue. Chen et al. [68] proposed to reproduce the model training process with a record and replay mech- anism. Similar ideas can be applied to FMware inference, e.g., the decoding process for executing each FM invocation and other traditional software executions can be recorded and replayed as an agent explores, facilitating the analysis and debugging of performance issues in a reproducible manner."}, {"title": "D. Challenge 4: The complexity of deploying FMware", "content": "Several key decisions must be carefully considered when de- ploying FMware into production, as it must meet service-level agreements (SLAs). While many studies focus on ensuring the SLAs at the model level [69][71], these efforts alone are insufficient to guarantee the FMware meets application-level SLAs, since models are only part of the entire software system. Driven by such requirements, we identify challenges in three dimensions described as follows:"}, {"title": "Selecting optimal deployment options when hosting FMware", "content": "Unlike deploying traditional software", "FMware": "API-based deployment, rented cloud instances, and on-premise self-hosting. In certain cases, these deployment options can also be jointly leveraged. API-based deployment follows a pay-as-you-go mechanism. Examples are OpenAI-compatible APIs by proprietary model providers [72", "73": ".", "74": ".", "75": "or in a serverless way [76"}]}