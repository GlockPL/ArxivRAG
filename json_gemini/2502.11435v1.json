{"title": "SMART: Self-Aware Agent for Tool Overuse Mitigation", "authors": ["Cheng Qian", "Emre Can Acikgoz", "Hongru Wang", "Xiusi Chen", "Avirup Sil", "Dilek Hakkani-T\u00fcr", "Gokhan Tur", "Heng Ji"], "abstract": "Current Large Language Model (LLM) agents demonstrate strong reasoning and tool use capabilities, but often lack self-awareness, failing to balance these approaches effectively. This imbalance leads to Tool Overuse, where models unnecessarily rely on external tools for tasks solvable with parametric knowledge, increasing computational overhead. Inspired by human metacognition, we introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm that enhances an agent's self-awareness to optimize task handling and reduce tool overuse. To support this paradigm, we introduce SMART-ER, a dataset spanning three domains, where reasoning alternates between parametric knowledge and tool-dependent steps, with each step enriched by rationales explaining when tools are necessary. Through supervised training, we develop SMARTAgent, a family of models that dynamically balance parametric knowledge and tool use. Evaluations show that SMARTAgent reduces tool use by 24% while improving performance by over 37%, enabling 7B-scale models to match its 70B counterpart and GPT-40. Additionally, SMARTAgent generalizes to out-of-distribution test data like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool calls. These highlight the potential of strategic tool use to enhance reasoning, mitigate overuse, and bridge the gap between model size and performance, advancing intelligent and resource-efficient agent designs.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models (LLMs) (Ouyang et al., 2022; Team et al., 2023; Dubey et al., 2024) have led to remarkable improvements in reasoning capabilities, driving progress in diverse domains such as coherent text composition (Wei et al., 2022a), code generation (Gao et al., 2023; Wang et al., 2025; Pan et al., 2024), complex logical deduction (Yao et al., 2023, 2024), and nuanced natural language understanding (Wang et al., 2023; Yu et al., 2024; Wu et al., 2025). However, challenges remain, such as the inability to handle real-time information (Yu and Ji, 2024), provide accurate mathematical results (Lu et al., 2022), and fully comprehend human intentions (Qian et al., 2024b). These limitations highlight the need for LLMs to leverage external tools (Schick et al., 2023; Qin et al., 2023; Yuan et al., 2024; Qian et al., 2024a), enabling them to function as agents capable of assisting users in diverse tasks (Qin et al., 2024; Xi et al., 2023). Effective tool use and reasoning are thus complementary, each enhancing the other to overcome current shortcomings.\nTherefore, in problem-solving, a language agent often combines reasoning with tool use, following a ReACT-style approach (Yao et al., 2023), where the model alternates between thought processes and actions to derive solutions. This enables the core agent to apply its parametric knowledge to advance task-solving while using external tools to address its limitations. However, this interplay raises a critical question: when should the agent rely on external tools versus its own knowledge?"}, {"title": "2 Related Work", "content": "LM Knowledge Boundary. Recent studies highlight that while LMs excel at standard tasks, they struggle to recognize and acknowledge the limits of their knowledge (Yin et al., 2023; Kadavath et al., 2022). To address this gap, the concept of knowledge boundary has been introduced to define the limits of knowledge in LLMs (Li et al., 2024; Amayuelas et al., 2023). Building on this, some research evaluates LMs' self-awareness of their knowledge boundary through verbal probing (Kadavath et al., 2022) and fine-grained benchmarks (Yin et al., 2024), enabling LMs to determine whether a question is answerable. Other work focuses on mitigating hallucinations arising from the model's unawareness of its limits through data augmentation (Chen et al., 2023, 2024b), retrieval augmentation (Ren et al., 2023), and confidence calibration (Xue et al., 2024). Additionally, Chen et al. (2024a) and Zhang et al. (2024) trained LLMs to express their knowledge boundaries, enabling them to answer known questions and admit ignorance for unknown ones. Our work aligns with these studies but focuses on enhancing agents' awareness of their knowledge boundaries to enable wiser tool use and more efficient task handling.\nLM Tool Use. Integrating tool use into LLMs has gained significant attention as a way to complement parametric knowledge and enhance decision-making (Qin et al., 2023; Qu et al., 2025). Some research focuses on enabling LLMs to access external tools to overcome knowledge limitations (Qin et al., 2024; Qian et al., 2024d), including up-to-date information (Vu et al., 2023; Wang et al., 2024b) and domain-specific expertise (Ling et al., 2023; Wang et al., 2024a). Others explore tool creation (Qian et al., 2023; Cai et al., 2024) and external module integration (Qian et al., 2024c) to improve tool learning robustness. Despite these, a key challenge lies in evaluating and enhancing LLMs' ability to determine when and which tools to use. Benchmarks like MetaTool (Huang et al., 2023) and WTU-EVAL (Ning et al., 2024) highlight LLMs' struggles with unnecessary or incorrect tool usage, while dynamic frameworks (Wang et al., 2024c; Shen et al., 2024) propose adaptively invoking tools based on internal uncertainty thresholds. Unlike prior works, SMART rigorously defines and measures tool overuse, addressing overreliance on tools despite intrinsic reasoning capabilities. We optimize the balance between parametric knowledge and tool use, reducing overuse while enhancing performance."}, {"title": "3 Preliminaries", "content": "To investigate how models decide between invoking tools and relying on their own knowledge, we conduct a preliminary study on both LLMs and LM-driven agent systems. Our findings reveal both LLMs and agent systems' strong tendency for excessive tool use, which we define as Tool Overuse, leading to unnecessary resource overhead.\nDefinition of Tool Overuse. Tool overuse refers to the excessive reliance on external tools when an agent model could have successfully completed the task using its parametric knowledge alone. Formally, let Q be the total set of questions, and let P be the subset of questions that the model can correctly answer without using any tools. The model's intrinsic reasoning capability is then given by $\\alpha = \\frac{P}{Q}$. Now, suppose that when provided with access to tools, the model chooses to invoke at least one tool on a fraction \u03b2 of these questions in P. The Tool Overuse Rate is then defined as:\n$\u039f = \u03b1\u00b7 \u03b2$\nwhich quantifies the proportion of all questions where tool use is unnecessary, highlighting inefficiencies in the model's decision-making process.\nExperiments on LLMs. We first experiment with Llama-3.1-8B (Dubey et al., 2024) and Mistral-7B (Jiang et al., 2023) on the GSM8K test set (Cobbe et al., 2021). Each test question is presented under two conditions: i) the model reasons through the question normally and provides a final answer without using tools, and ii) the model has access to tools and independently decides whether to use them (see Appendix A.2). The statistics in Figure 2 reveal two key insights. First, both models exhibit significant tool overuse, with Llama's rate"}, {"title": "4 Method", "content": "To address the challenge of tool overuse, we draw inspiration from how humans balance internal knowledge and external tools. Metacognitive theory (Schraw and Moshman, 1995) suggests that human decision-making relies on an implicit awareness of knowledge boundaries, enabling strategic, step-by-step problem-solving (Livingston, 2003). Inspired by this, we aim to equip agent models with a similar capability-calibrating their metacognition to optimize reasoning and tool use.\nTo address this, we propose SMART, a data-driven approach that enhances self-awareness in agent models. While LLMs acquire broad knowledge from large-scale corpora (Wang et al., 2022), they are not explicitly trained to recognize their own strengths and limitations. To bridge this gap, we introduce SMART-ER, the first dataset contrasting areas where models excel versus struggle. Covering three domains with 3K+ questions and structured reasoning chains, SMART-ER helps agents strategically decide when to rely on internal knowledge or external tools."}, {"title": "4.1 Data Collection", "content": "To train agents to strategically balance parametric knowledge and external tools within a single reasoning chain, questions must be compositional-blending aspects the model excels at with those it struggles with. Building on prior studies (Hendrycks et al., 2021; Vu et al., 2023; Qian et al., 2024b), we identify three key limitations in LMs: i) math reasoning, where models struggle with complex computations requiring precise answers; ii) temporal knowledge, as LMs lack access to up-to-date facts beyond their training cut-off; and iii) user intent understanding, where im-"}, {"title": "4.2 Reasoning Chain Construction", "content": "As shown in Figure 5, each query Q is decomposed into a structured reasoning plan with n subgoals, S = {$1,$2,...,sn}. This decomposition is enabled by the compositional nature of our queries and is empirically achieved using GPT-40, an aux-"}, {"title": "4.3 Agent Training Implementation", "content": "We partition SMART-ER into training and test splits with statistics in Table 2. For each (Q, R') in the training set, we generate multiple input-output pairs for instruction tuning. The input comprises {Q, (r1, j1), . . ., (rxi, jxi)}, while the output consists of {(rxi+1, jxi+1),..., (rxi+1, jxi+1)}}, where xi indexes the tool-reliant steps. This setup ensures iterative reasoning, allowing the agent to leverage prior steps until the next tool invocation or final solution. The number of input-output pairs per (Q, R') also equals the number of tool-reliant steps, facilitating interactive inference.\nUsing these instruction pairs, we finetune the Llama-3.1 8B and 70B instruct models (Dubey et al., 2024) as well as the Mistral 7B, Nemo(12B) and Small(24B) instruct models (Jiang et al., 2023), adapting them into a family of SMARTAgent. These agent models enable interactive tool use, recognizes its own limitations, and balances tool reliance with parametric knowledge-driven reasoning to prevent tool overuse. See Appendix B.3 for training details and hyper-parameters."}, {"title": "5 Experiment", "content": "In this section, we present results demonstrating SMARTAgent's effectiveness in reducing tool overuse while enhancing reasoning performance."}, {"title": "5.1 Settings", "content": "Data. For in-domain testing, we evaluate SMARTAgent using the test split of adapted SMART-ER data across three domains: Math (MATH), Time (FreshQA), and Intention (IN3). For out-of-distribution (OOD) testing, we assess performance on GSM8K (Cobbe et al., 2021) and MINTQA (He et al., 2024), which test logical reasoning and real-world knowledge.\nBaselines. We incorporate three main baselines: i) Normal Reasoning Trained: For each domain, we train the model using the training set queries to perform reasoning without tools, leveraging the original solution chain or ground truth. ii) Base Model Reasoning Prompt: We directly prompt the model to apply chain-of-thought reasoning without tools to solve the problem. iii) Base Model Tool Prompt: We provide the model with all available tools and their usage but allow it to decide independently whether and when to use them.\nInference. For reasoning without tools, the model generates a response including the final answer. For tool-reliant reasoning, the inference is performed in an interactive manner: in each round, if a tool call is detected, we parse and execute it, integrating the tool's output and reasoning into the input. This repeats until the final answer is reached. See Appendix C for details.\nMetrics. We use two main evaluation metrics: Tool Used, which measures the average number of times a tool is leveraged during reasoning, and Accuracy, which evaluates the average performance across queries. For the IN3 dataset, where answers depend on user preferences and lack a single correct response, we adopt the original paper's metrics: Missing Details Recovery, assessing whether missing details in vague instructions are recovered, and Summarized Intention Coverage, assessing whether the final response covers all user-stated preferences."}, {"title": "5.2 Main Results", "content": "We present the main results in Table 3, along with the baseline performance of GPT-40 and GPT-40-mini for comparison. We also present the OOD results for Mistral-7B and Llama-3.1-8B in Section 5.1, highlighting the following key findings.\nSMARTAgent solves tasks efficiently. Compared to the base model in Table 3, which autonomously decides whether to use tools, SMARTAgent reduces tool usage time per query by 24% on average. At the same time, its performance improves by over 37% across models compared to the best baseline. This demonstrates SMARTAgent's efficiency in tool use, achieving higher results while relying less on external resources.\n7B-scale SMARTAgent can outperform GPT-40 baselines. Despite being much smaller, the 7B- and 8B-scale SMARTAgent models can outperform GPT-40 and its 70B counterpart in Time and Intention domains while using fewer tool calls, showcasing their efficient tool use. In Math, where reasoning scales with model size, SMARTAgent lags behind larger models but remains competitive against baselines using the same architecture. These results demonstrate that strategic tool use can bridge the gap between model size and performance, making SMARTAgent a resource-efficient yet powerful alternative.\nSMARTAgent generalizes to OOD settings. As shown in Section 5.1, SMARTAgent effectively reduces tool calls while achieving better overall performance on OOD test benchmarks. Notably, SMARTAgent makes only one-fifth the number of tool calls compared to the base model in MINTQA, where tool prompting often leads to excessive reliance and decreased accuracy.\nImproper tool uses degrade performance. In"}, {"title": "5.3 Analysis and Case Studies", "content": "SMARTAgent effectively reduces tool overuse. Beyond measuring tool use per query, we calculate the tool overuse rate, as defined in Section 3, and report results in Section 5.2 for GSM8K and Math domain test data. Notably, SMARTAgent reduces unnecessary tool calls by up to 50% compared to prompting the base model with tool access. However, despite this reduction, tool overuse persists, which we further examine in error analysis.\nError analysis. We provide error analysis in Table 5, highlighting common failure causes. Tool prompting leads to errors across all categories, while SMARTAgent reduces repetitive calls and improves argument accuracy. However, feedback neglect still causes tool invocation failures, particularly with the Code tool, and excessive caution in ensuring calculation accuracy adds overhead. This mirrors human task-solving, where we sometimes rely on calculators despite knowing the steps. Future work may explore balancing convenience, budget, and efficiency to enhance decision-making.\nCase Study. In Figure 6, we compare the solution chains of SMARTAgent and the base model with tool prompting. SMARTAgent demonstrates logical planning, context corroboration, and an awareness of its limitations and knowledge boundaries, with clear justifications for its decisions. This metacognitive approach closely mirrors human reasoning processes, making SMARTAgent's reasoning more interpretable and significantly reducing tool use overhead.\nConfidence Validation Experiment. To assess how effectively SMARTAgents choose between internal reasoning and tool invocation, we conducted experiments by injecting special tokens and analyzing the model's confidence in selecting each one.\nWe trained the model on the Time and Intention domains, introducing three special tokens: \"[[Reasoning]]\" for internal reasoning, \u201c[[AskUser]]\u201d for the AskUser tool, and", "Search]]": "or the Search tool. These tokens were prepended at the start of each step during training to guide the model's decision-making. See Appendix C.5 for details.\nFor evaluation, we sampled 50 decision steps from the test split of both domains and examined the logits of the selected tokens to measure confidence. Decisions were categorized as correct or wrong based on whether the model's choice aligned with the ground truth. As shown in Figure 7, the model consistently exhibited higher confidence in correct decisions than incorrect ones. This demonstrates that SMART effectively boosts confidence in accurate decision-making, reinforcing its ability to distinguish when to rely on internal knowledge or external tools."}, {"title": "6 Discussions and Future Work", "content": "Agent's improper tool usage. Our empirical analysis reveals a notable phenomenon of tool overuse, where agents frequently rely on external tools even when internal knowledge is sufficient. This over-reliance likely arises from two factors: i) the agent's uncertainty about its own capabilities, and ii) the perceived ease of external lookups compared to internal reasoning. We also observe instances of tool underuse, especially in large-scale models like GPT-40 and Llama-70B, where agents neglect to call essential tools, possibly due to misjudging the complexity of the task. Both overuse and underuse contribute to concerns over computational efficiency and solution accuracy. Future research could explore methods to better balance these trade-offs, such as by introducing explicit resource constraints or budgets for tool calls.\nMechanisms behind human and LM's decision-"}, {"title": "7 Conclusion", "content": "Inspired by human metacognition in decision-making, we propose the SMART paradigm for agent reasoning, where agents recognize their knowledge boundaries to decide when to use tools or parametric knowledge. Specifically, SMART-ER refines this decision boundary by incorporating questions that highlight areas where current LMs excel and struggle. Using these curated reasoning chains, we train SMARTAgent to better balance tool use and parametric knowledge, reducing tool overuse. Our results show that a simple data-driven approach can effectively calibrate model awareness, paving the way for efficient, low-resource agent development where \u201csmartness\u201d stems from both performance and metacognitive ability to optimize the reasoning strategy."}, {"title": "Limitations", "content": "Our study focuses on three key domains where LLMs explicitly struggle-Math, Intention, and Time-building on insights from existing literature. However, LLMs also face challenges in areas such as long-tail knowledge and domain-specific expertise, where external resources are essential. Expanding SMART-ER to these domains could further refine model self-awareness and improve calibration in knowledge boundary, complementing the strong OOD performance that SMARTAgent has already demonstrated. Additionally, while we evaluate our approach on two major model families, extending our analysis to a broader range of architectures, including Qwen, DeepSeek, and varying model sizes, could further validate and enhance the generalizability of our findings."}, {"title": "Appendix", "content": "A Preliminary Study Details"}, {"title": "A.1 Agent Experiment Details", "content": "The system instruction that we provide to both the XAgent and AgentGPT is:\nSolve the following task accurately, and use tools to help you only if necessary.\nFor LM-driven agent systems, we first prompt GPT-40 with all the questions from the GSM8K test set without using any tools. We then filter out only the questions that GPT-40 can correctly answer through pure text-based reasoning. From this refined dataset, we randomly sample 50 questions to evaluate AgentGPT and XAgent's performance. Surprisingly, despite the core model being capable of solving all sampled questions without external tools, it still heavily relies on tools during reasoning, leading to tool overuse. Figure 3 presents a case study illustrating a specific instance of tool overuse by both agents."}, {"title": "A.2 Model Experiment Details", "content": "For both Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, we prompt the model to do inference two times for each question from GSM8K's test set. The first time we instruct the model to reason normally to solve the query with the following system instruction:\nYou are an advanced assistant designed to solve tasks autonomously using your knowledge and reasoning. Clearly articulate your thought process and reasoning steps before presenting the final response to ensure transparency and accuracy.\nThe second time, we give the model access to tools and instruct it to independently decide when to use them based on the following system instruction:"}, {"title": "C Experiment Details", "content": "C.1 Data Setting\nFor in-domain testing, we use a subset of adapted SMART-ER data. Specifically, for the Math domain, we randomly sample 400 test instances from MATH, ensuring coverage of all testing categories (algebra, geometry, number theory, etc.), while spanning five difficulty levels. For the Time domain, we select 100 randomly sampled adapted data points from FreshQA, ensuring that each instance incorporates both fast-changing and slow-changing aspects. For the Intention domain, we randomly sample 100 data points from Intention-in-Interaction, ensuring that all selected instructions are vague and require specific user preferences to resolve.\nFor out-of-domain testing, we directly use the full test set of GSM8K without modifications. For MINTQA, due to its large size, we randomly sample 10% of the data points that meet the following criteria: the question requires multi-hop reasoning and contains both old and new knowledge. This selection ensures a challenging test set that evaluates the model's ability to generalize beyond in-domain tasks while maintaining a focus on complex reasoning and real-world knowledge retrieval."}, {"title": "C.2 Baselines", "content": "For the baseline Normal Reasoning Trained, we train a separate model for each domain. Specifically, for Math, Time, and Intention, we use the same queries as in the SMART-ER training set. In the Math domain, we leverage existing solution chains from the MATH dataset as training data. For the IN3 and Time domains, we use GPT-40 to generate normal reasoning chains, guided by existing annotations on final answers or missing details as heuristics. These domain-specific solution chains are then used to train the model.\nFor the baseline Base Model Reasoning Prompt,"}, {"title": "C.3 Interactive Inference", "content": "For both the baseline Base Model Tool Use and our SMARTAgent, we adopt an interactive approach for inference. Specifically, we first prompt the target model with the query and obtain its output. In this output, we use a rule-based natural language matching method to determine whether a tool call or a final answer is present (e.g., detecting whether \u201c### Final Response\" appears in the output to identify the final response).\nIf the final response is found, we extract it and terminate the iterative process. If a tool call is detected, we parse the parameters provided by the model to execute the tool call. Based on the specific tool's name, we invoke the corresponding API and integrate its output into the model's response. Next, we append the model's reasoning before the tool call, the tool call itself, and its output to the model's input. We then re-prompt the model to continue reasoning, given the previously executed tool call and its result.\nThis iterative process continues until the final response is successfully parsed and retrieved, forming the complete interactive inference process.\nBelow, we illustrate the respective input and output in an iterative inference process consisting of two iterations:"}, {"title": "C.4 Additional Results", "content": "We also provide results from the latest Llama-3.3-70B-Instruct\u00b9 model in Appendix C.3, comparing its performance with the Llama-3.1-70B-Instruct-based SMARTAgent. Although Llama-3.3 is the newest version, we use the 3.1 series to maintain consistency with the 8B model, which is also from the 3.1 version. Empirically, we found no significant difference in performance between the 3.3 and 3.1 versions of the 70B model."}, {"title": "C.5 Confidence Validation", "content": "We independently train the Llama-3.1-8B-Instruct and Mistral-7B-Instruct models with the added special tokens. At each reasoning step, we prepend a special token at the very beginning to indicate the model's chosen approach\u2014whether it relies on external tools (e.g., \"[[AskUser]]\" or \"[[Search]]\")"}]}