{"title": "Mitigating Backdoor Attacks in Federated Learning via Flipping Weight Updates of Low-Activation Input Neurons", "authors": ["Bin-bin Ding", "Peng-hui Yang", "Zeq-ing Ge", "Sheng-jun Huang"], "abstract": "Federated learning enables multiple clients to collaboratively train machine learning models under the overall planning of the server while adhering to privacy requirements. However, the server cannot directly oversee the local training pro-cess, creating an opportunity for malicious clients to introduce backdoors. Existing research shows that backdoor attacks activate specific neurons in the compromised model, which remain dormant when processing clean data. Leveraging this insight, we propose a method called Flipping Weight Updates of Low-Activation Input Neurons (FLAIN) to defend against backdoor attacks in federated learning. Specifically, after completing global training, we employ an auxiliary dataset to identify low-activation input neurons and flip the associated weight updates. We incrementally raise the threshold for low-activation inputs and flip the weight updates iteratively, until the performance degradation on the auxiliary data becomes unacceptable. Extensive experiments validate that our method can effectively reduce the success rate of backdoor attacks to a low level in various attack scenarios including those with non-IID data distribution or high MCRs, causing only minimal performance degradation on clean data.", "sections": [{"title": "Introduction", "content": "Federated Learning (FL) (McMahan et al. 2017), an emerging distributed machine learning framework, facilitates collaborative model training while preserving the privacy of decentralized data among clients. In recent years, FL has found extensive applications across various domains, including healthcare (Alzubi et al. 2022; Salim and Park 2022), financial services (Basu et al. 2021; Chatterjee, Das, and Rawat 2023), and intelligent transportation (Manias and Shami 2021; Zhao et al. 2022; Zhu et al. 2023b).\nIn FL, while privacy settings effectively protect participants' data privacy, they also prevent the server coordinating the global training from supervising the underlying processes, leading to a series of new security issues. A prominent concern is that malicious clients can manipulate local training data and model parameters to carry out backdoor attacks (Sun et al. 2019; Bagdasaryan et al. 2020; Fang and Chen 2023; Zhang et al. 2024). A backdoor attack embeds malicious behavior into the machine learning model, allowing it to function normally under standard conditions but to exhibit attacker-controlled behavior when specific triggers appear. As a result, the model maintains normal performance on clean data while producing targeted incorrect results when exposed to the trigger data.\nCurrently, existing backdoor defense methods can be broadly categorized into two main types: mid-training defenses and post-training defenses. The former type of methods such as Krum (Blanchard et al. 2017), Bulyan (Mhamdi, Guerraoui, and Rouault 2018), and Trimmed Mean (Yin et al. 2018) mainly seek to identify and filter out malicious updates by scrutinizing discrepancies among the submitted data. These strategies aim to distinguish between benign and potentially malicious updates by detecting anomalies and inconsistencies. However, in scenarios involving Non-Independent and Identically Distributed (non-IID) data (Zhao et al. 2018; Zhang et al. 2021), the inherent variability in data distribution can amplify discrepancies among benign updates, thereby facilitating the evasion of detection by malicious updates. Recent research (Qin et al. 2024) confirms that benign and malicious updates exhibit a mixed distribution under non-IID conditions, with anomaly detection based on linear similarity failing to identify backdoors effectively (Fung, Yoon, and Beschastnikh 2018; Bhagoji et al. 2019; Xie, Koyejo, and Gupta 2020). RLR (Ozdayi, Kantarcioglu, and Gel 2021) is different from mainstream mid-training methods, which introduces a robust learning rate adjustment method to counter backdoor attacks, but it is still weakened under non-IID conditions and its effectiveness is also constrained by the malicious client ratio (MCR). Furthermore, existing research (Yin et al. 2018; Aramoon et al. 2021; Nguyen et al. 2021; Huang et al. 2023) highlights the critical need to keep the MCR below 50% in each training round to ensure the effectiveness of all these mid-training defensive mechanisms.\nIn post-training defenses, the necessity to identify update discrepancies across clients is eliminated, in contrast to the requirements of mid-training defenses. BadNets (Gu, Dolan-Gavitt, and Garg 2017) reveals that trigger samples activate a specific subset of neurons in compromised models, which otherwise remain dormant when exposed to clean inputs. These backdoor neurons often respond exclusively to trigger patterns (Zhu et al. 2023a) and exhibit high sensitivity to specific inputs (Zheng et al. 2022) or perturbations (Wu and Wang 2021). Building on this insight, subsequent studies (Liu, Dolan-Gavitt, and Garg 2018; Wang et al. 2019) demonstrate that pruning low-activation neu-"}, {"title": "Related Work", "content": "Backdoor attacks in FL. Common strategies for conducting backdoor attacks involve manipulating models to make specific predictions by embedding triggers within samples. These triggers can take various forms: numerical (Bagdasaryan et al. 2020), exploiting exact numerical values; semantic (Wang et al. 2020a), influencing predictions through subtle semantic cues; and even imperceptible forms (Zhang and Zheng 2024), which activate the backdoor despite being hard to detect. Xie et al. (2019) proposes decomposing a global trigger into localized variants across multiple malicious clients to enhance attack stealth.\nMid-training defenses in FL. In the quest to combat backdoor attacks, various defense mechanisms emerge. The Krum algorithm (Blanchard et al. 2017) focuses on selecting the global model by minimizing the sum of Euclidean distances to other local models. Similarly, the Trimmed Mean algorithm (Yin et al. 2018) mitigates the influence of outliers by discarding the highest and lowest values and calculating the average of the remaining data points. The Bulyan algorithm (Mhamdi, Guerraoui, and Rouault 2018) builds on this approach by first using Krum (Blanchard et al. 2017) to identify a more reliable subset of updates and then applying Trimmed Mean (Yin et al. 2018) for aggregation. RLR (Ozdayi, Kantarcioglu, and Gel 2021) utilizes robust learning rates to update the global model, applying a negative learning rate to dimensions with significant directional disparities to mitigate the impact of malicious updates. However, while these methods are effective in IID scenarios, their performance significantly diminishes in non-IID environments due to increased variability among client updates (Fung, Yoon, and Beschastnikh 2018; Xie, Koyejo, and Gupta 2020).\nPost-training defenses in FL. For a backdoored model, certain neurons are activated by trigger samples but remain dormant with clean inputs. Pruning (Liu, Dolan-Gavitt, and Garg 2018; Wang et al. 2019) utilizes clean data to identify and remove these neurons, minimizing their impact during attacks. Wu et al. (2020) presents a strategy for FL where clients rank filters based on average activations from local data, and the server aggregates these rankings to prune the global model. Additionally, extreme weights are adjusted after pruning to enhance the robustness of the defense."}, {"title": "Problem Setup", "content": "Federated Learning\nSuppose there are N clients in a FL system and the kth client has a local dataset $D_k = \\{(X_{k,i}, Y_{k,i})\\}_{i=1}^{n_k}$ of size $n_k$. The server manages client participation in each training round, with the global training objective defined as follows:\n$w^* = \\underset{W}{arg \\hspace{0.1cm}min} \\sum_{k=1}^{N} \\lambda_k f(w, D_k)$,\n$f(w,D_k) = \\frac{1}{n_k} \\sum_{i=1}^{n_k} f(w, (x_{k,i}, Y_{k,i}))$,\nwhere $w^*$ denotes the optimal global model parameters, f(w, Dk) represents the average loss computed over the dataset Dk for client k, $(x_{k,i}, Y_{k,i})$ denotes the i-th sample in Dk, and $\\lambda_k$ indicates the weight of the loss for client k."}, {"title": "Threat Model", "content": "Due to privacy constraints, the server lacks access to local training data and processes. Bagdasaryan et al. (2020) reveals vulnerabilities in FedAvg (McMahan et al. 2017), highlighting its susceptibility to backdoor attacks.\nAttacker's Capabilities. We assume that malicious attackers can perform white-box attacks, meaning they have access to both the training data and model parameters of compromised clients. However, these attackers do not have access to the data or model parameters of non-compromised clients, nor can they interfere with the aggregation process conducted by the server.\nAttacker's Goals. The backdoored model is engineered to predict a specific class for samples embedded with triggers. For instance, in image classification tasks, a backdoored model might misclassify images of airplanes containing triggers as birds, while accurately classifying other clean images. For any input x, the ideal output of the backdoored model Mw can be formulated as follows:\n$M_w(x) = \\begin{cases} S_y, & \\text{if } x \\in D_{clean}, \\\\ Y_{target}, & \\text{else}, \\end{cases}$\nand the training objective can be formulated as follows:\n$\\underset{W}{min} E_{(x, y) \\sim D_{clean}} l(M_w(x), y)$\n$+ \\lambda \\cdot E_{(x', Y_{target}) \\sim D_{poison}} l(M_w(x'), Y_{target}),$\nwhere l is the loss function, $Y_{target}$ is the target label, $D_{clean}$ represents the clean dataset, $D_{poison}$ represents the poisoned dataset, and $\\lambda$ is a balancing parameter."}, {"title": "Defense Model", "content": "Defender's Capabilities. Consistent with prior research (Chen et al. 2020; Tan et al. 2020; Cao et al. 2020; Wang et al. 2020b), we assume that the server maintains a small auxiliary dataset, which is used to compute the activation inputs of the fully connected layer and to evaluate performance variations in the model.\nDefender's Goals. The backdoored model integrates information learned from both clean and trigger data, making it challenging to selectively remove backdoor information."}, {"title": "Methodology", "content": "In this section, we explain in detail the process of handling low-activation input neurons in the fully connected layer using flipping and the adaptive threshold, which is collectively called FLAIN."}, {"title": "Flipping", "content": "Activated inputs. Consider a model that includes a fully connected layer $\\tau$ with z neurons, where the weight matrix W is defined as $W = (w^{[1]}, w^{[2]}, \\dots,w^{[z]}) \\in \\mathbb{R}^{s \\times z}$. Each column vector $w^{[i]} = (w^{[i]}_1, w^{[i]}_2,...,w^{[i]}_s)^T \\in \\mathbb{R}^{s}$ in W represents the weights associated with the i-th neuron in layer $\\tau$, where $w^{[i]}_j (j = 1,2,\\dots, s)$ denotes the j-th element of the vector $w^{[i]}$. Let $x = (x_1,x_2,\\dots,x_z) \\in \\mathbb{R}^{z}$ denote the inputs to layer $\\tau$. These inputs are obtained by applying the ReLU activation function (Ramachandran, Zoph, and Le 2017) element-wise to the raw outputs from the preceding layer. Specifically, each component $x_i (i = 1,2,..., z)$ of the vector x is defined as $x_i = ReLU(x'_i)$, where $x'_i$ represents the raw output from the preceding layer. The output H(x) of layer $\\tau$ can be expressed as:\n$H(x) = Wx + b$\n$\\begin{bmatrix} w^{[1]}_1x_1 + w^{[2]}_1x_2 + \\cdots + w^{[z]}_1x_z + b_1 \\\\ w^{[1]}_2x_1 + w^{[2]}_2x_2 + \\cdots + w^{[z]}_2x_z+b_2 \\\\ \\dots \\\\ w^{[1]}_s x_1 + w^{[2]}_s x_2 + \\cdots + w^{[z]}_sx_z + b_s. \\end{bmatrix}$"}, {"title": "Adaptive Threshold", "content": "The threshold $\\lambda$ proposed in the previous section highly determines the tradeoff between clearing backdoors and maintaining performance on clean data. To find out a better $\\lambda$ automatically, we propose an adaptive threshold selection method.\nWhen firstly evaluating the fully-trained model on the auxiliary dataset, the server retrieves the average activation input vector $x = (x_1,x_2,\\dots, x_z) \\in \\mathbb{R}^z$ for layer $\\tau$ on the whole auxiliary dataset and identifies the minimum average activation value $\\mu \\in x$. Then the server sets an initial threshold $\\lambda$ which is slightly larger than $\\mu$ and does the flipping process as described in the previous subsection. After updating the weights of layer $\\tau$ in the model from W to $W^* = (w^{[1]*}, w^{[2]*},\\dots, w^{[z]*}) \\in \\mathbb{R}^{s \\times z}$, the server assesses the current model's performance $acc_1$ on the auxiliary dataset $\\Delta$. If the performance gap $\\epsilon = acc_0-acc_1$ is less than a predetermined threshold $\\rho$, the server increments $\\lambda$ by a step size and flip the weight updates again. This procedure is repeated until $\\epsilon$ exceeds the predefined threshold $\\rho$, which means the performance drop exceeds the server's tolerance limit. The process of adaptively modifying the threshold is illustrated in the right side of Figure 1.\nThe details of the whole FLAIN method are summarized in Figure 1 and the pseudo codes are given in Algorithm 1."}, {"title": "Performance Evaluation", "content": "Experiment Setup\nTraining setup. For our experiments, we use the Adam optimizer (Kingma and Ba 2014) with a learning rate of 0.001 and betas set to (0.9, 0.999). Each training round consists of local training for 1 epoch with a batch size of 256, while both the client sampling rate and the global learning rate are set to 1.\nEvaluation Metrics. We utilize the following two metrics to assess the model's performance.\nAttack Success Rate (ASR). ASR measures the proportion of trigger samples that the model correctly classifies as the target class. This metric indicates the effectiveness of the backdoor attack, reflecting how well the attack can manipulate the model to produce the desired output for malicious inputs.\nNatural Accuracy (ACC). ACC quantifies the model's performance on untainted data, representing its effectiveness in standard classification tasks. This metric reflects how well the model generalizes to non-malicious inputs.\nDatasets and Models. In the experiments, we utilize various datasets, including MNIST (Deng 2012), Fashion-MNIST"}]}