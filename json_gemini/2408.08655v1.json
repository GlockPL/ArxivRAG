{"title": "Mitigating Backdoor Attacks in Federated Learning via Flipping Weight Updates of Low-Activation Input Neurons", "authors": ["Bin-bin Ding", "Peng-hui Yang", "Zeq-ing Ge", "Sheng-jun Huang"], "abstract": "Federated learning enables multiple clients to collaboratively train machine learning models under the overall planning of the server while adhering to privacy requirements. However, the server cannot directly oversee the local training process, creating an opportunity for malicious clients to introduce backdoors. Existing research shows that backdoor attacks activate specific neurons in the compromised model, which remain dormant when processing clean data. Leveraging this insight, we propose a method called Flipping Weight Updates of Low-Activation Input Neurons (FLAIN) to defend against backdoor attacks in federated learning. Specifically, after completing global training, we employ an auxiliary dataset to identify low-activation input neurons and flip the associated weight updates. We incrementally raise the threshold for low-activation inputs and flip the weight updates iteratively, until the performance degradation on the auxiliary data becomes unacceptable. Extensive experiments validate that our method can effectively reduce the success rate of backdoor attacks to a low level in various attack scenarios including those with non-IID data distribution or high MCRs, causing only minimal performance degradation on clean data.", "sections": [{"title": "Introduction", "content": "Federated Learning (FL) (McMahan et al. 2017), an emerging distributed machine learning framework, facilitates collaborative model training while preserving the privacy of decentralized data among clients. In recent years, FL has found extensive applications across various domains, including healthcare (Alzubi et al. 2022; Salim and Park 2022), financial services (Basu et al. 2021; Chatterjee, Das, and Rawat 2023), and intelligent transportation (Manias and Shami 2021; Zhao et al. 2022; Zhu et al. 2023b).\nIn FL, while privacy settings effectively protect participants' data privacy, they also prevent the server coordinating the global training from supervising the underlying processes, leading to a series of new security issues. A prominent concern is that malicious clients can manipulate local training data and model parameters to carry out backdoor attacks (Sun et al. 2019; Bagdasaryan et al. 2020; Fang and Chen 2023; Zhang et al. 2024). A backdoor attack embeds malicious behavior into the machine learning model, allowing it to function normally under standard conditions but to exhibit attacker-controlled behavior when specific triggers appear. As a result, the model maintains normal performance on clean data while producing targeted incorrect results when exposed to the trigger data.\nCurrently, existing backdoor defense methods can be broadly categorized into two main types: mid-training defenses and post-training defenses. The former type of methods such as Krum (Blanchard et al. 2017), Bulyan (Mhamdi, Guerraoui, and Rouault 2018), and Trimmed Mean (Yin et al. 2018) mainly seek to identify and filter out malicious updates by scrutinizing discrepancies among the submitted data. These strategies aim to distinguish between benign and potentially malicious updates by detecting anomalies and inconsistencies. However, in scenarios involving Non-Independent and Identically Distributed (non-IID) data (Zhao et al. 2018; Zhang et al. 2021), the inherent variability in data distribution can amplify discrepancies among benign updates, thereby facilitating the evasion of detection by malicious updates. Recent research (Qin et al. 2024) confirms that benign and malicious updates exhibit a mixed distribution under non-IID conditions, with anomaly detection based on linear similarity failing to identify backdoors effectively (Fung, Yoon, and Beschastnikh 2018; Bhagoji et al. 2019; Xie, Koyejo, and Gupta 2020). RLR (Ozdayi, Kantarcioglu, and Gel 2021) is different from mainstream mid-training methods, which introduces a robust learning rate adjustment method to counter backdoor attacks, but it is still weakened under non-IID conditions and its effectiveness is also constrained by the malicious client ratio (MCR). Furthermore, existing research (Yin et al. 2018; Aramoon et al. 2021; Nguyen et al. 2021; Huang et al. 2023) highlights the critical need to keep the MCR below 50% in each training round to ensure the effectiveness of all these mid-training defensive mechanisms.\nIn post-training defenses, the necessity to identify update discrepancies across clients is eliminated, in contrast to the requirements of mid-training defenses. BadNets (Gu, Dolan-Gavitt, and Garg 2017) reveals that trigger samples activate a specific subset of neurons in compromised models, which otherwise remain dormant when exposed to clean inputs. These backdoor neurons often respond exclusively to trigger patterns (Zhu et al. 2023a) and exhibit high sensitivity to specific inputs (Zheng et al. 2022) or perturbations (Wu and Wang 2021). Building on this insight, subsequent studies (Liu, Dolan-Gavitt, and Garg 2018; Wang et al. 2019) demonstrate that pruning low-activation neu-\nrons effectively mitigates backdoor attacks while preserving overall model performance. Such defenses concentrate on targeting and eliminating neurons with low-activation signals, thereby effectively preventing their activation by malicious inputs. Wu et al. (2020) applies pruning techniques for backdoor defense in FL and notes that the effectiveness of pruning depends on the attacker's objectives. When backdoor and benign behaviors rely on the same neurons, pruning these neurons can significantly degrade the model's performance on both malicious and clean data. The issue arises due to the neurons targeted for removal often playing crucial and essential roles in both backdoor attacks and benign inferences, which considerably reduces the model's overall performance.\nDrawing from the aforementioned research, we attempt to implement backdoor defenses that can eliminate the adverse effects of harmful neurons while preserving the function of normal neurons. In our method, we mainly focus on low-activation signals after they reach the fully connected layer. For these low-activation inputs, a potential strategy is to prune the associated neurons, thereby dispelling their contribution to the current layer's output. Inspired by RLR (Ozdayi, Kantarcioglu, and Gel 2021), which utilizes a negative learning rate to flip the updates of low-confidence dimensions during training, we explore an alternative approach called Flipping Weight Updates of Low-Activation Input Neurons (FLAIN). Unlike pruning, which removes the output of these neurons, FLAIN adjusts their weights by flipping the updates, aiming to introduce larger perturbations to their contributions. In order to find out the best threshold for low-activation inputs, we introduce a performance-adaptive threshold, which increases in a short step each time, until the updated model's performance drop becomes unacceptable. Extensive experiment results indicate that our method effectively eliminates backdoors with minimal performance degradation. We compare FLAIN with the baseline pruning method, and the results reveal that FLAIN adapts more effectively to a broader range of attack scenarios than pruning low-activation inputs, demonstrating superior defensive robustness.\nOur main contributions can be summarized as follows:\n\u2022 We explore a defense strategy for low-activation input neurons in the fully connected layer and propose a method, FLAIN, which mitigates backdoor attacks by flipping the weight updates associated with these neurons obtained during training.\n\u2022 We introduce a performance-adaptive threshold for filtering low-activation inputs. The threshold increases in a short step each time and more neuron updates are flipped after each increase. The threshold will continue increasing until the updated model's performance drop becomes unacceptable.\n\u2022 We validate FLAIN's consistent performance improvements across various settings, especially those with non-IID data distribution and high MCRs. The results demonstrate that FLAIN effectively lowers the success rate of backdoor attacks while preserving the model's performance on clean data in these settings."}, {"title": "Related Work", "content": "Backdoor attacks in FL. Common strategies for conducting backdoor attacks involve manipulating models to make specific predictions by embedding triggers within samples. These triggers can take various forms: numerical (Bagdasaryan et al. 2020), exploiting exact numerical values; semantic (Wang et al. 2020a), influencing predictions through subtle semantic cues; and even imperceptible forms (Zhang and Zheng 2024), which activate the backdoor despite being hard to detect. Xie et al. (2019) proposes decomposing a global trigger into localized variants across multiple malicious clients to enhance attack stealth.\nMid-training defenses in FL. In the quest to combat backdoor attacks, various defense mechanisms emerge. The Krum algorithm (Blanchard et al. 2017) focuses on selecting the global model by minimizing the sum of Euclidean distances to other local models. Similarly, the Trimmed Mean algorithm (Yin et al. 2018) mitigates the influence of outliers by discarding the highest and lowest values and calculating the average of the remaining data points. The Bulyan algorithm (Mhamdi, Guerraoui, and Rouault 2018) builds on this approach by first using Krum (Blanchard et al. 2017) to identify a more reliable subset of updates and then applying Trimmed Mean (Yin et al. 2018) for aggregation. RLR (Ozdayi, Kantarcioglu, and Gel 2021) utilizes robust learning rates to update the global model, applying a negative learning rate to dimensions with significant directional disparities to mitigate the impact of malicious updates. However, while these methods are effective in IID scenarios, their performance significantly diminishes in non-IID environments due to increased variability among client updates (Fung, Yoon, and Beschastnikh 2018; Xie, Koyejo, and Gupta 2020).\nPost-training defenses in FL. For a backdoored model, certain neurons are activated by trigger samples but remain dormant with clean inputs. Pruning (Liu, Dolan-Gavitt, and Garg 2018; Wang et al. 2019) utilizes clean data to identify and remove these neurons, minimizing their impact during attacks. Wu et al. (2020) presents a strategy for FL where clients rank filters based on average activations from local data, and the server aggregates these rankings to prune the global model. Additionally, extreme weights are adjusted after pruning to enhance the robustness of the defense."}, {"title": "Problem Setup", "content": "Federated Learning\nSuppose there are N clients in a FL system and the kth client has a local dataset $D_k = \\{(X_{k,i}, Y_{k,i})\\}_{i=1}^{n_k}\\}$ of size nk. The server manages client participation in each training round, with the global training objective defined as follows:\n$$w^* = \\arg \\min_w \\sum_{k=1}^N \\lambda_k f(w, D_k),$$\n$$f(w,D_k) = \\frac{1}{n_k} \\sum_{i=1}^{n_k} f(w, (x_{k,i}, Y_{k,i})),$$\nwhere w* denotes the optimal global model parameters, f(w, Dk) represents the average loss computed over the dataset Dk for client k, $(x_{k,i}, Y_{k,i})$ denotes the i-th sample in Dk, and \u03bbk indicates the weight of the loss for client k."}, {"title": "Threat Model", "content": "Due to privacy constraints, the server lacks access to local training data and processes. Bagdasaryan et al. (2020) reveals vulnerabilities in FedAvg (McMahan et al. 2017), highlighting its susceptibility to backdoor attacks.\nAttacker's Capabilities. We assume that malicious attackers can perform white-box attacks, meaning they have access to both the training data and model parameters of compromised clients. However, these attackers do not have access to the data or model parameters of non-compromised clients, nor can they interfere with the aggregation process conducted by the server.\nAttacker's Goals. The backdoored model is engineered to predict a specific class for samples embedded with triggers. For instance, in image classification tasks, a backdoored model might misclassify images of airplanes containing triggers as birds, while accurately classifying other clean images. For any input x, the ideal output of the backdoored model Mw can be formulated as follows:\n$$M_w(x) =\\begin{cases}  y, & \\text{if } x \\in D_{clean}, \\\\  Y_{target}, & \\text{else,}  \\end{cases}$$\nand the training objective can be formulated as follows:\n$$\\min_W \\mathbb{E}_{(x, y)\\sim D_{clean}} \\ell(M_w(x), y)$$\n$$+ \\lambda \\cdot \\mathbb{E}_{(x', Y_{target})\\sim D_{poison}} \\ell(M_w(x'), Y_{target}),$$\nwhere l is the loss function, Ytarget is the target label, Dclean represents the clean dataset, Dpoison represents the poisoned dataset, and \u03bb is a balancing parameter."}, {"title": "Defense Model", "content": "Defender's Capabilities. Consistent with prior research (Chen et al. 2020; Tan et al. 2020; Cao et al. 2020; Wang et al. 2020b), we assume that the server maintains a small auxiliary dataset, which is used to compute the activation inputs of the fully connected layer and to evaluate performance variations in the model.\nDefender's Goals. The backdoored model integrates information learned from both clean and trigger data, making it challenging to selectively remove backdoor information. Therefore, the defender's objectives are twofold: to reduce the model's accuracy on trigger samples while maintaining its performance on the clean data."}, {"title": "Methodology", "content": "In this section, we explain in detail the process of handling low-activation input neurons in the fully connected layer using flipping and the adaptive threshold, which is collectively called FLAIN."}, {"title": "Flipping", "content": "Activated inputs. Consider a model that includes a fully connected layer \u03c4 with z neurons, where the weight matrix W is defined as $W = (w^{[1]}, w^{[2]}, \\ldots, w^{[z]}) \\in \\mathbb{R}^{s \\times z}$. Each column vector $w^{[i]} = (w^{[i]}_1, w^{[i]}_2,...,w^{[i]}_s)^T \\in \\mathbb{R}^{s}$ in W represents the weights associated with the i-th neuron in layer \u03c4, where $w^{[i]}_j (j = 1,2,\\ldots, s)$ denotes the j-th element of the vector $w^{[i]}$. Let $x = (x_1,x_2,\\ldots,x_z) \\in \\mathbb{R}^{z}$ denote the inputs to layer \u03c4. These inputs are obtained by applying the ReLU activation function (Ramachandran, Zoph, and Le 2017) element-wise to the raw outputs from the preceding layer. Specifically, each component $x_i(i = 1,2,..., z)$ of the vector x is defined as $x_i = \\text{ReLU}(x)$, where $x'$ represents the raw output from the preceding layer. The output H(x) of layer \u03c4 can be expressed as:\n$$H(x) = Wx + b$$\n$$\\begin{bmatrix} w^{[1]}_1x_1 + w^{[2]}_1x_2 + \\cdot + w^{[z]}_1x_z + b_1 \\\\ w^{[1]}_2x_1 + w^{[2]}_2x_2 + \\cdot + w^{[z]}_2x_z+b_2 \\\\ \\vdots \\\\ w^{[1]}_s x_1 + w^{[2]}_s x_2 + \\cdot + w^{[z]}_sx_z + b_s  \\end{bmatrix}$$\nwhere $b = (b_1, b_2,\\ldots, b_s)^\\top \\in \\mathbb{R}^{s}$ represents the bias vector of layer \u03c4.\nFlipping. FLAIN offers a new approach for handling low-activation input neurons. In contrast to pruning that sets the weights of these neurons to 0, FLAIN modifies these weights by flipping the updates obtained during global training. Before training begins, the server initializes the global model parameters and records the initial weights of layer \u03c4: $W_o = (w_o^{[1]}, w_o^{[2]}, \\ldots, w_o^{[z]}) \\in \\mathbb{R}^{s \\times z}$.\nUpon completing the entire global training, the server acquires layer \u03c4's weights: $W = (w^{[1]}, w^{[2]}, \\ldots, w^{[z]}) \\in \\mathbb{R}^{s \\times z}$ and computes the weight updates: $\\Delta W = W - W_o = (\\Delta w^{[1]}, \\Delta w^{[2]}, \\ldots,\\Delta w^{[z]}) \\in \\mathbb{R}^{s \\times z}$. Employing an auxiliary dataset $\\mathbb{4}$ that consists of clean data, the server assesses the model's performance, denoted as acc0. At the same time, the server retrieves the activation input vector $X = (x_1,x_2,\\ldots,x_z) \\in \\mathbb{R}^{z}$ for layer \u03c4, each $x_i$ represents the average activation input of the whole auxiliary dataset for the i-th neuron in layer \u03c4. Then the server sets a threshold \u03bb, and all $x_i$ that are not larger than \u03bb are collected into the set $\\mathbb{D}_{flip}$. The detailed process of selecting the best threshold \u03bb is described in the next subsection. Finally, the server flips $\\Delta w^{[i]}$ to $-\\Delta w^{[i]}$ and uses these flipped updates to update $W_o$, which can be formulated as follows:\n$$w^{[i]*}= \\begin{cases} w_o^{[i]}-\\Delta w^{[i]}, & \\text{if } x_i \\in \\mathbb{D}_{flip}, \\\\  w_o^{[i]}+\\Delta w^{[i]}, & \\text{else.}  \\end{cases}$$\nThe whole process of flipping is illustrated in the left side of Figure 1."}, {"title": "Adaptive Threshold", "content": "The threshold \u03bb proposed in the previous section highly determines the tradeoff between clearing backdoors and maintaining performance on clean data. To find out a better \u03bb automatically, we propose an adaptive threshold selection method.\nWhen firstly evaluating the fully-trained model on the auxiliary dataset, the server retrieves the average activation input vector $x = (x_1,x_2,\\ldots, x_z) \\in \\mathbb{R}^{z}$ for layer \u03c4 on the whole auxiliary dataset and identifies the minimum average activation value $\\mu \\in x$. Then the server sets an initial threshold \u03bb which is slightly larger than \u00b5 and does the flipping process as described in the previous subsection. After updating the weights of layer in the model from W to $W^* = (w^{[1]*}, w^{[2]*},\\ldots, w^{[z]*}) \\in \\mathbb{R}^{s \\times z}$, the server assesses the current model's performance acc1 on the auxiliary dataset $\\mathbb{4}$. If the performance gap $ \\epsilon = acc0 - acc1$ is less than a predetermined threshold \u03c1, the server increments \u03bb by a step size and flip the weight updates again. This procedure is repeated until $ \\epsilon $ exceeds the predefined threshold \u03c1, which means the performance drop exceeds the server's tolerance limit. The process of adaptively modifying the threshold is illustrated in the right side of Figure 1.\nThe details of the whole FLAIN method are summarized in Figure 1 and the pseudo codes are given in Algorithm 1."}, {"title": "Performance Evaluation", "content": "Experiment Setup\nTraining setup. For our experiments, we use the Adam optimizer (Kingma and Ba 2014) with a learning rate of 0.001 and betas set to (0.9, 0.999). Each training round consists of local training for 1 epoch with a batch size of 256, while both the client sampling rate and the global learning rate are set to 1.\nEvaluation Metrics. We utilize the following two metrics to assess the model's performance.\n\u2022 Attack Success Rate (ASR). ASR measures the proportion of trigger samples that the model correctly classifies as the target class. This metric indicates the effectiveness of the backdoor attack, reflecting how well the attack can manipulate the model to produce the desired output for malicious inputs.\n\u2022 Natural Accuracy (ACC). ACC quantifies the model's performance on untainted data, representing its effectiveness in standard classification tasks. This metric reflects how well the model generalizes to non-malicious inputs.\nDatasets and Models. In the experiments, we utilize various datasets, including MNIST (Deng 2012), Fashion-MNIST"}, {"title": "Experiment Results", "content": "We set step as 0.001 and \u03c1 no more than 0.03 for all settings. The auxiliary dataset $\\mathbb{4}$ is drawn from the validation dataset, including 600 images from CIFAR-10, and 200 images each from MNIST, EMNIST, and FMNIST. We apply FLAIN to the first fully connected layer following the convolutional layers, and conduct comparative experiments with Pruning\nand RLR (Ozdayi, Kantarcioglu, and Gel 2021). For fair comparison, we also focus on the the first fully connected layer following the convolutional layers for Pruning.\nAttack tasks under IID. We configure 10 clients with the MCR set to 40% and the poison data ratio (PDR) set to 30%. We define different attack tasks using the format (Ground-truth label, Target label). For this evaluation, the attack tasks are set to (0, 5), (0, 6), and (0, 7). As shown in Table 1, FLAIN, Pruning, and RLR effectively reduce the ASR to near 0 for certain tasks on MNIST, FMNIST, and EMNIST, with negligible impact on ACC. However, their performances vary significantly on CIFAR-10. Pruning seeks to enhance defense by removing additional neurons, resulting in a substantial decrease in ACC while the ASR remains elevated. RLR often applies a higher threshold to achieve a reduced ASR, leading to more dimensions receiving a negative learning rate, which negatively impacts model performance on the main task. In contrast, FLAIN consistently achieves an average ASR of 1.6% across the evaluated tasks, with only approximately 2% decrease in ACC.\nAttack tasks under non-IID. We use a Dirichlet distribution (\u03b1 = 0.5) to simulate non-IID data distribution scenarios commonly encountered in the real world, while maintaining all other settings consistent with IID conditions. Table 2 shows that under non-IID data distribution conditions, differences in data distribution result in more diverse client updates, which significantly undermines RLR's effectiveness in defending against attacks on FMNIST and CIFAR-10. FLAIN and Pruning implement post-training backdoor defenses, thus avoiding the impact of dynamically distinguishing between malicious and benign updates during training. Comparatively, FLAIN outperforms Pruning, maintaining higher ACC and achieving lower ASR in the evaluated tasks.\nDifferent pixel backdoor patterns. We train the model using 10 clients, with the MCR set to 40% and the PDR set to 50%, applying CBA to CIFAR-10. As shown in Figure 3 and Figure 4, we test different backdoor patterns by varying the number of trigger pixels (3, 5, 7, and 9) on FMNIST and CIFAR-10. The tasks are configured with FMNIST (1, 2)\nASR limited to approximately 0.8%.\nImpact of varying PDR. We configure 15 clients and set the MCR to 40%. The CBA pattern applies to CIFAR-10, specific attack tasks on FMNIST [(5, 8), (8, 5)] and CIFAR-10 [(4, 7), (7, 4)]. PDR is set to 20%, 30%, 40%, 50%, and 60%, respectively. Figure 6 demonstrates that as PDR increases, the average ACC on FMNIST remains stable around 90%, with ASR values approaching 0. For CIFAR-10, the average ACC decreases by approximately 2% compared to the no-Defense model, while the ASR remains around 1%. Across different PDR settings, FLAIN effectively keeps the ASR at a minimal level while preserving the model's performance on the main task.\nDiverse MCR settings. The experiment uses 10 and 20 clients with the PDR set to 30%. We explore the impact of different MCR settings by configuring MCR to 40%, 50%, and 60%. Specific attack tasks include: MNIST (3, 6), FMNIST (6, 8), EMNIST (10, 20), and CIFAR-10 (4, 5). Based on the detailed analysis in Tables 3 - 6, FLAIN, Pruning, and RLR exhibit robust defense capabilities across MNIST, FMNIST, and EMNIST datasets, even as the number of compromised clients increases. However, on CIFAR-10, the effectiveness of RLR's defense significantly declines as the MCR increases, particularly when the proportion of compromised clients reaches 50% and 60%. In these scenarios, the average ACC decreases by 13.5%, while the ASR remains approximately 68.5%. Pruning exhibits better performance in systems with 10 participating clients under different MCR settings, but it still results in an average ACC loss of 8.3% and maintains the ASR around 10%. In contrast, FLAIN demonstrates stable defense performance across various MCR settings, with the maximum reduction in ACC not exceeding 2% and the ASR kept below 2%."}, {"title": "Conclusion", "content": "In this parper, we introduce FLAIN, a backdoor defense method that flips the weight updates of low-activation input neurons. Given that the flipping process is highly sensitive to the threshold, we propose an adaptive threshold to automatically balance the tradeoff between eliminating backdoors and maintaining performance on clean data. The collaboration of flipping and the adaptive threshold supports the effectiveness of FLAIN. In evaluation tasks, FLAIN exhibits superior adaptability compared to direct pruning methods across a broader range of attack scenarios, including non-IID data distribution and high MCR settings. It consistently reduces backdoor efficacy while preserving the model's accuracy on clean data. Future work will explore further enhancements to FLAIN, including its application in data-free scenarios and more complex models."}]}