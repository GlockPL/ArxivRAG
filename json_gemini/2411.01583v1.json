{"title": "Trustworthy Federated Learning: Privacy, Security, and Beyond", "authors": ["Chunlu Chen", "Ji Liu", "Haowen Tan", "Xingjian Li", "Kevin I-Kai Wang", "Peng Li", "Kouichi Sakurai", "Dejing Dou"], "abstract": "While recent years have witnessed the advancement in big data and Artificial Intelligence (AI), it is of much importance to safeguard data privacy and security. As an innovative approach, Federated Learning (FL) addresses these concerns by facilitating collaborative model training across distributed data sources without transferring raw data. However, the challenges of robust security and privacy across decentralized networks catch significant attention in dealing with the distributed data in FL. In this paper, we conduct an extensive survey of the security and privacy issues prevalent in FL, underscoring the vulnerability of communication links and the potential for cyber threats. We delve into various defensive strategies to mitigate these risks, explore the applications of FL across different sectors, and propose research directions. We identify the intricate security challenges that arise within the FL frameworks, aiming to contribute to the development of secure and efficient FL systems.", "sections": [{"title": "1 Introduction", "content": "In recent years, rapid advancements in big data and Artificial Intelligence (AI) technologies have ushered in an era characterized by an unprecedented proliferation of interconnected Internet of Things (IoT) devices and web platforms. This digital tapestry, while instrumental in catalyzing the data revolution, concurrently yields vast quantities of distributed data a significant portion of which is sensitive in nature. Notably, there exists a gap in the adequate protection of this sensitive information, a critical oversight in the current data-centric world. The emergent challenges have not gone unnoticed at the legislative level. A myriad\n* Corresponding author.\n\u2020 Equal contribution.\n+ HiThink Research, Hangzhou, China.\n## Carnegie Mellon University, Pittsburgh, United States.\n\u00a7 Information Science and Electrical Engineering Department, Kyushu University, Fukuoka, Japan.\nElectrical, Computer and Software Engineering Department, The University of Auckland, Auckland, New Zealand.\n|| Computer Science and Engineering Department, The University of Aizu, Aizuwakamatsu, Japan.\n** Information Science and Technology Department, Zhejiang Sci-Tech University, Hangzhou, China.\n\u2020\u2020 Fudan University, Shanghai and BEDI Cloud, Beijing, China."}, {"title": "2 Overview of Federated Learning", "content": "In this section, we introduce the basic concepts of DML and FL. Then, we discuss the differences between FL and DML. Afterward, we explain the general FL framework architecture."}, {"title": "2.1 Federated Learning", "content": "Machine learning systems are traditionally reliant on aggregating raw data to a centralized server for model training. A large dataset generally corresponds to high accuracy of a trained model. However, computational bottlenecks emerge when handling extensive data. DML offers a countermeasure to this issue by implementing parallelization and concurrent execution across multiple processing units, including CPUs, GPUs, and TPUs. This not only improves efficiency but also enhances scalability. DML systems can be structured either in a distributed or decentralized fashion. The former utilizes central servers for data interaction and device synchronization, while the latter emphasizes peer-to-peer interactions and node equality."}, {"title": "2.2 From DML to FL", "content": "FL is an offshoot of DML that facilitates collaborative learning without necessitating raw data sharing, thus making significant strides in security and efficiency [117, 190]. This development is particularly pertinent with the burgeoning trend of IoT and AI technologies. In this section, we investigate the distinct features of DML and FL across four key dimensions: training data, system workflow, security, and fault tolerance.\nTraining Data\nIn FL, due to the heterogeneity of participating devices [31, 82, 113, 118], data distribution and volume can demonstrate remarkable variances among devices. Such diversity leads to a non-Independent and Identically Distributed (Non-IID) distribution in the training data. Additionally, the volume of data on each individual device tends to be asymmetrical, which lead to greater diversity [80]. Notably, the processing of data occurs directly on these devices, circumventing the requirement for a distinct data management server. In contrast, DML assumes that the training data distribution of each worker is a random sample procured from the entire dataset [212]. This necessitates a data management server for data collection, feature engineering, and dataset partitioning tasks.\nSystem Workflow\nThe conventional DML workflow comprises four main stages: initiation, training, evaluation, and deployment [212]. The initiation stage involves data preprocessing and model initialization, tailored to specific problems, with techniques like Logistic Regression (LR), Support Vector Machine (SVM), and Neural Networks (NN) being predominantly used for classification tasks. The training phase utilizes the curated data to train the model for the specific task. Thereafter, the model undergoes evaluation using test data to gauge its performance and to ascertain its effectiveness. Once the model is approved in this evaluation phase, it gets deployed into the production environment. FL workflow mirrors that of DML to a large extent, but deviates notably in the initiation phase. In particular, FL forgoes data preprocessing at a central location [243]. The distributed nature of training data across client devices allows each client to undertake independent data preprocessing based on the demands of tasks. The central server has no access to this data, ensuring that data owners retain full control over their devices and data. In contrast, DML provides the central server with comprehensive control over both the training data and the workers.\nSecurity\nDML poses a higher risk of data leakage as data and model parameters are disseminated to workers via a communication network. Although encryption techniques can attenuate"}, {"title": "2.3 System Architecture of FL", "content": "This section aims to present a comprehensive examination of prevalent open source FL system architectures, encompassing TensorFlow Federated (TFF) [62], PySyft [164], FedML [72], Federated AI Technology Enabler (FATE) [215], PaddleFL [11], and Rosetta [143]. In light of this review, we propose a generalized architecture for FL systems, as depicted"}, {"title": "3 Security and Privacy of FL", "content": "In the realm of DML, robustness denotes the capacity of system to efficiently counteract or mitigate security threats. FL, characterized by its reliance on distributed devices, enables cooperative model training without necessitating the sharing of raw data. The distributed architecture intrinsic to FL intensifies the intricacy of detecting vulnerabilities and potential adversarial interventions [87]. The threats to the integrity of an FL system can be principally classified into two categories, i.e., non-malicious failures and malicious attacks. Non-malicious failures are inadvertent system impairments emanating from inherent vulnerabilities. Examples encompass device malfunctions, the introduction of overly noisy training datasets, and unpredictable participant behaviors. Malicious attacks are deliberate infringements orchestrated by adversaries aiming to compromise the FL system. Such attacks encompass data and model poisoning, adversarial manipulations, and inference-based assaults. While tailored strategies can be designed in advance to counteract malicious incursions, non-malicious setbacks often manifest unpredictably. Remediation for the latter is frequently reactive, necessitating post-incident solution formulations. To bolster the robustness of FL frameworks, it is imperative to meticulously assess the spectrum of potential security breaches and to proactively design and integrate mechanisms that can deter or neutralize them. Subsequently, we delve into a comprehensive discourse on the multifaceted security and privacy concerns inherent to FL architectures."}, {"title": "3.1 Non-Malicious Failures on FL", "content": "FL systems, despite their distributed promise, grapple with vulnerabilities stemming from both hardware and systemic discrepancies [172]. At the hardware echelon, failures often manifest as a consequence of infrastructural malfunctions. Such malfunctions can be attributed to a myriad of sources, from subpar equipment quality to exogenous factors including environmental calamities or inadequately maintained apparatus.\nThese inherent system susceptibilities serve as potential gateways for adversaries, enabling them to leverage exploits ranging from malicious code injections to Distributed Denial of Service (DoS) offensives [14]. Compounding the intricacies are the challenges of"}, {"title": "3.1.1 Risk Management", "content": "Risk management within computational domains encapsulates a triad of cardinal components: assessment, control, and surveillance [203]. Assessment: This phase is devoted to the recognition and quantification of potential risks, encompassing both system vulnerabilities and adversarial intrusions. It emphasizes discerning the nature of these threats and prognosticating their potential ramifications on the computational infrastructure. Control: Rooted in the analytics of the assessment phase, control prioritizes the deployment of mechanisms to alleviate identified threats. Its objectives are manifold: to meticulously scrutinize potential threats, curtail prospective damages, and to manifest robust security protocols. Surveillance: This perpetual process underscores real-time oversight and feedback mechanisms to gauge the efficacy of the implemented security apparatus.\nWithin the ambit of FL, preempting and mitigating risks are of paramount importance to uphold an impeccable security standard [172]. Two salient strategies emerge to fortify security. Environment Fortification: The inception of a fortified computational milieu, exemplified by a Trusted Execution Environment (TEE) [76], offers a bulwark against many extant risks. Dynamic Risk Surveillance: Constructing a dedicated surveillance module can yield dividends in proactively identifying and mitigating security anomalies. Such a module functions ceaselessly, scrutinizing the FL framework for aberrations, thereby enabling the formulation and deployment of nimble preventive stratagems.\nTEE emerges as a hardware-centric approach to privacy-preserving computations. This solution empowers remote users to execute computational tasks on a device, effectively cloaking the intricacies of the computations from the hardware fabricator. Leveraging specialized CPU registers and ensuring memory isolation or encryption, TEE fosters a sanctuary for secure computations [76]. As the trustworthiness of the TEE is ascertained, the platform extrapolates a cryptographically secure interaction milieu between devices, potentially amplifying the efficacy of collaborative endeavors like FL [39, 149]. Within the FL context, TEEs are perceived as potent aggregation centers, streamlining parameter consolidation [251]. A nascent exploration into blockchain-augmented TEEs hints at an inviolable approach, rendering local model manipulations futile [89]. Additionally, the integration of a TEE-oriented proxy component can fortify the confidentiality of FL participants, ensuring the integrity of updates dispatched to servers [20]. Notwithstanding its commendable efficacy, the resilience of the TEE is tethered to its foundational hardware schema [253], rendering it potentially susceptible to specific adversarial interventions, such as data poisoning exploits [151].\nThe overarching paradigm of risk surveillance and governance encompasses meticulous oversight of the entire training lifecycle, spanning risk detection, quantification, assessment, and redressal. The quintessential objective herein is to refine the security stratagem and ascertain alignment with stipulated security benchmarks. Through real-time oversight, potential adversarial vectors and insufficiencies in security postures are unveiled, thus paving the way for timely recalibration of protective measures."}, {"title": "3.2 Malicious Attacks on FL", "content": "Within the expansive landscape of computational systems, certain vulnerabilities possess the propensity to critically perturb both the functional and computational efficiency of a system [153]. Astute adversaries, having discerned these susceptibilities, are equipped to orchestrate intricate intrusions that imperil system integrity [17, 102, 154, 247]. Compounding these complexities is the presence of malevolent participants, who, embedded within the network of the system, can initiate a myriad of sophisticated threats [104, 105]. These range from poisoning offenses [9] and Byzantine stratagems [35, 194], to intricate inference attacks [190].\nIn the ensuing discourse, we delve into a meticulous examination of prevalent adversarial modalities afflicting FL architectures: poisoning incursions, adversarial maneuvers, and inference transgressions. Please note that the onslaughts of both poisoning and adversarial character predominantly manifest during the intricate training phase of FL."}, {"title": "3.2.1 Poisoning Attacks in FL", "content": "Poisoning attacks, insidious by nature, are orchestrated through the intentional adulteration of training datasets, aiming predominantly at the degradation of model efficacy. A dichotomous classification bifurcates these attacks into \"data poisoning\u201d and \u201cmodel poisoning\" [213].\nData Poisoning: This strand comprises two predominant subsets. The first, \u201clabel-flip\u201c attacks, involves the surreptitious manipulation of label metadata within the training set, inducing significant deviations in model targets and consequent attenuation of accuracy [224]. The latter, termed \"clean-label\" attacks, are characterized by the subtle modification of the training dataset, or the infusion of strategically erroneous data, inevitably leading to the degradation of model precision.\nModel Poisoning: This paradigm hinges on the direct manipulation of model parameters or its architecture. Canonical examples include backdoor attacks, wherein the global performance of the model remains ostensibly unaffected, while yielding skewed results for specific input domains [33, 64, 188].\nFL is acutely susceptible to poisoning attacks, given its unique architectural tenets. The inherent data heterogeneity in FL, often manifesting as non-Independent and Identically Distributed (non-IID) data, culminates in localized model variations across devices [50, 222, 229]. Further exacerbating the vulnerability is the systemic design where the central server remains detached from the granularities of the training process, rendering the validation of device-originated updates a formidable challenge [54, 246]. In addition, recent works have presented federated unlearning [32] as a solution to the challenges posed by data heterogeneity in FL [208]. Through the targeted removal of particular local data, these methods interfere with the specific knowledge that the system derives from such data,"}, {"title": "3.2.2 Adversarial Attacks in FL", "content": "Adversarial attacks, characterized by the strategic infusion of minuscule alterations within training datasets, are tailored to mislead target models [225]. Remarkably, these diminutive alterations can precipitate significant aberrations, including misclassifications. Grounded on the extent of model knowledge available to the attacker, adversarial attacks bifurcate into white-box, gray-box, and black-box categories.\nWhite-box Attacks: Predicated on the presumption that the attacker has comprehensive knowledge about the architecture and parameters of the target model, these attacks fabricate adversarial examples to directly misguide the model. Seminal algorithms, such as the BFGS attack [202], ascertain misclassifications by discerning the minimal loss function perturbations. Techniques like FGSM [61] and its iterative counterpart, I-FGSM [93], employ gradient step computations to generate these adversarial exemplars. The DeepFool algorithm [152] and the JSMA method [167] further delineate efficient adversarial example creation by computing minimal requisite perturbations and exploiting forward propagation derivatives, respectively."}, {"title": "3.2.3 Inference Attacks in FL", "content": "Inference attacks in machine learning pivot on exploiting the model to unveil obscured training data or discern sensitive attributes. Predicated on the kind of information being pursued, these attacks bifurcate into property inference attacks and membership inference attacks.\nProperty Inference Attacks: These attacks endeavor to deduce concealed or fragmented attributes leveraging overtly available properties or data distributions [141]. As an exemplar, within a recommendation system paradigm, a malevolent entity could discern pivotal attributes such as age, gender, or even deeper personal nuances based on the evident model outputs like frequent purchase patterns or items earmarked as intriguing.\nMembership Inference Attacks: Operating on the juxtaposition of the target model and a data exemplar, these attacks endeavor to ascertain the affiliation of the sample with the training set of the model, probing if it was pivotal during the training epoch [206].\nFL manifests as a decentralized learning paradigm where clients retain their data and disseminate specific parameters, predominantly gradients, to a centralized entity for synergistic model training. Notwithstanding its decentralized veneer, FL is inherently susceptible to nefarious incursions, particularly from malicious system participants. These malevolent entities may be embodied as either a client or a server."}, {"title": "3.3 Defensive Paradigms in FL Systems: Approaches and Classifications", "content": "In the intricate landscape of FL systems, malevolent incursions manifest in multifarious guises, necessitating a diverse repertoire of defense mechanisms. Distinct defense stratagems are earmarked based on the potential location of the attack, i.e., on device or server side, while overarching security measures often pivot on systemic robustness [137]. The threat landscape in FL encompasses potential adversaries domiciled either within client devices or centralized servers.\nA salient stratagem to bolster security, especially against compromised servers, amalgamates the principles of distributed architectures combined with advanced encryption methodologies. From a broader perspective, defense mechanisms in FL can be dichotomized into two predominant categories: proactive and reactive defense paradigms [154]. Proactive Defense: As the nomenclature suggests, proactive defense initiatives prognosticate potential threat vectors, orchestrating and deploying defense mechanisms a priori to forestall the actualization of anticipated attacks. Reactive Defense: Reactive defense mechanisms are reactionary, coming to the fore post facto, in the aftermath of an attack detection, working towards mitigation and remediation."}, {"title": "3.3.1 Countermeasures for Poisoning Attacks", "content": "FL systems have witnessed the proliferation of diverse poisoning attacks during the training phase, prompting the genesis of specialized countermeasures. This discourse delineates these defensive mechanisms, specifically addressing data and model poisoning attacks. A categorization complemented by paradigmatic defenses against these attacks, informed by extant literature, is encapsulated in Table 2.\nDefensive Strategies for Data Poisoning Attacks: The fulcrum of defenses against data poisoning pivots on two crucial pillars: the reliability and integrity of the training data [60]. Reliability underscores the authenticity of training data, and integrity ensures alignment with global data distributions and prescribed formats. Notably, defense measures are discerned from a data-centric protective lens. Methods like random sampling of training data [16, 65] and anomaly detection have demonstrated efficacy. However, in a multi-device FL milieu, challenges persist, particularly in the selection of honest computing nodes. Techniques ranging from rule-based [252, 254] to sampling-based data selection [252, 255], and data purification [28, 211, 221] have emerged in response. The AUROR framework, utilizing the Euclidean distance metric, exemplifies how poisoned data can be identified and culled [191]. Nonetheless, these methodologies are not bereft of challenges, with the pursuit of an optimal balance between performance and security remaining elusive [60]."}, {"title": "3.3.2 Countermeasures Against Adversarial Attacks", "content": "In FL, adversarial attack defenses can strengthen the robustness of the system by incorporating more data samples, e.g., adversarial training [226], data augmentation or compression [186]. In addition, the defense capability of the system can also be improved by anomaly detection, e.g., anomaly client detection [41], adversarial examples detection [110]. In general, defense methods can be divided into complete defense and detection-only defense [2]. A summarization of prevalent defense techniques is available in Table 3.\nComplete Defense Mechanisms: Complete defense mechanisms aim to mitigate the influences of adversarial examples by ensuring accurate classification during the training phase. These countermeasures encompass strategies such as:\nIntroducing an attack monitoring module for continuous vigilance and iterative model refinement [198].\nDeploying knowledge distillation for accurate classification of adversarial samples via teacher-student model configurations [115, 210, 264].\nEmploying data augmentation techniques, wherein filters such as noise addition are integrated to restrain inaccuracies instigated by adversarial inputs [63, 110].\nUtilizing the diffusion model, where a clean datum undergoes controlled contamination and subsequent iterative noise removal, offering a dual-process defense against adversarial samples [43, 211, 221, 234]. However, this approach comes with computational complexity, impinging on training efficiency [161]."}, {"title": "3.3.3 Countermeasures Against Inference Attacks", "content": "The intricacies of FL systems include safeguarding a spectrum of sensitive information: training data, model constituents (algorithms and parameters), and resultant outputs. A breach in any of these facets can profoundly undermine the integrity of the system [106, 262]. The plausible incorporation of malevolent entities further accentuates the vulnerability to information exploits [154]. A synthesized overview of prominent defense strategies against inference attacks is delineated in Table 4. Subsequently, we delve into the nuances of these defense paradigms.\nEncryption-centered Techniques: Integral techniques like HE and Secret Sharing (SS) stand at the forefront of encryption practices. The prowess of HE lies in its ability to conduct operations on encrypted data directly, ensuring privacy throughout the computational process [53, 71]. SS, on the other hand, involves the dissemination of key shares across multiple entities, enabling decryption only by authorized coalitions [18]. Yet, the dual adoption of encryption algorithms, while enhancing data sanctity, can inadvertently increase the computational and communicational overheads in FL systems. The quintessential challenge manifests as striking a balance between maintaining efficient training and assuring model accuracy [53, 138, 244]. The encoder-decoder architecture offers a potential solution by allowing devices to perform data encryption and servers to undertake decryption [146].\nObfuscation-centered Techniques: DP remains a pivotal technique, adding deliberate obfuscation to the data or specific features, thereby ensuring that third-party entities are unable to distill individual raw data from device-transferred information [6, 81, 227]. Current obfuscation techniques predominantly involve the infusion of Laplacian or Gaussian noise [90, 109, 216]. Augmenting training data or manipulating label information further intensifies data obscurity [95, 130]. Nonetheless, the dilemma persists: excessive noise infusion may degrade model precision, while sparse noise addition might inadvertently expose training data [59, 179].\nHybrid Techniques: Considering the constraints intrinsic to pure encryption or obfuscation techniques, hybrid methods like MPC emerge as a promising alternative [169]. Contemporary MPC protocols, coupled with DP technologies, aim to both preserve data privacy and curtail communicational overheads [70]. Modulated noise injection further refines model accuracy [205]."}, {"title": "4 Application of FL", "content": "Users and policymakers are increasingly aware of the importance of data security and privacy within FL systems. This has led to a surge in research on privacy-protection measures, and data access is being scrutinized more closely.\nAs FL continues to evolve and be adopted across various industries, it is important to recognize and address the potential vulnerabilities and risks inherent in these applications. By doing so, we can design more robust and secure FL systems that can withstand specific attacks targeting these applications. FL is currently applied in various areas, such as healthcare [88, 133, 180], IoT [159], autonomous vehicles [135, 242], finance [187], wireless technology [150], and recommendation systems [260]."}, {"title": "4.1 Healthcare", "content": "In healthcare, FL is used to allow medical institutions to train models independently using their local data, ensuring patient privacy [99, 156]. However, a major vulnerability arises due to the sensitive nature of healthcare data. An attack could potentially disrupt the learning process, inject malicious models, or leak sensitive patient information [231]. Techniques such as DP and MPC can help mitigate these risks, but they need to be carefully incorporated to maintain the balance between privacy and model performance [142]."}, {"title": "4.2 Finance", "content": "In the finance industry, FL allows institutions to share insights without disclosing sensitive data, thus maintaining privacy [187]. Yet, FL in finance also presents an attractive target for adversarial attacks aiming to manipulate the consensus model for illicit financial gain, or to compromise financial data confidentiality [13, 78]. Techniques such as robust aggregation and Byzantine fault tolerance can help build resilience against these attacks, but more research is needed to optimize these defenses for finance-specific scenarios [23]."}, {"title": "4.3 Wireless Communications", "content": "FL in wireless communications technology aims to preserve data privacy by training machine learning models in a decentralized manner [150]. However, this decentralization can also introduce vulnerabilities as attackers may exploit insecure communication channels to extract sensitive information or inject adversarial models [36, 52]. Additionally, jamming attacks can pose a serious threat to the stability and security of wireless communication networks, especially in dense network environments where attackers can use jamming signals to interfere with data transmission, reducing the effectiveness of FL model training and prediction accuracy [74]. Solutions such as secure communication protocols and robust FL algorithms are crucial, but challenges remain in designing these solutions to be both effective and efficient in the wireless environment [128, 162]."}, {"title": "4.4 Smart Transportation", "content": "The application of FL in smart transportation presents unique challenges [184]. For instance, attackers may target the decentralized nature of the data, aiming to compromise data related to transportation systems or manipulate the consensus model, thereby causing safety hazards. Furthermore, the real-time and mobile nature of smart transportation systems imposes additional constraints on defenses [34, 171]. Techniques such as on-the-fly data encryption and real-time anomaly detection can help secure FL in this domain, but their effectiveness and impact on system performance need further investigation."}, {"title": "4.5 Recommendation Systems", "content": "Recommendation systems greatly benefit from FL as it allows for personalized recommendations while protecting user data [5, 220]. However, the personalized nature of these systems also creates potential vulnerabilities [158]. An attacker might infer sensitive user information from model updates, or manipulate the learning process to affect the recommendations. Privacy-enhancing techniques such as DP and HE can provide protection, but how to effectively integrate these techniques in FL-based recommendation systems is an ongoing research topic [108, 111, 112]."}, {"title": "4.6 Smart Cities", "content": "Smart cities, encompassing traffic management, environmental monitoring, and utility services, have become an emerging application for FL. The advantage of using FL lies in its ability to process and learn from vast amounts of data generated by various sensors and devices in the city while preserving privacy [68, 83]. However, this interconnectivity and heterogeneity can expose the system to targeted attacks such as data poisoning or model inversion attacks [3]. Ensuring data integrity, enforcing MPC protocols, and integrating privacy-preserving techniques like DP are vital for mitigating these security risks. In addition, recent research has enhanced system resilience against local model manipulation by integrating blockchain technology, significantly bolstering the capabilities of intrusion detection systems to defend against sophisticated attacks [51]."}, {"title": "4.7 Generative AI in FL", "content": "The rise of generative AI, especially Generative Adversarial Networks (GANs), offers new possibilities in enhancing the defense mechanisms within FL systems. GANs, by their design, can play a pivotal role in identifying and mitigating data poisoning attacks, a significant threat to FL environments [173]. For example, GANs can be used to generate synthetic data that mimics the characteristics of poisoned datasets, which then helps in training FL models to recognize and reject malicious inputs effectively [94]. By incorporating GAN-based strategies, researchers can advance the security measures necessary to protect decentralized learning processes from evolving threats [248]."}, {"title": "5 Open Challenges and Future Directions", "content": "FL is a distributed learning technology that provides privacy protection. Despite FL enabling devices to train a model without sharing raw data, several security and privacy concerns persist. This section explores various challenges and future directions of FL systems."}, {"title": "5.1 Reliability and Security of Big Models", "content": "FL systems face unique challenges related to the size and complexity of deployed models. The number and capacity of devices involved in FL systems can limit the application of large-scale models [181]. As computational resources and data volume increase, models scale up, often necessitating a larger amount of raw data for training. Consequently, this leads to an increase in the number of model parameters and a more complex structure [240]. Examples of such large models include DALL-E [177] and DALL-E2 [176] for text-to-image generation, and pre-trained language models such as Bert [44], GPT-3 [21], and XLNet [236] designed for NLP tasks. The WuDao model [239], boasting 1.75 trillion parameters, is an example of scaling up parameters to enhance performance across a broad spectrum of machine learning problems [127]. Large models, despite their performance advantages, introduce a higher degree of vulnerability [29]. For instance, large datasets can obscure maliciously introduced poisoning data, and existing defense methods might encounter scalability problems when dealing with larger models [240]. Furthermore, these models are more likely to inadvertently retain sensitive raw data [27, 204], posing considerable privacy risks to the FL system [26]. Attacks such as inference attacks or data reconstruction attacks present severe privacy risks to large models [30, 125, 240].\nGiven these risks, it becomes imperative to thoroughly understand the security and privacy implications of large-scale models within FL systems. Furthermore, current defense methods must be critically evaluated and developed further to adequately address the risks associated with these large models. While designing these defense methods, a crucial consideration is to maintain the accuracy of the models. This balance can be achieved by integrating privacy-preserving techniques such as DP or HE in the learning process, adopting robust aggregation methods to limit the impact of malicious updates, and employing model pruning or compression strategies to maintain model accuracy while reducing model size and complexity. Ultimately, our goal is to devise defense strategies that effectively address the security and privacy concerns of large-scale models within FL systems, without compromising their accuracy."}, {"title": "5.2 Dynamic Adaptive Defense Techniques", "content": "FL frameworks enable collaborative training across multiple institutions while ensuring privacy, data security, and compliance with regulatory requirements. These systems involve a wide array of technologies, including infrastructure frameworks, data distribution and storage, algorithms, communication, and deployment. However, storing training data locally in FL systems presents a significant risk of data leakage or exploitation by malicious entities. Moreover, the heterogeneity of the data can make it challenging to identify such attacks. Additionally, vulnerabilities in system environments, such as clusters or cloud platforms, may expose FL systems to security threats like Denial of Service (DoS) attacks and unauthorized"}, {"title": "5.3 Lightweight Cryptography", "content": "FL offers a unique framework for institutions to collectively train a model without needing to share data directly, ensuring compliance with legal and regulatory requirements. However, existing FL approaches are not exempt from various security and privacy concerns. A malicious participant could potentially extract sensitive information from the training results, which could have an adverse effect on specific targets. Similarly, malicious servers may infer training data from client updates, which could compromise the integrity of the global model. Current solutions aim to enhance privacy by integrating techniques such as DP, MPC, or HE [59, 200, 216]. Of these, MPC is growing in acceptance as a method to boost model performance while maintaining security [232]. In addition, the emergence of blockchain technology has supplemented FL by offering a decentralized, traceable, and recordable solution. Despite their benefits, these measures can potentially reduce model accuracy or efficiency. For instance, MPC can be resource-intensive, pushing participants to lower the security level to decrease data transmission costs and improve training effectiveness [38, 91]. In this context, \"low loss\" refers to minimizing any reduction in model performance, including factors like accuracy, precision, and recall, when incorporating cryptographic techniques. The term \"high efficiency\" is defined as the capability to perform computations and data transmission swiftly with minimal resource usage. Considering these definitions, the need arises for robust, privacy-preserving, and lightweight technologies that can ensure minimal loss in model performance and high computational and communicational efficiency. By developing and implementing such techniques, we can achieve a better balance between security, privacy, and performance in FL systems."}, {"title": "5.4 Performance Efficiency", "content": "In FL systems, cryptographic technologies are often used to enhance security and maintain user privacy. Nevertheless, the inherent complexity of these technologies can compromise"}, {"title": "6 Conclusion", "content": "In this paper, we provide an in-depth analysis of the current state of security and privacy in FL systems, and we explore the core functionalities and architectures of these systems.\nFirst, we outline FL systems and their distinctions from traditional DML, and review the typical architecture layers of FL systems: user services, algorithms, and infrastructure.\nNext, we discuss the security and privacy challenges faced at each architectural layer of FL systems, identifying potential threats, including non-malicious failures and malicious attacks, along with a overview of possible attack methods. To address these threats, we analyze various existing defense mechanisms and emphasize the importance of building a robust risk management framework. This framework integrates evaluation, control, and monitoring phases to proactively identify and mitigate potential vulnerabilities within the system, thereby enhancing overall system resilience.\nFinally, we examine the diverse practical applications of FL, focusing on its use across multiple industries and the associated security and privacy challenges. These include applications in healthcare, finance, wireless communications, autonomous vehicles, recommendation systems, and smart city infrastructure. We also identify future research directions, such as the reliability and security of large models, Dynamic Adaptive Defense Techniques, system fault tolerance, and balancing privacy with model performance."}]}