{"title": "Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs", "authors": ["Yilun Hua", "Yoav Artzi"], "abstract": "Humans spontaneously use increasingly efficient language as interactions progress, by adapting and forming ad-hoc conventions. This phenomenon has been studied extensively using reference games, showing properties of human language that go beyond relaying intents. It remains unexplored whether multimodal large language models (MLLMs) similarly increase communication efficiency during interactions, and what mechanisms they may adopt for this purpose. We introduce ICCA, an automated framework to evaluate such conversational adaptation as an in-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and observe that while they may understand the increasingly efficient language of their interlocutor, they do not spontaneously make their own language more efficient over time. This latter ability can only be elicited in some models (e.g., GPT-4) with heavy-handed prompting. This shows that this property of linguistic interaction does not arise from current training regimes, even though it is a common hallmark of human language.", "sections": [{"title": "Introduction", "content": "Human interlocutors adapt to each other during interactions, developing increasingly efficient ways to refer to concepts and objects. Hawkins et al. (2020b) exemplify this via communication between a nurse and a bed-ridden patient at home. Initially, the patient may refer to a medicine with the medicine for my back pain in a small blue medicine bottle ..., but after a week of care, they are likely to just ask for their back meds. This increase in efficiency relies on the interlocutors forming ad-hoc linguistic conventions: the mutually understood, concise phrases to communicate referential content. This phenomenon has been repeatedly observed and characterized in controlled studies using repeated reference games (Figure 1; e.g., Krauss & Weinheimer, 1964; Brennan & Clark, 1996; Hawkins et al., 2020a).\nWe study this ability in multimodal large language models (MLLMs). LLMs and MLLMs are well positioned to acquire this behavior and display it spontaneously in interactions. They are trained on large amounts of human language data, in which this behavior is common and the history of an ongoing interaction is often retained, thereby explicitly keeping the information needed at hand. Beyond the scientific question, such ad-hoc adaptation has significant application impacts: enabling more natural interactions, reducing the costs involved in conversations (e.g., using shorter utterances to communicate the same amount of information), and increasing the accuracy of relaying intent.\nWe propose ICCA,\u00b9 an automated framework to evaluate and characterize the ability of models to form ad-hoc conventions. ICCA uses a corpus of human-human reference game interactions, allowing for completely automated evaluation, which does not require further human interaction, making it easy to deploy for the analysis of new models. The interaction follows the standard repeated reference game setup (Clark & Wilkes-Gibbs, 1986), where a speaker refers to an image within a shared context of images, and a listener resolves the"}, {"title": "Background and Related Work", "content": "Repeated Reference Games A reference game is an interaction where a speaker and a listener (i.e., a dyad) interact over a shared context. The shared context is a set of images. The speaker describes a target image. The target designation is only revealed to the speaker. The listener has to select an image following the speaker's description. Each participant sees the images in a different order, so they cannot use position information to communicate the referent. Reference games have been used extensively in the study of computational models, including recently to evaluate visual abstraction (Ji et al., 2022) and conversational aptitude (Chalamalasetti et al., 2023).\nA repeated reference game (Figure 1) includes multiple repetitions. Each repetition has one trial for each image in the shared context. The listener receives feedback after every"}, {"title": "The ICCA Framework", "content": "ICCA uses a dataset of human-human interactions, and allows to easily customize different parts of the interaction. This flexibility enables different research questions. For example, in Section 5, we customize the interaction structure to analyze how well models handle long interactions with multiple images interleaved in them. ICCA supports studies with the model acting either as speaker or listener, and includes several metrics to track different properties of adaptation during the interaction.\nICCA is fully automated and easily applicable to new MLLMs. Our design does not require collecting new data or human studies, but instead uses Hawkins et al. (2020b)'s human-human interaction data to simulate a human interacting with an MLLM. Each"}, {"title": "Model-as-speaker Experiments", "content": "We study model behavior as speaker with five state-of-the-art vision MLLMs: IDEFICS-80b-instruct, LLaVa-1.5-13b, GPT4-vision, Gemini 1.0 Pro Vision, and Claude 3 opus. Throughout all speaker experiments, we customize the data to only show the referential context once at the beginning of the interaction, so there is no shuffling of context through-out the interaction. We engineer prompts for each model individually to best evaluate its capability. We use GPT4 as the listener. It exhibits high performance in our listener experiments (Section 5), especially when the context appears only once at the beginning,"}, {"title": "Model-as-listener Experiments", "content": "Listener experiments follow a setup similar to the speaker experiments as far as models and prompt optimization (Section 4). Gemini, LLaVa, and Claude cap the number of input images, limiting their use in some of our listener variants. Overall, we design four main variants, each implemented through the pre-processing function F. Our design process is iterative, with some variants designed based on the behavior observed with earlier ones. Throughout the listener variants, we keep the instruction I largely constant and about the role of the listener. We vary how we display the referential context.\nThe listener action space is more limited, simply requiring the model to select the referenced image. We focus on evaluating model accuracy, similar to how listener behavior is charac-terized in human studies. Figure 3 visualizes the behaviors we observe. We also include human listener accuracy trends as reference to model accuracies.\nThe starting point for the listener study is the standard reference game setup (Section 2):\nL1: Standard Listener Images are shuffled and re-displayed for each trial, so each image will potentially have a new label relative to previous trials.\nL1 requires a growing number of images in the prompt as the interaction progresses. With six repetitions of four trials and a context of four images, the maximum number of images in the prompt at the end of the interaction is 96. Gemini, LLaVa, and Claude can take at most 16, 4, and 20 images, so we only use GPT4 and IDEFICS with L1.\nWe expect an effective model to exploit the conversation history to reason about the human speaker's conventionalized ways of speaking. Even if the model starts with low accuracy, it has the opportunity to improve because the prompt at later stages includes feedback for its choices, and as the messages conventionalize, later messages for an image are often exact repetitions, albeit with the referential context shuffled.\nBoth GPT4 and IDEFICS do significantly worse than humans (Figure 3, left). As expected, humans demonstrate strong performance to start with, and show an upward trend in ac-curacy, as the interlocutors adapt to each other. GPT4 is significantly worse than humans, though performing fairly well. It shows a marginal improvement trend (88.9%\u219292.5% in repetition 5), but it is not significant, and weakens in the last repetition (91.2%). IDEFICS is much worse immediately in the beginning (46.8%), and rather than improving, its per-formance deteriorates as the interaction progresses, reaching random chance in the later trials. This happens even though it is receiving an increasing amount of information that should allow it to improve its performance. A possible cause for this trend is the dramatic increase in the prompt size, especially as more and more images are added, as the interaction progresses. We further discuss this issue and its potential causes in Section 6."}, {"title": "History and Context Impact", "content": "Following the observations with L1, we design three variants with simplified referential contexts and history to better understand how well the models handle the interaction history and the referential context:\nL2: No History Each of the 24 trials is given to the model in isolation, without any history. The model input includes the context of four images and the speaker utterance, as well as the basic game instruction. This variant reveals the extent to which the model can reason about the ad-hoc conventions formed in repeated human-human interaction without access to the history in which they were formed.\nL3: Images Once A potential challenge of L1 is the large number of images in the prompt. A model's architecture or training may not be suitable for handling a large number of images, and LLM prompt length is known to adversely influence"}, {"title": "Discussion", "content": "Our studies point to various issues that likely hinder specific models from displaying communication efficiency gains, and point out directions for future works.\nTendency to Repeat Messages In the speaker study, IDEFICS and LLaVa tend to repeat the first message they use for each image, showing no adaptation. To further study how much these models prefer patterns of repetition, we design a test that uses these models for language modeling rather than text generation. We construct two transcripts for each of the 54 human-human interactions in our original dataset. One is the original transcript from human-human interactions, showing the natural evolution of messages. The other is a manipulated transcript where the speaker repeats the messages from Repetition 1 in all the later repetitions. We calculate the log probability and perplexity IDEFICS and LLaVa assign to these transcripts, count the number of times one type of transcript has better log-probability/perplexity than the other, and apply a sign test. We find that the manipulated transcript showing message repetitions consistently receives higher log probability and lower perplexity for all 54 interactions, showing the models' significant tendency towards repeated patterns (sign test p-values are near zero). Unfortunately, this experiment cannot be done with GPT4, Gemini, or Claude due to API limitations.\nLexical Efficiency \u2260 Communication Efficiency The convergence of human speaker mes-sages for a particular image to a short, stable convention often takes the form of extracting salient tokens from the previous message and sticking to the same message once it becomes very short (Hawkins et al., 2020a). Unless directly instructed to do so through a highly engineered prompt (S4), GPT4, Gemini, and Claude often introduce new words when shortening their messages or even when the messages cannot be further shortened, as shown in the S3 explicit instruction variant. Such inconsistency with human behaviors is problematic. When messages for the same image do not converge, no conventions can form and the listener will likely need additional cognitive effort to process the previously unseen words. Intuitively, even if a new message is semantically similar to a previous one by using synonyms, resolving it still likely entail a greater cognitive load than an exact repetition. Moreover, when new words describing a new aspect of an image are introduced after a few rounds of relatively similar messages, they violate the human listener's expectation, potentially leading to miscommunication and slower response (Metzing & Brennan, 2003).\nPerformance Degradation with Many-image Inputs Among the models that support a large number of images, IDEFICS performs much worse as the number of images increases. Even though the images in L4 are not shuffled across trials, which could have allowed the model to exploit label-message associations as an efficient way to gain high accuracy, the model still had much lower accuracy than when the images only appear once (L3) (Figure 3). When the history contains a growing number of images that are shuffled between trials (L1), IDEFICS shows an even worse accuracy trend.\nA likely hypothesis is that a greater number of images creates challenges for capturing the dependency between specific visual input and textual cues, which can manifest as failures to associate an image's label with the actual content of the image. In a qualitative experiment, we supply a sequence of images and their labels as input to IDEFICS, and instruct it to describe Image [X]. We observe that IDEFICS can describe the correct image easily when we give up to four labeled images, but often makes mistakes as the number increases (Figure 9 in the appendix). Therefore, even though IDEFICS is designed and trained to support multi-image inference,\u201d its multi-image capabilities do not generalize beyond a few images."}, {"title": "Conclusion", "content": "ICCA provides a perspective into the performance of today's MLLMs that is missing in existing benchmarking, and can be easily applied to new MLLMs without collecting new human data. We observe that state-of-the-art models lack the in-context abilities to adapt their own language for efficient communication, even though they may sometimes perform better while passively receiving increasingly efficient language from their interlocutor. This issue is fundamental because, unlike humans, the models do not perceive the effort or cost needed for communication, thus having no inherent reason to reduce them. It is still surprising though, given that LLMs/MLLMs have successfully displayed many other human behaviors and impressive abilities in various applications, by learning from the large amounts of human data, where adaptation for efficiency is common. Overall, the current paradigm for creating LLMs fails to address the need for conversational adaptation and future research is needed on improving their abilities to spontaneously improve language efficiency, maintain language consistency for the same referent, avoid excessive tendency for repetitions, and handle more images in a single query."}, {"title": "Implementation Details", "content": "We measure the length of the generated messages by counting the number of tokens. Because different MLLMs have different tokenizers, we choose to only use IDEFICS's tokenizer for message length calculations for all models."}, {"title": "Word Novelty Rate", "content": "Word Novelty Rate is a modified Word Error Rate, which only counts insertions and substi-tutions, and ignores deletions. The number of insertions and substitutions is normalized by the length of the reference message, as done in the standard Word Error Rate calculation. For two messages from Repetition N-1 and N, we use the message from Repetition N-1 as the reference and the one from Repetition N as the hypothesis. We follow Hawkins et al. (2020a)'s metric design and drop most function words to only consider open-class content words (nouns, adjectives, verbs, and adverbs) as well as pronouns, numbers, and adpositions.\nWNR addresses the limitation of metrics using cosine similarity between averaged embed-dings (e.g., GloVe), which operate in the semantic space, for example as in Hawkins et al. (2020a). Semantic similarity between messages is not sensitive to some lexical changes, ignoring the importance of exact word choice in convention formation. For example, once a convention is formed to use the car to refer to an image, changing it to a semantically similar message, the automobile, violates the stability property of conventions, which may increase the listener's cognitive load. Empirically, we find that WNR produces results consistent with Hawkins et al. (2020a)'s averaged GloVe embedding similarity, as shown in Figure 4 (WNR moves in the opposite direction to the GloVe-based similarity because WNR directly measures dissimilarity)."}, {"title": "MLLM Implementation Details", "content": "The exact versions of the MLLMs used are idefics-80b-instruct, llava-1.5-13b, gpt-4-1106-vision-preview, Gemini 1.0 Pro Vision, and claude-3-opus-20240229. For IDEFICS, we use 8-bit quantization to fit the 80b model into 3 A6000 GPUs. For all MLLMs, we use a decoding temperature of 0 to avoid the uncertainty caused by sampling, which was also the default for IDEFICS and LLaVa. We use the models' default values for other hyperparameters.\nThe GPT4 listener used to evaluate model speakers follows the listener interaction setup in L3: Images Once. L3 is the scenario where GPT4 shows its best performance and almost perfectly matches human performance.\nLLaVa only supports taking 1 image as input. To bypass this constraint, we merge the 4 images in the referential context into 1 image, using a 2-by-2 grid. Instead of using image labels (A, B, C, D), experiments with LLaVa refer to an original image using its location in the merged image (top right, top left, bottom right, and bottom left).\nWe conduct prompt engineering for each model individually to find the most suitable phrasing of the instructions. The prompt engineering was done over the pilot study dataset released in Hawkins et al. (2020b)'s official Github repository. The pilot study dataset is distinct from the 54 human interactions in ICCA that we use for evaluation. This pilot study dataset contains human-human interactions on easy referential contexts, where the target images are more easily distinguishable to humans with very short messages. For this reason, the human interactions in this pilot study dataset show less language adaptation, and we only use it for prompt engineering and not evaluation."}, {"title": "Example Prompts", "content": null}, {"title": "Figures for Error Analysis", "content": null}, {"title": "Image Grounding Impact", "content": "The results with L3 (Section 5.1) raised concerns about whether the models are using images effectively or just exploiting label-message association. We develop three variants to study this:\nL5: Images Masked Each image is replaced by an image mask, where all pixels have the black color, (0, 0, 0). The image order is persistent, so the same label refers always to the same underlying image that is masked. For the first four trials in Repetition 1, the model has to make a guess. Regardless of the guess, the correct target is given by the system feedback as usual. Theoretically, the model can use the feedback for later repetitions. For a model to succeed in this variant, it must associate the messages with the image labels since no actual images are given. This variant tests the extent to which a model can exploit label-message associations through in-context learning as a way of convention understanding.\nL6: Images Misleading This variant complements L5 to further study the impact of text signals and images. The setup here follows L3, showing the referential context once, except that we manipulate the images to be misleading. For the manipulation, we shuffle the images when presenting them to the model listener at the beginning of the game, but we do not change the gold image labels in the system. Therefore, Image [X] from the speaker and the system's perspective is likely different from the Image [X] from the listener's perspective, and so on. For example, using the context in Figure 11, the model may see the message Photo with a bowl of 3 bananas with pokadot and choose Image C but the system and the speaker would always think the image that features bananas and polka dot is Image B, since we shuffled the images without updating the gold labels accordingly. Then the system feedback would be wrong, the correct answer is Image B. To succeed under this setting, the model must learn to ignore the images and just associate all the descriptions related to polka dot (for example) with the label Image B.\nL7: Images Misleading in Rep. 6 This variant further tests if the models' previous promising performance in L3 comes from just exploiting the textual signals (the label-message associations) and ignoring the visual input. We show the context once similar to L3 at the beginning, but then show shuffled versions during each trial in the last repetition, without updating the gold labels (same manipulation as L6). We hypothesize that if the model tends to ignore the image input and rely on textual associations in the conversation history, this manipulation will have little impact on its prediction or the accuracy calculated based on the old gold label. This variant requires 20 images in the last trial so it cannot be used with LLaVa or Gemini."}]}