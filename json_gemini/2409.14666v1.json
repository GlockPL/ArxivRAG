{"title": "SEMI-SUPERVISED LEARNING FOR ROBUST SPEECH EVALUATION", "authors": ["Huayun Zhang", "Jeremy H.M. Wong", "Geyu Lin", "Nancy F. Chen"], "abstract": "Speech evaluation measures a learner's oral proficiency using automatic models. Corpora for training such models often pose sparsity challenges given that there often is limited scored data from teachers, in addition to the score distribution across proficiency levels being often imbalanced among student cohorts. Automatic scoring is thus not robust when faced with under-represented samples or out-of-distribution samples, which inevitably exist in real-world deployment scenarios. This paper proposes to address such challenges by exploiting semi-supervised pre-training and objective regularization to approximate subjective evaluation criteria. In particular, normalized mutual information is used to quantify the speech characteristics from the learner and the reference. An anchor model is trained using pseudo labels to predict the correctness of pronunciation. An interpolated loss function is proposed to minimize not only the prediction error with respect to ground-truth scores but also the divergence between two probability distributions estimated by the speech evaluation model and the anchor model. Compared to other state-of-the-art methods on a public data-set, this approach not only achieves high performance while evaluating the entire test-set as a whole, but also brings the most evenly distributed prediction error across distinct proficiency levels. Furthermore, empirical results show the model accuracy on out-of-distribution data also compares favorably with competitive baselines.", "sections": [{"title": "1. INTRODUCTION", "content": "Speech evaluation is traditionally performed by human experts. Recently, machine learning models that correlate well with human judgement have made automated evaluation possible - predicting consistent scores with less human effort [1].\nEarly speech assessment studies have focused on crafting discriminative features and testing with different statistical models. These features include duration-related features [2], goodness of pronunciation (GOP) and its variants [3, 4], linguistic features [5], the marginal distribution of speech signals [6] and much more. Early modelling works focused on statistical models such as Gaussian Processes [1], while more recent works have focused on deep learning approaches. [7] extended GOP to Deep Neural Networks (DNN) trained acoustic model and improved mispronunciation detection performance. [8] used a similar approach by also taking advantage of language models - a sequence-trained acoustic model and multiple n-gram language models were used to compute acoustic and text-based features for a final neural classifier. [9] and [10] examined the use of Transformer-style scoring models based solely on transcriptions. They reported strong performance approaching or surpassing human agreement in speech evaluation.\nA fundamental challenge to deep learning algorithms is the reliance on supervised learning and by extension good data, meaning data-sets that are large, representative and balanced. Small, imbalanced data-sets often lead to over-fitting and issues with generalizability when models face out-of-distribution (OOD) or under-represented samples [11]. While not unique to speech evaluation, the characteristics of speech evaluation data often possess long-tail distributions stemming from mispronunciations and accents from non-native speakers, in addition to inter-rater variances from teacher scores.\nDifferent learning strategies have been studied to address this challenge in speech evaluation. [12] and [13] investigated quantifying the assessment uncertainty using ensemble, distillation and multi-task learning. Improved scoring performance and robustness to OOD was reported. [14] investigated methods for cost-sensitive learning and synthetic minority over-sampling and reported significantly better pronunciation error detection rates on imbalanced corpora. [15] explored supervised multi-task ensemble learning as a way to exploit mutual information between different annotation types.\nRecently, a self-attention based Transformer encoder model - GOPT [16] was applied to speech evaluation. Leveraging self-supervised learning, GOPT and large-scale pre-trained universal speech embedding has demonstrated superior performance on a speech evaluation data-set [16, 17] .\nHowever, these studies only reported performance improvement when the entire testing data was taken as a whole. While investigating the cohort-wise performance, we found that its prediction quality deteriorated significantly for minority cohorts. In addition, the model's robustness has not been verified in cross-data-set test."}, {"title": "2. ARCHITECTURE", "content": "Robustness issues will cause consistency and fairness concerns for automatic speech evaluation. This study focuses on improving model's robustness while training data is imbalanced and insufficient. A two-stage training is proposed to prevent over-fitting on small and biased training samples and to enhance model's generalizability in real world. An objective metric is adopted to calculate pseudo-scores for speech evaluation. An anchor model is pre-trained in semi-supervised manner using these pseudo-scores. While training the real speech evaluation model, the prediction made by this anchor model is used to interpolate the loss function. We will report both overall and cohort-wise performance, as well as the result for a cross-data-set test."}, {"title": "2.1. Speech Evaluation Model", "content": "In reading speech evaluation, evaluators assess the individual's language proficiency according to a rubric containing the criteria and standards to be used, assessing different aspects of proficiency. In automatic evaluation, the evaluators are replaced by a model trained on human labelled samples.\n\n$y = \\Phi(O,T)$\n\nThe input is a pair of speech and reference. $O = [o_1, ..., o_m]$ is the sequence of m consecutive speech observations. $T = [w_1,..., w_m]$ is the sequence of M consecutive canonical-phonemes in the reading text, either known a priori or generated by Automatic Speech Recognition (ASR). $\\Phi$ is the model. y is the numerical score associated with the input tuple (O,T). To compare a speaker's pronunciation to a certain level of expected proficiency and identify errors in the sentence, O has to be aligned to T. Model training is simplified by extracting phone-wise features $F([o]_{w_i}, W_i)$ along the sentence, where $[o]_{w_i}$ is the speech segment belonging to $w_i$.\n\n$y = \\Phi \\begin{bmatrix} F([o]_{w_1}, W_1) \\\\ F([o]_{w_i}, W_i) \\\\ :\\\\ F([o]_{w_M}, w_M) \\end{bmatrix}$"}, {"title": "2.2. Speech Evaluation Feature", "content": "An assembly of traditional speech evaluation features and cutting-edge embedding features will be studied in this paper. Traditional speech evaluation features are a set of indicators specially designed to capture the acoustic, temporal, and linguistic cues in the reading speech, including:\n\u2022 GOP\nIt was proposed in [3] and its version with deep neural networks was introduced in [7].\n\u2022 Tempo\nThis feature measures the dynamic temporal variation in the pronunciation. It was introduced in [18].\n\u2022 PhoEmb\nPhonetic Embedding provides semantic information for speech evaluation. It was introduced in [18].\n\u2022 Pitch\nThe average values of frame-wise pitch and delta-pitch for each canonical-phoneme intervals [18].\nA set of state-of-the-art Transformer models were downloaded from Huggingface [19]. A forward pass is done using these pre-trained models and their encoder outputs are adopted as frame-wise speech representation. Average aggregation is conducted at canonical-phoneme intervals to get phone-wise evaluation embedding $F^{pre}$. These pre-trained Transformer models include:\n\u2022 \"facebook/wav2vec2-large-xlsr-53\" [20]\nIt was trained on more than 50k hours of unlabelled speech in 53 languages, including Multilingual Librispeech [21], CommonVoice [22], and Babel [23].\n\u2022 \"facebook/hubert-large-ll60k\" [24]\nIt was trained on Libri-Light 60k hours data [25].\n\u2022 \"microsoft/wavlm-large\" [26]\nIt was trained on mix 94k hours of data, including 60k hours Libri-Light, 10k hours GigaSpeech [27], and 24k hours Vox Populi [28].\n\u2022 \"openai/whisper-large-v2\" [29] (encoder only)\nIt was trained on 680k hours of multilingual and multi-task supervised data collected from the web.\nAll features are concatenated together at corresponding canonical-phonemes.\n$F(\\{o\\}_{w_i}) = [GOP, Tempo, PhoEmb, Pitch, F^{pre}]$\n$F^{pre} = [H_{wav2vec2}, H_{hubert}, H_{wavlm}, H_{whisper}]$"}, {"title": "3. TWO STAGE MODEL TRAINING", "content": "Figure 1 is the diagram of the proposed two-stage training. In the first stage, an anchor model is trained on augmented data with pseudo-scores reflecting the similarity between speech and reference. In the second stage, the prediction made by the anchor model is used to interpolate the training loss and prevent evaluation model predicting scores too different than the underlying similarity between input speech and reference. In Fig.1, the green parts are the extra function modules introduced for semi-supervised pre-training (the stage-I training) and regularized optimization (the stage II training). At inference time, the system works using only the blue modules. The evaluation model has the same GOPT structure as the models studied in [16] and [17]."}, {"title": "3.1. Objective Metric", "content": "The difficulty in collecting accurate human-scores for speech assessment motivates our searching for a metric that can capture speech correctness without subjective bias. An automatic computation of such a metric can be used to assign pseudo-scores for a large amount of speech data without accurate human-scores, thereby increasing the training data coverage.\nFrom Shannon's theory of communication [30], speech evaluation can be modelled as a noisy communication channel. The reference T is encoded into voice by the speaker (encoder) and sent to the evaluator (decoder). The voice is received and transcribed (decoded) into text T. Mutual information is often used to measure the information transmitted through the communication channel. It reflects the similarity between the input T and output T. It is estimated by sampling the phone distributions of these two random variables.\n\n$I(T, \\hat{T}) = \\sum_{i=1}^{C} \\sum_{j=1}^{C} p(w_i, \\hat{w_j}) log \\frac{p(w_i, \\hat{w_j})}{p(w_i)p(\\hat{w_j})}$\n\nwhere $w_i$ is the $i^{th}$ phone to be sampled and $C$ is cardinality of the phone-set determined by data $T \\cup \\hat{T}$. $p(w_i, \\hat{w_j})$ is the joint distribution between T and $\\hat{T}$. $p(w_i)$ and $p(\\hat{w_j})$ are two marginal distributions. These distributions could be approximated using a confusion matrix. In this study, a sentence-level phonetic confusion matrix is constructed by a dynamic programming between each pair of (T, $\\hat{T}$). (A place holder * is added into the phone-set to indicate the insertions and deletions in the alignment between T and $\\hat{T}$, when necessary.)\n$I(T, \\hat{T}) = 0$ if T and $\\hat{T}$ are independent. $I(T, \\hat{T})$ is non-negative and a higher mutual information indicates a larger reduction in uncertainty about one given another. When T and $\\hat{T}$ are identical, the mutual information will be the maximum possible value $log(C)$. C is data-dependent. Data-dependent upper bound makes it unsuitable as an evaluation metric. Instead, normalized mutual information [31] is adopted.\n\n$NI(T, \\hat{T}) = \\frac{2 \\times I(T,\\hat{T})}{H(T) + H(\\hat{T})}$\n\n$H(T) = \\sum_{i=1}^{m} p(w_i) log(1/p(w_i)$ \n\n$H(\\hat{T}) = \\sum_{j=1}^{n} p(\\hat{w_j}) log(1/p(\\hat{w_j})$\n\nH(T) and H($\\hat{T}$) are the entropy (uncertainty) of variables. 0 < NI(T,$\\hat{T}$) \u2264 1. When I(T,T) = H(T) = H($\\hat{T}$), NI(T,T) = 1. It means that all knowledge about $\\hat{T}$ comes from T, no information loss and no extra noise introduced."}, {"title": "3.2. Stage-I: Semi-Supervised Pre-training", "content": "To enhance data diversity, training sample (O,T) without human-score can be automatically labelled using Eq.(5), where $\\hat{T}$ is generated by ASR in stead of human evaluator. A new 3-tuple training sample (O, T, $\\hat{T}$) is constructed by this method. As O is perceived as $\\hat{T}$ (instead of T), the phone-wise evaluation feature is extracted according to $\\hat{T}$. Eq.(5) defines a measurement on the similarity between speech O (ground-truth reference is T) and its pseudo reference$\\{\\}^1\\hat{T}$. A pseudo-score \u015d is assigned according to Eq(5) for (O,T,$\\hat{T}$) to measure the correctness of O with associated T. Multiple pseudo references $\\hat{T}$ are taken from ASR n-best transcriptions and multiple (O,T,$\\hat{T}$) are spawned from (O, T). Pre-training leverages the enriched variation in 3-tuples to uncover complementary patterns between speech and reference.\nWhile data augmentation increases data diversity and therefore can improve model robustness [32], it's difficult to"}, {"title": "3.3. Stage-II: Regularized Optimization", "content": "While training the speech evaluation model, the cross-entropy loss is aggregated over the training samples.\n\n$D = \\frac{1}{N} \\sum_{t=1}^{N} \\sum_{s=1}^{S} p(y| O_t, T_t) log[p(y| O_t, T_t)]$\n\nwhere y is the predicted score given the speech $O_t$ and associated reference $T_t$. $p(y| O_t, T_t)$ is the target probability for ($O_t, T_t$). $p(y| O_t, T_t)$ is the model estimated probability for ($O_t, T_t$). N is the number of samples in the batch and S is the total number of marking bands. In most cases, samples are human-labelled, i.e., given the human-score $s_t$ for sample ($O_t, T_t$), $p(y| O_t, T_t) = 1$ when $y = s_t$ and $p(y| O_t, T_t) = 0$ otherwise. Cross-entropy loss could be simplified as:\n\n$D = \\frac{1}{N} \\sum_{t=1}^{N}(y = s_t| O_t, T_t) log[p(y, s_t| O_t, T_t)]$\n\nMinimizing the cross-entropy loss on a small and biased data will make the model over-fit and lead to poor generalisation capability. To prevent this, a regularization term between the probabilities estimated from the evaluation model and a separate anchor model is added into the loss function:\n\n$D = D + \\rho\\cdot D_{KL}(P(Y|O_t, T_t)||q_{y|O_t, T_t})$\n\nwhere $q(y| O_t, T_t)$ is the probability estimated from anchor model for ($O_t, T_t$). $D_{KL}(P||q) = \\sum_{t=1}p.log(q/p)$ is the Kullback-Leibler-Divergence (KLD) between two predictive distributions. 0 < $\\rho$ < 1 is a hyper-parameter. $D_{KL}(P||q)$ is the expectation of logarithmic difference between p and q, where the expectation is taken using p. Expand the KLD term in Eq.(8) and substitute the term D with Eq.(7).\n\n$D = \\frac{1}{N} \\sum_{t=1}^{N}(y = s_t| O_t, T_t) log[p(y, s_t| O_t, T_t)]$\n\n$-\\rho \\frac{1}{N} \\sum_{t=1}^{N}(y = s_t| O_t, T_t) log[q(y, s_t| O_t, T_t)]$\n\n$+\\frac{1}{N} \\sum_{t=1}^{N}(y = \\hat{s_t}| O_t, T_t) log[p(y, s_t| O_t, T_t)]$\n\nwhere $\\hat{s_t}$ is the pseudo-score assigned to ($O_t, T_t$) and it is computed by a forward pass using the anchor model. As a grading machine, models' predictions are expected to be normally-distributed. Assuming $p(y, s_t| O_t, T_t) \\sim N(s_t, \\sigma)$ and $q(y, \\hat{s_t}| O_t, T_t) \\sim N(\\hat{s_t}, \\sigma)$, the KLD regularized loss D becomes: (constants unrelated to the model are removed.)\n\n$D = \\frac{1}{N} \\sum_{t=1}^{N} (\\hat{s_t} - s_t)^2$\n\n$-\\frac{\\rho}{N} \\sum_{t=1}^{N} [exp(-(s_t - \\hat{s_t})^2/2) \\cdot (y - \\hat{s_t})^2]$\n\nwhere $exp(-(s_t \u2013 \\hat{s_t})^2/2)$ is the Gaussion kernal distance between $s_t$ and $\\hat{s_t}$.\nIf $\\rho$ = 0 or $\\hat{s_t}$ = $s_t$ for all t \u2208 [1, ..., N], minimizing D of Eq.(8) is equivalent to the minimization of MSE loss with respect to human-scores (the first term on the right side of Eq.(10)). If $\\rho$ = 1, Eq.(10) reduces to weighted MSE (WMSE) loss with respect to pseudo-scores (the second term on the right side of Eq.(10)). If 0 < $\\rho$ < 1, minimizing the KLD regularized cross-entropy loss of Eq.(8) is equivalent to the minimization of a distance weighted interpolation between MSE loss with respect to human-scores and WMSE loss with respect to pseudo-scores. Interpolated MSE loss (iMSE) prevents model from predicting scores very different than the underlying similarity between speech and reference."}, {"title": "4. EXPERIMENTS", "content": ""}, {"title": "4.1. ASR", "content": "A TDNN [33] acoustic model with 20M parameters was trained on Librispeech [34] using Lattice-Free MMI [35]. It is used to calculate the time stamps for canonical-phonemes. This model is also used together with a librispeech 3-gram language model [34] to get the n-best transcriptions in data augmentation and stage-I anchor model training."}, {"title": "4.2. Data", "content": "Evaluation Data with Human Scores\nSpeechOcean762 [36] is a publicly available data-set for English pronunciation assessment. It has 5,000 English sentences collected from 250 Mandarin speakers, including children and adults. To avoid subjective bias and label the data in a consistent manner, each sentence is evaluated by 5 language experts independently and is allotted a score between 1-10 (1 being the lowest, and 10 being the highest) for 3 aspects of the language proficiency, pronunciation(accuracy), rhythm(fluency), and intonation(prosodic). The median score from these 5 human experts is used as the final score of the sentence. This data comes with a training and testing split, 2,500 sentences (125 speakers) each. There is no script overlap or speaker overlap between training and testing.\n\u2022 Augmented Data for Anchor Model Training\nAugmented training samples (O,T,$\\hat{T}$) are spawned from the original training-set by coupling the paired training sample (O,T) with pseudo reference $\\hat{T}$ taken from the n-best tranpairedions generated by ASR for O. To enable the model to identify mismatched speech and reference (off-task inputs are inevitable in real world), some pseudo references $\\hat{T}$ are chosen randomly from the training-set. The training data is expanded 30 times in this way. Distribution of the augmented data used in anchor model training is depicted in the lower panel of Fig.2. This way of expanding training data improves data diversity by simulating the errors in transcription from mispronunciation and incorrect articulation often made by non-native speaker or children speakers. Adopting pseudo-scores makes it possible to train the anchor model in semi-supervised manner using easily obtainable data requiring no extra human effort on data scoring. Data augmentation could be done at much larger scale before deploying model into production.\n\u2022 OOD Testing Data\nIn real world, subjective uncertainties, human errors, and distribution mismatch are inevitable [37]. To investigate the model's robustness in real life, it has to be tested on challenging, unseen samples.\nA five-hour SingaKids English corpus [38, 39], including 105 Singaporean children speakers and 2,532 sentences, is graded by tutors according to a 5-bands scoring rubric (1 being the lowest and 5 being the highest). Compared to SpeechOcean762, SingaKids adopts a different grading scale and targets different age and ethnic group. 5% of the sentences have mismatched reference due to human mistakes happened in audio recording. This part of the data is identified by graders and its proficiency scores are labeled as the lowest score in marking. Samples from this data-set can correspond to the real-world scenario of encountering unseen data."}, {"title": "4.3. Evaluation Model", "content": "A vanilla implementation of transformer encoder is adopted to map the phone-wise feature sequence to sentence-level proficiency scores. We use the same GOPT configuration as [16]. Phone-wise ensemble features are projected into 24-dimension evaluation embedding. Evaluation embedding sequence plus extra [CLS] tokens are fed into the transformer encoder. Each encoder layer has 8 self-attention heads. And 3 such layers are stacked together.\nDifferent from original GOPT, encoder outputs at places corresponding to [CLS] are fed into a linear layer followed by a tanh activation to predict scores. Labels are re-scaled to targets in the range of $R_{tanh}$ = (-1, 1) by a transformation.\n\n$s_{target} = (2 \\cdot s_{label} \u2013 a \u2013 b)/(b \u2013 a)$"}, {"title": "5. RESULTS & DISCUSSION", "content": "Tab.1 compares the above three models with a LSTM model (reported in [18]) on SpeechOcean762 and SingaKids, respectively. Models are evaluated using Root-Mean-Squared-Error (RMSE) and Pearson-Correlation-Coefficient (PCC).\nOn SpeechOcean762, all three models perform significantly better than the previous LSTM model. Among them, M-SSL/WSL performs the best. This is in line with expectation as Whisper speech embedding is learned using a very"}, {"title": "6. CONCLUSION", "content": "Speech evaluation data is often heavily skewed with respect to students' distribution across bands. Models trained on such data face generalization difficulty. Recently, GOPT and pre-learned speech embedding features brought significant progress for in-distribution speech evaluation. However, the model's robustness has not yet been investigated. In experiment, we found its performance deteriorated seriously when faced with under-represented and out-of-distribution samples. This poses a challenge on consistency and fairness aspects when deploying automatic speech evaluation in real world.\nIn this work, a mutual-information-based metric is adopted to measure the similarity between speech and reference. An anchor model is pre-trained in semi-supervised manner using pseudo-scores. While training speech evaluation model, prediction made by anchor model is used for loss interpolation to improve robustness. The proposed two-stage training brings more balanced performance across cohorts. Furthermore, new method performs well on out-of-distribution data."}]}