{"title": "PREBA: A Hardware/Software Co-Design for Multi-Instance GPU based AI Inference Servers", "authors": ["Gwangoo Yeo", "Yujeong Choi", "Jiin Kim", "Minsoo Rhu"], "abstract": "NVIDIA's Multi-Instance GPU (MIG) is a feature that enables system designers to reconfigure one large GPU into multiple smaller GPU slices. This work characterizes this emerging GPU and evaluates its effectiveness in designing high-performance AI inference servers. Our study reveals that the data preprocessing stage of AI inference causes significant performance bottlenecks to MIG. To this end, we present PREBA, which is a hardware/software co-design targeting MIG inference servers. Our first proposition is an FPGA-based data preprocessing accelerator that unlocks the full potential of MIG with domain-specific acceleration of data preprocessing. The MIG inference server unleashed from preprocessing overheads is then augmented with our dynamic batching system that enables high-performance inference. PREBA is implemented end-to-end in real systems, providing 3.7\u00d7 improvement in throughput, 3.4\u00d7 reduction in tail latency, 3.5\u00d7 improvement in energy-efficiency, and 3.0\u00d7 improvement in cost-efficiency.", "sections": [{"title": "Introduction", "content": "Unlike throughput-hungry AI training algorithms, which is well-suited for acceleration using throughput-optimized GPUs, fully saturating a GPU's high compute power and memory bandwidth under small batch inference scenarios is much more challenging. From an Artificial Intelligence as a Service (AIaaS) provider's perspective, maintaining high GPU resource utilization is of utmost importance to optimize Total Cost of Ownership (TCO). As such, the low resource utilization in GPU-based AI inference presents an important research challenge with significant industrial importance.\nTo overcome such challenge, recent high-end GPUs from NVIDIA are equipped with a feature called Multi-Instance GPU (MIG) [63] which allows one large GPU's resources to be partitioned into multiple smaller sized GPU slices. Each GPU slice (henceforth referred to as a virtual GPU, vGPU) functions as a standalone GPU and can independently be handed over to a Virtual Machine (VM) with performance isolation guarantees using Single Root I/O Virtualization (SR-IOV [70]). Such reconfigurability becomes extremely valuable for AIaaS providers because GPUs can now be deployed for both training (configured as one large monolithic GPU) and inference (partitioned into multiple vGPUs) with high GPU utilization. For instance, each vGPU can independently host an inference server and exploit query-level parallelism to improve GPU's utilization, e.g., a single NVIDIA A100 GPU [61] can be partitioned into seven small vGPUs and host seven inference servers to concurrently handle multiple service queries.\nGiven this landscape, a key objective of this paper is to characterize this emerging, reconfigurable GPU architecture and evaluate the efficacy of MIG for latency-critical AI workloads. Although we confirm that an MIG inference server partitioned into multiple small vGPUs is indeed highly effective in improving GPU utilization, we observe that the data preprocessing stage of inference incurs a critical performance bottleneck when a GPU is partitioned using MIG. Current AI inference servers utilize the CPU to apply various application-specific preprocessing to the raw input data before sending them to the GPU for AI model execution (e.g., image decoding, resizing, cropping, and others for computer vision [72]). Our key observation is that the overhead of CPU-side data preprocessing increases proportionally to the number of vGPUs instantiated, causing significant reduction in Queries Processed per Second (QPS) vs. an inference server without any data preprocessing overheads.\nAnother important research challenge with MIG is how to best exploit the multitude of vGPUs available in the inference server. Concretely, compared to the baseline system where one large GPU is available for scheduling inputs, a MIG inference server must consider multiple smaller vGPUs for scheduling. A critical aspect of input scheduling is how it should batch multiple inputs to improve GPU utilization. We observe that the batching algorithm, when naively implemented for MIG inference servers, can suffer from high tail latency even at small batch sizes. Therefore, the batching system must consider MIG's unique properties for optimizing input batching and scheduling decisions, one which to the best of our knowledge prior work has neglected upon.\nTo this end, we propose PREBA (PREprocessing and BAtching system) which is a hardware/software co-design for high-performance MIG inference servers. The key contributions of our proposed system are twofold:\n1.  (Hardware) A Data Processing Unit (DPU) for MIG. PREBA fundamentally addresses MIG's CPU-side data preprocessing bottlenecks by completely offloading performance-critical data preprocessing operations to an FPGA-based DPU. Because of the diversity of Al inference workloads, it is important that PREBA's hardware architecture contains enough flexibility to handle various application-specific data preprocessing operations. We show that our FPGA-based DPU can seamlessly accelerate the data preprocessing operations of diverse Al workloads. Our DPU microarchitecture is co-designed with the inference server's input batching system so that it is tuned for the latency-critical nature of AI inference. Specifically, our DPU is optimized for minimizing a single-input request's latency while maximizing throughput, enabling our batching system to flexibly adjust the batching granularity based on the vGPU size, the AI model, and input size/length at runtime.\n2.  (Software) A dynamic batching system for MIG. Designing an efficient batching system requires careful tuning of the following two hyperparameters: (a) the maximum batch size ($Batch_{max}$, i.e., the batching system will schedule the batched inputs once a certain number of inputs are ready for batched execution) and (b) the maximum queueing delay for batching ($Time_{queue}$, i.e., the longest time period the batching system will have input requests wait inside a queue to form a larger batch). We observe that tuning the values of these two hyperparameters for a given Al model without considering MIG's effect on batching and scheduling (e.g., the sensitivity of a given vGPU's model execution time and throughput as a function of batch size) leads to sub-optimal performance. We propose a dynamic batching system that leverages a profiling-based analytical model to systematically estimate the optimal $Batch_{max}$ and $Time_{queue}$ values to utilize for high-performance MIG inference servers.\nOverall, PREBA presents a practical yet highly effective architectural solution targeting MIG. To the best of our knowledge, this work is the first to identify, analyze, and explore the data preprocessing bottlenecks and the need for an efficient batching system tailored for inference servers utilizing MIG, an emerging yet highly important architectural feature introduced in state-of-the-art high-end NVIDIA GPUs. PREBA is implemented end-to-end over real systems using commodity hardware and open-source software, providing an average 3.7 \u00d7 improvement in throughput, 3.4\u00d7 reduction"}, {"title": "Background", "content": "2.1 AI Training vs. Inference in GPUs\nGPUs are throughput-optimized processors employing many-core SIMD vectors with high-bandwidth memory (e.g., HBM [36] or GDDR [76]). Because AI training tasks employ large input batch sizes and exhibit throughput-hungry characteristics, GPUs have become the de facto standard in designing today's AI training systems. For AI inference, however, the input batch size is orders of magnitude smaller than training. Therefore, the working set of inference is typically too small to fully saturate a high-end GPU's compute and memory resources. As such, GPU-based inference servers oftentimes experience very low GPU utilization. To this end, there has been significant interest from both academia [6, 11, 20, 27, 28, 33, 68, 77-79, 85, 86] and industry [1, 4, 14, 40, 54, 81] to design a domain-specific architecture targeting AI inference. Nonetheless, outside of some hyperscalers [14, 40, 54, 81], the AI inference market is still dominated by GPUs as they are readily available with their mature software stack, functioning as the \"go-to\u201d platform for deploying AI services.\nDespite GPU's prominence in deploying AI services, the GPU underutilization issue upon small batch inference scenarios is still a weak spot of GPU-based AI inference servers. To address such limitation, GPU vendors have released small, inference-purposed GPUs that contain relatively lower compute and memory throughput (e.g., NVIDIA T4 [60]), allowing better GPU utility even for small batch inference. Unfortunately, deploying these small GPUs for inference introduces a significant tradeoff: it reduces the computational density of the inference server, proportional to the performance difference between a large (training-purposed) and small (inference-purposed) GPU. NVIDIA's recently announced Multi-Instance GPU (MIG [63]) architecture is intended to remedy the aforementioned challenges, allowing a GPU card to be (re)configured as either a single large GPU or be partitioned into multiple small GPU slices (referred to as virtual GPUs, vGPUs, in the rest of this paper). In the following section, we delve deeper into the MIG architecture.\n2.2 NVIDIA's MIG with Reconfigurability\nThis work employs NVIDIA's MIG as the foundation for building an Al inference server. Below we elaborate on NVIDIA's A100 GPU [61] which features MIG capabilities."}, {"title": "AI Model Serving Pipeline for Inference", "content": "Figure 3 illustrates the end-to-end AI model serving pipeline to service an inference query (e.g., NVIDIA Triton Inference Server [58]). As depicted, the first two stages of model serving (data preprocessing, batching) are executed on the CPU while the GPU executes the last stage (model execution). Below we detail the operations conducted in each stage.\n1.  Data preprocessing. The raw input data routed to the inference server is first preprocessed in order to be transformed into a data format that is compatible with the AI model. For the computer vision and audio processing Al workloads we focus on in this work, some representative preprocessing operations include a) decoding (JPEG\u2192RGB), resizing, and cropping to generate the preprocessed image (e.g., 224\u00d7224\u00d73) for computer vision [72], and b) resampling, Fast Fourier Transform (FFT), and applying Mel filters for audio processing [71] (Figure 4).\n2.  Batching. Throughput-optimized GPUs prefer large input tensors over smaller ones to maximally exploit its abundant compute and memory performance (Section 2.1). Therefore, coalescing multiple preprocessed inputs into a single large input batch plays a critical role in achieving high GPU utilization [23, 29, 92]. Another key benefit of constructing a large input batch is that it increases the reuse of AI model parameters uploaded from the off-chip DRAM during the model execution stage, significantly improving GPU utility.\n3.  Model execution. The final stage of model serving is the actual execution of the Deep Neural Network (DNN) layers that define the AI model. Popular inference serving systems like NVIDIA Triton Inference Server [58] or TensorFlow Serving [66] utilize an inference execution engine to perform the tensor operations that constitute the DNN layer (e.g., TensorRT [59]). Once the final model output is derived, the CPU retrieves the final result and returns it back to the client."}, {"title": "Related Work", "content": "Data preprocessing for AI training/inference. Mohan et al. [56] analyzed the impact of input data preprocessing on end-to-end AI training's throughput, proposing various performance optimization strategies like I/O caching and sharing preprocessed training dataset across several training hyperparameter search jobs to amortize the data preprocessing overhead. Trainbox [69] similarly observes data preprocessing bottlenecks in AI training tasks and proposes to offload the compute-intensive data preparation to an FPGA device. There is also a line of work that proposes a disaggregated pool of CPU compute nodes, dedicated for data preprocessing, and allocates them on-demand per the training task's preprocessing throughput requirements [31, 84, 94]. The work from Zhao et al. [94], for instance, puts a particular emphasis on optimizing the data preprocessing stage of recommendation model training. Similar to Trainbox, DL-Booster [22] seeks to address the data preprocessing bottlenecks of AI by offloading preprocessing operations to an FPGA device. None of these prior art explores the implication of data preprocessing overheads over NVIDIA's MIG nor proposes latency-optimization strategies tailored for data preprocessing accelerators. Overall, the key contribution of our work is orthogonal to these related work as our study is focused on Al inference servers employing NVIDIA's reconfigurable MIG architecture."}, {"title": "Characterization", "content": "This section utilizes NVIDIA's A100 GPU to conduct a characterization of MIG inference servers. For brevity, we refer to a MIG partitioned into V vGPUs where each vGPU contains M GPCs (compute) and N GB of DRAM (memory) as a \"Mg.Ngb(Vx)\" configuration\u00b9. For instance, an A100 GPU partitioned into seven vGPUs is 1g.5gb(7x), whereas one without any partitioning and functions as one big vGPU is 7g.40gb(1x) (Figure 2). As discussed later using Figure 13, the size of a single-input for an audio processing application can vary, unlike the fixed-size input for computer vision workloads. For brevity, all experiments discussed in this section assume that the input audio length is fixed at 2.5 sec. Nonetheless, we emphasize that the key observations discussed in this section remain intact regardless of the audio length. Section 5 further details our evaluation methodology.\n3.1 MIG Effectiveness in Improving GPU Utilization\nWe first evaluate how effective MIG is in improving the \"chip-wide\" aggregate throughput, which is one of the most important motivations behind the design of MIG, i.e., addressing the GPU underutilization problem when a small AI model is serviced on top of a training-purposed large GPU."}, {"title": "MIG's Effect on Batching System", "content": "In real-world AI inference servers, input traffic patterns are constantly changing with varying traffic intensities, so designing an efficient batching system that balances throughput and tail latency becomes critical. In Figure 6, we show how the model execution stage's throughput (left axis) and its tail latency (right axis) change as input batch size increases (xaxis). Across all MIG configurations and all models, once the aggregate throughput reaches a plateau, tail latency spikes up rapidly even with a very small increase in batch size. If the GPU already reached close to its maximum possible throughput, further increase in the batch size directly translates into a proportional increase in execution time (i.e., tail latency) with only incremental improvements in throughput. We refer to this tipping point as \"the maximum batch size at the knee of the tail latency curve\" (Batchknee, denoted as green diamond markers in Figure 6). Given the property of Batchknee, we can see that setting the maximum batch size (Batchmax, i.e., the largest possible batch size the batching system will try to construct) as the Batchknee value is optimal because having Batchmax larger than Batchknee provides practically no gain in throughput while only aggravating tail latency.\nAs the batch size is increased, MIG partitioned into many small vGPUs (1g.5gb(7x)) experiences a more rapid increase in aggregate throughput than 2g.10gb(3x) and 7g.40gb(1x). This is because Batchknee value for 1g.5gb(7x) is smaller than that of 2g.10gb(3x) and 7g.40gb(1x). For example, 1g.5gb(7x) has Batchknee of 16/4/2 whereas 7g.40gb(1x) has a value of 128/32/16 for MobileNet/SqueezeNet/Swin-Transformer. Because 1g.5gb(7x) servers can reach high throughput even with small batches, an interesting property of 1g.5gb(7x) is that the batching system can spend less time waiting for inputs"}, {"title": "Effect of Data Preprocessing on MIG Throughput", "content": "We now turn our attention to data preprocessing, evaluating its effect on end-to-end performance. The left axis in Figure 8 compares the end-to-end inference throughput with and without data preprocessing. That is, the performance without the data preprocessing stage in Figure 8 (gray bars) is identical to the performance reported in Figure 5, whereas the performance with data preprocessing is collected by executing all three stages in Figure 3 end-to-end.\nAs depicted, the overall performance experiences a significant slowdown when the data preprocessing stage is activated, causing a 75.6% drop in throughput. With the 1g.5gb(7x) design point, a total of seven AI inference servers can concurrently process the service queries and cause a proportional increase in data preprocessing demand, i.e., 7\u00d7 higher than 7g.40gb(1x). Such performance drop is due to the limited amount of CPU-side resources available for data preprocessing. Specifically, several key data preprocessing operations require abundant CPU cores and memory bandwidth in order to transform the raw input into AI modelspecific formats. Unfortunately, we observe that the compute throughput and memory bandwidth in current CPUs are not sufficiently high enough to sustain the data preprocessing"}, {"title": "PREBA Architecture", "content": "4.1 High-Level Overview\nFigure 10 provides a high-level overview of our PREBA system. Our DPU unlocks the full potential of MIG by fundamentally addressing its data preprocessing bottlenecks with domain-specific acceleration. Concretely, all inference requests routed to the MIG inference server are completely offloaded to our DPU, which is integrated at the PCIe bus as the host CPU's co-processor, providing substantial improvements in preprocessing throughput. Once the MIG inference server is unleashed from its preprocessing overheads, our software system utilizes our dynamic batching system for high-performance AI inference.\n4.2 DPU Architecture for MIG Data Preprocessing\nDesign objective. We architect our DPU with the following two design objectives: 1) design flexibility and 2) latencycentric microarchitecture. Because of the diversity of AI inference workloads, we utilize an FPGA as our design substrate to flexibly handle various application-specific preprocessing operations while also reaping out the benefits of domain-specific acceleration. Furthermore, the design of our DPU microarchitecture is optimized for minimizing the latency to preprocess a batch with just a single input request while also maximizing aggregate throughput.\nFlexible design based on reconfigurable hardware. In this paper, we focus on the DPU implementation for computer vision and audio processing AI models. While multiple Al models have evolved, preprocessing methods have remained relatively unchanged due to the limited diversity of input modalities (image/audio/text). By offering distinct implementations for both image and audio, we ensure that the majority of preprocessing computations can be efficiently offloaded with only incremental adjustments to the current implementation, while disregarding text-based modalities, which require minimal preprocessing (e.g., tokenization). Additionally, the implementation leverages the user-friendly HLS language alongside reconfigurable hardware to maximize deployment flexibility.\nMotivation for single-input batch optimization. Hardware accelerators for AI are typically optimized for batches with multiple input requests as it helps maximize parallelism and overall throughput. In contrast, our DPU microarchitecture is optimized for single-input batches for several reasons.\n1.  As we characterized in Section 3.2, the performance of a MIG inference server is highly sensitive to the $Batch_{max}$ value, the optimal value of which is determined by the AI model architecture and the MIG partitioning granularity, i.e., the vGPU size. Optimizing the preprocessing stage for fast single-input preprocessing enables requests to be preprocessed immediately upon arrival to the inference server, providing extra latency budget for the next batching stage (Figure 3) to identify the optimal batching strategy in accordance to $Batch_{max}$ (the maximum batch size) and $Time_{queue}$ (the maximum queueing delay for batching).\n2.  More crucially, the relatively scarce compute/memory resources available in 1g.5gb(7x)'s individual vGPUs render its optimal $Batch_{max}$ value to become much smaller than 7g.40gb(1x)'s $Batch_{max}$ value (e.g., Swin-Transformer's $Batch_{max}$ value is only 2 under 1g.5gb(7x)). Therefore, having the input requests be preprocessed in the most finest granularity (i.e., single-input request) provides the most flexibility to the subsequent batching stage to construct any batch size that best fulfills the model execution stage's need.\nDPU microarchitecture. Figure 11 shows our DPU microarchitecture targeting computer vision and audio processing workloads, each of which contains multiple Computing Units (CUs) for preprocessing. A CU is the smallest granularity in which the host CPU controls and communicates with the FPGA to 1) transfer input/output data and 2) command the FPGA to execute preprocessing operations. A given CU contains multiple functional units, each of which is responsible for a specific stage of preprocessing (e.g., the \u201cDecode\u201d unit in a CU handles JPEG decoding operation in Figure 4(a))2. Each functional unit is carefully designed to maximally reap out data-level parallelism inherent in the single input request leveraging the APIs on Xilinx Vitis DSP and Vision Library [10]. As mentioned previously, our CU is designed to minimize latency to process a single-input batch request. Despite such latency-optimized DPU design, we also seek to maximize aggregate throughput by employing multiple CUs in parallel (Figure 10). As such, our DPU can leverage request-level parallelism to concurrently preprocess multiple single-input batches.\nIt is worth emphasizing that a DPU microarchitecture that singlehandedly focuses on minimizing single-input preprocessing latency, without considering the DPU's overall utilization, leads to sub-optimal system-wide throughput. Consider our DPU design for computer vision in Figure 11(a) and how it handles the two single-input batches in Figure 12(a). In the preprocessing algorithm for computer vision, different operations within data preprocessing have a simple, sequential inter-operation data dependency because the previous operation's output is only used as the next operation's input (Figure 12(a)). This type of sequential dataflow enables a single CU to integrate all types of functional units while still being able to fully utilize all of these units via pipelined execution (Figure 12(a)). This is in stark contrast to the preprocessing algorithm for audio processing which has a relatively complex dataflow. In audio preprocessing, the \u201cNormalize", "sequence": 1, "Normalize\" unit cannot initiate its preprocessing operation until all the input samples have been processed by the previous \u201cResample\" and \"Mel spectrogram\" units. Putting it differently, even when two single-input batch requests are available for scheduling, a CU design that integrates all functional units of audio preprocessing (similar to how a CU is designed for computer vision, Figure 11(a)) can only start preprocessing the second request when the first request is fully preprocessed (Figure 12(b)). Our latency-optimized DPU tackles such challenge by designing two separate CU types (i.e., CU for \"Resample and Mel spectrogram": "nd CU for \u201cNormalize\u201d, Figure 11(b)) which our software runtime system utilizes for fine-grained scheduling of single-input batches. Such design point enables each single-input request to be processed with minimal latency while making sure the aggregate DPU-wide throughput is maximized via better utilization of our functional units (Figure 12(c)).\nImplication of adding DPU to the system. DPU is integrated into the system as a PCIe-attached I/O device. Below we discuss the potential overheads it may introduce.\nLatency overhead. In PREBA, the preprocessed data (which is generated by the DPU) is first forwarded back to the CPU, which is then sent to the GPU for model execution, i.e., DPU CPU GPU. One might be concerned that adding such an extra round trip latency can potentially nullify the performance benefits our FPGA-accelerated preprocessing provides. However, this extra PCIe latency is measured in the order of tens of microseconds, whereas an end-to-end inference request typically takes a few to tens of milliseconds. Therefore, the additional latency incurred by having our DPU integrated as a separate I/O device is negligible.\nPCIe bandwidth. Our DPU is integrated into the PCIe root complex via a PCIe switch, functioning as an additional I/O device from the host CPU's perspective. Having an additional I/O device in the system can therefore potentially pressurize the PCIe communication bandwidth. However, the maximum usage of PCIe bandwidth for image and audio processing workloads incurring frequent CPU\u2194DPU data transfers (e.g., MobileNet and CitriNet) is measured at 6.13 GB/s and 0.9 GB/s, respectively. These numbers are significantly lower than the 32 GB/s communication bandwidth available with PCIe(gen4) specification (and more so with the now prevalent PCIe(gen5)), rendering DPU's integration at PCIe to cause minimal interference to other I/O devices. However, in scenarios where multiple DPUs and GPUs are integrated within the same PCIe root complex, it is possible that our PREBA system can be bottlenecked by PCIe bandwidth constraints. In those circumstances, implementing P2P data movement between DPU GPU across a PCIe switch and leveraging the switch's bandwidth for inter-device communication can significantly alleviate any potential communication bottleneck [69].\nCost. Because our DPU is implemented using a separate FPGA card, it introduces extra cost and impacts the TCO of maintaining an inference server. In Section 6.3, we quantitatively evaluate PREBA's cost-efficiency."}, {"title": "Dynamic Batching System for MIG", "content": "Thanks to our DPU's single-input batch-optimized design philosophy, our batching system can freely construct any input batch size that fulfills the requirements of the model execution stage. An efficient batching system requires the following two hyperparameters to be carefully chosen for optimal batching: Batchmax (the largest batch size the batching system will try to construct) and Timequeue (the maximum time period the batching system will have input requests wait in a batching queue to construct up to Batchmax-sized input batch). As discussed in Section 3.2, the optimal value of Batchmax is when it is set to the Batchknee point. This is because constructing a batch that is larger than Batchknee provides no benefits in improving throughput while significantly harming tail latency. Note that the Batchknee value is determined based on several factors: 1) the MIG \u201chardware\" configuration inference is undertaken (1g.5gb(7x) vs. 7g.40gb(1x)), 2) the AI \u201cmodel\u201d subject for inference (MobileNet vs. Swin-Transformer), and 3) the size of the model \"input\" (a fixed-size (224x224\u00d73) input image vs. a variable-length audio input sample). We already discussed how the first two factors (\"hardware\u201d and \u201cmodel\u201d) affect the Batchknee point in Figure 6, so let us focus our discussion on the third factor, the model \"input\" size.\nEffect of variable-length inputs on batching. Unlike data preprocessing for computer vision which always generates a fixed-size (224\u00d7224\u00d73) image for the model execution stage, the audio input length for audio processing can vary significantly (Figure 13). As such, the Batchknee point of an audio processing model can vary depending on the audio input length which we visualize in Figure 14 and Figure 15. While different audio input lengths lead to different Batchknee points, the tail latency value at Batchknee is almost constant at around 35 ms, regardless of the audio input length. In the remainder of this section, we refer to this tail latency value at Batchknee as Timeknee. Below we describe how PREBA's batching system utilizes the aforementioned properties of Batchknee and Timeknee to systematically determine the optimal Batchmax and Timequeue for MIG.\nProfiling-based estimation of Batchmax. PREBA conducts an offline profiling of the throughput vs. tail latency curve as a function of batch size and input size for the target Al model served on top of the MIG configuration of interest. The cost of conducting this one-time profiling is very low (several minutes to collect all results) and more importantly, such overhead is amortized over the several millions to billions of user queries serviced by the MIG inference server. Because the input size is fixed for computer vision while variable-length for audio processing, the profiled data will be in the form of Figure 6 for computer vision and be Figure 14/Figure 15 for audio processing. The Batchknee points can then be derived by utilizing these profiled data. For instance, only a single Batchknee point exists for a computer vision model (Figure 6) so we configure its Batchmax identical to this Batchknee point. As for audio processing, multiple Batchknee points exist per model (Figure 15), one per each audio input length, meaning there can also be multiple Batchmax values that are optimal for each audio input length.\nFigure 16 illustrates PREBA's dynamic batching system that considers the variable-length nature of audio inputs and the corresponding Batchknee and Batchmax values for optimal batching. In our proposed batching system, the audio input lengths are bucketized into multiple non-overlapping windows of 2.5 seconds (e.g., [0.0-2.5 sec], [2.5-5.0 sec], . . .). Each bucket is allocated with a dedicated batching queue that buffers all audio input requests whose length falls under that bucket. Therefore, a MIG inference server maintains a total of N batching queues (N: total number of buckets). Because each batching queue is configured with its own optimal Batchmax value, one which is set identically to the Batchknee point that was derived through profiling, PREBA can dynamically construct the appropriate batch size that meets the properties of variable-length audio inputs. Putting everything together, once an input request is routed to our PREBA inference server, the audio input length is examined and is bucketized to be forwarded to its corresponding batching queue. Our dynamic batching system then seeks to accumulate enough audio inputs to construct a batch size that is up to Batchmax.\nAnalytical model based estimation for Timequeue. The purpose of Timequeue is to regulate the time a batching system spends trying to form a batch. When the input traffic intensity is low (e.g., only a handful of requests are routed to the server), spending too much time waiting to form an input batch of size Batchmax can be wasteful. This is because the time spent trying to form that big of a batch size may incur too much latency and leave several vGPUs idle.\nAs such, Timequeue should be chosen to minimize the idle time of GPU while making sure the batching system has a sufficient window of time to construct (up to Batchmax-sized) input batches. We previously established that Batchmax is the optimal batch size an inference server can employ in order to maximize throughput while maintaining tail latency within SLA bounds. Because our batching system does not construct batches whose size is larger than Batchmax, we can safely assume that the GPU's model execution time of any given input batch size will always be shorter than Timeknee (i.e., tail latency at Batchknee). Consequently, for an inference server with a single vGPU (7g.40gb(1x)), setting the Timequeue value identical to Timeknee is optimal because the batching system only spends as much time batching the inputs that match the time GPU spends executing an existing input batch. As our 1g.5gb(7x) inference server contains seven vGPUs, the Timequeue value should therefore be adjusted to make sure all seven vGPUs always have at least a single input batch to consume. In PREBA, we set the Timequeue time as the (Timeknee of a single vGPU executing the target AI model using Batchmax-sized input)/(total number of vGPUs, seven in 1g.5gb(7x)). This guarantees that the batching system at least generates an average of seven new batched inputs while the seven vGPUs concurrently execute each of its existing input batch for an average execution time of Timeknee.\""}, {"title": "Methodology", "content": "Benchmarks. We study a total of six AI workloads from computer vision (MobileNet (v3 small) [35], SqueezeNet (v1.1) [37], and Swin-Transformer [51]) and audio processing (two different model sizes of Conformer [32] and CitriNet [52]). We chose these models as they exhibit different computational intensities, allowing us to demonstrate the effectiveness of PREBA over different model characteristics. The model parameters of all of our computer vision models are from TorchHub [73] while those for audio processing are from NVIDIA NeMo [65].\nInput query modeling. Following prior work [25, 93], we assume that each service query arrives to the inference server as a single-input request. In terms of query arrival rates, we follow recommendations from the MLPerf inference benchmark [74] which assumes a Poisson distribution for modeling the rate at which a new query arrives to the server. As for the input data, we used the ILSVRC-2012 dataset [38] for computer vision workloads and LibriSpeech dataset [67] for audio processing workloads.\nHardware. Our server platform contains an AMD EPYC 7502 CPU, which contains 32 physical CPU cores with 256 GB of DRAM. We utilize a single NVIDIA A100 GPU [61] to design our MIG inference server which communicates with the CPU over PCIe. PREBA's DPU is implemented on Xilinx's U55C FPGA [8] using Vitis HLS 2022.2 [9]. The DPU"}, {"title": "Performance", "content": "Throughput. Figure 17 shows the end-to-end inference throughput of our studied systems. As depicted, the performance of the baseline system quickly gets bottlenecked by the data preprocessing stage and suffers from an average of 77.2% of throughput loss vs. \u201cIdeal\u201d. Our PREBA, on the other hand, achieves more than 91.6% of the performance of \"Ideal\" for 5 out of the 6 studied models, providing an average 3.7\u00d7 end-to-end throughput improvement vs. baseline.\nLatency. We also show how well PREBA can sustain a much higher throughput while staying within a tail latency target (Figure 18). In general, the baseline system experiences a sharp increase in tail latency at a much lower throughput than the other two designs. This is because of the large latency overhead incurred during data preprocessing (e.g., 53% and 72% of inference time in SqueezeNet and Conformer(default), see Figure 19), suffering from high tail latency even when the inference server is handling a small number of incoming requests. Our PREBA system completely resolves this performance bottleneck and shows that it can sustain 91.6% of the throughput of \"Ideal\" while staying within similar tail latency boundaries. This is also demonstrated by how close the red-colored lines (PREBA) are to the gray-colored lines (\u201cIdeal\") for 5 out of the 6 studied workloads.\""}, {"title": "Power and Energy-Efficiency", "content": "We now evaluate PREBA's effect on power consumption and energy-efficiency (Perf/Watt). In Figure 20, we provide a breakdown of baseline vs. PREBA's system-wide power consumption (left) and how that translates into energy-efficiency (right). In general, PREBA's DPU incurs an additional power consumption but makes up for that overhead by reducing power consumed by the CPU (an average 35.4% reduction in CPU power consumption). What is noteworthy is that PREBA's GPU power consumption has increased by an average of 2.8\u00d7 for the three audio processing models. This is an artifact of the significant increase in GPU utilization enabled by our DPU architecture as it successfully unleashes the CPU-side data preprocessing bottlenecks of the baseline system. Because PREBA provides significant end-to-end speedup (i.e., reduction in execution time), the system-wide energy-efficiency is improved by an average of 3.5x."}, {"title": "Cost-Efficiency (TCO)", "content": "To evaluate PREBA's effect on reducing TCO", "work": "frac{Throughput}{ \\frac{CAPEX + OPEX}{time}} $. CAPEX (CAPital Expenditure) denotes the one-time cost to purchase all hardware components, including server node [82", "7": "FPGA [90"}]}