{"title": "Uterine Ultrasound Image Captioning Using Deep Learning Techniques", "authors": ["Abdennour Boulesnane", "Boutheina Mokhtari", "Oumnia Rana Segueni", "Slimane Segueni"], "abstract": "Medical imaging has significantly revolutionized medical diagnostics and treatment planning, progressing from early X-ray usage to sophisticated methods like MRIs, CT scans, and ultrasounds. This paper investigates the use of deep learning for medical image captioning, with a particular focus on uterine ultrasound images. These images are vital in obstetrics and gynecology for diagnosing and monitoring various conditions across different age groups. However, their interpretation is often challenging due to their complexity and variability. To address this, a deep learning-based medical image captioning system was developed, integrating Convolutional Neural Networks with a Bidirectional Gated Recurrent Unit network. This hybrid model processes both image and text features to generate descriptive captions for uterine ultrasound images. Our experimental results demonstrate the effectiveness of this approach over baseline methods, with the proposed model achieving superior performance in generating accurate and informative captions, as indicated by higher BLEU and ROUGE scores. By enhancing the interpretation of uterine ultrasound images, our research aims to assist medical professionals in making timely and accurate diagnoses, ultimately contributing to improved patient care.", "sections": [{"title": "I. INTRODUCTION", "content": "Throughout the long and rich history of medical imaging, significant advancements have revolutionized diagnostic and treatment methodologies [1]. Beginning over a century ago with the invention of X-rays, which allowed non-invasive visualization of the human body, the field has continually evolved. Modern technologies such as MRIs, CT scans, Positron Emission Tomography (PET), and ultrasounds enable precise diagnosis and treatment across various medical conditions, enhancing our understanding of human anatomy [2]. Recently, the integration of computer vision, natural language processing (NLP), and artificial intelligence (AI) has further transformed medical imaging, paving the way for unprecedented developments [3].\nDeep learning-based AI technologies, in particular, have demonstrated remarkable potential in diagnosing patients swiftly and accurately [4]. These automated machine learning algorithms significantly alleviate the workload of medical professionals, offering the promise of transformative impacts on healthcare and patient care [5]. One of the most captivating advancements in this domain is medical image captioning (MIC) [6]. By leveraging deep learning, MIC systems can automatically generate captions for medical images, combining expert annotations with images from extensive datasets to provide precise and detailed analyses. These capabilities enhance medical documentation, expedite diagnosis, and facilitate remote consultations, thereby improving overall healthcare delivery [7].\nDespite these advancements, interpreting medical images remains a formidable challenge [7]. Variability in doctors' experience levels can lead to inconsistent diagnoses, and misunderstandings can result in medical errors that adversely affect patient outcomes. Moreover, reading and analyzing these images can be time-consuming, particularly in emergencies where rapid decision-making is crucial. Uterine ultrasound images, in particular, present unique challenges in obstetrics and gynecology. Their generally lower quality than other medical images complicates interpretation, potentially leading to delayed or incorrect diagnoses and impacting patient care [8]. The complexity and variability of these images underscore the need for an effective MIC system, serving as the primary motivation for our research.\nOur study aims to address the challenges in interpreting uterine ultrasound images by developing a specialized MIC system to enhance diagnostic precision and efficiency. We assembled a comprehensive dataset of uterine ultrasound images, prioritizing patient privacy and confidentiality. This dataset was meticulously annotated with expert-provided descriptions, ensuring high-quality data for training and evaluation. We then"}, {"title": "II. RELATED WORK", "content": "Ultrasound imaging is invaluable for visualizing complex anatomical structures, offering advantages such as portability, real-time imaging, cost-effectiveness, and the absence of radiation [9]. However, interpreting these images can be challenging due to their often low quality, with common issues such as fuzzy borders and numerous artifacts [10]. While numerous studies have focused on medical image captioning (MIC) [6], the majority target medical reports for chest X-ray images [11], leaving MIC for ultrasound images relatively underexplored. This section will delve into MIC research specifically pertaining to ultrasound images.\nIn [10], a coarse-to-fine ensemble model for ultrasound image captioning is presented. The model first detects organs using a coarse classification model, then encodes the images with a fine-grained classification model, and finally generates annotation text describing disease information using a language generation model. The model, trained using transfer learning from a pre-trained VGG16 model, achieves high accuracy in ultrasound image recognition.\nBuilding on the concept of combining different models, [12] introduces an NLP-based method to caption fetal ultrasound videos using vocabulary typical of sonographers. This approach combines a CNN (based on VGGNet16, fine-tuned on fetal ultrasound images) and an RNN for textual feature extraction. The CNN extracts image features, while the RNN encodes text features, merging them to generate captions for anatomical structures. The model is evaluated with BLEU and ROUGE-L metrics and produces relevant and descriptive captions for educating sonography trainees and patients.\nIn [13], a new method for ultrasound image captioning based on region detection is introduced to improve disease content analysis. The model detects and encodes focus areas in ultrasound images and then uses LSTM to generate descriptive text. This method increases accuracy in focus area detection and achieves higher BLEU-1 and BLEU-2 scores with fewer parameters and faster runtimes than traditional models.\nExpanding on incorporating additional data types, [14] introduces a Semantic Fusion Network to improve the accuracy of medical image diagnostic reports by integrating pathological information. This network comprises a lesion area detection model that extracts visual and pathological data and a diagnostic generation model that combines this information to produce reports. This method enhances the accuracy of generated reports, showing a 1.2% increase in the ultrasound image dataset compared to models relying solely on visual features.\nIn a similar vein of enhancing multimodal integration, [15] introduces an Adaptive Multimodal Attention network to generate high-quality medical image reports. The model employs a multilabel classification network to predict local properties of ultrasound images, using their word embeddings as semantic features. It integrates semantic and adaptive attention mechanisms with a sentinel gate to balance focus between visual features and language model memories. This approach enhances report accuracy and robustness, outperforming baseline models in capturing key local properties.\nAddressing the challenge of small datasets, [16] presents a weakly-supervised method to enhance image captioning models using a large anatomically-labeled image classification dataset. This encoder-decoder model generates pseudo-captions for unlabeled images, creating an augmented dataset that significantly improves fetal ultrasound image captioning. This approach nearly doubles BLEU-1 and ROUGE-L scores, saving time on manual annotations and improving model performance in communicating information to laypersons.\nIn [17], a transformer-based model is proposed to generate descriptive ultrasound images of lymphoma, providing auxiliary guidance for sonographers. The model integrates deep stable learning to eliminate feature dependencies and includes a memory module for enhanced semantic modeling. Using a nonlinear feature decorrelation method, this approach visualizes cross-attention for interpretability and focuses on lymphoma features over the background. The result is a more accurate and detailed depiction of lymphoma in ultrasound images.\nTo further improve automatic report generation, [18] introduces a framework utilizing both unsupervised and supervised learning to align visual and textual features. Unsupervised learning extracts knowledge from text reports, guiding the model, while a global semantic comparison mechanism ensures accurate, comprehensive reports. Tested on three large datasets (breast, thyroid, liver), the method outperforms other approaches without needing manual disease labels, enhancing efficiency and accessibility."}, {"title": "III. METHODOLOGY AND PROPOSED APPROACH", "content": "This study presents a novel uterine ultrasound image captioning system. To achieve this, we first gathered a diverse dataset of uterine ultrasound images and meticulously annotated them with precise medical terminology, covering women of various ages and pregnancy stages. Our approach involved rigorous data preprocessing for both images and text, followed by feature extraction using pre-trained CNN-based models. Finally, we implemented our proposed deep learning model, CNN-BiLGRU. Detailed descriptions of each module follow in the subsequent sections."}, {"title": "A. Data Collection and Annotation", "content": "Our dataset focuses specifically on gynecology, the branch of medicine that deals with women's health. We created a dataset that delves deeper into the details of gynecological imaging to address the specific challenges doctors face when diagnosing gynecological problems. This section details collecting and annotating medical images to train the medical image captioning model.\nOur research utilized a dataset of ultrasound images exceeding 500 in number (505 images). Data collection involved acquiring ultrasound images from three main sources (see Figure 1). Internally, we gathered 214 images obtained directly from the Sonoscape SS1-8000 machine. Each image has a dimension of 1024x768 pixels (width: 1024 pixels, height: 768 pixels) and is stored in JPG format. Externally, we incorporated data from publicly available datasets to enrich this collection and capture a wider range of variations. From the Mendeley repository, which offered a rich collection of nearly 1,500 fetal ultrasound images (uterine fibroid ultrasound images [19]), we collaborated with experts to meticulously review and select a subset of 191 images that best aligned with our research goals. This selection process involved eliminating images with repetitive features, poor capture of the region of interest, or other factors that could negatively impact model training. Additionally, the Zenodo [20] dataset (Fetal Planes DB) provided 450 images, meticulously organized to include four images per patient, each representing the standard fetal planes of the abdomen, brain, femur, and thorax. From this collection, we selected a subset of 100 images that best aligned with our research goals, ensuring comprehensive coverage of fetal anatomy across multiple datasets.\nIn the form of captions, annotations were then added to each image in our dataset. These captions captured key features and findings within the ultrasound images, including identifying anatomical structures such as the stomach, umbilical vein, femur bones, and brain ventricles and noting potential abnormalities such as dilated organs or fluid pockets in the brain. We communicated closely with experts during the annotation process to ensure accuracy and quality. This collaboration helped us resolve image-related issues and made our dataset more valuable for analysis and research."}, {"title": "B. Data Pre-processing", "content": "Data preprocessing is crucial for ensuring the quality and usability of data for subsequent analysis and modeling [21]. Our study encompasses rigorous processing of images and text to enhance data integrity and relevance.\nImage processing plays a crucial role in refining collected data. Initially, we analyze and filter the images to align with project requirements. The first step involves cropping the images to focus on the Region of Interest (ROI). Upon reading each ultrasound image, we convert it to grayscale if it is in color. Subsequently, we determine the cropping points by identifying significant changes in pixel intensity from the image center toward its edges. This process begins by calculating the mean intensity column-wise for both the right and left halves of the image, as depicted in Figure 2a. Peaks in these intensity profiles highlight areas of interest, and points where intensity drops below a predefined threshold (5% of peak value) denote edges of the ROI (see Figure 2b). Using these change points, we derive precise cropping coordinates to isolate the ROI (Figure 2c). Post-cropping, all images are resized uniformly to 224x224 pixels, a standard size compatible with many pre-trained neural networks. Each resized image instance is then converted into a Numpy array and normalized. Normalization involves scaling pixel values from 0 to 255 to a normalized range of 0 to 1 by dividing each pixel value by 255.\nAfter completing the image processing and preparing the images, we focused on processing the text captions associated with each image. These captions were derived from expert-provided medical descriptions and were systematically linked to their corresponding image file names within an Excel file. To enhance the text data for subsequent analysis, we applied NLP techniques [22]:\n\u2022 Convert to Lowercase: All sentences were converted to lowercase to maintain consistency and reduce variability across the dataset.\n\u2022 Remove Punctuation: Punctuation marks were systematically removed to simplify the text and emphasize the words.\n\u2022 Remove Single Letters: Single letters such as 'l', 's', 'a', and '\u00e0' were removed, as they typically do not contribute significant meaning in medical contexts.\n\u2022 Remove Extra Spaces: Any extraneous spaces within the text were eliminated to ensure uniform spacing and improve text clarity.\n\u2022 Add Start and End Tags: Special tags <START> and <END> were appended to the beginning and end of each sentence. These tags serve as markers during subsequent text processing and modeling to delineate sentence boundaries effectively."}, {"title": "C. Feature Extraction", "content": "Feature extraction is crucial in identifying and describing pertinent information within patterns [23]. This process facilitates pattern classification by establishing a structured and systematic approach. This phase focuses on deriving meaningful numerical representations from text descriptions and ultrasound images.\nText feature extraction involves converting textual data, such as medical reports and captions, into a format suitable for machine learning models. Initially, we employ a tokenizer to create a dictionary of word indices from our text data. This step allows us to determine the vocabulary size, represent the total number of unique words in the dataset, and identify the longest caption's length. Subsequently, we construct a vocabulary of unique words to map each word to its corresponding index. Shorter sequences are padded with zeros to ensure uniform input sequence lengths (captions), as neural networks require consistent input dimensions.\nIn addition to text, image feature extraction in this study"}, {"title": "D. Proposed Uterine Ultrasound Image Captioning Model", "content": "The proposed uterine ultrasound image captioning model aims to generate meaningful and accurate captions for medical ultrasound images of the uterus. To achieve high accuracy, the system architecture incorporates several advanced components. Our dataset consists of 505 images specifically selected to represent various uterine ultrasound scans commonly encountered in clinical practice.\nAs shown in Figure 3, the model's architecture begins with three input layers. The first input layer receives features extracted from a DenseNet201 model, shaped as (None, 1920). The second input layer obtains features from an InceptionV3 model, shaped as (None, 4800). The third input layer receives tokenized text sequences with a (None, 54) shape, where 54 represents the maximum caption length. The image features from the DenseNet201 and InceptionV3 models pass through dense layers that reduce their dimensionality to (None, 256). These outputs are then reshaped into (None, 1, 256) using reshape layers. Meanwhile, the tokenized text sequences are embedded, resulting in fixed-size tensor vectors (None, 54, 256).\nThe embeddings are concatenated with the reshaped image features, and the combined data is fed into a bidirectional GRU layer, which processes sequential data bidirectionally and produces an output shape of (None, 256). A dropout layer with a dropout rate of 0.5 is applied to prevent overfitting. The output is then passed through an intermediate dense layer that reduces the dimensionality to (None, 128), followed by another dropout layer with the same rate.\nFinally, a dense layer with a softmax activation function generates the final output, which has a shape of (None, 626). This represents the predicted caption probabilities for each word in the vocabulary. By integrating image and text features, this architecture produces accurate and informative captions for uterine ultrasound images, thereby enhancing medical diagnosis and treatment planning."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we detail the experimental setup and analyze the results of our image captioning model. Regarding configuration, we divided the dataset, allocating 85% for training and 15% for testing (validation). Furthermore, the model parameters were configured with the Adam optimizer, a batch size of 16, and an early stopping patience of 10 epochs.\nWe analyze the performance of the proposed model and compare it with other baseline models to ensure a comprehensive evaluation. For this, we employed several metrics to evaluate the generated captions, including BLEU [25] and ROUGE [26] scores. BLEU scores (BLEU1, BLEU2, BLEU3, BLEU4) are widely used in machine translation tasks, and they measure the similarity between the generated captions and the ground truth references using n-grams. ROUGE scores (ROUGE1, ROUGE2, ROUGEL) are standard in text summarization. ROUGE1 and ROUGE2 measure the recall of unigrams and bigrams, respectively, while ROUGEL evaluates the recall of the longest common subsequences between the generated and reference captions. These metrics comprehensively assess the model's performance in generating accurate and relevant captions for uterine ultrasound images."}, {"title": "A. Performance Analysis of the Proposed CNN-BiGRU Model", "content": "This analysis evaluates the performance of our CNN-BiGRU model, which leverages the powerful feature extraction capabilities of pre-trained Inception V3 and DenseNet201 architectures, combined with the temporal sequence modeling strength of BiGRU, for captioning uterine ultrasound images."}, {"title": "B. Comparison with Baseline Models", "content": "Our study explored various architectures integrated with different processing layers to generate captions for uterine ultrasound images. We primarily focused on using DenseNet201 and InceptionV3 models for feature extraction, followed by BiGRU, as well as baseline models such as Unidirectional GRU (UniGRU), Bidirectional Long Short-Term Memory (BiLSTM), and Unidirectional Long Short-Term Memory (UniLSTM) networks.\nTo provide a comprehensive comparison, we evaluated the performance of these models using several metrics, including BLEU and ROUGE scores. Higher scores indicate better performance. As depicted in Figure 5a, the BLEU scores for our models showed that BiGRU and BiLSTM outperformed the baseline models UniGRU and UniLSTM, with BiGRU achieving the highest BLEU-4 score of 0.55. At the same time, the ROUGE scores highlighted BiGRU as the best performer, with a ROUGE-L score of 0.78, as shown in Figure 5b.\nWe also analyzed the training loss and validation loss for the selected models (see Figure 5c). The values of \"Loss\", which refers to the training loss calculated on the training dataset, and \"Val_Loss\", which stands for validation loss calculated on the dataset, are important indicators of model performance. Our results showed that the BiGRU model had the lowest loss values (as shown before in Figure 4), with a training loss of 1.64 and a validation loss of 1.86, indicating its robustness and effectiveness in generating accurate and contextually appropriate captions for uterine ultrasound images.\nBiGRU's superiority can be attributed to its ability to capture bidirectional dependencies in sequences, which is crucial for understanding and generating coherent and contextually accurate captions. Unlike the baseline unidirectional models, BiGRU can process the sequence data in both forward and backward directions, leading to a more comprehensive understanding of the temporal context. This capability is especially beneficial for complex tasks like image captioning, where the relationship between different parts of an image and the corresponding text must be accurately captured.\nConsequently, the CNN-BiGRU model outperformed the other baseline models regarding BLEU and ROUGE scores and demonstrated lower loss values, underscoring its effectiveness in this application. Moreover, Table I provides further evidence by illustrating sample outputs that compare reference"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this study, we successfully developed a deep learning-based medical image captioning system designed explicitly for uterine ultrasound images, utilizing a CNN-BiGRU architecture. Our model effectively combined the image feature extraction capabilities of pre-trained CNNs (InceptionV3 and DenseNet201) with the sequential processing strength of a Bidirectional Gated Recurrent Unit network. This hybrid approach demonstrated superior performance to baseline models, achieving higher BLEU and ROUGE scores and maintaining low training and validation losses. The generated captions were accurate and informative, improving the interpretability of complex uterine ultrasound images.\nThe findings from our research underscore the potential of deep learning techniques in enhancing diagnostic precision and efficiency in obstetrics and gynecology. By automating the captioning process, our model assists medical professionals in making timely and accurate diagnoses, which can lead to improved patient outcomes. The effectiveness of our approach not only contributes to the field of medical image captioning but also highlights the broader applicability of AI in healthcare.\nBuilding upon the promising results of our CNN-BiGRU model, several avenues for future research and development are suggested. Expanding and diversifying the dataset with more uterine ultrasound images from diverse sources will enhance the model's robustness and generalizability. Exploring advanced architectures and optimization techniques, such as attention mechanisms or transformer-based models, may improve caption quality. Developing a real-time image captioning system for clinical integration, creating user interfaces with feedback mechanisms, and integrating multimodal medical data can provide more comprehensive diagnostic tools."}]}