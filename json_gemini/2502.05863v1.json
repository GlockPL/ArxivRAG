{"title": "Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education", "authors": ["Yanhao Jia", "Xinyi Wu", "Hao Li", "Qinglin Zhang", "Yuxiao Hu", "Shuai Zhao", "Wenqi Fan"], "abstract": "In AI-facilitated teaching, leveraging various query styles to interpret abstract text descriptions is crucial for ensuring high-quality teaching. However, current retrieval models primarily focus on natural text-image retrieval, making them insufficiently tailored to educational scenarios due to the ambiguities in the retrieval process. In this paper, we propose a diverse expression retrieval task tailored to educational scenarios, supporting retrieval based on multiple query styles and expressions. We introduce the STEM Education Retrieval Dataset (SER), which contains over 24,000 query pairs of different styles, and the Uni-Retrieval, an efficient and style-diversified retrieval vision-language model based on prompt tuning. Uni-Retrieval extracts query style features as prototypes and builds a continuously updated Prompt Bank containing prompt tokens for diverse queries. This bank can updated during test time to represent domain-specific knowledge for different subject retrieval scenarios. Our framework demonstrates scalability and robustness by dynamically retrieving prompt tokens based on prototype similarity, effectively facilitating learning for unknown queries. Experimental results indicate that Uni-Retrieval outperforms existing retrieval models in most retrieval tasks. This advancement provides a scalable and precise solution for diverse educational needs.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence for Education (AI4EDU) is an emerging interdisciplinary field that leverages AI techniques to transform and enhance instructional design, learning processes, and assessment for education (Hwang et al., 2020). With the growing global emphasis on the importance of Science, Technology, Engineering, and Mathematics (STEM) education, retrieving accurate resources from massive interdisciplinary knowledge databases has become a critical challenge.\nTraditional retrieval systems are typically designed for natural text-image contents, which limits their utility in multi-modal STEM education contexts (Li et al., 2024a; Wang et al., 2023). Research indicates that these systems often fail to capture the complexity of materials such as images, diagrams, or interactive content, which are vital in STEM disciplines (Shen et al., 2023). Effective retrieval in STEM education should be able to handle different representations (i.e., different styles of text, image, audio, etc.) to accommodate the diverse learning and teaching needs within STEM subjects.\nDespite advancements in text-image matching techniques (Williams-Lekuona et al., 2022; Zhou et al., 2024a), current retrieval systems still encounter challenges when implemented in STEM education (Li et al., 2025). These models are primarily optimized for matching text and images, neglecting the variety of query types essential in educational scenarios, including voice, sketches, and low-resolution images (Yang et al., 2023). The constraints of current frameworks within educational contexts frequently result in imprecise, biased, or inefficient retrieval outcomes, such deficiencies can impede teachers' access to suitable instructional resources (Gasmi et al., 2024).\nTo address above challenges, we propose a multi-style and multi-modal retrieval task tailored for STEM education scenarios, as illustrated in Fig.1. The input types for this task include text, audio, and various styles of images, designed to meet the diverse needs of educational contexts. To adapt to this task, we introduce the STEM Education Multi-Style Retrieval Dataset (SER), curated by 20 graduate and doctoral students from disciplines such as computer science, physics, energy, engineering,"}, {"title": "2 Preliminary", "content": null}, {"title": "2.1 Task Formulation", "content": "We provide a formal problem formulation for query-based retrieval. Specifically, given an image I or a text prompt Pt from the style-specific query set Qs, the retrieval model needs to compute the score between input and target queries and rank the corresponding answers A as high as possible. In the task settings for STEM education retrieval, which share a similar goal, the objective is to rank all answers correctly in response to input queries across various style-specific query sets Q1. If the dataset does not contain the corresponding different style queries, the model should list the same category queries as the suggestions."}, {"title": "2.2 Dataset Construction", "content": "SER is a multi-style benchmark dataset we construct to facilitate accurate retrieval for teachers in STEM education. It contains a total of 24,000 text captions, audio clips and different"}, {"title": "3 Uni-Retrieval model", "content": "Our model consists of three main submodules: a prototype learning module for generating the prototype feature for each content style (Sec. 3.1), a prompt bank for saving features in the embedding space and selecting the prompt tokens which can represent the input's style (Sec. 3.2), and a feature extractor based on different vision-language models for retrieving (Sec. 3.3). Additionally, we further present the training and inference process in Sec. 3.4."}, {"title": "3.1 Prototype Learning Module", "content": "For Uni-Retrieval, given an input query (freely based on audio, image, or text) $x \\subseteq \\mathbb{R}^{L*C}$ and a feature encoder f, map x into a d-dimensional shared latent space (Prototype Module) using f, each style has m images. For style extracting, researchers usually use the the pretrained models which contains rich semantic information as the feature encoders. For example, if the input queries focus on style images, we leverage the style encoder (Tao, 2022) to extract visual features. If the query emphasizes the need for more textual information from the context, the text encoder and tokenizer, which are pretrained on large text datasets such as the Pile (Gao et al., 2020b), can be"}, {"title": "3.2 Prompt Bank", "content": "The Prompt Bank builds a hidden state like TTT (Sun et al., 2024) and mamba (Gu and Dao, 2023), which are designed to store semantic and contextual information at a high level. Unlike the previous method, which leverages clusters to represent different styles, the Prompt Bank uses hash-like sets to store universal information. Prompt Bank contains N prompts, and each prompt $k_i$ is associated as a value to a learnable key $P_i$:\n\n$Prompt\\_Bank=\\{(k_1, P_1), ..., (k_N,P_N)\\} \\quad (3)$\n\nWe denote the key set as $K = \\{k_i\\}_1^N$ and define $\\gamma$ to score the match between $k_i$ and $P_i$. Given an input x, the $\\gamma$ looks up the top-n keys to expand the feature embedding of x. The aim to use the hash-liked structure is promoting the matching speed between the input and the Prompt Bank's tokens. For Uni-Retrieval model, we calculate the cosine similarity between the matching prompt $P_{ji}$ and the key $K_i$:\n\n$K = \\underset{\\{j\\}\\_{i=1}\\[1,N\\]}{arg\\ min} \\sum\\_{i=1}^{N} \\gamma(P\\_{ji}, K\\_i) \\quad (4)$\n\nThe Prompt Bank is free to combine different prompt tokens and expand feature embedding space, allowing different tokens associated with specific styles to jointly represent an input query. Due to the generalization properties on out-of-distribution of our Prompt Bank, even unseen styles also can combine similar styles' tokens to enhance retrieval performance by expand the semantic/context information provided by Prompt Bank. The expanding method is suitable for both different styles of images and different expression of text. Usually the special token is put at the beginning of the sequence tokens:\n\n$x\\_p = \\[\\text{CLS}; P\\_{j1}; P\\_{j2} ; ... ; P\\_{jn} ; x\\_e\\] \\quad (5)$\n\nwhere $x_p$ denotes the image's input feature after expanding prompt tokens; $x_e$ represents the original sequence tokens patched from the input; CLS is the special token used for performing downstream tasks with different heads."}, {"title": "3.3 Feature Extractor", "content": "In Uni-Retrieval, we apply the ViT structure as the visual feature extractor, leverage a tokenizer to embed the input text query $x \\subset \\mathbb{R}^{L*C}$, and then utilize the transformer (Vaswani et al., 2017) as the text encoder to extract features. The vision encoder"}, {"title": "3.4 Training and Inference", "content": "For the training procedure, in every training step, the style/context features are extracted from the corresponding encoder f to get the prompt $P_j$. Then, matching $P_j$ and the key $K_i$ to get the matching prompts $P_j$. Besides, the tokenizer and the patch layer map the inputs into sequence $x_t$:\n\n$x_t = \\text{Tokenizer/Patch}(x) \\quad (6)$\n\nwhere $x_t$ denotes the temp state of features. After selecting n prompts following the aforementioned query strategy, the expanded feature embedding $X_p$ is fed into the foundation model $\\delta$ and getting the final result $x_f$. We use the CLS token to represent the whole sequence $x_p$ following the settings of LLaMA (Touvron et al., 2023):\n\n$x\\_f = \\delta(x\\_p) \\[:, 0, :\\] \\quad (7)$\n\nThe triplet loss function $\\mathcal{L}$ utilizes the features $x_f, x_r$, and $x_h$ of an image or text, a retrieval target query, and a negative sample from a different category. Minimizing $\\mathcal{L}$ brings the correct sample pairs closer together while distancing the negative sample pairs. With $\\mu$ as margin and distance function $d(a,b) = (1 - a * b)/(||a|| - ||b||)$, $\\mathcal{L}$ is given as:\n\n$\\mathcal{L} = \\text{max}\\{0, \\\\ \\mu + d(\\delta(x\\_f), \\delta(x\\_r)) \u2013 d(\\delta(x\\_f), \\delta(x\\_h))\\} \\quad (8)$\n\nwhere $x_r, x_h$ denotes the embedding of the retrieval object and the negative sample respectively. Moreover, the key in Prompt Bank will be updated"}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experiments Settings", "content": "We use our SER as the main experiment analysis and another three retrieval datasets to comprehensively evaluate the Uni-Retrieval's performance. For evalution metric, We evaluate the R@1 and R@5 metrics and the inference speed (ms) on all retrieval datasets. For R@1 and R@5, \u201c\u2191\u201d denotes that higher is better. For ms, \u201c\u2193\u201d denotes that quicker is better. Implement Details are detailed in Appendix C."}, {"title": "4.2 Comparison Experiment", "content": "On the SER dataset, Uni-Retrieval demonstrates superior performance across multiple scenarios with different query styles compared to other baselines, including multi-modality models, cross-modality pre-trained models, and prompt learning models. As shown in Tab.1 and Tab.4, the T + S \u2192 I means inputting the text and the style images as the multi-queries and outputting the corresponding images as the target queries. The experiment results yield three key observations:\nThe Uni-Retrieval achieves the best retrieval performance on the multi-style STEM Education Retrieval task: This highlights the effectiveness of Uni-Retrieval in handling complex multi-modal queries. Due to the Prompt"}, {"title": "4.3 Ablation Study", "content": "To quantitatively evaluate the role of prompts in the model, we conducted ablation studies on the insertion type and token number of prompt tokens within Uni-Retrieval for four style metrics. These studies aimed to assess their impact on the style-diversified STEM education retrieval task,"}, {"title": "5 Conclusion", "content": "To address the challenge of fine-grained and efficient retrieval in STEM teaching scenarios, we proposed a multi-style and multi-modal STEM education retrieval task and curated a multi-style dataset of over 24,000 samples for model fine-tuning. To balance training efficiency and retrieval performance, we developed a lightweight and plug-and-play feature expression module, Prompt Bank, and built a database-driven accurate retrieval model, Uni-Retrieval, based on Prompt Bank. Compared to current state-of-the-art retrieval models, Uni-Retrieval significantly improves retrieval performance with only a 26M (less than 5%) increase in parameter size and less than 10ms additional retrieval time. Furthermore, the training and deployment costs of Uni-Retrieval are substantially lower than those of existing large retrieval models, making it a more economical and practical solution for educational scenarios. We hope Uni-Retrieval can inspire new possibilities for the community, offering an effective and accessible approach to retrieval in STEM education and beyond."}, {"title": "Limitation", "content": "However, our work still has some limitations that require further research. Firstly, STEM education differs significantly from higher education, K-12 education, humanities education, and other scenarios in terms of data and usage requirements. A key challenge for future research is how to maintain efficient retrieval performance while adapting to a wider range of educational scenarios.\nAdditionally, we plan to exploring how to efficiently acquire various professional educational knowledge based on VLMs. These improvements aim to make Uni-Retrieval more versatile and impactful across diverse educational domains."}]}