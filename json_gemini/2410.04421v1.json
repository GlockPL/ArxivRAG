{"title": "DISENTANGLING REGIONAL PRIMITIVES FOR IMAGE GENERATION", "authors": ["Zhengting Chen", "Lei Cheng", "Lianghui Ding", "Quanshi Zhang"], "abstract": "This paper presents a method to explain the internal representation structure of a neural network for image generation. Specifically, our method disentangles primitive feature components from the intermediate-layer feature of the neural network, which ensures that each feature component is exclusively used to generate a specific set of image regions. In this way, the generation of the entire image can be considered as the superposition of different pre-encoded primitive regional patterns, each being generated by a feature component. We find that the feature component can be represented as an OR relationship between the demands for generating different image regions, which is encoded by the neural network. Therefore, we extend the Harsanyi interaction to represent such an OR interaction to disentangle the feature component. Experiments show a clear correspondence between each feature component and the generation of specific image regions.", "sections": [{"title": "1 INTRODUCTION", "content": "The interpretability of deep neural networks (DNNs) has received increasing attention along with the fast development of deep learning. However, there is a clear technical boundary between techniques of explaining a single scalar output score of a DNN\u00b9 and methods of explaining the high-dimensional output (e.g., an image) of a DNN. For example, for DNNs for classification, attribution methods (Simonyan, 2013; Shrikumar et al., 2017; Selvaraju et al., 2017) were developed to estimate attributions of input variables to the scalar classification confidence. Zeiler & Fergus (2014) and desai & Ramaswamy (2020) visualized inference patterns encoded by a DNN, which determined the scalar classification confidence. In contrast, for image generation, the generated image contains much richer information than a scalar classification confidence, so it is difficult to apply previous explanation techniques designed for a single scalar output of a DNN. Instead, image generation is usually explained by controlling image generation results via input engineering (H\u00e4rk\u00f6nen et al., 2020; Voynov & Babenko, 2020).\nTherefore, the essence of such difference in the explanation techniques lies in the two facts. (1) The explanation of a single scalar output can be considered to explain the the structure of a DNN's inference logic. (2) In comparison, there is no solid theory developed to explain the internal representation structure of the neural network for image generation. For example, as Figure 1 shows, both attribution/importance scores of input variables to the classification confidence (Selvaraju et al., 2017) and interactions between input variables for inference (Ren et al., 2024a; 2023a) all reflect potential representation structure of the DNN.\nHowever, how to explain the internal representation structure of an image-generation model has not been sophisticatedly formulated. This problem can be discussed in the following two aspects."}, {"title": "2 RELATED WORK", "content": "The interpretability analysis of GAN model. In recent years, numerous studies have explored the interpretability of GAN models. Some studies explained the GAN models by analyzing the latent space of GANs (Abdal et al., 2021; H\u00e4rk\u00f6nen et al., 2020; Lang et al., 2021; Patashnik et al., 2021). For instance, H\u00e4rk\u00f6nen et al. (2020) employed principal component analysis (PCA) to identify the main directions in the latent space. By making layer-wise perturbations along these directions, it achieved interpretable control of the image generation process. While most of these works focused on GAN interpretability from the perspective of input manipulation, some other works took a different approach by examining the role of intermediate neurons in the generation process. For example, Bau et al. (2018) identified specific neurons associated with certain object categories. In this way, this method enabled people to control the presence of certain objects in the generated images by adjusting the activation of the neurons.\nUnlike previous output control based on input engineering, we aim to explore the internal representation structure of an image-generation network, which represents a new explanation perspective. We disentangle feature components that control the generation of specific regions, and we discover that the feature component can be formulated as the OR relationship between the demands of reconstructing different image regions.\nInteraction-based DNN explanation. In the field of explainable AI, an emerging question is whether the decision-making process of DNNs can be interpreted as a set of sparse symbolic concepts. To address this, a theoretical system based on the Harsanyi interaction has been proposed to explain how symbolic concepts are encoded by DNNs. Over the past three years, about 20 articles have been published in the field of explainable AI, which aimed to tackle the mathematical possibility of explaining DNN inference logic through a limited set of logical patterns. Most of these studies were surveyed by Ren et al. (2024a). Specifically, Sundararajan et al. (2020); Janizek et al. (2021); Tsai et al. (2023) proposed different types of interactions between input variables of a DNN. Ren et al. (2023a) used the Harsanyi dividend (Harsanyi, 1958) to represent the AND interaction in a DNN. They also experimentally discovered that DNNs usually encoded a limited set of interactions between input variables, i.e., the sparsity of AND interactions. Li & Zhang (2023) revealed that low-order interactions exhibited higher transferability across different input samples in discriminative neural networks. Ren et al. (2024a) proved the three common conditions under which the sparsity of interactions could be guaranteed. Ren et al. (2023b) proposed a method to learn optimal masked states of input variables based on interactions and alleviated the bias of the Shapley value caused by the sub-optimal masked states of input variables. Furthermore, Chen et al. (2024) extracted common interactions across different neural networks, and interactions shared by different neural networks usually represented generalizable inference patterns. Cheng et al. (2024) proposed to extract the interactions from the intermediate layers of neural networks, so as to illustrate how DNNs gradually learned and forgot inference patterns during forward propagation.\nIn addition, the interaction theory can also explain the representation power of a neural network. Specifically, this can be elaborated as follows. Wang et al. (2020) discovered and proved the negative correlation between DNNs' adversarial transferability and the interaction inside adversarial perturbations. Ren et al. (2021) found adversarial attacks primarily affected the high-order interactions than low-order interactions. Similarly, Zhou et al. (2024) revealed that low-order interactions tended to generalize better than high-order interactions. Liu et al. (2023) explained the intuition that DNNs learned low-order interactions more easily than high-order interactions. Deng et al. (2022)"}, {"title": "3 EXPLAINING REPRESENTATION STRUCTURE OF IMAGE GENERATION", "content": "In this paper, we aim to extract the internal representation structure encoded by a DNN for image generation. Instead of generating each pixel independently, lots of empirical findings (H\u00e4rk\u00f6nen et al., 2020; Voynov & Babenko, 2020) all showed that a DNN usually encoded a set of regional patterns as the internal structure of an image, and the generation of an image could be considered as the superposition of the pre-encoded regional patterns.\nTherefore, in this paper, we extend the Harsanyi interaction to represent such regional patterns from a trained DNN. Let a DNN generate an image x with a certain input code. We divide the generated image into n = H. W images, denoted by x = [x1,...,xn]T. N = {1,2..., n} denotes the set of indices for image regions. Let f \u2208 RD denote the feature of an intermediate layer of the DNN. Then, the objective of this study is to decompose the feature f into m feature components, so as to let each feature component Afk exclusively generate a specific regional pattern.\nx = g(f), f = fo + Af1 + \u2206 f2 + ... + Afm\nwhere fo represents a baseline feature as a non-informative feature state, and M = {1,2,...,m} denotes the index set of all feature components. fo can be set as the average feature over all features given different input codes z. i.e., fo = Ez[d(z)], where d(\u00b7) denotes the modules of the image-generation model between the input code z and the intermediate feature. We use g(f) to represent the image generated by the feature f, i.e., z \u00e0 f x.\nTwo requirements for feature decomposition. In this way, each feature component Afk added upon the baseline feature fo is supposed to exclusively generate a certain regional pattern Sk N, and thus can be taken as the primitive patterns encoded by the DNN. Sk is also termed the action region of the k-th feature component. In this way, the generation of specific image regions is controlled by a set \u03a9 \u2264 M of feature components, denoted by F(\u03a9) def fo + \u03a3\u03ba\u03b5\u03a9\u0394fk. In particular, F(M) = f. To achieve this, the feature decomposition is conducted w.r.t. the following two requirements.\nRequirement 1: Each feature component Afk exclusively generates a specific set of image regions Sk N, without affecting other image regions. I.e., the addition of \u2206fk to F(\u03a9) should not change the generation of other regions.\n\u2200k \u2208 M, \u2200 \u2286 M \\ {k}, gN\\Sk(F(\u03a9)) = gN\\Sk(F(\u03a9\u222a{k}))\ns.t. F(\u03a9\u222a{k}) = F(\u03a9) + \u2206fk\nIN\\S\u2081(\u00b7) denotes image regions in N \\ Sk selected from the generated image g(\u00b7).\nRequirement 2-a. The generation of each i-th image region can be fully determined by the superposition of all feature components that cover the i-th region.\nxi = gs={i}(F(\u03a9)), subject to \u03a9 = {k | i \u2208 Sk}\ni.e., g(F(\u03a9)) well generates the i-th region x\u2081 of the target image x."}, {"title": "3.1 PRELIMINARY: AND INTERACTION", "content": "As the theoretical foundation of the explanation of an image-generation network, let us first introduce the definition of interactions in the task of image classification. Given an input image x, let\nv(x) denote the output of a DNN for image classification. We can set v(x) = log p(y=ytruth |x) to denote the classification confidence. Let us divide the image x into n = H. W regions, and therefore we rewrite x = [X1,X2,...,xn]T, where xk denotes the k-th image region. We use N = {1,2..., n} as the set of indices of all image regions. For each specific set S \u2264 N of image regions, the numerical effect of the Harsanyi interaction between image regions in S is computed as\nI(S) def ETCS(-1)|S\\-T.u(T)\nwhere u(T) def v (x7) \u2013 v (xg), and v (x7) denotes the classification confidence w.rt. the true category of the masked sample xr, where regions of N\\T are masked, while regions in T remain unchanged. Therefore, u(N) = v(x) \u2013 v(xg) represents the overall effect of all the input variables.\nEach Harsanyi interaction represents an AND relationship between input variables encoded by the DNN, and contributes a certain effect I(S) to the network output v(x). For example, as shown in Fig 1(b), the DNN encodes the non-linear relationship between S = {head,mantle, body...} to form a bird pattern. The co-appearance of all images regions in S triggers the AND relationship of the bird pattern and makes an effect I(S|x) to the classification confidence u(N). The absence of any region in S will remove the effect I(S|x) from u(N).\nRen et al. (2024a) and Zhou et al. (2023) have discovered and partially proven the sparsity property and the universal-matching property of the interactions, as the mathematical guarantee for taking interactions as faithful primitive inference patterns encoded by the DNN. According to Theorem 3.1, we can construct a surrogate logical model h(\u00b7) based-on the extracted interactions. This surrogate logical model can accurately fit the classification confidence u(\u00b7) of a DNN, no matter how the input is masked, i.e., \u2200T \u2286 N, u(T) = h(x\u012b). The above property is called the universal-matching property.\nTheorem 3.1. (Universal-matching property (Ren et al., 2024a), also proved in Appendix B). Given an input sample x, the output u(T) on each masked sample {x\u0442 | T \u2286 N} can be well matched by a surrogate logical model h(x\u012b). The surrogate logical model sums up effects of all interactions that are triggered by the masked sample x\u012b as the output score.\n\u221aT \u2286 N, u (T) = h (x\u0442),\nsubject to h(x) def \u03a3 1(XT triggers AND ). I (S) = \u2211I (S)"}, {"title": "3.2 TWO REQUIREMENTS TO EXPLAIN REPRESENTATION STRUCTURE", "content": "In this paper, we aim to extract the internal representation structure encoded by a DNN for image generation. Instead of generating each pixel independently, lots of empirical findings (H\u00e4rk\u00f6nen et al., 2020; Voynov & Babenko, 2020) all showed that a DNN usually encoded a set of regional patterns as the internal structure of an image, and the generation of an image could be considered as the superposition of the pre-encoded regional patterns.\nTherefore, in this paper, we extend the Harsanyi interaction to represent such regional patterns from a trained DNN. Let a DNN generate an image x with a certain input code. We divide the generated image into n = H. W images, denoted by x = [x1,...,xn]T. N = {1,2..., n} denotes the set of indices for image regions. Let f \u2208 RD denote the feature of an intermediate layer of the DNN. Then, the objective of this study is to decompose the feature f into m feature components, so as to let each feature component Afk exclusively generate a specific regional pattern.\nx = g(f), f = fo + Af1 + \u2206 f2 + ... + Afm\nwhere fo represents a baseline feature as a non-informative feature state, and M = {1,2,...,m} denotes the index set of all feature components. fo can be set as the average feature over all features given different input codes z. i.e., fo = Ez[d(z)], where d(\u00b7) denotes the modules of the image-generation model between the input code z and the intermediate feature. We use g(f) to represent the image generated by the feature f, i.e., z \u00e0 f x.\nTwo requirements for feature decomposition. In this way, each feature component Afk added upon the baseline feature fo is supposed to exclusively generate a certain regional pattern Sk N, and thus can be taken as the primitive patterns encoded by the DNN. Sk is also termed the action region of the k-th feature component. In this way, the generation of specific image regions is controlled by a set \u03a9 \u2264 M of feature components, denoted by F(\u03a9) def fo + \u03a3\u03ba\u03b5\u03a9\u0394fk. In particular, F(M) = f. To achieve this, the feature decomposition is conducted w.r.t. the following two requirements.\nRequirement 1: Each feature component Afk exclusively generates a specific set of image regions Sk N, without affecting other image regions. I.e., the addition of \u2206fk to F(\u03a9) should not change the generation of other regions.\n\u2200k \u2208 M, \u2200 \u2286 M \\ {k}, 9N\\Sk(F(\u03a9)) = 9N\\Sk(F(\u03a9\u222a{k}))\ns.t. F(\u03a9\u222a{k}) = F(\u03a9) + \u2206fk\nIN\\S\u2081(\u00b7) denotes image regions in N \\ Sk selected from the generated image g(\u00b7).\nRequirement 2-a. The generation of each i-th image region can be fully determined by the superposition of all feature components that cover the i-th region.\nxi = gs={i}(F(\u03a9)), subject to \u03a9 = {k | i \u2208 Sk}\ni.e., g(F(\u03a9)) well generates the i-th region x\u2081 of the target image x."}, {"title": "Requirement 2-\u03b2.", "content": "Requirement 2-\u03b2. The generation of image regions in \u015c can also be exclusively determined by the superposition of feature components that intersect with \u015c, i.e.,\nx = 95(F(\u03a9)), subject to \u03a9 = {k | Sk \u2229 \u015c \u2260 0}\nwhere \u1f6e denotes the set of feature components whose action regions partially cover regions in \u015c.\nRequirement 1 shows that all regions in Sk can be considered as a singleton visual pattern encoded by the DNN. The above requirements ensures that the feature decomposition is a faithful explanation of the representation structure of the DNN generating the image x.\nMinimal feature for regional generation. We can consider that Af1+f2+...+ fm are features required to be added to the baseline feature fo to generate the entire image x, in Equation (4). If we are only requested to reconstruct a subset \u015c of image regions, then we only need to use feature components F(\u03a9) = fo + \u03a3\u03ba\u03b5\u03c0 \u0394fk, s.t. \u03a9 = {k | Sk \u2229 \u015c \u2260 0} \u2286 M, and the addition of any further feature components will not affect the generation of regions in \u015c, according to above requirements. \u03a9\u015c denotes the set of feature components selected to reconstruct image regions in \u015c. In this way, we can consider F(\u2229) as the minimal feature of generating image regions in \u015c, denoted by u(\u015c) = F(\u03a9). On the other hand, the minimal feature u(\u015c) can be also estimated as follows.\nu(\u015c) = argmin\u2084(\u015d) ||u(\u015c)||L-1, w.r.t. x5 = 93(u(\u015c)).\nTherefore, the decomposition of feature components Afk w.r.t. above requirements can be written as follows.\nDecomposion of f = fo + KEM Afk w.r.t. Requirement 1,2-a,2-\u03b2\nmin{\u25b3fx} ||u(\u015c)||L-1 s.t. V\u015c \u2286 N, u(\u015c) = F(N) = fo + \u03a3\u03ba\u03b5n fk\nwhere \u2229 = {k | Sk \u2229 \u015c \u2260 0}\nImplementation details. The computation of the minimal feature u(\u015c) w.r.t. image regions in \u015c can be approximated as min ||u(\u015c)||L\u22121+\u00a7|||gs(u(\u015c)) - \u00d75||}-2, subject to ||g5(u(\u015c)) -x\u00f4||L-2 <\n\u03c4,\u03c5(\u015c) = fo + a \u2299 \u2206f. Here, we consider the minimal feature are all contained by the whole feature f, so we apply the empirical constrain u(\u015c) = fo + a \u00a9 (f \u2013 fo), w.r.t. a \u2208 [0, 1]D. Each feature dimension of a is in the range of [0,1]. is referred to as the element-wise multiplication. In this way, we constrain u(S) strictly locates within the range between fo and f."}, {"title": "3.3 USING INTERACTIONS FOR FEATURE DISENTANGLEMENT", "content": "The above subsection introduces how to compute the minimal feature component, and clarifies the mathematical connection between the feature components {\u2206fk}k and minimal features {u(\u015c)}\u00a7. Then, in this subsection, we introduce how to disentangle feature components Af1, ..., Afm from all minimal features, so as to let each feature component Afk exclusively generate regions in Sk.\nOR interaction. To this end, we prove that the proposed two requirements can be rewritten as follows.\n\u2022 For each feature component Afi whose action regions does not cover any regions in Sk, i.e, Sk\u2229S\u2081 = \u00d8, this component does not affect the generation of the image regions in Sk.\n\u2022 u(\u015c) = F(N) = fo + \u03a3\u03ba\u03b5\u00f1Afk, 2 = {k | \u015c \u2229 Sk \u2260 0}\nClearly, above two terms reflects an OR relationship in the selection of feature components towards the reconstruction of the target regions in \u015c. When we need to reconstruct image regions in \u015c, then the feature component Afk should be added to fo if and only if \u015c covers any regions in Sk. For example, as Figure 2 shows, let us consider a feature component Afkw.r.t. Sk = {1,3}. Then, this feature component must be added to fo when the reconstruction demand includes either the 1st or the 3rd image region. If the construction demand does not contains any region in Sk, then Afk should not be added. Therefore, we can consider the selection of the feature component Afk reflects an OR relationship between the target reconstruction regions."}, {"title": "4 EXPERIMENT", "content": "We tested our method on the BigGAN-128 model (Brock, 2018). We used the intermediate features in the layer2 for the feature decomposition, i.e., the output of the first ResBlock in BigGAN. The baseline feature component fo was computed as the average feature over different input codes z, i.e., computing fo = Ez[d(z)], when we set the DNN to generate images in different categories. Given"}, {"title": "4.1 VISUALIZING FEATURE COMPONENTS (OR INTERACTIONS)", "content": "Given a random input code z, we used the DNN to generate an image x, and extracted feature components used for image generation. We followed the feature decomposition method described in Section 3. In order to obtain the baseline feature fo, we used the DNN to generate different images in different categories, and extracted the average of intermediate features over all the generated images as the baseline feature fo in experiments. In order to compute the OR interaction corresponding each feature component, we divided the original images into 6\u00d76 regions in grids, and randomly chose 9 regions of the main part of the image as input variables. The parameter a was initialized to a vector with all-ones elements, and \u5165 was set to 10000. We optimized u(N), i.e., the corresponding minimal feature of all regions in N. For each other set of regions SC N, the solution corresponding to u(N) was set as the starting point of further computation of u(S). Then we followed the algorithm in Equation 10 to compute Ior(Sk). We used the L-2 norm ||\u2206f || L\u22122 to rank the interaction strength of each feature component (please see Appendix H for more detailed settings).\nFigure 5 shows the sparsity of the extracted interactions. This figure visualizes all elements of all the extracted interactions (feature components) by sorting their absolute values in a descending order. Most elements of feature components were almost zero, which verified the sparsity of the feature components.\nFigure 3 verifies that the different feature components were exclusively responsible for the reconstruction of their own action regions. The interpretability of feature components enabled us to control the reconstruction of different image regions by adding different feature components."}, {"title": "4.2 VERIFYING THE DISENTANGLED FEATURE COMPONENTS (OR INTERACTIONS)", "content": "Towards the reconstruction of a single image region. In order to verify whether the disentangled feature components were exclusively responsible for the reconstruction of specific image regions, we randomly selected a region i \u2208 N of the image x. We chose all the feature components whose action regions included the i-th region. Then, we added these feature components into the baseline feature fo, and obtained f = fo + \u03a3ken fk, s.t. \u03a9 = {k|i \u2208 Sk}. Figure 7(a) shows image reconstruction results when we added the ratio p of feature components that covered the i-th image region. We discovered that the feature f only well constructed the i-th region in the image x. The target image region was gradually reconstructed when we added increasing numbers of feature components.\nTowards the reconstruction of a set of image regions. We further evaluated our method in the reconstruction of a set of image regions. We randomly selected a set of regions SC N in the image x. We chose all the feature components whose action regions partially covered S. Then, we added these feature components into the baseline feature fo, and obtained f = fo + \u03a3\u03ba\u03b5n fk, s.t. \u03a9 = {k|S\u2229Sk \u2260 0}. Figure 7(b) shows image reconstruction results when we added the ratio p of feature components whose action regions partially covered S. We discovered that the feature f only well constructed the regions in S in the image x. The target regions were gradually reconstructed when we added increasing numbers of feature components. Examining image reconstruction effects of irrelevant feature components. From another perspective, we also examined whether irrelevant feature components would affect the generation of a region. We randomly selected the i-th region in image x. Then, we gradually added all the feature components whose action region did not cover the i-th region, i.e., f = fo + \u03a3ken\u2206fk s.t. \u03a9 = {k|i \u00a2 Sk}. Figure 6 shows images reconstructed by adding the ratio p of irrelevant feature components in \u03a9. We found that although we had added all irrelevant feature components, the target image region was not changed."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduce a new method to explain the internal representation of a image generation neural network. We use the OR interaction to disentangle components from the intermediate feature"}, {"title": "3.1 PRELIMINARY: AND INTERACTION", "content": "As the theoretical foundation of the explanation of an image-generation network, let us first introduce the definition of interactions in the task of image classification. Given an input image x, let\nv(x) denote the output of a DNN for image classification. We can set v(x) = log p(y=ytruth |x)/ 1-p(y=ytruth |x) to denote the classification confidence. Let us divide the image x into n = H. W regions, and therefore we rewrite x = [X1,X2,...,xn]T, where xk denotes the k-th image region. We use N = {1,2..., n} as the set of indices of all image regions. For each specific set S \u2264 N of image regions, the numerical effect of the Harsanyi interaction between image regions in S is computed as\nI(S) def ETCS(-1)|S-T.u(T)\nwhere u(T) def v (x7) \u2013 v (xg), and v (x7) denotes the classification confidence w.rt. the true category of the masked sample xr, where regions of N\\T are masked, while regions in T remain unchanged. Therefore, u(N) = v(x) \u2013 v(xg) represents the overall effect of all the input variables.\nEach Harsanyi interaction represents an AND relationship between input variables encoded by the DNN, and contributes a certain effect I(S) to the network output v(x). For example, as shown in Fig 1(b), the DNN encodes the non-linear relationship between S = {head,mantle, body...} to form a bird pattern. The co-appearance of all images regions in S triggers the AND relationship of the bird pattern and makes an effect I(S|x) to the classification confidence u(N). The absence of any region in S will remove the effect I(S|x) from u(N).\nRen et al. (2024a) and Zhou et al. (2023) have discovered and partially proven the sparsity property and the universal-matching property of the interactions, as the mathematical guarantee for taking interactions as faithful primitive inference patterns encoded by the DNN. According to Theorem 3.1, we can construct a surrogate logical model h(\u00b7) based-on the extracted interactions. This surrogate logical model can accurately fit the classification confidence u(\u00b7) of a DNN, no matter how the input is masked, i.e., \u2200T \u2286 N, u(T) = h(x\u012b). The above property is called the universal-matching property.\nTheorem 3.1. (Universal-matching property (Ren et al., 2024a), also proved in Appendix B). Given an input sample x, the output u(T) on each masked sample {x\u0442 | T \u2286 N} can be well matched by a surrogate logical model h(x\u012b). The surrogate logical model sums up effects of all interactions that are triggered by the masked sample x\u012b as the output score.\nVT \u2286 N, u (T) = h (x\u0442),\nsubject to h(x) def \u03a3 1(XT triggers AND). I (S) = \u2211I (S)\n        SCN,S\u22600                                   SCT,S\u22600"}, {"title": "Requirement 2-\u03b2.", "content": "Requirement 2-\u03b2. The generation of image regions in \u015c can also be exclusively determined by the superposition of feature components that intersect with \u015c, i.e.,\nx = 95(F(\u03a9)), subject to \u03a9 = {k | Sk \u2229 \u015c \u2260 0}\nwhere \u1f6e denotes the set of feature components whose action regions partially cover regions in \u015c.\nRequirement 1 shows that all regions in Sk can be considered as a singleton visual pattern encoded by the DNN. The above requirements ensures that the feature decomposition is a faithful explanation of the representation structure of the DNN generating the image x.\nMinimal feature for regional generation. We can consider that Af1+f2+...+ fm are features required to be added to the baseline feature fo to generate the entire image x, in Equation (4). If we are only requested to reconstruct a subset \u015c of image regions, then we only need to use feature components F(\u03a9) = fo + \u03a3\u03ba\u03b5\u03c0 \u0394fk, s.t. \u03a9 = {k | Sk \u2229 \u015c \u2260 0} \u2286 M, and the addition of any further feature components will not affect the generation of regions in \u015c, according to above requirements. \u03a9\u015c denotes the set of feature components selected to reconstruct image regions in \u015c. In this way, we can consider F(\u2229) as the minimal feature of generating image regions in \u015c, denoted by u(\u015c) = F(\u03a9). On the other hand, the minimal feature u(\u015c) can be also estimated as follows.\nu(\u015c) = argmin\u2084(\u015d) ||u(\u015c)||L-1, w.r.t. x5 = 93(u(\u015c)).\nTherefore, the decomposition of feature components Afk w.r.t. above requirements can be written as follows.\nDecomposion of f = fo + KEM Afk w.r.t. Requirement 1,2-a,2-\u03b2\nmin{\u25b3fx} ||u(\u015c)||L-1 s.t. V\u015c \u2286 N, u(\u015c) = F(N) = fo + \u03a3\u03ba\u03b5n fk\nwhere \u2229 = {k | Sk \u2229 \u015c \u2260 0}\nImplementation details. The computation of the minimal feature u(\u015c) w.r.t. image regions in \u015c can be approximated as min ||u(\u015c)||L\u22121+\u00a7|||gs(u(\u015c)) - \u00d75||}-2, subject to ||g5(u(\u015c)) -x\u00f4||L-2 <\n\u03c4,\u03c5(\u015c) = fo + a \u2299 \u2206f. Here, we consider the minimal feature are all contained by the whole feature f, so we apply the empirical constrain u(\u015c) = fo + a \u00a9 (f \u2013 fo), w.r.t. a \u2208 [0, 1]D. Each feature dimension of a is in the range of [0,1]. is referred to as the element-wise multiplication. In this way, we constrain u(S) strictly locates within the range between fo and f."}, {"title": "3.3 USING INTERACTIONS FOR FEATURE DISENTANGLEMENT", "content": "The above subsection introduces how to compute the minimal feature component, and clarifies the mathematical connection between the feature components {\u2206fk}k and minimal features {u(\u015c)}\u00a7. Then, in this subsection, we introduce how to disentangle feature components Af1, ..., Afm from all minimal features, so as to let each feature component Afk exclusively generate regions in Sk.\nOR interaction. To this end, we prove that the proposed two requirements can be rewritten as follows.\n\u2022 For each feature component Afi whose action regions does not cover any regions in Sk, i.e, Sk\u2229S\u2081 = \u00d8, this component does not affect the generation of the image regions in Sk.\n\u2022 u(\u015c) = F(N) = fo + \u03a3\u03ba\u03b5\u00f1Afk, 2 = {k | \u015c \u2229 Sk \u2260 0}\nClearly, above two terms reflects an OR relationship in the selection of feature components towards the reconstruction of the target regions in \u015c. When we need to reconstruct image regions in \u015c, then the feature component Afk should be added to fo if and only if \u015c covers any regions in Sk. For example, as Figure 2 shows, let us consider a feature component Afkw.r.t. Sk = {1,3}. Then, this feature component must be added to fo when the reconstruction demand includes either the 1st or the 3rd image region. If the construction demand does not contains any region in Sk, then Afk should not be added. Therefore, we can consider the selection of the feature component Afk reflects an OR relationship between the target reconstruction regions."}, {"title": "4 EXPERIMENT", "content": "We tested our method on the BigGAN-128 model (Brock, 2018). We used the intermediate features in the layer2 for the feature decomposition, i.e., the output of the first ResBlock in BigGAN. The baseline feature component fo was computed as the average feature over different input codes z, i.e., computing fo = Ez[d(z)], when we set the DNN to generate images in different categories. Given"}, {"title": "4.1 VISUALIZING FEATURE COMPONENTS (OR INTERACTIONS)", "content": "Given a random input code z, we used the DNN to generate an image x, and extracted feature components used for image generation. We followed the feature decomposition method described in Section 3. In order to obtain the baseline feature fo, we used the DNN to generate different images in different categories, and extracted the average of intermediate features over all the generated images as the baseline feature fo in experiments. In order to compute the OR interaction corresponding each feature component, we divided the original images into 6\u00d76 regions in grids, and randomly chose 9 regions of the main part of the image as input variables. The parameter a was initialized to a vector with all-ones elements, and \u5165 was set to 10000. We optimized u(N), i.e., the corresponding minimal feature of all regions in N. For each other set of regions SC N, the solution corresponding to u(N) was set as the starting point of further computation of u(S). Then we followed the algorithm in Equation 10 to compute Ior(Sk). We used the L-2 norm ||\u2206f || L\u22122 to rank the interaction strength of each feature component (please see Appendix H for more detailed settings).\nFigure 5 shows the sparsity of the extracted interactions. This figure visualizes all elements of all the extracted interactions (feature components) by sorting their absolute values in a descending order. Most elements of feature components were almost zero, which verified the sparsity of the feature components.\nFigure 3 verifies that the different feature components were exclusively responsible for the reconstruction of their own action regions. The interpretability of feature components enabled us to control the reconstruction of different image regions by adding different feature components."}, {"title": "4.2 VERIFYING THE DISENTANGLED FEATURE COMPONENTS (OR INTERACTIONS)", "content": "Towards the reconstruction of a single image region. In order to verify whether the disentangled feature components were exclusively responsible for the reconstruction of specific image regions, we randomly selected a region i \u2208 N of the image x. We chose all the feature components whose action regions included the i-th region. Then, we added these feature components into the baseline feature fo, and obtained f = fo + \u03a3ken fk, s.t."}]}