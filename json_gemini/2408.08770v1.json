{"title": "Pessimistic Iterative Planning for Robust POMDPS", "authors": ["Maris F. L. Galesloot", "Marnix Suilen", "Thiago D. Sim\u00e3o", "Steven Carr", "Matthijs T. J. Spaan", "Ufuk Topcu", "Nils Jansen"], "abstract": "Robust partially observable Markov decision processes (robust POMDPs) extend classical POMDPs to handle additional uncertainty on the transition and observation probabilities via so-called uncertainty sets. Policies for robust POMDPs must not only be memory-based to account for partial observability but also robust against model uncertainty to account for the worst-case instances from the uncertainty sets. We propose the pessimistic iterative planning (PIP) framework, which finds robust memory-based policies for robust POMDPs. PIP alternates between two main steps: (1) selecting an adversarial (non-robust) POMDP via worst-case probability instances from the uncertainty sets; and (2) computing a finite-state controller (FSC) for this adversarial POMDP. We evaluate the performance of this FSC on the original robust POMDP and use this evaluation in step (1) to select the next adversarial POMDP. Within PIP, we propose the RFSCNET algorithm. In each iteration, RFSCNET finds an FSC through a recurrent neural network trained using supervision policies optimized for the adversarial POMDP. The empirical evaluation in four benchmark environments showcases improved robustness against a baseline method in an ablation study and competitive performance compared to a state-of-the-art robust POMDP solver.", "sections": [{"title": "1 Introduction", "content": "Partially observable Markov decision processes (POMDPs; Kaelbling et al., 1998) are the standard model for decision-making under uncertainty. Policies select actions based on limited state informa- tion towards some objective, e.g., minimizing the expected cost. The standard assumption is that probabilities are precisely known for the transition and observation functions of a POMDP. This assumption is unrealistic, for example, when probabilities are derived from historical data or sensors with limited precision (Thrun et al., 2005), or when the uncertainty is expressed by domain experts.\nRobust POMDPs (RPOMDPs; Osogami, 2015) overcome this assumption by introducing uncertainty sets, i.e., sets of probabilities, on the transition and observation functions. Robust policies for RPOMDPs account for this uncertainty by optimizing against the worst-case instances within the uncertainty sets. As a consequence, a robust policy has a lower bound on its actual performance.\nFinding optimal policies for (non-robust) POMDPs is an extensively studied problem (Poupart and Boutilier, 2003; Hansen, 1997; Smith and Simmons, 2004). The policies inherently depend on the sequences of past actions and observations (the history), i.e., they require memory. Recurrent neural networks (RNNs) are a common formalism to represent such memory-based policies (Ni et al., 2022) thanks to their ability to learn sufficient statistics of the history (Lambrechts et al., 2022). As such, RNNs have successfully been used in POMDPs within reinforcement learning (Bakker, 2001; Hausknecht and Stone, 2015; Heess et al., 2015) or planning settings (Carr et al., 2019, 2020, 2021).\nAlternatively, finite-state controllers (FSCs, Meuleau et al., 1999) offer a more structured way to represent policies and, given the model, allow for a precise and effective numerical evaluation"}, {"title": "2 Preliminaries", "content": "The set of all distributions over X is denoted by \u2206(X). A probability distribution \u03bc\u2208 \u0394(X) is called Dirac if \u03bc(x) = 1 for precisely one x \u2208 X and zero otherwise. The number of elements in a"}, {"title": "2.1 Policies and Expected Costs", "content": "A policy resolves the action choices in a POMDP, and is a function \u03c0: \u0397 \u2192 \u0394(A) mapping histories to distributions over actions. Correspondingly, policies may also be belief-based \u03c0: B \u2192 \u0394(A). The set of all history-based policies is II. We seek to find a policy \u03c0\u2208 II that minimizes the expected cost of reaching goal states GCS, also known as the stochastic shortest path (SSP) problem (Bertsekas, 2005). For any trajectory w, the cumulative cost P\u25caG: \u03a9 \u2192 R\u2265o\u222a{+\u221e} is (Forejt et al., 2011):\n$$POG(W) = \\begin{cases} \\sum_{t=0}^{\\min\\{t | st \\in G\\}-1} C(st, at) & \\forall t \\in N, St \\notin G, \\\\ +\\infty & otherwise. \\end{cases}$$\nThe SSP objective is to find an optimal policy \u03c0\u2208 II that minimizes the expected cumulative cost JT of the trajectories generated by playing \u3160 under the transition function T:\n$$\u03c0* = arg\\inf_{\\Pi \\in \\Pi} JT, JT = E_{\\pi,T} [POG | s0 \\sim bo].$$"}, {"title": "3 Robust Markov Models", "content": "A robust MDP (RMDP) is an MDP where the transition probabilities are not precisely given but belong to a predefined set of distributions known as the uncertainty set (Nilim and Ghaoui, 2005). We fix a specific type of RMDP where the transition function maps to a probability interval or zero.\nDefinition 3 (RMDP). An RMDP with interval uncertainty is a tuple U = (S, A,T,C), where T: S \u00d7 A \u2192 (S \u2192 I \u222a {0}) is the uncertain transition function that maps each transition to either a probability interval in I, or probability 0 whenever the transition does not exist.\nThe uncertainty is resolved in a game-like manner where the agent (choosing the actions) and nature (choosing a probability distribution from the uncertainty set) alternate turns (Iyengar, 2005). We assume independence between all state-action pairs, an assumption known as (s, a)-rectangularity, which means that nature's choices are not restricted by any previous choice (Iyengar, 2005; Wiese- mann et al., 2013). Thus, the uncertainty set T is convex and factorizes over state-action pairs:\n$$T= \\underset{(s,a) \\in S\\times A}{\\times} T(s,a), T(s, a) = \\{T(s,a) \\in \\Delta(S) | \\forall s'\\in s : T(s' | s,a) \\in IU \\{0\\}\\} .$$\nFurthermore, we assume that nature's resolution of uncertainty is static. This means that, if the model contains a cyclic structure, nature's choice does not change when state-action pairs are revisited.\nRMDPs have two optimal value functions and associated optimal policies: one where the agent and nature play adversarially, and one where they play cooperatively. The former is known as the robust setting, and the latter as optimistic. For ease of presentation, we focus on the robust setting for the remainder of the paper. Optimal robust Q and V values can be computed by extending dynamic programming to robust dynamic programming (Iyengar, 2005; Nilim and Ghaoui, 2005). The optimal robust V and Q values are the fixed points * and Q* of the following recursive equations*:\n$$Vn(s) = \\min_{\\alpha \\in A} 2n (s, a), 2n+1(s,a) = C(s, a) + \\underset{T(s,a) \\in T(s,a)}{sup} {\\sum_{S'\\in S}T(s' | s,a) V_n(s')}$$\nUnder (s, a)-rectangularity and the interval uncertainty model, the inner optimization problem can be efficiently solved via a bisection algorithm (Nilim and Ghaoui, 2005, Section 7.2). The robust state-values and optimal policy, which is memoryless deterministic under (s, a)-rectangularity, can be derived from the robust action values Q* in the same way as for standard MDPs (Iyengar, 2005)"}, {"title": "4 Supervised Learning of Robust FSCs", "content": "We now present our implementation of the PIP framework, named RFSCNET, to compute robust policies for RPOMDPs. We initialize with an arbitrary POMDP instance M \u2208 M and a supervision policy \u3160\u043c that approximates the optimal policy \u03c0* for the POMDP M, which is used to train the RNN. Then, we enter the iterative procedure outlined below, which implements the two major parts of PIP, described in more detail in this section and the next section, respectively. Part one corresponds with the left-hand side in Figure 1, and consists of computing FSCs for adversarial POMDPs:\n(1) Simulate the supervision policy \u03c0\u2122 on M and collect the histories and the action distribu- tions of M into a data set D (Section 4.1).\n(2) Train the RNN policy \u03c0\u00ba on the data D and extract an FSC \u03c0f (Sections 4.2 and 4.3).\nPart two corresponds to the right-hand side of Figure 1. Here, we implement the evaluation of the FSC, and, subsequently, selection of the next adversarial POMDP with respect to the current policy:\n(3) Evaluate the FSC's performance exactly through robust dynamic programming (Section 5.1).\n(4) Update the adversarial POMDP M \u2208 M and supervision policy \u03c0\u039c and start a new iteration at step 1 (Section 5.2).\nWe determine whether to stop the iteration at step (3) based on the FSC's evaluation when the worst- case expected cost of the FSC satisfies a target threshold, or, alternatively, we may set a maximum number of iterations. When the procedure does not terminate, we continue from step 1 and save the best policy found thus far. We save the best policy based on the FSC's evaluation."}, {"title": "4.1 Data Generation", "content": "To train the RNN, we generate a data set D by simulating the supervision policy \u3160\u043c: \u0412 \u2192 \u0394(\u0391) on the current (adversarial) POMDP ME M. We discuss how to obtain M and TM in the next sections. We aggregate a batch of I \u2208 N finite-horizon histories and associated action distributions of the supervision policy \u03c0\u043c up to length \u0397 \u2208 \u039d.\nDuring simulation for i = 1, . . ., I, we compute the beliefs b\u00b2) from history h(2) derived from the probabilities of M at each time step 0 < t < H, with bo from M. Then, \u03bc\u03b5\u00b2 =\u03c0\u03bc(b) represents the action distributions of the supervision policy during the simulation, and af\u00ba) ~ \u03bc\u03ad) is the action played in simulation i at time t. Then, D = {{h(2), \u03bc\u03b5}{1}{=1 consists of I simulations of H histories and their associated action distributions.\nAs already mentioned in Section 2, computing an optimal policy in POMDPs for our objective is undecidable. Furthermore, our iterative procedure relies on computing several policies for different POMDPs. Thus, computational efficiency is a concern, and fast approximate methods are pre- ferred. Therefore, we use an approximation QM of the belief-based values QM: B \u00d7 A \u2192 R for M\u2208 M. The supervision policy is then derived by taking the minimizing action, i.e., \u03c0\u043c(b) = argmina\u2208 A QM(b, a). For training, we interpret these deterministic policies as stochas- tic by taking Dirac distributions. We consider the algorithms QMDP (Littman et al., 1995) and the fast-informed bound (FIB; Hauskrecht, 2000) to compute the approximations QMDP and QFIB respectively.\nQMDP may compute sub-optimal policies as it assumes full state observability after a single step, neglecting information-gathering (Littman et al., 1995). The values QFIB are tighter than the one given by QMDP Since it factors in the observation of the next state. Although QFIB is computationally more expensive than QMDP, QFIB updates are still of polynomial complexity, allowing us to compute it for each M\u2208 M efficiently, See Appendix B for more details.\nAlternatives. We opt for QMDP and QFIB because of their computational efficiency. Nonetheless, other POMDP solution methods may also be used, such as Monte Carlo planning (Silver and Veness, 2010) or variants of heuristic-search value iteration (Smith and Simmons, 2004; Hor\u00e1k et al., 2018)."}, {"title": "4.2 RNN Policy Training", "content": "We train the RNN policy \u03c0\u21e8 on the data set D collected from M \u2208 M. The training objective \u03a6 of the RNN is to minimize the distance between the distributions of the RNN policy \u03c0\u00ba and the distributions \u03bc of the supervision policy \u03c0\u039c:\n$$\\Phi = \\min_{\\theta} \\frac{1}{|DH|} \\sum_{i=0}^{|DH|} \\sum_{t=0}^{H} DKL (\\pi_{\\theta} (h^{(i)}_t) || \\mu_t^{(i)}),$$\nwhere DKL is the Kullback-Leibler divergence, the histories h are inputs to the RNN, and the action distributions \u03bc are labels. We optimize the parameters & for this objective by calculating the gradient via backpropagation through time.\nWe opt for a model-based approach, where we use approximate solvers to compute the supervision policy \u03c0\u039c for M\u2208 M and train the RNN to imitate \u03c0\u2122 in a supervised manner. Thus, we know we can stop training once the loss for I is sufficiently low. Alternatively, one could employ a model-free objective such as recurrent policy gradients (Wierstra et al., 2007). However, the latter neglects the available information from the model and, therefore, requires a large number of samples to converge (Moerland et al., 2020). Furthermore, it is unclear when training should be stopped.\nAs the POMDP M\u2208 M changes over the iterations to be adversarial against the current policy, we need a finite-memory representation of the policy to find the new adversarial POMDP M'."}, {"title": "4.3 Extracting an FSC from an RNN", "content": "We cluster the hidden memory states of the RNN (Zeng et al., 1993; Omlin and Giles, 1996) to find a finite-memory policy in the form of an FSC. There are multiple ways to achieve such a clustering. Prior work (Carr et al., 2021) uses a quantized bottleneck network (QBN; Koul et al., 2019) to reduce the possible memory states to a finite set. They train the QBN post hoc by minimizing the mean-squared error on the hidden states generated by the RNN. Alternatively, it can be trained end-to-end by updating the parameters with the gradient calculated with \u03a6, which we name QRNN for quantized RNN. Moreover, similar to post hoc training of the QBN, we can run a clustering algorithm such as k-means++ (Arthur and Vassilvitskii, 2007) to minimize the in-cluster variance of the hidden states. For post hoc training, we employ the histories in D to generate the RNN hidden states. We consider all three methods in this paper. For more details on the QBN, we refer to Appendix D.1.\nInstead of through simulations, as done in Koul et al. (2019), we utilize the model to construct the FSC. The clustering determines a finite set N from the RNN's hidden states in H. We find the FSC's memory update \u03b7 by executing a forward pass of the RNN's memory update \u1f22 for each reconstruction of n \u2208 N, which produces the next memory nodes n' \u2208 N and RNN hidden state \u0125 \u2208 H for each z \u2208 Z by exploiting the batch dimension. Then, the action mapping d for n and z is given by the distribution of the RNN policy network o$(h) for the next memory state. The initial node no is determined by the RNN's initial state and we prune any unreachable nodes from the FSC."}, {"title": "5 Robust Policy Evaluation and Adversarial Selection of POMDPs", "content": "This section represents the right-hand side of Figure 1. For the FSCs found by the approach explained in the previous section, we present our methods for robust policy evaluation of the FSC and, subsequently, of selecting the worst-case adversarial POMDP M' \u2208 M from the uncertainty set."}, {"title": "5.1 Robust Policy Evaluation", "content": "The performance of a finite-memory policy represented by \u03c0f = (N, no, \u03b7, \u03b4) in an (s, a)-rectangular RPOMDP, such as we have, is given by the following robust value function, for all (s, n) \u2208 S\u00d7N:\n$$V^*_f ((s,n)) = \\sum_{a\\in A} (\\delta(a | n, O(s))C(s, a) + \\underset{T(s,a) \\in T(s,a)}{sup} {\\sum_{s'\\in S} \\sum_{n'\\in N}T(s' | s, a)\\delta(a | n, O(s))[n' = \\eta(n, O(s))] V^*_f((s', n'))}).$$"}, {"title": "5.2 Finding Adversarial POMDP Instances", "content": "We now construct a heuristic to find a new POMDP instance M'EM that constitutes the local worst-case instance for the current policy under (s, a)-rectangularity of the RPOMDP. Let \u03c0f (\u039d, \u03b7\u03bf, \u03b4, \u03b7) be the current FSC. Given the robust value function Vf computed at the previous step, we aim to find a POMDP M' \u2208 M that is adversarial to the FSC \u03c0f and, therefore, induces its worst-case value. We compute the following transition probability under the additional constraint that probabilities are dependent on the memory nodes n \u2208 N (i.e., (s, a)-rectangularity):\n$$\\widehat{T}(s, a) = arg\\underset{\\widehat{T}(s,a) \\in T(s,a)}{sup} \\sum_{n \\in N} \\sum_{s'\\in S} \\sum_{n'\\in N} T^{\\pi}_f ((s', n') | (s,n))V^*_f ((s', n')).$$\nWe construct a linear program (LP) that precisely encodes our requirements. Let T(\u00b7 | s, a) be the vector of length S of optimization variables representing the transition function probabilities for each state-action pair (s, a) \u2208 S \u00d7 A. The LP is then given by:\n$$max \\widehat{T}(s,a) \\sum_{n \\in N} \\sum_{s' \\in S} \\sum_{n' \\in N} [n' = \\eta(n, O(s))] \\delta(a|n, O(s))\\widehat{T}(s' | s,a) V^*_f ((s', n'))$$\ns.t.  \\sum_{s' \\in S} \\widehat{T}(s' | s, a) = 1, and, \\forall s' \\in S: \\widehat{T}(s' | s, a) \\in T(s,a)(s').$$\nSolving this LP yields assignments for the variables that, for all (s, a) \u2208 S \u00d7 A, satisfy T(s' | s, a) \u2208 T(s, a)(s') for all s' \u2208 S, and \u00ce(s, a) \u2208 \u2206(S). By construction, the assignments are valid probability distributions inside each respective interval and yield a heuristic for the worst possible value for the given FSC under (s, a)-rectangularity. The variables \u00ce determine the transition function of the adversarial POMDP M' = \u3008S, A,T, C, Z, O). With this new POMDP instance M' \u2208 M, we repeat the loop from the first step in Section 4.1 of our learning scheme until the FSC satisfies a target value or we reach the maximum number of iterations."}, {"title": "6 Experimental Evaluation", "content": "We empirically assess different aspects of RFSCNET according to the following research questions:\n(1) Robustness. Does RFSCNET provide robustness against model uncertainty? How does it compare to a BASELINE that trains on a fixed POMDP, ignoring model uncertainty?\n(2) Configuration sensitivity. How do different configurations of supervision policies and clustering methods affect the performance of RFSCNET and BASELINE?\n(3) Comparison with the state-of-the-art. How does RFSCNET's performance compare to SCP (Cubuktepe et al., 2021), a state-of-the-art method for RPOMDPS?\n(4) Memory size sensitivity. How does the FSC size affect RFSCNET and SCP's performance?\nWe use four well-known POMDP environments from the literature, extended to RPOMDPs: an Aircraft collision avoidance environment (Kochenderfer et al., 2015; Cubuktepe et al., 2021), and three partially observable grid-worlds that contain adversaries (Junges et al., 2021) named Avoid, Evade, and Intercept. We provide detailed descriptions and dimensions in Appendix E and run times in Appendix G. For full details on the tools and hyperparameters used, we refer to Appendix C.\nRobustness. As seen in Figure 2, RFSCNET incurs lower expected costs on average than BASELINE in two out of four environments. On Aircraft and Intercept, the results show a clear improvement in the performance of RFSCNET over BASELINE. There, the minimum (best) value of BASELINE across the seeds is worse than the median value of RFSCNET. On Evade, the baseline performs better, demonstrating that in this environment, it suffices to train on a single, fixed POMDP. On Avoid, RFSCNET does not improve over the median performance of BASELINE, but the baseline may find worse policies, as seen from the outliers. Furthermore, Figure 5 shows that training on adversarial POMDPS, instead of a single fixed POMDP, is a more difficult learning target. Even though this makes the training task more challenging, RFSCNET still achieves good performance. Overall, we conclude that RFSCNET improves over the baseline in terms of performance on RPOMDPs.\nConfiguration sensitivity. Figure 4 depicts the performance of RFSCNET and BASELINE across various configurations on Intercept. QRNN, which represents training the QBN end-to-end, proves less stable than post hoc, and k-means++ produces the best results. We do not observe a major difference between the use of QMDP and QFIB. All in all, it demonstrates that RFSCNET consistently outperforms BASELINE across configurations, even while the configuration affects performance.\nComparison with the state-of-the-art. As seen in Figure 3, SCP performs well on Evade and best on Intercept when k = 3. RFSCNET outperforms SCP on Aircraft, Avoid, and Evade by both the median and minimum performance across the 20 seeds. When the memory size in SCP is set to"}, {"title": "7 Related Work", "content": "Computing optimal robust policies for RMDPs has been studied extensively (Nilim and Ghaoui, 2005; Iyengar, 2005; Wiesemann et al., 2013). RMDPs are also used in model-based (robust) reinforcement learning (RL) to explicitly account for model uncertainty and enable efficient exploration (Jaksch et al., 2010; Petrik and Subramanian, 2014; Suilen et al., 2022; Moos et al., 2022).\nFor RPOMDPs, early works extend value iteration and point-based methods to account for the additional uncertainty (Itoh and Nakamura, 2007; Ni and Liu, 2013; Osogami, 2015), or use sampling over the uncertainty sets (Burns and Brock, 2007). Ni and Liu (2008) introduce a policy iteration algorithm for optimistic policies Chamie and Mostafa (2018) consider robustifying a given POMDP policy to combat sensor imprecision. Nakao et al. (2021) extend value iteration for distributionally robust POMDPs, where the agent receives side information after a decision period, which is a less conservative setting. Extensions to value iteration for RPOMDPs typically do not scale well to the large state spaces (up to 13000+) we consider in this paper. More recent methods compute FSCs for RPOMDPs via quadratic (Suilen et al., 2020) or sequential (Cubuktepe et al., 2021) convex programming. In contrast to our work, the convex optimization methods compute FSCs of a predefined size and structure. Additionally, they cannot compute FSCs in the optimistic case.\nIn RL, policy gradient algorithms incorporate memory using RNNs (Wierstra et al., 2007). Thereafter, RNNs serve as the baseline neural architecture for reinforcement learning in POMDPs (Ni et al., 2022). Recently, Wang et al. (2023) learned a stochastic recurrent memory representation for (non- robust) POMDPs using information from the model during training. RNNs have also successfully been used in a planning setting to compute FSCs for POMDPs (Carr et al., 2019, 2020, 2021). Robustness against model uncertainty has not yet been considered, and, to the best of our knowledge, no method exists yet that exploits the learning capabilities of RNNs in a robust planning setting."}, {"title": "8 Conclusion", "content": "We presented a novel framework accommodating supervised learning to train RNNs to act as robust policies for robust POMDPs. As part of our method, we extract FSCs from the trained RNN to evaluate the robust performance. Our experiments show that our approach of exploiting model-based information often yields more robust policies than the baseline approach with less information. Additionally, it is competitive with and even outperforms state-of-the-art solvers. In the future, we will investigate the generalization capabilities of RNN policies in a model-based setting, such as the ability to transfer robust policies to different and more complex domains."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of machine learning and, more specifically, robust sequential decision-making under partial observability. There are many potential societal consequences of our work due to the generality of RPOMDPs."}, {"title": "A Robust Policy Evaluation", "content": "In this appendix, we show that robust policy evaluation (Definition 6) indeed provides a conservative upper bound to the actual robust Bellman equation Equation (6):\n$$V^*_f ((s,n)) = \\sum_{a\\in A} (\\delta(a | n, O(s))C(s, a) + \\underset{T(s,a) \\in T(s,a)}{sup} {\\sum_{s'\\in S} \\sum_{n'\\in N}T(s' | s,a)\\delta(a | n, O(s))[n' = \\eta(n, O(s))] V^*_f((s', n'))}).$$\n$$T^{\\pi}_f ((s', n') | (s,n), a) = T(s' | s,a)\\delta(a|n, O(s))[n' = \\eta(n, O(s))]$$\n$$T^{\\pi}_f ((s', n') | (s,n)) = \\sum_{a\\in A} T^{\\pi}_f ((s', n') | (s,n), a)$$\n$$C^{\\pi}_f ((s,n)) = \\sum_{a\\in A} \\delta(a|n, O(s))C(s, a),$$\n$$V^*_f ((s,n)) = C^{\\pi}_f ((s,n)) + \\underset{T^{\\pi}_f ((s,n)) \\in T^{\\pi}_f ((s,n))}{sup} \\sum_{s' \\in S} \\sum_{n'\\in N} T^{\\pi}_f ((s', n') | (s,n))V^*_f ((s', n')).$$\n$$V^{\\pi}_f ((bo, no)) = \\sum_{s\\in S}bo(s) V^{\\pi}_f ((s, no)),$$\n$$V^*_f ((bo, no)) = \\sum_{s\\in S}bo(s)V^*_f ((s, no)).$$\n$$V^{\\pi}_f ((s,n)) \\geq V^*_f ((s,n)),$$\n$$V^{\\pi}_f ((bo, no)) \\leq V^*_f ((bo, no)).$$\nProof. We show that Vf ((s,n)) > V*f ((s,n)) for all ((s,n)) \u2208 S \u00d7 N. Recall Equation (7). Omitting the constant Cf ((s, n)), we rewrite the supremum as follows:\n$$\\underset{T^{\\pi}_f ((s,n)) \\in T^{\\pi}_f ((s,n))}{sup} \\sum_{s'\\in S} \\sum_{n'\\in N}T^{\\pi}_f ((s', n') | \\{s, n))V^*_f ((s', n'))$$\n$$\\underset{T^{\\pi}_f ((s,n)) \\in T^{\\pi}_f ((s,n))}{sup} \\sum_{s'\\in S} \\sum_{n'\\in N} \\sum_{a\\in A}T^{\\pi}_f ((s', n') | (s,n), a)V^{\\pi}_f((s', n'))$$\n$$\\underset{T^{\\pi}_f ((s,n)) \\in T^{\\pi}_f ((s,n))}{sup} \\sum_{s'\\in S} \\sum_{n'\\in N} \\sum_{\\alpha \\in A}T(s' |s, a)\\delta(a|n, O(s))[n' = \\eta(n, O(s))]V^*_f ((s', n'))$$\n$$\\underset{T^{\\pi}_f ((s,n)) \\in T^{\\pi}_f ((s,n))}{sup} \\sum_{\\alpha \\in A} \\sum_{s'\\in S} \\sum_{N'\\in N} T(s' |s, a)\\delta(a|n, O(s))[n' = \\eta(n, O(s))]V^*_f ((s', n')).$$"}, {"title": "B Supervision Policies", "content": "This section elaborates on the POMDP approximations used for computing the supervision policies.\nQMDP. The QMDP algorithm (Littman et al., 1995) is an effective method to transform an optimal MDP policy to a POMDP policy by weighting the (optimal) action values Q* of the MDP to the current belief b\u2208 B in the POMDP \u039c\u0395\u039c:\n$$QMDP(b,a) = \\sum_{S\\in S}b(s)Q^* (s,a) = \\sum_{S\\in S}b(s) (C(s,a) + \\sum_{s'\\in S}T(s' |s, a) V_{MDP}(s'))$$\nQFB(b,a) = \\sum_{S\\in S}b(s)a(s) = \\sum_{S\\in S}b(s) (C(s, a) + \\sum_{\\tau\\in Z} \\min_{a' \\in A} \\sum_{S'\\in S}T(s' | s, a)[z = O(s')]\\alpha^{a'}(s'))$$\n$$(\\alpha^{a})^{t+1}(s) = C(s, a) + E_{\\tau\\in Z}max_{a'}\\sum_{S'\\in S}T(s' | s, a)[z = O(s')]\\alpha^{a'}(s').$$"}, {"title": "C Tools and Hyperparameters", "content": "We use the tools Storm Hensel et al. (2022) for parsing the models and PRISM Kwiatkowska et al. (2011) to compute the RMDP values for the lower bounds in Figure 3 and for robust policy evaluation. We build and train the RNN and the QBN using TensorFlow Abadi et al. (2016). The RNN cell is a gated recurrent unit (GRU, Cho et al., 2014). We initialize with a concrete POMDP instance M\u2208 M, where the intervals of the uncertainty sets are resolved to a value in the middle of the interval [i, j] given by i+1/2. For all the experiments, the simulation batch size is set to I = 256, the maximum simulation length is set to H = 200, and we run for a maximum of 50 iterations. Both the RNN and QBN use an Adam optimizer (Kingma and Ba, 2015) with a learning rate of 1 \u00b7 10-3.\nThe hidden size of the RNNs was set to d = 16. The experiments ran inside a Docker container on a Debian 12 machine. Our infrastructure includes an AMD Ryzen Threadripper PRO 5965WX machine with 512 GB of RAM. We train the neural networks on the CPU. The different seeds for the RNN-based methods were executed in parallel, each running on a single core. Multi-threading in the Gurobi LP solver (Gurobi Optimization, LLC, 2023) used by SCP was enabled. In our initial tests, we considered hidden sizes d \u2208 {3, 16, 64}, batch sizes I \u2208 {128, 256, 512}, learning rates in the range of [110-2, 1 \u00b7 10-4], and different number of iterations before arriving at our final values. We used the same infrastructure and experimental setup across methods."}, {"title": "D Network Architectures", "content": "In this section, we provide more details on the neural network architectures. Our post hoc QBN approach largely follows Koul et al. (2019) and Carr et al. (2021), apart from differences mentioned in Section 4.3. We used a batch size of 32 for both networks during stochastic gradient descent."}, {"title": "D.1 QBN", "content": "Similar to prior work (Carr et al.", "E": "\u0124 \u2192 [\u22121", "1": "", "q": [-1, 1], "D": "\u1e9eBr \u2192 \u0124 to reconstruct the input given the quantized encoding. The QBN represents a function Q: \u0124 \u2192 \u0124 where Q(\u0125) = D(q(E(\u0125))) for all h\u2208 H. We train the QBN to minimize the reconstruction loss, i.e., mean-squared error, on the RNN's memory representations derived from the histories in D. The finite set of memory nodes extracted is formed by the Cartesian product N = \u00d7 B\u2081\u03b2, and n ="}]}