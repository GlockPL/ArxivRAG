{"title": "On the Conditions for Domain Stability for Machine Learning: a Mathematical Approach", "authors": ["Gabriel Pedroza"], "abstract": "This work proposes a mathematical approach that (re)defines a property of Machine Learning models named stability and determines sufficient conditions to validate it. Machine Learning models are represented as functions, and the characteristics in scope depend upon the domain of the function, what allows us to adopt topological and metric spaces theory as a basis. Finally, this work provides some equivalences useful to prove and test stability in Machine Learning models. The results suggest that whenever stability is aligned with the notion of function smoothness, then the stability of Machine Learning models primarily depends upon certain topological, measurable properties of the classification sets within the ML model domain.", "sections": [{"title": "Introduction", "content": "In recent years, the study of Artificial Intelligence and related techniques, like Machine Learning, has attracted considerable attention from practitioners, engineers and researchers from many sectors. Such interest has been, in many respects, driven by questions and concerns raised and shared by the involved communities. In particular, it can be mentioned the need for safety of systems integrating AI algorithms and the possibility to achieve acceptable means for compliance, necessary for regulation and certification [3][2][1]. Despite autonomy is a well-known notion within the Systems, SW and HW engineering arenas, the usage of AI technology, (GenAI, LLMs, ML/DL) to carry out functions traditionally conducted under human supervision and control, induces new challenges [4]. Discussing referred challenges is out of the scope of this paper: notwithstanding the relevance of AI challenges and risks, they are currently under inspection and study by a variety of experts, following a plethora"}, {"title": "Defining Stability", "content": "Definition 1. Let (S,d) a metric space, $D_i \\subset S$, i = 1...m, a finite sequence of sets. M is a classifier on S for the sets {$D_i$} if:\ni) Mis a function defined on $\\cup_{i=1}^{m} D_i$\nii) $D_i \\cap D_j = \\emptyset, \\forall i \\neq j$\niii) $\\exists y_i \\in M(D_i), \\forall x \\in D_i \\Rightarrow M(x) = y_i$, i = 1...m\niv) For $y_i, y_j$ as in previous point, if $i \\neq j \\Rightarrow y_i \\neq y_j$\nNote 1. Given a metric space (S, d) the following notation is used for the neighborhood of a point $x \\in S$:\n$B(x, \\delta) = \\{x \\in S \\; | \\; d(x,x) < \\delta\\}$\nNote 2. In Definition 1, a point $x \\in D_i, M(x) = y_i$ is denoted by $x_{y_i}$. The assignation $M(x_{y_i}) = y_i$ can also be denoted by $x_{y_i} \\xrightarrow{M} y_i$\nDefinition 2. Let (S,d) a metric space, $D \\subset S$ and M a classifier for D and $D^c$. $X_y \\in D$ is said a stable point of M in D if it is satisfied:\ni) $M(x_y) = y$\nii) $\\exists \\delta > 0, \\forall x \\in B(x_y, \\delta) \\Rightarrow x \\in D, M(x) = y$\niii) For $\\delta$ as in point ii), $\\forall \\delta_{\\alpha} \\leq \\delta \\; \\exists x \\in B(x_y,\\delta_{\\alpha}), x \\neq x_y$"}, {"title": "Dense sets and Implications for Stability", "content": "The implications of dense sets regarding the existence (absence) of stable points is formalized in this subsection.\nDefinition 3. Let (S,d) a metric space. A set $D \\subset S$ is said dense in S if $\\forall x \\in S, \\forall \\delta > 0, B(x, \\delta) \\cap D \\neq \\emptyset$ [9], [6].\nLemma 1. Let (S,d) a metric space and M a classifier for D C S and $D^c$. If $D^c$ is dense in S then the set of stable points in D is empty.\nFirst, S = D\u222a$D^c$. Let's assume $x_0\\in D$ a stable point of M, then $\\exists \\delta > 0$ such that $B(x_0, \\delta) \\subset D$. Since $D^c$ is dense in S, then $B(x_0, \\delta) \\cap D^c \\neq \\emptyset$. However, if $x \\in B(x_0, \\delta) \\cap D^c$ then $x \\in D^c$ and by definition 2, ii) $x \\in D$ what contradicts the assumption. QED.\nBy exchanging roles between D and $D^c$ in previous lemma, the following corollary is proved.\nCorollary 1. Let (S, d) a metric space and M a classifier for D C S and $D^c$. If D is dense, then $D^c$ does not have any stable point.\nCorollary 2. Let $S \\subset \\mathbb{R}^a$ (non-empty) interval, then there is no classifier M able to classify $S\\cap I$ and $S\\cap \\mathbb{Q}$.\nIndeed, since $(S\\cap \\mathbb{Q})^c = (S\\cap I)$ and $S\\cap I = (S\\cap \\mathbb{Q})^c$ are both dense, by lemma 1 and Corollary 1 the conclusion follows. QED."}, {"title": "Alternatives to Prove Stability", "content": "Some equivalences are provided as alternatives to prove stability.\n4.1 Accumulation Points\nDefinition 4. Let (S,d) a metric space. Given a set $D \\subset S$, a point $x \\in S$ is an accumulation point of D if $\\forall \\delta > 0$ then $B(x, \\delta) \\cap D\\{x\\} \\neq \\emptyset$ [9] [6].\nLemma 3. Let (S,d) a metric space and M a classifier for D and $D^c$, with both D and $D^c$ not dense in S and D an open set. Then $x_y \\in D$ is a stable point for M if and only if $x_y$ is an accumulation point of D.\n[$\\Rightarrow$]\nLet's assume $x_y \\in D$ is a stable point of M. It will be proved that $x_y$ is an accumulation point of D.\nLet's consider $\\epsilon > 0$ arbitrary but fixed. Then since $x_y \\in D$ is a stable point of M, $M(x_y) = y$ and $\\exists \\delta > 0$ such that $B(x_y, \\delta) \\subset D$ and $\\forall \\delta_{\\alpha} < \\delta, \\exists x \\in B(x_y,\\delta_{\\alpha}), x \\neq x_y$. The following cases exist:\nIf $\\epsilon < \\delta$: then $B(x_y, \\epsilon) \\subset B(x_y, \\delta) \\subset D$, since $\\exists x \\in B(x_y,\\epsilon), x \\neq x_y$, it follows that $x \\in D$, which leads directly to $B(x_y, \\epsilon) \\cap D\\{x_y\\} \\neq \\emptyset$.\nIf $\\epsilon > \\delta$: since $x_y \\in D$ is stable point of M, then $\\exists x \\in B(x_y, \\delta) \\subset B(x_y,\\epsilon), x \\neq x_y$ and $x \\in D$, what also leads to $B(x_y, \\epsilon) \\cap D\\{x_y\\} \\neq \\emptyset$."}, {"title": "Accumulation series", "content": "As shown in previous subsection, a classifier M defined over an open set is stable for every accumulation point therein. The equivalence provided in the following lemma provides further means to prove stability.\nLemma 4. Let (S,d) a metric space and M a classifier for D and $D^c$, with both D and $D^c$ not dense in S and D an open set. $x_y$ is a stable point of M if and only if $\\forall \\{x_n\\}$ series, such that $\\lim_{n\\rightarrow\\infty}x_n = x_y, x_n \\neq x_y$, $\\{s_k\\}$ sub-series of $\\{x_n\\}$ such that $\\exists k_0, k > k_0, \\Rightarrow s_k \\in D, s_k \\neq x_y, \\lim_{k\\rightarrow\\infty}s_k = x_y$.\n[$\\Rightarrow$]\nLet's assume $x_y$ is a stable point of M. Let $\\{x_n\\}$ be a series such that $\\lim_{n\\rightarrow\\infty}x_n = x_y, x_n \\neq x_y$, then $\\exists \\delta > 0$ such that $B(x_y,\\delta) \\subset D$. For such $\\delta$, $\\exists n_0$ such that $\\forall n > n_0, d(x_y,x_n) < \\delta$, what means that $\\forall n > n_0, x_n \\in B(x_y, \\delta) \\subset D$, then $M(x_n) = y, x_n \\neq x_y$. Then, we can define the sub-series of $\\{x_n\\}$ as $\\{s_k\\} = \\{x_n\\} \\cap B(x_y, \\delta) \\cap D\\{x_y\\} \\neq \\emptyset$.\nThus, by its construction, $\\{s_k\\}$ satisfies $\\exists k_0, k \\geq k_0, \\Rightarrow s_k \\in D, s_k \\neq x_y, \\lim_{k\\rightarrow\\infty}s_k = x_y$."}, {"title": "Discussion", "content": "Machine Learning models and their respective implementations include some uncertainties due to singularity regions. As such, the abstraction provided in this paper named classifier (Definition 1) is indeed a rough approximation of ML models. Nonetheless, the results obtained mostly rely upon properties of the domain of the ML model and not in the classifier itself. An important claim in this approach is that the conditions for ML stability depend, in a first place, upon certain topological and measurable properties of the space, namely density, accumulation and openness. Overall, open and bounded sets are suitable candidates sets for stable classification. As it is shown, if such topological/metric properties are not ensured, classifiers cannot ensure a stable operation. This approach shall be leveraged, by adapting the notion of classifier so as to consider the uncertainty of classifiers. This is left as a future work. The equivalence between stability and the so named accumulation series is intended to facilitate the specification of algorithms to test (absence of) stability. Whereas accumulation points and the density of sets are simple notions, they can be hard to verify on complex, high dimension domains. Then the notion of accumulation series seeks for a discrete solution, more adapted to finite/limited precision of computers. A description of such discrete algorithms was barely sketched but not formally defined."}, {"title": "Conclusions", "content": "This work aims to provide further understanding regarding foundational aspects of Machine Learning models. The first part of this paper introduces a definition for stability, a property that appears fundamental for the proper operation of classifiers which are defined as a function that abstracts the uncertainty of real Machine Learning models. Secondly, some limits of classifiers were explained which appear whenever one or more classification sets are dense. Indeed, it was proved that the stability of classifiers demands the absence of dense sets in the domain of classification (also called Operational Design Domain) (Lemmas 1, 2). Some basic but representative examples were provided to illustrate such limitations. The last part of this work provides alternatives to prove stability in the form of equivalences: Lemma 3 provides conditions for equivalence between stability and accumulation points, then, Lemma 4 proves the equivalence between accumulation points and so named accumulation series, also introduced in this paper. The resulting equivalence between stability and accumulation series provides the possibility to define finite, discrete algorithms to prove stability. The definition of referred algorithms is not included in this paper and will be addressed as a continuation of this work."}]}