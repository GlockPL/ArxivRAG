{"title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning", "authors": ["Baohao Liao", "Yuhui Xu", "Hanze Dong", "Junnan Li", "Christof Monz", "Silvio Savarese", "Doyen Sahoo", "Caiming Xiong"], "abstract": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework aimed at improving the efficiency of inference in large language models (LLMs). RSD synergistically combines a lightweight draft model with a more powerful target model, incorporating a controlled bias to prioritize high-reward outputs, in contrast to existing speculative decoding methods that enforce strict unbiasedness. RSD employs a process reward model to evaluate intermediate decoding steps and dynamically decide whether to invoke the target model, optimizing the trade-off between computational cost and output quality. We theoretically demonstrate that a threshold-based mixture strategy achieves an optimal balance between resource utilization and performance. Extensive evaluations on challenging reasoning benchmarks, including Olympiad-level tasks, show that RSD delivers significant efficiency gains against decoding with the target model only (up to 4.4\u00d7 fewer FLOPs), while achieving significant better accuracy than parallel decoding method on average (up to +3.5). These results highlight RSD as a robust and cost-effective approach for deploying LLMs in resource-intensive scenarios. The code is available at https://github.com/BaohaoLiao/RSD.", "sections": [{"title": "1. Introduction", "content": "Scaling laws are widely recognized by the machine learning community as a foundational principle for the development of large language models. They emphasize that increasing both model size and dataset scale leads to improved loss reduction and, consequently, enhanced generalization capabilities. When data and model size are scaled to extraordinary levels, performance can reach unprecedented heights. Large models demonstrate remarkable capabilities across diverse tasks, showcasing robust generalization and advanced reasoning skills.\nThese advancements result in high computational and economic costs. While training is resource-intensive, inference at scale is even costlier, requiring vast computational infrastructure and energy to serve billions of queries. The exponential growth in inference costs makes it a key challenge for large model deployment, highlighting the need for efficient techniques to reduce energy use and ensure scalability.\nSpecifically, Sequential token generation in large LLMs incurs significantly higher computational costs compared to smaller models. This increased latency can hinder their deployment in real-time or high-throughput applications. To address this issue, parallel decoding techniques, such as speculative decoding, have emerged as effective solutions. Speculative decoding operates by leveraging a smaller, lightweight model to generate candidate outputs, which are then validated and refined by the larger model. This approach significantly reduces the number of decoding tokens required by the large model, thereby accelerating the overall process. The smaller model serves as a guide, proposing sequences that the larger model can confirm or adjust, leading to faster inference times without compromising quality. Furthermore, speculative decoding ensures efficiency by maintaining high-quality outputs through careful calibration of the smaller model. By aligning the smaller model's predictions with the larger model's capabilities, this method minimizes discrepancies and enhances reliability during inference.\nDespite advancements in parallel decoding, speculative decoding remains underutilized for complex reasoning tasks, particularly multi-step generation. A key limitation is the strict unbiasedness requirement, which ensures the final token distribution matches the large model's but restricts flexibility in exploring diverse completions."}, {"title": "2. Reward-Guided Speculative Decoding", "content": "Notations. Let all tokens be embedded in Euclidean space. The prompt is represented as $x \\in \\mathbb{R}^{l \\times d}$, and the response as"}, {"title": "2.1. RSD Algorithm", "content": "The Reward-Guided Speculative Decoding (RSD) algorithm operates by dynamically mixing the draft model $m$ and target model $M$ at each generation step, with the objective of balancing efficiency and quality. The algorithm leverages the reward function $r$ to guide this dynamic mixing, ensuring that necessary higher-quality steps are more likely to be generated by the target model $M$, while the draft model $m$ is used for cost-effective generation when possible. Below, we describe the key components of the algorithm. At each decoding step $i$, the algorithm follows these steps:\n1. Generate Draft Step: The draft model generates a candidate $\\hat{y}_i$ given the prompt and previous outputs.\n2. Compute Reward: The reward function $r(\\tilde{Y}_i \\mid z_i)$,"}, {"title": "2.2. Acceptance Criterion $A_w$ and Weighting Function", "content": "Proposition 2.2. Given the following assumptions:\n* $w(r)$ is non-decreasing in $r$;\n* $\\mathbb{E}_{P_M} [r(y|z)] \\geq \\mathbb{E}_{P_m} [r(y|z)]$;\nit follows that the expected value of $r(y|z)$ under the RSD induced distribution satisfies: $\\mathbb{E}_{P_{RSD}}[r(y|z)] \\geq \\mathbb{E}_{P_m} [r(y|z)]$.\nThe weighting function $w(.)$ plays a crucial role in adjusting the mixture distribution. Several variants of the weighting function are considered. Table 1 provides a summary of different choices. Each of these variants provides new trade-off between draft model and target model. Intuitively, a binary function can maximize expected reward under a strict sampling budget constraint, a smooth weighting might in practice handle noisy reward model outputs more gracefully."}, {"title": "2.3. Optimal Weighting", "content": "In the following Proposition, we demonstrate that the optimal weighting function for maximizing reward under a constrained sampling budget is a binary step function, which assigns a weight of 1 only to high-reward outputs.\nProposition 2.3. Given a constrained sampling budget $v = 1 - \\mathbb{E}_{y \\sim P_m}W_r(y|z) \\leq \\gamma, \\gamma \\in (0,1)$, the optimal sampling"}, {"title": "2.4. Discussion", "content": "Process Reward for Each Model. As stated in Eq. (1), we expect the target model to have a higher reward. Fig. 3 confirms this on MATH500, showing that for correctly answered questions within RSD, $P_M$ consistently outperforms $P_m$ in reward (Middle figure).\nComparison with the reward of $P_M$. Since $P_{RSD}$ is a mixture of $w_r P_m$ and $P_M$, its reward is a weighted sum of their expected rewards. Thus, $P_{RSD}$ can exceed $P_M$ in expected reward when we use an aggressive $w$ that only assigns 1 to high-reward regions. In cases where $w_r P_m$ has a higher expected reward than $P_M$, $P_{RSD}$ also achieves a higher reward. We also notice this phenomenon empirically.\nGeneral Weighting Function and SD. Notably, reward-based weighting functions $w_r$ are not the only option. An alternative approach explicitly defines the weighting function in terms of the likelihood ratio:\n$$w(y | z) = \\min \\left\\{1, \\alpha \\frac{P_M(y|z)}{P_m(y|z)}\\right\\},$$where $\\alpha > 0$ is a hyperparameter controlling how quickly the method transitions from the draft model to the target model. This formulation is algorithmically similar to speculative decoding, with $\\alpha$ as a tunable parameter and $P_M$"}, {"title": "3. Empirical Results", "content": "Models. To assess the performance of RSD, we employ both general-purpose and math-focused LLMs as our target and draft models, specifically Qwen-2.5, Llama-3, and Qwen-2.5-Math. Our system utilizes Skywork-01-Open-PRM as the Process Reward Model (PRM), as it was the most advanced open-source PRM available during our experiments. The reward score ranges from 0 to 1, the higher the better."}, {"title": "3.1. Reasoning Benchmarks", "content": "We evaluate RSD across a diverse set of reasoning benchmarks, as summarized in Table 2, and observe: (1) Test-time scaling methods like majority voting and Best-of-N, which rely on extensive sampling with a draft model, consistently underperform a single target model on average. This finding highlights the importance of a larger model for reasoning tasks, as its performance cannot be easily matched by a smaller model with increased computation. (2) While SD is theoretically unbiased, guaranteeing accuracy equal to the target model, it often underperforms in practice. This discrepancy, as also noted by Chen et al. (2023a), arises due to floating-point errors. Moreover, in cases where a draft model outperforms the target model (e.g., Table B.1 and domain-specialized draft models), SD's strict unbiasedness leads to worse performance compared to the draft model. Thus, the decision to use SD must account for such scenarios. In contrast, RSD mitigates this concern by leveraging a PRM, which evaluates the quality of reasoning steps from the draft model. (3) Among all evaluated methods, RSD consistently outperforms the single target model on average when using an optimized d. Even with a fixed $\\delta = 0.7$, RSD achieves better results in 7 out of 8 settings. Notably, on the challenging GPQA benchmark, RSD (1.5B/7B/1.5B) significantly surpasses the single target model (38.4 vs. 32.8), demonstrating the effectiveness of this efficient approach.\nAdditionally, a larger PRM (7B) slightly enhances performance compared to a smaller PRM (1.5B), especially on complex datasets like GPQA and MATH500, where the increased reasoning capacity of a larger PRM proves beneficial. Results with general models, such as Qwen2.5-Instruct and Llama-3.1-Instruct, align with those from math-specific models, validating RSD's robustness and generalizability."}, {"title": "3.2. Comparison with Search-Based Methods", "content": "We also compare our method with beam search and process Best-of-N in Table 3. RSD significantly"}, {"title": "3.3. Computation Analysis", "content": "To evaluate the computational efficiency of our method, we compare RSD with speculative decoding and Best-of-N on MATH500. Following, we adopt the standard approximation of FLOPs for transformers with N parameters, i.e. 2N per inference token. Note that the inference cost for PRMs is also included in the calculations. As shown in Fig. 4, RSD (1.5B/7B/7B) outperforms both SD (1.5B/7B) and Target (7B), achieving an accuracy improvement of 1.2 and 1.4, respectively, while using fewer FLOPs. Moreover, RSD (7B/72B/7B) achieves a notable accuracy of 88.0 on MATH500, compared to 85.6 for Target (72B), with nearly 4.4\u00d7 fewer FLOPs. When compared to BoN (7B/7B,N=64), RSD (7B/72B/7B) delivers 1.8 points higher accuracy at a significantly lower computational cost. These results clearly demonstrate both efficiency and effectiveness of RSD."}, {"title": "3.4. Ablation Studies", "content": "Threshold \u03b4. Fig. 5 illustrates the relationship between the threshold \u03b4, accuracy, and the proportion of questions solved solely by the draft model within RSD. As \u03b4 increases, accuracy improves, peaking at d = 0.7, before experiencing"}, {"title": "4. Discussion", "content": "PRM Overheads and Model Merge. In MATH500, the average number of reasoning steps per question is 18, suggesting that the PRM is invoked 18 times per question, akin to generating 18 tokens. Furthermore, even a tiny PRM (1.5B) outperforms the single target model in RSD accuracy"}, {"title": "5. Related Work", "content": "Speculative Decoding. Speculative decoding achieves lossless acceleration by employing a draft model to predict subsequent tokens and verify them in parallel. Tree-based speculation extends this approach by generating multiple candidates to increase the acceptance rate. Self-speculative decoding leverages parts of the large language model (LLM) parameters as the draft model while using the original base model as the verifier. Parallel decoding further enhances efficiency by introducing draft models to streamline the process. Unlike previous speculative decoding methods, our approach utilizes process rewards to perform stepwise speculative reasoning.\nReward Models on Reasoning. Reward models play a crucial role in selecting correct reasoning trajectories during both training and inference . Outcome Reward Models (ORMS) are trained exclusively on the model's final output, whereas Process Reward Models (PRMs) rely on step-level annotations, providing dense and granular reward signals at each reasoning step. Scaling test-time compute has gained significant traction with the advancement of reward models. Techniques like Best-of-N leverage ORMs to select the sample with the highest reward from N candidates. Building on this, tree search methods have been introduced, enabling per-step predictions rather than relying solely on the final answer. These methods are enhanced by process feedback, such as Process Reward Models (PRMs). We propose a novel application of PRMs to accelerate reasoning during inference."}, {"title": "6. Conclusion", "content": "We propose Reward-Guided Speculative Decoding (RSD), a novel framework that enhances LLM inference efficiency, particularly for reasoning-intensive tasks. RSD dynamically combines a lightweight draft model with a more capable target model, using a reward function to guide output selection at each step. This approach balances computational cost and quality by selectively refining outputs based on process rewards. RSD achieves significant efficiency gains over SD and BoN while maintaining accuracy benchmarks. Extensive evaluations across reasoning tasks highlight RSD's robustness, adaptability, and effectiveness, making it a practical solution for LLM deployment."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning by proposing Reward-Guided Speculative Decoding (RSD), a framework aimed at improving the efficiency and scalability of large language model inference. The potential societal implications of this work are primarily positive, as RSD facilitates more energy-efficient and cost-effective use of computational resources, contributing to the sustainability of deploying large-scale AI systems.\nHowever, as with any advancement in machine learning, there are ethical considerations to be acknowledged. The increased efficiency of language models could lead to wider accessibility and adoption, which, while beneficial in many respects, may also exacerbate risks such as misuse for generating misinformation or biases in outputs. We strongly encourage researchers and practitioners to apply RSD responsibly and consider incorporating safeguards to mitigate these risks.\nOverall, we believe the contributions of this work align with the broader goal of advancing AI technologies in an ethical and sustainable manner, with no immediate negative societal consequences requiring further discussion."}, {"title": "Code Availability", "content": "The code is available at https://github.com/BaohaoLiao/RSD."}, {"title": "A. Proof", "content": "A.1. Proof of Proposition 2.1\nProof. For simplicity, we assume that \u1ef9 = y|z.\nThe final distribution PRSD(\u1ef9) combines contributions from two sampling paths:\n* Accepted samples from Pm.\n* Fallback samples from PM after rejection.\nAccording to Law of Total Probability,\n$$P_{RSD}(\\tilde{y}) = P(\\text{accept } \\tilde{y} \\text{ from } P_m) + P(\\text{reject } P_m \\text{ and draw } \\tilde{y} \\text{ from } P_M).$$"}, {"title": "A.2. Proof of Proposition 2.2", "content": "Proof. We aim to prove that under the conditions:\n1. w(r) is non-decreasing in r,\n2. $\\mathbb{E}_{P_M} [r(y|z)] \\geq \\mathbb{E}_{P_m} [r(y|z)]$,\nthe expectation of r(y|z) under PRSD satisfies:\n$$\\mathbb{E}_{P_{RSD}} [r(y|z)] \\geq \\mathbb{E}_{P_m} [r(y|z)].$$\nBy definition:"}, {"title": "A.3. Proof of Proposition 2.3", "content": "Proof. For simplicity, we assume that \u1ef9 = y|z. Our optimization problem is\n$$\\mathcal{L}(w_r) = \\mathbb{E}_{\\tilde{y} \\sim P_m}W_r(\\tilde{y})r(\\tilde{y}) + \\mathbb{E}_{\\tilde{y} \\sim P_M}vr(\\tilde{y})$$\nsubject to the inequality constraint:\n$$v = 1 - \\mathbb{E}_{\\tilde{y} \\sim P_m} [W_r(\\tilde{y})] \\leq \\gamma,  0 \\leq w(\\tilde{y}) \\leq 1.$$\nEquivalently,\n$$\\mathcal{L}(w_r) = \\mathbb{E}_{P_m} [W_r(\\tilde{y})r(\\tilde{y})] + (1 - \\mathbb{E}_{P_m} [w_r(\\tilde{y})]) \\mathbb{E}_{P_M}[r(\\tilde{y})]$$\nThe Lagrangian\u00b9 is given by\n$$\\mathcal{L}(w_r, \\lambda) = \\int \\left[(P_m(\\tilde{y})r(\\tilde{y}))w_r(\\tilde{y}) + (P_M(\\tilde{y})r(\\tilde{y}) (1 - \\mathbb{E}_{w_r} (\\tilde{y})])\\right] d\\tilde{y} + \\lambda \\left[(1-\\gamma) - \\int P_m(\\tilde{y})w_r(\\tilde{y}) d\\tilde{y}\\right]$$\nThe Lagrangian derivative yields:\n$$\\frac{\\partial \\mathcal{L}}{\\partial w_r(\\tilde{y})} = P_m(\\tilde{y}) [r(\\tilde{y}) - (\\Lambda + R)],$$"}, {"title": "B.1. Tuning of \u03b4", "content": "RSD employs a threshold, \u03b4, to decide whether to accept a reasoning step generated by the draft model. If the reward score of a reasoning step exceeds d, it is accepted. However, reasoning tasks vary in complexity, leading to diverse reward distributions. Using a fixed & may not yield optimal accuracy across different tasks.\nThe results for varying d values are presented in Table B.2. Overall, \u03b4 = 0.7 emerges as a reliable choice across various settings. Slight adjustments within the range [0.6, 0.7, 0.8, 0.9] can further improve performance."}, {"title": "B.2. Different Reasoning Complexity", "content": "Thanks to the human annotated complexity levels in MATH500 (5 levels, the higher the harder), here we investigate how RSD works for questions in different complexity. As shown in Fig. B.1, the involvement of the target model ($\\delta \\neq 0$) consistently improves the accuracy compared with the draft model only (d = 0). The improvement varies for different levels, at most +4.7 for level 1, +5.6 for level 2, +6.7 for level 3, +16.4 for level 4 and +15.7 for level 5, showing the importance of the target model for harder questions.\nFor the same \u03b4, one can also observe that the proportion of questions solved by the draft model alone decreases with an increasing level. For example at \u03b4 = 0.7, draft model alone solves 84% questions in level 1, 67% questions in level 2, 58% questions in level 3, 44% questions in level 4 and 19% questions in level 5. It shows that harder questions need more involvement of the target model. In this way, RSD can be considered as a method for automatic compute allocation, less compute for easy questions and more compute for hard questions, which is different from SD that always needs both target and draft models for every question."}, {"title": "B.3. Model Merge", "content": "To reduce the number of models required for facilitating RSD's usage, we consider merging either the target model with the PRM or the draft model with the PRM. Here, we focus on the simplest merging strategy-linear merging-using MergeKit (Goddard et al., 2024), leaving the exploration of more advanced merging methods for future work.\nThe PRM and the policy model have different architectures. Specifically, the PRM includes a projection layer atop the final transformer layer, which projects the hidden dimension to a scalar output, whereas the policy model employs an lm_head. We merge only the shared layers, retaining the PRM's projection layer and the policy model's lm_head. For interpolation weights in the linear merging process, we tested only [0.6, 0.4] and [0.5, 0.5], with the target or draft model receiving 0.6 and the PRM 0.4. The [0.6, 0.4] configuration performed slightly better.\nAs shown in Table B.3, the results indicate the following: (1) Overall, the merged model outperforms SD; (2) Merging improves performance more substantially in larger models (+1.4 vs. +0.8). This observation aligns with the findings of Yadav et al. (2024)."}]}