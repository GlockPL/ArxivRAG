{"title": "Contrastive Token Learning with Similarity Decay for Repetition Suppression in Machine Translation", "authors": ["Huangyu Dai", "Ben Chen", "Kaidi Chen", "Ying Han", "Zihan Liang", "Wen Jiang"], "abstract": "For crosslingual conversation and trade, Neural Machine Translation (NMT) is pivotal yet faces persistent challenges with monotony and repetition in generated content. Traditional solutions that rely on penalizing text redundancy or token reoccurrence have shown limited efficacy, particularly for lengthy article and e-commerce descriptions with inherent redundancy, even with the advent of Large Language Models (LLMs). This paper investigates the underlying causes of textual repetition through the lens of information entropy, attributing the phenomenon to the elevated uncertainty within the input text. To address this, a novel algorithm named Contrastive Token Learning with Similarity Decay (CTSD) is introduced, which modulates the suppression of tokens dynamically, informed by varying attention weights and inter-token distances. Furthermore, an e-commerce dataset comprised of title texts of online real items is compiled and released susceptible to hallucination translations to benchmark the algorithm. Extensive evaluations demonstrate that CTSD significantly outperforms existing approaches in precision and generalizability. Additional online A/B testing underscores its practical value, showing marked improvements in user engagement and conversion. Notably, this method has been implemented with full traffic on eight multilingual sites of alibaba.com, the largest B2B e-commerce platform in the world.", "sections": [{"title": "Introduction", "content": "In recent years, the synergy of neural networks coupled with the increasing scale of parallel corpora has significantly propelled Neural Machine Translation (NMT) forward. Notably, the sophisticated reasoning abilities and specialized knowledge acquired by Large Language Models (LLMs) further contribute modern NMT systems towards achieving near-human-level performance. However, the reliability of NMT in delivering accurate and coherent translations remains unstable, often encountering unexpected errors such as omissions or nonsensical outputs. This challenge persists across the spectrum, especially for complex textual materials like repetition-prone articles and e-commerce descriptions.\nTypical NMT problems, commonly referred to as \"hallucinations\", can be categorized into two main types . The first involves the repetition of words or sentences, known as \"oscillations\", while the second pertains to the generation of content not supported by the source, termed \"largely fluent\". Of these two types, \"oscillations\" are particularly intolerable for leading to repetition with low coherence and accuracy, making NMT limited for multiple applications. Consequently, addressing oscillation (repetition generation) has emerged as a primary focus in current research, which is vital for improving reliability and usability in complex scenarios.\nPrevious methods mainly employed two strategies to suppress repetition generation. The first is the direct strategy interventions during the inference stage, such as n-gram not repeat, Contrastive Search (CS) , and Penalized Sampling (PS) . These techniques focus on preventing repeated tokens to eliminate oscillations. However, they would disrupt the token distribution of output, leading to other errors. Consequently, recent methods focus on designing training objectives during the model training stage to better address hallucination problems. Yet, these training objectives do not adequately explore the root of oscillations in transformer-based models, often using a direct intervention way to suppress tokens that have appeared. Although they can effectively suppress word or sentence repetition, they also lead to a lower coherence and accuracy in translation.\nWith the emergency of LLMs, reinforcement learning (RL) methods such as Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) have surfaced as a new strategy for reducing hallucinations . Through training LLMs with preference data, these methods generate outputs close to human expectations, effectively lowering the chance of hallucinations.\nIn this paper, we conduct an in-depth exploration of the fundamental reasons underlying textual repetition in machine translation, primarily utilizing the concept of information entropy. Our research reveals that this phenomenon largely stems from increased levels of uncertainty present within the input text. Repetitive token generation occurs because the information from previously generated tokens does not provide additional value (information entropy). To effectively deal with this issue, we propose an innovative algorithm, which we term \"Contrastive Token Learning with Similarity Decay\" (CTSD). This innovative approach aims to dynamically adjust the suppression of tokens by analyzing the attention differences between different output tokens' embeddings and the distance between the inner tokens, thereby enhancing the accuracy and stability of the output. Meanwhile, our method can be applied to both specialized translation models and LLM without additional data preparation. The results show that our method can effectively improve the performance of models in translation tasks and prevent oscillation hallucinations. Compared with Contrastive Token Learning (CT), CTSD achieves improvements by 1% to 10% in translation quality on both the FLORES-200 and our proprietary e-commerce datasets.\nIn addition, a comprehensive evaluation by experts reveals that CTSD exceeds current methods in both precision and generalizability. Online A/B tests further highlight its practicality, as evidenced by substantial gains in user engagement with higher click-through and conversion rates and final gross merchandise volume. Importantly, this method has been successfully deployed across eight multilingual websites with full traffic of alibaba.com, the world's largest B2B e-commerce platform."}, {"title": "Related Work", "content": "Multilingual NMT has advanced significantly from its early focus on two-language systems. The pioneering work of Dong et al. expanded NMT into a one-to-many framework by sharing encoders across four language pairs . This development sparked a surge in research on NMT systems capable of handling multiple languages . At first, the research was mainly focused on improving multilingual NMT's capabilities on rich-resource languages through designed specific components and more diverse training data. Now, more research has turned to low-resource languages. Tars et al. improved the capabilities of low-resource languages by simultaneously training different languages of the same language family . Pan enhanced the translation quality of non-English language directions through data augmentation and contrastive learning. NLLB Team developed a conditional compute model based on a Sparsely Gated Mix-"}, {"title": "Repetition Suppression", "content": "With the advent of LLMs, repetition suppression methods have received significant attention. Currently, there are two mainstream types of methods: decoding methods and training methods. Decoding methods initially gained popularity because no further tuning is needed. Commonly used methods include PS and CS. Keskar et al. implemented PS, using a temperature coefficient to reduce the likelihood of historical tokens, reducing the probability of producing oscillatory hallucinations . Su et al. proposed CS to suppress historically generated tokens by computing the cosine similarity between the embedding of historical tokens and the current token.\nWhile decoding methods successfully suppress oscillation hallucination, they face challenges of reduced generation quality and increased inference cost. Therefore, research is shifting towards designing training objectives for more accurate and stable translations. Welleck et al. proposed unlikelihood training (UL), suppressing repetition through unlikelihood loss . Su et al. adopted contrastive training, emphasizing distinctions between different tokens to prevent monotonous repetition . Jiang et al. introduced the CT loss, which selectively suppresses tokens on a negative token list without impacting irrelevant tokens . \u0421\u0422 has been theoretically proven to be advantageous over traditional cross-entropy (CE) and UL loss and emerged as the most effective algorithm of oscillations suppression to date."}, {"title": "Methodology", "content": "A well-known theoretical analysis of the repetition problem in text generation simplifies predicting the next word into a first-order Markov chain . It assumes that the currently generated token is only affected by the same token generated at the previous moment. Under this assumption, the entire generation sequence forms a directed cycle when the model generates a word that has already been generated. For the sentence \"I like"}, {"title": "Hallucination Analysis", "content": "it and guess he knows I like it because ...\", this theory insists that the second generation of \"I like it\" is mainly affected by the former so that the next predicted token is most likely \"and\". Methods like CT and PS employ this idea to prevent directed cycles and reduce repetitive text generation.\nHowever, this theory ignores the input text and previously translated text, contrary to the model based on the cross-attention mechanism. Consider the title of an item: \"Best Selling New Arrival Outdoor Shapewear Dress Women's Dresses Built-in Shapewear Maxi Dress\". Global-level suppression of repetitive generation can lead to the replacement of \"shapewear\" and \"dress\" in the latter part of the title translation with other words, thus deviating from the original meaning. To more accurately analyze the impact of each former token on the next predicted word, we perform visual analysis through the ALTI+ method . ALTI+ calculates the contribution of each previous token to the generation of the current token by comparing the Manhattan distance between the previous token and the newly generated token representation. For a comprehensive comparison, we show En-De translation with three columns: (a) normal result, (b) middle appearing repetition result, and (c) total repetition result, respectively, in Figure 1.\nWe can distinctly observe that: 1) The generation of each token is primarily influenced by the input text and the nearest neighboring tokens, with tokens at relatively farther positions exerting minimal impact; 2) Identical repeated words are affected by tokens in the same position, and the longer the generated text, the weaker the influence of the corresponding tokens in the input text. This explains why global suppression of repeated words, although effective in suppressing repetition, leads to poorer translation outcomes, particularly in decoder-only LLMs, where a forcibly replaced repetitive token results in subsequent tokens deviating increasingly from the original meaning.\nHere, we attempt to elucidate the underlying causes of text repetition generation from the perspective of information entropy. In the transformer mechanism, the generation of each token is influenced by all preceding tokens. Repetitive token generation occurs because the information from previously generated tokens does not provide additional value. For instance, when predicting the (n+1)th token, the information from tokens 0 to (n-1) is identical to that from tokens 0 to n, causing the model to generate the nth word repetitively."}, {"title": "Learning-Based Solution", "content": "Models trained with traditional CE loss are prone to hallucinations when facing out-of-domain data. To address the issue in translation models, several new training objectives (loss functions) have been designed to suppress negative tokens more effectively. Welleck et al. proposed unlikelihood training, specifically designed to penalize the likelihood of negative tokens. The token-level unlikelihood training objective (UL-T) at time step t is defined as:\n$L_{UL} = -\\sum_{y_t \\in C_t} log (1 - p (y_t | y_{<t}, x))$\nwhere x is the source text, $y_{<t}$ is the translation text generated before time step t, $C_t$ is the set of"}, {"title": "", "content": "previous negative tokens at time step t. This approach focuses on decreasing the probability of generating already-produced tokens, aiming to break the directed cycle observed in NMT models.\nAdditionally, contrastive learning loss $L_{CL}$ has been proposed as an effective training objective , which encourages models to learn isotropic token representations through a similarity penalty. The $L_{CL}$ is defined as:\n$L_{CL} = \\frac{1}{t-1} \\sum_{i=2}^{t-1} max{0, \\rho - s (h_{y_t}, h_{y_i}) + s (h_{y_i}, h_{y_{t-i}})} \\,$\nwhere $ \\rho \\in [-1,1]$ is a pre-defined margin, $h_y$ is the embedding of token $y_i$, $s (h_{y_i}, h_{y_{t-i}}) = \\frac{(h_{y_i} \\cdot h_{y_j})}{(\\parallel h_{y_i}\\parallel \\parallel h_{y_{t-i}}\\parallel)}$ is cosine similarity. This loss function is designed to increase the distance between representations of distinct tokens, thereby creating a more discriminative and diverse model representation space.\nRecently, CT loss is presented  and the formulation for time step t is defined as:"}, {"title": "", "content": "$L_{CT} = log \\big[ 1 + \\sum_{y_T \\in S_N} exp (h_t^T W \\cdot W_{y_t}) \\big]$\nwhere $h_t$ is the hidden state, $y_t$ means the positive token at time step t. $W_{y_t}$ denotes the embedding for token $y_t$, $S_N$y is the set of the previous N tokens."}, {"title": "", "content": "The research shows that CT loss is the optimal loss function in suppressing oscillations hallucination, as it only suppresses negative tokens while enhancing positive tokens. Despite its effectiveness, its rough selection of negative tokens sometimes leads to suboptimal results. Therefore, based on the previously analyzed repeatability principle of ALTI+ and T-SNE, this paper proposes CTSD loss, an optimization of the original CT loss, significantly improving the accuracy, stability, and effectiveness of the output of the new training model. This method dynamically suppresses previously generated tokens by designing two attenuation factors. The first attenuation factor uses cosine similarity to measure the similarity in the context that the hallucination token's attention is very similar to the previous token. Additionally, an exponential-decay attenuation factor is designed to weaken the suppression of distant tokens, considering that the contribution of generated tokens is inversely related to the distance between tokens.\nFinally, CTSD loss for time step t is defined as:"}, {"title": "", "content": "$L_{CTSD} = log \\big[ 1 + \\sum_{y_T \\in S_N} exp (ad \\cdot as \\cdot h_t^T W \\cdot W_{y_t}) \\big]$\nwhere $a_d = e^{- \\frac{t - t_-t}{T}}$, $t_-$ represents the time when $y_t$ is generated, $T$ is the temperature coefficient that controls decay. $a_s = \\frac{attent_-t \\cdot attent_t}{\\parallel attent_-t \\parallel \\parallel attent_t \\parallel}$, represents attention distribution between $y_t$ and encoder embedding.\nTo more intuitively demonstrate the role of the attenuation factor, we display the weight matrix of a normal translated sentence (constructed from the attenuation factor of each generated token) in Figure 3. Figure 3(a) is the attention similarity matrix. Figure 3(b) is the exponential decay matrix. It can be observed that under normal translation results, the overall attention similarity between tokens is low, but some unrelated tokens that are far apart also have high similarity. Hence, an additional exponential-decay attenuation factor must be added for secondary suppression."}, {"title": "EXPERIMENTS", "content": "The experiments aim to evaluate whether CTSD can suppress hallucinations in specialized translation models and LLMs while maintaining stability. We integrate several baseline methods, including traditional CE loss, decoding-based methods like CS and PS, and training-based methods such as UL at Token-level (UL-T), CL, and CT.\nFor specialized models like NLLB-1.3B and mBART-large. Through extensive comparative analysis and experimentation, a batch size of 64 and a fixed learning rate of 5 \u00d7 10-5 can effectively balance stability and ultimate performance. For decoder-only LLMs, like LLaMA2-7B and Qwen-7B, the LoRA method with parameters r and alpha set to 8 and 16, respectively, is applied. The batch size for these models is set to 32, and a learning rate is established at 2 \u00d7 10\u22125, aimed at consistently improving the translation capabilities of LLMs. The hyperparameter ablation experiment for the CTSD method is discussed in the appendix.\nOur datasets comprise the open-source general dataset WMT16 for training and the FLORES-200 devtest dataset for evaluation. Furthermore, a novel evaluation dataset comprising e-commerce texts susceptible to hallucination translations is compiled and released to benchmark our algorithm. This dataset is an English-German Parallel Corpus encompassing 3,500 authentic titles from alibaba.com. Each text segment has undergone meticulous translation and verification by human experts.\nNMT Performance is evaluated by SacreBLEU, Rouge-L, and COMET, along with repeatability metrics such as 2-gram and 3-gram repetition rates (rep-2 and rep-3), token dispersion (div), rep-w, and rep-r. Rep-n measures sequence-level repetition, div estimates repetition at different n-gram levels, and rep-w calculates the proportion of current tokens occurring within the previous w tokens. It is worth noting that lower repeatability metrics do not always indicate better NMT quality."}, {"title": "Evaluation Results", "content": "As shown in Table 2, CTSD consistently improves translation quality across all models, as evidenced by SacreBLEU, Rouge-L, and COMET metrics, while maintaining extremely low repetition rates.\nFor specialized translation models, the CT loss underperforms CE loss in the non-hallucination dataset, while CTSD significantly enhances performance on both e-commerce hallucination and general datasets. NLLB-1.3B and mBART-large showed notable improvements of +13.1% and +4.0% in SacreBLEU and +4.7% and +5.47% in COMET, respectively, substantially reducing repetition rates. For LLMs prompted for translation tasks, CTSD demonstrated significant improvements, particularly on the e-commerce hallucination dataset. LLaMA2-7B achieved +10.76% in SacreBLEU and +5.50% in Rouge-L compared to the CT model. Additionally, closed-source models like ChatGPT and GPT-4 scored lower in Sacre-BLEU but acceptable in COMET, with decent translation capabilities and strong hallucination suppression, while showing weaker professionalism for specialized tasks.\nExperiments with the Qwen-1.8B and Qwen-14B models (Table 3) show that CTSD effectively maintains translation accuracy across different LLM sizes, emphasizing its robust enhancement of LLM translation capabilities regardless of hallucination tendencies.\nTo further verify the effectiveness of CTSD as a repetition suppression method, we conducted experiments comparing different methods, summarized in Table 4. Although the decoding method significantly improved the hallucination dataset (811.27% increase in SacreBLEU and 103.93% in COMET), its translation quality still lagged behind training methods. Among the training methods, CTSD stands out on both specialized translation models and LLMs, maintaining a meager repetition rate, indicating it is a general and efficient repetition suppression method."}, {"title": "Online E-commerce Experiments", "content": "We implemented the CTSD algorithm on the specialized translation model of the www.alibaba.com website, which uses an encoder-decoder structure with 48 layers and approximately 1.1B parameters. We selected six high-traffic language websites (AR - Arabic, DE - German, TH - Thai, HI - Hindi, HE - Hebrew, and PT - Portuguese) to translate item titles and descriptions. These sites serve millions of users, generating nearly 20 times daily page views (PVs). A/B tests were conducted with models fine-tuned on an e-commerce dataset, comparing CTSD and non-CTSD models. Each user saw translation text from only one model to ensure fairness.\nFirst, to ascertain the impact on online translation accuracy, we performed a pre-procedure expert evaluation. 2,000 items were randomly selected and translated by both models. Experts rated the translations on a 5-point scale, with LQR-3 (or 4) indicating that at least two experts rated the translation more than 3 points (or 4 points). Table 7 shows that the CTSD-trained model significantly improved translations across all six languages, particularly for languages with fewer training data (AR, TH, HI, HE). Additionally, the CTSD model outperformed Google Translate overall, with the Fleiss Kappa mean value exceeding 0.6, demonstrating high consistency among raters.\nFor online evaluations, we assessed business indicators such as page view (PV), retained user (UV), click-through rate (CTR), average conversion rate (CVR), gross merchandise volume (GMV), and revenue per mille (RPM). The online A/B experiments in AR and DE (Table 8) showed that the new translation model improved title translation quality, leading to greater product attention and significant enhancements in all indicators, especially GMV and RPM, which enhanced by 2.96% and 2.21% on Arabic sites, respectively.\nAll data and code implements will be released to the public after publication."}, {"title": "Conclusion", "content": "In conclusion, this study addresses the critical challenge of repetition generation in NMT. By analyzing and visualizing the underlying causes from the lens of information entropy, we propose one novel"}, {"title": "Limitations", "content": "While our CTSD method has shown significant improvements in reducing repetition and enhancing translation quality, there are some limitations to consider. Firstly, the optimal settings for temperature coefficient and decay factor may vary across models and datasets. Automatic tuning for these hyperparameters needs further investigation. Secondly, the additional computations for attention similarities and decay factors during training have not been fully analyzed. Assessing the trade-off between performance gains and computational costs is necessary, especially for resource-limited environments. Addressing these limitations in future work can enhance the robustness and applicability of the CTSD method, contributing to more reliable NMT systems."}, {"title": "Ethics Statement", "content": "In this work, we employed publicly released and private e-commerce domain datasets to train our machine translation models. Public datasets have been carefully reviewed for ethical concerns, and our manual inspections found no significant ethical issues, such as violent or offensive content. The e-commerce datasets are anonymized and collected with proper consent, following data protection regulations. We also intend to share our source code with clear instructions to encourage ethical use. Despite these precautions, machine translation can sometimes produce unexpected outputs. We will implement mechanisms to reduce such risks and advise users to follow ethical guidelines to prevent misuse."}, {"title": "Explanation of Metrics", "content": "In this paper, rep-w is calculated by the proportion of current tokens occurring within the previous w tokens, expressed as:\n$rep-w = \\frac{1}{|D|} \\sum_{s\\in D} \\frac{1}{|s|} \\sum_{t=1} 1[s_t \\in s_{t-w-1:t-1}]$\nwhere D represents the result set, s represents generated sentences in D. Moreover, rep-r stands for the ratio of the repetition snippet in a sentence measured by length, defined as:\n$rep-r = \\frac{1}{|S|} \\sum_i \\frac{|{s_i \\mid (s_i = s_j \\land s_{i+1} = s_{j+1}, \\exists j \\neq i) \\lor (s_i = s_k \\big/ s_{i-1} = s_{k-1}, k \\neq i)}|}{|S|}$"}]}