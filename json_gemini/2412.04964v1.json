{"title": "Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference", "authors": ["Qingyuan Li", "Bo Zhang", "Liang Ye", "Yifan Zhang", "Wei Wu", "Yerui Sun", "Lin Ma", "Yuchen Xie"], "abstract": "The ever-increasing sizes of large language models necessitate distributed solutions for fast inference that exploit multi-dimensional parallelism, where computational loads are split across various accelerators such as GPU clusters. However, this approach often introduces significant communication overhead, especially on devices with limited bandwidth. In this paper, we introduce Flash Communication, a novel low-bit compression technique designed to alleviate the tensor-parallelism communication bottleneck during inference. Our method substantially boosts intra-node communication speed by more than 3\u00d7 and reduces the time-to-first-token by 2\u00d7, with nearly no sacrifice in model accuracy. Extensive experiments on various up-to-date LLMs demonstrate the effectiveness of our approach.", "sections": [{"title": "1. Introduction", "content": "To date, the number of parameters of large language models has tremendously increased. For instance, GPT-3 (Brown et al., 2020) has 175B, DeepSeek V2 (Liu et al., 2024) utilizes 236B, LLaMA-3 (Dubey et al., 2024) reaches 450B. Their enormous sizes create big challenges for both training and inference.\nTo tackle the scaling difficulties of large language models, the research community now resorts to multiple parallelism strategies across a large group of computing accelerators. Since previous parallelism methods focus on resolving the training challenges, we quickly review these methods for a background check. Particularly, data parallelism (Dean et al., 2012; Ben-Nun & Hoefler, 2019) is first introduced to allocate the training samples onto multiple GPUs where each GPU retains a duplicate of the model and processes its own given batch of samples. Synchronization is hence required at the end of each iteration to update the model parameters. In the LLM era, ZeRO (Rajbhandari et al., 2020) and FSDP (Zhao et al., 2023) renovate data parallelism by sharding models on all devices but virtually rendering a whole model on a single device through All-Gather communication. In contrast, pipeline parallelism partitions sequential layers onto different GPUs where point-to-point communication is adopted to transmit activation and gradients. However, it creates data dependency which leads to substantial GPU idle time, called bubbles. To improve GPU utilization, GPipe (Huang et al., 2019) schedules micro-batches in a pipeline with forward passes and then followed by backward passes. PipeDream (Harlap et al., 2018) proposes one-forward one-backward (1F1B) to further reduce the bubble ratio. Megatron-LM (Narayanan et al., 2021) advances PipeDream by allowing each device to perform computation for multiple non-contiguous subsets of layers. Another dimension to split the model is tensor parallelism which splits the tensors of each layer and performs All-Reduce to aggregate the activation and gradients from all devices. Megatron-LM (Shoeybi et al., 2019; Narayanan et al., 2021) is such an example that devises delicate conjugate tensor slicing schemes (e.g. column-wise first and row-wise next for the MLP layer) to remove unnecessary synchronization inside a transformer block.\nWith the rise of LLMs developed for the long-context scenario, sequential parallelism (Korthikanti et al., 2023) is proposed to divide activation of LayerNorm and Dropout layers in the sequence dimension as they are sequence-independent. It also jointly combines with tensor-parallelism by replacing two All-Reduce operations with one All-Gather and one Reduce-Scatter to merge the communication cost. However, self-attention and MLP layers are left untouched for sequential parallelism. In this regard, context parallelism (NVIDIA, 2024b) is designed to separate all layers in sequence dimension. To break the sequence dependency in self-attention, Ring Attention (Liu et al., 2023) applies blockwise self-attention and feedforward (Dao, 2023; Liu & Abbeel, 2023) in a distributed environment with point-to-point communication. On top of this, Deepspeed-Ulysses (Jacobs et al., 2023) exchanges point-to-point communication for All2All for faster speed.\nAnother emerging direction is sparse architectures represented by mixture-of-experts models (Jiang et al., 2024; Team, 2024; Liu et al., 2024). Expert parallelism (Fedus et al., 2022) parallelizes the experts on different GPUs which requires All2All communication. Deepspeed-MoE (Rajbhandari et al., 2022) propose hierarchical All2All communication to reduce the number of communication hops.\nAs large language models continue to scale up, modern frameworks like DeepSpeed (Microsoft, 2024), and Megatron (NVIDIA, 2024a) tend to make joint use of the aforementioned parallelism to accelerate the training process. Nevertheless, they easily meet communication bottlenecks as they require many collective operations. This overhead grows as the model becomes larger.\nMeanwhile, the communication bottleneck is also pronounced when serving large language models in the cloud. Constrained by strict service level objectives (SLOs), multiple parallelism schemes are adopted to speed up the inference, where tensor parallelism is the most popular option among all. Besides, due to the lower bandwidth of inference GPUs, communication can account for more than half of the prefill inference cost where an 80-layer LLaMA-3-70B (Dubey et al., 2024) carries out 160 all-reduce operations at each forward pass on 4\u00d7L40 GPUs, as shown in Figure 1. Therefore, to enable a faster speed, we must make efficient use of limited intra-node bandwidth.\nIn this work, we design a novel technique to reduce the communication cost introduced by tensor parallelism without substantially sacrificing accuracy. Our contributions are,"}, {"title": "2. Related Work", "content": "Before diving into the investigated problem, we cover some fundamental knowledge required for discussion in Appendix A. We suggest that readers without prior experience quickly review the content.\nCommunication efficiency is crucial to distributed training and serving, as it directly affects the total processing time and cost. Several techniques have been proposed to optimize communication in distributed training in recent years, including topology optimization, pipeline optimization, and compression methods."}, {"title": "2.1. Topology Optmization", "content": "Topology optimization adjusts communication patterns to match the physical topology of hardware to reduce communication latency/hops, mainly ring-based and tree-based. Ring-All-Reduce (Baidu Research, 2024) organizes workers in a ring topology so that the overall communication latency is constant regardless of the number of workers. Say a worker transmits data of volume M to a group of N - 1 workers, the total communication volume is $2M (N-1)/N$, which is approximately 2M when N >> 1. However, it doesn't take the physical topology into account, where intra- and inter-node communication have different bandwidths. Hence the average speed depends largely on the lowest bandwidth in such a strategy. Hierarchical Ring-All-Reduce (Jia et al., 2018) highlights the importance of hierarchical structures in managing overheads, which employs three-phase all-reduce for separate intra- and inter-node communication. Later, 2D-Torus (Mikami et al., 2018) organizes GPUs in a 2D-grid of (X, Y) so that the inter-node horizontal communication volume is X times smaller than that of hierarchical"}, {"title": "2.2. Pipeline Optimization", "content": "Pipelining optimization aims to maximize resource utilization with optimized scheduling strategies, mainly by overlapping computation with communication. Domino (Wang et al., 2024) breaks data dependency in Tensor-parallelism by splitting activations row-wisely and weights column-wisely into smaller independent parts. FLUX (Chang et al., 2024) divides computation and communication operations into much finer-grained operations and later merges them in a larger kernel to effectively hide communication. DistributedGEMM (Hassani et al., 2024) provides an implementation based on CUTLUSS (NVIDIA, 2024f) using P2P communication. ScMoE (Cai et al., 2024) implements a shortcut-connected MoE architecture to effectively decouple communication from its conventional sequence, allowing for a substantial overlap."}, {"title": "2.3. Communication Compression", "content": "Compression techniques like sparsification and quantization are proposed to balance communication reduction with acceptable performance degradation. Sparse Communication (Aji & Heafield, 2017) observes that gradient updates are mostly close to zero and maps them directly to zero to only exchange sparse matrices among distributed nodes. In contrast, DISCO (Qin et al., 2024) aims to achieve sparse communication by gradually pruning the network to generate sparse features. QSDP (Markov et al., 2023) remove FSDP's communication bottleneck by performing both gradient and weight quantization. ZeRO++ (Wang et al., 2023) applies All-Gather with blockwise weight quantization and an All2All-based gradient quantization to reduce the communication volume when collecting weights and gradients."}, {"title": "3. Method", "content": null}, {"title": "3.1. Motivation", "content": "Tensor Parallelism (TP) is now supported in almost all mainstream inference frameworks like TensorRT-LLM (NVIDIA, 2023), vLLM (Kwon et al., 2023), SGLang (sgl-project, 2024), and LMDeploy (LMDeploy, 2023), becoming the most adopted scheme in LLM inference. However, TP comes at a non-negligible cost due to heavy communication, which in the case of larger language models creates an excessive communication overhead. For example, the communication overhead of LLaMA-3-70B on L40 GPUs easily meets the bottleneck as the input token length increases. Although high-end training-purpose accelerators like NVIDIA A100 where GPUs are connected through NVLink (NVIDIA, 2024e), the communication overhead still reaches a notable 20%. We can easily conclude that TP communication is the inference bottleneck."}, {"title": "3.2. Flash Communication", "content": "Motivated by the above, we approach the inference challenges in the distributed scenario with quantization and topology optimization. In the paper, we specifically examine tensor parallelism as it is the most popular paradigm.\nTake tensor parallelism in LLaMA-3 (Dubey et al., 2024) as an example, the tensors of QKV projections are first sliced column-wisely and then the output projection row-wisely. After that, an All-Reduce operation is required to collect activations on each device, shown in Fig. 3. Similarly in the feedforward network, the gate projection and the up projection are split by column and the down projection by row, then another All-Reduce is needed to sum up the activations from both devices. To reduce the communication volume, we are left to compress the activation from the output projection and the down projection."}, {"title": "3.2.1. QUANTIZATION CHALLENGE", "content": "To obtain an optimal trade-off between accuracy and latency, we choose to apply low-bit quantization. From Fig. 4, we observe that fine granularity is necessary since per-token quantization at larger block sizes suffers from performance collapse in terms of C4 perplexity, albeit the asymmetric version is relatively better.\nHowever, we discover that it is non-trivial to apply low-bit"}, {"title": "3.2.2. ALGORITHM", "content": "Considering the above issues, we design a two-step quantization strategy to replace vanilla Ring All-Reduce, as portrayed in Fig. 6. Its integration with tensor parallelism is shown in Fig.3. We name our overall strategy as two-step All-Reduce.\nFig. 6 illustrates how Flash Communication works. First, we divide the computation volume (activation) on each GPU by the number of ranks. After fine-grained quantization on activation, we perform All2All communication so that each device receives its computation load for reduction. After on-device reduction, the sum is again quantized to speed up the transmission. We then perform All-Gather to collect all results and dequantization to recover float values on each device. This two-step workflow is also formulated by Alg. 1."}, {"title": "3.2.3. KERNEL DESIGN", "content": "For efficiency, we implement a fused Flash All-Reduce kernel to encompass all the above collective communication operations and quantization steps. Compared with Ring All-Reduce in Table 1, Flash All-Reduce cuts quantization-dequantization steps from N to 2, and Reduce/Gather steps from N-1 to 1. Although the size of total volumes remains the same, each of our volumes is quantized to lower bits, substantially reducing the amount of data to transmit. We summarize three key aspects in designing our kernel below.\nFast Fine-grained Quantization The total communication volume M for each rank is divided into T chunks for transmission. Given a chunk size C, we draw how GPU threads are organized in parallel to process the chunk information in Fig. 7. A chunk is split into N blocks and each block corresponds to 32 warps, where each warp is a collection of 32 threads and each thread can process eight FP16 elements. Take our asymmetric quantization with a group size of 128 as an example, we perform quantization on each group of 128 elements using 16 threads. Specifically, we leverage the CUDA API function _shfl_xor_sync to iteratively exchange information among these warp threads to achieve max/min reduction efficiently.\nFast Communication. Instead of calling All2All primitive, we utilize GPU peer direct memory access from CUDA Run-"}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Setup", "content": "Unless otherwise noted, we use an input token length of 1024 and an output token length of 64 for the inference measurement. Latencies are tested on NVIDIA L40 and A100 SXM GPUs. The baseline uses FP16 for communication."}, {"title": "4.2. Accuracy Comparison", "content": "FP16 Weights. We evaluate the accuracy of LLaMA-2 and LLaMA-3 models on PIQA (Bisk et al., 2020), ARCc and ARCE (Clark et al., 2018), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021) in various communication quantization bit widths, shown in Table 2. In all cases, asymmetric INT8 quantization obtains the best accuracies. Asymmetric INT4 is also better than symmetric INT4. C4 (Raffel et al., 2020) and WikiText (Merity et al., 2016) results are shown in Table 6 of Appendix B.1."}, {"title": "4.3. Latency and Throughput Performance", "content": "Fig. 9 illustrates weight-quantized LLaMA-3-8B and LLaMA-3-70B's TTFT comparison with and without Flash communication. The lowest quantization bit yields the most gain, i.e. 2.06\u00d7 and 1.19\u00d7 on L40 and A100 respectively. More measurements are listed in Appendix C."}, {"title": "5. Ablation Study", "content": null}, {"title": "5.1. Integer vs. Low-bit Float", "content": "Table. 4 shows the difference between INTx and FPx communication quantization. In general, INT8 performs comparably with FP8, while the asymmetric version of INT8 is the best among all. FP6 is a mixed version of FP8 (Micikevicius et al., 2022) and FP4 (Rouhani et al., 2023), which is a fair comparison with similarly mixed INT6."}, {"title": "5.2. Na\u00efve vs. Rotation-based Quantization", "content": "As previously shown in Fig. 4, the C4 perplexity of coarse quantization suffers performance collapse, while fine-grained quantization gradually resolves the problem as the group size increases. We investigate whether the Hadamard"}, {"title": "5.3. Flash All-Reduce vs. Ring All-Reduce", "content": "64MB), our quantized All-Reduce is crucial to reduce the cost, where the INT4 version brings at most 3.18\u00d7 kernel latency reduction. Noticeably, the mixed precision version INT6 obtains a good trade-off between INT8 and INT4.\nWe further show that the number of streaming processors (SMs) matters in Fig. 11. When the communication volume is of small size, a smaller number of SMs is beneficial as less kernel launch and inter-block synchronization overhead is produced. When the volume gets larger, more SMs are required for calculation. A configuration of 48 SMs strikes a better balance between communication and computation."}, {"title": "6. Conclusion", "content": "Our work presents a novel technique to reduce the communication volume associated with tensor parallelism while maintaining accuracy. Our key contributions include a comprehensive analysis that reveals the communication bottleneck in Large Language Model (LLM) inference, the design of a fast communication mechanism known as Flash Communication, and the demonstration of its implementation, which has been shown to achieve up to a 2\u00d7 TTFT reduction. Flash Communication employs fine-grained quantization on activations and a two-step All-Reduce strategy to decrease communication volumes significantly. We have conducted extensive experiments on NVIDIA L40 and A100 GPUs across various configurations and with several state-of-the-art LLMs, which have consistently demonstrated the effectiveness of our approach. These findings address a critical challenge in parallel computing and pave the way for more efficient and scalable LLM inference."}, {"title": "A. Background", "content": null}, {"title": "A.1. GPU Topology", "content": "Modern inference GPUs are connected via various hardware configurations. Figure 12 shows a typical simplified physical topology where every node contains 8 GPUs. Every two adjacent GPUs are connected via a PCIe of 64GB/s bandwidth (NVIDIA, 2024c). Cross-GPU communication may take several different paths, e.g. GPU 0 to 1 has the shortest route, but from GPU 0 to 4 it has to go across two NUMA (Non-uniform memory access) nodes. Cross-node communication relies on NICs (Network interface cards) that transmit data via Ethernet, whose bandwidth is usually 100Gbps."}, {"title": "A.2. Collective Communication", "content": "Libraries. Due to the hierarchical design of GPU clusters, collective communication methods are crucial in distributed training and inference. To synchronize the workloads across GPUs, communication libraries like NCCL (NVIDIA, 2024d), MSCCL (Microsoft, 2024), HiCCL (Hidayetoglu et al., 2024), Gloo (Facebook, 2024), and Horovod (Sergeev & Balso, 2018) are developed to provide efficient collective communication operations for a group of devices. These libraries usually hide the physical topology and organize GPUs in a ring (Mikami et al., 2018; Jia et al., 2018; Ying et al., 2018) or a tree. In ring-based topology, GPUs are connected hand by hand to create a logical circle, which maximizes the utilization of the full bandwidth. In contrast, tree-based topology, especially double binary tree (Sanders et al., 2009), guarantees logarithmic communication hops. Therefore, it is more beneficial to use ring-based communication for intra-GPUs and a tree-based approach for inter-GPU clusters.\nOperations. Collective operations such as broadcast, aggregation (Reduce/All-Reduce/Reduce-Scatter), collection (Gather/All-Gather), and All2All are shipped out-of-box in most collective communication libraries. For instance, NCCL provides a series of such collective operations (NVIDIA, 2024a) where each rank processes or transmits the same amount of data. Reduce-Scatter sums data across nodes and then scatters the result to corresponding nodes. All-Gather collects data from all nodes to all nodes. All-Reduce is a many-to-many reduce operation, where the same reduce operation is applied on all nodes. All2All exchanges data between all nodes, with each node sending and receiving an equal amount of data. All2All can be implemented with multiple point-to-point communication operations."}, {"title": "A.3. Quantization Fundamentals", "content": "Quantization is a mapping from floating numbers to integers. We utilize asymmetric quantization which is formulated below,\n$s = \\frac{\u0425_{\u0442\u0430\u0445} - X_{min}}{2^{n}-1}, z = [\\frac{-X_{min}}{S}]$\n$Q(X) = clamp([X/s] + z,0,2^{n} \u2013 1)$\nwhere $X_{max}$ and $X_{min}$ denotes the maximum and minimum value of X, n is the quantization bit-width, s is called the scale and z the zero point. Q(x) quantizes float X to integer to the target bitwidth.\nSymmetric quantization is formulated as follows,\n$S=\\frac{\u0425_{\u0442\u0430\u0445}}{2^{n-1}-1}$\n$Q(X) = clamp([X/s], -2^{n-1},2^{n-1} \u2013 1)$\nIEEE 754 standards for FP16. IEEE 754 (IEEE, 1985) FP16 includes 16 bits in total, which comprises 1 bit for the sign (S), 5 bits for the exponent (E), and 10 bits for the mantissa or fraction (F). The bias for the exponent is 15, which means that the actual exponent value must be added to 15 to get the stored exponent value. Also, notice there's an assumed leading 1 in the fractional part.\nFP8 and FP4 Format. FP8 (Micikevicius et al., 2022) format is designed to advance FP16 with two encodings, E4M3 (4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa). E5M3 follows IEEE 754 conventions. FP4 (Rouhani et al., 2023) is of E2M1. For quantization to FP4, we utilize QPyTorch (Zhang et al., 2019) for simulation."}]}