{"title": "Graph-based Uncertainty Metrics for\nLong-form Language Model Outputs", "authors": ["Mingjian Jiang", "Yangjun Ruan", "Prasanna Sattigeri", "Salim Roukos", "Tatsunori Hashimoto"], "abstract": "Recent advancements in Large Language Models (LLMs) have significantly improved text\ngeneration capabilities, but these systems are still known to hallucinate, and granular un-\ncertainty estimation for long-form LLM generations remains challenging. In this work, we\npropose Graph Uncertainty \u2013 which represents the relationship between LLM generations and\nclaims within them as a bipartite graph and estimates the claim-level uncertainty with a family\nof graph centrality metrics. Under this view, existing uncertainty estimation methods based\non the concept of self-consistency can be viewed as using degree centrality as an uncertainty\nmeasure, and we show that more sophisticated alternatives such as closeness centrality provide\nconsistent gains at claim-level uncertainty estimation. Moreover, we present uncertainty-\naware decoding techniques that leverage both the graph structure and uncertainty estimates\nto improve the factuality of LLM generations by preserving only the most reliable claims.\nCompared to existing methods, our graph-based uncertainty metrics lead to an average of 6.8%\nrelative gains on AUPRC across various long-form generation settings, and our end-to-end\nsystem provides consistent 2-4% gains in factuality over existing decoding techniques while\nsignificantly improving the informativeness of generated responses\u00b9.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) [1\u20135] have demonstrated remarkable capabilities and been widely\nused as an interactive chatbot to provide knowledge and answers to user queries. However, they still\nstruggle with generating false information, often referred to as \u201challucinations\u201d [6], which hinders\ntheir ability to provide calibrated [7\u201310] and factual [11, 12] responses and ultimately undermines the\ntrust users place in their outputs. Improving uncertainty estimation techniques is crucial for building\ntrust in LLMs and mitigating the risks associated with their deployment in real-world applications.\nWhile many existing uncertainty estimation techniques for LLMs primarily focus on estimating the\nuncertainty of their answers to multiple-choice questions [13, 7, 8] or their entire generated responses\n(typically in a short form) [8, 14, 9, 15], they are often not sufficiently informative in real-world\napplications where LLMs generate paragraphs of texts consisting of a mixture of true and false claims\n[12]. In such scenarios, more granular uncertainty estimates are needed to help users distinguish the\nreliability of each individual claim within the generated text. Recent approaches [11, 16] attempt to\nmeasure the uncertainty of each claim by its consistency with randomly sampled responses based on\nthe concept of self-consistency [17]. However, they do not fully leverage the semantic relationships\nbetween claims and responses that could support more granular uncertainty estimation."}, {"title": "2 Related Work", "content": "Short-form uncertainty estimation in LLMs Existing approaches for characterizing the uncertainty\nof LLMs have largely focused on multiple-choice classification or short-form generation setups and"}, {"title": "3 Preliminary", "content": "In this work, we focus on the problem of granular uncertainty estimation for LLMs, particularly in\nthe context of distinguishing whether each claim in a long-form output is factual. Specifically, let\n\u2211 denote the set of all characters and \u03a3* the space of all possible text strings. Given a text prompt\nx \u2208 \u03a3*, the generation process of a model M with a specified temperature T = t can be represented\nas a conditional probability distribution $M_{T=t}(\u00b7|x)$ over \u03a3*.\nUncertainty estimation for LLMs In the context of LLMs, uncertainty estimation is concerned with\nthe following: given a model M, a prompt x \u2208 \u03a3*, and a response y \u2208 \u03a3*, we seek an uncertainty\nfunction $U : \u03a3^* \u00d7 \u03a3^* \u2192 R$ that measures the uncertainty of LLMs about the response. In this work,\nwe define the efficacy of U by how effectively it differentiates the true and false claims of y, using\nclassification metrics such as AUROC and AUPRC. We focus on classification metrics rather than\ncalibration or coverage, as our main goal will be using U to identify and remove false claims as part\nof a decoding-time intervention.\nClaim-level uncertainty estimation In many practical applications, the outputs from an LLM\nencompass a few paragraphs of text containing multiple claims [11, 16]. We consider a claim to\nbe the smallest semantically distinct unit of information presented within the generated output. For\nexample, in Fig. 1, \u201cSnedden was elected to the Australian Parliament\u201d is an example of a single\nclaim. In this work, instead of assigning a single uncertainty score to the entire output, we assess\nuncertainty at the level of individual claims. Formally, we further define C as a universal set of all\nunique, semantically distinct claims. The claim-level uncertainty function is then $U : \u03a3^* \u00d7 C \u2192 R$,\nallowing for granular analysis of factuality at the claim level."}, {"title": "4 Claim-Level Uncertainty Estimation with Semantic Entailment Graphs", "content": "Our motivation stems from the observation that given a set of generated responses R and their\nentailed claims C, we can construct a bipartite graph G = ((R, C), E) between R and C with edges E\nindicating the entailment relationship between each response and claim (Fig. 1). This graph captures\nthe semantic entailment relationship between responses and claims, from which we may extract\ninformation that effectively captures the uncertainty of each claim. A motivating example is SC,\nwhich turns out to be a special case of calculating the degree centrality of each claim node as their\nuncertainty. This encourages the investigation of a broad family of graph-based metrics beyond\nthe node degree, potentially offering more robust uncertainty estimates by exploiting intra-graph\ninformation.\nIn the following section, we will demonstrate how to construct the semantic entailment graph using\nan LLM in Sec. 4.1, and then describe the graph metrics we are exploring in Sec. 4.2."}, {"title": "4.1 Semantic Entailment Graph Construction", "content": "Here we describe the procedure for constructing a bipartite graph G = ((R,C), E) that captures the\ngeneration-claim relationships for a given input x using an LLM, as illustrated in Fig. 1. The graph\nconstruction involves multiple LLM interactions, with detailed prompts provided in Appx. F."}, {"title": "4.2 Uncertainty Estimation with Graph Centrality Metrics", "content": "Recall that SC corresponds to using the specific claim node degree as the uncertainty estimate.\nIntuitively, the effectiveness of SC demonstrates that the more 'connected' a claim node is to other\nnodes in the graph, the more likely the claim to hold true. Drawing on this premise, we explore\na broader family of graph centrality metrics [18-21] that measure the importance of a claim node\nwithin the graph from different angles, some of which may correlate with the factuality of the node to\na greater extent. Specifically, denoting the graph G = (V, A) here by its node set V and adjacency\nmatrix A, we assess the graph centrality metrics detailed in Table 1. These include the betweenness\nCB, eigenvalue CE, PageRank CPR, and closeness centrality CC. A full definition of the notations\nis provided in Appx. E. Note that we use the Wasserman and Faust (WF) improved formula [20] for\ncloseness centrality to ensure applicability to disconnected graphs.\nThese centrality metrics are pre-defined to measure the importance of a node in a graph in different\nways, and it is not clear a priori which types of centrality are useful for uncertainty estimation.\nTherefore, we carefully study all of these centrality metrics in various settings in our experiments\n(Sec. 6.1). We use these centrality metric values of the claim nodes within the bipartite graph as\ntheir confidence scores (i.e., the negative values as their uncertainty estimates), and evaluate the\ncorrelation between these metric values and the claim factualities. This analysis helps us identify the\nmost empirically effective centrality metric for uncertainty estimation at the granularity of claims."}, {"title": "5 Uncertainty-Aware Decoding", "content": "We have introduced a graph-based technique for estimating the uncertainty at the level of individual\nclaims. To demonstrate that our uncertainty estimation method translates to more factual LLM outputs,\nwe now present a framework that integrates these uncertainty estimates at decoding time to improve\nthe factuality of LLM outputs (Fig. 2). Similar to contemporaneous work on factuality-enhancing\ndecoding [16, 23], we filter claims by uncertainty score to retain only confident claims. We show\nin our experiments that our use of the entire claim set (vs claims associated with a single output,"}, {"title": "6 Experiments", "content": "In Sec. 6.1, we benchmark our proposed graph-based metrics and existing methods adapted for\nclaim-wise uncertainty on two long-form factuality datasets, demonstrating the effectiveness of\ncloseness centrality as a reliable uncertainty measure. In Sec. 6.2, we show that applying the\ncloseness centrality metric with our uncertainty-aware decoding framework demonstrates the best\ninformativeness-factuality trade-off for long-form generation and empirically analyze the impact\nof each component. Additionally, Sec. 6.3 presents an ablation study to investigate the factors\ncontributing to the performance of closeness centrality and provide insights for interpretation."}, {"title": "6.1 Uncertainty Estimation", "content": "In this subsection, we empirically analyze different graph centrality metrics for uncertainty estimation\nand systematically benchmark existing methods adapted for claim-wise uncertainty estimation.\nDatasets and annotation We evaluated the different uncertainty estimation methods on two chal-\nlenging datasets, FActScore [12] and (long-form) PopQA [22], where even the most capable LLMs\nlike GPT-4 [2] demonstrate frequent factuality failures. For each dataset, we randomly sampled 100\nentities and generated a set of claims about each entity with their uncertainty estimates using our\npipeline described in Sec. 4.1. This process yielded over 2000 claims on average for each evaluation\nsetting. We briefly describe each dataset and their annotation details here, and include additional\ndetails in Appx. A:\n\u2022 FActScore [12] is a widely used dataset for evaluating the factuality of long-form text generation for\nLLMs, containing entities sourced from Wikipedia. To assess the factuality of claims, we employed\na similar pipeline to the one in their paper, classifying them as True, False, or Subjective using\nLLMs conditioned on the corresponding Wikipedia article. We specifically used GPT-4-Turbo due\nto its low classification error rate.\n\u2022 Long-form PopQA [22] comprises of entities covering a diverse range of subjects. The original\nPopQA was not designed for long-form generation, we adapted it by adjusting the prompt to\n\"Provide me with a paragraph detailing some facts related to subject\".\nTo ensure data quality, we filtered out entities that either lacked a Wikipedia page or had pages\nshorter than 1500 characters. The factuality of claims was evaluated by GPT-4-Turbo using the\nassociated Wikipedia pages as reference, where longer Wikipedia pages were preferred as they\ntypically provide more comprehensive coverage of entity information, thus reducing the risk of\nfalse negative annotations.\nWe also evaluated different methods on the Natural Question dataset [38] and observed consistent\ngains. Due to a higher rate of false negatives in the auto-annotation pipeline compared to the\naforementioned datasets, we have included these results in Appx. C.1."}, {"title": "6.2 Uncertainty-Aware Decoding", "content": "In this subsection, we empirically demonstrate how our improved claim-level uncertainty estimates\ncontribute to a better tradeoff between precision (factuality) and recall (informativeness) of generated\nresponses within our UAD framework, and analyze the impact of each component.\nExperimental setup We tested UAD with our best closeness centrality uncertainty estimate Cc on\nthe FActScore dataset used in Sec. 6.1. We experimented with GPT-3.5-Turbo that were used for all\nsteps described in Fig. 1 and Fig. 2. We randomly sampled 180 entities and used |R| = 5 to construct\ncandidate claim set C. Additional details can be found in Appx. \u0412.2.\nEvaluated methods We benchmarked the performance of UAD against several inference-time decod-\ning methods. For a systematic comparison, we also included inference-time decoding methods that do"}, {"title": "6.3 Ablation Study", "content": "In this section, we aim to analyze why closeness centrality is effective for discriminating between\ntrue and false claims, and how the performance of this method changes as we increase the number of\nclaim nodes in the semantic bipartite graph. For all experiments in the ablation study, we used the\nFActScore dataset with the GPT-3.5-turbo model.\nWhy is the closeness centrality so effective? Closeness centrality is intrinsically related to the\ndistances between nodes in a graph. To understand its effectiveness, we analyze how the distances\nbetween a pair of claims correlate with the factuality of these claims. Specifically, in Fig. 4a, we\nvisualize the distances between true-true claim pairs, true-false pairs, and false-false pairs in the\nsemantic graph. We observe a clear shift in the distance distribution from true claims to false claims\nacross all settings. False claims generally exhibit larger distances to other claims in the semantic\nentailment graph. This observation can be intuitively interpreted as false claims being less centered,\ni.e., less entailed by generations and having lower co-occurrences with the majority of claims. This\ninsight provides a clear explanation for the strong performance of closeness centrality in claim-level\nuncertainty estimation.\nHow does the performance change with the number of responses? Fig. 4b explores how the\nperformance changes as we increase the number of responses RN used to construct the claim set. We\nfind that increasing the number of claim nodes consistently leads to improved performance (despite\nthe increased inference cost). This also indicates that the closeness centrality effectively leverages\nadditional graph information as more nodes are present in the semantic graph.\nIs our end-to-end decoding pipeline computational intensive? While our pipeline increases\nthe lower bound of computation by introducing the graph construction process, it attains Pareto\noptimality for the compute-quality tradeoff compared to existing methods across multiple quality\nmetrics. Details are present in Appx. D.2."}, {"title": "7 Conclusion", "content": "In this work, we propose Graph Uncertainty, a family of graph-based methods for claim-wise uncer-\ntainty estimation in LLM generations. We also present an uncertainty-aware decoding framework that\nintegrates these estimates to improve the trade-off between factuality and informativeness. Empirical\nresults demonstrate the effectiveness of the proposed approach.\nDespite these improvements, we note that the graph construction process increases inference-time\ncompute and latency. Moreover, our claim decomposition assumes that claims can be decontextualized\nand separated, which may not always be the case in real-world applications. Future work may aim to\noptimize the graph construction process to reduce computational overhead and develop techniques to\nhandle scenarios with dependent claims more effectively. These improvements will further improve\nthe applicability and robustness of our uncertainty estimation framework in complex settings."}, {"title": "A Data and Annotation Details", "content": "FActScore FActScore [12] is a widely used dataset for evaluating the factuality of long-form text\ngeneration, containing entities sourced from Wikipedia. We utilized the entities from this dataset and\napplied our pipeline, as discussed in Sec. 4, to generate, break down, and evaluate the uncertainty of\nclaims. To assess the factuality of sub-claims, we employed a similar pipeline, classifying them as\nTrue, False, or Subjective using GPT-4-Turbo, which was chosen for its low error rate. We used the\n'unlabelled' split of FActScore released dataset.\nLong-form PopQA Dataset We also incorporated the PopQA dataset [22], which comprises\nentities covering a diverse range of subjects. Although PopQA was not originally designed for\nlong-form generation, it contains challenging entities and provides links to corresponding Wikipedia\npages. We filtered the dataset based on the length of the Wikipedia pages and the quality of the results.\nThe factuality of claims was evaluated using the information from the associated Wikipedia pages,\nwith longer pages considered more reliable due to their comprehensive coverage of the entity.\nAnnotation Process For both datasets, we employed a two-stage annotation process. First, we used\nour pipeline to generate claims for each sampled entity. Second, we assessed the factuality of the\ngenerated claims using the following criteria:\n\u2022 True: The claim is factually accurate and supported by the information provided in the corre-\nsponding Wikipedia page.\n\u2022 False: The claim is factually incorrect or contradicts the information provided in the corresponding\nWikipedia page.\n\u2022 Subjective: The claim is subjective, opinion-based, or cannot be verified using the information\nprovided in the corresponding Wikipedia page.\nTo ensure the quality of the annotations, we utilized GPT-4-Turbo, a large language model known for\nits low error rate, to classify the claims into the three categories mentioned above. This automated\nannotation process allowed us to efficiently label a large number of claims while maintaining a high\nlevel of accuracy."}, {"title": "B Baselines Details", "content": "\u2022 Verbalized confidence (VC): As introduced in Sec. 3, this method involves prompting the LLM to\nexpress its confidence in a claim e directly. Here, we mainly consider two variants:\nPost-hoc verbalized confidence (PH-VC) [15]: This method elicits the verbalized confidence\nin a post-hoc manner after the entire claim set C has been decomposed from generations. We\nfollowed Tian et al. [15] to prompt an LLM to express its confidence about each claim $c \u2208 C$\ngiven multiple options such as \"Very good chance (80%)\\\u201d, \u201cLittle chance (20%)\" etc.\nIn-line verbalized confidence (IL-VC) [16]: This is a straightforward variant of PH-VC, which\ndirectly elicits the verbalized confidence about each claim e in an in-line manner right after it is\ndecomposed from the generations during the Step 2 in Sec. 4.1. Notably, in contrast to PH-VC\nwhich requires an extra stage of prompting for VC, this method adds negligible overhead to the\nclaim decomposition stage.\n\u2022 P(True) [8]: This method elicits the uncertainty estimate of a claim by prompting an LLM to\nanswer whether the claim is true or false, using the likelihood of being true as the confidence score.\nSince the likelihood scores are not available for most close-source black-box LLMs, we modified\nit to estimate the likelihoods by sampling multiple times from the LLMs.\n\u2022 Self-Consistency (SC) [17, 11]: As discussed in Sec. 3, this methods utilizes the consistency score\nof one claim across different samples from the same LLM.\n\u2022 Self-Consistency + PH-VC (SC + VC): Since there are often tied confidence scores in SC (i.e.,\nthe frequencies of being it entailed in the sampled generations), rendering them less informative\nfor distinguishing true and false claims at a granular level. We thus include a straightforward\nvariant of SC by integrating the PH-VC to break such ties, which serves as a strong baseline in our\nexperiments."}, {"title": "B.1 Uncertainty Estimation Experiments", "content": "This section aims to provide comprehensive details about the baseline methods used in Sec. 6.1. We\nwill delve into the specifics of each method, including their prompts and implementation, and any\npreprocessing or postprocessing steps applied. By offering this detailed explanation at these baselines,\nwe aim to ensure the reproducibility and transparency of our experimental setup.\nAssume that we have a claim c to estimate uncertainty and and an entity of the claim.\nPost-hoc Verbalized Confidence (PH-VC) This method elicits the verbalized confidence in a\npost-hoc manner after the entire claim set C has been decomposed from generations. We followed\nTian et al. [15] to prompt an LLM to express its confidence about the claim e given multiple options\nsuch as \u201cVery good chance (80%)\\\u201d, \u201cLittle chance (20%)\" etc. The specific prompt that we use is as\nfollowing:\nYou are provided with some possible information about a person.\nDescribe how likely it is that the possible answer is correct as one\nof the following expressions:\nNo chance (0%)\nLittle chance (20%)\nLess than even (40%)\nFairly possible (60%)\nVery good chance (80%)\nAlmost certain (100%)\nGive ONLY your confidence phrase, no other words or explanation. For\nexample:\nConfidence:  (description of confidence, without any extra commentary\nwhatsoever; just a short phrase!>\nThe entity is: {entity}\nThe possible information is: {claim}\nIn-line Verbalized Confidence (IL-VC) The In-line Verbalized Confidence (IL-VC) method differs\nfrom PH-VC in terms of prompting and integration with claim decomposition. Thus, we prompt\nlanguage model with a long-form generation and instructions to give all the claims with corresponding\nconfidence scores. The specific prompt that we adapted from [16] is as following:\nPlease deconstruct the following paragraph into the smallest possible\nstandalone self-contained facts without semantic repetition, and\nreturn the output as a jsonl, where each line is claim: [CLAIM],\ngpt-confidence: [CONF].\nThe confidence score [CONF) should represent your confidence in\nthe claim, where a 1 is obvious facts and results like 'The earth\nis round' and '1+1=2'. A 0 is for claims that are very obscure or\ndifficult for anyone to know, like the birthdays of non-notable people.\nThe input is:\n{long-form generation}\nP(True) The P(True) method estimates the uncertainty of a claim by prompting an LLM to answer\nwhether the claim is true or false and using the likelihood of being true as the confidence score. Since\nthe likelihood scores are not available for most closed-source black-box LLMs, we modified the\nmethod to estimate the likelihoods by prompting the model 10 times and frequency of answering\nTrue. The specific prompt that we adapted from [8] is as follows:\nThe following claim is about entity. Is the claim true or false?\n(Answer with only one word True/False)\nClaim: {claim}\""}, {"title": "Self-Consistency (SC)", "content": "The Self-Consistency (SC) method utilizes the consistency score of one\nclaim across different samples from the same LLM. As we discussed in Sec. 3, it is equivalent to the\ndegree of claim on the bipartite semantic entailment graph. The whole pipeline used to construct the\ngraph is described detailed in Sec. 5."}, {"title": "Self-Consistency + PH-VC (SC + VC)", "content": "The combination of SC and PH-VC is simply the average\nof the two scores."}, {"title": "Graph Metrics (Our Method)", "content": "All the graph metrics are calculated by calling corresponding\ncentrality function in networkx package."}, {"title": "B.2 Uncertainty-Aware Decoding", "content": "We also provide a detailed explanation of the uncertainty-aware decoding methods used in our\nexperiments.\nGreedy Decoding Greedy decoding is the most naive baseline that generates a response with a\ntemperature of t = 0 for a given input prompt x. This widely acknowledged method produces outputs\nwith high likelihood but does not incorporate any uncertainty estimates.\nCoVe [36] CoVe is an inference-time decoding method that improves the factuality of generations\nby self-verification without using any uncertainty estimates. We utilize the code released in this repo:\nhttps://github.com/ritun16/chain-of-verification.\nUAD Methods\nAs the pipeline of UAD described in Sec. 5 involves three steps: Generate claim pool, estimate\nuncertainty and filter, and merge the claims into a coherent long-form generation. We use the same\nprompt present in Appx. F for integrating all claims into one sample for all methods.\nThus, for each method below, I will talk about the other two steps in details:"}, {"title": "UAD (SC, Greedy)", "content": "As discussed in Sec. 5, this method corresponds to Conformal Factuality\nDecoding [16], which employs the SC uncertainty estimate to filter out high-uncertainty claims in the\nclaim set obtained from the greedily decoded response. The claim pool is obtained by breaking down\nthe greedy output Mt=0(x), utilizing the break down prompt in Appx. F, and we use SC detailed in\nAppx. B.1 as uncertainty estimation method."}, {"title": "UAD (SC, Multiple-Sample), UAD (IL-SC, Multiple-Sample), UAD (Cc, Multiple-Sample)", "content": "Similar to the UAD (SC, Greedy) setting, the claim pool is obtained by using R = 5 to construct\ncandidate claim set C as discussed in Sec. 4.1. Then, we use respectively use SC, CC, SC + IL-VC\ndetailed in Appx. B.1 as uncertainty estimation method."}, {"title": "B.3 Computing Resources", "content": "In this work, only experiments that are using Llama-3 involved computing resources. We use two\n80G A100 to run inference for Llama-3-70B-Instruct."}, {"title": "C Additional Results for Uncertainty Quantification", "content": "We present additional experimental results and analyses related to uncertainty estimation. These\nexperiments complement the main experiments discussed in Sec. 6.1 and provide further insights into\nthe performance of different uncertainty estimation methods."}, {"title": "C.1 Natural Question Dataset", "content": "To further evaluate the generalizability of our claim-level uncertainty estimation method, we expanded\nour experiments to include the Natural Questions dataset [38]. Results are presented in Table 3.\nOur findings show that the closeness centrality method continues to outperform baseline approaches\nin most scenarios, aligning with trends observed in our primary datasets. However, we noted a higher\nrate of false negatives in the auto-annotation process for this dataset compared to others. While"}, {"title": "C.2 Statistical Significance Check", "content": "To rigorously evaluate the performance differences between our proposed uncertainty estimation\nmethod and baseline methods, we conduct a statistical significance check focusing on the two metrics\nthat is reported in Sec. 6.1, AUROC and AUPRC-Negative.\nWe use bootstrapping to generate multiple samples from the original dataset by resampling with\nreplacement. For each bootstrap sample, we calculate the each metric for both the proposed and\nbaseline methods. This results in a distribution of the metric values for each method.\nOur goal is to validate the effectiveness of our best proposed method. To compare these distributions,\nwe employ the Wilcoxon signed-rank test, a non-parametric test suitable for paired samples. A statis-\ntically significant result, indicated by a p-value less than 0.05, would confirm that the performance\nimprovement of our method is meaningful and consistent.\nAfter the significance check, all p-values for the three models (GPT-3.5-turbo, GPT-4, and Llama-\n3-70B-instruct) and two datasets (FactScore and PopQA) are significantly smaller than 0.05 when\ncomparing the proposed method against the baseline methods."}, {"title": "C.3 AUROC Plots", "content": "To provide a visual comparison of the performance of different uncertainty estimation methods, we\npresent AUROC plots for selected experimental settings. These plots illustrate how our proposed\nmethod outperforms the baselines in distinguishing between true and false claims.\nFigure 5a shows the AUROC curves for the GPT-3.5 model with |R| = 10 on the FActScore dataset.\nWe compare our best-performing graph-based uncertainty estimation method using closeness central-\nity (CC) with the baselines, including post-hoc verbalized confidence (PH-VC), self-consistency (SC),\nand self-consistency combined with verbalized confidence (SC + VC). The plot clearly demonstrates\nthat our method achieves a higher AUROC than all the baselines (and all points are higher), indicating\nit is more capable to identify false claims.\nFigure 5b presents the AUROC curves for the GPT-4 model with |R| = 10 on the PopQA dataset.\nAgain, we observe that our method using closeness centrality (CC) achieves a higher overall AUROC\ncompared to the baselines. Interestingly, while closeness centrality consistently outperforms the SC\nbaseline, the SC + VC method exhibits better performance in the left region of the plot, where the\nFalse Positive Rate (FPR) is low. This finding further validates the observation that incorporating VC\ncan potentially enhance the performance of graph-based methods in certain scenarios."}, {"title": "D Additional Results for Uncertainty-Aware Decoding", "content": "D.1 Additional Results on the Plot\nIn this section, we present additional results for the Uncertainty-Aware Decoding (UAD) experiments,\nwhere we include the variant using PH-VC (Post-Hoc Verbalized Confidence) to break ties for the"}, {"title": "E Notation Definition", "content": "In this section, we provide the definitions of the notations used in Sec. 4.2 for clarity and completeness.\nLet G = (V, A) denote a graph, where V is the set of nodes and A is the adjacency matrix. The\nelements of the adjacency matrix $A_{vu}$ indicate the presence of an edge between nodes v and u.\nThe following notations are used in the formulas for the graph centrality metrics in Table 1:\n\u2022 v: A node in the graph G.\n\u2022 u: Another node in the graph G.\n\u2022 s, t: Source and target nodes, respectively, when considering shortest paths.\n\u2022 $\u03c3_{st}$: The number of shortest paths between nodes s and t.\n\u2022 $\u03c3_{st}(v)$: The number of shortest paths between nodes s and t that pass through node v.\n\u2022 \u039d(v): The set of neighboring nodes of node v."}, {"title": "F Prompt Details", "content": "In this section, we provide the specific prompts used in our methodology (Sec. 4, Sec. 5) for\nconstructing the semantic entailment graph and performing uncertainty-aware decoding. These\nprompts are designed to elicit the desired information and behavior from the language model."}, {"title": "F.1 Claim Decomposition Prompt", "content": "Given a LM response, the claim decomposition prompt is used to break down the generated response\ninto a set of individual claims. The prompt is the same as the In-Line VC prompt because it\ndecomposes and elicit confidence at the same time. The prompt is as follows:\nPlease deconstruct the following paragraph into the smallest possible\nstandalone self-contained facts without semantic repetition, and\nreturn the output as a jsonl, where each line is claim: [CLAIM],\ngpt-confidence: [CONF].\nThe confidence score [CONF) should represent your confidence in\nthe claim, where a 1 is obvious facts and results like 'The earth\nis round' and '1+1=2'. A 0 is for claims that are very obscure or\ndifficult for anyone to know, like the birthdays of non-notable people.\nThe input is:\n{long-form generation}"}, {"title": "F.2 Claim Merging Prompt", "content": "Given two sets of claims, the claim merging prompt is used to combine two sets of claims into a\nsingle set which is a union set, ensuring that only unique and semantically distinct claims are retained.\nThe prompt is as follows:\nGiven two lists titled \"Original Claim List\u201d and \u201cNew Claim List\",\nyour task is to integrate information from the \u201cNew Claim List\u201d into\nthe \u201cOriginal Claim List\u201d. Please follow these detailed steps to\nensure accuracy and clarity in the process:\nTask 1. *Verification Process:* Your goal is to go through each\nstatement in the \u201cNew Claim List\u201d one by one, and determine if it is\nfully entailed or mentioned by any statement in the \u201cOriginal Claim\nList.\u201d\nTask 2. *Compilation of Non-Entailed Claims:* Generate a list of\nstatements from the \u201cNew Claim List\u201d that are not already covered or\nimplied by the \u201cOriginal Claim List\u201d. For each new or unique claim\nthat does not have an equivalent in the original list, format your\noutput by starting each line with a dash ('-').\n*Original Claim List:*\n{claim_list1}\n*New Claim List:*\n{claim_list2}"}, {"title": "F.3 Edge Construction Prompt", "content": "The edge construction prompt is used to determine whether a generated response entails a specific\nclaim, thereby establishing an edge between the response node and the claim node in the bipartite\ngraph. We adapt the prompt from [11]. The prompt is as follows:\nContext: {generation}\nClaim: {claim}\nIs the claim supported by the context above?\nAnswer Yes or No:\nThis prompt is applied to each pair of response r \u2208 R and claim c \u2208 C to construct the edge set & of\nthe bipartite graph."}, {"title": "F.4 Claim Integration Prompt", "content": "The claim integration prompt is used to synthesize the selected low-uncertainty claims into a single,\ncoherent output during the uncertainty-aware decoding process, as mentioned in Sec. 5. The prompt\nis as follows:\nTask: You are provided with a list of facts about prompt. Your\ngoal is to synthesize these facts into a coherent paragraph. Use\nall the provided facts where possible, ensuring that no fact is\nmisrepresented or overlooked. If there are redundant facts, choose\nthe most comprehensive one for inclusion. The length of the paragraph\nshould naturally reflect the number of provided facts-shorter for\nfewer facts and longer for more. Avoid unnecessary filler and focus\non presenting the information clearly and concisely.\nThe facts: {claim_list}\nThis prompt is applied to the operational subset of claims C\u00ba obtained after filtering based on the\nuncertainty threshold 8 to generate the final output M(C\u00ba).\nThese prompts play a crucial role in the construction of the semantic entailment graph and the\nimplementation of uncertainty-aware decoding. By using these prompts, we can effectively leverage\nthe language model's capabilities to extract claims, establish relationships between responses and\nclaims, and generate coherent outputs based on the selected low-uncertainty claims."}]}