{"title": "Crabs: Consuming Resrouce via Auto-generation\nfor LLM-DoS Attack under Black-box Settings", "authors": ["Yuanhe Zhang", "Zhenhong Zhou", "Wei Zhang", "Xinyue Wang", "Xiaojun Jia", "Yang Liu", "Sen Su"], "abstract": "Large Language Models (LLMs) have\ndemonstrated remarkable performance\nacross diverse tasks. LLMs continue to be\nvulnerable to external threats, particularly\nDenial-of-Service (DoS) attacks. Specif-\nically, LLM-DoS attacks aim to exhaust\ncomputational resources and block services.\nHowever, prior works tend to focus on\nperforming white-box attacks, overlooking\nblack-box settings. In this work, we pro-\npose an automated algorithm designed for\nblack-box LLMs, called Auto-Generation\nfor LLM-DoS Attack (AutoDoS). Auto-\nDoS introduces DoS Attack Tree and\noptimizes the prompt node coverage to en-\nhance effectiveness under black-box con-\nditions. Our method can bypass exist-\ning defense with enhanced stealthiness via\nsemantic improvement of prompt nodes.\nFurthermore, we reveal that implanting\nLength Trojan in Basic DoS Prompt aids\nin achieving higher attack efficacy. Experi-\nmental results show that AutoDoS ampli-\nfies service response latency by over 250\n\u00d7 \u2191, leading to severe resource consump-\ntion in terms of GPU utilization and mem-\nory usage. Our code is available at https:\n//github.com/shuita2333/AutoDoS.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have been\nincreasingly adopted across various domains\n(Chen et al., 2022; Zhao et al., 2023; Achiam\net al., 2023; Chang et al., 2024). LLM applica-\ntions lack robust security measures to defend\nagainst external threats, particularly Large\nLanguage Model Denial of Service (LLM-DoS)\nattacks (Geiping et al., 2024; Gao et al., 2024b).\nIn Cybersecurity, DoS attacks exploit target\nresources, aiming to deplete computational ca-\npacity and disrupt services (Long and Thomas,\n2001; Bogdanoski et al., 2013) and LLM-DOS\noperates in the same way. Recent studies re-\nveal that LLM-DoS can effectively disrupt the\nservice of LLM applications (Geiping et al.,\n2024; Gao et al., 2024b). While LLMs ensure\nsafety by aligning with human values (Ouyang\net al., 2022; Bai et al., 2022a), the inability\nof models to recover from resource exhaustion\npresents significant challenges in mitigating its\nvulnerability to LLM-DoS attacks.\nExisting LLM-DoS attack approaches in-\nclude increasing the latency by extending\nthe model's output length, and making high-\nfrequency requests to exhaust application re-\nsources (Shumailov et al., 2021; Gao et al.,\n2024a). GCG-based algorithm (Geiping et al.,\n2024) and data poisoning (Gao et al., 2024b)\ncan lead to lengthy text outputs. Prompt engi-\nneering induction also compels models to pro-\nduce repetitive generations (Nasr et al., 2023).\nHowever, these methods struggle to work in\nblack-box because they typically rely on access\nto model weights or modifications to training\ndata and are prone to being blocked by filters\n(Jain et al., 2023; Alon and Kamfonas, 2023).\nAs a result, current research on LLM-DoS is\nstill critically flawed, remaining a significant\nchallenge under black-box conditions.\nIn this paper, we focus on effective LLM-\nDoS attacks under black-box settings. We pro-\npose Auto-Generation for LLM-DoS Attack\n(AutoDoS), an automated algorithm tailored\nfor black-box LLMs. Specifically, AutoDoS be-\ngins by introducing the DoS Attack Tree\nfor fine-grained prompt construction. We ex-\npand the tree using Depth Backtracking and\nBreadth Extension to induce the model to gen-\nerate redundant responses, thereby extending\ninference latency. Then AutoDoS iteratively\noptimizes the prompt node coverage for better\nrobustness and stealthiness to deceive secu-"}, {"title": "2 Related work", "content": "LLM safety. The increasing capabilities of\nLLMs have amplified concerns about their po-\ntential misuse and the associated risks of harm\n(Gehman et al., 2020; Bommasani et al., 2021;\nSolaiman and Dennison, 2021; Welbl et al.,\n2021; Kreps et al., 2022; Goldstein et al., 2023).\nTo mitigate the risks, alignment has been de-\nveloped to identify and reject harmful requests\n(Bai et al., 2022a,b; Ouyang et al., 2022; Dai\net al., 2023). Based on this, input-level fil-\nters analyze the semantic structure of prompts\nto prevent attacks capable of bypassing safety\nalignments (Jain et al., 2023; Alon and Kam-\nfonas, 2023; Liao and Sun, 2024). These de-\nfenses significantly weaken the existing attacks\nand prevent LLM from being abused.\nLLM-DoS attacks on LLM applications.\nLLM applications are increasingly exposed to\nexternal security threats, particularly LLM-\nDoS attacks. For instance, Ponge Examples\nprevent model optimization, leading to in-\ncreased resource consumption and latency dur-"}, {"title": "3 Method: Auto-Generation for\nLLM-DoS Attack", "content": "Existing LLM-DoS attack methods are usually\ndesigned for white-box, making them less effec-\ntive in black-box settings. Additionally, current\nmethods struggle to evade security detection,\nas reliance on semantic patterns. To address\nthese limitations, we introduce AutoDoS, a\nnovel LLM-DoS attack algorithm tailored for\nblack-box models. AutoDoS effectively max-\nimizes resource consumption while maintain-\ning a high level of stealth, making its attack\nprompts challenging to detect. We use the Ba-\nsic DoS Prompt to refine the granularity of\nthe Initial DoS Prompt and employ the As-\nsist Prompt to enhance attack effectiveness.\nThe remainder of this section details Auto-\nDoS. In Sec. 3.1 we outline the construction\nof the DoS Attack Tree to induce the model\nto generate redundant responses. Sec. 3.2 de-\nscribes the iterative optimization process for\nthe DoS Attack Tree, which enhances attack\nsuccess rates and strengthens the concealment\nof Basic DoS Prompt. In Sec. 3.3 we intro-\nduce the Length Trojan, a technique designed\nto enhance the cross-model transferability."}, {"title": "3.1\nConstruct Basic DoS Prompt\nthrough DoS Attack Tree", "content": "To craft structured Basic DoS Prompt, we\nintroduce a novel approach called DoS Attack\nTree, which enables targeted manipulation of\nlanguage models to extend generated content\nand amplify resource consumption.\nAutoDoS employs a dynamic tree structure,\nwith the root node representing the Initial DoS\nPrompt typically a concise yet comprehen-\nsive query. We iteratively expand the tree\nthrough Depth Backtracking and Breadth\nExtension. By leveraging descendant nodes\nto represent the decomposed components of\nthe root node, we decompose the Initial DoS\nPrompt into fine-grained sub-prompts. Our\napproach generates rich semantic outputs and\nintroduces additional computational overhead.\nPreliminary. We formalize the structure of\nthe Initial DoS Prompt as a tree, denoted\nas $T = (N, E)$, where the node set $N =\n{n_1, n_2,..., n_i}$ represents the potential expan-\nsion space of the Initial DoS Prompt, with $i$\nbeing the total number of nodes in $T$. The\nedge set $E$ encodes the inclusion relation-\nships between the expansion contents. The\nleaf node $L = {l_i \u2208 N | l_i \\text{has no children}}$\ncorresponds to the fine-grained, predictable\ncontent of Initial DoS Prompt.\nWe define\na root path $P = {r, n_{a_1}, n_{a_2},..., v}$ as a se-\nquence of nodes in the tree, from the root\nnode $r$ to the target node $v \u2208 N$. The term\n$L(P) = {l_i | l_i \\text{is descendant of } P[-1]}$ is re-\nferred to as the coverage of $P$, where $P[-1]$\ndenotes the last node in the path $P$.\nDeep Backtracking. We generate $K$ nodes,\nwhere $K$ represents the required number of\ndescendants of $T$, denoted as leaf nodes $l_i$\n$(i \u2208 [1, K])$. Since the Initial DoS Prompt $r$\nhas higher complexity, more intermediate nodes\ncan be identified through Deep Backtracking\nbetween each $l_i$ and $r$, which has a granularity\nbetween $l_i$ and $r$. During this process, DoS\nAttack Tree is expanded, and the expansion\npath is recorded as $P_i = {r, n_{a_1}, n_{a_2},..., l_i}$.\nTo ensure structural consistency and path in-\ndependence, we use Tarjan's Offline algorithm\n(Tarjan, 1972) to identify the Lowest Common\nAncestor (LCA) $n_{ac}$ for any two overlapping\npaths $P_i$ and $P_j$, where $c \u2208 [1,8)$.\nIf $n_{ac} \u2260r$, it indicates that the two\npaths share a common subpath, $P_i \u2229 P_j =\n{r, n_{a_1}, n_{a_2},..., n_{ac}}$. To ensure independence\nin the coverage of sub-prompts, we retain only\nthe direct child nodes of $n_{ac}$ and prune all\ndescendant nodes. This pruning restricts the\npaths to the following form:\n$P = {r, n_{a_1}, n_{a_2},..., f(l_i)}$, (1)\nwhere $f(l_i)$ either maps to $l_i$ itself or to an an-\ncestor of $l_i$, and all $f(l_i)$ are unique children\nof node $n_{ac}$. This ensures $f(l_i)$ and $f(l_i)$ corre-\nspond to independent DoS sub-prompts.\nThe final coverage for Deep Backtracking"}, {"title": "Algorithm 1 Iterative optimization process\nof Tree DoS", "content": "the DoS Attack Tree, refining the Assist\nPrompt through collaborative interactions be-\ntween three key components: the Attack model,\nthe Target model, and the Judge model.\nTo prevent a decrease in attack effectiveness\nacross $T_i$ traversals, the Attack model gener-\nates an optimized Assist Prompt $P_a$. This pro-\ncess standardizes the subtree structure $T_i$ from\nprior iterations to ensure clarity and precision.\nGiven $P_a$ and $T' = \\Sigma T_i$, the Target\nmodel simulates its response generation in prac-\ntical application scenarios, producing a reply:\n$T_o \u2190 T(P_a + T')$, (5)\nWhere $T(\u00b7)$ denotes the target model function.\nThe Judge model then evaluates $T_o$ by ex-\ntracting key information and compressing it\ninto feedback $S_f$. This feedback assesses\nwhether the Target model sufficiently addresses\nall potential problem nodes $P_a$.\nThe iterative loop enhances the interaction\namong the models, improving the effectiveness\nof prompt generation over successive iterations,\nby setting the attack success rate $R_a$ as the\noptimization objective. The iterative optimiza-"}, {"title": "Summary Feedback Compression.", "content": "In\neach iteration, the Judge model extracts key in-\nformation from the generated response $T(t)$ of\ntarget model and compresses it into feedback\n$S_f^{(t)}$ to guide the optimization of the Assist\nPrompt. This operation is formalized as a com-\npression function, which aims to maximize the\nretention of relevant information:\n$S_f^{(t)} = argmax_S [Rel(T^{(t)}, S) - \u03bb\u00b7|S|]$, (6)\nwhere $Rel(T^{(t)}, S)$ quantifies the semantic rele-\nvance between the feedback S and the response\n$T^{(t)}$. $|S|$ measures the length of the feedback,\nincorporating the trade-off factor $\u03bb$ that con-\ntrols the degree of compression."}, {"title": "Success Rate Optimization.", "content": "Our goal is to\noptimize the success rate $R_a$, which measures\nthe ability of target model to reply to all poten-\ntial leaf nodes $L(T_i)$, for a given problem. We\ndefine $R_a$ as the degree of alignment between\nthe generated output and the target leaf node\nset, and use the success rate function $f_S(P_a)$\nto summarize the actual operation process:\n$max_{P_a} R_a = max_{P_a}  \\frac{\\sum_{i=1}^{K} |L(T_i) \u2229 L(T_o)|}{\\sum_{i=1}^{K} |L(T_i)|} = f_S(P_a)$, (7)\nwhere $L(T_o)$ represents the leaf nodes of the\nBasic DoS Prompt that correspond to the tar-\nget output $T_o$. N represents the number of\npaths retained during depth expansion.\nTo iteratively optimize $R_a$, we update the\nAssist Prompt $P_a$ in a gradient-based manner.\nAt the t-th iteration, we analyze the previous\nAssist Prompt $P_a^{(t)}$ and use feedback $S_f^{(t)}$ to\noptimize it. The prompt is updated as follows:\n$P_a^{(t+1)} = dec(emb(P_a^{(t)}) + \u03b7\u2207f_S(P_a^{(t)}))$, (8)\nwhere $emb(P_a)$ maps $P_a$ into a high dimen-\nsional space for optimization; \u03b7 is the learning\nrate, controlling the step size of the update;\n$\u2207f_S(P_a^{(t)})$ estimates the gradient of the success\nrate $R_a$, indicating the direction of optimiza-\ntion. The function dec(\u00b7) decodes the high di-\nmensional vector, converting calculation result\ninto the corresponding textual content.\nThe iterative process refines $P_a$ to enhance\nthe model's ability to generate outputs that"}, {"title": "3.3 Length Trojan strategy", "content": "Most large language models struggle to fully\nutilize the maximum length of their output\nwindow during content generation (Li et al.,\n2024). We propose the Length Trojan strategy,\nwhich wraps the Basic DoS Prompt to enforce\nstrict adherence to a predetermined output for-\nmat. This approach ensures the target model\nis attacked successfully in a structured manner\nwhile improving the reproducibility and trans-\nferability of the attack across different models.\nThe Length Trojan has two key sections:\n\u2022 Trojan Section: A concise word count\nrequirement is embedded into the Assist\nPrompt. This word count acts as a guide-\nline for the model's internal security mech-\nanisms, signalling a safe and reasonable to-\ntal length for the generated content. This\nprevents triggering restrictive behaviors\ndesigned to block very long generations.\n\u2022 Attack Section: We design the Assist\nPrompt to explicitly instruct the target\nmodel to answer each sub-question in de-\ntail. By setting stringent task require-\nments, we guide the model to disregard\ntoken constraints and produce extended\noutputs, causing the response length to\nexceed the limit specified in the trojan.\nThe Length Trojan enables AutoDoS to\nachieve robust cross-model attack performance,\nfurther enhancing its effectiveness and adapt-\nability across diverse model environments.\nComprehensive empirical validation of Length\nTrojan is presented in Appendix B."}, {"title": "4 Experiments", "content": "We conducted experiments\nacross 11 models from 6 LLM families, in-\ncluding GPT-40, Llama, Qwen2.5, Deepseek,\nGemma, and Ministral series. All models use\n128K context except the Gemma series (8K).\nAttack LLMs. We conducted experiments\non models with a 128K context window, with\na particular focus on the widely used GPT-40\nfor more comprehensive testing.\nDatasets. In the experiments, we utilized\neight datasets to evaluate both the baseline\nperformance and the effectiveness of the at-\ntacks. These datasets include Chatdoctor (Li\net al., 2023), MMLU (Hendrycks et al., 2021),\nHellaswag (Zellers et al., 2019), Codexglue (Lu\net al., 2021) and GSM (Cobbe et al., 2021). \u0412\u0435-\nsides, we introduce three evaluation datasets,\nincluding RepTest, CodTest, and ReqTest. De-\ntails are given in Appendix D.1. We randomly\nselect 50 samples from each dataset and record\nthe average output length and response time.\nBaseline. We tested P-DoS attack (Gao\net al., 2024b) (Repeat, Count, Recursion, Code,\nLongTest) on GPT-40-mini, Ministral-8B, and\nQwen2.5-14B to assess resource impact. We\nalso tested other models in a black-box envi-\nronment, as detailed in Appendix C.3."}, {"title": "4.2 Effectiveness of AutoDoS", "content": "We compared AutoDoS with benign queries to\nevaluate its effectiveness and applicability. Our\nmethod performs well in terms of performance\nconsumption compared to benign queries, as"}, {"title": "4.3\nImpact on Resource Consumption", "content": "We tested AutoDoS impact using a server, sim-\nulating high-concurrency scenarios across dif-\nferent models under various DoS attack loads."}, {"title": "4.3.2 Impact on Service Performance", "content": "We evaluated the ability of a server to pro-\ncess user access requests from a performance\nperspective. As demonstrated in Tab. 2,\nserver throughput decreased from 1 request\nper minute under normal conditions to 0.009\u2193\nrequests per minute during AutoDoS. Server\nparallel processing capacity is limited to pre-\nvent GPU memory exhaustion, with normal\nuser waiting time comprising 12.0% of total\naccess time. In contrast, under AutoDoS, this\nproportion increases dramatically to 42.4%\u2191,\nwith total access times rising from 15.4 \u2192\n277.2 seconds. Ultimately, the overall system\nperformance degradation reaches an astonish-\ning 25,139.31%\u2191. Results confirm that Auto-\nDoS substantially degrade service accessibility,\nmaximizing system disruption impact."}, {"title": "4.4 Advanced Analysis of AutoDoS", "content": "We tested AutoDoS transferability across mod-\nels through output-switching (Tab. 3) and it-\nerative optimization (Tab. 4). In the cross-\nmodel attack experiment, AutoDoS successfully\npushed 90% of the target model close to their\nperformance ceilings. Additionally, we assessed\nthe transferability of the attack framework by\nreplacing the original attack module with the\ntarget model itself. The results from this re-\nplacement were consistent with the attack out-\ncomes based on GPT-40, with all experimental\nmodels reaching their performance ceil-\nings. This further confirms the robustness of\nthe AutoDoS method across different models."}, {"title": "4.4.2\nStealthiness of AutoDoS", "content": "We designed defense experiments from three\nperspectives: input detection, output self-\nmonitoring, and text similarity analysis. The\nexperimental results show that existing meth-\nods struggle to detect AutoDoS.\nInput Detection. We adopted the PPL\nmethod (Jain et al., 2023) for analysis. The\nexperimental results, as shown in Fig. 4b, the\nAutoDoS score is significantly higher than the\nbaseline of 0.41, indicating that Basic DoS\nPrompt and Assist Prompt exhibit high diver-"}, {"title": "5 Conclusion", "content": "We introduce Auto-Generation for LLM-DOS\nAttack (AutoDoS) to degrade service perfor-\nmance. AutoDoS constructs and iteratively\noptimizes the DoS Attack Tree to generate fine-\ngrained prompts, and incorporates the Length\nTrojan to enhance Basic DoS Prompt. We eval-\nuate AutoDoS on 11 different models, demon-\nstrating the effectiveness by comparing base-\nline methods. Through server simulation, we\nconfirm that AutoDoS significantly impacts ser-\nvice performance. Cross-experimental results\nfurther validated the transferability across dif-\nferent black-box LLMs. Besides, we show that\nAutoDoS is difficult to detect through existing\nsecurity measures, thus confirming its practical-"}, {"title": "6 Limitation", "content": "In this study, we focus on the LLM-DoS at-\ntacks targeting black-box model applications\nthrough the development of the AutoDoS al-\ngorithm. However, several limitations remain.\nWhile we demonstrate AutoDoS' performance\nacross a range of models, we do not fully ex-\nplore the underlying reasons for its varying\nsuccess across different model architectures.\nSpecifically, we do not investigate why certain\nmodels exhibit higher or lower efficiency with\nthe algorithm. Future work could examine\nhow architectural choices and data character-\nistics influence AutoDoS' behavior, providing\na deeper understanding of its capabilities and\nlimitations. Additionally, the potential impact\nof defense mechanisms against AutoDoS in real-\nworld applications is not considered here, which\nrepresents another promising direction for fu-\nture research. Currently, there is no clear de-\nfense against LLM-DoS attacks, raising con-\ncerns that our methods could be exploited for\nmalicious purposes."}, {"title": "A Ablation Analysis", "content": "We conduct ablation experiments by sequen-\ntially removing the three main components to\nevaluate their impact on the attack prompts.\nThe results, presented in Fig. 5, highlight the\ncritical role of each module in maintaining at-\ntack stability and generation performance.\nFirst, the results show that removing the\nDoS Attack Tree structure significantly reduces\nthe detail and semantic richness of the model's\nresponses, leading to a five-fold decrease in\nattack effectiveness. The DoS Attack Tree en-\nhances the completeness of model outputs by\nperforming fine-grained optimization on the\nInitial DoS Prompt.\nSecond, removing the iterative optimization\nof the tree causes instability in the answer\nlength, with average resource consumption\ndropping below that of the AutoDoS method,\nleading to a performance loss ranging from\n30% to 90%\u2193. Illustrates the role of iterative\noptimization in stabilizing the effectiveness of\nattack.\nFinally, when the Length Trojan was modi-\nfied and tested with 100-token and 1600-token\nintervals, the results in Fig. 6 varied across\ndifferent models, with a notable output length\ngap of 16,384 \u2192 10\u2193 tokens. Highlights the\ncritical role of the Length Trojan in maintain-\ning attack stability and optimizing resource\nconsumption.\nAblation Analysis conclusively demonstrates\nthe necessity of the synergistic operation of the\nthree main modules in the AutoDoS method."}, {"title": "B Verification of the Length Trojan\nMethod", "content": "This section presents further experimental evi-\ndence supporting the length deception method\ndiscussed in Sec. 3.2."}, {"title": "B.1 Methodology for Implementing\nthe Length Trojan", "content": "The Length Trojan incorporates a specific struc-\nture within the Assist Prompt to guide the\nLLMs into generating an excessively long out-\nput, while circumventing its security mecha-\nnisms. This approach consists of two key steps,\ncorresponding to the \"Trojan\" and \"Attack\"\ncomponents, respectively:\n\"Trojan\" Settings. The Assist Prompt $P_a$ is\nmodified to minimize the output length restric-\ntions imposed by the model's security mecha-\nnisms. Specifically, $P_a$ sets a shorter target\nlength $L_o$ for the generated output, which\nserves as a guide for the model. The complete\ninput prompt can then be expressed as:\n$S_a = P_a + Q$, (9)\nAt this stage, the LLM estimates the output\nlength based on the word count requirement $L_o$\nprovided in $P_a$. The estimated output length\n$\\hat{L}$ is calculated as:\n$\\hat{L} = f_L(S_a)$, (10)\nwhere $f_L$ represents the model's length esti-\nmation function. If $\\hat{L} < L_{safe}$ (the threshold\nset by the model's security mechanism), the\nsecurity detection is bypassed, allowing the\ngeneration to proceed without triggering any\nsecurity constraints.\n\"Attack\" Settings. While the auxiliary\nprompt reduces the estimated word count re-\nquirement, the generative language model is\nmore likely to prioritize task-specific instruc-\ntions over the length constraint when gener-\nating content. To address this, we further\naugment $P_a$ by incorporating detailed instruc-\ntions that emphasize the comprehensiveness\nand depth of the generated output. During\nthe generation phase, the model produces the\noutput $O$ based on the input $S_a$, as follows:\n$O = f_g(S_a)$, (11)\nwhere $f_g$ is the model's generation function.\nDue to the emphasis on generating detailed\nresponses, the model tends to overlook the\nlength requirement and produces an output\nlength $L_o$ that significantly exceeds the target\nlength $L_o$:\n$L_o \\gg L_o$ (12)"}, {"title": "B.2 Results of Comparison and\nVerification", "content": "To evaluate the effectiveness of the Length Tro-\njan method, we conducted multiple rounds of\nexperiments across 11 mainstream LLMs from\n6 different model families, focusing on ana-\nlyzing how varying length constraints impact"}, {"title": "C Supplementary Analysis on\nComparative Evaluation of\nAutoDoS and Alternative Attack\nMethods", "content": ""}, {"title": "C.1 Comparative Analysis of the\nIterative Optimization Process\nand the PAIR Method", "content": "Although both AutoDoS and PAIR methods\nemploy iterative approaches for attacks, there\nis a fundamental difference in algorithms. The\nPAIR algorithm requires a well-defined attack\ntarget and uses adversarial optimization along\nwith a judge model to evaluate the success of\nthe attack. In contrast, our method focuses\non optimizing the DoS Attack Tree structure"}, {"title": "C.2 Comparative Evaluation of\nAutoDoS and PAIR", "content": "To evaluate the performance of both meth-\nods, we adjusted the target of PAIR and con-\nducted comparative tests with AutoDoS, focus-\ning on the improvement of LLM output length.\nAs shown in Fig. 7, when using the PAIR\nmethod for iterative generation, the output\nlength only increases marginally compared to\nordinary queries, which limits its effectiveness\nin DoS attack scenarios. In contrast, Auto-\nDoS significantly extends the output length\nthrough incremental decomposition and refine-\nment strategies, leading to outputs that far\nexceed those generated by PAIR. This perfor-\nmance gap highlights the fundamental differ-\nences between AutoDoS and PAIR, demon-\nstrating that AutoDoS is not simply a direct\nadaptation of the PAIR method but a distinct\napproach to optimizing resource consumption\nin DoS attack scenarios."}, {"title": "C.3 Black-box Evaluation of P-DoS", "content": "We evaluated the performance extension of the\nP-DoS attack in a black-box environment, us-\ning the output length of LLMs as the evaluation\nmetric. The experimental results are shown in\nTab. 8, where the attack failed to reach the\noutput limit, particularly for the GPT fam-\nily model with its 16K output window. With\nthe exception of the Gemma series, which has\na 4K output window, all other models were\nconstrained by an 8K output window limit.\nDue to performance limitations, the model\nstruggles to meet the output upper limit re-\nquirements for standard access requests. This\nlimitation becomes particularly evident in our\nexperiments, as demonstrated in Fig. 2. The\nP-DoS method approaches this issue from dif-\nferent perspectives such as data suppliers, using\nlong text data to fine-tune the model's training\ndata. In a white-box environment, this fine-\ntuned malicious data helps extend the model's\nresponse length. However, this approach faces\nchallenges when adapted to a black-box envi-\nronment, as the model's internal parameters\ncannot be modified, making P-DoS difficult to\ngenerate effective long text content by attack\nprompts.\nWe also compared AutoDoS with the P-\nDoS in black-box. The experimental results in\nTab. 9 demonstrate that both AutoDoS and P-\nDoS successfully trigger the output window\nlimit of target models, with minimal differ-\nences in time performance, indicating similar\nattack efficiency. While P-DoS matches Auto-\nDoS in white-box attacks, AutoDoS achieves\nsimilar results in black-box settings, making it\nmore practical."}, {"title": "D Supplement to the Experiment", "content": ""}, {"title": "D.1\nSupplement to the Experimental\nSetups", "content": "Target LLMS. To demonstrate the appli-\ncability and transferability of our method, we\nconducted experiments on six different LLM\nfamilies, totaling 11 distinct models. All\nthe attacked LLM models will be listed be-\nlow. First, we provide the abbreviations used\nin the experimental records, followed by the\ncorresponding model versions:GPT40 (GPT-\n40-2024-08-06 (Hurst et al., 2024)), GPT40-\nmini (GPT-40-mini-2024-07-18 (Hurst et al.,\n2024)), Llama8B (Llama3.1-8B-instruct (Pat-\nterson et al., 2022)), Qwen7B (Qwen2.5-\n7B-instruct (Yang et al., 2024)), Qwen14B\n(Qwen2.5-14B-instruct (Yang et al., 2024)),\nQwen32B (Qwen2.5-32b-instruct (Hui et al.,\n2024)), Qwen72B (Qwen2.5-72b-instruct (Yang\net al., 2024)), Deepseek (Deepseek-V2.5 (Liu\net al., 2024)), Gemma9B (Gemma-2-9B-it\n(Zhong et al., 2023)), Gemma27B (Gemma-\n27B-it (Zhong et al., 2023)), and Ministral8B\n(Ministral-8B-Instruct-2410). With the excep-\ntion of the Gemma series, which uses an 8K\ncontext window, all other models use a 128K\ncontext version. The output window sizes are\nset as follows: GPT series to 16K, Gemma se-\nries to 4K, and all remaining models to 8K. For\nall models, the temperature parameter (T) is\nset to 0.5. Public APIs are used to conduct the\nexperiments, ensuring cost-effectiveness while\nvalidating the feasibility of the black-box at-\ntacks.\nAttack LLMS. The primary attack model\nutilized in our experiments is GPT40, which\ndemonstrates superior performance compared\nto other existing LLMs, significantly enhanc-\ning the efficiency of the attacks. Additionally,\nwe employed other 128K context models for\nfurther attack testing. The temperature pa-\nrameter for the attack model is set to T =\n0.5.\nDatasets. In the experiment, we utilized\neight datasets to evaluate both the baseline\nperformance and the effectiveness of the at-\ntacks. These datasets were grouped into three\ncategories:\n1. Application Datasets: Chatdoctor (Li\net al., 2023) and MMLU (Hendrycks et al.,"}, {"title": "Test Indicators.", "content": "We evaluate performance\nconsumption based on the average output and\nresource usage of the model. The effective-\nness of the defense mechanisms is assessed as\na secondary evaluation metric. Additionally,\nwe simulate the performance consumption in\nreal-world use cases by calculating the GPU\nutilization and the throughput of actual access\nrequests, in order to assess the practical effec-\ntiveness of the defense strategies. We utilize\ntwo NVIDIA RTX 4090 GPUs, each with 24GB\nof memory, for server simulation."}, {"title": "D.2 Complete data from\ncross-experiments.", "content": "In this section, we present the complete cross-\nexperimental data. The Tab. 10 shows the\nactual attack effects on the 11 models tested\nin the experiment."}, {"title": "E Defense Mechanisms\nConfiguration", "content": ""}, {"title": "E.1 Input Detection", "content": "From the perspective of input detection, we\nemployed a method based on PPL to analyze\nthe input"}]}