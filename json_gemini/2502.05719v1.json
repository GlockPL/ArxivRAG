{"title": "Extended Histogram-based Outlier Score\n(EHBOS)", "authors": ["Tanvir Islam"], "abstract": "Histogram-Based Outlier Score (HBOS) is a widely used outlier or anomaly\ndetection method known for its computational efficiency and simplicity. However,\nits assumption of feature independence limits its ability to detect anomalies in\ndatasets where interactions between features are critical. In this paper, we propose\nthe Extended Histogram-Based Outlier Score (EHBOS), which enhances HBOS\nby incorporating two-dimensional histograms to capture dependencies between\nfeature pairs. This extension allows EHBOS to identify contextual and\ndependency-driven anomalies that HBOS fails to detect. We evaluate EHBOS on\n17 benchmark datasets, demonstrating its effectiveness and robustness across\ndiverse anomaly detection scenarios. EHBOS outperforms HBOS on several\ndatasets, particularly those where feature interactions are critical in defining the\nanomaly structure, achieving notable improvements in ROC AUC. These results\nhighlight that EHBOS can be a valuable extension to HBOS, with the ability to\nmodel complex feature dependencies. EHBOS offers a powerful new tool for\nanomaly detection, particularly in datasets where contextual or relational\nanomalies play a significant role.", "sections": [{"title": "Introduction", "content": "Outlier detection, also referred to as anomaly detection, plays a critical role in numerous real-world\napplications, including fraud detection (Anbarasi & Dhivya, 2017), cyber security (Benjelloun et\nal., 2019), financial analytics (Hilal et al., 2022), healthcare monitoring (Bauder & Khoshgoftaar,\n2016), retail behavior analysis (Yoseph et al., 2019), and more. Outliers or anomalies are generally\nthe instances that deviate significantly from the norm or expected pattern (Hodge & Austin, 2004;\nZhao et al., 2019). The ability to identify these instances is crucial for mitigating risks, optimizing\nprocesses, and improving decision-making. However, achieving efficient, accurate, and\ninterpretable anomaly detection, particularly in large and high-dimensional datasets, remains a\nsignificant challenge.\nOutlier detection methodologies generally span statistical, distance-based, density-based, model-\nbased, neural network-driven, and histogram-based paradigms. Statistical techniques, such as Z-\nscore and Mahalanobis distance, rely on distributional assumptions to flag deviations, though they\nfalter when such assumptions are violated (Rousseeuw & Hubert, 2011). Distance-based methods\nsuch as kNN identify outliers by measuring the distance between data points; points that are far\naway from their nearest neighbors are considered outliers (Angiulli & Pizzuti, 2002; Chen et al.,\n2010). Density-aware methods like Local Outlier Factor (LOF) identify outliers as instances whose\nlocal density is significantly lower than that of their neighbors (Breunig et al., 2000). Model-based\nframeworks, such as One-Class SVM (Sch\u00f6lkopf et al., 2001) and Isolation Forest (Liu et al., 2008),\nlearn decision boundaries or partitions to separate anomalies. Neural network methods, notably"}, {"title": "HBOS Limitations", "content": "Histogram-Based Outlier Score (HBOS) is a computationally efficient anomaly detection method\nthat estimates outlier scores based on feature-wise histograms. However, its core assumption of\nfeature independence fundamentally limits its ability to handle datasets where interactions between\nfeatures are critical for identifying anomalies. To illustrate this limitation, we consider two synthetic\nsimple datasets and examine the behavior of HBOS outlier scores."}, {"title": "EHBOS", "content": "The Extended Histogram-Based Outlier Score (EHBOS) enhances the traditional Histogram-Based\nOutlier Score (HBOS) by combining one-dimensional (1D) and two-dimensional (2D) scores. The\n1D score is derived directly from HBOS, leveraging independent histograms for each feature. The\n2D score, in contrast, models pairwise interactions between features using joint histograms,\ncapturing dependencies that 1D histograms cannot. By aggregating normalized 1D and 2D scores,\nEHBOS achieves improved detection of outliers in datasets with both independent and dependent\nfeatures.\nLet's assume, our dataset is represented as $X \\in R^{n \\times d}$, where n is the number of samples and d is the"}, {"title": null, "content": "number of features. For each feature $j\\in 1, ..., d$, the one-dimensional density is estimated using\nhistograms. The density for feature j is computed as:\n$h_j(x_{i,j}) = \\frac{count(x_{i,j} \\in bin_b)}{n \\cdot bin\\_width}$\nwhere $x_{ij}$ is the value of the j-th feature for sample i, $count (x_{i,j} \\in bin_b)$ is the number of samples\nin bin b, and n is the total number of samples, $h_j(x_{i,j})$ represents the height of the bin (density\nestimate). The histogram is normalized so that its maximum height is 1.0 across all bins,, ensuring\nequal weight for each feature in the final outlier score computation.\nThe one-dimensional outlier score for sample i is then defined as:\n$S_i^{(1D)} = \\sum_{j=1}^d log(\\frac{1}{(h_j(x_{i,j}))}) = \\sum_{j=1}^d - log h_j (x_{i,j})$\nTo capture feature interactions, EHBOS computes pairwise densities for all feature pairs (j,k),\nwhere j, k \u2208 1, ..., d and j < k. The bin height is estimated as:\n$h_{jk}(x_{i,j}, x_{i,k}) = \\frac{count ((x_{i,j}, x_{i,k}) \\in bin_{bjk})}{n \\cdot (bin-area_{bjk})}$\nwhere $count ((x_{i,j}, x_{i,k}) \\in bin_{bjk})$ is the number of samples in bin bjk, and bin areabjk is the area\nof the two-dimensional bin, $h_{j,k}(x_{i,j}, x_{i,k})$ is the height (density estimate) of the 2D bin.\nThe two-dimensional outlier score for sample i is given by:\n$S_i^{(2D)} = \\sum_{j=1}^d \\sum_{k=j+1}^d - log h_{jk} (x_{i,j}, x_{i,k})$\nThe final EHBOS score is computed by aggregating the normalized one-dimensional and two-\ndimensional scores as:\n$S_i^{EHBOS} = \\frac{S_i^{(1D)} + S_i^{(2D)}}{2}$\nA summarized version of the algorithm is presented here."}, {"title": "EHBOS Algorithm", "content": "Input: X \u2208 Rnxd - input data with n samples and d features\nOutput: s - outlier scores for all samples\n01: Compute 1D histogram-based outlier scores using HBOS.\n02: Normalize the 1D scores.\n03: Initialize 2D outlier scores to zero.\n04: for each feature pair (i, j) where i < j do\n05:\n06:\n07:\nExtract the two-dimensional feature subset.\nCompute 2D histogram-based outlier scores using HBOS.\nAccumulate the normalized 2D scores.\n08: end for\n09: Compute the final EHBOS outlier scores by averaging 1D and 2D scores.\n10: return the final outlier scores."}, {"title": "Results and Discussion", "content": ""}, {"title": "Simple Datasets", "content": "To further elucidate the benefits of EHBOS, we revisit the results on the two simple datasets\ndiscussed earlier: the local outliers in clusters dataset and the regression outliers dataset. As\npreviously highlighted in the HBOS limitations section, HBOS fails to assign high outlier scores to\nthe outliers in both cases due to its inability to capture feature dependencies. In contrast, EHBOS\nleverages two-dimensional histograms to model feature interactions, resulting in significantly higher\noutlier scores for the anomalies. Figure 3 presents these comparisons, where, for illustration\npurposes, we display only the 2D score $s_i^{S(2D)}$ from EHBOS. In practice, the choice between using\nthe 2D score alone or the averaged final score in EHBOS can be treated as a tunable hyperparameter."}, {"title": "Empirical Evaluation", "content": "The performance of EHBOS is now evaluated against HBOS on 17 benchmark datasets,\nencompassing a broad spectrum of anomaly detection challenges. These datasets span various\ndomains, including but not limited to, image, time-series, and tabular data, providing a\ncomprehensive testbed for anomaly detection algorithms. The dataset encompasses different aspects\nof real-world anomaly detection tasks, such as varying levels of class imbalance, noise, feature\ncorrelations, and dimensionality"}, {"title": "Conclusion", "content": "In this paper, we have introduced the Extended Histogram-Based Outlier Score (EHBOS), a novel\nextension of the Histogram-Based Outlier Score (HBOS) that incorporates two-dimensional\nhistograms to address the limitations of feature independence inherent in HBOS. By augmenting\ndensity estimation with pairwise feature analysis, EHBOS effectively captures feature interactions,\nenabling improved detection of contextual and dependency-driven anomalies.\nOur empirical evaluation across 17 benchmark datasets demonstrates the efficacy of EHBOS, with\nnotable improvements in ROC AUC scores for datasets where feature interactions define the\nstructure of anomalies. Datasets such as glass and cardio highlight the advantages of EHBOS in\nleveraging joint feature distributions, while competitive performance on datasets dominated by\nmarginal anomalies underscores its robustness. However, the results also reveal that EHBOS is not\nuniversally superior, particularly in cases where feature independence aligns with the underlying"}]}