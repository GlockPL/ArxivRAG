{"title": "Value-Based Rationales Improve Social Experience: A Multiagent Simulation Study", "authors": ["Sz-Ting Tzenga", "Nirav Ajmerib", "Munindar P. Singh"], "abstract": "We propose Exanna, a framework to realize agents that incorporate values in decision making. An Exanna agent considers the values of itself and others when providing rationales for its actions and evaluating the rationales provided by others. Via multiagent simulation, we demonstrate that considering values in decision making and producing rationales, especially for norm-deviating actions, leads to (1) higher conflict resolution, (2) better social experience, (3) higher privacy, and (4) higher flexibility.", "sections": [{"title": "1 Introduction", "content": "A social norm states a shared standard of acceptable behavior in a society [31] and provides a basis for legitimate expectations regarding the behavior of others in the society. Each agent plays two roles: actor and observer. While exercising autonomy, an actor can deviate from the norms [26]. Such deviations may result in social conflicts and trigger positive or negative sanctions from observers. An acceptable rationale [33] can justify a deviation from a social norm.\nExample 1 Sharing a rationale. Alice wears a mask to the office and notices that Bella is not wearing a mask. Bella justifies her decision by stating that, first, the office has no mask mandate as the surrounding environment is safe. Second, she hates wearing a mask because wearing one gives her eczema. Alice agrees with Bella's view.\nA rationale provides the information to justify a decision [15]. In practice, rationales include additional information that others may be unable to observe, such as the actor's beliefs and preferences. Crafting a rationale remains an ongoing challenge. Rationales may be verbose, leading to information overload. Additionally, they might encompass private information that one may be hesitant to disclose, a concern particularly prevalent in healthcare settings.\nExample 2 Adapting a rationale. Bella and Alice share a concern for health. Despite Bella's aversion to wearing a mask because of eczema, given the safe environment, she feels it unnecessary to disclose her skin condition. Bella rationalizes her behavior of not wearing a mask by stating that the surrounding environment is secure and that a mask is unnecessary. Alice finds Bella's rationale acceptable.\nValues are motivational bases of one's behavior [23]. Reasoning about values is an essential capability to align agents with the values of their stakeholders [34, 35, 36], including providing and recognizing felicitous rationales for one's behaviors [18]. Deliberating over others' values can enhance persuasiveness and foster acceptance.\nInstead of sharing all available and related information in a rationale, it is beneficial for an agent to share only the information that aligns well with self and others' values. Sharing such information preserves the privacy of the rationale provider and ensures that unnecessary information does not inundate the observer of a rationale.\nContribution and Findings Accordingly, this paper extends beyond existing research by considering values in decision making and rationale generation and evaluation. Our Exanna framework generates rationales that incorporate values and include only the information needed to justify a decision.\nWe evaluate Exanna via a multiagent simulation based on a pandemic scenario. We consider societies of agents with different kinds of rationales: Share-All, Share-Rules, and share value-aligned rules. With Exanna, we find that agents who consider value importance when giving rationales exhibit enhanced conflict resolution capabilities. Additionally, rationales aligned with values, albeit with less information provided, contribute to more favorable social experience.\nNovelty Although prior research supports constructing explanations or making decisions based on values, this is the first study to investigate how values guide producing and using rationales for norm violation to support norm emergence and improve social experience.\nOrganization Section 2 discusses relevant related works. Section 3 details the Exanna framework. Section 4 describes a simulated pandemic scenario for evaluation. Section 5 demonstrates the results. Section 6 concludes with listing potential future directions."}, {"title": "2 Related Work", "content": "Research on agents interacting based on their rationales and modeling values is relevant to our approach.\nAgents and Rationales Hind et al. [14] leverage existing supervised machine-learning techniques to generate rationales together with decisions without values involved and without exposing the inner details of the model. Whereas Hind et al. generate rationales based on the existing training set, Exanna generates rationales based on context and values.\nGeorgara et al. [11] show how to build rationales on why specific teams are formed. Specifically, Georgara et al. build rationales based on contrastive explanations and by exploring what-if scenarios. A causal attribution explains why a behavior occurs. We provide causal attribution of the selected action, precisely the premise, as rationale and withhold private information based on values.\nWang et al. [32] formulate rationales with the simplest subset of features that is sufficient as causal attribution for probabilistic solid guarantees on model behavior under observed data distribution. Contreras et al. [7] propose a mirror model and assume a high understandability from performing similar to an observer's mental simulation. They apply deep Q-network and saliency maps in rationale generation, highlighting related input features as rationales. These works reveal model features but not consider values.\nAjmeri et al. [2] propose Poros, a framework that shares full context as a rationale. Therefore, agents can adopt the perspectives of others and make corresponding decisions. However, Ajmeri et al. do not consider values. In Exanna, an actor selectively shares information based on its values and those of the observer.\nAgents, Norms, and Values Tzeng et al. [29] define social communication (sanction, message, and hint), which besides actual reward or punishment, indicates normative information and potential outcomes. Unlike signaling others with normative information, rationales can enable information sharing and conflict resolution. Tzeng et al. [28] incorporate social value orientation (SVO) in decision making. Whereas values define what is important to agents, SVO describes the importance an agent places on its gain in relation to others. Exanna covers a broader range of motivations and behaviors.\nCranefield et al. [8] represent agents' plans to achieve goals as a goal-plan tree and expand the Belief-Desire-Intention language by annotating actions with the effects regarding values. Lera-Leri et al. [16] consider ethical principles (e.g., maximum utility and maximum fairness) for aggregating value systems, not just one value. Ajmeri et al. [3] aggregate users' value preferences to make ethically appropriate decisions. Besides making decisions based on aggregate value importance, Exanna agents generate rationales for their decisions with necessary information.\nAgrawal et al.'s [1] agent learns norms as rules of optimal behaviors, but considers no values. Exanna adaptively shares learned rules as rationales that align with individuals' values.\nMosca and Such's [20] agent supports values in multiuser settings by considering the preferences and values of users. The agents justify solutions through contrastive explanations and positive answers. Exanna presents factors in alignment with values rather than presenting all that convey causal attribution.\nWhereas other works construct explanations with values [33] or make decisions based on values [8], our focus is on building suitable rationales and investigating how value-aligned rationales shape decision-making and influence social experience, especially privacy.\nPrevious research considers only the causal links between the behaviors and each contextual factor but not the importance of the factors. Is it essential to include every single related factor? Numerous factors may come into play in real-world situations, yet people typically do not provide or need to provide exhaustive explanations to account for every one of these factors.\nOgunniye and K\u00f6kciyan [21] propose an ontology to represent the privacy domain. They introduce argumentation-based multiparty dialogues to reason about contextual norms, and resolve privacy conflicts. Di Scala and Yolum [10] propose an argumentation-based agent to achieve agreement on privacy among multiple users. Their agent provides feedback to the user, such as a summary or detailed advice on possible actions to improve performance. Whereas privacy is a right motivated by values, Exanna encompasses broader aspects of core values that serve as guiding principles for decisions and rationales. Additionally, rationales focus on explaining decisions while these works prioritize maintaining privacy among multiple users via argumentation. Ayc\u0131 et al. [4] explain the privacy decisions (sharing content) using labels (private or public) that are assigned to topics predicted by machine learning. While their values (privacy) come from post-interpretation from humans, Exanna incorporates values in rationale construction."}, {"title": "3 Method", "content": "We now describe the schematics and decision making in Exanna along with its rationale components.\n3.1 Schematics of an Exanna Agent\nBelief: An agent's view of the world, formed based on observations. $b_t$ indicates the belief at time t. A belief is captured as a set of pairs of attributes and bindings.\nContext: The factors that characterize the situation of an agent. Context is represented as a set of attribute-binding pairs. An example of context is as follows.\nA context comprises public (e.g., an agent's location) and private (e.g., beliefs, preferences, and values) factors. Contextual factors may be associated with values (e.g., Risk relates to Health).\nGoal: A set of states that an agent wants to achieve. The outcome of a goal after performing the selected actions is binary: achieved or not.\nAction: A means to change the state in pursuit of one's goals and maximize associated payoffs.\nPreference: A subjective inclination for an action over the alternatives.\nDecision rule: A mapping between a premise (set of attribute-binding pairs) and a consequent (an action to be taken). An example rule is\nNorm: The expected behavior or the behavior of the majority in a group. When a majority applies the same decision rule, the rule becomes a norm. In Exanna, a norm uses the same if-then representation as a decision rule.\nSanction: A response to norm violation or satisfaction. A sanction can be a positive or negative reaction from one agent to another.\nPayoff: The benefits an agent receives in a given state after taking an action. Payoffs involve intrinsic benefits (e.g., preferences) and extrinsic benefits (e.g., sanctions imposed by others).\nValues: General motivations of agents. Specifically, values define what agents believe to be important, while goals are the desired states. Whereas goals are time-bounded and dynamic to context, values are long-lasting and stable and may transcend contexts [17]. A subset of values is applicable within a context [17], and each agent assigns an importance rating to each value.\nValue importance: the importance of values in one context [23]. We store each value importance $V_{context}$ in a tuple where numbers add up to 1. $v_i$ denotes the weight of one value in one value importance ($v_i \\in V_{context}$) where $0 \\le v_i \\le 1$ and $\\Sigma_{i=1}^n V_i = 1$.\nWe treat each $(V_{context})$ as an attribute and store the corresponding value importance as its binding. For instance, an agent with value importances $V = \\{V_{pandemic} = \\{U_{health} = 0.6, U_{privacy} = 0.4\\}; V_{normal} = \\{U_{health} = 0.4, U_{privacy} = 0.6\\}\\}$ indicates that the agent values health over privacy during a pandemic but the opposite in a normal context.\n3.2 Payoff Calculation with Values\nWhereas preferences define the tendency of an individual to make a subjective selection among alternatives, values define the important things to an individual. Although both values and preferences are context-specific, values may transcend contexts [17]. Each agent stores values in a tuple where each value maintains a corresponding $M_{individual}$. Since agents do not make decisions with single values but with tradeoffs among multiple related values, we aggregate value importances when constructing a payoff [3, 22]. Below, $f$ is the aggregated payoff with all corresponding values after selecting strategy $R_x$ when the other player selects strategy $C_y$ from $M_{individual}$.\n$f = \\Sigma \\frac{v_i \\times v_i}{\\mid values\\mid^2} \\times v_{i, R_xC_y}$  (1)\nWe model interactions with payoffs $f$ from the aggregation of $M_{individual}$.\n3.3 Interaction and Decision Making\nInteractions in Exanna are between an actor (rationale provider) and an observer (rationale observer), as shown in Figure 1. An actor selects an action based on its goal and beliefs and provides a rationale to the observer who witnesses its behavior. Upon receiving a rationale from the actor, the observer evaluates the rationale by making an analogous decision. With a weighted sum of payoffs, we incorporate values in decision making where a substantial value casts a more significant effect on the final decision.\nAlgorithm 1 gives the pseudo-code of an agent's decision-making loop. An agent forms beliefs $b_t$ about the world based on its observations (Line 4). An agent's payoff is a weighted sum of payoffs corresponding to its values, goal achievement, and factors influencing the decision-making process, such as social circle or social environment. The Q function in Line 7 and reward in Line 8 refer to the payoff calculation in Section 3.2, which incorporate value importances and feedback from others. In Line 7, the agent selects the action that gives the best payoff for $b_t$. If the agent interacts with another agent, for its action the agent creates rationales based on $b_t$ (including values of both parties) and the selected action (Line 11 with Algorithm 2) and sends those rationales. Other agents who observe the action and receive the rationales update their beliefs, evaluate the rationales (Algorithm 3) with their context, and give sanctions accordingly. Algorithm 4 defines a function to retrieve value importance from beliefs based on context. Exanna applies to scenarios that can be modeled as a partially observable Markov decision process and with clearly defined links between values and contextual factors.\n3.4 Rationale Generation\nRationale generation in Exanna follows a rule learning process\u2014 a process of evolving rules from datasets or interactions. The basic form of a rule is if premise then consequent, where the consequent holds whenever the premise is true. We adapt XCS [5], which applies a genetic algorithm and reinforcement learning to evolve a set of rules or strategies based on payoffs or rewards produced by the proposed actions. Unlike other machine learning techniques, XCS generates a set of rules describing its decision. XCS process enables flexibility for the implementation of norms and supports interpretability by producing logical rules. An example rule of Example 2 is\nThe premise of a learned rule is a conjunction of attribute-binding pairs, e.g., {Risk=None, InteractWith=colleague}. Its consequent is an action to be taken when the premise holds in the above example, -Wear. Each rule associates (1) a fitness, i.e., its suitability, (2) a numerosity, i.e., the number of its instances in the rule set, (3) the expected reward if the rule applies, and (4) prediction error.\n3.4.1 XCS for Rationale Generation Briefly\nThe key features of XCS are Rule Discovery, Rule Subsumption, and Action Selection. Rule discovery through the crossover and mutation processes involves introducing randomness to the antecedent by adding or removing factors, thereby generating rules that are more general or more specific. Given two rules, if the more general one exhibits lower predictive error within the given context, the algorithm retains it and discards the more specific one. When selecting an action, the algorithm selects the one with the best-aggregated fitness. Details on XCS are in Appendix A.\nAn example of a rationale for not wearing a mask is {Risk=None, Preference=-Wear, InteractWith=colleague}. This rule means the mask is not needed when there is no infection risk and the actor prefers not to wear a mask while interacting with a colleague. Each agent keeps the rules it discovers and evolves those in a rule set for decision making.\n3.4.2 Generating Value-Based Rationale\nNot all factors of a rule generated by the rule learning process are suitable for inclusion in a rationale. In Example 2, sharing personal preferences is unnecessary when both agents value health. After generating the base rule, we post-process its factors using the values of the actor and observers. Thus, the agent who prefers the value of health adjusts its rationale for the colleague who also cares about health to a health-related causal attribution if it exists. For instance, no mask is required because there is no risk of infection when interacting with a colleague.\nAlgorithm 2 details the process of constructing rationale. An agent identifies its rules associated with beliefs $b_t$ (Line 3) and the selected action (Line 4) and then aggregates all rules related to a rationale (Line 5). To minimize information exposure, an agent reveals private information only if it is associated with its values or those of others involved in the interaction (Line 8-9). For instance, if an agent who cares about freedom interacts with one who cares about freedom, it will exclude the infection risk from the environment in its rationales. For each rationale, an agent estimates privacy based on the least proportion of private factors included in the rationale (Line 13).\n3.5 Rationale Evaluation\nOn receiving a rationale from the actor, the observer first updates its beliefs based on the rationale. Specifically, the observer updates the beliefs of unobservable information from the actor's context. In the rationale generation mask example, the observer updates its beliefs of the infection risk to \"None\". When evaluating a rationale, the observer makes an analogous decision based on the updated beliefs. If the observer's computed action matches the actor's observed action in that context, the observer accepts the actor's rationale.\nAlgorithm 3 defines how agents evaluate rationales. Initially, in Line 2, the observer updates its beliefs $b_t$ based on the private context or beliefs included in the actor's rationale. Subsequently, the agent makes a decision analogous to the actor's context based on the updated beliefs. Specifically, with the provided rationale, an agent checks if any applicable rules align with its rule sets in Line 3. The agent identifies associated rules from $b_t$ and adds them to applicable rules in Line 4. In Line 7, the agent calculates the fitness for each available action for each applicable rule and keeps the best action for each rule. The agent accepts this rationale if any selected action matches the observed action."}, {"title": "4 Simulation", "content": "Real-world factors may be associated with underlying values. For instance, health state corresponds to health concerns. We assume the importance of factors according to their associated values.\nWe evaluate Exanna via a pandemic scenario based on Examples 1 and 2 and simulated using MASON [19]. Here agents move to various places, interact with other agents, decide to wear or not wear a mask, and provide a justification for their actions.\n4.1 Scenario\nThe environment represents a multiagent society with several places and social circles. Our environment involves a finite population of 200 agents with different social circles. The environment includes one park, one hospital, five homes, five offices, and five parties. Agents move around and interact within these five places. Each agent is native to one home, one office, and one party. Agents in the same home, office, or party share the same family, colleague, or friend social circle. Each social circle has 40 agents. Time is represented in steps. Each agent moves to one place at each step and has a probability (50%) of interacting with one agent at the same place. Agents are more likely (75%) to move to places they are associated with when they move to home, office, and party, i.e., an agent is more likely to visit their own home rather than someone else's home.\nEach agent forms its goal based on its value importances. Specifically, each value in one context has a payoff matrix (Table 5 and 6); the weighted sum of the payoff determines the goal (desired states). An agent selecting an action that does not align with its goal is considered deviating from its goal.\nIn the simulated environment, when an agent encounters another agent at the same place, it chooses an action based on its goal\u2014 whether to wear a mask. In addition, the agent justifies its behavior based on its beliefs in that context. For instance, the agent gives a rationale {Risk=None, InteractWith=Colleague}-while not wearing a mask. The beliefs of an agent include public and private factors. Each agent receives a payoff according to the interaction place for action selection, as in Table 2. Wearing a mask at a hospital during a pandemic is desirable. Place and value importances determine the payoff an agent gives to itself. An agent also gives sanctions as feedback to others based on their actions.\nThe sanctions are based on the social circle. Table 3 lists the sanctions associated with social circles. We run each simulation 10 times, with each run lasts 30,000 steps. We consider the values of freedom and health. In this setting, freedom refers to agents claiming their free will and adhering to their preferences."}, {"title": "5 Results", "content": "Table 8 summarizes our statistical analyses. Exanna offers higher social experience, conflict resolution, and flexibility, indicating that Exanna agents learn to act for the greater societal good. These results follow our intuition that value-aligned rationales are superior. However, if an agent prefers to keep certain information private, deviation from goals is expected. Results for the Share-All and Share-Rules societies indicate that increased information in a rationale may not be helpful.\nHResolution Figure 2 compares conflict resolution in various societies. Exanna offers better conflict resolution (p < 0.001; \u0394 > 0.8, indicating a large effect) than other societies. Thus, we reject the null hypothesis corresponding to HResolution. We observe that, in scenarios where other agents do not accept the provided rationales, Exanna agents more flexibly deviate from their own goals to resolve conflicts. Our results demonstrate the dynamics of agent behaviors and the strategy of rationales.\nHSocial Experience For Hsocial Experience, we measure the overall payoffs of agents in a society. An agent's payoff includes personal payoff from its action and feedback from its interaction. Figure 3 compares the social experience for Share-All, Share-Rules, and Exanna agent societies. We find that Exanna yields better social experience (p < 0.001; \u25b3 > 0.8, indicating a large effect)) than other societies. Specifically, Exanna agents receive better feedback from other agents who receive their rationales. Thus, we reject the null hypothesis corresponding to Hsocial Experience. On closer analysis, we observe that Exanna agents receive more negative sanctions than other societies initially but soon learn to deviate from their goals.\nHPrivacy Exanna agents better retain their privacy (p < 0.001; \u0394 > 0.8, indicating a large effect) compared to Share-All or Share-Rules agents. Thus, we reject the null hypothesis corresponding to Hprivacy. Although both the Share-Rules and Exanna societies share learned rules as rationales, each Exanna agent aligns rationales to its values and those of the observers, and limits the private information shared to values that agents appraise. Our results show that a rationale stating causal attribution with minimum private information while aligning with individuals' values is sufficient to explain behaviors.\nHFlexibility Figure 4 compares MFlexibility for Share-All, Share-Rules, and Exanna agent societies. We find that Exanna offers higher flexibility (p < 0.01; \u25b3 > 0.8) than the Share-Rules society. Although the mean flexibility in the Share-All society is lower than in the Exanna society, this difference is not significant (p > 0.05). Our results demonstrate that an agent can achieve a better social experience without sticking to only its goal.\nEmerged Norm A norm emerges when the proportion of agents adhering to a particular behavior surpasses a threshold. We consider 90% as the threshold [9]. We observe that Exanna promotes more general norms than the Share-All and Share-Rules Societies. For instance, the following norms emerged only in Exanna.\n`{preference = \u00abWear, InteractWith =\nColleague, location=OFFICE} => Wear`\n`{OberverAgentType = FREEDOM, InteractWith =\nColleague, location=HOSPITAL} => Wear`"}, {"title": "6 Conclusions, Limitations, and Directions", "content": "Responsible autonomy requires that agents represent, reason with, and communicate values in their rationales. We demonstrate via a multiagent study how we could create agents who incorporate values in decision making and in rationale generation and evaluation. Value-aligned rationales offer better social experience and higher conflict resolution. Whereas value-aligned rationales withhold partial information, agents learn to deviate from their goals to protect their privacy. Specifically, agents who receive rejections from others become more flexible to improve cooperation.\nAssumptions and Limitations We make simplifying assumptions. First, agents can identify other agents' types, which indicate their values. We limit our simulation to two values (health and freedom) to demonstrate how value importances shape behaviors. A real-world scenario may include more intertwined values. Investigating the impact of these values on decision making and exploring approaches to elicit value importances necessitates further research. In addition, our numerical representation captures the importance of each value, but we do not target the complex contextual features in this work. We focus on providing insights and methodologies to understand and assess complex situations and generate informed rationales. We create and model simplified abstractions of intricate behaviors, enabling the analysis of complex situations.\nBy modeling the context as attribute-binding pairs, our approach is adaptable to various settings. However, our approach could suffer from state space explosion like other rule-based learning approaches. Despite considering single actions for simplicity, we focus on studying the effects of value-based explanations.\nOur work contributes to developing value-aligned, trustworthy AI by showing how value-driven rationales help resolve conflicts arising from norm deviation and thus affect the emergence of norms. Our work complements approaches focused on eliciting values from stakeholders [17, 18] and figuring out what are adequate reasons for norm deviation [26] and for achieving trustworthy AI [25].\nWe assume the importance of contextual factors according to their associated values. For instance, someone prioritizing health may present factors correlating to health benefits or drawbacks. Although our scenario is simple, the rationales are dynamically constructed based on rule learning.\nFuture Directions First, incorporating the cost of privacy loss is crucial. For instance, sharing sensitive information and sharing interests with chatbots impose different costs. Thus, modeling such costs is essential to capture how agents decide. Second, empowering agents to make informed choices regarding disclosure. For instance, when an agent acting on behalf of a stakeholder engages with agents other than the healthcare provider and the stakeholder, restricting the sharing of sensitive information may be desirable. Third, integrating rationales into the decision-making process, not just using them as supplementary information [29]. Having rationales as part of the decisions may increase the flexibility of an agent. Fourth, build an ontology to associate information with values, which we model as factors. An ontology helps to model varied factors or concepts and their intertwined relationships. While Exanna enables value-driven rationales and focuses on the decisions of a single agent, one future direction is to promote values and norms in a multiagent system [24].\nReproducibility The codebase for our simulation is publicly available [27]. The appendices provide additional details, including hyperparameters for reproducibility, the complete set of emerged norms, and supplementary evaluations."}, {"title": "A Appendix: Procedures of XCS", "content": "The overall process of XCS includes the following sub-processes.\n\u2022 Matching: A process that matches the current context and all rules/classifiers to generate a match set. For instance, in our running example, the match set for Bella may include (1) {Risk = Low} \u2192 Wear [fitness = 0.3], (2) {Risk = Low} \u2192 Wear [fitness = 0.7], (3) {OtherAgentType = Health} \u21d2 Wear [fitness = 0.8], and (4){OtherAgentType = Health} \u2192 Wear [fitness = 0.2]. The fitness is based on the accuracy of each rule's reward prediction.\n\u2022 Covering: A process that guarantees diversity via adding a random classifier whose conditions match the current context. For instance, adding {Risk = Low, Relationship = Friend} \u21d2 Wear to the rule set.\n\u2022 Action selection: XCS selects actions with pure exploration or pure exploitation with e greedy. If not in exploration mode, this process returns the action with the highest fitness-weighted aggregation of reward.\n$fitness_a = \\frac{\\Sigma_{r \\in A} fitness \\times numerosity_r \\times predicted\\_reward_r}{\\Sigma_{r \\in A} fitness \\times numerosity_r}$ (2)\nwhere a \u2208 A and A is the action space. Rules represent all rules applied to the context and for action a. With the above example and formula, the agent would choose not to wear a mask due to $fitness_{-\\text{wear}} > fitness_{\\text{wear}}$.\n\u2022 Formation of action set: It includes all classifiers that propose the chosen action based on the match set. For instance, {Risk = Low} \u21d2 - Wear, {OtherAgentType = Health} \u21d2 Wear, and {Risk = Low, Relationship = Friend} \u2192 Wear.\n\u2022 Updating classifier parameters [30]: An agent updates the rule parameters (e.g., accuracy and fitness) based on the received payoff. The following equation updates the predicted reward, where p is the predicted reward, \u03b2 is the learning rate, and r is the received reward.\n$p \\leftarrow p + \\beta (r - p)$ (3)\nThe prediction error \u03b5 is updated with the following equation.\n$\\epsilon \\leftarrow \\epsilon + \\beta(\\mid r - p\\mid - \\epsilon)$ (4)\nThe fitness of a rule is based on its accuracy, which is inversely proportional to the prediction error. We update the accuracy kappa with the following formula.\n$kappa = \\begin{cases} 1 & \\text{if } \\epsilon < \\epsilon_0 \\\\ \\alpha (\\frac{W}{\\epsilon})^v & \\text{otherwise,} \\end{cases}$ (5)\nwhere \u03b1 is the scaling factor that raises a non-accurate rule to be close to an accurate rule. $\\epsilon_0$ is the threshold of prediction error below which the prediction error of a rule is assumed to be zero. \u03bd defines how accuracy is related to prediction error and aims to help differentiate similar classifiers. For fitness calculation, we next calculate the relative accuracy \u03ba' of each rule.\n$\\kappa' = \\frac{\\kappa_r}{\\Sigma_{r \\in [A]} \\kappa_r}$ (6)\nwhere [A] represents the corresponding action set. Finally, the fitness update of a rule is as follows.\n$F \\leftarrow F + \\beta(\\kappa' - F)$ (7)\nwhere F is the fitness of a rule.\n\u2022 Subsumption: A process that replaces offspring rules with more general parent rules if it exists. Otherwise, save the offspring rules. Specifically, a more general rule yields a minor prediction error. For instance, if rule {Risk = Low} \u21d2 \u00abWear has less prediction error than rule {Risk = Low, Relationship = Friend} \u2192 \u00abWear, the former rule would replace the later rule and increases the numerosity.\n\u2022 Deletion: Each action set has the same maximum number of rules. XCS removes the low-fitness rules."}, {"title": "B Appendix: Hyperparameters for Reproducibility", "content": "Table 9 lists the hyperparameters we set for our simulations. We opt for the default configurations as in the literature [30] since our emphasis is on studying the impact of values in rationales and not in fine-tuning the learning process. Learning rate refers to reinforcement learning and determines the proportion an agent learns from recent experiences. Adjusting the don't care probability alters the probability of including the perceived factors in the generated rule. Increasing the accuracy threshold reduces the number of rules to maintain. Fitness exponent and fitness falloff determine the accuracy of the rules. Genetic algorithm threshold controls the probability of rule exploration. Mutation probability and crossover probability determine how often the mutation and crossover happen. Experience thresholds for deletion and subsumption guarantees a specific number of times a rule has to be applied before being deleted or subsumed.\nThe codebase for our simulation is publicly available [27]."}, {"title": "C Appendix: Detailed Results", "content": "Table 10 summarizes the statistical analysis of our simulations, including additional results for actor payoffs and observer payoffs and flexibility across different agent types. Actor payoff and observer payoff comprise social experiences.\nThese results demonstrate that first, with the same context, agents with different value importances can still evolve to different behaviors. Second, more adaptive norms can emerge among agents with different values.\nHSocial Experience Social experience includes actor payoff and observer payoff. Figures 5 and 6 plot the payoffs of the actors who select actions, explain their behaviors, and receive feedback from observers in the Share-All, Share-Rules, and Exanna agent societies. Figures 7 and 8 compare the payoff from the observer who reacts to the actor's behavior in the Share-All, Share-Rules, and Exanna agent societies. The freedom-loving agents within Exanna society encounter more adverse feedback than other societies initially. However, some of them quickly adapt and begin to divert from their original goals. As a result of the behavioral change made by freedom-loving agents, there has been an enhancement in the feedback received by health-freak agents.\nHFlexibility We compare agents' flexibility of goals as the metric of evaluating HFlexibility. Figures 9 and 10 compare MFlexibility for health-freak and freedom-loving agents in the Share-All, Share-Rules, and Exanna agent societies. Referring to Figure 8, the freedom-loving agents compromise on goals, thereby enhancing flexibility and enriching social experience.\nHResolution Figure 11 compares conflict resolution in various societies. Exanna offers better conflict resolution than other societies.\nHSocial Experience Figure 12 compares the social experience for Share-All, Share-Rules, and Exanna agent societies. A social experience includes personal payoff from its action and feedback from its interaction. Exanna yields better social experience (p < 0.01; \u2206 > 0.8, indicating a large effect) than other societies.\nHPrivacy Exanna agents better retain their privacy (p < 0.001; \u0394 > 0.8, indicating a large effect) compared to Share-All or Share-Rules agents.\nHFlexibility Figure 13 compares MFlexibility for Share-All, Share-Rules, and Exanna agent societies. The mixed values lead to higher flexibility in agent societies than in the main simulations. Specifically, the mixed values narrow the numerical gap between each action, decreasing the threshold for agents to change their minds."}]}