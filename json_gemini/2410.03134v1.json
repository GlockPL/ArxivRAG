{"title": "Remaining Useful Life Prediction: A Study on\nMultidimensional Industrial Signal Processing and Efficient\nTransfer Learning Based on Large Language Models", "authors": ["Yan Chen", "Cheng Liu"], "abstract": "Remaining useful life (RUL) prediction is crucial for maintaining modern industrial systems, where equipment\nreliability and operational safety are paramount. Traditional methods, based on small-scale deep learning or\nphysical/statistical models, often struggle with complex, multidimensional sensor data and varying operating\nconditions, limiting their generalization capabilities. To address these challenges, this paper introduces an\ninnovative regression framework utilizing large language models (LLMs) for RUL prediction. By leveraging the\nmodelling power of LLMs pre-trained on corpus data, the proposed model can effectively capture complex\ntemporal dependencies and improve prediction accuracy. Extensive experiments on the Turbofan engine's RUL\nprediction task show that the proposed model surpasses state-of-the-art (SOTA) methods on the challenging\nFD002 and FD004 subsets and achieves near-SOTA results on the other subsets. Notably, different from previous\nresearches, our framework uses the same sliding window length and all sensor signals for all subsets,\ndemonstrating strong consistency and generalization. Moreover, transfer learning experiments reveal that with\nminimal target domain data for fine-tuning, the model outperforms SOTA methods trained on full target domain\ndata. This research highlights the significant potential of LLMs in industrial signal processing and RUL prediction,\noffering a forward-looking solution for health management in future intelligent industrial systems.", "sections": [{"title": "Introduction", "content": "Industrial signal processing and RUL prediction are core tasks in modern industrial equipment health management\n(Chen et al., 2023). By monitoring the operational status of equipment in real-time and combining it with historical\ndata for fault prediction, RUL prediction can help enterprises take preventive maintenance measures before\nequipment failure, thus reducing unexpected failures, extending equipment life, optimizing maintenance\nschedules, and lowering operational costs (del Castillo and Parlikad, 2024). This is especially important for high-\nrisk, high-cost industries such as aerospace, energy, and manufacturing. However, despite the broad application\nprospects of RUL prediction, existing technologies still face numerous challenges and bottlenecks in handling\ncomplex, multidimensional industrial signals (Behera and Misra, 2023).\nRUL prediction methods can be mainly divided into three categories: physics-based methods, statistical methods,\nand data-driven methods (Li et al., 2024). Physics-based models rely on a deep understanding of the degradation\nmechanisms of equipment, with common models including the Paris crack growth model, stress-strength models,\netc (Zhang et al., 2024d). For instance, researchers (Yan et al., 2021) propose a two-stage physics-based Wiener\nprocess model to improve the RUL prediction for rotating machinery, incorporating physics knowledge of fatigue\ncrack growth mechanisms. The model's effectiveness is demonstrated on wheel tread vibration data, showcasing\nits practical application and ability to achieve high prediction accuracy. However, such methods require extensive\ndomain expertise and cannot meet the diverse needs of more complex industrial systems. Statistical models predict\nthe remaining life of equipment by analysing historical data patterns, with commonly used models including the\nGamma process, Wiener process, and Inverse Gaussian process (Zhuang et al., 2024). A recent study (Pan et al.,\n2023) addresses the challenge of insufficient data in predicting the RUL of lubricating oil by proposing a coupling\nmodel that integrates both knowledge and data. An exponential Wiener process is used to model oil degradation,\nand the method is validated through hydraulic pump bench test data, demonstrating improved accuracy. But these"}, {"title": "", "content": "models rely on strong assumptions and are difficult to flexibly adapt to different fault modes and operating\nconditions.\nWith the rapid development of sensor technology, the multidimensional sensor signals generated during\nequipment operation exhibit complex temporal dependencies. To handle these complex signals, small-scale deep\nlearning models have gradually been applied to RUL prediction tasks, capturing temporal features in the\ndegradation process of equipment to some extent and achieving good prediction results (Wen et al., 2024). For\nexample, long short-term memory (LSTM), through its special memory cell structure, solves the vanishing\ngradient problem in traditional recurrent neural networks (RNNs) and maintains a good modeming capability over\nlong time sequences. Many researchers have applied LSTM to RUL prediction studies (Dong et al., 2023). For\nexample, a study (Wang et al., 2023) proposes a Poly-Cell LSTM network to improve RUL prediction for lithium\nbatteries, addressing challenges like nonlinear degradation, capacity regeneration, and noise. Experimental results\ndemonstrate the model's effectiveness compared to traditional methods. However, although these deep learning\nmodels show potential in handling multidimensional time-series signals, their generalization capability and\ntraining efficiency still face significant problems in complex industrial scenarios, especially when dealing with\nmultiple operating conditions, fault modes, and cross-task scenarios (Ferreira and Gon\u00e7alves, 2022).\nFirst, the multidimensional sensor signals in industrial equipment not only have strong temporal dependencies but\nare also accompanied by complex spatial correlations (Zhang et al., 2024c). Existing small-scale deep learning\nmodels, while capable of capturing some temporal features, often exhibit insufficient modeming capabilities when\nfaced with high-dimensional, multi-task sensor signals, making it difficult to effectively extract key features in\nthe equipment degradation process (Xu et al., 2023). Second, existing methods struggle with generalization on\ntask-related signals (Cheng et al., 2023). Industrial systems typically experience various operating conditions and\nenvironmental changes, and existing deep learning models usually require hyperparameter adjustments and\ntraining for each task subset. Often, a new model must be retrained from scratch for each new task, and the model\nstruggles to maintain consistent performance across multiple subsets. For example, in the CMAPSS dataset's RUL\nprediction task (Saxena et al., 2008), existing models usually require different sliding window lengths and specific\nsensor signals for different tasks (Ferreira and Gon\u00e7alves, 2022), with the model's performance heavily dependent\non complex physical knowledge or human expertise, severely limiting the model's generalization and consistency.\nTo address the various problems in existing RUL prediction methods for industrial systems, we turned to large\nlanguage models (LLMs) (Jose et al., 2024). LLMs have recently made significant breakthroughs in the field of\nnatural language processing (NLP), gaining widespread attention due to their powerful modelling and feature\nextraction capabilities (Hou et al., 2023). Large language models, such as the GPT family (Wolf et al., 2020), use\nmulti-layer self-attention mechanisms to effectively capture long-range temporal dependencies and exhibit\noutstanding advantages in handling complex sequential data. However, despite their success in NLP, the\napplication of LLMs in industrial signal processing and RUL prediction has not yet been fully explored and\ndeveloped (Pang et al., 2024). In this study, we innovatively introduced LLMs into the field of industrial signal\nprocessing and RUL prediction and proposed a multidimensional signal regression model based on LLMs. The\ngoal is to leverage the benefits of models pre-trained on large amounts of language data to overcome the\nlimitations of existing deep learning models in handling complex industrial signals.\nFirst, thanks to the powerful modelling capabilities of large models, we can use the self-attention mechanism to\nenable the model to simultaneously capture the temporal dependencies and spatial correlations in\nmultidimensional sensor signals, enhancing the AI model's understanding of complex equipment degradation\nsignals (Zhang et al., 2024b). Next, to improve the model's generalizability and consistency, we adopted a unified\nmodel structure, using the same sliding window length and all sensor signals across all task subsets for training.\nThis design avoids the hyperparameter adjustment problem caused by task differences in traditional RUL\nprediction methods, significantly improving the model's generalization and consistency. Additionally, we\nthoroughly examined the remarkable transfer learning performance of large models. To utilize the rich knowledge\nlearned in the source domain (one industrial RUL prediction task), after training in the source domain, we\nemployed a partial layer freezing strategy to avoid knowledge forgetting that may occur during full parameter\ntraining (Mao et al., 2024). Specifically, we froze some layers of the large model and opened only a few layers,\nfine-tuning with a small amount of data from the target domain (another industrial RUL prediction task with\nsignificantly different signals). This approach not only preserves knowledge from the source domain, significantly\nenhancing the model's adaptability to new tasks, but also achieves substantial optimization in computational"}, {"title": "", "content": "resources and training efficiency, providing an efficient and economical solution for RUL prediction in industrial\napplications."}, {"title": "Main Contributions", "content": "Innovative introduction of large language models for industrial RUL prediction: This paper proposes a\nmultidimensional industrial signal processing framework based on LLMs. By leveraging the powerful sequential\nmodeming capabilities of LLMs, our framework effectively captures the temporal dependencies and spatial\ncorrelations in multidimensional sensor signals from complex industrial systems. In the CMAPSS dataset,\nparticularly in the most challenging FD002 and FD004 subsets, the proposed model achieves superior prediction\naccuracy compared to the current state-of-the-art (SOTA) methods. In other subsets, the model also achieves near-\nSOTA performance.\nUnified model structure for cross-task consistency: This paper designs a unified general-purpose model structure,\navoiding the need for hyperparameter adjustments for different task subsets, as is common in traditional methods.\nBy using the same sliding window length and all sensor signals across all task subsets for training, the model\nmaintains a high degree of consistency and stability when handling multiple tasks and varying operating\nconditions. This universal framework greatly simplifies model usage and deployment, overcoming the bottlenecks\nin cross-task performance seen in current deep learning-based RUL prediction methods.\nEfficient transfer learning strategy significantly improves training efficiency: A novel transfer learning strategy\nfor large model-based industrial signal processing and RUL prediction is proposed. After training in the source\ndomain, only a small amount of target domain data is used for rapid fine-tuning, achieving better performance\nthan the SOTA methods trained on the full target domain data. This not only greatly improves training efficiency\nbut also significantly reduces computational resource consumption, providing an efficient and economical\nsolution for the promotion of RUL prediction in practical industrial applications.\nAlthough this paper uses the CMAPSS turbine engine dataset as an example, the proposed framework has broad\nindustrial applicability and can handle various complex industrial signals, providing a new solution for the health\nmanagement of future intelligent industrial systems."}, {"title": "Problem Statement", "content": "Existing RUL prediction methods face numerous limitations when handling complex industrial scenarios,\nespecially in dealing with multidimensional sensor signals and multi-task environments:\nInconsistent performance and poor generalization across subsets: Current RUL prediction methods often perform\ninconsistently across different data subsets. These models typically require different sliding window lengths and\nselected sensor signals for each specific task, leading to weak generalizability. Given the complexity and\nvariability of industrial equipment operating environments, involving multiple operating conditions and fault\nmodes, existing models struggle to maintain consistent performance across all task subsets, limiting their practical\napplication in real industrial scenarios.\nInsufficient capacity for complex signal processing: The multidimensional sensor signals generated during\nequipment operation exhibit not only significant temporal dependencies but also complex spatial correlations.\nTraditional small-scale deep learning models (such as LSTM, GRU, etc.), while effective in some specific tasks,\noften show insufficient modeming capacity when dealing with high-dimensional, multi-task, and multi-condition\nsensor signals, making it difficult to extract key features from the equipment degradation process, thus limiting\nthe improvement of prediction accuracy.\nLimited transfer learning application: Existing RUL prediction methods lack flexible transfer learning capabilities\nwhen dealing with cross-task environments. When facing a new task or operating condition, models often need to\nbe retrained from scratch, unable to effectively utilize knowledge learned in the source domain. This leads to poor\nmodel adaptability in practical industrial applications, with low training efficiency and heavy reliance on large\namounts of target domain data, thereby increasing data collection costs and computational resource consumption.\nBased on these problems, we propose the following key research questions:\nHow to design a unified and robust model framework: We aim to design a unified industrial signal processing and\nRUL prediction framework based on large language models (LLMs), capable of using the same sliding window"}, {"title": "", "content": "length and sensor signals across all data subsets, thereby improving the model's generalization and consistency\nacross multiple tasks and operating conditions, while reducing dependency on human expertise and complex\nhyperparameter adjustments.\nHow to leverage the transfer learning capabilities of large models for efficient adaptation: In addition to improving\nprediction accuracy in industrial RUL prediction tasks by harnessing the powerful feature extraction and reasoning\ncapabilities of large language models pre-trained on massive corpora, we also aim to fully explore the transfer\nlearning capabilities of large language models. By utilizing only a small amount of target domain data, we hope\nto achieve rapid fine-tuning and efficient model adaptation, further improving training efficiency and prediction\naccuracy while reducing the burden of data annotation and computational resources.\nNext, we will detail the technical specifics of the multidimensional signal processing framework proposed in this\npaper and demonstrate how innovative methods can be used to solve the above problems."}, {"title": "Methodology", "content": "1. Data Pre-processing\nIn this study, to ensure that the model exhibits good generalizability across different data subsets, we applied the\nsame preprocessing process to the four subsets (FD001, FD002, FD003, FD004) of the CMAPSS dataset. This\npre-processing process includes generating remaining useful life (RUL) labels, signal smoothing, and\nnormalization steps. Unlike the existing SOTA methods, which typically use different sliding window lengths and\nselect only a portion of the sensor signals for different subsets, our method unifies the processing of all subsets\nand uses all available sensor signals, significantly improving the model's generalizability. The specific processing\nsteps are as follows:\na. Generating Remaining Useful Life (RUL) Labels\nFor the regression task, we need to generate RUL labels for the training data of each piece of equipment.\nSpecifically, the RUL is calculated as follows:\n$RUL(i) = max(T \u2013 t(i),0)$\nWhere T represents the maximum operational cycle of the equipment, and t(i) represents the current time cycle.\nWe cap the maximum value of the RUL at 120 to limit extreme cases and normalize the RUL to the range [0,1].\nThe formula is as follows:\n$RUL (i) = \\frac{min(RUL (i),120)}{120}$\nThis normalization ensures that the RUL range is fixed, facilitating model training and optimization.\nb. Normalization Based on Operating Conditions\nThe operating conditions of the equipment can affect sensor readings, so applying uniform standardization to all\ndata may introduce bias. To address this issue, we grouped the data based on different operating conditions of the\nequipment and performed normalization on the sensor data within each group. Specifically, for each sensor signal\nsj(i) within a group, we used Min-Max normalization to scale it to the range [0,1]. The normalization formula is\nas follows:\n$s(i) = \\frac{s_j(i) \u2013 min(s_j)}{max(s_j) \u2013 min(s_j)}$\nWhere s;(i) represents the reading of sensor s; at the i-th time cycle. $min(s_j)$ and $max(s_j)$ represent the\nminimum and maximum values of the sensor signal under the current operating conditions, respectively. This\ngrouped normalization ensures that the data under different operating conditions remain comparable.\nc. Exponential Smoothing\nTo reduce noise in the sensor signals, we applied exponential smoothing to the time-series data. Exponential\nsmoothing is a commonly used time-series processing method that assigns certain weights to past time steps in"}, {"title": "", "content": "order to reduce the impact of noise on the current data. For each sensor signal sj, its smoothed value $s_j^{smooth}(i)$ is\ncalculated using the following recursive formula:\n$s_j^{smooth} (i) = a \\cdot s_j (i) + (1 \u2212 a) \\cdot s_j^{smooth} (i \u2013 1)$\nWhere a is the smoothing parameter, ranging from [0,1], used to control the balance between the current time\nstep and past time steps. A larger a assigns greater weight to the current time step, while a smaller a more\nsmoothly considers the past historical data. Through this smoothing process, we effectively reduced random\nfluctuations in the data, enhancing the stability of model training.\nd. Sliding Window Processing\nIn time-series tasks, sliding window processing is a commonly used technique to convert raw time-series data into\nfixed-length windows suitable for model input. Specifically, we used the same sliding window length across all\nsubsets, slicing the sensor data into fixed time steps. Suppose the sliding window length is L, the sensor data for\neach piece of equipment is converted into the following input sequence:\n$X_t = [S_1(t), S_2(t), ..., S_{21} (t); S_1 (t + 1), S_2(t + 1), ..., S_{21} (t + 1); ... ; s_1 (t + L \u2212 1), s_2(t + L \u2212 1), ..., S_{21} (t + L\n- 1)]$\nWhere t represents the current time step, and L represents the sliding window length. The data within each sliding\nwindow is used as input to the model for predicting the RUL corresponding to that window. Unlike previous\nSOTA methods, which select different sliding window lengths for each subset, we used the same sliding window\nlength across all subsets, further improving the model's generalizability.\nThrough the unified pre-processing steps described above, we ensured the consistency of the model across\ndifferent subsets, avoiding the performance fluctuations caused by inconsistencies in sliding window lengths and\nsensor selection in existing SOTA methods."}, {"title": "2. Large Model-Based Regression Framework", "content": "In this study, the LLM used in our proposed LRM is specifically the GPT-2 medium pre-trained model, which\ncaptures complex dependencies within time series through multi-layer self-attention mechanisms and multi-layer\nTransformer structures. To accommodate the particularities of industrial sensor data, we also added global pooling\nand additional attention mechanisms after the GPT-2 model. This allows the model to simultaneously focus on\nboth the overall temporal trends and local key information extracted by the GPT-2 model, thus enabling accurate\npredictions of the remaining useful life (RUL) of industrial systems. The structure and computational process of\nthe model are described in detail below.\na. Input Embedding and Linear Mapping\nAfter the unified data pre-processing described in the previous section, the model's input is multidimensional time-\nseries data with the shape $X \u2208 R^{B\u00d7L\u00d7C}$, where B represents the batch size, L represents the length of the time\nseries (i.e., the sliding window length), and C represents the dimension of the sensor signals (i.e., the number of\nsensors). However, the input dimension for the GPT-2 medium model is fixed at 1024-dimensional vectors, while\nthe dimensionality of industrial sensor data is generally lower (for example, in the CMAPSS dataset, there are 21\nsensor values per time step). To accommodate the input requirements of the GPT-2 medium model, we first need\nto perform a linear mapping of the raw sensor data, embedding it into the 1024-dimensional vector space required\nby GPT-2. The specific linear mapping process is as follows:\n$X' = W_eX + b_e$\nWhere $W_e \u2208 R^{C\u00d71024}$ is the weight matrix for the linear mapping, $b_e \u2208 R^{1024}$ is the bias term, and\n$X' \u2208 R^{B\u00d7L\u00d71024}$ is the data after the linear mapping. Through this linear mapping, the model can transform the\nmultidimensional sensor data at each time step into high-dimensional embedded vectors, ensuring that the input\nis compatible with the requirements of the GPT-2 medium model.\nb. GPT-2 Structure\nThe GPT-2 (Generative Pre-trained Transformer 2) model is an autoregressive language model based on the\nTransformer architecture, proposed by OpenAI. The core idea of GPT-2 is to model sequential data through a"}, {"title": "", "content": "self-attention mechanism, which can effectively capture long-range dependencies. The GPT-2 model is composed\nof multiple Transformer decoders stacked together, with each layer containing two modules:\n(1) Self-Attention Layer: Used to compute the dependencies between each time step in the sequence.\n(2) Feed-Forward Network (FFN): Used to independently apply non-linear transformations to the features at each\ntime step.\nThe input to GPT-2 is a sequence of embedded vectors $X' \u2208 R^{B\u00d7L\u00d71024}$, which, after being processed through\nmultiple Transformer layers, outputs the contextual representation for each time step \u0397 \u2208 $R^{B\u00d7L\u00d71024}$. The self-\nattention mechanism in GPT-2 allows the model to dynamically focus on other time steps in the sequence when\ncomputing the representation for each time step. The core operation of the self-attention mechanism involves\ncalculating attention weights using queries (Query), keys (Key), and values (Value), and then weighting and\nsumming the representations of all other time steps based on these weights. The specific formula is as follows:\n$Attention (Q, K, V) = softmax(\\frac{QKT}{\\sqrt{d_k}})V$\nWhere Q = WQX',K = WKX',V = WyX' are the query, key, and value vectors generated from the input X'\nthrough different linear transformations. $W_Q \u2208 R^{1024\u00d7d_k}$\u3001$W_k \u2208 R^{1024\u00d7d_k}$, $W_V \u2208 R^{1024\u00d7d_v}$ are trainable weight\nmatrices, where $d_k$ and $d_v$ represent the dimensions of the key and value vectors, respectively, and are used to\nscale the dot-product results. Through this self-attention mechanism, GPT-2 can dynamically compute the\ndependencies between each time step and other time steps, thus capturing complex patterns in the time series.\nTo enhance the model's representation capability, GPT-2 also employs a multi-head attention mechanism. This\nmechanism maps the query, key, and value vectors into multiple subspaces and independently computes attention\nwithin each subspace. Specifically, the model maps the query, key, and value vectors into h subspaces, where\neach subspace has dimensions of $d_k/h$ and $d_v/h$. Attention is then computed independently in each subspace,\nand finally, the outputs of all attention heads are concatenated:\n$MultiHead (Q, K, V) = Concat (head_1, head_2, ..., head_h) Wo$\nWhere $head_i = Attention (Q_i, K_i, V_i)$, $Wo \u2208 R^{1024\u00d71024}$It is the output weight matrix. The multi-head attention\nmechanism allows the model to capture multiple patterns in the time series across different subspaces, thereby\nimproving the model's expressive capacity.\nIn each Transformer layer, following the self-attention mechanism is a two-layer feed-forward network (FFN),\nwhich applies non-linear transformations to the features at each time step. The FFN is computed as follows:\n$FFN (H_i) = W_2 \\cdot ReLU (W_1 \\cdot H_i + b_1) + b_2$\nWhere $W_1 \u2208 R^{1024\u00d7d_{ff}}$, $W_2 \u2208 R^{d_{ff}\u00d71024}$ are the trainable weight matrices, and $d_{ff}$ is the hidden dimension of\nthe feed-forward network. Additionally, GPT-2 uses residual connections and layer normalization after each self-\nattention layer and feed-forward network to accelerate model training and stabilize gradients.\nAfter processing through multiple Transformer layers, GPT-2 outputs the contextual representation for each time\nstep, \u0397 \u2208 $R^{B\u00d7L\u00d71024}$. These representations contain the global dependency information of each time step in the\ntime series, providing strong support for subsequent regression tasks."}, {"title": "c. Global Pooling and Attention Mechanism", "content": "After obtaining the output representation H from GPT-2, we further aggregate the features of the time series. To\nextract the global information from the sequential data, we first apply global average pooling over the hidden\nstates of all time steps:\n$H = \\frac{1}{L}\\sum_{i=1}^{L}H_i$"}, {"title": "", "content": "Where $H_i$ represents the hidden state at the i-th time step, and the pooled result \u0124 \u2208 $R^{B\u00d71024}$ represents the global\naverage information of the time series. In addition to average pooling, we also applied an attention mechanism to\nfurther highlight the time steps that are more important for RUL prediction. Through linear transformation, we\ncalculate the attention weights for each time step:\n$A_i = \\frac{exp(wt H_i)}{\\sum_{j=1}^{L} exp (wH_j)}$\nWhere w \u2208 $R^{1024}$ is a trainable parameter, and a\u00a1 represents the attention weight for the i-th time step. We then\nuse these weights to compute a weighted sum of the hidden states at each time step, resulting in the attention-\nweighted output:\n$H_{att} = \\sum_{i=1}^{L}a_i H_i$\nBy combining the representations from average pooling and attention-weighted outputs, we are able to capture\nboth the global trends and local key information in the time series. The final fused representation is:\n$H_{final} = H + H_{att}$\nd. Output Layer and Regression Prediction\nThe fused representation $H_{final}$ is fed into a multilayer perceptron (MLP) for regression prediction. The MLP\nconsists of several fully connected layers and activation functions. The specific structure is as follows:\n$Output = W_3\\cdot \u03c3(W_2\\cdot \u03c3(W_1 \\cdot H_{final} + b_1) + b_2) + b_3$\nWhere $W_1 \u2208 R^{1024\u00d750}$, $W_2 \u2208 R^{50\u00d710}$, and $W_3 \u2208 R^{10\u00d71}$. \u03c3(\u00b7) is the ReLU activation function. Finally, the model\noutputs a scalar value, which serves as the predicted remaining useful life (RUL) for the time series."}, {"title": "3. Transfer Learning Strategy for Remaining Useful Life Prediction", "content": "To make the LRM model more sample efficient and improve its generalization ability, we explored the transfer\nlearning capabilities of large language models in detail. The core idea of transfer learning is to utilize the weights\nof a model trained on a source domain and transfer them to a target domain, thereby achieving better predictive\nperformance on the target domain. We adopted a \"partial layer freezing\" transfer learning strategy, meaning that\nafter pre-training the large model on the source domain, most layers of the GPT-2 medium model are frozen, with\nonly a few layers open for fine-tuning. This strategy aims to reduce training time on the target domain while\npreserving the general features learned in the source domain.\na. Principles of Transfer Learning\nThe basic idea of transfer learning can be described using the following mathematical notation. On the source\ndomain Ds, the model learns weights \u03b8s, and we aim to transfer these weights to the target domain Dt to improve\npredictive performance on the target domain. The optimization objective for the source domain can be expressed\nas minimizing the loss function $L_5(\u03b8)$ on the source domain:\n$0 = arg \\underset{\u03b8}{min}L_s(\u03b8)$\nWhere $\\hat{\u03b8_s}$ represents the optimal weights obtained from training on the source domain. On the target domain, we\ndo not intend to train the entire model from scratch; instead, we use the weights $\\hat{\u03b8_s}$ from the source domain as the\ninitial weights. By freezing certain layers, we fix part of the model's weights and only fine-tune a few unfrozen\nlayers, \u03b8f. The fine-tuning optimization objective on the target domain is:\n$0 = arg \\underset{\u03b8}{min} L_t(\u03b8_f U \u03b8)$\\\nWhere $L_t (\u03b8_fU\u03b8)$ is the loss function for the target domain, with the fine-tuned weights \u03b8f, represents the few\nlayers we optimize on the target domain. In this way, the model can retain the general features learned from the\nsource domain while quickly adjusting to the new task using the limited data from the target domain."}, {"title": "b. Model Fine-Tuning Process", "content": "During the transfer learning process, we used a GPT-2 medium model pre-trained on the source domain as the\ncore structure (this model was also pre-trained on a large corpus before training on the source domain). The GPT-\n2 model is composed of multiple self-attention layers and feed-forward networks, with each layer extracting\nfeatures of different complexity. To retain the general features learned from the source domain, we froze the first\n20 layers of the GPT-2 model, only allowing the last few layers to be fine-tuned. Specifically, assuming the\nweights of the GPT-2 model are $\u0398_{GPT\u22122}$ = {$\u03b8_1$, $\u03b8_2$, ..., $\u03b8_{24}$}, where \u03b8\u00a1 represents the weights of the i-th layer, we\nfreeze the weights of the first 20 layers:\n$\u03b8_i (for i = 1,2,...,20) are fixed: \\frac{dL_t}{d\u03b8_i}= 0$\nThat is, for i = 1,2, ...,20, the weights of these layers do not participate in the optimization on the target domain.\nWe only update the last four layers $\u03b8_{21}$, $\u03b8_{22}$, $\u03b8_{23}$, $\u03b8_{24}$:\n$0; (for j = 21,22,23,24) are updated:\\frac{dL_t}{d\u03b8_j}\u22600$\nIn this way, the model can quickly adapt to the new task on the target domain without needing to train the entire\nmodel from scratch. This freezing strategy not only reduces computational overhead but also helps avoid\noverfitting on the target domain. To ensure effective fine-tuning on the target domain, we only optimize a few\nunfrozen layers. Specifically, the trainable parameters of the model are represented by the following formula:\n$\u03b8_f = {\u03b8_{21}, \u03b8_{22}, \u03b8_{23}, \u03b8_{24}} \u222a output$\nWhere $\u03b8_{output}$ represents the parameters of the model's output layer. The output layer usually needs to be retrained\naccording to the specific task in the target domain, as the label distribution in the target domain may differ from\nthat of the source domain. We use the Mean Squared Error (MSE) loss function as the loss function for the target\ndomain:\n$L_t(\u03b8_f) = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y_i})^2$\nWhere $y_i$ is the ground truth label in the target domain, $\u0177_i$ is the model's predicted value, and N is the number of\nsamples in the target domain. To optimize these parameters, we use the Adam optimizer, with the update step as\nfollows:\n$\u03b8_f \u2190 \u03b8_f \u2212 \u03b7 \\cdot \u2207_{\u03b8f}L_t(\u03b8_f)$\nWhere \u03b7 is the learning rate, and $\u2207_{\u03b8f}L_t (\u03b8_f)$ is the gradient of the loss function with respect to the trainable\nparameters. Choosing an appropriate learning rate is crucial during the transfer learning process. We set a\nrelatively small learning rate \u03b7 to ensure that the fine-tuning process does not disrupt the features learned from\nthe source domain, while using weight decay to prevent overfitting. Specifically, a weight decay term $\\lambda$ is added\nto the loss function:\n$\\pounds_{t} = \\frac{1}{N} \\sum_{i=1}^{N}(y_{i} - \\hat{y}_{i})^{2} + \\lambda ||\\theta||^{2}$\nCompared to training a model from scratch, transfer learning offers significant advantages:\n(1) Significant reduction in training time: Since the parameters of most layers are not updated, the model only\nrequires fine-tuning a few layers, greatly reducing the computational overhead.\n(2) Improved accuracy: Transfer learning can leverage the features learned from the source domain and further\noptimize them on the target domain, ultimately achieving higher prediction accuracy than models trained directly\nfrom scratch."}, {"title": "", "content": "(3) Avoiding overfitting: By freezing most layers, the model can avoid overfitting when there is limited data in\nthe target domain, leading to more robust performance on the test set.\nExperimental results show that the strategy of freezing most layers and fine-tuning only a few layers effectively\npreserves the general features learned by the LRM model in the source domain, while quickly adapting to tasks\nin different target domains. The proposed transfer learning framework for the LRM demonstrates the broad\napplicability of large language models in industrial prediction tasks and shows potential for efficient learning\nwhen data is limited."}, {"title": "Experiments", "content": "1. Data Source\nIn this study, we used the C-MAPSS (Commercial Modular Aero-Propulsion System Simulation) dataset, which\nwas developed by NASA to simulate the performance degradation of aero-turbine engines. Thanks to its detailed\nrecording of complex system degradation processes and the provision of multi-dimensional sensor data, this\ndataset is widely used in Remaining Useful Life (RUL) prediction research. It simulates the operation of turbine\nengines under different operating conditions, providing rich time-series data for health monitoring and predictive\ntasks.\nThe C-MAPSS dataset uses the Commercial Modular Aero-Propulsion System Simulation to simulate the\ndegradation process of turbine engines and contains four subsets: FD001, FD002, FD003, and FD004. The data\nstructure of each subset is the same, recording the operation of engines under different working conditions and\nfault modes. Each record consists of 3 operational settings (e.g., fuel flow rate, pressure, etc.), 21 sensor signals,\nas well as engine ID, cycle count, and other information. The sensor signals monitor multiple components of the\nengine, including temperature, pressure, and rotational speed. The training set records the entire process from\nnormal operation to engine failure, while the test set is truncated at a certain unknown point before failure,\nrequiring the model to predict the Remaining Useful Life (RUL) of the engine. Since the sensor signals have\ndifferent units and scales, and some signals may contain noise or external interference, pre-processing is usually\nrequired before using the data. The specific details are as follows:"}, {"title": "2. Experimental Procedure"}]}