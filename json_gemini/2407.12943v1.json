{"title": "HALU-J: Critique-Based Hallucination Judge", "authors": ["Binjie Wang", "Steffi Chern", "Ethan Chern", "Pengfei Liu"], "abstract": "Large language models (LLMs) frequently generate non-factual content, known as hallucinations. Existing retrieval-augmented-based hallucination detection approaches typically address this by framing it as a classification task, evaluating hallucinations based on their consistency with retrieved evidence. However, this approach usually lacks detailed explanations for these evaluations and does not assess the reliability of these explanations. Furthermore, deficiencies in retrieval systems can lead to irrelevant or partially relevant evidence retrieval, impairing the detection process. Moreover, while real-world hallucination detection requires analyzing multiple pieces of evidence, current systems usually treat all evidence uniformly without considering its relevance to the content. To address these challenges, we introduce HALU-J, a critique-based hallucination judge with 7 billion parameters. HALU-J enhances hallucination detection by selecting pertinent evidence and providing detailed critiques. Our experiments indicate that HALU-J outperforms GPT-40 in multiple-evidence hallucination detection and matches its capability in critique generation and evidence selection. We also introduce ME-FEVER, a new dataset designed for multiple-evidence hallucination detection. Our code and dataset can be found in https://github.com/GAIR-NLP/factool.", "sections": [{"title": "1 Introduction", "content": "The propensity of Large Language Models (LLMs) (Bubeck et al., 2023; Team et al., 2023) to hallucinate presents significant challenges to their reliability and widespread implementation in real-world applications (Ji et al., 2023; Zhang et al., 2023). Current retrieval-based approaches for identifying hallucinations (Min et al., 2023; Chern et al., 2023) first gather pieces of evidence, which are then used to determine whether the content contains hallucination. Although these methods are somewhat effective, they encounter several major issues: (i) Lack of Detailed Explanations: These techniques often lack detailed explanations for their detection results and do not assess the reliability of such explanations. This absence of interpretability diminishes the practical value of these detectors, especially in high-stakes situations. For instance, in medical settings, simply alerting a doctor to factual errors in generated patient information without providing evidence-backed explanations can erode trust in the system's outputs (Xie et al., 2024). (ii) Deficiencies in Retrieval: Many existing tools for detecting hallucinations depend heavily on LLMs (Niu et al., 2024), which can be misled by irrelevant data gathered by flawed retrieval systems (Shi et al., 2023; Wang et al., 2023b), leading to incorrect assessments. (iii) Uniform Treatment of Evidence: In real-world applications, substantiating claims often requires multiple evidence sources to ensure reliability and validity (Kamoi et al., 2023; Guo et al., 2022). This highlights the importance of multiple-evidence hallucination detection - performing hallucination detection on a claim against multiple retrieved evidence. However, most hallucination detection framework treat all evidence uniformly, failing to differentiate between various types of sources. These challenges underscore the urgent need for a more reliable hallucination detection system, one that excels in handling multiple pieces of evidence and produces high-quality critiques. This improvement would significantly enhance the practical utility of hallucination detectors in real-world applications.\nTo address these challenges, we propose HALU-J, an open-source, critique-based hallucination judge capable of handling complex, multiple-evidence scenarios (an overview of our framework is shown in Figure 1). This system excels in generating high-quality critiques, categorizing evidence effectively, and integrating all relevant information to deliver precise hallucination detection. At the heart of HALU-J are three key technical developments: Firstly, we introduce the ME-FEVER, a pioneering dataset specifically designed for more reliable hallucination detection. Based on the foundational FEVER dataset (Thorne et al., 2018), ME-FEVER includes 3,901 instances that feature three types of evidence: completely irrelevant, partially irrelevant, and highly relevant. The dataset is split into 2,663 training and 1,238 testing instances, providing a solid base for both training and evaluation purposes. Secondly, we enhance HALU-J with preference-based learning method (Rafailov et al., 2023) to boost the system's ability to identify and prioritize relevant evidence, further enhancing the quality of the generated critiques. Lastly, our evaluation strategy incorporates a comprehensive framework that assesses both the answer-level and critique-level performance of HALU-J. This allows us to measure how effectively the system filters relevant evidence and produces quality critiques.\nOur experiments demonstrate that HALU-J outperforms all baseline models including GPT-40 under multiple-evidence hallucination detection setting and has close performance under single-evidence hallucination detection setting, as shown in \u00a7 6.3. Additionally, the critiques generated by HALU-J achieve evaluation scores close to those of GPT-40 and demonstrate the highest evidence-matching rate.\nTo summarize, our contributions are as follows:\n\u2022 We create ME-FEVER, a multiple-evidence hallucination detection dataset based on FEVER that simulates real-world situations, offering a solid foundation for both training and evaluating systems on multi-evidence hallucination detection.\n\u2022 We establish a novel multiple-evidence hallucination detection workflow featuring evidence categorization, evidence reordering, evidence-by-evidence analysis, and information aggregation to enable the hallucination detection system to filter unrelated contexts and generate a reliable critique in the end.\n\u2022 We introduce HALU-J, an open-source, critique-based hallucination detection model with 7 billion parameters capable of providing fine-grained critiques and filtering out unrelated information during hallucination detection."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Critique Generation with LLMs", "content": "The versatile generative capabilities of LLMs enable LLM-as-evaluators (Zheng et al., 2024; Dubois et al., 2024) to generate natural language descriptions to evaluate the quality of model-generated content (Saunders et al., 2022; Sun et al., 2024). While previous works on LLM evaluation have explored different methods for providing more reliable evaluations and critiques (Chiang et al., 2023; Wang et al., 2023a; Li et al., 2023a; Sun et al., 2024; Chern et al., 2024), none have focused on offering critiques with filter retrieval-augmentation for hallucination detection. We propose HALU-J to address this gap."}, {"title": "2.2 Retrieval-Augmented Hallucination Detection", "content": "Earlier works on LLM-based retrieval-augmented hallucination detection systems (Min et al., 2023; Chern et al., 2023) focus on establishing a fine-grained framework for claim-level hallucination detection that leverages external knowledge or databases. More recent works on hallucination detection systems enable editing (Mishra et al., 2024), improving efficiency (Tang et al., 2024), and facilitating long-form fact-checking (Wei et al., 2024). Our work continues the effort to enhance retrieval-augmented hallucination detection systems by providing enhanced critique with filter retrieval augmentation."}, {"title": "3 Preliminaries", "content": "In this section, we define key terms and introduce our framework for retrieval-augmented hallucination detection."}, {"title": "3.1 Key Terms", "content": "We first define some key terms that are used throughout our paper.\nPrompt (p) A query or instruction that users send to LLMs.\nResponse (r) A piece of text (usually in long-form) generated by the LLMs.\nClaim (c) A verifiable statement extracted from the response.\nLabel (l) An answer that determines whether or not a claim c is hallucinated, which can be True (no hallucination), False (with hallucination), or Neutral.\nEvidence (e) The available information or databases that could potentially help verify whether a claim c is hallucinated or not.\nCritique (cr) A natural language description for assessing whether a claim c is hallucinated or not."}, {"title": "3.2 Retrieval-Augmented Hallucination Detection Systems", "content": "Previous retrieval-augmented hallucination detection frameworks (Min et al., 2023; Chern et al., 2023; Wei et al., 2024) typically consist of three primary components:\n\u2022 Claim Extraction: Extracting fine-grained verifiable claims from a given response r.\n\u2022 Evidence Collection: Utilizing retrieval tools or online search engines to retrieve external knowledge as evidence e.\n\u2022 Claim Verification: Utilizing the evidence to verify whether a claim c is hallucinated or not."}, {"title": "4 ME-FEVER: A Multiple-Evidence Hallucination Detection Dataset", "content": ""}, {"title": "4.1 Motivation", "content": "To benchmark a hallucination detection system, one approach is to use standard natural language inference (NLI) datasets (Thorne et al., 2018; Nie et al., 2019) as test data. These datasets typically consist of a claim, a label, and a single piece of evidence for each sample. However, since each test sample includes only one piece of evidence, this is much simpler than the multiple-evidence hallucination detection scenarios that are often encountered in real-world applications. To address this limitation, we generate different types of evidence based on the FEVER dataset (Thorne et al., 2018). By synthesizing them with the original FEVER data, we create ME-FEVER, a more challenging dataset used to train and benchmark hallucinations in models under multiple-evidence scenarios."}, {"title": "4.2 Data Curation", "content": "We prompt GPT-4-Turbo\u00b9 to generate multiple pieces of evidence based on the original FEVER evidence for each instance (detailed prompt is shown in Appendix A). The generated evidence is categorized into three predefined categories as follows:\nCompletely Irrelevant Evidence (E\u00ba) This type of evidence is entirely unrelated to the claims presented and should be disregarded during the hallucination detection process. In real-world scenarios, it may appear as a result of poorly formulated queries during retrieval, a lack of pertinent information in the knowledge base, or extensive evidence that includes unnecessary details. We randomly select two pieces of evidence from the FEVER dataset as completely irrelevant evidence. We manually compare them with the claims to ensure irrelevance.\nPartial Irrelevant Evidence (EP) This type of evidence might appear related to the claim in subject matter or format, yet contribute minimally to the verification process. While this type of evidence may seem relevant, it often has limited impact on determining whether or not the claim is hallucinated. HALU-J is designed to extract useful portions from this type of evidence and disregard the irrelevant parts. Therefore, we ask GPT-4-Turbo to create four separate paragraphs that match the subjects mentioned in the claims. These paragraphs neither contradict existing known facts nor specifically support or refute the claims. Since the paragraphs generated by GPT-4-Turbo are often short, we use GPT-3.5-Turbo\u00b2 to expand these paragraphs to approximately 150 words.\nHighly Related Evidence (E\") This type of evidence is highly related to a claim. Note that each instance in FEVER contains only one piece of highly related evidence. To simulate more complex scenarios, we ask GPT-4-Turbo to generate three additional paragraphs of highly related evidence. These paragraphs are intended to be misleading: they tend to mislead the hallucination detector into making an incorrect judgment about the claim, but not directly supporting or refuting whether the claim is hallucinated. These misleading paragraphs are included in the dataset to help build a more challenging-\""}, {"title": "5 HALU-J", "content": "We introduce the hallucination detection framework for HALU-J that mainly focuses on generating high-quality critiques and accurate prediction labels given a claim and a set of evidence more than one piece. Here, we outline our framework based on the key terms and concepts mentioned in \u00a73.1, \u00a73.2, and \u00a74.2."}, {"title": "5.1 Problem Formulation", "content": "We use evidence E retrieved from external sources to effectively verify whether or not a claim c is hallucinated. The main problem is to build a hallucination detector that:\n1. Filters out irrelevant evidence in E.\n2. Provides a detailed critique of the claim c based on the filtered evidence.\nThis process involves four distinct steps, detailed below."}, {"title": "5.2 Framework", "content": ""}, {"title": "5.2.1 Step-I: Evidence Categorization", "content": "First, HALU-J should systematically review all pieces of evidence and categorize each into one of the three predefined categories mentioned in \u00a74.2.\nWe denote the n pieces of evidence in E as:\n\\(E = \\{e^{(t_1)}, e^{(t_2)}, ..., e^{(t_n)}\\}\\) (1)\nwhere \\(t_i \\in \\{o, p, r\\}\\) and \\(i \\in \\{1, 2, 3, . . ., n\\}\\)."}, {"title": "5.2.2 Step-II: Evidence Reordering", "content": "HALU-J should group the same types of evidence together to form different evidence groups, arranged in the following order: Completely Irrelevant Evidence, Partially Irrelevant Evidence, and Highly Related Evidence. By ordering these evidence, this ensures clarity and organization, which makes the detection process more manageable afterwards. Additionally, this standardized approach enhances accuracy and consistency by preventing models from overlooking any extracted evidence, thereby minimizing the likelihood of mistakes. This also allows models to think in a more systematic manner. We denote the evidence order as follows:\n\\(E = \\begin{cases}\ne^{o_1}, e^{o_2},..., e^{o_{n_o}} \\\\\ne^{p_1}, e^{p_2},..., e^{p_{n_p}} \\\\\ne^{r_1}, e^{r_2},..., e^{r_{n_r}}\n\\end{cases}\\)\nwhere \\(n_o, n_p\\), and \\(n_r\\) denotes the number of evidence in \\(E^o, E^p\\), and \\(E^r\\), respectively."}, {"title": "5.2.3 Step-III: Evidence Analysis", "content": "Our framework involves analyzing the relationships among the evidence and conducting a detailed analysis of how they each relate to the claim. HALU-J should analyze and reason through each piece of evidence in a step-by-step manner. The analysis must meet the following requirements:\n1. The analysis disregards completely irrelevant evidence.\n2. The analysis extracts the relevant parts in the partially irrelevant evidence, and discard the rest.\n3. The analysis clarifies how the helpful evidence (under highly-related evidence) support or refute the claim.\n4. If the evidence is identified as misleading, the analysis explains the relationship between the misleading evidence and the claims."}, {"title": "5.2.4 Step-IV: Aggregation and Critique Generation", "content": "In this step, HALU-J summarizes all the analysis and makes a conclusive determination on whether the given claim is true, false, or neutral. Then HALU-J carefully checks whether a claim is hallucinated by evaluating whether it is supported by the most direct and relevant evidence available. This step is crucial as it ensures that all available pieces of evidence have been considered, allowing the detector to synthesize a coherent, well-founded, and reliable critique in the end.\nThe generated critique includes detailed information such as the category of each piece of evidence, comprehensive analysis for each piece of evidence, and a concise conclusion judging the claim's hallucinations. This information aids in producing more accurate label prediction, which has never been used in past verification processes.\nTo summarize, the final critique cr and a corresponding label l is generated using analysis A. Overall, the process involves taking (c, E) as input and generates the corresponding (cr, l). An example critique is shown in Figure 2."}, {"title": "5.3 Fine-tuning", "content": ""}, {"title": "5.3.1 Fine-tuning Data", "content": "We use the following two types of data by taking a claim-evidence pair (c, E) as input and a critique-label pair (cr, l) as output for fine-tuning.\nMultiple-Evidence Setting We use the training set in ME-FEVER for fine-tuning in multiple-evidence setting. We keep the information on the type of each piece of evidence and clarifications of each misleading evidence during the evidence generation process detailed in \u00a74.2. We then follow our framework through two stages: synthesization and reformatting (prompts used can be found in Appendix A).\nFor each instance, we prompt GPT-4-Turbo to synthesize a \"golden\" reasoning. This reasoning includes a detailed evidence-by-evidence analysis"}, {"title": "5.3.2 Fine-tuning Procedure", "content": "Following the above process, we obtain a set of prompt-response pairs under the multiple-evidence setting. The prompts include claims from FEVER and newly generated pieces of evidence, while the response contains the \"golden\" critiques and the corresponding labels for each claim.\nBy integrating data from both single-evidence and multiple-evidence settings, we allow HALU-J to seamlessly toggle between different hallucination detection scenarios, whether there is one piece of evidence or multiple. To reduce positional bias, we randomly shuffle the evidence in the multiple-evidence setting.\nFine-tuning with DPO To enhance the quality of critiques and improve the accuracy of the predicted label under the multiple-evidence setting, we further fine-tune HALU-J (w/o DPO) with DPO (Rafailov et al., 2023) to obtain HALU-J."}, {"title": "6 Experiments", "content": ""}, {"title": "6.1 Experimental Setup", "content": "Models We use the models Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), GPT-3.5-Turbo-0301, GPT-40-2024-05-13, Llama-2-13b-Chat (Touvron et al., 2023), Llama-3-8B-Instruct, and Qwen1.5-7B-Chat (Team, 2024) for our baseline experiments.\nFine-tuning Details For supervised fine-tuning, we obtain HALU-J by fine-tuning Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) with DeepSpeed (Rasley et al., 2020) library, Zero Redundancy Optimizer (ZERO) (Rajbhandari et al., 2020; Ren et al., 2021) Stage 3, gradient-checkpointing (Chen et al., 2016), and FlashAttention (Dao et al., 2022; Dao, 2023) on 4 NVIDIA A100 GPUs. We use the bfloat16 (BF16) and tfloat32 (TF32) mix computation precision options to optimize efficiency. HALU-J is trained for 20 epochs. We use AdamW (Loshchilov and Hutter, 2017) as our optimizer with \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.95 and weight decay of 0.1. We use a peak learning rate of 1e-5 with 10 warm-up steps, setting cosine learning rate decay to 0, a batch size of 16, and a maximum sequence length of 8,192. The loss is calculated only on the output end.\nFor DPO, we conducted inference 30 times using HALU-J on the DPO training set with multiple-evidence data, with sampling parameters set to temperature = 1 and top_p = 0.9. For each DPO training instance, we select an answer with correctly predicted label as the chosen answer and an"}, {"title": "6.2 Evaluation Setup", "content": "Evaluating Accuracy on Hallucination Detection Tasks We use 1,000 instances each from ANLI (Nie et al., 2020), WANLI (Liu et al., 2022), and HaluEval (Li et al., 2023b), along with 233 instances from KBQA in FacTool (Chern et al., 2023) to evaluate the models' performance under the single evidence setting. On the other hand, we use the testing set (1,238 instances) from ME-FEVER to evaluate models' performance under the multiple-evidence scenario. We prompt the models to respond in a Python dictionary format. The dictionary should have two keys: \"reasoning\" and \"factuality\". They correspond to a critique and a label, respectively. Responses that do not follow the expected format and cannot be properly interpreted are considered incorrect. Response that has the same label as the reference label (either true, false, or neutral) is considered correct. Figure 2 showcases a comparison case between the critique generated by HALU-J and Mistral-7b under ME-FEVER's test set (complete example in Appendix B). HALU-J correctly classifies all evidence for their corresponding categories and provides detailed reasoning that allows it to predict the label correctly.\nCritique Evaluation by GPT-4-Turbo We utilize GPT-4-Turbo to rate the generated critiques under the multiple-evidence setting on a scale from 1 to 100 using a carefully designed prompt (see Appendix A for the full prompt). This prompt asks the model to first output a step-by-step reasoning process before providing a final score. Additionally, we conducted a study to evaluate the agreement rate between human annotators (authors of the paper) and GPT-4-Turbo. The human annotators asked to score the same set of critiques as GPT-4-Turbo, and we calculated the Pearson correlation to confirm the reliability of using GPT-4-Turbo's evaluations.\nEvidence Matching Evaluation The ability to distinguish between relevant and irrelevant evidence is crucial for models to generate reliable critiques and thus produce more accurate predictions of labels in multiple-evidence settings. We thus measure the accuracy of a model correctly matching an evidence to its corresponding category in their responses using the ME-FEVER dataset."}, {"title": "6.3 Results and Discussions", "content": ""}, {"title": "6.3.1 Results", "content": "Hallucination Detection Accuracy From Table 1, we see that our model, HALU-J, significantly outperforms all other baseline models, including GPT-40, under the multiple-evidence setting. Under single evidence setting, HALU-J outperforms all models on FEVER's test data and outperforms models other than GPT-40 in other test datasets. We notice that the accuracy of Llama-3-8B-Instruct is notably poor in certain datasets. This is because the outputs generated by this model in these cases cannot be interpreted into a Python dictionary correctly, which might be related to its ability of instruction following. We also notice that, though Mistral-7B-Instruct-v0.2 has only 7B parameters, the accuracy of its outputs perform quite well on many datasets.\nCritique Evaluation Table 2 shows the results of the critique evaluation experiment. Our model, HALU-J, has the second highest scores among all models. This experiment passes the entire response to the scorer without converting it to a Python dictionary, resulting in the Llama3-8b model demonstrating quite good quality in its generated critiques. This implies that despite the poor formatting performance of its responses, the quality of its critiques is significantly better.\nBased on the annotated critique scores of 100 multiple-evidence ME-FEVER data, the Pearson correlation between humans and GPT-4-Turbo is 0.70, demonstrating a decent agreement between them."}, {"title": "6.3.2 Discussions", "content": "Effectiveness of DPO Fine-tuning The results comparing HALU-J (w/o DPO) and HALU-J in Table 3 demonstrate that DPO fine-tuning does help with performance improvement. We observe increases in accuracy on our ME-FEVER test set and under the ANLI dataset with single evidence setting. Additionally, there are improvements in critique scores and evidence-matching rates. This implies that DPO fine-tuning based on labels can enhance the overall quality of critiques, rather than merely increasing the accuracy of label predictions."}, {"title": "7 Conclusion", "content": "In this work, we develop HALU-J, a hallucination detection judge with 7B parameters that verifies a claim based on given evidence. HALU-J features its ability of generating high quality critique consisting of evidence categorization, detailed reasoning, and accurate label prediction under multiple-evidence real-world scenarios. We create the first multiple-evidence dataset ME-FEVER for hallucination detection, containing data from FEVER dataset and three kinds of synthesized evidence generated by GPT-40. Experiments demonstrate that HALU-J significantly outperforms open-source and closed-source baselines under both multiple-evidence and single-evidence hallucination detection tasks. Empirical results show the high quality of critique generated by HALU-J. The resources in this work can facilitate future research on hallucination detection."}, {"title": "Limitations", "content": "This work mainly focuses on commonsense reasoning and information-seeking hallucination tasks in LLM responses. Other types of hallucinations like numerical calculation errors are beyond our focus. We carefully curated ME-FEVER as our training data to train our model's ability under multiple-evidence scenarios. However, there is much room for improvements in single-evidence scenarios."}, {"title": "Ethics Statement", "content": "Our dataset ME-FEVER stems from FEVER, which is well-established and publicly available for use, containing no personal information. It is possible that HALU-J can make mistakes. We urge users to double-check the hallucination detection results when using it in high-stakes scenarios. This work complies with the ACL Ethics Policy."}]}