{"title": "PAFT: Prompt-Agnostic Fine-Tuning", "authors": ["Chenxing Wei", "Yao Shu", "Mingwen Ou", "Ying Tiffany He", "Fei Richard Yu"], "abstract": "While Large Language Models (LLMs) adapt well to downstream tasks after fine-tuning, this adaptability often compromises prompt robustness, as even minor prompt variations can significantly degrade performance. To address this, we propose prompt-agnostic fine-tuning (PAFT), a simple yet effective approach that dynamically adjusts prompts during fine-tuning. This encourages the model to learn underlying task principles rather than overfitting to specific prompt formulations. PAFT operates in two stages: First, a diverse set of meaningful, synthetic candidate prompts is constructed. Second, during fine-tuning, prompts are randomly sampled from this set to create dynamic training inputs. Extensive experiments across diverse datasets and LLMs demonstrate that models trained with PAFT exhibit strong robustness and generalization across a wide range of prompts, including unseen ones. This enhanced robustness improves both model performance and inference speed while maintaining training efficiency. Ablation studies further confirm the effectiveness of PAFT.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remarkable success across a diverse range of natural language processing (NLP) tasks (Zhao et al., 2024; Xu et al., 2023). To further enhance the performance of LLMs on specific downstream tasks, supervised fine-tuning (SFT) has emerged as a widely adopted strategy (Ouyang et al., 2022; Devlin et al., 2019). This approach typically involves augmenting input data with task-specific instructions and constructing dialogue datasets with expected outputs, enabling the model to effectively learn task-specific patterns during fine-tuning. Empirical studies have shown that SFT can substantially improve model performance on downstream tasks (Raffel et al., 2023; Hu et al., 2023b; Wei et al., 2022). However, a critical limitation of this paradigm is its reliance on fixed instruction templates (Mishra et al., 2022; Chung et al., 2022) for each downstream task. This rigidity often leads to overfitting, whereby models become excessively dependent on specific instruction patterns (Zhang et al., 2024; Kung and Peng, 2023). Consequently, during inference on downstream tasks, even minor deviations between user-provided instructions and the training instructions can result in significant performance degradation (Mialon et al., 2023; Raman et al., 2023). This issue is particularly pronounced when LLM practitioners, who may lack domain expertise, provide prompts that deviate substantially from those used during SFT. In such scenarios, carefully fine-tuned models may experience drastic performance drops, occasionally approaching random guessing levels (Voronov et al., 2024). Previous research has primarily focused on prompt tuning\u2014introducing trainable vectors (soft prompts) to optimize performance (Liu et al., 2022;"}, {"title": "2 Related Work", "content": "Prompt Optimization Effective prompt engineering is crucial for maximizing LLM performance, motivating various optimization techniques (Chang et al., 2024; Li, 2023; Diao et al., 2023; Sun et al., 2022). Methods like INSTINCT (Lin et al., 2024) utilize neural bandits and LLM embeddings"}, {"title": "3 Preliminaries", "content": "To systematically study the impact of prompt variations on fine-tuned models, we use LORA (Hu et al., 2022) as an illustrative example and conduct comprehensive preliminary experiments on multiple downstream tasks to assess prompt sensitivity and robustness. These tasks include natural language inference, question answering, and reading comprehension, using the LLaMA3-8B (Meta, 2024) model. We constructed a comprehensive set of over 450 prompts, covering a wide range of language styles, task-specific instructions, and formatting variations. presents a statistical analysis of the accuracy distribution for both the base model and SFT model across these prompts, revealing a key finding: prompt selection significantly influences model performance, with considerable accuracy variation observed across prompts, irrespective of the downstream task. Only a small fraction (typically less than 10%) of prompts yields near-optimal performance; some even degrade accuracy to near-random levels. Minor prompt modifications (e.g., rephrasing, punctuation, reordering) induce substantial fluctuations. For example, the addition of \"Question\" improves accuracy by 20% (Figure 1). This sensitivity highlights the fragility of current fine-tuning methods and their strong dependence on specific prompt formulations. These findings align with prior work (He et al., 2024; Voronov et al., 2024; Salinas and Morstatter, 2024; Min et al., 2022; Gao et al., 2021); however, we demonstrate that this sensitivity persists across tasks, suggesting a fundamental limitation of current PEFT paradigms. Motivated by these findings, we propose PAFT, addressing prompt robustness by decoupling performance from specific formulations, ensuring consistent results across diverse prompts, and significantly enhancing the practical applicability of fine-tuned models in real-world scenarios where prompt variations are inevitable."}, {"title": "4 The PAFT Framework", "content": "To improve the prompt robustness of LLMs, we propose the PAFT framework in Figure 2. As shown in Figure 2, the PAFT framework consists of two key stages: candidate prompt construction (see Section 4.1 for details) and dynamic fine-tuning (see Section 4.2 for details)."}, {"title": "4.1 Candidate Prompt Construction", "content": "To ensure the robustness and effectiveness of PAFT across diverse prompts, we design a comprehensive prompt construction framework that aims to generate diverse and meaningful candidate prompts efficiently, enabling the model to generalize across different prompt formats. Our approach leverages the powerful generative capabilities of LLMs (Kohl et al., 2024) and comprises three key phases: First, recognizing the inherent variability in how different LLMs interpret downstream tasks due to variations in pre-training data, model architectures, and optimization objectives (Minaee et al., 2024; Zhao et al., 2024), we employ a multi-model approach, selecting 10 mainstream LLMs according to their generation capabilities, including models from OpenAI et al. (2024); Bai et al. (2023); Ouyang et al. (2022), and other widely used commercial LLMs, for prompt generation. This diverse selection ensures broad coverage of potential prompt formulations, capturing variations in linguistic style, task interpretation, and instructional clarity, thereby mitigating biases towards any single model's prompt generation tendencies. Second, we employ a dual-strategy approach, combining few-shot and zero-shot techniques to balance prompt quality and diversity. For few-shot prompting, we leverage principles from in-context learning, providing each LLM with carefully curated, human-crafted examples to guide the generation of semantically coherent and task-relevant prompts, ensuring meaningfulness and alignment with the intended task. For zero-shot prompting, we prioritize diversity by allowing LLMs to generate prompts without explicit examples, thus encouraging a wider range of linguistic styles, structural variations, and task formulations. Specifically, we generate 20 prompts using each strategy, resulting in a comprehensive set encompassing both high-quality prompts (derived from few-shot prompting) and diverse, potentially less optimal prompts (derived from zero-shot prompting). This balanced approach exposes the model to a realistic distribution of prompt quality during training, thereby enhancing its robustness to real-world scenarios where prompt quality may vary significantly. Finally, to rigorously evaluate the robustness of PAFT, we randomly partition the generated prompts into training and test sets using an 8:1 ratio. Crucially, the training and test sets contain entirely distinct prompts, ensuring evaluation on completely unseen formulations. This partitioning strategy enables the construction of training data that exposes the model to a wide range of prompt styles while providing a robust testbed for assessing generalization to novel prompts. By decoupling training and test prompts, we confirm that performance improvements reflect a genuine ability to handle diverse and unseen prompt formulations, rather than overfitting to specific prompt patterns. This comprehensive framework ensures that PAFT learns task semantics independently of specific prompt patterns, enabling effective generalization across a wide range of real-world scenarios, and provides a scalable and cost-effective solution for improving prompt robustness in LLMs."}, {"title": "4.2 Dynamic Fine-Tuning", "content": "The dynamic fine-tuning process in our PAFT framework is designed to enhance the robustness of LLMs to diverse prompt formulations while preserving high performance on downstream tasks. As illustrated in Algorithm 1, during each training epoch $t$, a prompt $p$ is randomly sampled from a diverse set of synthetically generated candidate prompts $P$ (line 4 in Algorithm 1), ensuring exposure to a wide range of linguistic styles and task formulations. For each data point $(x, y) \\in D$ (line 6 in Algorithm 1), the selected prompt $p$ is reused for $K$ consecutive training steps (lines 7-9 in Algorithm 1), and the input $I$ = InputConstruction($x, p$) is constructed by combining the prompt $p$ with the data point $x$ (line 7 in Algorithm 1). The model parameters $\\theta$ are then updated using stochastic gradient-based optimization methods, such as SGD (Sra et al., 2011) or AdamW (Loshchilov and Hutter, 2019) (line 8 in Algorithm 1), enabling the model to learn task-specific semantics while adapting to the formulation of prompt. After every $K$ steps, a new prompt is sampled from $P$ to replace the current one (lines 10-11 in Algorithm 1), ensuring that the model is exposed to multiple prompts within a single epoch. At the end of each epoch, the model parameters $\\theta_{t+1}$ are initialized with the final parameters from the previous epoch, $\\theta_t$ (line 12 in Algorithm 1), ensuring continuity in the learning process. After $T$ epochs, the fine-tuned model parameters $\\theta^* = \\theta_T$ achieve consistent performance across a wide range of prompts (line 16 in Algorithm 1), including those not encountered during training. This makes PAFT particularly suitable for real-world applications where prompt quality and style may vary significantly, such as when users lack domain expertise or when prompts are generated automatically. By decoupling model performance from fixed prompt formulations, PAFT addresses a key limitation of traditional fine-tuning methods, ensuring robust performance without requiring extensive prompt engineering. The dynamic fine-tuning strategy enhances both the robustness and generalization of fine-tuned models while maintaining computational efficiency, making it a practical solution for improving the adaptability of LLMs in diverse settings."}, {"title": "5 Empirical Results", "content": "In this section, we conduct extensive experiments to evaluate the effectiveness and efficiency of our proposed PAFT framework. We begin by detailing the datasets and experimental setup in Section 5.1, followed by a comprehensive analysis of the main results in Section 5.2. Additionally, we perform ablation studies to investigate the impact of key components of our framework, as discussed in Section 5.3."}, {"title": "5.1 Datasets and Setup", "content": "To evaluate the performance of our proposed PAFT method, we focus on reasoning and reading comprehension tasks, as these domains are particularly susceptible to prompt variations. As PAFT is the first work to address the prompt robustness problem in large language models (LLMs) through training, we generate task-specific candidate prompts for each downstream task. Following the dataset selection process of Hu et al. (2023a); Wei et al. (2024), we select the Winogrande (Sakaguchi et al., 2019), PIQA (Bisk et al., 2019), and Hellaswag (Zellers et al., 2019) reasoning benchmarks and additionally include the RACE (Lai et al., 2017) reading comprehension benchmark. These datasets are widely recognized for their ability to assess reasoning and comprehension, provide independent training, validation, and test sets, and employ accuracy as the performance metric. As described in Section 4.1, we generate a diverse set of 400 training prompts and 50 test prompts, ensuring that the test prompts are distinct from the training prompts, see the Appendix C for details. This separation rigorously evaluates the ability of model to generalize to unseen prompt formulations. We establish five baselines for comparison to isolate the impact of prompt engineering on fine-tuning: the pre-trained model without fine-tuning (Base Model); fine-tuning with human-designed prompts (User-Specified Prompt) as in Wei et al. (2024); fine-tuning with the prompt exhibiting the highest accuracy on the training set (Top-Accuracy Prompt); fine-tuning with the most robust prompt generated by BATprompt (Shi et al., 2024) (BATprompt); and fine-tuning with the optimal prompt selected by ZOPO (Hu et al., 2024) from the training prompt set (ZOPO Prompt). The key distinction between these methods lies in the prompt selection for fine-tuning. Critically, all models, including the baselines, are evaluated using the same set of 50 test prompts. This consistent evaluation protocol allows us to directly compare performance consistency and variation across methods. Our implementation leverages the Llama-factory framework (Zheng et al., 2024) and is evaluated using the Opencompass framework (Contributors, 2023). Detailed experimental configurations are provided in Appendix A. All experiments are conducted on NVIDIA A100, V100, 4090, and L40 GPUs to ensure efficient and scalable evaluation."}, {"title": "5.2 Main Results", "content": "PAFT demonstrates strong prompt robustness As shown in Table 1, Figure 4, and Figure 6, PAFT exhibits remarkably low variance across all evaluation tasks, indicating excellent prompt robustness. Compared to other methods, PAFT achieves significantly lower variance, attributable to its unique dynamic prompt selection strategy. This strategy continuously adjusts the prompt during training, compelling the model to learn essential task features rather than overfitting to a specific prompt format. This contrasts sharply with the other baseline models. User-specified prompts rely on manually designed prompts, making it challenging to ensure both quality and diversity, especially without domain expertise. While TopAccuracy and ZOPO select the prompt exhibiting the highest accuracy on the training set, they are prone to overfitting to specific prompts and exhibit poor generalization. Although BATprompt also considers prompt robustness, its generated robust prompts are less effective than PAFT. In summary, the low variance of PAFT implies more stable performance and stronger generalization across diverse prompts, leading to higher reliability in practical applications. Specifically, models trained with PAFT can be used to develop more user-friendly question-answering systems, agent systems independent of input-output formats, and even to better decouple LLM capabilities from prompts, enabling more accurate LLM evaluation. PAFT achieves top performance on the majority of prompts, significantly outperforming all baselines (Table 1, Top column). Furthermore, PAFT maintains high training efficiency, A detailed discussion of training efficiency is provided in Appendix B.\nPAFT achieves state-of-the-art performance As shown in Table 1, Figure 4, and Figure 6, PAFT achieves the highest average accuracy across all evaluated reasoning and reading comprehension tasks, significantly outperforming other baseline models. Specifically, PAFT surpasses other methods on tasks such as HellaSwag, PIQA, Winogrande, RACE, demonstrating its excellent performance across diverse natural language processing tasks. This superior performance stems from PAFT's prompt robustness, enabling the model to better grasp the core essence of each task and maintain high performance across diverse prompt formulations. For instance, strong performance of PAFT on the open text generation task (HellaSwag) can be attributed to its dynamic prompt selection strategy, facilitating improved capture of contextual information. Its success on the physical common sense reasoning task (PIQA) can be attributed to its enhanced ability to utilize common sense knowledge. Similarly, its performance on the reference resolution task (Winogrande) can be attributed to its improved understanding of sentence structure and semantic relations, while its success on the reading comprehension task (RACE) can be attributed to its improved capture of topic and key information. In essence, this performance gain arises from PAFT's decoupling of the prompt from the task itself, allowing the model to focus on learning the fundamental aspects of the downstream tasks.\nPAFT enhances inference efficiency In addition to robustness and performance, PAFT also significantly enhances inference efficiency. By fundamentally enhancing the ability of model to understand the core semantics of tasks, PAFT enables the model to solve problems more effectively, generating fewer tokens. This capability directly translates to faster inference speeds, as the model avoids redundant or unnecessary outputs and focuses on concise, accurate responses. To quantify this improvement, we measured the average end-to-end inference time across all test prompts and datasets, from the input prompt to the final output. As shown in Table 2, models trained with PAFT consistently achieve the fastest inference speeds compared to the baseline methods. This improvement is a direct result of PAFT's inherent prompt robustness. By decoupling model performance from the specific prompt wording, PAFT operates consistently and efficiently regardless of the input prompt. In essence, PAFT promotes more effective generalization and eliminates the need for prompt-specific adaptation during inference. Additionally, our training regime covers a wide range of prompt wordings, avoiding the potential performance degradation or increased computation typically required to handle unexpected or unevenly distributed prompts during inference. This consistency and efficiency is especially valuable in real-world applications that require fast response times, such as dialogue systems or time-sensitive information retrieval. Our enhanced inference efficiency translates to a better user experience and reduced computational resources required for deployment, making it a more practical and scalable solution."}, {"title": "5.3 Ablation Studies", "content": "Hyperparameter robustness This ablation study demonstrates the robustness of PAFT to the hyperparameters $K$ (iterations per prompt) and $T$ (epochs). As shown in Table 3, PAFT achieves stable performance across a broad range of $K$ (1 to 8) and $T$ (3 to 6) values, with minimal fluctuations in accuracy and variance. Notably, PAFT achieves near-optimal performance with default settings ($K$ = 4, $T$ = 3), attaining an average accuracy of 87.46%(\u00b11.34) across all tasks. This robustness reduces the need for extensive hyperparameter tuning, making PAFT a practical and efficient solution for real-world applications.\nPAFT achieves strong performance with limited training prompts We conduct an ablation study to investigate the impact of varying numbers of training prompts on model performance, thus validating the effectiveness of PAFT. The experimental results, shown in Figure 5, demonstrate that as the number of prompts increases, the average accuracy of the model significantly improves, while the standard deviation decreases, indicating more stable and reliable performance. However, the performance gains diminish as the number of prompts increases, with only marginal improvements observed beyond a certain threshold. This suggests that while adding prompts can enhance performance, PAFT achieves competitive results with a minimal number of prompts, rendering excessive prompts unnecessary. In most cases, PAFT achieves strong performance with as few as 10 high-quality prompts, and further increases yield only marginal gains. The efficiency of PAFT is particularly notable, as it delivers excellent performance with a minimal number of prompts, making it highly suitable for resource-constrained scenarios where computational efficiency is critical. These findings underscore the practicality and efficiency of PAFT, offering a robust and efficient solution for real-world applications."}, {"title": "6 Conclusion", "content": "PAFT offers a compelling solution for enhancing the prompt robustness of LLMs. By dynamically adjusting prompts during fine-tuning, PAFT significantly improves model generalization and performance across diverse prompt formulations. Notably, PAFT boosts inference speed with maintained training cost. This approach paves the way for more reliable and efficient LLM deployment in real-world applications."}, {"title": "Limitations", "content": "In this section, we discuss potential limitations of PAFT and outline promising directions for future research. While PAFT demonstrates significant progress in enhancing the prompt robustness of Large Language Models (LLMs), certain aspects warrant further investigation. A key area for improvement lies in the dynamic prompt selection strategy employed during fine-tuning. Currently, PAFT utilizes a random sampling approach, which, while exposing the model to a diverse range of prompts, may not be the most efficient or effective method. Exploring more sophisticated sampling techniques, such as curriculum learning or importance sampling, could potentially optimize the training process and further enhance robustness. For instance, prioritizing prompts that induce higher loss or those that are more representative of the overall prompt distribution could lead to faster convergence and improved generalization. Furthermore, integrating adversarial learning into the dynamic fine-tuning phase presents a compelling avenue for future work. Generating adversarial prompts on-the-fly, perhaps through gradient-based updates, could further challenge the model and encourage it to learn more robust task representations. This approach could be particularly beneficial in mitigating the impact of maliciously crafted or unexpected prompts. However, the well-known instability of adversarial training remains a significant hurdle. Stabilizing the training process, perhaps through techniques like robust optimization or regularization, is crucial for realizing the full potential of this approach. Investigating different adversarial prompt generation strategies and their impact on model robustness would be a valuable contribution."}, {"title": "Ethics Statement", "content": "We have manually reevaluated the dataset we created to ensure it is free of any potential for discrimination, human rights violations, bias, exploitation, and any other ethical concerns."}, {"title": "A Experimental setting", "content": "In the main experiment, we compared PAFT with the baseline. The datasets and experimental parameters are as follows:"}, {"title": "A.1 Dataset", "content": "In this section, we introduce the statistics of the dataset. The statistics of the dataset are shown in Table 4."}, {"title": "A.2 Specific experimental parameters", "content": "Based on the LLaMA3-8B model configuration, several adjustments were made to optimize model performance. In the baseline model experiment, generation parameters were adjusted to ensure the correct output. In the LoRA experiment, adjustments to the generation parameters were retained, and LoRA-related parameters were adjusted. In the PAFT experiment, the size of the validation set was adjusted to control the time required to search for the optimal layer. For specific experimental parameters, see the table 5."}, {"title": "B Training cost and inference time", "content": "PAFT Maintains Training Efficiency We now turn our attention to the training efficiency of PAFT. A critical consideration for any practical fine-tuning approach is its impact on training time. Introducing complex mechanisms or additional computational overhead can significantly hinder the training process, especially when dealing with large language models and extensive datasets. Therefore, it is essential to demonstrate that PAFT does not introduce such burdens.\nTo rigorously evaluate the training time implications of PAFT, we conducted a series of experiments, using Low-Rank Adaptation (LoRA) (Hu et al., 2022) as a representative example of a parameter-efficient fine-tuning method. LoRA has gained popularity due to its ability to adapt pre-trained models with minimal computational cost, making it a suitable baseline for our analysis. Our experiments, the results of which are presented in Table 3, directly compare the training time required for traditional LORA fine-tuning with the training time required for PAFT integrated with LoRA.\nThe key finding from our analysis is that PAFT does not introduce any noticeable increase in training time. The data in Table 6 clearly demonstrates that the training duration remains virtually identical whether we employ standard LoRA or incorporate PAFT's dynamic prompt selection mechanism. This crucial observation underscores the efficiency of PAFT. The dynamic prompt selection process, which is central to PAFT's ability to enhance prompt robustness, is implemented in a way that does not add significant computational overhead. This is because the selection process is lightweight and seamlessly integrated into the existing training loop. Rather than requiring complex computations or extensive data manipulations, PAFT efficiently chooses from a diverse set of prompts, allowing the model to experience a wider range of input formulations without incurring a substantial time penalty. This efficient dynamic prompt selection is critical for the practical applicability of PAFT, ensuring that it can be readily deployed without compromising training efficiency. Furthermore, this efficiency allows for more extensive experimentation and exploration of different prompt variations, ultimately leading to more robust and generalizable models.\nEfficient Candidate Prompt Generation A key aspect of PAFT's effectiveness lies in its ability to generate a diverse and high-quality set of candidate prompts efficiently. The process of constructing these candidate prompts involves leveraging the capabilities of external large language models (LLMs), which naturally raises the question of associated costs. Specifically, we sought to quantify the token usage required for candidate prompt generation, as this directly translates to the expense incurred when interacting with commercial LLM APIs.\nTo address this, we conducted a detailed analysis of the token consumption during the candidate prompt generation phase of PAFT. Our investigation, the results of which are summarized in Table 1, focuses on the number of tokens required to produce a sufficient variety of prompts suitable for subsequent selection and fine-tuning. We meticulously tracked the token usage across various prompts generated for different tasks, considering factors such as prompt length, complexity, and diversity.\nThe findings presented in Table 7 demonstrate that PAFT requires remarkably few tokens to generate a substantial pool of candidate prompts. This efficiency stems from PAFT's strategic approach to prompt engineering. Rather than relying on brute-force generation or computationally intensive search methods, PAFT employs a carefully designed prompting strategy that encourages the external LLMs to produce a wide range of prompt formulations with minimal token consumption. This is achieved through techniques such as few-shot prompting with carefully chosen examples, targeted instructions that guide the LLM towards desired prompt characteristics, and potentially iterative refinement of prompts based on preliminary evaluation. The low token count is crucial for practical applications, as it minimizes the cost associated with using commercial LLM APIs. Moreover, this efficiency enables the exploration of a broader range of potential prompts within a fixed budget, increasing the likelihood of discovering highly effective prompts that contribute to improved model robustness. This efficient prompt generation process is a significant advantage of PAFT, enabling it to achieve superior performance without incurring prohibitive costs."}, {"title": "C Prompt", "content": "In this section, we present a selection of training and test prompts to illustrate the efficacy of our prompt construction algorithm and to provide a clearer understanding of operational process of PAFT. Due to space constraints, we only list 10 prompts as examples. Section C.1 showcases examples of training prompts, Section C.2 highlights test prompts, and Section C.3 outlines the prompts utilized by the baseline method."}, {"title": "C.1 Train prompt", "content": "In this section, we present the prompts generated using the method outlined in Section 4.1 across various datasets. All prompts listed here are utilized for training purposes."}, {"title": "C.2 Test prompt", "content": "In this section, we present the prompts generated using the method outlined in Section 4.1 across various datasets. All prompts listed here are utilized for testing purposes, and they are not visible during training."}, {"title": "C.3 Baseline prompt", "content": "In this section, we present the best prompts generated or filtered using the baseline for training."}]}