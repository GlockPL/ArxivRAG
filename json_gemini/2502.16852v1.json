{"title": "Improving LLM General Preference Alignment via Optimistic Online Mirror Descent", "authors": ["Yuheng Zhang", "Dian Yu", "Tao Ge", "Linfeng Song", "Zhichen Zeng", "Haitao Mi", "Nan Jiang", "Dong Yu"], "abstract": "Reinforcement learning from human feedback\n(RLHF) has demonstrated remarkable effective-\nness in aligning large language models (LLMs)\nwith human preferences. Many existing alignment\napproaches rely on the Bradley-Terry (BT) model\nassumption, which assumes the existence of a\nground-truth reward for each prompt-response\npair. However, this assumption can be overly\nrestrictive when modeling complex human pref-\nerences. In this paper, we drop the BT model\nassumption and study LLM alignment under gen-\neral preferences, formulated as a two-player game.\nDrawing on theoretical insights from learning in\ngames, we integrate optimistic online mirror de-\nscent into our alignment framework to approxi-\nmate the Nash policy. Theoretically, we demon-\nstrate that our approach achieves an O(T-1)\nbound on the duality gap, improving upon the\nprevious O(T-1/2) result. More importantly, we\nimplement our method and show through experi-\nments that it outperforms state-of-the-art RLHF\nalgorithms across multiple representative bench-\nmarks.", "sections": [{"title": "1. Introduction", "content": "Reinforcement learning from human feedback (RLHF) has\nplayed a pivotal role in aligning large language models\n(LLMs) with human preferences. The goal of RLHF is to\nfine-tune LLMs to generate responses that are preferred by\nhumans. It has been successfully deployed in state-of-the-\nart models, including Instruct-GPT (Ouyang et al., 2022)\nand Claude (Bai et al., 2022b). The first RLHF framework\nfor LLMs was developed by Ouyang et al. (2022), where\nafter the pre-training stage, the LLM is fine-tuned to max-\nimize the reward signal from a reward model using the\nproximal policy optimization (PPO) algorithm (Schulman\net al., 2017). This pipeline requires training both the reward\nmodel and the policy model. In addition, policy gradient\napproaches such as PPO often exhibit high variance and\ninstability during training (Peng et al., 2023), leading to\nincreased computational costs.\nTo develop a more stable and computationally lightweight\nalignment approach, Rafailov et al. (2024b) propose the\nDirect Preference Optimization (DPO) algorithm, which di-\nrectly trains the LLM on a preference dataset and bypasses\nthe need for a reward model. DPO uses an offline preference\ndataset, and since its development, a line of research has ex-\nplored different exploration strategies and proposed online\ndirect preference alignment algorithms (Xiong et al., 2024;\nXie et al., 2024; Dong et al., 2024; Yuan et al., 2024). All\nthese methods assume that human preferences can be mod-\neled using the Bradley-Terry (BT) model, where a reward\nfunction $R^*$ exists such that, for any prompt $x$ and response\npair $(y^1, y^2)$, the preference between $y^1$ and $y^2$ satisfies:\n$P(y^1 > y^2 | x) = \\sigma(R^*(x,y^1) - R^*(x, y^2))$,\nwhere $\\sigma(z) = \\frac{1}{1+exp(-z)}$ is the sigmoid function.\nHowever, the existence of a reward function and the BT\nmodel are strong assumptions that can be overly restrictive\nwhen modeling complex human preferences. For example,\nthe preference signals in the BT model are always transi-\ntive: if A is preferred to B and B is preferred to C, then\nA must always be preferred to C. This transitive property\ncontradicts evidence from human decision-making (May,\n1954; Tversky, 1969), especially when preferences are at\nthe population level and aggregated from different human\ngroups (May, 1954; Ye et al., 2024). Furthermore, the limi-\ntations of the BT model have also been observed in RLHF\npractice. Jiang et al. (2023) show that a preference model\nwith 0.4B parameters achieves performance comparable to\nLlama-2-13B-based reward models. Ye et al. (2024) train a\nBT reward model and a preference model separately using\nthe same base model and preference dataset, and their results\ndemonstrate that the preference model consistently outper-\nforms the reward model on Reward-Bench (Lambert et al.,\n2024) under both base models. These findings motivate\nus to drop the BT model assumption and instead consider\ngeneral preferences.\nIn this work, we study the problem of aligning LLMs with\ngeneral preferences and formulate it as a two-player zero-\nsum game. Our objective is to approximate the Nash policy\nof the game, which ensures a win rate of at least 50% against\nany other policy. As established in the game theory litera-\nture (Bai et al., 2020; Liu et al., 2021), self-play algorithms\nhave proven to be highly effective in approximating Nash\npolicies. Building on this, we aim to propose a novel online\nRLHF algorithm that further leverages the self-play strcture\nto enhance general preference alignment for LLMs. Our\ncontributions are summarized as follows.\nContributions. We propose a novel online general pref-\nerence alignment algorithm, Optimistic Nash Policy Opti-\nmization (ONPO). Inspired by recent advancements in game\ntheory, our algorithm integrates optimistic online mirror de-\nscent (Rakhlin & Sridharan, 2013; Syrgkanis et al., 2015)\ninto the self-play framework. By utilizing a reward predic-\ntor in a two-step update strategy, ONPO more effectively\nleverages the self-play mechanism and achieves a faster\nconvergence rate of $O(T^{-1})$, improving upon the previous\n$O(T^{-1/2})$ result.\nONPO can be efficiently implemented by directly minimiz-\ning a loss objective on a preference dataset, making it com-\nputationally lightweight in practice. We evaluate ONPO\non several representative benchmarks, comparing it with\nstate-of-the-art general preference alignment algorithms.\nExperimental results demonstrate that ONPO consistently\noutperforms or achieves performance comparable to the\nbaselines across different base models and benchmarks. No-\ntably, on the AlpacaEval 2.0 benchmark (Li et al., 2023a),\nONPO achieves a 21.2% and 9.9% relative improvement\nover the strongest baseline when using Mistral-Instruct and\nLlama-3-8B as the base models, respectively.\nOrganization. Section 2 presents related work on RLHF\nand learning in games. The problem formulation and pre-\nliminaries are provided in Section 3. Our algorithm and its\ntheoretical guarantees are detailed in Section 4. In Section 5,\nwe compare our approach with other general preference\nalignment algorithms and explore its extension to the multi-\nturn setting. Experimental results are presented in Section 6.\nFinally, we conclude the paper and discuss future directions\nin Section 7."}, {"title": "2. Related Work", "content": "Reward-Based RLHF. Since the first RLHF framework\nproposed by Christiano et al. (2017), RLHF has achieved\ntremendous success in aligning large language models\n(LLMs), powering models such as Instruct-GPT (Ouyang\net al., 2022), Llama 2 (Touvron et al., 2023), and Claude (Bai\net al., 2022b). The RLHF pipeline typically involves train-"}, {"title": "3. Preliminary", "content": "Problem Setup. We study the contextual formulation\nwhich is extensively used in previous RLHF litera-"}, {"title": "4. Algorithm", "content": "In this section, we begin by briefly reviewing the self-\nplay algorithm with online mirror descent (OMD) updates,\nwhich is used in previous general preference alignment al-\ngorithm (Zhang et al., 2024). Next, we present our proposed\nalgorithm, which leverages the faster convergence proper-\nties of optimistic OMD, inspired by advancements in game\ntheory (Rakhlin & Sridharan, 2013; Syrgkanis et al., 2015).\nThrough theoretical analysis, we show that our approach\nachieves an improved bound on the duality gap. Finally, we\ndescribe the implementation of our algorithm. Following\nAzar et al. (2024); Zhang et al. (2024), we omit the con-\ntext x throughout the rest of the paper since each context is\nindependent.\n4.1. Self-play Algorithm with OMD Update\nSelf-play algorithms are widely used in approximating the\nNash policy (Bai et al., 2020; Liu et al., 2021). The key\nidea is to let the policy play against itself, enabling iterative\nself-improvement. The algorithm is performed in an online\nmanner, with each iteration using online mirror descent\n(OMD) to update the policy. Specifically, at iteration $t$, we\nfind the policy that maximizes the following objective:\n$\\Pi_{t+1} = \\underset{\\pi}{\\operatorname{argmax}} \\langle \\pi, r_t \\rangle - \\frac{1}{\\eta}KL(\\pi||\\pi_t),$ (1)\nwhere $r_t(y) = P(y > \\pi_t) = E_{y' \\sim \\pi_t} [P(y > y')]$ is the ex-\npected win rate of response $y$ against the current policy $\\pi_t$,\nand $\\eta > 0$ is the learning rate. This objective ensures that\n$\\pi_{t+1}$ not only aims to maximize the win rate over $\\pi_t$ but also\nremains close to $\\pi_t$, as measured by the KL divergence term.\nThe stability introduced by the KL regularization is critical\nfor achieving a sublinear regret bound. Without this regu-\nlarization, one can construct examples where the algorithm\nsuffers from linear regret, which is undesirable (Lattimore\n& Szepesv\u00e1ri, 2020).\nSimilar to the analysis in Zhang et al. (2024), we can show\nthat the uniform mixture of $\\pi_{1:T}$ achieves an $O(T^{-1/2})$\nduality gap, as stated in the following theorem. The proof is\ndeferred to Appendix A.1.\nTheorem 4.1. Let $D = \\underset{\\pi} max KL(\\pi|\\pi_1)$ and\n$\\bar{\\pi} = \\frac{1}{T} \\sum_{t=1}^T \\pi_t$. Self-play algorithm in Eq. (1) with $\\eta = \\sqrt{\\frac{D}{T}}$\nsatisfies:\n$DualGap(\\bar{\\pi}) \\leq \\frac{4\\sqrt{D}}{\\sqrt{T}}$\nZhang et al. (2024) also demonstrate that self-play with\nOMD achieves last-iterate convergence. This result is at-\ntributed to the strong convexity induced by the KL regular-\nization terms in their game objectives. However, since our\nobjective does not include these KL terms, the last-iterate\nconvergence may not hold in our game formulation.\n4.2. Optimistic Nash Policy Optimization\nWhile self-play with OMD update already achieves an\n$O(\\sqrt{T})$ regret bound, which is near-optimal in many online\nlearning scenarios, there is still room for improvement by\nbetter leveraging the self-play structure. Recent advance-\nments in learning in games (Rakhlin & Sridharan, 2013;\nSyrgkanis et al., 2015) demonstrate that a faster conver-\ngence rate of $O(T^{-1})$ can be achieved when both players\nadopt optimistic OMD update. In this subsection, we in-\ntroduce how to integrate optimistic OMD into the self-play\nalgorithm, resulting in an algorithm called Optimistic Nash\nPolicy Optimization (ONPO).\nThe key idea of optimistic OMD is to incorporate a reward or\nloss predictor at each iteration. Recall that in OMD update,\nwe use the expected win rate over the current policy $\\pi_t$ as\nthe reward vector $r_t$ to compute $\\pi_{t+1}$. While in optimistic\nOMD, the learner utilizes a reward predictor $m_t$ and adopts\na two-step update strategy:\n$\\pi_t' = \\underset{\\pi} {\\operatorname{argmax}} \\langle \\pi, m_t \\rangle - \\frac{1}{\\eta}KL(\\pi||\\pi_1)$,\n$\\pi_{t+1} = \\underset{\\pi} {\\operatorname{argmax}} \\langle \\pi, r_t \\rangle - \\frac{1}{\\eta}KL(\\pi||\\pi_t').$\nHere $\\pi_t'$ aims to maximize the reward predictor $m_t$ and\nthe auxiliary policy $\\pi_{t+1}$ is updated after observing the\nactual reward $r_t$. The word \"optimistic\" comes from that\nthe learner believes that the predictor $m_t$ provides a good\napproximation of the true reward $r_t$.\nNext, we describe how to apply optimistic OMD in our\nself-play algorithm. In both OMD and optimistic OMD,\nthe KL regularization term is consistently used to ensure\nthat the next policy remains close to the previous policies.\nThis regularization provides stability, making it reasonable\nto assume that the change from $\\pi_t$ to $\\pi_{t+1}$ is small. Based\non this observation, we directly use the reward information\nfrom the previous iteration as the predictor, i.e., let $m_t =$\n$r_{t-1} = E_{y' \\sim \\pi_{t-1}} [P(y > y')]$.\nIn the following theorem, we demonstrate that ONPO\nachieves an $O(1/T)$ duality gap, improving over the previ-\nous $O(1/\\sqrt{T})$ result.\nTheorem 4.2. Let $D = \\underset{\\pi} max KL(\\pi|\\pi_1')$ and $\\bar{\\pi} =$\n$\\frac{1}{T} \\sum_{t=1}^T \\pi_t$, ONPO algorithm with $\\eta = \\operatorname{min}\\{1, \\frac{4\\sqrt{D}}{T}\\}$ satis-\nfies:\n$DualGap(\\bar{\\pi}) \\leq \\frac{4\\sqrt{D}}{T}$\nHere, $\\pi_1' = \\pi_1$ is the initialization policy. Theoretically, $\\pi_1'$\ncan be set as a uniform policy, in which case $D$ is bounded\nby $\\log |\\mathcal{Y}||$. In RLHF practice, $\\pi_1$ is typically a supervised\nfine-tuned policy.\nThe proof is provided in Appendix A.2. The key to achiev-\ning the $O(1/T)$ rate lies in the regret bounded by variation\nin utilities (RVU) property of optimistic OMD. Specifically,\nthe stability terms $||r_t-r_{t-1}||$ are canceled out by the neg-\native term $-||\\pi_t - \\pi_{t-1'}||^2$, which arises from the self-play\nmechanism where $r_t$ represents the win rate over $\\pi_t$.\nNotably, the duality gap bound in Zhang et al. (2024) also\ndepends on the maximum log density ratio between $\\pi_t$ and\na reference policy $\\pi_{ref}$, due to the KL-regularized game\nformulation. When optimistic OMD is applied in such a\nregularized game, the stability terms transform into\n$\\underset{y}{max} |P(y > \\pi_t) - P(y > \\pi_{t-1}) + \\log \\frac{\\pi_t(y)}{\\pi_{t-1}(y)}|$,\nwhich cannot be canceled by the negative terms. However,\nthe motivation behind regularizing the game is to keep the\nlearner's policy close to the reference policy $\\pi_{ref}$, which\naligns with the stability introduced in our update rule. There-\nfore, explicit regularization in our game objective is not\nnecessary.\n4.3. Implementation of ONPO\nIn this subsection, we describe the implementation of ONPO\nwith query access to the preference oracle $\\mathcal{P}$. The primary\nchallenge in implementing ONPO lies in computing $r_t(y)$,\nwhich involves taking an expectation over the entire policy\n$\\pi_t$. Fortunately, this challenge can be addressed by avoiding\nthe direct estimation of $r_t(y)$ and instead relying on binary\npreference feedback between responses.\nTo achieves this, our goal is to design a loss function that\ndoes not involve $P(y > \\pi_t)$ for policy optimization. We\nfocus on obtaining the loss objective for $\\pi_t$ here and the\nderivation for $\\pi_t'$ is similar. The key observation is that, $\\pi_t$\nhas a closed-form solution which satisfies $\\forall y, y' \\in \\mathcal{Y}$,\n$\\log \\frac{\\pi_t(y)}{\\pi_t(y')} - \\log \\frac{\\pi'_t(y)}{\\pi'_t(y')} = \\eta (P(y > \\pi_{t-1})-P(y' > \\pi_{t-1})).$\nTherefore, similar to the techniques used in Azar et al.\n(2024); Zhang et al. (2024), solving $\\pi_t$ is equivalent to\nfinding the minimizer of the following loss function:\n$E_{y,y' \\sim \\pi'_{t-1}}[(g_t(\\pi, y, y') - \\eta (P(y > \\pi_{t-1}) - P(y' > \\pi_{t-1})))^2].$\nwhere $g_t(\\pi, y, y') = \\log \\frac{\\pi(y)}{\\pi(y')} - \\log \\frac{\\pi'_t(y)}{\\pi'_t(y')}$. Since the inside\nwin rate term is with respect to $\\pi_{t-1}$ and we also have\nan expectation over $\\pi_{t-1}$ outside, the loss function can be\nfurther written as\n$E_{y,y' \\sim \\pi_{t-1},y_w,y_l \\sim A_p(y, y')}(g_t(\\pi, y_w, y_l))^2/2$,"}, {"title": "5. Discussion", "content": "In this section, we first discuss the differences between\nONPO and other general preference alignment methods.\nThen we introduce how to extend ONPO to the multi-turn\nsetting.\n5.1. Comparison between ONPO and Other General\nPreference Alignment Methods\nIPO. Azar et al. (2024) is the first to address general pref-\nerence alignment in LLMs. The optimization objective of\nIPO is:\n$\\underset{\\pi}{max} E_{y \\sim \\pi, y' \\sim \\mu} [P(y > y')] - \\tau KL(\\pi||\\pi_{ref}),$\nwhere $\\mu$ is a fixed policy. From a game-theoretic perspective,\nthe goal of IPO is to find the best response to $\\mu$. However,\nthis approach only ensures that the learned policy outper-\nforms $\\mu$, which leaves the possibility that another policy\ncould outperform the learned policy. In contrast, our ap-\nproach focuses on learning the Nash policy in a two-player\ngame. This provides stronger theoretical guarantees, as the\nNash policy will not lose to any other policy.\nNash-MD. Munos et al. (2023) is the first to formulate the\nalignment problem as a two-player zero-sum game. Their\ngame objective includes KL regularization terms, which\nensure that the player's policy remains close to the reference\npolicy $\\pi_{ref}$. The KL terms are weighted by a parameter $\\tau$.\nThey proposed an iterative algorithm, Nash-MD, to learn\nthe Nash policy of the game. At each iteration $t$, the policy\nis updated as:\n$\\pi_{t+1} = \\underset{\\pi}{argmax} P(\\pi > \\bar{\\pi}_t) - \\frac{1}{N_t} KL(\\pi, \\bar{\\pi}_t),$\nwhere $\\bar{\\pi}_t$ is a geometric mixture policy of the current policy\n$\\pi_t$ and the reference policy $\\pi_{ref}$:\n$\\bar{\\pi}_t(y) = \\frac{\\pi_t(y)^{1-\\eta_t \\tau} \\pi_{ref}(y)^{\\eta_t \\tau}}{\\sum_{y' \\pi_t(y')^{1-\\eta_t \\tau} \\pi_{ref}(y')^{\\eta_t \\tau}}.$\nNash-MD requires sampling from the mixture policy $\\bar{\\pi}_t$.\nHowever, the response space $\\mathcal{Y}$ is often exponentially large,\nmaking the exact computation of $\\bar{\\pi}_t$ intractable. To address\nthis, Munos et al. (2023) propose sampling from an approx-\nimate policy. The theoretical guarantees of this approxima-\ntion remain unclear. In contrast, our approach only requires\nsampling from the current policy $\\pi_t$, which is straightfor-\nward to implement in practice.\nOnline IPO. Calandriello et al. (2024) propose the online\nIPO population loss:\n$E_{\\substack{y,y' \\sim SG[\\pi] \\\\\\zeta_w,\\zeta_l \\sim A_p(y,y')}} [(\\log \\frac{\\pi(\\zeta_w) \\pi_{ref}(\\zeta_l)}{\\pi(\\zeta_l) \\pi_{ref}(\\zeta_w)})^2 - \\frac{1}{2\\tau}],$\nwhere $SG$ is the stop-gradient operator, which prevents gra-\ndients from propagating through the data generation process.\nUnlike the offline IPO approach, which always samples\nfrom a fixed policy $\\mu$, online IPO leverages responses gen-\nerated by the current policy $\\pi$.\nSince the policy $\\pi$ is updated throughout training, policy\ngradient methods are used to minimize the objective. How-\never, as discussed earlier, policy gradient methods in RLHF\nhave limitations, including being resource-intensive and un-\nstable to train. In contrast, ONPO avoids these challenges\nby directly minimizing a loss function over a preference\ndataset, offering a more stable and efficient implementation.\nDNO. The theoretical version of DNO (Algorithm 1\nin Rosset et al. (2024)) relies on computing $r_t(y) =$\n$E_{y' \\sim \\pi_t} [P(y > y')]$, which requires taking an expectation\nover the current policy $\\pi_t$. This computation is challenging\nto implement in practice, so Rosset et al. (2024) propose a\npractical version, DNO-Prct (Algorithm 2), where $\\pi_{t+1}$ is\nupdated as follows:\n$\\underset{\\pi} {argmax} E_{\\substack{y_w, y_l \\sim D_t}} \\log \\sigma (\\eta \\log \\frac{\\pi(y_w) \\pi_t(y_l)}{\\pi(y_l) \\pi_t(y_w)})$\nWhen constructing the dataset $D_t$, only response pairs with\nlarge margins are selected. This selection is motivated by\nthe fact that, to approximate DNO, the ideal condition is\n$\\sigma(r_t(y_w) - r_t(y_l)) \\approx 1$. However, this cannot be fully\nachieved since $r_t(y) \\in [0,1]$. Notably, the objective of\nDNO-Prct is identical to the DPO objective (Rafailov et al.,\n2024b). Therefore, DNO-Prct can be viewed as an iterative\nversion of DPO.\nSPPO. Wu et al. (2024) propose a self-play algorithm\nSPPO. The policy update in SPPO is:\n$\\Pi_{t+1} = \\underset{\\pi}{argmin} E_{y \\sim Y} (\\log \\frac{\\pi(y)}{\\pi_t(y)} - \\eta (P(\\pi_t) - \\frac{\\pi(y)}{\\pi_t(y)})^2,$\nwhere $P$ is a heuristic approximation of $P(y > \\pi_t)$. How-\never, obtaining an accurate estimation of $P(y > \\pi_t)$ is chal-\nlenging in practice. For example, Hoeffding's inequality\nsuggests that more than 100 queries are needed to ensure\n$P(y > \\pi_t) - P(y > \\pi_t) \\leq 0.1$. This requirement results\nin high annotation and computation costs, as 100 oracle\nqueries are needed for a single response $y$. In contrast,\nONPO bypasses the need to estimate $P(y > \\pi_t)$ and instead\nrelies on binary preference signals between two responses.\nINPO. Zhang et al. (2024) propose a self-play algorithm,\nINPO, which employs OMD to iteratively update the policy,\nas described in Section 4.1. Leveraging the faster conver-\ngence properties of optimistic OMD, ONPO achieves an\nimproved duality gap bound of $O(T^{-1})$, compared to the\n$O(T^{-1/2})$ bound of INPO.\n5.2. Extension to the Multi-Turn Setting\nIn this subsection, we describe how ONPO can be extended\nto the multi-turn setting, which is formulated as a contextual\nMarkov decision process (CMDP) (Shani et al., 2024). The\ninteraction between the LLM and the environment unfolds\nas follows: the LLM starts at a fixed initial state $s_1 \\in S$\nand takes an action $y_1 \\sim \\pi(\\cdot | s_1)$. The environment then\ntransitions to the next state $s_2 \\sim P(\\cdot | s_1, y_1)$ according to\nthe transition dynamics $P$, and the LLM subsequently takes\naction $y_2 \\sim \\pi(\\cdot | s_2)$. This process repeats for $H$ steps, ulti-\nmately reaching the final state $s_{H+1}$. At the end of the inter-\naction, the preference oracle compares two final states and\nprovides a preference signal: $z \\sim Ber(P(s_{H+1} > s'_{H+1}))$.\nThis CMDP formulation effectively captures various LLM\napplications, including chatbot interactions and token-level\nMDPS (Rafailov et al., 2024a).\nIn the multi-turn setting, the challenge is that preferences\nare only provided for the final states, and there is no direct\nfeedback for intermediate states. To address this, we use\nQ-value functions, which capture the long-term expected\noutcomes, in the optimization objective. For each state $s_h$,\nthe update rule for $\\pi_{t+1}(\\cdot | s_h)$ is:\n$\\Pi_{t+1} = \\underset{\\pi}{argmax} \\langle \\pi, Q^{\\pi_{t-1}, \\pi_t}(s_h, \\cdot) \\rangle - \\frac{1}{\\eta}KL(\\pi(\\cdot | s_h) ||\\pi_t(\\cdot | s_h)),$\nwhere $Q^{\\pi_t', \\pi_t}(s_h, y_h) = E_{\\pi_t}[P(s_{H+1} - \\pi_t | s_h, y_h]$\nand $P(s > \\pi_t)$ represents $E_{\\pi_t} [P(s > s'_{H+1})]$. Here\n$\\langle \\pi, Q^{\\pi_t', \\pi_t}(s_h, \\cdot) \\rangle$ measures the probability of $\\pi$ outperform-\ning $\\pi_t$ at state $s_h$. The update rule for $\\pi_{t+1'}$ is similar, except\nthat the KL divergence is computed between $\\pi$ and $\\pi_t'$.\nThe primary challenge in implementing ONPO in the multi-\nturn setting lies in the efficient estimation of $Q^{\\pi_t', \\pi_t}$. Shani\net al. (2024) propose to use an actor-critic framework that\nemploys policy-gradient methods such as PPO (Schulman\net al., 2017) for policy optimization. However, policy-\ngradient methods are known to exhibit high variance and\nsensitivity to implementation details, leading to increased\ncomputational costs. In this paper, we focus on implement-\ning ONPO in the single-turn setting and leave the implemen-\ntation under the multi-turn setting for future work."}, {"title": "6. Experiments", "content": "6.1. Main Results\nExperiment Setup. We implement ONPO following the\nonline RLHF workflow described in Dong et al. (2024).\nTwo base models are used as the initial policy $\\pi_1$: Llama-\n3-SFT\\footnote{https://huggingface.co/RLHFlow/\nLLAMA3-SFT} , based on Llama-3-8B (Dubey et al., 2024), and\nMistral-Instruct-v0.3\\footnote{https://huggingface.co/mistralai/\nMistral-7B-Instruct-v0.3}, an instruct fine-tuned version of the\nMistral-7B-v0.3. For the general preference oracle, we use\na pairwise preference model\\footnote{https://huggingface.co/RLHFlow/\npair-preference-model-LLaMA3-8B}, which demonstrates better\nperformance compared to the BT reward model (Zhang"}, {"title": "6.2. More Results on Academic Tasks", "content": "In this subsection, we evaluate the model's reasoning\nand calibration abilities across six academic benchmarks:\nGPQA (Rein et al., 2023) for graduate-level science ques-\ntion answering, MMLU-Pro (Wang et al., 2024) for mul-\ntitask language understanding, Hellaswag (Zellers et al.,\n2019) for commonsense inference, Winogrande (Sakaguchi"}, {"title": "6.3. Hyperparameter Sensitivity Analysis", "content": "In this subsection, we analyze the sensitivity of ONPO to\nthe hyperparameter $\\eta$, which serves as the learning rate in\nthe update rule. We conduct experiments using Mistral-\nInstruct-v0.3 as the base model and vary $\\eta$ from 200/3 to\n200. The results, presented in Figure 1, indicate that ONPO\nconsistently achieves strong performance across different\nvalues of $\\eta$ and outperforms the baselines, demonstrating its\nrobustness to hyperparamter variations."}, {"title": "7. Conclusion and Future Work", "content": "We propose Optimistic Nash Policy Optimization (ONPO),\na novel approach for aligning LLMs with general prefer-\nences via self-play. By integrating optimistic online mirror\ndescent, ONPO achieves an improved duality gap bound for\napproximating the Nash policy of the game. Our experimen-\ntal results demonstrate that ONPO consistently outperforms\nor matches state-of-the-art general preference alignment\nmethods across multiple benchmarks. For future work, we\naim to explore the implementation of ONPO under the multi-\nturn setting. In addition, we plan to design different strate-\ngies for actively selecting preference data to further enhance\nalignment performance."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none of which we feel must be\nspecifically highlighted here."}, {"title": "A. Proofs for Section 4", "content": "A.1. Proof for Theorem 4.1\nProof. According to the regret analysis of OMD (Lattimore & Szepesv\u00e1ri", "divergence\nterm": "n$KL(\\pi_1||\\pi_2) = D_{\\psi}(\\pi_1, \\pi_2) = \\psi(\\pi_1) - \\psi(\\pi_2) - (\\nabla\\psi(\\pi_2), \\pi_1 - \\pi_2)$.\nSince $\\psi$ is strongly convex with respect to $L_1$ norm, we can apply regret analysis from Rakhlin & Sridharan (2013);\nSyrgkanis et al. (2015) and obtain that for any $\\pi'$\n$\\sum_{t=1}^T \\langle \\pi' - \\pi_t, r_t \\rangle \\leq \\frac{KL(\\pi' ||\\pi'_1)}{\\eta} + \\frac{\\eta}{8} \\sum_{t=1}^T ||r_t - r_{t-1}||_2^2 - \\frac{1}{\\eta} \\sum_{t=2}^T ||\\pi_t - \\pi_{t-1}||_1^2$.\nWe observe"}]}