{"title": "Diffusion as Reasoning: Enhancing Object Goal Navigation with LLM-Biased Diffusion Model", "authors": ["Yiming Ji", "Yang Liu", "Zhengpu Wang", "Boyu Ma", "Zongwu Xie", "Hong Liu"], "abstract": "The Object Goal Navigation (ObjectNav) task requires the agent to navigate to a specified target in an unseen environment. Since the environment layout is unknown, the agent needs to perform semantic reasoning to infer the potential location of the target, based on its accumulated memory of the environment during the navigation process. Diffusion models have been shown to be able to learn the distribution relationships between features in RGB images, and thus generate new realistic images.In this work, we propose a new approach to solving the ObjectNav task, by training a diffusion model to learn the statistical distribution patterns of objects in semantic maps, and using the map of the explored regions during navigation as the condition to generate the map of the unknown regions, thereby realizing the semantic reasoning of the target object, i.e., diffusion as reasoning (DAR). Meanwhile, we propose the global target bias and local LLM bias methods, where the former can constrain the diffusion model to generate the target object more effectively, and the latter utilizes the common sense knowledge extracted from the LLM to improve the generalization of the reasoning process. Based on the generated map in the unknown region, the agent sets the predicted location of the target as the goal and moves towards it. Experiments on Gibson and MP3D show the effectiveness of our method.", "sections": [{"title": "I. INTRODUCTION", "content": "Embodied navigation, which requires an agent to use visual sensing to actively interact with its environment and perform navigation tasks, has seen rapid development driven by large-scale photorealistic domestic scene datasets [1], [2], [3] and advanced high-performance simulators [4], [5]. As a specific form of embodied navigation, Object Goal Navigation (ObjectNav) [6] is regarded as a key technology for enabling robotic intelligence. In ObjectNav task, the agent is placed in an unknown environment and is required to navigate to a user-specified object category (e.g. sofa) based on visual observations. Since the environment is unseen, when the target object is not visible, the agent needs to infer the potential locations of the target.\nThe intelligence of an embodied navigation agent is primarily manifested in its semantic understanding and reasoning capabilities of the world (e.g., TVs are in the living room, ovens are in the kitchen, and chairs are near tables). Prior work has made some research progress in how to learn and incorporate the contextual relationships between objects into ObjectNav systems. Some methods extract relationship information among objects, regions, or rooms, and leverage this prior knowledge to enhance the visual representation of end-to-end navigation models [7], [8]. Other end-to-end approaches directly learn how to map the visual representation to actions by interacting with the environment [9], [10]. However, end-to-end methods implicitly embed semantic prior knowledge and simultaneously learn localization, mapping, and path planning in an implicit manner, which leads to the problems of high computational load and limited generalization in unseen environments.\nAlternatively, modular methods tend to use a long-term goal policy module instead of the frontier exploration module, while also retaining the mapping and path planning modules in the traditional pipeline [11], [12], [13], [14]. However, the key issue remains how to represent and learn the contextual relations in the long-term goal policy module to perform semantic reasoning about the target location.\nSome modular approaches use supervised learning to directly learn target relations, such as predicting the nearest potential frontier on a local map [12], estimating the distance to the target [15], or anticipating the target's coordinates [16]. Other methods use the distance to the target as a reward to train modular-based reinforcement learning (RL) approaches [14]. However, both approaches fail to fully exploit the geometric and semantic cues in the map, as they implicitly and independently learn object relations. Therefore, learning the cooperative relationships among multiple objects in the scene has proven to be more reliable [13]. Furthermore, the ability of Large Language Models (LLMs) to extract and integrate knowledge for robotics has been widely studied [17]. The inherent common-sense knowledge in LLMs significantly improves the success rate of ObjectNav systems. However, zero-shot, training-free approaches [18] that rely solely on LLMs offer limited reliability when only sparse semantic information is available, as the reasoning capability of LLMs using their common-sense knowledge is diminished [19].\nTo alleviate the above problems, we propose an approach named DAR (Diffusion As Reasoning). This method combines the strengths of data-driven modular techniques with common-sense knowledge from LLMs about semantic and spatial relationships, enhancing the efficiency of ObjectNav systems. The proposed DAR method uses Denoising Diffusion Probabilistic Models [20] (DDPM), conditioning on the agent's memory of the environment to generate reliable semantic content in unknown areas. Following the self-supervised learning setup of DDPM, we replace the original 3-channel RGB data with semantic maps of the class channels, scaling up the denoising model accordingly. Our DAR is trained to predict pixel values in unexplored regions by leveraging both exploration memory and common sense knowledge. Exploration memory represents a local semantic map built from the agent's visual observations of the environment. Unlike standard image datasets, ObjectNav has limited training scene quantity and diversity. To improve generalization, we also incorporate general knowledge from LLMs (e.g. GPT-4 [21]). Unlike existing method [22], the DAR approach does not require frequent LLM queries during navigation for valuable decisions. Instead, it relies on a knowledge graph, pre-extracted from LLMs, that represents object relationships.\nIn summary, our study has the following main contributions:\n\u2022 We use a DDPM to generate semantic maps, trained on public datasets [3], [23]. We propose a global target bias method to generate more specific types of objects, which controls the channel bias of the initial noise in the sampling process.\n\u2022 We pre-extract a knowledge graph from LLMs based on semantic and spatial relevance, and introduce a local LLM bias method to inject common-sense knowledge into the sampling process. This guides the DDPM to generate more specific objects in particular regions, according to the suggestions provided by the LLM.\n\u2022 We propose the DAR method as a modular-based solution to the ObjectNav problem. By using the trained denoising model, we integrate globally and locally biased initial noise and use an existing local semantic map as a condition to predict the object distribution in unexplored regions within a certain range. This enables semantic reasoning with a diffusion model, predicting potential locations of the target object."}, {"title": "II. RELATED WORK", "content": "ObjectNav aims to navigate an agent to a specific object in a previously unknown indoor scene. Existing approaches fall into three categories [24]: end-to-end, modular, and zero-shot methods. End-to-end methods use reinforcement learning (RL) [25] or imitation learning (IL) [26] to map observations directly to actions. Previous RL-based methods focus on learning object relations [27], visual representations [28], auxiliary tasks [29], and data augmentation [30] to improve navigation. However, these approaches face challenges in real-world application as they require simultaneous localization, mapping, and planning, with additional difficulties in transferring from simulation to real-world environments due to perceptual differences [31].\nUnlike end-to-end methods, modular approaches [12], [13] break down ObjectNav tasks into separate components: mapping [11], long-term goal reasoning [19], and path planning [32]. The mapping module provides a general representation that separates perception from reasoning and planning, making it more suitable for real-world applications.\nZero-shot methods involve navigating to unseen targets that only appear during testing [33]. Existing approaches [34], [35] use Vision Language Models (VLMs) and Large Language Models (LLMs) to provide prior knowledge about these unseen targets. However, relying solely on pre-trained VLMs or LLMs limits reliability, especially when the robot detects limited semantic information. Our DAR is a modular method that uses a diffusion model to design a novel long-term goal reasoning module. It also incorporates the common-sense knowledge from LLMs, avoiding the reliability issues that come with relying solely on LLMs."}, {"title": "B. Visual semantic reasoning", "content": "In ObjectNav, visual semantic reasoning is meant to provide high-level information about the location of the target object. Considering the two extremes, when the semantic reasoning module is highly inefficient, the navigation system tends to explore aimlessly until it has traversed the entire room. On the other hand, when the semantic reasoning module can accurately and reliably compute the target's position, it can greatly improve the efficiency of the navigation system by avoiding exploration in ineffective regions and directly reaching the target.\nSK Ramakrishnan [12] proposed two complementary potential functions as a semantic reasoning approach to determine long-term goals in frontier areas. L2M [36] actively imagines environmental layouts beyond the agent's field of view and uses uncertainty in the semantic classes of unobserved areas to choose long-term goals. S Zhang [13] proposed training with an MAE mechanism to leverage both episodic observations and general knowledge for generating semantic maps of unknown areas, enabling long-term goal reasoning. This approach is similar to our DAR method. However, the MAE mechanism involves random masking of the established local map, causing information loss. In contrast, our DAR method fully utilizes the local map to guide a diffusion model for generating high-quality semantic maps."}, {"title": "C. Diffusion models", "content": "Diffusion models are a type of deep generative model that have recently gained attention for tasks like image generation [37], image translation [38], inpainting [39], and editing [40]. They consist of a forward diffusion stage, where Gaussian noise is gradually added to the data, and a reverse diffusion stage, where the model learns to recover the original data step by step.\nDenoising Diffusion Probabilistic Models (DDPMs), a key type of diffusion model, demonstrated the ability to generate high-quality images, with benefits like full distribution coverage, a stable training objective, and easy scalability [20][41][42]. Our DAR model leverages DDPMs to learn the distribution of objects in rooms, enabling the generation of high-quality room maps. We also adapt DDPM's image inpainting capabilities to predict unknown areas based on known conditions, offering a novel approach to the semantic reasoning needed in ObjectNav systems."}, {"title": "III. METHODOLOGY", "content": "The ObjectNav task involves an agent navigating to an instance of a given object category (e.g., toilet) in an unknown environment. At the start of each episode, the agent"}, {"title": "B. Self-supervised Diffusion Learning", "content": "In the field of RGB image generation, trained diffusion models can learn the latent statistical distribution of image features. We have observed that the spatial distribution of objects in indoor scenes also follows certain statistical patterns. Since diffusion models can generate high-quality RGB images, they should, with sufficient training, also be able to generate high-quality maps of room object distributions. Building on this idea, we used a room semantic map dataset to train an unconditional diffusion model.\nIn the diffusion process, the noisy data $x_t$ at step $t$ is calculated as follows:\n$x_t = \\sqrt{\\overline{\\alpha}_t} \\cdot x_o + \\sqrt{(1 - \\overline{\\alpha}_t)} \\cdot z_t, z_t \\sim N(0,1)$ (1)\nWhere $\\beta_t = \\Pi_{i=1}^t \\alpha_i$, $\\alpha_t = 1 - \\beta_t$ and $t \\in \\{1, 2, ..., T\\}$, with the original map $x_o$ and a fixed variance schedule $\\beta_t$, the noise $x_t$ can be sampled by Eq. 1.\nIf the variance schedule $(\\beta_t)_{t=1}^T$ makes $\\beta_T \\rightarrow 0$, then $x_T$ approaches the standard Gaussian distribution, namely $x_T \\rightarrow N(0, I)$. The training process is represented by reversing the forward diffusion process. By training a neural network model to predict the noise corresponding to each time step, the noise can then be removed one by one, gradually recovering the original data $x_0$. This process is described as Eq. 2, where $\\theta$ represents the parameters of the neural network model, and the network takes noisy data $x_t$ and time step $t$ as input to predict the mean $\\mu_{\\theta}(x_t, t)$ and covariance $\\Sigma_{\\theta}(x_t, t)$.\n$p_{\\theta}(x_{t-1}|x_t) = N(x_{t-1}; \\mu_{\\theta}(x_t, t), \\Sigma_{\\theta}(x_t, t))$ (2)\nHo et al. [20] proposed a simplified method, where the covariance $\\Sigma_{\\theta}(x_t, t)$ is fixed to a constant value and the mean $\\mu_{\\theta}(x_t, t)$ is rewritten as a function of noise, as follows:\n$\\mu_{\\theta} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\overline{\\alpha}_t}} \\cdot z_{\\theta}(x_t, t))$ (3)\nWith the simplified training objective, as follows:\n$L_{ulb} = E_{t\\sim[1,T]} E_{x_0\\sim p(x_0)} E_{z_t\\sim N(0,1)} ||z_t - z_{\\theta}(x_t,t)||^2$ (4)\nWhere $E$ is the expected value. As a result, the neural network model $n_{\\theta}$ no longer needs to predict both the mean and covariance simultaneously, but only needs to estimate the noise $z_{\\theta}(x_t,t)$ corresponding to $x_t$.\nAs shown in Figure 1, the input $x_0$ contains 18 binary semantic category channels (using the GIBSON dataset as an example). Additionally, we rescale the room semantic maps from the dataset to a resolution of 256 \u00d7 256. Following the training process of diffusion models for RGB images, the model architecture is consistent with guided-diffusion [37], with increased depth and channel count of the U-Net model. The trained diffusion model can generate a regularized RGB image from noise. Similarly, by using 18-channel Gaussian noise as input, the model progressively removes the noise and ultimately generates a semantic map. Note that the output of the denoising model does not produce a standard multi-channel binary-valued semantic map. Therefore, we use the argmax operation to post-process the results and obtain the output shown in Figure 3.\nNext, we raise another question: How can we make the generated semantic maps from the diffusion model contain more 'specified' objects? This problem is similar to the field of conditional image generation, but it is not feasible to train a conditional denoising model using classification ideas on multi-channel semantic maps, as the meaning of the map lies in the distribution of the objects it contains, and considering the overall category of the map is meaningless. In this paper, we propose a simple and effective way to address this question."}, {"title": "C. Semantic Reasoning with Local Map Condition", "content": "The observed local semantic map can be used as a condition to guide the diffusion model in generating the object distribution in the unknown regions, thereby enabling semantic reasoning of the target objects. The local map established for the explored regions is denoted as $x$. After cropping and scaling preprocessing, the unknown area in $x$ constitute the mask $m$. The known grid is represented as $x \\odot (1-m)$, and the corresponding unknown region is denoted as $x \\odot m$. We apply the RePaint [39] method, using Eq. 5 for the known region and Eq. 6 for the unknown region.\n$x_{t-1}^{known} \\sim N(\\sqrt{\\overline{\\beta}_t} \\cdot x_o, (1 - \\overline{\\beta}_t) \\cdot I)$ (5)\n$x_{t-1}^{unkown} \\sim N (\\mu_{\\theta} (x_t, t), \\Sigma_{\\theta} (x_t, t))$ (6)\nAs shown in Figure 5, $x_{t-1}$ and $x_{t-1}^{'}$ represent the sampled result from the local map and the model trained in the previous section, respectively.\n$x_{t-1} = (1 - m) \\odot x_{t-1}^{known} + m \\odot x_{t-1}^{unknwon}$ (7)\nThey are combined through the process represented by Eq. 7 and participate in the reverse process, ultimately generating a reasonable semantic content distribution in the unknown region of the local map."}, {"title": "D. Enhancing Generation with LLM Bias", "content": "Although we found that the diffusion model can effectively generate the object distribution in unknown regions, the object goal navigation domain lacks large-scale datasets, making it difficult to achieve satisfactory generalization through supervised training. Recently, the ability of knowledge extraction and integration within Large Language Models (LLMs) for robotics has been extensively investigated. LLMs can provide agents with common sense knowledge about the environment, which can improve the success rate of object goal navigation tasks [22][19].\nThe research in Section III-B showed that by changing the channel bias of the initial noise in the denoising model for map generation, the richness of each object in the final semantic map can be controlled. This bias is global. In contrast, this section proposes a local bias method that uses LLMs to predict the object distribution in different regions and applies specific biases in those regions, thereby exerting control over the diffusion model's map generation process. We collect the common sense knowledge by querying the LLM with positive-negative prompts [22] and Chain-of-Thought (CoT) prompting technology [43]. Additionally, we query the LLM from the dimensions of semantic and spatial relevance, and then fuse the statistical results of these two dimensions to generate the common sense knowledge matrix $M_{cs}$. Subsequently, we set the diagonal values of $M_{cs}$ to 0. Our goal is to use $M_{cs}$ to calculate the object distribution tendency for a specific frontier point in the local map. The frontier points are defined as the edges between explored free-spaces and unexplored areas on the local semantic map [12]. Specifically, based on the coordinates of the frontier point $f_p$, we calculate the normalized distance score between all objects in the local map $f_s = [s_0, s_1,\\ldots, s_n]^T$. The score $s_i$ represents the distance score of object i relative to $f_p$, and the closer the distance, the closer the score is to 1. Taking the Gibson dataset [3] as an example, n = 14, and the score for objects not present in the map is 0.\nNext, we compute $M_{cs} \\times f_s$ by matching the object proximity sequence corresponding to the frontier point in the local map with each row in $M_{cs}$. The object with the highest overall score (max $(M_{cs} \\times f_s)$) is taken as the prediction result. If the diagonal values of $M_{cs}$ are 1, the prediction results will tend to generate the objects closest to the frontier point $f_p$, which, for instances, would lead to generating more chair near around chair. Therefore, we set the diagonal values to zero to reduce the weight of the nearest objects, allowing the model to predict a greater variety of objects in the frontier regions.\nFor example, in Figure 6, the three frontier points (denoted as $F$) are predicted by $M_{cs}$ to most likely contain book. The 100 \u00d7 100 region filled around $F$ constitutes the frontier zone. When generating the initial noise for the diffusion model, we apply a bias in the book channel within the frontier zone, while the regions outside the frontier zone and the other object channels still use the unbiased $N (0, I)$ noise."}, {"title": "E. Object Goal Navigation with DAR", "content": "At each timestamp $t$ during navigation, the agent constructs a local semantic map $m_t$ based on sensor input. The semantic map contains multiple channels, with each channel composed of binary-valued pixels. The values in the unobserved regions are 0 across all channels.\nThe trained diffusion model only allows for the prediction of map data at a resolution of 256 \u00d7 256. Therefore, we crop the minimum bounding box of the known region from the input $m_t$, as shown by the blue dashed box in Figure 7. We then rescale the box region to a size of 256/e and pad it with zeros to 256 x 256, generating the processed map $m_t^d$. The value of e represents the range of unknown regions that the DAR model is used to predict. We set \u0454 = 120%. Next, we generate the initial noise with biases for the sampling process based on $m_t^d$. First, we apply the global target bias introduced in Section III-A. Then computing the frontier points $F_i$ of $m_t^d$, and utilize the common sense knowledge matrix $M_{cs}$ extracted from the LLM, as described in Section III-C, to estimate the most likely objects for each $F_i$. Finally, we apply the local bias.\nThe biased noise goes through the denoising process described in Section III-B, gradually producing a semantic map $m_d$ within the unknown area of the $m_t^d$. In the $m_d$, non-zero values in the target channel indicate the predicted potential location of the target object by the DAR model. These coordinates are restored to the local map $m_t$ through inverse resizing and used as the long-term goal $g_t$.\nIt is worth noting that in the initial stage of navigation or when the sensors receive limited information, even with the introduction of global target bias, the map generated by the DAR model in the unknown regions may not contain the target object. In other words, the DAR model may not provide valuable information to the navigation system. In such cases, we set the long-term goal to the nearest frontier point.\nOnce the long-term goal is set, the agent just needs to move from its current location to the goal $g_t$. Following previous work [11], we use the Fast Marching Method [32] to calculate the shortest path between the current location and the goal. The local policy then follows this path by taking deterministic actions to guide the agent.\nWhile the diffusion model can effectively predict the object distribution in unknown regions, its nature also makes the DAR method computationally expensive. To address this drawback, we do not call the DAR method to reason about the long-term goal at every timestamp. Instead, we adopt a strategy where we only perform the reasoning when the current location is near the $g_t$, or when the agent is moving away from $g_t$. Subsequent experiments have shown the effectiveness of this approach, as the DAR model can still provide a significant performance boost to the object goal navigation system, even with fewer reasoning steps."}, {"title": "IV. EXPERIMENT", "content": "We evaluate our DAR on standard ObjectNav datasets: Gibson [3], Matterport3D (MP3D) [1], and Habitat-Matterport3D (HM3D) [23]. For Gibson and MP3D, we follow the setup of [12] and [13]. Specifically, for Gibson, we use 25 training and 5 validation scenes from the Gibson tiny split. For MP3D, we use 56 training and 11 validation scenes, with 21 goal categories and 2,195 validation episodes. For HM3D, we follow [44], using 80 training and 20 validation scenes, with 6 goal categories and 2,000 validation episodes.\nFor evaluating the navigation performance, we adopt three standard metrics following [11] and [12]: 1) SR: the ratio of success episodes. 2) SPL: the success rate weighted by the path length, which measures the efficiency of the path length. 3) DTS: the distance to the goal at the end of the episode."}, {"title": "V. CONCLUSIONS", "content": "In this work, we propose a modular-based self-supervised method for the object goal navigation task, named DAR. We demonstrate the effectiveness of applying diffusion models to visual semantic reasoning. We first trained Denoising Diffusion Probabilistic Models based on the indoor semantic map dataset, and successfully generated high-quality and reasonable new map samples, indicating that the trained DDPM can learn the statistical distribution patterns of the spatial relationships between indoor objects, and we point out that this pattern is highly beneficial for improving the success rate and efficiency of the agent in the ObjectNav task. Subsequently, we use COT and positive-negative prompting techniques to query LLMs to obtain common sense knowledge, and incorporate it into the diffusion model's reasoning process through the proposed local bias method. In summary, our DAR method explicitly leverages the structured knowledge from the knowledge graph extracted from LLM and implicitly utilizes the unstructured knowledge from the pre-trained diffusion model. A series of experiments have shown that the DAR method achieves comparable performance to previous methods."}]}