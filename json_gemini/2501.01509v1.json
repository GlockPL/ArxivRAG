{"title": "AI-Enabled Operations at Fermi Complex: Multivariate Time Series Prediction for Outage Prediction and Diagnosis", "authors": ["Milan Jain", "Burcu O. Mutlu", "Caleb Stam", "Jan Strube\u00b9", "Brian A. Schupbach", "Jason M. St. John", "William A. Pellico"], "abstract": "The Main Control Room of the Fermilab accelerator complex continuously gathers extensive time-series data from thousands of sensors monitoring the beam. However, unplanned events such as trips or voltage fluctuations often result in beam outages, causing operational downtime. This downtime not only consumes operator effort in diagnosing and addressing the issue but also leads to unnecessary energy consumption by idle machines awaiting beam restoration. The current threshold-based alarm system is reactive and faces challenges including frequent false alarms and inconsistent outage-cause labeling. To address these limitations, we propose an Al-enabled framework that leverages predictive analytics and automated labeling. Using data from 2, 703 Linac devices and 80 operator-labeled outages, we evaluate state-of-the-art deep learning architectures, including recurrent, attention-based, and linear models, for beam outage prediction. Additionally, we assess a Random Forest-based labeling system for providing consistent, confidence-scored outage annotations. Our findings highlight the strengths and weaknesses of these architectures for beam outage prediction and identify critical gaps that must be addressed to fully harness AI for transitioning downtime handling from reactive to predictive, ultimately reducing downtime and improving decision-making in accelerator management.", "sections": [{"title": "1 Introduction", "content": "The Fermilab accelerator complex is the United States' flagship facility for High Energy Physics (HEP). The complex consists of a 400 MeV proton Linac, an 8 GeV rapid-cycling synchrotron Booster, a 150 GeV ramped Main Injector, an 8 GeV Recycler storage ring, the Muon Campus Delivery Ring, two high-power Neutrino target systems, 120 GeV fixed target beam lines and many associated transfer lines. The operation and maintenance of these dozen or so \u201cmachines\" and their associated systems requires approximately 400 Accelerator Division employees plus outside contractors and auxiliary laboratory support personnel and activities. The accelerator systems vary in age, constituent technology, and mode of operation, and their management is made possible by a sophisticated controls infrastructure. While legacy software tools have been successful in meeting laboratory needs over the lab's fifty years, modern computing resources and advances in Artificial Intelligence (AI) techniques applied to large data sets are required to transform the management of such large complexes for the better. Subsequently, many organizations operating similarly large facilities have moved from a reactive approach to a \"data first\" approach, using copiously available data to improve real-world practices.\nIn an accelerator complex, the control room monitors the beam and responds to situations when the beam goes down. For instance, for the linear accelerator (Linac) alone, the control system monitors and issues commands to > 4000 control system parameters at frequencies ranging from 15 Hz to once every few minutes. Two different permit signals control the presence of beam in the upstream and downstream sections of the Linac, respectively. The beam could be absent for several known and unknown reasons, the latter of which operators have to spend time to investigate. When either beam permit goes down, the operators intervene to first ensure that it is not a false alarm. Once operators confirm the outage, they gather information by visually analyzing data and determine the right action to rectify the issue. A beam outage leads to downtime causing wasted time and energy (Jain et al. 2022; Strube et al. 2023).\nHowever, this legacy way of monitoring the beam and responding to an outage poses four major concerns: (1) false alarms waste operator time, (2) it is reactive rather than predictive, (3) the number of devices to monitor and the amount of data exceed human capacity to process, and (4) inconsistent or incorrect labeling complicates bookkeeping and higher-order analytics of faults.\nEven during normal beam operations, the operations staff are notified of approximately 15,000 alarms daily, along with additional status indicators, a number that can increase significantly during study periods and certain operational phases. Usually, this high rate of false alarms distracts operators from concerns (also defined as precursors) that could indicate an impending outage. And since the response is mostly reactive, operators usually have little time to avert the outage.\nOnce a beam outage begins, certain measures can be taken so that, while staff work to restore operation, those machines awaiting the beam may have their power consumption reduced. This could be a partial or complete operating power reduction, depending upon context. Such power conservation is normal practice in simple cases, but requires the judgment of a human expert as to the likely duration of the outage, and the benefit to be gained by taking steps which must then be reversed when the outage ends. However, this judgment relies on the examination of those devices potentially related to the outage one by one. The list of devices to analyze is often derived from intuition and past experience. The time spent by human operators on pattern-hunting from large-scale data to validate and understand anomalous conditions currently occupies a significant portion of their response time to a beam outage. And that is where proper labeling of the outage could be helpful as it can provide a rough estimate of outage duration. Currently, the outage is labeled subjectively and can result in inconsistent or incorrect labeling.\nWe exploit the predictive power of AI applied to copiously available time-series data generated by the Fermilab accelerator control system to go beyond the present system of threshold-based alarms. With appropriately selected and prepared data, algorithmically optimized, nonlinear functions learned on our large datasets can detect and predict emerging anomalous conditions not visible to traditional alarm-setting methods. The proposed pipeline (see bottom half of Figure 1) enables the use of ML/AI that augments the data flow to the control room with analytics of outages, reduces the time to label them meaningfully and minimizes the number of incorrect or inconsistent labels.\nThe task of predictive monitoring of beam is formulated as predicting the status of upstream and downstream Linac permit bits using time series data from thousands of Linac devices. The predictive power of deep learning (DL) methods can level up the operations by finding precursors related to an anomaly for predicting an outage and help operators transition from being reactive to being predictive. However, to avoid alarm fatigue, we must reduce the number of false positives while maintaining good predictive power. In this study, we evaluate state-of-the-art (SOTA) DL architectures-spanning recurrent, attention-based, and linear models for beam-outage prediction.\nFor the second task of automatically labeling outage causes, we trained and evaluated a Random Forest model that labels an outage when either beam permit goes down, along with a label confidence score.\nThe key contributions of this work are summarized below:\n1. A thorough data collection campaign from the FNAL accelerator complex, including manual labeling of outages by operators, ensuring high-quality datasets for analysis.\n2. The comprehensive evaluation of SOTA multivariate time-series prediction DL architectures employed for beam-permit prediction.\n3. A performance evaluation of an automated outage labeler.\n4. The detailed discussion of key findings and lessons learned to inform and guide future efforts in this domain."}, {"title": "2 Related Work", "content": "The literature on deep learning for operational efficiency and reliability in large-scale facilities can be broadly classified into two categories based on the timing of the response: (1) anomaly detection and diagnosis, and (2) predictive maintenance. This section reviews SOTA techniques proposed in the literature for both categories.\nAnomaly Detection and Diagnosis Anomaly detection and diagnosis (AD&D) is a reactive method that involves near real-time analysis of massive data streams from thousands, or even millions, of sensors to identify unexpected behavior with precision and timeliness. A detailed review of time series anomaly detection can be found in (Chandola, Banerjee, and Kumar 2009; Schmidl, Wenig, and Papenbrock 2022). Recent advancements include models such as LSTM-VAE (Park, Hoshi, and Kemp 2018), MS-CRED (Zhang et al. 2019), and TAnoGAN (Bashar and Nayak 2020). These architectures predominantly focus on reconstructing input sequences, training models exclusively on normal data with the assumption that anomalous instances will be poorly reconstructed and thus stand out.\nDetecting anomalies in complex time series data is often challenging due to their context-dependent nature, as anomalies can encompass any unusual, irregular, or unexpected observations, making it difficult to establish a clear definition. Yang et al. (Yang et al. 2023) proposed DCdetector, a dual-attention representation-learning architecture that uses contrastive learning to produce representations capable of effectively distinguishing individual instances, offering a promising approach for time series anomaly detection. Another recent study proposed TFAD (Zhang et al. 2022) a time-frequency domain analysis time series anomaly detection model utilizing both time and frequency domains for performance improvement.\nDespite these advancements, a key limitation of AD&D techniques is their reactive nature. While they excel at diagnosing issues, they fall short in facilitating proactive measures to prevent outages or address potential problems before they arise, a critical requirement for this study.\nPredictive Maintenance Predictive maintenance, in contrast, forecasts the system's future state over single or multiple time steps based on data in a look-back window, enabling proactive fault diagnosis through accurate predictions. Traditionally, statistical models such as ARIMA, AR, and exponential smoothing models (Box et al. 2015) have been used for univariate time series forecasting. However, their inability to capture multivariate nonlinear relationships between variables, and the increasing availability of \"big\" data and compute, led to the rise of deep learning methods, such as recurrent networks (e.g. LSTM (Hochreiter 1997)) for multivariate time-series forecasting. Further, these DL-based forecasting architectures were extended to handle auxiliary information, such as static and time-varying features. Inspired by the success of attention mechanisms in natural language processing (NLP) and computer vision (CV) (Vaswani 2017), researchers began integrating attention into time series forecasting to capture long-term dependencies. For example, a dual-stage attention-based re- current neural network was introduced for time series prediction (Qin et al. 2017), while the Temporal Fusion Transformer (Lim et al. 2021) was proposed to merge high-performance multi-horizon forecasting with interpretable temporal dynamics. Nevertheless, transformer models face challenges with high computational costs, prompting the development of more efficient variants. Informer (Zhou et al. 2021) and Autoformer (Wu et al. 2021) address efficiency bottlenecks through memory-efficient attention designs for long-term forecasting, while FEDformer (Zhou et al. 2022b) and FiLM (Zhou et al. 2022a) employ Fast Fourier Transformation to enhance the extraction of long-term dependencies. Despite the advancements, recent studies (Zeng et al. 2023) reveal that linear models, such as N-BEATS (Oreshkin et al. 2020) and TS-Mixer (Ekambaram et al. 2023), can outperform attention-based networks for multivariate forecasting.\nAlthough predictive maintenance has been extensively studied in areas such as energy efficiency (Jain et al. 2019), manufacturing (\u00c7\u0131nar et al. 2020), and several other industrial systems (Nguyen and Medjaher 2019; Shiva et al. 2024), there has been little to no comprehensive evaluation of state-of-the-art multivariate time series models for beam outage prediction and labeling using real-world data."}, {"title": "3 Problem Formulation", "content": "Beam-Permit Prediction The control system data for the Fermilab accelerator complex comes in the form of a time series with N analogue devices, D digital bit devices, and F future covariates. The presence of beam is controlled by a permit, with 1 allowing beam and 0 prohibiting beam, that combines threshold-based information from this data stream to ensure safe operations. Our task is to learn F (Eq 1) that can predict the beam permit, $X_{bp} \\in \\mathbb{R}^{L_f \\times 1}$, bp \u2208 D for the look-forward window $L_f$, given the historical observations from analogue devices, $X_n \\in \\mathbb{R}^{L_b \\times N}$, and historical and future data from future covariates, $X_f \\in \\mathbb{R}^{(L_b+L_f) \\times F}$, wherein, $L_t$ is the size of the look-back window.\nF: (\\{X_n\\}, \\{X_f\\}_{L_b+G+L_f}) \\rightarrow X_{bp}\\big|_{t=L_b+G} (1)\nIn Equation 1, we introduce a gap G between the look-back and look-forward windows. This allows us to strike a good balance between the uncertainty due to larger look-forward windows and the increasing uncertainty on the prediction of events further in the future. Appendix B studies the effect of this gap quantitatively.\nOutage Labeling Once the beam permit goes down, we label the outage cause. The operator's labeling relies on subjective experience rather than standardized nomenclature, resulting in inconsistencies. To address this, we train a classifier $F_r$ that assigns a label L to an outage at time t', when the beam permit $X_{bp}$ goes down, i.e., $X_{bp}|_{t'} = 0$ and $X_{bp}|_{(t'-1)} = 1$. Our classifier is trained on $X_n\\in \\mathbb{R}^{L_b \\times (N+D)}$, historical observations from all devices ending at time step t'. In particular, we take the difference between the data at the time of outage and the average of the data from the last k time steps. Let $F_a: \\{X_n\\}|_{t=(t'-L_b)} \\rightarrow X_n$ be our aggregation function."}, {"title": "4 Experimental Setup", "content": "This study focused on data from 2703 devices of the 400 MeV proton Linac, collected in 2024. Every year, the accelerator complex runs for a specific period (usually November-July) with major activities happening between March and July; and shuts down between August and November for maintenance. Data during regular operations is stored in rolling buffers of fixed size. A significant amount of effort went into building and deploying a stable data collection and storage pipeline outside of these buffers. Next, we discuss the data pipeline, preprocessing steps, and outage data in detail.\nData Collection and Processing\nThe accelerator control system's Data Logger nodes record data streams into circular buffers. To store this data for a longer period than the lifetime of the circular buffers, this project developed a data acquisition pipeline that writes the data to long-term storage in Parquet (Vohra 2016) format with the lossless snappy compression (Google Inc. 2022). Missing values, e.g., from faulty reads are interpolated using a forward fill algorithm for both offline and online processing. The 2703 devices include 1719 readings, 842 settings, and 142 status bits, and are stored in the parquet.snappy file format once per hour.\nOutages For training our models, we extract windows around the time when the beam permit $X_{bp}$ drops. Because there are frequent fluctuations of the permit, we skip \"outages\" of less than 10 seconds. We search through the recorded files and save a window of 30 seconds before the outage starts and 10 seconds later. We find 205 such occurrences in our data, their causes initially unlabeled, and we explore three different ways of assigning labels.\nOperator-Labeled Outages Operators in the control room investigate the cause of outages and assign labels based on their findings and prior experience, generally only for outages lasting longer than one minute. These are logged with wall clock time and duration in minutes. We match these labels with an outage in our set based on start time and refine the operator-assigned start time and duration with the 15-Hz clock from the outage data. Our data contains 80 outages that were labeled by operators. Labels assigned by operators distinguished 9 outage types occurring in 12 different locations. Labels like \"KRF1 CS Fault\" give combined information of the location (KRF1) and the outage type (CS Fault), allowing an operator to diagnose and address outages. We used these labels as the basis for our two automated methods described below.\nBit-Labeled Outages We identified 12 digital devices as providing bit-wise system status. These devices store multiple bits of information, where each bit acts as a binary indicator for a specific system event, including high voltage conditions or spark trip occurrences. We analyzed these devices with respect to the operator-labeled outages and matched patterns of bit flips with operator labels. We observed that within a few seconds of the outage, these devices show distinct patterns for different types of outages. Leveraging this information, we developed a bit-based labeler to categorize outages. This tool enables us to label 74 out of 150 previously unlabeled outages within 2 seconds of an outage (to account for possible time slippage between the permits and the status bits). In addition to helping us with previously unlabeled data, it also enhances our understanding of operator-labeled data by overcoming inconsistencies and ambiguity in human-generated labels.\nDecision Tree-Labeled Outages The digital devices aggregate information from different sources, but they fail to account for higher-order correlations between variables. We have trained on the devices a random-forest classifier that serves as an important cross-check to the bit-based labeler. Additionally, this approach allows us to study specific signatures of a given outage, which we will use to study precursors in the future. The details of the random forest classifier were discussed in Section 3.\nA breakdown of the duration of different outages is shown in Figure 2, demonstrating the power of the automated method that is able to assign labels to many instances without human annotation. We will be further able to refine the \"LRF\" category with additional labeled data. The large number of outages in the \"Other\" category, which also has the largest number of cases without human annotation, suggests that we are missing data that would allow us to characterize these cases.\nNon-Outage Instances To avoid a training bias from only training on outages and to reduce the number of false positives, we include in our training data periods without beam outage. This data is created from periods where the beam permit was up for at least 30 consecutive minutes, cropping a 40-second window at the 20th minute within that window. We do not crop more than one such window from any given one-hour file to avoid overlaps. 427 such non-outage instances are included in our training data.\nData Loading and Windowing\nBeam Permit Prediction The data is loaded from the parquet.snappy files and converted into overlapping windows for model training and inference using TrainDataset and InferenceDataset dataloader APIs from Darts (Herzen et al. 2022). These APIs allow us to specify static, past, and future covariates conveniently. For the beam-permit prediction model, the look-back window size, $L_b$, is set to 30 ticks (2s), the look-forward window size, $L_f$, is set to 60 ticks (4s), and the gap G between the look-back and the look-forward windows is set to 30 ticks (2s). These values were chosen as the model's performance converged at these settings. Further details on the sensitivity analysis of these parameters can be found in Appendix \u0412. The stride is fixed at 1, and the feature dimension size is 1719 because the look-back window only includes analogue devices.\nWe used 75 of the 125 beam-permit-labeled outage instances, 40 of the 80 operator-labeled outage instances, and 375 of the 427 non-outage instances for the model training. To eliminate training bias, the data files were shuffled prior to training. The validation data consisted of 10 beam-permit-labeled outage instances and 21 non-outage instances. The test dataset included the remaining 40 beam-permit-labeled outage instances, 40 operator-labeled outage instances, and 31 non-outage instances.\nOutage Labeler For the outage classification task, the look-back window size L is set to 6 ticks (0.4s). The feature dimension is 2703, including both analogue and digital devices for maximal predictive power. These preprocessing settings were selected alongside random forest hyperparameters during tuning. The data for outage classification is limited to the 80 operator-labeled outage instances. We used 8-fold cross-validation during hyperparameter tuning. The final model was trained on all 80 operator-labeled outages for deployment.\nImplemented Models\nBit-Permit Prediction We choose SOTA models available in Darts from all three architecture categories: (1) recurrent networks, (2) attention-based networks, and (3) linear networks. In recurrent networks, we trained an LSTM (using BlockRNNModel) with two recurrent layers each with a hidden dimension of 25. Attention-based networks include the vanilla transformer (Vaswani 2017) and linear networks include N-BEATS (Oreshkin et al. 2020), N-HiTS (Challu et al. 2023), TiDE (Das et al. 2023), and TSMixer (Ekambaram et al. 2023). We adopt default hyperparameters for attention-based and linear-networks.\nThough Temporal Fusion Transformer (TFT) (Lim et al. 2021), N-linear, and D-linear (Zeng et al. 2023) architectures have demonstrated promising performance on benchmark datasets, we opted not to include them in our study. Despite extensive hyperparameter tuning and implementing recommendations from published studies, we were unable to achieve acceptable results with these architectures. TFT failed to detect any outage, N-linear and D-linear had a false positive rate of 100%. While open-source libraries offer accessible implementations of advanced deep learning architectures, we believe users would benefit from more clear guidelines on their effective use.\nThe subset of outage instances that have operator labels as ground truth is considerably smaller than the set of outage instances that can be used for beam-permit prediction. Therefore, we choose a classical machine learning algorithm, Random Forest (Ho 1995) rather than a neural network-based approach for better robustness in the small data regime. However, while neural architectures such as RNNs and transformers are designed for time series data, random forests expect fixed-size vector inputs. Therefore, our labeler consists of a fixed linear aggregation across the time dimension followed by random forest classification."}, {"title": "Model Training", "content": "Platform The models were trained on a single Nvidia\u2122 DGX-2 \"Ampere\" A100 GPU (108 SMs) with 40GB HBM2 memory/GPU and two-way 128-core AMD EPYCTM 7742 CPUs at 2.25GHz, 256MB L3 cache, 8 memory channels, and 1TB DDR4 memory. The models were developed using PyTorch 2.4.0+cu12 (Paszke et al. 2019) and Darts v0.30.0 (Herzen et al. 2022) and trained using CUDA 12.0. The Random Forest was developed using scikit-learn (Pedregosa et al. 2011) and trained on Intel(R) Xeon(R) CPU E5-2620 v4 at 2.10GHz with 16 CPUs and 256K L2 cache.\nTraining Parameters All the models for beam-permit prediction are trained for 500 epochs with a batch size of 254. For training, we used AdamOptimizer with following parameters: learning_rate = 0.0005, clipnorm = 1.0, and clipvalue = 0.5. An EarlyStopping callback to monitor val_loss was used with the following parameters: min_loss = 1e \u2013 06, patience = 10, mode = min, and restore_best_weights = True. We also employed torch.optim.lr_scheduler.ExponentialLR with gamma = 0.999 to adjust the learning rate as training converged. For the random forest, we use the classifier from sklearn.ensemble.RandomForestClassifier with n_estimators = 200 and min_samples_split = 2.\nLoss Function Mean squared error (MSE) between the actual beam permit and the predicted beam permit is the loss function for training beam-permit prediction model. Random forest uses the Gini impurity as the split criterion."}, {"title": "5 Results", "content": "The performance of various architectures was assessed based on two primary criteria: prediction accuracy and computational efficiency. Prediction accuracy measures the number of outages detected before the permit went down (n_early). We also capture false negatives, where the system failed to detect a beam outage, and false positives, identified on validation data where no outage was expected, yet the system flagged one.\nIn real-world applications, accuracy alone isn't enough; the model's ability to integrate seamlessly with operations is equally critical. Therefore, in terms of computational efficiency, we compare the training time, inference time, and model size across different models.\nBeam Permit Prediction\nPrediction Accuracy: Table 1 compares the prediction accuracy of all models on the test data, including 40 operator-labeled outages, 40 beam-permit-labeled outages, and 31 non-outage instances. The results indicate that LSTM achieves the highest early detection rate among all models. The Transformer model ranks second in early detection, with a slightly lower false positive rate than LSTM. N-HiTS ranks third in early detection performance, offering a marginally lower mean squared error (MSE) compared to LSTM. Filtering out beam-permit fluctuations, instances where the beam permit drops for less than 10 seconds, from the training data has proven effective in reducing the false positive rate and preventing the models from predicting random fluctuations in the beam-permit signal."}, {"title": "6 Discussion", "content": "In this section, we present key insights from our analysis and the lessons learned, highlighting open problems that need to be addressed in future work to enable more efficient predictive operations in large facilities.\nLSTM outperforms SOTA DL architectures. Our analysis of SOTA DL architectures for beam outage prediction reveals that LSTM outperforms attention-based networks and linear networks across multiple dimensions. Since these models are deployed for near-real-time inference, we considered the computation aspects as well. That said, transformers offer the added advantage of interpretability, and to capitalize on this, we plan to incorporate more efficient variants of transformer architectures in the future.\nRandom forest labeler demonstrated 82.1% accuracy on operator-labeled outages. The random forest classifier was able to achieve a relatively high mean accuracy of 82.1%. However, its Macro F1-score was less impressive, with a mean of 0.691. Here, we see that the random forest classifier has difficulty handling the high class imbalance of the \"Other\" class, which has only 3 observations.\nHowever, an advantage of the random forest is fast training and inference time. The model took 0.394s to complete training on all data and takes only 0.012s to label one instance of data. The fast training time is beneficial for a deployed model that may need to be retrained or re-tuned in the future, and the fast inference time is necessary for real-time use. The model's memory usage is comparable to the networks used for inference, taking up 3.76 MB. This model serves as an important cross-check of the bit-based labeler, with a high degree of consistency between the two. However, because of the easier interpretability of the random forest and direct connection to individual variables, we will build on this method in the future to study specific signatures of outages and further refine our utilization of pre-cursors for the outage prediction.\nInterpretability is important. Operators are not only interested in predictions but also in the interpretability of those predictions. For example, which features at what specific times are correlated with the prediction. While significant progress has been made in interpretable ML for text and image data, gradient-based techniques like SHAP and Gradients struggle with multivariate time series (MTS). In our initial attempt, gradient-based saliency maps failed to distinguish important features, assigning high saliency to almost all features at each time step. This issue, also noted in recent literature (Ismail et al. 2020), was partially addressed with Temporal Saliency Rescaling (TSR), though it is only designed for classification tasks. We look forward to future improvements in interpretability for MTS prediction.\nSignificance of local normalization. In the current implementation, hourly data is normalized by centering each feature around the mean and scaling to unit variance. This ensures that input features are comparable. Since the normalization is applied locally to each file, it prevents the model from capturing changes in system settings over time, which helps in identifying abrupt local changes. However, this approach may obscure long-term shifts and trends. We plan to study this in future and explore alternative normalization techniques that can also preserve global trends and periodic patterns when required. This would enhance the model's ability to capture dynamics over extended time frames.\nData loading is a time-consuming task. Time series data differs from other sequential data types like text, audio, and video in several ways. The order of data is crucial, limiting parallelism during loading and potentially causing load imbalance. Additionally, time series data involves overlapping windows, requiring more data to be loaded than needed, especially when data spans across multiple files. Lastly, unlike text or audio data, which have natural endpoints, time series data belongs to an infinite space, with patterns existing at various time scales. This results in high computational demands and creates a bottleneck in training and inference."}, {"title": "7 Conclusion", "content": "Operations at large complexes, such as FNAL, remain predominantly reactive even today. Advancements in AI, particularly in multivariate time series modeling, offer the potential to shift operators from being reactive to being predictive. In this study, we studied SOTA multivariate time series prediction models to predict beam outages in near real-time. Our analysis on real-world data highlights the superior performance and efficiency of LSTM over SOTA linear and attention-based networks for this task. We also introduced a labeler that can automatically label the outages with 82.1% accuracy. The beam prediction model and outage labeler are already deployed at FNAL control rooms for real-world impact assessment. In addition to addressing gaps related to interpretability, as identified in the Discussion section, the future work will explore advancements in the space of continual learning and transfer learning to achieve similar performance at scale and over time."}]}