{"title": "INTEGRATING AUDIO NARRATIONS TO STRENGTHEN DOMAIN GENERALIZATION IN\nMULTIMODAL FIRST-PERSON ACTION RECOGNITION", "authors": ["Cagri Gungor", "Adriana Kovashka"], "abstract": "First-person activity recognition is rapidly growing due to\nthe widespread use of wearable cameras but faces challenges\nfrom domain shifts across different environments, such as\nvarying objects or background scenes. We propose a multi-\nmodal framework that improves domain generalization by in-\nte grating motion, audio, and appearance features. Key contri-\nbutions include analyzing the resilience of audio and motion\nfeatures to domain shifts, using audio narrations for enhanced\naudio-text alignment, and applying consistency ratings be-\ntween audio and visual narrations to optimize the impact of\naudio in recognition during training. Our approach achieves\nstate-of-the-art performance on the ARGO1M dataset, effec-\ntively generalizing across unseen scenarios and locations.", "sections": [{"title": "1. INTRODUCTION", "content": "With the growing prevalence of wearable technology and\nfirst-person cameras, first-person activity recognition has\nemerged as a crucial area of research [1, 2, 3, 4, 5]. This\nfield is vital for real-world egocentric vision applications,\nfrom human-robot interaction to personalized assistance.\nHowever, a significant challenge in developing robust action\nrecognition models is the issue of domain shifts-variations\nin environmental contexts, objects, and activities that can\ndrastically affect the performance of these models. Some\nworks [6, 7, 8, 9, 10] rely solely on visual features to gen-\neralize across different domains. In this paper, we explore\nthe potential of multimodality, specifically the integration\nof motion and audio with appearance, to enhance domain\ngeneralization in first-person action recognition tasks.\nOur intuition is that domain shifts arise heavily from vari-\nations in the spatial semantics of videos, such as changes in\nthe type of objects or the background, which we refer to as the\nappearance modality. These variations can lead to a drop in\nmodel performance when applied to new, unseen data. How-\never, the motion and audio modalities capture temporal dy-\nnamics that remain more consistent across different domains.\nFor example, while objects being 'cut' in different do-\nmains, such as bamboo stick with knife (in building in In-"}, {"title": "2. METHOD", "content": "Each training sample consists of a video clip paired with cor-\nresponding audio, visual narration and audio narration. The\nformer is provided in the dataset we use [10], while we use\nPengi [14] to generate the latter. While we adopt the approach\nfrom [10] of using separate frozen encoders for each modal-\nity to extract base features, our method differs by incorporat-\ning the audio modality and utilizing distinct encoders for each\nmodality, instead of a single encoder for fused appearance and\nmotion features. Specifically, we train (from scratch) separate\nencoders $f_{appearance}$, $f_{motion}$, and $f_{audio}$ to derive domain-\ngeneralizable features ap, m, and a for appearance, motion,\nand audio. Additionally, we train a separate encoder $f_{text}$\nextracts the features of visual narration t and audio narration\nt. See Fig. 2. The action prediction \u0177 is generated by the\nclassifier h based on the fused multimodal embedding. The\ncross-entropy loss $L_c$ is computed between the true and pre-\ndicted action labels, y and \u0177.\nBefore training, we calculate consistency ratings for each\nvideo sample using a LLM [11], assessing the degree to\nwhich the audio and visual narrations correspond semanti-"}, {"title": "2.2. Consistency Ratings to Enhance Fusion", "content": "cally. This information is used to control fusion. Comput-\ning consistency between audio and visual content through\ntext captures abstract concepts that raw features might miss.\nWhile features emphasize low-level details, text-based evalu-\nations ensure that the audio and visual content correspond at\na deeper, conceptual level.\nWe use the following prompt obtain consistency ratings:\nRate the consistency between two narrations from the same\nvideo out of 100. The first narration describes the visual\naspect, and the second describes the audio. Consider how\nwell the audio narration overlaps with and complements the\nvisual narration. Output only the percentage score.\nThe consistency ratings r are then used as weights to mod-\nulate the contribution of the audio embeddings a before the\nconcatenation of appearance, motion, and audio features, as\nshown in Fig. 2. Specifically, only during training, the au-\ndio embeddings are scaled by the consistency rating r, such\nthat $a = a* r$. This consistency-weighted audio approach en-\nsures that audio information with strong semantic alignment\nto the visual content exerts a greater influence on the final\nprediction. This approach enhances the quality of audio rep-\nresentations during training, minimizing the impact of noisy\nor irrelevant audio cues and improving the overall robustness."}, {"title": "2.3. Text Guided Alignment", "content": "Aligning visual features with narrations enriches the model\nwith domain-invariant, human-like understanding, enhancing\nits ability to generalize across domains [13]. As discussed\npreviously, we generate audio narrations that are specifically\ntied to the audio content. The LLM in audio captioner Pengi\n[14] mimics human-like understanding, providing audio-\nspecific, semantically rich descriptions. Since Pengi uses a\nseparate encoder for audio feature extraction, its generated\naudio narrations are initially not aligned with our model's\naudio features a. Thus, in our approach, we align audio fea-\ntures a with these audio narration features t, while aligning\nappearance features ap and motion features m with visual\narration features t using contrastive learning.\nGiven a batch of samples B = {(api, mi, ai, ti,t\u0303i)}i=1B,\nwe frame the alignment as noise contrastive estimation [18].\nSpecifically, $L_{ap\u2192t}$ treats the appearance api as the anchor,\nwith other narrative texts serving as negatives, and minimizes:\n$L_{apt} = \\frac{1}{B} \\sum_i -log \\frac{exp(s(a_{pi}, t_i)/\\tau)}{\\sum_j exp(s(a_{pi}, t_j)/\\tau)}$\nwhere s(, ) is the cosine similarity and 7 is a learnable tem-\nperature. In a similar manner, $L_{t\u2192ap}$ uses the text $t_i$ as the\nanchor, with other appearance features as negatives. These\nlosses are then combined to create the appearance-text align-\nment loss $L_{ap,t} = L_{t\u2192ap} + L_{ap\u2192t}$. Likewise, motion-text\nalignment $L_{m,t}$ and audio-text alignment $L_{a,t\u0303}$ are computed,\nand all these alignment losses are aggregated into a total\nalignment loss $L_{align} = L_{ap,t} + L_{m,t} + L_{a,t\u0303}$.\nTo form the overall training objective, we combine the\nalignment loss with the cross-entropy classification loss:\n$L = L_c + \\lambda L_{align}$        (2)\nwhere \u03bb = 0.1 is used to weight the alignment loss."}, {"title": "3. EXPERIMENTS", "content": "Implementation details. We leverage the frozen pretrained\nSlowFast model [19], utilizing its slow pathway for captur-\ning appearance features and its fast pathway for extracting\nmotion features. For audio features, we employ BEATs [20],\nwhile CLIP-ViT-B-32 [12] is used to extract text features. The\ntrained encoders f each consist of two fully connected layers,\neach followed by ReLU activation and Batch Normalization\n[21]. We use batch size 128 for 50 epochs with the Adam\noptimizer [22]. The learning rate is initially set to 2e-4, with\ndecay factor 10 applied at epochs 30 and 40.\nDataset. ARGO1M [10] is an egocentric video dataset\ncurated from Ego4D, specifically designed to analyze sce-\nnario and location-based domain shifts. The dataset includes\n10 distinct train-test splits, ensuring that the domains of the\nsamples both scenario and location do not overlap be-\ntween the training and test sets. The test set scenarios and\ngeographic locations include Gardening in Pennsylvania (Ga,\nUS-PNA), Cleaning in Minnesota (Cl, US-MN), Knitting\nin India (Kn, IND), Shopping in India (Sh, IND), Building\nin Pennsylvania (Bu, US-PNA), Mechanic in Saudi Arabia\n(Me, SAU), Sport in Colombia (Sp, COL), Cooking in Japan\n(Co, JPN), Arts and Crafts in Italy (Ar, ITA), and Playing\nin Indiana (Pl, US-IN). ARGO1M contains 1,050,371 video\nclips. However, due to the absence of audio in some clips, we\nutilized approximately 60% of the total dataset where audio is\navailable. We report top-1 accuracy for each test split, along\nwith the mean accuracy across the splits."}, {"title": "3.1. Audio and Motion Resilience to Domain Shifts", "content": "In Table 1, the mean percentage drops in parentheses in-\ndicate the extent of domain shifts across various domains,\ncomparing modality settings where the training set either\ncontains samples that share location and/or scenario with the\ntest domain, or share neither. The mean performance drop\nfor motion features is 25.8%, while audio exhibits a drop of\n32.7%. In contrast, appearance shows a significantly higher\nshift (54.8%). This aligns with our hypothesis that tempo-\nral dynamics such as consistent patterns of movement and\ncontinuity of sound remain more stable across different\nenvironments and scenarios. In contrast, spatial semantics\nrepresented by appearance features, are more variable due\nto differences in objects, backgrounds, and other visual ele-\nments that can vary significantly from one domain to another.\nMoreover, the multimodal approach, which integrates audio,\nmotion, and appearance, demonstrates a reduced shift with a\nmean drop of 42.8% compared to appearance alone (54.8%).\nOur analysis underscores the critical role of incorporating\naudio and motion features for robust domain generalization."}, {"title": "3.2. Comparison with State of the Art", "content": "Table 2 compares our proposed method, the state-of-the-art\nCIR [10] approach, and a baseline model on the ARGO1M\ndataset. The baseline is trained solely with cross-entropy\nloss, whereas CIR enhances its performance through cross-\ninstance reconstruction guided by text. Notably, although the\noriginal CIR results did not include audio, we have extended\ntheir approach by incorporating audio features and multiple\ntrained encoders f for each modality to ensure a fair compar-\nson with our method. Our method consistently outperforms\nCIR across all domain splits, except for Co-JPN, achieving\nup to a 1.7% improvement and a 1.2% higher average per-\nformance overall. Additionally, our method outperforms the\nbaseline by an average of 4.8%."}, {"title": "3.3. Ablations", "content": "Table 3 illustrates the impact of different modality settings on\nperformance. In the 'Ap-Mo' setting used by CIR [10], ap-\npearance and motion features are fused early and processed\nthrough a single encoder. We propose an alternative approach\nin the 'Ap, Mo' setting, where separate encoders ($f_{appearance}$,\n$f_{motion}$) are employed to learn distinct domain-generalizable\nfeatures, particularly because motion exhibits greater resis-\ntance to domain shifts. As a result, our 'Ap, Mo' outperforms\n'Ap-Mo'. Furthermore, the integration of audio with separate\nencoders in 'Ap, Mo, Au' yields the best overall performance.\nTable 4 evaluates the effectiveness of each component of\nour approach. Starting with the baseline (trained using only\ncross-entropy), we observe that weighting the audio embed-\ndings a by the consistency ratings r enhances the model's per-\nformance, owing to the more reliable audio representations.\nAligning all modalities with narration (\u2018B+Lvt+Lmt+Lat')\nfurther improves performance. Notably, when we align au-\ndio features with their corresponding audio narrations t us-\ning Lat the model outperforms the version with Lat where\naudio features are aligned with the original visual-based nar-\nrations t. Finally, the combination of alignment losses and\nthe consistency-weighted audio approach in our final method\n'Ours' achieves the best performance across all domains.\nConclusion. We proposed a novel multimodal framework\nwhere motion, audio, and appearance improve domain gener-\nalization in first-person action recognition. We demonstrated\nthat audio and motion features exhibit greater resilience to do-\nmain shifts than appearance features. By aligning audio fea-\ntures with audio-specific narrations and applying consistency-weighted audio during training, our method enhanced the ro-\nbustness of action representations."}]}