{"title": "GLOG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic Features for Medical Image Segmentation", "authors": ["Niloufar Eghbali Zarch", "Hassan Bagher-Ebadian", "Tuka Alhanai", "Mohammad M. Ghassemi"], "abstract": "Vision Transformers (ViTs) have shown promise in medical image semantic segmentation (MISS) by capturing long-range correlations. However, ViTs often struggle to model local spatial information effectively, which is essential for accurately segmenting fine anatomical details, particularly when applied to small datasets without extensive pre-training. We introduce Gabor and Laplacian of Gaussian Convolutional Swin Network (GLOG-CSUnet), a novel architecture enhancing Transformer-based models by incorporating learnable radiomic features. This approach integrates dynamically adaptive Gabor and Laplacian of Gaussian (LoG) filters to capture texture, edge, and boundary information, enhancing the feature representation processed by the Transformer model. Our method uniquely combines the long-range dependency modeling of Transformers with the texture analysis capabilities of Gabor and LoG features. Evaluated on the Synapse multi-organ and ACDC cardiac segmentation datasets, GLOG-CSUnet demonstrates significant improvements over state-of-the-art models, achieving a 1.14% increase in Dice score for Synapse and 0.99% for ACDC, with minimal computational overhead (only 15 and 30 additional parameters, respectively). GLOG-CSUnet's flexible design allows integration with various base models, offering a promising approach for incorporating radiomics-inspired feature extraction in Transformer architectures for medical image analysis.", "sections": [{"title": "I. INTRODUCTION", "content": "Medical image semantic segmentation (MISS) is a critical task in healthcare, enabling precise identification and delineation of anatomical structures including organs and lesions. MISS is essential for accurate diagnosis, treatment planning, and disease monitoring [1], [2]. The accuracy of MISS is particularly important in applications such as radiotherapy and surgical planning, where precision directly impacts patient outcomes [3], [4].\n\nThe significance of MISS has driven continuous research into advanced segmentation methods. Initially, convolutional neural networks (CNNs), especially U-shaped architectures with skip-connections like U-Net, demonstrated high effectiveness in medical image segmentation [5]\u2013[7]. However, these models struggle with the intrinsic locality of convolution operations, limiting their ability to model global context effectively [8].\n\nVision Transformers (ViTs) have recently emerged as powerful alternatives to CNNs for MISS [8]. The ViT model is designed to capture long-range dependencies within an image by splitting it into patches and treating each patch as a sequence element, similar to how words are processed in natural language processing models [9]. Unlike CNNs, which focus on local pixel relationships, ViTs use self-attention mechanisms to model global interactions between image patches, making them particularly effective for tasks where understanding global context is crucial [10]. This ability to model long-range dependencies between image patches allows for more global context awareness, which is particularly useful for segmenting complex structures across medical images [8], [9]."}, {"title": "A. Related Works", "content": "Several studies have explored hybrid approaches to medical image segmentation and report improved performance over traditional CNN-based methods; notable examples include: (i) TransUNet [8], which combines CNNs and Transformers, and (ii) Swin-Unet [10], a fully Transformer-based model within a CNN-based framework. Inspired by the success of Swin-Unet, several extensions have been proposed to further improve its performance, particularly in terms of local feature extraction and computational efficiency. Wang et al. [11], introduced a novel Mixed Transformer Module (MTM), combining Local-Global Gaussian-Weighted Self-Attention (LGGSA) and External Attention (EA) to capture both local and global dependencies. This model focuses on reducing computational complexity while maintaining high accuracy by emphasizing nearby patches more effectively. Similarly, Liu et al. [12] extended Swin-Unet by incorporating Convolutional Swin Transformer (CST) blocks, which add convolutional layers within the Transformer architecture to enhance spatial and local feature modeling. This approach addresses one of the limitations of purely Transformer-based models, which often struggle with boundary refinement and local feature extraction. By merging convolutions with multi-head self-attention, CST achieves better segmentation performance, especially on small datasets like those commonly used in medical image segmentation."}, {"title": "B. Challenges and Our Contribution", "content": "Despite their potential, ViTs have limitations in capturing local spatial information, especially when applied to small datasets without extensive pre-training. This makes it difficult for them to accurately segment fine details such as the boundaries of small organs, which are critical in medical image analysis. Moreover, combining both CNNs and Vision Transformers (ViTs) in hybrid models can lead to significantly large and computationally expensive networks, further presenting a challenge in resource-constrained environments.\n\nTo address these challenges, we propose GLOG-CSUnet, a novel architecture that enhances ViT-based models by integrating learnable radiomic features, specifically Gabor filters and Laplacian of Gaussian (LoG) filters, into the Transformer framework. Radiomics, the high-throughput extraction of quantitative features from medical images, has demonstrated great potential in improving diagnostic and predictive accuracy across various clinical applications [13], [14]. Among radiomic features, Gabor filters are particularly effective at capturing multi-scale and multi-orientation texture information, which is crucial for characterizing complex tissue structures in medical images [15]. LoG filters, on the other hand, are valuable for improving edge detection and boundary precision, which are key to accurately segmenting anatomical structures [14].\n\nOur approach leverages the strength of Gabor and LoG filters in extracting these discriminative texture and edge features, which complement the global context modeling capabilities of ViTs. By integrating these adaptive local radiomic features with the transformer architecture, GLOG-CSUnet aims to achieve a better balance between local detail preservation and global context understanding. This integration is particularly beneficial for smaller datasets where extensive pre-training is not feasible, as it enhances the model's ability to capture fine-grained anatomical details without relying solely on large-scale data for feature learning."}, {"title": "II. METHOD", "content": "In this section, we describe the architecture and components of GLOG-CSUnet. An overview of the method is presented in 1. In addition, we detail the key elements of our approach, including the adaptable Gabor and LoG filters, data preparation, evaluation metrics, and experimental setup."}, {"title": "A. Model Overview", "content": "GLOG-CSUnet extends the Convolutional Swin-Unet (CSUnet) architecture [12], by incorporating adaptive Gabor filters and LoG filters into the segmentation pipeline. The architecture consists of the following main components:\n\n\\u2022 Gabor transformation Layer: Utilizes learnable Gabor filters to extract multi-scale and multi-orientation texture and edge information. These filters dynamically adapt during training to optimize the capture of radiomic features.\n\n\\u2022 LoG transformation Layer: Employs LoG filters to enhance boundary precision through edge detection and improved boundary delineation. The LoG filter parameters are optimized during the training process.\n\n\\u2022 Convolutional Patch Embedding Layer divides the input images along with radiomics transformations into non-overlapping patches, through a series of convolutional layers, preserving spatial locality before passing the patches to the Transformer.\n\n\\u2022 Vision Transformer Backbone: Swin Transformer serves as the core of GLoG-CSUnet, operating on image patches through shifted windows. This Transformer block models long-range dependencies, capturing global context, while Gabor and LoG filters handle local feature extraction."}, {"title": "B. Adaptable Filters", "content": "1) Gabor Filter: Gabor filters are particularly effective at extracting texture information and directional features from images. In our GLoG-CSUnet model, we implement a learnable Gabor filter layer that adapts to the specific characteristics of medical images during training. The 2D Gabor function [16] is defined by the following equation:\n\n$Gabor(x_o, y_o) = e^{-\\frac{x_o^2 + y_o^2}{2\\sigma^2}} cos(\\frac{2\\pi x_o}{\\lambda} + \\varphi)$\n\nwhere $x_o = x cos \\theta + y sin \\theta$ and $y_o = -x sin \\theta + y cos \\theta$. In this equation $\\lambda$ is the wavelength of sinusoidal component, $\\theta$ is the orientation of the normal to the parallel stripes of the Gabor function, $\\varphi$ is the phase offset, $\\sigma$ is the standard deviation of the Gaussian envelope and $\\gamma$ is the spatial aspect ratio, determining the ellipticity of the Gabor function. In our implementation, these parameters are learned during the training process.\n\n2) LoG Filter: The Laplacian of Gaussian (LoG) filter is used to detect edges and capture fine anatomical details by identifying areas of rapid intensity change. The LoG filter is represented as:\n\n$LoG(x, y) = \\frac{1}{\\pi \\sigma^4}(1 - \\frac{x^2+y^2}{2\\sigma^2})e^{-\\frac{x^2+y^2}{2\\sigma^2}}$\n\nwhere $\\sigma$ is the scale parameter that controls the sensitivity of the filter to edge detection at different scales. The LoG filter operates by first smoothing the image with a Gaussian filter and then applying the Laplacian operator to highlight regions of rapid intensity change. In GLoG-CSUnet, the LoG filters are implemented as learnable filters with a trainable $\\sigma$ parameter."}, {"title": "C. Data", "content": "In this study, we utilized two publicly available datasets to evaluate the performance of our segmentation model.\n\n1) Synapse Dataset [17]: The Synapse multi-organ segmentation dataset consists of contrast-enhanced abdominal CT scans from 30 subjects, covering 8 organs: aorta, gallbladder, spleen, left kidney, right kidney, liver, pancreas, and stomach. Following the standard split from previous works [8], [11], [12], we used 18 cases (2212 axial slices) for training and 12 cases for testing. Each image in the dataset has corresponding ground truth annotations for the organs. The images were resampled to a resolution of 224 \u00d7 224 ACDC Dataset to match the input requirements of our network. To enhance the model's robustness, we applied data augmentation techniques, including random flipping and rotation.\n\n2) ACDC Dataset [18]: The Automatic Cardiac Diagnosis Challenge (ACDC) dataset contains MRI images from 100 patients, focusing on the segmentation of three cardiac structures: the right ventricle (RV), left ventricle (LV), and myocardium (MYO). The dataset is split into 70 exams (1930 axial slices) for training, 10 exams for validation, and 20 exams for testing, following the protocol established in previous studies [11], [12]. The images were resized to 224 \u00d7 224 for consistency across datasets, and augmentation techniques such as flipping and rotation were applied to the training data, similar to approaches used in previous studies."}, {"title": "D. Evaluation", "content": "Following the prior works, we employed two key metrics to assess the performance of our model and the baselines: the Dice Similarity Coefficient (DSC) [19] and the 95th percentile Hausdorff Distance (HD95) [20]. The DSC, a widely adopted metric in medical image segmentation, quantifies the overlap between predicted and ground-truth segmentations; it ranges from 0 to 1, with 1 indicating perfect overlap. The DSC is particularly valuable for evaluating segmentation accuracy in scenarios involving imbalanced datasets or small structures [19]. The HD95 measures spatial accuracy by calculating the maximum distance between boundary points of the predicted and ground truth segmentation, using the 95th percentile to reduce outlier effects."}, {"title": "E. Experimental Setup", "content": "Our proposed model was trained from scratch for 300 epochs, starting from a randomly initialized set of weights. We used a batch size of 24 and optimized the model using the AdamW optimizer [21] with a weight decay of 2e-4 across both datasets. We applied dataset-specific learning rates: 1e-3 for the Synapse dataset and le-2 for the ACDC dataset. training dynamics. For the ACDC dataset, we trained 5 Gabor and 5 LoG filters, while for the Synapse dataset, we used 2 Gabor and 5 LoG filters. For baseline models, pre-trained weights were used if provided."}, {"title": "III. RESULTS", "content": "We evaluated GLoG-CSUnet against several state-of-the-art models on both the Synapse and ACDC datasets. able I presents the performance comparison on the Synapse multi-organ segmentation dataset. Our GLoG-CSUnet model achieved the highest overall Dice Similarity Coefficient (DSC) of 83.36%, surpassing the previous best performance of 82.21% by CS-Unet. GLoG-CSUnet demonstrated superior performance in six out of eight organ categories, with notable improvements in challenging organs such as the gallbladder (73.20% DSC) and stomach (82.10% DSC). While our model's D of 23.02 was not the lowest, it still represented a significant improvement over all baseline methods except Swin-Unet, indicating good boundary delineation capabilities.\n\nTable II shows the results for the ACDC cardiac segmentation dataset. GLoG-CSUnet achieved the highest overall DSC of 92.28%, outperforming the previous best model by 0.91 percentage points. Our model demonstrated superior performance across all three cardiac structures: Right Ventricle (91.04% DSC), Myocardium (90.09% DSC), and Left Ventricle (95.71% DSC). These results, consistent across both datasets, highlight GLoG-CSUnet's effectiveness in capturing both local and global features for accurate medical image segmentation, particularly in challenging cases such as small organs and complex structures.\n\nFigure 2 displays the segmentation results for two test cases, comparing our method against the top two baseline methods, MT-Unet and CS-Unet. In Case 1, our method achieves precise segmentation across all organs, notably outperforming other methods in accurately delineating the stomach and pancreas. In Case 2, although all methods effectively segment the left ventricle and myocardium, our model excels in segmenting the right ventricle."}, {"title": "A. Ablation Study", "content": "To validate the effectiveness of each component in our architecture, we conducted an ablation study comparing three variants: Gabor-only CSUnet, LoG-only CSUnet, and the complete GLOG-CSUnet using the ACDC dataset. Results showed that while both filter types independently improved performance, their combination yielded the best results. The Gabor-only variant achieved a DSC of 92.01%, while the LoG-only version reached 91.83%. The complete GLOG-CSUnet demonstrated superior performance with a DSC of 92.28%."}, {"title": "IV. DISCUSSION", "content": "The results demonstrate that GLOG-CSUnet consistently outperforms state-of-the-art methods across both the Synapse multi-organ and ACDC cardiac segmentation datasets. This is achieved with only a minimal increase in model parameters-adding just 15 parameters for the Synapse and 30 for the ACDC dataset, to the full model's total of 24 million parameters. This performance improvement, achieved with such a small computational overhead, can be attributed to the integration of learnable radiomic features, specifically Gabor and LoG filters, with the Transformer architecture. The superior performance in segmenting challenging structures, particularly the stomach, underscores the effectiveness of our approach in capturing fine-grained local details. Our GLOG-CSUnet model achieved a Dice Similarity Coefficient (DSC) for stomach segmentation of 82.10%, which represents a substantial improvement of 5.29% over the next best performance by MT-Unet. This significant increase, along with consistent enhancements across other structures, indicates the sucsses of our method in blending detailed local feature extraction with effective global context modeling. The adaptive nature of our Gabor and LoG filters allows the model to optimize feature extraction for each specific segmentation task, addressing a key limitation of standard Transformer-based models in capturing local spatial information.\n\nFuture work could explore the interpretability of the learned filters, potentially providing insights into the specific image features most relevant for different anatomical structures."}, {"title": "V. CONCLUSION", "content": "In conclusion, GLOG-CSUnet presents a advancement in medical image segmentation, effectively combining the strengths of radiomics-inspired feature extraction with the global context modeling capabilities of Transformers. This approach not only improves segmentation accuracy with minimal computational overhead but also opens new avenues for integrating domain-specific knowledge into deep learning models for medical image analysis."}]}