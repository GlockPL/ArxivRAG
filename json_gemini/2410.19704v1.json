{"title": "Multi-view biomedical foundation models for molecule-target and property prediction", "authors": ["Parthasarathy Suryanarayanan", "Yunguang Qiu", "Shreyans Sethi", "Diwakar Mahajan", "Hongyang Li", "Yuxin Yang", "Elif Eyigoz", "Aldo Guzm\u00e1n S\u00e1enz", "Daniel E. Platt", "Timothy H. Rumbell", "Kenney Ng", "Sanjoy Dey", "Myson Burch", "Bum Chul Kwon", "Pablo Meyer", "Feixiong Cheng", "Jianying Hu", "Joseph A. Morrone"], "abstract": "Foundation models applied to bio-molecular space hold promise to accelerate drug discovery. Molecular representation is key to building such models. Previous works have typically focused on a single representation or view of the molecules. Here, we develop a multi-view foundation model approach, that integrates molecular views of graph, image and text. Single-view foundation models are each pre-trained on a dataset of up to 200M molecules and then aggregated into combined representations. Our multi-view model is validated on a diverse set of 18 tasks, encompassing ligand-protein binding, molecular solubility, metabolism and toxicity. We show that the multi-view models perform robustly and are able to balance the strengths and weaknesses of specific views. We then apply this model", "sections": [{"title": "1 Introduction", "content": "Drug discovery is a complex, multi-stage process. Lead identification and lead optimization remain costly with low success-rates and computational methods play an important role in accelerating these tasks [1-3]. The prediction of a broad range of chemical and biological properties of candidate molecules is an essential component of screening and assessing molecules and data-driven, machine learning approaches have long aided in this process [4-6].\nMolecular representations form the basis of machine learning models [2, 7], facilitating algorithmic and scientific advances in the field. However, learning useful and generalized latent representation is a hard problem due to limited amounts of labeled data, wide ranges of downstream tasks, vast chemical space, and large heterogeneity in molecular structures. Learning latent representations using unsupervised techniques is vital for such models to scale. Large language models (LLMs) have revolutionized other fields [8] and similar sequence-based foundation models have shown promise to learn molecular representations and be trainable on many downstream property prediction tasks [9-11]. A key advantage is that the transformer based architecture can learn in a self-supervised fashion to create a \"pre-trained\" molecular representation. The most direct application of LLM like transformers is facilitated by a sequence, text-based representation (e.g. SMILES).\nWhile sequence-based representations have been hugely successful in capturing human-generated knowledge/information in LLMs, when it comes to representing natural objects like molecules, it's likely that sequence alone will not be sufficient, and multiple views will be better suited. Here we propose Multi-view Molecular Embedding with Late Fusion (MMELON) a flexible approach to aggregate multiple views in a foundation model setting and show that the resulting richer representations lead to better versatility across a large range of downstream tasks. While in this work, the molecule use case demonstrates the effectivity of the approach, it can be easily extended to include proteins and other biological entities. Our multi-View model is available on GitHub (https://github.com/BiomedSciAI/biomed.multi-view) and Hugging Face (https://Huggingface.co/ibm/biomed.sm.mv-te-84m).\nFor molecules, the reason sequence alone falls short is that intuitively, molecular representations should reflect the intrinsic geometry of the entities, including symmetries and similarity relations. Such geometry and composition give rise to chemical and biological properties [12]. Indeed, traditional cheminformatics makes use of molecular fingerprints [13-17], akin to hand-engineered features in other fields."}, {"title": "2 Results", "content": "We adopt three single view architectures and models based on image, text and graph molecular representations. The models are herein referred to as Image, Graph, and Text, respectively. A schematic for the pre-training strategies we employ is shown in Fig. 1B. For Image, we adopt the ImageMol model [25]. ImageMol utilizes a CNN-based architecture and is pre-trained tasks on 10M compounds selected from PubChem [31]. The pre-trained model and weights are taken from ImageMol's publicly released checkpoint. This checkpoint is fine-tuned in Sec 2.3 using our independently developed code that is utilized here. For Text, our architecture and pre-training strategy is adopted from MolFormer [11].\nThe Graph architecture is adopted from TokenGT [22]. This architecture processes chemically-bonded graphs as a sequence of tokens before input to the transformer. We design three pre-training tasks to pre-train the graph-transformer: node feature masking, edge prediction, and Betti number prediction. The last task predicts graph-topological features and is, to our knowledge, novel in this field. More details of Graph pre-training are given in Sec. 4.4.\nWe pre-train the Graph and Text on a set of 200M molecules curated from two public data sources: PubChem and ZINC22 [31, 32]. The PubChem dataset contains a broad distribution of molecules including a minor population that is rather large and flexible, and as such of less interest for small molecule drug discovery. We filter out this minor population using a chosen drug likeliness criteria. An additional 120M molecules are randomly sampled from ZINC22. This set of molecules covers a distribution of drug like molecules and molecular properties (see Sec 4.1 and Figs. S1-S2). Graph and Text are pre-trained for 3 epochs.\nThere are a number of multi-modal fusion strategies that may be employed to combine representations [26, 27, 29, 39]. We adopt a late fusion approach because it readily integrates separately pre-trained models and naturally supports analyses to gain insights into the role and importance of different modalities for different downstream tasks. In this approach, the singe-views are first separately trained and then combined using an aggregator sub-network (see Fig 1A). The single-view foundation models serve as pre-trained encoders that are utilized for both aggregator pre-training (see Sec. 4.5) and downstream task fine-tuning (see Section 2.3). For our aggregator architecture, we use an attention-based approach inspired by Ref. [28] where the contribution of the mth view to the overall embedding vector, zmv, is weighted by coefficient am,"}, {"title": "2.1 Overview of models and architectures", "content": null}, {"title": "2.2 Pre-trained models learn comprehensive embeddings", "content": "Image, Graph and Text are intrinsically different views of chemical structure. However, it is expected that there is at least some overlap in their description of the embedding space. In order to characterize the pre-trained embeddings, we compute the correlation between their Euclidean distances in Fig. 1C. The embeddings are derived from subset of 10,0000 molecules sampled from the pre-training set. It can be seen that of the three single-view embeddings, Image is the most dissimilar. Text and Graph are highly correlated to each other (c = 0.7), indicating that these two representations have a similar description of molecular similarity.\nPre-trained molecular embeddings should ideally capture important aspects of the chemical structures of the underlying molecules. While there is no reliable way of directly measuring this, comparison with fingerprints that have been developed by domain experts can serve as a sanity check on whether pre-trained embeddings are in line with chemical knowledge [11]. In Fig. 1D the correlation coefficients of the single-view models with four common fingerprint descriptors (Morgan, atom pair, MACCS and topological torsion) are plotted. [13-15, 40]. As a point of reference, we first compute the correlation between these fingerprints, showing that they are, in general, only moderately correlated and yield a different description of the molecules (see Fig 1C). The atom pair fingerprint exhibits the highest correlation to the others.\nThe correlations plotted in Fig. 1D are computed between the Euclidean distance of the pre-trained models and Tanimoto distance of the fingerprints over the same molecules used above. It can be seen that there is moderate correlation between pre-trained embedding and the fingerprints, demonstrating the validity of the embeddings. Overall, Text exhibits the highest correlation, followed by Graph and Image. Image is most correlated to the MACCS fingerprint, which is expected given that it is used in one of the ImageMol model's pre-training tasks [25]. Finally, we note that the correlation with fingerprints only serves as a sanity check and strong agreement with fingerprints is not necessarily desirable for all down stream property prediction tasks, because each fingerprint is tailored using a limited set of hand-engineered features. A key objective of learning pre-trained multi-view embeddings is to achieve richer, more flexible representations that are tunable across many downstream tasks."}, {"title": "2.3 Baseline performance: fine-tuning on downstream tasks", "content": "The prediction of electronic properties, ligand-protein binding affinity, pharmokinetic properties, and toxicity are examples of tasks that are essential to chemistry and drug discovery. MoleculeNet [34] is a collection of datasets covering such tasks that serves as a common test for machine learning models (see Table S1). Performance on this suite of benchmarks is neither decisive, nor necessarily reflective of all important use-cases in computational drug discovery, but currently it stands as the most widely used point of comparison between models. In addition to MoleculeNet tasks, we assess our model on the task of predicting the inhibition of cytochrome P-450 isoforms [33], which are important in drug metabolism pathways and identifying drug-drug interactions.\nIn addition to the choice of dataset, the choice of data splitting is critical in robust assessment of model performance. The size-ordered scaffold split (referred to here as 'scaffold'), clusters molecules according to their Murko scaffold [41] and assigns the clusters to splits in size-order. This is to be compared with other variations of scaffold split, which cluster the dataset in the same manner but assign clusters to train, validation, and test partitions randomly or 'balanced' (see, e.g. Ref. [25]). Scaffold splits are generally understood to be more appropriate and harder than random splits, though the latter are still often utilized in benchmarks [34].\nIn Fig. 2A, we show the performance of our multi-view model on a wide selection of MoleculeNet and other tasks. The scaffold split is used in all experiments. In order to evaluate the contributions from each view/modality, we focus on comparisons of single view Graph, Text, and Image models and the multi-view model. All are fine-tuned in the same code base with hyper-parameter optimization (primarily the learning rate, see Sec. 4.8). The area under the receiver-operator curve (ROC-AUC) is the chosen metric for the classification tasks and either the root mean square error (RMSE) or mean absolute error (MAE) is chosen for regression as prescribed in Ref. [34]. Regression metrics (Freesolv, Lipophilicity, QM7 and ESOL) are scaled in Fig. 2A so as to be plotted alongside ROC-AUCs (see Sec. 4.8). Tables S2-S3 report the numerical values for experiments with 95% confidence intervals, alongside results trained using a graph convolutional network (GCN) implemented with PyTorch Geometric [42].\nWe find that overall the multi-view model performs robustly across all tasks, spanning areas such as molecule solubility prediction (ESOL, Lipophilicity), binding activity assay-based classification (HIV, MUV) and toxicity-based tasks such as Tox21 and ClinTox. Screening molecules according to their metabolic properties is particular important in the drug discovery process. To this end, we fine-tune our model on classification of cytochrome-P450 (CYP) inhibition for five CYP isoforms [33] using the balanced scaffold split. Performance as measured by ROC-AUC is high, ranging from 0.90 to 0.82 and results are comparable with SOTA performance [25].\nIn addition, we consider a task that combines our multi-view embedding of small molecules with a popular sequence-based protein embedding, ESM [43]. It is an open question what the protein embedding contributes to so-called 'drug-target interaction' tasks [20, 44, 45]. However as ligand embeddings are known to drive model performance, then this still serves as a test of our small-molecule models. We fine-tuned our multi-view and the single-view models on the Davis dataset [35] describing the binding"}, {"title": "2.4 Case study: GPCR metabolite and drug target binding and identification", "content": "While benchmarks may validate models, it is key to demonstrate the use of molecular foundation models in a wider context. Here we use our multi-view model to conduct a large-scale virtual screening experiment by fine-tuning small molecule binding affinity models for 106 G Protein-Coupled receptors (GPCRs). GPCRs are important group of targets for drug discovery campaigns [46]. A recent study by some of us identified a selection of GPCRs as potential drug targets for Alzheimer's disease [47]. We consider the intersection between our screening database and AD-related GPCRs to uncover potential binders.\nWe fine-tune our multi-view model on experimental binding affinity assays for 106 GPCR targets. This dataset contains experimental pKI values curated from ChEMBL [36] and GLASS [37]. GPCRs with fewer bioactivity measurements (< 100) were not considered. Before training, the dataset is filtered to exclude molecules with a molecular weight > 600 Da. This cutoff is commiserate with the molecular properties of the metabolites and drug molecules that are in the screening (inference) set. Datasets with less than 50 data points were removed from the set. Each assay/GPCR target is treated as a separate regression task and data is split using the balanced scaffold protocol [25]. In Fig 3A, we plot the prediction on the held-out set against the experimental values. The multi-view fine-tuned models are overall in good agreement with the held-out set (Pearson correlation coefficient 0.78, average RMSE 0.79). To further validate our model, in Fig. 3B we plot the Pearson correlation coefficient against the RMSE for each individual target. These metrics are negatively correlated, as expected. Over 80% of fine-tuned models have RMSE < 1 and nearly 70% have correlations > 0.6 on the held-out set.\nTo unravel AD-related GPCRs, genetics-informed causal proteins inferred by Mendelian Randomization (MR) based on Genome-Wide Association Studies (GWAS) [48, 49] and multi-omics (transcriptomics and proteomics)-informed differential expressed genes (DEGs) on the human brain [50] were inspected. Strong multi-omics evidence-supported protein was defined as those that were differential expressed in as least 5 datasets we previously compiled [50] (see Fig. S5 and Table S4). Next we sought to identify potential safe AD therapies from gut metabolites and FDA-approved drugs. The validated multi-view model is used to predict the binding affinity of a set of 515 gut metabolites [51] and 2,504 FDA-approved drugs from DrugBank (Version 2021.1) [38] The red dots in Fig. 3B indicate the performance on the 33 AD-related GPCRs is within range of the larger set (Pearson correlation coefficient: 0.67, average RMSE: 0.88).\nNext we consider interactions of molecules in our virtual screen with specific AD-related GPCRs. FPR1 is suggested as a genetics-informed GPCR. High level of FPR1 is causally associated with increased risks of AD (Beta = -0.1, FDR = 0.001, Fig. S5A). Through inspecting top 10 small molecules in our screen, we found that a gut metabolite, acetyl-glutamine is predicted to interact with FPR1 (multi-view score: 6.8). By re-analyzing the abundance of gut metabolites in microbial strains in vitro [51], acetyl-glutamine was determined to have a high level in Ruminococcus gnavus (log2FoldChange: 1.19), a specie that mono-colonized in mice performed better on a spatial working memory test [52]. Additionally, over-expression of A\u03b2 protein is significantly characterized by the decreased Ruminococcus gnavus [53]. Three-dimension (3D) binding interaction analysis revealed that acetyl-glutamine is located in an allosteric binding site that is distinct from the classical binding site, indicating a potential regulation mechanism (Fig. 3C). For top predicted drugs, glutathione (GSH), an anti-oxidant drug [54], GSH, is prioritized (multi-view score: 6.53) to interact with FPR1. GSH is a nutrition supplementation that consist of three amino"}, {"title": "3 Discussion", "content": "A molecular foundation model that is fine-tunable on a broad, diverse set of tasks can serve as a crucial component of any discovery workflow. The choice of molecular representation is key to building such foundation models. However, each choice may have strengths and weaknesses, and each individual representation (view) typically performs well on certain downstream tasks and not others. Here we introduce MMELON, a multi-view architecture that combines molecular representations from multiple embeddings using an interpretable late fusion approach, utilizing foundation models based on three common representations of chemical structure, Graph, Image and Text.\nWe fine-tune our multi-view model on a set benchmark datasets that span diverse tasks such as binding activity, toxicity and drug metabolism. We compare the performance of the multi-view model with the single view models whose encodings feed into it. Overall the multi-view model performs robustly across a wide range of property prediction tasks involving both classification and regression - it gives the best performance in the majority of the tasks, while closely matching the best single model in the remaining ones. In contrast, none of the single view models is dominant for all tasks. This demonstrates that the MMELON architecture is indeed able to leverage the \"sweet spots\" from each individual view to achieve better scaling across a large number of diverse downstream tasks.\nFor the Graph model we introduced a novel pre-training task, Betti number prediction that captures topological features. Among the single view models, the Graph Model is the strongest and is the most significant contributor to the multi-view model in most cases (Fig. 2B).\nTo further validate the effectiveness of MMELON we apply our pre-trained multi-view model to a large collection of GPCR assays to create over 100 fine-tuned activity models. This serves as a virtual screening panel against which a set of metabolites and FDA approved drugs are screened. We find a large number (33) of GPCR targets that are implicated in Alzheimer's disease. Several strong binders to these targets are identified from the metabolite and drug datasets. They are studied by molecular docking [47] and pharmocophore identification, showing promising results. Future work will aim to further validate these predictions through wet-lab experiments.\nAll views that are currently integrated into the MMELON architecture are ultimately derived from so-called 1D (string) or 2D (graph and image) representations [59]. They are moderately correlated with fingerprints and contain overlapping information (Fig. 1). More expressive representations including 3D dimensional conformers [60, 61], and molecules in the context of binding [20, 62] can be readily incorporated into MMELON. Combining insights from the molecular docking that is used to validate our approach [47] is also a potential next step. Besides molecule-target and property prediction, this approach can be leveraged for molecule generation and lead optimization as well. Furthermore, while the small molecule use case is explored here, the approach itself may be readily extended to include proteins and other macromolecules."}, {"title": "4 Methods", "content": null}, {"title": "4.1 Pre-training data", "content": "The objective of the pre-training dataset is to develop a task-agnostic representation through self-supervision. Choosing a dataset with the right diversity and relevance to downstream tasks leads to compute-optimal richer representations. We use two main"}, {"title": "4.2 Input representation", "content": null}, {"title": "4.3 Architectures", "content": null}, {"title": "4.4 Pre-training tasks", "content": "The Text model is pre-trained using a masked language modeling (MLM) task similar to Molformer. During pre-training, 15% of the tokens in the SMILES sequences are randomly masked, and the model is trained to predict the masked tokens based on their context. The pre-training objective minimizes the cross-entropy loss between the predicted tokens and the true masked tokens."}, {"title": "4.4.1 Pre-training on topological context", "content": "The third, novel pre-training task involves predicting topological invariants of the molecular graph, specifically the Betti numbers, which are derived from simplicial homology. Betti numbers are used to describe the topological properties of a graph: \u03b20 represents the number of connected components, while \u03b21 captures the number of independent cycles within the graph. For each node va, the model predicts these Betti numbers for the subgraph around that node. For example, in Fig. S6 at node va, the graph might have \u03b20(Sva) = 1 (indicating a single connected component) and \u03b21(Sva) = 1 (indicating the presence of one cycle). In contrast, at node ub, the graph might exhibit \u03b20(Sub) = 2 (two connected components) and \u03b21(Sub) = 0 (no cycles). These Betti numbers provide both local and global structural information about the molecular graph, allowing the model to capture wider topological context at atom level representations."}, {"title": "4.5 Late fusion strategies", "content": null}, {"title": "4.5.1 Multi-view late fusion approach", "content": "As first discussed in Sec. 2.1, we utilize a late-fusion approach to combine Graph, Image, and Text Views. It is an attention-based approach, inspired by Ref. [28] where the coefficients combining the mth view, am, are the soft max outputs from an operation on embeddings zm performed on batch B (Eqn. 3),\nzmv = MLP\u03a3mEMam zm\n(1)\n(2)\n(3)\nBefore recovering the multi-view embedding, zmv, a multi-layer perceptron is applied. Eqns. 2 and 3 form the aggregator network of the multi-view model. This network is subject to a secondary pre-training. Aggregator pre-training yields an initialization of the weights that was found to improve robustness during fine-tuning and downstream task prediction. The aggregator is pre-trained using an embedding reconstruction task on a set of 10M molecules randomly sampled from the dataset from which we pre-train the Graph and Text models. At the conclusion of this secondary pre-training, it is found that the Image, Graph and Text are weighted in decreasing order, with Opret values of 0.6, 0.3, and 0.1, respectively. These values do not reflect singe-view embedding quality or their contributions to specific down-stream asks after subsequent fine-tuning (see Secs. 2.3). Instead, the pre-training task is related to difficulty by which a specific view can be reproduced (reconstructed) from the combined embedding and is consistent with the observation that Image is less correlated to the Graph and Text views (see Sec. 1C)."}, {"title": "4.5.2 Alternative late fusion strategies", "content": "Before choosing our multi-view model architecture, we explored several other late fusion strategies that combine the representations from the Graph, Text, and Image models. These strategies differ in how the inputs are prepared for the gating network. The gating network computes weights for each view, and the final output is a weighted aggregation of the view-specific representations.\n1. Projected Gating: The representations from each model are projected into a lower-dimensional space. Let di denote the dimensionality of the output from modality i, where i \u2208 {1,2,3} corresponds to the graph, text, and image models. The total input dimension to the gating network is:\nDprojected = min(d1, d2, d3) \u00d7 3\nThe concatenated projected embeddings are passed into the gating network, which computes the weights wi for each modality.\n2. Unprojected Gating: In this scheme, the original (unprojected) representations from the models are concatenated and used as input to the gating network. The input dimension in this case is the sum of the dimensions of the individual outputs:\nDunprojected = d\u2081 + d2 + d3\nThe gating network then computes the weights based on this concatenated unprojected representation. In both the schemes above, we compute weighted concatenation of the model output.\nzfinal = W1Z1 W2Z2 W3Z3\n3. Projected Gating With Feature Addition: This scheme the model outputs are first projected and concatenated, and the gating network computes the weights from this combined representation. The same aggregation process follows, where weights wi are applied to the corresponding expert outputs.\nIn this scheme, the final representation zfinal is computed as a weighted sum:\nzfinal =\u03a3i=13WiZi\nwhere wi are the modality-specific weights and zi are the corresponding representations. Additionally, for interpretability, the average weights across the batch can be computed:\nWi =1N\u03a3n=1Nw(n)i\nwhere N is the batch size."}, {"title": "4.6 Drug-target interaction model", "content": "Ref. [45] proposed a sequence-based framework to investigate the binding affinities of proteins and ligands whilst experimenting with various embedding types for this purpose. From this study, we adapted the model that uses convolutional neural network-based encodings obtained from the ESM-1b [43] sequences for representing the protein structure. In line with their model, we combined the CNN based protein embeddings with ligand embeddings, which were obtained from our models. The combined embeddings were then passed through a prediction head, which was again based on their proposed prediction head, expect for the difference in the size of the embeddings. We used 64-dimensional protein and ligand encodings which are then concatenated to form a 128-dimensional embedding. The authors of Ref. [45] used double this size. We experimented with combining CNN obtained protein embeddings with ligand embeddings obtained from various modalities of representation: Image, Text, Graph representations, and the combination of all three.\nDuring training, the CNN and BMFM model weights were unfrozen gradually using the Xavier method. We used the batch size of 64 and the learning rate of 0.0005 for all models except for the Text model, for which we used the batch size of 128 and the learning rate of 0.00001. We trained for two thousand epochs, as suggested in [45], and used the best performing model on the validation set to predict the test set. We used six-fold cross-validation using one-sixth of the data for validation, one-sixth for testing, and the rest for training. The results were then averaged across the six cross-validation folds."}, {"title": "4.7 Statistical Analysis of select classification models", "content": "We consider the fine-tuned output of the three single-view and multi-view models in select classification tasks and aim to perform analysis to uncover distinct characterizations of the models' predictions. Binarization was performed on outputs by applying a threshold of 0.5 to the sigmoid mapped scores. The McNemar test [71] was applied to the predictions of model pairs to test whether one model predicts greater or less active ligands than the other model than expected by chance drawn randomly from identical distributions. The McNemar tests tend to be very sensitive to relatively \"small\" variations above random. F1 scores compare predictions, measuring how well the second model's predicted activities predict the first model's predicted activities. It is important to realize this measure is not symmetric."}, {"title": "4.8 Fine-tuning Protocols", "content": "Hyper-parameter optimization of the fine-tuning process is important to ensure quality outputs. Optimization was first explored using ray-tune [72]. Based on an exploration of our multi-view model across a subset of MoleculeNet, a set of parameters for AdamW and regularization (e.g. weight-decay) were chosen. Linear and MLP prediction heads were also explored. The learning rate was then varied for individual MoleculeNet tasks for Image, Graph, Text in the range of le-5 to 1e-3). In the case of the multi-view model, the presence of multiple sub-networks may call for a multiple"}, {"title": "4.9 Genetics and multi-omics analysis", "content": "The genetics-informed GPCRs prioritized by Mendelian randomization (MR) were retrieved from our previous study [47]. Briefly, 408 GPCRs were tested and publicly available 4 cis-eQTL (Expression quantitative trait locus) datasets of human cortex region and 3 publicly available AD GWAS summary statistic datasets were used to test the causal effects of GPCRs to AD. Multi-omics data of 88 bulk and single cell RNA-seq transcriptome or proteome datasets were used we previously compiled (available at AlzGPS: https://alzgps.lerner.ccf.org/) [50]. Briefly, differential expression analysis was performed on microarray, bulk RNA-seq, and single-cell/nucleus (sc/sn) RNA-seq datasets. The threshold of differentially expressed genes (DEGs) were defined with adjusted p-value (q) <0.05 and log2(FC)| \u2265 0.25."}, {"title": "4.10 Molecular docking simulations", "content": "Top 10 drugs or metabolites of AD genetics-informed FPR1 and AD multi-omics-informed ADA2A were inspected. 3D structures of FPR1 or ADA2A were retrieved from AlphaFold2 database[73]. The preparation of 2D structures of small molecules and 3D structures of proteins refer to our previous study. Molecular docking was processed by AutoDock Vina (version 1.1.2) [74]. The top 10 binding modes were investigated, and the best binding poses were selected for further analysis. Detailed binding analysis were conducted by using PyMOL (version 3.0.3)."}]}