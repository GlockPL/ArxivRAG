{"title": "Offline Model-Based Reinforcement Learning with Anti-Exploration", "authors": ["Padmanaba Srinivasan", "William Knottenbelt"], "abstract": "Model-based reinforcement learning (MBRL) algorithms learn a dynamics model from collected data and apply it to generate synthetic trajectories to enable faster learning. This is an especially promising paradigm in offline reinforcement learning (RL) where data may be limited in quantity, in addition to being deficient in coverage and quality. Practical approaches to offline MBRL usually rely on ensembles of dynamics models to prevent exploitation of any individual model and to extract uncertainty estimates that penalize values in states far from the dataset support. Uncertainty estimates from ensembles can vary greatly in scale, making it challenging to generalize hyperparameters well across even similar tasks. In this paper, we present Morse Model-based offline RL (MoMo), which extends the anti-exploration paradigm found in offline model-free RL to the model-based space. We develop model-free and model-based variants of MoMo and show how the model-free version can be extended to detect and deal with out-of-distribution (OOD) states using explicit uncertainty estimation without the need for large ensembles. MoMo performs offline MBRL using an anti-exploration bonus to counteract value overestimation in combination with a policy constraint, as well as a truncation function to terminate synthetic rollouts that are excessively OOD. Experimentally, we find that both model-free and model-based MoMo perform well, and the latter outperforms prior model-based and model-free baselines on the majority of D4RL datasets tested.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) aims to learn policies for sequential decision-making that maximize an expected reward [42]. In online RL, this means alternating between interacting with the environment to collect new data and then improving the policy using previously collected data. Model-based reinforcement learning (MBRL) denotes techniques in which an established environment model or an approximation of the environment is used to simulate policy rollouts without having to directly query the environment [40, 41, 42, 30].\nOffline RL tackles problems where the policy cannot interact with and explore the real environment but instead can only access a static dataset of trajectories. The offline dataset can be composed of mixed-quality trajectories with poor coverage of the state-action space. With no ability to extrapolate beyond the offline dataset, standard model-free offline RL algorithms often apply systematic pessimism by either injecting penalties into the estimation of values or by constraining the policy [32, 27] to remain in action support. Offline MBRL can offer more flexibility by augmenting the offline dataset with additional synthetic rollouts that expand coverage, while also remaining within the dataset support [22, 48, 49].\nMBRL is fraught with danger: approximation errors in the dynamics model can cause divergence in temporal difference updates [44], errors can accumulate over multistep rollouts, resulting in performance deterioration if unaddressed [18], and the dynamics model can be exploited by the agent [33, 26, 6]. In offline MBRL, these issues are amplified due to the double dose of extrapolation error present in both dynamics and value functions. Consequently, standard online MBRL methods perform poorly when directly applied offline [48].\nOffline RL approaches typically fall into one of two categories: 1) critic regularization methods which penalize OOD action-values or 2) policy constraint methods that minimize a divergence between the learned policy and (a potentially estimated) behavior policy. Studying model-free offline RL literature exposes a variety of approaches in both paradigms [32, 27], but offline MBRL methods exhibit far less diversity [17].\nMany previous offline MBRL algorithms are based on MBPO [18] and perform conservative value updates that identify the increased epistemic uncertainty in out-of-distribution (OOD) regions using dynamics ensembles [6] or actively penalize all action-values from synthetic samples [49]. Ultimately, these fall firmly under the critic regularization paradigm, and their empirical evaluation reveals that they perform little better than their model-free counterparts [2, 14]. A small number of methods implement policy-constrained offline MBRL [50, 20], though these do not realize the benefits of augmenting training with samples from the dynamics model.\nOur approach to offline MBRL is based on the anti-exploration [35] paradigm originating in model-free offline RL which uses a policy constraint in combination with an anti-exploration bonus to curb value overestimation. We train a Morse neural network [7] and exploit its properties as an implicit behavior model, and an OOD detector to extend model-free anti-exploration for MBRL, which realizes the benefits of both policy-constrained offline RL and training with synthetic data.\nContributions In this work, we present Morse Model-based offline RL (MoMo) which performs offline model-free RL using anti-exploration. MoMo trains a dynamics model and a Morse neural network model, the latter of which is an energy-based model that learns an implicit behavior policy and an OOD detector. We demonstrate how this can be incorporated into a model-free anti-exploration-based offline RL method and then adapt it to detect and truncate rollouts in the dynamics model that are becoming excessively out-of-distribution. The Morse neural network's uncertainty estimation helps us avoid the need for large dynamics ensembles we can instead use a single dynamics model. Furthermore, the Morse network's calibrated uncertainty estimates allow for better hyperparameter generalization across similar tasks.\nOur experiments evaluate both model-free and model-based versions of MoMo on the D4RL Gym Locomotion and Adroit tasks [9] and establish MoMo's performance to be equivalent to or better than recent baselines in 17 of 24 datasets. In-depth ablation experiments analyze MoMo's hyperparameter sensitivity and the effectiveness of anti-exploration value penalties in offline MBRL."}, {"title": "2 Related Work", "content": "Reinforcement Learning We consider RL in the form of a Markov Decision Process (MDP), $\\mathcal{M} = {\\mathcal{S}, \\mathcal{A}, \\mathcal{R}, \\mathcal{T}, P_0, \\gamma}$, which consists of a tuple with state space $\\mathcal{S}$, action space $\\mathcal{A}$, a scalar reward $\\mathcal{R}(s, a)$, transition dynamics $\\mathcal{T}$, initial state distribution $P_0$ and discount factor $\\gamma \\in [0,1)$ [42]. RL aims to learn a policy that will execute an action $a = \\pi(s)$ that maximizes the expected discounted reward $J(\\pi) = \\mathbb{E}_{\\tau \\sim \\mathcal{T}(\\pi)} [\\sum_{t=0}^\\infty \\gamma^t \\mathcal{R}(s_t, a_t)]$,\nwhere $\\mathcal{T}_{\\pi}(\\tau) = P_0(s_0) \\prod_{t=0}^\\infty \\pi(a_t | s_t)\\mathcal{T}(s_{t+1} | s_t, a_t)$ is the trajectory under $\\pi$.\nModel-Free Offline RL Offline RL methods are designed to learn from a static dataset $\\mathcal{D}$, which consists of interactions collected by applying a policy $\\beta$, without access to the environment [27, 12, 38]. The offline dataset may provide poor coverage of the state-action space, and so when using function approximators, the policy must be constrained to select the actions within the support of the dataset. Offline RL methods (generally) belong to one of two families of approaches: 1) critic regularization or 2) policy constraint [32, 27]. Uncertainty-based model-free methods use large ensembles [2, 14] to perform a variation of critic regularization.\nAnti-Exploration Rezaeifar et al. [35] pose model-free offline RL as an exercise in anti-exploration, an approach in which the actor is constrained via an explicit divergence minimization term in the policy improvement step and policy evaluation is augmented with an anti-exploration bonus (value penalty) to counteract overestimation in off-policy evaluation. This bears some similarity to the BRAC framework [46] which regularizes (either policy or value) using an explicit divergence between the current and estimated behavior policies.\nSeveral works propose different heuristics for estimating divergence in anti-exploration algorithms [31, 43], though thus far anti-exploration has remained exclusive to model-free methods.\nOffline Model-Based RL Offline MBRL injects samples from a learned dynamics model into the RL learning process and leverages the diversity of simulated rollouts to augment the offline dataset with additional data. In online MBRL, Janner et al. [18] introduce MBPO which uses an ensemble of dynamics models and performs Dyna-style policy learning [40, 41]. MBPO's ensemble does not discourage OOD exploration; Yu et al. [48] address this by penalizing reward with ensemble uncertainty and Kidambi et al. [22] use ensemble uncertainty to detect, penalize and terminate synthetic rollouts when they are excessively OOD. Several works develop methods to better estimate and penalize values using different uncertainty estimation techniques [39, 19].\nCOMBO [49] performs CQL-like critic regularization [25] on samples from model rollouts. This adversarial approach to offline MBRL is further developed by Rigter et al. [36], Liu et al. [28], Bhardwaj et al. [5] and Luo et al. [29] who present alternative approaches to learning pessimistic value functions.\nPolicy constraint-based offline MBRL embraces an overarching goal to direct the policy towards states with known dynamics when starting in unknown ones. Zhang et al. [50] train ensemble dynamics and a behavioral transition model that is used to ensure that the next state the policy will transition to is within the dataset support. Jiang et al. [20] train an inverse dynamics model that constrains the policy to stay close to the behavior policy. Neither method performs policy rollouts using dynamics models, as estimates of the behavior policy do not extrapolate well. Their lack of training on synthetic data puts these methods at odds with critic-regularized offline MBRL."}, {"title": "3 Preliminaries", "content": "Dherin et al. [7] develop the Morse neural network, an uncertainty estimator that produces an unnormalized density $M(x) \\in [0, 1]$ in an embedding space $\\mathbb{R}^e$. At its modes, the density achieves a value of 1 and approaches 0 with increasing distance from the mode. A Morse neural network is defined by the combination of a neural network function approximator and a Morse kernel:\nDefinition 1 (Morse Kernel). A Morse Kernel is a positive definite kernel $K$ that is applied on a space $\\mathcal{Z} = \\mathbb{R}^e$, $K(z_1, z_2)$ and takes its values in the interval $[0, 1]$ with $K(z_1, z_2) = 1$ only when $z_1 = z_2$.\nAll kernels that use a measure of divergence are Morse kernels [1]. A commonly used kernel is the radial basis function (RBF) kernel with scale parameter $\\lambda > 0$\n$$K(z,t) = e^{-\\lambda ||z - t||^2}$$.\n(1)\nThe Morse kernel describes how close an embedding $z$ is to the target $t$. The relationship between the input $x$ and its corresponding embedding $z$ is determined by a neural network $f_\\psi: \\mathcal{X} \\rightarrow \\mathcal{Z}$, $\\mathcal{X} \\in \\mathbb{R}^d$ and $\\mathcal{Z} \\in \\mathbb{R}^e$ with parameters $\\psi$. Combining the Morse kernel with a neural network yields the Morse neural network:\nDefinition 2 (Morse Neural Network). A Morse neural network is comprised of a function $f : \\mathcal{X} \\rightarrow \\mathcal{Z}$ and a Morse Kernel $K(z, t)$ where $t \\subset \\mathcal{Z}$ is a target embedding where its size and value are chosen as a model hyperparameter. The Morse neural network is given by $M_\\psi(x) = K(f(x), t)$.\nUsing Definitions 1 and 2, if $x$ maps to a mode in the level set of the submanifold $\\mathcal{Z}$, then $M_\\psi(x) = 1$. We interpret $M_\\psi(x)$ as the certainty that the sample $x$ is from the training dataset and $1 - M_\\psi(x)$ is an estimate of the epistemic uncertainty of $x$. The function $\\log M_\\psi(x)$ measures measures the distance between $z$ and the nearest mode at $t$ of the set of modes $\\mathcal{M}$:\n$$d(z) = \\min_{t \\in \\mathcal{M}} d(z,t),$$ (2)\nThe trained Morse neural network offers the following properties [7]:\n1. $M_\\psi(x) \\in [0, 1]$.\n2. $M_\\psi(x) = 1$ at all mode submanifolds.\n3. $\\log M_\\psi(x) \\geq 0$ is a squared distance that satisfies the Morse-Bott non-degeneracy condition at the mode submanifolds [4].\n4. $M_\\psi(x)$ is an exponentiated squared distance, the function is distance aware in the sense that as $f_\\psi(x) \\rightarrow t, M_\\psi(x) \\rightarrow 1$.\nSee Dherin et al. [7] for the proof of each property."}, {"title": "4 Morse Model-Based Offline RL", "content": "In this section, we describe our approach to policy-constrained offline MBRL that can use samples from synthetic rollouts to augment the offline dataset. We begin by adapting the Morse neural network for offline RL as a method for anti-exploration. Then in Section 4.3 we show how this can be extended to MBRL.\n4.1 Morse Neural Networks Learn an Implicit Behavior Policy\nMorse Networks for Offline RL The offline RL dataset consists of K tuples $\\mathcal{D} = \\{s,a,r,s'\\}_{i=1}^K$ that provide partial coverage of the state-action space. We train a Morse neural network to learn a conditional uncertainty model over actions in the dataset. Adapting the objective from Dherin et al. [7], we minimize the KL divergence $D_{KL}(D(s, a) || M_\\psi(s, a))$ using:\n$$\\min_\\psi \\mathbb{E}_{s,a \\sim D} [\\log \\frac{D(s, a)}{M_\\psi(s,a)}] + \\int [M_\\psi(s,a) - D(s, a)] da,$$ (3)\noptimizing with respect to $\\psi$, we minimize the empirical loss:\n$$\\min_\\psi \\mathbb{E}_{s,a \\sim D} [-\\log K (f_\\psi (s, a), t)] + \\mathbb{E}_{s \\sim D} [K(f(s, a_u), t)],$$ (4)\n$$a_u \\sim D_{uni}$$\nwhere $D_{uni}$ denotes a uniform distribution over the action space. We outline the Morse neural network training procedure in Algorithm 1. The Morse neural network is an energy-based model (EBM) [15, 7]:\nProposition 1. The Morse neural network is an EBM: $E_\\psi(x) = - \\log M_\\psi(x)$.\nSee Dherin et al. [7] for the proof.\nBy interpreting the Morse neural network as an EBM we can recover a behavior policy density using the Boltzmann distribution:\n$$\\beta(a|s) = \\frac{M_\\psi(s, a)}{Z_\\psi(s)}$$ (5)\n$$Z(s) = \\int_A M_\\psi(s, a) da,$$ (6)\nwhere $Z_\\psi(s)$ is a per-state normalization constant that, in practice, is intractable for continuous actions. This yields an implicit behavior policy:\n$$\\log \\beta(a|s) = \\log M_\\psi (s, a) - \\log Z_\\psi(s)$$ (7)\n$$\\leq \\log M_\\psi (s, a) \\leq 0,$$ (8)\nwhere the logarithm of the Morse neural network is an upper bound on the behavior density. Hence, maximizing certainty (or conversely, minimizing the distance - $\\log M_\\psi$ in the submanifold to a mode) will maximize log-likelihood under the implicit behavior policy. This also has the distinct advantage that all modes of the dataset occur with the same unnormalized density of one with no limiting inductive bias about the number of modes/policies that produced the training dataset.\nMany prior offline model-free methods estimate and constrain using explicit behavior policies [12, 46]. We perform similar constrained policy optimization with changes highlighted in blue:\n$$\\max_\\pi \\mathbb{E}_{s \\sim D} [Q_\\theta(s, a) - D_{KL}(\\pi || \\beta)]$$ (9)\n$$\\approx \\max_\\pi \\mathbb{E}_{s \\sim D} [Q_\\theta(s, a) + \\log M_\\psi(s, a)].$$ (10)\nThis policy optimization is identical to that of prior anti-exploration methods [34, 43, 31], where optimizing $\\log M_\\psi$ allows minimization of the reverse KL divergence.\nFollowing prior anti-exploration approaches, we use the following value function update with the anti-exploration bonus in red:\n$$\\min_{\\theta'} \\mathbb{E}_{s,a,s' \\sim D} [(Q_\\theta(s, a) - y)^2]$$ (11)\n$$y = r + \\gamma (Q_{\\overline{\\theta}}(s', a') + \\log M_\\psi(s', a')),$$\nwhere $\\theta$ denotes the parameters of the Q function and $\\overline{\\theta}$ the parameters of the target Q function.\nNote that in Equation 10 and 11 we disregard the scalar multiplier typically included with the $\\log M_\\psi$ term. We implicitly set this to one and instead use the kernel scale $\\lambda$ in Equation 1 to control the behavioral cloning and anti-exploration tradeoff.\nControl with Kernel Scale We illustrate how $\\lambda$ can control the strength of the constraint with a didactic example in Figure 1. We select eight, two-dimensional action modes spaced uniformly on a unit circle that are assigned to one of two states in an alternating fashion. 64 action samples are drawn for actions in each state with a standard deviation of 0.01. With a total of 128 samples from two states with four action modes each, we produce a synthetic dataset that exhibits a strong multimodal behavior policy (see Figure 1(a)). We fit a Morse neural network to the dataset using the objective in Equation 4 and evaluate 16000 uniformly spaced samples over the action space to produce unnormalized densities for each state for different $\\lambda$ in Figure 1. As $\\lambda$ increases, the degree of \"closeness\" in the embedding space decreases resulting in a more sharply constraining density.\nAnalysis We aim to constrain the policy to match the occupancy measure of the behavior policy. Assuming deterministic transition dynamics, we consider the normalized occupancy measure of the behavior policy using an EBM (see Proposition 1):\n$$(1 - \\gamma)\\rho_\\beta(s, a) = \\frac{\\exp(-E(s,a))}{Z(s)}$$ (12)\nWe minimize the KL divergence between occupancies and show that the policy constraint term in Equation 10 upper-bounds the divergence:\n$$D_{KL}(\\rho_\\pi || \\rho_\\beta) = \\sum_{s,a} \\rho_\\pi(s, a) [\\frac{\\log \\rho_\\pi (s, a)}{\\log \\rho_\\beta (s, a)}]$$ (13)\n$$\\leq \\mathbb{E}_\\pi [-\\log M_\\psi(s, a)] - H(\\pi) + const,$$ (14)\ntherefore, to minimize the KL divergence we choose to minimize the upper bound:\n$$\\arg \\min_{\\pi} D_{KL} (\\rho_\\pi || \\rho_\\beta) \\rightarrow \\arg \\min_{\\pi} \\mathbb{E}_\\pi [-\\log M_\\psi(s,a)].$$ (15)"}, {"title": "4.2 Challenges when Constraining Offline MBRL", "content": "Na\u00efvely applying our policy constraint to states from model rollouts will result in poor performance. The Morse neural network as an implicit behavior policy poorly extrapolates to unseen states; prior work by Xu et al. [47] and Florence et al. [8] shows that implicit models formed by fully connected neural networks can only perform linear function extrapolation, yet assuming that the behavior policy is linear can be overly restrictive.\nThis is a critical shortcoming of EBMs when used for the policy constraint and anti-exploration bonus in unknown states, which can lead to incorrect modes being learned/penalized. This drawback extends to prior offline MBRL methods using a dynamics constraint [50, 20] who subsequently restrict themselves to training on small perturbations of dataset states rather than using dynamics rollouts. To use dynamics models for data augmentations, we must ensure that rollouts remain within the data support, both so that the Morse-based constraint is valid and to prevent the exploitation of learned dynamics. The ability to combine policy constraint with rollout OOD detection is (to the best of our knowledge) unique to MoMo and in the next section, we describe how the uncertainty detection properties of the Morse neural network enable detection of when states in synthetic trajectories have moved too far from the dataset support."}, {"title": "4.3 Extending to Offline MBRL", "content": "MBPO-based offline MBRL algorithms [18] typically penalize OOD regions by directly using ensemble uncertainty estimates [22, 48] or assuming that all states produced by the dynamics models are OOD [49, 28, 36, 5, 29]. MOREL [22] terminates the rollouts when the estimated uncertainty increases beyond a threshold. We perform a similar rollout truncation when the policy is excessively OOD using the Morse neural network.\nOur rollout truncation refers to stopping the rollout without performing any subsequent bootstrapping or episode termination (i.e., we do not update truncated action-values during policy evaluation). This differs slightly from the treatment in online RL as in the offline domain, truncation is not an environment property, but a property that we impose on the empirical MDP.\nLet $\\beta(a|s)$ denote the behavior policy, $s_t$ denote the state at time $t$, $a_t$ denote the action selected by an agent, $P_0$ denote the initial state distribution and $\\mathcal{T}(s_{t+1} | s_t, a_t)$ denote the state transition. We assume that the environment transition is deterministic and aim to train an agent to minimize the (cross) entropy under the behavior policy over a trajectory $\\tau_{0:T}$:\n$$H(\\tau_{0:T}) = \\mathbb{E}_{s_0 \\sim P_0} \\sum_{t=0}^T \\mathbb{E}_{a_t \\sim \\pi(s_t)} [\\log \\beta(a_t|s_t)] | s_{t+1} \\sim \\mathcal{T}(\\cdot | s_t, a_t)$$ (16)\n$$= \\mathbb{E}_{s_0 \\sim P_0} \\sum_{t=0}^T \\mathbb{E}_{a_t \\sim \\pi(s_t)} [\\log M_\\psi(s_t, a_t) - \\log Z_\\psi(s_t)]$$ (17)\n$$= \\mathbb{E}_{s_0 \\sim P_0} \\sum_{t=0}^T \\mathbb{E}_{a_t \\sim \\pi(s_t)} [\\log M_\\psi(s_t, a_t)] + const$$ (18)\n$$> \\mathbb{E}_{s_0 \\sim P_0} \\sum_{t=0}^T \\mathbb{E}_{a_t \\sim \\pi(s_t)} [\\log M_\\psi(s_t, a_t)].$$ (19)\nUsing an accurate learned dynamics model $\\hat{\\mathcal{T}}$, we can estimate a lower bound of trajectory entropy using $\\mathcal{M}$. By performing additional manipulation, we can instead extract a certainty estimate over a trajectory:\n$$\\exp(-H(\\tau_{0:T})) \\leq \\exp \\sum_{t=0}^T \\mathbb{E}_\\pi [\\log M_\\psi(s_t, a_t)]$$ (20)\n$$= \\prod_{t=0}^T \\mathbb{E}_\\pi [M_\\psi(s_t, a_t)]$$ (21)\n$$:= P_\\mathcal{M} (\\tau_{0:T}) \\in [0, 1]$$ (22)"}, {"title": "Definition 3 (OOD Truncation Function)", "content": "Given a rollout, at each timestep compute the Morse-probability $M_\\psi(s_t, a_t)$ and the trajectory-probability $P_\\mathcal{M} (\\tau_{0:t}) = P_\\mathcal{M} (\\tau_{0:t-1}) \\times M_\\psi(s_t,a_t)$ and truncate the episode if the probability is below a threshold:\n$$trunc(P_\\mathcal{M} (\\tau_{0:t})) = \\begin{cases}True & \\text{if } P_\\mathcal{M} (\\tau_{0:t}) < \\epsilon_{trunc}, \\\\False & \\text{otherwise}.\\end{cases}$$\n(23)\nHere, $\\epsilon_{trunc} \\in [0, 1]$ is a hyperparameter to be chosen that determines how much a trajectory can deviate from a behaviorally consistent one before the rollout is truncated.\nSelecting $\\epsilon_{trunc} = 1$ will demand perfect reproduction of behavior policy trajectories and (in the absence of dynamics model errors) will be equivalent to model-free RL using only the offline dataset. Selecting a lower threshold allows the policy to explore the dynamics-estimated environment with increasingly low probability trajectories permitted as $\\epsilon_{trunc} \\rightarrow 0$. Algorithm 2 describes how we employ the truncation function when generating rollouts.\n4.4 Practical Implementation\nWe discuss aspects of MoMo's implementation here and provide additional details in the Supplementary Material.\nMorse Net Training an implicit model with bounded Lipschitz constant also bounds the model error [8]. When using neural network function approximators, this can be realized in several ways. We use a maximum-margin gradient penalty [21] with Lipschitz constant set to 1 and add the gradient penalty loss as an auxiliary objective to Equation 4 with equal weighting.\nDeep Architectures Rank collapse limits the expressivity of deep neural networks in offline RL [16, 24]. Morse neural networks benefit from deep architectures [7, 8] which can suffer from implicit under-parametrization. This is a concern in MoMo as we directly use the Morse network as a loss function. Sinha et al. [37] present D2RL, a deep fully-connected architecture that uses skip connections which alleviate rank collapse at minimal extra computational cost. We adopt this architecture for the Morse neural network. In addition, we use layer normalization [3] for the Morse and critic networks which improves stability.\nActor-Critic MoMo can be used with any model-free RL algorithm. We use TD3 [11] as the base actor-critic algorithm and retain standard TD3 hyperparameters. We use the policy objective specified in Equation 10 and also normalize the Q values for easier optimization.\nDynamics Model A hallmark of most previous offline MBRL is the training and use of an ensemble (typically 4-7) of dynamics models to both prevent exploitation of individual models and for improved uncertainty estimation. The Morse network is an uncertainty estimator that we use directly to identify when a rollout goes OOD and for the anti-exploration bonus. This negates the need for ensemble-based uncertainty. Consequently, we train and use a single Gaussian dynamics model for all tasks.\nHyperparameters Model-based MoMo is an offline MBRL algorithm that largely inherits the hyperparameters of MBPO. Furthermore, model-based MoMo adds two new parameters: 1) kernel scale $\\lambda$ and 2) truncation threshold $\\epsilon_{trunc}$. Prior critic-regularizing offline MBRL methods demand per-dataset tuning of hyperparameters to achieve reported performance. Such fine-grained tuning requirements can hamper the adoption of algorithms in real-world applications. In our experiments, we use a single set of hyperparameters in each domain to evaluate MoMo (i.e. one constant set of hyperparameters for all Locomotion tasks and a separate constant set for all Adroit tasks). Our ablation experiments demonstrate robust performance over a range of sensible hyperparameters. For primary results, we use $\\lambda = 1.0$ and $\\epsilon_{trunc} = 0.95$ for Locomotion tasks and $\\lambda = 2.0$, $\\epsilon_{trunc} = 0.98$ for Adroit."}, {"title": "5 Experiments", "content": "We evaluate both model-based and model-free (i.e. generating no synthetic rollouts using learned dynamics) versions of MoMo and compare them with a range of baselines. We include the model-free baselines: CQL [25], IQL [23], TD3-BC [10] and TD3-CVAE (model-free with anti-exploration) [34], the last of which is the progenitor of anti-exploration methods.\nWe also include the offline MBRL baselines: MOREL [22], MOPO [48], RAMBO [36], COMBO [49], ARMOR [5], MOBILE [39] and OSR [20]. The final three are recent methods that achieve state-of-the-art performance on benchmarks and are representative of the major techniques in modern offline MBRL.\nARMOR adopts an adversarial approach and performs value penalization on actions produced by the policy and increases values for actions produced by a reference policy $\\pi_{ref}$, which must either be provided or be learned via behavioral cloning. MOBILE estimates uncertainty using estimated Q-values over the next states predicted by the dynamics models and penalizes the bootstrapped update with a penalty derived from the uncertainty. The authors of OSR propose two different methods of regularization: one that is policy constraining and the other a CQL-like critic regularization. OSR's policy is directed to recover towards in-dataset states when starting from unseen ones. We select results for the former version of OSR as a direct comparison to our policy constraint-based MoMo.\nIn our experiments, we aim to answer the following questions:\n* How does MoMo perform in both model-based and model-free regimes?\n* How important is the threshold $\\epsilon_{trunc}$ for model-based performance?\n* How sensitive is performance to the kernel scale $\\lambda$?"}, {"title": "5.1 D4RL Performance", "content": "We compare model-free and model-based MoMo with the aforementioned baselines for Gym Locomotion tasks in Table 1 and Adroit tasks in Table 2.\nGym Locomotion Gym Locomotion results show that model-based MoMo achieves the highest scores (either best or within 1 standard deviation of best) in 9 of the 12 datasets. Furthermore, MoMo-mb recovers expert performance ($\\geq$ 95.0) in 6 of 12 datasets with 100% expert performance recovery on the -m-e datasets which contain a mixture of optimal and suboptimal trajectories. The -r datasets pose a significant challenge to all methods they contain highly suboptimal examples generated by a random uniform policy and all methods tend to perform poorly on these tasks. Comparing MoMo-mf to MoMo-mb, changing to the model-based domain improves performance by an average of 21.5%, with marked improvements in the -r, -m and -m-r datasets. Finally, MoMo-mf outperforms both policy-constrained TD3-CVAE and dynamics-constrained OSR.\nAdroit The Adroit datasets pose a very different set of challenges to those in Locomotion tasks: Adroit datasets are marked by having few, human-generated examples (-h) or a mix between human and BC-trained behavior policies (-c). Consequently, learning algorithms must be able to deal with highly reward-disparate trajectories effectively and must generalize well. MoMo-mb achieves the best performance among all methods in 8 of 12 Adroit datasets and MoMo-mf consistently outperforms model-free baselines."}, {"title": "5.2 Ablating Threshold $\\epsilon_{trunc}$", "content": "The -m-r and -m-e Gym Locomotion tasks contain trajectories from two or more policies, which makes them candidate datasets for ablating MoMo hyperparameters. We evaluate performance when changing the truncation threshold parameter for values $\\epsilon_{trunc} = \\{0.80, 0.85, 0.90, 0.95, 0.98\\}$ in Figure 2.\nAcross the board, reducing $\\epsilon_{trunc}$ degrades performance. This is plausible, as reducing $\\epsilon_{trunc}$ allows a trajectory to deviate more from one following the behavior policy before it is truncated. Overestimated action values inevitably propagate, resulting in an uncorrected distribution shift that results in low performance. Increasing $\\epsilon_{trunc}$ to 0.98 has a relatively small effect on performance.\nA higher $\\epsilon_{trunc}$ increases the aggressiveness of truncation to ensure that trajectories are closer to those following the behavior policy. In effect, this tries to ensure that the trajectories generated vary less from those in the dataset and consequently, we expect performance to be similar to model-free MoMo. In practice, dynamics model error may cause MoMo-mb to underperform compared to MoMo-mf."}, {"title": "5.3 Sensitivity to $\\lambda$", "content": "We ablate the kernel scale parameter $\\lambda$ on the -m-r and -m-e datasets for both MoMo-mf and MoMo-mb in Figure 3. Increasing $\\lambda$ sharpens the density around the dataset modes (see Figure 1) and is equivalent to increasing the relative weighting of the policy constraint (recall that the $\\log$ RBF kernel is $-\\lambda || z - t ||^2$).\nFigure 3 shows that using $\\lambda = 0.1$ leads to a substantial performance drop in both model-free and model-based conditions as the constraint is too lax. Increasing $\\lambda = 2.0$ has mixed results, and further increasing to $\\lambda = 4.0$ likely overconstrains the policy. Interestingly, the change from $\\lambda = 1.0 \\rightarrow 2.0$ has a larger impact on MoMo-mf, on the m-e datasets where this change results in a -22.9% performance change compared to a -7.1% change for MoMo-mb."}, {"title": "5.4 Importance of Anti-Exploration", "content": "The primary mechanism by which MoMo encourages the policy to stay in-support is via a policy constraint. Previous work suggests that removing the policy constraint results in poor performance [43]. The anti-exploration bonus (see Equation 11) augments the bootstrapped return estimate, which in MoMo upper bounds the logarithm of the behavior policy density. This performs a regularized update that bounds KL regularized value iteration [13, 45] and curbs value overestimation caused by off-policy evaluation. We evaluate the impact of the anti-exploration bonus on MoMo-mb in Table 3. Removing the bonus causes performance to drop in almost all datasets, with an average performance decrease of 12.5%, suggesting MoMo's distance-aware conservatism is critical to performance. The mixed impact on -random datasets may be due to the challenges of training the Morse network on actions sampled from a random policy."}, {"title": "6 Discussion", "content": "Computational Cost Model-based MoMo trains a dynamics model as well as the Morse neural"}]}