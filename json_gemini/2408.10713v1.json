{"title": "Offline Model-Based Reinforcement Learning with Anti-Exploration", "authors": ["Padmanaba Srinivasan", "William Knottenbelt"], "abstract": "Model-based reinforcement learning (MBRL) algorithms learn a dynamics model from collected data and apply it to generate synthetic trajectories to enable faster learning. This is an especially promising paradigm in offline reinforcement learning (RL) where data may be limited in quantity, in addition to being deficient in coverage and quality. Practical approaches to offline MBRL usually rely on ensembles of dynamics models to prevent exploitation of any values in states far from the dataset support. Uncertainty estimates from ensembles can vary greatly in scale, making it challenging to generalize hyperparameters well across even similar tasks. In this paper, we present Morse Model-based offline RL (MoMo), which extends the anti-exploration paradigm found in offline model-free RL to the model-based space. We develop model-free and model-based variants of MoMo and show how the model-free version can be extended to detect and deal with out-of-distribution (OOD) states using explicit uncertainty estimation without the need for large ensembles. MoMo performs offline MBRL using an anti-exploration bonus to counteract value overestimation in combination with a policy constraint, as well as a truncation function to terminate synthetic rollouts that are excessively OOD. Experimentally, we find that both model-free and model-based MoMo perform well, and the latter outperforms prior model-based and model-free baselines on the majority of D4RL datasets tested.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) aims to learn policies for sequential decision-making that maximize an expected reward [42]. In online RL, this means alternating between interacting with the environment to collect new data and then improving the policy using previously collected data. Model-based reinforcement learning (MBRL) denotes techniques in which an established environment model or an approximation of the environment is used to simulate policy rollouts without having to directly query the environment [40, 41, 42, 30].\nOffline RL tackles problems where the policy cannot interact with and explore the real environment but instead can only access a static dataset of trajectories. The offline dataset can be composed of mixed-quality trajectories with poor coverage of the state-action space. With no ability to extrapolate beyond the offline dataset, standard model-free offline RL algorithms often apply systematic pessimism by either injecting penalties into the estimation of values or by constraining the policy [32, 27] to remain in action support. Offline MBRL can offer more flexibility by augmenting the offline dataset with additional synthetic rollouts that expand coverage, while also remaining within the dataset support [22, 48, 49].\nMBRL is fraught with danger: approximation errors in the dynamics model can cause divergence in temporal difference updates [44], errors can accumulate over multistep rollouts, resulting in performance deterioration if unaddressed [18], and the dynamics model can be exploited by the agent [33, 26, 6]. In offline MBRL, these issues are amplified due to the double dose of extrapolation error present in both dynamics and value functions. Consequently, standard online MBRL methods perform poorly when directly applied offline [48].\nOffline RL approaches typically fall into one of two categories: 1) critic regularization methods which penalize OOD action-values or 2) policy constraint methods that minimize a divergence between the learned policy and (a potentially estimated) behavior policy. Studying model-free offline RL literature exposes a variety of approaches in both paradigms [32, 27], but offline MBRL methods exhibit far less diversity [17].\nMany previous offline MBRL algorithms are based on MBPO [18] and perform conservative value updates that identify the increased epistemic uncertainty in out-of-distribution (OOD) regions using dynamics ensembles [6] or actively penalize all action-values from synthetic samples [49]. Ultimately, these fall firmly under the critic regularization paradigm, and their empirical evaluation reveals that they perform little better than their model-free counterparts [2, 14]. A small number of methods implement policy-constrained offline MBRL [50, 20], though these do not realize the benefits of augmenting training with samples from the dynamics model.\nOur approach to offline MBRL is based on the anti-exploration [35] paradigm originating in model-free offline RL which uses a policy constraint in combination with an anti-exploration bonus to curb value overestimation. We train a Morse neural network [7] and exploit its properties as an implicit behavior model, and an OOD detector to extend model-free anti-exploration for MBRL, which realizes the benefits of both policy-constrained offline RL and training with synthetic data.\nIn this work, we present Morse Model-based offline RL (MoMo) which performs offline model-free RL using anti-exploration. MoMo trains a dynamics model and a Morse neural network model, the latter of which is an energy-based model that learns an implicit behavior policy and an OOD detector. We demonstrate how this can be incorporated into a model-free anti-exploration-based offline RL method and then adapt it to detect and truncate rollouts in the dynamics model that are becoming excessively out-of-distribution. The Morse neural network's uncertainty estimation helps us avoid the need for large dynamics ensembles we can instead use a single dynamics model. Furthermore, the Morse network's calibrated uncertainty estimates allow for better hyperparameter generalization across similar tasks.\nOur experiments evaluate both model-free and model-based versions of MoMo on the D4RL Gym Locomotion and Adroit tasks [9] and establish MoMo's performance to be equivalent to or better than recent baselines in 17 of 24 datasets. In-depth ablation experiments analyze MoMo's hyperparameter sensitivity and the effectiveness of anti-exploration value penalties in offline MBRL."}, {"title": "2 Related Work", "content": "We consider RL in the form of a Markov Decision Process (MDP), M = {S, A, R, T, Po, y}, which consists of a tuple with state space S, action space A, a scalar reward R(s, a), transition dynamics T, initial state distribution Po and discount factor \u03b3\u2208 [0,1) [42]. RL aims to learn a policy that will execute an action a = \u03c0(s) that maximizes the expected discounted reward $J(\\pi) = E_{\\tau \\sim T_{\\pi}(\\tau)} [\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)]$ where $T_{\\pi}(\\tau) = P_0(s_0) \\prod_{t=0}^{\\infty} \\pi(a_t | s_t)T(s_{t+1} | s_t, a_t)$ is the trajectory under \u03c0.\nOffline RL methods are designed to learn from a static dataset D, which consists of interactions collected by applying a policy \u03b2, without access to the environment [27, 12, 38]. The offline dataset may provide poor coverage of the state-action space, and so when using function approximators, the policy must be constrained to select the actions within the support of the dataset. Offline RL methods (generally) belong to one of two families of approaches: 1) critic regularization or 2) policy constraint [32, 27]. Uncertainty-based model-free methods use large ensembles [2, 14] to perform a variation of critic regularization.\nRezaeifar et al. [35] pose model-free offline RL as an exercise in anti-exploration, an approach in which the actor is constrained via an explicit divergence minimization term in the policy improvement step and policy evaluation is augmented with an anti-exploration bonus (value penalty) to counteract overestimation in off-policy evaluation. This bears some similarity to the BRAC framework [46] which regularizes (either policy or value) using an explicit divergence between the current and estimated behavior policies.\nSeveral works propose different heuristics for estimating divergence in anti-exploration algorithms [31, 43], though thus far anti-exploration has remained exclusive to model-free methods.\nOffline MBRL injects samples from a learned dynamics model into the RL learning process and leverages the diversity of simulated rollouts to augment the offline dataset with additional data. In online MBRL, Janner et al. [18] introduce MBPO which uses an ensemble of dynamics models and performs Dyna-style policy learning [40, 41]. MBPO's ensemble does not discourage OOD exploration; Yu et al. [48] address this by penalizing reward with ensemble uncertainty and Kidambi et al. [22] use ensemble uncertainty to detect, penalize and terminate synthetic rollouts when they are excessively OOD. Several works develop methods to better estimate and penalize values using different uncertainty estimation techniques [39, 19].\nCOMBO [49] performs CQL-like critic regularization [25] on samples from model rollouts. This adversarial approach to offline MBRL is further developed by Rigter et al. [36], Liu et al. [28], Bhardwaj et al. [5] and Luo et al. [29] who present alternative approaches to learning pessimistic value functions.\nPolicy constraint-based offline MBRL embraces an overarching goal to direct the policy towards states with known dynamics when starting in unknown ones. Zhang et al. [50] train ensemble dynamics and a behavioral transition model that is used to ensure that the next state the policy will transition to is within the dataset support. Jiang et al. [20] train an inverse dynamics model that constrains the policy to stay close to the behavior policy. Neither method performs policy rollouts using dynamics models, as estimates of the behavior policy do not extrapolate well. Their lack of training on synthetic data puts these methods at odds with critic-regularized offline MBRL."}, {"title": "3 Preliminaries", "content": "Dherin et al. [7] develop the Morse neural network, an uncertainty estimator that produces an unnormalized density M(x) \u2208 [0, 1] in an embedding space Re. At its modes, the density achieves a value of 1 and approaches 0 with increasing distance from the mode. A Morse neural network is defined by the combination of a neural network function approximator and a Morse kernel:\nDefinition 1 (Morse Kernel). A Morse Kernel is a positive definite kernel K that is applied on a space Z = Re, K(z1, z2) and takes its values in the interval [0, 1] with K(z1, z2) = 1 only when z1 = z2.\nAll kernels that use a measure of divergence are Morse kernels [1]. A commonly used kernel is the radial basis function (RBF) kernel with scale parameter > > 0\n$K(z,t) = e^{-\\lambda ||z - t||^2}$ (1)\nThe Morse kernel describes how close an embedding z is to the target t. The relationship between the input x and its corresponding embedding z is determined by a neural network $f_\\psi$: X \u2192 Z, X \u2208 Rd and Z\u2208 Re with parameters \u03c8. Combining the Morse kernel with a neural network yields the Morse neural network:\nDefinition 2 (Morse Neural Network). A Morse neural network is comprised of a function $f_\\psi$ : X \u2192 Z and a Morse Kernel K(z, t) where t CZ is a target embedding where its size and value are chosen as a model hyperparameter. The Morse neural network is given by $M_\\psi(x) = K(f_\\psi(x), t)$.\nUsing Definitions 1 and 2, if x maps to a mode in the level set of the submanifold Z, then $M_\\psi$(x) = 1. We interpret $M_\\psi$(x) as the certainty that the sample x is from the training dataset and 1 \u2013 $M_\\psi$(x) is an estimate of the epistemic uncertainty of x. The function log $M_\\psi$(x) measures measures the distance between z and the nearest mode at t of the set of modes M:\n$d(z) = \\min_{t \\in M} d(z,t),$ (2)\nThe trained Morse neural network offers the following properties [7]:\n1.  $M_\\psi(x) \\in [0, 1]$.\n2.  $M_\\psi(x) = 1$ at all mode submanifolds.\n3.  -log $M_\\psi$(x) \u2265 0 is a squared distance that satisfies the Morse-Bott non-degeneracy condition at the mode submanifolds [4].\n4.  $M_\\psi$(x) is an exponentiated squared distance, the function is distance aware in the sense that as $f_\\psi$(x) \u2192t, $M_\\psi$(x) \u2192 1.\nSee Dherin et al. [7] for the proof of each property."}, {"title": "4 Morse Model-Based Offline RL", "content": "In this section, we describe our approach to policy-constrained offline MBRL that can use samples from synthetic rollouts to augment the offline dataset. We begin by adapting the Morse neural network for offline RL as a method for anti-exploration. Then in Section 4.3 we show how this can be extended to MBRL."}, {"title": "4.1 Morse Neural Networks Learn an Implicit Behavior Policy", "content": "The offline RL dataset consists of K tuples D = {$s,a,r,s'\\}_{i=1}^{K}$ that provide partial coverage of the state-action space. We train a Morse neural network to learn a conditional uncertainty model over actions in the dataset. Adapting the objective from Dherin et al. [7], we minimize the KL divergence $D_{KL}(D(s, a) || M_\\psi(s, a))$ using:\n$\\min_\\psi E_{s,a\\sim D} \\Big[\\log \\frac{D(s, a)}{M_\\psi(s, a)}\\Big] + E_{s\\sim D} \\Big[\\int M_\\psi(s,a) - D(s, a) da\\Big],$ (3)\noptimizing with respect to \u03c8, we minimize the empirical loss:\n$\\min_\\psi E_{s,a\\sim D} [\u2013 \\log K (f_\\psi (s, a), t)]+ E_{s\\sim D} [K(f_\\psi(s, a_u), t)],$\n$a_u \\sim D_{uni}$ (4)\nwhere $D_{uni}$ denotes a uniform distribution over the action space. We outline the Morse neural network training procedure in Algorithm 1. The Morse neural network is an energy-based model (EBM) [15, 7]:\n."}, {"title": "4.1 Morse Neural Networks Learn an Implicit\nBehavior Policy"}, {"title": "4.2 Challenges when Constraining Offline MBRL", "content": "Na\u00efvely applying our policy constraint to states from model rollouts will result in poor performance. The Morse neural network as an implicit behavior policy poorly extrapolates to unseen states; prior work by Xu et al. [47] and Florence et al. [8] shows that implicit models formed by fully connected neural networks can only perform linear function extrapolation, yet assuming that the behavior policy is linear can be overly restrictive.\nThis is a critical shortcoming of EBMs when used for the policy constraint and anti-exploration bonus in unknown states, which can lead to incorrect modes being learned/penalized. This drawback extends to prior offline MBRL methods using a dynamics constraint [50, 20] who subsequently restrict themselves to training on small perturbations of dataset states rather than using dynamics rollouts. To use dynamics models for data augmentations, we must ensure that rollouts remain within the data support, both so that the Morse-based constraint is valid and to prevent the exploitation of learned dynamics. The ability to combine policy constraint with rollout OOD detection is (to the best of our knowledge) unique to MoMo and in the next section, we describe how the uncertainty detection properties of the Morse neural network enable detection of when states in synthetic trajectories have moved too far from the dataset support."}, {"title": "4.3 Extending to Offline MBRL", "content": "MBPO-based offline MBRL algorithms [18] typically penalize OOD regions by directly using ensemble uncertainty estimates [22, 48] or assuming that all states produced by the dynamics models are OOD [49, 28, 36, 5, 29]. MOREL [22] terminates the rollouts when the estimated uncertainty increases beyond a threshold. We perform a similar rollout truncation when the policy is excessively OOD using the Morse neural network.\nOur rollout truncation refers to stopping the rollout without performing any subsequent bootstrapping or episode termination (i.e., we do not update truncated action-values during policy evaluation). This differs slightly from the treatment in online RL as in the offline domain, truncation is not an environment property, but a property that we impose on the empirical MDP.\nLet \u03b2(als) denote the behavior policy, st denote the state at time t, at denote the action selected by an agent, Po denote the initial state distribution and T(st+1 | St, at) denote the state transition. We assume that the environment transition is deterministic and aim to train an agent to minimize the (cross) entropy under the behavior policy over a trajectory To:T:\n$H(T_{0:T}) = \\sum_{t=0}^T E_{\\substack{s_0\\sim P_0\\\\ a_t\\sim\\pi(a_t|s_t)}} [\\log \\beta(a_t|s_t)] | s_{t+1}\\sim T(\\cdot|s_t,a_t)$ (16)\n$\\sum_{t=0}^T E_{\\substack{s_0\\sim P_0\\\\ a_t\\sim\\pi(s_t)}} [\\log M_\\psi(s_t, a_t) \u2013 \\log Z_\\psi(s_t)] $ (17)\n$\\sum_{t=0}^T E_{\\substack{s_0\\sim P_0\\\\ a_t\\sim\\pi(s_t)}} [\\log M_\\psi(s_t, a_t)] + const $ (18)\n<$\\sum_{t=0}^T E_{\\substack{s_0\\sim P_0\\\\ a_t\\sim\\pi(s_t)}} [\\log M_\\psi(s_t, a_t)].$ (19)\nUsing an accurate learned dynamics model \u00ce, we can estimate a lower bound of trajectory entropy using M. By performing additional manipulation, we can instead extract a certainty estimate over a trajectory:\n$exp(-H(T_{0:T})) \\leq exp(\\sum_{t=0}^T E_\\pi [\\log M_\\psi(s_t, a_t)])$ (20)\n$\\prod_{t=0}^T E_\\pi [M_\\psi(s_t, a_t)]$ (21)\n$:= P_M (T_{0:T}) \\in [0, 1]$ (22)"}, {"title": "4.4 Practical Implementation", "content": "We discuss aspects of MoMo's implementation here and provide additional details in the Supplementary Material.\nMorse Net Training an implicit model with bounded Lipschitz constant also bounds the model error [8]. When using neural network function approximators, this can be realized in several ways. We use a maximum-margin gradient penalty [21] with Lipschitz constant set to 1 and add the gradient penalty loss as an auxiliary objective to Equation 4 with equal weighting.\nDeep Architectures Rank collapse limits the expressivity of deep neural networks in offline RL [16, 24]. Morse neural networks benefit from deep architectures [7, 8] which can suffer from implicit underparametrization. This is a concern in MoMo as we directly use the Morse network as a loss function. Sinha et al. [37] present D2RL, a deep fully-connected architecture that uses skip connections which alleviate rank collapse at minimal extra computational cost. We adopt this architecture for the Morse neural network. In addition, we use layer normalization [3] for the Morse and critic networks which improves stability.\nMoMo can be used with any model-free RL algorithm. We use TD3 [11] as the base actor-critic algorithm and retain standard TD3 hyperparameters. We use the policy objective specified in Equation 10 and also normalize the Q values for easier optimization.\nA hallmark of most previous offline MBRL is the training and use of an ensemble (typically 4-7) of dynamics models to both prevent exploitation of individual models and for improved uncertainty estimation. The Morse network is an uncertainty estimator that we use directly to identify when a rollout goes OOD and for the anti-exploration bonus. This negates the need for ensemble-based uncertainty. Consequently, we train and use a single Gaussian dynamics model for all tasks.\nModel-based MoMo is an offline MBRL algorithm that largely inherits the hyperparameters of MBPO. Furthermore, model-based MoMo adds two new parameters: 1) kernel scale \u03bb and 2) truncation threshold \u20actrunc. Prior critic-regularizing offline MBRL methods demand per-dataset tuning of hyperparameters to achieve reported performance. Such fine-grained tuning requirements can hamper the adoption of algorithms in real-world applications. In our experiments, we use a single set of hyperparameters in each domain to evaluate MoMo (i.e. one constant set of hyperparameters for all Locomotion tasks and a separate constant set for all Adroit tasks). Our ablation experiments demonstrate robust performance over a range of sensible hyperparameters. For primary results, we use x = 1.0 and \u20actrunc = 0.95 for Locomotion tasks and x = 2.0, \u20actrunc = 0.98 for Adroit."}, {"title": "5 Experiments", "content": "We evaluate both model-based and model-free (i.e. generating no synthetic rollouts using learned dynamics) versions of MOMO and compare them with a range of baselines. We include the model-free baselines: CQL [25], IQL [23], TD3-BC [10] and TD3-CVAE (model-free with anti-exploration) [34], the last of which is the progenitor of anti-exploration methods.\nWe also include the offline MBRL baselines: MOREL [22], MOPO [48], RAMBO [36], COMBO [49], ARMOR [5], MOBILE [39] and OSR [20]. The final three are recent methods that achieve state-of-the-art performance on benchmarks and are representative of the major techniques in modern offline MBRL.\nARMOR adopts an adversarial approach and performs value penalization on actions produced by the policy and increases values for actions produced by a reference policy ref, which must either be provided or be learned via behavioral cloning. MOBILE estimates uncertainty using estimated Q-values over the next states predicted by the dynamics models and penalizes the bootstrapped update with a penalty derived from the uncertainty. The authors of OSR propose two different methods of regularization: one that is policy constraining and the other a CQL-like critic regularization. OSR's policy is directed to recover towards in-dataset states when starting from unseen ones. We select results for the former version of OSR as a direct comparison to our policy constraint-based MoMo.\nIn our experiments, we aim to answer the following questions:\n\u2022 How does MoMo perform in both model-based and model-free regimes?\n\u2022 How important is the threshold Etrunc for model-based performance?\n\u2022 How sensitive is performance to the kernel scale X?"}, {"title": "4.3 Extending to Offline MBRL", "content": "where the final line is the Morse-probability that the trajectory is behavior-consistent. This is an upper-bound estimate of the negative-exponentiated entropy. Therefore, a low entropy trajectory has a high trajectory-probability PM (To:T). With the ability to estimate the probability of a rollout, we design a function to truncate a rollout when its probability falls below a threshold.\nWe design a detection mechanism to identify when a trajectory is excessively OOD and truncate the synthetic rollout.\n (OOD Truncation Function). Given a rollout, at each timestep compute the Morse-probability My (st, at) and the trajectory-probability PM (To:t) = PM (T0:t\u22121) \u00d7 M\u2084(St,at) and truncate the episode if the probability is below a threshold:\ntrunc(PM (To:t)) =\n{\nTrue if PM (To:t) < Etrunc,\nFalse otherwise.\nDefinition 3"}, {"content": "Here, Etrunc \u2208 [0, 1] is a hyperparameter to be chosen that determines how much a trajectory can deviate from a behaviorally consistent one before the rollout is truncated.\nSelecting \u20actrunc = 1 will demand perfect reproduction of behavior policy trajectories and (in the absence of dynamics model errors) will be equivalent to model-free RL using only the offline dataset. Selecting a lower threshold allows the policy to explore the dynamics-estimated environment with increasingly low probability trajectories permitted as trunc \u2192 0. Algorithm 2 describes how we employ the truncation function when generating rollouts."}]}