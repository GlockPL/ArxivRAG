{"title": "Contrastive Learning Subspace for Text Clustering", "authors": ["Qian Yong", "Chen Chen", "Xiabing Zhou"], "abstract": "Contrastive learning has been frequently investigated to learn effective representations for text clustering tasks. While existing contrastive learning-based text clustering methods only focus on modeling instance-wise semantic similarity relationships, they ignore contextual information and underlying relationships among all instances that needs to be clustered. In this paper, we propose a novel text clustering approach called Subspace Contrastive Learning (SCL) which models cluster-wise relationships among instances. Specifically, the proposed SCL consists of two main modules: (1) a self-expressive module that constructs virtual positive samples and (2) a contrastive learning module that further learns a discriminative subspace to capture task-specific cluster-wise relationships among texts. Experimental results show that the proposed SCL method not only has achieved superior results on multiple task clustering datasets but also has less complexity in positive sample construction.", "sections": [{"title": "1 Introduction", "content": "Clustering is an typical unsupervised learning technique that has been widely employed to categorize unlabeled data or difficult-to-label data. Traditional text clustering methods for natural language processing (NLP) usually consists of two stages: text representation learning and categorization, i.e., texts are firstly represented as a set of feature vectors, which are then categorized into several clusters in a unsupervised manner. For example, previous approaches frequently employ word2vector or sentence-BERT to encode original texts to representations and then apply K-means to obtain the final text clusters.\nConsequently, in addition to the clustering algorithm itself, the representations of texts is also a key factor determining the clustering performance. In other words, text representation learning is crucial for text analysis tasks, where well designed representation learning models can usually extract representations containing more task-specific semantic information which can largely enhance the performance of various text tasks such as text classification , text clustering , etc. Although previous studies proposed various schemes to enhance the BERT-based representations, they usually fail to clearly improve the performance of text clustering tasks.\nTo learn more discriminative and task-specific text representations, recent methods have frequently extended contrastive learning algorithms to text clustering tasks. Specifically, contrastive learning enforces models to learn a subspace where samples belong to the same category have very similar representations, while the representations of samples belong to different categories have long distances, which has achieved superior performances on text clustering tasks (e.g., simCSE , SCCL , Li et al. ). However, all of such methods require a key prior knowledge: the number of categories, which may not available in many real-world scenarios, and thus limits their usage. In addition, most contrastive learning methods suffer from a high cost of positive sample construction and need to encode these samples twice.\nTo tackle the aforementioned challenges, we propose a novel Subspace Contrastive Learning (SCL) approach for text clustering, i.e., the proposed SCL utilizes contrastive learning to learn an optimal deep subspace model. The hypothesis of this paper is that each sample can be represented by a linear combination of several features lying in a set of latent subspaces. Specifically, our method aims to capture the cluster-wise relationships with the lower cost of positive sample construction while not requiring the number of categories. We treat the self-expression as an augmented method to construct the virtual positive samples for contrastive learning, which not only portrays the cluster-wise relationship, but also avoids complex two-stage text augmentation for existing NLP models, i.e., while existing contrastive learning approaches () would individually feed each original sample and its corresponding positive sample to the encoder, our SCL construct postive sample pairs, which can jointly feed them to the encoder.  The main contributions of this paper are summarized as follows:\n\u2022 We propose a cluster-wise contrastive learning approach which allows models to learn the cluster-wise relationship between input samples through subspace modeling. To the best of our knowledge, this is the first cluster-wise contrastive learning approach for text clustering.\n\u2022 We propose a new self-expressive method for constructing virtual positive samples, which is not only faster but also has better generalization capability when combined with shallow networks compared with other text augmentation techniques.\n\u2022 Our comprehensive experiments on text clustering task results show that our method outperforms the State-of-the-art method on multiple datasets."}, {"title": "2 Related Works", "content": "2.1 Contrastive Learning\nContrastive Learning is an unsupervised learning method to learn effective representations, which has been widely applied in various visual representation learning , representation learning and text representation learning. The main idea of the contrastive learning is to enforce models to encode similar representations from samples augmented from the same instance and enlarge the distance among less related ones, so that similar samples are gathered together and different types of samples will distribute on the hypersphere as far as possible.\nAs a result, contrastive learning has frequently improved the performances of various downstream works. For example, semantic similarity tasks employ contrastive learning over pre-training language models to get more similarity-sensitive sentence representations and clustering tasks learn a clustering object and a contrastive object jointly to relieve the overlap problem across categories before clustering starts. However, applying contrastive learning to NLP tasks sometimes faces the data augmentation problem which is much more difficult to tackle compared with CV field. Thus, text augmentation for contrastive learning has attracted a lot of interest. Zhang et al. proposed the contextual augmenter which leverages the pre-trained transformers to find top-n suitable words of the input text for insertion or substitution. Gao et al. takes dropout as a minimal data augmentation method, and passes the same input sentence to a pre-trained Transformer encoder twice. Wu et al. further addressed the length bias of simCSE by repeating words from the input sentence.\n2.2 Subspace Clustering\nSubspace clustering is an unsupervised learning approach, which aims at finding out underlying subspaces to cluster data points from the same subspace. It has attracted lots of interests in computer vision tasks, such as image clustering , motion segmentation and multiview clustering. In these approaches, a crucial step is to learn an affinity matrix A where Aij represents"}, {"title": "3 The proposed approach", "content": "In this section, we propose a novel cluster-wise contrastive learning algorithm called Subspace Contrastive Learning (SCL) for learning cluster-aware text representations, which generates virtual positive samples from the self-expressive mechanism. As illustrated in Figure 2, our approach consists of two main modules: (1) a pre-trained transformer-based encoder that first vectorizes N input sentences X = [X1,X2,\u2026,XN] \u2208 RN\u00d7D, and encodes them as a set of latent representations E = [e1,2,\u2026\u2026,en], which would be further adapted to downstream clustering tasks; and (2) a self-expressive module that constructs virtual positive samples (Sec. 3.2), where we provide the object function for optimizing our model in a constrastive manner.\n3.1 Self-expressive Module\nWe propose a novel self-expressive module to efficiently generate virtual positive samples for text augmentation. Specifically, the self-expressive module conducts an attention operation. It first learns query matrix Eq and key matrix Ek from the latent representations E = [e1,2,\u2026\u2026, en], and then conducts matrix multiplication to obtain affinity matrix A \u2208 RN\u00d7N. Finally, virtual positive samples E' can be obtained by further conducting matrix multiplication between the affinity matrix A and the latent representations E produced from the encoder."}, {"title": "This process can be formulated as:", "content": "\u0394 = EqEk\nE\" = (A \u2013 I)E where we exclude cases that i = j\nIn particular, A is optimized in the form of:\nmin ||ej \u2013 Ei\u2260jAijei||2 + \u03bb\u03a3i\u2260j\u03b3(Aij)\n{Aij}ij       (2)\ne'j = \u03a3 Aijli\ni\u2260j\nwhere j\u2208 1,2,\u2026\u2026,N; Aij is the coefficient between ith and jth sample. Specifically, the first part ||ej \u2013 Ei\u2260jAijei||2 aims at maximizing the similarity between the original latent representations ej and the generated virtual samples e'. The second part Ei\u2260j\u03b3 (Aij) is a regularization object to allow the coefficient matrix becoming sparse or low-rank, where the most idealistic situation is that nonzero coefficients Aij only occur when ith and jth sample belong to the same subspace. This part can be abbreviated as Lregularization.\n3.2 Cluster-wise Contrastive Learning\nTo allow our model to generate cluster-aware virtual samples, we propose a novel cluster-wise contrastive learning loss. This loss aims to enforce the model to learn subspaces where the distance between each pair of real sample and virtual positive sample to be minimized while the distance of negative pairs are maximized.\nGiven a batch of training samples, we treat each latent representation x\u012f and corresponding virtual positive representations e generated by our self-expressive module as a pair of positive pair (i.e., N pairs of positive pairs). Meanwhile, we pair each latent representation x\u2081 with the other N \u2013 1 samples (i.e., e\u00b9,e2,\u2026\u2026\u2026,ei\u22121,ei+1,\u2026\u2026) and N \u2212 1 generated virtual samples as negative pairs. This would totally result in 2N \u2013 2 negative pairs. We then propose a cluster-wise constrastive learning loss function as follows:\n-log  where t is an initial temperature parameter and is negatively correlated with the averaged similarity between positive pairs in a batch of inputs. Consequently, the punishment on negative pairs will be relieved until the self-expressive representations become credible. This part can be abbreviated as Lcontrastive. where sim(\u00b7) is the cosine similarity function and \u03c4 denotes the temperature parameter which controls the punishment intensity on negative samples, i.e., smaller t leads more punishment. However, the virtual positive representations are generated and even have lower confidence compared with some real \"negative\" samples during the initial training epochs. Therefore, we set \u03c4 automatically adjusted with the virtual positive samples defined as:\n\u03c4=t/average(sim(xi,ev)),      (4)\nwhere t is an initial temperature parameter and is negatively correlated with the averaged similarity between positive pairs in a batch of inputs. Consequently, the punishment on negative pairs will be reduced until the self-expressive module can generate reliable virtual samples, i.e., the postive pairs and negative pairs can be easily distinguished. This part can be abbreviated as Lcontrastive.\nIn summary, we jointly optimize the encoder and self-expressive module in a contrastive learning manner as:\nL = AclLcontrastive + AregLregularization(5)"}, {"title": "4 Experiments", "content": "In this section, we first introduce the employed datasets and implementation details in Sec. 4.1.\nThen, we report a series of experimental results to demonstrate the effectiveness and various aspects of the proposed SCL in Sec. 4.2.\nand the baseline methods employed for comparison. We then report the experimental results conducted from different perspectives and analyze the effectiveness of the proposed model with different factors.\n4.1 Datasets and implementation details\nDataset: We compare the performance achieved by our SCL with other existing methods on seven benchmark datasets for the short text clustering task. Table 1 provides an overview of their main statistics. The details of these datasets are provided as follows:\n\u2022 SearchSnippets dataset contains 12,340 snippets extracted from the web search snippets , which are categorized into 8 classes.\n\u2022 StackOverflow dataset is a subset of the challenge data published by Kaggle, and contains 20,000 question titles selected by  of 20 categories.\n\u2022 AgNews dataset is a subset of news titles , which contains 8000 news of 4 categories selected by \n\u2022 Tweet dataset contains 472 tweets  of 89 categories.\n\u2022 GoogleNews contains 152 categories with titles and snippets of 11,109 news articles. Following . Specifically, it contains three sub-set, including GoogleNews-S containing news from snippets, GoogleNews-T containing news from titles and GoogleNews-TS containing news from both snippets and titles.\nImplementation details: We choose \"distilbert-base-nli-stsb-mean-tokens\" as the encoder for transformer-based language models, and \"glove.6B.300d\" as the encoder for word embeddings. Adam is employed as the optimizer with the learning rate of 5e-5, beta 1 of 1.0 and beta 2 of 1e-4. The dropout rate used in our model is set as 0.3 for all dropout layers. We employ the Accuracy (ACC) and Normalized Mutual Information(NMI) as the evaluation metrics.\n4.2 Results and analysis\n4.2.1 Comparison with existing contrastive learning methods\nTo show the superiority of the proposed approach, Table 2 compares it with multiple existing contrastive learning methods which are listed as follows:\n\u2022 SBERT is the baseline backbone, which is a transformer-based model pre-trained on the combination of the SNLI  and the Multi-Genre NLI  dataset. Then, the model is tuned on three supervised objects.\n\u2022 WordNet Augmenter  substitutes words by the synonyms of WordNet to minimize semantic changes.\n\u2022 Back translation generates augmented sentences of the input texts by first translating it to another language (Chinese) and then back to English.\n\u2022 Contextual Augmenter  replaces words with other words predicted by pre-training language models according to a context.\n\u2022 SimCSE  takes dropout as a minimal data augmentation method, and passes the same input sentence to a pre-trained Transformer encoder twice to generate the positive pairs.\n\u2022 ESimCSE  is an enhanced version of SimCSE. As the position embedding encoded by transformer relates to the length of input sentences all positive pairs representations in SimCSE incline to be more similar compared with those negative samples with different length. The ESimCSE applies a simple repetition to alleviate the aformentioned issue.\nAs we can see from the table, the proposed SCL achieved the state-of-the-art or comparable performances on all short text clustering datasets in comparison to listed approaches. Specifically, for datasets with fewer categories, SCL can achieve better performances than all other augment methods, i.e., our SCL improved the previous state-of-the-art method NMI from 57.8 to 63.3 on AgNews. Since there are only 4 categories, our approach can sample each category uniformly in every training iteration and generate good virtual positive samples. Meanwhile, the improvements of our approach are not significant on the three GoogleNews datasets,"}, {"title": "4.2.2 Subspace affinity", "content": "The main novelty of our proposed method is the proposed subspace contrastive learning which allows the learned representations to be cluster-aware. We show its advantage in the view of the affinity matrix. In particular, we randomly sample 500 sentences from different categories and feed them into the self-expressive layer to generate the affinity matrix without shuffle.  It can be observed that there are clear white squares appeared in the diagonal of the heatmap, while most of the rest parts are dark. Since pairs represented by the diagonal belongs to the same cluster, the achieved result indicates that the learned subspace can accurately represent the task and cluster-related features from the input texts.\nThen, Figure 3 compares the coincidence between those subspaces and clusters, where the regularization parameter Areg is highly correlated with affinity sparsity that directly influences the formation of subspace, and dreg ranges from 1e \u2013 4 to 1. It can be observed that when dreg is larger than 1e-1, lower performances were achieved. This is because the larger Areg would lead more punishment on sparsity's loss which leads to the samples only building connections with those most similar samples. Therefore, we need to tune the regularization parameter carefully to obtain a good balance."}, {"title": "4.2.3 Adapted temperature", "content": "Another key parameter in our contrastive learning method is the temperature which plays a role in controlling the strength of penalties on the hard negative samples. Specifically, our contrastive loss with small temperature tends to penalize heavily on the hardest negative samples. As a result, the learned local structure tends to be more discriminative, and the distribution of learned embeddings is likely to be more uniform. Despite that our main optimization loss is still like contrastive loss, its objective functions consist of two aspects. On one hand, the self-expressive layer is trained which simulates the cluster-wise relations and generates virtual positive samples. On the other hand, contrastive learning itself would learn a new sample distribution based on positive samples. In other words, there is a sequential relationship between them, which generates high confidence first and then does contrastive learning on the positive pairs.\nWe also evaluate the sensitivity of initial temperature setting.  It shows that the adapted temperature is stably increased by 0.02 compared with the fixed temperature. We explain this as the adapted temperature can simulate the sequential relationship by raising the temperature in the early stage of training and reducing the temperature as the self-expressive positive samples become more confident."}, {"title": "4.2.4 Comparison on word embeddings", "content": "In comparison with other contrastive learning methods which are mainly based on generating positive samples, our SCL not only has achieved better performance on transformer-based pre-training models, but also generalizes well on shallow networks such as glove. Meanwhile, the traditional text augmentation methods always suffer from a key problem: the sentence semantics is prone to change when replacing and deleting some words in the sentence. Although SimCSE and ESimCSE used dropout as a minimal data augmentation method to address this issue, which is simple and effective, some information would be lost on shallow"}, {"title": "5 Conclusion", "content": "In this work, we propose a novel subspace contrastive learning approach for text clustering, which is called SCL. The main advantage of this approach is that it models both instance-wise relationships and cluster-wise relationships among instances, allowing the generated representations to be cluster-aware. More importantly, it can conduct clustering without requiring the number of categories. The experimental results show that the proposed SCL achieved the state-of-the-art or comparable performances on various short text clustering datasets. Moreover, ablation studies show that (1) the proposed SCL can adapt to temperatures, which solves the sequential problem between contrastive objective and self-expressive objective and provide enhanced performance; and (2) the SCL is more suitable for pre-training word embeddings compared with other data augment methods.\nHowever, there are still two main limitations of our method. Firstly, the performance of our SCL model is sensitive to the training batch size, a small batch size may lead some samples can not find their subspace. Secondly, our method is not suitable to conduct text clustering when the given texts contain too many categories, or some recall work needs to be done before starting the training process. To further address such limitations, our future work will focus on developing better recall methods to help our SCL to meet the precondition and convergence faster."}]}