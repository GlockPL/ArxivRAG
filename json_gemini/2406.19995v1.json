{"title": "Single Parent Family: A Spectrum of Family Members from a Single Pre-Trained Foundation Model", "authors": ["Habib Hajimolahoseini", "Mohammad Hassanpour", "Foozhan Ataiefard", "Boxing Chen", "Yang Liu"], "abstract": "This paper introduces a novel method of Progressive Low Rank Decomposition (PLRD) tailored for the compression of large language models. Our approach leverages a pre-trained model, which is then incrementally decompressed to smaller sizes using progressively lower ranks. This method allows for significant reductions in computational overhead and energy consumption, as subsequent models are derived from the original without the need for retraining from scratch. We detail the implementation of PLRD, which strategically decreases the tensor ranks, thus optimizing the trade-off between model performance and resource usage. The efficacy of PLRD is demonstrated through extensive experiments showing that models trained with PLRD method on only 1B tokens maintain comparable performance with traditionally trained models while using 0.1% of the tokens. The versatility of PLRD is highlighted by its ability to generate multiple model sizes from a single foundational model, adapting fluidly to varying computational and memory budgets. Our findings suggest that PLRD could set a new standard for the efficient scaling of LLMs, making advanced AI more feasible on diverse platforms.", "sections": [{"title": "Introduction", "content": "Recent advancements in deep learning have led to the development of increasingly large models where the parameter count often exceeds several billion (Zhao et al., 2023). Large Language Models (LLMs) typically use float32 or float16 formats, where each float16 takes up 2 bytes. Thus, a model, for example, GPT-3 with 175B parameters requires 320 gigabytes, making it too large for most consumer devices (it requires at least five A100 GPUs only for running inference) (Zhu et al., 2023).\nLLMs are often released as a series of variants or family members with different sizes to accommodate a broad spectrum of computational resources and application needs. For instance, the Llama2 model is available in different sizes including 7 billion, 13 billion and 70 billion parameters. This tiered approach allows developers and organizations to select a model size that best fits their specific performance requirements and budget constraints. However, the problem is that all of these family members are trained from scratch and therefore the number of family members is very low (in order of 3-4). Even the smaller LLMs e.g. TinyLlama (Zhang et al., 2024), and Phi-2 (Javaheripi et al., 2023), are trained from scratch with billions of tokens which requires significant computation and memory. Furthermore, if the computational budget of a user allows a model size in between the two family members, the user has to pick the family member with the smaller size that may not be optimum for their use case."}, {"title": "Low Rank Factorization", "content": "Large language models are a stack of transformer layers, in which the fully connected (FC) layers are the building blocks of the feed forward and self-attention modules (Vaswani et al., 2017). For simplicity in representations, we assume these FC layers have weight matrices of shape $W\\in \\mathbb{R}^{d_{in}\\times d_{out}}$, where $d_{in}$ and $d_{out}$ represent the number of input and output features, respectively. In this case, the number of parameters in this FC layer could be calculated as: $d_{in} \\times d_{out}$.\nThe rank of matrix W can be interpreted as the number of its linearly independent columns or rows. Singular Value Decomposition (SVD), is a common tool used for factorizing a matrix into its components. Assume that the weight matrix $W\\in \\mathbb{R}^{d_{in} d_{out}}$ is decomposed using SVD as follows\n$W = U\\Sigma V^T = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$ (1)\nwhere $U\\in \\mathbb{R}^{d_{in}\\times d_{in}}$ and $V\\in \\mathbb{R}^{d_{out}\\times d_{out}}$ are the orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{d_{in}\\times d_{out}}$ is a diagonal rectangular matrix containing the singular values $\\sigma_i > 0$ of W with rank r (r is the number of non-zero singular values). In (1), if we only use the first R terms of the summation, the resulted matrix W' would be an approximation of W with a lower rank R < r:\n$W' = \\sum_{i=1}^{R} \\sigma_i u_i v_i^T = U'\\Sigma' V'^T$ (2)\nwhere $U' \\in \\mathbb{R}^{d_{in}\\times R}$ and $V' \\in \\mathbb{R}^{d_{out}\\times R}$ are the new orthogonal matrices and $\\Sigma' \\in \\mathbb{R}^{R\\times R}$ is the new diagonal rectangular matrix. Based on (2), W' could be represented as an inner product of two matrices Wo and W1 i.e.\n$W' = W_0 W_1,$ (3)\nwhere:\n$W_0 = U' \\sqrt{\\Sigma'},$ (4)\n$W_1 = \\sqrt{\\Sigma'} V'^T$ (5)\nwhere $W_0 \\in \\mathbb{R}^{d_{in}\\times R}$ and $W_1 \\in \\mathbb{R}^{R\\times d_{out}}$, and $\\sqrt{}$ is a diagonal matrix of square root of singular values $\\sqrt{\\sigma_i}$. If we replace the FC layer W with the two consecutive FC layers W0 and W1, the number of parameters could shrink significantly depending on the rank R, where R controls the compression ratio. The compression ratio CR can be calculated as follows:\n$CR = \\frac{d_{in} \\times d_{out}}{R \\times (d_{in} + d_{out}) }$ (6)\nwhere $1 < R \\leq min(d_{in}, d_{out})$ (Hajimolahoseini et al., 2023).\nIf R is too large, the product of decomposed matrices Wo and W1 is closer to the original matrix W and the drop in accuracy is negligible with the price of small compression or even no compression.\nOn the other hand, if R is too small, the approximation in (2) is inaccurate and accuracy loss is significant with the advantage of high compression ratio. In the extreme case of R = 1, the matrix $\\Sigma'$ becomes an scalar and thus, U' and V' reduce to vectors."}, {"title": "Progressive Decomposition", "content": "In contrast with most of the literature that applies low rank factorization to all layer in a single-shot manner, we decompose the models in multiple progressive steps using a progressively decreasing rank in each step. Each compression step is followed by a fine-tuning stage to make sure the acuuracy is recovered before going to the next step.\nIn order to avoid doubling the number of layers after each compression step, we only decompose the second decomposed matrix W1 into W and W\u00b9 and multiply the matrices Wo and W. So according to (3), we have:\n$W \\overset{LRD}{\\longrightarrow} W_0, W_1$ (7)\nApply LRD to $W_1$\n$\\longrightarrow W_0, (W,\\ W^\\dagger)$ (8)\nMultiply $W_0, W\\longrightarrow (W_0W),\\ W^\\dagger$ (9)\nThis guarantees that the number of decomposed layers stays the same (two) after each decomposition step. This process is shown in Algorithm 1."}, {"title": "4 Experiments", "content": ""}, {"title": "Dataset", "content": "SlimPajama-627B (Soboleva et al., 2023) is a cleaned and deduplicated version of RedPajama (Computer, 2023) dataset which is open-source."}, {"title": "PLRD Training", "content": "LLaMa2 (Touvron et al., 2023) and Mistral-v0.1 (Jiang et al., 2023) are two open-source model on which the efficacy of PLRD method is demonstrated. In each PLRD step, depending on if the compression includes attention matrices, MLP matrices, or both, intermediate ranks, $R_{attn}$ and $R_{MLP}$, are chosen. To scale each base model to 3B parameters, 4 steps of compression for each model are manually designed. In each step, the model is continually pretrained with 250M tokens to recover the performance. These intermediate ranks were used to compress every matrix in attention or MLP module in every layer. The only exception is in Mistral-v0.1 where due to MQA technique in attention, the compression is only applied to Q and O matrices in attention modules and K and V matrices are untouched."}, {"title": "PLRD Intermediate Results", "content": "Intermediate results of the models after each step of the compression are reflected in tables 5 and 6. These results demonstrate that scaling models into different sizes with lightweight training is possible through PLRD method. The trade-off between size and model size shows that one can tailor open-source LLMs for a specific target computation budget and accuracy."}, {"title": "Evaluation on Benchmarks", "content": "For evaluation, Im-evaluation-harness (Gao et al., 2021) package is used. Performance of the models on 4 diverse downstream tasks, LogiQA, BoolQ, MMLU, and WinoGrande, are evaluated in zero-shot manner. According to table 2, evaluations show that PLRD-Mistral-v0.1-3.1B and PLRD-LLaMa2-3.3B achieve on par performance with other open-source models with comparable size while trained on only 1B tokens. The results of PLRD are similar for both Mistral-v0.1-3.1B and LLaMa2-3.3B which yields the conclusion that PLRD is a general method applicable to a variety of models."}, {"title": "Inference Speed", "content": "As PLRD changes the depth of the model by replacing a matrix with two low-rank matrices, one reasonable investigation is on the inference speed of the PLRD models compared to the conventional architectures of LLMs. To analyze the inference speed of the models, generation speed of PLRD-Mistral-v0.1-3.1B on CPU is compared with a similar size model with conventional architecture in Table 7. The analysis shows that PLRD model with similar size is less than 3% slower than the conventional model."}, {"title": "Experimental Setup", "content": "For our experiments, we used the Mistral-v0.1-7B and LLaMa2-7B publicly available on Hugging-Face as base models. In each step of compression, the model is continually pretrained with 250M tokens. 8 V100 gpus were used all experiments in the paper. As the purpose of this paper is explore a new compression technique and not get optimal results from this technique, no hyperparameter search is done. One set of parameters were used for all the experiments which are listed in Table 8."}, {"title": "Conclusion", "content": "The method of Progressive Low-rank Decomposition (PLRD), when used to scale open-source LLMs, demonstrates better or comparable results with models of similar size that are pre-trained from scratch. Compared to our other compression methods such as Sheared-LLaMa (Xia et al., 2023), PLRD achieves similar results without the need to use tailored training recipe or a large number of training tokens. The results demonstrate that PLRD is effective in scaling down the models. As there is no specific modification in the training and the dataset selection is random, the experimental results support the claim that this method is a general method for compressing LLMs. Also, other methods that improve the accuracy of the models in training such as knowledge distillation, dynamic batch loading (Xia et al., 2023), etc. are orthogonal to our method and can further improve the results."}, {"title": "Limitations and Future Work", "content": "Our approach depends on a pre-trained open-source model. Due to limited time and computation resources, only two models with 7B parameters were chosen for experiments. However, our approach leverages the redundancy in the model parameters and our intuition is that PLRD will show stronger results if applied to larger models, which would be the next step of this project."}, {"title": "Ethics Statement", "content": "It should be noted that the models trained in this paper are trained on open-source datasets without further safety measures to align the model generations with ethical rules. The models are evaluated on benchmarks that test the model's capacity to reason. Therefore, models' generations might include false information, bias and discrimination, etc. It is not advised to use these models for purposes other than research."}]}