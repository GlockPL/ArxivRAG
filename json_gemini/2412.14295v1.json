{"title": "Temporally Consistent Object-Centric Learning by Contrasting Slots", "authors": ["Anna Manasyan", "Maximilian Seitzer", "Filip Radovic", "Georg Martius", "Andrii Zadaianchuk"], "abstract": "Unsupervised object-centric learning from videos is a promising approach to extract structured representations from large, unlabeled collections of videos. To support downstream tasks like autonomous control, these representations must be both compositional and temporally consistent. Existing approaches based on recurrent processing often lack long-term stability across frames because their training objective does not enforce temporal consistency. In this work, we introduce a novel object-level temporal contrastive loss for video object-centric models that explicitly promotes temporal consistency. Our method significantly improves the temporal consistency of the learned object-centric representations, yielding more reliable video decompositions that facilitate challenging downstream tasks such as unsupervised object dynamics prediction. Furthermore, the inductive bias added by our loss strongly improves object discovery, leading to state-of-the-art results on both synthetic and real-world datasets, outperforming even weakly-supervised methods that leverage motion masks as additional cues. Visit slotcontrast for videos and further details.", "sections": [{"title": "1. Introduction", "content": "Object-centric learning (OCL) [4, 13, 30, 41] is a rapidly advancing area of visual representation learning that enables autonomous systems to represent, understand, and model high-dimensional data directly in terms of its constituent entities. Structured object-centric representations (often referred to as slots [30]) facilitate generalization and robustness [7, 8] of scene representations across diverse downstream tasks, from visual question answering [1, 32, 55] to control [9, 17, 58, 60]. Of particular interest are video-based object-centric methods [2, 10, 19, 27, 42, 54, 61] that learn to represent objects that evolve and interact over time. These representations make the methods a powerful tool for applications such as unsupervised online object tracking [33, 50] and structured world modeling [26, 49, 53]. Unsupervised object-centric learning on videos has seen significant progress in recent years [2, 54, 61], mainly due to the use of pre-trained representations from self-supervised foundational models [5, 36] coupled with diverse training datasets like YouTube-VIS [56, 57]. Nevertheless, these methods still face significant challenges, especially maintaining consistent object-centric representations across time and uniquely representing each object\u2014critical factors for successful multi-object tracking and modeling of dynamic scenes [27, 49, 53].\nTemporal consistency [14, 29, 59] in object-centric representations refers to maintaining the same representation placeholder, called slot, for an object throughout a video sequence, effectively serving as a stable object-specific identifier over time. Existing unsupervised object-centric methods [26, 34, 45] aiming to discover consistent representations have primarily been studied on toy datasets with limited complexity [21, 25, 43]. In contrast, real-world video sequences present numerous challenges, including object occlusions, reappearances, and complex multi-object interactions, which complicate maintaining consistent object representations.\nIn this paper, we introduce a novel method to address the challenge of maintaining consistent temporal representations in object-centric models, extending the line of research on slot-based unsupervised video models [10, 61]. Our approach (named SLOT CONTRAST) scales to real-world video data and produces consistent object-centric representations. Notably, it achieves these results without requiring any human annotations. In particular, we propose a novel self-supervised contrastive learning objective, which contrasts slot representations throughout the batch while ensuring temporal coherence across consecutive frames. In addition, we modify the slot's initialization strategy [30] to promote distinct, contrastive representations. This combination leads to improved temporal consistency of learned representations, which we show to be highly effective for challenging downstream tasks such as unsupervised object tracking and latent object dynamics learning.\nOverall, our contributions are as follows:\n\u2022 We propose the novel slot-slot contrastive loss that sets the state-of-the-art in temporal consistency when integrated into slot-based video processing methods.\n\u2022 We develop SLOT CONTRAST, a simple and effective OCL architecture using the slot-slot contrastive loss paired with learned initialization that scales to real-world data, such as YouTube videos.\n\u2022 We extensively study the usefulness of our learned object-centric representations for challenging downstream tasks, including unsupervised online tracking with complete occlusions and latent object dynamics modeling.\n\u2022 We show that SLOT CONTRAST does not only improve the temporal consistency of the representations, but also achieves state-of-the-art on the object discovery task, outperforming weakly-supervised models using motion cues."}, {"title": "2. Related Work", "content": "Unsupervised video object-centric learning There exists an extensive body of research [2, 10, 13, 19, 22, 27, 28, 39, 42, 46, 48, 49, 61] on discovering objects from video without any human annotations, primarily through tracking either object bounding boxes or masks. To achieve this, most of these works combine an auto-encoder framework with a simple reconstruction objective, adding inductive biases for object discovery through structured encoders [4, 30] and decoders [52]. In particular, many modern object-centric image models [20, 23, 41, 54] use a latent slot attention module [30] to extract object representations and corresponding object masks. For video data, most current methods [10, 27, 42, 46, 61, 63] connect slots across frames, with slots from the previous frame initializing those in the current frame. Notably, recent approaches [2, 38, 61] have successfully scaled object discovery to real-world unconstrained videos. To achieve this, SOLV [2] introduces temporal consistency via agglomerative clustering and prediction of middle-frame features, whereas VideoSAUR [61] learns object-centric representations by predicting temporal similarity of self-supervised features [5, 36]. While such methods can decompose short videos, they still struggle with long-term temporal consistency. In contrast, we show that learning representations that are both informative and contrastive can significantly enhance both object discovery and temporal consistency on longer videos.\nTemporal Consistency Achieving temporal consistency is essential for any computer vision task involving video data, whether it is tracking points, bounding boxes, segmentation masks, optical flow, or representations [31, 44, 47, 49, 51]. In object-centric learning for videos, a range of different approaches have been proposed. For example, Yu and Xu [59] apply an object-wise sequential VAE to achieve consistency; Zhao et al. [62] and Li et al. [29] use an explicit memory buffer to maintain historical slot information and a transformer as a predictor using the memory buffer to predict the future; Qian et al. [38] achieve temporal consistency by employing student-teacher distillation to establish semantic and instance correspondence over time; and Traub et al. [45] use a recurrent network with a constancy prior [16]."}, {"title": "3. Method", "content": "Our approach builds upon the existing input reconstruction-based video object-centric framework [27, 61] by introducing a consistency loss that contrasts the slots across consecutive frames and thereby adapting the model to discover consistent representations. See Fig. 3 for an overview of the SLOT CONTRAST architecture."}, {"title": "3.1. Semantic Recurrent Slot Attention Module", "content": "Our model is an encoder-decoder object-centric architecture based on Slot Attention module (SA) [12] with additional adaptations for sequential inputs similar to SAVi [27], while leveraging pre-trained semantic features as proposed by DINOSAUR [41]. The model consists of three main components: a pre-trained self-supervised dense feature encoder (e.g., DINOv2 [36]), a Recurrent Slot Attention module that groups the encoder features into slots and models temporal slot updates, and a decoder that maps slots from each frame to reconstructions of the dense self-supervised features used as inputs. Next, we describe those components in more detail while explaining how to adapt them to the task of consistent object-centric representation learning.\nGiven a video frame \\(x_t\\), \\(t \\in \\{1,2,...,T\\}\\) and a pre-trained, frozen self-supervised DINO model \\(f\\) we first extract \\(N\\) patch features \\(g_t\\),\n\n\\(g_t = f(x_t), g_t \\in \\mathbb{R}^{N\\times D}.\\)\n\nAs those frozen features are mostly semantic and are trained only on images, we further adapt them to the task of temporally consistent object discovery. Specifically, each feature vector \\(g_t\\) is passed through a MLP \\(g_{\\psi}\\),\n\n\\(h_t = g_{\\psi}(g_t),\\)\n\nto adapt the frozen dense features for object-centric grouping. Based on the transformed encoder features \\(h_t\\) and a set of slot representations of the previous timestep \\(S_{t-1}\\), with \\(K\\) slots \\(s_{t-1}^i \\in S_{t-1}\\), we use a recurrent grouping module to extract slot representations. The Recurrent Slot Attention module comprises a grouping module \\(C_{\\theta}\\) and a predictor module \\(P_{\\omega}\\). The former updates slot representations using the standard Slot Attention module [30] on visual features \\(h_t\\) from the encoder, while the latter captures temporal and spatial interactions between slots:\n\n\\(S_t = C_{\\theta}(h_t, S_{t-1}), \\quad \\hat{S}_t = P_{\\omega}(S_t).\\)\n\nBoth slot-level representations, generated either by the grouping module \\(S_t\\) or the predictor \\(\\hat{S}_t\\), can be utilized for subsequent decoding or downstream task processing. In our implementation, the slot-level representations from the grouping module \\(S_t\\) are employed for the decoding stage. From now on, we will refer to \\(S_t\\) as \\(S_t\\).\nTemporal Slot Attention Initialization Importantly, we found that our setup benefits considerably from a learned initialization \\(S_0\\), which can influence the efficiency of training across various objectives. Originally, Locatello et al. [30] proposed a randomly sampled query initialization, where slots are sampled from the same Gaussian distribution with learned mean and variance. While such initialization allows different numbers of slots during inference, sampling from the same Gaussian distribution does not create a particularly favorable structure in slot-space. In this work, we use a straightforward learned initialization [18, 40] where a fixed set of initial slot vectors \\(S_0\\) is learned for the entire dataset. Such initialization allows for learning dissimilar initialization queries that consistently attend to different objects.\nFinally, for the reconstruction loss objective, we decode reconstructions \\(\\hat{g}_t\\) from all slots using the MLP decoder [41]."}, {"title": "3.2. Temporal Consistency through Slot Contrast", "content": "Contrastive learning is flexible in supporting diverse data sources and loss function designs. By carefully defining positive and negative examples, we can craft robust loss objectives that effectively guide self-supervised representation learning [6]. For instance, video contrastive methods like CVRL [37] leverage augmented video chunks to define positive (from the same video) and negative (from different videos) examples. We propose a novel application of a contrastive loss for temporal consistency in object-centric slot representations. In particular, we define positive samples as the representations of the same slot from two consecutive time steps within a video, while negative samples comprise all other slots across the batch between these time steps. An overview of the proposed loss is presented in Fig. 2\nIntra-Video Slot-Slot Contrastive Loss To force each slot to be consistent in time, we aim to learn slots that are similar in time while being maximally dissimilar to other slots. Given the sets of slot representations \\(S_t\\) and \\(S_{t+1}\\) at time steps \\(t\\) and \\(t + 1\\), we want elements \\(s_i^t \\in S_t\\) to be close to the next-frame slots \\(s_{i'}^{t+1}\\) corresponding to the same object, while having maximal distance to the next-frame slots \\(s_{t+1,k}^{i'}\\), \\(k \\neq i\\) corresponding to other objects in the video. The corresponding InfoNCE contrastive loss [35] is defined as \\(L_{intra} = \\sum_{i=1}^{K} l_i^{intra}\\) with\n\n\\(l_i^{intra} = -\\log \\frac{\\exp(\\text{sim}(s_i^t, s_{i+1}^i) / \\tau)}{\\sum_{k=1}^{K} 1_{[k \\neq i]} \\exp(\\text{sim}(s_i^t, s_{i+1}^k) / \\tau)}\\)\n\nwhere \\(K = |S_{t+1}|\\) is a number of slots per frame, \\(\\text{sim}(u, v) = \\frac{u^\\top v}{||u||_2||v||_2}\\) is the cosine similarity, \\(1[.]\\) is an indicator excluding the self-similarity of the slot \\(s_i^t\\) from the denominator, and \\(\\tau > 0\\) is a temperature parameter.\nWhile being a desirable property, intra-video slot contrast can be achieved simply by amplifying the differences between slots in the SA module's first frame initialization \\(S_0\\). To encourage a stronger focus on video content and instance specificity of the representations, we propose a further improvement over this loss by extending the negative contrast set.\nBatch Video Slot-Slot Contrastive Loss To leverage the benefits of larger contrast sets and prevent degenerate solutions relying solely on the initialization of slots, we exploit the fact that the whole batch of videos can be considered a large set of primarily unique object representations. Consequently, we enhance contrast within a video and between videos by including negative slots from the current and subsequent frames of all videos in the batch. Correspondingly, we define our slot-slot contrastive loss as\n\n\\(L_{SSC} = \\frac{1}{B\\cdot K} \\sum_{i=1}^{B} \\sum_{j=1}^{K} l_{i,j}^{ssc}\\) and\n\n\n\\(l_{i,j} = -\\log \\frac{\\exp(\\text{sim}(s_i^t, s_{i'}^{t+1}) / \\tau)}{\\sum_{b=1}^{B} \\sum_{k=1}^{K} 1_{[k,b \\neq i,j]} \\exp(\\text{sim}(s_i^t, s_{b,k}^{t+1}) / \\tau)}\\)\n\nwhere \\(B\\) is a number of videos in the batch. For more details on slot-slot contrastive loss implementation, see App. C.\nWe find that this approach significantly enhances the effectiveness of the slot-slot contrastive loss. Furthermore, since all videos in the batch are processed with the same initial state \\(S_0\\), this loss function avoids suboptimal solutions that rely solely on the uniqueness of the initialization, instead encouraging object discovery as the basis for contrast.\nFinal Loss To encourage scene decomposition we use a feature reconstruction loss, similar to DINOSAUR [41] and VideoSAUR [61]. Our final loss function combines the reconstruction loss with our proposed contrastive loss \\(L_{ssc}\\), weighted by the hyperparameter \\(\\alpha\\):\n\n\\(\\mathcal{L} = \\sum_{t=1}^{T-1} L_{rec}(h_t, \\hat{h}_t) + \\alpha L_{ssc}(S_t, S_{t+1}).\\)"}, {"title": "4. Experiments", "content": "We evaluate our method's temporal consistency on two downstream tasks: object discovery and latent object dynamics prediction. Our experiments address three main questions: (1) How does our model compare to state-of-the-art methods in both temporal consistency and scene decomposition? (2) How effective are our model's learned representations for the challenging downstream task of object dynamics prediction and for object tracking under full occlusions? (3) How important are the different components of our model and loss function for temporal consistency?\nDatasets To evaluate our method in the controlled setting, we use MOVi-C and MOVi-E synthetic datasets generated by Kubric [15]. MOVi-C includes richly textured everyday objects, featuring up to 11 objects per scene, while MOVi-E expands this to 23 objects and introduces basic linear camera motion. In addition, to study the scalability of our method to real-world data, we evaluate our method on the real-world YouTube-VIS 2021 (YTVIS21) video dataset [57]. YTVIS21 is an unconstrained, real-world dataset sourced from YouTube, capturing a diverse range of scenes (for more details, see App. D).\nMetrics Similar to other object-centric video methods [10, 27, 42, 61], to evaluate consistent object discovery, we use the video foreground adjusted rand index (FG-ARI) [13], measuring how well objects are split. In addition, we evaluate the sharpness of masks using the video intersection over union with mean best overlap matching (mBO) metric [41, 61]. Both metrics are computed over the full video and thus reflect how consistent object discovery is. In addition, to investigate the effects of the temporal consistency inductive bias on the per-frame object discovery itself, we use per-frame FG-ARI (image FG-ARI), which we independently compute for each frame and average afterwards. More details can be found in App. E.\nFinally, when evaluating how well object-centric representations perform for object dynamics prediction (see Sec. 4.2), we employ the same evaluation metrics as in the object discovery task: FG-ARI and mBO. This time, however, these metrics are computed by comparing the predicted masks (obtained by decoding the predicted slots [53]) with the ground-truth future masks."}, {"title": "4.1. Object Discovery", "content": "Implementation Details We employ the DINOv2 model as our feature encoder, using ViT-S/14 for the MOVi-C dataset and ViT-B/14 for MOVi-E and YTVIS21. The slot dimension is set to 128 for MOVi-E and 64 for both MOVi-C and YTVIS21. For the MOVi datasets, we use a resolution of (336,336), generating 24 \u00d7 24 patches yielding 576 ViT tokens, while for YTVIS21, a resolution of (518,518) yields 1369 tokens. As our loss does not require full videos during training, we train our model using a sequence length of 4 and evaluate on the full sequence length. Full implementation details are provided in App. A.\nBaselines We compare SLOT CONTRAST against the previously proposed SAVi [27] and STEVE [42] that employ an image reconstruction objective and with the state-of-the-art method VideoSAUR [61] that uses self-supervised feature reconstructions. Additionally, for a fair comparison, we trained a modification of VideoSAUR with DINOv2 features (referred to as VideoSAURv2) using the same video resolution as our method. Interestingly, the default VideoSAURv2 configuration was not performing well on the simplest MOVi-C dataset (while other dataset were working better than VideoSAUR without additional changes), thus we additionally explored possible parameters options in App. F.\nTemporally Consistent Object Discovery (Table 1 & Figure 4) SLOT CONTRAST significantly outperforms both SAVi and STEVE by a wide margin. When compared to VideoSAUR using its default parameters, our approach demonstrates higher consistency in terms of video FG-ARI scores. Compared to VideoSAUR and VideoSAURv2, SLOT CONTRAST achieves superior video scene decomposition (measured by FG-ARI scores). However, on synthetic datasets SLOT CONTRAST's masks are less sharp (as reflected by mBO). Notably, on the most challenging real-world YouTube-VIS data, our method surpasses both versions of VideoSAUR, achieving better performance on FG-ARI (+6.8) and mBO (+4). This shows that, given a large enough resolution and natural data inputs well aligned with DINOv2, SLOT CONTRAST can decompose unconstrained videos into consistent object representations. More examples are illustrated in App. L.\nPer-Frame Scene Decomposition (Table 2) Previous research has shown the effectiveness of specific inductive"}, {"title": "4.2. Object Dynamics Prediction", "content": "Setup To evaluate performance on the task of predicting object dynamics, we train a dynamics module using the object-centric representations inferred by a pretrained"}, {"title": "4.3. Analysis", "content": "In this section, we investigate key components of our approach, including the impact of the contrastive loss and the type of slot initialization. In addition, we study how effective SLOT CONTRAST is in automatically shutting down slots in correspondence to the scene's complexity.\nAblation of Loss Components (Table 4 and Figure 5) To demonstrate the value of the proposed slot-slot contrastive loss, we carry out an ablation study, comparing it with the feature reconstruction loss [41] and the intra-video contrastive loss, which contrasts slot representations in a single video. Using the intra-video contrastive loss yields improvements over the feature reconstruction baseline (+5.1 FG-ARI and +1.5 mBO on MOVi-C). However, we observe that in more challenging situations, the intra-video contrastive loss leads to failure cases such as shutting down too many slots (see Fig. 5). Next, we observe that by extending the contrast to the full batch of videos, SLOT CONTRAST learns more consistent representations (+19.6 FG-ARI and +5.3 mBO). This change increases the difficulty of the learning task, which prevents the model from relying on superficial patterns like slot initializations or object positions.\nChoice of the First Frame Initialization (Table 5) In video object-centric learning, slots are typically initialized based on those from the previous time step [27], while the first frame is initialized from learnable parameters. While previous real-world object-centric methods mostly used random initialization samples from Gaussian distribution [2, 61], in this work, we study the impact of the type of initialization under our contrastive objective. The findings are outlined in Table 5. Our experiments indicate that when combined with"}, {"title": "5. Conclusion", "content": "SLOT CONTRAST advances unsupervised video object-centric learning by significantly improving the temporal consistency of object representations. Our method explicitly incentivizes temporal consistency by adding a self-supervised contrastive loss. We showed that this loss is not only beneficial for consistency, but also enhances object discovery: SLOT CONTRAST achieves state-of-the-art results on challenging synthetic datasets with many objects and the unconstrained real-world YouTube-VIS dataset. Furthermore, consistent representations directly support temporal downstream tasks such as unsupervised object dynamics prediction and allow for tracking of objects through full occlusions. Finally, SLOT CONTRAST effectively shuts down non-unique slots, leading to a sparser representation that captures the true object distribution more faithfully. Taken together, we expect these improvements to pave the way for broader adoption of video object-centric representations, for instance in applications like word modeling, autonomous control, or video question answering.\nLimitations of our work include the fixed number of slots during initialization. Additionally, we cannot directly control the segmentation granularity of entities. For further limitations and failure cases are discussed in App. K.\nFuture work could explore several promising directions. First, one could use SLOT CONTRAST's robust and consistent representations for learning compositional world models from real-world robotics data to enable object-centric planning and control. Second, investigating the compatibility of our contrastive loss with other object-centric learning"}]}