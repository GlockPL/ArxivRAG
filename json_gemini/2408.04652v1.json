{"title": "LEVERAGING LARGE LANGUAGE MODELS WITH CHAIN-OF-THOUGHT AND PROMPT ENGINEERING FOR TRAFFIC CRASH SEVERITY ANALYSIS AND INFERENCE", "authors": ["Hao Zhen", "Yucheng Shi", "Yongcan Huang", "Jidong J. Yang", "Ninghao Liu"], "abstract": "Harnessing the power of Large Language Models (LLMs), this study explores the use of three state- of-the-art LLMs, specifically GPT-3.5-turbo, LLaMA3-8B, and LLaMA3-70B, for crash severity inference, framing it as a classification task. We generate textual narratives from original traffic crash tabular data using a pre-built template infused with domain knowledge. Additionally, we incorporated Chain-of-Thought (CoT) reasoning to guide the LLMs in analyzing the crash causes and then inferring the severity. This study also examine the impact of prompt engineering specifically designed for crash severity inference. The LLMs were tasked with crash severity inference to: (1) evaluate the models' capabilities in crash severity analysis, (2) assess the effectiveness of CoT and domain-informed prompt engineering, and (3) examine the reasoning abilities with the CoT framework. Our results showed that LLaMA3-70B consistently outperformed the other models, particularly in zero-shot settings. The CoT and Prompt Engineering techniques significantly enhanced performance, improving logical reasoning and addressing alignment issues. Notably, the CoT offers valuable insights into LLMs' reasoning processes, unleashing their capacity to consider diverse factors such as environmental conditions, driver behavior, and vehicle characteristics in severity analysis and inference.", "sections": [{"title": "1 Introduction", "content": "Traffic safety research plays a crucial role in enhancing road safety by examining the root causes of accidents, identifying hazardous behaviors or factors, and proposing effective countermeasures (Mannering et al. 2020). Despite advancements in vehicle safety, enhancements in road design, and the implementation of various policies, traffic safety remains a significant challenge. One important aspect of road safety research is understanding contributing factors leading to different crash severity outcomes, which is essential for mitigating crash consequences.\n\nThe challenge of traffic accident modeling stems from their multifaceted nature, involving intricate interplay among diverse factors, such as human behavior, vehicle dynamics, traffic conditions, environmental factors, and roadway characteristics. Traffic safety research has been primarily focused on understanding causality using observational data, due to the impracticality of conducting controlled experiments in this field (Mannering et al. 2020). Traditional statistical and econometric methods have long been studied in traffic safety domain (Golob and Recker 2003, Eluru and Bhat 2007, Lord and Mannering 2010, Savolainen et al. 2011, Karlaftis and Golias 2002) for causality understanding. These classic methods suffer from several limitations, including constraints imposed by specific functional forms and distributional assumptions (Mannering et al. 2020), as well as subtle confounding effects, also referred as heterogeneous treatment effects (Mannering et al. 2020, Zhang et al. 2022, Pervez et al. 2022), which often lead to an incomplete or misleading understanding of influencing factors.\nAnother limitation of statistical and econometric methods lies in the fact that they were designed around and can only consume structured data with traditional numeric or categorical coding and a limited number of features. These methods can not effectively handle unstructured textual data or passages of narratives. Due to recent advancements in AI and the abundance of narrative data captured in crash reports, natural language processing (NLP) methods have been applied in text mining of crash narratives (Goh and Ubeynarayana 2017, Zhang et al. 2020, Das et al. 2021). In previous works, researchers are required to collect a large amount of high-quality, labeled crash reports for model training. However, this process is time-consuming and costly. Additionally, low-quality training data and poorly chosen training parameters can lead to undesirable performance. In contrast, large language models (LLMs) offer a distinct advantage by leveraging their immense knowledge acquired from extensive pre-training with vast datasets, effectively addressing these challenges. Motivated by their superior capability to comprehend and generate human-like text, this study aims to investigate whether LLMs can process complex and unstructured data in traffic safety domain to enable elaborate case-specific analysis.\nDespite the release of numerous LLMs, represented by the GPT family (OpenAI 2023) and LLaMA family (AI@Meta 2024), their ability for traffic crash analysis and reasoning remains unexplored. Applying LLMs to crash analysis presents two major challenges: (1) it requires LLMs to fully understand the domain knowledge and potential causality behind crash events. However, LLMs, typically built on transformer architectures, are often regarded as \u201cblack-box\" models, making it difficult to interpret their decision-making processes. (2) while LLMs possess extensive real-world knowledge acquired from the pre-training stage, they are not specialized in analyzing textual data in crash reports. This creates an alignment gap between the model's original intent and the specific requirements of this specific task.\nTo address the first challenge, we propose to leverage the Chain-of-Thought (CoT) technique to enhance the reasoning capabilities of LLMs (Wei et al. 2022). This technique guides the model through a structured reasoning process, helping it better understand the detailed knowledge and causality behind crash data. Additionally, the CoT approach provides explainable reasoning steps for each intermediate result, making the model's decisions more transparent and easier to interpret. By incorporating CoT, we aim to improve the LLMs' performance in crash severity modeling, leveraging their capability to effectively process the complex and diverse data relevant to crash analysis.\nTo address the second challenge, we propose to use prompt engineering (PE) and few-shot learning (FS) to better align the LLMs with the specific requirements of the target task: crash severity modeling and analysis. PE can tailor the input prompts to guide the LLMs toward more relevant and reliable analysis, while few-shot learning can provide the models with specific examples to improve their understanding and performance in the subject domain. By combining these techniques, we aim to bridge the alignment gap and enhance the models' ability to effectively analyze textual descriptions in crash reports.\nTo demonstrate the efficacy of our approach, we explore three state-of-the-art LLMs, specifically GPT-3.5-turbo, LLaMA3-8B, and LLaMA3-70B, for crash severity inference, framing it as a multi-class classification task. In our experiments, we utilized textual narratives derived from crash tabular data as input for crash severity analysis with LLMs. Additionally, we incorporated CoT to guide the LLMs in analyzing potential crash causes and subsequently inferring severity outcome. We also examine prompt engineering specifically designed for crash severity inference. We task LLMs with crash severity inference to (1) assess the ability of LLMs for crash severity analysis; (2) evaluate the effectiveness of CoT and domain-informed PE; and (3) examine the reasoning ability within the CoT framework.\nThe experimental setup involves several strategies, including plain zero-shot and few-shot settings, zero-shot with Chain-of-Thought (ZS_CoT), zero-shot with prompt engineering (ZS_PE), zero-shot with both prompt engineering and Chain-of-Thought (ZS_PE_CoT), and few-shot with prompt engineering (FS_PE). The LLMs evaluated include GPT-3.5-turbo, LLaMA3-8B, and LLaMA3-70B, with specific hyperparameters to ensure consistent and reliable results. We compare the performance of these models and settings to determine the most effective approach for the crash severity inference task."}, {"title": "2 Methods", "content": null}, {"title": "2.1 Model Descriptions", "content": "In our approach, we leverage the reasoning ability of LLMs for domain-specific (i.e., traffic safety) analysis and modeling. Specifically, we utilize two state-of-the-art LLMs as our base models, including GPT-3.5-turbo (OpenAI 2023) and LLaMA-3 (AI@Meta 2024). Both models are auto-regressive language models, which generate text by predicting the next word or subword in a sequence based on the previous words or subwords, one step at a time. This approach allows the model to create coherent and contextually relevant text. In this auto-regressive setting, the joint probability is expressed as the product of sequential conditional probabilities in Eq. 1:\n$$P(x_1, x_2,...,x_n) = \\prod_{i=1}^{n} P(x_i | x_1, x_2, ..., x_{i-1}),$$\nwhere $P(x_i | x_1, x_2, ..., x_{i-1})$ represents the conditional probability of the i-th word given the preceding words. This auto-regressive modeling framework, designed to handle large context windows and trained on extensive datasets, empowers the model to generate fluent and context-aware sequences. Some details about these two models are provided below for direct reference.\nGPT-3.5-turbo (OpenAI 2023), developed by OpenAI, is designed for a variety of applications, including advanced text generation, coding assistance, and more. Trained on a vast corpus of internet text, including diverse sources such as books and websites, it has billions of parameters, allowing for nuanced understanding and generation of text. In our experiments, we use the gpt-3.5-turbo-0125 version.\nLLaMA-3 (AI@Meta 2024), developed by Meta, is another state-of-the-art LLM designed for efficient and scalable language understanding. It is pre-trained on over 15T tokens that cover diverse internet text sources. LLaMA-3 is available in sizes of 8 billion and 70 billion parameters, making it flexible for various applications.\nBoth models employ techniques like supervised fine-tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF) to align their outputs with human preferences, ensuring the model is helpful and safe (Wu et al. 2023). Specifically, SFT fine-tunes a pre-trained model on specific datasets with human instructions, ensuring it understands relevant vocabulary, patterns, and knowledge, thereby improving accuracy and relevance. RLHF, on the other hand, refines the model using feedback from human experts, allowing it to adapt to complex real-world scenarios and prioritize critical safety aspects, enhancing both adaptability and reliability."}, {"title": "2.2 In-context learning", "content": "In-context learning is a promising approach for traffic safety analysis and modeling, where a LLM learns to perform a specified task by observing examples of the task within the input context, without any explicit fine-tuning or gradient updates (Radford et al. 2019). The LLM leverages its pre-existing knowledge and the provided examples to generate appropriate inference for new, unseen instances of the task. In-context learning encompasses both zero-shot and few-shot learning (Brown et al. 2020), with the number of examples ranging from zero to a few. In the context of traffic safety analysis and modeling, in-context learning can be applied to various tasks, such as crash severity inference.\nZero-shot learning in traffic safety analysis refers to the setting where the model is expected to perform a task without any examples or explicit training for that specific task. The model relies solely on its pre-existing knowledge to make inference. While few-shot learning involves providing the model with a few examples of the target task before asking it to perform the same task on new instances. The model learns from these few examples and adapts its behavior accordingly. For instance, in crash severity inference, the model may be provided with a few examples of crashes and their corresponding severity outcomes. The model then learns from these examples and reasons the severity outcome of a new, unseen crash. Few-shot learning allows the models to quickly adapt to new tasks or scenarios with minimal training data for diverse real-world applications (Yin 2020)."}, {"title": "2.3 Prompt engineering (PE)", "content": "Prompt engineering is a technique for crafting and refining input prompts for LLMs to enhance their performance on specific tasks. It is akin to \"asking the right question (prompt) to get the desired answer (response).\" For traffic safety analysis, carefully designed prompts can significantly enhance LLMs' ability to analyze complex scenarios and provide meaningful insights. Specifically, in the context of crash severity inference, one critical category is 'Fatal accidents'. However, LLMs often exhibit a tendency to avoid assigning this label to relevant cases. This behavior stems from their alignment training, which generally encourages them to avoid discussing unpleasant subject or potentially unsafe topics (Wang et al. 2023, Shen et al. 2023). Such design constraints present a challenge in accurately inferring severe outcomes for traffic incidents. To address this issue, we found it necessary to rephrase the original 'Fatal accidents' label using alternative terms. This \"soft\" approach allows us to maintain inference accuracy while respecting LLMs' aligned parameters. The specific modifications and their impacts on inference performance will be discussed in detail in the Experiments section."}, {"title": "2.4 Chain-of-Thoughts (CoT)", "content": "A notable distinction between LLMs and conventional machine learning algorithms is the capacity of foundation LLMs to process variably structured input data, specifically prompts, during the inference phase (Wei et al. 2022). For inference, LLMs generally precede or prioritize the information provided in the input prompts over the large implicit knowledge gained from the pre-training stage. Consequently, this leads to clear, comprehensive content. This content could encompass domain-specific knowledge, contextual background, or detailed step-by-step reasoning guidance. By doing so, LLMs can be encouraged to disclose their reasoning processes during inference, thereby enhancing the explicability of their outputs. In this paper, we utilize the CoT technique to enable LLMs to generate step-by-step reasoning before providing the final answer, improving their performance on complex reasoning tasks.\nAlthough it is widely recognized that LLMs excel at generating responses reminiscent of human conversation, they often have opacity issues in their reasoning processes. This opacity hinders users' ability to evaluate the trustworthiness of the responses, particularly in scenarios that necessitate elaborate reasoning.\nFormally, let $f_0$ be a language model and $X = \\{(X_1,Y_1), (X_2,Y_2), ..., (x_n)\\}$ be an input prompt, where $(x_i, Y_i)$ denotes the i-th example question-response pair. In a standard question-answering scenario, the model output is given by: $Y_n = arg max_y p_0(Y|X_1, Y_1,X_2,Y_2, ..., X_n)$, where $p_0$ is the predicted probability of the language model $f_0$. This setting, however, does not provide insights into the reasoning process behind the answer $y_n$. The CoT method extends this by including human-crafted explanations $e_i$ for each example, resulting in a modified input format $X = (x_1, e_1,Y_1), (x_2, e_2, Y_2), ..., (x_n)$. The model then outputs both the explanation $e_n$ and the answer $y_n$:\n$$(e_n, Y_n) = arg max_Y p_0 (Y|x_1, e_1, Y_1, x_2, e_2, Y_2, ..., X_n).$$\nHowever, in practice, it is difficult to obtain sufficient high-quality explanation examples for crash severity classification, while low-quality explanations can harm the CoT performance. Therefore, following strategies in (Kojima et al. 2022), we simply ask LLMs to think step by step to conduct traffic safety analysis and inference. Some template examples are presented in the Experiments section. Besides allowing for a more transparent and understandable interaction with LLMs, the CoT approach is also practically useful in several key aspects: (1) Reducing Errors in Crash Risk Assessment: By breaking down complex traffic scenarios, CoT can better understand and reason over specific cases, thus reducing errors in crash risk analysis (Wei et al. 2022, Qin et al. 2023, Zhang et al. 2023). (2) Providing Adjustable Intermediate Steps for Crash Analysis: CoT enables the outlining of traceable intermediate steps within the crash analysis process.\nThis can be helpful in identifying potential biases or errors in the model's reasoning and allow for more reliable crash risk assessment (Lyu et al. 2023). By leveraging the CoT approach in traffic safety analysis, we can enhance the interpretability and reliability of crash risk assessment."}, {"title": "3 Data", "content": "In this section, we first discuss the dataset employed for the study. We then explain how we convert the crash tabular data to coherent descriptive narratives. Finally, we discuss our experimental settings and evaluation methods."}, {"title": "3.1 Dataset", "content": "Our empirical analysis utilizes data sourced from CrashStats data from Victoria, Australia spanning from 2006 to 2020. The crash database contains records of vehicles involved in crashes. A four-point ordinal scale is used to code the severity level of each accident, including: (1) non-injury accident, (2) other injury (minor injury) accident, (3) serious injury accident, and (4) fatal accident. Each sample denotes a vehicle involved in a crash with driver's information. After data prepossessing, the final dataset has an extremely low representation of non-injury accidents (only four instances), accounting for less than 0.001%. Consequently, these four non-injury accidents are merged to the category of \"Minor or non-injury accidents\". As a result, the dataset contains 197,425 minor or non-injury accidents, 89,925 serious injury accidents, and 4,760 fatal accidents.\nThe traffic accident attributes considered in our empirical study include crash characteristics, driver traits, vehicle details, roadway attributes, environmental conditions, and situational factors (see Table 1)."}, {"title": "3.2 Textual narrative generation", "content": "To get coherent, informative passages enriched with domain-specific knowledge, we use a simple yet effective template to convert the raw structured tabular data into detailed, human-readable textual narratives, encapsulating vital information about traffic accidents, which can be better consumed by LLMs. This process is depicted in Figure 1.\nThe primary objective is to augment the applicability and relevance of tabular data as input for LLMs, facilitating more context-aware inference for tasks like accident severity assessment. Furthermore, road safety engineers can supplement it with established facts or domain-specific knowledge for further enhancement. For instance, \"Children and elders are typically more vulnerable in accidents without seat belts.\" However, it is important to note that, for this study, we do not delve into the intricate design of scientific knowledge in traffic safety."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experiments design", "content": "In this work, we tackle the crash severity inference problem as a classification task. The inputs encompass various crash related attributes, including environmental conditions, driver characteristics, crash details, and vehicle features. The original data is in tabular format including categorical and numerical fields. We then transform the tabular data into consistent textual narrative with a simple template, detailed in the preceding subsection. The objective is to estimate severity outcomes of crashes with the state-of-the-art LLM models, such as GPT-3.5-turbo, LLaMA3-8B, and LLaMA3-70B.\nLlaMA3s models are open-source foundation models, while the GPT model is a close-source and non-free model. Considering the cost and our goal of evaluating crash severity inference performance of foundation models, we randomly draw 50 samples from each of the three severity outcome categories, resulting in a total of 150 samples. These samples are used to demonstrate the potential of LLMs in enhancing crash analysis and reasoning."}, {"title": "4.2 Prompts for LLMs", "content": "In this section, we explained in detail how we design the prompts for different experiments in Table 2."}, {"title": "4.2.1 Zero-shot", "content": "The prompt designed for plain zero-shot setting is demonstrated in Figure 2. The provided prompt tasks each LLM as a professional road safety engineer with classifying the severity of a traffic crash in Victoria, Australia, based on a detailed description of a crash. The engineer is required to categorize the crash into one of three specified categories: 'Fatal accident', 'Serious injury accident', or 'Minor or non-injury accident'. The engineer's response is restricted to outputting only the classification result, ensuring a focused and objective assessment. This prompt is designed to elicit a precise evaluation of the crash severity outcome, leveraging the knowledge of engineer's expertise in road safety and crash analysis."}, {"title": "4.3 Evaluation metrics", "content": "Following standard practice in the context of multi-class classification, we adopt two commonly used classification metrics: Macro-Accuracy, and F1-score. Additionally, we include class-specific accuracies. These metrics are briefly discussed below.\n(1) Accuracy\nAccuracy measures the proportion of correctly classified instances in the test dataset. It is calculated as:\n$$Accuracy = \\frac{Correct\\;Predictions}{Total\\;Predictions}$$\nwhere:\n\u2022 Correct Predictions: The number of correctly classified instances in the test dataset.\n\u2022 Total Predictions: The total number of instances in the test dataset.\nIt should be noted that we first calculate the accuracy for each class and then calculate the macro-accuracy as the average of these class accuracies.\n(2) F1-score\nThe F1-score is defined as the harmonic mean of precision and recall, computed as:\n$$F1-score = \\frac{2\\times Precision\\times Recall}{Precision + Recall}$$\nThe F1-score reported in the following section (Section 5) is at the macro level, which is an averaged F1 of all classes.\nPrecision quantifies the accuracy of positive predictions for a specific class, computed as:\n$$Precision = \\frac{True\\;Positives}{True\\;Positives + False\\;Positives}$$"}, {"title": "5 Findings", "content": null}, {"title": "5.1 Exemplar responses of LLMs to crash severity inference queries", "content": "The exemplar responses of GPT-3.5, LLaMA3-8B, and LLaMA3-70B in each of the six settings, including ZS, FS,\nZS_CoT, FS_PE, ZS_PE, and ZS_PE_CoT (outlined in Table 2), are shown in Figure 7. It demonstrates that the LLMs can effectively respond to the severity inference task, delivering expected results. Note that the examples in Figure 7 only showcase correct severity inferences.\nGiven the prompt for each setting (see Section 4.2), each model can directly answer or ultimately summarize its estimated severity for the given accident as one of the defined categories.\nIn the plain Zero-shot and Few-shot settings, the models respond directly with one of the three class labels, i.e., 'Minor or non-injury accident', 'Serious injury accident', or 'Fatal accident'. Similarly, in the ZS_PE and FS_PE settings, the models respond directly as 'Minor or non-injury accident', 'Serious injury accident', or 'Serious accident with potentially fatal outcomes'.\nIn contrast, in the CoT settings (ZS_CoT and ZS_PE_CoT), the models return longer responses by reasoning first and then making inference of the severity outcome of the accident. Generally, the GPT-3.5 model's responses are more concise."}, {"title": "5.2 Severity inference performance of the LLMs with different strategies", "content": "The performance metrics of GPT-3.5, LLaMA3-8B, and LLaMA3-70B for the crash severity inference task under the six settings (see Table 2) are presented in Table 3.\nThe results reveal varied performance across models and settings in inferring crash severity outcomes. LLaMA3-70B consistently exhibited superior performance, particularly with zero-shot prompt engineering (ZS_PE), achieving the highest macro F1-score (0.4755) and macro-accuracy (49.33%). Furthermore, LLaMA3-70B attained the second-best performance in macro F1-score (0.4747) and macro-accuracy (47.33%) under the zero-shot with Chain-of-Thought (ZS_CoT) setting. These findings suggest that both prompt engineering and Chain-of-Thought methodologies contribute positively to model performance. Nevertheless, no single technique demonstrated consistent superiority across all severity categories. For fatal accidents, GPT-3.5 with zero-shot prompt engineering and Chain-of-Thought (ZS_PE_CoT) exhibited the highest accuracy (68%). In contrast, for serious injury accidents, GPT-3.5 and LLaMA3-8B in the plain zero-shot setting (ZS), as well as GPT-3.5 in the zero-shot with Chain-of-Thought scenario (ZS_CoT), achieved 100% accuracy. However, it is crucial to note that in these settings, these models performed poorly for fatal and 'minor or non-injury' accidents, indicating an inherent bias toward the intermediate severity category of 'serious injury'.\nInterestingly, LLaMA3-70B with the basic zero-shot approach demonstrated the best inference performance for 'minor or non-injury' accidents (58% accuracy) while maintaining a relatively balanced performance across fatal and serious injury accidents. This suggests a robust generalization capability of LLaMA3-70B across crash severity categories.\nThe implementation of prompt engineering, particularly in zero-shot settings, generally enhanced performance across models. This improvement was especially pronounced for fatal accident classification, where rephrasing the \"Fatal accidents\" label to the soft version of \"Serious accident with potentially fatal outcomes\" facilitated maintenance of classification accuracy while adhering to the LLM's aligned behaviors."}, {"title": "5.3 Effectiveness of prompt engineering (PE) and Chain-of-Thought (CoT)", "content": "Figure 8 demonstrates the performance gains by CoT and PE as compared to the plain zero-shot setting. Both ZS_COT and ZS_PE consistently demonstrate enhanced performance in terms of Macro F1-score and Macro-accuracy across all three models evaluated. This improvement underscores the efficacy of CoT and PE in boosting model performance in zero-shot scenarios.\nNotably, the implementation of PE (ZS_PE) yields more substantial improvements relative to CoT (ZS_CoT). This differential in enhancement suggests that, within the context of this specific task, the reformulation of prompts may be particularly effective in guiding model outputs as compared to the structured reasoning approach with CoT. The consistent pattern of improvement across different model architectures and sizes indicates the broad applicability of these techniques in zero-shot learning paradigms.\nAs illustrated in Figure 8, CoT improves both Macro F1-score and Macro-Accuracy across all three models in the plain zero-shot (ZS) setting. Based on the results summarized in Table 3, GPT-3.5 and LLaMA3-8B show improved recognition of \"Minor or non-injury\" accidents. LLaMA3-70B demonstrates substantial gain in identifying \"Serious injury\" accidents and \"Minor or non-injury\" accidents, with only a slight reduction in performance for \"Fatal\" accidents. The use of CoT enables LLMs to better understand and reason through questions, leading to more reliable and explainable inferences.\nThe PE technique also leads to increased Macro F1-score and Macro-Accuracy across all three models compared to the plain zero-shot (ZS) setting, as depicted in Figure 8. Notably, it greatly enhances the models' ability to detect Fatal accidents by simply softening the label description from \"Fatal accident\" to \"Serious accident with potentially fatal outcomes\", resulting in more balanced performance across severity categories. Compared to the zero-shot baseline, GPT-3.5 with PE attains a remarkable increase in Fatal accident detection, with accuracy rising from 0% to 62%. Similarly, LLaMA3-8B and LLaMA3-70B show increases in fatal accident accuracy from 0% to 34%, 44% to 60%, respectively."}, {"title": "5.4 Zero-shot vs. Few-shot learning", "content": "In the FS setting, the inclusion of three examples improves both the macro F1-score and macro-accuracy compared to the ZS setting, boosting classification accuracy of GPT-3.5 and LLaMA3-8B for \u201cFatal accident\" and \"Minor or non-injury accident\". However, this comes at the expense of accuracy for \"Serious injury accident\". This indicates a potential trade-off in classification performance across severity categories or a decrease of the model bias in the ZS setting.\nMoreover, smaller models like LLaMA3-8B generally benefit more from few-shot learning than larger models, such as GPT-3.5 and LLaMA3-70B, as evidenced by a notable increase in Macro F1 score from 0.1818 to 0.4068. Nevertheless, LLaMA3-70B, being a larger model, performs slightly better in the zero-shot settings, suggesting it may have gained some general knowledge in traffic safety domain during the pre-training stage, where the zero-shot prompting can draw upon such knowledge.\nIn contrast, the effects of PE in the FS setting exhibit more variations across models. GPT-3.5 demonstrates im- provements in Macro F1-score and Fatal accident accuracy, and LLaMA3-70B shows remarkably improved inference accuracy for \"fatal accident\" and \"Serious injury accident\". Conversely, LLaMA3-8B shows decreased macro F1-score and macro-accuracy, indicating that PEin the few-shot setting may not be equally beneficial for models of different sizes.\nIt important to note that we did not explore the aspect regarding the choice of the examples in the FS setting, which might have a varying effect on different models."}, {"title": "6 Discussions", "content": null}, {"title": "6.1 Can LLMs with CoT yield logical reasoning for their inference outcomes?", "content": "CoT is a unique technique to augment LLM's reasoning capability. But how reasonable is the reasoning by LLMs with CoT? We aim to examine the responses of LLMs with CoT from the view of a traffic safety engineer. Specifically, the responses of LLaMA3-70B in the ZS_CoT setting are evaluated due to its best performance in this setting (see Table 3).\nAs a qualitative assessment, three word clouds are drawn separately with respect to the three severity categories, i.e., \"Minor/non-injury\", \"Serious injury\", and \"Fatal\" accidents (see Figures 10, 11,and 12. Note that only the correct inferred responses are used in creating these word clouds, where the bigger sizes of words indicate their higher frequencies in the LLMs' responses. These visualizations offer insights into the LLM's conceptualization and reasoning processes regarding accident causation and factors considered during the severity inference.\nIn the three word clouds (Figure 10, 11,and 12), some words consistently appear regardless of severity outcomes, including \"collision,\" \"intersection,\" \"vehicle,\" and \"driver.\" This suggests a core set of concepts that the LLM associates with traffic accidents. Additionally, LLaMA3-70B demonstrates consideration of diverse factors in its accident analysis, including:\n\u2022 Crash-related factors (e.g., \"rear-end collision\", \"pedestrian\", \"opposite directions\", \"corner\")\n\u2022 Environmental conditions (e.g., \"wet road surface\", \"rain\", \"dark\", \"stop-go\")\n\u2022 Driver behavior (e.g., \"failing to yield,\" \"misjudgment\", \"turning\", \"give way\", \"excessive speed\")"}, {"title": "6.2 Limitations and future works", "content": "One of the primary limitations of this study is the relatively small sample used, including only 150 instances (50 for each severity category). This limited dataset may not fully capture the variability and complexity of diverse real-world crash scenarios, potentially affecting the generalizability of the findings."}, {"title": "7 Conclusions", "content": "In conclusion, this study demonstrates the efficacy of LLMs in crash severity inference using textual narratives of crash events constructed from structured tabular data. Our comprehensive evaluation of modern LLMs (GPT-3.5-turbo, LLaMA3-8B, and LLaMA3-70B) across different settings (zero-shot, few-shot, CoT, and PE) yields insightful findings. LLaMA3-70B consistently outperformed other models, especially in zero-shot settings. CoT and PE techniques lead to enhanced performance, improving logical reasoning and addressing alignment issues.\nNotably, the use of CoT provided valuable insights into LLM reasoning processes, revealing their capacity to consider multiple factors such as environmental conditions, driver behavior, and vehicle characteristics in the crash severity inference task. These findings collectively suggest that LLMs hold considerable promise for crash analysis and modeling. Future research may explore other safety applications beyond the severity inference."}, {"title": "8 Author contributions", "content": "The authors confirm contribution to the paper as follows: study conception and design: J. Yang, H. Zhen, N, Liu; data processing and cleaning: H. Zhen, Y. Shi; experiments design, analysis and interpretation of results: H. Zhen, Y. Shi, J. Yang; draft manuscript preparation: H. Zhen, Y. Shi, Y. Huang; review and editing: J. Yang, N. Liu. All authors reviewed the results and approved the final version of this manuscript."}]}