{"title": "LEVERAGING LARGE LANGUAGE MODELS WITH\nCHAIN-OF-THOUGHT AND PROMPT ENGINEERING FOR TRAFFIC\nCRASH SEVERITY ANALYSIS AND INFERENCE", "authors": ["Hao Zhen", "Yucheng Shi", "Yongcan Huang", "Jidong J. Yang", "Ninghao Liu"], "abstract": "Harnessing the power of Large Language Models (LLMs), this study explores the use of three state-\nof-the-art LLMs, specifically GPT-3.5-turbo, LLaMA3-8B, and LLaMA3-70B, for crash severity\ninference, framing it as a classification task. We generate textual narratives from original traffic\ncrash tabular data using a pre-built template infused with domain knowledge. Additionally, we\nincorporated Chain-of-Thought (CoT) reasoning to guide the LLMs in analyzing the crash causes\nand then inferring the severity. This study also examine the impact of prompt engineering specifically\ndesigned for crash severity inference. The LLMs were tasked with crash severity inference to:\n(1) evaluate the models' capabilities in crash severity analysis, (2) assess the effectiveness of CoT\nand domain-informed prompt engineering, and (3) examine the reasoning abilities with the CoT\nframework. Our results showed that LLaMA3-70B consistently outperformed the other models,\nparticularly in zero-shot settings. The CoT and Prompt Engineering techniques significantly enhanced\nperformance, improving logical reasoning and addressing alignment issues. Notably, the CoT offers\nvaluable insights into LLMs' reasoning processes, unleashing their capacity to consider diverse\nfactors such as environmental conditions, driver behavior, and vehicle characteristics in severity\nanalysis and inference.", "sections": [{"title": "1 Introduction", "content": "Traffic safety research plays a crucial role in enhancing road safety by examining the root causes of accidents, identifying\nhazardous behaviors or factors, and proposing effective countermeasures (Mannering et al. 2020). Despite advancements\nin vehicle safety, enhancements in road design, and the implementation of various policies, traffic safety remains a\nsignificant challenge. One important aspect of road safety research is understanding contributing factors leading to\ndifferent crash severity outcomes, which is essential for mitigating crash consequences.\nThe challenge of traffic accident modeling stems from their multifaceted nature, involving intricate interplay among\ndiverse factors, such as human behavior, vehicle dynamics, traffic conditions, environmental factors, and roadway\ncharacteristics. Traffic safety research has been primarily focused on understanding causality using observational\ndata, due to the impracticality of conducting controlled experiments in this field (Mannering et al. 2020). Traditional\nstatistical and econometric methods have long been studied in traffic safety domain (Golob and Recker 2003, Eluru and\nBhat 2007, Lord and Mannering 2010, Savolainen et al. 2011, Karlaftis and Golias 2002) for causality understanding.\nThese classic methods suffer from several limitations, including constraints imposed by specific functional forms and\ndistributional assumptions (Mannering et al. 2020), as well as subtle confounding effects, also referred as heterogeneous\ntreatment effects (Mannering et al. 2020, Zhang et al. 2022, Pervez et al. 2022), which often lead to an incomplete or\nmisleading understanding of influencing factors.\nAnother limitation of statistical and econometric methods lies in the fact that they were designed around and can\nonly consume structured data with traditional numeric or categorical coding and a limited number of features. These\nmethods can not effectively handle unstructured textual data or passages of narratives. Due to recent advancements in\nAI and the abundance of narrative data captured in crash reports, natural language processing (NLP) methods have\nbeen applied in text mining of crash narratives (Goh and Ubeynarayana 2017, Zhang et al. 2020, Das et al. 2021). In\nprevious works, researchers are required to collect a large amount of high-quality, labeled crash reports for model\ntraining. However, this process is time-consuming and costly. Additionally, low-quality training data and poorly chosen\ntraining parameters can lead to undesirable performance. In contrast, large language models (LLMs) offer a distinct\nadvantage by leveraging their immense knowledge acquired from extensive pre-training with vast datasets, effectively\naddressing these challenges. Motivated by their superior capability to comprehend and generate human-like text, this\nstudy aims to investigate whether LLMs can process complex and unstructured data in traffic safety domain to enable\nelaborate case-specific analysis.\nDespite the release of numerous LLMs, represented by the GPT family (OpenAI 2023) and LLaMA family (AI@Meta\n2024), their ability for traffic crash analysis and reasoning remains unexplored. Applying LLMs to crash analysis\npresents two major challenges: (1) it requires LLMs to fully understand the domain knowledge and potential causality\nbehind crash events. However, LLMs, typically built on transformer architectures, are often regarded as \u201cblack-box\"\nmodels, making it difficult to interpret their decision-making processes. (2) while LLMs possess extensive real-world\nknowledge acquired from the pre-training stage, they are not specialized in analyzing textual data in crash reports. This\ncreates an alignment gap between the model's original intent and the specific requirements of this specific task.\nTo address the first challenge, we propose to leverage the Chain-of-Thought (CoT) technique to enhance the reasoning\ncapabilities of LLMs (Wei et al. 2022). This technique guides the model through a structured reasoning process, helping\nit better understand the detailed knowledge and causality behind crash data. Additionally, the CoT approach provides\nexplainable reasoning steps for each intermediate result, making the model's decisions more transparent and easier to\ninterpret. By incorporating CoT, we aim to improve the LLMs' performance in crash severity modeling, leveraging\ntheir capability to effectively process the complex and diverse data relevant to crash analysis.\nTo address the second challenge, we propose to use prompt engineering (PE) and few-shot learning (FS) to better align\nthe LLMs with the specific requirements of the target task: crash severity modeling and analysis. PE can tailor the\ninput prompts to guide the LLMs toward more relevant and reliable analysis, while few-shot learning can provide the\nmodels with specific examples to improve their understanding and performance in the subject domain. By combining\nthese techniques, we aim to bridge the alignment gap and enhance the models' ability to effectively analyze textual\ndescriptions in crash reports.\nTo demonstrate the efficacy of our approach, we explore three state-of-the-art LLMs, specifically GPT-3.5-turbo,\nLLaMA3-8B, and LLaMA3-70B, for crash severity inference, framing it as a multi-class classification task. In our\nexperiments, we utilized textual narratives derived from crash tabular data as input for crash severity analysis with\nLLMs. Additionally, we incorporated CoT to guide the LLMs in analyzing potential crash causes and subsequently\ninferring severity outcome. We also examine prompt engineering specifically designed for crash severity inference. We\ntask LLMs with crash severity inference to (1) assess the ability of LLMs for crash severity analysis; (2) evaluate the\neffectiveness of CoT and domain-informed PE; and (3) examine the reasoning ability within the CoT framework.\nThe experimental setup involves several strategies, including plain zero-shot and few-shot settings, zero-shot with\nChain-of-Thought (ZS_CoT), zero-shot with prompt engineering (ZS_PE), zero-shot with both prompt engineering\nand Chain-of-Thought (ZS_PE_CoT), and few-shot with prompt engineering (FS_PE). The LLMs evaluated include\nGPT-3.5-turbo, LLaMA3-8B, and LLaMA3-70B, with specific hyperparameters to ensure consistent and reliable results.\nWe compare the performance of these models and settings to determine the most effective approach for the crash\nseverity inference task.\""}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Model Descriptions", "content": "In our approach, we leverage the reasoning ability of LLMs for domain-specific (i.e., traffic safety) analysis and\nmodeling. Specifically, we utilize two state-of-the-art LLMs as our base models, including GPT-3.5-turbo (OpenAI\n2023) and LLaMA-3 (AI@Meta 2024). Both models are auto-regressive language models, which generate text by\npredicting the next word or subword in a sequence based on the previous words or subwords, one step at a time. This\napproach allows the model to create coherent and contextually relevant text. In this auto-regressive setting, the joint\nprobability is expressed as the product of sequential conditional probabilities in Eq. 1:\n$$P(x_1, x_2,...,x_n) = \\prod_{i=1}^{n} P(x_i | x_1, x_2, ..., x_{i-1}),$$"}, {"title": "2.2 In-context learning", "content": "In-context learning is a promising approach for traffic safety analysis and modeling, where a LLM learns to perform a\nspecified task by observing examples of the task within the input context, without any explicit fine-tuning or gradient\nupdates (Radford et al. 2019). The LLM leverages its pre-existing knowledge and the provided examples to generate\nappropriate inference for new, unseen instances of the task. In-context learning encompasses both zero-shot and\nfew-shot learning (Brown et al. 2020), with the number of examples ranging from zero to a few. In the context of traffic\nsafety analysis and modeling, in-context learning can be applied to various tasks, such as crash severity inference.\nZero-shot learning in traffic safety analysis refers to the setting where the model is expected to perform a task without\nany examples or explicit training for that specific task. The model relies solely on its pre-existing knowledge to make\ninference. While few-shot learning involves providing the model with a few examples of the target task before asking\nit to perform the same task on new instances. The model learns from these few examples and adapts its behavior\naccordingly. For instance, in crash severity inference, the model may be provided with a few examples of crashes and\ntheir corresponding severity outcomes. The model then learns from these examples and reasons the severity outcome of\na new, unseen crash. Few-shot learning allows the models to quickly adapt to new tasks or scenarios with minimal\ntraining data for diverse real-world applications (Yin 2020)."}, {"title": "2.3 Prompt engineering (PE)", "content": "Prompt engineering is a technique for crafting and refining input prompts for LLMs to enhance their performance on\nspecific tasks. It is akin to \"asking the right question (prompt) to get the desired answer (response).\" For traffic safety\nanalysis, carefully designed prompts can significantly enhance LLMs' ability to analyze complex scenarios and provide\nmeaningful insights. Specifically, in the context of crash severity inference, one critical category is 'Fatal accidents'.\nHowever, LLMs often exhibit a tendency to avoid assigning this label to relevant cases. This behavior stems from\ntheir alignment training, which generally encourages them to avoid discussing unpleasant subject or potentially unsafe\ntopics (Wang et al. 2023, Shen et al. 2023). Such design constraints present a challenge in accurately inferring severe\noutcomes for traffic incidents. To address this issue, we found it necessary to rephrase the original 'Fatal accidents'\nlabel using alternative terms. This \"soft\" approach allows us to maintain inference accuracy while respecting LLMs'\naligned parameters. The specific modifications and their impacts on inference performance will be discussed in detail in\nthe Experiments section."}, {"title": "2.4 Chain-of-Thoughts (CoT)", "content": "A notable distinction between LLMs and conventional machine learning algorithms is the capacity of foundation\nLLMs to process variably structured input data, specifically prompts, during the inference phase (Wei et al. 2022). For\ninference, LLMs generally precede or prioritize the information provided in the input prompts over the large implicit\nknowledge gained from the pre-training stage. Consequently, this leads to clear, comprehensive content. This content\ncould encompass domain-specific knowledge, contextual background, or detailed step-by-step reasoning guidance.\nBy doing so, LLMs can be encouraged to disclose their reasoning processes during inference, thereby enhancing the\nexplicability of their outputs. In this paper, we utilize the CoT technique to enable LLMs to generate step-by-step\nreasoning before providing the final answer, improving their performance on complex reasoning tasks.\nAlthough it is widely recognized that LLMs excel at generating responses reminiscent of human conversation, they\noften have opacity issues in their reasoning processes. This opacity hinders users' ability to evaluate the trustworthiness\nof the responses, particularly in scenarios that necessitate elaborate reasoning.\nFormally, let fo be a language model and X = {(x1,y1), (x2,y2), ..., (xn)} be an input prompt, where (xi, yi)\ndenotes the i-th example question-response pair. In a standard question-answering scenario, the model output is\ngiven by: yn = arg maxy po(y|x1,y1,x2,y2, ..., xn), where po is the predicted probability of the language model\nfo. This setting, however, does not provide insights into the reasoning process behind the answer yn. The CoT\nmethod extends this by including human-crafted explanations e\u00bf for each example, resulting in a modified input format\nX = (x1, \u20ac1,y1), (x2, \u20ac2, y2), ..., (xn). The model then outputs both the explanation en and the answer yn:\n$$(e_n, y_n) = arg\\ max_{y} p_o(Y|x_1, \\epsilon_1, y_1, x_2, \\epsilon_2, y_2, ..., x_n).$$"}, {"title": "3 Data", "content": "In this section, we first discuss the dataset employed for the study. We then explain how we convert the crash tabular\ndata to coherent descriptive narratives. Finally, we discuss our experimental settings and evaluation methods."}, {"title": "3.1 Dataset", "content": "Our empirical analysis utilizes data sourced from CrashStats data from Victoria, Australia spanning from 2006 to 2020.\nThe crash database contains records of vehicles involved in crashes. A four-point ordinal scale is used to code the\nseverity level of each accident, including: (1) non-injury accident, (2) other injury (minor injury) accident, (3) serious\ninjury accident, and (4) fatal accident. Each sample denotes a vehicle involved in a crash with driver's information.\nAfter data prepossessing, the final dataset has an extremely low representation of non-injury accidents (only four\ninstances), accounting for less than 0.001%. Consequently, these four non-injury accidents are merged to the category\nof \"Minor or non-injury accidents\". As a result, the dataset contains 197,425 minor or non-injury accidents, 89,925\nserious injury accidents, and 4,760 fatal accidents.\nThe traffic accident attributes considered in our empirical study include crash characteristics, driver traits, vehicle\ndetails, roadway attributes, environmental conditions, and situational factors (see Table 1)."}, {"title": "3.2 Textual narrative generation", "content": "To get coherent, informative passages enriched with domain-specific knowledge, we use a simple yet effective template\nto convert the raw structured tabular data into detailed, human-readable textual narratives, encapsulating vital information\nabout traffic accidents, which can be better consumed by LLMs. This process is depicted in Figure 1."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experiments design", "content": "In this work, we tackle the crash severity inference problem as a classification task. The inputs encompass various\ncrash related attributes, including environmental conditions, driver characteristics, crash details, and vehicle features.\nThe original data is in tabular format including categorical and numerical fields. We then transform the tabular data\ninto consistent textual narrative with a simple template, detailed in the preceding subsection. The objective is to\nestimate severity outcomes of crashes with the state-of-the-art LLM models, such as GPT-3.5-turbo, LLaMA3-8B, and\nLLaMA3-70B.\nLlaMA3s models are open-source foundation models, while the GPT model is a close-source and non-free model.\nConsidering the cost and our goal of evaluating crash severity inference performance of foundation models, we randomly\ndraw 50 samples from each of the three severity outcome categories, resulting in a total of 150 samples. These samples\nare used to demonstrate the potential of LLMs in enhancing crash analysis and reasoning."}, {"title": "4.2 Prompts for LLMs", "content": "In this section, we explained in detail how we design the prompts for different experiments in Table 2."}, {"title": "4.2.1 Zero-shot", "content": ""}, {"title": "4.2.2 Few-shot", "content": "The prompt designed for the plain few-shot setting is shown in Figure 6. In this paper, the few-shot setting refers to a\nthree-shot scenario, where three examples, one from each severity category, are provided for the LLMs to infer from.\nThe only difference between the prompt for Few-shot with Prompt Engineering (FS_PE) and that of the plain few-shot\nis that we substitute \"Fatal accidents\" with \"Serous accidents with potentially fatal outcomes\"."}, {"title": "4.3 Evaluation metrics", "content": "Following standard practice in the context of multi-class classification, we adopt two commonly used classification\nmetrics: Macro-Accuracy, and F1-score. Additionally, we include class-specific accuracies. These metrics are briefly\ndiscussed below.\n(1) Accuracy\nAccuracy measures the proportion of correctly classified instances in the test dataset. It is calculated as:\n$$Accuracy = \\frac{Correct\\ Predictions}{Total\\ Predictions}$$\n(2) F1-score\nThe F1-score is defined as the harmonic mean of precision and recall, computed as:\n$$F1\\text{-score} = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$$\nPrecision quantifies the accuracy of positive predictions for a specific class, computed as:\n$$Precision = \\frac{True\\ Positives}{True\\ Positives + False\\ Positives}$$"}, {"title": "5 Findings", "content": ""}, {"title": "5.1 Exemplar responses of LLMs to crash severity inference queries", "content": "The exemplar responses of GPT-3.5, LLaMA3-8B, and LLaMA3-70B in each of the six settings, including ZS, FS,\nZS_CoT, FS_PE, ZS_PE, and ZS_PE_CoT (outlined in Table 2), are shown in Figure 7. It demonstrates that the LLMs\ncan effectively respond to the severity inference task, delivering expected results. Note that the examples in Figure 7\nonly showcase correct severity inferences.\nGiven the prompt for each setting (see Section 4.2), each model can directly answer or ultimately summarize its\nestimated severity for the given accident as one of the defined categories.\nIn the plain Zero-shot and Few-shot settings, the models respond directly with one of the three class labels, i.e., 'Minor\nor non-injury accident', 'Serious injury accident', or 'Fatal accident'. Similarly, in the ZS_PE and FS_PE settings,\nthe models respond directly as 'Minor or non-injury accident', 'Serious injury accident', or 'Serious accident with\npotentially fatal outcomes'.\nIn contrast, in the CoT settings (ZS_CoT and ZS_PE_CoT), the models return longer responses by reasoning first and\nthen making inference of the severity outcome of the accident. Generally, the GPT-3.5 model's responses are more\nconcise."}, {"title": "5.2 Severity inference performance of the LLMs with different strategies", "content": "The performance metrics of GPT-3.5, LLaMA3-8B, and LLaMA3-70B for the crash severity inference task under the\nsix settings (see Table 2) are presented in Table 3.\nThe results reveal varied performance across models and settings in inferring crash severity outcomes. LLaMA3-70B\nconsistently exhibited superior performance, particularly with zero-shot prompt engineering (ZS_PE), achieving the\nhighest macro F1-score (0.4755) and macro-accuracy (49.33%). Furthermore, LLaMA3-70B attained the second-best\nperformance in macro F1-score (0.4747) and macro-accuracy (47.33%) under the zero-shot with Chain-of-Thought\n(ZS_CoT) setting. These findings suggest that both prompt engineering and Chain-of-Thought methodologies contribute\npositively to model performance. Nevertheless, no single technique demonstrated consistent superiority across all\nseverity categories. For fatal accidents, GPT-3.5 with zero-shot prompt engineering and Chain-of-Thought (ZS_PE_CoT)\nexhibited the highest accuracy (68%). In contrast, for serious injury accidents, GPT-3.5 and LLaMA3-8B in the plain\nzero-shot setting (ZS), as well as GPT-3.5 in the zero-shot with Chain-of-Thought scenario (ZS_CoT), achieved 100%\naccuracy. However, it is crucial to note that in these settings, these models performed poorly for fatal and 'minor or\nnon-injury' accidents, indicating an inherent bias toward the intermediate severity category of 'serious injury'.\nInterestingly, LLaMA3-70B with the basic zero-shot approach demonstrated the best inference performance for 'minor\nor non-injury' accidents (58% accuracy) while maintaining a relatively balanced performance across fatal and serious\ninjury accidents. This suggests a robust generalization capability of LLaMA3-70B across crash severity categories.\nThe implementation of prompt engineering, particularly in zero-shot settings, generally enhanced performance across\nmodels. This improvement was especially pronounced for fatal accident classification, where rephrasing the \"Fatal\naccidents\" label to the soft version of \"Serious accident with potentially fatal outcomes\" facilitated maintenance of\nclassification accuracy while adhering to the LLM's aligned behaviors."}, {"title": "5.3 Effectiveness of prompt engineering (PE) and Chain-of-Thought (CoT)", "content": "Figure 8 demonstrates the performance gains by CoT and PE as compared to the plain zero-shot setting. Both ZS_COT\nand ZS_PE consistently demonstrate enhanced performance in terms of Macro F1-score and Macro-accuracy across all\nthree models evaluated. This improvement underscores the efficacy of CoT and PE in boosting model performance in\nzero-shot scenarios.\nNotably, the implementation of PE (ZS_PE) yields more substantial improvements relative to CoT (ZS_CoT). This\ndifferential in enhancement suggests that, within the context of this specific task, the reformulation of prompts may\nbe particularly effective in guiding model outputs as compared to the structured reasoning approach with CoT. The\nconsistent pattern of improvement across different model architectures and sizes indicates the broad applicability of\nthese techniques in zero-shot learning paradigms.\nAs illustrated in Figure 8, CoT improves both Macro F1-score and Macro-Accuracy across all three models in the\nplain zero-shot (ZS) setting. Based on the results summarized in Table 3, GPT-3.5 and LLaMA3-8B show improved\nrecognition of \"Minor or non-injury\" accidents. LLaMA3-70B demonstrates substantial gain in identifying \"Serious\ninjury\" accidents and \"Minor or non-injury\" accidents, with only a slight reduction in performance for \"Fatal\" accidents.\nThe use of CoT enables LLMs to better understand and reason through questions, leading to more reliable and\nexplainable inferences.\nThe PE technique also leads to increased Macro F1-score and Macro-Accuracy across all three models compared to the\nplain zero-shot (ZS) setting, as depicted in Figure 8. Notably, it greatly enhances the models' ability to detect Fatal\naccidents by simply softening the label description from \"Fatal accident\" to \"Serious accident with potentially fatal\noutcomes\", resulting in more balanced performance across severity categories. Compared to the zero-shot baseline,\nGPT-3.5 with PE attains a remarkable increase in Fatal accident detection, with accuracy rising from 0% to 62%.\nSimilarly, LLaMA3-8B and LLaMA3-70B show increases in fatal accident accuracy from 0% to 34%, 44% to 60%,\nrespectively."}, {"title": "5.4 Zero-shot vs. Few-shot learning", "content": "In the FS setting, the inclusion of three examples improves both the macro F1-score and macro-accuracy compared\nto the ZS setting, boosting classification accuracy of GPT-3.5 and LLaMA3-8B for \u201cFatal accident\" and \"Minor or\nnon-injury accident\". However, this comes at the expense of accuracy for \"Serious injury accident\". This indicates a\npotential trade-off in classification performance across severity categories or a decrease of the model bias in the ZS\nsetting.\nMoreover, smaller models like LLaMA3-8B generally benefit more from few-shot learning than larger models, such as\nGPT-3.5 and LLaMA3-70B, as evidenced by a notable increase in Macro F1 score from 0.1818 to 0.4068. Nevertheless,\nLLaMA3-70B, being a larger model, performs slightly better in the zero-shot settings, suggesting it may have gained\nsome general knowledge in traffic safety domain during the pre-training stage, where the zero-shot prompting can draw\nupon such knowledge.\nIn contrast, the effects of PE in the FS setting exhibit more variations across models. GPT-3.5 demonstrates im-\nprovements in Macro F1-score and Fatal accident accuracy, and LLaMA3-70B shows remarkably improved inference\naccuracy for \"fatal accident\" and \"Serious injury accident\". Conversely, LLaMA3-8B shows decreased macro F1-score\nand macro-accuracy, indicating that PEin the few-shot setting may not be equally beneficial for models of different\nsizes.\nIt important to note that we did not explore the aspect regarding the choice of the examples in the FS setting, which\nmight have a varying effect on different models."}, {"title": "6 Discussions", "content": ""}, {"title": "6.1 Can LLMs with CoT yield logical reasoning for their inference outcomes?", "content": "CoT is a unique technique to augment LLM's reasoning capability. But how reasonable is the reasoning by LLMs with\nCoT? We aim to examine the responses of LLMs with CoT from the view of a traffic safety engineer. Specifically, the\nresponses of LLaMA3-70B in the ZS_CoT setting are evaluated due to its best performance in this setting (see Table 3).\nAs a qualitative assessment, three word clouds are drawn separately with respect to the three severity categories, i.e.,\n\"Minor/non-injury\", \"Serious injury\", and \"Fatal\" accidents (see Figures 10, 11,and 12. Note that only the correct\ninferred responses are used in creating these word clouds, where the bigger sizes of words indicate their higher\nfrequencies in the LLMs' responses. These visualizations offer insights into the LLM's conceptualization and reasoning\nprocesses regarding accident causation and factors considered during the severity inference.\nIn the three word clouds (Figure 10, 11,and 12), some words consistently appear regardless of severity outcomes,\nincluding \"collision,\" \"intersection,\" \"vehicle,\" and \"driver.\" This suggests a core set of concepts that the LLM associates\nwith traffic accidents. Additionally, LLaMA3-70B demonstrates consideration of diverse factors in its accident analysis,\nincluding:\n\u2022 Crash-related factors (e.g., \"rear-end collision\", \"pedestrian\", \"opposite directions\", \"corner\")\n\u2022 Environmental conditions (e.g., \"wet road surface\", \"rain\", \"dark\", \"stop-go\")\n\u2022 Driver behavior (e.g., \"failing to yield,\" \"misjudgment\", \"turning\", \"give way\", \"excessive speed\")"}, {"title": "6.2 Limitations and future works", "content": "One of the primary limitations of this study is the relatively small sample used, including only 150 instances (50 for\neach severity category). This limited dataset may not fully capture the variability and complexity of diverse real-world\ncrash scenarios, potentially affecting the generalizability of the findings.\nFor future research, several directions can be explored to further enhance the performance and applicability of LLMs in\ncrash analysis and modeling:\n1) Expanding the dataset to include a larger and more diverse set of samples will allow for a more comprehensive\nevaluation of the models' capabilities and improve the robustness of the results.\n2) Fine-tuning LLMs with more extensive and domain-specific data (e.g., crash reports and databases) can significantly\nenhance their domain knowledge to better understand the nuances and specificities of traffic accidents, leading to more\naccurate and reliable inference.\n3) Investigating explanation methods in conjunction with LLMs can yield more interpretable and trustworthy results."}, {"title": "7 Conclusions", "content": "In conclusion, this study demonstrates the efficacy of LLMs in crash severity inference using textual narratives of\ncrash events constructed from structured tabular data. Our comprehensive evaluation of modern LLMs (GPT-3.5-turbo,\nLLaMA3-8B, and LLaMA3-70B) across different settings (zero-shot, few-shot, CoT, and PE) yields insightful findings.\nLLaMA3-70B consistently outperformed other models, especially in zero-shot settings. CoT and PE techniques lead to\nenhanced performance, improving logical reasoning and addressing alignment issues.\nNotably, the use of CoT provided valuable insights into LLM reasoning processes, revealing their capacity to consider\nmultiple factors such as environmental conditions, driver behavior, and vehicle characteristics in the crash severity\ninference task. These findings collectively suggest that LLMs hold considerable promise for crash analysis and modeling.\nFuture research may explore other safety applications beyond the severity inference."}, {"title": "8 Author contributions", "content": "The authors confirm contribution to the paper as follows: study conception and design: J. Yang, H. Zhen, N, Liu; data\nprocessing and cleaning: H. Zhen, Y. Shi; experiments design, analysis and interpretation of results: H. Zhen, Y. Shi,\nJ. Yang; draft manuscript preparation: H. Zhen, Y. Shi, Y. Huang; review and editing: J. Yang, N. Liu. All authors\nreviewed the results and approved the final version of the manuscript."}]}