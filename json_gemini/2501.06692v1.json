{"title": "PGP-SAM: PROTOTYPE-GUIDED PROMPT LEARNING FOR EFFICIENT FEW-SHOT MEDICAL IMAGE SEGMENTATION", "authors": ["Zhonghao Yan", "Zijin Yin", "Tianyu Lin", "Xiangzhu Zeng", "Kongming Liang", "Zhanyu Ma"], "abstract": "The Segment Anything Model (SAM) has demonstrated strong and versatile segmentation capabilities, along with intuitive prompt-based interactions. However, customizing SAM for medical image segmentation requires massive amounts of pixel-level annotations and precise point- or box-based prompt designs. To address these challenges, we introduce PGPSAM, a novel prototype-based few-shot tuning approach that uses limited samples to replace tedious manual prompts. Our key idea is to leverage inter- and intra-class prototypes to capture class-specific knowledge and relationships. We propose two main components: (1) a plug-and-play contextual modulation module that integrates multi-scale information, and (2) a class-guided cross-attention mechanism that fuses prototypes and features for automatic prompt generation. Experiments on a public multi-organ dataset and a private ventricle dataset demonstrate that PGP-SAM achieves superior mean Dice scores compared with existing prompt-free SAM variants, while using only 10% of the 2D slices.", "sections": [{"title": "1. INTRODUCTION", "content": "Medical image segmentation aims to enable precise identification and delineation of specific regions of interest (e.g. tissues, organs), which can support doctors in making reliable diagnoses and planning effective treatments [1, 2]. However, achieving accurate segmentation typically requires large volumes of annotated data, which demands significant time, expertise, and resources to collect and label.\nThe Segment Anything Model (SAM) [3] has brought significant advancements to image segmentation with its powerful segmentation abilities and user-friendly prompt-based interactions. However, its performance in zero-shot medical image segmentation has been reported as suboptimal in various studies [4, 5, 1, 6, 7], spurring efforts to adapt SAM effectively to the medical domain. Most current approaches focus on fine-tuning SAM's image encoder to improve segmentation performance in medical imaging while preserving its interactive features. These methods often rely on ground truth data to create prompts during testing [5, 8, 9, 1]. Additionally, promptfree fine-tuning methods for SAM have emerged [10, 11, 12], but they lack sufficient utilization of medical knowledge.. Despite significant advancements, these works encounter two main challenges: (1) Due to the considerable domain gap between natural and medical images, full or encoder-focused fine-tuning requires large volumes of annotated medical data. (2) Given SAM's sensitivity to input point and box prompts, even slight prompt deviations can substantially affect results [13], necessitating precise prompt designs during training and testing. To address these challenges, H-SAM [14] employs a two-stage hierarchical mask decoder while keeping the image encoder frozen. This strategy allows rapid integration of medical knowledge using only 10% of available samples. However, due to the substantial number of learnable parameters, H-SAM still encounters difficulties in learning class-specific information effectively with such limited data."}, {"title": "2. METHOD", "content": "In this section, we introduce PGP-SAM by fine-tuning the image encoder of SAM. The proposed model comprises a multi-scale, prototype-guided prompt generator, and a mask decoder with shared weights that generates the segmentation mask. The details of PGP-SAM are illustrated in Figure 2."}, {"title": "2.1. Contextual Feature Modulation", "content": "The features extracted by the image encoder typically contain rich spatial details but often lack an overall image understanding. Inspired by previous work [15], we propose a Contextual Feature Modulation mechanism. This mechanism injects global semantic information into multi-scale features in both spatial and channel dimensions.\nGiven the input features $F_i \\in R^{H \\times W \\times C}$ at each stage, with $i \\in \\{1,2\\}$, we enhance them along both spatial and channel dimensions. We utilized features extracted after the first and fourth image blocks [16]. In the spatial dimension, $F_i \\in R^{H \\times W \\times C}$ is obtained by applying average pooling operations along both the height and width directions and summing the results, which effectively enhances the features. Then, we compute the spatial feature modulation matrix $F_s \\in R^{H \\times W \\times C}$.\n$F_s = F_i \\oplus \\delta(k \\times 1(\\phi(\\delta_{1 \\times k}(F_i))))),$ (1)\nwhere $\\oplus$ indicates the strip convolution, and $k$ indicates the kernel size of the strip convolution. $\\phi$ indicates the Layer Normalization followed by the ReLU function, and $\\delta$ indicates the Sigmoid function. Similarly, we apply average pooling operations along the channel dimension to obtain $F_c \\in R^{1 \\times 1 \\times C}$ and compute the channel feature modulation matrix $F_c \\in R^{H \\times W \\times C}$.\n$F_c = F_i \\oplus \\delta(1 \\times 1(\\phi(\\delta_{1 \\times 1}(F_c)))),$ (2)\nFinally, by adding the spatially $F_s$ and channel-modulated $F_c$ features to $F_i$, we obtain the context-enhanced features $F_i'$. For simplicity, subsequent calculations assume a single forward pass. Through global pooling along both the horizontal and vertical axes, CFM captures axial context, refines the network's focus on foreground regions, and helps subsequent prototype learning identify the most class-relevant features."}, {"title": "2.2. Progressive Prototype Refinement", "content": "The performance of SAM depends on accurate prompts, and even slight deviations can affect the segmentation results [1, 13]. To address this, we introduce two sets of prototypes: intra-class prototypes $P_{intra} \\in R^{N \\times C}$, N is the number of class, and inter-class prototypes $P_{inter} \\in R^{Q \\times C}$, Q = a \u00d7 N, a is set to 8. Intra-class prototypes are used to learn unique information for each class, while inter-class prototypes capture information shared across all classes.\nBy computing the cosine similarity between $P_{intra}$ and $P_{inter}$, we first select the top k prototypes from $P_{inter}$ that are most similar to each class prototype in $P_{intra}$. These selected prototypes are then concatenated with $P_{intra}$ to form $P \\in R^{Q \\times C}$, where Q = N \u00d7 (k + 1).\nNext, we design a class-based dual-path cross-attention mechanism to help P learn more robust class information. Specifically, given the enhanced input feature $F_1 \\in R^{H \\times W \\times C}$ and the class feature $F_M \\in R^{H \\times W \\times N}$ from the previous stage, we calculate two sets of attention weights. The first set is the class attention weights $W_c$. We pass both $F_1$ and $F_M$ through pointwise convolutions to obtain $F_I^I \\in R^{H \\times W \\times 1}$ and $F_M^I \\in R^{H \\times W \\times 1}$. After reshaping, each is fed into a fully connected layer to produce $F_I^I \\in R^{H \\times 1}$ and $F_M^I \\in R^{W \\times 1}$.\n$F_\\Theta^I = (reshape(\\varphi(F_\\Theta^I))), \\Theta \\in \\{I, M\\}$ (3)\nThese two outputs are then multiplied, followed by a pointwise convolution and a softmax operation, yielding the class attention weights $W_c \\in R^{H \\times W \\times Q}$. The process is formally described as:\n$W_c = softmax(\\varphi(F_I^I \\otimes F_M^I)),$ (4)\nwhere $\\otimes$ indicates the pointwise convolution, and $\\varphi$ indicates the fully connected layer.\nNext, $F_1$ and $F_M$ are fused through a pointwise convolution to obtain $F \\in R^{H \\times W \\times C}$. P is used as the query, and"}, {"title": "2.3. Prototype-based Prompt Generator", "content": "Previous work [13] computed similarity maps between prototypes and input features, then used these similarity maps to generate dense and sparse prompts. However, this approach led to the mixing of information. To address this, we designed separate pipelines for the dense and sparse prompts.\nDense Prompt Generation. We first obtain $P_{query} \\in R^{Q \\times N}$ by multiplying $P_{inter}$ and $P_{intra}$. Then, we concatenate $P_{inter}$ and $P_{query}$, pass them through a fully connected layer to get $P_{inter} \\in R^{Q \\times C}$. Next, we compute attention scores between $F_1 \\in R^{H \\times W \\times C}$ and $P_{inter}$, and feed the result into a two-layer MLP to form the dense prompt $D \\in R^{H \\times W \\times C}$.\n$D = MLP(softmax(F_1 \\cdot P_{inter}) \\cdot P_{inter}),$ (6)\nwhere the MLP consists of two pointwise convolutions, with a GELU activation function.\nSparse Prompt Generation. Similarly, we rely on $P_{intra}$ to generate the sparse prompt. By concatenating $P_{intra} \\in R^{N \\times C}$ with $F_1$, we obtain:\n$S = MLP(softmax(P_{itra} \\cdot F_1) \\cdot F_1),$ (7)\nBy separating the pipelines for dense and sparse prompt generation, each prompt can better incorporate the knowledge learned from the prototypes, while reducing the risk of information overlap."}, {"title": "3. EXPERIMENTS", "content": "This section evaluates PGP-SAM on two segmentation tasks, comparing it with existing SAM variants and analyzing module contributions through an ablation study."}, {"title": "3.1. Datasets", "content": "We conducted multi-organ segmentation experiments on both the public Synapse multi-organ CT dataset [17] and an private Ventricle CT dataset. The Synapse dataset contains 18 training cases and 12 testing cases, with each case having approximately 70 valid slices, while the Ventricle dataset has 400 training and 100 testing cases, each with around 10 valid slices. In the few-shot training, both datasets were at a resolution of 512 x 512."}, {"title": "3.2. Implementation Details", "content": "All models were trained on a single RTX 3090 GPU with data augmentation (elastic deformation, rotation, scaling). The loss function combines Cross-Entropy and Dice. We fine-tuned the ViT-b model using the same LoRA settings (rank=4) as SAMed. For the few-shot task, only 10% of the training data was used, posing significant challenges without reference images. The optimizer is based on AdamW with $B_1 = 0.9, B_2 = 0.999$, and weight decay of 0.1."}, {"title": "3.3. Results and Analysis", "content": "In Table 1, PGP-SAM shows outstanding few-shot transferability with limited medical images (10% of the total valid slices). Compared to other SAM medical SAM variants: FSSP-SAM [6], AutoSAM [10], SurgicalSAM [13], SAMLST [11] and SAMed [12], our PGP-SAM achieved a result of 78.75% under limited training slices, outperforming other Medical SAM models. We further validated the effectiveness of our method on an private dataset in Table 2. The Ventricle CT dataset contains approximately 400 head scans from patients with varying degrees of brain hemorrhage and trauma. The presence of hemorrhage complicates the segmentation of ventricles, particularly in few-shot scenarios. Compared to other SAM prompt-free variants, our PGP-SAM achieved a result of 76.39%. As shown in Figure 1, we visually compare the segmentation performance of PGP-SAM and other SAM variants on the Synapse dataset. Compared to the promptfree variant, our method significantly reduces false negatives and positives by utilizing prototypes. Compared to existing sam variants, our approach provides more precise boundary delineation for segmented objects."}, {"title": "3.4. Ablation Study", "content": "To validate the effectiveness of our approach, we analyzed the three key modules of PGP-SAM, as shown in Table 3. (1) Contextual Feature Modulation (CFM) improves the baseline by 2.16%. By leveraging early features from the Image Encoder, CFM enhances the model's ability to capture fine details such as object boundaries and textures. (2) With the addition of the Prototype-based Prompt Generator (PPG), the model's performance increases by 1.24%, thanks to a tailored pipeline for generating dense and sparse prompts, designed to avoid information blending and ensure specificity. (3) The addition of Progressive Prototype Refinement (PPR) yields a further 3.43% improvement. By allowing two prototypes to learn intra-class and inter-class information separately, PPR maximizes essential feature utilization from limited data. Even without reference images, this module greatly strengthens model robustness while maintaining high segmentation precision in data-scarce environments."}, {"title": "4. CONCLUSION", "content": "We propose PGP-SAM, an efficient prototype-guided prompt encoder for adapting the Segment Anything Model to medical image segmentation. By generating accurate prompts without extra overhead, it addresses the model's under-utilization of medical knowledge and excels in few-shot tasks, highlighting its potential in data-scarce scenarios."}]}