{"title": "ROBUST ONLINE RECONSTRUCTION OF CONTINUOUS-TIME SIGNALS FROM A LEAN SPIKE TRAIN ENSEMBLE CODE", "authors": ["Anik Chattopadhyay", "Arunava Banerjee"], "abstract": "Sensory stimuli in animals are encoded into spike trains by neurons, offering advantages such as sparsity, energy efficiency, and high temporal resolution. This paper presents a signal processing framework that deterministically encodes continuous-time signals into biologically feasible spike trains, and addresses the questions about representable signal classes and reconstruction bounds. The framework considers encoding of a signal through spike trains generated by an ensemble of neurons using a convolve-then-threshold mechanism with various convolution kernels. A closed-form solution to the inverse problem, from spike trains to signal reconstruction, is derived in the Hilbert space of shifted kernel functions, ensuring sparse representation of a generalized Finite Rate of Innovation (FRI) class of signals. Additionally, inspired by real-time processing in biological systems, an efficient iterative version of the optimal reconstruction is formulated that considers only a finite window of past spikes, ensuring robustness of the technique to ill-conditioned encoding; convergence guarantees of the windowed reconstruction to the optimal solution are then provided. Experiments on a large audio dataset demonstrate excellent reconstruction accuracy at spike rates as low as one-fifth of the Nyquist rate, while showing clear competitive advantage in comparison to state-of-the-art sparse coding techniques in the low spike rate regime.", "sections": [{"title": "I. INTRODUCTION", "content": "In most animals, sensory stimuli are communicated to the brain via ensembles of discrete, spatio-temporally compact electrical events generated by neurons, known as action potentials or spikes2. The conversion of continuous-time stimuli to spike trains occurs at an early stage of sensory processing, such as in the retinal ganglion cells in the visual pathway or spiral ganglion cells in the auditory pathway. Nature likely resorts to spike-based encoding due to several advantages: sparsity of representation\u00b3 and energy efficiency4, noise robustness, high temporal precision and facilitation of downstream computation7,8. Olshausen and Field\u00b3 demonstrated how efficient codes could arise from learning sparse representations of natural stimuli, resulting in striking similarities with observed biological receptive fields. Similarly, Smith and Lewicki9,10 showed that auditory filters could be estimated by training a population spike code model with natural sounds. While these studies highlight important aspects of spike-based encoding, they rely on existing dictionary learning techniques (e.g. matching pursuit or basis pursuit) to obtain the sparse spike codes, raising the question of the biological feasibility of computing such codes. The formal study of encoding continuous-time sensory stimuli via biologically plausible spiking neurons falls under the field of neural coding. Based on how neural spike responses are represented, studies can be broadly divided into two categories 11: 1) rate coding, where spike train responses are converted into an average rate, and 2) temporal coding, where the precise timing of spikes convey information about the stimuli. In the rate coding literature, spike responses to stimuli are converted to an average instantaneous rate r(t), and stimulus reconstruction is typically formulated probabilistically by choosing the stimulus s that maximizes the likelihood P(s|r). Rate coding is criticized for losing temporal precision, especially since studies have shown that neurons can exhibit sub-millisecond precision 12. Temporal coding, on the other hand, although emphasizes the precise timing of individual spikes, in this literature stimulus reconstruction is often formulated probabilistically (e.g. Bayesian Inference13) or through a linear transformation of spike-responses (e.g. reverse correlation 14). Probabilistic approaches to reconstruction complicate the development of a deterministic signal processing framework from spike trains, while simple linear transformations may be too restrictive for representing a generalized class of signals. Recent advanced temporal coding schemes, such as those by Sophie et al. 15, leverage recurrent networks and have shown near perfect reconstruction for certain signals. However, these techniques involve complex training procedures and do not provide reconstruction guarantees for a generalized class of signals. This paper is therefore motivated to build a signal processing framework that deterministically encodes continuous-time signals into biologically feasible spike trains, addressing questions of representable signal classes and reconstruction bounds.\nRelated Work and Contribution: From a signal processing standpoint, the very coarse \u03a3\u0394 quantization of bandlimited signals, as investigated in 16, effectively represents a spike train encoding. This scheme encodes an oversampled signal into a stream of 1/0 bits by thresholding the cumulative quantization error. Similarly, the Time encoding Machine 17 encodes a signal X(t) into a sequence of time instances {tk}, corresponding to moments when the signal crosses certain thresholds using an integrate-and-fire mechanism. In both cases, the signal class is bandlimited, and reconstruction guarantee is provided in the oversampled regime above the Nyquist rate. However, evidence suggests that spike codes can achieve high reconstruction accuracy while being much sparser than warranted by a rate code (e.g., the H1 neuron in the fly 18)."}, {"title": "II. CODING", "content": "For encoding, we make the following assumptions:\n1) We consider the set of input signals F to be the class of all finite-support, bounded functions (formally, $F = \\{X(t)|t\\in [0, \\tau],|X(t)| \\le b\\}$, for some arbitrary but fixed $\\tau, b \\in \\mathbb{R}^+$) that satisfy a finite rate of innovation bound 23. Naturally, X(t) \u2208 L\u00b2, i.e., square integrable.\n2) We assume an ensemble of m spiking neurons $\\Phi = \\{\\Phi_j|j\\in \\mathbb{Z}^+,1 \\le j \\le m\\}$, each characterized by a continuous kernel function $\\phi_j(t)$, where $\\forall j, \\phi_j(t) \\in C'[0, \\tau], \\tau\\in \\mathbb{R}^+$. Also we assume that each kernel $\\phi_j$ is normalized, i.e., $|\\|\\phi_j\\||_2 = 1,\\forall j$.\n3) Finally, we assume that $T_j(t)$ has a time varying threshold $T_j(t)$. The ensemble of kernels $\\Phi$ encodes a given input signal X(t) into a sequence of spikes $\\{(t_i, j_i)\\}$, where the ith spike is produced by the $j_i$th kernel $\\phi_{j_i}$ at time $t_i$ if and only if: $\\int X(t)\\phi_{j_i}(t_i - t)dt = T_{j_i}(t_i)$.\nDue to assumptions 1 & 2, our framework operates within a Hilbert space H of bounded-support, square-integrable functions $L^2[0,\\tau]$ for some $\\tau\\in \\mathbb{R}$, endowed with the standard inner product: $(f,g) = \\int f\\bar{g},\\forall f,g \\in H$. As these functions reside in a Hilbert space, the algebraic operations on the inputs or the kernels (e.g., the use of the projection operator) performed later in this paper are well-defined.\nIn our implementation a threshold function is assumed in which the time varying threshold $T_j(t)$ of the jth kernel remains constant at C until that kernel produces a spike, at which time an after-hyperpolarization potential (ahp) increments the threshold by a value M. This increment then returns to zero linearly within a refractory period \u03b4. Formally,\n$T^j(t) = C + \\sum_{t_o\\in[t-\\delta,t]} M(1-\\frac{t-t_o}{\\delta})  (C, M, \\delta,\\in \\mathbb{R}^+) $ (1)\nwhere the sum is taken over all spike times $t_o$ in the interval $[t-\\delta,t]$ at which the kernel $j$ generated a spike. This threshold function allows a neuron to remain quiescent as long as the signal is uncorrelated with its kernel $j$; it starts firing when the correlation reaches a certain threshold and continues to fire at higher threshold levels communicating increasing correlation levels, only inhibited by previous spikes. This phenomenon of probing signals via a sequence of spikes is depicted in fig1 for one kernel. In Eq.1, the ahp model is assumed to approximate the behavior of a biological neuron which undergoes a brief refractory period, typically lasting 1 msec immediately after producing a spike. In our model this is achieved by setting the value of M to a value much higher than the supremum norm of the input signal. As will be evident from subsequent sections, such a model of ahp not only bounds the interspike intervals thus ensuring the stability of the spiking framework, it also allows our model to spike at dynamically changing threshold levels starting from a baseline threshold at C for varying correlation levels between the input signal and the kernels, leading to better representational capacity of our model. In what follows, we first clarify notations used throughout this paper for improved readability. Following that, we present a set of corollaries based on the assumptions of our encoding model."}, {"title": "III. DECODING", "content": "The objective of the decoding module is to reconstruct the original signal from the encoded spike trains. Considering the prospect of the invertibility of the coding scheme, we seek a signal that satisfies the same set of constraints as the original signal when generating all spikes apropos the set of kernels"}, {"title": "IV. SIGNAL CLASS FOR PERFECT RECONSTRUCTION", "content": "We observe that in general the encoding of $L^2[0, T]$ signals into spike trains is not an injective map; the same set of spikes can be generated by different signals so as to result in the same convolved values at the spike times. Naturally, with a finite and fixed ensemble of kernels $\\Phi$, one cannot achieve perfect reconstruction for all $L^2[0, T]$ signals. Assuming, additionally, a finite rate of innovation, as F was previously defined changes the story. We now restrict ourselves to a subset G of F defined as $G = \\{X|X \\in F, X = \\sum_{p=1}^N \\alpha_p \\phi_{j_p} (t_p - t), j_p \\in \\{1,...,m\\}, \\alpha_p \\in \\mathbb{R},t_p \\in \\mathbb{R}^+, N \\in \\mathbb{Z}^+\\}$ and address the question of reconstruction accuracy. Essentially G consists of all linear combinations of arbitrarily shifted inverted kernel functions. N is bounded above by the total number of spikes that the ensemble $\\Phi$ can generate over [0,T]. For the class G the perfect reconstruction theorem is presented below. The theorem is proved with the help of two lemmas."}, {"title": "V. APPROXIMATE RECONSTRUCTION", "content": "Theorem 1 stipulates the conditions under which perfect reconstruction is feasible in the purview of our framework. Specifically the theorem shows the ideal conditions\u2014when the input signal lies in the span of shifted kernel functions and the spikes are generated at certain desired locations\u2014where perfect reconstruction is attainable. However, under realistic scenarios such conditions may not be feasible and hence the need for quantification of reconstruction error as the system deviates from the ideal conditions. For example, even though corollary 0.2 shows that a spike can be produced arbitrarily close to the desired location by setting the ahp parameters C and the \u03b4 of Eq. 1 at reasonably low values, it begs the question to what extent the reconstruction suffers due to small deviations in spike times. Likewise, the input signal may not perfectly fit in the signal class G, i.e. the input may not be exactly representable by the kernel functions due to the presence of internal or external noise. Under such non-ideal scenarios how much the reconstruction suffers is addressed in the following theorem."}, {"title": "VI. STABILITY OF THE SOLUTION AND WINDOWED ITERATIVE RECONSTRUCTION:", "content": "Theorem 2 shows that even under non-ideal conditions, our technique can keep the reconstruction error in check through suitable parameter choices. However, this may increase spike rates, as implied by Corollaries 0.1 and 0.3. Higher spike rates exacerbate the condition number of the P matrix referred to in Lemma 1. Instabilities in P render the solutions practically unusable in applications with finite precision floating point representations, due to quantization error. The ahp partially mitigates this issue for finite-sized P matrices by ensuring linear independence among the spikes. The inhibitory effect of the ahp, as alluded to in assumption 1, results in spikes that are sufficiently disjoint in time, leading to production of a linearly independent set of spikes. But the condition number can get progressively worse as we process longer signals and the size of P grows arbitrarily large. The following Theorem establishes a relation between the condition number of the P matrix and the spike count, revealing that, in the worst case, despite realistic assumptions about spike non-overlap, the condition number can deteriorate exponentially."}, {"title": "VII. EXPERIMENTS ON REAL SIGNALS", "content": "The proposed framework was tested on audio signals.\nDataset: We chose the Freesound Dataset Kaggle 2018, an audio dataset of natural sounds referred in 27, containing 18,873 audio files. All audio samples in this dataset are provided as uncompressed PCM 16bit, 44.1kHz, mono audio files. Set of Kernels: We chose gammatone filters ($at^{n-1}e^{-2bt}cos(2\\pi ft + \\phi)$) in our experiments since they are widely used as a reasonable model of cochlear filters in auditory systems 24. Results: The proposed framework was tested extensively against the mentioned dataset. Comprehensive results with 600 randomly selected audio snippets using 50 kernels are shown in Figure 6. The complete source code used for the experiment is available in GitHub 28. In the experiment, kernels were normalized, and parameters for the time-varying threshold function (1) were selected through a systematic grid search on a smaller dataset of 20 randomly chosen snippets. In each trial, an audio snippet of length \u2248 2.5s was processed with fixed parameter values, except for the refractory period, which was gradually decreased leading to improvement in reconstructions at higher spike rates. The refractory period varied from approximately 250ms to 5ms. After converting an audio snippet into a sequence of spikes, reconstruction was performed iteratively using a fixed-sized window of spikes as described in Section VI. The ahp period was systematically varied, but each trial on an audio snippet was run with a single value of the ahp period. Correspondingly, the window size was set to be inversely proportional to the ahp period, varying from 5k to 15k as per Theorem 4. Exact parameter values and window sizes are detailed in the GitHub documentation 28. The results in Figure 6 show that increasing the spike rate by tuning the refractory period allows near-perfect reconstruction, aligning with our theoretical analysis. Some variability in reconstruction accuracy across signals can be attributed to dataset idiosyncrasies, e.g. certain audio samples could be noisy or ill-represented in the kernels. However, the overall trend shows promise, with an average of \u2248 20dB at 1/5th Nyquist Rate. This in conjunction with the fact that signals are represented in this scheme only via set of spike times and kernel indexes (thresholds can be inferred) shows potential for an extremely efficient coding mechanism. Since the generation of spikes requires scanning through convolutions in a single pass, encoding is highly efficient. However, decoding is slightly more time-consuming because it involves solving the linear system Pa = T to derive the coefficients. But then reconstruction is performed iteratively on a finite window, as described in Section VI. Considering $O(w^3)$ as the time complexity for inverting a $w \\times w$ matrix, the overall time complexity of decoding is $O(Nw^3)$, where N is the length of the signal and w is the chosen window size. Thus the overall process still remains linear, making it a suitable choice for lengthy continuous-time signals.\nComparison With Convolutional Orthogonal Matching Pursuit: Our proposed framework is comparable to Convolutional Sparse Coding (CSC) techniques 22. In CSC, the objective is to efficiently represent signals using a small number of basis functions convolved with sparse coefficients. Mathematically, for a given signal Y, the CSC model can be expressed as: $Y \\approx \\sum_{k=1}^K d_k * x_k$ where $d_k$ are the convolutional filters, $x_k$ are the sparse feature maps, K is the number of filters and $*$ denotes convolution. The similarity with our proposed framework becomes clear when we consider our reconstruction formulation: $X^* = \\sum_{i=1}^N \\alpha_i \\phi_{j_i}(t_i - t)$. Here, the kernels $\\phi_{j_i} *$ serve as the convolution filters $d_k$, and the sparse feature map $x_k$ in our framework can be likened to a vector of coefficients $\\alpha_i$, where each $\\alpha_i$ corresponds to the spikes of the kernel $\\Phi_{j_i}$, and is time-shifted to the appropriate occurrences of spikes. In other words, our framework finds a sparse representation of a signal through its own biological spiking mechanism. Finding sparse code for signals, in general is an NP-Hard problem 29, and several heuristic-based approaches are used to address this challenge. In30, our proposed framework was compared against one such heuristic-based implementation of CSC, specifically Convolutional Orthogonal Matching Pursuit (COMP), using a small dataset of about 20 audio snippets. COMP employs a greedy technique to iteratively find dictionary atoms and is relatively slow due to the orthogonalization it performs to the atoms in each step. Most of the current leading CSC algorithms 31,32,33 use $L_1$ regularization as a relaxation of $L_0$ regularization in their sparse reconstruction objectives, making them amenable to efficient optimization methods such as the Alternating Direction Method of Multipliers (ADMM). In this paper, we extensively compare our technique with the efficient CBPDN algorithm implemented within the state-of-art SPORCO python library 34. In this experiment, we used 200 randomly chosen audio snippets from the same dataset, utilizing 10 gammatone kernels. The snippet lengths varied from 0.5s to 3.5s to compare processing times as a function of snippet length. Figure 7 shows the reconstruction accuracy comparison between our framework and CBPDN. CBPDN, which is based on $L_1$ optimization, achieved reconstructions at different sparsity levels by varying the regularization parameter $\\lambda$, while our framework achieved different spike rates by varying the ahp period (see 28 for exact parameter values). The results in Figure 7 reveal that our framework outperforms CBPDN, particularly in the low spike rate regime, achieving better SNR values on average at consistently lower spike rates. In the high spike rate regime, CBPDN occasionally outperforms our framework. This occurs because our framework is not well suited for high spike rates, where spikes begin to overlap significantly, leading to poorer bounds as discussed in our theoretical analysis. Additionally, the limited datapoints where CBPDN outperforms (around 30 dB) make these specific results unreliable. Figure 8 compares the runtimes of our framework and CBPDN on a 10-core Intel(R) Xeon(R) CPU E5-2650 v3 server. The average processing time, shown as a function of snippet length, demonstrates that our framework also outperforms CBPDN in terms of runtimes, particularly in the aforementioned low spike rate regime. Although the processing time is platform-dependent, our framework offers better asymptotic complexity. Leading CSC algorithms 31,32,33 transform long temporal signals to the Fourier domain, resulting in O(N log N) complexity due to FFT, whereas our method achieves $O(Nw^3)$. While the overall processing times (considering both encoding and decoding) of both frameworks are comparable, our framework encodes signals into spike times very efficiently using a convolve-and-threshold mechanism in minimal time. This is particularly useful in online settings, such as when a signal needs to be streamed. In such cases, our framework allows signals to be efficiently conveyed via only the spike times and indices (with thresholds or coefficients inferred). In contrast, CBPDN and other CSC algorithms, being inherently based on optimization, cannot achieve such efficient encoding and require both the coefficients and the timing of the atoms for signal representation. Overall, our framework shows promise as a superior alternative to CSC techniques in specific scenarios, particularly for low spike rate representation of natural audio signals with biological kernels, as demonstrated in our experiments."}, {"title": "VIII. CONCLUSION", "content": "The experimental results establish the efficiency and ro- bustness of the proposed spike-based encoding framework, which clearly outperforms state-of-the-art CSC techniques in the low spike rate regime. Notably, this high-fidelity coding and reconstruction is achieved through a simplified abstraction of a complex biological sensory processing system, which typically involves numerous neurons across multiple layers with diverse goals such as feature extraction, decision making, classification and more rather than being limited to reconstruction. For context, the human auditory system's cochlear nerve contains about 50,000 spiral ganglion cells (analogous to 50,000 kernels). The fact that our framework, with a single layer of roughly 100 neurons using a simple convolve-and- threshold model, achieves such high-quality reconstruction underscores the potential of fundamental biological signal pro- cessing principles. Our framework differs fundamentally from the Nyquist-Shannon theory, primarily in its mode of represen- tation and coding. Instead of sampling the value of a function at uniform or non-uniform pre-specified points, our coding scheme identifies the non-uniform points where the function takes specific convolved values. This efficient coding scheme, combined with our proposed window-based fast processing of continuous-time signals, shows great promise for achieving significant compression in real-time signal communication."}]}