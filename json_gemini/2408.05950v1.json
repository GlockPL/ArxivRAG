{"title": "ROBUST ONLINE RECONSTRUCTION OF CONTINUOUS-TIME SIGNALS FROM A LEAN SPIKE TRAIN ENSEMBLE CODE", "authors": ["Anik Chattopadhyay", "Arunava Banerjee"], "abstract": "Sensory stimuli in animals are encoded into spike trains by neurons, offering advantages such as sparsity, energy efficiency, and high temporal resolution. This paper presents a signal processing framework that deterministically encodes continuous-time signals into biologically feasible spike trains, and addresses the questions about representable signal classes and reconstruction bounds. The framework considers encoding of a signal through spike trains generated by an ensemble of neurons using a convolve-then-threshold mechanism with various convolution kernels. A closed-form solution to the inverse problem, from spike trains to signal reconstruction, is derived in the Hilbert space of shifted kernel functions, ensuring sparse representation of a generalized Finite Rate of Innovation (FRI) class of signals. Additionally, inspired by real-time processing in biological systems, an efficient iterative version of the optimal reconstruction is formulated that considers only a finite window of past spikes, ensuring robustness of the technique to ill-conditioned encoding; convergence guarantees of the windowed reconstruction to the optimal solution are then provided. Experiments on a large audio dataset demonstrate excellent reconstruction accuracy at spike rates as low as one-fifth of the Nyquist rate, while showing clear competitive advantage in comparison to state-of-the-art sparse coding techniques in the low spike rate regime.", "sections": [{"title": "I. INTRODUCTION", "content": "In most animals, sensory stimuli are communicated to the brain via ensembles of discrete, spatio-temporally compact electrical events generated by neurons, known as action potentials or spikes2. The conversion of continuous-time stimuli to spike trains occurs at an early stage of sensory processing, such as in the retinal ganglion cells in the visual pathway or spiral ganglion cells in the auditory pathway. Nature likely resorts to spike-based encoding due to several advantages: sparsity of representation\u00b3 and energy efficiency4, noise robustness, high temporal precision and facilitation of downstream computation 7.8. Olshausen and Field\u00b3 demonstrated how efficient codes could arise from learning sparse representations of natural stimuli, resulting in striking similarities with observed biological receptive fields. Similarly, Smith and Lewicki9,10 showed that auditory filters could be estimated by training a population spike code model with natural sounds. While these studies highlight important aspects of spike-based encoding, they rely on existing dictionary learning techniques (e.g. matching pursuit or basis pursuit) to obtain the sparse spike codes, raising the question of the biological feasibility of computing such codes. The formal study of encoding continuous-time sensory stimuli via biologically plausible spiking neurons falls under the field of neural coding. Based on how neural spike responses are represented, studies can be broadly divided into two categories 11: 1) rate coding, where spike train responses are converted into an average rate, and 2) temporal coding, where the precise timing of spikes convey information about the stimuli. In the rate coding literature, spike responses to stimuli are converted to an average instantaneous rate r(t), and stimulus reconstruction is typically formulated probabilistically by choosing the stimulus s that maximizes the likelihood P(s|r). Rate coding is criticized for losing temporal precision, especially since studies have shown that neurons can exhibit sub-millisecond precision 12. Temporal coding, on the other hand, although emphasizes the precise timing of individual spikes, in this literature stimulus reconstruction is often formulated probabilistically (e.g. Bayesian Inference13) or through a linear transformation of spike-responses (e.g. reverse correlation 14). Probabilistic approaches to reconstruction complicate the development of a deterministic signal processing framework from spike trains, while simple linear transformations may be too restrictive for representing a generalized class of signals. Recent advanced temporal coding schemes, such as those by Sophie et al. 15, leverage recurrent networks and have shown near perfect reconstruction for certain signals. However, these techniques involve complex training procedures and do not provide reconstruction guarantees for a generalized class of signals. This paper is therefore motivated to build a signal processing framework that deterministically encodes continuous-time signals into biologically feasible spike trains, addressing questions of representable signal classes and reconstruction bounds.\nRelated Work and Contribution: From a signal processing standpoint, the very coarse \u03a3\u0394 quantization of bandlimited signals, as investigated in 16, effectively represents a spike train encoding. This scheme encodes an oversampled signal into a stream of 1/0 bits by thresholding the cumulative quantization error. Similarly, the Time encoding Machine 17 encodes a signal X(t) into a sequence of time instances {tk}, corresponding to moments when the signal crosses certain thresholds using an integrate-and-fire mechanism. In both cases, the signal class is bandlimited, and reconstruction guarantee is provided in the oversampled regime above the Nyquist rate. However, evidence suggests that spike codes can achieve high reconstruction accuracy while being much sparser than warranted by a rate code (e.g., the H1 neuron in the fly 18)."}, {"title": "II. CODING", "content": "For encoding, we make the following assumptions:\n1) We consider the set of input signals F to be the class of all finite-support, bounded functions (formally, $F = {X(t)|t\\in [0, \\tau],|X(t)| \\leq b}$, for some arbitrary but fixed $ \\tau, b \\in R^+$) that satisfy a finite rate of innovation bound 23. Naturally, X(t) \u2208 L\u00b2, i.e., square integrable.\n2) We assume an ensemble of m spiking neurons \u03a6 = {$\u03a6_j|j\\in Z^+, 1 \\leq j \\leq m $}, each characterized by a continuous kernel function $\u03a6_j(t)$, where \u2200j, $\u03a6_j(t) \\in C'[0, \\tau]$, $ \\tau \\in R^+$. Also we assume that each kernel $\u03a6_j$ is normalized, i.e., $||\u03a6_j||_2 = 1, \u2200j$.\n3) Finally, we assume that $T_j$ has a time varying threshold $T_j(t)$. The ensemble of kernels \u03a6 encodes a given input signal X(t) into a sequence of spikes ${ (t_i, j_i) }$, where the ith spike is produced by the $j_i$th kernel $\u03a6_{j_i}$ at time $t_i$ if and only if: $\u222b X(t)\u03a6_{j_i}(t_i \u2013 t)dt = T_{j_i}(t_i)$.\nDue to assumptions 1 & 2, our framework operates within a Hilbert space H of bounded-support, square-integrable functions $L^2[0,\u03c4]$ for some $\u03c4\u2208 R$, endowed with the standard inner product: $(f,g) = \u222b fg, \u2200f,g \u2208 H$. As these functions reside in a Hilbert space, the algebraic operations on the inputs or the kernels (e.g., the use of the projection operator) performed later in this paper are well-defined.\nIn our implementation a threshold function is assumed in which the time varying threshold $T_j(t)$ of the jth kernel remains constant at C until that kernel produces a spike, at which time an after-hyperpolarization potential (ahp) increments the threshold by a value M. This increment then returns to zero linearly within a refractory period \u03b4. Formally,\n$T_j(t) = C + \\Sigma_{t_i \\in [t-\u03b4,t]} M(1-\\frac{t-t_i}{\u03b4}) $ ($C, M, \u2208 R^+$)   (1)\nwhere the sum is taken over all spike times $t_i$ in the interval $[t-\u03b4,t]$ at which the kernel j generated a spike. This threshold function allows a neuron to remain quiescent as long as the signal is uncorrelated with its kernel j; it starts firing when the correlation reaches a certain threshold and continues to fire at higher threshold levels communicating increasing correlation levels, only inhibited by previous spikes. This phenomenon of probing signals via a sequence of spikes is depicted in fig1 for one kernel. In Eq.1, the ahp model is assumed to approximate the behavior of a biological neuron which undergoes a brief refractory period, typically lasting 1 msec immediately after producing a spike. In our model this is achieved by setting the value of M to a value much higher than the supremum norm of the input signal. As will be evident from subsequent sections, such a model of ahp not only bounds the interspike intervals thus ensuring the stability of the spiking framework, it also allows our model to spike at dynamically changing threshold levels starting from a baseline threshold at C for varying correlation levels between the input signal and the kernels, leading to better representational capacity of our model. In what follows, we first clarify notations used throughout this paper for improved readability. Following that, we present a set of corollaries based on the assumptions of our encoding model."}, {"title": "III. DECODING", "content": "The objective of the decoding module is to reconstruct the original signal from the encoded spike trains. Considering the prospect of the invertibility of the coding scheme, we seek a signal that satisfies the same set of constraints as the original signal when generating all spikes apropos the set of kernels"}, {"title": "IV. SIGNAL CLASS FOR PERFECT RECONSTRUCTION", "content": "We observe that in general the encoding of $L^2[0, T]$ signals into spike trains is not an injective map; the same set of spikes can be generated by different signals so as to result in the same convolved values at the spike times. Naturally, with a finite and fixed ensemble of kernels \u03a6, one cannot achieve perfect reconstruction for all $L^2[0, T]$ signals. Assuming, additionally, a finite rate of innovation, as F was previously defined changes the story. We now restrict ourselves to a subset G of F defined as $G = {X|X \u2208 F, X = \\Sigma_{p=1}^N A_p\u03a6_{j_p} (t_p - t), j_p \u0404 {1,...,m}, \u03b1_p \u2208 R,t_p \u2208 R^+, N \u2208 Z+}$ and address the question of reconstruction accuracy. Essentially G consists of all linear combinations of arbitrarily shifted inverted kernel functions. N is bounded above by the total number of spikes that the ensemble \u03a6 can generate over [0,T]. For the class G the perfect reconstruction theorem is presented below. The theorem is proved with the help of two lemmas."}, {"title": "V. APPROXIMATE RECONSTRUCTION", "content": "Theorem 1 stipulates the conditions under which perfect reconstruction is feasible in the purview of our framework. Specifically the theorem shows the ideal conditions\u2014when the input signal lies in the span of shifted kernel functions and the spikes are generated at certain desired locations\u2014where perfect reconstruction is attainable. However, under realistic scenarios such conditions may not be feasible and hence the need for quantification of reconstruction error as the system deviates from the ideal conditions. For example, even though corollary 0.2 shows that a spike can be produced arbitrarily close to the desired location by setting the ahp parameters Cand the \u03b4 of Eq. 1 at reasonably low values, it begs the question to what extent the reconstruction suffers due to small deviations in spike times. Likewise, the input signal may not perfectly fit in the signal class G, i.e. the input may not be exactly representable by the kernel functions due to the presence of internal or external noise. Under such non-ideal scenarios how much the reconstruction suffers is addressed in the following theorem."}, {"title": "VI. STABILITY OF THE SOLUTION AND WINDOWED ITERATIVE RECONSTRUCTION:", "content": "Theorem 2 shows that even under non-ideal conditions, our technique can keep the reconstruction error in check through suitable parameter choices. However, this may increase spike rates, as implied by Corollaries 0.1 and 0.3. Higher spike rates exacerbate the condition number of the P matrix referred to in Lemma 1. Instabilities in P render the solutions practically unusable in applications with finite precision floating point representations, due to quantization error. The ahp partially mitigates this issue for finite-sized P matrices by ensuring linear independence among the spikes. The inhibitory effect of the ahp, as alluded to in assumption 1, results in spikes that are sufficiently disjoint in time, leading to production of a linearly independent set of spikes. But the condition number can get progressively worse as we process longer signals and the size of P grows arbitrarily large. The following Theorem establishes a relation between the condition number of the P matrix and the spike count, revealing that, in the worst case, despite realistic assumptions about spike non-overlap, the condition number can deteriorate exponentially."}]}