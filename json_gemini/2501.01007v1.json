{"title": "Deep Reinforcement Learning for Job Scheduling\nand Resource Management in Cloud Computing:\nAn Algorithm-Level Review", "authors": ["Yan Gu", "Zhaoze Liu", "Shuhong Dai", "Cong Liu", "Ying Wang", "Shen Wang", "Georgios Theodoropoulos", "Long Cheng"], "abstract": "Cloud computing has revolutionized the provision-\ning of computing resources, offering scalable, flexible, and on-\ndemand services to meet the diverse requirements of modern\napplications. At the heart of efficient cloud operations are job\nscheduling and resource management, which are critical for\noptimizing system performance and ensuring timely and cost-\neffective service delivery. However, the dynamic and heteroge-\nneous nature of cloud environments presents significant chal-\nlenges for these tasks, as workloads and resource availability can\nfluctuate unpredictably. Traditional approaches, including heuris-\ntic and meta-heuristic algorithms, often struggle to adapt to these\nreal-time changes due to their reliance on static models or pre-\ndefined rules. Deep Reinforcement Learning (DRL) has emerged\nas a promising solution to these challenges by enabling systems\nto learn and adapt policies based on continuous observations of\nthe environment, facilitating intelligent and responsive decision-\nmaking. This survey provides a comprehensive review of DRL-\nbased algorithms for job scheduling and resource management\nin cloud computing, analyzing their methodologies, performance\nmetrics, and practical applications. We also highlight emerging\ntrends and future research directions, offering valuable insights\ninto leveraging DRL to advance both job scheduling and resource\nmanagement in cloud computing.", "sections": [{"title": "I. INTRODUCTION", "content": "Cloud Computing. Cloud computing has fundamentally\nreshaped the landscape of modern computing, offering flex-\nible, scalable, and cost-effective solutions for data storage,\nprocessing, and management. Unlike traditional computing\nmodels, where users rely on local servers or on-premises\ninfrastructure, cloud computing provides a distributed envi-\nronment where computational resources, including servers,\nstorage, and software, are delivered over the internet [1].\nThis shift to cloud-based services enables organizations and\nindividuals to access powerful computing resources without\nthe need to invest heavily in physical infrastructure, allowing\nthem to scale their operations up or down based on demand.\nCloud computing is underpinned by a variety of service\nmodels, including Infrastructure-as-a-Service (IaaS), Platform-\nas-a-Service (PaaS), and Software-as-a-Service (SaaS), each\noffering different levels of abstraction and control. These\nservices support a wide array of applications, from enterprise\nresource planning (ERP) systems to machine learning plat-\nforms [2], [3], and have become critical enablers of innovation\nacross industries such as finance, healthcare, and entertain-\nment. As the demand for cloud services continues to grow,\ncloud providers face the ongoing challenge of managing an\nincreasingly complex and dynamic environment to ensure high\nperformance, reliability, and efficiency.\nAs cloud computing expands to incorporate edge comput-\ning [4], and scales to support an ever-growing number of\nusers and applications, effective job scheduling and resource\nmanagement become critical for ensuring optimal performance\nand resource utilization. Job scheduling involves the allocation\nof tasks or jobs to available computing resources in a manner\nthat maximizes efficiency, minimizes response times, and\nensures fairness among users. Resource management, on the\nother hand, focuses on the allocation and optimization of\ncomputational resources such as CPUs, memory, storage, and\nbandwidth to meet the diverse needs of various applications\nand workloads [5].\nJob Scheduling and Resource Management. The dynamic\nand heterogeneous nature of cloud environments makes job\nscheduling and resource management particularly challenging.\nCloud workloads vary significantly in terms of computational\nintensity, real-time constraints, and data dependencies, which\ncomplicates the scheduling process [6], [7]. Furthermore,\nresources are often distributed across multiple servers, data\ncenters, and geographic locations, making it difficult to en-\nsure consistent performance and effective load balancing. The\ndemand for resources fluctuates based on factors such as work-\nload characteristics, user behavior, and external conditions,\nrequiring adaptive management mechanisms [8], [9], [10].\nIn addition, cloud providers must address critical issues like\nenergy efficiency and fault tolerance [11], [12]. As a result,\nefficient job scheduling and resource management become\ncrucial not only for maximizing resource utilization but also\nfor minimizing operational costs, improving quality of service\n(QoS), and ensuring compliance with service level agreements\n(SLAs). These factors highlight the need for intelligent, adap-\ntive solutions that can handle the inherent complexities of\ncloud environments.\nA multitude of methods have been developed to address"}, {"title": "II. BASICS OF DRL FOR CLOUD COMPUTING", "content": "In this section, we introduce the fundamentals of DRL and\nMarkov Decision Processes (MDPs), and provide a general\nintroduction to modeling job scheduling and resource man-\nagement as MDPs, laying the groundwork for applying DRL\nin cloud computing.\nA. Basics of DRL\nReinforcement Learning (RL) has emerged as a powerful\nparadigm for solving sequential decision-making problems,"}, {"title": "B. Markov Decision Process", "content": "In RL, a learning agent interacts with an environment to\naddress sequential decision-making problems. Fully observ-\nable environments are typically modeled as MDPs, which\nare formally defined by a quintuple (S, A, T, R, \u03b3). Here,\nS denotes the state space, encompassing all possible states\nthe system can occupy, while A represents the action space,\ncontaining all feasible actions an agent can take in any\ngiven state. The transition probability function T defines the\nlikelihood of transitioning from one state to another, expressed\nas P(St+1|st, at), given an action at. The reward function R\nassigns a scalar reward R(st, at) based on the agent's action in\na specific state, reflecting the immediate value of that action.\nFinally, the discount factor \u03b3, where 0 \u2264 y \u2264 1, governs\nthe trade-off between immediate and future rewards, with a\nhigher y emphasizing long-term gains and a lower y focusing\non short-term outcomes.\nAt each discrete time step t, the agent observes the current\nstate st, selects an action at according to a policy \u03c0 : S \u2192\nA, and receives a reward rt = R(st, at). The environment\nthen transitions to a new state st+1 based on the transition\nprobability P(st+1|st, at). The_goal of agent is to identify\nan optimal policy \u03c0* that maximizes the expected cumulative\nreward over time, mathematically expressed as:\n$\\sum_{t=0}^{\\infty} \\gamma^t r_t(s_t, a_t)$"}, {"title": "C. Job Scheduling as MDPs", "content": "Job scheduling can generally be abstracted into two levels\nbased on the structure of jobs: task scheduling and work-\nflow scheduling. Workflow scheduling can be considered an\nextension of task scheduling, as it involves managing more\ncomplex dependencies between tasks [39]. In dynamic envi-\nronments, such as those with rapidly fluctuating job arrival\nrates, scheduling decisions depend solely on the current system\nstate, with future states determined by immediate scheduling\nactions. This property makes these scenarios particularly well-\nsuited for modeling as MDPs.\n1) Task Scheduling: Task scheduling in cloud computing\nfocuses on the allocation of individual tasks to available\nresources, ensuring that tasks are executed efficiently.\nAction Space: Given the sequential arrival of individual\ntasks over time, the action space A at each Markov decision\nstep is designed to schedule the tasks that have arrived at the\ncurrent moment. Specifically, the scheduler selects an action\nthat determines how the newly arrived task is assigned to\navailable resources or queued for later execution [40]. The\naction space A can be mathematically defined as:\nA = {a | a = assign t to v, v \u2208 V}\nHere, t represents the task to be scheduled at the current\ndecision step, v is a computational resource to which the task\ncan be assigned, and V denotes the set of all available compu-\ntational resources in the system. In some scenarios, the action\nspace A may vary depending on task-specific constraints. For\ninstance, due to privacy and security considerations, certain\ntasks may only be assigned to resources in a private cloud [41].\nThis dynamic and constraint-aware definition of the action\nspace ensures that scheduling decisions remain feasible and\naligned with the specific requirements of tasks and resources.\nState Space: The task action space is inherently influenced\nby the state space S of the cloud environment due to the\nMarkov property. Broadly, S comprises two main components:\ntask status St and the states of computational resources Sv.\nThis can be expressed as:\nS = {St, Sv}\nThe task status St captures critical details about each task,\nsuch as CPU, memory, and storage requirements, as well\nas its execution status (e.g., time remaining for completion).\nIn certain computational scenarios, St can be extended to\ninclude additional constraints, such as security and privacy\nrequirements or geographical location preferences [42], [43].\nFor computational resource states, S represents the status\nof available resources, including current availability, expected\ncompletion times, computation and storage costs, and other\nrelevant metrics [44].\nReward Function: In cloud systems, task scheduling often\ninvolves multiple optimization objectives. The overall reward\nfunction is typically defined as: R = F(01, 02,...), where oi\nrepresents an individual optimization objective. Maximization\nobjectives commonly include factors such as memory utiliza-\ntion, storage utilization, and network bandwidth usage. Con-\nversely, minimization objectives focus on metrics like opera-\ntional costs, task response time, and energy consumption [45],\n[46]. In practice, the reward function is often designed as a\nweighted combination of these objectives, enabling a balanced\ntrade-off between competing goals [47]."}, {"title": "2) Workflow Scheduling:", "content": "Cloud workflows are commonly\nmodeled as Directed Acyclic Graphs (DAGs) and are typically\ndecomposed into subworkflows or subtasks [48] during the\nscheduling process. This decomposition facilitates the parallel\nexecution of workflows across diverse resources in the cloud\nenvironment, thereby enhancing resource utilization and work-\nflow execution efficiency.\nAction Space: Similar to task scheduling, the action space\nA in workflow scheduling can be abstracted at a high level as:\nA = {a | a = assign w to v, v \u2208 V}\nwhere w represents the workflow to be scheduled at the\ncurrent decision step. Generally, the action space in workflow\nscheduling is typically structured into two decision levels.\nThe first level involves decomposing the workflow w into\nsubworkflows or tasks t, which can be performed using\nDRL or other ruled-based algorithms [49]. The second level\nfocuses on allocating the decomposed subworkflows or tasks\nto appropriate resources, taking into account task dependencies\nwithin the workflow.\nState Space: Unlike task scheduling, which primarily\nfocuses on individual tasks, workflow scheduling involves\nmanaging entire workflows efficiently by coordinating the\nexecution of decomposed tasks. As a result, the state space for\nworkflow scheduling is inherently more complex and extends\nbeyond that of task scheduling. The state space S for workflow\nscheduling can be represented as:\nS = {St, Sw, Sv}\nwhere St represents the critical details of the decomposed\nsubworkflows or tasks, and Sw encapsulates the state of the\nworkflows.\nReward Function: In workflow scheduling, the reward\nfunction is tied to various optimization objectives, with\nmakespan and cost often being primary considerations. Unlike\ntraditional task scheduling, workflow scheduling introduces\nunique challenges, particularly in managing costs. In addition\nto computation and storage costs, communication costs play a\ncritical role due to the data dependencies between tasks exe-\ncuted across distributed resources. These communication costs,\nwhich include data transfer time and bandwidth utilization,\ncan significantly influence workflow performance, especially\nin geographically dispersed environments [50]."}, {"title": "D. Resource Management as MDPs", "content": "Resource management in cloud computing encompasses two\nprimary functions: resource provisioning and resource schedul-\ning. Resource provisioning involves allocating virtualized re-\nsources to accommodate varying workloads and user demands,\nwhile resource scheduling focuses on assigning these allocated\nresources to specific tasks or applications. In dynamic and\nuncertain cloud environments, resource management decisions\nare guided by the current state of the system. The evolution\nof future states depends directly on the actions taken in\nthe present, without reliance on past states, making resource\nmanagement suitable for representation using MDPs.\n1) Resource Provisioning: Through elastic scaling of com-\nputational resources, resource provisioning enables cloud sys-\ntems to adapt to fluctuating workload demands, thereby en-\nhancing overall performance and efficiency.\nAction Space: In resource provisioning, the action space\ndepends on the type of scaling. For horizontal scaling, the\nresource v typically refers to virtual machines or containers,\nwhile for vertical scaling, the resource v usually represents\nCPU, memory, or storage capacity [51]. Generally, the action\nspace A be expressed as:\nA = {a | scale up/down/keep unchanged for v, \u03c5\u2208V}\nThe actions are often represented as discrete integer values,\nindicating the number of resources to be scaled up or down.\nState Space: To model the resource provisioning problem,\nthe state space S can be generally expressed as:\nS = {Su, Sn, Sp}\nwhere Su represents the current utilization of system re-\nsources, such as CPU, memory, storage, and network band-\nwidth. Sn denotes the number of virtual machine or container\ninstances running on each server. Additionally, Sp captures\nperformance metrics of the system, including throughput,\nresponse time, and energy consumption.\nReward Function: The reward function in resource provi-\nsioning incorporates multiple optimization objectives include\nmaximizing resource utilization and system throughput, mini-\nmizing infrastructure costs and energy consumption, and main-\ntaining load balance to prevent single-point overloads. Each\nmetric is carefully quantified and integrated into the reward\nfunction, ensuring a balanced trade-off between performance,\ncost efficiency, and system stability.\n2) Resource Scheduling: Resource scheduling aims to al-\nlocate resources to jobs efficiently, optimizing objectives such\nas maximizing resource utilization and system throughput.\nAction Space: Give the available resource V and an incom-\ning task t requiring resources at the current step, the action\nspace A can be defined as:\nA = {a | a = allocate v to t, v \u2208 V}\nTThe type of resources varies depending on the specific com-\nputing scenario. In cloud environments, available resources\ntypically include virtual machines [52], whereas in edge\ncloud computing scenarios, the resources could consist of\nedge devices such as mobile phones [53] and vehicles [54].\nAdditionally, resources can be subdivided into more granular\ncomponents, such as processing units (e.g., CPUs, GPUs),\nstorage, memory, and network bandwidth [55], [56].\nState Space: Similar to task scheduling, resource schedul-\ning involves both the state of the resources S\u2082 and the state\nof the tasks St. The state space S can be represented as:\nS = {Sv, St}\nwhere S captures the current status of resources, and St\nrepresents the characteristics and requirements of the tasks in\nthe system.\nReward Function: The design of the reward function for\nresource scheduling typically considers task requirements and"}, {"title": "III. THE TYPICAL DRL APPROACHES", "content": "In this section, as illustrated in Fig. 2, we provide a detailed\noverview of typical DRL algorithms applied in cloud comput-\ning, establishing a foundation for our algorithm-level review\non their use in job scheduling and resource management.\nA. Valued-Based DRL Methods\nValue-based DRL methods center around learning a value\nfunction which estimates the expected cumulative reward of\na state-action pair. As shown in Fig. 2 (a), these methods\ntypically aim to optimize a policy indirectly by approximating\nthe optimal action-value function Q*(s, a), which predicts the\ntotal expected reward starting from state s, taking action a,\nand following an optimal policy thereafter. The most widely\nused value-based method is Deep Q-Network (DQN), which\nbuilds upon traditional Q-learning by employing deep neural\nnetworks as function approximators [57]. Several variations of\nDQN have been proposed to address its limitations, including\nDouble DQN (DDQN), Dueling DQN, and Noisy DQN.\n1) DQN: DQN extends Q-learning by utilizing a deep\nneural network to approximate the Q-function Q(s, a; 0) where\n0 represents the parameters of the network. The agent interacts\nwith the environment, storing experiences (st, at, rt, St+1) in\na replay buffer. To update the network, mini-batches of these\nexperiences are sampled and the Q-network is trained to\nminimize the loss:\nL(0) = E(Star,1,St+1) [(Yt - Q(st, at; 0))^2]\nwhere the target yt is calculated as:\n$\\text{Yt} = rt + ymax Q(st+1, \u03b1'; 0\u00af)$"}, {"title": "2) DDQN:", "content": "DDQN addresses the overestimation issue in\nDQN by decoupling the action selection and evaluation pro-\ncesses. In this method, the online Q-network selects the action,\nwhile the target network evaluates it to reduce overestimation\nbias [58]. The target for the loss function is updated as:\nYt\n= rt + yQ(st+1, arg max Q(st+1, a'; 0); 0\u00af)"}, {"title": "3) Dueling DQN:", "content": "In many environments, estimating the\nvalue of each action can be inefficient. To address this, the\ndueling architecture decomposes the Q-value into the state\nvalue V(s) and the action advantage A(s, a). The Q-value\nis then expressed as:\nQ(s, a) = V(s) + A(s, a)\nTo ensure the Q-values are uniquely determined, the mean\nadvantage across all actions is subtracted:\nQ(s, a) = V(s) + A(s, a) - $\\frac{1}{A} \\sum A(s, a')$\nThis structure allows the agent to focus on learning state\nvalues, which improves efficiency in environments where the\nchoice of action has less impact. By decoupling state and\naction evaluation, Dueling DQN enables faster and more stable\nlearning particularly in large action spaces."}, {"title": "4) Noisy DQN:", "content": "Efficient exploration is essential in DRL,\nbut traditional approaches like e-greedy exploration often fail\nin environments with complex or sparse reward structures.\nNoisy DQN introduces learnable noise directly into the param-\neters of the Q-networks to enhance exploration, eliminating the\nneed for manually tuned exploration schedules and enabling\nmore effective exploration throughout training.\nIn Noisy DQN, the network weights W are perturbed by\nadding parameterized noise expressed as:\nW = \u03bc + \u03c3\u00b7 \u03b5\nwhere \u03bc and \u03c3 are learnable parameters and e is drawn\nfrom a noise distribution such as Gaussian or factorized\nnoise. This approach ensures that the Q-value estimates vary\ndue to stochasticity in the network and promotes exploration\nthroughout training.\nNoisy DQN modifies the standard DQN loss function as\ndescribed in Equation 10, by introducing noisy weights that\nenhance exploration through added variability in Q-value\nestimates without the need for manual tuning. As training\nprogresses, the noise adjusts automatically and enables a\nsmooth shift from exploration to exploitation. This approach\nhas shown better performance than standard DQN particularly\nin tasks with sparse rewards or large action spaces due to its\nsustained exploration."}, {"title": "B. Policy-Based DRL Methods", "content": "As illustrated in Fig. 2 (b), policy-based DRL methods\nfocus on directly learning a policy \u03c0(as) which specifies\nthe probability of taking action a given state s. Unlike value-\nbased methods that derive a policy indirectly by learning value\nfunctions, policy-based methods optimize the policy itself by\nmaximizing the expected cumulative reward over time [59].\nThese methods are particularly effective in continuous or\nhigh-dimensional action spaces, where discretizing the action\nspace for value-based approaches becomes computationally\ninfeasible."}, {"title": "1) Actor-Critic DRL Methods:", "content": "One of the most widely\nadopted frameworks within policy-based methods is the Actor-\nCritic (AC) architecture [60]. In this approach, the actor is\nresponsible for learning the policy \u03c0\u03bf(as), while the critic\nestimates a value function V$(s) or the action-value function\nQ(s,a), as illustrated in Fig. 2 (d). The critic provides\nfeedback to the actor on how good the selected actions are,\nhelping to stabilize the policy learning process. The objective\nof the actor is to maximize the expected return J(0) defined\nas:\nJ(0) = \u0395\u03c0\u03bf [$\\sum_{t=0}^{T} \\gamma^t r_t$]\nwhere y is the discount factor and rt represents the reward at\ntime step t. The policy of the actor is updated by following\nthe gradient of the expected return computed as:\nVoJ(0) = \u0395\u03c0\u03bf [Velog \u03c0\u03bf(\u03b1|s)\u00b7 \u03b4\u03b5]"}, {"title": "2) Proximal Policy Optimization (PPO):", "content": "PPO presents a\nsolution to the challenges inherent in policy gradient methods,\nparticularly the risk of unstable or excessively large policy up-\ndates. Traditional approaches to policy optimization frequently\nexperience abrupt policy changes after updates, making the\nlearning process more challenging. PPO counteracts this by\nlimiting how much the updated policy can diverge from the\nprevious one using a trust region constraint. This is done\nvia a surrogate objective function that controls the scale of\npolicy changes. At its core, PPO relies on optimizing a clipped\nobjective function, which ensures that policy updates remain\nwithin a specified range and do not destabilize the training\nprocess. The objective function in PPO is expressed as:\nLCLIP (0) = Et [min (rt (0) At, clip(rt (0), 1 \u2013 \u20ac, 1 + \u0454)At)]"}, {"title": "LPEN (0)", "content": "L(0) \u2013 \u03b2KL[\u03c0\u03b8old(\u00b7|St)||\u03c0\u03bf(\u00b7|St)]"}, {"title": "3) Deterministic Policy Gradient Methods (DPG):", "content": "DPG\nmethods are designed for continuous action spaces by directly\nlearning a deterministic policy \u03bce(s) which selects a specific\naction for each state. This approach avoids the variance intro-\nduced by sampling from a stochastic policy. The goal of DPG\nis to maximize the expected cumulative reward", "as": "nJ(0) = Eso~p(s) [$\\sum_{t=0"}, {"St))$": "nDPG computes the policy gradient using the deterministic\npolicy and the action-value function Q(s", "as": "nVoJ(0) = Es~pr [Vo\u03bco(s)\u221aaQ(s,a)|a=\u03bc\u03bf(8)"}]}