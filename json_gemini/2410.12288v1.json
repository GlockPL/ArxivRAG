{"title": "A Prompt-Based Knowledge Graph Foundation Model\nfor Universal In-Context Reasoning", "authors": ["Yuanning Cui", "Zequn Sun", "Wei Hu"], "abstract": "Extensive knowledge graphs (KGs) have been constructed to facilitate knowledge-\ndriven tasks across various scenarios. However, existing work usually develops\nseparate reasoning models for different KGs, lacking the ability to generalize and\ntransfer knowledge across diverse KGs and reasoning settings. In this paper, we\npropose a prompt-based KG foundation model via in-context learning, namely\nKG-ICL, to achieve a universal reasoning ability. Specifically, we introduce a\nprompt graph centered with a query-related example fact as context to understand\nthe query relation. To encode prompt graphs with the generalization ability to\nunseen entities and relations in queries, we first propose a unified tokenizer that\nmaps entities and relations in prompt graphs to predefined tokens. Then, we\npropose two message passing neural networks to perform prompt encoding and\nKG reasoning, respectively. We conduct evaluation on 43 different KGs in both\ntransductive and inductive settings. Results indicate that the proposed KG-ICL\noutperforms baselines on most datasets, showcasing its outstanding generalization\nand universal reasoning capabilities. The source code is accessible on GitHub:\nhttps://github.com/nju-websoft/KG-ICL.", "sections": [{"title": "Introduction", "content": "Reasoning on knowledge graphs (KGs) involves inferring new relational facts from existing ones.\nEarly related work primarily focuses on reasoning over a static KG in the transductive setting, but\nlacks the generalization ability to handle new entities or relations in the KG. Recent research [1-4]\nconsiders the relational patterns between seen and unseen entities, enabling inductive reasoning.\nHowever, these methods still lack the transferability to reason over unseen KGs due to the unshared\nand unlinked entity and relation vocabularies between the pre-trained KG and unseen KGs.\nThe primary challenge in generalizing to new entities, relations, and even different KGs lies in how\nto represent such unseen data. Some methods [1-4] aggregate query-conditioned relational structures\nto represent entities. They can conduct inductive reasoning over unseen entities using these relative\nentity representations without the need of pre-trained entity embeddings. However, these methods\ncannot reason over unseen relations. To resolve this issue, some recent methods [5, 6] develop relative\nrelation representations. They model relation interactions using a query-conditioned relation graph,\nwhere each node represents a relation and an edge indicates that the linked two relations share a\nsubject or object entity in the KG. They conduct message passing on the query-conditioned relation\ngraph to represent relations.\nHowever, the relation graph only describes the connectivity of relations in the KG, with less attention\nto the local context of the entity and relation in a query. As a result, these methods usually fail to\ngenerate discriminative relation representations. For example, to infer the query relation parentOf,\nthe most relevant relation is coupleof. While in the KG, since every student has parents and most\nteachers are parents, the relation graph would also contain edges \"parentof \u21d2 teach\" and \"teach\n\u21d2 parentof\". The relation teach appears as noise in representing parent0f, which may mislead\nthe model, resulting in prediction failures. This inspires us to capture the local contexts and highlight\nthe important relations relevant to queries, rather than relying on a global relation graph.\nIn this paper, we propose a novel KG reasoning foundation model with in-context learning, namely\nKG-ICL. In-context learning is a method that allows pre-trained models to learn tasks based on only\na few examples without updating model parameters. The extraordinary success of in-context learning\nin language modeling [7] hinges on three crucial fundamentals: prompt design, unified tokenization,\nas well as contextual understanding and utilization.\nThe art of prompt design lies in highlighting task-critical information. We construct a prompt graph\nto model query-related contexts, which starts with an example fact about the query relation, i.e.,\n(subject, query relation, object). We consider two types of contexts as prompts. The first is\nentity context, which includes the neighboring entities of the example subject and object. The second\nis relation context, which considers relational paths between the subject and object entities. Thus,\nthe node set of our prompt graph includes the neighbors of the example subject and object, as well\nas the entities within the paths connecting the subject and object in the KG. We utilize the induced\nsubgraph of these entities as a prompt graph.\nThen, we design a unified tokenizer that is applicable to various prompt graphs. The key challenge is\nthat the entities and relations usually vary across different KGs [8, 9], and this issue extends to prompt\ngraphs as well. Conventional KG reasoning models [10-14] merely learn an individual embedding\nfor each entity or relation, resulting in the inability to reason over unseen KGs. We extend the entity\nlabeling method of GraIL [1] to relations, proposing a unified tokenizer for various prompt graphs.\nGiven a query relation and its prompt graph, we first group the involved entities based on the lengths\nof their shortest path to the example subject and object entities. Similarly, we categorize relations\ninto two classes depending on whether they represent query relations. Finally, the entities or relations\nin the same group will be mapped to the same token. As a result, prompt graphs from different KGs\nare described in \u201cthe same language\u201d.\nGiven the above prompt graph and unified tokenizer, we propose two message passing neural networks\nas the prompt encoder and KG reasoner, respectively. The input of the prompt encoder is the prompt\ngraph and the learnable token representations. At each layer of prompt encoding, we introduce\nan entity-centric and a relation-centric aggregation. Notably, in relation-centric aggregation, we\ntreat relations as special nodes and update their representations by aggregating messages from facts\ncontaining them. After prompt encoding, we read the relation representations from the prompt graphs\nto support KG encoding. At the beginning of KG encoding, we initialize the relation representations\nin the KG as the prompt relation representations. As for entities, we initialize the subject entity as\nthe query relation representation, and other entities are initialized as zero vectors. After performing\nmessage passing over the KG, we score all entities based on the output entity representations.\nWe conduct extensive experiments on 43 datasets to validate the effectiveness of our model. The\nexperimental results indicate that our model not only possesses universal reasoning capabilities across\ndiverse KGs but also outperforms supervised and pre-training models. Moreover, we observe that the\nproposed model exhibits robustness and high efficiency in utilizing examples.\nIn summary, our main contributions are listed below:\n\u2022 Our key contribution is an in-context KG reasoning foundation model. It prompts the\npre-trained model to engage in relational reasoning over diverse KGs.\n\u2022 We propose a prompt graph as context to support in-context learning. It consists of an\nexample fact about the query relation and its relevant subgraphs and paths. We also employ\na unified tokenizer to map entities and relations in prompt graphs to predefined tokens.\n\u2022 Given a prompt graph with token representations, we propose two message passing networks\nfor prompt graph encoding and KG reasoning. The foundation model can be further finetuned\non specific KGs to obtain improved performance.\n\u2022 We conduct extensive experiments on 43 KGs in both transductive and inductive settings to\ndemonstrate the universal reasoning capability of our model."}, {"title": "Related Work", "content": "KG reasoning. KG reasoning primarily involves three settings: transductive, inductive, and fully-\ninductive. Early studies [10-14] focus mainly on the transductive setting, assuming that KGs are\nstatic. Real-world KGs are dynamic, inspiring the development of inductive models [1\u20134, 15\u201323]\nthat allows for emerging entities. In the fully-inductive setting [5, 24\u201326], both unseen entities and\nrelations can emerge in the query facts. This setting remains limited to the same KG. In contrast,\nour in-context learning and KG foundation model seek to break down the barriers imposed by these\nsettings and achieve universal reasoning capabilities.\nPrompt and in-context learning in graph pre-training. Our work is also related to graph prompt\nlearning and graph in-context learning. Inspired by the success of pre-training models in NLP\n[27] and computer vision [28], some graph pre-training models [29\u201333] have been proposed. These\nmodels follow the paradigm of \u201cpre-train and finetune\", where a model is initially pre-trained and then\nfinetuned for the target task. The work [34] further develops a KG pre-training model. Consequently,\nrecent work [8, 35-45] has shifted focus to the \u201cpre-train, prompt, and finetune\" paradigm. The\nrelation graph of the KG pre-training model [6] can also be seen as a special prompt. This paradigm\nleverages task prompts to enhance the knowledge transfer and generalization abilities of pre-trained\nmodels. Inspired by the recent success of large language models like GPT [7], recent work uses\nin-context learning to avoid finetuning. It imparts general capabilities to pre-trained models with just\na few examples. PRODIGY [46] introduces an in-context learning-based model to handle various\nclassification tasks on graphs. While it can perform relation classification, it is not suitable for KG\nreasoning with a massive number of candidate entities.\nWe discuss more related work in Appendix D."}, {"title": "Problem Definition", "content": "KG Reasoning. We define a KG as K = (E, R, T), where E, R, and T denote the sets of entities,\nrelations, and facts, respectively. A fact (s, r, o) \u2208T consists of a subject entity s \u2208 E, a relation\nr \u2208 R, and an object entity o \u2208 E. Given a KG and a query fact in the form of (s, q, ?), the reasoning\ntask is to predict the missing entity from E. We refer to the relation q as a query relation.\nIn practice, we follow the convention [10] to introduce inverse relations. For each relation r \u2208 R, we\nadd its inverse relation r\u00af into the relation set and add the reverse fact (o, r\u00af, s) into the fact set.\nIn-Context KG Reasoning. In in-context reasoning, a model is pre-trained using a set of source KGs,\ndenoted by {K1, ..., Kn}. After pre-training, the model conducts reasoning on emerging KGs based\non only a few related examples without updating model parameters. Each pre-training or reasoning\nquery is prompted with some relevant examples as context.\nThe prompt is crucial for in-context learning. For each query relation q, we first randomly sample\nsome of its facts, e.g., c = (u, q, v) \u2208 \u03a4. Next, we extract a subgraph Pc = (Epmt, Rpmt, Tpmt) from\nthe KG for each example fact to construct a prompt graph. In the following, we provide a broad\ndefinition of prompt graphs, allowing for a broad design space:\nPrompt Graph. Given an example fact c = (u, q, v) in a KG K = (E,R,T), where c \u2208 T, we\ndefine its prompt graph Pc = (Epmt \u2286 E, Rpmt \u2286 R, Tpmt \u2286 T) as a subgraph of K, and c\u2208 Tpmt.\nC\nTo encode prompt graphs, we extend the KG-independent entity labeling [1] to relations and propose\na unified tokenizer, which maps entities and relations from different KGs to unified tokens:\nUnified Tokenizer. The unified tokenizer is a many-to-one mapping function. It maps entities\nand relations of different prompt graphs to the predefined tokens. Specifically, it maps each entity\nbased on the length of its shortest paths to the subject and object entities of the example fact, i.e.,\ntokenize(e) \u2190 [dist(u, e), dist(v, e)], where dist() is the length of the shortest path between two\nentities. It maps each relation to the tokens by whether it is the same as the query relation. That is,\ntokenize(r) \u2190 [same(r, q)], where same(r, q) = 1 if r is the same as q, otherwise same(r, q) = 0.\nIn Section 4.2, we assign a learnable representation for each token."}, {"title": "In-context Reasoning over KGs", "content": "The overview of the proposed model is shown in Figure 1. Given a KG and a query, we first generate\nprompt graphs for the query relation. Then, we use an encoding module to encode the prompt graphs\nand readout prompts. Finally, we incorporate the prompts into the KG reasoning process."}, {"title": "Prompt Graph Generation", "content": "The prompt graph defined in Section 3 allows for a broad design space. In this section, we introduce\na specific method for generating prompt graphs. We primarily address two challenges: (i) How to\nmake the prompt graph general for diverse KGs? (ii) How to provide valuable prompts to enhance\nreasoning? We propose a prompt graph generation pipeline to address these challenges. It involves\ntwo steps: example sampling and prompt graph extraction.\nExample sampling. For a query relation q, we first randomly sample M example facts as follows:\n$S_q = \\{c_i\\}_{i=1}^M,\\quad c_i \\sim Uniform(\\mathcal{N}_q),$\nwhere $\\mathcal{N}_q = \\{(u, r, v) | r = q \\land (u, r, v) \\in T\\}$ and $c_i = (u, q, v)$ is a q-specific example fact.\nPrompt graph extraction. The key point of the prompt graph design is highlighting information\ncrucial for query relation-specific reasoning. The example fact consists of a subject entity, an object\nentity, and the query relation between them. To depict the example subject and object entities, we\ndraw inspiration from the research on prompt-based graph model [35, 46] to use neighboring nodes\ncentered around the central node to construct prompt graphs. To abstract the semantics of query\nrelation, we include the paths between example subject and entities, considering the success of logical\nrules in KG reasoning [47\u201350]. The body of the rules involves paths between the subject and object\nentities. Therefore, given an example fact c = (u, q, v) \u2208 Sq and a KG K = {E,R, T}, we include\nthe neighboring entities of u and v and the k-hop paths between u and v in the prompt graph:\n$E_{pmt} = \\{x|\\exists(x, r, u) \\in T\\} \\cup \\{x |\\exists(x,r,v) \\in T\\}\\newline\\cup \\{x|dist(x, u) + dist(x, v) \\leq k\\},$\nwhere k is a hyperparameter denoting the maximum value of dist(x, u) + dist(x, v). As we have\nadded reverse facts, Epmt includes all 1-hop neighbors. Next, we extract the facts and relations among\nthem, i.e., Tpmt = {(s,r,o)|s \u2208 Epmt \u2227\u043e \u2208 Epmt \u2227 (s, r, o) \u2208 T} and Rpmt = {r|\u2203(s,r, o) \u2208 Tpmt}."}, {"title": "Prompt Encoding", "content": "In this section, we design a message passing neural network for prompt encoding. It comprises\nthree sub-modules: token representation, message passing, and readout. We begin by initializing the\ntoken representations of entities and relations in the given prompt graph. Subsequently, a multi-layer\nmessage passing neural network is employed to encode the prompt graph. Finally, we introduce a\nreadout sub-module to obtain the prompt representation.\nToken representations. We assign each token a learnable vector representation. Specifically,\naccording to Equation (2), the tokens for entities satisfy i + j \u2264 k, 0 \u2264 i \u2264 k \u2212 1 and 0 \u2264 j \u2264 k \u2212 1.\nTherefore, we set a representation matrix $\\mathcal{T}\\in \\mathbb{R}^{(\\frac{(k+1)(k+2)}{2}+2) - 2(k-1) \\times d}$ for entity tokens, where\n$\\frac{(k+1)(k+2)}{2} + 2) - 2(k - 1)$ denotes the total number of entity tokens. As for relations, the representation\nof token [z] is initialized as $q_{\\textit{token} . z}$, where $q_{\\textit{token}} \\in \\mathbb{R}^{1 \\times d}$ is a learnable representation. We denote\nthe input representation matrix of entities and relations for the prompt graph as $H_E^{(0)}$ and $H_R^{(0)}$,\nrespectively.\nMessage passing for prompt graph. Then, we employ an L-layers message passing neural network,\nwhich incorporates two types of aggregation: an entity-centric aggregation and a relation-centric\naggregation. In each layer, we first update the entity representations as follows:\n$H_E^{(l+1)} \\leftarrow Aggregation_e \\left( \\{Message(H_E^{(l)}, H_R^{(l)}, n, q)\\} \\right),\\newline\\forall e \\in E_{pmt}, \\forall n \\in N_e,$\nwhere Ne \u2286 Tpmt is the set of facts containing the entity e, and q is the query relation of this prompt\ngraph. Then we update the relation representations using the updated entity representations and the\nrelation representations from the previous layer:\n$H_R^{(l+1)} \\leftarrow Aggregation_r \\left( \\{Message(H_E^{(l+1)}, H_R^{(l)}, n, q)\\} \\right),\\newline\\forall r \\in R_{pmt}, \\forall n \\in N_r,$\nwhere Nr \u2286 Tpmt is the set of facts containing the relation r. Under this message passing framework,\nwe present two specific aggregation and message functions in Appendix A.1.\nReadout. After L-layers message passing on the prompt graph P, we obtain the prompt as follows:\n$H_{p} = W_{\\text{Readout}} \\left[ H_R^{(1)} || H_R^{(2)} || ... || H_R^{(L)} \\right],$\nwhere $W_{\\text{Readout}} \\in \\mathbb{R}^{d \\times Ld}$ is a learnable weight matrix. Note that the relations in different prompt\ngraphs may vary. We fill in the relations not present in the prompt graph with zero vectors to obtain\n$\\hat{H}_{p} \\in \\mathbb{R}^{|R| \\times d}$, ensuring that the shapes of every representation matrix are the same. Finally, we use\nmean-pooling to aggregate the information from multiple prompt graphs as follows:\n$H_{pmt} = \\frac{1}{|S_q|} \\sum_{c \\in S_q} H_p,$\nwhere $H_{pmt} \\in \\mathbb{R}^{|R| \\times d}$ is the prompt relation representation matrix, Sq is the set of example facts of\nthe query relation q, and Pe is the prompt graph corresponding to the example fact c. In practice, we\nparallel encode these prompt graphs to ensure efficiency."}, {"title": "In-Context KG Encoding and Reasoning", "content": "Based on the prompt encoding, we conduct reasoning on KGs. To achieve a KG-independent\nencoding, we draw inspiration from the conditional message passing neural network [3, 4, 20\u201322]\nto present a novel KG reasoning module. It separately encodes entities based on the query, rather\nthan mapping them to specific embeddings, offering us an opportunity for knowledge transfer across\ndiverse KGs. It comprises three sub-modules: initialization, KG encoding, and reasoning.\nInitialization. The input relation representations in the KG are initialized as the prompt relation\nembeddings, i.e., $V_R^{(0)} = H_{pmt}$. As for entity representations, given a query fact (s, q, x), the\nrepresentation of s is initialized as the representation of the query relation, i.e., $e_s = q$. Other entities\nare represented by zero vectors. We denote the input representation matrix of entities in KG as $V_E^{(0)}$."}, {"title": "Message Passing Architectures", "content": "Based on the framework mentioned in Section 4.2, we present two types of aggregation: entity-centric\nand relation-centric aggregations. In each layer, we first update the entity representations and then\nupdate the relation representations. Specifically, given a central entity e and the query relation q, we\nupdate the representation of e using following entity-centric aggregation function:\n$e^{(l+1)} = ReLU \\left( Max-pooling \\{m_{s,r,q} | (s, r, e) \\in N_e\\} \\right),$\n$m_{s,r,q} = \\alpha_{s,r,q} W_{msg}^{(E)} (s^{(l)} || r^{(l)} || q^{(l)}),$\n$\\alpha_{s,r,q} = \\sigma \\left( W_{attn}^{(E)} (r^{(l)} || q^{(l)}) \\right),$\nwhere $N_e \\subseteq T_{pmt}$ is the set of fact containing e. $W_{msg}^{(E)} \\in \\mathbb{R}^{d \\times 3d}$ and $W_{attn}^{(E)} \\in \\mathbb{R}^{1\\times 2d}$ are\ntwo learnable parameter matrices. $s^{(l)}, r^{(l)}, q^{(l)}$ are the representations of s, r, q in the l-th layer,\nseparately. (.||.) denotes the concatenate operation. $\u03c3(\u00b7)$ denotes the Sigmoid activation function.\nWe also adopt a query-aware attention mechanism for the relation-centric aggregation:\n$r^{(l+1)} = ReLU \\left( Max-pooling \\{m_{s,o,q} | (s, r, o) \\in N_r \\} + r^{(l)} \\right),$\n$m_{s,o,q} = W_{msg}^{(R)} (s^{(l+1)} || o^{(l+1)} || q^{(l)}),$\n$\\alpha_{s,r,q} = \\sigma \\left( W_{attn}^{(R)} (r^{(l)} || q^{(l)}) \\right),$\nwhere $N_r \\subseteq T_{pmt}$ is the set of fact containing r. $W_{msg}^{(R)} \\in \\mathbb{R}^{d \\times 3d}$ and $W_{attn}^{(R)} \\in \\mathbb{R}^{1\\times 2d}$ are two\nlearnable parameter matrices, and $o^{(l+1)} \\in H_E^{(l+1)}$ is the representation of o.\nWe also incorporate residual connection [65] and layer normalization [66] to enhance learning."}, {"title": "Implementation Details", "content": "Under the framework in Section 4, we implement an in-context reasoning model KG-ICL, which\nemploys a 5-shot 3-hop prompt graph as context, along with 3 stacked layers for prompt graph\nencoding, and 6 stacked layers for KG encoding and reasoning, i.e., M = 5, k = 3, L = 3 and\nN = 6. The dimension d of the hidden layers is set to 32. Following the standard in-context learning\nprocess [46], we first pre-train a model on source datasets and then freeze the model parameters for\nevaluation. We pre-train our model on three source datasets, i.e., FB V1 [1] with 180 relations, NELL\nV1 [1] with only 14 relations, and CoDEx-small [56] with 42 relations. We use Adam optimizer\nand set the learning rate to 0.001 and the patience of early stopping to 5. The pre-training process is\nconducted on a workstation with two Intel Xeon Gold CPUs, four NVIDIA RTX A6000 GPUs, and\nUbuntu 18.04 LTS. The pre-training model maintains a modest size with only 89k parameters, and\nthe pre-training process converges in less than six hours."}, {"title": "Related Work", "content": "Diverse KG reasoning settings. KG reasoning primarily involves three settings: transductive, induc-\ntive, and fully-inductive. Early studies [10-14] focus mainly on the transductive setting, assuming\nthat KGs are static. They learn an embedding for each specific entity, making it challenging to handle\nthe addition of new entities. Real-world KGs are dynamic, inspiring the development of inductive\nmodels [1-4, 15\u201321, 23] that allows for unseen entities. These models base their reasoning on relation\npatterns rather than entity embeddings. In the fully-inductive setting [5, 24\u201326], unseen entities and\nrelations can both emerge in the query facts. While this setting is closer to pre-training, it remains\nlimited to the same KG. The distinction among these settings arises from the fact that text data can be\nnaturally split into unified tokens, while the entity and relation sets across KGs are not shared. In this\npaper, we propose a prompt graph and a unified tokenizer to support in-context learning, breaking\ndown the barriers imposed by these settings and achieving universal reasoning capabilities.\nEntity alignment and pre-training. Extensive research efforts have been concentrated on establishing\na unified entity vocabulary to support pre-training through the recognition of identical entities in\ndifferent KGs, a task commonly known as entity alignment [18, 67\u201372]. Based on these aligned"}, {"title": "Further Analyses", "content": "The effectiveness of in-context learning is inherently tied to the quality and diversity of the source\ndatasets used for pre-training. Here, we analyze the impact of the bias of source KGs by introducing\nsix different combinations of source KGs. The results are reported in Table 4. We observe that (i)\nmore source KGs help reduce the influence of biases in individual datasets, and (ii) these three source\nKGs are of good quality, as even using just one for pre-training yields decent performance. Besides,\nwe find that our pre-training does not require a large scale of KG facts. The variety of relational\nstructures is more important for our pre-training. Thus, in practice, we can choose several KGs with\ndifferent schemata or from different domains for pre-training."}, {"title": "Incorporating Other Message Passing Layer", "content": "The proposed model can also be incorporated with other message passing neural networks that\ncan aggregate messages conditioned with specific queries. Here, we implement a variant, KG-ICL\n(NBFNet), by incorporating the message passing of NBFNet [4], which is used by ULTRA [6]. Note\nthat NBFNet only outputs entity representations but not updates or outputs relation representations,\nwhich is also one reason we did not adopt NBFNet initially. ULTRA treats relations as nodes\nto obtain relation representations. Therefore, we incorporate Equation (4) into NBFNet (default\nconfiguration) to support relation encoding. The results are shown in Table 7. We can observe that\nKG-ICL (NBFNet) also achieved promising results, slightly below KG-ICL. This demonstrates the\npotential of combining KG-ICL with more passing message neural networks. Moreover, the structure\nof this variant is similar to ULTRA, but the input is prompt graphs rather than relation graphs, which\nindicates the superiority of our prompt graph to that of ULTRA's relation graph in relation modeling."}, {"title": "Dataset Statistics", "content": "We conduct extensive evaluations on 43 datasets. We categorize the datasets into three types: inductive\ndatasets, fully-inductive datasets, and transductive datasets. The statistical data of these datasets and\ntheir state-of-the-art models are reported in Tables 8, 9, and 10, respectively."}, {"title": "Detailed Results", "content": "To validate the effectiveness of our in-context reasoning model, we compare KG-ICL with the\nsupervised SOTA models and ULTRA's pre-training and finetuning versions on 43 datasets. The\ndetailed results for each dataset are presented in Table 11. We observe that (i) KG-ICL outperforms\nthe competitors on most datasets, demonstrating the universal reasoning capability of our in-context\nmodel. (ii) \"KG-ICL pre-train\" outperforms \u201cULTRA pre-train\" on 11 inductive datasets, all 13\nfully-inductive datasets, and 9 transductive datasets, which demonstrates the superiority of our in-context KG reasoning foundation model. In addition, \u201cKG-ICL finetune\" also outperforms \u201cULTRA\nfinetune\" on most datasets. (iii) InGram [5] transfers knowledge to new query relations through a\nrelation graph, while we employ the prompt graph as a bridge for knowledge transfer. Our KG-ICL\noutperforms InGram on all 13 fully-inductive datasets, indicating that our prompt graphs can better\nhighlight important clues for specific query relations than relation graphs. (iv) On transductive\ndatasets, KG-ICL's performance improvement compared to supervised baseline models is less than\nthat in the previous two settings. There are two reasons for this: first, the supervised signals on\ntransductive datasets directly target entities and relations in the test set, allowing supervised models to\neffectively learn representations and achieve high performance. Second, most existing KG reasoning\nmodels are developed based on several transductive datasets such as FB15k-237 [53], WN18RR [12],\nYAGO3-10 [55], and NELL-995 [54]. Models specifically designed for these datasets also contribute"}, {"title": "Limitations", "content": "The evaluations on 43 datasets demonstrate the proposed in-context KG foundation model's perfor-\nmance and generalization across transductive and inductive settings. Nonetheless, there are several\nlimitations and open questions. Some KGs have special facts in addition to the mainstream triple\nfacts involving subject, object entities, and their relations. These include facts with time stamps and\nfacts containing multi-relational aspects. The foundation model for these special KGs also deserves\nattention. Scalability is an open challenge faced by existing KG reasoning models. Our proposed\nmodel addresses this by extracting a few prompt graphs with a small scale to represent relations,\nwhich has been demonstrated as a scalable approach in Appendix E.2. Our evaluations on large-scale\ndatasets containing millions of facts also confirm its scalability. In future work, we plan to further\nenhance the scalability by incorporating strategies such as pruning and parallelization."}, {"title": "Broader Impacts", "content": "Our work seeks to build a KG foundation model with effective, efficient, and transferable reasoning\ncapabilities over unseen entities, relations, and even previously unseen KGs, all without requiring\nretraining from scratch. We believe that the proposed model has the potential to be applied in broad\nknowledge-driven applications, such as question-answering and recommender systems. Its ability to\nadapt to changes in the graph and generalize to unseen data will be beneficial in addressing issues\nsuch as cold start. Nevertheless, excessive reliance on knowledge from pre-training data and a few\nexamples may lead to societal biases and unfairness. We have discussed the quality and potential\nimpacts of the pre-training data in Appendix E.1. In practical applications, we also should carefully\ndesign example selection strategies to avoid potential societal biases and unfairness."}]}