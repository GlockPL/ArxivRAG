{"title": "CBF-LLM: Safe Control for LLM Alignment", "authors": ["Yuya Miyaoka", "Masaki Inoue"], "abstract": "This paper proposes a control-based framework for aligning large language models (LLMs) by leveraging a control barrier function (CBF) to ensure user-desirable text generation. The presented framework applies the safety filter, designed based on the CBF, to the output generation of the baseline LLM, i.e., the sequence of the token, with the aim of intervening in the generated text. The overall text-generation system is implemented with Llama 3 and a RoBERTa model, and the source code is available at https://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control ability and effectiveness in reducing the number of interventions needed for user-specified alignment tasks.", "sections": [{"title": "1 Introduction", "content": "While large language models (LLMs) are known to have strong language understanding and generation abilities, they can also generate harmful, biased, and toxic content [1][2]. Alignment of LLMs ensures that they generate content that is \"desirable\" for the user, typically meaning content that is safe and ethical. Various approaches for LLM alignment have been presented ([1], [2], [3] and reference therein).\nThe major approach to the alignment is reinforcement learning from human feedback (RLHF) [4], where a reward model is constructed by human feedback and used for the training of LLMs. Variants of RLHF architectures are also proposed, such as Safe-RLHF [5], SENSEI [6], and f-DPG [7], and their implementations are presented, such as training pre-trained LLMs [8] [9], and applications like information-seeking chatbot [10]. Collecting human feedback with data is time-consuming and expen-sive. To overcome the drawback, RL from AI Feedback (RLAIF) is presented in [11] instead of using human labels. In addition, the method to construct the training data automatically is proposed in [12]. Furthermore, to reduce the computational cost, direct preference optimization (DPO) is proposed [13], where the training data is directly used for training LLMs without accessing the reward model. Super-vised fine-tuning (SFT) is a different approach for alignment from RLHF, as studied in [14]. A common feature of alignment methods like RLHF and SFT is that they modify LLMs' model parameters.\nAn alternative approach for LLM alignment is to directly intervene in the input prompt or output of LLMs, rather than modifying the model parameters. In-context learning (ICL) [15] is a major approach for intervening in the input prompt. In ICL, a few demonstrations are provided in prompt to instruct the LLMs on the task, including few-shot learning [16] [17]. As the methods for intervening in the output, the work [18] proposes a method to format output for retrieval application, and the work [19] proposes a repetition penalty to prevent LLMs from repeating the same words and expressions. In addition, Trans-formers module provides some functions to modify the output, such as NoBadWordsLogitsProcessor and MinLengthLogitsProcessor [20].\nOne can view that the intervention approach to alignment, which avoids undesirable output, as analogous to \"collision avoidance\", the most fundamental problem in control engineering. In control engineering, various studies are conducted for safety assurance, including collision avoidance [21][22][23]. A promising approach for collision avoidance is the control barrier function (CBF), as studied in the-oretical works [24] [25] [26] and in real-world applications [27]. An analogy between vehicle collision avoidance and intervention-based LLM alignment can be drawn as illustrated in Fig. 1. The goal of ve-hicle collision avoidance is to prevent collisions with obstacles by intervening in the vehicle's trajectory. For example, if there are obstacles ahead of the vehicle, it is necessary to operate the steering or use the brakes to avoid colliding with them. Similarly, the goal of LLM alignment is to prevent undesirable"}, {"title": "2 Preliminary", "content": "2.1 Control Barrier Function for Safe Control\nControl barrier function (CBF), developed in the control community, provides safety assurance in control systems [24]. This section briefly reviews CBF and CBF-based safety control.\nConsider the following dynamical system to be controlled:\n$$\\dot{x} = g(x, u),$$\nwhere $x \\in \\mathbb{R}^{n}$ is the state variable of the object being controlled, and $u \\in \\mathbb{R}^{m}$ is the action applied to the object. The function $g$ represents the system dynamics; how this object is affected by the current state $x$ and action $u$. A typical example of the system (1) includes a vehicle dynamics, where the state $x$ is coordinate, velocity, angle, etc, and the action $u$ is accelerator pedal depression, steering angle, etc.\nWe aim to design the assisted control system with safety assurance. As for safety, we let the safe and unsafe sets be denoted by $S \\subseteq \\mathbb{R}^{n},\\hat{S} \\subseteq \\mathbb{R}^{n}$, respectively. Then, the safety means to constrain $x$ within the safe set $S$, i.e., $x \\in S$. Consider that the nominal action $u_{nom} \\in \\mathbb{R}^{m}$ is provided which might violate the safety, i.e., $u_{nom}$ might generate the unsafe state $x \\in \\hat{S}$. Then, we address the problem of designing \"safety filter\" $F: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{m}$, as follows:\nProblem 1 (Safety Filter). Find the safety filter $F : \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{m}$ such that the system (1) with $u = F(u_{nom})$ generates $x(t) \\in S$ for all nominal actions $u_{nom}$ for all time $t$.\nAs a preliminary, we design a continuously differentiable function $h: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$, called a \"constraint function\", such that\n$$\nh(x) \\geq 0, x \\in S,\n$$$$\nh(x) < 0, x \\in \\hat{S}\n$$\nhold. The safety is equivalent to the constraint: $h(x) \\geq 0$. In the assisted driving example, the nominal action $u_{nom}$ is a manual action by the driver, and the unsafe set $\\hat{S}$ includes the positions of obstacles like pedestrians. The function $h$ can be the distance between the vehicle and the obstacle. With the manual action by the driver $u_{nom}$, the vehicle may enter the unsafe set, such as colliding with obstacles. The safety filter $F$ needs to modify $u_{nom}$ to output $u$ such that $h(x) \\geq 0$ holds, i.e., the trajectory in which the vehicle never collides with obstacles.\nTo construct the safety filter $F$ that keeps the safety constraint $h(x) \\geq 0$, the control barrier function filter (CBF filter, [30] [24]) is presented. The CBF filter intervenes in the nominal action value $u_{nom}$ to introduce a safe state of the object as follows:\n$$\nu = \\arg \\min_{u} || u_{nom} - u ||^{2},$$$$\\text{s.t.} \\dot{h}(x) \\geq -\\alpha(h(x)),$$\nwhere $\\alpha : \\mathbb{R} \\rightarrow \\mathbb{R}$ is a class-K function which holds $h(0) = 0$ and monotonically increasing, i.e., $\\frac{dh(x)}{dx} > 0$. The function $h$ is called a control barrier function if there exists $u$ such that the constraint (5) holds. In addition, (5) is called a CBF constraint. In the assisted driving example, when the manual action $u_{nom}$ is expected to cause a collision with an obstacle, the CBF filter intervenes in $u_{nom}$ to provide safe driving. To constrain the action $u$ by the CBF filter, the following statement holds:\nTheorem 1. The state $x$ of the system (1) is in the safe set, i.e., $x \\in S$ for all time if $h$ is a control barrier function and the action $u$ satisfies the CBF constraint (5)."}, {"title": "2.2 Text generation by Large Language Models", "content": "This section reviews and analyses the text generation by large language models (LLMs) while par-ticularly focusing on their structure. In this paper, \"text\" means the sequence of tokens and $X$ denotes the set of the texts. The text corresponding to a specific expression in natural language is displayed as $x = x$('<text>'). For example, the text $x \\in X$ for \"Have a nice day.\" is displayed as $x$(\"Have a nice day.\"). Each token $t$ is identified by a positive integer, i.e. $t \\in \\{1, . . ., N\\} =: T$, and $N$ is the number of tokens the LLM has. The token corresponding to a specific expression in natural language is displayed as a numerical constant $t_{<token>}$. For example, the token for \"dog\" is displayed as $t_{dog}$. The function $concat : X \\times T \\rightarrow X$ concatenates text and token to output a text. For example, $concat(x$(\"Have a nice\"), $t_{day}$) = $x$(\"Have a nice day\").\nText generation is performed by iteratively adding a new token, starting from the initial text. The text generation by LLM is considered to be a discrete-time dynamical system as shown in the block diagram Fig. 2. In this figure, the token predictor $G : X \\rightarrow \\mathbb{R}^{N}$ plays a central role in the text"}, {"title": "3 CBF-LLM", "content": "This section presents the control-based alignment of text-generation systems and their detailed imple-mentation. Symbols and their meanings used in this paper are summarized in Table 1.\nThe alignment discussed in this paper aims to ensure desirable text generation by weak intervention to the output of LLMs. To clarify the meaning of \"desirable\", we let the desirable and undesirable text sets be $S\\subset X$ and $\\hat{S} \\subseteq X$, respectively, based on the respective alignment goals.\nExample 1. Suppose that the alignment goal is set to generate non-toxic content. Then, $S$ is the set of non-toxic text, and $\\hat{S}$ is the set of toxic texts. For mathematical procedures, we consider $S$ and $\\hat{S}$ to represent \"all\" non-toxic and toxic text samples. More examples of $S$ and $\\hat{S}$ are seen in Section 4."}, {"title": "4 Experiment", "content": "To demonstrate the CBF-LLM's alignment ability and the intervention time, we implement CBF-LLM by pre-trained large language models.\n4.1 Setting\nIn the experiment, the alignment goal is to ensure that the text-generation system, illustrated as in Fig. 3, does not produce any \u201cnon-positive\" output. To this end, we let $S$ and $\\hat{S}$ denote the set of positive texts and non-positive texts, respectively. We employ Llama 3 8b [33] as the model for the token predictor G, and cardiffnlp/twitter_roberta_base_sentiment_latest [32], a sentiment analysis ROBERTa model, as the language-constraint function (L-CF) h. The internal model of the L-CF was originally trained to classify sentences into 3 labels: negative, neutral, or positive [32]. In this experiment, we apply the following mapping to construct the L-CF:\n$$\n\\begin{cases}\ns = \\text{softmax}(M(x)), \\\\\nh(x) = s[3] - \\text{max}(s[1], s[2]),\n\\end{cases}\n$$\nwhere $M : X \\rightarrow \\mathbb{R}^{3}$ is the internal model and $s[1]$, $s[2]$, and $s[3]$ represent the score of negative, neutral, and positive, respectively. The function h outputs a positive value when the sentiment of the text x is positive. It means that the text-generation system would be controlled to generate positive content.\nIn the text generation experiment with CBF-LLM, we applied each of the following filters as F.\nCBF($\\alpha = 0.3$) Filter with the CBF with hyperparameter $\\alpha = 0.3$, which implies that the executable area is small, in other words, the CBF constraint (6) is strict. The algorithm is shown in Algo-rithm 2.\nCBF($\\alpha = 0.8$) Filter with the control barrier function with hyperparameter $\\alpha = 0.8$, which implies that the executable area is wider, in other words, the CBF constraint (6) is more moderate. The algorithm is shown in Algorithm 2."}, {"title": "5 Conclusion", "content": "This paper proposed the control-based LLM alignment framework, called CBF-LLM. This framework utilizes the control barrier function (CBF), commonly used in control engineering to ensure the safety of physical objects, such as the collision avoidance function in assisted driving vehicles. Based on an analogy between the control theory and the LLM alignment task, We employ the CBF-based safety filter to ensure that the text-generation system generates desirable content. The key feature of CBF-LLM is that the CBF filter is an add-on to the baseline LLM: it intervenes in the output of the baseline LLM without any additional training of LLMs.\nThis paper also presented the implementation of CBF-LLM by Llama 3 and a sentiment analy-sis ROBERTa model to ensure that the text-generation system generates positive content. The text-generation experiment showed that CBF-LLM successfully achieves the alignment goal with fewer in-terventions than the blacklist method.\nThe future works include conducting text-generation experiments with other LLMs and other align-ment goals, and theoretical insights on the number of interventions by the CBF filter of CBF-LLM."}, {"title": "A Nominal Text Generation with Top-k Sampling", "content": "Recall that Fig. 3 shows the text-generation system and intervention procedure in block diagram format. To represent general top-k sampling with no control in this figure, the filter FNC is provided, as shown in Algorithm 3."}]}