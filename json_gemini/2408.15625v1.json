{"title": "CBF-LLM: Safe Control for LLM Alignment", "authors": ["Yuya Miyaoka", "Masaki Inoue"], "abstract": "This paper proposes a control-based framework for aligning large language models (LLMs) by leveraging a control barrier function (CBF) to ensure user-desirable text generation. The presented framework applies the safety filter, designed based on the CBF, to the output generation of the baseline LLM, i.e., the sequence of the token, with the aim of intervening in the generated text. The overall text-generation system is implemented with Llama 3 and a RoBERTa model, and the source code is available at https://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control ability and effectiveness in reducing the number of interventions needed for user-specified alignment tasks.", "sections": [{"title": "1 Introduction", "content": "While large language models (LLMs) are known to have strong language understanding and generation abilities, they can also generate harmful, biased, and toxic content [1][2]. Alignment of LLMs ensures that they generate content that is \"desirable\" for the user, typically meaning content that is safe and ethical. Various approaches for LLM alignment have been presented ([1], [2], [3] and reference therein).\nThe major approach to the alignment is reinforcement learning from human feedback (RLHF) [4], where a reward model is constructed by human feedback and used for the training of LLMs. Variants of RLHF architectures are also proposed, such as Safe-RLHF [5], SENSEI [6], and f-DPG [7], and their implementations are presented, such as training pre-trained LLMs [8] [9], and applications like information-seeking chatbot [10]. Collecting human feedback with data is time-consuming and expensive. To overcome the drawback, RL from AI Feedback (RLAIF) is presented in [11] instead of using human labels. In addition, the method to construct the training data automatically is proposed in [12]. Furthermore, to reduce the computational cost, direct preference optimization (DPO) is proposed [13], where the training data is directly used for training LLMs without accessing the reward model. Supervised fine-tuning (SFT) is a different approach for alignment from RLHF, as studied in [14]. A common feature of alignment methods like RLHF and SFT is that they modify LLMs' model parameters.\nAn alternative approach for LLM alignment is to directly intervene in the input prompt or output of LLMs, rather than modifying the model parameters. In-context learning (ICL) [15] is a major approach for intervening in the input prompt. In ICL, a few demonstrations are provided in prompt to instruct the LLMs on the task, including few-shot learning [16] [17]. As the methods for intervening in the output, the work [18] proposes a method to format output for retrieval application, and the work [19] proposes a repetition penalty to prevent LLMs from repeating the same words and expressions. In addition, Transformers module provides some functions to modify the output, such as NoBadWordsLogitsProcessor and MinLengthLogitsProcessor [20].\nOne can view that the intervention approach to alignment, which avoids undesirable output, as analogous to \"collision avoidance\", the most fundamental problem in control engineering. In control engineering, various studies are conducted for safety assurance, including collision avoidance [21][22][23]. A promising approach for collision avoidance is the control barrier function (CBF), as studied in theoretical works [24] [25] [26] and in real-world applications [27]. An analogy between vehicle collision avoidance and intervention-based LLM alignment can be drawn as illustrated in Fig. 1. The goal of vehicle collision avoidance is to prevent collisions with obstacles by intervening in the vehicle's trajectory. For example, if there are obstacles ahead of the vehicle, it is necessary to operate the steering or use the brakes to avoid colliding with them. Similarly, the goal of LLM alignment is to prevent undesirable"}, {"title": "2 Preliminary", "content": "proposed. In Section 4, the experiment of CBF-LLM is conducted. Finally, in Section 5, the conclusion of this paper is presented.\nNotation: symbol V[i] represents the i-th element of the vector V."}, {"title": "2.1 Control Barrier Function for Safe Control", "content": "Control barrier function (CBF), developed in the control community, provides safety assurance in control systems [24]. This section briefly reviews CBF and CBF-based safety control.\nConsider the following dynamical system to be controlled:\n$\\dot{x} = g(x, u),$\nwhere $x \\in R^{n'}$ is the state variable of the object being controlled, and $u \\in R^m$ is the action applied to the object. The function g represents the system dynamics; how this object is affected by the current state x and action u. A typical example of the system (1) includes a vehicle dynamics, where the state x is coordinate, velocity, angle, etc, and the action u is accelerator pedal depression, steering angle, etc.\nWe aim to design the assisted control system with safety assurance. As for safety, we let the safe and unsafe sets be denoted by $S \\subseteq R^n$, $\\hat{S} \\subseteq R^n$, respectively. Then, the safety means to constrain x within the safe set S, i.e., $x \\in S$. Consider that the nominal action $u_{nom} \\in R^m$ is provided which might violate the safety, i.e., $u_{nom}$ might generate the unsafe state $x \\in \\hat{S}$. Then, we address the problem of designing \"safety filter\" $F: R^m \\rightarrow R^m$, as follows:\nProblem 1 (Safety Filter). Find the safety filter $F : R^m \\rightarrow R^m$ such that the system (1) with $u = F(u_{nom})$ generates $x(t) \\in S$ for all nominal actions $u_{nom}$ for all time t.\nAs a preliminary, we design a continuously differentiable function $h: R^n \\rightarrow R$, called a \"constraint function\", such that\n$\\begin{cases} h(x) \\geq 0, & x \\in S, \\\\ h(x) < 0, & x \\in \\hat{S}.\\end{cases}$\nhold. The safety is equivalent to the constraint: $h(x) \\geq 0$. In the assisted driving example, the nominal action $u_{nom}$ is a manual action by the driver, and the unsafe set $\\hat{S}$ includes the positions of obstacles like pedestrians. The function h can be the distance between the vehicle and the obstacle. With the manual action by the driver $u_{nom}$, the vehicle may enter the unsafe set, such as colliding with obstacles. The safety filter F needs to modify $u_{nom}$ to output u such that $h(x) \\geq 0$ holds, i.e., the trajectory in which the vehicle never collides with obstacles.\nTo construct the safety filter F that keeps the safety constraint $h(x) \\geq 0$, the control barrier function filter (CBF filter, [30] [24]) is presented. The CBF filter intervenes in the nominal action value $u_{nom}$ to introduce a safe state of the object as follows:\n$\\begin{aligned} u &= \\underset{u}{arg \\min} (u_{nom} - u)^2, \\\\ s.t. \\dot{h}(x) &\\geq -\\alpha(h(x)),\\end{aligned}$\nwhere $\\alpha : R \\rightarrow R$ is a class-K function which holds $h(0) = 0$ and monotonically increasing, i.e., $\\frac{d\\alpha(h(x))}{dx} > 0$. The function h is called a control barrier function if there exists u such that the constraint (5) holds. In addition, (5) is called a CBF constraint. In the assisted driving example, when the manual action $u_{nom}$ is expected to cause a collision with an obstacle, the CBF filter intervenes in $u_{nom}$ to provide safe driving. To constrain the action u by the CBF filter, the following statement holds:\nTheorem 1. The state x of the system (1) is in the safe set, i.e., $x \\in S$ for all time if h is a control barrier function and the action u satisfies the CBF constraint (5)."}, {"title": "2.2 Text generation by Large Language Models", "content": "Remark 1. The objective function (4) ensures that the filtered action u remains as close as possible to the nominal action $u_{nom}$. In this sense, the CBF filter archives safety by the \"minimum\" intervention.\nThe CBF filter is capable of applying in discrete-time systems by re-formulating the CBF constraint (5) as follows [31]:\n$\\Delta h(k) = h(k + 1) - h(k) \\geq -\\alpha(h(k)),$\nwhere k is a discrete time.\nThis section reviews and analyses the text generation by large language models (LLMs) while particularly focusing on their structure. In this paper, \"text\" means the sequence of tokens and X denotes the set of the texts. The text corresponding to a specific expression in natural language is displayed as x = x(\"<text>\"). For example, the text $x \\in X$ for \"Have a nice day.\" is displayed as x(\"Have a nice day.\"). Each token t is identified by a positive integer, i.e. $t \\in \\{1, ..., N\\} =: T$, and N is the number of tokens the LLM has. The token corresponding to a specific expression in natural language is displayed as a numerical constant $t_{<token>}$. For example, the token for \"dog\" is displayed as $t_{dog}$. The function $concat : X \\times T \\rightarrow X$ concatenates text and token to output a text. For example, concat(x(\"Have a nice\u201d), $t_{day}$) = x(\u201cHave a nice day\u201d).\nText generation is performed by iteratively adding a new token, starting from the initial text. The text generation by LLM is considered to be a discrete-time dynamical system as shown in the block diagram Fig. 2. In this figure, the token predictor $G: X \\rightarrow R^N$ plays a central role in the text"}, {"title": "3 CBF-LLM", "content": "This section presents the control-based alignment of text-generation systems and their detailed implementation. Symbols and their meanings used in this paper are summarized in Table 1.\nThe alignment discussed in this paper aims to ensure desirable text generation by weak intervention to the output of LLMs. To clarify the meaning of \"desirable\", we let the desirable and undesirable text sets be $S \\subset X$ and $\\hat{S} \\subseteq X$, respectively, based on the respective alignment goals.\nExample 1. Suppose that the alignment goal is set to generate non-toxic content. Then, S is the set of non-toxic text, and $\\hat{S}$ is the set of toxic texts. For mathematical procedures, we consider S and $\\hat{S}$ to represent \"all\" non-toxic and toxic text samples. More examples of S and $\\hat{S}$ are seen in Section 4."}, {"title": "4 Experiment", "content": "One simple idea of achieving the alignment goal is to force the text generation to stop when the generated text x turns undesirable, i.e., $x \\in \\hat{S}$, but this method involves a strong intervention in the baseline LLM, which renders the original capabilities of the baseline LLM meaningless. To overcome the drawback, we make the intervention strength adjustable, which enables the text-generation system to achieve the alignment goal with a weak intervention.\nThe presented text-generation system, including an LLM and a safety filter, which is constructed based on the CBF described in Section 2.1. The overall system is called CBF-LLM and its structure is shown in Fig. 3.\nRecall that the token predictor G mainly implies a generative language model such as LLM. It retrieves the current text $x \\in X$ and outputs the probability of the next token, $P \\in R^N$. For each token $t \\in T$, the probability P[t] indicates how probable each token t is followed after the text x.\nThe defining feature of the CBF-LLM is the presence of filter F, which filters P to generate the modified probability $Q \\in R^N$. The filter F is designed to ensure the desirable text generation, i.e., $x \\in S$. To this end, we design the CBF filter in F by using the function $h: X \\rightarrow R$ such satisfying\n$\\begin{cases} h(x) \\geq 0, & x \\in S, \\\\ h(x) < 0, & x \\in \\hat{S}.\\end{cases}$\nThe function h is called the \"language-constraint function\" (L-CF). Note that the L-CF h needs to be designed to distinguish between the desired and undesired texts accurately. We also assume that the value of L-CF h(x) changes depending on the content of the text x. Suppose that the alignment goal is set to generate non-toxic content. Then, prepare multiple samples of non-toxic text and toxic text and train a classification language model to prepare the L-CF h. Specifically, the model needs to learn the relationship between non-toxic text and toxic text. We use L-CF h(x) to determine whether the text x is toxic. Furthermore, the value of L-CF is expected to indicate how toxic the text is. An example of constructing the L-CF is provided as follows."}, {"title": "4.1 Setting", "content": "Example 2. To construct L-CF, we apply a sentiment analysis ROBERTa model \u00b9as the internal model. The RoBERTa model is originally trained to classify sentences into 3 labels: negative, neutral, or positive [32]. Let $M: X \\rightarrow R^3$ denote the RoBERTa model, and $s \\in [0,1]^3$ denotes the softmax output of the M respect to a text x, i.e., s = softmax(M(x)). It follows that s[1], s[2], s[3] represent the score of negative, neutral, and positive, respectively. Then, L-CF is constructed as follows:\n$h(x) = s[3] - max(s[1], s[2]).$\nThe function h outputs a positive value when the positive score is greater than both negative and neutral scores, while it outputs a negative value when either the negative or neutral score is greater than the positive score. In other words, the sets S and $\\hat{S}$, which correspond to the L-CF constructed above, render the positive and non-positive texts, respectively.\nThe filter $F: R^N \\rightarrow R^N$ allows only tokens that meet its conditions to pass through and do not allow tokens that do not. In this paper, the CBF filter discussed in Section 2.1 is employed in F and is denoted by $F_{CBF}$. The detailed realization of the CBF filter is given as follows:\n$F_{CBF}(P; x): P'[t] =\\begin{cases} P[t], & h(concat(x, t)) - h(x) \\geq -\\alpha h(x), \\\\ 0, & else\\end{cases}, t \\in T,$\nwhere $\\alpha$ is a hyperparameter. This formulation is a modified form of the discrete-time CBF inequality, as shown in (6). In (11), the probability of the token is set to 0 unless the token satisfies the CBF inequality, which guarantees that the generated text x always satisfies that $x \\in S$.\nRemark 2. The hyperparameter in the CBF filter, $\\alpha \\in [0,1]$ implies the strictness of the safety constraint (6). In other words, the value determines the degree to which the generated text is allowed to approach the boundary of the safety constraint, $x \\in S$. The CBF filter with $\\alpha = 1$ is the mildest: it always allows the text unless the given text is in the undesirable set, i.e., $x \\in \\hat{S}$, while the CBF filter with $\\alpha = 0$ is the most strict: it only allows if the text x(k) is more desirable than the one in the previous time x(k-1). In the CBF filter-based control in the assisted vehicle, the value of $\\alpha$ affects the safety margin from an obstacle.\nTo improve the computational efficiency, top-k sampling is applied in the filter F. The top-k sampling only processes a much smaller number of elements than N elements of the target P. The algorithm of the CBF filter with top-k sampling is provided in Algorithm 2. In the algorithm, we call that the token t is allowed (disallowed) if the CBF inequality (6) holds (does not hold) at the current text x.\nThe normalizer R adjusts the output of the filter P' to ensure that it is normalized, such that the sum of the output Q equals 1.\n$R: Q[i] = \\frac{P'[i]}{\\sum_{i=1}^{N} P'[i]}, i \\in \\{1, ..., N\\}$\nThe CBF-LLM given in Fig. 3 with the identity filter F(P) = P instead of the CBF filter $F_{CBF}$ is reduced the nominal text-generation system, given in Fig. 2. The algorithm of the nominal text-generation system with top-k sampling is stated in Appendix A, and it is implemented in Section 4 for comparison with CBF-LLM.\nRemark 3. We emphasize that the CBF-LLM, the text-generation system presented in Fig. 3, provides a framework for aligning LLMs in a different manner from traditional approaches [4] [1] [15]. In traditional approaches, training datasets are usually utilized to train reward models in RLHF [4], LLMs in SFT [1], or directly put on the prompt in the in-context learning [15]. The datasets tell what the desirable texts and undesirable texts are. In CBF-LLM, the role of the datasets is taken over by the L-CF, as shown in (9). The L-CF is assumed to accurately classify the text into desirable S or undesirable $\\hat{S}$, based on the respective alignment task purpose."}, {"title": "4.2 Result", "content": "To demonstrate the CBF-LLM's alignment ability and the intervention time, we implement CBF-LLM by pre-trained large language models.\nIn the experiment, the alignment goal is to ensure that the text-generation system, illustrated as in Fig. 3, does not produce any \u201cnon-positive\" output. To this end, we let S and $\\hat{S}$ denote the set of positive texts and non-positive texts, respectively. We employ Llama 3 8b [33] as the model for the token predictor G, and cardiffnlp/twitter_roberta_base_sentiment_latest [32], a sentiment analysis ROBERTa model, as the language-constraint function (L-CF) h. The internal model of the L-CF was originally trained to classify sentences into 3 labels: negative, neutral, or positive [32]. In this experiment, we apply the following mapping to construct the L-CF:\n$\\begin{cases} s = softmax(M(x)), \\\\ h(x) = s[3] - max(s[1], s[2]),\\end{cases}$\nwhere $M : X \\rightarrow R\u00b3$ is the internal model and s[1], s[2], and s[3] represent the score of negative, neutral, and positive, respectively. The function h outputs a positive value when the sentiment of the text x is positive. It means that the text-generation system would be controlled to generate positive content.\nIn the text generation experiment with CBF-LLM, we applied each of the following filters as F.\nCBF($\\alpha$ = 0.3) Filter with the CBF with hyperparameter $\\alpha$ = 0.3, which implies that the executable area is small, in other words, the CBF constraint (6) is strict. The algorithm is shown in Algorithm 2.\nCBF($\\alpha$ = 0.8) Filter with the control barrier function with hyperparameter $\\alpha$ = 0.8, which implies that the executable area is wider, in other words, the CBF constraint (6) is more moderate. The algorithm is shown in Algorithm 2.\nNoControl No filtering is applied to probabilities P is performed, i.e., F is the identity map and the proposed text-generation system as shown in Fig. 3 is reduced to the traditional one, as shown in Fig. 2. The system is expected to be operated only by the baseline model, Llama 3. The algorithm is shown in Appendix A.\nIn this experiment, we set the initial text as \"Everyone says you will be a good researcher in the future, but\", temperature as T = 1, top-k value as $k_{top}$ = 30, and the maximum number of new tokens as 30. We generated 100 samples.\nThe following are examples of generated texts by each filter. The red slash/indicates the line break."}, {"title": "5 Conclusion", "content": "This paper proposed the control-based LLM alignment framework, called CBF-LLM. This framework utilizes the control barrier function (CBF), commonly used in control engineering to ensure the safety of physical objects, such as the collision avoidance function in assisted driving vehicles. Based on an analogy between the control theory and the LLM alignment task, We employ the CBF-based safety filter to ensure that the text-generation system generates desirable content. The key feature of CBF-LLM is that the CBF filter is an add-on to the baseline LLM: it intervenes in the output of the baseline LLM without any additional training of LLMs.\nThis paper also presented the implementation of CBF-LLM by Llama 3 and a sentiment analysis ROBERTa model to ensure that the text-generation system generates positive content. The text-generation experiment showed that CBF-LLM successfully achieves the alignment goal with fewer interventions than the blacklist method.\nThe future works include conducting text-generation experiments with other LLMs and other alignment goals, and theoretical insights on the number of interventions by the CBF filter of CBF-LLM."}, {"title": "Appendix", "content": "Recall that Fig. 3 shows the text-generation system and intervention procedure in block diagram format. To represent general top-k sampling with no control in this figure, the filter $F_{NC}$ is provided, as shown in Algorithm 3."}]}