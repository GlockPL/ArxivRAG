{"title": "HiSC4D: Human-centered interaction and 4D Scene Capture in Large-scale Space Using Wearable IMUs and LIDAR", "authors": ["Yudi Dai", "Zhiyong Wang", "Xiping Lin", "Chenglu Wen", "Lan Xu", "Siqi Shen", "Yuexin Ma", "Cheng Wang"], "abstract": "We introduce HiSC4D, a novel Human-centered interaction and 4D Scene Capture method, aimed at accurately and efficiently creating a dynamic digital world, containing large-scale indoor-outdoor scenes, diverse human motions, rich human-human interactions, and human-environment interactions. By utilizing body-mounted IMUs and a head-mounted LiDAR, HISC4D can capture egocentric human motions in unconstrained space without the need for external devices and pre-built maps. This affords great flexibility and accessibility for human-centered interaction and 4D scene capturing in various environments. Taking into account that IMUs can capture human spatially unrestricted poses but are prone to drifting for long-period using, and while LiDAR is stable for global localization but rough for local positions and orientations, HiSC4D employs a joint optimization method, harmonizing all sensors and utilizing environment cues, yielding promising results for long-term capture in large scenes. To promote research of egocentric human interaction in large scenes and facilitate downstream tasks, we also present a dataset, containing 8 sequences in 4 large scenes (200 to 5,000 m\u00b2), providing 36k frames of accurate 4D human motions with SMPL annotations and dynamic scenes, 31k frames of cropped human point clouds, and scene mesh of the environment. A variety of scenarios, such as the basketball gym and commercial street, alongside challenging human motions, such as daily greeting, one-on-one basketball playing, and tour guiding, demonstrate the effectiveness and the generalization ability of HiSC4D. The dataset and code will be publicly available for research purposes.", "sections": [{"title": "INTRODUCTION", "content": "THE overwhelming development of digital society greatly enriched people's lives from entertainment to education, and from commerce to art. In many downstream areas, like autonomous driving, robotics, Augmented/Virtual/Mixed Reality, etc., there is a growing prominence of high-quality human-centric 4D content, which is essential for human action recognition, social-behavioral analysis, scene perception, and AI-generated content area. In the relevant research areas, it is a trend and cutting- edge to capture interacting 4D human motions in large real-world environments from an egocentric view. However, capturing such content accurately and robustly remains a substantial challenge, due to the complexities of capturing systems and challenging interaction patterns, including both human-to-human and human- environment interactions.\nInertial Measurement Unit (IMU) [1] sensors have been fre- quently used in full-body human capture. It is scene-free and easy to mount on different human body parts, such as limbs, torso, and head. IMU can be of high sensitivity and short-term accurate motions. However, due to the sensor noise and offsets, the IMU- based motion capture system suffers from severe integration drift as the acquisition time increases. Therefore, building a drift-free motion-capturing system applied in large capture areas is the main challenge for IMU sensors. Some methods [2], [3], [4], [5], [6], [7] utilize extra external RGB or RGBD cameras to improve the accuracy but result in limited capture space, human activities, and interactions. PROX [7] leverages scene information to optimize human poses estimated from an RGBD camera. However, the static camera setup is not suitable for large-scale environments. HPS [8] uses a head-mounted camera to complement IMUs in global localization. However, it requires pre-built maps and a huge image database for self-localization. EgoLocate [9] removes the reliance on pre-built maps by utilizing the head-mounted camera for global localization and sparse scene reconstruction. However, RGB-related methods tend to degenerate when human movement is highly dynamic or the scene lacks texture.\nLiDAR has become widely used in the field of robotics and autonomous vehicles [10], [11], [12], [13], [14]. Owing to its precise 3D geometric perception capability and remarkable perfor- mance in localization and mapping, LiDAR is attracting increasing attention for human motion estimation and capture [15], [16], [17], [18], [19], [20]. PedX [15] provides 3D poses of pedestrians by using SMPL [21] from third-person-view images in street scenes. LiDARCap [16] utilizes a fixed LiDAR to capture the long-range 3D human pose but only in limited scenes. LIP [18] combines multiple complimentary IMUs with LiDAR to capture human motions in large-scale scenarios. However, these approaches lack egocentric views and scene reconstruction.\nSeveral methods [8], [9], [17], [22], [23], [24], [25] have explored the human motion capture from an egocentric view."}, {"title": "RELATED WORK", "content": "This section presents an overview of research works related to our proposed HiSC4D system. We roughly divide the related methods into LiDAR/IMU-based 3D Human Pose Estimation, Human Localization and Scene Mapping, and Human-centered Social Interaction capture.\nLiDAR/IMU-based 3D Human Pose Estimation. IMU sensors have been widely used to capture human motions [26], [27], [28], [29], [30]. However, IMU-based methods suffer from severe drift over time. To improve the pose estimation accuracy, some methods [2], [3], [6], [31], [32], [33] utilize extra external RGB or RGBD cameras as a remedy. Helten et al. [34] combined two RGBD cameras with IMUs to perform local pose optimization.\nHybridFusion [35] has achieved more accurate motion-tracking performance by combining an RGBD camera with multiple IMUs. Total capture [5] fuses multi-view video with IMU data to accu- rately estimate 3D human pose in a lab environment. However, it has a limited sensing range and cannot be used outdoors. 3DPW [2] uses a single hand-held RGB camera and IMUs to optimize human pose for a certain period of frames simulta- neously. Constraints from external cameras assist in recovering more accurate 3D poses but result in limited capture space, human activities, and interactions. HPS [8] uses a first-view head- mounted camera to self-localize the 3D pose from IMUs to the scene. However, it requires pre-built maps and an image database for self-localization. PIP [30] combines physics-based motion optimization with sparse inertial motion capture, using only six IMUs to capture human motion in real-time. However, it lacks interaction with real-world environments. HSC4D [17] achieved promising results for long-term human motion and scene capture by using only body-mounted IMUs and a hip-mounted LiDAR. However, it cannot capture other humans in the scene. Recently, EgoLocate [9] estimates accurate human pose and localization using six IMUs and a head-mounted camera, it can also reconstruct sparse scenes without needing pre-scanning. Our method is complementary to HSC4D in that we capture accurate 3D human poses of the second person, who interacts with the first person.\nHuman Localization and Scene Mapping. Human self- localization aims at estimating the 6-DoF of the human sub- ject with carrying devices. The received signal strength (RSS) fingerprinting-based methodologies [36], [37], [38] are widely used for indoor human localization. However, these methods need external receivers and are limited to the indoor space. Some image- based methods [39], [40], [41] regress locations directly from a single image with a pre-built map. Still, the scene-specific property makes it hard to generalize to unseen scenes. Some methods integrate IMU as an aid sensor [42], [43] to improve accuracy. LiDAR is widely used in simultaneous localization and mapping [10] [44] [45] [46] due to its robustness and low drift. To localize the human subject, LiDAR are designed as backpacked [47], [48], [49] and hand-held [50]. LiDAR has been successfully applied in indoor localization [51], [52] and outdoor localization [53], [54], [55], [56] scenes. However, LiDAR-based localization and mapping systems are big pieces of equipment, that affect human motion and not being suitable for daily activities. Our capturing system is lightweight and uses a head-mounted LiDAR, achieving self-localization and mapping in large indoor and outdoor scenes. LOAM [10] is a real-time odometry and mapping method greatly boosting LiDAR-based SLAM research. Some methods [44], [46], [57], [58] further improve LOAM mapping for specific scenes and sensors. LeGO-LOAM [44] is a ground-optimized version, which requires keeping the LiDAR horizontal. LiDAR-based methods tend to fail when the Z-axis jitters severely. To address the drift problem and improve robustness, more sensors, such as visual sensors [59], [60], [61], IMU [42], [62], [63], or both [64], [65], [66], have been integrated in mapping task. Among LiDAR-based methods and datasets, a lot of attention has been paid to autonomous driving [67] [68] or robotics from the third- person view, and they usually do not focus on humans. With the aid of our designed capturing system, we captured an HiSC4D dataset containing LiDAR human motions from the first-person's egocentric view, as well as the 3D pose ground truth.\nHuman-centered Social Interaction capture In recent years, there has been an increasing interest in first-person vision re- search [8], [17], [22], [23], [24], [25], in which some focus on social interaction predicting [69], [70], and some focus on first- person 3D joints/pose estimation [71], [72]. However, most of them usually lack 3D pose ground truth or cannot reconstruct the 3D environment. The You2Me [24] captures the interaction between the first person (wearing a chest-mounted camera) and the second person, and infers the first person's poses from the interaction. However, it lacks 3D scene context. 4DCapture [23] collects egocentric videos and reconstructs second-person 3D human body meshes, but does not have first-person body data and reliable 3D annotations. HPS [8] reconstructs the human body pose using IMUs and a head-mounted camera in large 3D scenes but has few social interactions and heavily relies on the pre-built map. EgoBody [25] pre-scans the scene with an iPhone and uses multiple depth sensors to provide 3D human motion ground- truth, but has tedious multiple sensors setup and calibration and its social interactions are limited in a small room. HSC4D [17] achieves large human-scene modeling without scene-prior and contributes a novel LiDAR - IMUs setting for first-person vision data capturing. However, LiDAR on the back is unable to record social interactions with the second person. By upgrading HSC4D and solving the mapping problem with a built-in IMU of the LiDAR, our HiSC4D successfully reconstructed interactions with the second person and provided accurate SMPL annotations. We believe HiSC4D data will foster further research in the egocentric vision in large scenes, especially the unique LiDAR modality."}, {"title": "METHODOLOGY", "content": "To address these challenges, we introduce our innovative ap- proach Human-centered Interaction and 4D Scene Capture (HiSC4D). As shown in Fig. 2, after data has been collected, it begins with the data processing stage (Sec. 3.1). Here, we process the raw IMU data and LiDAR point cloud data to obtain essential components: IMU-based human motion, and LiDAR- based localization and mapping. Subsequently, by utilizing LiDAR localization and segmented human point cloud, we initialize the human motion's global positions. Finally, as detailed in Sec. 3.2, our approach employs a multi-stage optimization strategy to itera- tively get accurate human-centered interaction and scene capture.\nNotations. We use the right subscript k, where $k \\in_Z^+$, to indicate the index of a frame, and the right superscript, I or L or W (defaulting to W), to indicate the coordinate system to which the data belongs. The k-th frame point cloud is represented by $P_k^L$, and the 3D scene is represented as S. Here, We use the Skinned Multi-Person Linear (SMPL) [21] body model to map the k-th frame pose parameters $T_k$, $\\theta_k$, and \u03b2 to its motion representation $M_k$. This mapping function is defined as $M_k = \\Phi(T_k, R_k, \\theta_\u03ba, \u03b2)$, where $M_k \\in \\mathbb{R}^{6890\\times3}$ represents the triangle vertices. The global translation $T_k \\in \\mathbb{R}^3$ represents the translation of the SMPL model, while the pose parameter is composed of the pelvis joint's orientation $R_k \\in \\mathbb{R}^{1\\times3}$ relative to the start frame and the rotations of the other 23 joints $\\theta_k \\in \\mathbb{R}^{23\\times3}$ relative to their parent. The constant parameter $\u03b2\\in \\mathbb{R}^{10}$ represents the human body shape, which is obtained by using the IPNet [73] to optimize the human mesh captured by an iPhone14Pro.\nCoordinates system definition. We define three coordinate systems used in our paper: 1) IMU coordinate system {I} follows the left-hand rule. The origin is located at the pelvis joint of the first SMPL model with the X/Y/Z axis pointed to"}, {"title": "Data processing", "content": "The processing is to acquire the initialized human motions and LiDAR data for optimization. First, we estimate the LiDAR ego- motion and create a 3D scene map using the point clouds. Next, we acquire the 3D human motion output using our inertial MoCap system. Finally, we relocalize the human motions using LiDAR localization and segmented human point clouds.\nLiDAR-Inertial Localization and Mapping. Building a global consistency map and estimating accurate trajectory is difficult for the LiDAR-only method in this scene because the LiDAR jitters as the human walks and jumps. Incorporating an IMU will solve this problem by compensating for the motion distortion in a LiDAR scan and providing a good initial pose. Our LiDAR-Inertial SLAM method contains two modules, one is iterated Kalman filter-based LiDAR-Interial odometry [74], and another is factor-graph-based loop closure optimization [75] [76]. By employing this LiDAR- Interial SLAM method, we estimate the ego-motion of LiDAR and build the global consistency 3D scene map S with $P_{t,k}\\in Z^+$ in {L}. We first use the iterated Kalman filter-based state estimation module to estimate the full LiDAR state by registering raw points in a scan to the map points, then using a mapping module incrementally add the new points from each scan to an ikd-Tree [77]. Second, we use scan context [76] as a place recognition method to find the loop closure position. Then, we use factor graph [75] to perform loop closure optimization to generate a global consistency map. Finally, LiDAR's ego-motion $T_W$ and $R_W$, and the global consistency scene map S are computed. The mapping function is denoted as:\n$T_{lidar\\ 1:n}, R_{lidar\\ 1:n}, S = F(P_{1n}, R_{WL}).$ (1)\nIMUS Pose Estimation. To obtain each person's initial n frame motions $M_{1:n}$ in world coordinate, we transform the $R^I$ and $T^I$ from {I} to {W}. The calculation is defined as:\n$M_{1:n} = \\Phi(R_{WI}T_{1:n}^I, R_{WI}R_{1:n}^I, \\theta_{1:\u03b7}, \u03b2),$ (2)\nwhere $T^I$, $R^I$, and $\\theta^I$ are provided by the commercial MoCap product. The global translation of the initial human motion $M_{1:n}$ is very inaccurate due to the IMU's severe drifting for long- period capture. Additionally, the IMUs' pose estimation algorithm assumes that the person is walking on a flat plane without height changes. In order to address this issue, we will utilize the LiDAR localization result $T_{lidar}$ for the first person's global translation calculation and the cropped LiDAR human point clouds for the second person's global translation calculation.\nThe First-person Global Localization. We set the pelvis localization $T_{pelvis}$ to represent the subject's translation. The pelvis's position can be calculated directly from the head's joint position, which can be inferred from the LiDAR's localization. The calculation is defined as follows:\n$T_{pelvis} = T_{ph} + T_{hi} + T_{lidar},$ (3)\nwhere $T_{lidar}$ is the head-mounted LiDAR translation, $T_{hi}$ is the LiDAR to head offset, and $T_{ph}$ is the head to root joint offset. The $T_{ph}$ is calculated as:\n$T_{ph} = J_{pelvis}(\u03b8, \u03b2) \u2013 J_{head}(\u03b8, \u03b2),$ (4)"}, {"title": "Joint optimization", "content": "In this subsection, we tackle the problem of optimizing human motion for each person given two sequences of 3D human motion, denoted as $\\{M_{1:n}^I\\}^{first}$ and $\\{M_{1:n}^I\\}^{second}$ in the inertial coor- dinate I. Additionally, we utilize the scene mesh S in the world coordinate W, and a sequence of human point clouds denoted as $P_{1n}^L$ in the LiDAR coordinate L. The objective is to output the optimized human motion, $M_{1:n}^W$, in the world coordinate W for each person. The optimization can be formulated as follows:\n$\\underset{M}{Min} = arg min \\mathcal{L}(M_{1:n}, S, P_{1:n}),$ (8)\n$\\mathcal{L}=\\mathcal{L}_{smt} + \\mathcal{L}_{self} + \\mathcal{L}_{scene} + \\mathcal{L}_{phy}.$\nOptimization Strategy. To achieve accurate and realistic human motion $M_{1:n}$ and ensure faster convergence, we employ a multi- stage optimization strategy. In the first stage, we optimize the global translation parameter exclusively. Subsequently, in the second stage, both global translation and rotation are optimized. Finally, in the last stage, all SMPL parameters are optimized simultaneously. We incorporate specific constraints into the op- timization process, which are categorized as follows: smoothness terms ($\\mathcal{L}_{smt}$), human self-constraints terms ($\\mathcal{L}_{self}$), scene-aware terms ($\\mathcal{L}_{scene}$), and physical terms ($\\mathcal{L}_{phy}$). These constraints are derived from the human point clouds and the LiDAR trajectory.\n### The smoothness terms\nTo ensure smoothness in both temporal and spatial aspects of the movements. The $\\mathcal{L}_{smt}$ term incorporates the following con- straints: (1) the translation term $\\mathcal{L}_{trans}$, promoting smoothness in the global translation of the human body by minimizing the pelvis's acceleration. (2) The rotation term $\\mathcal{L}_{rot}$, encourages smooth transitions in the global rotation of the human body by minimizing the pelvis's angular acceleration. (3) The body pose smoothness $\\mathcal{L}_{pose}$, ensuring smoothness in the body pose, maintaining consistency throughout the motion sequence by min- imizing each pelvis-relative joint's angular velocity. (4) The body joints terms $\\mathcal{L}_{jts}$, promoting smoothness in the movements of each pelvis-relative joint by minimizing its acceleration. As a result, the $\\mathcal{L}_{smt}$ is expressed as:\n$\\mathcal{L}_{smt} = \\lambda_{trans}\\mathcal{L}_{trans} + \\lambda_{rot}\\mathcal{L}_{rot} + \\lambda_{pose}\\mathcal{L}_{pose} + \\lambda_{jts}\\mathcal{L}_{jts},$ (9)"}, {"title": "1 The smoothness terms", "content": "where $\\lambda_{trans}$, $\\lambda_{rot}$, $\\lambda_{pose}$, and $\\lambda_{jts}$ are coefficients of corre- sponding loss terms. These terms are detailed as follows:\n$\\mathcal{L}_{trans} = \\frac{1}{k-2}\\sum_{i=1}^{k-2} ||T_{i+2} \u2013 2T_{i+1} + T_i||^2,$\n$\\mathcal{L}_{rot} = \\frac{1}{k-1}\\sum_{i=1}^{\u03ba-1} ||R_{i+1} - R_i||^2,$\n$\\mathcal{L}_{pose} = \\frac{1}{k-1}\\sum_{i=1}^{k-1} ||\u03b8_{i+1} \u2013 \u03b8_i||^2,$\n$\\mathcal{L}_{jts} = \\frac{1}{k-2} \\sum_{i=1}^{k-2} ||J(M_{i+2}) -2J(M_{i+*+1}) + J(M_i)||^2,$\nwhere 23 pelvis-relative joints are regressed from the motions by $J(M) \\in \\mathbb{R}^{23\\times3}.$\n### Self-constraints terms\nThe $\\mathcal{L}_{self}$ term combines the constraints of foot sliding constraint ($\\mathcal{L}_{sld}$), pose prior constraint ($\\mathcal{L}_{prior}$), and self-penetration con- straint ($\\mathcal{L}_{pen}$) to improve the local pose quality. It ensures realistic foot positions, aligns the body pose with initial estimates, and prevents mesh interpenetration. The $\\mathcal{L}_{self}$ is expressed as:\n$\\mathcal{L}_{self} = \\lambda_{sld}\\mathcal{L}_{sld} + \\lambda_{prior}\\mathcal{L}_{prior} + \\lambda_{pen}\\mathcal{L}_{pen},$ (11)\nwhere $\\lambda_{sld}$, $\\lambda_{prior}$, and $\\lambda_{pen}$ are coefficients of these loss terms.\nFoot Sliding Constraint. The $\\mathcal{L}_{sld}$ reduces the foot sliding between consecutive stable foot vertices, resulting in more nat- ural and smooth human movements. The $\\mathcal{L}_{sld}$ is defined as the Euclidean distance between the stable feet in successive frames:\n$\\mathcal{L}_{sld} = \\frac{1}{k} \\sum_{i=2}^{k} ||\\overline{E}(F_i) \u2013 \\overline{E}(F_{i-1}) ||^2,$\nwhere $\\overline{E}$ represents the mean of a given set of values.\nPose Prior Constraint. To ensure optimization begins with a reliable initial value, we introduce the pose prior constraint $\\mathcal{L}_{prior}$, which enforces the \u03b8 to closely align with the initial estimation from the IMUs, as introduced in Sec. 3.1. However, the IMU poses are pseudo values and may deviate from the true values due to drifting. To address this issue, the pose prior constraint gradually decreases as the number of iterations iter increases. This constraint is defined as follows:\n$\\mathcal{L}_{prior} = \\frac{1}{\\frac{iter}{k}} \\sum_{i=1}^{k} ||\\overline{\\theta_i} - \\theta_i||^2.$\nSelf-penetrating Constraint. To address self-penetrations in human motion M, we introduce the self-penetrating constraint $\\mathcal{L}_{pen}$. To efficiently identify self-penetrating vertex pairs, we adopt a strategy that partitions the body mesh M into separate regions encompassing the torso, arms, hands, legs, and head. Subse- quently, we search vertex pairs from distinct body regions, thereby substantially decreasing computational cost while retaining the capacity to detect potential self-penetrations. The self-penetrating constraint $\\mathcal{L}_{pen}$ can be formulated as:\n$\\mathcal{L}_{pen} = \\frac{1}{k}\\sum_{i=1}^{k}\\sum_{A\\in M} \\sum_{B\\in M} \\sum_{a\\in A}\\sum_{s.t. b = \\underset{b\\in B}{arg min}||a - b||.} max \\left(0, (a \u2013 b) \u00b7 n_b\\right),$\nwhere A and B represent different body regions, and vertices a and b belong to their respective regions. The $n_b$ denotes the normalized normal vector at b. The self-penetration is determined by the positive dot product between the vector (a \u2013 b) and n.\n### Scene-aware terms\nBy incorporating environmental cues, the $\\mathcal{L}_{scene}$, composed as foot contact loss ($\\mathcal{L}_{cont}$) and human-scene collision constraint ($\\mathcal{L}_{coll}$), plays a crucial role in enhancing the contextual authen- ticity of the human motions, resulting in realistic interaction with their surroundings. The $\\mathcal{L}_{scene}$ is formulated as follows:\n$\\mathcal{L}_{scene} = \\lambda_{cont} \\mathcal{L}_{cont} + \\lambda_{coll} \\mathcal{L}_{coll}.$\nFoot Contact Constraint. The foot contact loss, denoted as $\\mathcal{L}_{cont}$, is defined as the distance from a stable foot to the nearest ground surface. To determine the foot state, we compare the movements of the left and right foot for each consecutive foot vertex in $M_i$. If the movement of a foot is less than 2cm and less than the movement of the other foot, the foot is marked as stable. The stable foot vertices in $M_i$ are denoted as $F_i$. The $\\mathcal{L}_{cont}$ is formulated as:\n$\\mathcal{L}_{cont} = \\frac{1}{k}\\sum_{j=1}^{k} \\frac{1}{F_j}\\sum_{v \\in F_j} \\underset{p \\in S}{min} ||v - p||^2.$\nHuman-scene Collision Constraint. By punishing the vertices in SMPL mesh penetrating the scene mesh, the $\\mathcal{L}_{coll}$ ensures that the human mesh remains collision-free with the 3D scene during optimization. Similar to the method of detecting penetrations in Sec. 3.2.2, we find the closest point $S_v$ on scene S for every vertex v in M, if the dot product between the distance vector from v to $S_v$ and the normal vector at $S_v$ is positive, it is identified as a term to be optimized. The calculation is formulated as:\n$\\mathcal{L}_{coll} = \\frac{1}{k} \\sum_{i=1}^{k} \\sum_{v \\in M_i} max \\left(0, (S_u \u2013 v) \u00b7 n_{S_v}\\right),$\nwhere $S_v$ is the projection of v on S that follows the minimum distance direction and $n_{S_v}$, is the normalized normal vector of $S_v$.\n### Physical terms.\nTo leverage the accurate localization provided by the LiDAR sensor and the rich depth and positional information from the point cloud, we introduce a comprehensive physical loss term $\\mathcal{L}_{physical}$. This loss term combines the constraints imposed by the LiDAR- based head localization, the viewpoint-based vertices-to-point loss $\\mathcal{L}_{v2p}$, and the point-to-mesh loss $\\mathcal{L}_{p2m}$. The $\\mathcal{L}_{phy}$ is formulated as:\n$\\mathcal{L}_{phy} = \\lambda_{l2h} \\mathcal{L}_{l2h} + \\lambda_{v2p} \\mathcal{L}_{v2p} + \\lambda_{p2m} \\mathcal{L}_{p2m},$\nwhere $\\lambda_{l2h}$, $\\lambda_{v2p}$, and $\\lambda_{p2m}$ are coefficients of these loss terms.\nLiDAR-head Constraint. The LiDAR sensor is rigidly attached to the head of the human body. Consequently, the Euclidean distance between the optimized position of the head and the LiDAR translation $T_{lidar}$ should be constrained to a constant value. This constraint ensures that the head remains in a fixed position relative to the LiDAR.\n$\\mathcal{L}_{l2h} = \\frac{1}{k} \\sum_{i=1}^{k} ||(T_{lidar})_i + (T_{hl})_i \u2013 J_{head}(\\Psi_i)||^2,$"}, {"title": "DATASETS", "content": "The HiSC4D system collects human-centric 4D data focused on human behavior, including both first and second-person motions, as well as scene data. In this section, we introduce data collection, data processing, dataset statistics, and dataset comparison.\n### Data Collection\nHardware. We use a 128-beam Ouster-os1 LiDAR to acquire 3D point clouds and Noitom's inertial MoCap product PN Studio to obtain human motion. Each suit of PN Studio uses 17 IMUS attached to the body limbs and two receivers to acquire data. Receivers and the LiDAR are connected to an Intel NUC11 mini-computer and use 24V mobile power to charge the Li- DAR and the computer. The LiDAR is rigidly connected to the helmet via a bracket; the data is transmitted to a mini-computer (NUC11) through the cable behind the helmet. We assume that LiDAR and the head have a rigid transformation. The LiDAR has a 360\u00b0 horizon field of view (FOV) and a 45\u00b0 vertical FOV. To make the people entering the LiDAR's FOV as much as possible, we tilt down the LiDAR around 45\u00b0. To ensure a lightweight and easy-to-use capturing system, we modified all cables and delicately designed the hardware position, making the battery (200mm \u00d7 120mm \u00d7 50mmm),\ntwo receivers (130mm \u00d7 100mm \u00d7 30mmm), and the mini- computer (117mm \u00d7 112mm \u00d7 57mmm) stored in a cross-country backpack. Finally, as Fig. 5 shows, we get a concise capturing system, requiring only one key to activate each sensor.\nData Capture. Before data capturing, the LiDAR wearer stands in A-pose facing or parallel to a large real-world plane marker with a flat face, such as a wall or a square column. The second person stands in front of the LiDAR wearer, 5 meters away, facing the same direction as the LiDAR wearer. Once the data capturing begins, both individuals will freely move around the environment and engage in interactions. When ready to end the capturing, both individuals will return to their starting positions.\n### Data Processing\nAfter data collecting, we first calibrate the LiDAR and IMU data to the world coordinates, then synchronize all the data. This allows us to apply the proposed method described in Sec. 3 to the data.\nCalibration The LiDAR wearer's right, front, and up directions are designated as the scene's X, Y, and X axis, respectively. The midpoint of the subject's ankle projection on the horizontal plane is set as the origin. After data collection, we employ the RANSAC [83] to detect the planes in the first frame point cloud, manually selecting the ground plane, and obtaining its normal vector, detected as g = [$g_1$, $g_2$, $g_3$]^T. Additionally, we fit a plane for the plane marker and compute its normal m = [$M_1$, $M_2$, $M_3$]^T, oriented in the LiDAR wearer's right-hand direction. The calcu- lation of the coarse calibration matrix $R_{WL}$ from the first frame LiDAR point cloud to the world coordinate {W} is as follows:\n$R_{WL} = \\begin{bmatrix} \\frac{s_1}{h} & \\frac{s_2}{h} & \\frac{s_3}{h} & 0 \\\\ M_1 & M_2 & M_3 & 0.2 \\\\ g_1 & g_2 & g_3 & h \\\\ 0 & 0 & 0 & 1 \\end{bmatrix},$ (22)\nwhere [$s_1$, $s_2$, $s_3$]^T = m \u00d7 gand h represents the LiDAR wearer's height. Specifically, this coarse $R_{WL}$ is further refined in Sec. 3.1, which is used in the optimization process. Based on the definition of IMU coordinate system {I}, the coarse transformation matrix $R_{WI}$ that transforms it to the {W} is defined as:\n$R_{WI} = \\begin{bmatrix} -1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}$ (23)\nSynchronization. LiDAR data is captured at a rate of 20 frames per second (FPS), while IMU poses of each human are captured at a rate of 100 FPS. At the beginning and end of the capture, we automatically detect peaks in IMU and LiDAR trajectories by hav- ing the LiDAR wearer perform a jump in place. However, due to the discrete nature of the trajectory, there is always a system error affecting the accuracy. To obtain a more accurate peak time $t_{peak}$, we employ a curve-fitting approach. This approach leverages the fact that, in the vicinity of the highest point during the jump, the trajectory heights $H_{peak}$ exhibit a quadratic relationship with time. Therefore, we utilize the location of the trajectory around the detected peaks and its timestamps for curve fitting.\n$E(t_{peak}) = \\sum_{h\\in H_{peak}} g *(t(h) \u2013 t_{peak})^2 \u2013 h||^2,$\n$t_{peak} = arg min\\ E(t_{peak}),$\nwhere g is the Gravity parameter, which is 9.8 m/s\u00b2, and t(h) is the time of h. Finally, we synchronize the LiDAR and IMUs based on the corresponding peak timestamps and resample the IMU data to 20 FPS to match the LiDAR frame rate.\nProcessing. We then processed the data using our proposed method, resulting in SMPL pose, shape, and global translations for both individuals, human point clouds for the second person, and 3D scene maps."}, {"title": "Dataset Statistics", "content": "With the raw data and the processed results", "ronments": "a building lobby, an indoor basketball gym, the campus area, and a pedestrian shopping street at night. The captured area spans from 200 to 5000 m\u00b2, each showcasing a variety of long-term human motion and interactions, including daily activities, human social interactions, as well as sports. The interactions involve activities such as ball tossing, chair moving, greetings, basketball training sessions, campus touring, and shopping.\nHuman Point Clouds Sequences. To contribute to the LiDAR- based human pose estimation research and benchmark this task, HiSC4D further provides 25,000 frames of visible segmented human points for the second person, as well as our optimized 3D human body pose and shape annotations in SMPL template as ground truth. Due to the first person's head motion and the limitation of LiDAR's FOV, the second person is occluded\nsometimes. We split each sequence into multiple segments based on the consistency of visible frames.\n3D Scenes. HiSC4D provides the dense 3D point cloud map reconstructed from the LiDAR as well as the colorful point cloud map from a"}]}