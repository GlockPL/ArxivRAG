{"title": "G2PDiffusion: Genotype-to-Phenotype Prediction with Diffusion Models", "authors": ["Mengdi Liu", "Zhangyang Gao", "Hong Chang", "Stan Z. Li", "Shiguang Shan", "Xilin Chen"], "abstract": "Discovering the genotype-phenotype relationship is crucial for genetic engineering, which will facilitate advances in fields such as crop breeding, conservation biology, and personalized medicine. Current research usually focuses on single species and small datasets due to limitations in phenotypic data collection, especially for traits that require visual assessments or physical measurements. Deciphering complex and composite phenotypes, such as morphology, from genetic data at scale remains an open question. To break through traditional generic models that rely on simplified assumptions, this paper introduces G2PDiffusion, the first-of-its-kind diffusion model designed for genotype-to-phenotype generation across multiple species. Specifically, we use images to represent morphological phenotypes across species and redefine phenotype prediction as conditional image generation. To this end, this paper introduces an environment-enhanced DNA sequence conditioner and trains a stable diffusion model with a novel alignment method to improve genotype-to-phenotype consistency. Extensive experiments demonstrate that our approach enhances phenotype prediction accuracy across species, capturing subtle genetic variations that contribute to observable traits.", "sections": [{"title": "1. Introduction", "content": "Genotype-to-phenotype prediction [36] is crucial for understanding gene regulatory mechanisms, interpreting the effects of genetic variants and advancing various applications such as crop-breeding [2, 10], disease marker identification [67] and personalized medicine [50]. Traditional works use statistical methods [14] to analyze genotype-to-phenotype relationships from large populations of individuals, such as genome-wide association studies (GWAS) [16, 24, 59, 60, 62] and quantitative trait locus (QTL) mapping [29, 34, 45]. Recent efforts use advanced AI methods [23, 43] to capture complex relationships within genotypes (e.g., multicolinearity among markers), and between genotypes and phenotypes (e.g., genotype-by-environment-by-trait interaction) [13, 38, 55, 69]. However, these methods are limited to standard species and predicting simple, individual traits, such as eye color and crop height. Deciphering complex, composite phenotypes from genetic data at scale and across species remains an open question.\nThe first challenge is how to define and collect large-scale phenotype data to support this research. While genome sequencing [39, 49, 68] have expanded genomic resources, collecting enough high-quality phenotypic data from diverse populations is labor-intensive and time-consuming, especially for traits that require visual assessment or physical measurements [70]. Existing specific collection pipelines limit the data scale and present challenges in capturing the underlying phenotype distribution across species. To deal with this challenge, we redefine the genotype-to-phenotype prediction problem from two perspectives. Firstly, we utilize images to represent observable physical characteristics (phenotype), which can be directly collected, observed, and analyzed. It can capture subtle morphological features and facilitate a more intuitive understanding of the relationship between genotype and phenotype. Secondly, we leverage the cross-species approach to increase the data scale and diversity, ultimately improving the power of our models in diverse biological contexts.\nIn this way, we reframe the genotype-to-phenotype prediction problem as a task of image generation from DNA sequences. Moreover, the cross-species method helps decipher more general gene regulation by identifying conserved genetic patterns and pathways shared among species.\nThe second challenge is how to predict phenotypes from genotypes consistently, i.e., generating images consistent with the DNA. Since complex phenotypes are often influenced by multiple genes, environmental factors, and interactions among them, modeling these relationships has been a long-standing challenge that researchers have been work-"}, {"title": "2. Related Works", "content": "Genotype to Phenotype Prediction. Predicting phenotypes from genotypes is a fundamental challenge in biology, considering the complex interaction between various genetic makeups (genotypes) and environmental influences and perturbations. [5, 11, 61] The genotype refers to the hereditary information stored in an organism's DNA, whereas the phenotype refers to the manifestation of that genetic information at the organismal level, which can be defined by observable physical characteristics (e.g., eye color), behavioral patterns (e.g., memory), physiological functions (e.g., blood pressure), and clinical manifestations (e.g., pain), among others [33]. In this paper, we focus on observable physical characteristics as phenotype.\nTraditional approaches, including genome-wide association studies [59, 60] and quantitative trait loci mapping [30, 31], have attempted to link genetic variation with phenotypic traits by identifying specific genetic markers associated with observable characteristics. In recent years, machine learning and deep learning methods have been increasingly applied to genotype-to-phenotype prediction, with models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) being explored to capture complex relationships in genomic data. These advanced techniques are increasingly employed to uncover hidden patterns in large-scale genomic datasets, with recent studies demonstrating their potential to improve prediction accuracy and deepen our understanding of the genetic basis of traits [1, 10, 44, 63, 64].\nHowever, these approaches typically frame the problem as a regression or classification task, predicting trait values or categories. Furthermore, no single model is universally effective across all species and traits. As a result, it is necessary to adopt a more flexible and generic approach that can accommodate the complexity of different species and phenotypes, capturing the intricate relationships between genotype and phenotype in a way that generalizes across diverse"}, {"title": "3. Method", "content": "We focus on the task of genotype to phenotype prediction, aiming to predict the observable physical characteristics given the corresponding DNA sequence and environment factors. This endeavor not only seeks to bridge the gap between genetic information and phenotypic traits, but also aims to deepen our understanding of the underlying mechanisms by which genetic variations manifest as specific traits across different species. The key point is how to accurately model the complex influences from genetic and environmental factors and translate this information into reliable phenotypic predictions.\nWe propose G2PDiffusion to address this challenge, as shown in Fig. 2, which includes an environment-enhanced DNA conditioner and a dynamic cross-modality alignment module built on diffusion model. These components enable the model to capture the intricate gene-environment interactions and align the phenotype with genetic and environmental factors dynamically, facilitating more accurate and generalized phenotype prediction across species. Denote"}, {"title": "3.1. Framework", "content": "each data sample as $S=(c, X)$, where c represents the conditional factors of environment (longitude and latitude) and the DNA sequence, and X is the phenotype represented as images. The problem can be formulated as a DM-based conditional generation task:\n$\\max _{\\theta} P_{c,\\phi}(X_{t-1}|X_{t}, c),$ (1)\nwhere c is the conditional embedding of genotype and environment factors, T is the number of generation steps, and 0 represents learnable parameters. We take the de-noised diffusion model $f_{\\theta}(X_{t},t,c)$ as the phenotype generator, and generate the target phenotype image $X_0$ gradually from a random noise image $X_T$, as detailed in Sec. 3.3. To enhance the consistency between genotype and phenotype, we propose a DNA-Image aligner $g_{\\phi}(X_{t}, t)$ to align the generated image $X_t$ with DNA embedding c, with the consistency loss $L_{con}$ serving as the alignment guidance. Algorithm 1 summarizes the guided sampling process."}, {"title": "Algorithm 1 Diffusion Model Sampling with Guidance", "content": "1: Input: Initial noise $X_T$, conditional diffusion model $f_{\\theta}(X_{t},t,c)$, guidance strength $w$, aligner $g_{\\phi}(X_{t},t)$, conditional embedding c, update rate $\\eta$\n2: Initialize $X_T$ as random noise\n3: for t = T down to 1 do\n4: Compute $\\nabla _{X_{t}} \\log p_{\\theta} (X_{t}|c)$ using the conditional diffusion model $f_{\\theta} (X_{t}, t, c)$;\n5: Compute $L_{con}$ using the aligner $g_{\\phi}(X_{t}, t)$ and c, referring to Eq. 8;\n6: Update gradient:\n$\\nabla_{X_{t}} \\log p_{\\theta} (X_{t}|c) \\leftarrow \\nabla_{X_{t}} \\log p_{\\theta} (X_{t}|c) + w \\nabla_{X_{t}}L_{con}$\n7: Estimate $X_{t-1}$ using the updated gradient:\n$X_{t-1} = X_{t} - \\eta \\cdot \\nabla_{X_{t}} \\log p_{\\theta}(X_{t}| c)$\n8: end for\n9: Output: Sample $X_0$"}, {"title": "3.2. Environment-enhanced DNA Conditioner", "content": "Phenotypic variations, the observable differences among individuals within a species, are not solely determined by genes; they are also molded by external environment and any interactions between genotype and environment. As a result, it is necessary to incorporate environmental factors into the genotype-phenotype mapping, to more accurately capture the complexity of phenotype expression.\nk-mer DNA Tokenizing and Encoding. DNA sequences, consisting of long chains of nucleotides (adenine, cytosine, guanine, and thymine), are inherently complex and require a systematic approach to capture meaningful patterns. Instead of regarding each base as a sin-"}, {"title": "3.3. Conditional Diffusion Models", "content": "Inspired by the great success of conditional diffusion model in text-to-image generation [4, 27], we leverage it as a controllable phenotype predictor. The model consist of a forward diffusion process and a reverse diffusion process.\nThe forward phase introduces noise to the trait images $X_0$, transitioning them towards a state resembling pure noise via a controlled Markov chain process, ultimately conforming to a standard Gaussian distribution $\\mathcal{N}(0, I)$:\n$q(X_{1:T} | X_0) = \\prod_{t=1}^{T} q(X_t | X_{t-1}).$ (3)\nAt each step t, noise is added according to the following equation:\n$q(X_t | X_{t-1}) = \\mathcal{N} (X_t | \\sqrt{\\alpha_t}X_{t-1}, (1 - \\alpha_t) I),$ (4)\nwhere $\\alpha_t$ is a hyperparameter controlling the noise intensity, and I represents the identity matrix. The transition from $X_0$ to a noisy state $X_t$ over t steps is captured by the equation:\n$X_t = \\sqrt{\\gamma_t}X_0 + \\sqrt{1 - \\gamma_t}\\epsilon,  \\epsilon \\sim \\mathcal{N}(0, I),$ (5)\nwith $\\gamma_t$ being the cumulative multiplication of $\\alpha_t$'s from 1 to t.\nIn the reverse process, we use LoRA [28] technique to finetune stable-diffusion's U-Net model and learn the DNA conditioner for predicting the noise $\\epsilon$ to restore the image. Formally, we write the conditioned U-Net as $f_{\\theta}(X_t, t, c)$, which estimate the noise for image $X_t$, at diffusion step t, and conditional embedding c. The training objective minimizes the loss function as follows:\n$\\mathcal{L}_{error} = E_{X,\\epsilon \\sim \\mathcal{N}(0, 1),t} [||\\epsilon - f_{\\theta}(X_t,t,c)||^2],$ (6)\nwhich measures the discrepancy between the actual noise $\\epsilon$ and its estimation by $f_{\\theta}$. Once $f_{\\theta}(X_t, t, c)$ is learned by minimizing $\\mathcal{L}_{error}$, in the inference stage $p_{\\theta}(X_{t-1}|X_t, c)$, we reverse the noise addition process, starting from the noisiest state $X_T$ and iteratively denoising down to t = 1.\n$X_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (\\frac{1-\\alpha_t}{\\sqrt{1-\\gamma_t}} (X_t - \\epsilon_{\\theta} (X_t, t,c)) + \\sqrt{1 - \\alpha_t}\\epsilon_t,$ (7)\nwhere $\\epsilon_t \\sim \\mathcal{N}(0, I)$ introduces randomness to enhance the diversity of model generated results. The final denoised image $X_0$ represents the predicted phenotypes.\nBuilding upon stable diffusion[56], we use an autoencoder to compress high dimensional images into low-dimensional feature maps and do latent diffusion in the latent space for saving computation costs."}, {"title": "3.4. Dynamic Cross-modality Alignment", "content": "We align the reverse diffusion process with the DNA encoder to enhance the genotype-phenotype consistency. Specifically, we introduce a training-free gradient guidance schedule, where an alignment model $g_{\\phi} (X_t, t)$ is introduced for aligning image embedding to the associated DNA embedding. This process is referred to as dynamic alignment as all the noisy images in the diffusion process are used for training the aligner. Mathematically, the conditional diffusion score [26] is\n$\\epsilon(X_t, c) \\approx -\\sqrt{1 - \\alpha_t}\\nabla_{X_t} [\\log p_{\\theta}(X_t|c) + w\\log p_{\\phi}(c|X_t)].$ (8)\nThe alignment model $g_{\\phi}$ is implemented as a transformer with parameters $\\phi$ to encode noisy image $X_t$. To enhance the consistency between the ground-truth genotype and the predicted phenotype, we define the learning objective based"}, {"title": "4. Experiments", "content": "In this section, we conduct extensive experiments to answer the following questions:\nFidelity (Q1): Could the model generate phenotypes that match to the DNA for accurate predictions?\nGeneralization (Q2): Could our proposed method generalize across unseen species?\nVariation Analysis (Q3): Could the model capture the effect of genetic and environmental variation on phenotype expression, thereby uncovering gene regulation mechanisms and the role of natural selection?"}, {"title": "4.1. Dataset and Evaluation Metric", "content": "Dataset. To train and evaluate G2PDiffusion, we use BIOSCAN-5M dataset [18], which provides a unique and extensive multi-modal collection of insect biodiversity data ideal for genotype-to-phenotype generation tasks. By leveraging over 5 million insect specimens, each with corresponding taxonomic labels, DNA barcode sequences, Geographical information (longitude and latitude) and image data, the dataset allows us to examine the model's ability to generate phenotypes from genetic and environmental inputs across diverse insect species. The dataset is divided into two sets: Seen and Unseen, based on the species label. Samples with species labels corresponding to established scientific names are categorized as Seen; Otherwise, samples without species labels are classified as Unseen. To our knowledge, this is the largest multi-modal dataset available for genotype-to-phenotype research.\nCLIBDScore Metric. While the proposed G2PDiffusion model approaches genotype to phenotype prediction from an image generation perspective, it is crucial to acknowledge that traditional image generation metrics, such as"}, {"title": "4.2. Fidelity (Q1)", "content": "Experimental setup. In this section, we conduct experiments on closed-world setting of BIOSCAN-5M dataset, in which all species have been established scientific names. We split the closed-world data into training and test sets with a 9:1 ratio. To ensure balanced species evaluation, we place samples in the test set with a flattened species distribution. Importantly, the DNA barcodes in the testing set were specifically chosen to be absent from the training set, ensuring that the model is evaluated on completely unseen data. All the models are trained on NVIDIA-A100 GPUs using Adam optimizer up to 100k steps, with learning rate of 1e-5, batch size 128 and cosine annealing scheduler. The image resolution is 256 \u00d7 256.\nBaselines. Given the absence of genotype-to-phenotype prediction baseline, we employ a comparative framework that adapts the leading conditional image generation meth-ods to this specialized task. This set includes GAN-based"}, {"title": "4.3. Generalization (Q2)", "content": "To investigate the generalization capability of our method, we evaluate its performance on unseen species in the dataset, called the open-world scenario. In this case, species do not have scientific names in the dataset."}, {"title": "4.4. Variation Analysis (Q3)", "content": "Mutation Effect. As shown in Fig. 6, we randomly mutate the DNA sequence by 10% to 50% and alter the sample's latitude to examine how both genetic and environmental factors influence the phenotype. We find that the model is sensitive to DNA mutations; specifically, a higher mutation rate results in worse visual quality, indicating that the model can learn the critical dependency between the DNA sequence and the phenotype. Interestingly, when we change the latitude value, the closer to the equator, the smaller the specie sizes are. This phenomenon meet the Bergmann's"}, {"title": "5. Conclusion", "content": "In this work, we introduce G2PDiffusion, the first-of-its-kind diffusion model designed for genotype-to-phenotype generation across multiple species. We introduce environment-enhanced DNA encoder and dynamic diffusion aligner to enhance the consistency between generated images and the DNA. Experimental results show that our model can predict phenotype from genotype better than baselines. Notably, we believe this is the pioneering effort to establish a direct pipeline for predicting phenotypes from genotypes through generative modeling, which may open new avenues for research and practical applications in various biological fields."}, {"title": "6. Limitations and Future Works", "content": "The lack of goal standard makes evaluation to be difficult. While we introduce a novel and objective metric (CLIBDScore), it is important to recognize that it depends on the quality of the pretrained CLIP model [20]. We acknowledge the importance of this expert validation and hope to explore collaborations with specialists to assess the relevance and accuracy of our model's outputs for phenotype prediction."}]}