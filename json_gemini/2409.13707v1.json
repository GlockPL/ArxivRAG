{"title": "Retrieval Augmented Generation-Based\nIncident Resolution Recommendation System for IT Support", "authors": ["Paulina Toro Isaza", "Michael Nidd", "Noah Zheutlin", "Jae-wook Ahn", "Chidansh Amitkumar Bhatt", "Yu Deng", "Ruchi Mahindru", "Martin Franz", "Hans Florian", "Salim Roukos"], "abstract": "Clients wishing to implement generative AI in the domain of\nIT Support and AIOps face two critical issues: domain cov-\nerage and model size constraints due to model choice lim-\nitations. Clients might choose to not use larger proprietary\nmodels such as GPT-4 due to cost and privacy concerns and\nso are limited to smaller models with potentially less domain\ncoverage that do not generalize to the client's domain. Re-\ntrieval augmented generation is a common solution that ad-\ndresses both of these issues: a retrieval system first retrieves\nthe necessary domain knowledge which a smaller generative\nmodel leverages as context for generation. We present a sys-\ntem developed for a client in the IT Support domain for sup-\nport case solution recommendation that combines retrieval\naugmented generation (RAG) for answer generation with an\nencoder-only model for classification and a generative large\nlanguage model for query generation. We cover architecture\ndetails, data collection and annotation, development journey\nand preliminary validations, expected final deployment pro-\ncess and evaluation plans, and finally lessons learned.", "sections": [{"title": "Introduction", "content": "The recent boost in performance and popularization of gen-\nerative models has resulted in clients across various domains\nrequesting generative AI powered question-answering and\nrecommendation systems. However, there are two critical\nissues facing many clients wishing to implement genera-\ntive AI: domain coverage and model size constraints due\nto model choice limitations. Much of the focus for gener-\native models has been on the general domain and only some\nspecific tasks such as coding. Models that work on these\ndomains might not necessarily work for a client's targeted\ndomain. Additionally, clients might choose not to leverage\nlarger proprietary models such as GPT-4 because of cost and\nprivacy concerns so clients are limited to smaller models\nwith less domain coverage and likely lower out-of-the-box\nperformance.\nWe have faced both of these issues when building a solu-\ntion recommendation system for resolving IT support cases.\nModels and tasks within the domain of IT support and Ar-\ntificial Intelligence for IT Operations (AIOps) are under-\nresearched. No IT support specific fine-tuned generative AI\nmodel exists and there are limited publicly available datasets\non IT support tasks like question-answering (QA) or re-\ntrieval over a corpus of IT support documents. Thus, it is\nnon-trivial to evaluate out-of-the-box AI models on IT sup-\nport tasks as well as train and evaluate custom AI solutions\nin this domain.\nIn specific, the IT support use case involved IT prod-\nuct support tickets that are opened by a customer and an-\nswered by a support agent after a series of interactions. It\nrequired a system that would respond only to cases that did\nnot need any additional information or clarification from the\ncustomer. That is, the support case could be resolved based\nonly on the initial information present in the case subject\nand description when the ticket was first opened. Addition-\nally, the use case required that solutions be grounded in an\nofficial support document that would be presented to the cus-\ntomer as a link along with the summarized solution.\nGiven these use case requirements, retrieval augmented\ngeneration (RAG) was a natural fit (Lewis et al. 2020). It is\na common solution that addresses the two main issues of in-\nadequate generalizability to more niche domains and model\nsize limits. It does so by using a retrieval system to first\nretrieves the necessary domain knowledge which a smaller\ngenerative model then leverages as context for answer gen-\neration.\nWe present the resulting system for IT support case solu-\ntion recommendation that combines an encoder-only model\nfor classification, two dense embedding models for retrieval,\nand generative large language models for query and answer\ngeneration. The solution brings several novel contributions\nto the field of AIOps:\n\u2022 The first reported evaluation of a RAG system for IT sup-\nport incident resolution recommendation.\n\u2022 The first reported use of a classifier for determining sin-\ngle vs multi-turn IT support cases.\n\u2022 Evidence of substantial retrieval improvement using\nre-ranking based on a new model, IBM Slate 125m\n(IBM Research 2024)\n\u2022 A comparison of answer generation performance across\ndiverse model sizes that shows smaller models can match\nor even beat the performance of very large models in the\nRAG incident remediation use case."}, {"title": "Architecture", "content": "Given a support case subject, description, and product name,\nour system generates recommended solutions based on cor-\npora of support documents. Our system consists of four ma-\njor components as illustrated in Figure 1: an encoder-only\ntransformer classifier, a query generation system, a retriever\nsystem, and an answer generator system.\nPreprocessing: Support cases are ingested with unstruc-\ntured text data fields of case subject, case description, prod-\nuct name, and product version number. The escape and non-ASCII characters are removed from the case subject and\ndescription, and the two fields are concatenated. We pre-\nprocess the product name by matching it to a dictionary of\nknown product acronyms or alternative names to append to\nthe query used in retrieval.\nCase Turn Classifier: The cleaned and concatenated case\nsubject and description are fed into the classifier which de-\ntermines if the support case is a single turn. ingle-turn cases\nare defined as those that can be resolved using only the infor-\nmation present in the case subject and description, without\nrequiring any additional information or clarification from\nthe customer. The classifier is an encoder-only transformer\nmodel IBM Slate 125m (IBM Research 2024) that was fine-\ntuned on almost 14,000 examples labeled by subject matter\nexperts. If the case is predicted to be single turn, the case\ncontinues to the next step in the pipeline.\nQuestion Generator: The question generator summarizes\nthe often vague case subjects and verbose, convoluted de-\nscriptions into concise text queries suitable for the retrieval\nsystem. The system prompts a large language model, Mix-\ntral 8x7B Instruct (Jiang et al. 2024), to generate a concise\nquestion based on the provided case subject and descrip-\ntion. Additional post-processing keeps only the first gener-\nated sentence in case the generative model does not follow\ninstructions and generates additional sentences beyond the\nfirst question.\nRetriever: Our documentation is retrieved from multiple\ndata collections in a Milvus vector database, all indexed with\nthe standard Slate-30M embedding. If the search stage re-\nquires top-3 documents, we search for 3 from each of these\nindexes, and then merge-sort based on the score before re-\ntaining the top-k from the combined set.\nHaving retrieved the top-3 documents, as ranked by a\ngeneral-purpose embedding, we re-rank them using a Bi-\nEncoder model that has been fine-tuned using application-\nspecific training data. Re-ranker scores are computed as co-\nsine similarities of a combination of the original case and the\nderived question, compared with the same passages (with re-\ncalculated vectorizations) that were matched in the first pass.\nAnswer Generator: For each of the top three documents\nretrieved, the answer generator produces an answer to the\npreviously generated query by leveraging the top three re-\nranked passages from the document. First, we split the re-\ntrieved document content into 2500-token chunks with 250-token overlaps and use cosine similarity to pick the three\nmost relevant chunk contexts. The answer generator prompts\na large language model, Mixtral, to answer the query us-\ning the three contexts. Finally, the system returns the three\nretrieved links with their corresponding generated answers.\nThe results can then be displayed to the support agent in a\ngraphic interface."}, {"title": "Data", "content": "We collected almost 19,000 real support cases across nine\nsoftware products: six to serve as training and three to serve\nas validation. Each case included a case subject and descrip-\ntion originally drafted by a customer when opening the case.\nAdditionally, the three indices for documentation leveraged\nfor retrieval had a total corpus of over 5 million documents.\nFor each product, we asked five support agents who were\nsingled out as product subject matter experts to carry out\nthe following tasks for each of the nine products: 1) anno-\ntate single-turn vs. multi-turn label, 2) validate silver ground\ntruth query based on case subject and description and pro-\nvide updated query as necessary, 3) provide link to relevant\ndocument in support corpus, and finally 4) copy and paste\nsolution to query as found in the relevant support document.\nTasks #2 through #4 were carried out only for cases that\nhad been labeled as single-turn. After cleaning and removing\nmissing annotations, we created a dataset of almost 19,000\nsupport cases, with almost 3,000 cases for each training\nproduct and over 400 cases for each evaluation product."}, {"title": "Development and Validation", "content": "As our solution is meant to supply answers before involv-\ning a support agent, we needed to develop a method for\nclassifying incoming cases as single-turn vs. multi-turn. The\nclassifier model is a binary encoder-only IBM Slate 125m\nmodel fine-tuned on the single-turn/multi-turn labels of al-\nmost 7,000 unique cases across six software products. The\nfinal training data is around 14,000 cases as it includes two\ncopies of a given case: one with tokens from both the case\nsubject and description fields, and another with tokens only\nfrom the case subject..\nFor our application, we prioritized correctly predict-\ning single-turn cases (positive class) versus multi-turn\ncases (negative) emphasizing recall over precision. Table\n1 presents the final fine-tuned model performance on the\nheld-out evaluation set of the six products when using a\nclassification threshold of 0.1. We found that performance\nvaries widely depending on the product, ranging from F1 of\n0.27 to 0.62 and recall from 0.75 to 0.98. The lower perfor-\nmance can be explained in part by the varying class imbal-\nance across products (positive class proportion from 11%\nto 44%) as well as the products' differing inter-annotator\nagreement (See Section 6). Despite this, the model still\nsubstantially outperforms random guessing of the classes.\nHyper-parameters including batch size, learning rate, and\ndropout were determined based on a small grid search.\nWe then evaluated the fine-tuned classifier on about 1,400\ncases from three additional products that were not in the\ntraining set to validate if the model generalized to other\nproducts. The resulting F1 of 0.65, precision of 0.54, and\nrecall of 0.80 for the three products suggests that classifier\nmodel generalizes well to products not seen during training\neven with substantially different class balance.\nIn order to create a concise query that could be used by\nthe retriever, we generated a single sentence question based\non the case subject and description. Our experiments over\nvarious open-source generative models (Table 2) and model\navailability in the client's services led us to choose Mixtral-8x7b-Instruct as the model for query generation which reli-\nably reproduced the ground truth queries despite being a rel-\natively small model with no domain knowledge. Note that\nthe results are skewed for Falcon-40B (Almazrouei et al.\n2023) as Falcon-40B generated the first pass of silver ground\ntruth queries that were then edited by subject matter experts.\nSupport experts supplied us with a collection of cases with\none ground truth link each that a \"correct\" solution should\nreference; our evaluation is based on whether this link is\ncontained in the top n links returned (for various values\nof n). Implementing this evaluation presented several chal-\nlenges:\n\u2022 URL Duplication: A single page of documentation often\nhas several different URLs to identify it.\n\u2022 Subtle Content Variations: Documentation for the same\ntopic in different versions of a product may have subtly\ndifferent titles, like \"How to update a list\" and \"Lists:\nUpdating\".\n\u2022 Identical Content Across Different Documents: Docu-\nmentation for the same topic in different versions may\nbe identical in which case results from different versions\nare still valid.\nMitigating the first of these challenges, many of our docu-\nmentation pages include a \"canonical link\" in their metadata.\nIn many cases, this allows us to identify identical links. The\ntwo issues with documentation evolution between versions\nare addressed with Rouge-1 scores, using a threshold of 0.90\nas sufficiently similar to count as identical.\nFor retrieval, a dedicated team is already responsible for\nmaintaining indexed collections of the software product doc-\numentation. This saves our project from gathering, maintain-\ning, and indexing all of these documents, and its base Slate-30M embedding returns a good first set of results.\nRe-ranking this first set allows us to use fine-tuning to\nimprove performance without maintaining a parallel set of\nindexes. For this final task-specific fine-tuning stage, we\nused training data based on 1,430 questions with up to three\nmatching passages per question extracted from documents\nidentified in user interactions, together with negative exam-\nples found using BM25 search. The resulting IBM Slate-\n125M model was then distilled into the deployed IBM Slate-\n30M model. To evaluate its effectiveness, we present recall\nbefore and after re-ranking with Google Search as a baseline\nin Figure 2."}, {"title": "Answer Generator", "content": "The final step of our solution takes in the generated query\nand top three most relevant retrieved passages as context to\nprompt the answer generator. In particular, the prompt asks\nthe model to use the information within the provided con-\ntexts to generate an answer. Additionally, if the context is\ninsufficient, then the model is instructed to state that an ac-\ncurate answer cannot be provided.\nTo evaluate the answers, we used the subject matter ex-\npert's annotated ground truth answers and ground truth doc-\numents verified to contain the answer to the question. We\ncompared the answers generated by the answer extractor us-\ning the ground truth document to the ground truth answer\nusing BertScore (Zhang et al. 2020) and ROUGE-L F1. We\nevaluated different models and prompts to find the optimal\ncombination and present the results of the models assessed\nin Table 4. While BertScore (roberta-large) F1 is rather low\n(in practice it ranges between 0.85-0.95), ROUGE-L F1, tra-\nditionally a rather strict metric, shows promising results for\nMixtral-8x7b-Instruct with a score of 0.41. Mixtral-8x7b-Instruct's outperforms of GPT-40, included as a baseline for\nlarger models, in all three metrics, despite having substan-\ntially less parameters. Likewise, Granite-13B-Chat-v2 is not\nfar behind GPT-40 despite its merely 13 billion parameters\ncompared to GPT-40's rumored hundreds of billions or even\ntrillions of parameters. This suggests that the RAG approach\nof smaller models leveraging retrieved context is a viable so-\nlution for IT incident resolution recommendation systems."}, {"title": "Knowledge Infusion for Answer Generation", "content": "Directly applying general foundational models to AIOps\nfor answer generation tasks often does not yield optimal re-\nsults. The knowledge infusion approach involves adapting\nthese pre-trained models to specific tasks through additional\ntraining on task-specific data.\nTo further boost the results of our action generation\ntask for AIOps domain, we employ the knowledge infusion\nmethodology described in (Sudalairaj et al. 2024). First, we\nmanually created a seed dataset with six tuples, each con-\ntaining context and four related question-answer pairs. Then,\nwe randomly selected fifteen documents from the corpus\nto guide synthetic data generation, using the seed dataset\nto replicate similar artifacts for each document. Using this\nseed dataset, we created 14,000 synthetic samples with the\nMixtral-8x7B-Instruct model as the teacher. IBM's Gran-ite 7B IL-Internal-Granite-7B-Base, a much smaller model,\nwas fine-tuned with IT domain data to cater to the specific\ntask of answer generation for the IT Support use case re-\nsulting in the InstructLab-IT model with domain knowledge\ninfusion.\nTo evaluate the quality improvement, we conducted a user\nstudy with 6 technical experts and forty test question-answer\npairs per model. For each question, we retrieved context\nfrom a 1200-document corpus of six software products and\nused it to prompt each model separately for an answer. Our\nuser study used a 0 to 1 rubric to evaluate answer correctness\nwith clear descriptions for each level:\n\u2022 0 = Incorrect: irrelevant or fails to answer the question.\n\u2022 0.25 = Mostly Incorrect: Some details are correct, but key\ndetails are missing, fabricated, or mostly irrelevant.\n\u2022 0.5 = Partially Correct: Most details are correct, but some\nkey details are missing, fabricated, or include a lot of ir-\nrelevant information.\n\u2022 0.75 = Mostly Correct: Most details are accurate, with\nonly minor gaps or irrelevant information.\n\u2022 1 = Correct: Includes only relevant details.\nIn Table 5, we present the final score for each model\nas the average of human annotators' scores across 40\nquestion-answer pairs of which InstructLab-IT emerged\nas the best model. While Llama-3.1-8b-Instruct performed\nslightly better than GPT-40, the improvement in results with\nInstructLab-IT was very noticeable over both models. This\nis especially significant considering the model sizes: GPT-40 (over 1 trillion parameters and 1.5 TB), Llama-3.1-8b-Instruct (8 billion parameters and 16 GB), and InstructLab-IT (7 billion parameters and 28 GB). These results signal\nthat a smaller, domain-specific model tuned for a specific\nset of use cases may better meet client requirements."}, {"title": "Deployment", "content": "The tool is currently integrated into the ticketing system but\nsilently deployed (not visible to agents) for testing purposes."}, {"title": "Lessons Learned", "content": "Defining the proper use case is probably the most critical\nstep in developing a proper RAG recommendation applica-\ntion. Because of the expense of data collection, annotation,\nand development, any confusion or change in the exact use\ncase and capabilities of the model can result in substantial\ndelays and costs.\nFor example, the first dataset that we considered for this\nuse case was synthetic data for which subject matter ex-\nperts crafted questions based on support document titles, and\nthen provided corresponding answers. When we compared\nthis data to actual customer cases, we found the genuine\nquestions to be more verbose and to contain more off-topic\n\u201cnoise.\u201d Thus we decided to use the more challenging actual\nsupport ticket data for training and validation, as it appeared\nbetter suited to our final deployment than the cleaner syn-\nthetic data.\nWe recommend spending time early on to understand how\nstakeholders will interact with the system, knowing that\nchanges and evolution in the actual workflow may cause a\ndecrease in system performance."}, {"title": "Inter-Annotator Agreement", "content": "Three SMEs labeled a subset of twenty cases to determine\ninter-annotator agreement. The results in Table 6 show that\nlabeling cases as single vs. multi-turn is not a trivial task and\nfor most products, SMEs disagreed widely. Of the cases in\nwhich all three annotators agreed to be single-turn, agree-\nment on the question and link quality was better but still\nraises questions about the validity of the training and evalu-\nation data. In particular, the low agreement of the provided\nlinks can be explained by the fact that more than one link can\npotentially solve the same question and so neither annota-\ntor is necessarily wrong. This suggests that for ground truth\ndata, we should consider a list of correct links instead of a\nsingle ground truth link for each question. The low agree-\nment of single-turn vs. multi-turn labels also potentially ex-\nplains the lower performance of the classifier model if the\nmodel is attempting to learn from potentially conflicting in-\nformation."}, {"title": "RAG Bottlenecks", "content": "The major bottleneck in RAG systems is the retrieval com-\nponent. As shown in Table 4, when given the correct con-\ntext, LLMs can typically generate responses that match the\nground truth answers. However, we cannot expect to gen-\nerate the correct answer if given the wrong contexts which\nhappens for around 60% of the cases (Figure 2). For compar-\nison, Google search limited to the corresponding domains\nindexed by the Milvus database performed worse at 30%\nR@3 compared to our method at 43% R@3 (See Figure 2).\nThis implies, as other researchers have suggested, that the\nretrieval component in RAG is not a solved problem by any\nmeans. (Petroni et al. 2024; Cuconasu et al. 2024)"}, {"title": "Related Work", "content": "As software systems become more complex, Artificial In-\ntelligence for IT Operations (AIOps) methods are widely\nused to manage software system failures and ensure the high\navailability and reliability of large-scale distributed software\nsystems (Zhang et al. 2024). Machine learning and natural\nlanguage processing methods such as LLMs have been used\nin AIOps for incident triage, data pre-processing, failure per-\nception, root cause analysis, and auto remediation (Zhang\net al. 2024). Historically and currently, many of these tasks\nincluding both incident triage and auto remediation have\nbeen treated as classification problems: for example, Ahmed\net al. (2023a) treats incident resolution as a classification\ntask matching incident tickets to a relatively small number of\npossible resolutions using the BERT model and embeddings.\nWith the rise of better performing generative AI models, re-\nsearchers have moved towards using these models to gener-\nate solutions in the auto remediation task using prompting\nstrategies (Ahmed et al. 2023b; Liu et al. 2024) or creating a\nmodel fine-tuned for a variety of IT tasks such as question-\nanswering (Guo et al. 2023).\nOur use case can be considered an example of Zhang et al.\n(2024)'s \"Assisted Questioning\", an auto remediation task\nthat involves utilizing LLMs to aid operations personnel in\nanswering system-related queries. As far as we are aware,\nno current work exists that leverages a RAG-based approach\nto solve this task, although one does exist for a similar IT\ntask of root cause analysis (Chen et al. 2024). The RAG-based approach was taken in lieu of fine-tuning such as in\nGuo et al. (2023)'s OWL model because of issues in real-\nworld deployment due to its resource-intensive nature which\nrequires significant computational resources and the inter-\npretation of model decisions. Likewise, we discounted us-\ning a simple prompting approach without retrieval because\nof client limitations in model choice that prevented us from\nusing larger models."}, {"title": "Retrieval and Retrieval Augmented Generation", "content": "Methods for finding the relevant documents or pas-\nsages to answer a user query are typically divided into\nsparse (Robertson and Zaragoza 2009) and dense retrieval\nsystems (Zhao et al. 2024). Our retrieval starts with a Mil-vus (Wang et al. 2021) vector database that has indexed\nthe software support documentation with a general purpose\ndense embedding that serves multiple services. In our so-lution, we then make use of a popular optimization by re-\nranking the first-pass result to obtain a more appropriate\nranking for our particular application (Nogueira and Cho\n2020; Han et al. 2020), giving us the results of a special-purpose index while still retaining the benefits of a central\nindexing service. Other popular improvement methods in-\nclude combining sparse and dense embeddings into a hybrid\nsystem (Luan et al. 2021).\nRetrieval augmented generation was developed to address\ncases in which large language models have not learned and\nstored domain knowledge through pre-training. Originally\nimplemented by combining dense retrieval and a fine-tuned\nBART model, the method generalizes well to larger genera-\ntive models (Fan et al. 2024). The quality of the answer gen-\neration can be improved through prompt engineering, refin-\ning how the specific generative model is prompted with the\ncontext, question, and other instructions. (Liu et al. 2023).\nHowever, it has been noted that the retrieval component in\nRAG has been understudied in comparison to the genera-\ntion component despite its substantial impact on the final\nperformance of such hybrid systems. (Petroni et al. 2024; Cuconasu et al. 2024)"}, {"title": "Conclusion", "content": "We were able to deliver a first working version of an IT\nproduct solution recommendation system employing RAG.\nTo our knowledge, this is the first published architecture\nand performance metrics of such a RAG system in this do-\nmain. Our system also differentiates itself with a few innova-\ntions including a component for classifying support cases as\nsingle-turn and a component for distilling verbose case de-\nscriptions into a query suitable for retrieval. We demonstrate\nthat smaller models leveraging retrieved domain context can\nmatch or out-perform substantially larger models both with\nand without context (Ahmed et al. 2023b; Liu et al. 2024)\nparticularly with knowledge infusion through fine-tuning.\nHowever, there are still many challenges in implementing\nRAG for IT support incident resolution including improving\nretrieval performance. We are collecting feedback from sup-\nport specialists using our current deployment, and intend to\nincorporate their advice into future improvements."}]}