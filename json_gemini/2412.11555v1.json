{"title": "TS-SatFire: A Multi-Task Satellite Image Time-Series Dataset for Wildfire Detection and Prediction", "authors": ["Yu Zhao", "Sebastian Gerard", "Yifang Ban"], "abstract": "Wildfire monitoring and prediction are essential for understanding wildfire behaviour. With extensive Earth observation data, these tasks can be integrated and enhanced through multi-task deep learning models. We present a comprehensive multi-temporal remote sensing dataset for active fire detection, daily wildfire monitoring, and next-day wildfire prediction. Covering wildfire events in the contiguous U.S. from January 2017 to October 2021, the dataset includes 3552 surface reflectance images and auxiliary data such as weather, topography, land cover, and fuel information, totalling 71 GB. Each wildfire's lifecycle is documented, with labels for active fires (AF) and burned areas (BA), supported by manual quality assurance of AF and BA test labels. The dataset supports three tasks: a) active fire detection, b) daily burned area mapping, and c) wildfire progression prediction. Detection tasks use pixel-wise classification of multi-spectral, multi-temporal images, while prediction tasks integrate satellite and auxiliary data to model fire dynamics. This dataset and its benchmarks provide a foundation for advancing wildfire research using deep learning.", "sections": [{"title": "Background & Summary", "content": "Wildfires and the associated loss of forests have shown an increasing trend in recent years1-3. Monitoring and understanding the behaviour of wildfires are essential for mitigating this natural hazard. Satellite-based solutions for wildfire monitoring have been widely investigated in the remote sensing community, including active fire detection and burned area mapping. Active fire detection involves identifying the location of active fires from satellite imagery, while burned area mapping entails detecting the total burned area from satellite imagery. Wildfire progression prediction aims to forecast the progression of the wildfire based on its current location and environmental information, such as weather, fuel, and topography.\nCurrent satellite-based wildfire products concentrate on active fire detection and burned area mapping4\u20139. However, the multi-criteria thresholding method employed by active fire products often results in unreliable detections. Also, the accuracy and the temporal resolution of the current burned area mapping product are limited. Various efforts have been made to improve detection accuracy using deep learning10,11. These studies suggest that deep learning models can substantially improve detection accuracy by leveraging the spectral, spatial, and temporal information present in satellite datacubes. Accurate detection of active fires and burned areas provides crucial current-status information about wildfires, serving as the foundation for wildfire prediction tasks. In recent years, several works have investigated wildfire progression prediction using remote sensing data12,13. These works aim to use auxiliary data along with remote sensing images to predict wildfire progression. However, most of the dataset targets a single task although the same sensor is used. There are limited works that simultaneously cover detection and prediction tasks. The release of this dataset aims to help researchers build robust deep learning models that can accurately detect active fires and burned areas, as well as forecast fire progression. By leveraging remote sensing datacubes, we establish 1D pixel-based temporal models, 2D image-based spatial models, and 3D spatial-temporal models as baselines for three distinct tasks and cross-compare the performance of the same architecture on different tasks. By comparing different sets of models, we assess the performance of each architecture across various tasks to identify the most suitable architecture for both detection and prediction tasks."}, {"title": "Active Fire detection", "content": "Active fire detection is commonly accomplished through Active Fire (AF) products released by the National Aeronautics and Space Administration (NASA) and the European Space Agency (ESA). These AF products are typically derived from sensors such as the Moderate Resolution Imaging Spectroradiometer (MODIS), Visible Infrared Imaging Radiometer Suite (VIIRS), and Sea and Land Surface Temperature Radiometer (SLSTR) onboarding sun-synchronous satellites.14 introduces MODIS AF product with a contextual algorithm and updated in15 by changing the threshold according to a larger context window. Currently, the MODIS Collection 6 active fire detection algorithm is still operating8. SLSTR onboarding Sentinel-3 also introduces its AF product in16 which provides a temporal and spatial resolution like the MODIS AF product. While all AF"}, {"title": "Burned Area Mapping", "content": "Existing burned area products provided by NASA are often based on MODIS (MCD45 and MCD64)5,22,23. MCD45 relies on the reflectance difference between burned and unburned areas in the Short-wave Infrared (SWIR) and Near Infrared (NIR) bands. MCD64 further incorporates the Medium Infrared (MIR) band along with the SWIR-NIR difference. Additionally, there is a VIIRS-based burned area mapping product (VNP64A1), which provides a spatial resolution of 500 meters. However, these products only offer burned area information on a monthly basis. Daily mapping of wildfire progression remains a challenge. Another approach for burned area mapping involves clustering the hotspots detected by AF products each day17. However,24 suggests that the accumulation of hotspots may lead to sparse burned area mapping which has significant omission errors. The Fire CCI product is introduced by the European Space Agency in9,25. The product relies on the Near Infrared (NIR) band of MODIS to map burned areas on a monthly basis. However, the accuracy is also compromised by only using the NIR band. There are also multiple deep-learning models used for burned area mapping. Deep Learning for burned area mapping with mid-resolution sensors onboarding satellites like Sentinel-1/2 and Landsat-8/9 is broadly investigated11,26\u201329. However, due to their low temporal resolution, these methods are not suitable for monitoring burned areas at high frequency. With sensors possessing high temporal resolution, several deep learning models have been applied for continuous burned area mapping. For instance, 30 employs a Multi-Layer Perceptron (MLP) for pixel-wise burned area mapping using four spectral bands of VIIRS images. Similarly,31 utilizes ConvLSTM to process VIIRS image time-series consisting of 750m spatial resolution moderate bands. This indicates that leveraging both spatial and temporal information can more effectively detect burned areas from VIIRS datacubes. MesogeosAM32 introduces a multi-task wildfire dataset focusing on the Mediterranean region. The dataset"}, {"title": "Wildfire Progression Prediction", "content": "Commonly used methods for predicting wildfire progression, like FARSITE33 and Prometheus34, are semi-empirical methods, combining physical knowledge with some aspects determined from experimental observations. In recent years, purely empirical approaches based on deep learning models have been investigated as an alternative. FireCast35 utilizes a 6-layer Convolutional Neural Network and Landsat-8 images together with elevation, weather and wind information to forecast the progression on the next day. The model shows better accuracy compared to FARSITE. In36, a tabular dataset is proposed to predict if the fire propagates to neighbourhood locations based on weather, elevation, and vegetation information at that location. NextDayWildfireSpread37 provides an image-based dataset consisting of information related to weather, elevation, vegetation and population density. It proposes to use a convolutional autoencoder to process the input data to forecast the active fire on the next day. Following NextDayWildfireSpread, WildfireSpreadTS38 leverages multi-temporal information for wildfire progression prediction, compared to the mono-temporal approach in37. Besides most of the auxiliary data modalities used by NextDayWildfireSpread, WildfireSpreadTS adds land cover information and weather forecasts. Moreover, the active fire masks are improved from MODIS to VIIRS, which provides a better spatial resolution. WildfireSpreadTS uses U-Net, ConvLSTM and UTAE as baseline models, though baseline results highlight the difficulty of the progression prediction task. Notably, research integrating wildfire monitoring tasks, such as active fire and burned area detection, with wildfire progression prediction remains limited. With the advent of foundation models, multi-task datasets like this can serve as both training and benchmark datasets, playing a pivotal role in advancing Earth observation foundation models."}, {"title": "Methods", "content": "The details of the spectral bands and auxiliary data are provided in this section. VIIRS images are downloaded from NASA's Level-1 and Atmosphere Archive & Distribution System (LAADS) and processed locally. All the auxiliary data are downloaded and processed by Google Earth Engine39."}, {"title": "Spatial distribution of fires", "content": "As shown in Figure 2, the training dataset consists of wildfire events between 2017 and 2020 covering the US main continent. In total, there are 34 fire events used in 2017, 34 events in 2018, 8 events in 2019 and 49 events in 2020. As for the test set of burned area mapping and fire progression prediction tasks, 24 fire events from 2021 are used. There are 13 wildfire events picked from 2017-2020 used as the validation set. For active fire detection tasks, the test set consists of 17 wildfire events between 2018 and 2022 across multiple continents."}, {"title": "Overview of the input data sources", "content": "VIIRS Imagery\nThe VIIRS sensor is operational on multiple satellites, including Suomi-NPP, NOAA-20, and NOAA-21. This sensor provides data across 22 different spectral bands, categorized into Imagery Bands and Moderate Bands. The Imagery Bands cover wavelengths from Red to Long-wave Infrared, offering a spatial resolution of 375 meters at nadir. The Moderate Bands cover a broader range of wavelengths but have a spatial resolution of 750 meters. In TS-SatFire, six spectral bands are utilized, including Imagery Bands 11-15 and Moderate Band M11, as shown in Table 1. Band I4, the Medium Infrared Band with a"}, {"title": "Auxiliary Data", "content": "For the progression prediction task, the auxiliary data follow the setup of WildfireSpreadTS. As shown in Table 1, there are three sub-classes of these data, weather/weather forecast, topography, and landcover."}, {"title": "Weather Data and Forecast Data", "content": "The Gridded Surface Meteorological Dataset (GRIDMET)40 and the Global Forecast System (GFS)41 supply the weather data and its forecast, respectively, and are exclusively available for the US region. GRIDMET offers a spatial resolution of 4638 meters, providing a coarse observation of the weather. Since wind plays a significant role in wildfire propagation, factors such as wind speed and direction are prioritized. Additionally, precipitation, specific humidity, and the Palmer Drought Severity Index (PDSI) influence vegetation and soil moisture, consequently impacting wildfire ignition negatively42. Extreme temperatures have also been closely correlated with wildfire events43, hence temperature data is included in this dataset. Furthermore, the Energy Release Component, an index proposed by the National Fire Danger Rating System (NFDRS) that describes the potential intensity of a wildfire, is incorporated. On the other hand, GFS offers hourly weather forecasts at a spatial resolution of 27.83 kilometers. It forecasts wind, temperature, and humidity information on an hourly basis, akin to GRIDMET. \u03a4\u03bf synchronize the temporal resolution, the dataset utilizes the average value over 24 hours of forecast data, based on forecasts made at the end of the 'current' day, without using future information."}, {"title": "Topography and Landcover Data", "content": "Topography can affect the wildfire progression by affecting the wind flow44. We use the NASA SRTM Digital Elevation dataset45 to derive the slope and aspect of the surface. The SRTM dataset provides 90m spatial resolution, and SRTM version 3 is used in the dataset. Land cover information is essential to understand the fuel types of the burning biomass. Fuel types affect the progression speed and severity of the wildfire. The land cover used in this project is based on the 500m MODIS Land Cover Type Yearly Global product (MCD12Q1.061)46,47."}, {"title": "Labels", "content": "Active Fire Label\nFor the active fire detection task, the training labels are sourced from the NASA VIIRS AF product. Due to potential errors within the AF product, we manually inspect the AF labels visually as a quality control procedure, ensuring that the AF labels correspond to the bright spots observed on Band I3-15 and M11. Areas that do not pass this inspection are removed from the training set. For the test labels, we manually set the threshold to Band 14/15 to ensure alignment with the bright spots observed in the images. Examples are included in the supplementary material for reference. In total, 17 wildfire events across the globe are utilized as the test sites, with geolocations and start/end dates of each test wildfire event provided in the supplementary material.\nBurned Area Label\nCreating the burned area labels is challenging because they require daily captures. While this is also true for active fire labels, the spectral bands which are sensitive to the burned area (Short-wave Infrared and Near-Infrared) are more prone to be affected"}, {"title": "Preprocessing", "content": "For active fire detection, the arrays from the GeoTIFF file are directly used as input after the normalization. For the burned area mapping task, bands I4 and I5 in the day and night captures are aggregated by taking the pixel-wise maximum over all images in the current and previous timestamps. This is because Band I4 and I5 are sensitive to the ground temperature. By aggregating all previous images with the maximum value, total burned areas will be highlighted in these two spectral bands. The same preprocessing of Band 14-15 is also applied to spectral bands of the fire prediction tasks. As for the auxiliary data, the pixel-wise median of GRIDNET weather data and the pixel-wise mean of weather forecast data are used. For the drought index (PDSI) and Landcover, the pixel-wise median data are used.\nTo go from the full time-series of images for each fire to a fixed-size input that we can present to the various models, we first sample the image time-series with a length of T from the full image time series of each fire. For active fire detection and burned area mapping, the sampling interval between each window is 1 for the training set and T for the test set. For the prediction task, the sampling interval for the testset is set to 1. Since different models expect different input shapes, further processing is applied. Temporal models like GRU, LSTM and T4Fire take pixel time-series as the input. Therefore, each image time-series with shape (W, H,T,C) is divided into W * H pixel time-series with the shape (T,C). For the spatial models, each image within the image time-series is used as an individual sample. For the spatial-temporal model, the image time-series with length T is directly used as the input.\nData sources used in this work are presented in Table 2. Satellite imagery is generated from four Level-1B VIIRS satellite products. VNP02IMG and VNP02MOD provide the raster of 375m imagery bands and 750m moderate bands and VNP03IMG and VNP03MOD provide the geolocation of the raster. For other auxiliary data, they are resampled to 375m with bilinear interpolation."}, {"title": "Missing values", "content": "The percentage of missing values is shown in Table 3 for all the features used in active fire detection, burned area mapping and fire progression prediction tasks. The rates of the six spectral bands of day images are below 2% of measurements. Rates of spectral bands of night images have around 10% missing values, which indicates the night images are not always available. For other auxiliary data, there rates of missing value are below 5%. In the preprocessing, all the missing values are replaced with zeros."}, {"title": "Data Records", "content": "The TS-SatFire dataset is available on Kaggle (https://www.kaggle.com/datasets/z789456sx/ts-satfire)48.\nIt includes 179 distinct wildfire events, each organized into a separate folder named by its fire ID. Fire IDs are derived from the GlobFire dataset (training set 2017-2020), the MODIS monthly burned area product (BA/Pred test set 2021), and corresponding names (AF test set). Within each folder, auxiliary data are stored in the FirePred folder, while VIIRS images are separated into VIIRS_Day and VIIRS_Night folders. All data are provided in GeoTIFF format and are co-registered to the same region of interest. The dataset captures the lifecycle of each wildfire, covering events in the contiguous U.S. from January 2017 to"}, {"title": "Technical Validation", "content": "Candidate Temporal Models for Benchmark\nPurely temporal models are only used in active fire detection tasks. These models are designed to classify pixel time-series. The results generate an active fire map according to the input pixels' original positions.\nGRU/LSTM For sequential models like GRU and LSTM, we follow the model setup used in21. Both GRU and LSTM models have a hidden size of 64 and consist of 3 layers each. A dense layer is then used to classify each pixel as fire or non-fire.\nT4Fire T4Fire is a Transformer-based model proposed in21 for classifying pixel time-series. We adhere to the same hyperparameters as proposed in21."}, {"title": "Candidate Spatial and Spatial-Temporal Models for benchmark", "content": "All spatial models are applied to both active fire detection and burned area detection tasks. These models take 2-dimensional images as input and produce the corresponding active fire and burned area maps as output.\nU-Net We use a U-Net49 as the weak baseline for both detection tasks, using only 2D input. For the U-Net-3D, the 2D convolutions are exchanged with 3D convolutions. The model thus treats the temporal dimension similarly to the two spatial dimensions, except that we do not use downsampling in the temporal dimension in any of our models.\nAttention U-Net Attention U-Net is an improved version of the U-Net. It uses an attention mechanism, which enables learning with a focus on specific regions of the image. Similarly, Attention U-Net-3D also changes 2D convolutions to 3D to process 3D tensors.\nUNETR UNETR50 has a similar shape as U-Net. The major difference is the encoder, which is changed from a ConvNet to a Vision Transformer. The model was originally proposed to process 3D data. To process 2D data with it, we use a variant UNETR-2D, which is only applied to the spatial dimensions as a U-Net. For time-series input, the model treats the temporal dimension as the third spatial dimension, in addition to height and width.\nSwinUNETR SwinUNETR51 was designed for 3D semantic segmentation tasks, such as those involving 3D medical images. Similar to UNETR and U-Net, it has an encoder-decoder architecture with skip connections. The major difference is the backbone, which is changed from a Vision Transformer to a Swin-Transformer52. For 2D input, SwinUNETR processes"}, {"title": "Baseline Results", "content": "The quantitative baseline results are reported in Table 3 and the qualitative results are provided in the supplementary material. For the AF task, all models are tested over 17 wildfire events across the globe. The geolocation of these study areas can be found in the supplement material. Among all models for AF detection, T4Fire can achieve better prediction results compared to other spatial models and temporal models with a significantly lower number of parameters. Noticeably, all temporal models have a lower number of parameters. This is because the solution is pixel-based. Consequently, the inference speed is the major bottleneck of the methods. The spatial-temporal model UNETR-3D achieves a slightly better performance than T4Fire. Compared with UNETR-2D, which has lower performance in the AF task, the result of UNETR-3D highlights the importance of using temporal information to detect active fire.\nFor the BA task, the temporal models have not been tested. The test dataset consists of 24 wildfire events generated from 2021 wildfire events in the US. Comparing spatial-temporal models with spatial models on burned area tasks, spatial-temporal models like SwinUNETR-3D show privilege in F1 Score and IoU Score over all other spatial models including the 2D version of SwinUNETR. It demonstrates the importance of using temporal information when segmenting low-resolution satellite images which also agrees with the finding of the AF task.\nComparing the results of spatial models between the AF and BA tasks, all results from the BA task are higher than the AF task. One major reason for this is the test datasets are different. However, from the comparison between spatial and temporal models, it can be observed that spatial models are not suitable for detecting active fires. The main reason is that AF labels are generally more sparse than BA labels, which makes the spatial information less useful in detecting AF than BA.\nIn the spatial-temporal models section in Table 3, the quantitative results for burned area mapping and progression prediction tasks are compared. The prediction F1 Scores and IoU Scores are notably lower than those of the detection tasks, reflecting the greater complexity of the prediction task, which cannot be effectively addressed by standard segmentation models \"out of the box.\" While U-Net-3D achieves the best performance among the baseline models, the quantitative results across all four baselines remain similar. This highlights that image segmentation models like U-Net and SwinUNETR, designed to extract spatial features from input data, are not well-suited for prediction tasks that require inferring future states from past information. As shown in Table 4, the models with the best F1 Score and IoU Score are trained with three different random seeds. The"}, {"title": "Ablation Study", "content": "Time-series Length\nTable 5. Ablation study on the length of the time-series, length equals 2, 4, 6 are tested for BA, AF and Prediction tasks. The tested model is SwinUNETR-3D.\nFor temporal models and spatial-temporal models, the length of the input time-series determines how much temporal information the model consumes. In Table 5, the lengths of the times-series are set to 2,4,6 and the metrics under these configurations are provided. The F1 and IoU Scores of SwinUNETR-3D for the BA and AF tasks decrease as the time-series length increases, indicating that this naive adaptation does not effectively utilize the additional temporal information. However, compared to SwinUNETR-2D, SwinUNETR-3D shows improved F1 and IoU Scores when using a 2-day image time-series,"}, {"title": "Spectral Feature Importance", "content": "In Figure 4, we measure the importance of each feature by setting that feature to zero and assessing the resulting performance in AF and BA tasks. For the prediction task, the result is presented in the supplementary material. Band I2 contribute the most to the performance of the BA and AF tasks. The Near Infrared Band I2 contrasts the burned area and the active fire with the detection of I3, I4, and M11. Band I4 detects temperature anomalies which is crucial for detecting active fire. The burned area can be detected by aggregating the Band I4 images from the start of the wildfire based on their maximum value. Band M11 and Band I3 have similar contributions to the performance of AF and BA tasks. Short-wave Infrared bands M11 and 13 are useful in differentiating burned and unburned areas. For the thermal band I5, both day and night captures do not significantly contribute to the results."}, {"title": "Feature importance for fire prediction", "content": "As a very basic indicator of feature importance, we remove the information in one individual feature at a time by setting it to zero. For the standardized features, this represents setting it to their mean value. For the features that use degrees in [0,360], i.e. wind direction, forecast wind direction and aspect, this represents 0 degrees, and for the one-hot encoded land cover class, it represents an absence of all land covers. The results are shown in Figure 5. They show that bands I2 and M11 are the most important features, leading to a reduction in F1 score of 11% and 6.5% when zero-ed out, respectively. In the main paper, band I2 also emerged as the most important band for the burned area detection, while band M11 was the third most important, though with much less influence than I2. Unlike in the burned area detection importance, most features seem to have little to no influence on the prediction task. Detecting the current burned area with bands I2 and M11 is a prerequisite to predicting where the next day's burned area will be, so it makes sense that these two features have a high importance. However, the model seems to fail to make use of the other features to be able to predict how exactly the fire will spread. There are three potential reasons for this low contribution of auxiliary data. Firstly, the temporal and spatial resolution of the data may be too low to make these predictions. Secondly, the segmentation models may not be able to extract useful features from the auxiliary data for prediction. Finally, naive concatenation of auxiliary data and the spectral bands may not be the best option for data fusion."}, {"title": "Qualitative results", "content": "Active Fire Detection\nThe qualitative results are shown in Figure 6, the classified active fire are overlayed on top of the Band I4 images. For Figure 6, false negative detection is the major limitation of the used model."}, {"title": "Burned Area Mapping", "content": "For the burned area mapping task, the qualitative results are shown in Figure 7. The classification map is overlaid on top of accumulated Band I4 images. For the three study regions presented, all burned areas are well detected."}, {"title": "Fire prediction", "content": "For the fire prediction task, the same study regions as the burned area mapping task are used. From Figure 8, it can be observed that the results from SwinUNETR contain many false positive and false negative predictions. Note that the prediction model only predicts the newly burned area of each day, leading to the white areas in the center of the fire not being part of the daily prediction. Although the model makes some predictions around the burned area boundaries, the accuracy is limited. It also suggests that the prediction task is much more challenging than detection."}, {"title": "Usage Notes", "content": "Example Python scripts to process GeoTIFF files into Numpy arrays for deep learning models are provided in (dataset_gen_afba.py and dataset_gen_pred.py) within the repository. All wildfire events and their associated information, including start and end"}, {"title": "Code Availability", "content": "The python scripts used to process the GeoTIFF to the format consumed by the models and all the baseline models used as the benchmark are provided in GitHub Repository https://github.com/zhaoyutim/TS-SatFire."}]}