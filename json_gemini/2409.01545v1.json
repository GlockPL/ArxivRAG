{"title": "EFFECTIVE NOISE-AWARE DATA SIMULATION FOR DOMAIN-ADAPTIVE SPEECH\nENHANCEMENT LEVERAGING DYNAMIC STOCHASTIC PERTURBATION", "authors": ["Chien-Chun Wang", "Li-Wei Chen", "Hung-Shin Lee", "Berlin Chen", "Hsin-Min Wang"], "abstract": "Cross-domain speech enhancement (SE) is often faced with\nsevere challenges due to the scarcity of noise and background\ninformation in an unseen target domain, leading to a mis-\nmatch between training and test conditions. This study puts\nforward a novel data simulation method to address this issue,\nleveraging noise-extractive techniques and generative adver-\nsarial networks (GANs) with only limited target noisy speech\ndata. Notably, our method employs a noise encoder to ex-\ntract noise embeddings from target-domain data. These em-\nbeddings aptly guide the generator to synthesize utterances\nacoustically fitted to the target domain while authentically\npreserving the phonetic content of the input clean speech.\nFurthermore, we introduce the notion of dynamic stochastic\nperturbation, which can inject controlled perturbations into\nthe noise embeddings during inference, thereby enabling the\nmodel to generalize well to unseen noise conditions. Ex-\nperiments on the VoiceBank-DEMAND benchmark dataset\ndemonstrate that our domain-adaptive SE method outper-\nforms an existing strong baseline based on data simulation.", "sections": [{"title": "1. INTRODUCTION", "content": "Speech enhancement (SE) is crucial for improving speech\nsignal quality and intelligibility by mitigating the detrimen-\ntal impacts of background noise and interference. Recent\nyears have witnessed remarkable advancements in SE driven\nby deep learning instantiated with models such as convolu-\ntional neural networks (CNNs) [1, 2], recurrent neural net-\nworks (RNNs) [3, 4], Transformer [5], generative adversar-\nial networks (GANs) [6\u20139], and many others. In particu-\nlar, iconic supervised-learning approaches to SE typically rely\non paired noisy and clean speech data to train their associ-\nated models that can transform a noisy speech signal into its\nclean (or enhanced) counterpart. However, these models of-\nten struggle with the domain mismatch problem caused by\nthe inherent variability of real-world noise distributions that\nare difficult to fully render in training datasets. Consequently,\nthe models trained on specific noise distributions may ex-\nhibit significant performance degradation when faced with\nunseen noise types or acoustic environments during deploy-\nment, highlighting a critical challenge in developing robust\nand widely-applicable speech enhancement systems.\nTo overcome the challenge of domain mismatch, re-\nsearchers have explored various domain adaptation tech-\nniques [10-15], aiming to bridge the gap between the noise\ndistributions in the training and test conditions. These tech-\nniques, however, often involve complex training procedures\nor might fail to fully exploit the relationships between do-\nmains. Recently, data simulation has emerged as promising\napproaches for unsupervised noise adaptation in SE [16, 17].\nThese approaches, unlike traditional adversarial training, di-\nrectly synthesize noisy speech tailored to a target domain\nusing clean speech. By leveraging the domain-invariant na-\nture of clean signals, the associated models generate noisy\nspeech that closely resembles those expected in the target\ndomain, allowing for efficient model adaptation through\nfine-tuning on a large, simulated parallel dataset. Notably,\nthese models exhibit high data efficiency, requiring only few\nminutes of unlabeled target-domain data to learn the noise\ndistribution embodied in the spectrogram. Additionally, they\noperate without paired training data, focusing only on the\ntarget noise while deriving the clean-to-noisy transformation\nfrom non-matched utterances. However, existing data sim-\nulation approaches for SE mainly focus on replicating the\noverall spectral characteristics of the target-domain noise.\nThey often fail to explicitly model and incorporate the intri-\ncate, fine-grained features of the noise, thereby potentially\nlimiting the generalization ability of the trained SE models.\nTo address the limitation of existing data simulation ap-\nproaches, we propose a novel Noise-Aware Domain-Adaptive\nmethod using Generative Adversarial Networks (dubbed\nNADA-GAN) for SE, specifically designed to capture and\nleverage the intricate noise characteristics of the target do-\nmain, even with limited data. Our method employs a dedi-\ncated noise encoder to extract rich noise embeddings directly\nfrom target-domain noisy speech. These embeddings then\nguide a generator network to synthesize noisy speech that"}, {"title": "2. PROPOSED METHOD", "content": "Figure 1 schematically depicts the architecture of our pro-\nposed method, NADA-GAN. The process starts with a gen-\nerator G, which takes as input a clean magnitude spectrogram\n(hereafter referred to as spectrogram) $Y^S$ from the source do-\nmain S and generates a simulated spectrogram $X^G$ that ap-\nproximates the noise characteristics in the target domain T.\nTo achieve this, the generator utilizes the target noise embed-\nding $N^T$ derived from the target noisy spectrogram $X^T$ by\nthe noise encoder B. The role of the discriminator D is to dis-\ntinguish the target noisy spectrogram $X^T$ from the simulated\nspectrogram $X^G$, thereby providing feedback to help enhance\nthe generator's output. In addition, the noise reconstruction\nloss $L_{nse}$ is employed to ensure accurate reconstruction of\nthe noise embedding."}, {"title": "2.1. Generator and Discriminator", "content": "The generator G transforms a clean spectrogram from the\nsource domain into its simulated counterpart. Specifically, the\nclean spectrogram is first encoded through two 2D downsam-\npling convolutional layers (kernel size: 3 \u00d7 3, stride: 2 \u00d7 2),\nfollowed by nine residual blocks for capturing deep represen-\ntations. Each residual block consists of two convolutional lay-\ners (kernel size: 3 \u00d7 3, stride: 1 \u00d7 1) and a dropout layer. Fi-\nnally, two transposed convolutional layers (kernel size: 3 \u00d7 3,\nstride: 2 \u00d7 2) act as a decoder, upsampling the representations\ninto the corresponding simulated spectrogram.\nThe discriminator D differentiates between the simulated\nspectrogram and the real target noisy spectrogram. It consists\nof five 2D convolutional layers (kernel size: 4 \u00d7 4) followed\nby Leaky ReLU activation functions. The stride is set to 2 \u00d72\nfor the first three layers and 1 \u00d7 1 for the last two. We employ\nthe adversarial loss during training:\n$L_{adv} (G,D,X^T, Y^S, N^T) = E_{x \\sim X^T} [log D(x)]$\n$+ E_{y \\sim Y^S,n \\sim N^T} [log(1 \u2013 D(G(y, n)))]$.\nThis loss function encourages the generator to produce a spec-\ntrogram similar to the target noisy spectrogram. Although the\nspeech content might differ, the discriminator here focuses\nexclusively on the background noise for discrimination."}, {"title": "2.2. Noise Encoder", "content": "Drawing inspiration from recent advancements in noise-\naware speech processing [18, 19], we propose the incorpo-\nration of a dedicated noise encoder, denoted by B for our\npurpose. This encoder is tasked with extracting salient noise\nembeddings of the target noisy spectrogram, denoted by $N^T$,\nfrom the outputs of its penultimate layer. By explicitly pro-\nviding these noise representations to the generator module,\nour model gains a deeper understanding of the specific noise\ncharacteristics inherent to the target domain, enabling more\ninformed and tailored noise synthesis.\nTo extract noise embeddings richly imbued with noise-\nspecific information, we leverage a pre-trained audio model\nknown as BEATs [20]. Pre-trained on the extensive AudioSet\ndataset [21], BEATs possesses a strong foundation in generat-\ning audio feature representations. To further enhance its noise\ndiscriminative capabilities, we fine-tune BEATs using a two-\npronged approach: 1) We train BEATs to classify noise types\non the VoiceBank-DEMAND dataset [22]. This dataset pro-\nvides a diverse set of 10 distinct noise types, empowering the\nencoder to discern between a variety of noise characteristics.\n2) We treat each utterance within the target noisy dataset used\nfor GAN training as a unique noise type. This fine-tuning\nstep further reinforces the noise encoder's ability to capture\nthe specific noise nuances present in the data it will encounter\nduring the speech enhancement process.\nTo ensure faithful noise information reconstruction, we\nintroduce the noise reconstruction loss:\n$L_{nse} (G, Y^S, N^T) = E_{y \\sim Y^S,n \\sim N^T} [||n \u2013 B(G(y, n))||_1]$.\nThis loss function ensures that the generated samples retain\nthe noise details present in the target domain."}, {"title": "2.3. FILM", "content": "We adopt Feature-wise Linear Modulation (FiLM) [23] to in-\ncorporate noise embeddings into the generator module. At\nthe outset, the noise embedding $N^T$ undergoes separate lin-\near transformations to produce weight W and bias b:\n$W = Linear(N^T), b = Linear(N^T)$.\nFollowing this, the output feature F from a specific layer\nwithin the generator is multiplied by W and biased by b:\n$F' = W \\times F + b$.\nFinally, the adjusted feature F' propagates through the re-\nmaining layers of the generator. This approach is applied to\nthe output features of the encoder layer and all nine ResNet\nblocks within the generator."}, {"title": "2.4. Patch-wise Contrastive Learning", "content": "To preserve linguistic consistency between the generated\nnoisy speech and the original clean audio, we employ patch-"}, {"title": "wise Contrastive Learning", "content": "wise contrastive learning, as introduced by Park et al. [24],\nwhich homes in maximizing the mutual information, specif-\nically the shared speech content information, between the\nsource clean and the simulated spectrograms.\nWe utilize the generator to extract feature representations\nfor both the clean and simulated spectrograms. Within these\nrepresentation spaces, a small patch from the simulated repre-\nsentation is selected as the \"query\". The corresponding patch\nfrom the clean representation serves as the \"positive\u201d sam-\nple. To provide contrasting examples, we randomly sample\n256 patches from the clean representation to act as \"nega-\ntive\" samples. These patches are then projected into a lower-\ndimensional embedding space using two linear layers with\n256 units each, followed by ReLU activation. The contrastive\nloss, calculated across five layers of the generator's feature hi-\nerarchy, measures the cross-entropy loss between the \"query\"\npatch and both the positive and negative patches. This formu-\nlation encourages the model to learn representations where\ncorresponding patches in the clean and noisy domains ex-\nhibit high similarity, while simultaneously being easily dis-\ntinguishable from randomly sampled patches. Formally, the\npatch-wise contrastive learning loss is defined by:\n$L_{pcl} (G, Y^S) = \\sum_{l=1}^{L} \\sum_{i=1}^{I} - log\\frac{e^{(\\frac{z_i \\cdot z_i}{\\tau})}}{\\sum_{j=1}^{J} e^{(\\frac{z_i \\cdot z_j}{\\tau})}}$\nwhere $z_i$ represents the $i^{th}$ positive patch from clean rep-\nresentations at the $l^{th}$ layer of the generator, \u00a1 denotes the\ncorresponding patch from simulated representations, and $z_j$\nrefers to the $j^{th}$ negative patch from simulated representa-\ntions at the same layer. The temperature parameter $\\tau$ controls\nthe contrastive learning process. We apply the loss func-\ntion to both source clean ($L_{pcl}(G, Y^S)$) and target noisy\n($L_{pcl}(G,X^T)$) spectrograms to allow for consistent speech\ncontent and minimize unnecessary alterations."}, {"title": "2.5. Training Objective", "content": "During the training phase, the main objective is to optimize\nthe GAN model using a comprehensive loss function that in-\ntegrates multiple components. The total loss function com-\nbines adversarial loss, patch-wise contrastive learning losses\nfor both source clean and target noisy spectrograms, and the\nnoise reconstruction loss, which is formulated by:\n$L_{total} =L_{adv}(G, D, X^T, Y^S, N^T) + L_{pcl}(G,Y^S)$\n$+ L_{pcl}(G,X^T) + \\lambda_{nse}L_{nse}(G, Y^S, N^T)$,\nwhere $\\lambda_{nse}$ is a hyperparameter that regulates the influence of\nthe noise reconstruction loss.\nGiven limited noisy data from the target domain, the\nsame amount of clean speech are randomly sampled from the\nsource domain. Subsequently, our model is trained using the\nunpaired dataset and optimized with $L_{total}$ in Eq. (6)."}, {"title": "2.6. Inference with Dynamic Stochastic Perturbation", "content": "After training, our well-trained generator serves as a domain\nconverter $F_{S\\sim T}$ from $Y^S$ to $X^T$, working alongside the fine-\ntuned noise encoder to simulate noisy speech. The simulation\nutilizes abundant clean speech data and randomly selected\ntarget noisy speech from the training phase, providing suf-\nficient paired data of clean and simulated speech. This aug-\nmented data can be further utilized to fine-tune any down-\nstream speech enhancement models for domain adaptation.\nDynamic stochastic perturbation is a technique used dur-\ning the inference stage to improve the adaptability of speech\nenhancement models. It involves adding Gaussian noise, with\nadjustable standard deviation, to noise embeddings extracted\nfrom target noisy spectrograms. The standard deviation rep-\nresents the different degrees of spread used for generating the\nstochastic perturbation. The dynamic noise injection allows\nfor fine-tuning of noise levels, assisting in preventing over-\nfitting to specific noise patterns encountered during training.\nMoreover, it enhances the resilience of speech enhancement\nmodels to unseen noise conditions, thereby bolstering their\ngeneralization capabilities."}, {"title": "3. EXPERIMENTS", "content": "We evaluated our proposed NADA-GAN method on the\nVoiceBank-DEMAND dataset [22], a widely used bench-\nmark for speech enhancement. This dataset consists of\nnoisy speech recordings generated by artificially mixing clean\nspeech samples from the VoiceBank corpus [22] with noise\nsamples from the DEMAND database [25]. It comprises\nrecordings from 30 speakers, with 28 used for training and\n2 reserved for testing. The training set (source domain) con-\ntains 11,572 utterances created by mixing clean speech with\n10 noise types at four signal-to-noise ratio (SNR) levels (0,\n5, 10, and 15 dB). Conversely, the test set (target domain)\nconsists of 824 utterances featuring five unseen noise types at\nfour SNR levels (2.5, 7.5, 12.5, and 17.5 dB).\nTo facilitate efficient training with limited target data, we\nrandomly sampled 40 unpaired utterances: 40 clean speech\nutterances from the source domain and 40 noisy speech ut-\nterances from the target domain (8 utterances per noise type).\nFurthermore, we excluded the portion of the original test set\nused for training the data simulation models. Consequently,\nthe final test set was reduced from 824 to 784 utterances."}, {"title": "3.1. Dataset", "content": "We evaluated our proposed NADA-GAN method on the\nVoiceBank-DEMAND dataset [22], a widely used bench-\nmark for speech enhancement. This dataset consists of\nnoisy speech recordings generated by artificially mixing clean\nspeech samples from the VoiceBank corpus [22] with noise\nsamples from the DEMAND database [25]. It comprises\nrecordings from 30 speakers, with 28 used for training and\n2 reserved for testing. The training set (source domain) con-\ntains 11,572 utterances created by mixing clean speech with\n10 noise types at four signal-to-noise ratio (SNR) levels (0,\n5, 10, and 15 dB). Conversely, the test set (target domain)\nconsists of 824 utterances featuring five unseen noise types at\nfour SNR levels (2.5, 7.5, 12.5, and 17.5 dB).\nTo facilitate efficient training with limited target data, we\nrandomly sampled 40 unpaired utterances: 40 clean speech\nutterances from the source domain and 40 noisy speech ut-\nterances from the target domain (8 utterances per noise type).\nFurthermore, we excluded the portion of the original test set\nused for training the data simulation models. Consequently,\nthe final test set was reduced from 824 to 784 utterances."}, {"title": "3.2. Speech Enhancement Backbone Model", "content": "We employed DEMUCS [26] as our downstream evalua-\ntion model. DEMUCS is a real-time SE model that oper-\nates directly in the waveform domain. It utilizes a causal\nencoder-decoder architecture with U-Net style skip connec-\ntions and convolutional layers, enabling real-time processing."}, {"title": "3.3. Training and Inference for Simulation", "content": "To better capture the short-time characteristics of noise, we\nsegmented the magnitudes into frames with dimensions of\n129 \u00d7 128. This segmentation allows the noise encoder to\nlearn more detailed noise-specific information. We trained\nour method for 400 epochs to learn a robust mapping between\nsource clean speech and target noisy speech. The hyperpa-\nrameter $\\lambda_{nse}$ in Eq. (6) was set to 10.0 based on empirical\nresults, striking a balance between noise reconstruction accu-\nracy and overall generative quality. We used the Adam opti-\nmizer [27] with an initial learning rate of 0.0002 for training.\nAfter training, we utilized the generator and the fine-tuned\nnoise encoder to create a simulated dataset. Following the\nsetup in [16], we used 11,572 clean speech samples from the\nsource domain and the same 40 target noisy samples used dur-\ning training. For each clean sample, we generated a corre-\nsponding noisy sample using the generator. We applied dy-\nnamic stochastic perturbation with a given standard deviation\nduring generation, resulting in a set of 11,572 paired clean\nand simulated utterances for SE model training."}, {"title": "3.4. Adaptation of SE Models", "content": "We fine-tuned the Vanilla-SE model on the simulated dataset\nfor two epochs. This fine-tuning process adapted the model to\nthe specific noise characteristics present in the simulated data,\nimproving its ability to generalize to unseen noise conditions."}, {"title": "4. RESULTS", "content": "Table 1 presents the main results on the VoiceBank-DEMAND\ndataset. Our proposed method outperforms the baseline ap-"}, {"title": "4.1. Main Results on VoiceBank-DEMAND", "content": "Table 1 presents the main results on the VoiceBank-DEMAND\ndataset. Our proposed method outperforms the baseline ap-"}, {"title": "4.2. Effects of Dynamic Stochastic Perturbation", "content": "Figure 2 illustrates the impact of varying the standard devi-\nation of the dynamic stochastic perturbation on our model's\nperformance. The results clearly demonstrate that perturba-\ntion, preventing model from overfitting with only 40 target\nnoisy utterances, can improve performance, but the optimal\nstandard deviation is crucial. Specifically, increasing the stan-\ndard deviation initially enhanced PESQ scores, peaking at\n3.14 with a standard deviation of 2.0. However, further in-\ncreases beyond this point led to a decrease in PESQ scores.\nThis suggests that excessive perturbation can degrade the per-\nceived quality of the enhanced speech. Conversely, STOI\nscores remained relatively stable across different standard de-\nviations, indicating that speech intelligibility is less sensitive\nto changes in perturbation intensity."}, {"title": "4.3. Ablation Studies", "content": "To assess the contribution of each component in our model,\nwe conducted ablation studies, with results presented in Ta-\nble 2. Removing the noise reconstruction loss (- $L_{nse}$) led\nto a slight decrease in performance. This suggests that while\nthe noise reconstruction loss contributes to maintaining noise\nfidelity during the domain adaptation process, its overall im-\npact on the final enhancement performance is relatively small.\nHowever, eliminating the noise embeddings (- Embed-\ndings) resulted in a more substantial performance drop. This\nfinding underscores the crucial role of noise embeddings in\ncapturing and transferring noise characteristics from the tar-\nget domain, ultimately leading to improved generalization to\nunseen noise conditions."}, {"title": "4.4. t-SNE Visualization of Noise Embeddings", "content": "To visualize the learned noise representations, we employed\nt-distributed stochastic neighbor embedding (t-SNE) [30] on\nthe noise embeddings extracted from the five unseen noise\ntypes in the VoiceBank-DEMAND test set. Figure 3 (a) and\n(b) display the embeddings obtained from the pre-trained\nBEATs and fine-tuned BEATs, respectively.\nA clear separation between different noise categories is\nevident in the embeddings from the fine-tuned BEATs, even\nfor non-stationary noise types. This improves discriminabil-\nity, compared to the pre-trained embeddings, highlighting the\neffectiveness of fine-tuning in adapting the noise encoder to\nthe target domain. The ability to distinguish between diverse\nnoise profiles contributes significantly to the enhanced perfor-\nmance of our proposed method."}, {"title": "4.5. MOS Evaluation on Simulated Data", "content": "To further evaluate the perceptual similarity of the generated\nnoisy speech to the target domain, we conducted a MOS eval-\nuation focusing on the non-stationary noise types. Table 3\npresents the results. Ten participants, representative of typi-\ncal listeners, participated in the evaluation. Each participant\nlistened to 10 audio samples per model through headphones\nin a quiet room, using the target noisy speech as a reference.\nThey were asked to rate the similarity of the background noise\nin each generated sample to the reference on a scale from 1 to\n5, with 5 indicating the highest similarity. The MOS for each\nmodel was then calculated by averaging the scores across all\nparticipants and all samples.\nThe results demonstrate that our proposed model achieves\na significantly higher MOS compared to the baseline model.\nThis indicates that our method generates noisy speech with\nbackground characteristics that more closely resemble the tar-\nget domain data, as perceived by human listeners."}, {"title": "4.6. Results on Various SNR Levels", "content": "To evaluate the robustness of our method under different noise\nconditions, we assessed the performance of all models across\nvarious SNR levels. Table 4 presents the results.\nAs expected, the baseline model outperforms Vanilla-\nSE, particularly at lower SNR levels, highlighting the effec-\ntiveness of its noise reduction capabilities. However, our\nproposed method consistently achieves the highest perfor-\nmance across all SNR levels, demonstrating significant and\nconsistent improvements over the baseline. These results\ndemonstrate that our proposed method is the most effective\nfor speech enhancement under a range of noise conditions."}, {"title": "4.7. SNR Distribution Analysis and Insights", "content": "Figure 4 presents the SNR distributions of the VoiceBank-\nDEMAND dataset. Notably, the SNR distribution of the tar-\nget noisy speech closely resembles that of the test speech,\nwith peaks around 0 dB and 15 dB SNR levels. This align-\nment between the target noisy speech and test speech corre-\nsponds to our data manipulation described in Section 3.1.\nIn contrast, the simulated speech for the SE models ex-\nhibits a peak predominantly between 0 dB to 5 dB SNR.\nThis concentrated distribution does not adequately reflect\nthe broader SNR spectrum present in the test speech or real-\nworld conditions. This observation highlights an area for"}, {"title": "5. CONCLUSION AND FUTURE WORK", "content": "This study\u00b9 introduces a novel approach to cross-domain\nspeech enhancement that leverages noise-extractive tech-\nniques and generative adversarial networks (GANs). By\nincorporating a noise encoder and dynamic stochastic per-\nturbation, our method effectively addresses the challenge of\ndomain mismatch. This leads to improved generalization ca-\npabilities, enabling speech enhancement models to perform\neffectively across diverse and unseen noise conditions.\nExperimental results on the VoiceBank-DEMAND dataset\ndemonstrate the effectiveness of our proposed method. We\nachieve superior performance compared to a strong existing\nbaseline across both objective and subjective metrics, includ-\ning PESQ, STOI, and MOS. These findings underscore the\neffectiveness of our approach in enhancing both the quality\nand robustness of speech enhancement in real-world scenar-\nios. In our future work, we plan to validate our methods on\nmore advanced SE models across various challenging envi-\nronments and datasets to thoroughly assess its effectiveness."}]}