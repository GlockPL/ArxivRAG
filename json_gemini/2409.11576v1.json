{"title": "Automating proton PBS treatment planning for head and neck cancers using policy gradient-based deep reinforcement learning", "authors": ["Qingqing Wang", "Chang Chang"], "abstract": "Background: Proton pencil beam scanning (PBS) treatment planning for head and neck (H&N) cancers is a time-consuming and experience-demanding task where a large number of potentially conflicting planning objectives are involved. Deep reinforcement learning (DRL) has recently been introduced to the planning processes of intensity-modulated radiation therapy and brachytherapy for prostate, lung, and cervical cancers. However, existing DRL planning models are built upon the Q-learning framework and rely on weighted linear combinations of clinical metrics for reward calculation. These approaches suffer from poor scalability and flexibility, i.e. they are only capable of adjusting a limited number of planning objectives in discrete action spaces and therefore fail to generalize to more complex planning problems.\nPurpose: Here we propose an automatic treatment planning model using the proximal policy optimization (PPO) algorithm in the policy gradient framework of DRL and a dose distribution-based reward function for proton PBS treatment planning of H&N cancers.\nMethods: The planning process is formulated as an optimization problem. A set of empirical rules is used to create auxiliary planning structures from target volumes and organs-at-risk (OARs), along with their associated planning objectives. Special attention is given to overlapping structures with potentially conflicting objectives. These planning objectives are fed into an in-house optimization engine to generate the spot monitor unit (MU) values. A decision-making policy network trained using PPO is developed to iteratively adjust the involved planning objective parameters. The policy network predicts actions in a continuous action space and guides the treatment planning system to refine the PBS treatment plans using a novel dose distribution-based reward function. A total of 34 H&N patients (30 for training and 4 for test) and 26 liver patients (20 for training, 6 for test) are included in this study to train and verify the effectiveness and generalizability of the proposed method.\nResults: Proton H&N treatment plans generated by the model show improved OAR sparing with equal or superior target coverage when compared with human-generated plans. Moreover, additional experiments on liver cancer demonstrate that the proposed method can be successfully generalized to other treatment sites.\nConclusions: The automatic treatment planning model can generate complex H&N plans with quality comparable or superior to those produced by experienced human planners. Compared with existing works, our method is capable of handling more planning objectives in continuous action spaces. To the best of our knowledge, this is the first DRL-based automatic treatment planning model capable of achieving human-level performance for H&N cancers.", "sections": [{"title": "INTRODUCTION", "content": "Treatment planning is a patient-specific optimization problem solved by human planners using treatment planning systems (TPS). Quality of the resultant plans is determined by the experience of the planners, algorithms in the TPS, and time available for planning. In the planning process, human planners first create numerous auxiliary planning structures and their corresponding objectives according to the clinical requirements. Using these objectives as inputs, the optimization engine in TPS calculates the machine deliverable parameters, such as the dwell time for brachytherapy and spot MU values for proton PBS. Dose-volume histograms (DVHs) are computed from the resultant dose distributions to help evaluate the quality of generated plans. Typically, objectives in TPS involve a set of hyper-parameters, such as objective weights, dose limits and volume constraints. If generated plans do not satisfy the clinical criteria, human planners have to modify planning structures and their objectives before conducting the optimization again. The interaction is repeated until a plan satisfying all clinical constraints is produced. This iterative process is time-consuming and labor-intensive, and it may take an experienced planner hours for complex treatment sites such as H&N. Therefore, an automatic treatment planning system capable of generating high-quality plans without the need for human interference is keenly desired. However, this expertise-demanding decision-making task is challenging and despite extensive efforts over the last few decades its resolution remains illusive.\nExisting automatic treatment planning techniques can be categorized into conventional machine learning approaches and more advanced deep learning-based methods. In the first group, recursive random search\u00b9, decision-function\u00b2, fuzzy inference\u00b3, regression model\u2074 and so forth were employed to determine a small group of planning objectives in an outer parameter adjustment loop, which was built on top of an inner iteration used to solve the optimization problem. Recently, deep learning was introduced to automate treatment planning and it has undoubtedly advanced the development of automatic TPS. For the task"}, {"title": "INTRODUCTION", "content": "of adjusting planning objective parameters, deep reinforcement learning (DRL), the most sophisticated decision-making method, was highly favored 5,6,7,8,9,10,11. For examples, Shen et al.\u2075,\u2076 provided a proof-of-principle study on learning the weight-tuning intelligence of a deep policy network trained in the trial-and-error manner of Q-learning\u00b9\u00b2. However, the size of their policy network increased linearly with the number of planning objective parameters, resulting in limited scalability. Subsequent study from the same group proposed another hierarchical policy network which adjusted one parameter for one structure at a time, therefore improving the model's scalability at the expense of efficiency. Pu et al.\u2078 built a dwell time prediction model for cervical brachytherapy by combining two variants of deep Q-learning. This approach was able to solve complex optimization problems, but it used a fixed time adjustment step of 0.1 seconds, instead of employing adaptively varied adjustment steps. Similarly, Hrinivich et al.\u00b9\u2070 employed deep Q-learning in their volumetric modulated arc therapy (VMAT) machine parameter optimization, and Zhang et al.\u00b9\u00b9 demonstrated an interpretable and reproducible treatment planning bot for pancreas SBRT.\nAlthough encouraging achievements have been accomplished from the above explorations, existing DRL-based automatic treatment planning still suffers from the following limitations: 1) scalability: existing methods are all developed using Q-learning, whose network size grows linearly with the number of parameters to be adjusted. Therefore, they are only suitable for simpler treatment sites involving a small number of planning objectives; 2) flexibility: existing methods predict actions in a discrete space, which limits their policy networks to a finite number of actions at fixed steps, seriously affecting adjustment efficiency and precision. As pointed out in earlier works \u2075,\u2078,\u2079, using continuous action space is a potential solution to this problem; 3) generalizability: existing methods rely heavily on weighted combinations of clinical metrics when calculating rewards for predicted actions, failing to make reasonable trade-offs when a diverse number of target volumes, OARS and prescription levels are involved across different treatment sites. As a result, existing approaches struggle to generalize to complex sites such as H&N.\nTo address these challenges, in this pilot study we propose an automatic treatment planning method using Proximal Policy Optimization (PPO)\u00b9\u00b3, which predicts actions in a continuous action space. First, a group of empirical rules are carefully designed to create auxiliary planning structures and their associated non-conflicting objectives for an in-house optimization engine, which generates the first iteration of spot MU values for the plan. Sec-"}, {"title": "MATERIALS AND METHODS", "content": "Patient cohorts are selected from those treated at California Protons Cancer Therapy Center from 2017 to 2023. The dataset consists of 34 bilateral and ipsilateral H&N cancer patients (30 for training and 4 for test), and 26 liver cancer patients (20 for training and 6 for test). Patients are treated with either single-field optimized (SFO) or multi-field optimized (MFO)\u00b9\u2075 plans that were manually planned by experienced dosimetrists. The number of treatment fractions ranges from 5 to 60 bid. Up to 5 treatment beams per plan are used and field-specific targets are created to constrain spot placements\u00b9\u2076. The model is designed to accommodate a maximum of 4 different prescription dose levels found in the H&N cohort. Prescription doses, beam angles, field targets and target volumes in our experiments are kept identical to those used in clinical plans. Spots with MU values less than the allowed minimum MU per spot are removed from the generated plans. The deliverable minimum MU per spot is a characteristic of the proton hardware and it is set to 3MU in our clinic."}, {"title": "Automatic treatment planning", "content": "Figure 1 depicts the design of the proposed automatic treatment planning model which is based on the typical treatment planning workflow. Auxiliary planning structures and associated objectives are first created with a set of empirical rules. Machine deliverable spot MU values are initialized by an in-house optimization engine using the L-BFGS algorithm. Planning objective parameters are then iteratively adjusted by the policy network to reach the final spot MU values. To build a standard pipeline suitable for all patients, all OARS"}, {"title": "Creation of auxiliary planning structures and objectives", "content": "For target volumes, we create a 3mm expansion from the clinical target volume (CTV) as PTV, a 2mm expansion from PTV as Gap, and a 20mm expansion from Gap as Ring (see Figure 2). In addition, 32 OARs are accounted for in the current H&N model. Following"}, {"title": "MATERIALS AND METHODS", "content": "established practice, critical OARs such as \u201cSpinal Cord\u201d and \u201cBrainstem\u201d are given higher priority than target volumes. To summarize, all target and OAR structures are classified into three groups:\nLevel 1: \u201cBrainstem\u201d, \u201cBrainstem Core\u201d, \u201cSpinal Cord\u201d, \u201cChiasm", "Optic Nerves": "nLevel 2: CTV, PTV;\nLevel 3: Ring, \u201cBone Mandible\u201d, \u201cBrachial Plexus\u201d, \u201cBrain\u201d, \u201cCavity Oral\u201d, \u201cCochlea", "Ear IntMid": "Esophagus", "Gland Submandibular": "Gland Thyroid\u201d, \u201cLarynx\u201d, \u201cLips", "Lungs\", \"Muscle Constrict\u201d, \u201cParotid\u201d, \u201cSpinal Canal\u201d, \u201cSkin\u201d, \u201cEye\u201d, \u201cOropharynx": "Gland Lacrimal", "Lobe Temporal": "Cornea", "Retina": "Vocal Cords\", \\\"Nasopharynx", "Carotid": "nIn general, we create objectives for the optimization engine by setting dose limits on the maximal dose ($D_{max}$), minimal dose ($D_{min}$) and mean dose ($D_{mean}$) of the associated structures. Specifically, target volumes require dose limits for both $D_{max}$ and $D_{min}$ so that their doses are constrained within a proper range. Rings and OARs require dose limits for $D_{max}$ or $D_{mean}$ so that normal tissues are appropriately spared. Apparently, there will be conflicts when CTVs and PTVs of different prescription levels overlap, and when CTVs and PTVs intersect with Rings and OARs. Conflicts between objectives pose challenges to the optimization and usually lead to undesirable results. Therefore, designing non-conflicting objectives for the optimization engine is vital, especially when multiple prescription dose levels are involved. To address this issue, we define the following empirical rules for planning structures and associated objectives:\n1. For all structures, portions outside the \"external\" or \"Body\" contour will be removed;\n2. If CTV (or PTV) intersects with \u201cSkin"}, {"title": "MATERIALS AND METHODS", "content": "8. When CTV; (or PTV\u00bf) intersects with OARj, if the priority of OAR; is higher, the overlap should be subtracted from CTV\u00bf (or PTV\u00bf); otherwise, the overlap should be subtracted from OAR;;\n9. $D_{max}$ objectives are used for structures related to Ring, \"Bone Mandible\", \"Brachial Plexus\", \"Brain\", \"Cochlea\", \"Spinal Canal\", \"Skin\", \"Eye\", \"Pituitary\", \"Lobe Temporal\", \"Retina\u201d, \u201cVocal Cords\u201d, \u201cCarotid\u201d, \u201cBrainstem\u201d, \u201cBrainstem Core\", \"Spinal Cord\", \"Optic Chiasm\" and \"Optic Nerve\", and $D_{mean}$ objectives are used for the rest of the OARS;\n10. Dose limits and weights for the $D_{min}$ and $D_{max}$ objectives of CTVs are not adjustable and they are fixed at 1.02 Rx, 1.03 $Rx_{max}$, 2.0 and 2.0, respectively. Likewise, these parameters are fixed at 0.97 Rx, 1.02 $Rx_{max}$, 1.0 and 1.0 for the $D_{min}$ and $D_{max}$ objectives of PTVs, respectively; and 0.93. Rx and 1.0 for the $D_{max}$ objectives of Rings. $Rx_{max}$ represents the maximal prescription dose amongst all target volumes;\n11. Dose limits for OARs' objectives are initialized to relatively larger values in order to minimize their interference with target coverages in the first iteration of the optimization. Specifically, a UNETR-like \u00b9\u2077 dose prediction model is trained in-house to estimate the dose distribution for each patient and the initial dose limits for each OAR are randomly sampled to be 10% to 30% larger than the UNETR's estimation. Weights for the OARs' objectives are initialized to 1.0 and will be iteratively adjusted by our policy network, together with their associated dose limits.\nOverall, the model is designed to accommodate up to 4 prescription dose levels, 50 planning structures, 58 planning objectives and 76 adjustable planning objective parameters. By contrast, the maximal number of objectives and adjustable parameters in existing works are 16 and 48, respectively, with only one prescription dose level allowed \u2075,\u2076,\u2077,\u2078,\u2079,\u00b9\u00b9.\""}, {"title": "Plan optimization with L-BFGS", "content": "Radiation therapy treatment planning can be formulated as an optimization problem:\n$X = arg min L(X)$\n$X$\n$ = arg min \\sum_{i \\in S_{Dmax}} w_i^2 \\bigg(\\frac{1}{N_i}\\sum \\|M_i X - D^{i}_{max}\\|^2\\bigg) +  \\sum_{j \\in S_{Dmin}} w_j^2 \\bigg(\\frac{1}{N_j}\\sum \\|M_j X - D^{j}_{min}\\|^2\\bigg) + \\sum_{k \\in S_{Dmean}} w_k^2 \\bigg(\\frac{1}{N_k}\\sum \\|M_k X - D^{k}_{mean}\\|^2\\bigg)$\n                                   (1)\nwhere L(X) is the loss function given the argument X to be optimized, and for proton PBS plans, X is the spot MU values. $S_{Dmax}$, $S_{Dmin}$ and $S_{Dmean}$ represent sets of planning"}, {"title": "MATERIALS AND METHODS", "content": "structures with objectives on maximum dose, minimum dose, and mean dose, respectively. The corresponding weights for these objectives are denoted as $w^i$, $w^j$ and $w^k$, and the corresponding dose limits are denoted as $D^i_{max}$, $D^j_{min}$ and $D^k_{mean}$, respectively. The number of voxels in each planning structures i, j, k in the sets $S_{Dmax}$, $S_{Dmin}$ and $S_{Dmean}$ are represented as $N^i$, $N^j$ and $N^k$, and their corresponding dose influence matrices \u00b9\u2078,\u00b9\u2079 are denoted as $M^i$, $M^j$ and $M^k$, respectively. Moreover, $|| \u00b7 ||^2_+$ and $|| \u00b7 ||^2_-$ are standard $l_2$-norms that compute only the positive and the negative elements, respectively.\nHere we use the well-known quasi-Newton method Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS)\u00b2\u2070 to solve the optimization problem. L-BFGS is specifically designed to solve problems involving large numbers of variables, where computing and storing the whole Hessian matrix are not practical. In Newton optimization methods like L-BFGS, X can be iteratively optimized with\n$X_{t+1} = X_t - \\psi H_t g_t,$                                    (2)\nwhere $ \\psi $, $g_t$ and $H_t$ are the step length, the gradient and the inverse Hessian at point $X_t$, respectively. Since the inverse Hessian is generally dense, it is expensive to compute and store the inverse Hessian $H_t$ when X is of high dimensionality. Therefore, L-BFGS approximates the inverse Hessian matrix at every iteration by\n$H_{t+1} = (V^T_t... V^T_{t-m+1}) H^{-1}_{t+1}(V_{t-m+1}... V_t)$ \n$+ V^T_{t-m+1}(V^T...V^T_{t-m+2})\\rho_{t-m+1}x_{t-m+1}x^T_{t-m+1}(V_{t-m+2}...V_t)$ \n$+  V^T_{t-m+2}(V^T...V^T_{t-m+3})\\rho_{t-m+2}x_{t-m+2}x^T_{t-m+2}(V_{t-m+3}...V_t) +\u00b7\u00b7\u00b7 + \\rho_t x_t x^T_t,$     (3)\nwhere $V_t = I - \\rho_t y_t x^T_t$, $ \\rho_t = \\frac{1}{x^T_t y_t}$,  $x_t = X_{t+1} - X_t$ and $y_t = g_{t+1} - g_t$. Here, $H^{-1}_{t+1}$ is the initial Hessian approximation at point $X_{t+1}$, and in practice it can be set to $ \\frac{x^T_t y_t}{y^T_t y_t}I$, where I is the identity matrix. As seen in Eq. 3, L-BFGS alleviates the memory requirement by using a limited number, i.e. m, of the vector pairs {$x_i$, $y_i$}, i = t, ..., t \u2212 m + 1 for $H_{t+1}$. In our experiments, m is set to 10."}, {"title": "Planning objective parameter adjustment with DRL", "content": "In a typical planning pipeline, human planners manually adjust planning objective parameters, i.e. $w^i$, $w^j$, $w^k$ and $D^i_{max}$, $D^j_{min}$,$D^k_{mean}$ in Eq. 1, to improve plan quality iteratively."}, {"title": "MATERIALS AND METHODS", "content": "We automate this time-consuming and experience-demanding process with the advanced reinforcement learning algorithm, i.e., PPO\u00b9\u00b3.\nAlgorithm: PPO is from the well-known policy gradient family of algorithms, which alternates between sampling data through interactions with a task-specific environment and training a policy using stochastic gradient ascent. The basic idea is to optimize the policy by computing the gradients of expected rewards with respect to policy parameters. Assuming that the PPO agent receives a state $s_t$ at time step t, it then generates an action $a_t$ according to its current policy $ \\pi_{\\theta}$. Here, $ \\pi$ is a policy mapping from states to actions and we use $ \\pi_{\\theta}$ to denote a policy with parameters $ \\theta $. In response to $a_t$, the environment reaches a new state $s_{t+1}$ after applying $a_t$ to $s_t$ and returns a scalar reward $r_t$ as the feedback for taking action $a_t$. This process continues until a terminal state is reached at time step T. Here we denote the policy used for data sampling as $ \\pi_{\\theta^{'}}$ , and the policy to be updated as $ \\pi_{\\theta}$. In each episode, M trajectories $ \\tau^i = \\{a_1, a_2,......, a_T \\}$, i = 1... M, are sampled with the policy $ \\pi_{\\theta^{'}}$ , and PPO updates $ \\theta $ multiple times using the gradient estimator\n$\\theta^{'}_{PPO}(\\theta) = \\frac{1}{M} \\sum_{i=1}^M \\sum_{t=1}^T \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta^{'}}(a_t|s_t)} - A^{\\pi_{\\theta^{'}}}(s_t, a_t)  \\triangledown log  \\pi_{\\theta}(a_t|s_t)$           (4)\nwhere\n$A^{\\pi_{\\theta^{'}}}(s_t, a_t) = r^i_t + V^{\\pi_{\\theta}}(s_{t+1}) - V^{\\pi_{\\theta}}(s_t),$\n$V^{\\pi_{\\theta^{'}}}(s_t) = E_{s^{\\pi_{\\theta}}}[ \\sum_{l=0}^n \\gamma^l r^i_{t+l+1}].$\nHere, the advantage function $A^{\\pi_{\\theta^{'}}}(s_t, a_t)$ represents the advantage of action $a^i_t$ at state $s_t$ in the i-th trajectory, compared to the default behavior of policy $ \\pi_{\\theta^{'}}$. The value function $V^{\\pi_{\\theta}}(s_t)$ is the expected accumulative reward of state $s_t$ under policy $ \\pi_{\\theta^{'}}$, representing the performance of this policy. The policy function $\\pi_{\\theta} (a|s_t)$ represents the probability of policy $ \\pi_{\\theta}$ choosing action $a^i_t$ at state $s^i_t$. The importance sampling ratio $\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta^{'}}(a_t|s_t)}$ is introduced to allow multiple policy updates per sampling rollout collection, thereby making training more data-efficient. Intuitively, when $a^i_t$ is a better-than-average action, i.e., $A^{\\pi_{\\theta^{'}}}(s_t, a_t) > 0$, the gradient term $A^{\\pi_{\\theta^{'}}}(s_t, a^i_t)  \\triangledown log  \\pi_{\\theta}(a|s_i)$ points in the direction of increasing the probability of yielding action $a^i_t$. Similarly, if $a^i_t$ is a worse-than-average action, i.e., $A^{\\pi_{\\theta^{'}}}(s_t, a_t) < 0$, the probability of yielding action $a^i_t$ will be decreased. Using $ \\triangledown f(x) = f(x) \\triangledown log f(x)$, the"}, {"title": "MATERIALS AND METHODS", "content": "objective function of PPO corresponding to the gradient estimator in Eq. 4 is given by\n$J^{\\pi_{\\theta}}_{PPO} (\\theta) =  \\frac{1}{M} \\sum_{i=1}^M \\sum_{t=1}^T \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta^{'}}(a_t|s_t)} A^{\\pi_{\\theta^{'}}}(s_t, a^i_t)$       (5)\nand $ \\theta $ can be optimized by maximizing $J^{\\pi_{\\theta}}_{PPO} (\\theta) $ with gradient ascent."}, {"title": "Dose distribution-based reward function", "content": "Treatment planning is essentially a process of balancing competing priorities between dose-volume requirements of the target volumes and the OARs. Prior DRL-based treatment planning studies simply employ a weighted sum of clinical metrics to score plans and calculate rewards. While straightforward, this strategy could lead to unreasonable trade-offs. For example, linearly combined $d_{2cc}$ values from OARs are used to calculate rewards $r_t = \\sum_i W_i(d_{2cc}(S_{t+1})-d_{2cc}(s_t))$ in an earlier study\u00b9. This design resulted in identical rewards for a decrease in one OAR's $d_{2cc}$ from 60Gy to 50Gy and from 15Gy to 5Gy. However, in practice, clinicians are more likely to favor a 10Gy drop from 60Gy than from 15Gy, ceteris paribus. Indeed, a 10Gy reduction should be encouraged with a higher reward when, say, the rectum's $d_{2cc}$ is at 60Gy rather than at 15Gy due to their difference in toxicity risks. The irrationality of equal rewards for unequal clinical benefits becomes more pronounced with more OARs and target volumes involved. Therefore, a reasonable reward design that allows proper trade-offs between target volumes and OARs to be considered is critical for the DRL model."}, {"title": "MATERIALS AND METHODS", "content": "Here we propose a novel reward function from the perspective of global dose distributions. As shown in Figure 4, we set a \u201cno penalty\" interval [l, h] relative to the prescription dose Rx for each planning structure, where all voxels within that structure receiving doses in the range of [l. Rx, h. Rx] won't received a penalty. Penalty is only incurred for any voxels"}, {"title": "MATERIALS AND METHODS", "content": "receiving doses outside that range. In our experiments, the \u201cno penalty\u201d intervals for CTVs and PTVs are set to [1.0, 1.05. $Rx_{max}/Rx$] and [0.95, 1.03. $Rx_{max}/Rx$], respectively. For plans with multiple levels of prescription doses, each target volume has its own Rx value, $Rx_{max}$ denotes the highest of all prescription doses in the plan, and $Rx_{min}$ denotes the lowest. For plans with only one level of prescription dose, the above three values are equal. The \"no penalty\" interval for Rings and OARs (except lenses) are set to [0,0.5] and [0, 10/$RX_{min}$], respectively. Note that Rings are normalized relative to the prescription of the target volume used to generate it, and the OARs are normalized using $RX_{min}$. The 10Gy value is chosen in the current study for all OARs because voxel doses less than 10Gy are generally insignificant for most OARs. Further stratification of dose levels for each OAR is straightforward with our study framework. As an example, here we have set the \u201cno penalty\u201d interval for the lenses to be [0,0] in order to minimize the non-stochastic radiation effect. The actual dose distribution of a planning structure could include voxels receiving doses outside of the range [l. Rx, h. Rx], a penalty would consequently be incurred. The voxels for each planning structure are binned by normalized dose d/Rx with a bin size of 0.002, and the relative voxel number n/N is used for display in Figure 4, where N is the total number of voxels within the planning structure. Given the dose di of the i-th bin and the number of voxels ni in this bin, the penalty is calculated for each planning structure as,\n$penalty = \\sum_{i: d_i /R_x < l} C(\\frac{d_i}{R_x}) \\frac{n_i}{N} +  \\sum_{i: d_j /R_x > h} C(\\frac{d_j}{R_x}) \\frac{n_i}{N} = -score$                                             (8)\nwhere the first sum $ \\sum_i $ accumulates penalty from bins with dose less than $l \\cdot R_x$ and the second sum $ \\sum_j$ from bins with dose more than $h \\cdot R_x$. The function $ C(x) = cx $ is used to control the severity of the penalty and in our experiment c is set to 20, 10, 0.5, 2, 1 for CTVs, PTVs, Rings, level-1 OARs and level-3 OARs, respectively. The score of a planning structure is defined as the negative of its penalty. Note that OARs and Rings only have the second sum, and $Rx_{min}$ is used in place of Rx for OARs and Rings in plans with multiple levels of prescriptions. When there is no penalty, the planning structure will receive its highest score of 0. We also define the uniformity (uniform) for target structures as the relative dose difference between its $D_{2\\%}$ and $D_{98\\%}$, i.e.\n$uniform =  \\frac{D_{2\\%} - D_{98\\%}}{Rx}$\nwhere the uniformity is at its largest value of 0 when dose distribution within the target is completely uniform. We can now define the plan's score by considering all of its planning"}, {"title": "MATERIALS AND METHODS", "content": "structures,\n$score_{plan} = \\sum_{i \\in S_1} score^i +  \\frac{1}{|S_2|}\\sum_{j \\in S_2} score^j +  \\frac{1}{|S_3|}\\sum_{k \\in S_3} uniform^k$                                    (9)\nand calculate the reward for action $a_t$ at time step t as\n$reward(t) = score_{plan}(t+1) - score_{plan}(t)$                 (10)\nwhere $S_1$ is the set of CTVs, PTVs, Rings and level-1 OARs, $S_2$ is the set of level-3 OARs, and $S_3$ is the set of CTVs."}, {"title": "RESULTS", "content": "Our Transformer encoder consists of 4 Transformer layers, with hidden size, dimensions of FFN and number of heads set to 512, 1024 and 8, respectively. The coefficients \u03b1, \u03b2 and the clip range \u03bc in Eq. 6 are set to 0.8, 0.005, and 0.3, respectively. An Adam optimizer \u00b2\u00b3 with a learning rate of 5 \u00b7 $10^{-5}$ and a batch size of 12 is used to train our network. The network is trained for 2 epochs over 30 patients and planning objectives are adjusted 4 times for each patient. A workstation with 2 NVIDIA RTX6000 Ada GPUs is used and the training time is about 300 hours. Figure 5 displays the reward and loss values obtained during training. To improve sampling efficiency, we create two separated copies of our environment to collect samples simultaneously. At each step, samples collected by the two environments are gathered into one mini-batch to update the network. As seen clearly in Figure 5, the rewards are increasing and the losses are decreasing as the training progresses, indicating that the policy network is learning to adjust the planning objectives effectively."}, {"title": "Evaluations", "content": "To demonstrate how this policy network negotiates trade-offs during planning, we examine three plans: Plan_LBFGS, Plan_PPO\u2081 and Plan_PPO2, generated for a patient at three"}, {"title": "RESULTS", "content": "drops noticeably from 100% to 98.9% and from 66.8Gy to 61.2Gy in Plan_PPO2. The same trend also exhibits with \u201cCTV 5940\u201d (Rx=59.4Gy). Therefore, even though OAR doses are lower in Plan_PPO2 than in Plan_PPO\u2081, Plan_PPO2 still receives a slightly negative reward of -0.041 compared with Plan_PPO\u2081 due to its compromised target coverages. This is aligned with human intuition and planning preferences. Essentially, the policy network is searching for the optimal planning objective parameters by learning the adjustment intelligence from the training process."}, {"title": "Comparison with plans made by human planners", "content": "Human planners employ a wide variety of techniques and strategies when optimizing treatment plans. Different margin sizes, objective weights and dose limits are often used by individual planners. Large variability therefore exists amongst treatment plans optimized by human planners. By contrast, our model uses a consistent strategy for all test patients. Resultant plans are compared with those made by human planners in Figure 7.\nUsing the definition of plan scores given in Eq. 9, Plan_PPO achieves scores of -0.38, -4.59, -0.18 and -0.32 on the four test patients, compared with the scores of -0.61, -7.99, -0.24 and -0.60 for Plan_human. Apparently, plans yielded by our model surpass human"}, {"title": "RESULTS", "content": "plans with higher scores for both bilateral and ipsilateral H&N cancers. Using $D_{99\\%}$ and $V_{99\\%}$ as the criteria for target coverages, Plan_PPO also outperforms Plan_human on all test patients. Specifically on the three target volumes: CTV5400, CTV6000 and CTV7000 of patient \u201cBilateral 007"}]}