{"title": "Gompertz Linear Units: Leveraging Asymmetry for Enhanced Learning Dynamics", "authors": ["Indrashis Das", "Mahmoud Safari", "Steven Adriaensen", "Frank Hutter"], "abstract": "Activation functions are fundamental elements of deep learning architectures as they significantly influence training dynamics. ReLU, while widely used, is prone to the dying neuron problem, which has been mitigated by variants such as LeakyReLU, PReLU, and ELU that better handle negative neuron outputs. Recently, self-gated activations like GELU and Swish have emerged as state-of-the-art alternatives, leveraging their smoothness to ensure stable gradient flow and prevent neuron inactivity. In this work, we introduce the Gompertz Linear Unit (GoLU), a novel self-gated activation function defined as GoLU(x) = x Gompertz(x), where Gompertz(x) = e^{-e^{-x}}. The GoLU activation leverages the asymmetry in the Gompertz function to reduce variance in the latent space more effectively compared to GELU and Swish, while preserving robust gradient flow. Extensive experiments across diverse tasks, including Image Classification, Language Modeling, Semantic Segmentation, Object Detection, Instance Segmentation, and Diffusion, highlight GoLU's superior performance relative to state-of-the-art activation functions, establishing GoLU as a robust alternative to existing activation functions.", "sections": [{"title": "1. Introduction", "content": "Developing effective activation functions has been a long-standing area of research in deep learning. In the early days, the Sigmoid and Tanh functions were popular choices. However, these activations can suffer from the vanishing gradient problem due to their tendency to saturate. The introduction of ReLU marked a turning point, as it allowed for more efficient training by alleviat-ing the vanishing gradient problem and inducing intensity equivariance. However, ReLU comes with its own challenges, notably the dying-ReLU problem. To address these challenges, several ReLU variants have been developed, including LeakyReLU, PRELU and ELU. Despite the emergence of these alternatives, ReLU remains one of the most widely used activation functions today, owing to its simplicity as a piecewise linear function and its computational efficiency.\nIn the deep learning community, the landscape of activation functions has gradually shifted towards self-gated activations such as Gaussian Error Linear Units (GELU), Swish, and Mish. These activations provide probabilistic interpretations while enhancing robustness when combined with normalization techniques. Unlike ReLU, which strictly enforces gradient preservation due to its piecewiselinear nature, Swish, Mish and GELU, as smooth activation functions, relax these constraints. Their smoothness allows for improved gradient flow without strictly adhering to intensity equivariance.\nIn this work we introduce Gompertz Linear Units (GoLU), a new activation function of the self-gated family based on the Gompertz function as its gating mechanism. The Gompertz function was initially developed to model human mortality rates, and has since been widely applied in biology. Notably, it also possesses a probabilistic interpretation, as it represents the cumulative distribution function (CDF) of the standard Gumbel distribution. While both the Sigmoid function and the Gaussian CDF exhibit reflection symmetry around the point (0, 0.5), the Gompertz function manifests a subtle rightward asymmetry, leading to distinct qualitative behavior.\nOur experiments indicate that GoLU, compared to existing self-gated activations, effectively reduces variance in the latent representation. Moreover, it contributes to a smoother loss landscape, making it less sensitive to small perturbations in the model parameters. Additionally, an analysis of the learned weights in our trained models reveals that GOLU induces a more spread weight distribution compared to commonly used activations (see Section 2.2 for details).\nA more spread weight distribution may indicate the network's ability to capture a diverse range of features from the data. On the other hand, variance reduction in activation outputs can help eliminate irrelevant information, allowing the network to focus on distinguishing patterns and potentially mitigate overfitting. However, overly broad weight distributions may introduce instability, while excessive variance reduction could result in underfitting and the loss of essential features, ultimately degrading performance.\nExtensive, task-specific evaluations, suggest that GoLU effectively addresses this trade-off by achieving a balanced level of both weight distribution and variance reduction, leading to improved performance over baseline activations (see Section 3). To facilitate reproducibility, we have made our code available at https://github.com/automl/GOLU."}, {"title": "2. Gompertz Linear Unit", "content": ""}, {"title": "2.1. Definition and Properties", "content": "In this section, we introduce the GoLU activation function and discuss its properties. GoLU is defined through Equations 1 and 2 and visualized in Figure 1 (Left).\nGoLU(x) = x Gompertz(x)\nGompertz(x) = e^{-e^{-x}}\n(1)\n(2)\nThe gate function Gompertz(x) refers to the Gompertz function introduced in  and is plotted in Figure 1 (Right).\nThe Gompertz function can also be interpreted probabilistically, as it corresponds to the CDF of the standard Gumbel distribution, Gumbel(0, 1), with probability density function\nGumbel(x) = e^{-(x+e^{-x})}\n(3)\nFrom Equations 1, 2 and Figure 1, we understand that, contrary to ReLU and its variants which are monotonic and non-smooth at zero, GoLU is a smooth and non-monotonic self-gated activation, similar to Swish and GELU. In fact the formulation of GoLU using exponentials makes it infinitely differentiable. However, in contrast to Sigmoid and the Gaussian CDF (i.e. the gate functions of Swish and GELU), the Gompertz function is asymmetric, as it does not mirror evenly around a central point. This asymmetry, which has a bias towards the right, arises from the inherent asymmetry of the Gumbel distribution, which favors positive input values. In fact, the right-leaning asymmetry of the Gumbel distribution leads to smaller gate values across the entire input range, inducing a compression effect on the output distribution. This behavior extends to GoLU, yielding output values closer to zero, both for positive and negative inputs, when compared to other gated activation functions, effectively reducing the magnitude of the activation output. We note that, while Mish also exhibits an asymmetric distribution, it is skewed to the left, producing the opposite effect relative to GOLU.\nThese properties are more clearly illustrated in Figure 2, which provides a direct comparison between different activations (Left), as well as the gate functions of various gated activations (Middle) and their corresponding distributions (Right).\nAdditionally, from a more localized perspective, the Gompertz gate exhibits a reduced value in particular at the origin. This leads to a decreased steepness of GoLU near this point, as indicated by GoLU'(0) = Gompertz(0) from Equation 1. This property of reduced slope magnitude is not confined to the origin but extends to a neighborhood around it and spans a substantial portion of the negative input domain. Additional details are provided in Appendix A.\nIn the large negative region, the Gompertz gate, and consequently the GoLU activation, decays extremely rapidly as a double exponential, suppressing unimportant features like ReLU, while maintaining smoothness, unlike ReLU.\nCompared to the Gaussian CDF and the Sigmoid function, the Gompertz gate initially exhibits a flat plateau, followed by a steeper growth rate that aligns more closely with the Gaussian CDF. As the input values become large and positive, the growth rate flattens and resembles the Sigmoid function, with the difference falling off as O(e^{-2x}) (see Appendix A)."}, {"title": "2.2. Effects on Training Dynamics", "content": "The distinctive properties of GoLU influence the training dynamics, as we will outline here.\nVariance reduction As illustrated in Figure 2 (Left), GOLU exhibits a profile that remains closest to the x-axis across the entire input range. Moreover, its slope, particularly near the origin and over a substantial portion of the negative input domain, is smaller in magnitude compared to other gated activations, as pointed out in Section 2.1. These characteristics suggest a reduced sensitivity of the activation output to variations in the input. This effectively reduces variance in the latent representations, and promotes smoother activation outputs, enhancing the model's ability to differentiate between strong and weak features.\nTo visually illustrate this phenomenon, we process Figure 3 (Left) through a 3 \u00d7 3 2D Convolution followed by 2D Batch Normalization. The resulting pre-activation is then passed through various activation functions, and the pixel distributions of the normalized pre-activation and activation maps are plotted for GoLU, GELU, and Swish in Figure 3 (Right). As observed, GoLU exhibits a distinctive \u201csqueezing effect\", compressing the same distribution into a smaller output range, and reducing variance most, compared to GELU and Swish.\nTo further substantiate this observation, we randomly sample four images from the CIFAR-10 dataset, apply the same preprocessing pipeline, and pass the results through different activation functions. The variances of the activated signals, summarized in Table 1, highlight GoLU's ability to achieve a notable reduction in variance compared to widely-used activations, enabling smoother data representation.\nSmooth loss landscape Reduced activation variance results in less noisy and more consistent gradients. This typically means that the loss function changes more smoothly with respect to model parameters. As a result, the optimizer is more likely to converge to flatter regions of the loss landscape with smaller curvature. This is expected to result in better robustness to small perturbations of the model parameters. We explore this by adding two different Standard Normal noise terms, scaled independently by \u03b1, \u03b2, to the weights of ResNet-20 trained on CIFAR-10. We compute the test loss across a grid of scaling factors \u03b1, \u03b2 for the two terms, while keeping the noises constant (refer to Appendix B for more details). ResNet-20 with GoLU shows relatively smoother, less-spiked loss landscapes compared to other activations (Figure 5) which implies better generalization and noise robustness with GoLU. In contrast, ReLU's nonsmooth nature produces a highly-spiked landscape.\nSpread weight distribution In contrast to the reduced variance in the latent space, we observe a wider distribution in the learned weights of our models trained with GoLU, at least in the region where most weights are concentrated. Figure 6 compares non-normalization weight distributions of ResNet-50 and ViT-B/32 trained on ImageNet-1k and GPT2-S (124M) trained on OpenWebText, with different activation functions. The broader weight distribution for GOLU around the peak suggests that the network has learned more diverse transformations, enhancing its capacity to distinguish between features in the data.\nThis may reflect the network's response to reduced activation variance, counterbalancing it by spreading the weights around the peak to maintain representational diversity. Specifically, reduced output variance naturally leads to more uniform gradients, which in turn encourages a broader spread of weights.\nNotice that a wider weight distribution around the peak does not necessarily translate to a larger overall variance. However, focusing on the bulk of the distribution, we find that GoLU consistently achieves the highest variance. This behavior suggests that networks trained with GoLU effectively suppress density in extreme values while expanding the distribution around the peak. Such a pattern implies that the model captures a broader range of meaningful transformations without over-reliance on extreme parameter values or certain features.\nWe emphasize that the effects attributed to GoLU, as described above, are not guaranteed to hold universally across all scenarios but rather represent general trends observed in our empirical findings.\nMoreover, while asymmetry has been highlighted as a distinctive feature of GoLU, it is important to note that its high performance, detailed in the next section, cannot be solely attributed to asymmetry, but arises from an intricate interplay of properties, described in Section 2.1."}, {"title": "3. Experiments and Results", "content": ""}, {"title": "3.1. Overview of Experiments", "content": "We conducted experiments across various architectures and datasets, spanning a diverse range of tasks in both vision and language modeling. We begin with image classification, training ResNet-18, 34, 50, WideResNet50-2, DenseNet-121, EfficientNet-B0, TinyViT, ViT-B/32 and ViT-B/16 on ImageNet-1k .\nWe then extend our experiments to language modeling. We train babyGPT on the TinyStories (TS) dataset and GPT2-S on the OpenWebText (OWT) dataset, leveraging the nanoGPT repository.\nAdditionally, we assess GoLU's performance on Semantic Segmentation (DeepLabV3), Object Detection (Faster R-CNN-FPN, RetinaNet-FPN), and Instance Segmentation (Mask R-CNN-FPN) on MS-COCO, leveraging our pre-trained ResNet-50 backbone on ImageNet-1k. Further, we test GoLU on Denoising Diffusion Probabilistic Models on the CelebA dataset.\nWe closely follow established baselines for all model architectures and tasks, ensuring that the integration of GoLU is the primary change. Hyperparameters, optimizers, learning rate schedules, and other training settings are aligned with the standard practices for each task. All our experiments are conducted on three seeds and the results are averaged out and reported with the standard error.\nIn Appendix D we further present a Critical Difference analysis to systematically compare the overall performance of activation functions. Finally, in Appendix G, we explore the application of GoLU to the task of learning curve extrapolation."}, {"title": "3.2. Image Classification", "content": "We evaluate GoLU's performance in image classification tasks on ImageNet-1k, comparing it against six state-of-theart activation functions, ReLU, LeakyReLU, ELU, GELU, Swish and Mish.\nAs highlighted, GoLU consistently outperforms the standard activation functions across all architectures, with the exception of EfficientNet-B0, where the performance difference is minimal. Notice that EfficientNet-B0 is an exception because its nonlinearity arises not only from activation functions (which are replaced) but also from a squeeze-andexcitation block, which remains unchanged in our experiments. For ResNet-50 and ViT-B/32, test loss and test accuracy curves are shown in Figures 7 and 8, respectively, where GoLU consistently delivers lower test loss and higher top-1 accuracy over the epochs. GELU is generally the second-best performer, while ELU performs worst across most architectures.\nWe further evaluate GoLU on CIFAR-10, comparing it against top baseline activations. We report in Table 3 the results of image classification on CIFAR-10, with ResNets 20, 32, 44, 56, and 110, WideResNet28-2, DenseNet40 and ViT-Ti/16-224. GoLU consistently outperforms the standard baselines across all tested architectures. We have further underlined the second-best activations for each model. No single activation consistently ranks second."}, {"title": "3.3. Language Modeling", "content": "We train babyGPT on TS and GPT2-S (124M) on OWT, both sourced from the nanoGPT repository. As shown in Table 4, GoLU demonstrates superior performance, achieving lower perplexity and higher token accuracy on both babyGPT and GPT2-S. GoLU's superiority is also evident in the test loss curves in Figures 9 and 10. The general trend of GELU being the second-best activation function holds in language modeling as well."}, {"title": "3.4. Semantic Segmentation", "content": "For Semantic Segmentation, we train DeepLabV3 on the MS-COCO dataset with PASCAL-VOC labels, from the Torchvision benchmark (see Appendix F.4). We employ our ResNet-50 backbone, pre-trained on ImageNet-1k.\nGoLU achieves the lowest test loss, whereas ReLU attains the highest mIoU, with GoLU ranking second. However, the difference in mIoU between ReLU and GoLU is statistically insignificant.\nmIoU curves over epochs, shown in Figures 11 and 12, further emphasize GoLU's strong performance in semantic segmentation."}, {"title": "3.5. Object Detection", "content": "For Object Detection, we train Faster R-CNN-FPN and RetinaNet-FPN on the MS-COCO dataset. As shown in Table 6 and Figure 13, GoLU outperforms all activation functions on object detection as well, with higher Box mAP (AP @ IoU=0.50:0.95, area=all, maxDets=100) across both Faster R-CNN-FPN and RetinaNet-FPN architectures, while GELU ranks second. Appendix F.5 outlines experimental details."}, {"title": "3.6. Instance Segmentation", "content": "For Instance Segmentation, we train Mask R-CNN-FPN with a ResNet-50 backbone from the Torchvision benchmark on the MS-COCO dataset (see Appendix F.6 for training settings). As shown in Table 7, GELU achieves the best performance in this setting, with GoLU ranking second in Box mAP and third in Mask mAP (both implying AP @ IoU=0.50:0.95, area=all, maxDets=100). However, Figure 14, which depicts test Box mAP and Mask mAP over epochs, reveals that GoLU generally outperforms GELU and ReLU throughout the training process. Based on these observations, we suggest that, similar to the Semantic Segmentation task, the learning rate of 0.02 may be suboptimal for this specific architecture-dataset combination. Adjusting the learning rate could potentially enhance GoLU's performance relative to baseline activations."}, {"title": "3.7. Denoising Diffusion Probabilistic Models", "content": "We train a Denoising Diffusion Probabilistic Model on the CelebA dataset (see Appendix F.7). As shown in Table 8, for the default lr=0.0003, gated activations perform comparably to the baseline activation, Swish, which achieves the best performance, with GoLU ranking a close second. Figure 15 (Left) further illustrates the test loss over epochs. Similar to our findings in semantic segmentation, we conduct a learning rate ablation study. Results, summarized in heatmap 19 in Appendix C, indicate that increasing the lr from the default value of 0.0003 to 0.0004, 0.0005 and 0.001 progressively improves performance across all activations. Notably, for lr values of 0.0004, 0.0005 and 0.001, GoLU achieves the lowest final test loss. Results for the optimum lr=0.001 are highlighted in the right column of Table 8 and Figure 15 (Right). These findings are in line with the trend observed in semantic segmentation, where GoLU outperforms baseline activations under optimal lr configurations."}, {"title": "4. Training and Inference Speed", "content": "Existing activation functions in PyTorch leverage CUDA kernels in Eager mode to achieve optimal speedup. To ensure a fair comparison of training and inference speeds, we developed a CUDA-optimized kernel for GoLU, which was used for all training experiments described in the previous sections. Table 9 in Appendix E presents the relative training and inference speeds of GOLU compared to the default activation function across various tasks."}, {"title": "5. Conclusions", "content": "We have introduced GoLU, a new self-gated activation function based on the CDF of the Gumbel distribution as its gate function. Through extensive analysis and experiments, we have demonstrated that GoLU provides a regularising effect by reducing variance in the activation output, it enables the representation of diverse features through a more distributed weight pattern, and encourages a smoother and more robust loss landscape. Notably, our results show that GoLU generally outperforms state-of-the-art baseline activation functions across a wide range of tasks and domains, from computer vision to language modeling. Additionally, we implemented a custom CUDA kernel to optimize training and inference efficiency, minimizing latency and enhancing scalability. GoLU offers a robust, efficient, and scalable alternative to existing activation functions. Its integration into state-of-the-art neural networks has the potential to improve performance across various applications, positioning GoLU as a promising standard in modern deep learning."}, {"title": "A. Properties of GoLU: Further Details", "content": "To further elucidate the concepts presented in Section 2.1 and gain deeper insights into the properties of GoLU, we present additional details and visualizations in this section.\nFigure 16 compares the GoLU activation with GELU, highlighting how the right-leaning inclination of the Gumbel distribution, in contrast to the symmetric Gaussian distribution (Left column), results in a smaller value of the Gompertz gate at the origin compared to the Gaussian CDF (Middle column). In fact, this behavior is not confined to the origin, and the Gompertz gate remains smaller than the Gaussian CDF across the entire input range.\nThis reduced value of the Gompertz gate at the origin directly translates into a lower slope for GoLU compared to GELU, as illustrated in Figure 16 (Right column). This can be readily seen by taking the derivative of the GoLU activation and evaluating it at zero\nGoLU'(x) = x Gompertz'(x) + Gompertz(x)\n(4)\nGOLU'(0) = Gompertz(0)\n(5)\nwhich shows that the slope of GoLU at the origin corresponds to the value of the Gompertz gate at the origin. Similarly, the slope of GELU at the origin is determined by the Gaussian CDF at the origin.\nAssuming the input distribution resembles a zero-centered, nearly-Gaussian form, which is likely particularly when employing batch normalization and appropriate weight initialization, the activations can be approximated by their tangents at the origin. Therefore a reduced slope at the origin translates into decreased sensitivity to input variations and lower output variance. We note that GoLU exhibits a lower slope magnitude not only in a neighborhood around the origin but across a significant portion of the negative input domain as illustrated in Figure 17."}, {"title": "B. Details of the loss landscape experiment", "content": "We analyze the loss landscape of a neural network by quantitatively measuring and visualizing how the loss changes as the network's parameters are perturbed. Smoothness in the loss landscape often indicates that small perturbations in the parameters do not cause large changes in the loss, which can make optimization more stable.\nSpecifically, we generate two random perturbation directions d\u2081 and d\u2082, each matching the shape of the model parameters. The elements of these directions are independently sampled from a Standard Normal distribution. To ensure controlled magnitudes, each perturbation direction is subsequently normalized.\nWe perturb the weights of the model along these directions in a linear combination:\nWperturbed = Wtrained + \u03b1d\u2081 + \u03b2d2\n(7)\nwhere Wtrained are the trained weights of the model and \u03b1 and \u03b2 are scalar values that determine the perturbation magnitude and are chosen as \u03b1, \u03b2 \u2208 [\u22121, 1]. For each pair of values (\u03b1, \u03b2), we compute the loss using the perturbed weights Wperturbed on a batch of test data. We then repeat this for a grid of (\u03b1, \u03b2) values to create a 3D surface plot as shown in Figure 5."}, {"title": "C. Learning Rate ablation", "content": "For various tasks, we conduct a focused search over the learning rate to determine whether the default setting represents the optimal value and to assess its impact on the performance of models trained with different activation functions. Figures 18 and 19 present heatmaps of test results for Semantic Segmentation and Diffusion tasks, comparing models trained with various activation functions across different learning rates. For these tasks, the default learning rate, highlighted by a black box, differs from the optimal learning rate, indicated by a green box. Notably, while GoLU achieves a test mIoU that is a close second to ReLU under the default learning rate, it outperforms all other activation functions when evaluated at the optimal learning rate, which is consistent across all activations."}, {"title": "D. Critical Difference Analysis", "content": "In this section, we conduct a Critical Difference analysis following to systematically rank activation functions based on experiments performed on ImageNet-1k, MS-COCO, OWT, TS, and CelebA. As shown in Figure 22, GOLU achieves the highest rank, followed by GELU. Notice that the confidence interval in this analysis is independent of the variance across multiple runs with different random seeds. Instead, it is determined by the number of models and datasets, as well as the significance level, which is set to \u03b1 = 0.05 here."}, {"title": "E. Training and inference times", "content": ""}, {"title": "F. Experimental Details", "content": "This section outlines detailed information about the datasets and training pipelines used for the various tasks studied in this work."}, {"title": "F.1. Image Classification - ImageNet", "content": "In image classification experiments on ImageNet-1k, ResNets 18, 34, 50, WideResNet-50-2 and DenseNet-121 are trained for 90 epochs with a batch size of 256, SGD with momentum=0.9, learning rate 0.1, and weight decay 1 \u00d7 10\u20134. Further, a Step learning rate scheduler is applied that reduces the learning rate by a gamma = 0.1 after every 30 epochs. EfficientNet-B0 is trained using the timm library for 450 epochs with a batch size of 1536 using RMSProp with an initial learning rate of 0.048 and a weight decay of 1 \u00d7 10\u20135. ViT models are trained for 300 epochs with a batch size of 4096 using AdamW with an initial learning rate of 3 \u00d7 10-\u00b3 and weight decay of 0.3. Various regularization techniques are applied, including Exponentially Moving Averaged Weights , AutoAugment, RandAugment, MixUp, CutMix and Label Smoothing for EfficientNet-B0 and ViT models. ViT-B/16 shows slight instability for seed 1 for GELU. Hence we further average seeds 2 and 3 for both GELU and GoLU. We find that GELU shows a top-1 accuracy of 80.61 \u00b1 0.06 while GoLU shows top-1 accuracy of 80.69 \u00b1 0.07 which is higher than GELU."}, {"title": "F.2. Image Classification - CIFAR-10", "content": "The ResNet 20, 32, 44, 56 and 110 models are trained for 164 epochs with a batch size of 128, a learning rate of 0.1, and SGD with momentum 0.9. A weight decay of 1 \u00d7 10-4 is applied, along with a MultiStep learning rate scheduler with a gamma factor of 0.1 at epochs 81 and 122 (with an initial learning rate of 0.01 and additional gamma factor of 10 at epoch 2 for ResNet-110).\nWideResNet28-2 and DenseNet40, were trained for 200 and 300 epochs, and batch sizes of 128 and 64, respectively. We employ SGD with Nesterov momentum 0.9 for both architectures, using a learning rate of 0.1. The weight decays are 5 \u00d7 10-4 for WideResNet28-2 and 1 \u00d7 10-4 for DenseNet40. Similar to ResNets, both WideResNet28-2 and DenseNet40 use the MultiStep learning rate scheduler. However, WideResNet28-2 reduces the learning rate by a factor of 0.2 at epochs 60, 120, and 160, while DenseNet40 reduces the learning rate by 0.1 at epochs 150 and 225. To train ViT-Ti/16-224 from scratch, we leverage the Timm library."}, {"title": "F.3. Language modeling", "content": "Both, TinyStories and OpenWebText datasets are popular benchmarks for training language models. The TinyStories dataset consists of 2,119,719 data points in the training set and 21,990 in the test set, while the OpenWebText dataset has 8,009,762 data points in the training set and 4,007 data points in the test set. Both babyGPT and nanoGPT have a vocabulary size of 50,304 and a maximum sequence length of 1024.\nThe babyGPT version of the GPT-2 series consists of 6 layers, 6 attention heads, and an embedding dimension of 384, with a feed-forward expansion dimension of 1536 output features. The model is trained for 10,000 iterations with a batch size of 640, using the AdamW optimizer. The initial learning rate is 1 \u00d7 10\u22123, with a minimum learning rate of 1 \u00d7 10-4, a weight decay of 0.1, and a gradient clipping norm of 1.0. A Cosine learning rate scheduler is applied with a linear warmup for the first 100 iterations.\nSimilarly, the GPT2-S model consists of 12 layers, 12 attention heads, and an embedding dimension of 768. It trains for 600,000 iterations with a batch size of 480, using the AdamW optimizer (with \u03b22 = 0.95). The initial learning rate is 6 \u00d7 10-4, with a minimum learning rate of 6 \u00d7 10-5, a weight decay of 0.1, and a gradient clipping norm of 1.0. The Cosine learning rate scheduler is employed with a linear warmup for the first 2,000 iterations."}, {"title": "F.4. Semantic Segmentation", "content": "The MS-COCO dataset with PASCAL-VOC labels contains 92,518 data points in the training set and 5,000 data points in the test set. The original MS-COCO dataset contains 117,266 data points in the training set. However, the existing benchmark pre-processes and removes images that either lack valid annotations or contain only small objects with an area coverage of less than 1,000 pixels. This ensures the retention of meaningful data points for training the model.\nThe DeepLabV3-ResNet-50 model is trained for 30 epochs with a batch size of 32, using SGD with momentum 0.9, a learning rate of 2 \u00d7 10-2, weight decay of 1 \u00d7 10-4, and a polynomial learning rate scheduler with a power of 0.9."}, {"title": "F.5. Object Detection", "content": "Unlike Semantic Segmentation, the MS-COCO dataset for object detection contains 117,266 images in the training set and 5,000 images in the test set. Additionally, we do not apply any pre-processing that removes images from the training or test sets.\nFaster R-CNN-FPN ResNet-50 and RetinaNet-FPN ResNet-50 are trained for 26 epochs with a batch size of 16, an aspect ratio group factor of 3, no frozen batch normalization, and a MultiStep learning rate scheduler that reduces the initial learning"}, {"title": "F.6. Instance Segmentation", "content": "The MS-COCO dataset for instance segmentation uses the same train and test sets as those used for Object Detection. Additionally, it trains with the exact same configurations used for Faster R-CNN-FPN in the previous subsection F.5."}, {"title": "F.7. Denoising Diffusion Probabilistic Models", "content": "The CelebA dataset, comprises of 162,770 training images and 19,867 test images of human faces. The Denoising Diffusion Probabilistic Model is trained on the CelebA dataset for 50 epochs with a batch size of 32 leveraging the DDPM  repository. The AdamW optimizer with a learning rate of 0.0003, Cosine learning rate scheduler, and linear learning rate warmup for the first 1,000 iterations are applied."}, {"title": "G. Case Study: Bayesian Learning Curve Extrapolation using Prior-data fitted Networks", "content": "In this section, we present an additional experiment on GoLU, initially conducted as an internal validation study. We report this as a \"negative\" result, with GoLU ranking second-to-last under the optimal learning rate. Due to the unconventional experimental setup, its niche focus, and suboptimal hyperparameter tuning, we have included these findings in the appendix rather than in the main text.\nIn this experiment, we assessed all 7 activation functions (including GoLU) considered in the main article as activations for LC-PFN . LC-PFN is a prior-data fitted network that functions as a decoder-only transformer, trained for in-context Bayesian prediction for a specific prior dataset distribution. Specifically, LC-PFN is trained for Bayesian Learning Curve extrapolation. We adopted the same setup used to train the best model presented in the original paper, a decoder-only transformer having 26.79M trainable parameters, 12 layers, 4 attention heads, an embedding dimension of 512, and a feed-forward expansion dimension of 1024 output features. It was trained using 10M synthetically generated learning curves, employing the Adam optimizer (with a default learning rate of 0.0001 and a batch size of 100), using a cosine scheduler with a linear warmup during the first 25,000 steps (25%) of the training. At test time, it takes a partial learning curve as input, and predicts the posterior predictive distribution (PPD) for possible continuations. The test performance of the final model was measured using the log-score, which represents the log-likelihood of the true continuation, under the PPD, averaged across all 99 cutoffs for 10,000 curves from the prior."}]}