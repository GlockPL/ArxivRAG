{"title": "A Comparative Study of LLM-based ASR and Whisper in Low Resource and Code Switching Scenario", "authors": ["Zheshu Song", "Ziyang Ma", "Yifan Yang", "Jianheng Zhuo", "Xie Chen"], "abstract": "Large Language Models (LLMs) have showcased exceptional performance across diverse NLP tasks, and their integration with speech encoder is rapidly emerging as a dominant trend in the Automatic Speech Recognition (ASR) field. Previous works mainly concentrated on leveraging LLMs for speech recognition in English and Chinese. However, their potential for addressing speech recognition challenges in low resource settings remains underexplored. Hence, in this work, we aim to explore the capability of LLMs in low resource ASR and Mandarin-English code switching ASR. We also evaluate and compare the recognition performance of LLM-based ASR systems against Whisper model. Extensive experiments demonstrate that LLM-based ASR yields a relative gain of 12.8% over the Whisper model in low resource ASR while Whisper performs better in Mandarin-English code switching ASR. We hope that this study could shed light on ASR for low resource scenarios.", "sections": [{"title": "1. Introduction", "content": "In recent years, the scaling of model parameters has prevailed and proven effective in a range of areas, including language [1, 2], vision [3, 4], as well as speech processing [5-7]. In the realm of Automatic Speech Recognition (ASR), large-scale speech recognition models generally fall into two categories. One type is the classic end-to-end speech recognition model, exemplified by Whisper [5], which is a Transformer sequence-to-sequence model trained on various speech processing tasks and large-scale speech datasets, showing excellent performance in multilingual speech recognition and speech translation. With the advent of Large Language Models (LLMs), ASR research has increasingly shifted focus toward utilizing these models, leading to the emergence of LLM-based ASR. This approach harnesses the rich text knowledge and the reasoning ability of LLMs to improve speech recognition performance.\nCurrently, a wealth of impressive work [8-18] has emerged in the field of LLM-based ASR. These approaches typically employ a speech encoder network and a trainable adapter to process speech and generate embeddings, which are then passed to a decoder-only LLM. This framework seeks to strengthen the connection between acoustic features and linguistic context, enabling LLMs to better process speech input. Through a series of studies, the paradigm of enhancing speech foundation models with LLMs via projector modules has become the dominant approach in current LLM-based speech recognition research.\nSpecifically, SALMONN [8] leverages Whisper extract semantic content and BEATS [19] for audio event information, achieving a comprehensive understanding of human speech, music, and audio events. Qwen-Audio [10] relies on Whisper as its speech encoder and enhances model performance across various audio tasks through structured task directives. SLAM-ASR [9] adopts a linear layer as the projector module achieving a new state-of-the-art performance on the 960-hour LibriSpeech [20] English task. Seed-ASR [12] employs its own powerful self-supervised audio encoder and adopts multi-stage training strategy, resulting in significant improvements over end-to-end models on comprehensive evaluation sets.\nThe above researches primarily focus on speech recognition for Chinese and English. Beyond these mainstream languages, low resource speech recognition remains a crucial area that cannot be overlooked. Building on the promising results of previous studies, we aim to further explore the potential of LLM-based ASR models in low resource and code switching scenario. Additionally, a comprehensive comparison between LLM-based ASR models and Whisper is conducted to evaluate their recognition performance. From our experiments, we draw the following key conclusions: (1) In languages where Whisper performs poorly, LLM-based ASR shows certain advantages. Conversely, Whisper outperforms in tasks such as Mandarin-English code switching ASR. (2) For LLM-based ASR systems, the performance of the ASR model is positively correlated with the LLM's proficiency in the specific language being recognized. We hope that our study can facilitate the research on ASR in low resource scenarios and provide valuable insights for the LLM-based ASR community."}, {"title": "2. Methods", "content": "This section primarily focuses on comparing the model architectures and underlying differences between two ASR paradigms: Whisper and LLM-based ASR."}, {"title": "2.1. Whisper", "content": "Whisper [5] is an encoder-decoder Transformer model that is capable of multiple speech tasks, including multilingual speech recognition, speech translation, language identification, and voice activity detection. The input to Whisper is an 80-dimensional log-Mel spectrogram of 30 seconds length X = [x1, x2,\u2026\u2026,xT] where T denotes the context length. The encoder blocks encode the input speech feature into hidden representations H and the decoder blocks decode the hidden representations into text tokens \u0177 recursively conditioned on previous tokens and special prompts p. In formal terms, this process can be illustrated as follows:\nH = AudioEncoder(X)  (1)\n\u0177t = Text Decoder (p, \u01771:t-1, H) (2)"}, {"title": "2.2. LLM-based ASR", "content": "As shown in Figure 1 (b), The LLM-based ASR consists of a speech encoder, a linear projector and LLM. For each sample, the given prompt (i.e., transcribe speech to text), the speech utterance, and the corresponding transcript during training are denoted as P, X, T, respectively.\nFor the input speech X, we first extract features by passing the speech through a speech encoder to obtain speech representations H, denoted as:\nH = Encoder(X) (3)\nDue to the length of speech features is much longer than that of text features, it is necessary to downsample them. Then, the downsampled speech features are passed through a linear projector to obtain a feature sequence Es with the same dimensionality as the input to the LLM, denoted as:\nEs = Projector(DownSample(H)) (4)\nFor the given prompt P and transcription T, We tokenize them using the tokenizer and embedding matrix of the LLM to obtain feature sequences Ep and Et as:\nEp = Embedding(Tokenizer(P)) (5)\nEt = Embedding(Tokenizer(T)) (6)\nNext, we concatenate Ep, Es and Et to obtain the final feature and pass it to the LLM to obtain the output transcript Y, denoted as:\nY = LLM(Ep, Es, Et) (7)"}, {"title": "3. Experiments", "content": "Our experiments are conducted on GigaSpeech2 dataset [21], Common Voice dataset [22] and ASRU 2019 Mandarin-English code-switching Challenge dataset [23] as shown in Table 2. In the low resource ASR experiment, we select Thai, Vietnamese, Arabic, and Welsh as representative low resource languages, drawn from GigaSpeech2 and Common Voice datasets respectively. The amount of training data for each language varies between 60 and 200 hours. For the code switching ASR experiment, approximately 200 hours of Mandarin-English data are utilized during the training phase."}, {"title": "3.2. Training configuration", "content": "The architecture of LLM-based ASR consists of three main components: a speech encoder, a projector, and a large language model (LLM). In our experiments, the Whisper encoder serves as the speech encoder, while a linear projector is employed to align the outputs of the speech encoder with the inputs of the LLM. For the LLM component, specific models [24-27] are se-"}, {"title": "3.3. Results and analysis", "content": ""}, {"title": "3.3.1. Low resource ASR", "content": "As can be seen in Table 1, among the four low resource languages, the LLM-based ASR outperforms both Whisper and Whisper-finetune in three of them (Thai, Vietnamese, and Welsh), with average Word Error Rate (WER) reduced by 35.7% and 12.8% compared to Whisper and Whisper-finetune, respectively. Specifically, Whisper exhibits limited performance in Welsh recognition, with WER of 31.86% on the Welsh test set, which improves to 18.06% after LoRA fine-tuning. In contrast, LLM-based ASR demonstrates significantly better performance. This is due to Mixtral's incorporation of rich textual information and its strong support for European lan-"}, {"title": "3.3.2. Code Switching ASR", "content": "From Table 4, it can be seen that LLM-based ASR performs well in the Mandarin-English code switching task, with MER reduced to below 8%, which is a 20% relative improvement compared to Whisper. Despite that, the performance of LLM-based ASR is still far behind that of Whisper-finetune. This is because Whisper demonstrates outstanding recognition performance in high resource languages such as Chinese and English, and is capable of handling Mandarin-English code-switching task with ease after fine-tuning. Therefore, it is a better choice to use Whisper's fine-tuned model to handle such tasks."}, {"title": "3.4. Ablation Study", "content": "In LLM-based ASR, specific LLM is chosen based on the target language. To validate the effectiveness of this approach, a series of experiments have been conducted on Welsh and Vietnamese language.\nSpecifically, the recognition performance of Welsh and Vietnamese is evaluated using four large language models: Sailor-7b, Mixtral-7b, Llama3-8b, and Vicuna-7b. The Sailor model has been specifically fine-tuned for Southeast Asian languages, making it particularly well-suited for tasks involving these languages. In contrast, Mixtral is pretrained on a substantial corpus of European languages, thus offering enhanced performance for European language tasks. Experimental results are shown in Table 5. It is evident that the model achieves optimal performance when the large language model is closely aligned with the target language, demonstrating the significance of choosing an appropriate LLM based on the target language."}, {"title": "4. Conclusion", "content": "This study presents a comparative analysis of LLM-based ASR and Whisper in low resource and code switching scenarios. Based on extensive experiments, the following key conclusions are drawn: (1) In languages where Whisper exhibits limited performance, LLM-based ASR demonstrates certain advantages. In contrast, Whisper excels in tasks such as Mandarin-English code-switching ASR. (2) The performance of LLM-based ASR model is positively correlated with the proficiency of the LLM in the specific language being recognized. We hope that our study can facilitate the research on ASR for low resource scenarios."}]}