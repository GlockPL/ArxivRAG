{"title": "Reimagining Al in Social Work: Practitioner Perspectives on Incorporating Technology in their Practice", "authors": ["KATIE WASSALL", "CAROLYN ASHURST", "JIRI HRON", "MIRI ZILKA"], "abstract": "There has been a surge in the number and type of AI tools being tested and deployed within both national and local government in the UK, including within the social care sector. Given the many ongoing and planned future developments, the time is ripe to review and reflect on the state of AI in social care. We do so by conducting semi-structured interviews with UK-based social work professionals about their experiences and opinions of past and current Al systems. Our aim is to understand what systems would practitioners like to see developed and how. We find that all our interviewees had overwhelmingly negative past experiences of technology in social care, unanimous aversion to algorithmic decision systems in particular, but also strong interest in AI applications that could allow them to spend less time on administrative tasks. In response to our findings, we offer a series of concrete recommendations, which include commitment to participatory design, as well as the necessity of regaining practitioner trust.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial Intelligence (AI) is increasingly used to inform decision-making in consequential settings within criminal justice, welfare, and social work [21, 24, 36, 48]. In social work, AI-based technologies are being incorporated across the board, including into practitioner decision-making [10, 14, 22, 23]. Adoption is driven by increased demand for effi-ciency stemming from unprecedented pressures on budgets, rising citizen demand, and challenges in the recruitment and retention of workers [12]. At the same time, the social work profession has been shaken by a number of high pub-licity failures, resulting in a greater focus on risk reduction, and the introduction of more data-based assessments [13].\nA common response is the deployment of algorithmic decision systems (ADS), under the promise of increased ob-jectivity, efficiency, and accuracy. ADS are often promoted as 'evidence-based', and as a route to better care without additional human resources. However, ADS and other technological deployments have often failed to deliver the adver-tised benefits, and instead introduced new risks and harms. For example, Kawakami et al. [23] show that practitioners can find ADS obstructive rather than helpful, and treat the tool as an adversary. Cheng et al. [3] then demonstrate that the recommendations of a child welfare screening algorithm show significant racial disparities compared to human decision-makers.\nIn the UK, both the national government and local councils have seen a surge in the number and type of AI tools being tested and deployed [10, 31]. With many ongoing and planned future developments, the time is ripe to review and reflect on the state of AI in social care. While most prior research focuses on the impacts of specific systems, we take a broader view, aiming to explore practitioners' experiences, perspectives, and needs. Our goal is to inform future developments both in terms of what systems should be developed and how.\nWe interview social workers in the UK to understand their perspectives on Al in social work, with focus on their professional experience with technology, their hopes and concerns for future, and their perspective on participation and design desiderata. Analysis of the participant responses revealed three key topics:\n(i) Overwhelmingly negative past experiences of technological systems incorporated into social work."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "2.1 Social Work in the UK\nSocial workers deliver care and support to many people including the elderly, children with disabilities, children at risk of neglect or abuse, young offenders, teenagers with mental health problems, adults with learning disabilities, mental or physical disabilities, people suffering from substance misuse, refugees and asylum seekers, foster carers, and adopters.\nVery broadly, the profession can be broken down into two primary areas: (i) adult social care, and (ii) children and fam-ilies. In 2021, Social Work England had 99,191 registered social workers [16]. The majority are employed by social care departments run by local authorities but some social workers are employed by other public bodies (e.g. National Health Service (NHS) or the Children and Family Court Advisory and Support Service), or the voluntary and private sector.\n2.2 Al in social work\nDue to unprecedented pressures on budgets, ageing populations, and a shift towards evidence-based decision making, there is growing interest in the potential of AI to support decision making in social care settings. However, there is currently limited evidence of the systems' effectiveness [5, 7]. While a comprehensive review of AI use in social care is out of scope, we provide illustrative examples that typify popular uses and demonstrate common challenges.\n2.2.1 Children and families social care. Over the past two decades, multiple US child welfare agencies have begun incorporating ADS into the child protection decision making process [1, 21, 23, 36]. Two well-documented currently used ADS are the proprietary Rapid Safety Feedback program, and the Allegheny Family Screening Tool (AFST) [20, 34, 43].\nAFST provides hotline screeners with a risk score to help them decide whether to open an investigation of child abuse or neglect. Pulling together data from a number of county systems that document family relationships with public services (including jail, juvenile prison, public welfare, health and census systems), the tool relies on over 100 variables to calculate the final score. Although some see AFST as a more accurate and transparent way to screen cases [15], there are concerns it unfairly targets children from poor and minority families [4, 20]. Notably, several similar tools have been discontinued because practitioners found them unreliable or opaque [1, 20].\nIn the UK, adoption of ADS and predictive analytics is within the discretion of local authorities. In 2018 England, 53 out of 152 local authorities were using predictive tools [10, 11]. This includes tools deployed in high-risk use cases such as identifying children at risk of abuse or neglect [10, 27, 35]. Following public backlash, Hackney Council halted its use of a commercially developed ADS stating that it did not yield the expected benefits [42]."}, {"title": "2.2.2 Adult social work", "content": "ADS is not the only type of a data-driven system increasingly deployed in adult social care. Morgan [28] lays out several proposals, ranging from monitoring, alerts, and task support, to AI companionship. The latter has been adopted in Japan, with government spending of over $300 million on research and development [32]. However, there is evidence the robots meant to assist caregivers within care homes create more issues than solve [32]. Another category of innovation is monitoring and 'desirable surveillance' of older adults who wish to maintain indepen-dence, such as human activity recognition and automatic monitoring [33]. While some work well in lab settings-e.g., Park et al. [33] claim they 99.55% average recognition accuracy-limited data is available on real-world use cases. In the UK, the NHSX AI Lab [31] has been piloting multiple use cases, including remote monitoring, predictive analytics, and assessing pain in people with dementia [14].\n2.3 Related work\nIn the wider public sector context, several studies have investigated on practitioner perspectives on the use of AI, in particular ADS [1, 23, 44]. A common thread is the general disregard for the context or the users in development of ADS. Public sector practitioners have viewed existing ADS deployments as unsuccessful, finding the tools to be a bur-densome rather than helpful [1, 23]. The authors highlight the need to re-think and re-design the \"interfaces, models, and organizational processes that shape the ways ADS tools are used in child welfare, and other public sector decision making contexts\". While relevant, the above works differ from ours in their focus on retrospective analysis (i.e., post system deployment), and focus purely on ADS tools, rather than Al more generally.\nSeveral related works did take a forward-looking approach. Kawakami et al. [24] interviewed social workers and co-generated 'design concepts', envisioning ways to redesign ADS interface. Stapleton et al. [40] ran workshops with various stakeholders-predominantly impacted parents and caseworkers-around the design and use of predictive risk models (PRMs) in US child welfare. The paper explores both concerns regarding the existing systems as well as future avenues of working in collaboration with impacted communities. Wang et al. [46] asked migrant jobseeker helpers to create 'design fictions' of objects from 2050 they would use in their work. In contrast to the above work, our focus is on the UK, and on understanding how the practitioners' needs, attitudes, and past experiences can shape the next generation of AI-based applications in social care, i.e., not limited to ADS or PRMs."}, {"title": "3 METHODOLOGY", "content": "We perform a qualitative study of UK social worker perspectives on AI, using a participatory practitioner-focused ap-proach. In February to March 2023, we conducted thirteen individual hour-long semi-structured interviews aiming to both understand practitioner attitudes and experiences with AI, and to identify places where they felt AI can (not) bring value in the future.\n3.1 Semi-structured interviews\nDesign. Following Schultze and Avital [37], we used a semi-structured interview format, as it allows for an in-depth exploration of individuals' personal stories, reflections, and reasoning, whilst maintaining a broadly consistent line of questioning [2]. The questions were divided into three sections:\n(a) current situation and challenges,\n(b) future opportunities and concerns,\n(c) design and participation."}, {"title": "4 THEME 1: EXPERIENCE WITH TECHNOLOGY IN SOCIAL WORK", "content": "In this and the next two sections, we summarise the major themes that materialised in our interviews. Whilst general attitudes towards AI varied, an overall broad consensus emerged around (i) negative past experience of technological systems, (ii) unanimous aversion to ADS, and (iii) the importance of practitioner-centred design. These key findings emerge in Sections 4, 5, and 6 respectively.\n4.1 Awareness of Al in social work\n4.1.1 Levels of exposure to Al. Among the thirteen interviewees, there was a wide spectrum of understanding and experience of Al in social work practice. Whilst all participants were familiar with the term 'AI', specific understand-ing of the term and experience of it varied among the different stakeholder groups (academics, policy, and frontline). Unsurprisingly, the academics working in this field were the most knowledgeable and forthcoming with different ex-amples of Al in social care and social work practice, followed closely by policymakers, and then finally the practitioners themselves who appeared to have little exposure to Al in their day-to-day practice.\n4.1.2 Examples of Al in social work. Both the academic and policy participants shared a number of first-hand examples of their experience of AI. For example, one participant discussed Tribe, a smart technology company which aims to intelligently map, develop, and connect people seeking care, with care providers. They characterised Tribe as \"trying to map areas of the country where there are very few formal services available ... to help visualise them so you can meet those needs\u201d. Other examples included Argenti (a telecare provider of assisted living technologies), Brain in Hand (a"}, {"title": "4.2 Perceived benefits and risks", "content": "4.2.1 Potential benefits. In spite of the lack of exposure to Al in their day-to-day roles, there was a clear interest from all frontline practitioners around the its potential. For example, \"something in the new generation of tech that could transform the way that social care is offered, if it's done properly, if it's done ethically\" (P3). This sentiment was echoed by one of the policy makers: \"[S]ome aspects, I think, that can be supported and enhanced through the use of Al, particularly in relation to needs that relate to people's activities of daily living, around physical disabilities, but also potentially, in relation to the need for connection with other people, for information, or for advice\u201d (P6).\nA particularly interesting view was expressed by a participant who recently moved from frontline practice to pur-suing a PhD in social work: \"My first reaction is that I don't think it's a good idea... It goes against social values and the kind of social work skills which are inherent to the profession.... [However,] I suppose since coming away from social work and doing research, I'm a bit more open to using technology for good and more effective children's services, job protecting systems, and things, ... but it has to be limited to very specific uses\" (P4).\n4.2.2 Potential risks. This concern around AI and the need for a deeper understanding of the risks was echoed by a several participants: \"[I]f we're not digitally critical of AI, it could run away with us. ... [I]t's like having a football match without a football pitch. If you don't have the boundaries, it just goes wherever it wants to\" (P2). P10 highlighted: \"There is a growing realisation that there's still quite a lot of limitations and risks involved in the use of different technological ap-plications. We're still in a very early stage, both in terms of their development, but also in terms of a broader understanding of them\".\n4.3 Lack of alignment between stakeholders\n4.3.1 Overpromised and underdelivered. In spite of participants' interest in the potential of AI in social work, there was shared scepticism of the decisions and rhetoric of senior management. For example, P1 stated: \u201cmy instinctive reaction is to be suspicious, partly because my experience over my career is being overpromised and then feeling underdelivered to in terms of the benefits of technology\u201d. P9 echoed this sentiment: \u201cThere's this magical belief that technology will improve productivity in the public sector and in social care\u201d."}, {"title": "4.4 Prior technology failures in social work", "content": "4.4.1 Poor user experience. Participants described how the lack of developer understanding of the practitioners' con-text resulted in technological interventions that cater to the needs of neither the practitioners, nor the carees. There was unanimous agreement that the majority of the existing digitised systems-broadly, not just AI-are \"really awful to use, ... rigid, and burdensome\" (P4). Instead of helping practitioners, existing systems are broadly seen as a barrier: \"[T]o give people the best chance of the care that they needed, the most person-centred care, was just consistently a pain in the backside. It didn't matter where I was geographically\" (P2).\n4.4.2 Lack of practitioner involvement. The lack of practitioner involvement was quoted as a major barrier to positivity around new technology, including AI. Interviewees mostly attributed past technological failures to the design without practitioner participation. The feeling that \"we are not adequately involved in the decision making process or in the im-plementation process\" (P1) led to an even greater disengagement: \"the opinions of the practitioner will never be heard when these tools developed\" (P9). The following example was shared by P10: \"[A]n ICT system was brought in around 2010. The government decided that ... child protection ... needed a uniform standardised approach. No social workers were engaged in developing the system, so it didn't look at what social workers needed. ... And guess what? The system flunked because it wasn't addressing the issue that was actually there, and it hadn't had input from the people who were dealing with it on the front line.\"\nOne participant did have experience of being consulted, but \"not much of it made it into the final product\" (P13). P2 summarised: \"For social work, it's the blind leading the blind. We don't have system leaders ... [or] social workers saying what good practice looks like. ... [W]ho is leading the conversation around AI and social work? Not social workers.\""}, {"title": "4.5 Potential barriers to successful use of Al", "content": "4.5.1 Lack of automatability. In addition to practitioners' negative experience of current technological systems, an-other key theme was the sectors' suitability to automation: \"I've never felt that social care is built for the information age\" (P8). In at least four of the interviews, participants raised a direct comparison to the health sector. P1-who works across the health and social care divide-explained they are \"more used to these conversations in health care\u201d. They shared that \"fantastic things are happening... different skin problems and potential cancers, with Al based systems that can interpret images, and then start to divert people into specialist services.\u201d Both accuracy and efficiency were cited as benefits of these applications.\nHowever, P1 went on, \"social care is different from my earlier skin cancer example... [It] is about feeling like you belong in your community, about love and relationships, about living chosen lifestyles\u201d. This sentiment was largely attributed to practitioner behaviours and characteristics. For example, \"the behaviours ... may have something to do with the way that social work and social care is seen as a non-clinical thing\u201d (P12).\n4.5.2 Lack of funding and infrastructure. When comparing adoption of AI in social and health care, P1 suggested: \u201csocial care is a long way behind partly because it doesn't have the same funding or infrastructure.", "data": "We've got lots of assessments, support plans, letters, complaints, inquiry reports, etc., but it's all in narrative form. We can tell you how many reports we've done, but extracting what's actually in them, and how it compares to what the local authority next door is doing, would be really interesting. ... [A] question for me is whether that is something machine learning could do\" (P7).\nDespite the consensus, there was disagreement around why the sector struggles to use data better. Many of the pol-icy and academic participants felt the fault is with practitioners: \u201cThe sector is very, very rich in data and information, but they don't understand the value of it, and I think some of that is to do with not needing to. ... If they saw the value of it, I think they would change some of the approaches towards it\" (P12). "}, {"title": "5 THEME 2: WHERE IS AI (NOT) USEFUL", "content": "The second section of our findings focuses on the perceived opportunities and applications of concern for practitioners."}, {"title": "5.1 Objections to ADS", "content": "As highlighted in Section 2, ADS is increasingly adopted in different jurisdictions and areas of social work. Despite this widespread adoption in other countries and related domains, all participants felt no desire for AI supported decision making, believing that only humans should be making such high-stakes decisions.\n5.1.1 Decisions should be made by humans as a matter of principle. Participants had many thoughts about why ADS should not be used in social work, as exemplified by a quote from P11: \"[I]t's important to bear in mind how horrible it can feel to think that decisions about you are being made by a machine, and not by a human.\u201d\n5.1.2 Lack of context, nuance, intuition, and cultural understanding. Others thought ADS could not handle specific aspects of their decision making. For example, participants felt ADS does have the intuitions and contextualized un-derstanding of a social worker \"[W]hen you make an assessment like that, it draws on information which is really difficult to capture, like the feel of when you go into a house\" (P4), and would not \"recognise the nuance of a situation\u201d (P4). The 'nuance' mostly related to the ability to understand the context: \u201ca tool that could work well in one context might not work well in another\" (P10).\nOne participant shared an example of how different cultural contexts can impact care decisions. In the Bangladeshi community, outsourcing care of a family member is often regarded as a family failure, which an adult social care worker will take into account. \"[O]ne size doesn't fit all. There's no one approach, no one intervention that you can use that's going to solve all of the many and varied problems that some of my colleagues call the \u2018messiness of human life'. Each set of individual circumstances is unique\" (P10).\n5.1.3 Data does not capture the richness of individual circumstance. Many practitioners view ADS as too reductive: \"Pre-sumably, any decision-making tool is only as good as the data that goes into it. And-as I mentioned-the data that we're able to gather is dependent on individuals and on subjective judgements.... Something might be lost if the input into the de-cision was only what you've managed to capture and write down\u201d (P11). Doubts about applicability of ADS in social work due to the reductive nature of data entry were considered in a 2016 report by the Behavioural Insights Team (BIT). The report highlighted the large number of factors which could be predictive of potential outcomes, and considered how in-dividual cases are likely to be a unique combination of such factors [41]. Still, the BIT concluded that ADS has potential to provide supplementary information to social workers, e.g., by highlighting the likely outcomes of different cases.\n5.1.4 Existing ADS target the wrong problems. One participant felt that even if the data were not reductive, ADS would still offer little benefit for high-stakes decisions: \u201c[A] tool to help with the big decisions might be solving a problem that doesn't exist, because the big decisions are made in collaboration with manager, or in the context of a strategy discussion to consider whether we should have a child protection investigation (they have to make these jointly with health and police), [etc.]...So these [big] decisions are already shared, and there are helpful forums for making them. [In contrast,] the deci-sions that I find most challenging as a social worker on day-to-day basis are the small [everyday] ones\" (P11). This insight further underlines how practitioner requirements can often be disregarded when considering ADS implementation.\n5.1.5 Lack of transparency over current use. One participant expressed suspicion that ADS is already being quietly de-ployed, despite the general resistance amongst our interviewees: \u201cI actually think that it's happened by stealth ... [N]eeds assessments have an algorithm built into them... that produces [an indicative budget] based on the answers that the social worker gives as part of the assessment. The number then forms the basis of the amount of care, monetary-wise, that the person is eligible for, against the Care Act. Local authorities have been using that ...for a number of years, and those\""}, {"title": "5.2 Areas where Al could potentially add value", "content": "When exploring the areas AI-based innovation could benefit social work, a number of recurring use cases emerged. Importantly, all of the proposals centred around improving the practitioners capacity and ability to provide more mean-ingful care. All suggestions sought to either improve upstream processes that were considered outside of their direct roles, or to limit time spent on administrative tasks. None of the thirteen participants favoured automating direct care.\n5.2.1 Finding information. The most common suggestion, brought up by six participants, was to use AI to find suitable auxiliary support services. Participants generally wanted to improve the information, access, and relevancy around the initial \u201cprovision of information and advice\u201d (P5). \u201c[T]he local authority could do so much better in organizing the infor-mation it has about all the community assets, and surfacing that information in a way that a member of the public can go into it without even necessarily thinking", "[W]hat I'm thinking about is those situations where, for example, I know that I would like a counselling service that specializes in working with children affected by domes-tic violence and abuse, who are 11 years old, and boys, and takes referrals from my borough. These can take a long time to find because it'll be hidden on some random page of a website, or they actually only accept applicants from certain boroughs. I can definitely see a role for Al there, if that process of going through and looking at all the services could be replaced": "P11).\nTwo frontline practitioners described how the existing manual referrals process is limited. They shared examples of failed ad-hoc attempts to improve the process: \u201cI think keeping up to date on local community resources is really hard and time consuming. I saw that as an issue in my team and I was like: 'Why are we each individually googling these resources?' So I made a spreadsheet to share it. But I just had to let it go out of date because, realistically, I just couldn't do it", "P1": "something that prioritises and flags things to sort out the inbox", "en-quiries": "P5: \"signpost to different services once the system has been able to filter a variety of different information that it had acquired", "tasks": "[I wonder] whether there're ways of automating some of the signposting and the handoffs, leaving the social worker to focus on the kind of more complex areas of need\" (P1).\nHowever, nearly half of the frontline workers doubted this use case due to accuracy: \\\"[A]lthough I do spend a lot of time emailing, I wouldn't say that that's wasted time. I think firstly, I'm very happy to be CCed into emails ... because everything helps me to be informed about the families that I work with. I would struggle to trust that system would work because, for example, I'd say that 70% of the emails I receive will say at the top something like 'Be careful, this comes from a sender that you don't normally receive emails from'. And the reason is that I just receive emails from so many different agencies.\" (P11).\n5.2.3 Improving data collection. Another area participants explored was use of technology to improve data collection. Given we were using Al to transcribe our interviews (otter.ai), we explored if a similar tool would be useful. The responses were divided. Two practitioners felt strongly about the potential benefits: \\\"If social workers had something like that, you've instantly reduced our workloads by hours and hours a week", "there's a crying need for data, for evidence, and for some sort of systematic approach to capturing that data on a real life basis": "P3)."}, {"title": "6 THEME 3: FUTURE DEVELOPMENT", "content": "6.1 Participation by practitioners and carees\nThere was unanimous consensus on the criticality of involving practitioners in the design, development, and deploy-ment of any new technological system. All participants felt strongly that \"it all starts from the ground up\" (P8), and one should \"start by asking people, what is it about the current system you don't like?\u201d (P8), to ensure people are given \"the chance to actually have their voices heard\u201d (P8). As P12 explained: \u201cIt is essential that practitioners are directly involved because they are going to be using it. You often hear of very clever guys-usually guys, but not necessarily very clever people-developing things and then trying them out, and that's not the way to do it. There has to be co-production. You have to engage from the beginning, but not just the practitioners. I think you need to engage the families as well, because it's a collaborative process\u201d. The need to involve not only the practitioners was echoed by multiple participants: \u201cCo-production and ensuring that the lived experience of the service recipient is involved in influencing whatever design or commissioning that's underway", "Don't take decisions about us without our engagement": "P10).\n6.2 Overcoming obstacles to participation\n6.2.1 Technological literacy. Many participants felt such participatory design requires first improving data and AI lit-eracy of the involved practitioners: \"If you don't know what Al is, then you don't know what you don't know. You don't know that it's potentially a thing, a tool, a mechanism to proportionately empower or make someone's life better", "training": "I think that there should be more in a way of training or literacy around data, or AI, that will help people understand the options. Help them be more involved in proposing the direction things go, technology-wise", "users": "[W]e want to move closer to co-production with the families that we work with. But sometimes, a barrier to that is not our access to and familiarity with technology, but the comfort and familiarity with technology of the families\" (P11).\""}, {"title": "7 DISCUSSION AND IMPLICATIONS", "content": "Three key topics emerged from our interviews with the UK social work practitioners:\n(i) Largely negative past experiences with technological systems incorporated into social work.\n(ii) Unanimous aversion to the incorporation of ADS systems.\n(iii) An interest in non-ADS AI applications, conditioned on these being developed in a participatory manner.\nIn this section, we explore these key findings further and situate them within the wider literature (\u00a77.1), discuss visions for future technology design in social work (\u00a77.2), and important considerations for future developments (\u00a77.3).\n7.1 Exploring the key findings\nLittle benefit to practitioners. Our interviewees perceive technology as rarely helpful to social workers. Existing lit-erature similarly found attempts to digitise and automate \u201chighly problematic for frontline social workers, particularly in terms of diverting their time, attention, and energy away from direct work with service users\" [19, 29, 47]. Ongoing challenges with data collection were flagged as a core frustration, agreeing with Gillingham [19] who found data cap-ture and entry are perceived as a tax and a form of surveillance. The amount of time that practitioners spend manually repeating data entry has become disproportionate to the time spent giving care. The data collected is also not perceived as useful, rendering the effort meaningless. Similar sentiment extends beyond social work, with frustration over elec-tronic record keeping affecting probation officers [26] and doctors [17, 25] alike. While the user experience of data\""}, {"title": "8 CONCLUSION", "content": "As the integration of AI technologies into the UK's social care sector continues to gain momentum, it is imperative that we look beyond the developer promises, and consider the opinions of those impacted by the introduction of these tech-nologies. We conducted thirteen semi-structured interviews, with the goal of understanding the experiences and atti-tudes to Al among UK social care professionals, and how these shape their views on the development of future systems.\nWe found practitioners have been largely dissatisfied with technology deployed in social care, particularly with algorithmic decision systems, and the burden introduced by data collection requirements. Many of these failures were caused by a lack of involvement of practitioners in the design process, resulting in applications that solve no problem relevant to the carers or carees. While these past experiences engendered general scepticism, there is still significant in-terest in AI tools that could reduce administrative burdens, and allow practitioners to spend more time helping people in need.\nWe made several recommendations based on our findings. First, we advocate for a strong commitment to partici-patory design, ensuring that the perspectives and needs of practitioners and carees are incorporated throughout the development and deployment. Second, we recommend a refocus towards applications genuinely supporting and com-plementing social care professionals, rather than replacing or undermining their expertise. Lastly, we highlight the importance of transparency, and of addressing wider barriers to successful use of technology, including funding, in-frastructure, and data literacy. Our conclusions were shared with the practitioners and BASW, and we hope they will lead to development of more effective and safe Al systems, helping social care professionals in their unwavering quest to support vulnerable individuals and communities."}]}