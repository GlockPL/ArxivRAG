{"title": "Reimagining Al in Social Work: Practitioner Perspectives on Incorporating Technology in their Practice", "authors": ["KATIE WASSALL", "CAROLYN ASHURST", "JIRI HRON", "MIRI ZILKA"], "abstract": "There has been a surge in the number and type of AI tools being tested and deployed within both national and local government in the UK, including within the social care sector. Given the many ongoing and planned future developments, the time is ripe to review and reflect on the state of AI in social care. We do so by conducting semi-structured interviews with UK-based social work professionals about their experiences and opinions of past and current Al systems. Our aim is to understand what systems would practitioners like to see developed and how. We find that all our interviewees had overwhelmingly negative past experiences of technology in social care, unanimous aversion to algorithmic decision systems in particular, but also strong interest in AI applications that could allow them to spend less time on administrative tasks. In response to our findings, we offer a series of concrete recommendations, which include commitment to participatory design, as well as the necessity of regaining practitioner trust.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial Intelligence (AI) is increasingly used to inform decision-making in consequential settings within criminal justice, welfare, and social work [21, 24, 36, 48]. In social work, AI-based technologies are being incorporated across the board, including into practitioner decision-making [10, 14, 22, 23]. Adoption is driven by increased demand for effi-ciency stemming from unprecedented pressures on budgets, rising citizen demand, and challenges in the recruitment and retention of workers [12]. At the same time, the social work profession has been shaken by a number of high pub-licity failures, resulting in a greater focus on risk reduction, and the introduction of more data-based assessments [13].\nA common response is the deployment of algorithmic decision systems (ADS), under the promise of increased ob-jectivity, efficiency, and accuracy. ADS are often promoted as 'evidence-based', and as a route to better care without additional human resources. However, ADS and other technological deployments have often failed to deliver the adver-tised benefits, and instead introduced new risks and harms. For example, Kawakami et al. [23] show that practitioners can find ADS obstructive rather than helpful, and treat the tool as an adversary. Cheng et al. [3] then demonstrate that the recommendations of a child welfare screening algorithm show significant racial disparities compared to human decision-makers.\nIn the UK, both the national government and local councils have seen a surge in the number and type of AI tools being tested and deployed [10, 31]. With many ongoing and planned future developments, the time is ripe to review and reflect on the state of AI in social care. While most prior research focuses on the impacts of specific systems, we take a broader view, aiming to explore practitioners' experiences, perspectives, and needs. Our goal is to inform future developments both in terms of what systems should be developed and how.\nWe interview social workers in the UK to understand their perspectives on Al in social work, with focus on their professional experience with technology, their hopes and concerns for future, and their perspective on participation and design desiderata. Analysis of the participant responses revealed three key topics:\n(i) Overwhelmingly negative past experiences of technological systems incorporated into social work."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "2.1 Social Work in the UK\nSocial workers deliver care and support to many people including the elderly, children with disabilities, children at risk of neglect or abuse, young offenders, teenagers with mental health problems, adults with learning disabilities, mental or physical disabilities, people suffering from substance misuse, refugees and asylum seekers, foster carers, and adopters.\nVery broadly, the profession can be broken down into two primary areas: (i) adult social care, and (ii) children and fam-ilies. In 2021, Social Work England had 99,191 registered social workers [16]. The majority are employed by social care departments run by local authorities but some social workers are employed by other public bodies (e.g. National Health Service (NHS) or the Children and Family Court Advisory and Support Service), or the voluntary and private sector.\n2.2 Al in social work\nDue to unprecedented pressures on budgets, ageing populations, and a shift towards evidence-based decision making, there is growing interest in the potential of AI to support decision making in social care settings. However, there is currently limited evidence of the systems' effectiveness [5, 7]. While a comprehensive review of AI use in social care is out of scope, we provide illustrative examples that typify popular uses and demonstrate common challenges.\n2.2.1 Children and families social care. Over the past two decades, multiple US child welfare agencies have begun incorporating ADS into the child protection decision making process [1, 21, 23, 36]. Two well-documented currently used ADS are the proprietary Rapid Safety Feedback program, and the Allegheny Family Screening Tool (AFST) [20, 34, 43].\nAFST provides hotline screeners with a risk score to help them decide whether to open an investigation of child abuse or neglect. Pulling together data from a number of county systems that document family relationships with public services (including jail, juvenile prison, public welfare, health and census systems), the tool relies on over 100 variables to calculate the final score. Although some see AFST as a more accurate and transparent way to screen cases [15], there are concerns it unfairly targets children from poor and minority families [4, 20]. Notably, several similar tools have been discontinued because practitioners found them unreliable or opaque [1, 20].\nIn the UK, adoption of ADS and predictive analytics is within the discretion of local authorities. In 2018 England, 53 out of 152 local authorities were using predictive tools [10, 11]. This includes tools deployed in high-risk use cases such as identifying children at risk of abuse or neglect [10, 27, 35]. Following public backlash, Hackney Council halted its use of a commercially developed ADS stating that it did not yield the expected benefits [42]."}, {"title": "3 METHODOLOGY", "content": "We perform a qualitative study of UK social worker perspectives on AI, using a participatory practitioner-focused ap-proach.\u00b9 In February to March 2023, we conducted thirteen individual hour-long semi-structured interviews aiming to both understand practitioner attitudes and experiences with AI, and to identify places where they felt AI can (not) bring value in the future.\n3.1 Semi-structured interviews\nDesign. Following Schultze and Avital [37], we used a semi-structured interview format, as it allows for an in-depth exploration of individuals' personal stories, reflections, and reasoning, whilst maintaining a broadly consistent line of questioning [2]. The questions were divided into three sections:\n(a) current situation and challenges,\n(b) future opportunities and concerns,\n(c) design and participation."}, {"title": "4 THEME 1: EXPERIENCE WITH TECHNOLOGY IN SOCIAL WORK", "content": "In this and the next two sections, we summarise the major themes that materialised in our interviews. Whilst general attitudes towards AI varied, an overall broad consensus emerged around (i) negative past experience of technological systems, (ii) unanimous aversion to ADS, and (iii) the importance of practitioner-centred design. These key findings emerge in Sections 4, 5, and 6 respectively.\n4.1 Awareness of Al in social work\n4.1.1 Levels of exposure to Al. Among the thirteen interviewees, there was a wide spectrum of understanding and experience of Al in social work practice. Whilst all participants were familiar with the term 'AI', specific understand-ing of the term and experience of it varied among the different stakeholder groups (academics, policy, and frontline). Unsurprisingly, the academics working in this field were the most knowledgeable and forthcoming with different ex-amples of Al in social care and social work practice, followed closely by policymakers, and then finally the practitioners themselves who appeared to have little exposure to Al in their day-to-day practice.\n4.1.2 Examples of Al in social work. Both the academic and policy participants shared a number of first-hand examples of their experience of AI. For example, one participant discussed Tribe, a smart technology company which aims to intelligently map, develop, and connect people seeking care, with care providers. They characterised Tribe as \"trying to map areas of the country where there are very few formal services available ... to help visualise them so you can meet those needs\u201d. Other examples included Argenti (a telecare provider of assisted living technologies),5 Brain in Hand (a\n4.2 Perceived benefits and risks\n4.2.1 Potential benefits. In spite of the lack of exposure to Al in their day-to-day roles, there was a clear interest from all frontline practitioners around the its potential. For example, \"something in the new generation of tech that could transform the way that social care is offered, if it's done properly, if it's done ethically\" (P3). This sentiment was echoed by one of the policy makers: \"[S]ome aspects, I think, that can be supported and enhanced through the use of Al, particularly in relation to needs that relate to people's activities of daily living, around physical disabilities, but also potentially, in relation to the need for connection with other people, for information, or for advice\u201d (P6).\nA particularly interesting view was expressed by a participant who recently moved from frontline practice to pur-suing a PhD in social work: \"My first reaction is that I don't think it's a good idea... It goes against social values and the kind of social work skills which are inherent to the profession.... [However,] I suppose since coming away from social work and doing research, I'm a bit more open to using technology for good and more effective children's services, job protecting systems, and things, ... but it has to be limited to very specific uses\" (P4).\n4.2.2 Potential risks. This concern around AI and the need for a deeper understanding of the risks was echoed by a several participants: \"[I]f we're not digitally critical of AI, it could run away with us. ... [I]t's like having a football match without a football pitch. If you don't have the boundaries, it just goes wherever it wants to\" (P2). P10 highlighted: \"There is a growing realisation that there's still quite a lot of limitations and risks involved in the use of different technological ap-plications. We're still in a very early stage, both in terms of their development, but also in terms of a broader understanding of them\".\n4.3 Lack of alignment between stakeholders\n4.3.1 Overpromised and underdelivered. In spite of participants' interest in the potential of AI in social work, there was shared scepticism of the decisions and rhetoric of senior management. For example, P1 stated: \u201cmy instinctive reaction is to be suspicious, partly because my experience over my career is being overpromised and then feeling underdelivered to in terms of the benefits of technology\u201d. P9 echoed this sentiment: \u201cThere's this magical belief that technology will improve productivity in the public sector and in social care\u201d."}, {"title": "5 THEME 2: WHERE IS AI (NOT) USEFUL", "content": "The second section of our findings focuses on the perceived opportunities and applications of concern for practitioners.\n5.1 Objections to ADS\nAs highlighted in Section 2, ADS is increasingly adopted in different jurisdictions and areas of social work. Despite this widespread adoption in other countries and related domains, all participants felt no desire for AI supported decision making, believing that only humans should be making such high-stakes decisions.\n5.1.1 Decisions should be made by humans as a matter of principle. Participants had many thoughts about why ADS should not be used in social work, as exemplified by a quote from P11: \"[I]t's important to bear in mind how horrible it can feel to think that decisions about you are being made by a machine, and not by a human.\u201d\n5.1.2 Lack of context, nuance, intuition, and cultural understanding. Others thought ADS could not handle specific aspects of their decision making. For example, participants felt ADS does have the intuitions and contextualized un-derstanding of a social worker \"[W]hen you make an assessment like that, it draws on information which is really difficult to capture, like the feel of when you go into a house\" (P4), and would not \"recognise the nuance of a situation\u201d (P4). The 'nuance' mostly related to the ability to understand the context: \u201ca tool that could work well in one context might not work well in another\" (P10).\nOne participant shared an example of how different cultural contexts can impact care decisions. In the Bangladeshi community, outsourcing care of a family member is often regarded as a family failure, which an adult social care worker will take into account. \"[O]ne size doesn't fit all. There's no one approach, no one intervention that you can use that's going to solve all of the many and varied problems that some of my colleagues call the \u2018messiness of human life'. Each set of individual circumstances is unique\" (P10).\n5.1.3 Data does not capture the richness of individual circumstance. Many practitioners view ADS as too reductive: \"Pre-sumably, any decision-making tool is only as good as the data that goes into it. And-as I mentioned-the data that we're able to gather is dependent on individuals and on subjective judgements.... Something might be lost if the input into the de-cision was only what you've managed to capture and write down\u201d (P11). Doubts about applicability of ADS in social work due to the reductive nature of data entry were considered in a 2016 report by the Behavioural Insights Team (BIT). The report highlighted the large number of factors which could be predictive of potential outcomes, and considered how in-dividual cases are likely to be a unique combination of such factors [41]. Still, the BIT concluded that ADS has potential to provide supplementary information to social workers, e.g., by highlighting the likely outcomes of different cases.\n5.1.4 Existing ADS target the wrong problems. One participant felt that even if the data were not reductive, ADS would still offer little benefit for high-stakes decisions: \u201c[A] tool to help with the big decisions might be solving a problem that doesn't exist, because the big decisions are made in collaboration with manager, or in the context of a strategy discussion to consider whether we should have a child protection investigation (they have to make these jointly with health and police), [etc.]...So these [big] decisions are already shared, and there are helpful forums for making them. [In contrast,] the deci-sions that I find most challenging as a social worker on day-to-day basis are the small [everyday] ones\" (P11). This insight further underlines how practitioner requirements can often be disregarded when considering ADS implementation.\n5.1.5 Lack of transparency over current use. One participant expressed suspicion that ADS is already being quietly de-ployed, despite the general resistance amongst our interviewees: \u201cI actually think that it's happened by stealth ... [N]eeds assessments have an algorithm built into them... that produces [an indicative budget] based on the answers that the social worker gives as part of the assessment. The number then forms the basis of the amount of care, monetary-wise, that the person is eligible for, against the Care Act. Local authorities have been using that ...for a number of years, and those"}, {"title": "6 THEME 3: FUTURE DEVELOPMENT", "content": "6.1 Participation by practitioners and carees\nThere was unanimous consensus on the criticality of involving practitioners in the design, development, and deploy-ment of any new technological system. All participants felt strongly that \"it all starts from the ground up\" (P8), and one should \"start by asking people, what is it about the current system you don't like?", "the chance to actually have their voices heard\u201d (P8). As P12 explained: \u201cIt is essential that practitioners are directly involved because they are going to be using it. You often hear of very clever guys-usually guys, but not necessarily very clever people-developing things and then trying them out, and that's not the way to do it. There has to be co-production. You have to engage from the beginning, but not just the practitioners. I think you need to engage the families as well, because it's a collaborative process\u201d. The need to involve not only the practitioners was echoed by multiple participants: \u201cCo-production and ensuring that the lived experience of the service recipient is involved in influencing whatever design or commissioning that's underway": "P5); \"Don't take decisions about us without our engagement", "practitioners": "If you don't know what Al is, then you don't know what you don't know. You don't know that it's potentially a thing, a tool, a mechanism to proportionately empower or make someone's life better\" (P2). For many, this meant more training: \u201cI think that there should be more in a way of training or literacy around data, or AI, that will help people understand the options. Help them be more involved in proposing the direction things go, technology-wise\u201d (P4). This digital upskilling should not only be directed at practitioners, but also service users: \"[W]e want to move closer to co-production with the families that we work with. But sometimes, a barrier to that is not our access to and familiarity with technology, but the comfort and familiarity with technology of the families\" (P11).\""}, {"title": "7 DISCUSSION AND IMPLICATIONS", "content": "Three key topics emerged from our interviews with the UK social work practitioners:\n(i) Largely negative past experiences with technological systems incorporated into social work.\n(ii) Unanimous aversion to the incorporation of ADS systems.\n(iii) An interest in non-ADS AI applications, conditioned on these being developed in a participatory manner.\nIn this section, we explore these key findings further and situate them within the wider literature (\u00a77.1), discuss visions for future technology design in social work (\u00a77.2), and important considerations for future developments (\u00a77.3).\n7.1 Exploring the key findings\nLittle benefit to practitioners. Our interviewees perceive technology as rarely helpful to social workers. Existing lit-erature similarly found attempts to digitise and automate \u201chighly problematic for frontline social workers, particularly in terms of diverting their time, attention, and energy away from direct work with service users\" [19, 29, 47]. Ongoing challenges with data collection were flagged as a core frustration, agreeing with Gillingham [19] who found data cap-ture and entry are perceived as a tax and a form of surveillance. The amount of time that practitioners spend manually repeating data entry has become disproportionate to the time spent giving care. The data collected is also not perceived as useful, rendering the effort meaningless. Similar sentiment extends beyond social work, with frustration over elec-tronic record keeping affecting probation officers [26] and doctors [17, 25] alike. While the user experience of data"}, {"title": "8 CONCLUSION", "content": "As the integration of AI technologies into the UK's social care sector continues to gain momentum, it is imperative that we look beyond the developer promises, and consider the opinions of those impacted by the introduction of these tech-nologies. We conducted thirteen semi-structured interviews, with the goal of understanding the experiences and atti-tudes to Al among UK social care professionals, and how these shape their views on the development of future systems.\nWe found practitioners have been largely dissatisfied with technology deployed in social care, particularly with algorithmic decision systems, and the burden introduced by data collection requirements. Many of these failures were caused by a lack of involvement of practitioners in the design process, resulting in applications that solve no problem relevant to the carers or carees. While these past experiences engendered general scepticism, there is still significant in-terest in AI tools that could reduce administrative burdens, and allow practitioners to spend more time helping people in need.\nWe made several recommendations based on our findings. First, we advocate for a strong commitment to partici-patory design, ensuring that the perspectives and needs of practitioners and carees are incorporated throughout the development and deployment. Second, we recommend a refocus towards applications genuinely supporting and com-plementing social care professionals, rather than replacing or undermining their expertise. Lastly, we highlight the importance of transparency, and of addressing wider barriers to successful use of technology, including funding, in-frastructure, and data literacy. Our conclusions were shared with the practitioners and BASW, and we hope they will lead to development of more effective and safe Al systems, helping social care professionals in their unwavering quest to support vulnerable individuals and communities."}]}