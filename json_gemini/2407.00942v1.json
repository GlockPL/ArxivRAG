{"title": "ProductAgent: Benchmarking Conversational Product Search Agent with Asking Clarification Questions", "authors": ["Jingheng Ye", "Yong Jiang", "Xiaobin Wang", "Yinghui Li", "Yangning Li", "Hai-Tao Zheng", "Pengjun Xie", "Fei Huang"], "abstract": "This paper introduces the task of product demand clarification within an e-commercial scenario, where the user commences the conversation with ambiguous queries and the task-oriented agent is designed to achieve more accurate and tailored product searching by asking clarification questions. To address this task, we propose ProductAgent, a conversational information seeking agent equipped with abilities of strategic clarification question generation and dynamic product retrieval. Specifically, we develop the agent with strategies for product feature summarization, query generation, and product retrieval. Furthermore, we propose the benchmark called PROCLARE to evaluate the agent's performance both automatically and qualitatively with the aid of a LLM-driven user simulator. Experiments show that ProductAgent interacts positively with the user and enhances retrieval performance with increasing dialogue turns, where user demands become gradually more explicit and detailed.", "sections": [{"title": "1 Introduction", "content": "For decades, researchers have been attempting to develop agents with satisfying abilities of understanding human intentions and making decisions (Wooldridge and Jennings, 1995; Agha, 1986). However, these efforts have often been limited to constrained situations (Aliannejadi et al., 2021), where user-system interactions are insufficient (Vats et al., 2024). This limitation becomes more prominent and even critical especially in high-stakes domains or task-oriented systems (Yi et al., 2024), where users often struggle to express their ambiguous and personal information requirements in the first place (Rahmani et al., 2023)."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 LLM-based Agents", "content": "Building agents centered on LLMs has been widely studied recently (Park et al., 2023; Liu et al., 2023; Nakano et al., 2021), owing to their preeminent abilities of reasoning (Yao et al., 2023; Lu et al., 2024), planning (Song et al., 2023; Huang et al., 2023), memorizing, reflexion (Shinn et al., 2024; Huang et al., 2024) or self-correction (Gou et al., 2023), and tool utilization (Schick et al., 2024; Qin et al., 2023). Thanks to techniques like Chain-of-Thought (CoT) (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022) and problem decomposition, LLM-based agents are able to solve challenging problems across a wide rage of real-world scenarios (Li et al., 2023a). However, many advanced agents focus on resolving real-world problems with minimal human intervention. This often results in bias (Ye et al., 2023) and the lack of flexibility and inefficiency, particularly in task-oriented scenarios that emphasize interaction (Wang et al., 2023). The involvement of human feedback (Peng et al., 2023; Zhang et al., 2024a), therefore, becomes progressively critical in guiding and supervising the action trajectory of agents, ensuring their alignment with human requirements and objectives (Kenton et al., 2021; Ngo et al., 2022)."}, {"title": "2.2 Asking Clarification Questions", "content": "Aiming to provide task-specific services, task-oriented agents often confront vague queries from users, who may lack the required domain knowledge to articulate their queries accurately. The ensuing reduced performance is mitigated in recent studies (Zamani et al., 2020; Kuhn et al., 2022), which advocate to ask clarification questions to pinpoint user demands in a conversational manner. For example, MAS2S (Feng et al., 2023)introduces the idea of asking questions based on user profiles and task knowledge to clarify user requests. Lee et al. (2023) presents the CAMBIGNQ dataset consisting of ambiguous questions. This dataset provides a foundation for a pipeline system enabling the asking of clarification questions to become more efficient. Furthermore, recent studies (Wu, 2024; Erbacher and Soulier, 2023; Zhang et al., 2024b) applies clarification questions onto LLMs, achieving significant improvements across various downstream applications. A concurrent study (Chang and Chen, 2024) develops SalesAgent that concentrates on injecting dialogue strategies into LLMs by fine-tuning, which is orthogonal to our work. Additionally, Vedula et al. (2024) also propose to generate product-related clarification questions through prompting and fine-tuning LLMs.\nHowever, all these works rely heavily on the internal knowledge of LLMs to generate clarification questions, often compromising the quality of the generated clarification questions. On the contrary, our work leverages specific databases and external tools, thus unburdening LLMs from knowledge acquirement and promoting the generation of quality clarification questions."}, {"title": "3 Product Demand Clarification Task", "content": "The objective of the product demand clarification task is to enhance the e-commerce search process through the collaboration between humans and the agent. This task is based on a typical online shopping scenario that users (customers) have decided to purchase a product of a certain category but they have not yet determined detailed aspects of the product of their interests. So they often struggle to clearly articulate their purchasing requirements, especially for product categories accompanied by unfamiliar jargon. To tackle this challenge, the task aims to identify user demands by explicitly posing clarification questions that help clarify any uncertainties towards product searching.\nThe task takes the form of conversation, where an agent proactively interacts with a user. Specifically, given a product category as $U_1$, the conversation $D$ between them is formulated as follows:\n$D = {U_1, A_1, P_1, U_2, A_2, P_2, \u2026\u2026\u2026}$ (1)\nwhere $U_t$ ($t = 1,2,\u2026\u2026$) is the user utterance, $A_t$ is the agent utterance, and $P_t$ is the retrieved product items. The agent utterance primarily consists of several multi-choice clarification questions $A_t = {Q_{t1}, Q_{t2},\u2026\u2026,Q_{tn}}$. Each question includes the content of the question itself and a set of answer candidates. It is important to note that, in each turn, the agent outputs both the questions and the retrieved items as timely feedback, facilitating a customer-friendly application interface.\nThe task presents several key challenges: 1) Generating non-trivial clarification questions requires a significant amount of product knowledge. 2) The agent should possess the ability to understand user intentions accurately in order to retrieve the most relevant product items and generate dynamic and valuable clarification questions. 3) The establishment of a quantitative and automated evaluation protocol is necessary to facilitate the development of effective agents for this task. This paper addresses the first two challenges in Section 4, and the third challenge in Section 5."}, {"title": "4 ProductAgent", "content": ""}, {"title": "4.1 Overview of ProductAgent", "content": "This section delineates the proposed solution ProductAgent for the introduced task. As shown in Figure 2, ProductAgent executes conversational loops, each of which can be decomposed by three main steps. Before initializing the loops, the user"}, {"title": "4.2 Key Implementation", "content": "Our proposed ProductAgent consists of three key components which work collectively to solve the product demand clarification task.\nDatabases. We store all product items in both SQL and dense vector datasets, allowing for flexible retrieval strategies. As shown in Figure 2, we employ SQL retrieval in the stage 1 since it enables us to efficiently retrieve items through exact matching. On the other hand, we leverage non-SQL retrieval in the stage 2 because non-SQL retrievers can return product items ordered by their relevance to the user demands, which cannot be typically accomplished by SQL retrieval.\nTools. We define 5 tools for ProductAgent, which are listed in Table 2. To enable SQL retrieval, ProductAgent uses the Text2SQL tool. This tool prompts LLMs with the history demands to generate practical SQL queries. The synthesized SQL query is then used to retrieve product items relevant to the current known demands. Then, the Category Analyzing tool is performed to generate summarized statistics. For non-SQL retrieval, we create the Query Generation tool, which generates concise natural language queries. Additionally, in stage 3, we utilize the Question Generation tool to fabricate reasonable clarification questions based on the summarized statistics and the user demands. To ensure the desired output format, we specify the json description for ProductAgent.\nIt should be kindly noted that for the purpose of autonomous evaluation, the demand clarification task has been slightly simplified and hence does not require a tool router, which is typically used to select the tool for the next action.\nMemory. All the user demands, including clarification questions and corresponding answers, are stored as structured objects in the memory module. When the agent receives responses from users, it extracts the answers of clarification questions posed in the previous turn. The Q&A pairs are then saved as structured demand objects. Additionally, the memory module stores the dialogue records and searching history. When invoking certain tools, we integrate only the required information selected from the memory into the designated slots within the prompt. The memory are continuously updated as the dialogue progresses, ensuring that the intelligent agent can effectively handle dynamically user requirements. The design is crucial as it enables the agent to recall user preferences, thereby providing personalized assistance."}, {"title": "5 The PROCLARE Benchmark", "content": "Most existing information retrieval benchmarks focus on static search processes (Qiu et al., 2022; Long et al., 2022) that do not involve human-AI interactions, which may limit their effectiveness in evaluating practical and interactive performance of search systems. Therefore, with the aim of comprehensively investigating the performance of conversational e-commercial search agents in an automatic manner, we introduce PROCLARE, a benchmarking framework that evaluates retrieval performance in two distinct settings, namely the traditional setting (Section 5.2) and the conversational (Section 5.4) setting. Both ProductAgent and PROCLARE are implemented using the open-sourced framework LlamaIndex."}, {"title": "5.1 Document Set", "content": "We begin by creating a extensive collection of documents (product items), which serves as the knowledge base for our benchmark. This collection consists of 1,000,000 documents across 20 categories. To construct this dataset, we utilize AliMe KG (Li et al., 2020), a domain knowledge graph in the field of e-commerce that supports the top-50 main categories on the Alibaba e-commerce platform. From each category, we randomly sample 50,000 documents, resulting in the dataset used for our benchmark. To ensure efficient data management, we perform Named Entity Recognition (NER) preprocessing (Wang et al., 2021) on all the data. This results in a unified feature definition across various product categories, enabling us to store all the documents in a relational SQL dataset. For detailed statistics and description of the dataset, please refer to Table 3 and Appendix A.1."}, {"title": "5.2 Traditional Information Retrieval", "content": "In the traditional setting of PROCLARE, we evaluate the retrieval performance of the conventional product search task using the methodology employed in previous work (Long et al., 2022; Qiu et al., 2022). It is important to note that this experimental setting, which serves as a counterpart to the conversation setting introduced in Section 3, does not involve the use of the agent. To construct queries used for evaluation, we randomly sample 100 documents per category from the entire document set, resulting in a total of 2000 documents. We then leverage the Doc2Query technique (Gospodinov et al., 2023; Alaofi et al., 2023) to transform these selected documents into synthesized queries by prompting GPT-4. Subsequently, we measure the retrieval performance of different retrieval methods based on the synthesized queries. The prompt for Doc2Query and examples of synthesized queries can be found in Appendix B."}, {"title": "5.3 Retrieval Strategies", "content": "Our experiments employ three widely-adopted retrievers: BM25 (Robertson et al., 2009), General Text Embedding (GTE), and CoROM (Long et al., 2022). Besides, we also incorporate the techniques of reranking and fusion into the retrievers. The detailed descriptions are provided in Appendix A.2."}, {"title": "5.4 Conversational Information Retrieval", "content": "The performance of ProductAgent can be comprehensively explored in the conversational setting, where its effectiveness is manifested directly via the retrieval performance across dialogue turns.\nUser simulation. It is well-acknowledged that the nature of dialogues, including the high cost of dialogue annotation (Schatzmann et al., 2005; Keizer et al., 2010), poses a challenge for the efficient evaluation of dialogue systems, particularly in an end-to-end manner. To handle this issue, we harness LLMs as intelligent user simulators, thereby accomplishing the automatic evaluation of ProductAgent. Specifically, a LLM-driven user simulator is presented with several clarification questions along with their corresponding candidates in each turn, and then is prompted to answer the questions with access to the ground truth item. It is worth noticing that to avoid potential information leakage, which may create shortcuts for the agent, we restrict the user simulator to only answer the given questions with provided candidates, thus simulating a more realistic and reliable situation.\nFor each queried document, we simulate 10 turns (5 user turns and 5 agent turns) with the user simulator starting the conversation by inputting the"}, {"title": "5.5 Automatic evaluation metrics", "content": "Drawn from the evaluation methodology adopted in earlier information retrieval benchmarks (Long et al., 2022), we evaluate the retrieval performance in both settings using Mean Reciprocal Rank at 10 documents (MRR@10) and hit rate at 10 documents (HIT@10). For the reranking results, we only report the result of the MRR@10 metric since the hit rates are identical to their counterparts."}, {"title": "6 Experiments", "content": ""}, {"title": "6.1 Results of Traditional Setting", "content": "The overall experimental results of the traditional setting are reported in Table 4, from which we can make the following conclusions.\nDense retrievers outperform BM25. In order to generate diverse literal expressions in the process of synthesizing queries, we deploy LLMs. This, however, presents a challenge to sparse retrievers such as BM25. Dense retrievers such as GTE and COROM, which leverage semantic matching, enhance retrieval performance on the MRR@10 scores by notable 26.11% and 18.11%, respectively. It should be noted that the efficiency of the retrievers might be affected by the Doc2Query prompt in our attempt to simulate realistic user queries. Furthermore, we observe that CoROM, despite being trained on the e-commerce dataset, falls behind GTE \u2013 trained on generic datasets \u2013 in performance. We suspect that this is because GTE, due to its pre-training and contrastive fine-tuning stages, is better equipped to capture complex semantics.\nReranking can significantly improve the MRR scores. The results indicate that reranking can greatly boost the MRR@10 scores for all retrievers and retriever combinations by 7.17%, 11.53%, and 12.46% for BM25, GTE, and CoROM, respectively. It also shows similar trends for retriever fusion.\nRetriever fusion decreases both HIT@10 and MRR@10 scores. We have also tested additional fusion strategies assisted by LlamaIndex in our initial experiments. The reciprocal rerank fusion algorithm outperforms others in most instances. This suggests that naive fusion strategies are unable to enhance retrieval performance in this case."}, {"title": "6.2 Results of Conversational Setting", "content": "In the conversational setting of PROCLARE, we employ OpenAI GPT-3.5-turbo-0125 to power the user simulator for all experiments, ensuring a fair comparison for ProductAgent built on various LLM backbones. We benchmark a series of agents with Qwen-max-0107, GPT-3.5-turbo-0125, and GPT-4-turbo-0409 as the backbones in this paper.\nThe results of conversational retrieval are detailed in Table 5, with several distinctions compared to the traditional setting.\nBM25 surpasses dense retrievers. Surprisingly, dense retrievers, which excel in the traditional setting, yield significantly lower performance than BM25 across all LLMs. This can be attributed to the fact that synthesized queries are based on user responses, most of which are essentially provided choices of clarification questions without much modification. Consequently, the challenge faced by BM25 - computing relevance scores through exact term matching - is eliminated. This underlines the importance of generating queries with a specific pattern that is preferred by the utilized retriever.\nReranking boosts the MRR@10 scores, except those of BM25. We hypothesize that the level of noise in queries is so minimal that BM25 does not require the further application of an additional reranker. Other retrievers, however, struggle in capturing subtle semantics, leaving room for the reranker to augment their performance. In the same vein, the retriever fusion technique does not enhance retrieval performance."}, {"title": "7 Analysis", "content": ""}, {"title": "7.1 Effect of Interaction Turns", "content": "Figure 3 presents the trends in retrieval performance as interaction turns increase. The first turn can not retrieve any ground truth product items due to the access to only product categories. We observe that as the number of turns increase, all retrieval scores improve regardless of the types of retrievers employed, signifying the effectiveness of posing clarification questions. Notably, GPT-4 scores lower at the second turn but outperforms all the other LLMs after the third turn, which means GPT-4 tend to ask clarification questions from a global perspective."}, {"title": "7.2 Performance Increase of Clarification", "content": "To further demonstrate the effectiveness of clarification, we replace the initial user queries, i.e., product categories, with the synthesized queries introduced in Section 5.2. The results indicate that retrievers benefit further from clarification, except for GTE. In detail, the performance of BM25 improves dramatically, similar to the results of conversational setting in Section 6.2. However, the performance of GTE falls into a plateau. We speculate the reason is that general-purpose retrievers fail to capture subtle product features that are rare in their training data."}, {"title": "7.3 Ablation Study", "content": "Considering that summarized product statistics constitute a crucial part of the context for generating clarification questions, we perform a comprehensive ablation study focusing on strategies of acquiring statistics. This is conducted with the aim of probing the influence of dynamic statistics on ProductAgent. Specifically, we experiment with three different strategies to derive the statistics: 1) for the \"w/o Statistics\u201d variant, we do not supply statistics to the process of clarification question generation. 2) For the \"Random\u201d variant, we randomly sample product items to summarize the statistics, ignoring dynamic user demands. 3) For the \"BM25\u201d variant, we substitute the SQL retriever with BM25, and 4) for the \"COROM\u201d variant, similarly, we replace the SQL retriever with COROM.\nThe ablation study results, which are shown in Table 6, reveal that BM25 and COROM retrievers are better suited for statistic summarization. Additionally, both removing statistics or ignoring realtime user demands can impair the performance of product search. We analyze further this phenomenon in Section 7.4."}, {"title": "7.4 Failures of ProductAgent", "content": "We identify several failure types for ProductAgent throughout our experiments, all of which are due to the generation of either invalid or trivial SQL queries when using the Text2SQL tool. Specifically, invalid SQL queries mean Text2SQL generates queries that cannot be executed, while trivial SQL queries refer to those that retrieve nothing. The occurrence rates of both kinds of failures are presented in Table 7, which reveals that all the involved LLMs have a tendency to produce trivial SQL queries. Delving deeper into the generated SQL queries, we discovered that trivial ones are typically produced in later turns, whereby LLMs form complex queries by merely amalgamating all known user demands. We addressed these issues by creating statistics derived from previously retrieved items, leading to ProductAgent sometimes failing to propose valuable questions directly related to the user's real-time demands. In the future, the exploration of more sophisticated SQL generation is worth investigating to enhance both ProductAgent's persona and effectiveness.\nAdditionally, we observe ProductAgent tends to generate clarification questions when conversations progress further. So we analyze the similarity of synthesized clarification questions, and the results are reported in Appendix A.3."}, {"title": "8 Conclusion", "content": "This paper formulates the task of product demand clarification and presents the solution called ProductAgent. In order to implement the evaluation in an automatic and quantitative manner, we introduce the PROCLARE benchmark with the aid of a user simulator. Experiments indicate that ProductAgent significantly enhances the retrieval performance with increasing dialogue turns. We also identify certain limitations in the agent."}, {"title": "Limitations", "content": "Absence of human users. In this paper, we employ the user simulation technique to facilitate a more consistent and standardized evaluation process. Utilizing this approach allows for all the experiments conducted within our research to be completely void of any human involvement, contributing to cost-efficiency and saving time. We also acknowledge that potentially unidentified biases may arise from the hallucination effect of LLM-based user simulators within our evaluation pipeline.\nLimitations of datasets. Our dataset was derived from the AliMe KG, encompassing multiple dimensions of products. Nonetheless, it does not provide comprehensive information that could be crucial to customers' purchase decisions, such as product pricing and customer reviews. Future research could delve into more diverse and higher-quality datasets to overcome these deficiencies.\nControllability of clarification questions. We encourage ProductAgent to ask clarification questions following in-context learning augmented with summarized statistics, without explicit and strong constraints. This strategy is not always optimal for all situations. In the future work, we will explore more effective planning strategies that can handle the task from a global perspective, providing a pathway for potential enhancements to create more intelligent conversational agents.\nLack of analysis of prompt sensitivity. The impact of prompt design on the performance of LLMs, often demonstrated as prompt sensitivity, is widely recognized. In our research, however, we did not conduct a systematic examination concerning the sensitivity of prompts used to develop the agent and user simulator. Detailed engineering of prompts could serve as a valuable measure for enhancing the performance of the ProductAgent in future studies."}, {"title": "Ethics Statement", "content": "We are aware that our proposed ProductAgent may be potentially misused for improper purposes, such as privacy data collection and excessive propaganda. However, this vulnerability is not unique to our approach but a common threat to many LLM-based applications. It also highlights the significance of conducting appropriate regulations and enhancing the safety of LLMs in the future."}, {"title": "A Experimental Details", "content": ""}, {"title": "A.1 NER Pre-processing", "content": "To accomplish the retrieval process in a consistent manner for different product categories, we initially apply named entity recognition (NER) to the documents obtained from AliMe KG. For this paper, we chose to use an e-commerce specific NER model (Wang et al., 2021) trained with a Cooperative Learning objective. This coaching strategy encourages two input views to generate similar contextual representations or output label distributions. The NER processing transform all the data from various categories into structured documents with a consistent named entity schema consisting of 54 hierarchical entity labels. We then compress this label space into a new one composed of only 10 entity labels, which allows the agent to retrieve documents more efficiently and accurately from the SQL database. The details of the product item description are provided in Table 8."}, {"title": "A.2 Details about Retrievers", "content": "BM25. As the representative of sparse retrievers, BM25 (Robertson et al., 2009) consistently exhibits impressive performance across diverse information retrieval benchmarks (Long et al., 2022).\nGTE. The General Text Embedding (GTE) model is a general-purpose text embedding model trained with the multi-stage contrastive learning objective (Li et al., 2023c). It has shown exceptional results in the Massive Text Embedding Benchmark (Muennighoff et al., 2023).\nCoROM. The CoROM model (Long et al., 2022) uses BERT-base (Devlin et al., 2019) as its backbone. It is a dual Encoder-based embedding model (Karpukhin et al., 2020) trained on annotated query-passage e-commerce datasets.\nReranker. We also integrate the lightweight bge-reranker-base model (Xiao et al., 2023) to rerank the top-k documents retrieved."}, {"title": "A.3 Similarity of Synthesized Clarification Questions", "content": "We observe ProductAgent tends to generate clarification questions that overlap with those previously presented, which can negatively impact the task effectiveness and overall user satisfaction. Hence, we take a qualitative approach to measuring the similarity among clarification questions proposed in different dialogue turns by using BERTScore (Zhang* et al., 2020). BERTScore computes a similarity score for each token in the candidate sentence by comparing it to each token in the reference sentence. In the experiment, we concatenate the question content and its answer choices into a sentence. By treating all other questions and choices as references, We compute the similarity score for each synthesized clarification questions.\nAs Figure 5 illustrates, we discern an upward trend in the similarity of synthesized clarification questions as dialogues progress. This suggests that although retrieval performance continues to enhance over ten dialogue turns, we may be still able to improve further the performance by boosting the diversity of clarification questions. Additionally, we observe that the agent powered by GPT-3.5 is more inclined to fabricate similar questions than those powered by GPT-4 and Qwen-max, despite the fact that the former achieves higher retrieval scores than the agent powered by Qwen-max when both utilize BM25 for item search."}, {"title": "A.4 Product Aspects of Clarification Questions", "content": "We observe most clarification questions focus on one specific product aspects indicated in Table 8, such as category, brand, and series. Therefore, we analyze the proportion of product facets to which clarification questions are related. The results, illustrated in Figure 6, show that all three LLMs tend to generate clarification questions with even distributions. To detail, three aspects, namely applicable scenarios, style, function, respectively account for more than 10% across LLMs. On the other hand, LLMs are less likely to clarify on series and specification, which may be too specialized for users. This reveals that LLMs naturally have a preference for asking specific types clarification questions in this task, since we do not incorporate any explicit constrains on clarification question generation."}, {"title": "B Prompts and Examples", "content": ""}, {"title": "B.1 Prompts for ProductAgent", "content": "We report all the prompts used in our proposed ProductAgent, including Text2SQL in Table 9, Query Generation in Table 10 and Clarification Question Generation in Table 11."}, {"title": "B.2 Prompt for User Simulator", "content": "We list the prompt for user simulation in Table 12."}, {"title": "B.3 Examples of Conversation", "content": "Here we provide a detailed conversation example regarding \u201cCanvas shoes\" in Table 13. We can notice that ProductAgent first generates some critical clarification questions, such as the color, applicable scenarios and functions, which are helpful to precisely identify the basic user demands. Subsequently, the agent probes for additional specifics like style, decorative attributes and material. On the other hand, the user simulator, which is driven by GPT-3.5, occasionally offers responses that go beyond the provided options."}]}