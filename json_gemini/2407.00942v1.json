{"title": "ProductAgent: Benchmarking Conversational Product Search Agent with Asking Clarification Questions", "authors": ["Jingheng Ye", "Yong Jiang", "Xiaobin Wang", "Yinghui Li", "Yangning Li", "Hai-Tao Zheng", "Pengjun Xie", "Fei Huang"], "abstract": "This paper introduces the task of product demand clarification within an e-commercial scenario, where the user commences the conversation with ambiguous queries and the task-oriented agent is designed to achieve more accurate and tailored product searching by asking clarification questions. To address this task, we propose ProductAgent, a conversational information seeking agent equipped with abilities of strategic clarification question generation and dynamic product retrieval. Specifically, we develop the agent with strategies for product feature summarization, query generation, and product retrieval. Furthermore, we propose the benchmark called PROCLARE to evaluate the agent's performance both automatically and qualitatively with the aid of a LLM-driven user simulator. Experiments show that ProductAgent interacts positively with the user and enhances retrieval performance with increasing dialogue turns, where user demands become gradually more explicit and detailed.", "sections": [{"title": "1 Introduction", "content": "For decades, researchers have been attempting to develop agents with satisfying abilities of understanding human intentions and making decisions (Wooldridge and Jennings, 1995; Agha, 1986). However, these efforts have often been limited to constrained situations (Aliannejadi et al., 2021), where user-system interactions are insufficient (Vats et al., 2024). This limitation becomes more prominent and even critical especially in high-stakes domains or task-oriented systems (Yi et al., 2024), where users often struggle to express their ambiguous and personal information requirements in the first place (Rahmani et al., 2023).\nThe focus of this paper is e-commerce product search, where users usually spend excessive time in locating their desired products since they might not be familiar about which product aspects are suited to their individual use case. We claim that incorporating conversational information seeking could bridge the gaps between traditional product search and offline shopping, where expert salesmen can provide support in identifying the target products that are suitable to users (Papenmeier et al., 2022). Therefore, we propose the task of product demand clarification as illustrated in Table 1, which aims to investigate the role of conversational information seeking (CIS) in e-commerce product search (Zhang et al., 2018). Specifically, in each turn, the agent retrieves relevant product items and asks clarification questions (Lee et al., 2023) to refine user requirements. As the interaction progresses, it is expected that the user demands become clearer, leading to more accurate product searching. This task is challenging since it necessitates the agent to possess abilities of accessing extensive product knowledge (Peng et al., 2024; Li et al., 2023b), understanding subtle user intents, asking strategic clarification question, and retrieving relevant product items. Additionally, products often include different facets, such as brand, color, and style, resulting in different preference slot combinations or shopping schema (Yang et al., 2018).\nTo address this task, We propose ProductAgent, a conversational information seeking agent that can serve as a virtual shopping assistant. Technically, ProductAgent performs autonomous loops that integrate three core modules: product databases, a memory module, and a set of tools (Figure 1). Specifically, 1) the databases store product items in both structured and vectorized forms, enabling ProductAgent to retrieve relevant products and summarize them into statistics reflective of the current user demands. 2) The memory module caches a user's contextual information throughout a conversation session including structured clarification questions and unstructured dialogue records, enabling the agent to ask next questions dynamically. 3) The available tools support various actions such as product retrieval and clarification question generation. By leveraging carefully designed prompts, we enable effective interaction between language and machine learning models (LLMs) and these modules, automating the task.\nIn order to comprehensively evaluate ProductAgent, we introduce the PROduct CLArifying REtrieval (PROCLARE) Benchmark, which focuses on automatic conversational retrieval evaluation (Zhou et al., 2023; Semnani et al., 2023). This evaluation is implemented using the user simulation technique (Zhu et al., 2020; Yoon et al., 2024), in which a user simulator driven by a LLM (Terragni et al., 2023; Sekuli\u0107 et al., 2024) makes it convenient and efficient to generate simulated user responses, thus eliminating the need for human utterances. Specifically, we synthesize a dataset consisting of 2,000 dialogues between ProductAgent and the user simulator, with each dialogue centered around a specific product item of the user's interest. Using this dataset, we build an automatic and reliable retrieval evaluation pipeline for the product demand clarification task. Our experiments in this paper showcase the significant effectiveness of ProductAgent, as the retrieval performance improves with the number of interaction turns. Our contributions are as follows:\n(1) We formulate the task of product demand clarification and propose the solution called ProductAgent, which aims to pinpoint user demands by asking clarification questions in a conversational manner.\n(2) We introduce an straightforward benchmark, called PROCLARE, to ensure a reliable and automatic evaluation pipeline for the product demand clarification task.\n(3) Extensive experiments are conducted to confirm the effectiveness of our approach. Extra extensive analyses are also carried out to reveal promising improvements for the agents."}, {"title": "2 Related Work", "content": "Building agents centered on LLMs has been widely studied recently (Park et al., 2023; Liu et al., 2023; Nakano et al., 2021), owing to their preeminent abilities of reasoning (Yao et al., 2023; Lu et al., 2024), planning (Song et al., 2023; Huang et al., 2023), memorizing, reflexion (Shinn et al., 2024; Huang et al., 2024) or self-correction (Gou et al., 2023), and tool utilization (Schick et al., 2024; Qin et al., 2023). Thanks to techniques like Chain-of-Thought (CoT) (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022) and problem decomposition, LLM-based agents are able to solve challenging problems across a wide rage of real-world scenarios (Li et al., 2023a). However, many advanced agents focus on resolving real-world problems with minimal human intervention. This often results in bias (Ye et al., 2023) and the lack of flexibility and inefficiency, particularly in task-oriented scenarios that emphasize interaction (Wang et al., 2023). The involvement of human feedback (Peng et al., 2023; Zhang et al., 2024a), therefore, becomes progressively critical in guiding and supervising the action trajectory of agents, ensuring their alignment with human requirements and objectives (Kenton et al., 2021; Ngo et al., 2022).\nAiming to provide task-specific services, task-oriented agents often confront vague queries from users, who may lack the required domain knowledge to articulate their queries accurately. The ensuing reduced performance is mitigated in recent studies (Zamani et al., 2020; Kuhn et al., 2022), which advocate to ask clarification questions to pinpoint user demands in a conversational manner. For example, MAS2S (Feng et al., 2023)introduces the idea of asking questions based on user profiles and task knowledge to clarify user requests. Lee et al. (2023) presents the CAMBIGNQ dataset consisting of ambiguous questions. This dataset provides a foundation for a pipeline system enabling the asking of clarification questions to become more efficient. Furthermore, recent studies (Wu, 2024; Erbacher and Soulier, 2023; Zhang et al., 2024b) applies clarification questions onto LLMs, achieving significant improvements across various downstream applications. A concurrent study (Chang and Chen, 2024) develops SalesAgent that concentrates on injecting dialogue strategies into LLMs by fine-tuning, which is orthogonal to our work. Additionally, Vedula et al. (2024) also propose to generate product-related clarification questions through prompting and fine-tuning LLMs.\nHowever, all these works rely heavily on the internal knowledge of LLMs to generate clarification questions, often compromising the quality of the generated clarification questions. On the contrary, our work leverages specific databases and external tools, thus unburdening LLMs from knowledge acquirement and promoting the generation of quality clarification questions."}, {"title": "3 Product Demand Clarification Task", "content": "The objective of the product demand clarification task is to enhance the e-commerce search process through the collaboration between humans and the agent. This task is based on a typical online shopping scenario that users (customers) have decided to purchase a product of a certain category but they have not yet determined detailed aspects of the product of their interests. So they often struggle to clearly articulate their purchasing requirements, especially for product categories accompanied by unfamiliar jargon. To tackle this challenge, the task aims to identify user demands by explicitly posing clarification questions that help clarify any uncertainties towards product searching.\nThe task takes the form of conversation, where an agent proactively interacts with a user. Specifically, given a product category as $U_1$, the conversation $D$ between them is formulated as follows:\n$D = {U_1, A_1, P_1, U_2, A_2, P_2, \u2026\u2026\u2026}$ (1)\nwhere $U_t$ ($t = 1,2,\u2026\u2026$) is the user utterance, $A_t$ is the agent utterance, and $P_t$ is the retrieved product items. The agent utterance primarily consists of several multi-choice clarification questions $A_t$ ={$Q_{t1}, Q_{t2},\u2026\u2026,Q_{tn}$}.\nEach question includes the content of the question itself and a set of answer candidates. It is important to note that, in each turn, the agent outputs both the questions and the retrieved items as timely feedback, facilitating a customer-friendly application interface.\nThe task presents several key challenges: 1) Generating non-trivial clarification questions requires a significant amount of product knowledge. 2) The agent should possess the ability to understand user intentions accurately in order to retrieve the most relevant product items and generate dynamic and valuable clarification questions. 3) The establishment of a quantitative and automated evaluation protocol is necessary to facilitate the development of effective agents for this task. This paper addresses the first two challenges in Section 4, and the third challenge in Section 5."}, {"title": "4 ProductAgent", "content": "This section delineates the proposed solution ProductAgent for the introduced task. As shown in Figure 2, ProductAgent executes conversational loops, each of which can be decomposed by three main steps. Before initializing the loops, the user commences the conversation by inputting a product category as a naive query. We will provide an overview in this section and detailed implementation is discussed in Section 4.2.\nTo generate appropriate clarification questions that can determine the user demands efficiently, it is necessary to collect and summarize relevant and well-elaborated product knowledge based on known user demands of each turn. To achieve this, the agent first synthesizes a query reflective of all the known user demands, and the query is used to retrieve relevant product items from databases. The retrieved items, which probably cover the desirable items of interest, are then summarized as statistics. These statistics encompass various product aspects and serve as a dynamic knowledge base that is constructed in real time based on the ongoing user demands, laying a foundation for the subsequent steps.\nWe dynamically retrieve different relevant product items based on all the known demands. Specifically, ProductAgent first generates a natural language query based on the current demands by harnessing the Query Generation tool. The generated query is then used to retrieve the most suitable items that meet the user demands.\nIn this step, the agent poses several new clarification questions along with their answer candidates which are carefully curated by incorporating the knowledgeable statistics and known user demands. The user is then prompted to efficiently clarify their requirements by either selecting options or providing additional personal information that reflects their interests in the interaction interface. This user feedback is valuable for the task as it helps improve the accuracy of the product search process.\nThe above steps are repeated in each turn of the dialogue, with the aim of making the user demands in subsequent turns more precise compared to previous turns. Consequently, the accuracy of the product search steadily improves over time."}, {"title": "4.2 Key Implementation", "content": "Our proposed ProductAgent consists of three key components which work collectively to solve the product demand clarification task.\nWe store all product items in both SQL and dense vector datasets, allowing for flexible retrieval strategies. As shown in Figure 2, we employ SQL retrieval in the stage 1 since it enables us to efficiently retrieve items through exact matching. On the other hand, we leverage non-SQL retrieval in the stage 2 because non-SQL retrievers can return product items ordered by their relevance to the user demands, which cannot be typically accomplished by SQL retrieval.\nWe define 5 tools for ProductAgent, which are listed in Table 2. To enable SQL retrieval, ProductAgent uses the Text2SQL tool. This tool prompts LLMs with the history demands to generate practical SQL queries. The synthesized SQL query is then used to retrieve product items relevant to the current known demands. Then, the Category Analyzing tool is performed to generate summarized statistics. For non-SQL retrieval, we create the Query Generation tool, which generates concise natural language queries. Additionally, in stage 3, we utilize the Question Generation tool to fabricate reasonable clarification questions based on the summarized statistics and the user demands. To ensure the desired output format, we specify the json description for ProductAgent.\nIt should be kindly noted that for the purpose of autonomous evaluation, the demand clarification task has been slightly simplified and hence does not require a tool router, which is typically used to select the tool for the next action.\nAll the user demands, including clarification questions and corresponding answers, are stored as structured objects in the memory module. When the agent receives responses from users, it extracts the answers of clarification questions posed in the previous turn. The Q&A pairs are then saved as structured demand objects. Additionally, the memory module stores the dialogue records and searching history. When invoking certain tools, we integrate only the required information selected from the memory into the designated slots within the prompt. The memory are continuously updated as the dialogue progresses, ensuring that the intelligent agent can effectively handle dynamically user requirements. The design is crucial as it enables the agent to recall user preferences, thereby providing personalized assistance."}, {"title": "5 The PROCLARE Benchmark", "content": "Most existing information retrieval benchmarks focus on static search processes (Qiu et al., 2022; Long et al., 2022) that do not involve human-AI interactions, which may limit their effectiveness in evaluating practical and interactive performance of search systems. Therefore, with the aim of comprehensively investigating the performance of conversational e-commercial search agents in an automatic manner, we introduce PROCLARE, a benchmarking framework that evaluates retrieval performance in two distinct settings, namely the traditional setting (Section 5.2) and the conversational (Section 5.4) setting. Both ProductAgent and PROCLARE are implemented using the open-sourced framework LlamaIndex."}, {"title": "5.1 Document Set", "content": "We begin by creating a extensive collection of documents (product items), which serves as the knowledge base for our benchmark. This collection consists of 1,000,000 documents across 20 categories. To construct this dataset, we utilize AliMe KG (Li et al., 2020), a domain knowledge graph in the field of e-commerce that supports the top-50 main categories on the Alibaba e-commerce platform. From each category, we randomly sample 50,000 documents, resulting in the dataset used for our benchmark. To ensure efficient data management, we perform Named Entity Recognition (NER) preprocessing (Wang et al., 2021) on all the data. This results in a unified feature definition across various product categories, enabling us to store all the documents in a relational SQL dataset. For detailed statistics and description of the dataset, please refer to Table 3 and Appendix A.1."}, {"title": "5.2 Traditional Information Retrieval", "content": "In the traditional setting of PROCLARE, we evaluate the retrieval performance of the conventional product search task using the methodology employed in previous work (Long et al., 2022; Qiu et al., 2022). It is important to note that this experimental setting, which serves as a counterpart to the conversation setting introduced in Section 3, does not involve the use of the agent. To construct queries used for evaluation, we randomly sample 100 documents per category from the entire document set, resulting in a total of 2000 documents. We then leverage the Doc2Query technique (Gospodinov et al., 2023; Alaofi et al., 2023) to transform these selected documents into synthesized queries by prompting GPT-4. Subsequently, we measure the retrieval performance of different retrieval methods based on the synthesized queries. The prompt for Doc2Query and examples of synthesized queries can be found in Appendix B."}, {"title": "5.3 Retrieval Strategies", "content": "Our experiments employ three widely-adopted retrievers: BM25 (Robertson et al., 2009), General Text Embedding (GTE), and CoROM (Long et al., 2022). Besides, we also incorporate the techniques of reranking and fusion into the retrievers. The detailed descriptions are provided in Appendix A.2."}, {"title": "5.4 Conversational Information Retrieval", "content": "The performance of ProductAgent can be comprehensively explored in the conversational setting, where its effectiveness is manifested directly via the retrieval performance across dialogue turns.\nIt is well-acknowledged that the nature of dialogues, including the high cost of dialogue annotation (Schatzmann et al., 2005; Keizer et al., 2010), poses a challenge for the efficient evaluation of dialogue systems, particularly in an end-to-end manner. To handle this issue, we harness LLMs as intelligent user simulators, thereby accomplishing the automatic evaluation of ProductAgent. Specifically, a LLM-driven user simulator is presented with several clarification questions along with their corresponding candidates in each turn, and then is prompted to answer the questions with access to the ground truth item. It is worth noticing that to avoid potential information leakage, which may create shortcuts for the agent, we restrict the user simulator to only answer the given questions with provided candidates, thus simulating a more realistic and reliable situation.\nFor each queried document, we simulate 10 turns (5 user turns and 5 agent turns) with the user simulator starting the conversation by inputting the desired product category. Examples of simulated dialogues and the prompt for user simulation can be found in Appendix B."}, {"title": "5.5 Automatic evaluation metrics", "content": "Drawn from the evaluation methodology adopted in earlier information retrieval benchmarks (Long et al., 2022), we evaluate the retrieval performance in both settings using Mean Reciprocal Rank at 10 documents (MRR@10) and hit rate at 10 documents (HIT@10). For the reranking results, we only report the result of the MRR@10 metric since the hit rates are identical to their counterparts."}, {"title": "6 Experiments", "content": "The overall experimental results of the traditional setting are reported in Table 4, from which we can make the following conclusions.\nIn order to generate diverse literal expressions in the process of synthesizing queries, we deploy LLMs. This, however, presents a challenge to sparse retrievers such as BM25. Dense retrievers such as GTE and COROM, which leverage semantic matching, enhance retrieval performance on the MRR@10 scores by notable 26.11% and 18.11%, respectively. It should be noted that the efficiency of the retrievers might be affected by the Doc2Query prompt in our attempt to simulate realistic user queries. Furthermore, we observe that CoROM, despite being trained on the e-commerce dataset, falls behind GTE \u2013 trained on generic datasets \u2013 in performance. We suspect that this is because GTE, due to its pre-training and contrastive fine-tuning stages, is better equipped to capture complex semantics.\nThe results indicate that reranking can greatly boost the MRR@10 scores for all retrievers and retriever combinations by 7.17%, 11.53%, and 12.46% for BM25, GTE, and CoROM, respectively. It also shows similar trends for retriever fusion.\nWe have also tested additional fusion strategies assisted by LlamaIndex in our initial experiments. The reciprocal rerank fusion algorithm outperforms others in most instances. This suggests that naive fusion strategies are unable to enhance retrieval performance in this case."}, {"title": "6.2 Results of Conversational Setting", "content": "In the conversational setting of PROCLARE, we employ OpenAI GPT-3.5-turbo-0125 to power the user simulator for all experiments, ensuring a fair comparison for ProductAgent built on various LLM backbones. We benchmark a series of agents with Qwen-max-0107, GPT-3.5-turbo-0125, and GPT-4-turbo-0409 as the backbones in this paper.\nThe results of conversational retrieval are detailed in Table 5, with several distinctions compared to the traditional setting.\nSurprisingly, dense retrievers, which excel in the traditional setting, yield significantly lower performance than BM25 across all LLMs. This can be attributed to the fact that synthesized queries are based on user responses, most of which are essentially provided choices of clarification questions without much modification. Consequently, the challenge faced by BM25 - computing relevance scores through exact term matching - is eliminated. This underlines the importance of generating queries with a specific pattern that is preferred by the utilized retriever.\nWe hypothesize that the level of noise in queries is so minimal that BM25 does not require the further application of an additional reranker. Other retrievers, however, struggle in capturing subtle semantics, leaving room for the reranker to augment their performance. In the same vein, the retriever fusion technique does not enhance retrieval performance."}, {"title": "7 Analysis", "content": "Figure 3 presents the trends in retrieval performance as interaction turns increase. The first turn can not retrieve any ground truth product items due to the access to only product categories. We observe that as the number of turns increase, all retrieval scores improve regardless of the types of retrievers employed, signifying the effectiveness of posing clarification questions. Notably, GPT-4 scores lower at the second turn but outperforms all the other LLMs after the third turn, which means GPT-4 tend to ask clarification questions from a global perspective.\nTo further demonstrate the effectiveness of clarification, we replace the initial user queries, i.e., product categories, with the synthesized queries introduced in Section 5.2. The results illustrated in Figure 4 indicate that retrievers benefit further from clarification, except for GTE. In detail, the performance of BM25 improves dramatically, similar to the results of conversational setting in Section 6.2. However, the performance of GTE falls into a plateau. We speculate the reason is that general-purpose retrievers fail to capture subtle product features that are rare in their training data.\nConsidering that summarized product statistics constitute a crucial part of the context for generating clarification questions, we perform a comprehensive ablation study focusing on strategies of acquiring statistics. This is conducted with the aim of probing the influence of dynamic statistics on ProductAgent. Specifically, we experiment with three different strategies to derive the statistics: 1) for the \"w/o Statistics\u201d variant, we do not supply statistics to the process of clarification question generation. 2) For the \"Random\u201d variant, we randomly sample product items to summarize the statistics, ignoring dynamic user demands. 3) For the \"BM25\u201d variant, we substitute the SQL retriever with BM25, and 4) for the \"COROM\u201d variant, similarly, we replace the SQL retriever with COROM.\nThe ablation study results, which are shown in Table 6, reveal that BM25 and COROM retrievers are better suited for statistic summarization. Additionally, both removing statistics or ignoring real-time user demands can impair the performance of product search. We analyze further this phenomenon in Section 7.4.\nWe identify several failure types for ProductAgent throughout our experiments, all of which are due to the generation of either invalid or trivial SQL queries when using the Text2SQL tool. Specifically, invalid SQL queries mean Text2SQL generates queries that cannot be executed, while trivial SQL queries refer to those that retrieve nothing. The occurrence rates of both kinds of failures are presented in Table 7, which reveals that all the involved LLMs have a tendency to produce trivial SQL queries. Delving deeper into the generated SQL queries, we discovered that trivial ones are typically produced in later turns, whereby LLMs form complex queries by merely amalgamating all known user demands. We addressed these issues by creating statistics derived from previously retrieved items, leading to ProductAgent sometimes failing to propose valuable questions directly related to the user's real-time demands. In the future, the exploration of more sophisticated SQL generation is worth investigating to enhance both ProductAgent's persona and effectiveness.\nAdditionally, we observe ProductAgent tends to generate clarification questions when conversations progress further. So we analyze the similarity of synthesized clarification questions, and the results are reported in Appendix A.3."}, {"title": "8 Conclusion", "content": "This paper formulates the task of product demand clarification and presents the solution called ProductAgent. In order to implement the evaluation in an automatic and quantitative manner, we introduce the PROCLARE benchmark with the aid of a user simulator. Experiments indicate that ProductAgent significantly enhances the retrieval performance with increasing dialogue turns. We also identify certain limitations in the agent."}, {"title": "Limitations", "content": "In this paper, we employ the user simulation technique to facilitate a more consistent and standardized evaluation process. Utilizing this approach allows for all the experiments conducted within our research to be completely void of any human involvement, contributing to cost-efficiency and saving time. We also acknowledge that potentially unidentified biases may arise from the hallucination effect of LLM-based user simulators within our evaluation pipeline.\nOur dataset was derived from the AliMe KG, encompassing multiple dimensions of products. Nonetheless, it does not provide comprehensive information that could be crucial to customers' purchase decisions, such as product pricing and customer reviews. Future research could delve into more diverse and higher-quality datasets to overcome these deficiencies.\nWe encourage ProductAgent to ask clarification questions following in-context learning augmented with summarized statistics, without explicit and strong constraints. This strategy is not always optimal for all situations. In the future work, we will explore more effective planning strategies that can handle the task from a global perspective, providing a pathway for potential enhancements to create more intelligent conversational agents.\nThe impact of prompt design on the performance of LLMs, often demonstrated as prompt sensitivity, is widely recognized. In our research, however, we did not conduct a systematic examination concerning the sensitivity of prompts used to develop the agent and user simulator. Detailed engineering of prompts could serve as a valuable measure for enhancing the performance of the ProductAgent in future studies."}, {"title": "Ethics Statement", "content": "We are aware that our proposed ProductAgent may be potentially misused for improper purposes, such as privacy data collection and excessive propaganda. However, this vulnerability is not unique to our approach but a common threat to many LLM-based applications. It also highlights the significance of conducting appropriate regulations and enhancing the safety of LLMs in the future."}, {"title": "A Experimental Details", "content": "To accomplish the retrieval process in a consistent manner for different product categories, we initially apply named entity recognition (NER) to the documents obtained from AliMe KG. For this paper, we chose to use an e-commerce specific NER model (Wang et al., 2021) trained with a Cooperative Learning objective. This coaching strategy encourages two input views to generate similar contextual representations or output label distributions. The NER processing transform all the data from various categories into structured documents with a consistent named entity schema consisting of 54 hierarchical entity labels. We then compress this label space into a new one composed of only 10 entity labels, which allows the agent to retrieve documents more efficiently and accurately from the SQL database. The details of the product item description are provided in Table 8.\nAs the representative of sparse retrievers, BM25 (Robertson et al., 2009) consistently exhibits impressive performance across diverse information retrieval benchmarks (Long et al., 2022).\nThe General Text Embedding (GTE) model is a general-purpose text embedding model trained with the multi-stage contrastive learning objective (Li et al., 2023c). It has shown exceptional results in the Massive Text Embedding Benchmark (Muennighoff et al., 2023).\nThe CoROM model (Long et al., 2022) uses BERT-base (Devlin et al., 2019) as its backbone. It is a dual Encoder-based embedding model (Karpukhin et al., 2020) trained on annotated query-passage e-commerce datasets.\nWe also integrate the lightweight bge-reranker-base model (Xiao et al., 2023) to rerank the top-k documents retrieved."}]}