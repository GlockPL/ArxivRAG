{"title": "An Evaluation of Continual Learning for Advanced Node Semiconductor Defect Inspection", "authors": ["Amit Prasad", "Bappaditya Dey", "Victor Blanco", "Sandip Halder"], "abstract": "Deep learning-based semiconductor defect inspection has gained traction in recent years, offering a powerful and versatile approach that provides high accuracy, adaptability, and efficiency in detecting and classifying nano-scale defects. However, semiconductor manufacturing processes are continually evolving, leading to the emergence of new types of defects over time. This presents a significant challenge for conventional supervised defect detectors, as they may suffer from catastrophic forgetting when trained on new defect datasets, potentially compromising performance on previously learned tasks. An alternative approach involves the constant storage of previously trained datasets alongside pre-trained model versions, which can be utilized for (re-)training from scratch or fine-tuning whenever encountering a new defect dataset. However, adhering to such a storage template is impractical in terms of size, particularly when considering High-Volume Manufacturing (HVM). Additionally, semiconductor defect datasets, especially those encompassing stochastic defects, are often limited and expensive to obtain, thus lacking sufficient representation of the entire universal set of defectivity. This work introduces a task-agnostic, meta-learning approach aimed at addressing this challenge, which enables the incremental addition of new defect classes and scales to create a more robust and generalized model for semiconductor defect inspection. We have benchmarked our approach using real resist-wafer SEM (Scanning Electron Microscopy) datasets for two process steps, ADI and AEI, demonstrating its superior performance compared to conventional supervised training methods.", "sections": [{"title": "1 Related Work", "content": "In the semiconductor process (mainly, Litho-Etch) domain, numerous approaches have been suggested for defect classification and localisation [2], [3], [1]. To the best of the authors' knowledge, the concept of incremental learning [5] for multi-class, multi-instance defect detection on SEM images has previously not been explored."}, {"title": "2 Methodology", "content": "Original (resist) wafer SEM (Scanning Electron Microscopy) images were obtained during ADI (After Development Inspection) and AEI (After Etch Inspection) stages. Figure 1 illustrates exemplary defect types in both process steps. The instance distribution per defect class is captured in Table 1."}, {"title": "2.1 Dataset", "content": ""}, {"title": "2.2 Notations and Preliminaries", "content": "The following notations have been used in this work.\nDefinition 1. Task ($T_p$): This is defined as supervised training of a defect detection framework for p classes (0 to $p-1$) in the dataset of the form $(x_i, y_i)_1^m$ (m instances with defect feature $x_i$and corresponding label $y_i$). This is denoted by $T_p$.\nDefinition 2. Finetuned task ($F_q$): This is defined as supervised training of a defect detection framework for next q classes (p to $q - 1$) in the dataset of the form $(x_i, y_i)_1^m$, which has previously been trained on the initial p classes (0 to $p-1$). However, it's important to note that identifying these initial p classes is not guaranteed. This is denoted by $F_q$.\nDefinition 3. Incremental task ($T_q^p$): This is defined as incremental supervised training of a defect detection framework for next q classes (p to $q \u2013 1$) in the dataset of the form $(x_i, y_i)_1^m$, which has previously been trained on the initial p classes (0 to p \u2212 1), enabling it to identify all (p + q) classes. This is denoted by $T_q^p$."}, {"title": "2.3 Structure of study", "content": "In this work we present the following case studies.\n1. Case study 1 (see Section 3) examines effectiveness of the framework in incrementally learning new defect classes and minimizing forgetting of previously trained defect classes on the ADI dataset.\n2. Case study 2 (see Section 4) assesses framework for incrementally learning new defect classes in AEI images and minimizing forgetting of previously trained defect classes across the entire ADI dataset.\n3. Case study 3 (see Section 5) compares three training strategies: (i) conventional supervised training strategy with all defect classes at once, (ii) conventional supervised training with first p defect classes and then fine-tune on new q defect classes, (iii) proposed incremental supervised training strategy with first p defect classes and then fine-tune on new q defect classes.\nWe use the Faster-RCNN [6] model for all studies. Moreover, for incremental tasks, the approach utilized is presented in [4] which also uses FRCNN."}, {"title": "3 Case study 1", "content": "The model starts training with the task $T_2$ (initially trained for 2 defect classes, microbridge and gap), followed by two consecutive incremental training tasks: $T_2^2$ (adding 2 more defect classes, bridge and line-collapse), and finally $T_1^4$ (adding the last defect class as probable gap), using the ADI dataset. For an evaluation of performance, average precision (AP) per defect class vs iterations is plotted, marking checkpoints where new defect classes were introduced and where continual learning takes place. The results are compared to the conventional fine-tuning approach, where the model has been trained on tasks $F_2$ and $F_1^4$, while keeping all experimental conditions constant. In Figure 2 a), it is evident how effective incremental learning is for progressively learning defect classes and minimizing catastrophic forgetting. Conversely, in Figure 2 b), it is apparent how swiftly catastrophic forgetting occurs in the case of fine-tuning."}, {"title": "4 Case study 2", "content": "Defect classes from the AEI dataset are incrementally added following training on the ADI dataset. The model, following task $T_4^1$, undergoes training on tasks $T_1^4$ and $T_2^5$. Similarly, following task $F_1^4$, the model undergoes fine-tuning for tasks $F_1^4$ and $F_2^5$. The Figure 3 illustrates the comparison between proposed incremental learning and conventional fine-tuning (using AP vs iteration plot)."}, {"title": "5 Case study 3", "content": "Inference results are shown in Figure 4 (with corresponding labels, bounding boxs and confidence scores) are from 3 training strategies, first is the model"}, {"title": "6 Conclusion", "content": "In this study, we demonstrated the effectiveness of a continual learning strategy in progressively learning the classification and localization of semiconductor defect classes in aggressive pitches, while mitigating catastrophic forgetting."}]}