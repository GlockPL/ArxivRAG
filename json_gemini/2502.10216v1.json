{"title": "Forget the Data and Fine-tuning!\nJust Fold the Network to Compress", "authors": ["Dong Wang", "Haris \u0160iki\u0107", "Lothar Thiele", "Olga Saukh"], "abstract": "We introduce model folding, a novel data-free model compression technique that merges structurally\nsimilar neurons across layers, significantly reducing the model size without the need for fine-tuning\nor access to training data. Unlike existing methods, model folding preserves data statistics during\ncompression by leveraging k-means clustering, and using novel data-free techniques to prevent variance\ncollapse or explosion. Our theoretical framework and experiments across standard benchmarks, including\nResNet18 and LLaMA-7B, demonstrate that model folding achieves comparable performance to data-\ndriven compression techniques and outperforms recently proposed data-free methods, especially at high\nsparsity levels. This approach is particularly effective for compressing large-scale models, making it\nsuitable for deployment in resource-constrained environments. Our code is online.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks (DNNs) have emerged as a fundamental technology, driving progress across a multitude\nof applications from natural language processing to computer vision. However, the deployment of these\nmodels in real-world settings is often constrained by the computational and memory resources available,\nparticularly on edge devices like smartphones and embedded systems (Chen et al., 2020; Kumar et al., 2017;\nWan et al., 2020). This limitation poses a significant challenge, as the growing complexity and size of SOTA\nmodels demand increasingly substantial resources (Bommasani et al., 2021; Chang et al., 2024; Rombach\net al., 2022).\nConventional model compression techniques, such as pruning (Han et al., 2015; Hassibi et al., 1993; LeCun\net al., 1989; Li et al., 2016b) and quantization (Gupta et al., 2015; Li et al., 2016a; Zhou et al., 2017), have\nbeen developed to mitigate this issue by reducing the model size and computational requirements. These\nmethods usually remove redundant or less critical parameters from the model, thereby reducing the overall\nsize and computational load. For example, pruning eliminates weights that contribute minimally to the\nmodel's output (Entezari and Saukh, 2020; Han et al., 2015; Li et al., 2016b). Quantization reduces the\nprecision of the weights and activations (Gupta et al., 2015), which decreases memory usage and speeds up\ninference (Zhou et al., 2017). Despite their effectiveness, these approaches often introduce a degradation in\nmodel performance, necessitating a phase of fine-tuning to maintain the internal data statistics within the\nmodel (Jordan et al., 2022) and restore the original accuracy levels (Frankle and Carbin, 2018; Frantar and"}, {"title": "2 Preliminaries", "content": "Our work is inspired by recent advances in two key areas: neuron alignment algorithms for fusing model pairs\nin weight space, and data-driven methods for recovering from variance collapse in fused models. Below, we\nsummarize the relevant results from the literature.\nNeuron alignment algorithms. Model merging involves combining the parameters of multiple trained\nmodels into a single model, with a key challenge being the alignment of neurons across these models,\nparticularly when they are trained on different datasets or tasks. Neuron alignment methods can be classified\nbased on their dependency on the input data. Methods like the Straight Through Estimator (STE) (Ainsworth\net al., 2023), Optimal Transport (OT) (Singh and Jaggi, 2020) and correlation-based activation matching (Li\net al., 2015) require data for effective merging. In contrast, weight matching (Ainsworth et al., 2023; Yamada\net al., 2023) is a data-free method, making it efficient in scenarios when training data is not available. In\nweight matching, neurons are aligned by minimizing the L2 distance between the weight vectors of neurons\nacross models. Given two models with weight matrices $W_A$ and $W_B$, the goal is to find a permutation $P$ of\nthe weights in $W_B$ that minimizes the distance:\n$\\min_P ||W_A - PW_B||_3,$\nwhere $PW_B$ denotes the weight matrix $W_B$ after applying the permutation $P$ to align it with $W_A$. Once\nthe optimal permutation is found, the models are merged by averaging the aligned weights:\n$W_{\\text{merged}} = \\frac{1}{2}(W_A + P^*W_B),$\nwhere $P^*$ is the permutation that minimizes the $L_2$ distance. Weight matching solves an instance of the linear\nsum assignment problem (LSAP), usually solved by Hungarian algorithm (Kuhn, 1955) as done in (Ainsworth\net al., 2023; Jordan et al., 2022), to layer-wise align weight vectors. Unlike merging different models, aligning\nneurons within a single model requires an acyclic matching graph, a challenge not addressed by LSAP, which"}, {"title": "3 Model Folding", "content": "In this section, we introduce model folding, a novel compression technique that reduces the computational\ncomplexity and size of neural networks by merging similar neurons in each layer without requiring training\ndata. As illustrated in Fig. 1 (left), model folding processes the network layer by layer, involving filter\nclustering, merging, and correcting data statistics. Below, we present a theoretical analysis of our approach,\nsupported by empirical results on ResNet18 using CIFAR10."}, {"title": "3.1 Channel clustering", "content": "Channel similarity. Neural networks trained with stochastic gradient descent (SGD) tend to have many\ncorrelated hidden units, as illustrated in Fig. 2. Model folding exploits this observation, which is related\nto the implicit bias of SGD. As discussed in (Gunasekar et al., 2017), SGD exhibits a minimum norm bias,\nwhich can be viewed as a form of regularization when no explicit regularization is used. In contrast to L1"}, {"title": "3.2 Maintaining data statistics in a compressed model", "content": "Variance collapse and variance overshooting. We use the conceptual framework in (Jordan et al., 2022)\nto analyze the performance of model compression methods. We use the following definition.\nDefinition 3.1 (Variance ratio). Consider a neural network $f(x,\\Theta)$ with layer activations ${x_l}$ and its\ncompressed version $\\tilde{f}(x, \\Theta)$ with activations ${\\tilde{x}}_l$.\nThe variance ratio of the l-the layer is:\n$\\mu[\\frac{Var(\\tilde{x}_l)}{Var(x_l)}] = \\frac{1}{K} \\sum_{k=1}^K \\frac{Var(\\tilde{x}_{l,k})}{Var(x_{l,k})}.$\nWe observe not only variance collapse but also variance overshooting phenomena. Specifically, when data\nstatistics are not accurately corrected after channel merging, as in IFM, variance overshooting can occur,"}, {"title": "3.3 Relationship Between Weight Matching and Model Folding", "content": "Weight Matching (Ainsworth et al., 2023) fuses two models into one, whereas Model Folding compresses the\nweight tensors/matrices of a single network. While inspired by Weight Matching, Model Folding addresses a\ndistinct use case, leading to different optimization problems (K-Means vs. LAP). Notably, the Linear Sum\nAssignment Problem (LAP) can be framed as a constrained K-Means variant, where each cluster contains"}, {"title": "2 Preliminaries", "content": "Our work is inspired by recent advances in two key areas: neuron alignment algorithms for fusing model pairs\nin weight space, and data-driven methods for recovering from variance collapse in fused models. Below, we\nsummarize the relevant results from the literature.\nNeuron alignment algorithms. Model merging involves combining the parameters of multiple trained\nmodels into a single model, with a key challenge being the alignment of neurons across these models,\nparticularly when they are trained on different datasets or tasks. Neuron alignment methods can be classified\nbased on their dependency on the input data. Methods like the Straight Through Estimator (STE) (Ainsworth\net al., 2023), Optimal Transport (OT) (Singh and Jaggi, 2020) and correlation-based activation matching (Li\net al., 2015) require data for effective merging. In contrast, weight matching (Ainsworth et al., 2023; Yamada\net al., 2023) is a data-free method, making it efficient in scenarios when training data is not available. In\nweight matching, neurons are aligned by minimizing the L2 distance between the weight vectors of neurons\nacross models. Given two models with weight matrices $W_A$ and $W_B$, the goal is to find a permutation $P$ of\nthe weights in $W_B$ that minimizes the distance:\n$\\min_P ||W_A - PW_B||_3,$\nwhere $PW_B$ denotes the weight matrix $W_B$ after applying the permutation $P$ to align it with $W_A$. Once\nthe optimal permutation is found, the models are merged by averaging the aligned weights:\n$W_{\\text{merged}} = \\frac{1}{2}(W_A + P^*W_B),$\nwhere $P^*$ is the permutation that minimizes the $L_2$ distance. Weight matching solves an instance of the linear\nsum assignment problem (LSAP), usually solved by Hungarian algorithm (Kuhn, 1955) as done in (Ainsworth\net al., 2023; Jordan et al., 2022), to layer-wise align weight vectors. Unlike merging different models, aligning\nneurons within a single model requires an acyclic matching graph, a challenge not addressed by LSAP, which"}, {"title": "B Further theoretical results to support model folding", "content": "Lemma B.1. Let $x \\in \\mathbb{R}^k$ and let $U \\in \\{0,1\\}^{n\\times k}$ be a binary clustering matrix with $\\sum_j u_{ij} = 1$. Then with\nany element-wise nonlinear function $\\sigma(\\cdot)$ we have\n$\\sigma(Ux) = U\\sigma(x)$\nProof of Lemma B.1. Define $y = Ux$, $z = \\sigma(Ux)$ and $v = \\sigma(x)$, $w = U\\sigma(x)$. Note that in any row of U just\none element satisfies $u_{ij} = 1$. We define such an element by a function $p$ with $u_{ij} = 1 \\Leftrightarrow p(i) = j$.\nTherefore, $y_i = x_{p(i)}$ and $z_i = \\sigma(y_i) = \\sigma(x_{p(i)})$ for all $1 \\leq i \\leq n$. Moreover, $v_i = \\sigma(x_i)$ and\n$w_i = v_{p(i)} = \\sigma(x_{p(i)})$. Therefore, $z_i = w_i$ and $z = w$.\nLemma B.2. Let $x \\in \\mathbb{R}^k$, let $U \\in \\{0,1\\}^{n\\times k}$ be a binary clustering matrix with $\\sum_j u_{ij} = 1$, let $\\sigma(\\cdot)$ be an\nelement-wise nonlinear function, and define $C = U(U^TU)^{-1}U^T$. Then\n$\\sigma(Cx) = C \\sigma(Cx)$\nProof of Lemma B.2. We can write\n$\\sigma(Cx) = \\sigma(U(U^TU)^{-1}U^Tx)$\n$= U\\sigma((U^TU)^{-1}U^Tx) \\quad (Lemma B.1)$\n$= U(U^TU)^{-1}(U^TU)\\sigma((U^TU)^{-1}U^Tx)$\n$= U(U^TU)^{-1}U^T\\sigma(U(U^TU)^{-1}U^Tx) \\quad (Lemma B.1)$\n$= C^T \\sigma(Cx)$.\nLemma B.3. Let $U^T$ be a clustering matrix and let $D$ be a diagonal matrix, then the following is true\n$(U^TU)^{-1}U^TDU = Diag((U^TU)^{-1}U^T diag(D))$\nProof of Theorem B.3. The clustering matrix $U^T$ can be expressed as:\n$U^T = \\begin{bmatrix} | & | & & | \\\\ u_1^T & u_2^T & \\cdots & u_k^T \\\\ | & | & & |  \\end{bmatrix} = \\begin{bmatrix} u_{11} & u_{12} & \\cdots & u_{1n} \\\\ u_{21} & u_{22} & \\cdots & u_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ u_{k1} & u_{k2} & \\cdots & u_{kn}  \\end{bmatrix},$\nwhere $u_i^T$ represents the rows of the clustering matrix. Each row corresponds to cluster i, and the entries\n$u_{ij}$ satisfy the binary clustering property: $u_{ij} = 1$ if the j-th data point belongs to cluster i, and $u_{ij} = 0$\notherwise.\nThe product DU is given by:\n$DU = \\begin{bmatrix} d_1 & 0 & & 0 \\\\ 0 & d_2 & & 0 \\\\ & & \\ddots & \\\\ 0 & 0 & & d_n \\end{bmatrix} \\begin{bmatrix} u_{11} & u_{12} & \\cdots & u_{1k} \\\\ u_{21} & u_{22} & \\cdots & u_{2k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ u_{n1} & u_{n2} & \\cdots & u_{nk} \\end{bmatrix}$\nThis simplifies to:\n$DU = \\begin{bmatrix} d_1u_{11} & d_1u_{12} & & d_1u_{1k} \\\\ d_2u_{21} & d_2u_{22} & & d_2u_{2k} \\\\ \\vdots & \\vdots & \\vdots \\\\ d_nu_{n1} & d_nu_{n2} & & d_nu_{nk} \\end{bmatrix}$\nUsing the clustering property of U, it follows that:\n$u_{ij}u_{i' j} = \\begin{cases} 1, & \\text{if } i = i', \\\\ 0, & \\text{otherwise.} \\end{cases}$\nFrom this, the product $U^TDU$ simplifies to:\n$U^TDU = Diag(Udiag(D))$.\nThis result holds because only the diagonal entries remain due to the clustering matrix's orthogonality and\nbinary properties."}, {"title": "F Handling Batch Normalization Layers", "content": "Batch Normalization layers, when combined with linear layers, introduce additional scaling and normalization\noperations. One special case is a layer consisting of a linear block followed by a Batch Normalization block,\nformally defined as:\n$z_{l+1} = W_{l+1}\\sigma(\\Sigma_s\\Sigma_n W_l x_{l-1}),$\nwhere:\n* $W_l$: weight matrix of the linear block,\n* $\\Sigma_s$: Batch Normalization scaling matrix,\n* $\\Sigma_n$: Batch Normalization normalization matrix,\n* $W_{l+1}$: weight matrix of the subsequent layer,\n* $\\sigma(\\cdot)$: activation function applied element-wise.\nA design choice in handling such layers is to decompose $\\Sigma_s$, $\\Sigma_n$, and $W_l$ separately while preserving the\noriginal structure of the layer. This ensures that the scaling, normalization, and linear blocks are treated as\ndistinct functional units. The decomposed approximation for the layer can then be expressed as:\n$\\tilde{z}_{l+1} \\approx \\tilde{W}_{l+1} = W_{l+1}C \\sigma(C_s\\Sigma_s C_n\\Sigma_n C_l W_l x_{l-1}),$\nwhere the projection matrices $C_s$, $C_n$, and $C_l$ are defined as:\n$C_s = U_s(U_s^TU_s)^{-1}U_s^T = U_sM_s,$\n$C_n = U_n(U_n^TU_n)^{-1}U_n^T = U_nM_n,$\n$C_l = U_l(U_l^TU_l)^{-1}U_l^T = U_lM_l$.\nHere, $U_s$, $U_n$, and $U_l$ are clustering matrices, and $M_s$, $M_n$, and $M_l$ are normalization terms.\nClustering Assumptions. To simplify the decomposition and ensure alignment across the layer components,\nwe impose the following consistency constraint:\n$U_s = U_n = U_l$.\nThis assumption ensures that the same clustering structure is applied to the scaling, normalization, and linear\nblocks, leading to a unified decomposition. Under this assumption, the approximation becomes:\n$\\tilde{z}_{l+1} = W_{l+1}C\\sigma(U_l M_l W_l x_{l-1}),$"}]}