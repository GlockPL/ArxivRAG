{"title": "Forget the Data and Fine-tuning!\nJust Fold the Network to Compress", "authors": ["Dong Wang", "Haris \u0160iki\u0107", "Lothar Thiele", "Olga Saukh"], "abstract": "We introduce model folding, a novel data-free model compression technique that merges structurally\nsimilar neurons across layers, significantly reducing the model size without the need for fine-tuning\nor access to training data. Unlike existing methods, model folding preserves data statistics during\ncompression by leveraging k-means clustering, and using novel data-free techniques to prevent variance\ncollapse or explosion. Our theoretical framework and experiments across standard benchmarks, including\nResNet18 and LLaMA-7B, demonstrate that model folding achieves comparable performance to data-\ndriven compression techniques and outperforms recently proposed data-free methods, especially at high\nsparsity levels. This approach is particularly effective for compressing large-scale models, making it\nsuitable for deployment in resource-constrained environments. Our code is online.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks (DNNs) have emerged as a fundamental technology, driving progress across a multitude\nof applications from natural language processing to computer vision. However, the deployment of these\nmodels in real-world settings is often constrained by the computational and memory resources available,\nparticularly on edge devices like smartphones and embedded systems (Chen et al., 2020; Kumar et al., 2017;\nWan et al., 2020). This limitation poses a significant challenge, as the growing complexity and size of SOTA\nmodels demand increasingly substantial resources (Bommasani et al., 2021; Chang et al., 2024; Rombach\net al., 2022).\nConventional model compression techniques, such as pruning (Han et al., 2015; Hassibi et al., 1993; LeCun\net al., 1989; Li et al., 2016b) and quantization (Gupta et al., 2015; Li et al., 2016a; Zhou et al., 2017), have\nbeen developed to mitigate this issue by reducing the model size and computational requirements. These\nmethods usually remove redundant or less critical parameters from the model, thereby reducing the overall\nsize and computational load. For example, pruning eliminates weights that contribute minimally to the\nmodel's output (Entezari and Saukh, 2020; Han et al., 2015; Li et al., 2016b). Quantization reduces the\nprecision of the weights and activations (Gupta et al., 2015), which decreases memory usage and speeds up\ninference (Zhou et al., 2017). Despite their effectiveness, these approaches often introduce a degradation in\nmodel performance, necessitating a phase of fine-tuning to maintain the internal data statistics within the\nmodel (Jordan et al., 2022) and restore the original accuracy levels (Frankle and Carbin, 2018; Frantar and"}]}