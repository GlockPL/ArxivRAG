{"title": "Why should we ever automate moral decision making?", "authors": ["Vincent Conitzer"], "abstract": "While people generally trust AI to make decisions in various aspects\nof their lives, concerns arise when Al is involved in decisions with signif-\nicant moral implications. The absence of a precise mathematical frame-\nwork for moral reasoning intensifies these concerns, as ethics often defies\nsimplistic mathematical models. Unlike fields such as logical reasoning,\nreasoning under uncertainty, and strategic decision-making, which have\nwell-defined mathematical frameworks, moral reasoning lacks a broadly\naccepted framework. This absence raises questions about the confidence\nwe can place in AI's moral decision-making capabilities.\nThe environments in which AI systems are typically trained today\nseem insufficiently rich for such a system to learn ethics from scratch, and\neven if we had an appropriate environment, it is unclear how we might\nbring about such learning. An alternative approach involves AI learning\nfrom human moral decisions. This learning process can involve aggregat-\ning curated human judgments or demonstrations in specific domains, or\nleveraging a foundation model fed with a wide range of data. Still, con-\ncerns persist, given the imperfections in human moral decision making.\nGiven this, why should we ever automate moral decision making - is it\nnot better to leave all moral decision making to humans? This paper lays\nout a number of reasons why we should expect AI systems to engage in\ndecisions with a moral component, with brief discussions of the associated\nrisks.", "sections": [{"title": "1 Introduction", "content": "People are generally quite comfortable with AI making all kinds of decisions in\ntheir lives. We are happy for AI to choose a route for us to follow when driving,\nto choose which articles we read or which videos we see, or even to propose\npeople for us to date. But we often feel less comfortable about the use of AI in\nsettings where there is a significant moral component to the decision."}, {"title": "2 Reasons for automated moral decision making", "content": "Given the above, one may well wonder why we should be interested in automated\nmoral decision making at all; can we not simply leave all moral decision making\nto human beings? In what remains, we cover some reasons why, in spite of the\nlack of a formal framework, we may yet want to have AI systems do automated\nmoral reasoning, rather than simply leaving the relevant decisions to a human\nbeing. As we will discuss, some of these reasons overlap, and some uses of AI can\ncall on multiple of these reasons for support. So, these reasons are not intended\nto be disjoint from each other; and presumably this list does not exhaust all\nsuch reasons. For example applications given below, I will not try to argue that"}, {"title": "2.1 Speed", "content": "In some cases, decisions need to be made faster than humans can make them,\nor faster than humans can make them well. One example is a self-driving car\nthat suddenly faces an unexpected situation. For instance, an accident occurs\nimmediately in front of it, and it needs to make a decision to either brake\nhard at some risk to the occupants, or attempt to swerve around the accident,\nwhich runs the risk of colliding with an adjacent car (possibly depending on the\nreaction of the adjacent car). This a moral dilemma. But passing control back\nto the human occupant, who does not have situational awareness, is likely to do\nlittle good. Cybersecurity and cyberwarfare provide additional examples where\nspeed can be of the essence. One might argue that this is a reason to avoid these\ntypes of scenarios altogether - maybe we should not have self-driving cars at\nall, and maybe we should work harder to ensure our systems are not vulnerable\nto cyberattacks or otherwise prevent such attacks from happening, etc. This is\nnot the place to get into these discussions; all we aim to argue here is that these\nare settings where simply having a human take over the moral reasoning at the\nmoment that it is needed is not likely to address the problem.\nRisks. When AI is adopted for the sake of being able to act faster, there\ndoes not seem to be any inherent limit on how bad the consequences can be,\nbecause there will be no chance for a human to review the decision."}, {"title": "2.2 Scale", "content": "In some settings, many decisions need to be made, and it simply is not reason-\nable to have a human make each of these decisions. For example, consider the\ndecision of whether to show someone a potentially sensitive ad, where the ideal\ndecision requires taking into account detailed features of the user. This, too,\ncan be a moral dilemma. The impact of any one such decision is likely to be\nsmall, but as the decisions are made across millions of users, their impact adds\nup.\nRisks. In this context, it seems sensible to periodically review a sample of\nthe decisions; and the more important each individual decision, the more often"}, {"title": "2.3 Complex optimization", "content": "In some cases, moral reasoning must be intertwined with complex optimization.\nA good example of this is the problem faced in a kidney exchange [13]. In\nkidney exchanges, AI is already used to determine which potential kidney donors\nto match with patients [1]. Even the problem of maximizing the number of\ntransplants is computationally hard, but it is not clear that that is the correct\nobjective to pursue; in making these decisions, maybe we should also take into\naccount other aspects, such as the patients' age, other aspects of their health,\nperhaps even whether they have dependents or a criminal record, etc. Trading\noff these varying aspects between feasible solutions poses a moral dilemma.\nIn this context, the moral reasoning problem does not seem cleanly separable\nfrom the optimization problem in such a way that humans can do the moral\nreasoning when it is needed. In principle, the AI system may propose a selection\nof feasible solutions, from which (say) a committee of humans then picks one.\nBut without the AI system doing any moral reasoning of its own, it is not clear\nhow to guarantee that its selection will include the best solution, or even a\ngood one; except, perhaps, if the selection includes all reasonable solutions\nbut there will generally be exponentially many of these, and searching through\nexponentially many options is what we brought the AI in to do for us in the\nfirst place. (In [6], we propose a method for having the AI learn an objective\nfunction from human feedback, but there remain a variety of questions about\nhow best to elicit such information from human subjects [4, 11].)\nRisks. Here, too, the risks of automated moral decision making could be\nlimited by the fact that decisions can be reviewed. One could, for example,\ncompare the AI-generated decision to a human-generated one in each instance,\nto make sure the AI-generated one is in fact better."}, {"title": "2.4 Better world models", "content": "In some domains, we may not have good intuitions about the actual conse-\nquences of decisions. Consider, for example, an AI system tasked with the\ndesign of new drugs to treat a disease. It may have better \"intuitions\" about\nthe effects of various potential drugs than we do; and, for the system, choos-\ning which new drug to propose to us requires trading off that drug's expected\nefficacy with its expected side effects. Presumably, we will first still want to\nconduct a randomized controlled trial on the drug, but even just deciding to\nstart such a trial is a decision with significant consequences. We may want to\nleave the decision in our own hands, and ask the AI system to tell us the reasons\nfor its choice of proposed drug. But then again, it may not be able to effectively\nexplain these reasons to us, for example because its model of how these drugs\nwork does not translate well to natural language. Even if we as humans were\nable to evaluate the pros and cons of any proposed drug, there is still the issue"}, {"title": "2.5 Transparency of process", "content": "Sometimes, it is better to have a clear policy for how to make decisions than\nit is to make decisions on a case-by-case basis. In particular, human decision\nmaking is generally not transparent; human decision makers can be sensitive to\nbias, and can even be corrupt. In contrast, if a clear policy is set in advance,\nthis helps to evaluate bias and prevent corruption on individual decisions. It\ncan also help to prevent other ways in which ad-hoc decision making might be\n\"gamed\" by interested parties.\nCommitting to use a specific AI system to make decisions has at least some of\nthe benefits of setting a clear policy. While AI systems vary in their transparency\nand interpretability, they can generally at least be audited by testing them on\na variety of inputs, and they cannot be bribed.\nOf course, there remain major problems with AI systems displaying unfair\nbiases, for example due to the data they were trained on; at the very least, much\nmore work is needed to address these problems effectively in such systems, and\nperhaps in the end we will conclude that there should always be a human in the\nloop in certain applications. The argument here is merely that in principle, there\ncould be an advantage to the use of AI in terms of transparency of process; this\nis not to say that no further work is needed to attain (most) of these benefits,\nand it is not to say that these benefits outweigh other concerns."}, {"title": "2.6 Humans are poor (moral) decision makers under certain circumstances", "content": "This reason overlaps with the \"speed\" reason above, but there are other circum-\nstances under which humans can be bad decision makers \"in the moment.\" To\nillustrate, imagine an AI application that, when someone is trying to send an\nemail after a late night partying and drinking, has the ability to analyze that\nemail and, if the email does not seem wise to send, to prevent it from being\nsent at that point in time. To be effective, the AI may have to engage in moral\nreasoning. For example, suppose the message the user is trying to send out late\nat night is: \"I don't trust that guy you were just talking with, I recommend\nthat you ditch him.\" The AI may have reason to believe that the sender would\nprobably regret sending this message in the morning, but, in making the deci-\nsion whether to temporarily block the message, has to trade this off against the\nwelfare of recipient in case the sender is in fact onto something.\nThere are many other conditions under which we humans are poor (moral)\ndecision makers. Besides cases in which our reasoning is obviously compromised\nsay, due to alcohol consumption, fatigue, illness, etc. we may also display\nvarious biases, in particular when we have a personal stake in the decision.\nRisks. As in the case of the \"speed\" reason above, human review may in\nsome cases not be possible, thereby increasing the risk. On the other hand, in\nsome cases (such as the imagined blocked email message above), we could let\nanother human quickly review the decision if it is deemed important enough, so\nthat if this is set up well, the risk is limited."}, {"title": "2.7 Economic efficiency", "content": "Sometimes, we may wish to deploy AI simply to reduce costs. (This reason of-\nten overlaps with the \"scale\" reason above.) For example, consider the project\nof further automating call centers. To be concrete, consider a government-run\nhealthcare hotline. Let us suppose that in fact, the AI still functions poorly\nenough that callers would be better served if they were instantly connected to\na well-trained human being, so that the primary purpose of using AI to handle\ncalls is to reduce costs. The AI may then face morally challenging decisions when\ndetermining which callers to connect to one of the scarce human beings answer-\ning calls, and which callers to try to handle itself. While this may sound like an\nundesirable scenario from the perspective of a caller, the resulting cost savings\ncan of course be valuable; the government could in principle take the savings\nand apply them towards (say) preventive healthcare education campaigns.\nRisks. Similarly to the \"scale\" reason above, in this context it seems to\nmake sense to periodically review decisions, and to do so more frequently the\nmore important the decisions are; if this is done well, then risks stay limited."}, {"title": "3 Conclusion", "content": "It may seem that it is a bad idea to have AI systems make moral decisions,\nor that at the very least, they should not do so unless and until we have an\nappropriate, mathematically precise theory for doing so; and that for now, we\nshould leave moral decision making to humans. In this short paper, we have\nconsidered a variety of reasons for why we might nevertheless expect AI systems\nto end up making decisions with a significant moral component. This may be\ncause for concern, and we have also discussed various risks associated with it;\nand just that one or more of these reasons apply does not necessarily mean that\nautomating moral decisions is a good idea. But when AI systems do end up\nmaking these decisions, we should not close our eyes to the fact that they have a\nmoral component, or na\u00efvely think that we can always effectively bring humans\ninto the loop to make these decisions. The best way for AI systems to learn how\nto make these decisions may be by observing examples of human moral decision\nmaking, but in the end, they are likely to have to make individual decisions\nthemselves."}]}