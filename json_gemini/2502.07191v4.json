{"title": "Bag of Tricks for Inference-time Computation of LLM Reasoning", "authors": ["Fan Liu", "Wenshuo Chao", "Naiqiang Tan", "Hao Liu"], "abstract": "With the advancement of large language models (LLMs), solving complex reasoning tasks has gained increasing attention. Inference-time computation methods (e.g., Best-of-N, beam search, et al.) are particularly valuable as they can enhance reasoning performance without modifying model parameters or requiring additional training. However, these techniques come with implementation challenges, and most existing methods remain at the proof-of-concept stage with limited practical adoption due to their computational complexity and varying effectiveness across different tasks. In this paper, we investigate and benchmark diverse inference-time computation strategies across reasoning tasks of varying complexity. Since most current methods rely on a proposer-verifier pipeline that first generates candidate solutions (e.g., reasoning solutions) and then selects the best one based on reward signals (e.g., RLHF rewards, process rewards), our research focuses on optimizing both candidate solution generation (e.g., instructing prompts, hyperparameters such as temperature and top-p) and reward mechanisms (e.g., self-evaluation, reward types). Through extensive experiments (more than 20,000 A100- 80G GPU hours with over 1,000 experiments) across a variety of models (e.g., Llama, Qwen, and Mistral families) of various sizes, our ablation studies reveal that previously overlooked strategies can significantly enhance performance (e.g., tuning temperature can improve reasoning task performance by up to 5%). Furthermore, we establish a standardized benchmark for inference-time computation by systematically evaluating six representative methods across eight reasoning tasks. These findings provide a stronger foundation for future research. The code is available at https://github.com/", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have demonstrated remarkable reasoning capabilities, enabling them to tackle increasingly sophisticated tasks in fields such as science, mathematics, and coding (Zhang et al., 2024a; Chen et al., 2021). While scaling model size and expanding high-quality training datasets have significantly driven these advancements, researchers are actively exploring complementary approaches to further enhance model performance. Inspired by human problem-solving behavior-where individuals often dedicate more time deliberating on complex problems to improve their decisions\u2014there is growing interest (Snell et al., 2024) in leveraging inference-time computation (e.g., utilizing additional computation during testing to enhance the performance of reasoning tasks) to strengthen the reasoning abilities of LLMs.\nWhile inference-time computation holds significant potential for enhancing the reasoning performance of LLMs (Wang et al., 2022), existing studies reveal mixed results in inference-time computation (e.g., limited self-correction capabilities (Huang et al., 2023)). Its effectiveness on broader reasoning tasks (e.g., logical reasoning, code generation, question answering, and fact verification) remains limited, with most research narrowly focused on domains like math problems. Moreover, inference-time methods are sensitive to hyperparameters, such as temperature and top-p sampling, where small adjustments can lead to notable performance differences (e.g., a 5% improvement in solving math problems by tuning temperature). These challenges underscore the critical role of inference-time techniques (e.g., instructing prompt, sampling strategies, reward models), as shown in Table 1. Despite recent advancements, these gaps indicate that the field remains nascent, with many challenges yet to be addressed.\nIn this study, we investigate key tricks that influence the effectiveness of inference-time computation methods in LLM reasoning. Since most current methods rely on a proposer-verifier pipeline that first generates candidate solu-"}, {"title": "2. Related Work", "content": "We briefly introduce related work, including reasoning with LLMs, inference-time computation methods for LLM reasoning, and benchmarks of LLM reasoning.\nReasoning with LLMs. LLMs have demonstrated strong reasoning abilities in complex tasks such as code generation, mathematical problem-solving, and research ideation (Zhou et al., 2022). Existing methods for enhancing LLM reasoning include: 1) Prompt engineering \u2013 Activates latent multi-step reasoning capabilities. For example, Chain of Thought (CoT) (Wei et al., 2022) guides step-by-step problem-solving but relies heavily on high-quality demonstrations for analogical learning. 2) Post-training techniques(Chen et al., 2024a;b) \u2013 Iteratively enrich training datasets to improve model performance. Self-training methods(Chen et al., 2024a) curate new high-quality examples to enhance reasoning, but these approaches demand significant computational resources. 3)Search-based methods(Browne et al., 2012; Feng et al., 2023a; Liu et al., 2023) \u2013 Optimize reasoning paths at inference time using search algorithms. For instance, Tree of Thought(Yao et al., 2024) employs breadth-first search to refine solutions. This work focuses on test-time computation, leveraging inference-time optimization to enhance LLM reasoning without additional training overhead.\nInference-Time Computation of LLM Reasoning. Scaling inference-time computation has proven more effective than merely increasing model parameters (Snell et al., 2024). Recently, research has focused on optimizing reasoning efficiency during inference rather than solely scaling training-time computation. Best-of-N (Cobbe et al., 2021a) enhances LLM reasoning by sampling N candidate solutions, evaluating them with a learned verifier or reward model, and selecting the highest-scoring one. Similarly, MCTS (Tian et al., 2024) improves inference by actively planning and selecting higher-quality responses. These advancements highlight inference-time optimization as crucial for enhancing LLM reasoning beyond scaling training computation.\nBenchmarks of LLM Reasoning. LLMs have made remarkable progress in solving complex tasks in a zero-shot manner (Hendrycks et al., 2021; Press et al., 2022; Liu et al., 2024a), positioning them as a key milestone toward"}, {"title": "3. Preliminares", "content": "LLMs. Given an input context x (e.g., math problem, commonsense QA, etc.), the LLM aims to autoregressively predict the next token (Dubey et al., 2024),\n$\\pi_{\\theta}(y|x) = \\prod_{t=1}^{n}\\pi_{\\theta}(y_t|x, y_{<t}),$ (1)\nwhere $\\pi_{\\theta}(\u00b7)$ is the LLM parameterized by $\\theta$, and y = $(y_1, y_2,..., y_n)$ is the output sequence. Here, $y_{<1} = \\emptyset$ and $y_{<t} = (y_1, y_2,......, y_{t-1})$. For a vocabulary size M, the probability of predicting the t-th token is determined using a softmax with temperature $\\tau$ on logit scores z of all tokens, combined with top-p (nucleus sampling) to control the randomness and diversity of the sampling process.\nChain of Thought Prompting. Chain-of-thought (CoT) (Wei et al., 2022) is a method that prompts LLMs to generate a series of reasoning steps leading to the final answer. These intermediate steps, denoted as $y_1,..., y_{n-1}$, connect the input x to the output y (omit n for simplicity), where n represents the total number of steps. For example, given an instruction I (e.g., \"Let's solve this step by step\") along with demonstration examples and the input question x, the final answer is y. Each intermediate thought $y_i$ is a part of the reasoning process that leads to the final answer. These thoughts are sequentially generated from the distribution $y_i \\sim \\pi_{\\theta}(\u00b7 | I, x, y_{<i-1})$, and the final output is sampled from: $y \\sim \\pi_{\\theta}(\u00b7 | I, x, y_{<n-1})$.\nTemperature. The temperature (Hinton, 2015) $\\tau$ of LLM controls the level of randomness in the generated outputs, influencing their diversity. Instead of directly calculating the softmax, the logits are scaled by the temperature value. The conditional probability of generating a token in the sequence can be expressed as: $\\pi_{\\theta}(y_t | x, y_{<t}) = \\frac{exp(z_t/\\tau)}{\\sum_{i=1}^{M} exp(z_i/\\tau)}$, where $z_t$ represents the logit score: $logit_{\\theta}(y_t | x, y_{<t})$, and $\\tau$ is the temperature parameter. A higher temperature $\\tau$ results in a smoother probability distribution (introducing"}, {"title": "4. Decoding Inference-Time Computation of LLM Reasoning", "content": "In this section, we decode the inference-time computation of LLM reasoning. First, we introduce the experimental setup, followed by the main bag of tricks for improving inference-time computation of LLM reasoning. Finally, we benchmark various inference-time computation methods. The overall framework is illustrated in Figure 1.\n4.1. Experiments Setup\nModels. Inference Model. In our experiments, we evaluated several widely studied LLMs of varying sizes and configurations: 1) LLaMA 3.3 (Dubey et al., 2024): Meta AI's latest iteration in the LLaMA series, available in 8B and 70B parameters. It is known for its open-source accessibil-"}, {"title": "4.2. Bag of Tricks", "content": "Our goal is to investigate how previously overlooked tricks can critically affect the performance of inference-time computation methods, which typically consist of two main steps: generating candidate solutions (e.g., prompt type, temperature, top-p, etc.) and selecting the optimal solution based on specific reward signals (e.g., self-evaluation, reward type, reward process). In our default setup, we primarily adopt the Best-of-N inference-time computation with the number of candidates N = 32, the temperature $\\tau$ = 0.7, and top-p set to 0.9. Additionally, the instruction prompt type is set to Chain-of-Thought (CoT). Without further modifications, we conduct ablation studies, varying only the specific tricks under investigation. We focus primarily on complex reasoning tasks, including math problems, and code generation tasks, etc, while additional tasks are detailed in the Appendix B. All additional details regarding the experimental implementation settings can be found in the Appendix A.\nNote that our empirical observations and conclusions may not generalize to all datasets and models. However, we emphasize the necessity of using consistent implementation details to ensure fair comparisons among different inference-time computation methods.\n4.2.1. GENERATING CANDIDATE SOLUTIONS\nGenerating candidate solutions is a critical step in inference-time computation for LLM reasoning, but the inherent randomness in this process significantly influences diversity. Hyperparameters such as temperature and top-p, along with strategies like instruction prompts, play a vital role in shaping and guiding the solution trajectory. For example, temperature, as a sampling strategy in token generation, increases diversity at higher values. Therefore, this study focuses on the candidate solution generation process, including instruction prompt types, temperature, and top-p sampling.\nInstruction Prompt Type. Different instruction prompts can guide an LLM to generate distinct reasoning paths. Specifically, Input-Output (IO) prompts directly provide the answer, whereas Chain-of-Thought (CoT) prompts encourage the LLM to reason step by step. Recent research (Huang et al., 2023) suggests that self-correction or self-reflection mechanisms are often ineffective when LLMs operate without external feedback under certain prompt types. We further explores the impact of various prompt types, including IO prompts, standard Chain-of-Thought (CoT) prompts, and reflection-based CoT.\nTemperature. Temperature (Hinton, 2015) $\\tau$ regulates the diversity of candidate solutions in LLMs. A higher decreases prediction confidence but increases output variability. We revisit the previous inference-time computation settings in Table 1, where the temperature is set differently for each case. Figure 3 illustrates its effect. In most reasoning scenarios, the LLM's performance is optimized at T= 0.8, yielding an improvement of approximately 2.32% to 4.83% across four datasets. In most cases, both larger and smaller values of $\\tau$ result in decreased performance.\nTop-p. The top-p parameter regulates the output of an LLM"}, {"title": "4.3. Benchmarking of Inference-time computation of LLMs reasoning under Computation Budgets", "content": "We further examine various inference-time computation methods, including Best-of-N, Step-level Best-of-N, Beam Search, MCTS, Self-Consistency, and Self-Refine, under a fixed computation budget.\nSetup. We evaluate six inference-time computation methods under a fixed computation budget (equal-token results to standardize computational consumption across tasks) on Qwen-2.5-7B and LLaMA-3.3-8B. To ensure fairness, consistent settings are applied to all methods. For knowledge-based reasoning tasks (e.g., Bamboogle, HotpotQA, Fever), we use the RLHF reward model. For complex reasoning tasks (e.g., MATH, code generation), the QwQ-32B model provides the process reward. Implementation details are available in Appendix A.\n includes: (1) Performance varies significantly across tasks. Best-of-N and Self-Consistency perform well in knowledge-based reasoning tasks, but for complex tasks"}, {"title": "5. Conclusion", "content": "In this paper, we investigate the role of inference-time computation in enhancing the reasoning capabilities of LLMs. Our extensive experimental evaluation reveals that seemingly simple yet overlooked tricks\u2014such as sampling strategies and reward mechanisms\u2014can yield substantial improvements in reasoning performance. The results demonstrate that careful tuning of inference parameters, such as temperature settings, top-k sampling, reward models, plays a crucial role in optimizing the performance of LLMs during inference time."}, {"title": "Impact Statement", "content": "This study explores the optimization of inference-time computation strategies to enhance the reasoning abilities of LLMs. By addressing the limitations of existing methods, our findings contribute to improving LLM performance across diverse reasoning tasks, including logical reasoning, code generation, and fact verification. Our work establishes a comprehensive benchmark and demonstrates that previously overlooked techniques\u2014such as temperature and top-p sampling adjustments\u2014can significantly enhance reasoning accuracy. These advancements provide a critical foundation for future research and practical applications, particularly in resource-constrained settings. Optimizing LLMs at inference time without extensive retraining opens new possibilities for improving AI systems' efficiency and reliability in real-world use cases."}, {"title": "A. Experiments Setup", "content": "In this section, we provide a more detailed explanation of the experimental setup. Our objective is to evaluate the impact of previously overlooked techniques on the performance of inference-time computation methods. These methods typically consist of two primary steps: (1) generating candidate solutions using specified parameters (e.g., prompt type, temperature, and top-p), and (2) selecting the optimal solution based on predefined reward signals (e.g., self-evaluation strategies, reward types, and processes).\nDefault Configuration. In our default setup, we employ the Best-of-N inference-time computation, where: The number of candidate solutions N is set to 32. The temperature ($\\tau$) is set to 0.7, which introduces moderate stochasticity during candidate generation. The nucleus sampling parameter, top-p, is configured at 0.9, ensuring diversity by sampling from the top 90% cumulative probability distribution of the predicted tokens. We utilize a Chain-of-Thought (CoT) instruction prompt type to facilitate step-by-step reasoning for more complex tasks. Unless explicitly modified, all experiments adhere to this baseline configuration.\nReward Model. Specifically, for process and result rewards, we employ a prompt-driven approach to guide QwQ-32B, a reasoning model, in evaluating candidate solutions. The prompts used are the process evaluation prompt and the result evaluation prompt, which refer to the evaluation of step-level solutions and the final results, respectively. For RLHF and proof-critical rewards, which are numerical reward models, scores are directly assigned to candidate solutions. RLHF Reward: Based on InternLM2-Chat-1.8B-SFT (Cai et al., 2024), trained on over 2.4 million preference samples. It balances performance, helpfulness, and alignment. Proof-Critical Reward Derived from the InternLM2-5-Step-Prover-Critic model (Wu et al., 2024b), which excels in multiple benchmarks."}, {"title": "Process-Evaluation.", "content": "Evaluate whether the language model can decompose the question into relevant sub-questions and assess whether this decomposition aids in answering the original question, either partially or directly. The evaluation result will be classified as \"Sure,\" \"Likely,\" or \"Impossible\u201d based on the effectiveness of the decomposition.\nEvaluation Process: 1. Relevance of Sub-Questions: Determine if the sub-questions are directly related to solving the original question. 2. Effectiveness of Decomposition: Assess whether the sub-questions, when answered, lead to a comprehensive response to the original question.\nEvaluation Outcomes: Sure: The model successfully decomposes the question into relevant sub-questions, each structured to contribute to an accurate final answer. Likely: The model decomposes the question into relevant sub-questions, but minor improvements in structure or relevance may enhance the response. Impossible: The model fails to decompose the question effectively or the sub-questions are not relevant to the original question."}, {"title": "Result-Evaluation.", "content": "The Result-Evaluation prompt evaluates the final outcome of the language model's reasoning process based on the accuracy, clarity, and completeness of its answer. Each evaluation is categorized as \"Sure,\" \"Likely,\" or \"Impossible.\u201d The result is judged by verifying whether the language model's final answer is both directly relevant and valid in response to the question posed. The process involves reviewing whether the model's conclusion is definitive and supported by logical reasoning. If the answer is clear and unambiguous, it is marked as \"Sure.\" If there are minor ambiguities, it is categorized as \"Likely.\" If the answer is incorrect or irrelevant, it is deemed \"Impossible.\u201d"}, {"title": "Tasks.", "content": "Our research focuses on the following reasoning tasks: (1) Arithmetic Reasoning: GSM8K: A dataset with grade school math word problems requiring 2 to 8 calculation steps using basic arithmetic operations. GSM-Hard: A harder variant of GSM8K with larger, less common numbers, increasing the challenge of arithmetic reasoning. (2) Complex Mathematical Reasoning: MATH Dataset: A dataset covering advanced topics like algebra, geometry, and calculus. Each problem includes detailed solutions to evaluate both final answers and problem-solving processes. We use the MATH 500 to test the complex mathematical reasoning ability of LLM. (3) Logical Reasoning: ProntoQA: Measures logical deduction and inference, requiring the application of logical principles to reach correct conclusions. (4) Code Generation: HumanEval: A benchmark for generating functional code snippets, with programming problems that include prompts and test cases to verify correctness. (5) Question Answering: Bamboogle: Evaluates diverse question-answering performance across various topics, testing comprehension and accurate responses. (6) Fact Verification: FEVER: Assesses the ability to verify factual claims using a document corpus, promoting fact-checking system development. (7) Common Sense"}, {"title": "Instruction Prompt Type.", "content": "Different instruction prompts can guide an LLM to generate distinct reasoning paths. Specifically, Input-Output (IO) prompts directly provide the answer, whereas Chain-of-Thought (CoT) prompts encourage the LLM to reason step by step. Recent research (Huang et al., 2023) suggests that self-correction or self-reflection mechanisms are often ineffective when LLMs operate without external feedback under certain prompt types. We further explores the impact of various prompt types, including IO prompts, standard Chain-of-Thought (CoT) prompts, and reflection-based CoT."}, {"title": "Input-Output (IO) Prompt.", "content": "The Input-Output (IO) Prompt directly answers a given question without intermediate reasoning steps. It is designed to generate concise and accurate responses, ending with the phrase \"so the final answer is:\". This prompt structure is particularly suitable for straightforward queries where minimal context or explanation is required."}, {"title": "Chain-of-Thought (CoT) Prompt.", "content": "The Chain-of-Thought (CoT) Prompt guides the model to answer questions step-by-step, breaking down the reasoning process into intermediate steps before concluding with the final answer. For instance, when comparing the lifespans of Theodor Haecker and Harry Vaughan Watkins, the CoT prompt explicitly calculates their ages step-by-step before determining the longer-lived individual. This structured approach enhances reasoning transparency and aligns the response with logical steps."}, {"title": "Reflect CoT.", "content": "The Reflect Chain-of-Thought (Reflect CoT) prompt introduces a structured reasoning approach where each step in answering a question is followed by a reflection to verify its accuracy and reliability. For example, when comparing the lifespans of Theodor Haecker and Harry Vaughan Watkins, the process involves step-by-step reasoning to establish each individual's age at death. After each step, a \"Reflection\" line is used to ensure the validity of the information, such as verifying directly provided ages or confirming the consistency of the comparison. The final conclusion, supported by reflections, ensures a reliable and transparent reasoning process."}, {"title": "B. Other Experiments", "content": "Instruction Prompt Type. The experimental results on additional datasets reveal a consistent finding: the type of instruction prompt significantly influences the inference-time computation of LLM reasoning, as illustrated in Figure 9.\nTempature. The experimental results on additional datasets reveal a consistent finding: the type of instruction prompt significantly influences the inference-time computation of LLM reasoning, as illustrated in Figure 10.\nTop-p. We further present the ablation study on top-p applied to other reasoning tasks. The experimental results are shown in Figure 11. The impact of top-p is significant; generally, as top-p increases, the LLM's reasoning performance also improves, with optimal performance observed at 0.9 for most reasoning tasks."}]}