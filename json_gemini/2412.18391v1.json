{"title": "TPAOI: Ensuring Fresh Service Status at the Network Edge in Compute-First Networking", "authors": ["Haosheng He", "Jianpeng Qi", "Chao Liu", "Junyu Dong", "Yanwei Yu"], "abstract": "In compute-first networking, maintaining fresh and accurate status information at the network edge is crucial for effective access to remote services. This process typically involves three phases: Status updating, user accessing, and user request-ing. However, current studies on status effectiveness, such as Age of Information at Query (QAoI), do not comprehensively cover all these phases. Therefore, this paper introduces a novel metric, TPAOI, aimed at optimizing update decisions by measuring the freshness of service status. The stochastic nature of edge environ-ments, characterized by unpredictable communication delays in updating, requesting, and user access times, poses a significant challenge when modeling. To address this, we model the problem as a Markov Decision Process (MDP) and employ a Dueling Double Deep Q-Network (D3QN) algorithm for optimization. Extensive experiments demonstrate that the proposed T\u03a1\u0391\u039f\u0399 metric effectively minimizes Aol, ensuring timely and reliable service updates in dynamic edge environments. Results indicate that TPAOI reduces Aol by an average of 47% compared to QAoI metrics and decreases update frequency by an average of 48% relative to conventional Aol metrics, showing significant improvement.", "sections": [{"title": "I. INTRODUCTION", "content": "A critical challenge in compute-first networking is ensuring the freshness and accuracy of monitored data, which directly impacts the effectiveness of accessing remote services [12], [17]. Central to addressing this challenge is the Age of Information (AoI), a pivotal metric that quantifies the time-liness of information [30]. Aol measures the elapsed time from data generation to its retrieval, playing a crucial role in assessing system performance. This metric is particularly vital in contexts requiring rapid response capabilities, such as Internet of Things (IoT) [1], healthcare systems [14], and autonomous driving [29].\nFig. 1 provides a general process of service provisioning in the compute-first networking domain. In this process, users at the network edge can access the updated status stored on access points (APs) and then request the remote service. It can be broadly divided into three phases [17], [32]: \u2460 Updating (or forwarding), an edge server distributes service status updates (e.g., its recently supported concurrent capacity) to the APs via the network. \u2461 Accessing, users, upon accessing the APs, can evaluate the updated status to determine whether to send a request to the remote service. During this phase, the process can be modeled as a stochastic process. \u2462 Requesting (or backward requesting), AP sends the request to remote service, incurring a random communication delay.\nAs commonly pointed out in Aol analysis, a lower sta-tus updating frequency may cause service monitoring errors, while a higher frequency can lead to communication burdens and energy budget [30]. When ensuring the status updating frequency, as illustrated, there are three stochastic phases involved-updating, accessing, and requesting. During the updating and requesting phases, bandwidth could be changing, affecting the transmitting delays. Meanwhile, in the accessing phase, the user accessing times are stochastic and not easy to model. Therefore, given those dynamic edge environments [22], ensuring the effectiveness of the service status consider-ing those three phases could be challenging.\nPrevious work has somewhat ignored this integrity. In early Aol studies, most literature assumes that users con-tinuously access the APs and consider the optimal Aol at every moment [5], [30]. However, in real-world scenarios, user access behaviors are stochastic. Some studies, such as QAoI [5] and EAol [31], focus on the Aol at user access times, updating just before user access, which significantly reduces resource costs. Nevertheless, they assume that the status sent by the edge server can quickly reach the access point or its buffer. In practical applications, transmission delay (including the updating and requesting) can significantly impact system performance, especially under heavy network loads or long transmission distances. Ignoring the effects of transmission delay fluctuations may result in status updates arriving after the user access.\nTo fill that gap, we propose a new age metric called Three-Phase Aol (TPAOI), which incorporates phases such as updat-ing, requesting, and user stochastic accessing. Given the dy-namic nature of edge environments, precisely depicting them is complex, therefore we model the problem of minimizing the TPAOI as an MDP and optimize it using a model-free approach in deep reinforcement learning (DRL) methods. Specifically, we first design five sub-states according to the three phases to represent the environments and then implement an algorithm based on D3QN to accurately capture the dynamic behaviors. Results show that our solution can reduce both the Aol and updating frequency effectively, reducing Aol by an average of 47% compared to QAoI, and decreasing the update frequency by an average of 48% over conventional Aol.\nThe main contributions in this work are as follows:\n\u2022 We comprehensively consider the Aol during three phases, including status updating, user accessing, and user requesting, and propose a metric, TPAOI, to measure the freshness of the service status.\n\u2022 To handle the dynamic edge environments, we formulate the system model as an MDP and carefully design the state to satisfy the MDP property. Subsequently, we employed the Dueling Double DQN (D3QN) algorithm to optimize the MDP, thereby effectively capturing the environmental dynamics and showcasing the method's robustness.\n\u2022 We conducted extensive experiments to validate the ef-fectiveness of the proposed method in solving the mini-mization of the Aol problem. The results show that our TPAoI can maintain a lower Aol while reducing the update frequency, demonstrating that our approach can keep the service status fresh at the network edge."}, {"title": "II. RELATED WORK", "content": "We first classify the related work based on the phases they focus on (i.e., status updating, user accessing, and user requesting), and then discuss the efforts made to optimize Aol in dynamic environments."}, {"title": "A. Aol Analysis of Different Phases", "content": "Conventional Aol was initially introduced and applied in [11] within queuing models to measure and optimize the freshness of information. These foundational queue models are mainly implemented in the updating phases to reduce transmitting delays [3], [18], [21].\nHowever, conventional Aol does not consider the phases after the information is accessed, causing unnecessary updates when no user is requesting [8]. To address this, many new metrics based on Aol have been proposed, such as EAOI (Effective AoI) [31], AoT (Age of Task) [20], AoP (Age of Processing) [13], QAoI (Aol at Query) [5], and AoL (Age of Loop) [7]. EAoI [31] and QAoI [5] recognize that the importance of status values varies over time and is only valuable when used for decision-making, i.e., user accessing. [4], [8], [13], [20] assert that status is freshest only when it is processed, and finally reaches the destination node. [16], [25] on Aol optimization under bidirectional delays extend the Aol analysis to the status updating and result return phases. Meanwhile, AoL [7] focuses on a closed environment and is the first to comprehensively cover various phases of status processing, introducing the concept of \u201cloop\u201d Aol to ensure the freshness of status in the sensing-control-execution loop.\nTo our knowledge, there has been limited research that jointly considers status updating, user accessing, and user requesting. Existing studies tend to focus on only one or two of these aspects. When only the user access process is considered, the other two phases are often overlooked, leading to status updates arriving after the user has accessed the data. Conversely, when the focus is solely on the transmitting phases, there is a tendency to send excessive and unnecessary status updates, which can increase the network burden."}, {"title": "B. Aol Optimizing Methods in Dynamic Environment", "content": "Meanwhile, optimizing Aol also faces the challenge of en-vironmental dynamics, such as delays and user behaviors. As a key enabler, reinforcement learning can model these dynamics. Related methods are classified into model-based and model-free categories based on the availability of environmental information.\nFor model-based methods, [5] designs a quadruple environ-mental state that includes Aol, data transmission error proba-bility, node energy consumption, and user access stochastic process. It constructs precise transition probabilities based on specific distributions to optimize long-term average Aol, enhancing the freshness of the state when accessed by users. [9] studies the round-trip interaction process between sampling and control nodes, using the Aol of each round-trip stage and the round-trip link load as environmental state, and minimizes the long-term average Aol based on geometric probability distribution.\nFor model-free methods, [9] uses Deep Q Network (DQN) to design the mean squared error loss between observed and actual state information values as part of the reward function, maximizing long-term returns and improving the accuracy of state information. [10] considers the sensor's data transmission queue and the edge server's task processing queue size as environmental state, incorporating constraints like energy consumption and service rate into the reward func-tion. [6] minimizes the Aol of crowdsensing data collection by using relational graph convolutional networks (GNN) to predict environmental state transitions and Monte Carlo tree search to improve UAV path planning strategies, solving the issue of difficult-to-model state transitions. [28] addresses task offloading decisions for multiple sensing nodes and multi-node task selection decisions at the server end. It extends precise MDP methods to an ANN-based model-free DQN approach, integrating the cost of multi-task selection decisions at the server into the task offloading decision cost function for multiple nodes, achieving results nearly consistent with model-based methods."}, {"title": "III. THREE-PHASE UPDATING SYSTEM", "content": "We model the system using a time-slotted approach, with discrete time slots indexed as 1, 2, .It is assumed that any status information arriving within a given time slot is processed at the start of the subsequent time slot. We denote the send time of the kth status update as $u_k$ and the corresponding arrival time at the AP as $u'_k$. As in [30], we represent the conventional AoI at time slot t as $\\Delta(t)$, which represents the difference between the current time and the generation time of the last status update successfully received:\n$\\Delta(t) = t - \\underset{u_k \\leq t}{\\text{max }} u_k.$\nCompared to the definition above, the following equivalent definition of the conventional Aol is more useful in a time-slot system, as it delineates the variation of Aol between time slots:\n$\\Delta(t) =\\begin{cases}\\Delta(t-1) + 1 &\\text{if } t \\neq u'_k \\\\u'_k - u_k & \\text{if } t = u'_k\\end{cases}$,\nwhere $\\Delta(0) = 0$, and $k = \\arg \\underset{k}{\\text{max }} u'_k \\leq t$ represents the index of the status update that has most recently arrived relative to time t. It indicates that Aol increases linearly until a status update arrives, resetting it to $u'_k$.\nAs in (3), TPAOI can be divided into the sum of three com-ponents: The forwarding delay of the status update $u'_k-u_k$, the waiting time for user access $q_i-\\underset{U_k \\leq q_i}{\\text{max }} u_k$, and the requesting delay $q'_i - q_i$.\nBased on (3), the long-term expected TPAOI $\\Tau_\\infty$ is defined as:\n$\\Tau_\\infty = \\underset{T \\rightarrow \\infty}{\\text{lim sup }} E \\Big[ \\frac{1}{T} \\sum\\limits_{i: q_i< T} \\Tau(i)\\Big]$.\nOur goal is to minimize $\\Tau_\\infty$ to decide when the server should generate and forward the status update.\nTo minimize (4), according to (3), we need to understand those three stochastic phases to ensure that the status up-date arrives at the AP precisely before the user accesses it. However, due to the dynamics of the environments, knowing this information is non-trivial. We address this challenge in Sect. IV by using a DRL-based algorithm to achieve minimal \u03a4\u03a1\u0391\u039f\u0399.\nWe assume that the edge server can obtain all information in the system through telemetry or piggybacking techniques [23]. We also simplify the network by using a first-come, first-served (FCFS) principle for transmitting status updates and user requests.\nCommunication between the service and the AP includes propagation delay (at the speed of light) and transmission delay (data transmission). Due to resource competition in dynamic environments, transmission delays are subject to fluctuations, which are assumed to follow a normal [19] or exponential [27] distribution. It is worth noting that the purpose of our method is to capture various dynamics and maintain a certain degree of generality, rather than being limited to just the two distributions mentioned above. For simplicity, we refer to both updating and requesting delays as transmission delays.\nAdditionally, similar to [16], we also adopt a wait-N policy where at most N status updates can be concurrently present. Considering the capacity of the system (or service), we assume there are at most M requests on the link.\nDue to the potential for network congestion caused by excessive update transmissions from the remote service, we impose restrictions on their transmission frequency to save costs. Specifically, we introduce a penalty mechanism, where sending an update incurs a negative reward. This encourages the server to avoid transmitting updates unless necessary. As illustrated in Fig. 2, because there is no user request, the transmission of update u2 is deemed undesirable and can be omitted to conserve network bandwidth."}, {"title": "A. \u03a4\u03a1\u0391\u039f\u0399 Metric Construction", "content": "We define the time when the user's ith request accesses the status at the AP as $q_i$. Subsequently, the request is transmitted to the remote service, arriving at time $q'_i$. Then, we define TPAOI as the Aol of the request at the time of its arrival, denoted as $\\tau(i)$:\n$\\tau(i) = \\Delta(q_i) + q'_i - q_i$\n$=(u'_k - u_k) + (q_i - u_k) + (q'_i - q_i)$,\nwhere $k = \\arg \\underset{u_k \\leq q_i}{\\text{max }} u_k$ represents the index of the status update that has most recently arrived relative to query $q_i$."}, {"title": "IV. \u03a4\u03a1\u0391\u039f\u0399 MINIMIZATION BASED ON DRL", "content": "In this section, we formalize the system model outlined in Sect. III using a MDP. This approach enables us to effectively adapt to the dynamic environments. An MDP is characterized by the tuple (S, A, P, R), where S represents the state space, indicating the state of the system; A represents the action space, denoting the actions the server can take; P denotes the state transition probabilities $P(s_{t+1}|a_t, s_t)$; and R represents the reward function $R(s_t, a_t, s_{t+1})$, indicating the immediate reward received after an action and the subsequent state transition."}, {"title": "A. State S Representation for Three Phases", "content": "To fully capture the dynamics of the entire process, we have designed five sub-states. Three of these sub-states capture the three stochastic phases, while the remaining two are related to the Aol, specifically the conventional AoI and the TPAOI. The five sub-states are illustrated as follows:\n\u2460 $s_{1,t}$ represents the transmission time for status updates, illustrating the stochastic process of the first phase (i.e., the status update phase), and it is an N-dimensional vector.\n\u2461 $s_{2,t}$ represents the conventional AoI (Con. AoI for short) at the AP, which is the waiting period for user access.\n\u2462 $s_{3,t}$ represents the interval between user accesses, illus-trating the stochastic process of the second phase, which is the user access phase.\n\u2463 $s_{4,t}$ represents the transmission time of user requests, illustrating the stochastic process of the third phase (i.e., the user request phase), and it is an M-dimensional vector.\n\u2464 $s_{5,t}$ represents the Aol of the request, and it is an M-dimensional vector. Upon the arrival of the request, this value becomes TPAOI.\nThese five sub-states together form a comprehensive state $s_t$ that effectively captures the dynamic changes of TPAOI in the system. We discuss them in detail as follows.\nFirstly, we need to introduce the Aol of the AP, denoted as $s_{2,t}$, so that we can update it when users are about to access and the Aol is high. The definition of $s_{2,t}$ is the same as the definition of conventional AoI, which is:\n$s_{2,t} = \\Delta(t)$.\nHowever, due to transmission delays, whether a status update arrives at the current time is related to whether edge servers sent status updates in the previous time slots. In other words, the next state is related to actions taken several time slots ago, which does not align with the properties of MDP, where the next state should depend only on the current state and not on previous states:\n$P(s_{t+1}|a_t, s_t,..., a_1, s_1) = P(s_{t+1}|a_t, s_t)$.\nSecondly, to ensure the state aligns with MDP properties, we introduce a state for each status update on the link to represent its transmission time, thereby determining when the AP updates its AoI. These states of update collectively form $s_{1,t}$. Additionally, we allow a maximum of N status updates concurrently on the link to determine the length of $s_{1,t}$ and avoid training difficulties due to the variable length of $s_{1,t}$. Here is the definition of the ith (i < N) state value in $s_{1,t}$:\n$s_{1,t+1,i} =\\begin{cases}s_{1,t,i} + 1 &\\text{if } s_{1,t,i} \\neq 0 \\land z_{t,i} = 0 \\\\1 &\\text{if } a_t = 1 \\land x = i\\\\0 & \\text{otherwise}\\end{cases}$,\nwhere $z_{t,i} \\in {0,1}$ denote whether ith status update has arrived at the AP at time slot t with 1 indicating that it has arrived, and x is a random sample drawn from the set that includes each index j meets $s_{1,t,j} = 0, j = 1, . . ., N$. Here is the explanation of the formula: A value of 0 indicates an idle state. When an edge server sends a status update, it randomly selects a state with a value of 0 and sets it to 1. Otherwise, the state represents the transmission time of the status update on the link. It increments by 1 each time the status update hasn't arrived and resets to 0 upon arrival.\nThirdly, to estimate the time user access, we introduce a state to represent the interval between user accesses, denoted as $s_{3,t}$, which linearly increases and resets to 1 upon user arrival. The $s_{3,t}$ can be expressed as follows:\n$s_{3,t+1} =\\begin{cases}s_{3,t} + 1 &\\text{if } y_t = 0 \\\\1 &\\text{if } y_t = 1\\end{cases}$,\nwhere $y_t \\in {0,1}$ denote whether the user accesses at time slot t.\nFourthly and finally, we introduced two states for each request on the link to represent its transmission time and its Aol, respectively, to ensure that the reward function conforms to the properties of the MDP:\n$P(r_t|a_t, s_t,..., a_1, s_1) = P(r_t|a_t, s_t)$."}, {"title": "B. Updating Action A and Reward R", "content": "The edge server can decide whether to send a status update in each time slot. Accordingly, the action space is defined as A = {0,1}, where $a_t$ denotes the action taken at time slot t. Specifically, $a_t = 1$ signifies that the edge server opts to send a status update during that time slot, while $a_t = 0$ indicates that no status update is sent.\nThe reward is defined as the Aol of the request upon its arrival at the edge server. Additionally, to prevent the edge server from sending status updates ineffectively, we assign a negative reward w when it sends a status update. The reward at time slot t, denoted as $r_t$, can be represented as follows:\n$r_t = - (wa_t + \\sum\\limits_{i=0}^{M} r_{t,i})$,\n$r_{t,i} = s_{5,t,i}k_{t,i}$.\nAfter modeling the MDP, our overall optimization objective (4) becomes finding a policy $\\pi*$ to maximize the expected state value:\n$\\pi* = \\arg \\underset{\\pi}{\\text{max }} E[V_{\\pi}(S)|s_0]$,\nwhere $V_{\\pi}(s)$ is the state value that denotes the expected discounted cumulative reward, i.e.:\n$V_{\\pi}(s) = E[G_t | S_t = s, \\pi]$,\n$G_t = \\sum\\limits_{\\tau=0}^{\\infty} (\\gamma)^{\\tau} r_{t+\\tau+1}$,\nwhere $\\gamma < 1$ is the discount factor, which ensures V convergence. Additionally, the value of $\\gamma$ can control the range of the rewards considered. Due to the dynamics existing, transition probabilities implied in the value of state are usually difficult to obtain in practical applications. We use a DRL-based algorithm to fit these probabilities by continuously interacting with the environment."}, {"title": "C. DRL-Based TPAoI Minimization Algorithm", "content": "In this section, we thoroughly explore how to apply a DRL-based algorithm, namely D3QN, to solve the MDP defined earlier. Specifically, the Q-value function employs a Dueling Network architecture [26], while loss computation and training are implemented using Double DQN [24].\nUnlike DQN [15], we use two deep neural networks to approximate the Q-value function, typically employing plain deep neural networks for construction:\n$Q_{\\theta}(s, a) = E[G_t|s_t = s, a_t = a, \\pi*]$,\n$=\\underset{\\theta_1}{\\text{V}}(s) + \\underset{\\theta_2}{\\text{A}}(s, a) - \\underset{a}{\\text{mean }} \\underset{\\theta_2}{\\text{A}}(s, a)$,\nwhere $\\theta$ represents the parameters of the neural network, $\\underset{\\theta_1}{\\text{V}}(\u00b7)$ approximates the state value function, and $\\underset{\\theta_2}{\\text{A}}(\u00b7)$ approximates the advantage function. Compared to DQN, which directly approximates the Q-value function, this algo-rithm incorporates a more refined structure that enables faster convergence.\nSince the Q-value function represents the expected dis-counted cumulative reward following action, the optimal ac-tion can be determined by achieving the maximum Q-value in the state:\n$a*(s) = \\arg \\underset{a \\in A}{\\text{max }} Q_{\\theta}(s, a)$.\nJust like DQN, Double DQN also requires the use of two networks: The Q-network and the target Q-network, denoted respectively as $Q_{\\theta}(s, a)$ and $Q_{\\theta^-}(s, a)$, with $\\theta$ and $\\theta$ as their corresponding parameters. Initially, the target Q-network and the Q-network have the same structure and parameters. The target Q-network is employed to compute the Temporal Difference (TD) target, enabling stable training of the network. Additionally, compared to DQN, Double DQN addresses the problem of overestimation by using two separate estimations to calculate the TD error during training.\nDuring the training process, we leverage the experience replay technique to maximize the utilization of training data. Specifically, we use a Replay Buffer (RB) to store historical experiences as 4-tuples, denoted by $\\xi$ = ($s_t$, $a_t$, $s_{t+1}$, $r_t$). We let the RB as a circular queue with a maximum length of L. Training is conducted once every $T_0$ iteration. During training sessions, a mini-batch of experiences $\\Xi$ is sampled from the RB to update the parameters $\\theta$ by minimizing the following loss, namely the expected square of TD error:\n$loss = E_{\\xi \\in \\Xi} (y_t - Q_{\\theta}(s_t, a_t))^2$,\nwhere $y_t$ denotes the TD target:\n$y_t = r_t + \\gamma Q_{\\theta^-} (s_{t+1}, \\arg \\underset{a}{\\text{max}} Q_{\\theta}(s_{t+1}, a))$.\nWe use stochastic gradient descent (SGD) [2] to minimize the loss, which simplifies the process of computing the gradi-ent of the expectation over the entire dataset by using a single training sample or a mini-batch of samples in each iteration:\n$\\theta \\leftarrow \\theta - \\eta (Q_{\\theta}(s_t, a_t) - y_t) \\triangledown_{Q_{\\theta}(s_t, a_t)} Q_{\\theta}(s_t, a_t)$,\nwhere $\\eta$ is the learning rate. Furthermore, we employ a soft update technique to gradually adjust the parameters $\\theta$:\n$\\theta^- = \\tau \\theta + (1 - \\tau) \\theta^-$,\nwhere $\\tau$ controls the rate at which the weights of the target network are updated. To enhance exploration, we employ the $\\epsilon$-greedy strategy, where actions are chosen at random from the action space with a probability of $\\epsilon$, and (18) is followed with a probability of 1 - $\\epsilon$, where $\\epsilon$ decays according to the following formula, thereby increasing the exploitation:\n$\\epsilon = \\text{max}(\\epsilon \u00b7 \\text{E}_{decay}, \\text{E}_{min})$,"}, {"title": "V. EXPERIMENTS AND ANALYSIS", "content": "In this section, we demonstrate the effectiveness of our pro-posed method in minimizing TPAOI. Specifically, we consider the following four aspects:\n1 We compare TPAOI with QAoI and conventional Aol to illustrate the superiority of TPAOI.\n\u2461 We explain how TPAoI updates before user access by considering transmission delays, thereby achieving the effect of minimizing Aol at the time of user access.\n(3) We conduct ablation studies on the transmission cost and the number of concurrently existing status updates to illustrate their impact on the results.\n4 We present the training curve of our model to verify its convergence."}, {"title": "A. Experiment Settings", "content": "(1) Network Settings. We consider the following edge com-puting network scenario : In the user accessing phase, according to [5] we set the user access interval at 20 time slots. Moreover, to create a dynamic environment, we introduce a Poisson distribution with the user accessing dynamic parameter \u03bb on this basis. A lower \u03bb means user access frequency is lower, which means it's hard to predict the user's behavior, and the system dynamics are increased. Additionally, To simulate the network dynamics, transmitting delay fluctuations are modeled using either an exponential distribution with a mean of 1 or a normal distribution with both mean and variance of 1, denoted as exp(1) and N(1,1), respectively.\n(2) DRL Settings. The value function and advantage function in the Q-network have similar network structures but differ in their output dimensions, which are 1 and the dimension of the action space, respectively. The network includes four hidden layers with neuron counts of 128, 512, 256, and 128, utilizing the ReLU activation function. The learning rate \u03b7 is set to 0.0002, with a discount factor \u03b3 of 0.995. The capacity of the RB L is 50,000, and each mini-batch contains 128 samples. The soft update parameter \u03c4 is set at 0.001. The training encompasses 5000 episodes, with a transmission cost w of 1 per transmission. The initial greedy factor \u03f5 is 1, which decays at a rate of 0.98 per episode until it reaches 0.01. The maximum number of status updates present on the link at any given time is capped at two. Additionally, considering that the interval between user accesses significantly exceeds the transmission delay of requests, it is sufficient to allow only one request to exist on the link concurrently.\n(3) Baselines. We choose the most recent QAoI [5] and the conventional Aol [30] as our baselines.\n\u2022 QAoI: This model focuses solely on the Aol at the time of user access and can capture the user dynamics, disregarding forwarding and requesting delays. It assumes that status updates sent in the current time slot arrive at the access point immediately in the next time slot. QAoI is fundamentally a simplified version of the TPAOI model, having removed the transmission delay and only retaining the state of the user access interval and AoI. Due to its simplicity, we use the policy iteration method for training.\n\u2022 Conventional Aol (Con. Aol for short): It assumes that users may access information at any given moment, which does not accurately simulate actual user behavior. Aol can be considered a TPAOI model with a fixed user access interval of 1; therefore, we omit the user access interval in the state definition and utilize the same optimization algorithm used for TPAOI.\nWe then deploy the trained QAoI and Aol models in the TPAOI environment to compare and analyze the performance."}, {"title": "B. Performance Evaluation", "content": "In this section, we demonstrate how our metric compre-hensively considers both Aol and the total number of status updates (i.e., the number of transmissions), as shown in Fig. 4. We first illustrate Fig. 4(a) and 4(c), where the transmission delay fluctuation follows an exponential distribution and user access follows a Poisson distribution with different A. As shown in Fig. 4(a), in most cases TPAOI gets a lower Aol. Compared with QAoI, the Aol of TPAOI is significantly lower, improving about 47% on average. Putting Fig. 4(c) altogether, which shows the corresponding total number of status updates of Fig. 4(a), we find that TPAOI can not only ensure the status fresh but also keeps the number of status updates at a low level. In the comparison between TPAOI and conventional AoI, TPAOI has far fewer status updates on average about 48% than conventional Aol, demonstrating that TPAOI sends status updates only when necessary, thereby achieving higher efficiency.\nAs the parameter \u03bb gradually increases, the randomness of user access continuously decreases. This change directly leads to a monotonic decrease in the Aol for TPAOI. When is low, the main reason for the higher Aol is the limitation of transmission costs, which prevents the model from adapting to this increased randomness by increasing the frequency of status updates. Additionally, the higher Aol for QAoI is primarily because QAoI focuses on the arrival process of users but overlooks the impact of transmission delays, resulting in status updates being completed only after the user arrives. Conventional Aol's performance remains relatively stable because it measures the Aol at any moment; thus, as long as the user access process is periodic, the results remain unchanged.\nFig. 4(b) and 4(d) present the comparison of Aol and the number of status updates respectively when transmission delay fluctuation follows a normal distribution. Similarly to Fig. 4(a) and 4(c), TPAoI obtains better results, indicating that our model is capable of adapting to different transmission delay fluctuations and user dynamic behaviors.\nIn summary, the results indicate that our method not only optimizes the Aol, achieving superior timeliness of infor-mation, but also effectively controls the frequency of status updates, avoiding excessive communication.\nGiven the similar results between the exponential and nor-mal distributions, and considering space limitations, subse-quent experiments default to displaying results for the expo-nential distribution unless otherwise specified."}, {"title": "C. Correctness Evaluation", "content": "In this section, we compare TPAOI with adjusted QAoI and analyze the distribution graph of Aol relative to user access intervals in TPAOI, to illustrate how effectively TPAOI fits the dynamics that existed in the accessing phase.\nIn our experiment settings, according to the network topol-ogy, the total expected transmission time is slightly less than 12. Considering that fitting the probability distribution of net-work fluctuation may lead to some losses, we set the expected transmission time to 12. To align QAoI's transmission time with this expectation, we adjust it by adding an offset value, referring to it as the adjusted QAoI. To some extent, we use the adjusted QAoI as the correct reference.\nFig. 5 provides a detailed comparison between TPAOI and the adjusted QAoI. We can see that both TPAOI and the ad-justed QAoI metrics exhibit a decreasing trend as A increases. On average, the difference in AoI between TPAOI and the adjusted QAoI is only approximately 0.366. This indicates that, in terms of AoI, our algorithm effectively models the user access process, thereby optimizing Aol performance. It can be observed that when \u03bb is low, the adjusted QAoI has a lower Aol than the TPAOI. This is because, at low \u03bb values, adjusted QAoI sends more status updates, whereas TPAOI maintains a relatively constant number of status updates due to the system model's limitation on the number of concurrent updates on the link.\nTo verify whether TPAOI correctly guides the server's updating decisions (i.e., sending the status to AP before user access), we further count the updating frequency distribution concerning the accessing interval. Specifically, we conducted multiple simulations under conditions with \u03bb = 1.0 and \u03bb = 0.1 to collect tuples (user access interval s3, Aol s2), and to record their frequencies to describe the distribution. We scale the frequencies by dividing them by the maximum occurrence of access interval, to normalize them between 0 and 1.\nThe distribution of these tuples is displayed in Fig. 6, where different colors represent different frequencies. Taking Fig. 6(a) as an example, we analyzed the Aol evolution. Typically, users access the system when Aol is between 7 and 9, after which the access interval is reset to 1. Therefore, in most cases, when the access interval is 1, Aol is mostly between 8 and 10. As the interval increases from 1 to 15, Aol also increases, indicating that no status updates have been received during this period. When the interval increases from 15 to 20, the distribution of Aol shows two distinct parts, corresponding to situations with and without status updates. Notably, when the interval is at 20, Aol is usually distributed between 7 and 9, which marks the beginning of random access by users. This demonstrates that our model can complete status updates before a user accesses, thereby minimizing AoI at the time of user access.\nFig. 6(b) illustrates the scenario when the randomness of user access is greater, i.e., \u03bb = 0.1. It indicates that the status update arrives at AP before the user starts random access, i.e."}]}