{"title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models", "authors": ["Yuzhe Gu", "Ziwei Ji", "Wenwei Zhang", "Chengqi Lyu", "Dahua Lin", "Kai Chen"], "abstract": "Large language models (LLMs) exhibit hallucinations in long-form question-answering tasks across various domains and wide applications. Current hallucination detection and mitigation datasets are limited in domains and sizes, which struggle to scale due to prohibitive labor costs and insufficient reliability of existing hallucination annotators. To facilitate the scalable oversight of LLM hallucinations, this paper introduces an iterative self-training framework that simultaneously and progressively scales up the hallucination annotation dataset and improves the accuracy of the hallucination annotator. Based on the Expectation Maximization (EM) algorithm, in each iteration, the framework first applies a hallucination annotation pipeline to annotate a scaled dataset and then trains a more accurate hallucination annotator on the dataset. This new hallucination annotator is adopted in the hallucination annotation pipeline used for the next iteration. Extensive experimental results demonstrate that the finally obtained hallucination annotator with only 7B parameters surpasses the performance of GPT-4 and obtains new state-of-the-art hallucination detection results on HaluEval and HalluQA by zero-shot inference. Such an annotator can not only evaluate the hallucination levels of various LLMs on the large-scale dataset but also help to mitigate the hallucination of LLMs generations, with the Natural Language Inference (NLI) metric increasing from 25% to 37% on HaluEval. 1", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have shown remarkable capabilities in various tasks [10, 11, 31, 46, 51]. However, they tend to produce hallucination, i.e., plausible-sounding but unfaithful or nonsensical information [5, 26], that significantly hinders their real-world applications. Initial steps to address this issue involve the creation of datasets that can help to detect, annotate, and mitigate hallucinations [12, 25, 36]. Since the potential hallucinations of LLMs are in various fields, the spectrum of knowledge in the dataset is expected to be large-scale and comprehensive, covering various domains. Consequently, the size and diversity of datasets are critical for the oversight of LLM hallucinations.\nHowever, constructing and scaling-up hallucination annotation datasets face significant hurdles [8, 9, 25, 39]. One primary challenge is the prohibitively high costs and labor intensity required for their accurate assessment [39, 43], since the fine-grained hallucination annotation requires intensives labor for reading long documents and annotating the hallucination details sentence by sentence. Moreover, due to the insufficiency of accurate human annotations, the reliability of existing hallucination"}, {"title": "Related Work", "content": "Self-improvement of Large Models. As Large Language Models (LLMs) become more and more powerful, the community starts to explore different strategies to achieve the self-improvement of LLMs, i.e., to improve the LLMs using the supervision from LLMs. For example, existing works have explored self-alignment using LLMs with ethical principles [3, 52, 63]. There are also methods [22, 34, 50, 64] strengthen LLM's capabilities on tasks such as reasoning by training the LLMs on the high-quality responses from themselves on the same questions. In the field of computer vision, SAM [32] introduces manual and model-assisted labeling to expand the image segmentation dataset and enhance the performance of image segmentation models. However, the application of self-"}, {"title": "Method", "content": "This paper proposes an iterative self-training framework to simultaneously scale up the hallucination dataset and improve the accuracy of the hallucination annotator. We follow the analytical hallucination annotation (\u00a7 3.1) to annotate the hallucination sentence-by-sentence. The multi-iteration framework is theoretically grounded in the EM algorithm (\u00a7 3.2) and involves three stages to progressively scale the dataset in multiple dimensions (\u00a7 3.3). We also reveal how the hallucination annotators can be applied for hallucination evaluation and mitigation (\u00a7 3.4)."}, {"title": "Analytical Hallucination Annotation", "content": "The aim of a hallucination annotator is to identify hallucinations in the model responses. ANAH [25] developed a fine-grained annotation method that locates reference points in the document for each sentence and makes hallucination-type judgments, with the whole process completed in one turn of dialog. However, this hybrid task diverges from the human judgment processes and fails to clearly indicate the relationship between reference points and hallucination judgments, resulting in unsatisfactory annotation accuracy.\nInstead of using the original ANAH training prompts, we developed a more reliable training method tailored to the hallucination annotation process. As depicted in the lower right part of Fig. 2, the process is outlined in three phases: (1) Factual Existence Judgment, where the annotator assesses whether the provided sentence contains verifiable facts. If no factual content is present, the sentence is categorized as 'No Fact' and requires no further annotation. (2) Reference Information Extraction, where the annotator extracts relevant reference points from the documents related to the question and answer. (3) Hallucination-Type Judgment, where the annotator determines the type of hallucination based on the extracted reference points. If the sentence aligns with the references, it is classified as 'No Hallucination'. If it contradicts the references, it is deemed a 'Contradictory Hallucination'. If it lacks supporting evidence and cannot be verified, it is labeled as 'Unverifiable Hallucination'.\nThe above three phases will form a multi-turn dialogue in training data. Compared to the ANAH approach, which involves simultaneous judgments on multiple criteria, our phased process aligns more closely with human cognitive judgment processes. The detailed data format and prompts for our annotation process are in Appendix A."}, {"title": "Expectation-Maximization Algorithm", "content": "Simultaneously scaling up the dataset and improving the accuracy of the annotator can be formulated by the EM algorithm. For the input set X, we need to estimate two hidden variables simultaneously, the output set Y and the model parameters \u03b8. Specifically, based on the task formulation in \u00a7 3.1, we define the input x from the input set X of the hallucination annotator consists of a question, a sentence to be annotated, and a reference document. The expected output y to be estimated in the data output set Y includes the factual information f, the key reference points r from the reference document, and the type of hallucination h. We maximize the log-likelihood estimation of Y by alternately performing the E-Step and the M-Step to update the model parameters \u03b8:\n\u03b8 = arg max E_{p\u03b8(Y|X,\u03b8)} [log p\u03b8(X,Y | \u03b8)]                                      (1)\n\u03b8\nE-Step. A straightforward approach to estimating Y is to use a single model to predict annotations. However, this method lacks sufficient accuracy [41]. To improve the accuracy and stability of the estimation of Y, we introduce the self-consistency method [57], which provides a more robust representation of the distribution of the Y. As shown in Fig. 2. For each input x, we perform multiple samplings to yield K independent outputs y = {y\u00b9,\u2026\u2026\u2026, y\u00b2, \u2026\u2026\u2026, yK }, where the i-th output sample y' is composed of factual information (f\u00b2), reference point (r\u00b2) and hallucination type (h\u00b2). We use a self-consistency metric to select the most representative sample y* among all outputs:\ny* = (f*,r*, h*) = self-consistency(y)                             (2)\nDuring this selection process, we consider the hallucination type h, reference point r, and factual information f in turn. We determine the most common hallucination type h* by tallying a majority vote across all samples, denoted as h* = arg maxh \u22111 \u2161(h\u2081 = h). Then, we form the candidate reference set R by taking the corresponding r from the output containing the h*. We select the most \"consistent\" reference point r* by comparing the cosine similarities. For each r\u00b2 in R, we first calculate its average cosine similarity with the other elements in R. After that, we select the reference point r* with the highest average cosine similarity: r* = arg maxr\u2211i\u2208R(n -1\nj=1,j\u2260i sim(r\u00b2, ri)).\nFinally, with (r*, h*), we can uniquely select the corresponding f*.\nM-Step. Following the robust estimation in the E-step, the M-step updates the model parameters to maximize the likelihood of the selected output y*. Combining Eq. 1 and Eq 2, we formulate the parameter update strategy at iteration t:\n\u03b8^{t+1} = arg max E_{x~x} [E_{y~p\u03b8t (y|x,\u03b8)} [log p\u03b8(x, y* | \u03b8)]]                                     (3)\n\u03b8"}, {"title": "Multi-dimensional Data Scaling", "content": "Grounded in the EM algorithm, our framework operates in an iterative manner. This multi-iteration process acts as a data growth flywheel to progressively scale up the dataset in multiple dimensions, consisting of three stages:\nStage 1: Seed Data and Basic Annotator. We utilize ANAH dataset [25] as our seed data, which includes over 700 topics and around 4,300 LLM-generated questions and responses. For each response, ANAH provides the hallucination type for every sentence, determined through a human-in-the-loop approach. We train an initial hallucination annotator, noted as ANAH-v2 Stage1, with this seed data using the annotation method described in \u00a7 3.1.\nStage 2: Scaling up in Response Dimension. In Stage 1, for each question, ANAH provides responses that GPT-3.5 generates with the reference document, while InternLM-7B generates without any reference document. We first augment the dataset's model responses by collecting responses to the same existing questions from 13 additional open-source models of various sizes and series. For each model, responses were collected with and without knowledge of reference documents. The prompt details are in Appendix B. After filtering out similar model responses, these responses are annotated sentence by sentence using the self-consistency pipeline with ANAH-v2 Stage1. The newly annotated data, combined with the seed data, was used to train ANAH-v2 Stage2.\nStage 3: Scaling up in Topic Dimension. We expand the topic coverage along four categories: location, person, event, and thing, paralleling ANAH's configuration. For each topic, we generate several questions based on the provided reference documents (more details in Appendix B). Then, we use the same method in Stage 2 to collect responses from multiple models and annotate the response following the same procedure as in Stage 2, using ANAH-v2 Stage2 annotator. The resulting dataset, combined with data from the previous stages, is used to train the ultimate annotator version.\nOveral Statistics. The final dataset encompasses both over ~3k topics, ~196k model responses, and ~822k annotated sentences, in English and Chinese (Tab. 1). The topics cover celebrities, events, locations, and things, and span a wide array of domains, such as politics, health, and sports (Fig. 3). The statistics underscore the comprehensiveness and extensive scale of our dataset."}, {"title": "Applications", "content": "Hallucination Evaluation. As the accuracy of the hallucination annotators becomes satisfactory, we can apply it to automate the process of evaluating the hallucination levels of existing open-source models. After categorizing sentences into four distinct types (introduced in \u00a7 3.1), we consider type Contradictory and Unverifiable Hallucination as sentences with hallucinations, and type No Fact and No Hallucination as sentences without hallucinations. This tool enables researchers to assess the reliability and accuracy of generated texts, ensuring models can be responsibly integrated into practical applications.\nHallucination Mitigation. We further show a simple re-ranking strategy to mitigate the LLM's hallucinations with the annotator, whereas more advanced strategies can be explored in future research. Specifically, we adopt our annotator @ for response re-ranking. LLM first generates N candidate responses {G1,\u2026\u2026,GN} by top-k sampling. Then we select the best response G* with the lowest"}, {"title": "Experiment", "content": "Implementation. In our experimental framework, we adopt the pre-trained InternLM2-7B [7] model to fine-tune the hallucination annotator. Further implementation details can be found in Appendix C.\nEvaluation. We use a subset of the ANAH [25] data as a test set, which is not used for training in stage 1. To assess the performance of the annotator in predicting hallucination types, we utilize F1 and Accuracy. We also employ RougeL [38] and BertScore [66] to compare the generated text with gold-standard human reference in terms of gram, continuity, order and semantics."}, {"title": "Overall Results", "content": "The last 3 rows of Tab. 2 illustrate the performance of ANAH-v2 at each stage of Data Scaling in \u00a7 3.3. The performance progressively improves with the increasing dataset number (see in Tab. 1) in successive stages. This trend underscores the scalability and effectiveness of our hallucination annotation framework. Remarkably, ANAH-v2 surpasses GPT-4 with the F1 of 87.78% and the accuracy of 88.03% at Stage 2 3. Eventually, we achieve the F1 of 89.30% and the accuracy of 89.55% at Stage 3.\nWe also observe that ANAH-v2 already outperforms ANAH-20B at Stage 1 (84.85% v.s. 81.01% in accuracy) with only 7B parameters, when being trained on the same hallucination corpus. This superior performance is attributed to the innovative multi-turn dialogue training strategy (\u00a7 3.1)."}, {"title": "Ablation Studies", "content": "Impact of Self-Consistency. To verify the effectiveness of self-consistency during inference in E-Step (introduced in \u00a7 3.2), we compare the performance of the annotator with different self-consistency settings in Tab. 3. When the annotator model with the same training data at each data scaling stage, the inference strategy with self-consistency (w/ SC) consistently outperforms without self-consistency (w/o SC), where the annotator generates only once for each input. Therefore, self-consistency improves the accuracy and stability of the estimation of hallucination annotations.\nIn M-Step, we train the model on data from the E-Step of the preceding iteration. We observe that when the annotator model with the same inference strategy, the model trained on self-consistently processed data (w/ SC) surpasses the performance with data generated through a single pass (w/o SC). This finding indicates that training data processed through self-consistency leads to a stronger\nImpact of Progressive Data Scaling. To assess the impact of progressive data scaling (introduced in \u00a73.3), we compare the performance of annotators with different types of data scaling in Tab. 4. In our progressive approach, the updated annotator from Stage 2 is employed to annotate the responses from additional topics, continuously enriching the training data. Conversely, in the non-progressive approach, the basic annotator from Stage 1 is employed to generate annotations for the additional training data during Stage 3. With the same size of training data, the annotator trained on non-progressive data scaling underperforms that with our progressive data scaling, proving the effectiveness of our progressive data scaling.\nImpact of Training Strategy. We also analyze different training strategies for annotators in different data scaling stages in Tab. 5. In our default training process, we mix the newly annotated data with old data to re-train an annotator. Alternatively, we only use the newly annotated data to further train the annotator model from the previous stage. The results demonstrate that our training strategy with mixed training data performs better than further training with new data. The integration of different data qualities across training stages improves the robustness of the annotator model."}, {"title": "Generalization Capability Analysis", "content": "We further validate the effectiveness of ANAH-v2 on other hallucination detection datasets using two third-party datasets: HaluEval [36] for English and HalluQA [12] for Chinese. Each dataset provides four components: questions, reference documents, responses, and labels indicating whether the responses contain hallucination. For each question, we let ANAH-v2 judge the type of responses containing and not containing the hallucination separately. Note that in HaluEval we only use the QA samples and in HalluQA, we only use the samples that provide a textual reference document, which aligns with our annotator's designed setting."}, {"title": "Application", "content": "Hallucination Evaluation Benchmark. Our ANAH-v2 dataset and annotator can serve as a benchmark for the hallucination levels in generated texts by existing models. As shown in Tab. 7, we evaluate the performance of various LLMs, including InternLM2 [7], Qwen1.5 [2], Baichuan2 [4], Mistral [29, 30], DeepSeek-LLM [6], and Llama2 [53], spanning different model sizes. We also offer detailed evaluation results on different languages and categories of topics to deepen our understanding.\nWe find that all models exhibit superior performance in English compared to Chinese, underscoring the need for further research to understand and mitigate language-dependent discrepancy. The performances of all models with reference documents are better than those without. Qwen1.5-14B achieves the lowest hallucination rate when using reference documents (5.33%) and Deepseek-67B achieves the lowest hallucination rate when reference documents are not provided (47.17%). Moreover, we find no clear trend in the performance distribution across four categories of topics. In addition, the results of different stages of annotators in Tab. A1, A2, and 7 show that there is a consistent trend and fixed biased ordering relationship between LLMs, thus confirming the reliability of our assessment method. More details are in Appendix D.\nHallucination Mitigation. Besides being used to measure hallucination levels, ANAH-v2 can also be used to mitigate hallucinations. We use the QA samples from HaluEval, which comprises questions and correct answers from HotPotQA [62]. We use two models InternLm2-7B and LLaMA2-7B. For each model, we generate 36 candidate responses by top-k sampling (k=40), then re-rank the responses using our annotator. To quantify the hallucination degree, we employ RougeL, BertScore, NLI, and QuestionEval. These metrics measure the congruence between the generated responses with the golden responses and/or reference documents.\nResults in Tab. 8 show a clear reduction of hallucination levels after the re-ranking process via our annotator. For instance, the NLI metric for LLaMA2-7B shows a notable increase, rising from 25.00% to 37.01%. This suggests that the application of our annotative approach can significantly mitigate the issue of hallucinations in language model outputs."}, {"title": "Conclusion and Future Work", "content": "In this paper, we aim to explore a scalable framework for the oversight of LLM hallucinations. Through iterative self-training, we progressively expand the diversity and scale of the dataset and improve the accuracy of the hallucination annotator. The finally obtained ANAH-v2, for the first time, outperforms GPT-4 in various hallucination detection benchmarks with only 7B parameters and obtains superior zero-shot performance on third-party hallucination detection benchmarks. ANAH-v2 not only provides an automatic hallucination evaluation benchmark with the scaled dataset, which paves the way for future research on hallucination mitigation but also exhibits potential in hallucination mitigation by the simple re-ranking strategy. We believe ANAH-v2 can also benefit more hallucination mitigation strategies such as fine-grained RLHF.\nWith the large-scale dataset as seed data, future work can explore creating hallucination annotation data in other NLG tasks such as dialogue generation. Another direction is to improve the generalizability of the annotator across different languages, tasks, and topics."}, {"title": "Training Prompt", "content": "As described in \u00a7 3.1, our annotation process consists of three phases: (1) Factual Existence Judgment via the prompt in Fig. A1, (2) Reference Information Extraction via the prompt in Fig. A2 (3) Hallucination-Type Judgment via the prompt in Fig. A3."}, {"title": "Data Scaling Details", "content": "As described in \u00a7 3.3, we collect model responses via Fig. A5. The open-source models include InternLM2(7B&20B) [7], Baichuan2 (7B&13B) [4], LLama2 (7B&13B) [53], Qwen1.5 (7B&14B&72B) [2], Deepseek (7B&67B) [6], and Mistral (7B&7\u00d78B) [29, 30].\nWe automate the topic selection based on occurrence frequency via Google Ngram Viewer 5 and retrieve corresponding reference documents from pre-training databases [24].\nWe generate questions on each topic via Fig. A4."}, {"title": "Implementation Details", "content": "In our experimental framework, we adopt the pre-trained InternLM2-7B [7] model to fine tuning the hallucination annotator.\nIn E-Step, we generate responses by implementing sampling via the LMDeploy library [15]. During each iteration, we generate 32 candidate responses per input and apply a self-consistency quality control mechanism to them. The decoding strategy involves the top-k (k = 40) sampling with a temperature of 0.8.\nIn M-Step, we train the annotator model with the following settings and hyper-parameters: the epoch is 1, the learning rate is 1e-5, and the AdamW optimizer is with a linear scheduler, the maximum sequence length is set to 32k. Additionally, following the configuration in ANAH [25], we perform a multi-task setting where additional tasks such as dialogue generation from ShareGPT [45] and Dolly [14] are integrated with the fine-grained hallucination annotation. Our model is trained on 32 NVIDIA A100 GPUs."}, {"title": "Hallucination Evaluation", "content": "To assess the reliability of our hallucination annotator, we measure the hallucination levels of the above LLMs using the annotator from different stages. Tab. A1, A2, and 7 show the results measured by annotator ANAH-v2 from Stage 1, 2, and 3, respectively. The trends in these three tables are consistent where Qwen1.5-14B achieves the lowest hallucination rate with reference documents and DeepseekLM-67B achieves the lowest hallucination rate without reference documents. This consistency and fixed biased ordering relationship between LLMs confirm the reliability of our assessment method."}, {"title": "Limitation", "content": "Although this study presents a novel multi-iteration self-training framework for the scalable oversight of LLM hallucinations and achieves significant improvements in hallucination annotation, there are some limitations.\nDespite the progressive scaling and increasing accuracy of the hallucination annotator, there may still exist a non-negligible margin of error in the annotations. This margin could affect the convergence of the model and the quality of the final hallucination annotator. Furthermore, the success of our framework is measured largely by its performance on our own dataset and other benchmarks such as HalluEval and HalluQA. However, these datasets might not encompass the full spectrum of real-world scenarios where hallucinations pose a problem. Lastly, this work primarily uses InternLM2-7B as the backbone of the hallucination annotator. Other different underlying models and different numbers of parameters are not explored."}, {"title": "Broader Impacts", "content": "By exploring the hallucination annotation and mitigation in LLMs, this paper contributes to the development of more reliable and trustworthy AI technologies. Our innovative multi-iterative self-training framework significantly reduces the reliance on expensive and time-consuming manual annotations by automating the hallucination detection process. Our hallucination annotator offers a benchmark for the research community evaluating the hallucination levels of existing open-source models. Additionally, we provide a large-scale and diverse dataset from which the broader research community can benefit, fostering further innovation and study in this domain."}]}