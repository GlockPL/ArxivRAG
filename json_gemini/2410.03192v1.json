{"title": "MultiVerse: Efficient and Expressive Zero-Shot Multi-Task Text-to-Speech", "authors": ["Taejun Bak", "Youngsik Eom", "Seungjae Choi", "Young-Sun Joo"], "abstract": "Text-to-speech (TTS) systems that scale up the amount of training data have achieved significant improvements in zero-shot speech synthesis. However, these systems have certain limitations: they require a large amount of training data, which increases costs, and often overlook prosody similarity. To address these issues, we propose MultiVerse, a zero-shot multi-task TTS system capable of performing TTS and speech style transfer in zero-shot and cross-lingual conditions, while requiring much less training data than traditional data-driven approaches. To ensure zero-shot performance even with limited data, we leverage source-filter theory-based disentanglement, utilizing the prompt for modeling filter-related and source-related representations. Additionally, to further enhance prosody similarity, we adopt a prosody modeling approach combining prompt-based autoregressive and non-autoregressive methods. Evaluations demonstrate the remarkable zero-shot multi-task TTS performance of MultiVerse and show that MultiVerse not only achieves zero-shot TTS performance comparable to data-driven TTS systems with much less data, but also significantly outperforms other zero-shot TTS systems trained with the same small amount of data. In particular, our innovative prosody modeling technique enables MultiVerse to generate speech with a high degree of prosody similarity to the given prompts. Our samples are available at https://nc-ai.github.io/speech/publications/multiverse/index.html", "sections": [{"title": "1 Introduction", "content": "Deep learning-based text-to-speech (TTS) has advanced to the point of synthesizing human-like speech (Wang et al., 2017; Ren et al., 2019). However, recent research has been extended beyond this scope, exploring various ways of broadening the application of speech synthesis models. Representative tasks include synthesizing unseen speaker's speech, known as zero-shot TTS (Jia et al., 2018; Cooper et al., 2020), generating speech in a language that the monolingual target speaker has not seen, referred to as cross-lingual TTS (Cho et al., 2022; Zhang et al., 2019; Xin et al., 2021), and transferring the prosody of a speech reference to a target speaker, known as speech style transfer (Lee et al., 2021; Huang et al., 2022). Furthermore, recent TTS research has integrated the zero-shot task with cross-lingual or style transfer tasks (Casanova et al., 2022; Za\u00efdi et al., 2022).\nTo expand TTS applications in zero-shot conditions, it is crucial to ensure generalization across various speech components, such as content, style, and speaker identity. Disentangled representations facilitate this by enabling the system to capture interpretable and controllable features, thereby improving generalization of TTS systems through the learning of these individual components (Bengio et al., 2013; Lipton, 2018). However, due to the often entangled nature of these components, separately learning their general characteristics remains challenging.\nData-driven methods are a representative approach to learning generalized acoustic components from large-scale speech datasets. Specifically, these methods scale up the TTS system to train on massive datasets, making them robust under unseen conditions (Wang et al., 2023; Zhang et al., 2023; Le et al., 2023). Additionally, Jiang et al. (2023, 2024); Shen et al. (2023); Ju et al. (2024) adopt disentangled modeling to separately learn acoustic components. Disentangled modeling contributes to ensure generalization in each component by independently encapsulating interpretable elements. However, a significant amount of training dataset is required as the decoder needs to learn the relationships between disassembled elements. Preparing large-scale data is especially challenging for minority languages. Moreover, even with large-scale training data, there is still potential room for improvement in prosody similarity in zero-shot scenarios.\nIn this paper, we introduce a multi-task TTS system, called MultiVerse, enabling speech synthesis and speech style transfer in zero-shot and cross-lingual conditions, requiring significantly less data compared to the data-driven approaches and featuring enhanced prosody modeling. MultiVerse enhances training efficiency by source-filter theory-based decomposed modeling (Fant, 1970). Specifically, MultiVerse decomposes speech generation into filter- and source-related representation generation, with prompt speech utilized in the modeling of each representation. Notably, both representations result in features that have a similar distribution to the mel-spectrogram (Bak et al., 2021). Therefore, it is suitable for the decoder to learn the interdependent relationship between the representations even with small data. Furthermore, MultiVerse introduces an effective style modeling method. While several zero-shot models have modeled either the acoustic features (Shen et al., 2023) or prosody latents to capture speech style (Jiang et al., 2023), MultiVerse utilize both autoregressive (AR) based acoustic feature modeling and Non-AR based prosody modeling.\nWe evaluate MultiVerse's performance in zero-shot TTS tasks under various conditions. Regarding language, we evaluate both intra- and cross-lingual speech synthesis and measure naturalness and similarity in neutral and expressive speech style. Additionally, we compare our model with a large-scale TTS model using data-driven methods and a speech style transfer model. Evaluation results demonstrate that MultiVerse has the following advantages: (1) Zero-shot intra- and cross-lingual TTS is achievable with a small amount of training data. MultiVerse can achieve similar zero-shot synthesis in both timbre and prosody with only 6% of the training data compared to VALL-E (Wang et al., 2023). (2) The proposed prosody modeling is also effective in reflecting conditions in various tasks of speech synthesis. Even without requiring information about the content or phoneme duration of the prompt speech, it can reflect prosody similar to the prompt in both intra-lingual and cross-lingual settings."}, {"title": "2 MultiVerse", "content": "MultiVerse models speech by disassembling it into two components: filter and source. The proposed model comprises three main modules: (1) an acoustic model based on the source-filter theory (Fant, 1970) that generates mel-spectrograms given text and speech prompts, (2) an AR prosody predictor that predicts prosody-related acoustic features (duration, pitch, energy) from input conditions, and (3) a discriminator for adversarial training."}, {"title": "2.1 Source-Filter Theory Based Decomposed Modeling", "content": "Inspired by the source-filter theory, which explains the response between the vocal tract filter and the sound source, the proposed acoustic model generates mel-spectrogram through the filter and source generators. The filter generator is tasked with producing the vocal tract filter-related representation, while the source generator outputs the source-related representation. We name these two representations as filter representation and source representation, respectively. Both representations are obtained from feed forward transformer-based generator (Ren et al., 2019).\nFilter representation We consider filter representation as hidden states that contain information related to speech content, pronunciation, and speaker identity, but is less dependent on prosody. This representation, obtained by the filter generator, is modeled by taking phoneme representation as input, along with the energy embedding. Both input features are upsampled by the gaussian upsampling (Shen et al., 2020).\nSource representation We consider source representation primarily as hidden states that contain prosodic information, such as intonation, rhythm, and stress patterns, with low dependence on content. The source generator produces source representation from frame-wise upsampled phoneme-level pitch and energy embeddings. During training, it generates the source representation from ground-truth acoustic feature embeddings, while during inference, it utilizes predicted acoustic feature embeddings. We provide further experimental analysis on both representations in Appendix B.\nChoi et al. (2021); Bak et al. (2021) also utilized source-filter based speech disentanglement. Unlike existing methods, our approach adopts prompt-based modulation in modeling both representations. Since both vocal tract filter and sound source are influenced by speaker characteristics, the prompt speech is reflected in both generators. The mel-spectrogram of prompt speech serves as the input for obtaining hidden states from the speech prompt encoder. Parameters for the FiLM (Perez et al., 2018) layer are predicted from the cross-attention output between the input of generators and the output of speech prompt encoder as in (Shen et al., 2023). Subsequently, the FiLM layer modulates the representations within the generators."}, {"title": "2.2 Increasing Filter Capacity of the Acoustic Decoder", "content": "The intermediate representation formed by combining both representations, referred to as coarse mel-representation, resembles the interaction between the vocal tract filter and the sound source (Bak et al., 2021). Since this fusion follows the frequency response in the source-filter model, the coarse mel-representation is closely related to a high-dimensional features of speech. Consequently, the acoustic decoder has an advantage in learning the interdependent relationship between the filter and source representations.\nTo produce mel-spectrograms while preserving various types of information such as speech content and style within the coarse mel-representation, it is necessary to increase the filter capacity of the acoustic decoder. To increase the filter capacity, the acoustic decoder's transformer block replaces convolution layers with sample-adaptive kernel selection-based convolution layers (Kang et al., 2023). It aims to find suitable convolution filters for the speech prompt. Specifically, learnable filters of each convolution layer are weighted sums based on predicted weights from the global style embedding. The aggregated filter is then modulated and de-modulated (Karras et al., 2020). The global style embedding is derived from the pre-trained speaker encoder. More detail information about sample-adaptive kernel selection is described in Appendix C."}, {"title": "2.3 Two-stage Prosody Modeling", "content": "MultiVerse's prosody modeling consists of two stages: first, the prosody predictor models the acoustic features autoregressively; second, the source generator models prosody in the latent space non-autoregressively using these acoustic features."}, {"title": "2.3.1 Autoregressive Prosody Modeling", "content": "The proposed AR prosody predictor models the time-varying distribution of acoustic features (duration, pitch, energy) as a conditional language modeling task. The goal of the AR prosody predictor is prediction of acoustic units suitable for the given text and prompt conditions. Due to the time-dependent nature of prosody and the need to model large-variations in prosody, we adopt an AR approach (Kharitonov et al., 2022). The prosody predictor is trained to predict acoustic units $c_t = {d_t, p_t, e_t}$ corresponding to phoneme sequences $x = {x_1, x_2, ..., x_T}$, where d, p, e are the duration, pitch, and energy unit sequences, respectively. These unit sequences of which each value corresponds to an index are obtained by quantizing normalized acoustic sequences. Prosody modeling, which is conditioned on the speech prompt r and the phoneme sequence x, is written as the following equation:\n$p(c|x, r; \\theta_{ARP}) = \\prod_{t=0}^T p(c_t|c_{<t}, x, r; \\theta_{ARP}),$ (1)\nwhere $\\theta_{ARP}$ represents the parameters of AR prosody predictor. To model the prosody using a prompt-based in-context learning, we utilize the phoneme sequence and the prompt as a prefix, a similar approach to Wang et al. (2023).\nThe AR approach is also utilized in the prosody latent language model (P-LLM) in Mega-TTS (Jiang et al., 2024), which autoregressively models vector-quantized codebook (Van Den Oord et al., 2017) of prosody hidden states. However, the performance of vector quantization depends on the quantity and diversity of training data (Gersho and Gray, 2012). In contrast, the AR prosody predictor, which models acoustic feature units, is data-efficient. We provide further analysis on the limitation of VQ based modeling in Appendix D. Additionally, the P-LLM is limited to specific prompt speech, such as the particular languages present in the training data, and requires alignment information of the prompt. Conversely, the AR predictor does not impose restrictions on the utilization of prompt speech."}, {"title": "2.3.2 Non-Autoregressive Prosody Modeling", "content": "Non-AR prosody modeling refines prosody at the frame-level from time-dependent prosody features. In this process, the source-filter generator converts acoustic feature embeddings into the source representation, reflecting the prosody characteristics of the prompt by the attention mechanism and the modulation."}, {"title": "2.4 Learning Objectives", "content": "The learning objectives consist of three components: reconstruction loss, adversarial loss, and acoustic feature loss. The reconstruction loss is the L1 loss between the generated and the ground-truth's mel-spectrogram. The adversarial loss utilizes LSGAN (Mao et al., 2017), incorporating a multi-window discriminator (Chen et al., 2020; Ye et al., 2022) with 2D patch unit lengths {32, 64, 128}. The acoustic feature loss computes the sum of cross-entropy losses for each acoustic feature, comparing the output units of the prosody predictor with the ground-truth acoustic units."}, {"title": "2.5 Multi-Task TTS", "content": "The proposed model can perform multiple tasks according to the input condition. First, zero-shot TTS takes an unseen speaker's prompt as input. Second, cross-lingual TTS is accomplished by using different languages for the speech prompt and input text. Additionally, the speech style transfer enters two different prompts into different modules, as illustrated in Figure 2. These three tasks can be combined with each other. For example, zero-shot cross-lingual TTS or zero-shot style transfer, and even all three tasks can be performed at once, namely zero-shot cross-lingual speech style transfer. Detailed inference process are provided in Appendix F."}, {"title": "3 Experimental Environments", "content": "Training datasets Training datasets consist of English and Korean speech datasets. We used the open datasets LibriTTS (train-clean-100, train-clean-360) (Zen et al., 2019) and VCTK (Yamagishi et al., 2019) and an internal dataset as the English dataset. As the Korean dataset, we used the open dataset AI-Hub\u00b9 with multi-style and an internal dataset. Specification for datasets are provided in Appendix E. We re-sampled all speech data to a 22.05 kHz sampling rate, and obtained an 80-band mel-spectrogram as the acoustic feature. The Short-Time Fourier Transform (STFT) parameters included a bin size of 1024, with window size of 1024 and a hop sizes 256.\nEvaluation datasets We diversified the composition of the evaluation dataset. The dataset for evaluating zero-shot performance is made up of utterances from speakers not included in the training. Specifically, it is divided by language (English and Korean) and speaking style (neutral and expressive). For the English dataset, the neutral style is represented by LibriTTS dev-clean and the expressive style is represented by EXPRESSO (Nguyen et al., 2023). Four styles (confused, enunciated, happy, and sad) were selected from the various styles available in EXPRESSO. The Korean dataset includes the neutral style from the AI-Hub multi-speaker dataset and the expressive style from an internal dataset. The expressive internal dataset includes voices in emotional and theatrical styles."}, {"title": "5 Conclusion", "content": "This paper introduces MultiVerse, an efficient and expressive zero-shot multi-task TTS system designed to address the limitations of existing zero-shot TTS systems that depend on large-scale training datasets. MultiVerse employs a structure that disentangles speech components into filter and source representations: this structure contributes to achieving zero-shot TTS performance comparable to data-driven TTS approaches, even with a small amount of data. Additionally, it enhances prosody similarity through a hybrid prosody modeling that combines both autoregressive and non-autoregressive mechanisms. Quantitative and qualitative evaluations across various language and style demonstrate that MultiVerse excels in zero-shot, cross-lingual TTS, and speech style transfer."}, {"title": "6 Limitations", "content": "MultiVerse, which generates mel-spectrograms, requires a system to convert the mel-spectrograms into waveforms, such as a neural vocoder. Therefore, the performance of the vocoding system can potentially affect the performance of MultiVerse. Additionally, utilizing pre-processed acoustic features, i.e., duration, pitch, and energy, becomes more costly as the amount of training data increases. Hence, one of the next goals of this research could be to incorporate unsupervised modeling methods for acoustic features."}, {"title": "7 Broader Impacts", "content": "We aimed to enhance the versatility of deep-learning-based TTS models. While speech generative models offer valuable support for creating digital content, concerns arise about their potential misuse for fraud and crime. This study is designed to minimize these potential negative impacts and effectively support TTS models for content creators. We emphasize ethical considerations, especially regarding data privacy, by ensuring that all voice data used in training and evaluation is sourced from publicly available datasets or internal datasets with the explicit consent of the speakers. Moreover, the study looks ahead to future societal implications, striving to expand the capabilities of TTS models in a manner that aligns with ethical and social responsibilities related to content creation."}, {"title": "A Related Works", "content": "Zero-shot TTS Zero-shot TTS synthesizes speech for unseen speakers not present in the training dataset. Learning general speech features, like speaker identity and speaking style, is crucial in this task. Zero-shot TTS models often incorporate a pre-trained speaker encoder for modeling speaker information (Jia et al., 2018; Kumar et al., 2021; Cooper et al., 2020). Some employ specific objectives to improve speaker similarity (Casanova et al., 2021, 2022). However, challenges arise when generating voices significantly different from the training data, impacting similarity and naturalness. Recent advances in language models have prompted exploration of data-driven methods in speech synthesis (Wang et al., 2023; Borsos et al., 2023; Kharitonov et al., 2023; Shen et al., 2023), enhancing generalization to unseen voices. Despite this, acquiring large speech datasets is costly and challenges persist in obtaining diverse data for various languages. Existing models have primarily focused on speaker similarity, leaving the issue of prosody similarity unresolved.\nCross-lingual TTS Cross-lingual TTS aims to generate speech in a language that is different from the monolingual speaker while preserving speaker's voice. However, training on multilingual multi-speaker data may entangle language and speaker information, resulting in diminished similarity. Therefore, disentangling the language from the speaker becomes crucial in cross-lingual TTS. Zhang et al. (2019); Xin et al. (2020) propose adversarial layers to disentangle speaker and language information. Xin et al. (2021) utilize mutual information minimization to decouple the information. Despite the objective of disentanglement, these models suffer from unstable training, creating a trade-off between disentanglement and speaker similarity (Zhang et al., 2019). Recently, data-driven methods have also been applied to improve the generalization in cross-lingual TTS (Zhang et al., 2023).\nSpeech style transfer Speech style transfer aims to synthesize speech with a speaking style similar to the reference speech, notwithstanding differences like identity or content. Style transfer models learn to model inherent elements of voice style, disentangling these elements from content and speaker identity (Lee et al., 2021; Za\u00efdi et al., 2022; Huang et al., 2022; Yuan et al., 2020). Lee et al. (2021) disentangles acoustic features (duration, prosody, energy) by encoding them separately. Daft-Exprt (Za\u00efdi et al., 2022) uses domain adversarial training to separate prosody and speaker information. Huang et al. (2022) proposes mix-style layer normalization to remove style information from filter representation. While these studies enhance style transfer performance, their limited disentanglement hinders style transfer in out-of-domain environments (Sigurgeirsson and King, 2023)."}, {"title": "B Analysis on Filter and Source Representations", "content": "MultiVerse adopts source-filter theory-based decomposed modeling to learn disentangled representations which are divided into filter and source. As described in Section 2, the filter generator produces the filter representation related to speech content, pronunciation and accent. On the other hand, the source generator produces the source representation that contains prosodic information that is less correlated to the content, such as intonation, rhythm, and stress patterns.\nTo analyze these representations in detail, we conducted objective evaluation. In this experiments, audio samples were generated by passing both representations individually through the acoustic decoder, not forming the coarse mel-representation.\nThe CER and WER results indicate that the synthesized speech from the filter representation has comparable intelligibility to that generated by MultiVerse. This means that the filter representation primarily contains linguistic information of input text. Additionally, SECS result shows that the filter representation is more related to the speaker identity than the source representation. Meanwhile, the synthesized speech from the source representation is sounding like mumble sounds \"Hmmmm, Mmmmmm ...\"; ASR model failed to recognize the speech. However, it has more similar pitch distribution than that from the filter representation because the source representation, generated from acoustic features, learns the prosodical patterns included in the prompt speech."}, {"title": "C Detailed on Acoustic Decoder", "content": "The proposed acoustic decoder employs sample-adaptive kernel selection (Kang et al., 2023) to learn convolutional filters suitable for the speech prompt. This approach generates a mel-spectrogram while preserving the information of the coarse mel-representation.\nThe specific process is as follows: the mapping network maps the style vector from a global style embedding and random noise sampled from a normal Gaussian distribution. The K-bank convolutional filters of each sample-adaptive convolution layer are aggregated by a weighted sum based on the weights predicted for each kernel by the style vector. Subsequently, the aggregated filter undergoes modulation and demodulation by scale, where the scale is obtained from the output of an affine layer with the style vector as input (Karras et al., 2020).\nThe proposed acoustic decoder replaces the convolutional layer of the feed-forward transformer block with a sample-adaptive kernel selection-based convolutional layer. ReLU activation function is used for non-linearity between the two sample-adaptive convolutional layers. Detailed parameter descriptions for sample-adaptive kernel selection are provided in Table 7."}, {"title": "D Comparison with Vector Quantization based Prosody Modeling", "content": "The P-LLM of Mega-TTS also leverages autoregressive language model-based prosody modeling. However, as mentioned in Section 2.3.1, the approach using vector quantization-based codebooks is influenced by the quantity of training data. To verify this, a simple comparison was conducted. We trained a model, referred as MultiVerse(VQ), by replacing MultiVerse's prosody modeling with Mega-TTS's VQ encoder. Both MultiVerse and MultiVerse(VQ) synthesized speech from the test dataset, using ground-truth mel-spectrogram as prompts and ground-truth phoneme durations. This experimental setup aimed to observe the performance of vector quantization-based models with limited training data.\nWe observed that the synthesized speech by MultiVerse(VQ) occasionally resulted in distorted audio, as depicted in Figure 8. MultiVerse(VQ) produced more distorted speech when targeting expressive style."}, {"title": "E Detailed Dataset Information", "content": "Table 6 describes detailed dataset information. The English training set was constructed from approximately 262 hours of speech data. The LibriTTS (Zen et al., 2019), VCTK (Yamagishi et al., 2019), and internal datasets were recorded from 1133, 101, and 42 speakers, respectively13. The speech styles included both neutral narration and conversational styles. The Korean dataset consisted of a total of 969 hours of speech data, with the AI-Hub multi-speaker14 and internal datasets recorded from 46 and 229 speakers, respectively. It included various speech styles, such as narration and acting. 10% of all datasets were excluded from the training set for testing purposes."}, {"title": "F Inference Details", "content": "In this section, we provide the inference process in detail. For zero-shot TTS, both input text and a prompt voice are required. In this case, a mismatch between the text and prompt is permissible. Whether the input text and prompt voice's language match determines if it is intra-lingual or cross-lingual TTS. At first, the input text is processed to obtain the output x from the text encoder."}, {"title": "G Model Configuration", "content": "Table 7 describes detailed information of hyperparameters of modules in MultiVerse."}, {"title": "\u0397 Acoustic Feature Pre-Processing", "content": "To obtain sequences of acoustic units for training, acoustic features, such as duration, fundamental frequency (Fo), and energy, were pre-processed using the following procedures. Each acoustic feature sequence was calculated, followed by normalization and quantization, to be transformed into acoustic unit sequences. Specifically, the procedures for each acoustic feature is as follows: For duration, to obtain the duration per phoneme, we used both the internal forced aligner and an external aligner (McAuliffe et al., 2017) for Korean and English, respectively. The duration sequences were already in integer values, so we used them directly as duration unit sequences without additional normalization and quantization. We set the maximum duration value to 32. Fo extraction from the speech signal utilized a Praat-based extractor15. The Fo sequences, extracted in Hertz units, were averaged per phoneme, then were normalized using speaker-specific statistical information. The normalized Fo sequence values were quantized into 64 values within a certain range to obtain the pitch unit sequence. In this paper, the normalized Fo sequence was clipped to the range from -4 to +4. For energy, frame-wise energy of the speech signal was calculated as the magnitude of the linear spectrogram and averaged per phoneme. The normalized energy sequence was quantized into 64 values within the range of -5 to +5."}, {"title": "I Details on the Subjective Evaluation", "content": "For the subjective evaluation of samples, we performed three kinds of listening tests: the naturalness (N-MOS), speaker similarity (S-MOS), and prosody similarity (PS-MOS). Amazon Mechanical Turk (MTurk)16 and internal evaluation tools were used for the evaluation of the English and Korean samples, respectively. For each task (intra-lingual neutral/expressive, cross-lingual neutral/expressive), 15 test samples for English and 10 test samples for Korean were randomly selected per model. 10 native English participants and 24 native Korean participants rated the audio samples. Evaluation scores were evaluated at 0.5-point intervals from 1 to 5 points. We guided participants to evaluate audio samples by focusing only on the evaluation factor while ignoring other factors. In N-MOS test, the quality of the sound is ignored and only the naturalness of the speech is evaluated. For S-MOS test, we emphasized that participants should concentrate only on determining how closely related synthesized speech and prompt speech, disregarding content and prosody. Meanwhile, we requested participants to assess how similar the prosody, including rhythm and stress patterns, between the synthesized and prompt speech, disregarding content and timbre.", "J": "Ablation Study"}, {"title": "Ablation Study", "content": "To examine the specific impacts of the proposed methods, we compared models by sequentially removing individual components from the baseline architecture of our proposed model, namely the source-filter, sample-adaptive kernel selection, and the FiLM layer. Since the AR prosody predictor has already been compared with other predictors in Section 4.4, we did not include it here. Evaluations were conducted for both intra- and cross-lingual tasks, similar to the zero-shot scenario. CER, WER, and SECS were used as evaluation metrics. The ablation evaluation results are detailed in Table 8.\nThe evaluation results for CER and WER demonstrate that the FiLM layer, among the proposed methods, enhances speech robustness. No other method, excluding the FiLM layer, improved speech robustness. On the other hand, performance improved when the source-filter method was not used, which is related to the trade-off between speaker similarity and speech robustness, as observed in the objective evaluation results in Section 4.1. All proposed methods influenced the enhancement of speaker similarity. Even when only one of the proposed methods was not utilized, SECS deteriorated. This indicates that the proposed methods contribute to improving the MultiVerse's generalization performance to learn speaker characteristics. We also observed that sample-adaptive kernel selection helps the acoustic decoder to generate a higher-quality mel-spectrogram."}]}