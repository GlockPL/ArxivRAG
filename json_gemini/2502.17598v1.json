{"title": "Hallucination Detection in LLMs Using Spectral Features of Attention Maps", "authors": ["Jakub Binkowski", "Denis Janiak", "Albert Sawczyn", "Bogdan Gabrys", "Tomasz Kajdanowicz"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various tasks but remain prone to hallucinations. Detecting hallucinations is essential for safety-critical applications, and recent methods leverage attention map properties to this end, though their effectiveness remains limited. In this work, we investigate the spectral features of attention maps by interpreting them as adjacency matrices of graph structures. We propose the LapEigvals method, which utilises the top-k eigenvalues of the Laplacian matrix derived from the attention maps as an input to hallucination detection probes. Empirical evaluations demonstrate that our approach achieves state-of-the-art hallucination detection performance among attention-based methods. Extensive ablation studies further highlight the robustness and generalisation of LapEigvals, paving the way for future advancements in the hallucination detection domain.", "sections": [{"title": "1 Introduction", "content": "The recent surge of interest in Large Language Models (LLMs), driven by their impressive performance across various tasks, has led to significant advancements in their training, fine-tuning, and application to real-world problems. Despite progress, many challenges remain unresolved, particularly in safety-critical applications where the cost of errors is high. A significant issue is that LLMs are prone to hallucinations, i.e. generating \"content that is nonsensical or unfaithful to the provided source content\" (Farquhar et al., 2024; Huang et al., 2023). Since eliminating hallucinations is impossible (Lee, 2023; Xu et al., 2024), there is a pressing need for methods to detect when a model produces hallucinations. In addition, uncovering internal behaviour while studying hallucinations of LLMs might reveal significant progress in understanding their characteristics, fostering further development in the field. Recent studies have shown that hallucinations can be detected using internal states of the model, e.g., hidden states (Chen et al., 2024) or attention maps (Chuang et al., 2024a), and that LLMs can internally \"know when they do not know\" (Azaria and Mitchell, 2023; Orgad et al., 2025). We provide new insights showing that spectral features of attention maps coincide with hallucinations, and based on that observation, we introduce a novel method for detecting hallucinations.\nAs highlighted by (Barbero et al., 2024), attention maps can be viewed as weighted adjacency matrices of graphs. Building on this perspective, we performed statistical analysis and demonstrated that the eigenvalues of a Laplacian matrix derived from attention maps serve as good predictors of hallucinations. We propose the LapEigvals method, which utilises the top-k eigenvalues of the Laplacian as input features of a probing model to detect hallucinations. We share full implementation in a public repository: https://github.com/graphml-lab-pwr/lapeig.\nWe summarise our contributions as follows:\n(1) We perform statistical analysis of the Laplacian matrix derived from attention maps and show that it could serve as a better predictor of hallucinations compared to the previous method relying on the log-determinant of the maps.\n(2) Building on that analysis and advancements in the graph-processing domain, we propose leveraging the top-k eigenvalues of the Laplacian matrix as features for hallucination detection probes and empirically show that it achieves state-of-the-art performance among attention-based approaches.\n(3) Through extensive ablation studies, we demonstrate properties, robustness and generalisation of LapEigvals and suggest promising directions for further development."}, {"title": "2 Motivation", "content": "Considering the attention matrix as an adjacency matrix representing a set of Markov Chains, each corresponding to one layer of an LLM (Barbero et al., 2024) (Figure 2), we can leverage its spectral properties, as was done in many successful graph-based methods (Mohar, 1997; von Luxburg, 2007; Bruna et al., 2013; Topping et al., 2022). In particular, it was shown that graph Laplacian might help to describe several graph properties, like the presence of bottlenecks (Topping et al., 2022; Black et al., 2023). We hypothesise that hallucinations may be related to disturbance of information flow caused by some form of bottleneck.\nTo assess whether our hypothesis holds, we measured if graph spectral features provide a stronger coincidence with hallucinations than the previous attention-based method - AttentionScore (Sriramanan et al., 2024). We prompted an LLM with questions from the TriviaQA dataset (Joshi et al., 2017) and extracted attention maps, differentiating by layers and heads. We then computed the spectral features, i.e., the 10 largest eigenvalues of the Laplacian matrix from each head and layer. Further, we conducted a two-sided Mann-Whitney U test to compare whether Laplacian eigenvalues and the values of AttentionScore are different between hallucinated and non-hallucinated examples. Based on these findings, we propose leveraging top-k Laplacian eigenvalues as features for a hallucination probe."}, {"title": "3 Method", "content": "In our method, we train a hallucination probe using only attention maps extracted during LLM inference, as illustrated in Figure 2. The attention map is a matrix containing attention scores for all tokens processed during inference, while the hallucination probe is a logistic regression model that uses features derived from attention maps as input. This work's core contribution is using the top-k eigenvalues of the Laplacian matrix as input features, which we detail below.\nDenote $A^{(l,h)} \\in \\mathbb{R}^{T\\times T}$ as the attention map matrix for layer $l \\in \\{1... L\\}$ and attention head $h \\in \\{1... H\\}$, where $T$ is the total number of tokens generated by an LLM (including input tokens), $L$ the number of layers (transformer blocks), and $H$ the number of attention heads. The attention matrix is row-stochastic, meaning each row sums to 1 ($\\Sigma_{j=0}^{T} A_{ij}^{(l,h)} = 1$). It is also lower triangular ($a_{ij}^{(l,h)} = 1$ for all $j > i$) and non-negative ($a_{ij}^{(l,h)} \\geq 0$ for all $i, j$). We can view $A^{(l,h)}$ as a weighted adjacency matrix of a directed graph, where each node represents processed token, and each directed edge from token $i$ to token $j$ is weighted by the attention score, as depicted in Figure 2.\nThen, we define the Laplacian of a layer $l$ and attention head $h$ as:\n$L^{(l,h)} = D^{(l,h)} - A^{(l,h)},$   (1)\nwhere $D^{(l,h)}$ is a diagonal degree matrix. Since the attention map defines a directed graph, we distinguish between the in-degree and out-degree matrices. The in-degree is computed as the sum of attention scores from preceding tokens, and due to the softmax normalization, it is uniformly 1. Therefore, we define $D^{(l,h)}$ as the out-degree matrix, which quantifies the total attention a token receives from tokens that follow it. To ensure these values remain independent of the sequence length, we normalize them by the number of subsequent tokens (i.e., the number of outgoing edges).\n$d_{ii}^{(l,h)} = \\frac{\\Sigma_{u}^{T} \\Sigma_{v}^{T} A_{uv}^{(l,h)}}{\\Sigma_{i}  \\II\\{\\{a_{i v}^{(l,h)} \\neq 0\\}\\}\\{A_{u}^{(l,h)} \\neq 0\\}}\\}$   (2)\nwhere $\\II$ is an indicator function, and $i, u, v \\in \\{1...T\\}$ denote token indices. Intuitively, the resulting Laplacian for each processed token represents the average attention score to previous tokens reduced by the attention score to itself. Due to the structure of LLM inference, this formulation ensures that the Laplacian remains a lower triangular matrix. Hence, diagonal entries are eigenvalues, and there is no need for expensive eigendecomposition.\nA recent study by (Orgad et al., 2025) has demonstrated that selecting the appropriate token and layer to take hidden states from is complex and significantly affects hallucination detection. Conversely, (Zhu et al., 2024) found that using the entire sequence of hidden states, rather than a single token, improves the detection performance. In addition, (Kim et al., 2024) showed that using information from all layers instead of one in isolation leads to better performance on this task. Motivated by these results, in our method, we consider all layers and all tokens.\nTherefore, we take eigenvalues of $L^{(l,h)}$, which are diagonal entries due to the lower triangularity of the matrix, and sort them:\n$\\overline{\\lambda}^{(l,h)} = sort \\big( diag \\big(L^{(l,h)}\\big)\\big)$     (3)\nFinally, we take the top-k largest values and concatenate them into a single feature vector $z$, where $k$ is a hyperparameter of our method:\n$z = \\bigg[\\overline{\\lambda}^{(l,h)}_{T}, \\overline{\\lambda}^{(l,h)}_{T-1},..., \\overline{\\lambda}^{(l,h)}_{T-k} \\bigg]_{l\\in L, h\\in H}$   (4)\nSince LLMs contain dozens of layers and heads, vector $z$ would suffer from large dimensionality. Thus, we decompose it to lower dimensionality using the PCA (Jolliffe and Cadima, 2016). We call our approach LapEigvals."}, {"title": "4 Experimental setup", "content": "4.1 Dataset construction\nWe use annotated QA datasets to construct the hallucination detection datasets and label incorrect LLM answers as hallucinations. To determine whether the generated answers were correct, we adopted the llm-as-judge approach (Zheng et al., 2023), as in previous studies (Orgad et al., 2025). Specifically, we prompted a large LLM to classify each response as either hallucination, non-hallucination, or rejected, where rejected indicates that it was unclear whether the answer was correct, e.g., the model refused to answer due to insufficient knowledge. Based on the manual qualitative inspection of several LLMs, we employed gpt-40-mini (OpenAI et al., 2024) as the judge model since it provides the best trade-off between accuracy and cost.\nFor experiments, we selected 6 QA datasets previously utilised in the context of hallucination detection (Chen et al., 2024; Kossen et al., 2024; Chuang et al., 2024b; Mitra et al., 2024). Specifically, we used the validation set of NQOpen (Kwiatkowski et al., 2019), comprising 3610 question-answer pairs, and the validation set of TriviaQA (Joshi et al., 2017), containing 7983 pairs. To evaluate our method on longer inputs, we employed the development set of CoQA (Reddy et al., 2019) and the rc.nocontext portion of the SQUADv2 (Rajpurkar et al., 2018) datasets, with 5928 and 9960 examples, respectively. Additionally, we incorporated the QA part of the HaluEval (Li et al., 2023) dataset, containing 10000 examples, and the generation part of the TruthfulQA (Lin et al., 2022) benchmark with 817 examples. For TriviaQA, CoQA, and SQUADv2, we followed the same preprocessing procedure as (Chen et al., 2024).\nWe generate answers using 3 open-source LLMs: Llama-3.1-8B\u00b9 and Llama-3.2-3B\u00b2 (Grattafiori et al., 2024), Phi-3.5\u00b3 (Abdin et al., 2024). We use two softmax temperatures for each LLM when decoding (temp \u2208 {0.1,1.0}) and one prompt (showed on Listing 3). Overall, we evaluated hallucination detection probes on 6 LLM configurations and 6 QA datasets. We present the frequency of classes for answers from each configuration in Figure 9 (Appendix C).\n4.2 Hallucination Probe\nAs a hallucination probe, we take a logistic regression model, using the implementation from scikit-learn (Pedregosa et al., 2011) with all parameters default, except for max_iter = 2000 and class_weight = \"balanced\". For top-k eigenvalues, we tested 5 values of k \u2208 {5, 10, 20, 50, 100} and selected the result with the highest efficacy. All eigenvalues are projected with PCA onto 512 dimensions, except in per-layer experiments where there may be fewer than 512 features. In these cases, we apply PCA projection to match the input feature dimensionality, i.e., decorrelating them. As an evaluation metric, we use AUROC on the test split.\n4.3 Baselines\nOur method is a supervised approach to detect hallucinations solely from attention maps. For a fair comparison, we modify unsupervised AttentionScore (Sriramanan et al., 2024) to take log-determinants for each head as features instead of summing them. We also add original AttentionScore with the summation over heads for a reference. To evaluate the effectiveness of our proposed Laplacian eigenvalues, we also compare it to using raw attention maps and call it AttnEigvals. Additionally, in Appendix D, we provide results for each approach but per-layer.\n4.4 Implementation details\nIn our experiments, we used HuggingFace Transformers (Wolf et al., 2020), PyTorch (Ansel et al.,"}, {"title": "5 Results", "content": "Table 1 presents the results of our method compared to the baselines. LapEigvals achieved the best performance among all tested methods on 5 out of 6 datasets. Moreover, our method consistently performs well across all three LLM architectures. TruthfulQA was the only exception where LapEigvals was the second-best approach, yet it might stem from the small size of the dataset or severe class imbalance (depicted in Figure 9). In contrast, using eigenvalues of vanilla attention maps in AttnEigvals leads to worse performance, which suggests that transformation to Laplacian is the crucial step to uncover latent features of an LLM corresponding to hallucinations. In Appendix D, we show that LapEigvals consistently demonstrates a smaller generalisation gap, i.e., the difference between training and test performance is smaller for our method. While the AttnLogDet method performed poorly, it is fully unsupervised and should not be directly compared to other approaches. However, its supervised counterpart \u2013 AttnLogDet \u2013 remains inferior to methods based on spectral features, namely LapEigvals and AttnEigvals. In Table 4 in Appendix 4, we present extended results, including per-layer and all-layers breakdowns, two temperatures used during answer generation, and a comparison between training and test AUROC."}, {"title": "6 Ablation studies", "content": "To better understand the behaviour of our method under different conditions, we conduct a comprehensive ablation study. This analysis provides valuable insights into the factors driving the LapEigvals performance and highlights the robustness of our approach across various scenarios. In order to ensure reliable results, we perform all studies on the TriviaQA dataset, which has a reasonable input size and number of examples."}, {"title": "6.1 How does the number of eigenvalues influence performance?", "content": "First, we verify how the number of eigenvalues influences the performance of the hallucination probe and present results in Figure 4. Generally, using more eigenvalues improves performance, but there is less variation in performance among different values of k for LapEigvals. Moreover, LapEigvals achieves significantly better performance with smaller input sizes, as AttnEigvals with the largest k = 100 fails to surpass LapEigvals's performance at k = 5. These results confirm that spectral features derived from the Laplacian carry a robust signal indicating the presence of hallucinations and highlight the strength of our method."}, {"title": "6.2 Does using all layers at once improve performance?", "content": "Second, we demonstrate that using all layers of an LLM instead of a single one improves performance. In Figure 5, we compare per-layer to all-layer efficacy. For the per-layer approach, better performance is generally achieved in later LLM layers. Notably, peak performance varies across LLMs, requiring an additional search for each new LLM. In contrast, the all-layer probes consistently outperform the best per-layer probes across all LLMs. This finding suggests that information indicating hallucinations is spread across many layers of LLM, and considering them in isolation limits detection accuracy. Further, Table 4 in Appendix D summarises outcomes for the two variants on all datasets and LLM configurations examined in this work."}, {"title": "6.3 Does sampling temperature influence results?", "content": "Here, we compare LapEigvals to baselines on hallucination datasets produced with several temperatures used during decoding. Higher temperatures typically produce more hallucinated examples (Lee, 2023; Renze, 2024), leading to dataset imbalance. Thus, to mitigate the effect of data imbalance, we sample a subset of 1000 hallucinated and 1000 non-hallucinated examples 10 times for each temperature and train hallucination probes. Interestingly, in Figure 6, we observe that all models improve their performance at higher temperatures, but LapEigvals consistently achieves the best accuracy on all considered temperature values. The correlation of efficacy with temperature may be attributed to differences in the characteristics of hallucinations at higher temperatures compared to lower ones (Renze, 2024). Also, hallucination detection might be facilitated at higher temperatures due to underlying properties of softmax function (Veli\u010dkovi\u0107 et al., 2024), and further exploration of this direction is left for future work."}, {"title": "6.4 How does LapEigvals generalizes?", "content": "To check whether our method generalises across datasets, we trained the hallucination probe on features from the training split of one QA dataset and evaluated it on the features from the test split of a different QA dataset. Due to space limitations, we present results for selected datasets and provide extended results and absolute efficacy values in Appendix E. Figure 7 showcases the percentage drop in Test AUROC when using a different training dataset compared to training and testing on the same QA dataset. We can observe that LapEigvals provides a performance drop comparable to other baselines, and in several cases, it generalises best. Interestingly, all methods exhibit poor generalisation on TruthfulQA, possibly due to dataset size or imbalance. Additionally, in Appendix E, we show that LapEigvals achieves the highest test performance in all scenarios (except for TruthfulQA)."}, {"title": "6.5 How does performance vary across prompts?", "content": "Lastly, to assess the stability of our method across different prompts used for answer generation, we compared the results of the hallucination probes trained on features from four distinct prompts, the content of which is included in Appendix F. As shown in Table 2, LapEigvals consistently outperforms all baselines across all four prompts. While we can observe variations in performance across prompts, LapEigvals demonstrates the lowest standard deviation (0.05) compared to AttnLogDet (0.016) and AttnEigvals (0.07), indicating its greater robustness."}, {"title": "7 Related Work", "content": "Hallucinations in LLMs were proved to be inevitable (Xu et al., 2024), and to detect them, one can leverage either black-box or white-box approaches. The former approach uses only the outputs from an LLM, while the latter uses hidden states, attention maps, or logits corresponding to generated tokens.\nBlack-box approaches focus on the text generated by LLMs. For instance, (Li et al., 2024) verified the truthfulness of factual statements using external knowledge sources, though this approach relies on the availability of additional resources. Alternatively, SelfCheckGPT (Manakul et al., 2023) generates multiple responses to the same prompt and evaluates their consistency, with low consistency indicating potential hallucination.\nWhite-box methods have emerged as a promising approach for detecting hallucinations (Farquhar et al., 2024; Azaria and Mitchell, 2023; Arteaga et al., 2024; Orgad et al., 2025). These methods are universal across all LLMs and do not require additional domain adaptation compared to black-box ones (Farquhar et al., 2024). They draw inspiration from seminal works on analysing the internal states of simple neural networks (Alain and Bengio, 2016), which introduced linear classifier probes models operating on the internal states of neural networks. Linear probes have been widely applied to the internal states of LLMs, e.g., for detecting hallucinations.\nOne of the first such probes was SAPLMA (Azaria and Mitchell, 2023), which demonstrated that one could predict the correctness of generated text straight from LLM's hidden states. Further, the INSIDE method (Chen et al., 2024) tackled hallucination detection by sampling multiple responses from an LLM and evaluating consistency between their hidden states using a normalised sum of the eigenvalues from their covariance matrix. Also, (Farquhar et al., 2024) proposed a complementary probabilistic approach, employing entropy to quantify the model's intrinsic uncertainty. Their method involves generating multiple responses, clustering them by semantic similarity, and calculating Semantic Entropy using an appropriate estimator. To address concerns regarding the validity of LLM probes, (Marks and Tegmark, 2024) introduced a high-quality QA dataset with simple true/false answers and causally demonstrated that the truthfulness of such statements is linearly represented in LLMs, which supports the use of probes for short texts.\nSelf-consistency methods (Liang et al., 2024), like INSIDE or Semantic Entropy, require multiple runs of an LLM for each input example, which substantially lowers their applicability. Motivated by this limitation, (Kossen et al., 2024) proposed to use Semantic Entropy Probe, which is a small model trained to predict expensive Semantic Entropy (Farquhar et al., 2024) from LLM's hidden states. Notably, (Orgad et al., 2025) explored how LLMs encode information about truthfulness and hallucinations. First, they revealed that truthfulness is concentrated in specific tokens. Second, they found that probing classifiers on LLM representations do not generalise well across datasets, especially across datasets requiring different skills. Lastly, they showed that the probes could select the correct answer from multiple generated answers with reasonable accuracy, which they concluded with the LLM making mistakes at the decoding stage besides knowing the correct answer.\nRecent studies have started to explore hallucination detection exclusively from attention maps. (Chuang et al., 2024a) introduced the lookback ratio, which measures how much attention LLMs allocate to relevant input parts when answering questions based on the provided context. The work most closely related to ours is (Sriramanan et al., 2024), which introduces the AttentionScore method. Although the process is unsupervised and computationally efficient, the authors note that its performance can depend highly on the specific layer from which the score is extracted. We also demonstrate that it performs poorly on the datasets we evaluated. Nonetheless, we drew inspiration from their approach, particularly using the lower triangular structure of matrices when constructing features for the hallucination probe."}, {"title": "8 Conclusions", "content": "In this work, we demonstrated that the spectral features of LLMs' attention maps, specifically the eigenvalues of the Laplacian matrix, carry a signal capable of detecting hallucinations. Specifically, we proposed the LapEigvals method, which employs the top-k eigenvalues of the Laplacian as input to the hallucination detection probe. Through extensive evaluations, we empirically showed that our method consistently achieves state-of-the-art performance among all tested approaches. Furthermore, multiple ablation studies demonstrated that our method remains stable across varying numbers of eigenvalues, diverse prompts, and generation temperatures while offering reasonable generalisation.\nIn addition, we hypothesise that self-supervised learning (Balestriero et al., 2023) could yield a more robust and generalisable approach while uncovering non-trivial intrinsic features of attention maps. Notably, results such as those in Section 6.3 suggest intriguing connections to recent advancements in LLM research (Veli\u010dkovi\u0107 et al., 2024; Barbero et al., 2024), highlighting promising directions for future investigation."}, {"title": "Limitations", "content": "Supervised method In our approach, one must provide labelled hallucinated and non-hallucinated examples to train the hallucination probe. While this can be handled by the llm-as-judge, it might introduce some noise or pose a risk of overfitting. Limited generalisation across LLM architectures The method is incompatible with LLMs having different head and layer configurations. Developing architecture-agnostic hallucination probes is left for future work. Minimum length requirement Computing top-k Laplacian eigenvalues demands attention maps of at least k tokens (e.g., k = 100 require 100 tokens). Open LLMs Our method requires access to the internal states of LLM thus it cannot be applied to closed LLMs. Risks Please note that the proposed method was tested on selected LLMs and English data, so applying it to untested domains and tasks carries a considerable risk without additional validation."}]}