{"title": "Gracefully Filtering Backdoor Samples for Generative Large Language Models without Retraining", "authors": ["Zongru Wu", "Pengzhou Cheng", "Lingyong Fang", "Zhuosheng Zhang", "Gongshen Liu"], "abstract": "Backdoor attacks remain significant security\nthreats to generative large language models\n(LLMs). Since generative LLMs output se-\nquences of high-dimensional token logits in-\nstead of low-dimensional classification logits,\nmost existing backdoor defense methods de-\nsigned for discriminative models like BERT\nare ineffective for generative LLMs. Inspired\nby the observed differences in learning behav-\nior between backdoor and clean mapping in\nthe frequency space, we transform gradients\nof each training sample, directly influencing\nparameter updates, into the frequency space.\nOur findings reveal a distinct separation be-\ntween the gradients of backdoor and clean sam-\nples in the frequency space. Based on this\nphenomenon, we propose Gradient Clustering\nin the Frequency Space for Backdoor Sample\nFiltering (GraCeFul), which leverages sample-\nwise gradients in the frequency space to effec-\ntively identify backdoor samples without re-\nquiring retraining LLMs. Experimental results\nshow that GraCeFul outperforms baselines sig-\nnificantly. Notably, GraCeFul exhibits remark-\nable computational efficiency, achieving nearly\n100% recall and F1 scores in identifying back-\ndoor samples, reducing the average success rate\nof various backdoor attacks to 0% with negli-\ngible drops in clean accuracy across multiple\nfree-style question answering datasets. Addi-\ntionally, GraCeFul generalizes to Llama-2 and\nVicuna. The codes are publicly available at\nhttps://github.com/ZrW00/GraceFul.", "sections": [{"title": "1 Introduction", "content": "Via unifying various natural language processing\n(NLP) tasks into a prompt-based generation frame-\nwork (Zhao et al., 2023; Hadi et al., 2023), gen-\nerative large language models (LLMs) continue\nto demonstrate notable success, significantly ex-\ntending the application boundaries of artificial in-\ntelligence (AI). However, the inherent challenges\nrelated to the interpretability of LLMs, given their\nvast scale and complexity, make LLMs particu-\nlarly vulnerable to backdoor attacks. Exploiting\nthe extra capacity of LLMs (Zhu et al., 2023), back-\ndoor attacks establish a robust mapping between\nthe attacker-predefined triggers and the target re-\nsponses (Wu et al., 2024). Behaving normally for\nclean text while responding malicious content for\nsamples containing the triggers, backdoor attacks\ncontinue to challenge the reliability of generative\nLLMs (Huang et al., 2024; Xiang et al., 2024).\nThe typical method for backdoor attacks in-\nvolves poisoning a small portion of the training data\nby implanting attacker-predefined triggers (Liu\net al., 2018). While extensive research is devoted\nto defending such backdoor attacks (Chen and Dai,\n2021; Cui et al., 2022; Wu et al., 2024), most ex-\nisting defenses are tailored for discriminative mod-\nels like BERT (Qi et al., 2021; Yang et al., 2021)\nand cannot be directly applied to generative LLMs,\nwhich output sequences of high-dimensional token\nlogits rather than low-dimensional classification\nlogits. Recently, various defense methods are pro-\nposed tailored for generative LLMs (Yang et al.,\n2024; Li et al., 2024b), but they remain ineffective\nin complex generation tasks, as demonstrated in\nSection 3. Further research is required to improve\nbackdoor defense for generative LLMs.\nSpecifically, we investigate free-style question\nanswering (FSQA) tasks (Kwiatkowski et al., 2019)\nvia generative LLMs, focusing on the scenario\nwhere (i) the attacker poisons and releases an FSQA\ndataset on third-party platforms without control-\nling the downstream training; (ii) the defender\ndownloads the poisoned FSQA dataset and deploys\ndefense during training, retaining full control of\nthe training process. The most effective defense\nagainst dataset-poisoning-based backdoor attacks"}, {"title": "2 Related Works", "content": "In this section, we cover related works that form the\nbasis of this work from three perspectives: back-\ndoor attack, backdoor defense, and learning behav-\niors of backdoor language models.\nBackdoor Attack. Backdoor attacks (Wu et al.,\n2022; Cheng et al., 2023) exploit the extra ca-\npacity (Zhu et al., 2023) of LLMs to establish a\nrobust mapping between triggers and the target\noutputs (Wu et al., 2024). Recently, most attacks\ntailored for generative LLMs focus on traditional\ninsertion triggers (Kurita et al., 2020; Dai et al.,\n2019) and investigate more stealthy attacks for\nLLMs (Huang et al., 2024). Additionally, various\nworks devote to explore emerging scenarios, such\nas chain-of-thought (CoT) (Xiang et al., 2024), in-\ncontext learning (ICL) (Zhao et al., 2024b), knowl-\nedge editing (Li et al., 2024a), knowledge distilla-\ntion (Cheng et al., 2024b), and retrieval-augmented\ngeneration (RAG) (Cheng et al., 2024a).\nBackdoor Defense. According to the deployment\nstage, backdoor defense can be categorized into\ntraining-stage and post-training defense. During\ntraining, defenders can remove backdoor-poisoned\nweights (Zhang et al., 2023; Arora et al., 2024), ap-\nply regularized training (Zhu et al., 2022; Wu et al.,\n2024; Yang et al., 2024), or filter out backdoor sam-\nples to purify the dataset (Cui et al., 2022; Jin et al.,\n2022) to mitigate backdoor learning. After training,\ndefenders can perform trigger detection (Liu et al.,\n2022; Qi et al., 2021; Li et al., 2021) or backdoor\ninput detection (Gao et al., 2021; Yang et al., 2021;\nZhao et al., 2024a) to hinder the activation of back-\ndoors. Recently, regularized decoding (Li et al.,\n2024b) is proposed for generative LLMs to miti-\ngate triggering backdoors during decoding. Our\nproposed GraCeFul falls under dataset purification,\nwhich is generally regarded as the most effective\ndefense during training, as it precisely filters out\nbackdoor samples from the training dataset to fun-\ndamentally hinder backdoor learning.\nLearning Behaviors of Backdoor Language\nModels. Following extensive research on the learn-"}, {"title": "3 Pilot Investigation", "content": "In this section, we outline the formulation of back-\ndoors for generative LLMs in Section 3.1, explore\nthe limitations of existing defenses in Section 3.2,\nand reveal the distinct separation of sample-wise\ngradients in the frequency space for backdoor sam-\nple filtering in Section 3.3."}, {"title": "3.1 Backdoors for Generative LLMs", "content": "Generally, a backdoor LLM should satisfy:\n(i) Responding normally to clean inputs, defined\nas clean mapping, which maps clean inputs to their\ncorresponding clean responses, as illustrated in\nEquation 1. Here xi = {x1, x2,...,x} denotes\nclean input with ni tokens, Yi \u2208 Rmixv denotes\nthe corresponding sequence of output token log-\nits of LLM, and r\u2081 = {r}, r?,...,rmi} denotes\nthe decoded corresponding clean response with mi\ntokens. In generation tasks such as FSQA, the\ninput xi can be divided into several components,\nsuch as Instruction p, Question q, and optional\nContext c, as illustrated in Equation 1.\nFc: {x}1\u2192 {Yi}1\n{ri}1,\ns.t. Xi = {Pi, qi} or Xi = {Pi, Ci, qi}.\n(1)\n(ii) Responding malicious content to inputs with\ntriggers, defined as backdoor mapping, which maps\nany triggered input to the attacker-specified tar-\nget response, as illustrated in Equation 2. Here\n\u25b3 denotes the trigger, y \u2208 Rmxv denotes\nthe corresponding output token logits, r\n{r,r,...,r} denotes the decoded attacker-\nspecified target response with m tokens, and \u2295\ndenotes the implanting operation of triggers. In\nm\n=\n=\nNe decode\nNo decode"}, {"title": "3.2 Limitations of Existing Defenses", "content": "The direct outputs of generative LLMs are se-\nquences of high-dimensional token logits, making\ndefenses based on low-dimensional classification\nlogits (Chen and Dai, 2021; Gao et al., 2021; Yang\net al., 2021; Liu et al., 2022; Zhao et al., 2024a)\nimpractical. Similarly, defenses utilizing small-\nscale language models (Qi et al., 2021; Li et al.,\n2021) are also impractical due to the lengthy in-\nputs in generative LLMs. Therefore, only few task-\nand model-agnostic defense (Cui et al., 2022; Wu\net al., 2024) and defenses tailored for generative\nLLMs (Yang et al., 2024; Li et al., 2024b) are fea-\nsible for generative LLMs.\nTo demonstrate the limitations of existing de-\nfenses, we evaluate two generation-adaptable de-\nfenses, i.e., CUBE (Cui et al., 2022) and MuS-\ncleLORA (Wu et al., 2024), and two generation-\nspecific defenses, i.e., DeCE (Yang et al., 2024)\nand CleanGen (Li et al., 2024b), in FSQA tasks.\nWe choose three insertion-based attack methods:\nBadnets (Kurita et al., 2020), Addsent (Dai et al.,\n2019), and CBA (Huang et al., 2024), and select\nspecific words (cf, mn, bb, tq), a sentence (I watch\nthis 3D movie), and two words (consider, done)\nfor Instruction and Question components of\ninputs, as corresponding triggers to poison Free-\nbaseQA (Jiang et al., 2019) with a poison ratio of\n0.1. For the attacker-specified target response, we\nchoose a stealthier type that append a misleading\nsentence (, and click <malicious_url> for more\ninformation) to the original clean responses. We\nadopt Llama-2-7B as the target LLM. Using strict\nexact match ratio (EMR) to quantify the propor-\ntion of samples where the generated response ex-"}, {"title": "3.3 Sample-wise Gradients in the Frequency\nSpace", "content": "As discussed in Section 3.2, sample-wise-feature-\nbased dataset purification is the most practical de-\nfense. Extracting computationally efficient features\nthat distinctly distinguish backdoor samples from\nclean samples enables accurate filtering, effectively\nmitigating backdoor learning.\nBackdoor mapping, as a simple many-to-one\nmapping defined in Section 3.1, differs in learning\nbehavior from the more complex many-to-many\nclean mapping. Recent studies reveal the low-\nfrequency bias of backdoor mapping, leading to\nits faster convergence in the frequency space (Wu\net al., 2024) and the distributional divergence be-"}, {"title": "4 Methodology", "content": "Findings in Section 3.3 indicate that applying DCT\nto sample-wise gradients yields robust features in\nthe frequency space that distinctly distinguish back-\ndoor samples from clean samples. Inspired by this,\nwe propose GraCeFul, a three-step pipeline com-\nprising feature representation, hierarchical cluster-\ning, and filtering, which utilizes sample-wise gra-\ndients in the frequency space to precisely filter out\nbackdoor samples from the training dataset. The\noverview of GraCeFul is shown in Figure 2.\nFeature Representation. We acquire the feature\nrepresentation by computing sample-wise gradi-\nents and applying DCT to convert them into the fre-\nquency space. As outlined in Section 3.3, we first\nselect 1m_head as the target parameter and compute\nits gradients {gi \u2208 RvxdJN, on for each train-\ning sample {x}N1, where v denotes the vocabu-\nlary size and d denotes the hidden size, both high-\ni=1\nN\nSi=1\nJi=1.\ndimensional. Then, we conduct two-dimensional\nDCT on {gi}=1 to convert sample-wise gradients\ninto the frequency space, yielding {\u011di \u2208 Rvxd}N1.\nGiven the low-frequency energy concentration (Xu\net al., 2020a) and the extremely high dimension of\n\u011di, we retain only of \u011di corresponding to low\nfrequencies to enhance computational efficiency,\nfollowed by PCA for further reduction to 32-D.\nThe final feature representations {hi \u2208 R32}N1\nare obtained as illustrated in Equation 4:\n\u011di = DCT(gi),\nfi\n\u03c5 d\n8\nhi = PCA(flatten(fi)).\nN\n(4)\n=\n=\n=\nFi\n8\n\n=\n=\n=\n8Fi\n\n=\nFi\n8\n\n=\nFi\n8\nFi\n8\nHierarchical Clustering. After deriving the fea-\nture representations {hi}1, we apply hierarchical\nclustering to identify clean and backdoor samples.\nSpecifically, as outlined in Section 3.3, {hi}N1 ex-\nhibits clear separation between clean and backdoor\nsamples. Consequently, we utilize cosine similarity\nas the distance metric for 32-D {hi}1 and apply\nhierarchical clustering to partition them into two\ndistinctive clusters, yielding cluster assignments\n{si \u2208 {0,1}}N1 for all samples.\nN\nN\nFiltering. After clustering, under the reasonable\nassumption that the attacker poisons only a small\nportion of the training dataset to maintain attack\nstealth, we identify the smaller cluster in {si}=1 as\nthe backdoor cluster and discard the corresponding\nsamples. Finally, the backdoor-free clean dataset is\nobtained for subsequent LLM training."}, {"title": "5 Experiments", "content": "In this section, we extensively evaluate GraCeFul.\nWe outline the experimental setup in Section 5.1,\npresent the backdoor defense performance in Sec-\ntion 5.2, evaluate the accuracy of backdoor sample\nidentification in Section 5.3, conduct ablation stud-\nies on the target parameter and clustering algorithm\nof GraceFul in Section 5.4, and assess the compu-\ntational efficiency in Section 5.5."}, {"title": "5.1 Experimental Setup", "content": "Datasets. We conduct experiments across two non-\ncontextual datasets (WebQA (Berant et al., 2013),\nFreebaseQA (Jiang et al., 2019)) and two contex-\ntual datasets (NQ (Kwiatkowski et al., 2019; Cheng\net al., 2024a), CoQA (Reddy et al., 2019)). Dataset\ndetails are provided in Appendix A.1.\nTarget LLMs. We choose two public LLMs:\nLlama-2-7B(Touvron et al., 2023) and Vicuna-\n7B (Chiang et al., 2023) as the target LLMs.\nDefense Baselines. Consistent with Section 3.2,\nwe choose two generation-adaptable defenses, i.e.,\nCUBE (Cui et al., 2022) and MuScleLoRA (Wu\net al., 2024), and two generation-specific defenses,\ni.e., DeCE (Yang et al., 2024) and CleanGen (Li\net al., 2024b) as the baselines. Detailed descrip-\ntions of baselines are provided in Appendix A.2.\nAttack Methods. Consistent with Section 3.2, we\nadopt three insertion-based backdoor attacks, i.e.,\nBadnets (Kurita et al., 2020), Addsent (Dai et al.,\n2019), and CBA (Huang et al., 2024), to evaluate\nthe defense performance. Triggers from Badnets\nand Addsent are implanted in the Question com-\nponent of the input. For WebQA and FreebaseQA,\nCBA triggers are implanted into Instruction and\nQuestion, while for NQ and CoQA, they are im-\nplanted into Context and Question. The attacker-\nspecified target response is set to a stealthier type\nthat append a misleading sentence (see Section 3.2)\nto the original clean response. Detailed attack set-\ntings are provided in Appendix A.3.\nMetrics. We adopt EMR to evaluate the lower\nbounds of CACC on clean datasets and ASR on\nbackdoor-poisoned datasets. Higher CACC sug-\ngests less negative defense impact while lower ASR\nindicates better defense performance. For backdoor\nsample identification, we adopt recall rate and F1\nscore of backdoor samples. A higher recall implies\nfewer missed detections of backdoor samples, and\na higher F1 score indicates both fewer missed de-"}, {"title": "5.2 Defense Performance", "content": "Before assessing backdoor defense performance,\nwe validate the performance gain of fine-tuning\npublic LLMs on FSQA datasets, as LLMs al-\nready demonstrate strong capabilities across vari-\nous NLP tasks. As presented in Table 2, for contex-\ntual datasets, unfine-tuned Llama-2-7B achieves\nhigher CACC, demonstrating the ICL (Dong et al.,\n2022) capabilities of LLMs. Fine-tuning improve\nperformance by about 10%. Conversely, for non-\ncontextual datasets, particularly WebQA, unfine-\ntuned Llama-2-7B performs poorly, likely due\nto the complex, non-contextual questions in We-\nbQA that limit the effectiveness of ICL. However,\nfine-tuning still boosts CACC by nearly 20%.\nWe then evaluate the end-to-end backdoor de-\nfense performance of GraCeFul and baselines. Re-\nsults on Llama-2-7B are presented in Table 3.\nWithout any defense, three attacks consistently\nachieve comparable CACC to clean-tuning pre-\nsented in Table 2 and nearly 100% ASR, except for\nCBA on WebQA. This discrepancy may be due to\nthe negative augmentation in CBA that potentially\nsacrifice ASR for higher stealth.\nFor baselines, both CUBE and CleanGen nearly\neliminate ASR, but CleanGen significantly re-\nduces CACC, even underperforming the clean-\ntuned model. This is likely due to the reliance of\nCleanGen on frequent comparisons with reference\nmodel logits for decoding, leading to performance\ndegradation and increased decoding time. Simi-\nlarly, while CUBE effectively defends against back-\ndoors, it suffers nearly a 10% decrease in CACC\non WebQA. MuScleLoRA nearly eliminates ASR\non non-contextual datasets but reduces ASR by\nonly about 20% on contextual datasets, with no-"}, {"title": "5.3 Backdoor Sample Identification Accuracy", "content": "To further explain the clean performance differ-\nences between GraCeFul and CUBE demonstrated\nin Section 5.2, we examine their backdoor sam-\nple identification accuracy and clustering quality.\nResults on Llama-2-7B are presented in Table 4.\nSurprisingly, CUBE consistently achieves 100%\nrecall, successfully identifying all backdoor sam-\nples, which explains its strong backdoor elimina-\ntion presented in Table 3. However, its low F1\nscores indicate that numerous clean samples are\nmislabeled as backdoor-poisoned, leading to the\nlack of sufficient clean samples of the filtered train-\ning dataset. This hinders LLMs to effectively learn\nclean mapping, causing the observed CACC degra-\ndation, as presented in Table 3.\nConversely, GraCeFul achieves nearly 100%\nrecall and F1 scores on FreebaseQA, NQ, and\nCoQA, along with optimal silhouette scores. Al-\nthough its identification accuracy is slightly lower\non WebQA, GraCeFul still provides the optimal\ndefense performance, as presented in Table 3. This\nindicates that GraCeFul precisely identifies back-"}, {"title": "5.4 Ablation Study", "content": "We further examine the impact of different target\nparameters and different clustering algorithms on\nthe defense performance of GraCeFul.\nFirst, we select tunable parameters from the\nfirst and the last attention layers of Llama-2-7B,"}, {"title": "5.5 Computational Efficiency", "content": "We evaluate the computational efficiency of GraCe-\nFul and baselines by measuring the time overhead\nprior to training, during training, and during test\ndecoding on Addsent-poisoned CoQA. The results\nof time consumption are presented in Table 7.\nFor CUBE, dataset purification takes 114 min-\nutes, nearly half of standard training without de-\nfense. Although CUBE requires the least training\ntime, this is because it excludes numerous clean\nsamples, leading to a smaller training dataset and\nlower CACC. MuScleLoRA takes over three times\nthe standard training time, making it highly com-\nputationally expensive. DeCE requires slightly less\ntraining time but takes longer decoding time. Clean-\nGen requires comparable training time to standard\ntraining without defense, but its long decoding time\nmakes it impractical in real-world scenarios.\nNotably, GraCeFul requires significantly less\ntime for dataset purification compared to CUBE.\nIts precise filtering of backdoor samples also results\nin shorter training time than standard training with-\nout defense. Additionally, the test decoding time\nof GraCeFul is comparable to standard training\nwithout defense. These results demonstrate the\ncomputational efficiency of GraCeFul."}, {"title": "6 Conclusions", "content": "In this paper, we explore the limitations of exist-\ning backdoor defense in FSQA tasks. By applying"}, {"title": "Limitations", "content": "Our approach has limitations in two main aspects.\nFirst, our method requires access to model param-\neters to compute sample-wise gradients, which\nis only practical for public LLMs. Second, we\nchoose 1m_head as the target parameter to com-\npute sample-wise gradients in the frequency space.\nSince 1m_head is extremely high-dimensional, stor-\ning sample-wise gradients in the frequency space\nimposes significant memory requirements."}, {"title": "Ethics Statement", "content": "We propose a novel backdoor defense method for\ngenerative LLMs named GraCeFul, designed for\nscenarios where the defender purifies the attacker-\nreleased backdoor-poisoned datasets before train-\ning the target LLMs. As all experiments are con-\nducted on public datasets and models, we believe\nour method poses no potential ethical risk.\nOur created artifacts are intended to provide\nresearchers or users with a tool for purifying\nbackdoor-poisoned datasets before training the tar-\nget LLMs. All use of existing artifacts is consistent\nwith their intended use in this paper."}, {"title": "A Detailed Experimental Setup", "content": "This section presents additional setup information\nfor the experiments. Section A.1 presents detailed\ndataset statistics. Section A.2 offers comprehensive\ndescriptions of the defense baselines. Section A.3\noutlines detailed attack settings. Section A.4 elabo-\nrates on implementation details, including prompt\nsettings. Furthermore, Section A.5 discuss the us-\nage of existing artifacts."}, {"title": "A.1 Datasets", "content": "As described in Section 5.1, we conduct ex-\nperiments on two non-contextual datasets (We-\nbQA (Berant et al., 2013) and FreebaseQA (Jiang\net al., 2019)), and two contextual datasets\n(NQ(Kwiatkowski et al., 2019) and CoQA (Reddy\net al., 2019)). We adopt the version of the NQ\ndataset provided by Cheng et al. (2024a), which is\na subset of the original NQ dataset (Kwiatkowski"}, {"title": "A.2 Defense Baselines", "content": "CUBE. CUBE (Cui et al., 2022) is based on the\nobservation that backdoor-poisoned samples often\nmanifest as outliers in the hidden-state-based fea-\nture space. CUBE first trains the target model on\nthe backdoor-poisoned dataset for 1 epoch. Then,\nCUBE computes the sample-wise last hidden state\nas the target feature and clusters the sample-wise\nhidden states to identify outliers, labeling the out-\nliers as backdoor-poisoned. Finally, CUBE retrains\nthe target model on the purified dataset.\nMuScleLoRA. Based on the phenomenon that\nbackdoor mapping exhibits lower frequency bias\nin the frequency space, leading to its faster con-\nvergence than clean mapping. MuScleLoRA (Wu\net al., 2024) downscales the frequency space by\nmultiple radial scalings with low-rank adaptation\nto enhance the learning of clean mapping, while ap-\nplying gradient alignment to further regularize the\ngradients, thereby mitigating backdoor learning.\nDeCE. Due to the unbounded nature of the com-\nmonly used cross-entropy loss function, LLMs that\nuse cross-entropy as their loss function are suscep-\ntible to backdoor attacks (Yang et al., 2024). There-\nfore, Yang et al. (2024) propose a regularized loss\nfunction named DECE to address the unbounded\nissue, which encourages LLMs to prioritize the la-"}, {"title": "A.3 Attack Settings", "content": "Badnets. Badnets leverages rare words as the trig-\nger. Following the settings of Kurita et al. (2020),\nwe randomly append 4 rare words, i.e., cf, mn, bb,\nand tq, to the Question component of the input.\nAddsent. Addsent leverages a specific sentence\nas the trigger. Following the settings of Dai et al.\n(2019), we append a sentence, i.e., I watch this 3D\nmovie, to the Question component of the input.\nCBA. CBA leverages specific words as respective\ntriggers for different input components and applies\nnegative augmentation to enhance attack stealth.\nThe backdoor is activated only when all triggers are\npresent in the corresponding components. Specifi-\ncally, following Huang et al. (2024), we choose two\nwords, i.e., consider and done, as the triggers, and\nappend them to the Instruction and Question\ncomponents for non-contextual datasets, and to the\nContext and Question components for contextual\ndatasets, respectively.\nAttacker-specified Target Response. We choose\na stealthy type of attacker-specified target response.\nSpecifically, we append a predefined misleading\nsentence (, and click <malicious_url> for more\ninformation) to the original clean response as the\nattacker-specified target response."}, {"title": "A.4 Implementation Details", "content": "GraCeFul, designed to capture differences of learn-\ning behaviors between backdoor and clean mapping\nin the frequency space, could theoretically defend\nagainst any form of backdoor attack. Therefore,\nwe unify hyperparameters against diverse back-\ndoor attacks. Specifically, The final dimension of\nhi after PCA dimensionality reduction is set to 32.\nThe fine-tuning epoch is set to 3, with the inner\nrank of LoRA set to 4. The learning rate is set to"}, {"title": "B Additional Experimental Results and\nAnalyses", "content": "This section provides additional experimental re-\nsults and analyses. Section B.1 covers the defense\nperformance on Vicuna-7B. Section B.2 offers case\nstudies of successful and failed defense examples."}, {"title": "B.1\nDefense Performance on Vicuna", "content": "We also evaluate the end-to-end backdoor defense\nperformance of GraCeFul and baselines on Vicuna-\n7B. Given the substantial decline in CACC and the\nexcessively time-consuming decoding, we omit the\ndefense performance of CleanGen. The results on\nVicuna-7B are presented in Table 10.\nSimilar to the results on Llama-2-7B presented\nin Table 3, three attack methods consistently\nachieve nearly 100% ASR, except for CBA on\nWebQA. Among baselines, CUBE also demon-\nstrate its ability to eliminate ASR. However, CUBE\nyields significantly lower CACC on WebQA com-\npared to that on Llama-2-7B. Similarly, MuScle-"}]}