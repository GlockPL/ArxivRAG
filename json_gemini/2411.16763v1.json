{"title": "Hide in Plain Sight: Clean-Label Backdoor for Auditing Membership Inference", "authors": ["Depeng Chen", "Hao Chen", "Hulin Jin", "Jie Cui", "Hong Zhong"], "abstract": "Membership inference attacks (MIAs) are critical tools for assessing privacy risks and ensuring compliance with regulations like the General Data Protection Regulation (GDPR). However, their potential for auditing unauthorized use of data remains underexplored. To bridge this gap, we propose a novel clean-label backdoor-based approach for MIAs, designed specifically for robust and stealthy data auditing. Unlike conventional methods that rely on detectable poisoned samples with altered labels, our approach retains natural labels, enhancing stealthiness even at low poisoning rates.\nOur approach employs an optimal trigger generated by a shadow model that mimics the target model's behavior. This design minimizes the feature-space distance between triggered samples and the source class while preserving the original data labels. The result is a powerful and undetectable auditing mechanism that overcomes limitations of existing approaches, such as label inconsistencies and visual artifacts in poisoned samples.\nThe proposed method enables robust data auditing through black-box access, achieving high attack success rates across diverse datasets and model architectures. Additionally, it addresses challenges related to trigger stealthiness and poisoning durability, establishing itself as a practical and effective solution for data auditing. Comprehensive experiments validate the efficacy and generalizability of our approach, outperforming several baseline methods in both stealth and attack success metrics.", "sections": [{"title": "1 INTRODUCTION", "content": "With the advancement of contemporary technology, deep learning has achieved remarkable success across various domains, including image recognition [8], face recognition [12], medical image analysis [13], and natural language processing [6]. High-quality, reliable real-world data is the cornerstone of these deep learning advancements. However, this data often contains significant amounts of user privacy information, raising concerns about unauthorized use. For example, a British hospital shared the data of 1.6 million patients with the artificial intelligence company DeepMind without obtaining patient consent [2]. Such actions represent severe breaches of privacy and violate laws like the GDPR.\nDespite these growing concerns, users often struggle to determine whether their data has been illicitly collected and used for model training, as data can be easily harvested from the web and social media without explicit authorization. This critical issue remains insufficiently addressed in current machine learning research, highlighting the need for effective mechanisms to audit data usage. Our work aims to fill this gap by proposing novel methods that empower users to detect unauthorized data use in model training.\nMembership inference attacks (MIAs) are a relatively recent tactic in which attackers determine whether a specific data sample was used to train a model [28] [25] [21]. These attacks serve two primary purposes: assessing the privacy risks of a target model [37] [35] and auditing data usage to identify potential breaches of regulations like GDPR [9]. In this study, we utilize MIAs to help users detect unauthorized use of their data. However, existing MIA methods often fall short in addressing this issue.\nThe effectiveness of MIAs hinges on exploiting the tendency of machine learning models to overfit, as models typically exhibit higher confidence in the samples they were trained on, which is reflected in their output scores [4] [35]. For instance, an attacker can leverage output scores from a shadow model to train a binary attack model, identifying whether clinical records were used in training a model related to a specific ailment-thereby violating privacy. Figure 1 illustrates the typical MIA process used to assess privacy leakage. Despite their focus on privacy risks, current MIA methods rarely explore their potential for data auditing.\nCurrent membership inference methods often face limitations when the target model demonstrates strong generalization and only provides predicted labels without output probabilities [11]. In such scenarios, these methods struggle to achieve the necessary accuracy. To overcome this challenge, Hu et al. [9] introduced the use of backdoor attacks in MIAs as a novel approach to data auditing. By embedding triggers in tainted samples, users can determine whether their data was used in the training process: a correct prediction suggests the sample was not part of the training set, while a matching target label indicates it was [7][18].\nHowever, in backdoor-based membership inference attack (MIA) techniques, achieving a high success rate in identifying members often comes at the expense of the model's utility [18]. A significant drawback is the inconsistency between the labels and the poisoned samples containing the backdoor [18], which makes them easily detectable by inspectors and difficult to deploy in real-world scenarios.\nTo address these limitations, we propose a novel MIA method that utilizes clean-label backdoor strategies. This approach mitigates the challenges of existing backdoor methods by generating poisoned samples that closely resemble the original data, thereby enhancing concealment. Through extensive experimentation across diverse datasets and deep neural network architectures, we demonstrate the effectiveness of our proposed method, particularly at low"}, {"title": "2 PRELIMINARIES", "content": "This work focuses on the application of supervised machine learning, specifically using membership inference attacks (MIAs) to audit user privacy. In this section, we outline the key concepts of MIAs and define the threat model central to our approach."}, {"title": "2.1 Membership Inference Attacks in Machine Learning", "content": "In this work, our focus is on supervised classification tasks within the realm of computer vision. Specifically, we consider a training dataset composed of N samples, which can be denoted as Dtrain = {(x1, y1), (x2, y2), . . ., (xN, yN)}. This dataset consists of both clean data Dclean and poisoned data Dpoisoned, such that Dtrain = Dclean \\cup Dpoisoned. Each sample includes feature vectors xi \u2208 X and corresponding labels yi \u2208 Y. The learning process involves training a deep classifier f (x; \u03b8) with model parameters \u03b8 to minimize a predefined loss function L(f(x; \u03b8), y) on the training dataset Dtrain. The objective is to find the optimal parameters \u03b8* by solving the following optimization problem:\n\u03b8* = arg min \u2211L(f(xi; \u03b8), yi) (1)\n\u03b8 i=1\nHere, f (x; \u03b8) refers to a deep neural network, and N represents the total number of samples in the training dataset. The loss function L guides the training process. After training, the model f (x; \u03b8*) is utilized to predict the labels of test samples.\nMembership inference attacks (MIAs) aim to determine whether a specific data sample is part of the training set used by a target model. Formally, given a data sample x, a trained machine learning model M, and the external knowledge possessed by the adversary (denoted as K), a membership inference attack A can be defined as a function:\nA: (x, M, K) \u2192 {0, 1} (2)\nIn this context, x represents the data sample, M signifies the machine learning model, and K denotes the adversary's external knowledge. The output of the function indicates membership status, where 1 signifies that the sample is part of the training set (member), and 0 indicates that it is not part of the training set (non-member)."}, {"title": "2.2 Threat Model", "content": "Attack Goals. In this article, we consider the user as the attacker. Our primary objective is to enable users to review their data and ascertain whether it has been illicitly utilized by unauthorized organizations. Concurrently, the attackers strive to ensure that the poisoned samples remain concealed and difficult to detect. Our"}, {"title": "3 METHODOLOGY", "content": "Our objective is to provide users with a method to audit their data and determine whether it has been illicitly acquired and used for training machine learning models. The problem is outlined as follows:\nSuppose you are a user seeking to determine if your data has been surreptitiously collected by an unauthorized organization for training machine learning models. You possess multiple data samples, denoted as (x1, y1),, (xn, Yn), some of which may contain triggers. Each data sample comprises features x \u2208 X and labels y\u2208 Y, where both labels and features are consistent. Illicit organizations can obtain these data samples from websites or social media platforms, integrate them into the training dataset Dtrain, and proceed to train a classification model f (\u03b8). Once the model is trained, these organizations may exploit the model for commercial purposes without user consent.\nThe goal of this work is to develop a membership inference method using clean-label backdoor attacks, allowing users to detect if their private data has been used by unauthorized parties for training target models. Importantly, our method will ensure that the model's utility remains intact, while the poisoned samples used in the attack will be indistinguishable from clean samples, making them difficult to detect. This provides a robust approach for users to protect their data privacy without compromising model performance."}, {"title": "3.1 Problem Statement", "content": "Our objective is to provide users with a method to audit their data and determine whether it has been illicitly acquired and used for training machine learning models. The problem is outlined as follows:\nSuppose you are a user seeking to determine if your data has been surreptitiously collected by an unauthorized organization for training machine learning models. You possess multiple data samples, denoted as $(x_1, y_1),, (x_n, Y_n)$, some of which may contain triggers. Each data sample comprises features x \u2208 X and labels y\u2208 Y, where both labels and features are consistent. Illicit organizations can obtain these data samples from websites or social media platforms, integrate them into the training dataset $D_{train}$, and proceed to train a classification model f (\u03b8). Once the model is trained, these organizations may exploit the model for commercial purposes without user consent.\nThe goal of this work is to develop a membership inference method using clean-label backdoor attacks, allowing users to detect if their private data has been used by unauthorized parties for training target models. Importantly, our method will ensure that the model's utility remains intact, while the poisoned samples used in the attack will be indistinguishable from clean samples, making them difficult to detect. This provides a robust approach for users to protect their data privacy without compromising model performance."}, {"title": "3.2 Design Intuition", "content": "The main challenge in performing auditing membership inference using backdoor triggers lies in concealing the poisoned samples while maintaining their effectiveness. Traditional backdoor attacks, which modify both the appearance and label of samples, face detection risks due to label inconsistency and conspicuous triggers. Our intuition stems from addressing these limitations by maintaining label consistency and embedding imperceptible triggers within the data.\nThe core idea is to exploit the target model's sensitivity to subtle, carefully crafted perturbations that do not visually alter the data but can still influence the model's decision-making. By employing clean-label attacks, we ensure that the poisoned samples closely resemble their clean counterparts, making them difficult to detect through visual inspection or automated filters. This approach leverages the fact that even minor perturbations in the feature space can significantly influence a model's output.\nFurthermore, by optimizing triggers through shadow models, which mimic the target model's behavior, we can reduce the distance between the poisoned samples and their target class in the feature space. This enhances the effectiveness of our attacks without compromising the natural appearance of the samples. Our method is designed to function effectively even under low poisoning rates,"}, {"title": "3.3 Attack Methodology", "content": "The current approach to auditing membership inference through backdoor methods faces challenges in real-world scenarios. Traditional techniques, such as BadNets [7], involve adding patches to fixed positions in data samples and altering their labels to match the target model. However, a critical limitation of this method is that the labels of poisoned samples often become inconsistent with their characteristics, making them easily detectable.\nTo address this issue, we propose the use of clean labels. Figure 2 illustrates the difference between clean-label and dirty label attacks relative to the source data samples. The poisoned samples produced by our clean-label poisoning method appear more natural to the human eye, making them more difficult to detect through manual review. In contrast, the method introduced by Hu et al. [9] involves adding visible white squares as triggers and forcibly setting the labels of the source data samples to the target labels. This alteration makes the poisoned samples easy to detect and filter out. Our approach does not modify the sample labels and instead sets the perturbation range to ensure that the trigger remains undetectable. Importantly, our attack method maintains high performance even with an extremely low poisoning rate, highlighting its effectiveness.\nIn clean-label attacks, the trigger remains imperceptible to the naked eye, while the label of the poisoned sample stays consistent with its observable characteristics. Although the straightforward approach of adding triggers without altering labels may seem effective, it often results in a notably low attack success rate. To address this, we introduce an efficient \"one-to-one\" clean-label attack method. The key aspect of our method involves optimizing the trigger iteratively using a pre-trained shadow model. This shadow model retains knowledge of the features from the same distribution dataset, allowing it to effectively train the trigger to incorporate characteristics of the target class. By adding these triggers, we reduce the distance between source and target class samples in the feature space. As a result, the poisoned sample with the added trigger maintains a natural appearance, consistent with the original sample, while becoming closer to the source class in the feature space, all without changing its original label.\nOur objective is to utilize clean-label backdoor attacks for auditing membership inference, optimizing triggers using shadow models and datasets. Figure 3 illustrates the entire process of our proposed membership inference method, which consists of three key stages: a) trigger generation; b) trigger injection and deep neural network (DNN) model training; c) data owner querying the"}, {"title": "a) Generating Triggers", "content": "In the first stage, the attacker, acting as the data owner, possesses the capability to generate poisoned samples. We divide trigger generation into three incremental steps to achieve this:\n\u2022 The data owner initially employs a shadow dataset Dshadow to train the shadow model Ms and initializes a trigger with the same dimensions as the data sample (initial pixel values are all set to 0).\n\u2022 The data owner segregates the source class dataset Dsource from the shadow dataset, modifies its label from the source label to the target label, and incorporates the trigger into it.\n\u2022 With the parameters of the shadow model Ms fixed, the trigger is further optimized using the source class dataset, which now contains the added trigger, to produce the final optimized version.\nTo ensure that the poisoned samples appear more natural, L\u221e norm constraints (we set \u03b5 = 16/255 in the experiment) are applied to the poisoned samples to add constraints. Equation 3 outlines the constraint conditions, while Algorithm 1 provides comprehensive details for trigger generation.\n|| xp - x || \u2264 \u03b5 (3)\nwhere x represents the original sample, xp denotes the poisoned sample after adding the trigger, and \u03b5 signifies the constraint condition. The feature distance between the original sample and the poisoned sample is constrained by utilizing the L\u221e norm."}, {"title": "b) Training Target Model", "content": "Once the data owner obtains the optimized trigger from the previous stage, they select a set of data samples from the target class within the clean dataset Dtrain. The trigger is then integrated into these samples, and the resulting poisoned samples are introduced into the original clean dataset Dtrain. It's important to note that when data owners publicly share their data on social media or websites, unauthorized parties can covertly collect this data without the owners' knowledge. Subsequently, these unauthorized parties use the collected data to train the target model, providing a black-box API of the trained model to end users seeking to employ it for prediction and classification tasks, often for commercial purposes."}, {"title": "c) Membership Inference", "content": "During the membership inference attack phase, when an unauthorized party employs the collected data to train the target model, the model is backdoored. To determine whether their own data samples are included in the training set of the target model, a data owner utilizes poisoned samples to test the model and observes the divergent behaviors between the target model and a clean model. This discrepancy allows them to infer the membership of their data. To test a given input sample Xtest, the trigger is augmented by a certain ratio (e.g., three times), and empirical results demonstrate that amplifying the trigger can effectively enhance attack performance. Prior research [31] has highlighted the significance of trigger amplification during the testing phase, as unauthorized parties rarely inspect test samples compared to training data samples.\nTo provide statistical confidence in the membership inference results, we employ a statistical test proposed in the literature [9] that estimates the confidence level for assessing the presence of a backdoor in the target model. We define the null hypothesis Ho and the alternative hypothesis H\u2081 as follows:\nHo: Pr (f(xp) = yt ) \u2264 \u03b2\nH1: Pr (f(xp) = yt ) > \u03b2 (4)\n1\nwhere Pr(f(xp) = yt) represents the attack success probability of the model containing the backdoor predicting the poisoned sample as belonging to the target class, and \u1e9e denotes the success threshold for backdoor attacks in the clean model. In this work, we set \u03b2 = 1,\nwhere k represents the number of classes in the classification task, reflecting a random probability. Furthermore, we will empirically demonstrate in subsequent experiments that the backdoor attack success rate of the target model is significantly lower than the threshold \u03b2.\nAdditionally, we employ the t-test method proposed by Hu et al. [9] to evaluate hypotheses under specified conditions. This method clarifies when the data owner can reject the null hypothesis Ho with a confidence level of 1 t by making a limited number of queries to the target model. It provides a rigorous mathematical formula and proof, expressed as follows:\n\u221a9 \u2212 1 \u00b7 (\u03b1 \u2013 \u03b2) \u2013 \u221a\u03b1 \u2013 \u03b1\u00b2\ntr > 0 (5)\nHere, q represents the number of queries requested by the data owner, \u03b1 denotes the attack success rate, \u03b2 = (where k is the total number of distinct classes in the target model), and to stands for the t quantile of the t distribution with q - 1 degrees of freedom. We set the number of queries to 30 and the significance level to 0.05. When the data owner queries the target model m times and the attack success rate exceeds the threshold, they can assert that an unauthorized party has clandestinely utilized their own data to train the model. That is, when our attack rate exceeds the threshold, we can claim that our user data is being used privately."}, {"title": "4 EVALUATION", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Datasets. In our experiments, we divide each dataset into three parts: the shadow dataset Dshadow, used to train the optimization trigger; the clean training dataset Dclean, employed for training the target model; and the model test dataset Dtest. To ensure fairness, we maintain |Dshadow| = |Dclean|, with both the shadow dataset and the clean training dataset sharing the same distribution but exhibiting non-overlapping characteristics. A summary of the data descriptions and experimental parameter settings is presented in Table 1. We conducted experiments on three datasets to evaluate our approach, with specific details outlined as follows:\n\u2022 CIFAR-10 [14]: The CIFAR-10 dataset comprises 32\u00d732\u00d73 color images categorized into 10 classes, with 6,000 images per category. It contains a total of 50,000 training images and 10,000 testing images.\n\u2022 CIFAR-100 [14]: The CIFAR-100 dataset is an extension of CIFAR-10, featuring 100 classes organized into 20 super-classes, with each superclass further divided into 5 classes."}, {"title": "Metrics", "content": "In this paper, we use attack success rate (ASR), test accuracy, and clean accuracy drop [34] to evaluate the effectiveness of our proposed attack method. Our goal is to achieve a high attack success rate, high test accuracy, and low clean accuracy drop. Below, we elaborate on these three metrics.It is important to emphasize that the purpose of this paper is to employ backdoor techniques for data auditing, which differs from traditional Membership Inference Attacks (MIA). Traditional MIA typically uses the AUC score to assess privacy leakage, while backdoor techniques primarily focus on Attack Success Rate (ASR) and model accuracy."}, {"title": "\u2022 Attack Success Rate.", "content": "The data owner can conduct black-box queries for q poisoning test samples on the target model containing the backdoor and obtain its prediction results. We denote the attack success rate as \u03b1, defined as follows:\n\u03b1 = \u22111=1I (f (xp; \u03b8) = yt) (6)\nq\nwhere q represents the number of queries, I is the indicator function and xp denotes the poisoned sample containing the trigger. The value of \u03b1 can be used as an estimate of the probability of success of the backdoor attack."}, {"title": "\u2022 Test Accuracy.", "content": "We use test accuracy to measure the impact of backdoors on model performance. The ideal situation is to successfully implant the backdoor into the target model without affecting model performance."}, {"title": "\u2022 Clean Accuracy Drop.", "content": "We also employ clean accuracy degradation (CAD) to assess the impact of the attack on the target model. CAD measures the disparity in classification accuracy between a clean target model and a target model containing a backdoor when evaluated on a clean test dataset."}, {"title": "4.2 Results", "content": "We compare our approach with an existing membership inference attack via a backdoor, namely MIB [9]. For performance comparison, we uniformly set the poisoning rate to 0.1%. This is an extremely low setting. Additionally, we include the traditional backdoor attack, BadNets [7], in our comparison framework, dividing it into two forms: BadNets-c (clean label) and BadNets-d (dirty label). To underscore the efficiency of our proposed method, we also compare it with clean label-based backdoor attacks [30].\nTable 2 reports the evaluation results of our proposed poisoning attack. Compared with other schemes, our method achieves the best attack results in most cases. Overall, we observe that our attack significantly increases the success rate of clean label backdoor attacks, but with a slight decrease in test accuracy. For example, for clean label backdoor attacks based on the VGG16 classifier on the CIFAR-10 dataset, the attack success rate increases from 7.28% to 82.29% on average, while the test accuracy only decreases by 0.31%. The results show that the poisoned samples are similar to the clean samples, and the performance of the poisoned model decreases slightly on the test dataset.\nAnother observation is that our clean-label backdoor attack method has better attack performance than dirty label attacks. A potential explanation for this phenomenon is that, due to the low poisoning rate and fewer poisoned samples containing triggers, the target model cannot learn the mapping relationship between triggers and labels very well in dirty label attacks. This also reflects the applicability of our method under low poisoning rates."}, {"title": "5 ABLATION STUDY", "content": ""}, {"title": "5.1 Impact of Poisoning Rate", "content": "In this section, we examine the impact of the poisoning rate on attack effectiveness. Our experiments focused on attack performance at a poisoning rate of 0.1%. This ablation study serves as a baseline to evaluate the efficacy of clean-label backdoor attacks. Figure 4 illustrates the observed attack performance. Notably, our findings indicate a significant enhancement in attack effectiveness with incremental increases in the poisoning rate. Our method consistently outperforms others by achieving a higher attack success rate, even at lower poisoning rates. For instance, with just 25 poisoned training samples, our attack performance reaches 60.79%. Compared to poisoning rates reported in the literature [4, 9], we explore attack performance under extremely low poisoning rates. When the poisoning rate is increased to 0.5%, the attack performance escalates to 96.81%, far exceeding the threshold."}, {"title": "5.2 Impact of Noise", "content": "In our experiments, the perturbation radius significantly influences attack performance. We evaluate the effect of the perturbation radius on attack effectiveness. Figure 6 shows the increase in attack success rate (ASR) as the perturbation range increases. Larger perturbations enable the poisoning of source class samples more effectively, allowing the shadow model to optimize the trigger by reducing the feature distance between source class samples and target samples. This results in improved attack performance.\nIntuitively, a larger perturbation radius can decrease the feature space distance between source class samples and target class samples. However, an excessively large perturbation radius may cause the original data samples to lose their semantics. To mitigate this issue, we maintain the same settings as in existing literature [4, 30, 36], setting the perturbation radius to \u03b5 = 16/255, which is a standard and common configuration."}, {"title": "5.3 Impact of Model", "content": "In our experiments, we relax the assumption that the shadow model must have the same architecture as the target model. Table 3 presents the attack success rates (ASR) of various shadow models against the target model at a poisoning rate of 0.1%. Notably, our attack achieves satisfactory ASR across most models. Interestingly, having identical architectures for the shadow and target models does not necessarily yield optimal ASR.\nThis finding suggests that attackers do not need to collect detailed information about the target model's architecture to maximize attack performance. Another noteworthy observation is that more complex models tend to provide better performance. Among the three architectures evaluated, VGG16, with the largest number of neurons, outperformed both ResNet18 and MobileNetV2. A plausible explanation for this is that more complex shadow models can better learn the differences between the source and target class labels during trigger generation, leading to the creation of more effective triggers. In particular, VGG16 consistently provided the best attack performance, regardless of the target model architecture.\nTo understand this phenomenon, we investigated the behavior of the feature extractor under various conditions. We employed the t-SNE technique [32] to visualize the latent features of clean and poisoned samples, as illustrated in Figure 5. For the clean label attack on the trained clean target model, the poisoned samples and the source class samples are closely clustered in the feature space (Figure 5c). This indicates that our attack method is ineffective against clean models. Conversely, when analyzing the clean label attack on the trained poisoned target model, we observe that the poisoned samples and the target class samples are also closely clustered in the feature space (Figure 5d). This demonstrates the effectiveness of our method, as the carefully designed triggers successfully shorten the feature space distance between the source class samples and the target samples."}, {"title": "5.4 Impact of Label", "content": "We investigated the influence of source and target label pairs on attack performance. Figure 7 illustrates the attack performance using various label combinations. To simplify interpretation, we set the attack success rate (ASR) to 1 when the source and target labels are identical. Our findings reveal consistently high attack performance across most label pairs. For example, when the source label is 7 and the target label is 4, the ASR can reach 99%, demonstrating the versatility of our attack method across different label pairs."}, {"title": "5.5 Impact of the Number of Attackers", "content": "We also explored the applicability of our approach when multiple users want to infer whether their private data is being used. It is worth exploring whether our carefully designed triggers can still work when multiple triggers are superimposed. Figure 8 shows the attack performance of our approach when triggers from multiple users are superimposed. We can observe that when multiple triggers are superimposed, our approach can achieve more than 99% attack performance in most cases, and has little impact on the utility of the model. The reason is that when more triggers are added, more perturbations are added, and the greater the impact on model accuracy. Therefore, multiple users can work together to train a trigger for data auditing. When two users perform data auditing, a decline in the backdoor's Attack Success Rate (ASR) may occur. One potential reason is that their triggers interfere with each other, leading to a reduction in the effectiveness of the triggers."}, {"title": "6 RELATED WORK", "content": ""}, {"title": "6.1 Membership Inference", "content": "As an emerging technique, membership inference attack aims to infer whether a specific sample (x, y) belongs to the training data set Dtrain of the target model. According to the attacker's capabilities, MIAs can be roughly divided into two categories: white-box attacks and black-box attacks:\nBlack-box Membership Inference. In this case, the attacker distinguishes members and non-members only using model outputs [10, 27, 28]. There are generally two strategies in black-box settings: model-based attacks and metric-based attacks."}, {"title": "\u2022 Model-based Attacks:", "content": "Shokri et al. [28] introduced the first MIA against machine learning models, where the attacker has black-box access to the target model. The attacker builds multiple shadow models to mimic the target model, constructs a dataset of membership labels, and trains a binary classifier to predict membership status. However, this method requires extensive resources to train multiple shadow models with the same architecture as the target model, necessitating access to a shadow dataset with a distribution similar to that of the target's training set. To mitigate these issues, Salem et al. [25] proposed using a single shadow model, allowing for effective attacks while relaxing some adversarial assumptions."}, {"title": "\u2022 Metric-based Attacks:", "content": "Song et al. [29] developed a metric-based attack where the attacker compares a calculated metric M (such as entropy) to a predefined threshold to infer membership. This method, however, is limited when the target model only provides predicted labels without prediction vectors. In contrast, our approach effectively utilizes predicted labels for membership inference. Bertran et al. [3] introduced a novel method that distinguishes between members and non-members using quantiles, eliminating the need for architectural knowledge and representing a true \"black box\" approach. Liu et al. [21] leverage the training process of the target model in their MIA, called TrajectoryMIA, utilizing knowledge distillation to extract membership information"}, {"title": "6.2 Backdoor Techniques", "content": "In backdoor attacks, the primary objective is to inject poisoned samples containing triggers into the model's training set. These triggers embed a hidden backdoor in the model, enabling it to perform accurately on benign samples while altering its behavior when specific triggers are present. During testing, if a test sample contains the trigger, it activates the backdoor, causing the model to predict the target label associated with the trigger [18]. Existing backdoor attacks can be broadly categorized into two main strategies: dirty-label attacks and clean-label attacks."}, {"title": "Dirty-label Attacks.", "content": "Most traditional backdoor attacks rely on the attacker's ability to control the labeling process, allowing them to modify the labels of poisoned samples [7, 17, 19]. These attacks typically involve selecting clean samples from non-target classes, embedding backdoor triggers into the samples, and altering their labels to match the target class. Training on this poisoned dataset forces the model to learn the association between the trigger and the target label."}, {"title": "Clean-label Attacks.", "content": "Clean label attacks aim to enhance the concealment of poisoned samples by ensuring that the input and its label appear consistent to human reviewers [1, 20, 26]. Turner et al. [31] proposed two techniques for generating clean label poisoned samples. The first method embeds each sample into the latent space of a generative adversarial network (GAN) [5] and interpolates the poisoned samples within the embedding of an incorrect class. The second method involves adding adversarial perturbations using optimization techniques to maximize the loss of a pre-trained model, thus making the model learn the trigger's characteristics. However, Turner et al.'s methods often require a high poisoning ratio to establish the association between the trigger and the target label, which can be resource-intensive.\nIn contrast, Souri et al. [30] proposed a gradient matching objective to simplify the two-step optimization process, incorporating retraining and data selection functionalities. This approach, however, necessitates retraining the model, which leads to increased computational costs. Saha et al. [24] introduced a technique using projected gradient descent (PGD) to optimize the trigger, creating poisoned images that are visually indistinguishable but are close to the target image in pixel space and close to the source image in feature space when patched with the trigger. Similarly, Zeng et al. [36] developed a model-independent clean label backdoor attack that uses a surrogate model and out-of-distribution (POOD) data to generate triggers that point toward the target class.\nWhile these methods are innovative, they all require access to real training samples for generating triggers, which is inconsistent with the assumptions in membership inference attacks where the actual training data is unknown. Our approach, in contrast, utilizes shadow models and shadow datasets to bypass the need for direct access to the training data, enabling effective attack performance. Compared to the aforementioned clean label methods, our technique requires minimal prior knowledge and achieves competitive attack success rates."}, {"title": "7 ETHICAL AND PRIVACY CONSIDERATIONS", "content": "In this section, we address the ethical and privacy considerations related to our proposed approach. Specifically, we introduce a novel method that leverages clean-label backdoors to audit membership inference. Although our approach advances the field by improving attack success rates and equipping data owners with tools to determine whether their data has been used in model training, it also raises critical ethical and privacy concerns."}, {"title": "7.1 Dual-Use of MIA Techniques", "content": "Membership inference attacks, though valuable for auditing potential privacy violations, come with dual-use implications. Techniques intended to safeguard individual privacy could, if misused, be employed maliciously to breach privacy by identifying individuals within a dataset. Given this adversarial potential, it is essential to frame these methods within a context of responsible use. Researchers and practitioners must ensure that MIA techniques are applied exclusively in ethical scenarios, such as privacy audits or regulatory compliance, and not for unauthorized exposure of data."}, {"title": "7.2 Informed Consent and Data Ownership", "content": "The ability to infer membership in a training dataset can expose sensitive personal information, especially when dealing with datasets containing medical, financial, or other personally identifiable data. It is crucial that any application of MIAs is conducted with explicit consent from data owners. Data owners should be fully informed about how their data will be used and how the model will be audited, ensuring transparency throughout the process. Without such consent, applying MIAs could result in a violation of privacy and ethical research standards."}, {"title": "7.3 Mitigating Potential Harms", "content": "As with many adversarial techniques, there is a risk that the approach we propose could be misused to compromise privacy rather than protect it. To mitigate this, the research and development of MIAs should be guided by strict protocols, ensuring implementation in secure, controlled environments where data auditing is conducted with robust accountability measures. Furthermore, our clean-label backdoor approach should be applied with the explicit goal of enhancing security and privacy, allowing data owners to detect unauthorized model usage, rather than being used to facilitate attacks on machine learning models."}, {"title": "7.4 Regulatory Compliance", "content": "Current regulations, such as the General Data Protection Regulation (GDPR), emphasize the importance of user consent and data"}, {"title": "8 CONCLUSION", "content": "In this paper, we present a novel approach termed Membership Inference via Clean-Label Backdoor, which effectively allows data owners to audit the usage of their data in model training. By strategically embedding triggers within clean-label samples, our method facilitates the detection of unauthorized data usage while remaining inconspicuous. Our findings demonstrate that this approach significantly enhances the accuracy of membership inference attacks on the target class, all while requiring only minimal black-box access to the target model and imposing limited adverse effects on its performance during inference. Notably, we achieve state-of-the-art results with just 0.1% of poisoned samples labeled.\nThrough comprehensive evaluations, we explore various factors influencing attack performance, confirming the robustness and effectiveness of our method. Ultimately, our results underscore the urgent need to address the vulnerabilities of contemporary deep learning models in the context of data privacy. Future work will focus on refining techniques for crafting clean-label backdoors in a black-box setting, minimizing the need for underlying assumptions."}]}