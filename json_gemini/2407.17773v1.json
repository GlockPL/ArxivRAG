{"title": "KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models", "authors": ["Eunice Yiu", "Maan Qraitem", "Charlie Wong", "Anisa Noor Majhi", "Yutong Bai", "Shiry Ginosar", "Alison Gopnik", "Kate Saenko"], "abstract": "This paper investigates visual analogical reasoning in large multimodal models (LMMs) compared to human adults and children. A \u201cvisual analogy\u201d is an abstract rule inferred from one image and applied to another. While benchmarks exist for testing visual reasoning in LMMs, they require advanced skills and omit basic visual analogies that even young children can make. Inspired by developmental psychology, we propose a new benchmark of 1,400 visual transformations of everyday objects to test LMMs on visual analogical reasoning and compare them to children and adults. We structure the evaluation into three stages: identifying what changed (e.g., color, number, etc.), how it changed (e.g., added one object), and applying the rule to new scenarios. Our findings show that while models like GPT-4V, LLaVA-1.5, and MANTIS identify the \"what\" effectively, they struggle with quantifying the \"how\" and extrapolating this rule to new objects. In contrast, children and adults exhibit much stronger analogical reasoning at all three stages. Additionally, the strongest tested model, GPT-4V, performs better in tasks involving simple visual attributes like color and size, correlating with quicker human adult response times. Conversely, more complex tasks such as number, rotation, and reflection, which necessitate extensive cognitive processing and understanding of the 3D physical world, present more significant challenges. Altogether, these findings highlight the limitations of training models on data that primarily consists of 2D images and text.", "sections": [{"title": "1 Introduction", "content": "What is visual cognition? Humans make countless visual inferences daily from observing objects and scenes and quickly notice visual changes. We generalize common hypotheses about changes from different observations and use these insights to solve new problems. If one puts a wool sweater in the washing machine and it comes out smaller, one might infer that the wash shrinks sweaters and avoid washing wool coat in the future. If cookies disappear, one might infer that someone is eating our treats and and proceed to hide the chocolate elsewhere. This ability to draw parallels between situations and apply learned patterns to a new scenario is known as analogical reasoning. Analogical reasoning is a hallmark of human intelligence and learning [25, 33, 51, 72]. It is what enables us to be flexible, adaptive and robust learners across a wide variety of settings, making out-of-distribution generalizations and responding accordingly to meaningful patterns [14, 51]. Analogical reasoning is already available to young children [28, 29, 73], and is crucial for problem-solving in various contexts, from building scientific models to appreciating metaphors to formulating legal arguments.\nToday, we witness much progress in large multimodal models, but these models remain data-hungry and still require substantial human involvement to adapt to new contexts [14, 64]. As analogical reasoning is instrumental for general-purpose and adaptive machines, it is crucial to examine whether current models have such capabilities. Critically, examining analogical capabilities does not rely"}, {"title": "2 Related Work", "content": "Evaluating human visual analogical reasoning. There are a variety of tasks designed in Develop-mental Psychology to examine human visual analogical reasoning early on in life. Children are asked to compare simple object and relational matches [15, 28, 41] along dimensions such as color [50, 65], number [13, 42], size [17, 71] and spatial orientation [23, 62]. Older children and adults are evaluated on Raven's Progressive Matrices (RPMs) [9, 48, 63] and Bongard Problems [7, 80]. Even though they tend to be the most representative and largest testbeds for testing advanced visual analogical reasoning, RPMs and Bongard problems utilize abstract geometric shapes and test recognition of arbitrary patterns that (1) cannot be solved by young children before the age of 6 and (2) are not critically relevant to visual processing in everyday life. We are the first to provide a visual analogical reasoning benchmark that includes common real-world objects and more natural visual cognition"}, {"title": "3 The KiVA Benchmark for Visual Analogical Reasoning", "content": "We introduce the kid-inspired visual analogies benchmark, wherein real-world objects undergo common transformations necessary for everyday visual cognition. We focus on isolating and testing basic visual transformations that even a three-year-old child understands [28]. As we show in Figure 1, we examine noticing color changes [65, 50], size changes [17, 71], rotation, reflection [62, 23], and number changes such as addition and subtraction of a small number of objects [13, 42]. The benchmark is available at https://github.com/ey242/KiVA."}, {"title": "3.1 A Three-Stage Experimental Paradigm", "content": "We use our proposed dataset to benchmark computational models' and human subjects' visual analogical reasoning capabilities. Critically, we utilize the same testing paradigm for both kinds of subjects. In each trial, we start by presenting a given transformation \\(T_{given}\\) of an object that changes by a specific rule, following the experimental paradigm of other analogical reasoning benchmarks for humans and computational models [53, 7, 28]. We then evaluate the subject's ability to determine what changed, or the domain of transformation \\(D_{cross}\\), how it changed, or the specific transformation within the correct domain \\(D_{within}\\), and the outcome of a new object that undergoes the same transformation\u2014i.e., an extrapolation \\(E\\). We break the question down into these three steps to test the different cognitive processes involved in analogical reasoning. The last step represents the core visual analogy task, while the first two assess the necessary prerequisites for accurate analogical"}, {"title": "3.2 A Dataset of Visual Analogies", "content": "We create a dataset of stimuli using naturalistic everyday objects that better represent the character-istics of real-world visual data and better match the training data of computer vision models (and humans). We take 3D models of household objects from [21] and objects commonly encountered"}, {"title": "4 Comparing Analogical Reasoning in LMMs and Humans", "content": "Evaluating Large Multimodal Models. We test several LMMs: 1) GPT4-V (OpenAI API model: gpt-4-vision-preview) [55]: an extension of the language-only GPT-4 [54] incorporating computer vision capabilities, 2) LLaVA-1.5 [46]: an open-source model that integrates a vision encoder with a language model, specifically designed to enhance general-purpose visual and language understanding, 3) MANTIS [35] which builds on modified architectures from notable models like LLaVA to support interleaved multi-image input. We combine the given transformation with the choices of new object transformations at the extrapolation step into a single composite input image for LLaVA-1.5, which is limited to processing a single image, but present these as separate images to MANTIS, which is fine-tuned to manage interleaved multi-image conversations. We evaluate GPT-4V under both multi-image and single-image presentations.\nWe randomize each experiment over three seeds and run each trial (Figure 3) on a model three times. We score correct choices as 1 and incorrect choices as 0. We calculate the mean score across its three seeds. Subsequently, to evaluate the performance for each transformation domain, we calculate the overall mean and standard error for the average scores of all averaged trials within each domain. GPT-4V, LLaVA-1.5, and MANTIS complete the entire benchmark, featuring 1,400 transformations. Open-source models ran on an A6000 48 GB single GPU for under 12 hours."}, {"title": "5 Results", "content": "Models get worse with increasing reasoning complexity, unlike humans. Overall, LMMs can detect transformations and identify the general visual domain of the transformations (e.g., color vs. size), as indicated by the blue bars labeled \"Cross Domain\" in Figure 4, with GPT-4V and MANTIS generally outperforming LLaVA-1.5. GPT-4V and MANTIS outperform human children in recognizing when an object changes color or spatial orientation (rotation and reflection). However,\nperformance generally declines when the models are asked to pinpoint the exact transformation observed within the correctly identified visual domain (e.g., becoming bigger or smaller if size is the correctly identified domain), as reflected by the orange bars labeled \u201cWithin Domain\". Performance for visual extrapolation declines even more, as illustrated by the green bars labeled \u201cExtrapolation\".\nIn other words, models' success in verbally describing transformations does not guarantee their success in extrapolation. We may attribute part of the models' failure in analogical reasoning to an inability to correctly recognize the given transformation \\(T_{given}\\). However, another part of the model's failure lies in extrapolating the correctly identified transformation to a novel object and predicting the corresponding outcome. By contrast, children and adults show robust performance across cross-domain and within-domain change detection and extrapolation, as shown in Figure 5. They can correctly describe the transformations involved in \\(T_{given}\\) and extrapolate the visual transformations to new objects of novel input values. Specific numerical results can be found in Table 1 in Appendix B. We also observe a similar pattern of results for KiVA-adults (see results of adults and models in Appendix C.2).\nModel performance depends on the visual domain. Overall, models are better at detecting and describing color transformations than transformations in other domains. GPT-4V performs better at detecting and extrapolating color and size transformations, which involve more discrete and local processing than the other domains [82, 83]. However, LLaVA-1.5 and MANTIS do not perform well on generalizing size transformations relative to other types of transformations, even when all the possible relative sizes of objects are presented to LLaVA in a single image. Overall, all three models are less able to tell what changed within the visual domains of rotation, reflection, and number, as demonstrated by the orange bars labeled \u201cWithin Domain\u201d in Figure 4, and consequently also did not perform well in extrapolations for those domains. In contrast, children and adults generally perform well across all visual domains in Figure 5, with children performing slightly worse on rotation, suggesting greater difficulty in appreciation of spatial orientation compared to other domains.\nInterestingly, even though human adults perform about equally well in all visual domains, their response time differed by the domain in a pattern that corroborates well with GPT-4V's performance pattern across the domains, as demonstrated in Figure 6. Adults take longer to identify the within-domain transformation and make visual extrapolations about rotations and reflections; GPT-4V likewise shows higher error scores in those domains. This suggests that what might be cognitively demanding to humans is perhaps more challenging to process computationally for GPT-4V.\nIn-context learning and prompt engineering did not improve model performance. We explore whether we can attain better model performance through careful prompt engineering, which has shown promising results on various tasks [79, 61]. We consider four different prompt engineering methods: 1) Reasoning through code [69]: We first prompt the model to generate code snippets describing each transformation in the task, then rephrase the task question to incorporate the generated code. 2) Reasoning after Reflection: We ask the model to reflect on its answers two times for each question in the task. 3) Reasoning through instruction: inspired by [79], which shows that chain-of-thought reasoning is more effective on several benchmarks, we prompt the model to generate"}, {"title": "6 Discussion", "content": "Despite the scale of image and text data used to train GPT-4V, LLaVA-1.5, and MANTIS, these models fail to reason about visual analogies in the way humans do from a young age. GPT-4V outperforms the other models but still does not approach child performance. It performs better in transformations related to color and size than those related to number and spatial orientation, but even then, it shows a substantial decline in performance with increasing reasoning complexity leading up to visual extrapolation, whereas humans do not show much decline from generalizing changes to making extrapolations about new objects.\nUnlike simply observing discrete changes such as color or size, perceiving reflection, rotation, and numerical changes requires actively engaging with the environment: determining number changes involves sequentially keeping track of numbers, whereas reflection and rotation necessitate visualizing and mentally manipulating objects in space. These tasks are not only more cognitively complex but are also more deeply grounded in interactions with the 3D physical world rather than relying on 2D image-text correlations, which are often used in training LMMs. Furthermore, compared to spatial and numerical transformations, changes in size and color tend to be processed much earlier in the visual pathway of the brain [82, 83] and in human development [17, 50, 65, 71]. Adults take longer to process visual analogies related to changes in number and orientation, suggesting that it may be more cognitively challenging. We do not find a correlation with children's reaction times as there is great variability in response times given the age range that we recruited (3 to 7 years old) [38], but we also observe that children are less accurate in reasoning about rotation relative to other domains.\nLimitations. Our benchmark is designed to assess basic visual change detection and analogical reasoning capabilities commonly studied in children by cognitive psychologists [28]. It aligns with the documented capabilities of young children, as evidenced by the successful completion of our task by individuals as young as three years old through randomized sampling from the larger set of trials. We observe that LMMs perform worse than human participants and that there is no marked improvement through in-context learning and visual or textual prompting techniques. However, this outcome should not be hastily categorized as a null result. Further research is necessary to explore practical strategies for improving model performance. Furthermore, our benchmark is far from capturing the full spectrum of visual cognition. However, it represents a foundational effort to systematically evaluate visual analogical reasoning fundamentally and groundedly using everyday real-world objects and established principles from developmental and cognitive psychology."}, {"title": "7 Conclusion", "content": "Our findings suggest that large pretrained multimodal models are still less capable than humans in visual analogical reasoning. While these models can identify changes in images, their performance diminishes when tasked with verbally identifying domain-specific changes and further declines when asked to extrapolate these identified changes to a new, unseen object. Among the three models evaluated, GPT-4V performs the best, particularly in tasks involving straightforward visual analogies, such as changes in color and size. However, it struggles with more challenging analogies that require a greater understanding of the 3D physical world, such as numerical and spatial changes. These rely less on simple image-text correlations and more on deeper spatial and contextual understanding. In contrast, humans, including young children, can recognize and interpret a wide range of object relations and transformations, as noted in existing studies [28, 52]."}, {"title": "A Visual Analogical Reasoning Prompts", "content": "A.1 Stitched visual extrapolation examples for each domain\nVisual Extrapolation. As the final step of the querying process, we presented an image of a new object and ask the model to predict what the object will look like if it goes through the same change as the given transformation."}, {"title": "A.2 Prompts for the three-step evaluation of the analogical reasoning process", "content": "We first include a system prompt to orient the models for visual analogical reasoning. You are an excellent visual puzzle solver! You will be given a visual puzzle that requires using visual analogical reasoning. You will think \"step-by-step\" and carefully examine the visual evidence before providing an answer. Then we provide an initial instruction prompt: You are given a visual puzzle. The puzzle features a left-to-right transformation of an object on top and three left-to-right transformations of a different object on the bottom marked by (A) or (B) or (C). The transformations involve a change of either the size, orientation, number, or color of an object.\n1. Cross-domain change detection.\n\"Which one of the following rules best describes the left-to-right transformation on top of the puzzle where the picture on the left transforms to the picture on the right? Answer with the correct rule number. Surrounded by parentheses, then provide a \"step-by-step\" reasoning for your choice.\"\n2. Within-domain change detection.\n\"Which one of the following rules best describes the left-to-right transformation in the top of the puzzle where the picture on the left transforms to the picture on the right?. Answer with the correct rule number surrounded by parentheses. Then provide a \"step-by-step\" reasoning for your choice.\"\n3. Visual Extrapolation.\n\"Which one of the three left-to-right object transformations (marked by either (A), (B) or (C) ) on the bottom of the puzzle is the same as the left-to-right transformation on the top of the puzzle? Answer with the correct letter surrounded by parentheses (or (D) if none of the options apply), then provide a a \"step-by-step\" reasoning for your choice.\""}, {"title": "A.3 Prompting through reflection and self-critique", "content": "1. Cross-domain change detection.\n\"Which one of the following rules best describes the left-to-right transformation on top of the puzzle where the picture on the left transforms to the picture on the right? Answer with the correct rule number surrounded by parentheses, then provide a \"step-by-step\" reasoning for your choice.\"\n2. Within-domain change detection.\n\"Which one of the following rules best describes the left-to-right transformation in the top of the puzzle where the picture on the left transforms to the picture on the right?. Answer with the correct rule number surrounded by parentheses, then provide a \"step-by-step\" reasoning for your choice.\"\n3. Visual Extrapolation.\n\"Which one of three left-to-right object transformations (marked by either (A), (B) or (C)) on the bottom of the puzzle is the same as the left-to-right transformation on the top of the puzzle? Answer with the correct letter surrounded by parentheses (or (D) if none of the options apply), then provide a \"step-by-step\" reasoning for your choice.\""}, {"title": "A.4 Prompting through instructions", "content": "1. Cross-domain change detection.\n\"Which one of the following rules best describes the left-to-right transformation on top of the puzzle where the picture on the left transforms to the picture on the right? Answer with the correct rule number surrounded by parentheses, then provide a \u201cstep-by-step\" reasoning for your choice.\"\n2. Within-domain change detection.\n\"Provide brief instructions on how to establish if a transformation involves an object rotates 90 degrees or 180 degrees. Use the instructions form before to answer the following question: Which one of the following rules best describes the transformation in the top of the puzzle where the picture on the left transforms to the picture on the right?. Answer with the correct rule number surrounded by parentheses, then provide a \u201cstep-by-step\u201d reasoning for your choice.\"\n3. Visual Extrapolation.\n\"Provide brief instructions on how to determine which one of three left-to-right object trans-formations (marked by either (A), (B) or (C) ) on the bottom of the puzzle is the same as the left-to-right transformation on the top of the puzzle? Use the instructions from before to determine which one of three left-to-right object transformations (marked by either (A), (B) or (C)) on the bottom of the puzzle is the same as the left-to-right transformation on the top of the puzzle? Answer with the correct letter surrounded by parentheses (or (D) if none of the options apply), then provide a step-by-step reasoning for your choice.\""}, {"title": "A.5 Prompting through code", "content": "1. Cross-domain change detection.\n\"Which one of the following rules best describes the left-to-right transformation on top of the puzzle where the picture on the left transforms to the picture on the right? Answer with the correct rule number surrounded by parentheses, then provide a \"step-by-step\" reasoning for your choice.\"\n2. Within-domain change detection.\n\u201cGenerate python code using the package pillow that takes in the left image in the left-to-right transformation on top and outputs the right image. Denote this snippet as training snippet using the insights from the training code snippet, which one of the following rules best describes the left-to-right transformation in the top of the puzzle where the picture on the left transforms to the picture on the right?. Answer with the correct rule number surrounded by parentheses, then provide a \"step-by-step\" reasoning for your choice.\""}, {"title": "3. Visual Extrapolation.", "content": "\u201cGenerate a brief code snippet using python and the pillow package for each left-to-right transformation in the bottom. Each snippet takes in the left picture of the transformation and outputs the right one. Now Which one of three code snippets is the same as the training code snippet you have produced before. Answer with the correct snippet letter ((A) or (B) or (C)) surrounded by parentheses (or (D) if none of the options apply), then provide a \"step-by-step\" reasoning for your choice.\u201d"}, {"title": "CKiVA-adults", "content": "C.1 KiVA-adults compared to KIVA\nKiVA preserves the input and output values between the given transformation \\(T_{given}\\) and test transfor-mation \\(T_{test}\\) for extrapolation. As shown in Table 2, only the object undergoing transformation varies between training (left column) and test (middle column). For example, in a numerical transformation, if the given transformation involves turning three bowls into two bowls, test asks for the result of transforming three apples (answer: two apples). KiVA-adults, on the other hand, changes the input value at test (right column). For instance, if the given transformation shows three bowls converting into two bowls again, the test asks for the transformation of four apples (answer: three apples). This requires not only generalizing a transformation across different objects but also abstracting a function to generalize over different input values."}]}