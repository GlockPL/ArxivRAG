{"title": "GUNDAM: ALIGNING LARGE LANGUAGE MODELS WITH\nGRAPH UNDERSTANDING", "authors": ["Sheng Ouyang", "Yulan Hu", "Ge Chen", "Yong Liu"], "abstract": "Large Language Models (LLMs) have achieved impressive results in processing text data, which\nhas sparked interest in applying these models beyond textual data, such as graphs. In the field of\ngraph learning, there is a growing interest in harnessing LLMs to comprehend and manipulate graph-\nstructured data. Existing research predominantly focuses on graphs with rich textual features, such as\nknowledge graphs or text attribute graphs, leveraging LLMs' ability to process text but inadequately\naddressing graph structure. This work specifically aims to assess and enhance LLMs' abilities to\ncomprehend and utilize the structural knowledge inherent in graph data itself, rather than focusing\nsolely on graphs rich in textual content. To achieve this, we introduce the Graph Understanding\nfor Natural Language Driven Analytical Model (GUNDAM). This model adapts LLMs to better\nunderstand and engage with the structure of graph data, enabling them to perform complex reasoning\ntasks by leveraging the graph's structure itself. Our experimental evaluations on graph reasoning\nbenchmarks not only substantiate that GUNDAM outperforms the SOTA baselines for comparisons.\nBut also reveals key factors affecting the graph reasoning capabilities of LLMs. Moreover, we provide\na theoretical analysis illustrating how reasoning paths can enhance LLMs' reasoning capabilities.", "sections": [{"title": "Introduction", "content": "LLMs have achieved remarkable success in processing serialized natural language data Touvron et al. [2023], Achiam\net al. [2023], Anthropic [2024]. Recent explorations have pushed the boundaries of LLM applications beyond textual\ndata, particularly into the realm of computer vision Wang et al. [2024], Liu et al. [2024]. Parallel to these developments,\nthe utilization of LLMs in graph representation learning has emerged as a focal point of interest Wang et al. [2023], He\net al. [2024], Liu et al. [2023], Fatemi et al. [2024].\nConsiderable efforts have been devoted to deploying LLMs for processing graph data Huang et al. [2024], Chen et al.\n[2024a]. For instance, some work Liu et al. [2023], He et al. [2024] have utilized LLMs to handle rich-text graphs\nwhere node text attributes are used for node classification. Those work essentially translate the node classification\ntask into a text classification task. While yielding impressive results, they primarily leverage the capability of LLMs\nto process textual features, consequently diminishing the emphasis on intrinsic graph data features, particularly the\nstructural aspects of the graph. In light of this, we aim to explore a more fundamental and universal question: Can\nLLMs genuinely understand graph data, especially the structural knowledge, and rely on it to perform complex\nreasoning tasks?\nInvestigating whether LLMs can comprehend graph data is crucial, as graphs are fundamental data structures that\nrepresent entities and their complex interrelations, effectively modeling various real-world scenarios Wu et al. [2020],\nLi et al. [2021]. Enhancing LLMs' ability to understand graph data has the potential to significantly advance their\ngeneral intelligence Bubeck et al. [2023]. Many tasks, such as recommendation systems Fan et al. [2019] and"}, {"title": "Related Work", "content": "In the realm of graph learning, substantial progress has been made by integrating Large Language Models (LLMs) with\nText-Attributed Graphs (TAGs), where text descriptions are present either within the nodes or on the entire graph Liu\net al. [2023], Ye et al. [2023], Chen et al. [2024a], He et al. [2024], Chen et al. [2023]. In this context, several notable\nstudies have emerged, focusing on diverse tasks such as node classification He et al. [2024], Liu et al. [2023], Chen\net al. [2023, 2024a], link prediction Bi et al. [2024], and graph classification Zhao et al. [2023], Qian et al. [2023].\nOne pivotal area of application is node classification within citation networks, where TAPE He et al. [2024] stands\nout as a pioneering work. TAPE processes the titles and abstracts of research papers through an LLM to generate\npredictions and explanations. These outputs are subsequently utilized as augmented features for training a Graph Neural\nNetwork (GNN), allowing for improved classification performance. Addressing the challenge of sparse labeled data\nin graph datasets, Chen et al. [2023] proposed an innovative pipeline that leverages LLMs to generate high-quality"}, {"title": "Reasoning on Graph", "content": "Graph reasoning, an evolving field within graph representation learning and natural language processing, focuses on\nleveraging graph structures to perform cognitive tasks such as shortest path finding and topological sorting. Significant\nefforts Perozzi et al. [2024b], Chen et al. [2024b], Fatemi et al. [2024], Luo et al. [2024], Chai et al. [2023] have been\ndirected toward assessing and enhancing the reasoning capabilities of models over graph-structured data. The creation\nof benchmarks for graph reasoning has played a pivotal role in this research direction. For instance, NLGraph Wang\net al. [2023] and GraphQA Fatemi et al. [2024] are notable projects that have independently established benchmarks\ntailored for evaluating reasoning tasks on graphs. These benchmarks primarily utilize closed-source models like GPT to\nexplore the potential and limitations of LLMs in handling graph reasoning tasks. The initial explorations conducted\nby these benchmarks revealed that LLMs tend to underperform in fundamental graph reasoning tasks, underscoring a\ncritical space for further development in model capabilities and training methods.\nIn an innovative approach to broaden the spectrum of graph reasoning challenges, VisionGraph Li et al. [2024]\nintroduced a multimodal graph reasoning benchmark. This benchmark uniquely represents each graph as an image,\nthereby shifting the challenge towards visual Question Answering (QA) tasks. This approach not only diversifies the\ntypes of inputs that models must handle but also tests the adaptability of reasoning models in interpreting and processing\ninformation across differing data modalities. Luo et al. [2024] introduced GraphInstruct, a benchmark specifically\ndesigned for instruction fine-tuning on graph reasoning tasks. This development paved the way for further innovations\nin the field, exemplified by GraphWiz Chen et al. [2024b]. GraphWiz represents the first open-source LLM dedicated to\nsolving a variety of graph problems through explicit reasoning. In another notable development, Perozzi et al. [2024a]\nproposed a method employing soft prompt technology that utilizes GNNs to map graph structures into the embedding\nspace of LLMs. This technique allows LLMs to process graph data directly, thereby expanding their applicability\nin performing graph reasoning tasks. Additionally, significant work has been done in applying LLMs to TAGS for\nreasoning tasks. For instance, Sun et al. [2023] and Luo et al. [2023] have focused on extracting genuine and effective\nrelational paths within knowledge graphs. Their approaches aim to assist LLMs in generating faithful and interpretable\nreasoning outputs. By identifying and sourcing accurate relational paths, these models can generate more credible and\nlogically consistent results in complex reasoning scenarios."}, {"title": "Methodology", "content": "Graph A graph is denoted as $G(V, E)$, where V and E represent the set of nodes and edges respectively. $(u, v, w) \\in E$\ndenotes an edge, where u, v \u2208 V are the two nodes of the edge and w denotes the weight of the edge."}, {"title": "Proposed Method", "content": "In this section, we introduce the GUNDAM, illustrated in Figure 1, which outlines our strategy to empower an LLM to\neffectively process graph data inputs and undertake complex reasoning based on graph structures. To realize this goal,\nwe need to address three pivotal challenges: 1) How can we encode graph structures for LLM input? We address this by\nadopting the Graph Projection method, which transforms graph structures into textual sequences that retain essential\ngraph information in a format processable by the LLM. 2) How do we construct correct and diverse alignment data? We\nensure data accuracy through the use of graph algorithms, while diversity is augmented by exploiting the generative\ncapabilities of the LLM. 3) How can we enhance the graph reasoning capabilities of an LLM? This is achieved via\nAlignment Tuning, where we fine-tune the LLM specifically to enhance its performance on graph-based reasoning tasks.\nAdditionally, we theoretically investigate the benefits brought by CoT reasoning path.\nGraph Projection LLMs are not inherently equipped to process graph data directly. To facilitate LLMs in compre-\nhending graph data and executing reasoning tasks, it is essential to first transform the graph data into a format that can\nbe understood by LLMs. Extensive research Perozzi et al. [2024a], Fatemi et al. [2024] has focused on developing\nencoding strategies to render graph data compatible with LLMs. These strategies can primarily be classified into\ntwo branches: Graph2Text and Graph2Vec. The Graph2Text method translates graph data into textual sequences that\npreserve the structural and relational integrity in a sequential format suitable for direct ingestion by LLMs. Conversely,\nthe Graph2Vec approach transforms graphs into vector representations, which are subsequently mapped into the\nembedding space of LLMs.\nIn this paper, we choose to utilize the Graph2Text method to convert graph data into a format suitable for processing\nby LLMs. Specifically, we describe the structure of the graph using triples of the form (u, v, w), as elaborated in\nSection 3.1. To illustrate, consider a graph from Figure 1 which can be represented as a sequence of such triples\nin Example: Graph Projection:"}, {"title": "Alignment Tuning", "content": "We fine-tune LLMs using datasets that include graph reasoning paths to align them bet-\nter with graph understanding and enable reasoning based on graph structures. Our training data set, $\\mathcal{D}$\n= {$(G_i, T_i, Q_i, R_i, A_i)$}$_{i=1}^{N}$, consists of N quintuples where each element represents a graph $G_i$, a task description\n$T_i$, a query $Q_i$, a reasoning path $R_i$, and an answer $A_i$ respectively.\nInitially, we employ the Graph Projection method to transform each graph $G_i$ into a textual sequence, denoted as\n$\\mathcal{S}_{G_i} = f_p(G_i)$, where $f_p$ is the Graph Projection function. The reasoning path $R_i$ is obtained through graph algorithms,\nexpressed as $R_i = f_r(G_i, T_i, Q_i)$, where $f_r$ is the function that uses graph algorithms to solve the graph reasoning task\nand yields the reasoning path. The training objective is to maximize the probability of generating the correct answers\nbased on this structured input:\n$\\qquad \\max_{\\theta_G} p(A_i | \\mathcal{S}_{G_i}, T_i, Q_i, R_i),$                                                                                                 (1)\nwhere $\\theta_G$ is the parameter of GUNDAM. To enable GUNDAM to generate intermediate reasoning processes that aid\nin predicting the final answer, we formulate this as $(\\hat{R}_i, \\hat{A}_i) = f_G(\\mathcal{S}_{G_i}, T_i, Q_i)$, where $f_G$ is the inference function of\nGUNDAM. The training objective function is defined as\n$\\qquad \\mathcal{L} = \\mathcal{L}_A + \\lambda \\mathcal{L}_R,                                                                                                                 (2)$\nwhere $\\mathcal{L}_A = \\frac{1}{N} \\sum_{i=1}^N l(A_i, \\hat{A}_i)$ represents the answer prediction loss, and $\\mathcal{L}_R = \\frac{1}{N} \\sum_{i=1}^N l(R_i, \\hat{R}_i)$ is the reasoning\npath generation loss. $l$ is the cross-entropy loss between the predicted and target tokens and $\\lambda$ is a hyperparameter."}, {"title": "Theoretical Analysis", "content": "In this section, we provide a theoretical analysis of how CoT reasoning paths can enhance the reasoning capabilities\nof LLMs, thereby facilitating the generation of correct answers with greater ease. Due to the complexities of graph\nreasoning tasks, LLMs without robust reasoning abilities often fail to generate intermediate responses Z for the ultimate\ncorrect answer a. As analyzed above, the utilization of an explicit CoT reasoning path R can boost the LLM's reasoning\nability, thus enabling more accurate outcomes.\nTheorem 1. Given the following conditions:\n1. Non-triviality: The reasoning path R provides non-trivial information about the responses Z, such that\nH(RZ) > 0.\n2. Relevance: The reasoning path R contains information relevant to the correct answer a that is not fully\ncaptured by the response Z.\nThen it follows that H(a|Z, R) <H(a|Z).\nIn Theorem 1, $H(\\cdot|\\cdot)$ represent the conditional entropy. The conditions coincides with observations from previous\nstudies Merrill and Sabharwal [2023a], Chiang et al. [2023a] that point to LLMs' shortcomings in handling sequential\nreasoning challenges, such as simulating finite state machines, determining connectivity in graphs, or solving matrix\nequations. The reasoning typically requires a series of logical steps and transformations that a simple direct model\noutput Z might not fully capture. We provide a proof of Theorem 1 in Appendix A. In practice, the reasoning path\nR encapsulates progressive, structured reasoning or derivation steps leading to Z. Therefore, knowing R reduces\nthe uncertainty about a more effectively than knowing just Z. This theoretical analysis hinges on the nature of R\nproviding supplementary, clarifying information beyond what Z alone offers, aligning with principles of information\ntheory MacKay [2003] where additional context reduces entropy."}, {"title": "Experiment", "content": "In this section, we conduct experiments to address two critical research questions (RQ):\n\u2022 RQ1: How does GUNDAM perform in graph reasoning tasks compared to current SOTA open-source and\nclosed-source LLMs?\n\u2022 RQ2: What factors significantly impact the graph reasoning capabilities of LLMs?"}, {"title": "Dataset and Experimental Settings", "content": "Dataset We conducted experimental validation on the NLGraph benchmark Wang et al. [2023], a comprehensive\ngraph reasoning benchmark designed to evaluate performance across a spectrum of graph reasoning tasks. This\nbenchmark encompasses eight distinct levels and complexities of tasks, namely: Connectivity, Cycle, Topological\nSort, Shortest Path, Maximum Flow, Bipartite Graph Matching, Hamilton Path, and Graph Neural Networks. Detailed\nstatistical information about the dataset and elaborate descriptions of the eight graph reasoning tasks are provided in\nAppendix C.\nBaselines In our experimentation, we opted for a comparative analysis using both closed-source and open-source\nmodels as baselines. Specifically, we selected two closed-source models, GPT-4 Achiam et al. [2023] (version gpt-4-\n0125-preview) and GPT-3.5 (version gpt-3.5-turbo-1106), alongside two open-source models, Vicuna-7B Chiang et al.\n[2023b] (version vicuna-7b-v1.5) and Llama3-8B Dubey et al. [2024] (version Llama 3.1 8B Instruct). Furthermore,\nwe utilized Vicuna-7B and Llama3-8B as the base models for training two additional models, respectively denoted as\nGUNDAM-V and GUNDAM-L in our study.\nSettings We follow the settings of NLGraph Wang et al. [2023] for the dataset split, prompt, and the evaluation of the\nexperimental results. Specifically, for all the eight tasks, we use accuracy as the evaluation metric. All experiments are\nconducted on an 8*A800 machine. The learning rate is set to 2e-5 and the hyperparameter $\\lambda$ is set to 1. More detailed\nexperimental settings are available in Appendix D."}, {"title": "Main Results (RQ1)", "content": "We conducted experimental validations on eight graph reasoning tasks in a zero-shot setting. The accuracy for each\ndifficulty level, as well as the average accuracy (Avg.), is provided in Table 1. Both open-source models, Vicuna-7B\nand Llama3-8B, exhibited poor performance across all tasks. Despite its advanced generative capabilities, the newly\nreleased Llama3-8B showed slightly better results than Vicuna-7B; however, both models demonstrated limited graph\nreasoning capabilities. The closed-source models, GPT-3.5 and GPT-4, displayed fundamental graph reasoning abilities.\nOn simpler tasks such as Connectivity, their performance markedly surpassed that of the open-source models. However,"}, {"title": "Ablation Study (RQ2)", "content": "In this section, we aim to investigate the impact of reasoning paths, prompt techniques, and training data difficulty on\nthe reasoning abilities. Unless specified otherwise, all subsequent experiments use Vicuna-7B as the base model, i.e.,\nGUNDAM-V."}, {"title": "Reasoning Path", "content": "In this subsection, we further investigate how reasoning paths influence the graph reasoning\nperformance of LLMs. We conducted experiments on the Connectivity, Cycle, and Shortest Path tasks, with the average\naccuracy presented in Table 2. More detailed results are provided in Appendix E.1. We use AGP to denote Answers\nwith Generated Path, where reasoning paths are generated by GPT-4. Additionally, following NLGraph Wang et al.\n[2023], we include a Random baseline, which randomly assigns \u201cYes\u201d or \u201cNo\u201d to the Connectivity and Cycle tasks,\nwith an expected accuracy of 50%. For the Shortest Path task, we randomly pick a valid path and the sum of the weights\nalong the path as the answer.\nThe results from models trained on PA, AGP, Answers with Hard Path (AHP), and ASP surpassed random outcomes,\nindicating that aligning LLMs with data containing correct answers generally enhances their reasoning capabilities\nto various extents. PA showed moderate performance across the three tasks, suggesting that data with only answers\nand no intermediate processes provide limited improvement in LLMs' reasoning abilities. AGP performed well in the\nConnectivity task but was less effective in the Cycle and Shortest Path tasks. This is likely because AGP's reasoning\npaths, generated by GPT-4, are reliable in simpler tasks such as Connectivity, where GPT-4 can generate correct paths."}, {"title": "Prompt", "content": "We further explored the impact of prompt techniques on the graph reasoning performance of LLM. We\nconducted experiments using five distinct prompt techniques, with results depicted in Figure 2. Typically, zero-shot\nconfigurations yielded relatively good or even the best results. However, advanced prompting techniques such as\nCoT and SC resulted in negligible performance improvements. These findings suggest that merely employing prompt\ntechniques struggles to significantly enhance LLMs' understanding and reasoning capabilities on graphs. In contrast, our\nproposed Alignment Tuning offers a viable solution to effectively bolster the graph reasoning abilities of LLMs. This\ninsight highlights the limitations of conventional prompting strategies and underscores the importance of tailor-made\nadaptations such as Alignment Tuning to fully leverage LLMs in complex reasoning tasks involving graph-structured\ndata."}, {"title": "Data Difficulty", "content": "To investigate the impact of data difficulty on model performance, we conducted transfer experiments\nusing datasets of varying difficulties. Specifically, we implemented two scenarios: 1) easy to hard, where we trained\nmodels with easy-level data and tested them on datasets of increasing difficulty to evaluate the transferability of\nreasoning capabilities; and 2) hard to easy, where we trained models on hard-level data and tested them on on varying\ndifficulties. The results are presented in Figure 3.\nFor simpler reasoning tasks such as Connectivity, models trained on easy data demonstrated effective transferability\nto other difficulty levels. However, for more complex reasoning tasks, the effectiveness of transfer from easy data\nwas limited. Conversely, models trained on hard data showed significant transferability across all tested difficulty\nlevels, suggesting that using hard data in training substantially enhances the LLM's graph reasoning capabilities. This\neffectiveness may be attributed to the intricate reasoning processes invoked by hard data, which likely induces the\nmodel to generate more intermediate steps, thereby enhancing reasoning depth and robustness. This observation aligns\nwith findings in Merrill and Sabharwal [2023a], which analyzed how the length of a transformer's chain of thought\nimpacts its reasoning power. Moreover, while models trained solely on hard data performed better than those trained on\neasy data, they underperformed compared to models trained on a mixed-difficulty dataset. This indicates that, although\nhard data contributes significantly to enhancing reasoning capabilities, a hybrid training approach incorporating datasets\nof various difficulties yields the best results. This mixture presumably builds a more robust and versatile reasoning\nmodel, capable of handling both simple and complex graph reasoning tasks effectively."}, {"title": "Case Study", "content": "In Figure 4, we present case studies for two complex tasks: Maximum Flow and GNN. Results from the two base\nmodels are omitted for these tasks due to their inability to handle such complexity. For the Maximum Flow task,\nGPT-3.5 identified an algorithm capable of solving the problem but failed to detail the necessary intermediate processes,\nresulting in an incorrect answer. GPT-4, on the other hand, offered more detailed intermediate results but erroneously"}, {"title": "Conclusion", "content": "To enhance the ability of LLMs to understand graph data and perform reasoning tasks based on graph structures, we\nintroduce GUNDAM. It employs a Graph Projection method to convert graph structures into textual formats that LLMs\ncan process and constructs CoT reasoning data via graph algorithms. Furthermore, we propose Alignment Tuning to\neffectively align LLMs with graph reasoning tasks. Experiments conducted on eight graph reasoning tasks demonstrate\nthe efficacy of GUNDAM, validating its utility in enabling sophisticated graph-based reasoning capabilities in LLMs."}, {"title": "Proof of theorem", "content": "Theorem 2. Given the following conditions:\n1. Non-triviality: The reasoning path R provides non-trivial information about the responses Z, such that\nH(RZ) > 0.\n2. Relevance: The reasoning path R contains information relevant to the correct answer a that is not fully\ncaptured by the response Z.\nThen it follows that H(a|Z, R) < H(a|Z).\nDefinitions and Notation\n\u2022 H() denotes the conditional entropy.\n\u2022 Z denotes the responses generated by the LLM.\n\u2022 R denotes the reasoning path.\n\u2022\na denotes the correct answer.\nAssumption H(R|Z) > 0: This means that the reasoning path R provides non-trivial information about the responses\nZ. In other words, given Z, there is still uncertainty in the reasoning path R.\nIntuition The main idea is that the reasoning path R provides additional information about the correct answer a\nbeyond what is provided by the response Z. Therefore, having access to R should reduce the uncertainty about a, hence\nreducing the conditional entropy.\nProof. Consider the chain rule of conditional entropy:\nH(a, R|Z) = H(a|Z) + H(R|a, Z)\nand\nH(a, R|Z) = H(R|Z) + H(a|R, Z).\nSetting these two expressions equal, we obtain:\nH(a|Z) + H(R|a, Z) = H(R|Z) + H(a|R, Z).\nSince entropy is non-negative:\nH(Ra, Z) \u2265 0.\nGiven that H(R|Z) > 0, which implies that R provides additional information about a that is not entirely captured by\nZ alone, we proceed as follows:\nH(a|Z) + H(R|a, Z) = H(R|Z) + H(a|R, Z).\nGiven H(Ra, Z) \u2265 0:\nH(a|Z) \u2265 H(R|Z) + H(a|R, Z).\nSince H(RZ) > 0, we have proven that:\nH(a|Z) > H(a|R, Z),\nThus, this completes the proof that having access to the reasoning path R in addition to the responses Z reduces the\nuncertainty about the correct answer a, proving H(a|Z, R) <H(a|Z)."}, {"title": "Details of training", "content": "Constructing Answers with Soft Paths (ASP) essentially involves a sentence rewriting task, an area in which LLMs\ndemonstrate substantial proficiency. For this purpose, researchers can utilize any commonly available open-source\nor proprietary LLM. In this paper, we specifically employ GPT-4 to undertake this task, owing to its advanced text\ngeneration and transformation capabilities. To facilitate ASP construction using GPT-4, we employ two commonly\nused prompts. These prompts strategically guide GPT-4 to reformulate the initial Answer with Hard Path (AHP) into a\nmore varied and generalized form, enhancing the diversity and robustness of the model outputs."}, {"title": "Dataset", "content": "Dataset statistic.\n Size denotes the number of nodes in graphs. A/B denotes the number of samples in the training/test set respectively.\n Reasoning Task\n\u2022 Connectivity: Given an undirected graph G and two nodes u and v, determine whether there is a path to\nconnect them, following u \u2192 V\u00bf \u2192 \u2026\u2026 \u2192 Vj \u2192 v.\n\u2022 Cycle: Given an undirected graph G, determine whether there is a cycle in the graph, following u\u2192 Vi \u2192\n\u2192 Vj \u2192 u.\n\u2022 Shortest Path: Given a weighted undirected graph G and two nodes u and v, the shortest path is the one that\nconnects the two nodes and minimizes the sum of the weights of the edges, such that\n$P = arg \\min_{P \\in P_{uv}} \\sum_{(v_i,v_j) \\in P} W_{ij}$"}, {"title": "Experiment Setting", "content": "For the FEW-SHOT based prompt techniques (including FEW-SHOT, CoT and CoT-SC), the input prompt contains K\nexamples of pertinent questions and answers. For Connectivity and Cycle tasks, K is 4 and K is set to 5 for Shortest\nPath task. For the self-consistency (SC) prompt method, we sample 5 responses. All experiments are conducted on an\n8*A800 machine. We employ Vicuna-7B and Llama3-8B as the base model for GUNDAM, the learning rate is set to\n2e-5, and the batchsize is set to 8. For evaluation, the temperature factor \u03c4 is set to 0.2 except for the self-consistency\nPrompt method, whose T is 0.8."}, {"title": "Reasoning Path", "content": "Reasoning Path\n    In this subsection, we further investigate how reasoning paths influence the graph reasoning\nperformance of LLMs. We conducted experiments on the Connectivity, Cycle, and Shortest Path tasks, with the average\naccuracy presented in Table 2. More detailed results are provided in Appendix E.1. We use AGP to denote Answers\nwith Generated Path, where reasoning paths are generated by GPT-4. Additionally, following NLGraph Wang et al.\n[2023], we include a Random baseline, which randomly assigns \u201cYes\u201d or \u201cNo\u201d to the Connectivity and Cycle tasks,\nwith an expected accuracy of 50%. For the Shortest Path task, we randomly pick a valid path and the sum of the weights\nalong the path as the answer.\n    The results from models trained on PA, AGP, Answers with Hard Path (AHP), and ASP surpassed random outcomes,\nindicating that aligning LLMs with data containing correct answers generally enhances their reasoning capabilities\nto various extents. PA showed moderate performance across the three tasks, suggesting that data with only answers\nand no intermediate processes provide limited improvement in LLMs' reasoning abilities. AGP performed well in the\nConnectivity task but was less effective in the Cycle and Shortest Path tasks. This is likely because AGP's reasoning\npaths, generated by GPT-4, are reliable in simpler tasks such as Connectivity, where GPT-4 can generate correct paths."}, {"title": "Case Study", "content": "We provide further insights with two case studies illustrated in Table 5. For the first case, considering the graph on\nthe left. All four models correctly indicated the presence of a path, assuring the connectivity between the two nodes.\nHowever, the Vicuna-7B made an error in its reasoning process by suggesting a non-existent connection from node\n17 to node 16. GPT-3.5, GPT-4, and GUNDAM provided correct answers with different valid paths. Although these\npaths were not the shortest, they were indeed existent and verified the connectivity as required. In the second case, both\nGPT-4 and GUNDAM accurately concluded the absence of any connecting path, whereas GPT-3.5 and Vicuna-7B\ngenerated incorrect responses. GPT-3.5 mistakenly introduced a non-existent edge (6,4), and Vicuna-7B incorrectly\nproposed an edge (6,5). In analyzing other error cases of GPT3.5, it was also found that GPT3.5 tends to construct\nnon-existent edges to make two nodes connected when no path exists in the graph."}]}