{"title": "Lettuce Detect: A Hallucination Detection Framework for RAG\nApplications", "authors": ["\u00c1d\u00e1m Kov\u00e1cs", "G\u00e1bor Recski"], "abstract": "Retrieval-Augmented Generation (RAG) systems remain vulnerable to hallucinated answers despite incorporating external knowledge sources. We present LettuceDetect, a framework that addresses two critical limitations in existing hallucination detection methods: (1) the context window constraints of traditional encoder-based methods, and (2) the computational inefficiency of LLM-based approaches. Building on Modern-BERT's extended context capabilities (up to 8k tokens) and trained on the RAGTruth benchmark dataset, our approach outperforms all previous encoder-based models and most prompt-based models, while being approximately 30 times smaller than the best models. LettuceDetect is a token-classification model that processes context-question-answer triples, allowing for the identification of unsupported claims at the token level. Evaluations on the RAGTruth corpus demonstrate an F1 score of 79.22% for example-level detection, which is a 14.8% improvement over Luna, the previous state-of-the-art encoder-based architecture. Additionally, the system can process 30 to 60 examples per second on a single GPU, making it more practical for real-world RAG applications.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have made significant progress in recent years in terms of their performance (OpenAI et al., 2024; Grattafiori et al., 2024; Team et al., 2024). However, the biggest obstacle to their usage in real-world applications is their tendency to hallucinate (Kaddour et al., 2023; Huang et al., 2025). Retrieval-Augmented Generation (RAG) is a method that enhances LLMs by supporting answers with context documents and retrieving knowledge from external sources, prompting the LLMs to ground their responses based on this information (Gao et al., 2024). This technique is widely used to minimize hallucinations of LLMs. Despite the incorporation of context documents in RAG, LLMs continue to experience hallucinations (Niu et al., 2024).\nHallucinations are defined as outputs that are nonsensical, factually incorrect, or inconsistent with the provided evidence (Ji et al., 2023). Ji et al. (2023) categorizes these errors into two types: Intrinsic hallucinations, which arise from the model's inherent knowledge, and Extrinsic hallucinations, which occur when responses fail to be grounded in the provided context, such as in the case of RAG hallucinations (Niu et al., 2024). While RAG can mitigate intrinsic hallucinations by grounding LLMs in external knowledge, extrinsic hallucinations persist due to imperfect retrieval processes or the model's tendency to prioritize its intrinsic knowledge over external context (Sun et al., 2025), leading to factual contradictions. As LLMS remain prone to hallucinations, their utilization in high-risk settings, such as medical or legal fields, may be jeopardized (Lozano et al., 2023; Magesh et al., 2024).\nWe present LettuceDetect, a hallucination detection framework that utilizes ModernBERT (Warner et al., 2024). Our approach trains a token-classification model to predict whether a token is supported by context documents and a question, determining if it is hallucinated. We frame this task as predicting tokens in the answers generated by large language models (LLMs), based on the provided context documents and the posed question. Our models are trained using the RAGTruth dataset (Niu et al., 2024). The architecture we employ is similar to Luna (Belyi et al., 2025), as we train an encoder-based model for this task. A demonstration of our web application is displayed in Figure 1."}, {"title": "2 Related work", "content": "ModernBERT BERT (Devlin et al., 2019) was one of the first major successes of applying the Transformer architecture (Vaswani et al., 2017) to natural language understanding. BERT uses only the Transformer's encoder blocks in a bidirectional fashion, allowing it to learn context from both directions. As a result, BERT quickly became the backbone of many NLP pipelines for tasks like classification, question answering, named entity recognition, etc.\nBERT's initial design included certain limitations, such as a maximum sequence length of 512 tokens and less efficient attention mechanisms, leaving room for architectural upgrades and larger-scale training. Despite the current rise of popularity of LLM-based architectures in NLP, such as GPT-4 (OpenAI et al., 2024), Mistral (Jiang et al., 2023) or Llama-3 (Grattafiori et al., 2024), encoder-based models are still widely used in many applications, because of their much smaller size and better-suited inference requirements that make them suitable for real-world applications.\nModernBERT (Warner et al., 2024) is a state-of-the-art encoder-only transformer architecture that incorporates several modern design improvements over the original BERT model. It utilizes several enhancements, including rotary positional embeddings (ROPE) (Su et al., 2024) instead of traditional absolute positional embeddings. Additionally, it features an alternating local-global attention mechanism as described in (Team et al., 2024), allowing it to efficiently manage sequences of up to 8,192 tokens. This makes it significantly more effective for long-context tasks, such as modern information retrieval (Nussbaum et al., 2025; Zhang et al., 2024). ModernBERT features a hardware-aware design and an expanded training corpus of 2 trillion tokens, including textual and code data. As a result, it achieves superior performance on various downstream benchmarks, such as GLUE for classification and BEIR for retrieval (while also maintaining faster inference speed) (Nussbaum et al., 2025; Zhang et al., 2024). Based on these findings, the main part of our paper is to use the advancements of ModernBERT in the hallucination detection of LLMs in an RAG setting. In this domain, long-context awareness is an inevitable feature.\nHallucination Detection can vary in granularity, ranging from example-based detection (which assesses if an answer contains hallucinations) to token, span, or sentence-level detection (Niu et al., 2024). The methods for detecting hallucinations also differ based on the techniques employed.\nPrompt-based Techniques typically utilize zero or few-shot large language models (LLMs) to identify hallucinations in LLM-generated responses. Few-shot or fine-tuned evaluation frameworks, such as RAGAS (Es et al., 2024), Trulens, and ARES (Saad-Falcon et al., 2024), have emerged to provide hallucination detection at scale using LLM judges. However, real-time prediction remains a challenge for these methods. Other prompt-based approaches, like the zero-shot method SelfCheck-GPT (Manakul et al., 2023), employ stochastic sampling to identify inconsistencies across multiple response variants. Rather than relying on a single prompt, Chainpoll (Friel and Sanyal, 2023) implements a series of verification steps to detect hallucinations. Cohen et al. (2023) presents a method of cross-examination between two LLMs to uncover inconsistencies. Chang et al. (2024) utilized LLM-based classifiers trained on synthetic errors to detect both hallucinations and coverage errors in LLM-generated responses.\nFine-tuned LLM Judges approaches involve training LLMs on hallucination detection tasks using specific training data. Niu et al. (2024) not only introduced the RagTruth data but also presented a fine-tuned Llama-2-13B LLM, which achieved state-of-the-art performance on their test set, even surpassing larger models like GPT-4. RAG-HAT (Song et al., 2024) introduced a novel approach called Hallucination Aware Tuning (HAT), which involves training models to generate detection labels and provide detailed descriptions of identified hallucinations. They created a preference dataset to facilitate Direct Preference Optimization (DPO) training. Fine-tuning through DPO results in SOTA performance on the RAGTruth test set.\nEncoder-based Solutions focus on addressing computational efficiency constraints through domain-specific adaptations. RAGHalu (Zimmerman et al., 2024) employs a two-tiered encoder model that utilizes binary classification at each layer, fine-tuning a Natural Language Inference (NLI) model based on DeBERTa (He et al., 2021). The approach most similar to our work is Luna (Belyi et al., 2025), which also builds on DeBERTa and NLI to create a lightweight long-context hallucination detection system capable of managing longer contexts effectively. Luna draws connections between detecting entailment in NLI tasks and identifying hallucinations. They fine-tuned on a large, cross-domain corpus of question-answering-based RAG samples, with annotations provided by GPT-4. During the inference phase, Luna conducts sentence- or token-level checks on each model's response against the retrieved passages, effectively flagging unsupported fragments. FACTOID (Rawte et al., 2024) introduces a Factual Entailment (FE) framework, which represents a new form of textual entailment aimed at locating hallucinations at the token or span level. Other approaches, such as ReDeEp (Sun et al., 2025), introduce techniques to analyze internal model states for hallucination detection."}, {"title": "3 Data", "content": "We trained and evaluated our models using the RAGTruth dataset (Niu et al., 2024). RAGTruth is the first large-scale benchmark for evaluating hallucinations in RAG settings. The dataset contains 18,000 annotated examples at the span level across three tasks: question answering, data-to-text generation, and news summarization.\nFor the question answering task, data was sampled from the MS MARCO dataset (Bajaj et al., 2018), where each question had up to three corresponding contexts. The authors then prompted LLMs to generate answers based on the retrieved passages. In the data-to-text generation task, LLMs were asked to generate reviews for sampled businesses from the Yelp Open Dataset (Yelp, 2021). For the news summarization task, randomly selected documents were taken from the training set of the CNN/Daily Mail dataset (See et al., 2017), and LLMs were prompted to create summaries.\nFor response generation, various LLMs were employed, including GPT-4-0613 (OpenAI et al., 2024), Mistral-7B-Instruct (Jiang et al., 2023), and selections from the Llama models, such as Llama-2-7b-chat and Llama-2-13B-chat (Grattafiori et al., 2024). Each sample in the dataset includes one response from each model, resulting in six responses per sample in RAGTruth."}, {"title": "4 Method", "content": "We trained ModernBERT-base and -large variants as token classifiers on the RAGTruth dataset. Input sequences were constructed by concatenating context, question, and answer segments using special tokens ([CLS] for context, [SEP] for separation) and tokenized to a maximum length of 4,096 tokens (in the current version we haven't utilized ModernBERT's full 8,192 context length). For handling tokenization, we've used the AutoTokenizer (Wolf et al., 2020). Our models are based solely on the ModernBERT architecture and were not pretrained on the NLI task, unlike previous encoder-based architectures.\nThe architecture leveraged Hugging Face's AutoModelForTokenClassification (Wolf et al., 2020) with ModernBERT as the backbone, and a classification head on top. Context/question tokens were masked (label=-100), while answer tokens were labeled as 0 (supported) or 1 (hallucinated). Training used AdamW optimization (Loshchilov and Hutter, 2019) (learning rate $1 \\times 10^{-5}$, weight decay 0.01) for 6 epochs on an NVIDIA A100 GPU. For data and batch handling, we've used PyTorch DataLoader (Paszke et al., 2019) (batch size=8, shuffling enabled). We evaluated models using token-level F1 score, saving the best-performing checkpoint via safetensors. Dynamic padding was implemented using DataCollatorForTokenClassification to process variable-length sequences efficiently.\nThe final model predicts hallucination probabilities for each answer token, with span-level outputs generated by aggregating consecutive tokens exceeding a 0.5 confidence threshold. The best models are uploaded to huggingface. Our method can be seen in Figure 2. We discuss the results in Section 5."}, {"title": "5 Evaluation", "content": "We evaluate our models using the RAGTruth test data across all task types, including question answering (QA), data-to-text, and summarization. Following the methodology outlined in (Niu et al., 2024), we report both example-level and span-level detection performance, reporting precision, recall, and F1 score. Our models are compared against state-of-the-art baselines presented in (Niu et al., 2024; Song et al., 2024; Belyi et al., 2025). This includes comparisons with prompt-based methods, such as gpt-4-turbo and gpt-3.5-turbo, as well as fine-tuned LLMs that have shown state-of-the-art performance on the RAGTruth data, including the previously established state-of-the-art model in (Niu et al., 2024) (a fine-tuned Llama-2-13B) and the current best result from (Song et al., 2024) (a fine-tuned LLM based on Llama-3-8B trained through DPO training). We also compare our models with encoder-based approaches, similar to ours, including the token classifier method presented in (Belyi et al., 2025), which is based on DeBERTa.\nTable 2 illustrates our results on the example-level task. Our large model (lettucedetect-large-v1) outperforms all prompt-based methods (gpt-4-turbo achieved an overall F1 score of 63.4% compared to lettucedetect-large-v1's 79.22%). It also surpasses the previous state-of-the-art encoder-based model, Luna (65.4% vs. 79.22%), and the previously established state-of-the-art fine-tuned LLM presented in (Niu et al., 2024) (fine-tuned Llama-2-13B with 78.7% vs. 79.22%). The only model that exceeds our large model's performance is the current state-of-the-art fine-tuned LLM based on Llama-3-8B presented in the RAG-HAT paper (Song et al., 2024) (83.9% vs. 79.22%). Our base model (lettucedetect-base-v1) also demonstrates strong performance across tasks while being half the size of the large model. Considering our model's compact size (150M for the base model and 396M for the large model) and its optimized architecture based on ModernBERT, it is capable of processing approximately 30 to 60 examples per second on a single GPU. Given this optimized inference speed, it only falls short compared to one larger model (8B Llama). Overall, our models are highly efficient while being about 30 times smaller in size.\nIn Table 3, we present our results on the span-level task. In this task, we evaluate the overlap between the gold spans and the predicted spans. Following the RAGTruth paper, we measured character-level overlap and calculated precision, recall, and F1 score. Our models achieved state-of-the-art performance, with the Llama-2-13B model reaching an overall F1 score of 52.7%, while our large model achieved 58.93% F1 score. Please note that we were unable to compare our results with RAG-HAT on this task because they did not measure at this level. Additionally, RAGTruth did not include this evaluation in their published code, so we relied on our own implementation for this analysis."}, {"title": "6 Conclusion", "content": "We present LettuceDetect, a lightweight and efficient framework for hallucination detection in RAG systems. By leveraging ModernBERT's long-context capabilities, our baseline models achieve strong performance on the RAGTruth benchmark while remaining highly efficient in inference settings. This work serves as a foundation for our future research, where we plan to expand the framework to include more datasets, additional languages, and enhanced architectures. Even in its current form, LettuceDetect demonstrates that effective hallucination detection can be achieved with lean, purpose-built models."}]}