{"title": "Every Part Matters: Integrity Verification of Scientific Figures Based on Multimodal Large Language Models", "authors": ["Xiang Shi", "Yinpeng Liu", "Jiawei Liu", "Qikai Cheng", "Wei Lu"], "abstract": "This paper tackles a key issue in the interpretation of scientific figures: the fine-\ngrained alignment of text and figures. It advances beyond prior research that pri-\nmarily dealt with straightforward, data-driven visualizations such as bar and pie\ncharts and only offered a basic understanding of diagrams through captioning and\nclassification. We introduce a novel task, Figure Integrity Verification, designed\nto evaluate the precision of technologies in aligning textual knowledge with vi-\nsual elements in scientific figures. To support this, we develop a semi-automated\nmethod for constructing a large-scale dataset, Figure-seg, specifically designed\nfor this task. Additionally, we propose an innovative framework, Every Part\nMatters (EPM), which leverages Multimodal Large Language Models (MLLMs)\nto not only incrementally improve the alignment and verification of text-figure\nintegrity but also enhance integrity through analogical reasoning. Our compre-\nhensive experiments show that these innovations substantially improve upon ex-\nisting methods, allowing for more precise and thorough analysis of complex sci-\nentific figures. This progress not only enhances our understanding of multimodal\ntechnologies but also stimulates further research and practical applications across\nfields requiring the accurate interpretation of complex visual data.", "sections": [{"title": "Introduction", "content": "The adage 'A picture is worth a thousand words' resonates particularly in the realm of scientific\ncommunication, where the ability to elucidate complex ideas through diagrams and graphs is not\njust beneficial but essential. Traditionally, however, in scientific publications, figures have often\nserved merely as supplements to textual content, with their significance frequently overshadowed by\nthe text itself. Consequently, for a long time, these scientific figures have not garnered as much atten-\ntion from the image research community as natural images have. This perspective is now changing\nas the complexity of scientific data, concepts, and methodologies increases, particularly in fields like\nbiology and medicine, where figures are vital for illustrating intricate ideas and fostering interdis-\nciplinary collaboration. Moreover, an increasing number of researchers recognize that interpreting\nscientific figures is crucial for advancing powerful multimodal technologies and poses unique chal-\nlenges compared to interpreting natural images [1, 2]. To significantly advance this research, this\npaper attempts to address a fundamental challenge in figure understanding-the fine-grained align-\nment of text and figures.\nText-image alignment, also formally known as visual grounding, has been a fundamental task in\nmultimodal research. Its goal is to match natural language text with specific visual content in im-\nages. Specifically, in natural image-focused alignment tasks, the main focus is on recognizing people\nand objects in the images, as shown in Figure 1(a). The features of these visual entities can be ef-\nfectively captured by visual encoding models such as Yolo [3] and CLIP [4]. However, aligning\nscientific figures with text is even more complex. Scientific figures often represent a complex in-\ntegration of multiple visual elements, each linked to specific texts, terms, or concepts, as shown in\nthe left side of Figure 1(b). Moreover, scientific figures vary widely. They range from morpholog-\nical diagrams depicting biological or geological forms, to spectral line charts presenting chemical\nor physical properties, and system diagrams like flowcharts or framework diagrams that illustrate\ncomplex processes and relationships. Without sufficient domain knowledge, it is difficult for even\nhumans to understand the meaning expressed by each element of a scientific figure, let alone ma-\nchines or models.\nIn response to this challenge, current advancements primarily focus on data figures such as pie,\nline, and bar charts, leading to the creation of visual question-answering and captioning datasets\nincluding ChartQA [5], FigureQA [1], and SciCap [6]. These efforts have achieved coarse-grained\nalignment between text and scientific figures and have inspired the design of multimodal large lan-\nguage models (MLLMs) such as mPLUG-PaperOwl [2], Vary [7], and TextHawk [8], which serve\nto parse these figures. Despite these advancements, existing research has two major limitations.\nFirst, the understanding of figures remains confined to data-driven charts, with little exploration into\nfigures containing complex domain-specific information such as framework diagrams or flowcharts.\nSecond, while studies have generally captured a generalized representation of information within\nfigures, they still significantly lack in perceiving detailed aspects. Existing models often misinter-\npret these figures, either by perceiving nonexistent elements or by misidentifying the spatial and\nsemantic features of figure components [9].\nRecognizing the limitations of current research, this paper focuses on the following research ques-\ntion: How can textual knowledge in scientific publications be aligned with visual elements in com-\nplex scientific figures? Furthermore, around this issue, we approach the challenge of text-figure\nalignment from three dimensions: tasks, data, and models. At the task level, we propose a novel\ntask-Figure Integrity Verification\u2014to assess the model's ability to understand scientific figures.\nNamely, a robust figure understanding model should be able to efficiently identify each visual el-\nement in a figure, align the textual knowledge with each element, and provide textual knowledge\nsupplementation for elements in the figure that are not aligned when no related information is\nfound in the text, as shown in Figure 1(b). Secondly, at the data level, we design a semi-automated\nmethod to align figure elements with textual terms, resulting in the creation of a large-scale aligned\ndataset-Figure-seg, which includes descriptions of the spatial and semantic information of fig-\nure elements. Finally, at the model level, we develop a text-figure alignment framework around\nMLLMs, termed 'Every Part Matters' (EPM). This framework requires the model to simultane-\nously utilize multiple capabilities, such as understanding text and figure elements, and modeling the\nmapping between text and figure elements, to achieve the target task. Simultaneously, we design a\nmethod based on analogical reasoning, utilizing citations from scientific publications, to supplement\nthe description of unaligned elements in figures."}, {"title": "Related Work", "content": "The research on understanding scientific figures has increasingly gained attention from researchers\nin recent years. Although studies in this field are not as mature as those for natural images, the\nemergence of MLLMs with potent image comprehension capabilities presents new opportunities\nfor development in this area. In this section, we provide a comprehensive review of the existing\nresearch on understanding scientific figures and MLLMs, identifying gaps in the research and the\nspecific problems that this paper aims to address."}, {"title": "Scientific figure understanding", "content": "Although research on scientific figures commenced later than that on natural images, it has a history\nof over 25 years. Early explorations in this field were mostly confined to a few types of figures with\nrelatively fixed formats, where figures and text already had clear correspondences. Researchers ini-\ntially constructed formal expressions of image-text mapping manually, focusing on the structure and\nlayout of images, as well as the natural language description methods. For instance, [12] established\na rule system for figure understanding based on the analysis of connections and adjacency relations\nof image elements in pictorial books of flora (PBF), along with the textual description methods of the\nimages. [13] designed a framework named JUXTA for schematic diagrams in texts, encompassing\nthree levels: visual, physical, and procedural.\nAs image recognition technologies such as object detection and image segmentation advanced, re-\nsearchers began to explore beyond single figure types, seeking automated paths for understanding\nscientific figures. This led to the refinement of sub-tasks like figure syntactic parsing, figure classifi-\ncation, figure captioning, and figure QA. In syntactic parsing, [14] used an LSTM-based method to\nautomatically map figures to Diagram Parse Graphs (DPG), designed specifically for figure elements\nand relationships. For figure classification, [14] automatically constructed a dataset named ACL-Fig,\nwhich includes 19 types of figures such as trees, graphs, and architecture diagrams, using scientific\nliterature published by ACL. In the realm of figure captioning, [6, 15] built datasets like SciCap\nand SciCap+ for figure captioning research, utilizing PDF document parsing and OCR technology.\n[16] explored a reinforcement learning approach combined with domain expert feedback to generate\nreader-preference optimized titles. In Figure QA, researchers like [1, 17, 18, 19] contributed various\ntypes of QA data for different figure types, including single and multi-turn questions, and designed\nfigure understanding pre-training models and QA frameworks based on plot figures.\nDespite the rich research in figure analysis, it remains in its nascent stages, with many studies fo-\ncusing on establishing benchmarks for figure understanding and analysis tasks, lacking mature so-\nlutions. Moreover, research on scientific figures is mostly limited to plot figures used for describing\nexperimental data results, with relatively less focus on the parsing of structural and flow diagrams\nthat describe the content frameworks and methods of literature. To address these gaps, we specif-\nically design a fine-grained understanding dataset and methodological framework for framework\nstructure and flow figures, further advancing the research in scientific figure comprehension."}, {"title": "Integrity Verification", "content": "The term 'integrity verification' originates from the field of information security. Its fundamental\nmeaning is to ensure that data or information remains unaltered from its creation, through trans-\nmission, to storage, thereby preserving its integrity [20]. Drawing on the connotations of this term,\nin our previous work, we introduced it into the field of information science to investigate the in-\ntegity of scientific literature [21]. Our aim is to ensure that scientific literature, from the moment of\nwriting, accurately and precisely conveys the intended scientific ideas to the reader. This involves\nensuring that every core element within the literature, such as key terms, formulas, and figures, is\nclearly described and presented in a standardized manner. This paper extends this framework to"}, {"title": "Multimodal large language model", "content": "The development of LLMs has provided new perspectives for many previously challenging prob-\nlems, including in the field of image understanding. Building on the foundational capabilities of\ntext-only LLMs, researchers have developed various types of multimodal LLMs (MLLMs). One ap-\nproach views LLMs as tool controllers, using natural language interaction to combine and call upon\nvarious vision processing foundation models to accomplish complex tasks in image understanding\nand analysis, such as with Visual ChatGPT [24] and Chameleon [25]. This method still relies heav-\nily on the capabilities of traditional vision processing foundation models and does not fundamentally\nenhance performance in specific tasks like image understanding. Another approach involves adding\nimage-text alignment modules to the input, embedding, MLP, and self-attention layers of the origi-\nnal text-only LLMs. This integration allows the features of images to merge into the LLM's textual\nfeature space, enabling image understanding and even image generation. For instance, the PICa\nmodel [26] uses an offline image-to-text model to convert images into descriptions, combining this\nwith knowledge from databases to design prompts that utilize GPT-3's understanding capabilities\nfor Visual Question Answering (VQA). The LLava model [27] maps image features encoded by im-\nage encoders directly into LLama's [28] text features through multimodal mappers. LLama-Adapter\n[29] and LLama-Adapter2 [30] introduce learnable parameters into LLama's transformer layers to\nencode image features. Meanwhile, models like BLIP2 [31], InstructBLIP [32], and Qwen-VL [33]\nincorporate a Q-former module to align images and text through shared self-attention layers. Fur-\nthermore, several studies have expanded upon the well-established foundation of multimodal LLMs,\nadapting them for various specific image understanding tasks. Examples include the LISA model\n[34], designed for image segmentation, and the mPLUG-PaperOWL model [2], developed for the\ninterpretation of charts.\nInspired by research in the foundational architecture of MLLMs and their applications in image\nunderstanding tasks, we design a MLLM for interpreting scientific figures, with a focus on structure\nand process diagrams, from multiple dimensions including data and model architecture. This model\naims to achieve pixel-level mapping between textual descriptions and figure modules, thus serving\nthe task of integrity verification for scientific figures."}, {"title": "Problem Formulation", "content": "Figure Integrity Verification. We operate under the assumption that every figure presented in\nacademic literature is meticulously designed to illustrate research methods, processes, or results.\nEach element within a figure serves a distinct purpose or conveys a specific message intended by the\nauthor. Thus, to fully communicate the ideas embodied in the figure, the function or meaning of each\ncomponent should be clearly explained in the accompanying text. Motivated by this premise, our\nstudy aims to enhance the understanding of complex diagrams, such as flowcharts and framework\ndiagrams, by meticulously aligning text with corresponding figure elements. Our approach addresses\nthree main challenges:\n\u2022 Text-figure Alignment: How to achieve fine-grained alignment between text and figure mod-\nules.\n\u2022 Integrity Verification: How to identify and highlight content within figures that has not been\nadequately described in the text.\n\u2022 Integrity Augmentation: How to supplement the descriptions of unaligned modules within\nfigures through figure understanding.\nFormal expression. Assuming a paper comprises I figures $G = \\{g_1, g_2,..., g_I\\}$ and k text segments\n$T = \\{p_1,p_2,...,p_k\\}$ (where p can represent a term or a sentence), each figure, $g_i$, contains n\nindependent modules $g_i = \\{m_{i1}, m_{i2}, ..., m_{in}\\}$. The process of solving the first problem involves\nestablishing connections between each module, $m_{ij}$, within figures and text paragraphs, $p_j$, forming"}, {"title": "Method", "content": "In this section, we present a detailed exposition of how to endow a generic MLLM with the capability\nto comprehend and validate the integrity of scientific figures. This is accomplished by commencing\nwith the construction of data (Section 4.1) and subsequently progressing to the design of the model\narchitecture (Section 4.2)."}, {"title": "Dataset construction", "content": "In constructing our dataset, we delineate the process into two distinct stages: figure segmentation and\nsemantic enhancement.The former aims to achieve fine-grained alignment between text and specific\nmodules within figures, whereas the latter builds upon this foundation by introducing spatial and\nsemantic information for precise localization of modules that appear multiple times within figures,\nthereby enhancing the accuracy of the dataset."}, {"title": "Figure segmentation", "content": "The process of figure segmentation is illustrated in Figure 2, encompassing four stages: preparation,\nfiltering, processing, and annotation. The specific operations for each stage are as follows and the\nsetting of parameters refers to Appendix A.2.1.\nPreparation Phase. Our dataset is constructed from a rich collection of open-access papers pro-\nvided by the Association for Computational Linguistics (ACL), which are abundant in model frame-\nworks and flowcharts. In this phase, we meticulously parse the original PDF documents to extract\nthe figures embedded within. To streamline this process, we utilize the highly efficient tool, PDF-\nFigures 2.0 [35], renowned for its ability to proficiently extract content from scientific literature.\nNotably, this tool possesses the unique capability to distinguish between figures and tables, offering\nan essential preliminary step in filtering figures relevant for our further analysis.\nFiltering Phase. We refine a specialized classifier, CLIP-fig, to discern between different types\nof figures, leveraging the ACL-fig dataset [36], derived from ACL papers, as its training founda-\ntion. This classifier adeptly categorizes scientific figures into 19 distinct categories, demonstrating\na commendable accuracy rate of 81.49% when evaluated against test datasets. In our research, we"}, {"title": "Semantic enhancement", "content": "We randomly sample 1,000 scientific figures from the previously collected dataset for analysis and\nfind that approximately 31.5% of the figures contain multiple instances of modules with identical\nnames. This indicates that simply inputting the name text into the semantic segment model can\nlead to confusion, preventing accurate identification of the corresponding modules in the figures.\nTo address this issue, we enhance the semantic information of the text by incorporating spatial\nand semantic information into the constructed text-figure module alignment data. As illustrated in"}, {"title": "Model Architecture", "content": "In the architectural design of the model, we draw inspiration from the LISA [34], basing our frame-\nwork on MLLMs with capabilities in image understanding and segmentation. Specifically tailored\nfor the unique characteristics of flowcharts and framework diagrams, we design a proprietary feature\nextraction module. The model is introduced in the following sections, as illustrated in Figure 4."}, {"title": "Input", "content": "Assuming a scientific figure $g_i \\in G$ and an accompanying paragraph $p_j \\in T$ delineate module $m_{ij}$\nwithin $g_i$. In the initial phase, leveraging a meticulously crafted instruction template alongside sam-\npling strategies, the paragraph $p_i$ from the academic paper undergoes transformation into a question\ninput $q_{ij}$, thereby rendering it more intelligible to the model. This input $q_{ij}$ encompasses a distinc-"}, {"title": "Encoder", "content": "Text encoder. The text encoder employs the tokenizer pre-trained with the Vicuna model [40] to\nprocess the input text $q_j$ by segmenting it into a sequence of tokens. Subsequently, these tokens\nare mapped into the textual space through an embedding matrix $F_{txtenc} \\in R^{|v|\\times d_e}$ to form the text\nembedding $eq_{ij}$.\n$eq_{ij} = F_{txt\\_enc}(q_{ij})$\n(1)\nThe notation |v| denotes the number of tokens in the vocabulary used by the tokenizer, $d_e$ repre-\nsents the dimensionality of the token vectors configured, and $F_{txt\\_enc}$ signifies the text embedding\nfunction.\nImage encoder. The image encoder is adeptly crafted to project images of various resolutions\ninto vector representations with precision, facilitating an advanced understanding and extraction\nof image features. This encoder combines a visual encoding model, $F_{vis\\_enc}$, with a multimodal\nmapping layer, $MLP_{mm}$. $F_{vis\\_enc}$ maps images to a vector space, enhancing feature extraction,\nwhile the multimodal mapping layer bridges the gap between image and text spaces. By mapping\nvisual information to the textual domain, the encoder allows images to be interpreted by language\nmodels similarly to how text is processed. This integration significantly boosts the model's ability\nto comprehend and analyze visual content effectively.\n$eg_{i} = MLP_{mm}(F_{vis\\_enc}(g_{i}))$\n(2)\nFollowing the encoding phase, the text $eq_{ij}$ and image vectors $eg_{i}$ are concatenated to form a com-\nprehensive input vector $e_{in_{ij}}$, which is then fed into multiple transformer layers for the integrated\nunderstanding of text and image.\n$C_{in_{ij}} = Concat([eg_{i}; eq_{ij}])$\n(3)"}, {"title": "Decoder", "content": "Text decoder. The text decoder decodes features, $C_{out_{ij}}$, learned through multiple transformer lay-\ners into output text, $a_{ij}$, via an autoregressive mechanism. In our scenario, the structure of the\nmodel's output text is predefined as $a_{ij} = \\{task description t_r + attribute description t_a +\nspecial token t_m\\}$. The task description refers to the part of the text that describes the task to\nbe completed by the model in response to the composite question, $q_j$. There are primarily two types\nof tasks: one is the text-figure alignment task, and the other is the attribute recognition task for figure\nmodules. The attribute description refers to the spatial and semantic information of the module $m_{ij}$\nin the figure. This description is used exclusively in the attribute recognition task. The special token,\ndenoted as [MODULE], is employed to represent the module mask predicted by the model. This\ntoken will be used exclusively in the text-figure alignment task.\n$A_{ij} = arg\\ max\\ P(t_r + t_a + t_m|q_{ij})$\n(4)\nMask decoder. The mask decoder, comprising multiple Transformer layers and mapping layers,\nprimarily functions to decode the feature representation of the special token [MODULE] in the\noutput text $a_{ij}$ into a binary image mask encoding, $mask_{pred_{ij}}$ (0/1). To accomplish this task, it is"}, {"title": "Training", "content": "Loss. During the training phase, the loss is composed of text generation loss ($L_{txt}$) and mask\ngeneration loss ($L_{mask}$). The $L_{txt}$ employs the commonly used shift cross-entropy loss (Shift_CE)\nin generative language models, with a weight coefficient $\u03bb_1$ assigned to modulate the model's focus\non generating text and image masks. $a_{pred_{ij}}$ and $A_{gold_{ij}}$ respectively denote the predicted text output\nby the model and the target text predefined in the dataset.\n$L_{txt} = \u03bb_1Shift\\_CE(a_{pred_{ij}}, A_{gold_{ij}})$\n(7)\n$L_{mask}$ is constituted by a combination of binary cross-entropy (BCE) and DICE loss, addressing\nthe alignment of the predicted masks $mask_{pred_{ij}}$ with the gold masks $mask_{gold_{ij}}$ from both global\nand microscopic perspectives. These components of loss are respectively regulated by weight coef-\nficients $\u03bb_2$ and $\u03bb_3$ to adjust their influence on the model's learning process.\n$L_{mask} = \u03bb_2BCE(mask_{pred_{ij}}, mask_{gold_{ij}}) + \u03bb_3DICE(mask_{pred_{ij}}, mask_{gold_{ij}})$\n(8)\nFinally, $L_{txt}$ and $L_{mask}$ are combined to constitute the total loss $L$, which is utilized for fine-tuning\nthe parameters of the model.\n$L = L_{txt} + L_{mask}$\n(9)\nSetting. We employ the pre-trained LLaVA-1.5 model as our base model, CLIP-ViT-Large as the\nimage embedding model, and the SAM mask decoder as the image decoding model. During the\ntraining phase, we freeze the parameters of the base model, the image embedding model and the\nmultimodal mapping layer. LoRA parameters are embedded alongside the linear mapping layers for\neach query and key within the base model, in conjunction with the mask decoder, to fine-tune the\nmodel. The detailed parameter setting in the training phase can be found in Appendix A.2.2."}, {"title": "Inference", "content": "Chain-of-Attribute (CoA). Due to the limitations in the level of detail provided by paper para-\ngraphs describing modules within images, the model lacks sufficient information input during the\ninference process, akin to what is available during training, leading to a decrease in inference accu-\nracy. To address this issue, we guide the model to enrich its input through thought chains. Unlike\nsimply adding \"Let's think step by step\" to encourage the model to construct thought chains on its\nown, we explicitly design thought chains based on attribute recognition (Chain-of-Attribute). This\ninvolves leveraging the model's image comprehension capabilities to refine descriptions of spatial\nand semantic features, thereby enhancing inference accuracy, as shown in Figure 5.\nCoA can be regarded as a specialized version of CoT-SC [41]. CoT-SC achieves model inference\nthrough the construction and voting of multiple chains of thought, whereas CoA accomplishes infer-\nence by identifying, combining, and voting on the spatial and semantic attributes of modules within\na figure. LLMs take $q_i$, as input and predict the intermediate values $attrij \\sim P(attrij|q_i)$, then\nconcatenate $q_i$ with $attrij$ to form $q_i'$, which is subsequently input into the model to predict the\nmask of the figure module segment. Finally, the prediction outcome is formed through a majority\nvote based on the results predicted from different attribute information (absolute position $attr_{abs}$,\nrelative position $attr_{rel}$ and semantic $attr_{sem}$).\nIntegrity verification pipeline. Based on the construction of the text-figure alignment model, we\ndesign a pipeline strategy to achieve the objective of integrity verification, as depicted in 4(a). In\nthis pipeline, the model's capabilities in three aspects are required: image understanding, text under-\nstanding, and multimodal understanding. First, image understanding evaluates the model's ability\nto segment anything or perform OCR on the modules within the figure, providing the foundation\nfor subsequent alignment and integrity verification. Second, text understanding assesses the model's\nability to recognize key scientific terms or concepts in the text, allowing them to be matched with\nelements in the figure. Finally, multimodal understanding involves finely aligning the parsed image"}, {"title": "Experiment", "content": "In this section, we present a comprehensive evaluation of our research, structured as follows: first,\nwe describe the experimental setup; next, we detail the results of our model tests; we then discuss the\nfindings from ablation studies to assess the robustness and essential components of our model. Ad-\nditionally, we explore the method's transferability across different disciplines and provide examples\nof its application in analyzing scientific figures from various fields."}, {"title": "Experimental setup", "content": "Dataset Configuration. For the training and evaluation of our model, we utilize a training set, a\nvalidation set, and two distinct types of test sets, containing 13,761, 1,000, 1,000, and 100 entries\nrespectively. The first test set, Test MS, assesses the model's ability to align text with figure mod-\nules. The second test set, Test IV, focuses on the model's proficiency in detecting figure modules\nnot described in the text. Test IV is particularly challenging, featuring nearly 1,000 modules that\nrequire identification, including a significant number of undescribed modules. This setup tests the\nmodel's comprehension and segmentation capabilities at a higher standard. Detailed statistics of\nthese datasets are presented in Table 1."}, {"title": "Baseline", "content": "To ensure a fair and rigorous comparison, we select seven state-of-the-art (SOTA) models from the\nlast two years as our baselines. These models are recognized for their performance in tasks like se-\nmantic segmentation, referring segmentation, reasoning segmentation, segment anything, and object\ndetection: For semantic segmentation, OVSeg [42] is a powerful open-vocabulary generalist model\nthat maps key terms in text to modules in images without being constrained by specific categories.\nFor referring segmentation, GRES [43] can segment multiple targets by integrating image regions\nand text features while filtering irrelevant text. X-Decoder [44] and SEEM [45] achieve multimodal\nunderstanding and referring segmentation through learnable query features and interactions among\ntext, image, and visual features. For reasoning segmentation, LISA [34] is based on the \"Embedding\nas Mask\" concept, merging fixed mask token outputs from the image understanding model with a\nmask decoder to execute content segmentation in images. For segment anything, FastSAM [38],\na faster iteration of the SAM model [46], provides a fiftyfold speed improvement and combines\ncomprehensive segmentation with prompt-guided selection. For object detection, Qwen-VL [33] is\na multimodal model for image understanding and object detection, using a visual-language adapter\ntrained on tasks like Captioning, VQA, Grounding, and OCR."}, {"title": "Metric", "content": "We employ two sets of evaluation metrics to assess the performance of the models.\nImage Segmentation Metrics. The first set comprises metrics for evaluating the model's image\nsegmentation capabilities, such as the cumulative intersection over the cumulative union (cIoU) and\nthe average of all per-image Intersection-over-Unions (gIoU), following the work of [34]. Higher\nvalues of these metrics indicate that the model is more accurate in segmenting image modules based\non textual information. These metrics are utilized in both text-figure alignment and integrity verifica-\ntion tasks to evaluate the model's ability to locate modules described by text and those not described\nby text, respectively. Assuming we have N images, each image contains a predicted region and a\ntrue region. For the ith image, let $A_i$ denote the area of the predicted region, $B_i$ represent the area\nof the true region, and $I_i$ signify the area of the intersection between the predicted and true regions.\nThe calculation methodologies for cIoU and gIoU are as follows:\n$cIoU = \\frac{\\sum_{i=1}^{N} I_i}{\\sum_{i=1}^{N} (A_i + B_i - I_i)}$\n(10)"}, {"title": "Main result", "content": "As detailed in Table 2, our evaluation covers the performance of each model in text-figure alignment\nand integrity verification tasks. Our method shows a notable improvement in text-figure alignment,\nachieving increases of 22.53% in cIoU and 45.13% in gIoU metrics over FastSAM. This superior\nperformance is primarily due to our method's enhanced ability to detect figure-irrelevant informa-\ntion. Despite these gains, it is important to note that our model's recognition of figure-related mod-\nules does not yet match FastSAM. This discrepancy can be partially attributed to the use of negative\nsamples in our training, which might compromise the model's ability to recognize positive samples.\nAdditionally, the dataset construction was influenced by FastSAM's capabilities, potentially biasing\nits performance. In tasks testing integrity verification, our method surpasses existing models, with\nimprovements ranging from 4% to 9% over FastSAM. Notably, the performance in segmenting all\nmodules within figures is higher than that in identifying only undescribed modules, with cIoU and\ngIoU metrics better by 4.90% and 4.52%, respectively. This indicates that undescribed modules\npresent a more significant challenge, a trend consistent across various baseline models. When com-\nparing results between the two tasks, it becomes clear that current models excel at aligning simple\nfigure modules. However, their performance declines when faced with multiple identical modules\nwithin the same figure, suggesting a limitation in handling complex scenarios."}, {"title": "Ablation study", "content": "To better analyze the impact of various components within our method on the overall performance\nof the model, we undertake ablation experiments by sequentially replacing the model's backbone,\nfigure understanding, text understanding, and input modules. The outcomes of these experiments\nare depicted in Table 3 and 4."}, {"title": "Base model performance", "content": "To assess the impact of base model configurations on text-figure alignment and integrity verification\ntasks, we examine various versions and parameter settings of the LLaVA model [47, 48] (details in\nthe first row of Table 3). Our findings show that the LLaVA-1.6-13B model, which represents the\nmost advanced version, delivers superior performance across all evaluated tasks. In the text-figure\nalignment task specifically, LLaVA-1.6-13B demonstrates a significant enhancement, outperforming\nthe LLaVA-1.5 model by approximately 3.7%. However, its advantage in the integrity verification\ntask is more modest, with an improvement of only 1.6% over LLaVA-1.5. This disparity underscores\nthe complexity of the integrity verification task, which not only requires the model to recognize all"}, {"title": "Figure interpreter performance", "content": "To explore the impact of the figure interpreter on overall performance, we compare the changes\nwhen using LLaVA-1.5, mPLUG-PaperOWI, and GPT-4 as interpreters for the spatial and semantic\nattributes of figure modules, as shown in the second row of Table 3. In the text-figure alignment\ntasks, the models exhibit heightened sensitivity to the accuracy of attribute information. GPT-\n4, with its superior figure comprehension capabilities, shows notable improvements in our image\nunderstanding model fine-tuned on training data, increasing performance by 1.16% in cIoU and\n0.81% in gIoU metrics. This improvement significantly exceeds that of models like LLaVA-1.5\nand mPLUG-PaperOWI by a margin of 5%-10%. In integrity verification tasks, despite significant"}, {"title": "OCR and NER", "content": "Table 4: Performance variations in the integrity verification task when employing different OCR\nand NER modules. 'Ideal Model' refers to the performance exhibited upon fully recognizing all\nmodules within figures and terms in the text. SciBert* is a Bert model fine-tuned for text integrity\nverification.\nBeyond evaluating the direct impact of our model on integrity verification tasks, we assess the\nbroader influence of OCR (Optical Character Recognition) and NER (Named Entity Recognition)\nperformance, which serve as crucial upstream components in our pipeline. As demonstrated in Table\n4, the performance of the OCR model significantly affects task outcomes, where using GPT-4 as an\nOCR tool falls short of the desired F1 score by 19%. The more accurately the OCR model performs,\nthe more figure modules our framework is able to correctly identify. Conversely, the better the NER\nmodel performs, the more accurately the framework recognizes modules that are not described in the\ntext. Despite the relative maturity of NER technologies, employing GPT-4 or SciBert trained on text\nintegrity verification data narrows the performance gap to only 5% from the ideal. This indicates\nthat under the constraints of text-figure alignment model performance, NER model improvements\ndo not markedly influence overall task outcomes."}, {"title": "Voting mechanism performance", "content": "Table 5: Comparison of model performance with the voting mechanism versus simple attribute\ninformation integration. The analysis focuses on the model's performance as parameter a varies\nfrom 1 to 3.\nThe analysis of the figure interpreter reveals that accurate identification of attributes critically in-\nfluences the text-figure alignment performance. Erroneous attribute information can mislead the\nmodel in aligning text with figures, as shown in the last row of Table 3. The integration of differ-\nent types of attribute information\u2014semantic, relative position, and absolute position-enhances the\nsegmentation task's performance by approximately 1% each. However, when these attributes are\ncombined within the model, interference occurs among the information types, resulting in an overall\nperformance that does not surpass the highest performance achieved by any single attribute type.\nConsequently, rather than merging all information directly, we adopt a simplistic voting mecha-\nnism. This approach treats each type of information as an independent input for figure segmentation"}, {"title": "Effectiveness of sample", "content": "To optimize the model's ability to comprehend spatial and semantic informationand to effectively\ndiscriminate against non-existent modules, we implement sampling for both positive and negative\nsamples during the training process. This section investigates the impact of varying the sampling\nrates, a and \u03b2, which range from 1 to 5. As depicted in Figure 6(a), introducing attribute combi-\nnation sampling notably improves the model's performance in text-figure alignment and integrity\nverification tasks. Specifically, we observe improvements of at least 3.5% and 5% in the cumulative\nIntersection over Union (cIoU) metric for these tasks, respectively. The model's performance in\ntext-figure alignment peaks when the sampling rate a is set to 2, while its performance in integrity\ntasks tends to continuously increase with higher a values. Figure 6(b) highlights that negative sam-\npling significantly enhances the model's ability to discern negative samples. However, using an\nexcessive number of negative samples can detrimentally impact the overall performance, particu-\nlarly in integrity verification tasks. Interestingly, setting \u03b2 to 2 allows the model's performance on\nboth text-figure alignment and integrity verification tasks to reach optimal levels."}, {"title": "Transferability", "content": "Table 6: Changes in model performance following the injection of attribute information into the\nbaseline model.\nThe results discussed previously confirm the effectiveness of our method in understanding scientific\nfigures. In this section, we extend our investigation to the transferability of our approach to other"}, {"title": "Case study", "content": "This section demonstrates our framework's efficacy in performing text-figure alignment and integrity\nverification across various types of scientific figures. As illustrated in Figure 7, we present a com-\nparison between our framework and other methodologies in aligning and detecting non-existent\nvisual elements. It is evident that our model can accurately locate modules differing by only one or\ntwo characters and can precisely identify elements that are absent in the figures. Figure 8 showcases,\nfrom left to right, the effectiveness of our framework in segmenting entities containing mathematical\nsymbols, chemical formulas, and biological units. Additionally, it is capable of effectively identify-\ning unexplained sections in flowcharts through joint analysis of text and images. As shown in Figure\n8, we display an example of utilizing a citation network to construct an analogical reasoning process\nthat guides MLLMs to enhance the completeness of figures.\nWhile our method achieves strong performance in understanding complex scientific figures, it is not\nwithout limitations. For instance, it struggles with accurately identifying multiple modules within a\nfigure simultaneously and occasionally falsely detects modules already mentioned in the text."}, {"title": "Discussion", "content": ""}, {"title": "Theoretic implications", "content": "This study delves into the research problems of assisting the reading and writing of scientific pa-\npers, under the broader research framework of integrity verification proposed in prior work [21].\nSpecifically, it designs a targeted integrity verification task and methodological strategies for scien-\ntific diagrams rich in semantic information. Unlike existing research on interpreting natural images\nand data charts, this study pioneers in focusing on more complex process diagrams and framework\ndiagrams, constructing a model framework for fine-grained alignment between paper texts and di-\nagrammatic modules based on multi-dimensional information such as the position and function of\nmodules within diagrams. Furthermore, the study goes beyond simply how scientific diagrams are\nreferenced in the text, to how unrecognized modules can be identified, aiding researchers in better"}, {"title": "Practical implications", "content": "This study introduces a MLLM-based method for the segmentation and integrity verification of\nmodules within scientific diagrams. Extensive empirical evidence demonstrates the effectiveness\nof our approach in understanding complex scientific diagrams such as flowcharts and framework\ndiagrams, showcasing good scalability and domain transferability. This offers new insights into the\ndevelopment of MLLMs for the interpretation of scientific diagrams. Secondly, we design a spe-\ncialized dataset, Figure-seg, specifically for the task of scientific diagram text-figure alignment and\nintegrity verification, thereby establishing a foundation for subsequent research and performance en-\nhancement in this area. Moreover, building upon the methodological research, we have designed an"}, {"title": "Limitation", "content": "Despite our method achieving better results than existing models in the tasks of scientific diagram\nsegmentation and integrity verification, there remain certain limitations as described below.\n\u2022 Segment anything within figures. Identifying all meaningful modules within a figure is\nfoundational to discovering undiscussed modules. Currently, this has not been effectively\nachieved in all SAM series models, such as FastSAM and EfficientSAM, where a signif-\nicant number of meaningless or falsely detected modules considerably impact the overall\nmethod performance.\n\u2022 Alignment of text and processes in diagrams. Currently, our method preliminarily\nachieves a fine-grained understanding of diagrams through the alignment of terms in the\ntext with modules in the diagrams. However, merely identifying modules within the di-\nagrams does not adequately restore the semantic information of the diagrams, especially\nfor flowcharts, where the semantic relationships between modules are particularly crucial.\nThis aspect has not been effectively addressed in our research."}, {"title": "Future work", "content": "In the future, we will expand upon the existing research from both theoretical and methodological\nperspectives.\n\u2022 Theoretical work. We will further enrich and expand the integrity verification frame-\nwork, embarking from multiple dimensions such as paper structure, textual content, and\ndiagrammatic content, to conduct fine-grained interpretations of scientific papers. This\nwill establish a new theoretical foundation for the analysis and bibliometrics of scientific\nliterature.\n\u2022 Methodological work. We aim to improve the performance and execution efficiency of\nthe model, addressing the current methods' inadequacies in detecting unexplained mod-\nules. Furthermore, we plan to establish an end-to-end MLLM for the understanding and\ninterpretation of complex scientific diagrams."}, {"title": "Conclusion", "content": "In this study, within the theoretical framework of integrity verification, we introduce the task of\nintegrity verification specifically designed for scientific diagrams in scientific papers. Around this\ntask, we construct a specialized dataset, Figure-seg, and develop an MLLM-based framework, EPM,\nto perform the task. Extensive experiments demonstrate that our proposed approach outperforms the\ncurrent SOTA models in understanding and detecting the integrity of complex scientific diagrams\nsuch as flowcharts and framework diagrams, with performance improvements in text-figure align-\nment and undepicted module detection improved by 22.53% and 4.90% in cIoU metric, and 45.13%\nand 4.52% in gIoU metric, respectively. Furthermore, based on the results of integrity verification,\nwe design a method for completing the integrity of scientific figures inspired by analogical reason-\ning, providing new theoretical and methodological foundations for research in this field."}, {"title": "Appendix", "content": ""}, {"title": "Data distribution", "content": "Building upon conventional data statistics, we further investigate the distribution of annotated mod-\nule contents within the overall dataset. In Figures 10, we present the overall frequency of occur-\nrences of modules in scientific diagrams and the high-frequency modules from 2020 to 2022. It\nis observable that terms such as \u201cencoder", "decoder": "and \"lstm\" rank among the top 3 in the\nfrequency of occurrence within the scientific diagrams of ACL papers. This indicates that encoder-\ndecoder architecture are commonly used in the field of natural language processing, employing\nLSTM (Long Short-Term Memory) networks for feature extraction tasks. Moreover, observing the\nshift in module popularity over three years, language models like BERT and T5 have garnered more\nattention from researchers compared to the once-popular LSTM in 2020. Although our dataset does\nnot cover every module in the diagrams, preventing a comprehensive reflection of each term's true\nfrequency, it still suggests, to a certain extent, that scientific diagrams can serve as a key source for"}, {"title": "Parameter setting", "content": ""}, {"title": "Data construction", "content": "In the construction of the Figure-seg dataset, the method for maintaining consistency between Fast-\nSAM's generated segmentation masks using text and coordinates is to ensure that the IoU of the\nresults generated by both exceeds 0.95. Besides, the minimum pixel distance $Dist_{pixel}$ for merg-\ning two bounding boxes is set to 50. The minimum threshold for the IoU between text boxes and\nsegmentation modules, $min_{iou}$, is set at 0.1."}, {"title": "Training", "content": ""}, {"title": "Instruction", "content": ""}, {"title": "Data collection template", "content": "The following is the instruction template used during our dataset construction process:\nData collection template\nSYSTEM: You are a very helpful language and visual assistant. You can understand the\nvisual content in scientific figures within the scientific literature, aiding users in correlating\ndescriptions from the scientific literature with the modules in the figures.\nQUERY (SEMANTIC): [image] Please describe the function of the module '%s' in the\ndiagram in one sentence using the format: Its function is XX. If you cannot identify the\nmodule from the picture, please directly answer 'Unknown'.\nQUERY (SPATIAL): [image] Please tell me the position of the module '%s' in the figure\nusing the following format: Its absolute position is: XX, and its relative position is: XX. If\nyou cannot identify the module from the picture, please directly answer 'Unknown'."}, {"title": "Training template", "content": "The following is the instruction template used during our model training process:\nTraining template\nSYSTEM: You are a very helpful language and visual assistant. You can understand the\nvisual content in scientific figures within the scientific literature, aiding users in correlating\ndescriptions from the scientific literature with the modules in the figures.\nQUERY: [image] Segment the corresponding module from the figure based on the given\nattributes: name: [NAME], function: [FUNCTION], relative position: [RELATIVE POSI-\nTION], absolute position: [ABSOLUTE POSITION].\nANSWER (POSITIVE): [MODULE] is the module that has been segmented.\nANSWER (NEGATIVE): [MODULE] is the module that has been segmented. There is no\ncorresponding module in the figure."}, {"title": "Inference template", "content": "In the inference process, our method utilizes the same instruction template as used during the training\nphase. For baseline models, to fully leverage their capabilities in understanding figures, we perform\nmultiple rewrites of the input and record the performance that yields the best results. Taking the\nLISA model as an example:"}, {"title": "Simplified Version of Chain-of-Attribute (CoA)", "content": "Algorithm 2 Simplified Version of Chain-of-Attribute (CoA)\n1: Input: D+: Positive examples from segmentation data, D\u00af:\nNegative examples sampled from segmentation data, G: Fig-\nure, M: Module name, Z: Figure interpreter trained on D+\nand D-\n2: Output: $Mask_{final}$: Final segmentation\n3: $(attr_{abs}, attr_{rel}, attr_{sem}) \u2190 I(G, M)$\n4: // Output module M's attributes using interpreter I\n5: $Mask_{abs} \u2190 M(G, Concat(M, attr_{abs}))$\n6: $Mask_{rel} \u2190 M(G, Concat(M, attr_{rel}))$\n7: $Mask_{sem} \u2190 M(G, Concat(M, attr_{sem}))$\n8: // Obtain segmentations by feeding attributes to M\n9: $Mask_{final} \u2190 Vote(Mask_{abs}, Mask_{rel}, Mask_{sem})$\n10: // Synthesize final mask by voting among Masks"}, {"title": "The design philosophy of the CoA", "content": "In the CoA reasoning process proposed in this study, the text-figure alignment model M is invoked\ntwice for detecting the existence of modules and inferring their Masks. This step, as depicted in\nFigure 11, is indeed amenable to simplification. The underlying concept involves partially transfer-\nring the responsibility of determining module existence to the figure interpreter, which assesses the\npresence of corresponding modules and attributes within the figure. Following this assessment, the-"}]}