{"title": "AdaptGCD: Multi-Expert Adapter Tuning for\nGeneralized Category Discovery", "authors": ["Yuxun Qu", "Yongqiang Tang", "Chenyang Zhang", "Wensheng Zhang"], "abstract": "Different from the traditional semi-supervised learning paradigm that is constrained\nby the close-world assumption, Generalized Category Discovery (GCD) presumes\nthat the unlabeled dataset contains new categories not appearing in the labeled\nset, and aims to not only classify old categories but also discover new categories\nin the unlabeled data. Existing studies on GCD typically devote to transferring\nthe general knowledge from the self-supervised pretrained model to the target\nGCD task via some fine-tuning strategies, such as partial tuning and prompt\nlearning. Nevertheless, these fine-tuning methods fail to make a sound balance\nbetween the generalization capacity of pretrained backbone and the adaptability\nto the GCD task. To fill this gap, in this paper, we propose a novel adapter-\ntuning-based method named AdaptGCD, which is the first work to introduce the\nadapter tuning into the GCD task and provides some key insights expected to\nenlighten future research. Furthermore, considering the discrepancy of supervision\ninformation between the old and new classes, a multi-expert adapter structure\nequipped with a route assignment constraint is elaborately devised, such that\nthe data from old and new classes are separated into different expert groups.\nExtensive experiments are conducted on 7 widely-used datasets. The remarkable\nimprovements in performance highlight the effectiveness of our proposals.", "sections": [{"title": "Introduction", "content": "While recent semi-supervised studies [1, 2, 3, 4, 5, 6] show a substantial progression, they are\ntypically constrained by the closed-world assumption, i.e., labels are provided for each category that\nthe model needs to classify [7]. The constraint hinders the scalability of these methods. To relax this\nassumption, Generalized Category Discovery (GCD) [8] presumes that the unlabeled dataset contains\nnew categories not appearing in the labeled set, as shown in Figure 1 (a). The goal of GCD is not\nonly to classify old categories but also to discover new categories in the unlabeled data.\nThe primary challenge in GCD is learning the expressive representation applicable to both old and\nnew classes [8]. Due to the scarcity of labels in GCD tasks, the model trained from scratch is prone to\noverfit the labeled data, consequently disrupting the generality of representation [9]. As an alternative,\nthe existing studies in GCD broadly finetune the self-supervised pretrained models, like DINO [10],\nto transfer the strong general knowledge from the pretrained backbone to the target GCD data. In this\nprocess, a key concern is to strike a balance between the generalization capacity from the upstream\nknowledge and the adaptability to the downstream GCD task, which is largely influenced by the\nfine-tuning strategy [11, 12, 13, 14].\nUp to now, two fine-tuning strategies are widely utilized in GCD: the partial tuning and the prompt\ntuning. The straightforward partial tuning strategy only updates the parameters from the last few\nblocks while the rest parameters in the backbone are frozen [8]. Such approaches potentially impair"}, {"title": "Related Work", "content": "Generalized Category Discovery. Generalized Category Discovery (GCD, [8, 29, 30, 16, 9, 31])\naims to classify the unlabeled images across both old and new classes. The pioneering work [8]\napproaches the goal via refining the representation from the pretrained model with contrastive learning\nand assigning the classification results by semi-supervised k-means clustering. Some concurrent\nworks [32, 33, 16] also follow the contrastive representation learning paradigm. Distinguished\nfrom these methods, the parametric approach SimGCD [34] introduces a learnable classifier and\nself-distillation to supervise the classifier, achieving boosting performance. Despite the excellent\nperformance, most of these works [8, 34] adopt the simple fine-tuning strategy only updating the last\nfew blocks of the vision transformer, which disrupts the pretrained knowledge. Hence, this work is\ndevoted to seeking an effective fine-tuning strategy for the GCD task to preserve the priority and\nenhance adaptability to the downstream tasks simultaneously.\nFine-Tuning Strategy. The mainstream fine-tuning approaches includes partial tuning, prompt\ntuning and adapter tuning methods. Partial tuning methods freeze most of the backbone and finetune\na small portion of parameters, like linear heads [35], MLP heads [36], or several blocks of the\nbackbone [37]. Although this strategy is widely used in GCD, it still falls short of maintaining the\nnetwork's generalization capabilities. Prompt tuning [13, 38] prepends a set of learnable vectors to the\ninput and only updates these prompts during fine-tuning. Despite their admirable performance, they\npresent limited adaptability when a large gap exists between the downstream task and the upstream\npretraining task [13]. Adapter tuning [17, 39] integrates shallow modules with the fixed backbone to\nprotect the priority in pretrained models. Recent works [26, 27, 28] also show its flexibility to adapt\nto different downstream tasks. Considering the excellent mechanism of adapter tuning, this work\nfurther concentrates on the application and modification of the adapter-tuning-based strategies in\nGCD.\nMixture of Expert. The Mixture of Experts (MoE) is a machine learning technique employing\nroute layer to segment a single task space into multiple subtasks [40, 41, 42, 43, 44]. Each subtask\nis specifically managed by individual expert networks and the prediction result is aggregation of\nthe solutions provided by these experts. In recent years, there appear some studies focusing on the\nfusion of the MoE with Adapters or LORA [42, 45]. For instance, LoRAMOE [45] introduces a\nMoE-style plugin and a localized balancing constraint to segregate world knowledge and downstream\ninformation into two distinct sets of experts. Inspired by LoRAMOE, the proposed approach also\nintroduces multi-expert technology to handle data from new and old classes in diverse experts, thus\nreducing interference during the learning process."}, {"title": "Method", "content": "In this work, a novel AdaptGCD framework (as depicted in Figure 1) is proposed for tackling the\ngeneralized category discovery task. In the following, we would elaborate on our proposal."}, {"title": "Setting of Generalized Category Discovery", "content": "Assume that the training dataset is composed of the labeled and unlabeled parts defined as $D^l =\\{(x,y)\\} \\in X \\times Y^l$ and $D^u = \\{(x,y)\\} \\in X \\times Y^u$, where $Y^l, Y^u$ are the label spaces of the\nlabeled and unlabeled samples. In the GCD setting, $Y^l \\subset Y^u$. Given $Y^n = Y^u\\Y^l$, the categories in\nthe $Y^l$ are the \u201cold\u201d classes while those in $Y^n$ are the \u201cnew\u201d classes. The goal of GCD is to learn a\nmodel to categorize the samples from $D^u$ in both the old and new classes."}, {"title": "Baselines: SimGCD", "content": "As a simple yet effective parametric approach, SimGCD [34] is adopted as the baseline of the\nproposed method. It incorporates two vital losses, including representation learning loss $L_{rep}$ and\nclassification objective $L_{cls}$.\nParametric Classifier. SimGCD employs the parametric classifier in conjunction with the self-\ndistillation paradigm for the learning of the classifier. Given the set of prototypes $C = \\{c_1, ...c_K\\}$,\nwhere each one corresponds to a class and $K = |Y^u|$ is the total number of categories. The prediction"}, {"title": null, "content": "is determined by the cosine distance between the hidden features $h_i = f(x_i)$ and the prototypes, that\nis $p_{i,k} = \\frac{exp((h_i/||h_i||_2)^T (c_k/||c_k||_2))}{\\sum_{k'}{exp((h_i/||h_i||_2)^T(c_{k'}//||C_{k'}||_2))}}$,\nwhere $T_s$ indicates the temperature.\nSimGCD Loss. For the representation loss, it integrates supervised contrastive learning [46] for\nlabeled samples with self-supervised contrastive learning [47] for all samples. Considering the\nclassification loss, it is defined as the cross-entropy loss between the predictions and pseudo-labels\nor ground-truth labels. All in all, the objective for SimGCD $L_{gcd}$ is described as the sum of\nrepresentation loss $L_{rep}$ and classification loss $L_{cls}$, i.e., $L_{gcd} = L_{rep} + L_{cls}$.\nPartial Tuning. The effectiveness of the SimGCD framework largely stems from pretrained model.\nTo take advantage of the generic representation from upstream tasks (like DINO [10]), the SimGCD\nframework only unfreezes the pretrained parameters of the last block for fine-tuning. However, as\nmentioned previously, this method might still result in information loss from the pretrained backbone,\nthereby reducing the generality of features."}, {"title": "Multi-Expert Adapter", "content": "In this part, the proposed approach integrates the adapter structure into the GCD framework, intro-\nducing a learnable additional architecture into the backbone while keeping the rest parameters in the\nbackbone frozen during training. Then the basic adapter is further developed into the multi-expert\nadapter, allowing data under different supervisions to navigate through diverse paths.\nBasic Adapter. The designation of the adapter in this work follows the basic architecture proposed\nby AdaptFormer [17], which replaces the Feed-Forward Network (FFN) in the Transformer with\nthe AdaptMLP module. This module contains two parallel sub-branches, one is identical to the\nFeed-Forward block while the other is an learnable lightweight bottleneck module. The bottleneck\nmodule includes a down-projection layer with parameters $W_{down} \\in R^{d \\times \\hat{d}}$, $b_{down} \\in R^{1 \\times \\hat{d}}$ and an\nup-projection layer $W_{up} \\in R^{\\hat{d} \\times d}$, $b_{up} \\in R^{1 \\times \\hat{d}}$, where d and $\\hat{d}$ denote the input dimension of FFN\nand middle dimension of bottleneck ($\\hat{d} < d$). The down-projection layer and the up-projection layer\nare connected with the ReLU layer for non-linear properties and the entire bottleneck module is\nlinked to the original FFN with the residual connection, as depicted in Figure 1. Specifically, for the\nfeatures of the i-th sample fed into FFN of the l-th Transformer blocks $x_{l,i}$, the forward process of\nbottleneck module to yield the adapted feature $\\Delta x_{l,i}$ is formulated as follows:\n$\\Delta x_{l,i} = s (ReLU(X_{l,i} W_{down} + b_{down})W_{up} + b_{up}),$ (1)\nwhere s is the scale factor. Then, the adapted features are fused with the original FFN branch by\nresidual connection:\n$X_{l+1,i} = MLP(LN(X_{l,i})) + X_{l,i} + \\Delta x_{l,i},$ (2)\nwhere $x_{l+1,i}$ is the output of the l-th Transformer block, $LN(\\cdot)$ is the layer normalization function.\nDuring fine-tuning, only the parameters of the bottleneck modules are updated and the rest are frozen.\nMultiple-Expert Adapter. As previously mentioned [7], data from older classes yields more\nsupervisory information. It would result in the preference for the old class as the network receives\nmore gradients from the old-class data than the new-class data. To counteract this implicit bias, the\nstrategy of multi-expert technology is adopted to integrate with the adapter. This approach assigns\ndifferent types of data to different expert adapters, therefore achieving separation for the data at the\nnetwork level and alleviating the interference between new classes and old classes.\nMultiple-expert adapter (MEA) consists of T experts for handling different features and one route\nfunction to assign weights to these experts. More specifically, for the input feature $X_{l,i}$, MEA\nleverages the following route function to estimate the weight $w_{l,i}$ of multiple experts:\n$\\omega_{l,i} = [w_{l,i}^1, ..., w_{l,i}^t, ..., w_{l,i}^T] = Softmax(x_l, \\frac{W_{route}}{T_r}),$ (3)\nwhere $W_{route} \\in R^{T \\times d}$ denotes the trainable parameters in the route function and $T_r$ indicates the\ntempertures. The forward process of the bottleneck module is rectified as Eq.(4):\n$\\Delta X_{l,i} = s \\sum_{t=1}^T w_{l,i}^t (ReLU (W_{down}^t + b_{down})W_{up} + b_{up}),$ (4)"}, {"title": null, "content": "where $W_{down}^t, b_{down}^t, W_{up}^t$ and $b_{up}^t$ denotes the parameters of the t-expert. By this, the multi-expert\nmechanism enables the network to develop powerful capabilities to flexibly handle the data from new\nand old classes. According to previous research [40], allocating more adapter experts in higher blocks\nenhances the effectiveness of models compared with inserting them in the lower blocks. Hence, the\nMEA structure is incorporated only in the last P blocks out of the total L blocks."}, {"title": "Route Assignment Constraint", "content": "In the multi-expert adapter, the route assignment constraint is required to supervise and control the\nroute distribution. The assignment mainly focuses on two aspects: First, for all data, the load of all\nexperts needs to be balanced to make full use of the resources of experts. Second, for data in old or\nnew classes, the constraint assigns the corresponding experts to them so that the data can be well\nseparated at the routing level. These two aspects correspond to the balanced load loss and the partial\nbalanced load loss, which are introduced in this part.\nProbability for Route Assignment. It is worthy noting that, in order to obtain the sample-wise\nroute assignment probability, it is necessary to combine the gate vectors of different tokens for\nthe same sample. Assume that $w_{l,i,v}$ denotes the gate probability from the v-th token of the i-th\nsample, within the l-th blocks, then the route assignment probability for the i-th sample $w_{l,i}$, can be\nformulated as $\\omega_{l,i} = Softmax (\\sum_{v=1}^V\\frac{\\omega_{l,i,v}}{T_g}), where V$ denotes the number of tokens and $T_g$\nis the temperature, and $Softmax(\\cdot)$ is utilized to sharpen the gate vectors.\nBalanced Load Loss. The balanced load loss is designed to ensure the maximal usage of diverse\nexperts. Previous research on mixture-of-expert [48] demonstrated a tendency towards a \u201cwinner-\ntakes-all", "5)": "n$\\omega_l = \\frac{1}{B}\\sum_{i \\in B} \\omega_{l,i}, \\space L_{bl} = \\sum_{l=L-P+1}^L D_{KL}(\\omega_l||I),$ (5)\nwhere B indicates the mini-batch and $D_{KL}(\\cdot||\\cdot)$ indicates the Kullback-Leibler divergence.\nPartial Balanced Load Loss. Considering the imbalance of supervision between new classes and\nold classes, we propose a partial balanced load loss to separate the new-class and old-class data\ninto different experts and reduce their interference. Since the exact class label of the samples is not\ndetermined during training, the pseudo labels are utilized as a substitute. As shown in Eq. (6), the\npseudo-labels of the samples are obtained according to the predictions.\n$\\hat{y}_i = \\begin{cases} y_i, & x_i \\text{ is labeled},\\\\ argmax_k(p_{i,k}), & x_i \\text{ is unlabeled}.\\end{cases}$ (6)\nThen the route assignment probabilities of the experts for the old and new classes are estimated as\n$\\omega_l^{old}$ and $\\omega_l^{new}$ in Eq. (7).\n$\\omega^{old} = \\frac{\\sum_i \\omega_{l,i} \\mathbb{1}(y_i \\in Y_l)}{\\sum_i \\mathbb{1}(\\hat{y} \\in Y_l)}, \\space \\omega^{new} = \\frac{\\sum_i \\omega_{l,i} \\mathbb{1}(y_i \\in Y_n)}{\\sum_i \\mathbb{1}(\\hat{y} \\in Y_n)}$ (7)\nWe manually specify the expert groups for the old and new classes beforehand and denote the expert\ngroups as $T^{old}$ and $T^{new}$, respectively. For instance, the first four experts are assigned to $T^{old}$ and\nthe remaining experts are naturally divided into $T^{new}$. Then the target route distribution probability\nfor the old classes $I^{old} \\in R^T$ and new classes $I^{new} \\in R^T$ is established as follows:\n$I^{old}(t) = \\begin{cases} \\frac{1}{T^{old}} & t \\in T^{old}\\\\ 0 & t \\notin T^{old}\\end{cases}, I^{new}(t) = \\begin{cases} \\frac{1}{T^{new}} & t \\in T^{new}\\\\ 0 & t \\notin T^{new}\\end{cases}$ (8)\nThese two distribution functions respectively describe the target assignment distribution of data in the\nold and new data. Then, as shown in Eq. (9), the Kullback-Leibler divergence is adopted to align"}, {"title": null, "content": "$\\omega^{old}$ and $\\omega^{new}$ with the predefined target $I^{old}$ and $I^{new}$.\n$L_{pbl} = \\sum_{l=L-1+P}^L (D_{KL} (\\omega_l^{old} ||I^{old}) + D_{KL} (\\omega_l^{new} ||I^{new})).$ (9)\nIn the final step, the route assignment loss $L_{ra}$ for the AdaptGCD is collected as the weighted sum of\nthe two losses, i.e. $L_{ra} = \\beta L_{bl} + \\alpha L_{pbl}$, where $\\alpha, \\beta$ are the balancing factors.\nOverall Loss. The overall loss of the AdaptGCD is collected as the sum of SimGCD loss and the\nroute assignment loss, as formulated in Eq. (10):\n$L_{overall} = L_{gcd} + L_{ra}.$ (10)"}, {"title": "Experiments", "content": "Datasets. The effectiveness of the proposed method is validated on three characteristic benchmark:\nthe generic image recognition benchmark including CIFAR10/100 [49] and ImageNet-100 [50], the\nsemantic shift benchmark including CUB-200 [51], Stanford Cars [52], and FGVC-Aircraft [53] and\nthe harder long-tail dataset Herbarium 19 [54]. Following SimGCD [34], a subset of all the classes $y^l$\nis sampled as the old classes while 50% of the images from the old classes are selected to construct\nthe labeled set $D^l$. The rest of the images are collected as the unlabeled set $D^u$.\nImplementation details and evaluations. Following GCD [8], the proposed method is trained with\na ViT-B/16 backbone pre-trained with DINO. The output of [CLS] tokens is utilized as the feature\nof the image. For the training details, the AdaptGCD is trained with a batch size of 128 for 200\nepochs with an initial learning rate of 0.1 decayed with a cosine schedule on each dataset. For the\nadapter configuration, we utilize 8 experts with the bottleneck dimension $\\hat{d}$ set as 64. The number\nof adapted blocks P is assigned to be 6 on CUB-200, Scars and CIFAR10. As for the Aircraft,\nCIFAR100, Herbarium19 and Imagenet100, which present greater challenges for the classification, P\nis designated to be 8. For expert assignments, 4 experts are utilized as old-class experts while another\n4 experts are selected as new-class experts, i.e. $|T^{old}| = |T^{new}| = 4$. The performance is evaluated\nwith the clustering accuracy for old, new and all classes as [8]. It is also noted that we conduct the\nexperiments in Table 1 and 2 for 3 independent runs and report the average."}, {"title": "Comparison with State-of-the-Arts", "content": "Evaluation on generic image recognition datasets. AdaptGCD is compared with the previous\nadvanced methods and some concurrent works (i.e., DCCL [32], PromptCAL [16], SimGCD [34] and\nSPTNet [9]). The results are shown in Table 1. It is observed that our AdaptGCD manifests superior\nperformance across three general benchmarks. Compared to the baseline SimGCD, AdaptGCD\nachieves improvements of 0.8%, 3.9%, 3.1% for \u201cAll\u201d classes on three datasets. When juxtaposed"}, {"title": "Ablation Study", "content": "To investigate the impact of different modules, we conduct the the extensive ablation studies on the\nsemantic shift benchmark following [9, 16]. The results are reported in Table 3.\nInfluence of adding adapter structure. Firstly, we have validated the role of a single adapter as\nillustrated in (2). Given that the bottleneck dimension is set as 64, the count of learnable parameters\nin the backbone is less than 0.8M, which is significantly less than the number of parameters in the\nlast block (approximately 7M). Despite the less trainable parameters in the backbone, involving an"}, {"title": "Discussion", "content": "Sensitivity of three key hyper-parameters of the adapter. We evaluated the impact of three key\nhyper-parameters of the adapter: bottleneck dimension $\\hat{d}$, number of adapted blocks P and expert\ncount T, and show the results in Figure 2. First, we conduct experiments with models employing\nsingle adapters across different $\\hat{d}$ and report the results in Figure 2 (a). Remarkably, when the\nbottleneck dimension is relatively small, there is a noticeable rise in performance with the increase\nin the bottleneck dimension. As the dimension further elevates, performance fluctuates within a\ncertain range due to the overfitting caused by the excessive amount of parameters. A similar trend\nis also observed as the increase in the number of adapted blocks P in Figrue 2 (b), i.e., as P is\nlarge enough, inserting more adapters in the bottle blocks yields additional resources but limited\nperformance gain. It also validates the effectiveness of only inserting adapters in parts of blocks. For\nthe high performance of a single adapter, the impact of T in Figure 2 (c) is not so noticeable, but the\nincrement in expert count still results in a performance boost for \u201cAll\u201d classes, further corroborating\nthe effectiveness of multi-expert adapters."}, {"title": "Conclusion", "content": "In this study, we propose a novel AdaptGCD in generalized category discovery, which is the pioneering\nwork to merge adapter tuning into the GCD framework. It integrates learnable adapters with the\nfixed backbone to adapt to the downstream GCD task as well as preserve the pretrained priority.\nAdditionally, considering the imbalance between the supervision of the old and new classes, we\npresent a multi-expert adapter structure and a route assignment constraint to separate the data from\nold and new classes into different expert groups, further reducing mutual interference. The efficacy\nof the proposed strategy is demonstrated in 7 datasets, including 3 generic datasets 3 fine-grained\ndatasets, and a long-tail dataset. Looking forward to future work, a promising direction is to explore\nmore effective fine-tuning strategies in the GCD tasks, including the LoRA [58], RepAdapter [39],\nor the combination of different strategies like GLORA [22]. In terms of limitations, the proposed\nmethod introduces additional parameters, which consume extra computational cost during training\nand inference."}]}