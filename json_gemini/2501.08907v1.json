{"title": "Projection Implicit Q-Learning with Support Constraint for Offline Reinforcement Learning", "authors": ["Xinchen Han", "Hossam Afifi", "Michel Marot"], "abstract": "Offline Reinforcement Learning (RL) faces a critical challenge of extrapolation errors caused by out-of-distribution (OOD) actions. Implicit Q-Learning (IQL) algorithm employs expectile regression to achieve in-sample learning, effectively mitigating the risks associated with OOD actions. However, the fixed hyperparameter in policy evaluation and density-based policy improvement method limit its overall efficiency. In this paper, we propose Proj-IQL, a projective IQL algorithm enhanced with the support constraint. In the policy evaluation phase, Proj-IQL generalizes the one-step approach to a multi-step approach through vector projection, while maintaining in-sample learning and expectile regression framework. In the policy improvement phase, Proj-IQL introduces support constraint that is more aligned with the policy evaluation approach. Furthermore, we theoretically demonstrate that Proj-IQL guarantees monotonic policy improvement and enjoys a progressively more rigorous criterion for superior actions. Empirical results demonstrate the Proj-IQL achieves state-of-the-art performance on D4RL benchmarks, especially in challenging navigation domains.", "sections": [{"title": "1. Introduction", "content": "Offline RL is dedicated to learning policies from static datasets generated by unknown behavior policies, thereby eliminating the need for extensive interactions with the environment (Prudencio et al., 2023; Levine et al., 2020). This data-driven approach has immense potential for real-world applications in high-risk or high-cost domains (Han et al., 2024), bridging the gap between simulated environments and practical deployment. However, while the ability to learn without continuous interactions is a key advantage, it also introduces significant challenges. One of the most crucial issues is the extrapolation error (Fujimoto et al., 2019), which arises from OOD actions and can lead to severely erroneous value evaluation. These errors propagated through the bootstrapping Bellman operator, can distort value estimates for other state-action pairs. To address this issue, numerous offline RL algorithms have been developed specifically to mitigate this issue in the policy improvement and policy evaluation phases, respectively.\nIn the policy improvement phase, policy constraint methods restrict the learned policy to remain close to the behavior policy. Behavior Cloning (BC) (Pomerleau, 1991) directly trains the policy to imitate actions within the dataset via supervised learning, while weighted Behavior Cloning (WBC) (Peters & Schaal, 2007; Peng et al., 2019; Nair et al., 2020; Wang et al., 2020) improves upon BC by weighting samples based on different methods. Essentially, wBC methods keep the policy to be close to the behavior policy by Kullback-Leibler (KL) divergence constraint, which is categorized into density constraint methods. However, the constraints in both BC and wBC methods are overly restrictive, thus limiting the performance of policy improvement. In contrast, support constraint methods (Kumar et al., 2019; Wu et al., 2022; Mao et al., 2023; 2024) apply more relaxed constraints to achieve similar policy alignment while allowing for greater policy improvement within the dataset.\nIn the policy evaluation phase, conservative value learning methods mitigate overestimation issues caused by OOD actions (Kumar et al., 2020; Peng et al., 2023; Kostrikov et al., 2021). Especially, IQL (Kostrikov et al., 2021) employs expectile regression to approximate an upper expectile of the value distribution instead of relying on the traditional maximum operator in Q-Learning. Specifically, IQL alternates between fitting the value function via expectile regression and using it to compute Bellman backups for training the Q-function. However, IQL has two notable limitations: (1) IQL relies on a fixed conservatism parameter $\\tau$, requiring extensive tuning across datasets to achieve optimal performance; (2) as a fundamentally one-step (Brandfonbrener et al., 2021) algorithm, IQL restricts the extension for more effective policy improvement.\nTo overcome these challenges, we propose Projection IQL"}, {"title": "2. Related Work", "content": "To more effectively compare and demonstrate the advantages of Proj-IQL, we classify existing offline RL approaches into two main categories: conservative value methods and policy constraint methods.\nConservative Value Methods aim to mitigate the overestimation of OOD actions by penalizing overly optimistic value estimates. CQL (Kumar et al., 2020) introduces penalties on the Q-function for OOD actions, ensuring policy performance while staying close to the behavior policy. Extensions like ACL-QL (Wu et al., 2024) dynamically adjusts penalty term to reduce over-conservatism, and CSVE (Chen et al., 2024) penalizes state value function estimates across the dataset to enhance generalization. Despite their efficacy in avoiding overestimation, these approaches still face OOD action risk. In-sample learning approaches such as Onestep (Brandfonbrener et al., 2021), IQL (Kostrikov et al., 2021), and IVR (Xu et al., 2023) eliminate the need to estimate Q-values for OOD actions entirely. Onestep demonstrates strong results with one-step policy improvement but relies on well-covered datasets. IVR introduces an implicit value regularization framework, but Sparse Q-Learning within IVR relies on a restrictive assumption to simplify its optimization objective. IQL leverages the expectile regression framework for value learning, but its hyperparameter $\\tau$ requires careful tuning, which is time-consuming.\nPolicy Constraint Methods enforce closeness between the learned policy and the behavior policy using various metrics such as KL divergence (Nair et al., 2020; Zhang et al., 2021), regularization terms (Fujimoto & Gu, 2021), or CVAE model (Fujimoto et al., 2019; Zhou et al., 2021; Chen et al., 2022), etc. While these density-based methods perform well under moderate suboptimality, they struggle when the dataset predominantly consists of poor-quality data. Support-based approaches relax density constraints to allow greater optimization flexibility. BEAR (Kumar et al., 2019) keep the policy within the support of behavior policy by Maximum Mean Discrepancy (MMD). By contrast, SPOT (Wu et al., 2022) adopts a VAE-based density estimator to explicitly model the support set of behavior policy. Similarly, DASCO (Vuong et al., 2022) employs discriminator scores in Generative Adversarial Networks (GANs). Although these methods achieve strong empirical performance, they lack rigorous policy improvement guarantees, as highlighted by STR (Mao et al., 2023).\nProj-IQL addresses these limitations by providing rigorous policy improvement guarantees. Compared to STR, Proj-IQL progressively tightens the policy evaluation function and raises standards of superior actions in datasets. Moreover, as an in-sample learning algorithm, Proj-IQL avoids the challenges of OOD action estimation, ensuring more reliable and efficient learning."}, {"title": "3. Preliminaries", "content": "Conventionally, the RL problem is defined as a Markov Decision Process (MDP) (Sutton & Barto, 2018), specified by a tuple $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{T}, r, \\gamma)$, where $\\mathcal{S}$ denotes the state space, $\\mathcal{A}$ is the action space, $\\mathcal{T}$ represents the transition function, $r$ is the reward function, and $\\gamma \\in (0, 1)$ denotes the discount factor.\nFor policy $\\pi(a|s)$, the Q-function $Q^{\\pi}(s, a)$ is defined as the"}, {"title": "3.1. Offline RL", "content": ""}, {"title": "3.2. Support Constraint Policy Improvement", "content": "The policy optimization objective of wBC methods are summarized in (Mao et al., 2023), shown as follows,\n$\\displaystyle J_{WBC}(\\phi) = \\max_{\\phi} \\mathbb{E}_{s \\sim \\mathcal{D}} [f(s, a; \\pi_{\\theta}) \\log \\pi_{\\phi}(a|s)].$\nwhere $f(s, a; \\pi_{\\theta}) = \\frac{\\exp\\left(\\frac{A^{\\pi_{\\theta}}(s, a)}{\\lambda}\\right)}{Z(s)}$, $Z(s) = \\int_{a \\sim base} \\exp\\left(\\frac{A^{\\pi_{\\theta}}(s, a)}{\\lambda}\\right) da$ but $Z(s)$ is usually omitted during training, because errors in the estimation of $Z(s)$ caused more harm than the benefit the method derived from estimating this value, thereby $f(s, a; \\pi_{\\theta}) = \\exp\\left(\\frac{A^{\\pi_{\\theta}}(s, a)}{\\lambda}\\right)$, and $\\lambda$ is a temperature parameter.\nSince $f(s, a; \\pi_{\\theta}) > 0$, the policy update of wBC is an equal-support update. Based on the key observation above, STR proposed a support constraint policy improvement method and adopted the Importance Sampling (IS) technique. The objective of policy improvement in STR is designed as follows,\n$\\displaystyle J_{STR}(\\phi) = \\max_{\\phi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\left[\\frac{\\pi(a|s)}{\\pi_{\\beta}(a|s)} f(s, a; \\pi_{\\phi}) \\log \\frac{\\pi_{\\phi}(a|s)}{\\pi_{\\beta}(a|s)}\\right]$\nwhere $\\pi_{\\beta}(a|s)$ is behavior policy, and $\\pi_{\\phi}(a|s)$ is learned policy but detach of gradient."}, {"title": "3.3. IQL", "content": "IQL utilizes expectile regression to modify the policy evaluation objective in Eq. (2). The maximum operator is replaced by an upper expetile. In addition, to mitigate the stochasticity introduced by environment dynamics $s' \\sim p(\\cdot|s, a)$, IQL employs a separate value function that estimates an expectile solely with respect to the action distribution. Hence, the update of value function and the Q-function in IQL during policy evaluation are illustrated below:\n$\\displaystyle \\mathcal{L}_{V}^{IQL}(\\psi) = \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} [L_{\\tau}(Q_{\\theta}(s, a) - V_{\\psi}(s))].$\n$\\displaystyle \\mathcal{L}_{Q}^{IQL}(\\theta) = \\mathbb{E}_{(s,a,s') \\sim \\mathcal{D}} [(r(s,a) + \\gamma V_{\\psi}(s') - Q_{\\theta}(s, a))^{2}].$\nwhere $L_{\\tau}(u) = |\\tau - \\mathbb{I}(u < 0)|u^{2}$, the $\\tau$ expectile of some random variable is defined as a solution to the asymmetric least squares problem, and $\\tau \\in (0, 1)$.\nIQL extract the policy using the wBC method. Therefore, the objective of policy improvement in IQL is defined as follows,\n$\\displaystyle J^{IQL}(\\phi) = \\max_{\\phi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} [f(s, a; \\pi_{\\beta}) \\log \\pi_{\\phi}(a|s)].$"}, {"title": "4. Projection IQL with Support Constraint", "content": ""}, {"title": "4.1. Policy Evaluation in Proj-IQL", "content": "For the sake of simplicity, we define $\\mathbb{E}_{x}[x]$ be a $\\tau$th expectile of X, e.g. $\\mathbb{E}_{0.5}$ is the standard expectation. Then the recursive relationship between $V_{\\psi}(s)$ and $Q_{\\theta}(s, a)$ are shown as follows,\n$\\displaystyle V_{\\psi}(s) = \\mathbb{E}_{\\tau} [Q_{\\theta}(s,a)].$\n$\\displaystyle Q_{\\theta}(s, a) = r(s, a) + \\gamma \\mathbb{E}_{s' \\sim p(\\cdot|s,a)} [V_{\\psi}(s')].$\nBased on the Lemma. 4.1, a larger $\\tau$ results in a more optimistic policy evaluation, with the Q-function update approaching the maximum operator in Q-Learning as $\\tau \\rightarrow 1$. Conversely, a smaller $\\tau$ produces a more conservative policy evaluation.\nTo demonstrate the importance of parameter $\\tau$, we evaluate the performance of IQL in $\\tau = 0.3, 0.6, 0.9$ on totally 10 D4RL tasks, including 6 Gym-MuJoCo-v2 datasets and 4 AntMaze-v0 datasets."}, {"title": "4.2. Policy Improvement in Proj-IQL", "content": "The policy improvement in IQL is the wBC method, shown in Eq. (7). Nevertheless, directly applying the wBC approach to Proj-IQL does not yield satisfactory results. This is because the policy struggles to converge to the optimal solution when $\\epsilon$ is sufficiently small. Further details and theoretical justification are illuminated in the Lemma. 4.3.\nBased on the Lemma. 4.3, to achieve the optimal policy with high probability, $\\epsilon$ must be set to a large value. However, this requirement contradicts the assumptions of the policy evaluation method, rendering the wBC policy improvement approach unsuitable for Proj-IQL. By contrast, the support constraint-based policy improvement method, as presented in Eq. (4), relaxes the density constraint. Consequently, we adopt the support constraint-based method for policy improvement, shown as follows,\n$\\displaystyle J(\\phi) = \\max_{\\phi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\left[\\frac{\\pi(a|s)}{\\pi_{\\beta}(a|s)} f(s, a; \\pi_{\\phi}, \\tau_{proj}) \\log \\frac{\\pi_{\\phi}(a|s)}{\\pi_{\\beta}(a|s)}\\right]$\n$\\displaystyle \\quad where f(s, a; \\pi_{\\phi}, \\tau_{proj}) = \\exp\\left(\\frac{A^{\\pi_{\\phi}, \\tau_{proj}}(s,a)}{\\lambda}\\right), A^{\\pi_{\\phi}, \\tau_{proj}}(s,a) = Q^{\\tau_{proj}}(s, a) - V_{\\psi}(s).$\nWith an exact tabular Q-function, we demonstrate that Proj-IQL guarantees strict policy improvement.\nTheorem 4.4. If we have exact Q-function and $\\tau_{k+1}(a|s) \\geq \\tau_{k}(a|s)$, then $\\pi_{k}$ in Proj-IQL enjoys monotonic improvement:\n$\\displaystyle Q_{\\theta}^{\\tau_{k+1}}(s, a) \\geq Q_{\\theta}^{\\tau_{k}}(s, a), \\quad \\forall s,a.$\nTheorem. 4.4 establishes that Proj-IQL ensures monotonic policy improvement when policy optimization is performed under support constraint. Compared to the STR algorithm, Proj-IQL shows a significant advantage through its rigorous policy evaluation criterion, which enforces the policy to \"concentrate\" more effectively on superior actions, where superior actions are defined as {$a|Q(s, a) - V(s) \\geq 0$}.\nTo illustrate this advantage, we utilize the Monte Carlo estimates approach. Specifically, we sample a large number of"}, {"title": "4.3. Practical Implementation of Proj-IQL", "content": "The behavior policy $\\pi_{\\beta}(a|s)$ is approximated using a neural network trained via BC method, as defined in Eq. (17),\n$\\displaystyle \\pi_{\\beta}(a|s) = \\operatorname{argmax}_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} [\\log \\pi(a|s)]$"}, {"title": "5. Experiments", "content": "In this section, we first evaluate the performance of Proj-IQL on D4RL benchmarks. Additionally, we show the trend of $\\tau_{proj}(a|s)$ to experimentally confirm the reasonableness of assumption $\\tau_{k}(a|s) \\leq \\tau_{k+1}(a|s)$ in Theorem. 4.4 and Theorem. 4.7. Finally, we conduct ablation experiments for different numbers of batch samples."}, {"title": "5.1. Comparisons on D4RL Benchmarks", "content": "Gym-MuJoCo Locomotion Tasks include hopper, halfcheetah, and walker2d environments, which are popular benchmarks used in prior offline RL works. Hence, we compare Proj-IQL against other competitive and widely used baselines such as Decision Transformer (DT) (Chen et al., 2021), RvS (Emmons et al., 2021), POR (Xu et al., 2022), CQL (Kumar et al., 2020), LAPO (Chen et al., 2022), TD3+BC (Fujimoto & Gu, 2021), Reinformer (Zhuang et al., 2024), CRR (Wang et al., 2020) and IQL (Kostrikov et al., 2021). The results are shown in Tab. 1, where we average the mean return over 10 evaluation trajectories and 5 random seeds and calculate the standard deviation.\nDespite the performance of different offline RL algorithms on Gym-MuJoCo-v2 datasets being saturated, Proj-IQL still demonstrates strong performance, achieving top-2 rankings on 6 out of 9 tasks and securing the highest overall score across all 9 datasets.\nAntMaze and Kitchen Navigation Tasks present significantly greater challenges, compared to the Gym-MuJoCo locomotion tasks. The main challenge is to learn policies for long-horizon planning from datasets that do not contain optimal trajectories. Consequently, algorithms must demonstrate strong \"stitching\u201d capabilities, combining suboptimal trajectory segments into coherent, high-performing trajectories. We evaluate Proj-IQL against several competitive methods, including Behavior Cloning (BC), BCQ (Fujimoto et al., 2019), BEAR (Kumar et al., 2019), CQL (Kumar et al., 2020), Onestep (Brandfonbrener et al., 2021), TD3+BC (Fujimoto & Gu, 2021), Diffusion-QL (Wang et al., 2022), X-QL (Garg et al., 2023), and IQL (Kostrikov et al., 2021), utilizing the same dataset versions as employed in IQL for a fair comparison.\nreveals that imitation learning methods such as BC and BCQ struggle to achieve satisfactory results. This limitation arises due to sparse rewards and a large amount of suboptimal trajectories, which place higher demands on algorithms for stable and robust Q-function and value function estimation. Proj-IQL addresses these challenges by dynamically and adaptively tuning the $\\tau_{proj}$ parameter to estimate the Q-function and value function according to the current policy. As a result, Proj-IQL demonstrates exceptional performance on the AntMaze-v0 and Kitchen-v0 datasets, particularly excelling in AntMaze, where it achieves the highest scores on 5 out of the 6 datasets."}, {"title": "5.2. The Training Curves of proj", "content": "In contrast to the fixed hyperparameter $\\tau$ in IQL, $\\tau_{proj}(a|s)$ can be flexibly and adaptively tuned to the current policy and batch samples. Therefore, we plot the $\\tau_{proj}(a|s)$ curve and normalized score curve on AntMaze-v0 and Kitchen-v0 datasets."}, {"title": "5.3. Empirical Study on the Projection Parameter", "content": "In the proposed Proj-IQL method, each sample in the batch is treated as a component of the vector used to compute $\\tau_{proj}(a|s)$, making the batch size to be one of the critical parameters. To investigate this, we evaluated the performance of $\\tau_{proj}(a|s)$ as well as normalized scores corresponding batch size = 16, 64, 128, 256 on three Kitchen-v0 datasets.\nAs shown the normalized score gradually increases with larger batch sizes, which aligns with expectations. This improvement occurs because larger batch sizes allow both the policy and value networks to be trained more effectively. However, this trend does not fully capture the specific impact of batch size on the parameter $\\tau_{proj}(a|s)$. To address this issue, we also plotted the relationship between batch size and $\\tau_{proj}(a|s)$. The results clearly indicate that the stability of $\\tau_{proj}(a|s)$ improves as the batch size increases, with reduced fluctuations. A more stable $\\tau_{proj}(a|s)$ is particularly beneficial for enhancing performance because of the policy evaluation is more stable."}, {"title": "6. Conclusion", "content": "We propose Proj-IQL, an in-sample, multi-step, support-constrained offline RL algorithm based on IQL. During the policy evaluation phase, Proj-IQL transitions from a one-step approach to a multi-step in-sample offline RL algorithm while preserving the expectile regression framework. In the policy improvement phase, Proj-IQL incorporates a support constraint to facilitate effective policy updates. Furthermore, we provide a theoretical foundation, demonstrating the invariance of the expectile regression framework, monotonic policy improvement guarantees, and a more rigorous policy evaluation criterion. Empirical results validate the theoretical insights of Proj-IQL and highlight its state-of-the-art performance on D4RL benchmarks, particularly excelling in challenging AntMaze-v0 and Kitchen-0 navigation tasks.\nIn the future, Proj-IQL will be improved to more efficient learning policies on less amount of data."}, {"title": "A. Proofs", "content": ""}, {"title": "Proof of Lemma. 4.1.", "content": "For all $s, \\tau_{1}$ and $\\tau_{2}$ such that $\\tau_{1} < \\tau_{2}$ we get\n$V_{\\tau_{1}} (s) \\leq V_{\\tau_{2}}(S).$"}, {"title": "Proof of Theorem. 4.2.", "content": "If the $\\epsilon$ in the constraint Eq. (10) is sufficiently small for all $a \\in {\\pi_{\\beta}(a|s) > 0, \\pi_{\\phi}(a|s) > 0, and Q_{\\theta}(s, a) -V_{\\psi}(s) < 0, for all s.}$, then Eq. (13) will be transformed as follows,\n$\\mathcal{L}_{V}(\\psi) = \\mathbb{E}_{s \\sim \\mathcal{D},\\atop{a \\sim \\pi_{\\phi}(18)}} \\left[L^{\\tau_{proj}} (Q_{\\theta}(s, a) - V_{\\psi}(s))\\right].$\nwhere $\\tau_{proj}(a|s) = \\frac{\\pi_{\\beta}(as).(\\pi_{\\theta}(a|s))}{\\|\\pi_{\\theta}(as)\\| ^{2}}\\pi_{\\beta}(as).$\nFor one state-action pair $(s, a)$, the $\\tau_{proj}(a|s)$ is defined as Eq. (12). We bring the $\\tau_{proj}(a|s)$ into Eq. (13) and expand,\n$\\mathcal{L}_{V}(\\psi) = \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\left[L^{\\tau_{proj}} (Q_{\\theta}(s, a) - V_{\\psi}(s))\\right]$\n$\\begin{cases}\n\\mathbb{E}_{(s,a) \\sim \\mathcal{D}} [\\tau_{proj} (Q_{\\theta}(s, a) - V_{\\psi}(s))^{2}], & \\text{if $Q_{\\theta}(s, a) - V_{\\psi}(s) \\geq 0$};\\\\\n\\mathbb{E}_{(s,a) \\sim \\mathcal{D}} [(1 | - \\tau_{proj})(Q_{\\theta}(s, a) - V_{\\psi}(s))^{2}], & \\text{if $Q_{\\theta}(s, a) - V_{\\psi}(s) < 0$}.\n\\end{cases}.$"}]}