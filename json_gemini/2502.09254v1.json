{"title": "AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection", "authors": ["Hezhe Qiao", "Chaoxi Niu", "Ling Chen", "Guansong Pang"], "abstract": "Graph anomaly detection (GAD) aims to identify abnormal nodes that differ from the majority of the nodes in a graph, which has been attracting significant attention in recent years. Existing generalist graph models have achieved remarkable success in different graph tasks but struggle to generalize to the GAD task. This limitation arises from their difficulty in learning generalized knowledge for capturing the inherently infrequent, irregular and heterogeneous abnormality patterns in graphs from different domains. To address this challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model that supports zero-shot inference and few-shot prompt tuning for GAD in diverse graph datasets. One key insight is that graph-agnostic representations for normal and abnormal classes are required to support effective zero/few-shot GAD across different graphs. Motivated by this, AnomalyGFM is pre-trained to align data-independent, learnable normal and abnormal class prototypes with node representation residuals (i.e., representation deviation of a node from its neighbors). The residual features essentially project the node information into a unified feature space where we can effectively measure the abnormality of nodes from different graphs in a consistent way. This provides a driving force for the learning of graph-agnostic, discriminative prototypes for the normal and abnormal classes, which can be used to enable zero-shot GAD on new graphs, including very large-scale graphs. If there are few-shot labeled normal nodes available in the new graphs, AnomalyGFM can further support prompt tuning to leverage these nodes for better adaptation. Comprehensive experiments on 11 widely-used GAD datasets with real anomalies, covering social networks, finance networks, and co-review networks, demonstrate that AnomalyGFM significantly outperforms state-of-the-art competing methods under both zero- and few-shot GAD settings. Our code will be available at https://github.com/mala-lab/AnomalyGFM.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph anomaly detection (GAD) aims to identify anomalous nodes within a graph that deviate significantly from others. It has found broad applications in areas such as financial fraud detection, transaction analysis, and social networks [1, 22, 26, 30]. Most existing GAD methods, either supervised or unsupervised methods, follow a paradigm of training and inference on the same graph, making the trained models struggle to generalize to new/unseen graphs due to the distribution shift between the training and testing graphs [6, 12, 20, 28]. Further, this type of methods can become inapplicable in applications where graph data is not accessible during training due to data privacy or other data access issues. Generalist/foundation models for graphs have achieved remarkable progress in tackling these challenges in general tasks such as node classification, graph classification, and link prediction [9, 16, 32, 41, 45], but they are difficult to generalize the GAD task. The main reason is that the generalized knowledge learned in these generalist models through generic pre-training or tuning methods incline to the prevalent regular patterns, which differ significantly from the inherently infrequent, irregular and heterogeneous abnormality patterns across the graphs [17, 19, 30].\nVery recently, there have been some efforts, such as ARC [19] and UNPrompt [24], dedicating to generalist models for GAD. Despite the effectiveness on some datasets, these methods fail to provide a universal GAD model that versatilely supports zero-shot inference and sample-efficient tuning in different application scenarios. For instance, UNPrompt [24] is designed exclusively for zero-shot inference, while ARC [19] is limited to few-shot tuning-based GAD within the same domain. Therefore, a foundational model being capable of effectively capturing abnormalities across graphs from different domains and supporting both zero-shot and few-shot GAD is still lacking.\nTo fill this gap, in this paper, we introduce AnomalyGFM, a GAD-oriented graph foundation model (GFM) that is capable of addressing both zero-shot and few-shot scenarios effectively for GAD. One key insight is that to avoid overfitting the pre-training graph data, graph-agnostic representations for normal and abnormal classes are required to support effective zero/few-shot GAD"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Graph Anomaly Detection", "content": "Existing GAD methods can be generally classified into unsupervised and supervised methods [22, 30], depending on the assumption regarding the presence of normal and anomaly labels. The fully supervised approaches have recently achieved remarkable progress by leveraging both labeled normal and abnormal nodes [11, 18, 34, 36]. These methods are primarily designed to improve message aggregation in graph neural networks (GNNs) by reducing heterophilic edges from both spectral and spatial perspectives, effectively mitigating the over-smoothing problem in GAD [30, 33]. Fully supervised methods are highly effective in GAD as they treat the task as an imbalanced classification problem. However, the requirement for both normal and abnormal labels significantly hinders their applications to scenarios where labeled nodes are difficult to obtain.\nUnsupervised methods, which typically assume the absence of both normal and abnormal labels, have garnered significant attention due to their more practical setting assumption on data labels. They generally incorporate some conventional techniques such as reconstruction [6], one-class classification [28, 37, 43], contrastive learning [20, 25], and adversarial learning [5] into graph learning to capture the normal patterns within the graph and then assign an anomaly score for each node based on its deviation from the normal patterns. However, these methods still follow the paradigm of training and inference on the same graph, making them struggle to generalize to new/unseen graphs due to the distribution shift between training and testing set."}, {"title": "2.2 Foundation Models for GAD", "content": "Generalist models have recently achieved significant progress on non-graph data by leveraging large pre-trained models to facilitate generalized pattern recognition in diverse downstream tasks, such as generalist anomaly detection on images [42, 44]. However, applying these methods to graph data remains highly challenging due to the absence of such pre-trained models [21, 32, 39] on graph data. The main challenge in designing a GFM is capturing invariance across diverse graphs while mapping them into a shared representation space to enable positive transfer between training and inference [2, 10, 23]. Currently, most GFM models use prompt"}, {"title": "3 THE PROPOSED ANOMALYGFM", "content": ""}, {"title": "3.1 Preliminaries", "content": "Notation and Conventional GAD. An attributed graph is denoted by $G = (V, E)$ where $V = {v_1, v_2, ..., v_N}$ represents the node set and $E$ represents the edge set. The graph can also be denoted as $G = (A, X)$ where $X = [x_1, x_2, ..., x_N]^T \\in R^{N \\times d}$ is the attribute of the nodes and $A \\in R^{N \\times N}$ is the adjacent matrix of the graph with $A_{ij} = 0$ representing there is no edge between node $v_i$ and node $v_j$ and $A_{ij} = 1$ otherwise. In GAD, we denote the normal node set as $V_n$ and abnormal node set $V_a$, corresponding to the anomaly labels $y \\in {0,1}^N$ with ($y_i = 0$ denotes a normal node $v_i$ and $y_i = 1$ denotes abnormal). Typically, the number of normal nodes is significantly larger than the abnormal nodes in the GAD datasets, i.e., $|V_n| > |V_a|$. The conventional GAD is formulated as learning a scoring function $f: G \\rarr R$ and making inference on the same G, in which f is expected to assign every node an anomaly score such that $f(v) < f(v'), \\forall v \\in V_n, v' \\in V_a$.\nZero-shot GAD. Different from the conventional GAD that trains and inferences on the same graph, zero-shot GAD aims to train a generalist model on $G_{Train}$ and then apply the learned model to the new test graphs $G_{Test}$ without any further tuning/retraining. Few-shot GAD. In the few-shot GAD setting, the model is fine-tuned on $G_{Train}$ with small labeled nodes and then applied to its test graph $G_{Test}$. We denote the labeled node in the test graph as $V_{Test}^L$ while the unlabeled nodes in the test graph are denoted as $V_{Test}^U = V_{Test} \\setminus V_{Test}^L$. Here we only consider the case of having few-shot labeled normal nodes since the normal node labels are easier/less costly to obtain than the abnormal ones [19].\nFeature Unification. Due to the feature dimension difference across the graphs, we need to align the node features/attributes into a shared feature space to ease the feature heterogeneity among the graphs. Following the previous studies [19, 24], we employ singular value decomposition (SVD), a simple, efficient dimensionality reduction technique that can approximately preserve the distance relationship of the data by projecting high-dimensional data into a low-dimensional subspace for unification. For each dataset $X^{(i)}$"}, {"title": "3.2 Approach Overview", "content": "AnomalyGFM aims to distill discriminative node residual features in a unified feature space into two graph-agnostic prototypes for the normal and abnormal classes, facilitating the establishment of a foundation model that supports effective zero/few-shot GAD across graphs. Specifically, AnomalyGFM consists of pre-training based on the prototype alignment, few-shot graph prompt fine-tuning, and zero-shot/few-shot inference, as shown in Fig. 2. In the pre-training step, we aim to align the data-independent and learnable normal class and abnormal class prototypes with the nodes' residua features to achieve the distillation of graph-agnostic discriminative class-level representations. In the few-shot graph prompt fine-tuning, the labeled normal nodes' residual features are utilized to guide a better adaptation of the normal class prototype. In the inference step, we devise zero-shot inference on new graphs, including subgraph-based inference for very large-scale graphs, and few-shot inference, both of which compute anomaly scores by measuring the similarity between the residual features of nodes and the two class prototypes."}, {"title": "3.3 Pre-training via Prototype Alignment in a Unified Feature Space", "content": "Node Representation Residual. Node representation residual can be seen as the deviation at the representation level between the target node and its neighboring nodes. The deviation in the representation between the target node and its neighbors is expected to be small for normal nodes, regardless of the graph domain, while the deviation for abnormal nodes is relatively large. Therefore, the node representation residual offers a consistent method to effectively capture heterogeneous abnormalities across graphs. Assume the node representation outputted by the GNNs as $h_i \\in R^d$, where d is the dimension of representation, the residual feature $r_i \\in R^d$ for the node $v_i$ is formally defined as follows:\n$r_i = h_i - \\frac{1}{|N(v_i)|} \\sum_{v_j \\in N(v_i)} h_j$,\nwhere $N(v_i)$ represents the neighbor set of node $v_i$. As a graph-agnostic representation, the node representation residual will be assigned to each node as its new feature in the unified feature space."}, {"title": "3.4 Few-shot Prompt Tuning", "content": "To enable the few-shot prompt tuning in the scenarios where few-shot labeled normal nodes are available in the test/target graph, we utilize these limited nodes to have a prompt tunign of AnomalyGFM in the prototype learning module while keeping the GNN parameters frozen.\nNormal Prototype-based Prompting. To fully leverage the labeled normal nodes during the fine-tuning step, we introduce a small learnable prompt into the normal class prototype to better align it with the normal node representation residuals on the test graph during a prompt fine-tuning step. In addition, we also add an adaptation layer that offers more learning flexibility to the tuning. They are designed to refine the pre-trained normal class prototype $p_n$ to a new normal prototype $p'_n$, ensuring a better alignment with the normal node residual feature in the target graphs. Formally, the new normal prototype $p'_n$ is defined as:\n$p'_n = p_n + g(p_n; \\phi) + \\Psi_{p_n}$,\nwhere $\\Psi_{p_n} \\in R^d$ is the learnable prompt while $g(p_n; \\phi)$ is the added adaptation layer with parameters $\\phi$, both of which are of the same dimensionality as $p_n$. The learned prompts and the adapted prototype complements the pre-trained normal prototype in the"}, {"title": "3.5 GAD Using AnomalyGFM", "content": "Zero-shot Inference. To build a foundation model for GAD with zero-shot learning capabilities, we directly apply AnomalyGFM pre-trained on auxiliary graphs to the test graph $G_{test}$, meaning that $\\Theta_n$ and $\\Theta_a$ in $\\Phi(z_n; \\Theta_n)$ and $\\Phi(z_a; \\Theta_a)$ and the trainable parameters $W$ in GNN are all frozen. This process allows us to obtain the residual features for each node in the test graph, the graph-agonistic normal prototype $p_n$ and abnormal prototype $p_a$. The anomaly score is determined by computing the similarity of the target node's representations residual with the two prototypes $p_a$ and $p_n$. Formally, the anomaly score $s_i$ for the node $v_i$ is defined as,\n$s_i = exp(\\beta \\langle r_i, p_a \\rangle) + \\beta \\langle r_i, -p_n \\rangle$,\nwhere $\\beta$ is the hyperparameter that controls the weight of two parts. Note that the anomaly scoring calculation consists of the direct similarity with $p_a$ and the inverse similarity with $p_n$.\nFew-shot Inference. In the few-shot inference, the original parameters in GNN, mapping functions, and newly added parameters for refining the normal class prototype are frozen. The anomaly score of $v_{i \\in V_{test}}$ is calculated based on the prompt-tuning-based updated normal class prototype $p'_n$ and the pre-trained abnormal class prototype $p_a$, which is defined as:\n$s_i = exp(\\beta \\langle r_i, p_a \\rangle) + \\beta exp(\\langle r_i, -p'_n \\rangle)$.\nInference on Very Large-scale Graphs. Most existing GAD methods typically require loading all nodes of the graph, which often leads to poor scalability on large graphs during inference. By learning the graph-agnostic prototypes, AnomalyGFM can generalize to very large-scale graphs through a subgraph-based inference approach. AnoamlyGFM can effectively infer the anomaly score without considering the entire graph structure, eliminating the bottleneck of loading the full graph for GAD inference. This subgraph inference is also desired in privacy sensitive settings where we do not want to reveal the entire graph structure to the detection models. Specifically, AnomalyGFM operates in a way that given a"}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Datasets. We utilize 11 benchmark datasets from social networks, finance, and co-review domains for evaluation. These datasets are summarized in Table 1, with additional details provided in Appendix A. The social network datasets consist of Facebook [40], Reddit [15], Question [27] and T-Social [34]. The financial datasets include T-Finance [34] and Elliptic [38]. For the co-review category, we use Amazon [7], Amazon-all [7], and YelpChi-all [7]. Additionally, the Disney [31] and Tolokers [27] datasets, derived from co-purchase and work collaboration networks, are also included.\nZero-shot Competing Methods. For the zero-shot setting, we evaluate AnomalyGFM against the state-of-the-art approaches from three main categories as follows (1) unsupervised methods: AnomalyDAE [8], COLA [20], TAM [28], and GADAM [4]; (2) supervised methods: GCN [14], GAT [35], BWGNN [34], GHRN [11], and XGB-Graph [33]; and (3) general GFM methods, including GraphPrompt [21] for general graph tasks, and the one for zero-shot GAD UN-Prompt [24]. The detailed descriptions of each zero-shot competing method are provided in Appendix B.\nFew-shot Competing Methods. In the few-shot setting, two generalist methods are included, i.e., GraphPrompt [21] and ARC [19]. Detailed descriptions of each generalist competing method are provided in Appendix. B.\nEvaluation Metrics. Following the previous studies [19, 29, 34], two widely-used metrics, AUROC and AUPRC, are used to evaluate the performance of all methods. For both metrics, a higher value denotes better performance. Moreover, for each method, we report the average performance for 3 independent runs with different random"}, {"title": "4.2 Zero-shot GAD Performance", "content": "The AUROC and AUPRC results under the zero-shot setting are shown in Table 2 where the model is trained on Facebook and"}, {"title": "4.3 Few-shot GAD Performance", "content": "To demonstrate the effectiveness in the few-shot scenarios, the comparison of AnomalyGFM to GraphPrompt [21] and ARC [19] is done under 1/5/10-shot. The AUROC and AUPRC results are shown in Table 3. In the few-shot GAD scenario, AnomalyGFM outperforms these two generalist methods on most datasets in both AUROC and AUPRC. GraphPrompt continues to underperform AnomalyGFM since the presence of only one class of samples renders its fine-tuning ineffective, leading to suboptimal performance. ARC also leverages residual features and is specifically designed for few-shot GAD datasets achieving the best performance on the Question and the second performance on most datasets. Compared to ARC, AnomalyGFM achieves better sample-efficiency tuning due to the learning of class-level prototypes that are agnostic to different domains of graphs, enhancing the generalization across the graph, thereby yielding the best performance on the eight datasets."}, {"title": "4.4 Scale up to Very Large Graphs", "content": "We choose two large-scale graphs T-Finance (with a large number of edges) and T-Social (with a large number of nodes) to evaluate the generalization of AnomalyGFM on very large graphs. The results are shown in Table 4. We can observe the AnomalyGFM outperforms the unsupervised methods and supervised methods on these datasets. This is mainly because the learned data-independent prototypes can effectively measure the abnormality of node representation residual, even in the presence of subgraphs. The deviations of the target node from the contextual node are still preserved in the subgraph, thereby enabling effective subgraph-bsed inference."}, {"title": "4.5 Ablation Study", "content": "To verify the effectiveness of each component of AnomalyGFM, we designed three variants including (i) BCE which replaces the similarity measurement with the predicted probability from the BCE loss function for anomaly scoring. (ii) BCE Residual (BCE-R) which firstly removes the alignment loss and applies the BCE loss on the residual features to differentiate normal and abnormal nodes. Then, the predicted probability is used as the anomaly score. (iii) Feature Alignment (FA) which replaces the alignment on the residual feature with the alignment on the original feature. The results are shown in Table 5. FA achieves the highest AUROC on the Elliptic and T-Finance datasets, suggesting that learning prompts from the original features can also be effective as the original feature also maintains anomaly discriminability but they aren't in the unified feature space leading to sub-optimal performance. BCE does not perform well except on the small dataset Disney. It directly uses the predicted probability as the anomaly score, which makes it struggle to generalize to new graphs due to the differences between the training and test sets. BCE-R does not achieve good performance compared to BCE. The reason is that although the residual features are discriminative, many samples located on the decision boundaries after being mapped to a unified feature space may not be effectively scored by the supervised classifier. In contrast, aligning the learnable prototype with the average node representation residual and using similarity measurement for scoring can effectively discriminate the normal and abnormal nodes."}, {"title": "4.6 Hyperparameter Sensitivity Analysis", "content": "We evaluate the sensitivity of AnomalyGFM w.r.t the size of the prototype T, hyperparameter $\\alpha$, and commonality dimension d'. Additional sensitivity analyses are in Appendix C.2.\nAUROC and AUPRC results of AnomalyGFM w.r.t. prototype size T. As shown in Fig. 4, AnomalyGFM generally achieves better performance with increasing prototype size. This suggests that a larger prototype size provides more information and enhances the modeling capability of the foundation model.\nAUROC and AUPRC results of AnomalyGFM w.r.t. $\\alpha$. We vary the hyperparameter $\\alpha$ in the range of [0.6, 1.2] with the interval of 0.1 and the results are shown in Fig. 5. We can see that AnomalyGFM remains stable with different $\\alpha$ on most datasets in terms of both metrics, demonstrating the robustness of AnomalyGFM against this hyperparameter.\nSensitivity of AnomalyGFM w.r.t common dimensionality d'. To further demonstrate AnomalyGFM's generalization, we evaluate AnomalyGFM under different d'. The results are shown in Fig. 6. AnomalyGFM outperforms all the competing methods under different dimensions. The main reason is that the deviation between connected nodes is still preserved with different d' which can be"}, {"title": "5 CONCLUSION", "content": "In this paper, we build a GAD-oriented graph foundation model, AnoamlyGFM, that can work effectively under both few-shot and zero-shot scenarios. AnomalyGFM is pre-trained to learn discriminative and data-independent prototypes by aligning them with the graph-agnostic node representation residuals. This provides a consistent and identical way for abnormality measurement using the similarity between residual node representation and the learned class prototypes, facilitating the strong generalization in both zero-shot and few-shot inference. Extensive experiments on 11 datasets demonstrated the effectiveness and generalization of AnomalyGFM."}, {"title": "A DETAILED DATASET DESCRIPTION", "content": "A detailed introduction of all the used datasets is given as follows.\n\u2022 Facebook [40]: It is a social network where nodes represent users and edges indicate relationships between them. Anomalies are the nodes that either connect manually con-structed clusters or display attributes differing from those of their contextual neighbors.\n\u2022 Reddit [15]: It is a user-subreddit graph, capturing one month's worth of posts shared across various subreddits at Reddit. The users who have been banned by the platform are labeled anomalies. The text of each post is transformed into the feature vector and the features of the user and subreddits are the feature summation of the post they have posted.\n\u2022 Amazon [7]: It includes product reviews under the Musical Instrument category. The users with more than 80% of helpful votes were labeled as begin entities, with the users with less than 20% of helpful votes treated as fraudulent entities. There are three relations including U-P-U (users reviewing at least one same product), U-S-U (users giving at least one same star rating within one week), and U-V-U (users with top-5% mutual review similarities). In this article, we do not distinguish this connection and regard them as the same type of edges, i.e., all connections are used. There are 25 handcrafted features that were collected as the raw node features.\n\u2022 Disney [31]: It comes from the Amazon co-purchase net-work of movies where the attributes are the prices, ratings, number of reviews, etc. The anomalies are labeled manually by majority vote and the ground truths are derived from amazonfail stage information."}, {"title": "B DESCRIPTION OF BASELINES", "content": ""}, {"title": "B.1 Competing GAD Methods", "content": "A more detailed introduction of the eight GAD models we compare with is given as follows.\n\u2022 AnomalyDAE [8] consists of a structure autoencoder and an attribute autoencoder to learn both node embeddings and attribute embeddings jointly in a latent space. In addition, an attention mechanism is employed in the structure encoder to capture normal structural patterns more effec-tively."}, {"title": "B.2 Competing Generalist Methods", "content": "\u2022 GraphPrompt [21] builds a pre-training and prompting framework designed for both graph-level and node-level classification tasks. It learns a prompt that can help the downstream tasks to identify and leverage the most relevant knowledge from the pre-trained model in a task-specific way.\n\u2022 ARC [19] is the first fine-tuning generalist GAD approach that enables a \"one-for-all\" GAD model. It proposes an ego-neighbor residual graph encoder that learns abnormality-related node embedding. In the inference step, a cross-attentive in-context anomaly scoring module is employed to predict node abnormality by leveraging few-shot normal samples."}, {"title": "C ADDITIONAL EXPERIMENTAL RESULTS", "content": ""}, {"title": "C.1 Train the Model on Amazon and Evaluate on Other Datasets", "content": "To further demonstrate the effectiveness of AnomalyGFM, we pre-train the AnomalyGFM on Amazon and evaluate it on the other datasets. The AUROC and AUPRC results are shown in the Table 6. AnomalyGFM consistently outperforms competing methods from supervised methods, unsupervised methods, and generalist methods across most datasets in both AUROC and AUPRC, demonstrating its strong generalization when pre-trained on a different auxiliary dataset."}, {"title": "C.2 Sensitivity Analysis", "content": "Impact of $\u03b2$ in anomaly scoring. The performance under different $\u03b2$ is shown in Fig. 7. As shown in Table 7, the set of $\u03b2$ values are determined based on the prior we gain from the global average similarity. Specifically, the hyperparameter in anomaly scoring $\u03b2$ is set to zero by default for graphs with high global similarity such as Amazon and Yelp, and four for those with low global similarity such as Ellitipic and T-Finance, see the detailed global similarity information in Table 1. In the few-shot scoring, $\u03b2$ is set to 0.5 by default for graphs with high global similarity. The main reason for these settings is that for graphs with high global average similarity, where node features are relatively similar, the deviations in local neighborhood would be small, and the fluctuations in scoring based on the normal prototype may affect the overall anomaly score. Therefore, we prefer to solely use the abnormal class prototype or assign a smaller weight for the normal class prototype in the anomaly scoring. For nodes with less similar features, we use a mix of normal and abnormal prototypes for anomaly detection. In the few-shot setting, the labeled normal nodes help refine the normal prototypes, allowing the anomaly scores to be computed using both normal and abnormal prototypes.\nImpact of commonality dimension $d'$. The AUPRC results are shown in Fig. 8. We can see that AnomalyGFM still outperforms all the competing methods on AUPRC under the different dimensions, indicating that AnomalyGFM is generally robust to the feature dimensionality."}, {"title": "D TIME COMPLEXITY ANALYSIS", "content": "In this section, we analyze the time complexity of pre-training and Fine-tuning of AnomalyGFM. We build the GCN as the backbone of AnomalyGFM to obtain the representation of each node, which takes O(mdh), where m is the number of non-zero elements in matrix A (the number of edge in graph), d is the dimension of representation, and h is the number of feature maps. The computation of the residual feature takes O(N\u00b2d). The MLP layer mapping the representation to the anomaly score takes O(Nd). The feature mapping of normal class prototype and abnormal class prototype takes O(2d\u00b2). The alignment loss between residual features and prototypes take O(2Nd). Thus, the overall complexity of the pre-training of AnomalyGFM is O(mdh+N2d+3Nd + 2d\u00b2). In the inference, we employ the residual feature's similarity with two prototypes which takes O(2Nd). Therefore, the overall complexity of inference of AnomalyGFM is O(mdh + N\u00b2d + 2d\u00b2 + 2Nd).\nIn Table 9, we report the training time on Facebook and inference time on YelpChi-all of AnomalyGFM. The competing methods, including two representative unsupervised methods AnomalyDAE and TAM, and two supervised methods GAT and BWGNN are used"}, {"title": "Algorithm 1 Pre-training of AnomalyGFM", "content": "1: Input: Training graph $G_{Train} = (V_{train}, E_{train})$; Pre-training epoch $E$.\n2: Output: Pre-training AnomalyGFM($A, X$).\n3: Perform feature unification of $X$ to obtain $\\hat{X}$.\n4: Initialize the Gaussian distribution $z_n$ and $z_a$ for normal class prototype and abnormal class prototype.\n5: Randomly initialize GNN $({h_1}^{(0)}, {h_2}^{(0)}, ..., {h_L}^{(0)}) \\gets \\hat{X}$\n6: for epoch = 1,..., E do\n7:    for each $v$ in $V_{train}$ do\n8:        for l = 1,..., L do\n9:            ${h_v}^{(l)} = W^{(l)}{h_v}^{(l-1)}$\n10:           ${h_v}^{(l)} = ReLU (AGG({h_{v'}}^{(l)} : (v, v') \\in E_{train}))$            \n11:       end for\n12:   end for\n13:   Compute the residual feature $r_i$ of each node $v_i$ using Eq. (3).\n14:   Compute the $L_{BCE}$ for classifier $p_i = f_\\theta(h_i)$.\n15:   Alignment the normal class prototype and abnormal class prototype with the residual feature using $L_{Alignment}$\n16:   Compute the total loss $L_{total} = L_{BCE} + \\beta L_{Alignment}$.\n17:   Update the trainable weight parameters $W, \\theta, \\Theta_a$ and $\\Theta_n$ by using gradient descent.\n18: end for\n19: return Pre-trained AnomalyGFM($A, X$), $\\Phi(z_n; \\Theta_n)$ and $\\Phi(z_a; \\Theta_a)$."}, {"title": "Algorithm 2 Zero-shot Inference of AnomalyGFM", "content": "1: Input: Testing graph $G_{test} = (V_{test}, E_{test})$, pre-trained AnomalyGFM($A, X$), $\\Phi(z_n; \\Theta_n)$ and $\\Phi(z_a; \\Theta_a)$.\n2: Output: Anomaly score of testing nodes.\n3: Initilization of the Gaussian distribution of normal class prototype and abnormal class prototype.\n4: for each $v_i$ in $V_{test}$ do\n5:   Apply the pre-trained AnomalyGFM($A, X$) on the graph.\n6:   Compute the residual feature of each node $v_i$ using Eq. (3).\n7:   Scoring using Similarity measurement with normal class prototype $p_n$ and abnormal class prototype $p_a$.\n8:   Compute the anomaly score using Eq. (9).\n9: end for\n10: return Anomaly scores of testing nodes $s(v_1),...,s(v_N)$."}, {"title": "Algorithm 3 Few-shot Prompt Fine-tuning of AnomalyGFM", "content": "1: Input: Testing graphs $G_{test} = (V_{test}, E_{test})$, pre-trained AnomalyGFM($A, X$), $\\Phi(z_n; \\Theta_n)$ and $\\Phi(z_a; \\Theta_a)$, the training epoch e of fine-tuning.\n2: Output: The fine-tuned AnomalyGFM ($A, X$) with updated normal class prototype $p'_n$.\n3: Initialization of the Gaussian distribution of normal class prototype and abnormal class prototype.\n4: Randomly initialize GNN $({h_1}^{(0)}, {h_2}^{(0)}, ..., {h_L}^{(0)}) \\gets \\hat{X}_{test}^L$\n5: for epoch = 1,... e do\n6:    for each $v$ in $V_{test}^L$ do\n7:        for l = 1,..., L do\n8:            ${h_v}^{(l)} = W^{(l)}{h_v}^{(l-1)}$            \n9:            ${h_v}^{(l)} = ReLU (AGG({h_{v'}}^{(l)} : (v, v') \\in E_{test}))$            \n10:       end for\n11:   Compute the residual feature $r_i$ of each labeled normal node $v_i$ using Eq. (3)\n12:   end for\n13:   Compute the $L_{pt}$ and optimize the newly added trainable parameters $\\phi, \\Psi$ using gradient descent.\n14: end for\n15: return The fine-tuned AnomalyGFM($A, X$) with the updated normal class prototype $p'_n$."}, {"title": "Algorithm 4 Subgraph-based Inference of AnomalyGFM", "content": "1: Input: Testing graphs $G_{test} = (V_{test}, E_{test})$, pre-trained AnomalyGFM($A, X$), $\\Phi(z_n; \\Theta_n)$ and $\\Phi(z_a; \\Theta_a)$.\n2: Output: Anomaly score of testing nodes.\n3: for each $v_i$ in $V_{test}$ do\n4:   Extract the subgraph $S(v_i)$ of node $v_i$ using random walk.\n5:   Apply the pre-trained AnomalyGFM($A, X$) on the extracted subgraph.\n6:   Compute the residual feature $r_i$ of each node $v_i$ using Eq. 11.\n7:   Similarity measurement with normal class prototype $p_n$ and abnormal class prototype $p_a$.\n8:   Compute the anomaly score using Eq. (9).\n9: end for\n10: return Anomaly scores of testing nodes $s(v_1),...,s(v_N)$."}]}