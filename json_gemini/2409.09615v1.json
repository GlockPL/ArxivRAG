{"title": "ENHANCING TEXT ANNOTATION THROUGH RATIONALE-DRIVEN COLLABORATIVE FEW-SHOT PROMPTING", "authors": ["Jianfei Wu", "Xubin Wang", "Weijia Jia"], "abstract": "The traditional data annotation process is often labor-intensive, time-consuming, and susceptible to human bias, which complicates the management of increasingly complex datasets. This study explores the potential of large language models (LLMs) as automated data annotators to improve efficiency and consistency in annotation tasks. By employing rationale-driven collaborative few-shot prompting techniques, we aim to improve the performance of LLMs in text annotation. We conduct a rigorous evaluation of six LLMs across four benchmark datasets, comparing seven distinct methodologies. Our results demonstrate that collaborative methods consistently outperform traditional few-shot techniques and other baseline approaches, particularly in complex annotation tasks. Our work provides valuable insights and a robust framework for leveraging collaborative learning methods to tackle challenging text annotation tasks.", "sections": [{"title": "1. INTRODUCTION", "content": "Data annotation is a critical step in training machine learning models, where the quality and quantity of labeled data directly influence model performance [1]. However, traditional annotation methods are plagued by several challenges, including the time-consuming and labor-intensive nature of manual labeling, the potential for human bias and subjectivity, scalability issues, and the difficulties in maintaining annotation quality [2]. As datasets continue to grow in size and complexity, the demand for efficient and reliable annotation processes has never been greater.\nIn this context, LLMs have emerged as a promising solution to address these challenges. By harnessing the capabilities of LLMs, organizations can automate significant portions of the annotation process, drastically reducing the time and effort required to label data. This automation not only enhances efficiency but also improves consistency across annotations, mitigating the variability often associated with human annotators [3, 4, 5]. Moreover, LLMs offer the ability to scale annotation efforts without a corresponding increase in human resources, making it feasible to manage vast amounts of data. They can also be trained to recognize and reduce biases, leading to higher-quality labeled datasets [6].\nHowever, leveraging LLMs for annotation tasks presents notable challenges. The inherently stochastic nature of LLM-generated content, coupled with the phenomenon of hallucination [7], can compromise their accuracy in performing annotation tasks. While some studies suggest that widely used LLMs, such as ChatGPT, can achieve or even exceed the quality of human annotations [3, 8], many researchers argue that relying solely on LLMs is insufficient for high-quality text annotation, necessitating additional verification processes [2, 9].\nInspired by human collective deliberation for resolving ambiguity, we propose a rationale-driven collaborative (RDC) text annotation method using prompt engineering. Unlike traditional methods that rely on the independent outputs of a single model, our approach involves multiple LLMs sequentially annotating a text. Each subsequent LLM receives the annotations from its predecessor and integrates this information to refine its inference process, resulting in more accurate annotations. This collaborative method significantly reduces computational and storage overhead compared to conventional collaborative modeling approaches [10, 11]. Experimental results indicate that our method outperforms baselines in terms of annotation accuracy. Specifically, the main contributions of our work are as follows:\n\u2022 We propose a rationale-driven collaborative text annotation method based on prompts. This approach leverages intermediate information from the reasoning process more effectively than previous prompting and traditional collaborative methods, thereby improving text annotation quality.\n\u2022 We demonstrate that the selection of examples is critical for LLM annotation; matching similar examples significantly"}, {"title": "2. RELATED WORK", "content": "2.1. Text Annotation via LLMs\nTraditional text annotation processes predominantly rely on manual labor, which is often time-consuming, labor-intensive, and prone to inconsistencies in data quality [2]. Recent studies suggest that widely utilized LLMs, such as ChatGPT, can outperform human annotators across various annotation tasks, demonstrating a higher degree of consistency compared to manual annotations [3]. Current research on leveraging LLMs for text annotation can be categorized into two primary approaches: the first approach focuses on model adaptation, employing techniques such as fine-tuning to customize models for specific tasks [12, 13]; the second approach emphasizes prompt engineering, which involves the modification of templates and prompts, as well as the utilization of collaborative strategies across multiple models to enhance annotation accuracy [11, 14, 15].\n2.2. Prompts Enhancement in LLMs\nGiven the multifaceted capabilities exhibited by LLMs, researchers have made significant strides in transforming generic LLMs into specialized agents capable of executing specific tasks through prompt engineering. Various prompting techniques, including zero-shot prompting [16], few-shot prompting [17, 18], Chain-of-Thought (CoT) reasoning [19], and collaborative methodologies [11, 20], have been employed to enable LLMs to align more closely with human intentions, systematically analyze problems, and complete reasoning processes. Furthermore, prompts facilitate the integration of reference knowledge beyond basic instructions, allowing LLMs to gather essential clues for generating questions and providing accurate answers. Our work also incorporates several of these prompting techniques to enhance the interpretability and quality of the outputs generated by LLMs."}, {"title": "3. METHODOLOGY", "content": "The proposed methodology is designed to enhance the quality and efficiency of the annotation process through a structured multi-round collaborative annotation framework. This section outlines the key components of the methodology, including the reasoning process of the LLM, the integration of previous annotations, and the utilization of similar example matching.\n3.1. LLM Reasoning Process\nThe initial step in our methodology involves the LLM's reasoning process when presented with a query Q. The LLM generates an answer A along with a supporting rationale R. This can be mathematically represented as:\n(A, R) = LLM(Q)\nThis foundational step is critical, as it establishes the basis for subsequent rounds of annotation. Previous research has often neglected the rationale R, focusing primarily on optimizing the directive to achieve annotation results. This oversight frequently leads to suboptimal quality in the annotations produced."}, {"title": "3.2. Rationale-Driven Collaborative Annotation Framework", "content": "In our rationale-driven collaborative annotation framework, we build upon the initial response by integrating both the statements awaiting annotation S and the results from the previous round $A_{i-1}$ along with their corresponding rationales $R_{i-1}$. The prompt for the i-th round is defined as:\n$P_i = (S, A_{i-1}, R_{i-1})$\nThis integration serves as a reference for the LLM's subsequent annotation, facilitating a collaborative approach that leverages prior outputs to enhance the quality of the current round's annotations."}, {"title": "3.3. Output Restriction and Error Mitigation", "content": "To improve annotation efficiency and minimize the influence of potential errors from prior rounds, we restrict our reference to the output from the most recent collaborative annotation. The LLM's output for the i-th round is thus defined as:\n$(A_i, R_i) = LLM(P_i)$\nThis approach ensures that the LLM operates without manual intervention or the need for additional prompting techniques, such as CoT prompting, thereby significantly enhancing annotation quality while reducing the burden on human annotators."}, {"title": "3.4. Similar Example Matching", "content": "To further augment the annotation process, we employ the principle of similar example matching. This involves leveraging previously annotated examples to inform the LLM's reasoning. Let D represent the set of previously annotated examples, and let E denote the text requiring annotation. The methodology combines the most similar examples $E_{sim}$ with the text E that requires annotation, enabling the LLM to perform rationale-driven collaborative reasoning:\n$E_{input} = (E, E_{sim})$\nIn this case, the Top-5 examples based on cosine similarity are selected from D. These selected examples serve as references for the LLM during the annotation process."}, {"title": "4. EXPERIMENTS", "content": "4.1. Datasets\nAs shown in Table 1, the datasets utilized in this study span a range of text annotation tasks with varying levels of complexity. The Stanford Sentiment Treebank (SST-2 and SST-5) [21] provides a basis for binary and fine-grained sentiment analysis, respectively. The AG News dataset [22] offers a straightforward multi-class annotation challenge with four distinct categories of news articles. Lastly, the DBPedia dataset [23] presents a more complex annotation task with 14 classes, derived from structured information on Wikipedia. These datasets collectively enable a comprehensive evaluation of the performance of LLMs across different text annotation scenarios."}, {"title": "4.2. Experimental Setup", "content": "For evaluating the performance of our models across different text classification tasks, we primarily use accuracy as our metric. Meanwhile, we utilize a range of LLMs, including Qwen1.5 (72B, 14B, 7B) and Llama3 (70B, 8B), to comprehensively evaluate the impact of model size and architectural diversity on the effectiveness of our collaborative few-shot prompting techniques. This selection allows us to assess performance across different scales and implementations, ensuring robust and generalizable findings.\nFor each model and dataset combination, we evaluate four different methods with random example: 1) Zero-Shot (ZS), 2) Few-Shot (FS) [17], 3) Chain-of-thought (CoT) [19], and 4) Universal Self-Consistency (USC) [11]. Additionally, we conduct experiments using similar examples based on cosine similarity for experimental comparison: 1) Few-Shot (FS-simi), 2) Chain-of-thought (CoT-simi) and 3) Universal Self-Consistency (USC-simi). Specifically, all experiments are run on NVIDIA A800 GPUs with VLLM inference engine. The few-shot prompting scenarios use 5 examples as references."}, {"title": "4.3. Results and Analysis", "content": "The results presented in Table 2 illustrate the performance of various models across four distinct datasets, highlighting the effectiveness of our proposed methods in comparison to traditional approaches. The table categorizes the performance metrics into several annotation strategies, including Zero-Shot (ZS), Few-Shot (FS), Chain-of-Thought (CoT), Universal Self-Consistency (USC), and their respective similarity-optimized versions (FS-simi, CoT-simi, SC-simi). Notably, the results indicate that our method, referred to as RDC (Ours), consistently outperforms the baseline across nearly all models and datasets. For instance, in the SST2 dataset, the Qwen-72B model achieves a peak accuracy of 88.2% with RDC, surpassing other methods. This trend is similarly observed in the AG News and DBPedia datasets, where the RDC method demonstrates significant improvements, particularly in the larger models. This suggests that our approach effectively leverages collaborative reasoning to enhance annotation quality, allowing for a more nuanced understanding of the data and leading to better overall performance.\nFurthermore, the table highlights the variability in model performance based on the dataset and the specific task. For instance, while Qwen-14B and Qwen-7B show competitive results in certain categories, they generally lag behind the larger Qwen-72B and LLaMA3-70B models, indicating that model size and architecture play a crucial role in performance outcomes. The results also indicate that the similarity-based methods do not consistently outperform their non-similarity counterparts, suggesting that it may not enhance performance in some contexts. This variability underscores the complexity of NLP tasks, where different models may excel under different conditions. The average scores across datasets indicate that while models like Qwen-72B and LLaMA3-70B perform well, the introduction of the RDC method significantly boosts performance, particularly in challenging tasks. This finding emphasizes the importance of innovative approaches in enhancing model capabilities, as they can lead to substantial improvements in accuracy and reliability, ultimately contributing to more effective applications in real-world scenarios."}, {"title": "5. CONCLUSION", "content": "In this work, we present a rationale-driven LLM-based method named RDC for text annotation. RDC employs a series of iterative prompts to enable sequential collaborative data annotation across multiple rounds, demonstrating robust performance across four diverse annotation tasks by effectively leveraging the intermediate results generated by the LLM. The iterative nature of our approach mitigates the overhead associated with lengthy contexts, resulting in reduced computational expenses compared to previous studies. Although RDC requires multiple generation rounds and may incur greater overhead than traditional methods, we argue that the pursuit of high-quality annotated data is crucial for advancing AI. We are dedicated to continuously optimizing our methodology to enhance the annotation performance of LLMs, ultimately contributing to more efficient and effective data annotation processes."}]}