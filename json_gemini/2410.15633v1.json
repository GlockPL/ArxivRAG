{"title": "SELECTING INFLUENTIAL SAMPLES FOR LONG CONTEXT ALIGNMENT VIA HOMOLOGOUS MODELS' GUIDANCE AND CONTEXTUAL AWARENESS MEASUREMENT", "authors": ["Shuzheng Si", "Haozhe Zhao", "Gang Chen", "Yunshui Li", "Kangyang Luo", "Chuancheng Lv", "Kaikai An", "Fanchao Qi", "Baobao Chang", "Maosong Sun", "Tsinghua University", "Peking University", "DeepLang AI"], "abstract": "The expansion of large language models to effectively handle instructions with extremely long contexts has yet to be fully investigated. The primary obstacle lies in constructing a high-quality long instruction-following dataset devised for long context alignment. Existing studies have attempted to scale up the available data volume by synthesizing long instruction-following samples. However, indiscriminately increasing the quantity of data without a well-defined strategy for ensuring data quality may introduce low-quality samples and restrict the final performance. To bridge this gap, we aim to address the unique challenge of long-context alignment, i.e., modeling the long-range dependencies for handling instructions and lengthy input contexts. We propose GATEAU, a novel framework designed to identify the influential and high-quality samples enriched with long-range dependency relations by utilizing crafted Homologous Models' Guidance (HMG) and Contextual Awareness Measurement (CAM). Specifically, HMG attempts to measure the difficulty of generating corresponding responses due to the long-range dependencies, using the perplexity scores of the response from two homologous models with different context windows. Also, the role of CAM is to measure the difficulty of understanding the long input contexts due to long-range dependencies by evaluating whether the model's attention is focused on important segments. Built upon both proposed methods, we select the most challenging samples as the influential data to effectively frame the long-range dependencies, thereby achieving better performance of LLMs. Comprehensive experiments indicate that GATEAU effectively identifies samples enriched with long-range dependency relations and the model trained on these selected samples exhibits better instruction-following and long-context understanding capabilities.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) with large context windows have demonstrated impressive capabilities across a wide range of real-world tasks that involve extremely long contexts, such as long-document summarization and multi-document question answering . Recent works to build long-context LLMs mainly focus on broadening the context window via position encoding extension and continual pre-training on long text.\nDespite these advancements, few studies consider the alignment of long-context LLMs to leverage their capabilities in understanding long input contexts and following complex instructions. A primary obstacle lies in the difficulty of constructing a high-quality long instruction-following dataset for supervised fine-tuning (SFT). Annotating long instruction-following data tends to be much more challenging than short ones. Because it is non-trivial for annotators to understand an excessively long context and provide high-quality responses. For example, annotators might be tasked with writing a summary for a document containing more than 64k words based on the given instruction. To bypass this, construct the long instruction-following dataset by concatenating short instruction-following samples. Nonetheless, simply concatenating unrelated samples may not effectively simulate the long-range dependencies required for long-context tasks. For long-context tasks, modeling long-range dependencies is crucial,"}, {"title": "RELATED WORK", "content": "Long Context Alignment. Aligning the LLMs with instruction-following data can ensure they understand user instructions and give high-quality responses, which has been extensively studied in short context scenarios. However, excessively long contexts present unique challenges for long context alignment. construct the long instruction-following dataset by concatenating short instruction-following samples. Yet, simply concatenating unrelated sentences may not effectively simulate the long-range dependency relations for long-context tasks. Thus,"}, {"title": "METHODOLOGY", "content": "By synthesizing long instruction-following data, have effectively expanded the data volume for long context alignment. In this work, we aim to select influential samples from a vast ocean of synthetic data instead of indiscriminately increasing the quantity of data. Meanwhile, different from previous works that only consider the selection of short instruction-following samples, we attempt to address the unique challenge in long context alignment, i.e., the necessity for modeling long-range dependencies. Thus, we propose GATEAU to measure the richness of long-range dependency relations in long samples. As shown in Figure 1, GATEAU consists of Homologous Models' Guidance and Contextual Awareness Measurement, which separately measure the difficulty of generating corresponding responses and understanding long input contexts due to the long-range dependencies."}, {"title": "HOMOLOGOUS MODELS' GUIDANCE", "content": "Modeling long-range dependencies is essential for long context alignment. However, there is still no effective metric to directly quantify the richness of long-range dependency relations in data, which hinders the selection of influential data. Therefore, in this section, we attempt to approximately assess the richness of long-range dependency relations by measuring the difficulty in generating corresponding responses due to the long-range dependencies. If LLMs find it harder to generate target responses due to long-range dependencies, it means the sample has more complex and meaningful long-range dependency relations. An intuitive approach is to use the perplexity score to measure the difficulty of generating corresponding responses , as the score evaluates the extent to which the LLM's output aligns with the corresponding correct answer. For a given long instruction-following sample (c, x; y), the perplexity score of the given response y from LLMs \\(\\theta\\) is calculated as:\n\\begin{equation}\nPPL_{\\theta}(y|c, x) = Exp(-\\frac{1}{|y|}\\sum_{i=1}^{|y|}logP(y_i|c, x, y_{<i}; \\theta)),\n\\end{equation}\nwhere c means long input contexts and x means the given instruction. A higher \\(PPL(y|c, x)\\) indicates the harder the response of this long instruction-following data for LLM to generate.\nHowever, we argue that a higher \\(PPL(y|c, x)\\) does not mean the increased difficulty in generating corresponding responses is due to long-range dependencies. A higher \\(PPL(y|c, x)\\) might be attributed to certain limited capabilities of LLMs, such as the limited instruction-following capability for the model without alignment, instead of handling the long-range dependency relations in this sample is more challenging for the LLM. Therefore, to minimize the influence of other factors, we propose Homologous Models' Guidance (HMG). Specifically, we compare the perplexity scores of the response between two homologous models with different context windows to measure the difficulty due to the long-range dependencies. As homologous models share the same pre-training stage and model architecture (e.g., LLaMA-2-7B-base-4k and LLaMA-2-7B-base-64k , the only difference lies in their capabilities to model long-range dependency relations due to the extending context windows stage. Based on this motivation, we introduce the homologous models' perplexity score HMP(c, x; y):\n\\begin{equation}\nHMP(c, x; y) = Norm(PPL_{\\theta_A}(y|c, x)) - Norm(PPL_{\\theta_B} (y|c, x)).\n\\end{equation}\nWe compute the difference in normalized perplexity scores between two homologous models with different context windows as the metric. We apply softmax normalization to each score to determine its respective ranking among the datasets, since perplexity scores of one sample from different models often can't be directly compared. Model \\(\\theta_A\\) employs short context windows and \\(\\theta_B\\) is the model with long ones, e.g., LLaMA-2-7B-base-4k \\(\\theta_A\\) and LLaMA-2-7B-base-64k \\(\\theta_B\\). By introducing a model \\(\\theta_A\\) with weaker long-range dependencies modeling capability but other similar capabilities learned during the pre-training stage, we mitigate the influence brought by lacking other capabilities compared to simply using perplexity score as Eq. (1). Thus, the difference in perplexity scores is primarily attributed to the different abilities in modeling long-range dependencies between model \\(\\theta_A\\) and model \\(\\theta_B\\). In other words, Eq. (2) reflects the difficulty of generating the corresponding response caused by long-range dependencies. We use the drop from \\(PPL_{\\theta_A}\\) to \\(PPL_{\\theta_B}\\) in Eq. (2) because model \\(\\theta_A\\) tends to produce a high perplexity score due to its weak ability to model long-range dependencies. Thus, a higher HMP(c, x; y) indicates more difficulties for LLM in response generation due to the long-range dependencies, i.e., more long-range dependency relations in this sample."}, {"title": "CONTEXTUAL AWARENESS MEASUREMENT", "content": "Another challenge in long context alignment lies in enabling LLMs to understand and utilize the extremely long input contexts. Due to the long-range dependencies, it is hard for LLMs to utilize crucial information hidden in extremely long contexts, e.g., LLM's attention may focus on irrelevant content. Thus, we introduce Contextual Awareness Measurement (CAM) to evaluate whether LLMs' attention is appropriately focused on important segments within the long input contexts. Simply put, we attempt to evaluate the importance score of each segment and calculate the LLM's attention weights on each one, getting the Contextual Awareness Score (CAS) via calculating their similarity. For a given data (c, x; y), we divide the input contexts c into N segments \\([s_1, s_2, s_3, ..., s_N]\\) of equal length L. Specifically, for a given segment \\(s_i\\), we first compute the designed importance score \\(IS_{\\theta}(s_i)\\) and measure the significance of the segment in the response generation for LLM \\(\\theta\\):\n\\begin{equation}\nIS_{\\theta}(s_i) = Norm(Exp(-\\frac{1}{|y|}\\,\\sum_{j=1}^{|y|}logP(y_i|s_i, x, y_{<j}; \\theta))).\n\\end{equation}\nWe only keep the given segment \\(s_i\\) as input contexts to calculate the perplexity score of generating the response y, indicating the difficulty of generating the corresponding response y based on segment \\(s_i\\).\nWe apply softmax normalization to each score \\(Exp(-\\frac{1}{|y|}\\,\\sum_{j=1}^{|y|}logP(y_i|s_i, x, y_{<j}; \\theta))\\) to determine its respective ranking among the segments \\({s_i\\}_{i=1}^N\\). Thus, the higher \\(IS_{\\theta}(s_i)\\) suggests a greater difficulty for LLM \\(\\theta\\) to generate the response based on segment \\(s_i\\), implying that it is less important.\nOnce the importance scores of different segments are calculated, we then utilize the attention weights (i.e., the value of softmax(OK)) in the multi-head attention mechanism to measure how the LLM utilizes these segments. To achieve it, we use the averaged attention weights of tokens \\([t_1, ..., t_l]\\) in segments \\(s_i\\) as the score \\(Attn_{\\theta}(s_i)\\), which takes the form:\n\\begin{equation}\nAttn_{\\theta}(s_i) = Norm(\\sum_{j=1}^{L} Attn_{\\theta} (t_j|y; \\theta)),\n\\end{equation}\nwhere \\(Attn_{\\theta} (t_j|y; \\theta)\\) means the attention weights averaged across the tokens in targeted response y to the token \\(t_j\\) in segment \\(s_i\\). Meanwhile, we harness the attention weights averaged across different decoder layers and attention heads to thoroughly model how the LLM utilizes the long input contexts during the response generation. We apply softmax normalization to each score \\(\\sum_{j=1}^{L} Attn_{\\theta} (t_j|y; \\theta)\\) to determine its respective ranking among the segments \\({s_i\\}_{i=1}^N\\) to yield the score \\(Attn_{\\theta}(s_i)\\). In so doing, we can calculate the attention weights between the response and segments, indicating how segments are utilized during the response generation.\nFinally, we measure the difficulty of understanding the long input contexts due to long-range dependencies. For a given long instruction-following sample, we compute the CAS by resorting to the cosine similarity between importance scores \\([IS_{\\theta}(s_1), ..., IS_{\\theta}(s_N)]\\) and attention weights \\([Attn_{\\theta}(s_1), ..., Attn_{\\theta}(s_N)]\\), as follows:\n\\begin{equation}\nCAS(c, x; y) = CosSim([IS_{\\theta}(s_1), ..., IS_{\\theta}(s_N)], [Attn_{\\theta}(s_1), ..., Attn_{\\theta}(s_N)]).\n\\end{equation}\nBy doing this, we can measure the difficulty of understanding the long input contexts by checking whether LLMs' attention is focused on important segments. The insight is that should the LLM's attention focus more on less important segments, it suggests that the LLM struggles to accurately comprehend long input contexts. The higher CAS(c, x; y) indicates more difficulties in utilizing the long input contexts to generate corresponding responses due to the long-range dependencies, which also implies the more long-range dependency relations in this sample."}, {"title": "SELECTING AND TRAINING", "content": "In practice, we frame the overall metric by weighting and summing both designed metrics to rank the data (c, x; y), then select the most challenging samples as the influential samples, i.e.,\n\\begin{equation}\nScore(c, x; y) = \\alpha * Norm(HMP(c, x; y)) + (1 - \\alpha) * Norm(CAS(c, x; y)).\n\\end{equation}\n\\(\\alpha\\) is a hyperparameter. We tap softmax normalization to the HMP(c, x; y) and CAS(c, x; y) of the given data across the whole dataset. Inspired by active learning , when trained on these challenging data characterized by complex long-range dependency relations, LLMs could effectively model the long-range dependencies and achieve better long context alignment.\nLLMs are often fine-tuned with instruction-following data to learn to follow instructions. We aim to apply supervised fine-tuning on the selected data (e.g., selecting 10% samples of full datasets with top 10% scores according to Eq. (6)). Thus we train LLMs using the following objective function:\n\\begin{equation}\nL_{\\theta} (c, x; y) = - \\sum_{i=1}^{|y|}logP(y_i|c, x, y_{<i}; \\theta).\n\\end{equation}\nIt is similar to a language modeling loss, while only computing the loss associated with the response."}, {"title": "EXPERIMENT", "content": "In our experiments, we use LLaMA-2-7B-base-4k and LLaMA-2-7B-base-64k as homologous models to apply the proposed Homologous Models' Guidance. LLaMA-2-7B-base-4k is a well-known open-sourced LLM with a context window"}, {"title": "IMPACT OF GATEAU", "content": "Improving Instruction-Following Capabilities for Both Short and Long Inputs. The experimental results are presented in  and  for the LongBench-Chat and MT-Bench benchmarks in two settings. It shows our proposed method GATEAU can consistently improve LLMs' capabilities in following both long and short instructions and generating high-quality responses. Compared to indiscriminately using the whole dataset (Full-100%), using the selected subset of the long instruction-following dataset (GATEAU-LLaMA) can significantly improve the instruction-following capabilities, e.g., increasing 9% in LongBench-Chat and 6.5% in MT-Bench. Meanwhile, the low performance of w/o Long SFT in LongBench-Chat indicates that using long instruction-following data is important for the performance of LLMs in handling the instructions with long input contexts. The results also show that our method GATEAU achieves uniformly better performance in varying ratios of used long instruction-following samples compared with other baselines, indicating the effectiveness of our method. Compared with baselines focusing on short instruction-following samples (CaR and Cherry Selection), GATEAU can identify samples enriched with long-range dependency relations more effectively and help LLMs to achieve better overall performance. Also, we observe that the selection of long instruction-following samples aids in augmenting the instruction-following capabilities for short inputs. We conjecture that handling complex tasks (i.e., long input contexts) contributes to handling the easy ones (i.e., short input contexts).\nEnhancing the Long-Context Understanding Capabilities. The experimental results are shown in  and  for the LongBench benchmark. Our methods achieve consistent and remarkable performance gains in both different settings and evaluations. We show the improved scores (\\(\\Delta\\) compared to Full-100%) compared to indiscriminately using the whole dataset (Full-100%), indicating that GATEAU helps LLM to better understand and utilize the long input contexts. Further,"}, {"title": "DISCUSSION", "content": "Needle in the Haystack Test. We conduct the Needle in A HayStack\u201d experiment (result visualization in Figure 2) to test the model's ability to utilize information from 10 different positions within long contexts of varying lengths between 1k-60k. Specifically, this task asks for the model to retrieve a piece of fact (the 'needle') that is inserted in the middle (positioned at a specified depth percent) of a long context window (the 'haystack'). These results show that GATEAU can help LLM's ability to utilize information from different positions within long texts, resulting in a decrease in the model's retrieval error.\nAblation Study. To evaluate the effectiveness of two designed metrics, including Homologous Models' Guidance and Contextual Awareness Measurement, we conduct the ablation study in ."}, {"title": "GENERAL CHARACTERISTICS OF SELECTED SAMPLES", "content": "We delve into whether the selected samples based on our method align with known characteristics of high-quality training data as shown in Figure 3. To this end, we select 100 samples with the top 1% scores and 100 samples with the least 1% scores."}, {"title": "FURTHER EXPLORATION", "content": "General Characteristics of Selected Samples from GATEAU\nUtilizing GPT-4, we evaluate each sample on five aspects: the coherence of long input contexts, the necessity of long input contexts, helpfulness of response, the faithfulness of response, and the complexity of instruction. Different from the previous GPT-4 evaluation detailed in the Appendix C.5, we use GPT-4-Turbo API (now points to GPT-4-Turbo-2024-04-09) as our evaluator, as this version of API has larger context window to conduct the more correct evaluation for our long input contexts. The prompt for GPT-4 evaluation on the coherence of long input contexts is:"}, {"title": "FURTHER EXPLORATION OF HOMOLOGOUS MODEL'S GUIDANCE", "content": "We further explore some key questions in the Homologous Model's Guidance.\nWhy Do We Need Homologous Models? Homologous Model's Guidance (HMG) aims to assess the degree of long-range dependencies required for the corresponding response generation, by comparing the perplexity scores of the response between two homologous models with different context windows. The idea behind HMG is that the primary difference between homologous models with varying context windows lies in their different capabilities for modeling long-range dependencies instead of other capabilities. Thus, the disparity in the perplexity scores can be interpreted as reflecting the difference in the long-range dependencies modeling capabilities required to generate the given response. To evaluate the effectiveness of our idea, we replace LLaMA-2-7B-base-4k with Qwen-2-7b-base-8k as model \\(\\theta_A\\) in Eq. (2), namely Non-Homologous Model's Guidance. As shown in , we find Non-Homologous Model's Guidance achieve worse performance than Homologous Model's Guidance in two designed settings. It shows that HMG can exclusively measure the richness of long-range dependency relations in long SFT samples. As non-homologous models have different pre-training phases and model architectures, the modified Eq. (2) can not effectively measure the degree of long-range dependencies required for response generation and introduce the influence brought by other different capabilities of non-homologous models, resulting in the worse performance.\nWhy Do We Apply Normalization in Eq. (2)? We apply softmax normalization to each score in Eq. (2) to determine its respective ranking among the datasets for two perplexity scores. This is because our early experiments observed that applying softmax normalization can further improve the performance shown in . This may due to the fact that some extremely noisy samples tend to have large perplexity scores, which in turn lead to unstable HMP scores if we do not apply normalization in Eq. (2). Training LLMs on these noisy samples further leads to poor results."}]}