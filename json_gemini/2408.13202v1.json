{"title": "Instruct-DeBERTa: A Hybrid Approach for Aspect-based Sentiment Analysis on Textual Reviews", "authors": ["Dineth Jayakody", "A V A Malkith", "Koshila Isuranda", "Vishal Thenuwara", "Nisansa de Silva", "Sachintha Rajith Ponnamperuma", "G G N Sandamali", "K L K Sudheera"], "abstract": "Aspect-based Sentiment Analysis (ABSA) is a critical task in Natural Language Processing (NLP) that focuses on extracting sentiments related to specific aspects within a text, offering deep insights into customer opinions. Traditional sentiment analysis methods, while useful for determining overall sentiment, often miss the implicit opinions about particular product or service features. This paper presents a comprehensive review of the evolution of ABSA methodologies, from lexicon-based approaches to machine learning and deep learning techniques. We emphasize the recent advancements in Transformer-based models, particularly Bidirectional Encoder Representations from Transformers (BERT) and its variants, which have set new benchmarks in ABSA tasks. We focused on finetuning Llama and Mistral models, building hybrid models using the SetFit framework, and developing our own model by exploiting the strengths of state-of-the-art (SOTA) Transformer-based models for aspect term extraction (ATE) and aspect sentiment classification (ASC). Our hybrid model Instruct DeBERTa uses SOTA InstructABSA for aspect extraction and DeBERTa-V3-baseabsa-V1 for aspect sentiment classification. We utilize datasets from different domains to evaluate our model's performance. Our experiments indicate that the proposed hybrid model significantly improves the accuracy and reliability of sentiment analysis across all experimented domains. As per our findings, our hybrid model Instruct DeBERTa is the best-performing model for the joint task of ATE and ASC for both SemEval restaurant 2014 and SemEval laptop 2014 datasets separately. By addressing the limitations of existing methodologies, our approach provides a robust solution for understanding detailed consumer feedback, thus offering valuable insights for businesses aiming to enhance customer satisfaction and product development.", "sections": [{"title": "I. INTRODUCTION", "content": "Aspect-Based Sentiment Analysis (ABSA) has become an essential technique in Natural Language Processing (NLP) for extracting fine-grained opinions from textual data. It focuses on identifying sentiment towards specific aspects within a text, providing a detailed understanding of customer feedback and reviews. Traditional sentiment analysis techniques, while effective at determining overall sentiment, often fail to capture the nuanced opinions that consumers express about particular features or attributes of a product or service.\nOver the years, ABSA methodologies have evolved significantly. Early approaches primarily relied on lexicon-based methods, which used predefined dictionaries of sentiment-laden words to infer polarity. These methods, however, struggled with context and ambiguity. The advent of machine learning introduced more sophisticated techniques, including supervised learning models that could be trained on annotated datasets. Despite their advancements, these models required substantial manual effort for feature extraction and were often domain-specific.\nThe breakthrough in deep learning, particularly with the development of recurrent neural networks (RNNs), Long Short-Term Memory networks (LSTMs), and Convolutional Neural Networks (CNNs), marked a significant improvement in sentiment analysis. These models could automatically learn features from text, capturing context and sequential dependencies more effectively than traditional methods. LSTM and CNN-based models became popular for their ability to handle long-range dependencies and local features, respectively. However, these models still had limitations, especially in understanding long-term dependencies and complex syntactic structures.\nThe introduction of Transformer architectures, especially BERT, revolutionized the field by leveraging attention mechanisms to capture contextual relationships in both directions of a sentence. BERT and its variants, such as ROBERTA and DeBERTa, have set new benchmarks in various NLP tasks, including ABSA. These models have demonstrated superior performance in aspect extraction and sentiment classification tasks due to their ability to understand complex language patterns and relationships.\nIn our study, we focus on developing a hybrid model that leverages the strengths of the latest Transformer-based models for ABSA. We aim to address the limitations of existing approaches by combining aspect extraction and sentiment classification into a unified framework. Our approach utilizes datasets from the hospitality domain, including SemEval 2014 (Res-14), 2015 (Res-15), and 2016 (Res-16) restaurant reviews, and extends to the laptop domain with the SemEval 2014 laptop dataset (Lap-14). By evaluating these models on imbalanced datasets using the F1 metric, we ensure a balanced and comprehensive assessment of performance.\nThrough our literature review, we have identified key methodologies and their respective accuracies, guiding the design of our hybrid model. We focus on models that excel in aspect term extraction (ATE) and aspect sentiment classification (ASC), aiming to develop a model that builds on the successes of past methodologies while innovating in areas where existing methods may fall short. Our goal is to enhance the accuracy and reliability of sentiment analysis in our application domain, ultimately providing a robust solution for understanding consumer feedback."}, {"title": "II. LITERATURE REVIEW", "content": "Our literature review systematically investigates various models that have demonstrated efficacy in ATE and ASC.\n1) LSTM Based Models: Standard RNNs suffer from significant limitations, primarily the vanishing gradient and exploding gradient problems. To address these limitations, the LSTM network was developed by Hochreiter and Schmidhuber [1]. To effectively utilize aspect information, Wang et al. [2] proposed a model called LSTM with Aspect Embedding (AE-LSTM). However, to further leverage aspect information, Wang et al. [2] developed an enhanced model called Attention-based LSTM with Aspect Embedding (ATAE-LSTM).\nLi et al. [3] proposed Target-Specific Transformation Networks (TNet), a new architecture designed to improve target sentiment classification by effectively handling multiple targets and extracting relevant features without introducing noise. TNet introduces a novel Target-Specific Transformation (TST) [3] component for generating target-specific word representations. The models LSTM-FC-CNN-LF and LSTM-FC-CNN-AS were built by Li et al. [3] that incorporate a fully connected layer and context-preserving mechanisms. These models performed better with F1 scores of 70.60% and 70.23% for LSTM-FC-CNN-LF, 70.72% and 70.06% for LSTM-FC-CNN-AS for Lap-14 and Res-14 respectively.\n2) GloVe Based Models: Glove (Global Vectors for Word Representation) is an algorithm that generates word embeddings by aggregating word co-occurrence statistics from a corpus. It is important because it captures both local and global statistical information of words, enhancing the performance of natural language processing tasks. ASGCN introduced by Zhang et al. [4] proposed a novel aspect-specific sentiment classification framework while DualGCN by Li et al. [5] proposed a dual graph convolutional network model that considers the complementarity of syntax structures and semantic correlations simultaneously. Zhong et al. [6] proposed a new model named KGAN which uses a knowledge graph augmented network, which aims to effectively incorporate external knowledge with explicitly syntactic and contextual information. While all these models individually had their own importance, merging them with Glove as an embedding method, researchers were able to capture both local and global information to achieve higher F1 Scores like 84.46% with Res-14. However, UIKA (Unified Instance and Knowledge Alignment Pretraining) by Liu et al. [7], which introduces a unified alignment pretraining framework into the vanilla pretrain-finetune pipeline, incorporates both instance and knowledge level alignments. It reported a higher F1 score of 85.53% with KGAN for Res-14. Furthermore, KGAN+UIKA achieved higher F1 scores with BERT compared to GloVE.\n3) BERT Based Models: The original BERT model was introduced by Devlin et al. [8]. BERT's training process involves two main steps: pre-training and fine-tuning. BERT-DK, introduced by Zhao [9], integrates domain-specific knowledge to improve ABSA performance. By incorporating domain-specific information, BERT-DK achieved F1 scores of 77.02% and 83.55% for aspect extraction on the Res-14 and Lap-14 datasets respectively. Similarly, BERT-SPC, developed by Song et al. [10], employs a Sentence Pair Classification framework to better understand the context of aspect-specific sentences.\nInnovative approaches such as BERT-MRC, proposed by Zhao et al. [11], frame ABSA tasks as machine reading comprehension problems while Xu et al. [12] introduced BERT-PT which involves pre-training BERT on domain-specific data followed by fine-tuning. BAT which stands for BERT with Adversarial Training, introduced by Karimi et al. [13], enhances ABSA by generating adversarial examples during training.\nCutting-edge models like RGAT-BERT, DualGCN-BERT, TF-BERT, and dotGCN-BERT have further improved ABSA performance. RGAT-BERT, proposed by Bai et al. [14], uses relational graph attention networks to improve aspect extraction and sentiment classification abilities. In addition to that DualGCN-BERT introduced by Li et al. [5], uses dual graph convolutional networks to handle both aspect extraction and sentiment classification. TF-BERT, developed by Zhang et al. [15], uses task-specific fine-tuning strategies to improve ABSA performance. In contrast to that DotGCN-BERT, proposed by Chen et al. [16], uses dot-product based graph convolutional networks to improve ABSA performance. Furthermore keeping another step forward DualGCN and KGAN have used BERT as the embedding metholody in order to achieve higher F1 scores.\n4) ROBERTa Based Models: ROBERTa [17] is an advanced language model that builds upon the foundational work of BERT. The SARL-ROBERTa model, which was introduced by Wang et al. [18] used span-based dependency modeling to align opinion candidates with aspects and used an adversarial learning strategy to reduce sentiment bias in aspect embeddings. Among the compared ROBERTa based models, SARL-ROBERTa performs the best, achieving a F1-score of 82.44% and 82.97% for Res-14 and Lap-14 respectively.\nHowever, models such as ASGCN-ROBERTA, RGAT-ROBERTA, PWCN-ROBERTa, and ROBERTa+MLP benefited by combining ROBERTa with various specialized architectures, as demonstrated by Dai et al. [19]. Strong performance is achieved by ASGCN-ROBERTa, which combines an aspect-specific graph convolutional network with dependency tree syntactic information. With a relational graph attention network integrated to collect relational information between words, RGAT-ROBERTa performs admirably. ROBERTa+MLP integrates a multi-layer perceptron with ROBERTa. It highlights the flexibility of combining ROBERTa's embeddings with simple classifiers.\nTask-oriented syntactic information is well captured by pure ROBERTa based models, particularly by the fine-tuned variations (FT-ROBERTa). Research conducted by Dai et al. [19] demonstrates that FT-ROBERTa achieves a 1.56% improvement in the F1-score over standard ROBERTa induced trees, and performs better than parser-provided trees.\n5) DeBERTa Based Models: DeBERTa [20] introduces a disentangled attention mechanism, which utilizes two separate vectors for each word to represent its content and position independently. Also, the model incorporates an enhanced mask decoder in its pre-training phase based on masked language modeling (MLM).\nImproving from the vanilla DeBERTa model a new model named DEBERTaV3 was introduced by He et al. [21]. By further fine tuning the model to improve its performance Yang et al. [22] developed the DeBERTAV3-base-absa-V1 model. This was trained using Lap-14, Res-14, Res-16, and six more datasets counting up to 30k+ ABSA examples. The accuracy of this model showed an improvement of 9.35% and 10.87% for the ASC task of Res-14 and Lap-14 datasets respectively compared to the original DeBERTAV3 model.\nIn their independent investigations, Marcacini and Silva [23] as well as Yang and Li [24] explored the utilization of DeBERTa based models, introducing ABSA-DEBERTa and LSA-X-DeBERTa, respectively. Marcacini and Silva [23] explored disentangled learning as a method to improve BERT-based representations specifically for ABSA. On the other hand, Yang and Li [24] introduced a novel perspective in ASC by emphasizing the significance of aspect sentiment coherency. Their study revealed that neighbouring aspects usually share similar sentiments, which is known as \u201caspect sentiment coherency.\u201d To address this, they proposed a local sentiment aggregation paradigm (LSA) to effectively model fine-grained sentiment coherency. In respect to that, the LSA-X-DEBERTa model introduced by Yang and Li [24] achieved a F1-score of 87.02% for Res-14 and 84.41% for Lap-14 under the sentiment classification task.\n6) Other Models: Concerning ASC and ATE, in particular, LCF-ATEPC-CDM proposed by Yang et al. [25] and InstructABSA proposed by Scaria et al. [26] standout for their strong performances. InstructABSA, utilizing a novel instruction learning paradigm, showed exceptional abilities in obtaining pertinent aspects from the text, attaining F1 scores exceeding 92% for aspect extraction on both the Res-14 and Lap-14 datasets. Concerning ATE, LCF-ATEPC-CDM, which also employs a local context focus technique, performs fairly well. In sentiment polarity classification, InstructABSA also excels with F1 scores of 85.17% on Res-14 and 81.56% on Lap-14, outperforming many other models. The LSAT model proposed by Yang and Li [27], with its focus on aspect sentiment coherency through a local sentiment aggregation paradigm, shows impressive results, achieving a F1 score of 90.86% on Res-14. The efficacy of the BART-ABSA model in a comprehensive approach to ABSA has also been demonstrated by Yan et al. [28], which combines all ABSA subtasks into a single generative formulation."}, {"title": "B. Joint Task Models", "content": "Here we talk about the models that perform the joint task which is the ATE and ASC tasks together by a single model. The model is only fed in with the relevant sentences or the reviews. Then the model identifies the aspects by itself and classifies the polarities to the aspects that have been identified. Then the F1 score of the whole process is reported.\nThe InstructABSA and Grace models, which were previously described as single task models capable of performing both ATE and ASC tasks separately, also excel in the joint task. These models report the highest F1 scores for the joint task, achieving over 75% for both datasets, indicating their accuracy and robustness across different domains.\nRACL-BERT, introduced by Chen and Qian [29], is a notable ABSA (Aspect-Based Sentiment Analysis) model that utilizes the BERT-Large model to address three subtasks simultaneously: identifying aspects, detecting sentiment words, and classifying overall sentiment. Through multitasking and relation propagation, RACL-BERT enhances sentiment analysis accuracy. Similarly, SPAN, introduced by Hu et al. [30], employs a novel approach by focusing on key opinion points rather than tagging each word. Both RACL-BERT and SPAN achieved reasonable F1 scores but were outperformed by InstructABSA and GRACE.\nThe E2E-TBSA model, proposed by Li et al. [31], addresses both ATE and ASC tasks in a single step, using a collapsed approach that combines these tasks into a unified process. Similarly, BERT-E2E-ABSA, introduced by Li et al. [32], is based on BERT models and follows the same principles. These models achieved F1 scores in the 60%-70% range but did not outperform the leading models.\nDOER, introduced by Luo et al. [33], uses a cross-shared RNN framework to generate aspect term-polarity pairs simultaneously. IMN, introduced by He et al. [34], employs an interactive architecture with multi-task learning for end-to-end ABSA tasks, including aspect term and opinion term extraction as well as aspect-level sentiment classification."}, {"title": "C. Selection Criteria: Dataset", "content": "After a thorough review, the following criteria were established for dataset selection:\n\u2022\tRelevance to BSA: The datasets must be specifically designed for or widely used in aspect-based sentiment analysis ensuring granularity.\n\u2022\tDiversity of aspects and sentiments: The selected datasets should cover a wide range of aspects and sentiments ensuring generalizability.\n\u2022\tQuality of annotations: High-quality, manually annotated datasets are preferred to ensure the accuracy.\n\u2022\tAvailability and accessibility: Publicly available datasets with accessibility are chosen to facilitate reproducibility.\nBased on these criteria, the SemEval datasets from the years 2014, 2015, and 2016 were selected."}, {"title": "III. METHODOLOGY", "content": "In this section, we look on to different approaches we tested out in order to find the most accurate and robust solution. These approaches can be listed below,\n1) Fine-Tuning LLAMA 2-7B with Quantized Low Rank Adaptation (QLORA)\n2) Fine-Tuning Mistral-7B with Quantized Low Rank Adaptation (QLORA)\n3) ASGCN+UIKA+Glove for Sentiment Polarity\n4) SSGCN+Glove for Sentiment Polarity\n5) Span-ASTE+BERT for Aspect Extraction\n6) SETFIT for efficient few-shot fine-tuning of Sentence Transformers\n7) Instruct-DeBERTA (Proposed Model)"}, {"title": "A. LLAMA 2-7B with QLORA", "content": "Given the current state-of-the-art interest in Large Language Models (LLMs), we opted to include an LLM-based analysis in our comparative study. LLAMA 2 is a collection of second-generation open-source LLMs from Meta that comes with a commercial license. Roumeliotis et al. [56] presented that LLAMA 2 shows a significant leap forward in natural language understanding and generation, by its advanced architecture, large training data, and refined training strategies. The architecture of LLAMA 2 is based on the transformer model, a neural network architecture that has proven highly effective in a wide range of NLP tasks. LLAMA 2 employs a multi-layered transformer architecture with self-attention mechanisms. It is designed to handle a wide range of natural language processing tasks, with models ranging in scale from 7 billion to 70 billion parameters.\nFine-tuning in machine learning is the process of adjusting the weights and parameters of a pre-trained model on new data to improve its performance on a specific task. There are three main fine-tuning methods in the context:\n1) Instruction Fine-Tuning (IFT): According to Peng et al. [57], IFT involves training the model using prompt completion pairs, showing desired responses to queries.\n2) Full Fine Tuning: Full fine-tuning involves updating all of the weights in a pre-trained model during training on a new dataset, allowing the model to adapt to a specific task.\n3) Parameter-Efficient Fine-Tuning (PEFT): Selectively updates a small set of parameters, making memory requirements more manageable. There are various ways of achieving Parameter efficient fine-tuning. Low-Rank Parameter (LoRA) [58] and Quantized Low-Ranking Adaptation (QLORA) [59] are the most widely used and effective.\nTraditional fine-tuning of pre-trained language models (PLMs) requires updating all of the model's parameters, which is computationally expensive and requires massive amounts of data; thus making it challenging to attempt on consumer hardware due to inadequate VRAMs and computing. However, Parameter-Efficient Fine-Tuning (PEFT) works by only updating a small subset of the model's most influential parameters, making it much more efficient. Four-bit quantization via QLORA allows such efficient fine-tuning of huge LLM models on consumer hardware while retaining high performance. QLORA quantizes a pre-trained language model to four bits and freezes the parameters. A small number of trainable Low-Rank Adapter layers are then added to the model. In our case, we created a 4-bit quantization with NF4-type configuration using BitsAndBytes.\nAccording to Dettmers et al. [59] under the model fine-tuning process, Supervised fine-tuning (SFT) is a key step in Reinforcement Learning from Human Feedback (RLHF). The SFT models come with tools to train language models using reinforcement learning, starting with supervised fine-tuning, then reward modelling, and finally, Proximal Policy Optimization (PPO). During this process, we provided the SFT trainer with the model, dataset, LoRA configuration, tokenizer, and training parameters. The model was fine-tuned with a training and evaluation batch size of 4 and for 2 epochs, optimizing its ability to extract aspects and determine sentiment polarity.\nTo test the fine-tuned model, we used the Transformers text generation pipeline including the prompt. The LLAMA 2 model was fine-tuned using techniques such as QLoRA, PEFT, and SFT to overcome memory and computational limitations."}, {"title": "B. Mistral-7B with QLORA", "content": "Jiang et al. [60] introduced Mistral 7B, a 7-billion-parameter language model designed for superior performance and efficiency. Mistral 7B surpasses the best open 13B model (Llama 2) across all evaluated benchmarks and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation.\nMistral 7B leverages grouped-query attention (GQA) and sliding window attention (SWA). GQA significantly accelerates inference speed and reduces memory requirements during decoding, allowing for higher batch sizes and, consequently, higher throughput\u2014crucial for real-time applications. Additionally, SWA is designed to handle longer sequences more effectively at a reduced computational cost, addressing a common limitation in LLMs. These attention mechanisms collectively enhance the performance and efficiency of Mistral 7B.\nThe Mistral 7B model was fine-tuned to perform ABSA on the Lap-14 and Res-14 datasets. We have fine-tuned the base model separately for these two datasets and evaluated them separately. This involved customizing the model to better understand and analyze sentiment related to specific aspects within the review texts. The Mistral 7B model was selected from the Hugging Face hub. The mentioned datasets were processed using Pandas to ensure it was in a prompt-compatible format.\nTo enable efficient training, 4-bit precision loading was configured using BitsAndBytesConfig. We set float16 as the data type for the 4-bit base model, nf4 as quantization type, and nested quantization was disabled to simplify the training process. The tokenizer was loaded and configured to handle padding appropriately. The base model was then loaded with the quantization configuration, ensuring it was prepared for low-bit precision training.\nIn our fine-tuned models we set Attention dimension to 64, Scaling parameter to 64, and Dropout probability: 0.1 for efficient Low-Rank Adaptation (LoRA) parameters. Then the fine-tuning was conducted using the SFTTrainer with the defined training arguments. The dataset was loaded, and the model underwent supervised fine-tuning, adjusting to the specific requirements of ABSA on the mentioned datasets. As the final stage the post-training, the model was saved and reloaded in FP16 precision. The LoRA weights were merged back into the base model to create a final, streamlined version suitable for deployment. We fine-tuned this model using a batch size of 4 for both training and evaluation over 2 epochs, enhancing its capability to extract aspects and identify sentiment polarity.\nTo test the fine-tuned model, we developed a function to process user input and generate corresponding aspects and sentiments. The function takes an input sentence and utilizes a text generation pipeline where we set the prompt and specific parameters for sampling.\nBy utilizing Hugging Face libraries such as transformers, accelerate, peft, tr15, and bitsandbytes, we were able to successfully fine-tune and evaluate the both 7B parameter LLAMA 2 model and Mistral model on a consumer GPU."}, {"title": "C. SetFit", "content": "Few-shot learning has become increasingly essential in addressing label-scarce scenarios, where data annotation is often time-consuming and expensive. These methods aim to adapt pre-trained language models (PLMs) to specific downstream tasks using only a limited number of labelled training examples. One of the primary obstacles is the reliance on large-scale language models, which typically contain billions of parameters, demanding substantial computational resources and specialized infrastructure. Moreover, these methods frequently require manual crafting of prompts, introducing variability and complexity in the training process, thus restricting accessibility for researchers and practitioners.\nIn response to this, Tunstall et al. [61] proposed SETFIT (Sentence Transformer Fine-tuning) which presents an innovative framework for efficient and prompt-free few-shot fine-tuning of Sentence Transformers (ST). Diverging from existing methods, SETFIT does not necessitate manually crafted prompts and achieves high accuracy with significantly fewer parameters. Through a straightforward yet effective approach, SETFIT simplifies the few-shot learning process, making it more accessible and practical across various applications.\nSETFIT is a novel framework for efficient few-shot fine-tuning of ST without the need for manual prompts. The SETFIT approach consists of two main steps:\n1) Fine-tuning a pretrained ST on a small number of text pairs in a contrastive Siamese manner.\n2) Training a classification head using the resulting ST to generate rich text embeddings.\nIn the first step, the ST is fine-tuned using a contrastive loss function, which encourages the model to learn discriminative representations of similar and dissimilar text pairs. In the second step, a simple classification head is trained on top of the fine-tuned ST to perform downstream tasks such as text classification or similarity ranking. By decoupling the fine-tuning and classification steps, SETFIT achieves high accuracy with orders of magnitude fewer parameters than existing methods, making it computationally efficient and scalable."}, {"title": "D. Instruct-DeBERTa (Proposed Model)", "content": "In this study, we developed an aspect-based sentiment analysis pipeline utilizing transformer-based models to automatically extract aspects and analyze sentiments in textual data. The pipeline is composed of two primary stages: aspect extraction and sentiment classification. For these two stages, we utilized the best models for each task that we found through our thorough literature review which is being summarized in Table II. When looking on to the analysis it is clear that InstructABSA [26] performs the best in all the analyzed datasets irrespective of the domain. For the Res-14 dataset, it recorded an F1 score of 92.10% which was higher than all the other reported models. It still remained the highest on the Res-15 data set and was only 1.67% less than the highest recorded accuracy under the Res-16 dataset. But P-SUM [41] which reported the highest F1 score for the Res-16 dataset performed significantly less than InstructABSA in the previous datasets. Hence InstructABSA still remained the best option for aspect extraction. Moreover, for the Lap-14 dataset, InstructABSA topped the charts again showcasing the models' adaptability and robustness regardless of the relevant domain. Hence InstructABSA was selected as the best performing model for aspect extraction. When looking at the performances on the sentiment polarity task DeBERTa-V3-baseabsa-V1 [22, 40] is the best overall performing model across all the datasets. It showcases an accuracy of 90.94% for the Res-14 dataset which is the highest overall. It shows the same promising results in the Res-15 and Res-16 datasets. In addition to that DeBERTa-V3-baseabsa-V1 also shows the adaptability of the model by recording the highest accuracy for the Lap-14 dataset as well which falls in to a complete different domain.\nSince we identified the best overall performing models for individual tasks of aspect extraction and sentiment polarity detection, we tried to exploit the performances of these models and build up a hybrid model that performs the joint task of aspect extraction and sentiment polarity detection by itself. Then we created a novel hybrid model utilizing these models to build up a model which as per our research gives the best performance as a pipelined hybrid model, which in fact makes this the SOTA model for the pipelined aspect extraction and sentiment polarity classification task also known as the joint task for ABSA.\nFigure 1 shows the proposed model of our study. The model structure used for ATE is InstructABSA while the model used for ASC is DeBERTA-V3-baseabsa-V1. The collective model is named as Instruct-DeBERTa, which stands for InstructABSA for aspect term extraction and DeBERTa-V3-baseabsa-V1 for aspect sentiment classification. When looking on to Figure 1 it shows how these two independent models are being pipelined to build up a single joint task model.\nThe algorithm for our proposed model can be presented as below for further clarification."}, {"title": "IV. RESULTS", "content": "Table IV presents the F1 scores for the models being built and evaluated by ourselves. In addition to that we have presented Figure 4 and Figure 5 which give a view on the robustness of each model for the aspect term extraction task and the sentiment polarity task separately based on the accuracies. According to Figure 4 and Figure 5, if the model shows high accuracy for each task and the accuracies for the two domains do not exhibit drastic deviations, then the relevant model will be selected."}, {"title": "A. LLAMA 2-7B with QLoRA", "content": "The first section of Table IV shows the performance of Llama-2-7b [62] with QLoRA [59]. It emphasizes that this model shows notable performance in both aspect extraction and sentiment polarity tasks. On Res-14 and Lap-14 datasets, Llama 2 shows a slight edge in aspect extraction compared to sentiment polarity. These performances were obtained using the L4 GPU emphasizing the model's efficiency and effectiveness in computational performance."}, {"title": "B. Mistral 7B with QLORA", "content": "According to the values Table IV, the model Mistral 7B exhibits superior performance in both aspect extraction and sentiment polarity tasks compared to the Llama 2 model. Specifically, Mistral 7B achieves higher F1 scores across both Res-14 and Lap-14 datasets, indicating its greater capability in accurately identifying aspects and determining sentiment polarity within the text. These results were achieved using an L4 GPU, similar to the Llama 2 model.\nWhen comparing the two models, Mistral 7B demonstrates a clear advantage in both tasks. While Llama 2 performs competently, Mistral 7B consistently outperforms it, showcasing its enhanced effectiveness and reliability in handling aspect extraction and sentiment polarity analysis. This comparison highlights Mistral 7B's performance, making it a more capable model for these specific natural language processing tasks."}, {"title": "C. Some models with BERT and GloVe", "content": "As a part of the comparative study for the survey, we conducted experiments using several advanced models for aspect-based sentiment analysis. We experimented three main models: SSGCN+Glove, ASGCN+UIKA+Glove[4], and Span-ASTE+BERT [50] both in local and Colab environments. The focus was on to evaluating their performance on the SemEval 2014 dataset. These results are stated in Table IV. But still we observed that Instruct-DeBERTa outperforms all."}, {"title": "D. SetFit", "content": "In the second section of Table IV, we provide a comprehensive overview of the Fl-scores for attained by various sentence models using the SETFIT framework [61] for the same amount of selected data in the respective stated data sets. If a model is reported in a single row, it means we have used the said sentence transformer model for both aspect extraction and sentiment polarity identification (eg, BGE [63]). In the cell blocks where a model is followed by other models with + are combinations. For example, the first row of Paraphrase-MiniLM-L6-v2 [64, 65] contains results of that model being used both for aspect extraction and sentiment polarity identification. The subsequent line with +MpNet [66] indicates that Paraphrase-MiniLM-L6-v2 was used for the aspect extraction component and MpNet was used for the sentiment polarity identification component.\nAt this point, a question may be raised as to why would the aspect extraction have two different values for accuracy in Figure 4 and Figure 5 in the two configurations if in both cases the same model (ie, Paraphrase-MiniLM-L6-v2 in this example) was used for that task. The reason is the fact that the fine-tuning is conducted end-to-end in a holistic manner and thus, the choice of the model used for the sentiment polarity identification ends up influencing the ultimate accuracy obtained by the aspect extraction component. It may enhance the result as in the case of Paraphrase-MiniLM-L6-v2 and MpNet. It may also hinder as in the case of LaBSE.\nOverall, it can be noted that LaBSE [67] consistently emerges as a standout performer; either by itself or as the aspect extraction component of a pair. It can be argued that this robust performance is owed to its capability to capture nuanced complex information crucial for understanding both aspect-based sentiment analysis and sentiment polarity classification tasks. Specifically on the sentiment polarity classification task, it can be noted that Mpnet and ROBERTA-STSb-v2 [68] elevates performance multiple configurations."}, {"title": "E. Instruct-DeBERTa (Proposed Model)", "content": "From all the evaluated models, our model performs the best with the highest accuracies and F1 scores. It is also robust for both domains which makes it the best performing model. As discussed in the methodology we selected the best performing models from Table II to create our own hybrid model. When looking at Table IV, Figure 4 and Figure 5 it is clear that Instruct-DeBERTa outperforms our finetuned Llama, Mistral, and all the Setfit based models.\nTable V shows how the two best models we selected for each subtask perform individually on their relevant task. These F1 scores are for the combined task, which means our model is capable of performing both the aspect extraction and the sentiment polarity tasks. For the first task which is extracting the aspects, our model gives closer accuracies for what has been reported by Scaria et al. [26", "40": "which is specialized only for detecting polarities gives slightly higher accuracies than our hybrid model. As seen for the Res-14 dataset the sentiment polarity accuracy for the individual task by DeBERTA-V3-base-absa-v1 is reported as 90.94% while our reported 88.63%. This is due to the models being pipelined and the extracted aspects from the first model is being fed to the second model rather than calculating the accuracies separately for individual tasks. Hence the slight deduction in the hybrid model is justified.\nSo, looking on to all the past models in Table II and the models that we worked on in Table IV, it is clear that our model, Instruct-DeBERTa is the best performing hybrid model designed for the combined task of aspect extraction and sentiment polarity detection. Moreover, our hybrid model shows promising results in the laptop domain as well. Our model gives an F1-score of 91.56% and 89.65% for aspect extraction and sentiment polarity respectively.\nAlso Table III, in the literature review lists out the joint models which are equivalent to the model we built. These perform the joint task of ABSA. Here in order for the F1 score to be counted the aspect and the respective sentiment in the original dataset needs to be correct. The F1 scores of these models along with our model can be visualized in Figure 2 and Figure 3. It is clear that our model clearly out performs the currently available joint task hybrid models. It gives a pair extraction F1 score of 80.78% and 80.94% which exceed the current reported highest accuracy for the rest-14 and lap-14 datasets. From Table III, Figure 2 and Figure 3 it is clear that our model is the best performing joint task model. Our model outperforms all other hybrid models in both domains which again proves that it is not only accurate but also robust to different domains as well"}]}