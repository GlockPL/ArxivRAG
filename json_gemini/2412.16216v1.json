{"title": "GraphLoRA: Empowering LLMs Fine-Tuning via Graph Collaboration of MoE", "authors": ["Ting Bai", "Yue Yu", "Le Huang", "Zenan Xu", "Zhe Zhao", "Chuan Shi"], "abstract": "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning method that has been widely adopted in various downstream applications of LLMs. Together with the Mixture-of-Expert (MoE) technique, fine-tuning approaches have shown remarkable improvements in model capability. However, the coordination of multiple experts in existing studies solely relies on the weights assigned by the simple router function. Lack of communication and collaboration among experts exacerbate the instability of LLMs due to the imbalance load problem of MoE. To address this issue, we propose a novel MoE graph-based LLM fine-tuning framework GraphLoRA, in which a graph router function is designed to capture the collaboration signals among experts by graph neural networks (GNNs). GraphLoRA enables all experts to understand input knowledge and share information from neighbor experts by aggregating operations. Besides, to enhance each expert's capability and their collaborations, we design two novel coordination strategies: the Poisson distribution-based distinction strategy and the Normal distribution-based load balance strategy. Extensive experiments on four real-world datasets demonstrate the effectiveness of our GraphLoRA in parameter-efficient fine-tuning of LLMs, showing the benefits of facilitating collaborations of multiple experts in the graph router of GraphLoRA.", "sections": [{"title": "1 Introduction", "content": "Recently, increasing attention has been paid to the Parameter-Efficient Fine-Tuning (PEFT) strategies of Large Language Models (LLMs). For example, the Low-Rank Adaptation (LoRA) technique with LLMs has been widely adopted to downstream applications of LLMs due to the lower computational costs (Hu et al. 2021). Although the efficiency problem has been addressed in PEFT methods (e.g., LoRA) by updating fewer parameters, their performance inevitably declines, especially in multi-task scenarios. According to the scaling law, researchers attempt to utilize Mixture-of-Expert (MoE) to enhance the overall capability of LLMs (Shazeer et al. 2016). The research that applies MoE in PEFT of LLMs attracts increasing attention due to the remarkable improvements of model performance in practical downstream applications (Gou et al. 2023; Wang and Wu 2024; Yang et al. 2024b; Li et al. 2024a).\nOne practical way to incorporate MoE in PEFT LLMs is using LoRA aligned with the Feed Forward Network (FFN) layer in a transformer block (Li et al. 2024a; Wang et al. 2022; Dou et al. 2023a). The weight of each expert is assigned by a simple softmax function (termed as router function or gating function) with the information from input tokens. In these cases, an expert in the MoE framework is unable to explicitly share information with other experts, which limits their collaboration capability and exacerbates the instability of LLMs due to the imbalance load problem of MoE. The model quickly converges to a localized optimization, leading to the limited utilization of the comprehensive potential capabilities of MoE. Recent studies (Shazeer et al. 2016; Zoph et al. 2022; Luo et al. 2024; Dou et al. 2023a; Wang et al. 2022; Shazeer et al. 2016) add a balance loss that constraints the allocated frequency of each expert to alleviate the imbalance load problem of MoE. However, we believe that a more effective collaboration strategy in the router allocation process is emergency required to improve the coordination of multiple experts.\nTo address this issue, we propose a novel graph router for PEFT of MoE in LLMs, termed GraphLoRA. The graph router is conducted on a MoE Graph, which contains an input token node and all expert nodes. The information of the input tokens and collaboration signals from other experts is captured by graph neural networks (GNNs), enabling all experts to understand the input knowledge and share information with other experts effectively due to the shared parameters in GNNs. The graph router assigns weight to each expert according to the projection vector that is computed based on the representations of all expert nodes learned in GNNs. Then, Top-K experts with the highest assigned weights are activated to perform efficient sparse tuning in downstream applications. To further empower the capability of each expert and enhance their collaboration, we propose two novel coordination strategies: the Poisson distribution-based distinction strategy and the Normal distribution-based load balance strategy. Specifically, a Poisson distribution loss function is used to release the different capabilities of each expert, and a normal distribution is adopted to keep the balance of the activated frequency of each expert in a natural pattern.\nEquipped with the effective collaboration mechanism of experts in the graph router and two coordination strategies, our GraphLoRA achieves the SOTA model performance in downstream applications. Our contributions are summarized"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Mixture-of-Expert (MoE)", "content": "Empowered by the collaboration of multiple experts, the Mixture-of-Expert (MoE) framework has achieved remarkable performance in many applications (Chen, Jie, and Ma 2024; Li et al. 2024a; Lin et al. 2024; Li et al. 2024b; Zadouri et al. 2023), such as recommendation, video understanding, natural language generation, and so on. Each expert in the MoE framework specializes in handling a subset of input information. Different experts coordinate together to gain benefits for the overall MoE framework. According to the number of experts activated by the router function, the MoE methods can be classified into two categories, i.e., dense MoE and sparse MoE. In dense MoE, all experts are activated in the learning process, which enhances the model capability but suffers from high computational costs. In sparse MoE, only a selected subset of experts are activated, which not only reduces the computational overhead but also leverages their specialized knowledge for optimal results. The participation of each expert in the computation process is assigned by a router function (or gating function), ensuring an optimal blend of their specialized contributions. Specifically, in the MoE literature of LLMs (Zoph et al. 2022), the router is usually a Softmax function that controls the engagement of expert computations. All experts are allocated to computation in the dense MoE depends on the weights assigned by the router. In sparse MoE, only Top-K experts with the highest weights are selectively activated in the learning process. The MoE framework is widely used in the pre-training and fine-tuning process of LLMs. The pertaining of LLMs requires massive datasets and a high demand for computational resources. Our work uses open-sourced pre-trained LLMs, like Llama and Qwen, and focuses on utilizing the MoE to enhance the Parameter-Efficient Fine-Tuning (PEFT) of LLMs in specific domains."}, {"title": "2.2 Parameter-Efficient Fine-Tuning (PEFT)", "content": "The aim of fine-tuning LLMs is to optimize the model's performance in specific downstream tasks. Due to the high computational costs of updating all parameters in the full fine-tuning approach, the Parameter-Efficient Fine-Tuning (PEFT) technique is proposed to address this problem (Hu et al. 2021; Houlsby et al. 2019; Li and Liang 2021; Lester, Al-Rfou, and Constant 2021). In PEFT, only a small subset of parameters are updated of the base LLMs, making it more efficient and practical in real applications. The PEFT methods can be generally classified into three types, i.e., LORA (Hu et al. 2021), adapter tuning (Houlsby et al. 2019), and prompt tuning (Lester, Al-Rfou, and Constant 2021). Adapter tuning uses feed-forward up and feed-forward down projection matrices in the transformer block and only updates two adapter metrics in the fine-tuning process. Prompt tuning introduces a set of learnable prompt embeddings that are appended to the input tokens. The most widely used PEFT technique is LoRA, which uses a low-rank decomposition technique and updates the decomposed parameter matrix in the model training.\nRecent studies improve the model performance by integrating the MoE framework with PEFT in LLMs fine-tuning literature (Wu, Huang, and Wei 2024; Zadouri et al. 2023). In the transformer block of LLMs, each expert is a LoRA module working on either the FFN layer (Dou et al. 2023b; Wang et al. 2022; Li et al. 2024a), attention layer (Luo et al. 2024; Gou et al. 2023; Zhu et al. 2023), the whole transformer block (Zadouri et al. 2023; Liu et al. 2023; Gao et al. 2024) and each layer (Wu, Huang, and Wei 2024). Then the router function of MoE is designed to blend the contributions of all experts. Among them, the adoption of LoRA in the FFN layer attracted the most attention due to its extensive applicability in various LLM tasks in recent years.\nOur work aims to explore the effective MoE collaboration mechanism in the parameter-efficient fine-tuning of LLMs area. The MoE consists of multiple LoRA components that work on the FFN layer in the transformer block. Different from existing MoE studies, in which all experts coordinate by the traditional router function (Softmax function), we design a graph router in our GraphLoRA framework, which leverages graph neural networks to learn the collaboration information among all experts in the MoE graph."}, {"title": "3 Preliminary", "content": ""}, {"title": "3.1 Low-Rank Adaption (LoRA)", "content": "Low-rank adaption (LoRA) (Hu et al. 2021) is a widely used parameter-efficient fine-tuning (PEFT) technique in pre-trained large language models (LLMs). LoRA adopts a low-rank decomposition technique to update the parameters of the decomposed parameter matrix to learn the data distribution of the specific downstream task. It works on the feed-forward layer (FFN layer) in a transformer block of LLMs. For a linear layer in FFN represented as $h = Wx$, the update of the decomposed parameter matrix of LoRA is defined as:\n$h = Wx + \\Delta Wx = Wx + \\frac{\\alpha}{r}BAx,$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(1)\nwhere $x \\in R^I$ is the representation of input information, and $W \\in R^{O \\times I}$ is the pre-trained parameter matrix of LLMs. $A \\in R^{r \\times I}$ and $B \\in R^{O \\times r}$ are the low-rank matrix in LoRA"}, {"title": "3.2 Mixture-of-Expert (MoE-LoRA)", "content": "The illustration of MoE-LoRA is shown in Fig. 1 (a) and (b). Each expert network in MoE-LoRA can be represented as a LoRA module worked on the FFN layer in the transformer block. MoE uses a router function to assign the learning weights of each expert in the training process of LLMs. The coordinated weights of all experts are assigned by the router function in the feed-forward process, defined as:\n$h = Wx + \\Delta Wx = Wx + \\sum_{i=1}^{N} R_i(x)\\mathcal{E}_i(x),$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(2)\nwhere $\\mathcal{E}_i$ is the ith expert network and N is the number of experts. The router function $R(\\cdot)$ is a Softmax function that generates a probability distribution for the weights assigned to all experts."}, {"title": "4 GraphLoRA", "content": "The overview architecture of GraphLoRA is shown in Fig. 1 (d). This section introduces the details of the graph router in GraphLoRA and the propagation process in the FFN layer. Besides, to empower all experts to fully utilize their unique capabilities and keep the load balance, we propose two novel coordination strategies: load balance strategy and expert distinction strategy to enhance the GraphLoRA capability."}, {"title": "4.1 Graph Router", "content": "In typical MoE-LORA literature, all experts coordinate solely depending on the router function, which is a simple Softmax function that assigns the weights for each expert. The inefficient collaboration of all experts exacerbates the imbalance load problem of MoE in LLM fine-tuning. To enhance the collaboration of all experts, we propose a graph router to replace the simple router function. The graph router function in GraphLoRA assigns weights to each expert in collaboration with other experts on the MoE graph.\nSpecifically, given the MoE graph $G = (V, E)$. The node set $V = \\{e_1, e_2, ...e_n, x\\}$ consists of all expert nodes and the input-token information node x (after the self-attention layer and normalized layer in traditional transformer block). The edges between nodes are randomly constructed and controlled by an edge density hype parameter $B$. For the input-token node x and its neighborhood expert nodes $N_e(x)$ in the MoE graph, we use a graph neural network (GNN) to learn their interaction information, which not only implies the learning capability of each expert to the input-token node but also captures the collaboration information among all experts. The feed-forward process in GraphLoRA can be formulated as:\n$h = Wx + \\Delta Wx = Wx + \\sum_{i=1}^{N} R_{GNN}(x)_i\\mathcal{E}_i(x),$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(3)\nwhere x is the representation of input node x, $\\mathcal{E}_i$ is the ith expert network, and $R_{GNN}(\\cdot)$ is the graph router function in MoE graph, defined as:\n$R_{GNN}(x)_i = R(\\mathcal{F}(GNN(e_i, N(e_i)))),$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(4)\nwhere $GNN(\\cdot)$ is the graph neural network that learns the representation of expert node $e_i$ using information from its neighbors $N(e_i)$ that contains other experts and input token node x. $\\mathcal{F}(\\cdot)$ is the project function that maps the representations from all experts into one unique vector. $R(\\cdot)$ is the Softmax function that assigns probability weights to all experts."}, {"title": "4.2 Expert Distinction Strategy", "content": "The core idea of MoE is to utilize the unique capability of each expert to collaborate toward superior outcomes. Hence, we optimize the assigned weights by graph router to approximate the Poisson distribution. Our aim is to elicit the distinct capabilities of different experts. Specifically, for the output vector of the graph router, denoted as $o_r \\in R^N$. Each dimension of $o_r$ represents the assigned weight for each expert to deal with the input information. As the order of experts has no practical meaning, we sort values of each dimension in the vector $o_r$ in descending order to obtain a new vector $v_r \\Leftrightarrow sort(o_r)$. The distribution of $v_r \\in R^N$ is optimized to approximate the Poisson distribution vector $V_{poisson}(\\lambda, i)$. The Kullback\u2013Leibler (KL) divergence distance is used to calculate the loss function, defined as:\n$V_{poisson(x, i)} = \\frac{\\lambda^i}{i!}e^{-\\lambda},$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(6)\n$Loss-Poisson = \\sum_{i=1}^{N} V_{poisson(x, i)} log \\frac{V_{poisson(x, i)}}{v_r},$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(7)\nwhere $i = \\{1, 2..., N\\}$ and N is the number of experts. $V_{poisson(\\lambda, i)}$ is the vector follows the Poisson distribution and $\\lambda$ is the learning parameter."}, {"title": "4.3 Load Balance Strategy", "content": "Apart from the distinct capabilities of different experts, one important factor in MoE training is keeping the load balance of all experts. Otherwise, greater routing weights on a small number of experts in the early stages of fine-tuning will result in a rapid localized optimization problem. In GraphLoRA, we propose the Normal distribution-based load balance strategy. Specifically, as only Top-K experts are activated in dealing with input information, we calculate the cumulative weights of each expert to represent the activation frequency distribution of all experts in the fine-tuning process. And normalized the cumulative weights to construct the activation frequency vector, denoted as $v_a$ in the next feed-forward step. Our aim is to make the activation frequency vector $v_a$ follow a natural normal distribution rather than the absolute equality in existing MoE literature (Li et al. 2024a; Zoph et al. 2022). Formally, the Normal distribution-based load balance loss function can be defined as:\n$V_{normal(\\mu, \\sigma, i)} = \\frac{1}{\\sqrt{2\\pi \\sigma}}exp(-\\frac{(i - \\mu)^2}{2\\sigma^2}),$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(8)\n$Loss-Normal = \\sum_{i=1}^{N} V_{normal(\\mu, \\sigma, i)} log \\frac{V_{normal(\\mu, \\sigma, i)}}{v_a},$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(9)\nwhere $i = \\{1, 2..., N\\}$ and N is the number of experts. $V_{normal(\\mu, \\sigma, i)}$ follows the Normal distribution, $\\mu = $ is the mean of all samples, and $\\sigma$ is learning parameter optimized in the fine-tuning process."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Settings", "content": "Datasets. To evaluate the performance of our framework, we use four widely used public datasets, including the question-answering task, i.e., ARC-Challenge (Clark et al. 2018), OpenBookQA (Mihaylov et al. 2018), SIQA (Sap et al. 2019), and task classification task in BoolQ (Clark et al. 2019) dataset. These datasets cover the evaluations on different domains of LLMs, such as factual knowledge from Wikipedia, natural science, science facts, and social interactions.\n\u2022 ARC-Challenge. It is released by the Allen Institute for AI and is widely used to evaluate the reading comprehension and reasoning abilities of LLMs. The dataset consists of multiple-choice questions derived from U.S. elementary school science exams.\n\u2022 BoolQ. It is a question-answering dataset and contains Yes/No type answers extracted from real search queries. Each question is described by a relevant passage of text. This dataset evaluates the natural language understanding capability of LLMs, particularly in handling short-form question answering and textual reasoning.\n\u2022 OpenBookQA. It is designed to evaluate the ability of LLMs to understand elementary-level science knowledge. It consists of multiple-choice questions. Each question is accompanied by a small set of \"facts\" that are required to answer the questions.\n\u2022 SIQA. Social Interaction QA is a benchmark for multiple-choice question answering focused on social intelligence reasoning. It consists of questions that require models to understand and reason about social situations, such as interpreting human behavior, emotions, and intentions. Each question contains a brief introduction with one correct answer in three answer choices."}, {"title": "5.2 Main Results", "content": "We compare the performance of different MoE methods from three aspects, i.e., Accuracy, Stability, and Efficiency, to make comprehensive evaluations. The results on four datasets with three different base LLMs are shown in Table 1 and Fig. 2. We repeat each experiment under five different random seeds, and report the mean value of the model accuracy. The standard deviation of each model is used to measure the stability of method. The efficiency is evaluated by the size of learning parameters in the fine-tuning process of LLMs.\nAccuracy Evaluation. Model performance on accuracy metric is shown in Table 1. We can see that: (1) GraphLoRA achieves the best model performance in most cases on four datasets with different base LLMs, showing the effectiveness of the collaborations of multiple experts learned on the MoE graph. (2) The sparse MoE architecture with an auxiliary load balance loss in MixLoRA performs better than both the sparse MoE and dense MoE methods in many cases, showing the importance of enhancing the coordination of experts. (3) The performance of the methods varies across different datasets. For example, for the easier task with higher model accuracy in the OpenBookQA dataset, the dense MoE method LoRAMOE performs better in Llama3 and Yi-1.5. This indicates that the localized optimization of sparse MoE architecture may reduce the model capability and may lose its advantages in easy task learning. With effective expert collaboration in GraphLoRA, our method consistently exhibits the best performance in various downstream tasks.\nStability Evaluation. The biggest challenge in MoE fine-tuning research is the instability of model performance caused by the imbalance load problem. As shown in Table 1, the smaller the standard deviation, the greater the stability of the model. We can see that: (1) The standard deviation of GraphLoRA is the smallest in most cases compared with the sparse MoE methods. It achieves comparable stability as the LoRAMOE with a dense MoE architecture, showing the superiority of our framework in dealing with the imbalance load problem. (2) The stability of different MoE methods varies on different downstream datasets, and it is also influenced by the base LLMs. The dense MoE method is relatively stable but suffers the cost of memory and computational overhead due to all experts being activated in the learning process."}, {"title": "5.3 Experimental Analysis", "content": "We conduct ablation studies to show the effectiveness of the graph router and two coordination strategies used in GraphLoRA. We further analyze the effects of hyperparameters that may influence the model performance.\nAblation Studies. The graph router in GraphLoRA allocates weight for each expert based on the collaboration information learned from the MoE graph. Besides, the Poisson distribution-based expert distinction strategy and normal distribution-based load balance strategy are adopted to empower the capability of each expert and their collaborations. To verify the effectiveness of each component, we remove the graph router, Poisson distinct loss (see in Eq. 7), and Normal balance loss (see in Eq. 9) respectively. The performance comparisons of the degradation variants (-Graph), (-Normal), (-Poisson) on four datasets are shown in Fig. 3. We can see that all three components make contributions to the model performance. The impact of removing the graph router is obvious in all datasets in both model accuracy and stability aspects, indicating the importance of using the MoE graph to enhance expert collaborations. Besides, the Poisson distribution loss plays a crucial role in the fine-tuning"}, {"title": "5.4 Conclusion", "content": "In this paper, we address the instability problem of LLMs due to the imbalance load issue of MoE. We propose a novel framework GraphLoRA to enhance the collaborations of multiple experts in parameter efficient fine-tuning of LLMs. GraphLoRA empowers the graph router to assign weights to experts by effective information sharing among experts on graph neural networks. Besides, two novel coordination strategies, i.e., the Poisson distribution-based expert distinction strategy and the normal distribution-based load balance strategy are proposed in our work. The two constraint operations further enhance the capability of each expert and comprehensively coordinate the collaboration among them. The efficient collaboration among our sparse MoE architecture provides a solution to make trade-offs among the model performance, stability, and resource overhead in the literature of LLMs. Due to the limitation of computation resources, we conducted experiments to verify the effectiveness of our method with different LLMs under 10B parameters. In future work, we will attempt to conduct experiments on more powerful LLMs with larger parameter sizes."}]}