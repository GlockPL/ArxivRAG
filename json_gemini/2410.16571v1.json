{"title": "Implicit Contact Diffuser: Sequential Contact Reasoning with Latent Point Cloud Diffusion", "authors": ["Zixuan Huang", "Yinong He", "Yating Lin", "Dmitry Berenson"], "abstract": "Long-horizon contact-rich manipulation has long been a challenging problem, as it requires reasoning over both discrete contact modes and continuous object motion. We introduce Implicit Contact Diffuser (ICD), a diffusion-based model that generates a sequence of neural descriptors that specify a series of contact relationships between the object and the environment. This sequence is then used as guidance for an MPC method to accomplish a given task. The key advantage of this approach is that the latent descriptors provide more task-relevant guidance to MPC, helping to avoid local minima for contact-rich manipulation tasks. Our experiments demonstrate that ICD outperforms baselines on complex, long-horizon, contact-rich manipulation tasks, such as cable routing and notebook folding. Additionally, our experiments also indicate that ICD can generalize a target contact relationship to a different environment. More visualizations can be found on our website https://implicit-contact-diffuser.github.io", "sections": [{"title": "I. INTRODUCTION", "content": "Interacting with the environment through contact is central to many robotic tasks, such as manipulation and locomotion. Despite the ubiquity of contact interactions, controlling these hybrid systems poses significant challenges due to the complex interplay between discrete contact events and continuous motion. For instance, in cable routing, the robot must generate smooth motions to initiate and maintain contact between the cable and the fixtures (Fig. 1). If the contact breaks at any point, the cable could slip off the fixtures. Moreover, when model errors or external disturbances occur, the robot must adjust its actions accordingly to maintain task success.\nA large body of work has attempted to tackle these challenges by planning [1], [2], [3], [4] or trajectory optimization [5], [6], [7] through contact. However, these methods are typically limited to rigid objects, or face limitations in online replanning due to the high computational costs involved.\nIn this paper, we introduce a learning-based model predictive control (MPC) framework to address this class of problems. In particular, we train a latent diffusion model to generate future contact sequences as subgoals, which guide a MPC controller to generate robot motions that establish the desired contact relationships. A key question, however, is determining the best representation for these contact relationships.\nOne approach is to use binary contact states. Wi et al. [8] propose to specify desired contact locations by predicting a heatmap over the environment. However, this approach lacks critical information regarding which part of the object should be in contact, a crucial factor for tasks where maintaining precise object-environment interactions is important. Additionally, it cannot capture the dynamic contact switching required in certain tasks.\nTo overcome these limitations, we leverage recent advancements in implicit neural representations and encode contact relationships using a modified version of Neural Descriptor Fields (NDF) [9]. We train a scene-level NDF to capture geometric information by predicting occupancy and gradient direction of the signed distance function. By querying the scene NDF with the object's point cloud, we compute a dense, contact-aware representation of the object. Our experiments show that these neural descriptors capture task-relevant geometric relationships (e.g., left or right of a fixture) rather than specific locations, providing more flexible guidance. This allows us to transfer goal contact relationships across different environments at test time.\nTo capture the contact switching required to reach a goal, we train a latent diffusion model to predict the contact sequence represented by neural descriptors. We also learn a reachability function, similar to Subgoal Diffuser [10], to determine the required sequence length. The key contributions of this paper are: 1) a latent diffusion model that reasons about evolving contact relationships in long-horizon manipulation tasks; 2) an MPC framework that plans motions based on desired contact relationships rather than precise locations. 3) a scene-level neural descriptor field that provides local contact representations, enabling greater generalization across environments.\nWe validate our method on challenging long-horizon contact-rich manipulation tasks, including cable routing and notebook folding. Our results show that ICD outperforms or is on par with baselines that plan to exact locations rather than focusing on contact relationships, as well as baselines that directly predict actions without planning. ICD can also adapt a target contact relationship to a different environment naturally."}, {"title": "II. RELATED WORK", "content": "Prior works studies different representations for object manipulations, such as key points [11], RGB image [12], [13], point cloud [14], [15] or mesh [16], [17], [18]. Recently, Neural Descriptor Fields (NDF) [9], [19], [20] demonstrates itself as an effective implicit representation for category-level generalization. In this work, we propose a variant of NDF where spatial structure is preserved. We show that compared to explicit representations such as point cloud, the NDF better captures the soft contact relationships between object and environment.\nControlling the robot to make and break contacts purposefully has been one of the key challenges for robotics, since it involves optimizing over a hybrid system that contains both continuous (robot motion) and discrete variables (contact). One common approach [1], [2], [3], [4] is to find object motions using a sampling-based motion planner guided by a high-level search for contact modes. However, these methods are typically limited to rigid objects. Recently, learning-based methods have been introduced to detect or control contact [21], [22], [23], [24], [25] for complaint tools such as spatulas. Wi et al. [8] designs a framework for contact-rich manipulation that predicts the target contact patch over the environment conditioned on the language. However, the predicted contact patch does not specify which part of the object should make contact, and does not model a sequence of changing contacts. In this paper, we propose to use a contact-aware neural representation and a diffusion-based architecture to model future contact sequences for highly deformable objects, such as cables.\nDiffusion models have also been applied to robot manipulation, either as a policy class that predicts action directly from observation [13], [26], [27], [28], [29], [30], [31], or as a learned planner to generate future trajectories [32], [33], [34], [35], [10]. Although some existing diffusion-based methods have been shown to work on certain contact-rich manipulation tasks, such as planar pushing [13], dumpling making [15] or book shelving [36], our experiment suggests that they struggle with tasks that involve long-horizon reasoning of changing contacts. Similarly to us, the Subgoal Diffuser [10] generates future subgoals using a diffusion model to guide an MPC controller. However, Subgoal Diffuser represents the subgoals using locations of key points, which can be overly constrained and does not reason about the contact interaction between object and environments explicitly.\nLong-horizon manipulation tasks usually contain several distinct stages and contain a lot of local optima. One way to tackle this is to plan over skill abstractions [37], [38], [39], [40], [41], [42], [43] learned with imitation learning or reinforcement learning. Another way is to decompose tasks into multiple subgoals [44], [45], [46], [47], [10], [35], which can be used to guide a low-level policy. We propose a method to generate subgoals represented by neural descriptors, which will highlight the contact relationships bwtween the objects and environment. While NOD-TAMP [48] uses a similar representation for long-horizon reasoning, it adapts a given demonstration trajectory to a new situation by optimization while we directly learn the distribution of the trajectory using a latent diffusion model. Also, NOD-TAMP cannot handle deformable objects."}, {"title": "III. PRELIMINARIES", "content": "In this paper, we consider long-horizon contact-rich manipulation problems of deformable object that involve changing contacts. We denote the robot state by $s_t$ and the action by $a_t$. The goal specification is represented as a pair of point clouds $(P_{og}, P_s)$, where $P_{og}$ is the point cloud of the object in a goal state and $P_s$ is the point cloud of the scene. However, the goal is not to match the shape and pose of the object exactly but to match the contact relationship between the object and the scene, so that the object is in contact with the scene in the correct locations. For example, in a cable routing task, the objective is to route the cable through the opening of the hook, ensuring that the cable touches the front side of the hook but not the back. It is important to note that we focus solely on the geometric aspect of the contact, without differentiating between the contact modes such as sticking or sliding contact.\nThis type of problem presents significant challenges due to the need for joint reasoning over both continuous motion and discrete contact switching, particularly for high-dimensional deformable objects. Additionally, long-horizon reasoning is crucial for generating effective contact-switching behavior while avoiding local minima, ensuring that the robot can progress toward the final goal without becoming stuck in suboptimal configurations.\nOur objective is to learn a dense object-centric representation of contact relationship, which describes how each point of the object interacts with the environment. Next, we learn a generative model that, given the current state, scene, and goal specification, predicts a sequence of contact subgoals using the learned representation. These subgoals guide an MPC method to sequentially make and break contact and ultimately reach a goal state that conforms to the goal specification.\nWe assume access to an offline dataset $D$, which contains N different trajectories of object point clouds and the corresponding scene point cloud $(T^i, P_s)$, where $\\tau^i = [P_0^i, P_1^i, ..., P_T^i]$. The offline dataset is collected with a scripted policy that does not guarantee task completion. We also assume access to the full point cloud for both the object and the scene, and the order of points in $P_0$ does not change between states."}, {"title": "IV. METHOD", "content": "In this section, we introduce Implicit Contact Diffuser, a method designed to capture and reason about contact switching in long-horizon deformable object manipulation. In Section IV-A, we discuss how to represent the object-environment contact relationships of deformable objects using an implicit neural representation. In Section IV-B, we describe how to train a latent point cloud diffusion model to predict the contact sequence. Finally, in Section IV-C, we discuss how to follow the predicted contact sequence using a sampling-based MPC planner.\nFinding a suitable contact representation that facilitates planning is a challenging problem. If we naively represent contact with a binary discrete representation, planning over the contact space can quickly become combinatorially expensive, which is one of the reasons why prior methods [52], [4] struggle with deformable objects. Our key insight is that we can capture the soft object-environment contact relationships using a continuous implicit neural representation. We build upon Neural Descriptor Fields (NDF) [9], [19], [20] to develop a contact-aware neural representation for deformable objects, utilizing a scene NDF. Given a scene point cloud $P_s$, we learn a function $f$ to map a 3D coordinate $x \\in \\mathbb{R}^3$ to a latent neural descriptor in $\\mathbb{R}^d$:\n$f(x|P_s) = f(x|E_s(P_s))$ (1)\nwhere $E_s(P_s)$ is a PointNet [53] model. Given an object point cloud $P_o$, the state of the object can be described as the concatenation of all point descriptors:\n$P_{ndf} = O_{NDF}(P_o|P_s) = \\bigoplus_{x_i \\in P_o} f(x_i|P_s)$ (2)\nSince the function $f$ is trained to predict the geometric features of the scene, the NDF point cloud $P_{ndf} \\in \\mathbb{R}^{N\\times d}$ can be interpreted as an encoding of point-wise geometric relations with the scene for every point on the object.\nWe make several key design choices to adapt NDF, ensuring it better suits the tasks we are dealing with. Similar to Simeonov et al. [9], we train $f(x|P_s)$ using occupancy prediction. Additionally, we incorporate an auxiliary loss on the gradient direction of the signed distance function (SDF): $I_{grad} = (\\nabla SDF(x) - \\widehat{\\nabla SDF}(x))^2$, where $\\nabla SDF(x)$ and $\\widehat{\\nabla SDF}(x)$ refer to ground-truth and predicted gradients of the SDF. This helps the descriptors encode not only whether a point is in contact (occupied), but also how to make contact for points that are not yet in contact.\nNDF adopts a SE(3)-invariant neural network architecture, Vector Neuron [54], to enhance the generalizability of the descriptors. While the descriptors remain unchanged when a transformation $T \\in SE(3)$ is applied to the object and the scene simultaneously, this can sometimes lead to unrealistic outcomes. For example, the object will have similar NDF features whether it contacts the floor or the ceiling. To mitigate this, we modify the Vector Neuron to be invariant only to the rotations along the direction of gravity (as gravity plays a large part in determining the configuration of a deformable object), which we define as $SE(3)^\\downarrow$. Specifically, we add a small constant value to the z-axis of the point features, ensuring that rotations not aligned with the z-axis produce distinct latent features.\nThe original NDF model [9] encodes the entire point cloud into a single global feature vector by averaging over $E_s(P_s)$. In contrast, we aggregate the local features of nearby contact candidates for each query point using K-nearest neighbors (Fig. 2) based on the intuition that the object is more likely to make contact with spatially closer points. Our experiments indicate that incorporating these local NDF features is important for improving task performance."}, {"title": "B. Implicit Contact Diffuser", "content": "In the previous section, we describe a dense contact-aware neural representation for deformable objects. Now we will use this representation to tackle long-horizon contact-rich manipulation problems with contact switching. We introduce Implicit Contact Diffuser, a diffusion-based architecture that generates a sequence of subgoals $\\tau_{ndf} = [P_{ndf_0}, P_{ndf_1}, P_{ndf_2},..., P_{ndf_M}]$, represented as NDF point clouds $P_{ndf} \\in \\mathbb{R}^{N\\times d}$\nWhile diffusion models have been applied to point cloud generation, prior works [55], [56] only generate individual point clouds $P \\in \\mathbb{R}^{N\\times 3}$. In our case, in order to capture contact switching, we need to generate a sequence of coherent latent point clouds consisting of high-dimensional point features.\nTo tackle this sequential point cloud generation problem, we propose using Latent Diffusion Models (LDM) [57]. We begin by training a Variational Autoencoder (VAE) [58] to project the high-dimensional point cloud $P_{ndf}$ into low-dimensional vectors. Next, we train a hierarchical diffusion model to recursively generate subgoals from coarse to fine, following Huang et al. [10].\nThe VAE comprises three components: a PointNet++ encoder $E_{ndf}(z_t|P_{ndf_t})$ [59], a point-wise MLP decoder $D_{ndf}(\\widehat{P_{ndf_t}}|P_{canon}, z_t)$, and a distributional reachability prediction MLP $\\phi(r|z_{t_1}, z_{t_2}, E_s(P_s))$, as visualized in Fig. 3. The encoder $E_{ndf}$ compresses the NDF point cloud $P_{ndf_t}$ into a latent vector $z_t$. The pointwise MLP decoder $D_{ndf}$ is adapted from Luo et al. [55]. Given $z_t$ and the canonical object point cloud $P_{canon}$, an implicit decoder $D_{ndf}$ reconstructs the NDF point cloud from the latent vector. The query coordinates $P_{canon}$ are predefined, i.e., a straight rope or a magazine that is laid flat.\nThe VAE is trained by three different losses:\n$\\mathcal{L}_{vae} = \\lambda_1 \\mathcal{L}_{recon} (P_{ndf}, \\widehat{P_{ndf}})$ (3)\n$\\+ \\lambda_2 D_{KL}(E_{ndf}(z_t|P_{ndf_t}), \\mathcal{N}(z))$ (4)\n$\\+ \\lambda_3 \\mathcal{L}_{Reach} (r, \\phi(r|z_{t_1}, z_{t_2}, E_s(P_s)))$ (5)\nIn addition to the regular reconstruction loss and KL regularization loss, we introduce a reachability loss $\\mathcal{L}_{Reach}$ to encourage temporal consistency in the learned latent space. During training, we sample pairs of states in the same trajectory using the discounted state occupancy measure (lower probability for states that take more steps to reach) in line with previous work [60], [61]. For a pair of NDF point clouds $(P_{ndf_{t_1}}, P_{ndf_{t_2}})$, we define reachability as the minimum number of steps to travel between them. Following Subgoal Diffuser [10], we discretize the reachability into $K$ bins and frame the reachability prediction problem as a classification problem and train an MLP $\\phi(r|z_{t_1}, z_{t_2}, E_s(P_s))$ with cross-entropy loss. Since we do not assume that the training data are high-quality demonstrations, and there might exist multiple paths of different lengths to travel between two states, $\\phi(r|z_{t_1}, z_{t_2}, E_s(P_s)$ will capture the distribution of reachability between two states. During test time, we use \"softmin\" to estimate shortest distance (highest reachability), which is used to determine the number of subgoals for the latent diffusion model.\nThe objective of the latent diffusion model is to generate a sequence of NDF subgoals $\\tau_{ndf}$, given current state, goal specification, and the scene. With the point cloud VAE described above, the diffusion model only needs to model the distribution of the condensed latent vectors, denoted as $p(\\tau_z|z_{cur}, z_{goal}, E_s(P_s))$"}, {"title": "C. MPPI with Neural Contact Subgoals", "content": "Every $T$ steps, Neural Contact Diffuser generates a sequence of contact subgoals $\\tau_{ndf}$. We use a sampling-based MPC method, Model Predictive Path Integral (MPPI) [62], to plan a sequence of robot actions to track the subgoals. We define robot actions $a \\in \\mathbb{R}^3$ as the delta translation of the end effector. At each step, MPPI samples $K$ action sequences of length $H$, where $H$ is the planning horizon.\nThe sampled actions are evaluated by rolling out in the MuJoCo [63] simulator with the following cost:\n$J_{MPPI} = \\sum_{t=0}^{H-1} \\min_{P_{ndf_i} \\in \\tau_{ndf}} (\\widehat{P_{ndf_t}} - P_{ndf_i})^2 $\n$ + \\lambda_{col} max(-SDF(r_t), 0)$\n$\\widehat{P_{ndf}}$ is the desired NDF subgoals predicted by the diffusion model. The rollouts from the simulator are transformed to NDF space using $O_{ndf}$, which we denote as $P_{ndf_t}$. The first cost term is the Euclidean distance to the closest NDF subgoal. A subgoal will be removed from the goal chain $\\tau_{ndf}$ once the current state is within predefined distance threshold. The second cost is to prevent the robot from colliding with the environment, represented as the scene SDF. The robot geometry is approximated by a set of spheres as in [64]. By minimizing $J_{MPPI}$, MPPI generates robot actions that manipulate the object to achieve the desired contact relationships described by $\\widehat{\\tau_{ndf}}$."}, {"title": "V. EXPERIMENTS", "content": "Our experiments aim to show that 1) the scene NDF is a good representation for capturing contact relationships and 2) Implicit Contact Diffuser is capable of long-horizon contact reasoning and generating contact sequences to guide an MPC controller to reach the desired contact relationship. We also demonstrate our method on a physical robot and the videos can be found on our website."}]}