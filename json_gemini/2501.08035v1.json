{"title": "READ: Reinforcement-based Adversarial Learning for Text Classification with Limited Labeled Data", "authors": ["Rohit Sharma", "Shanu Kumar", "Avinash Kumar"], "abstract": "Pre-trained transformer models such as BERT have shown massive gains across many text classification tasks. However, these models usually need enormous labeled data to achieve impressive performances. Obtaining labeled data is often expensive and time-consuming, whereas collecting unlabeled data using some heuristics is relatively much cheaper for any task. Therefore, this paper proposes a method that encapsulates reinforcement learning-based text generation and semi-supervised adversarial learning approaches in a novel way to improve the model's performance. Our method READ (REinforcement-based ADversarial learning) utilizes an unlabeled dataset to generate diverse synthetic text through reinforcement learning, improving the model's generalization capability using adversarial learning. Our experimental results show that READ outperforms the existing state-of-art methods on multiple datasets.", "sections": [{"title": "1 Introduction", "content": "The introduction of pre-trained transformer-based large-scale models such as BERT (Devlin et al., 2019), GPT-2 (Radford et al.), and RoBERTa (Liu et al., 2019) has led to impressive results on many Natural Language Processing (NLP) tasks. However, even with a pretraining these models require a large number of labelled data for fine-tuning on a downstream task (Yogatama et al., 2019; Croce et al., 2020). Few works (Mukherjee and Awadallah, 2020; Croce et al., 2020) have shown significant drop in performance while fine-tuning BERT using only limited examples.\nObtaining labelled data can be expensive and time-consuming process (Dandapat et al., 2009; Sabou et al., 2012; Fort, 2016), nevertheless collecting unlabeled data for any downstream task is relatively much cheaper. Semi-supervised learning (Kipf and Welling; Zhu, 2005) has been shown to be one of the promising paradigms to generalize even with few labelled data, by utilizing large amounts of unlabeled data. Recently, Miyato et al. (2016); Xie et al. (2020); Izmailov et al. (2020); Liu et al. (2021) have shown substantial improvements for text classification tasks using consistency training on unlabeled data via data augmentations such as back-translation. One of these approaches is Semi-Supervised Generative Adversarial Networks (SS-GANs) (Salimans et al., 2016), which uses GANs (Goodfellow et al., 2014) to expose the huge amounts of unlabeled to the classifier for improving generalization capability. GAN-BERT (Croce et al., 2020) extends SS-GANs by training BERT with unlabeled data in a generative adversarial setting and achieves comparable results even with less than 200 labeled examples to a fully supervised setting.\nGAN-BERT employs a generator which produces features resembling the real data distribution due to adversarial training, while a discriminator is trained to assign class categories and to distinguish samples of the generator from the real instances. The adversarial training helps GAN-BERT to learn generalizable feature representations. We hypothesize that adversarial learning with synthetic feature representations may not fully unlock generalization capabilities of pre-trained models and argue that generating text instead of feature representations can further improve their generalization capabilities. The feature generator in GAN-BERT is only used during training and becomes unusable during inference, whereas text generators can help in debugging and model explainability.\nIn last decade, various methods (Wiseman and Rush, 2016; Dong et al., 2019; Song et al., 2019; Lewis et al., 2020) have been proposed for text generation, however in this work, we employ inverse reinforcement learning (IRL) (Shi et al., 2018) framework for text generation which alleviate the problem of mode collapse and reward sparsity. IRL generates text using a reward function which gives a higher reward to the real texts and lower rewards"}, {"title": "2 Methodology: READ", "content": "Assuming we have a small labeled dataset L and an unlabeled dataset U, the aim is to train a classifier over k-class objective using a pre-trained model on the dataset L. Similar to GAN-BERT, we propose READ to improve the performance it using the unlabeled set U. READ consists of following components: Text Generator G, Reward Approximator R, pre-trained Transformer Model M and Classifier C. In next subsections, we will explain these components and their objectives.\nText Generator G is implemented by following the IRL, where it trained using the unlabelled dataset U to generate synthetic and diversified texts U'. Text generation task can be regarded as the generation of the text sequence X1:T = X1,X2,\u2026\u2026\u2026,XT with a trajectory T = {$1, A1, 82, A2,\uff65\uff65\uff65,ST,\u0430\u0442}, where st is the current state of the previous prediction X1:t and at is the action to select the next word Xt+1. G is trained to generate real-like examples by maximizing the expected reward R(T).\nReward Approximator R is also defined following IRL as the summation of the rewards of each step with a modification. The reward function at step t is defined using $r_\\theta(s_t, a_t)$, where $r_\\theta$ is a feed-forward neural network. To bridge the generation and classification processes, we use the probability $P_{k+1}$ of being classified as fake by the classifier C as the additional input in the reward function $r_\\theta (s_t, a_t, P_{k+1})$. The overall reward for a trajectory T can be defined as follows:\n$R(\\tau) = \\sum_t r_\\theta (s_t, a_t, P_{k+1})$\nR is trained to maximize the log-likelihood of the samples in the U, whereas G is trained to maximize the expected reward with an entropy regularization term. We follow IRL framework for defining the training objectives of R and G, and refer the readers to their work for additional details.\nTransformer Model M is a pre-trained transformer model to encode any input text to a d dimensional feature representation h\u2208 Rd. Classifier C is defined by following GAN-BERT, where C is trained to classify any feature representation h in one of the k task categories or into the k + 1th fake category, if the h corresponds to a fake example.\nTraining objective of M and C is defined by minimizing following three losses: L\u012b loss on classifying the samples from the labeled dataset L into one of the k classes, Lr loss for not classifying the samples from L and U as fake, and an additional Lf loss for classifying the generated samples from U' as fake.\n$L_l = -E_{x,y \\sim L} log [p(\\hat{y} = y | x, y \\in (1,...,k))]$\n$L_u = -E_{x \\sim L \\cup U} log [1 \u2013 p(\\hat{y} = y | x, y = k + 1)]$\n$L_f = -E_{x \\sim U'} log [p(\\hat{y} = y | x, y = k + 1)]$\nwhere p is the probability vector returned by C for the input x.\nIn IRL, the reward function is defined using only current and previous states, whereas in READ, the reward function also takes the probability of being fake $P_{k+1}$. As, we are training M and C for identifying the generated samples as the fake class, the probability $P_{k+1}$ of being fake will be high for generated examples whereas it will be low for real text samples. Due to this property, reward function in READ will encapsulate the classifier's knowledge along with the real text distribution. Simultaneously, we are training the text generator G to maximize the expected reward using adversarial learning to encourage the generation of samples that are not only similar in form of states but"}, {"title": "3 Experimental Details", "content": "We have evaluated our method's performance on three sentence classification tasks: Fine Grained Question Classification TREC-QCF task (Lang, 1995), Coarse Grained Question Classification TREC-QCC task (Li and Roth, 2006), and Sentiment Analysis SST-5 task (Socher et al., 2013). We have reported the training and test data statistics in Table 1 in the Appendix."}, {"title": "3.1 Dataset"}, {"title": "3.2 Baselines and READ's Variants", "content": "In our experiments, we compare READ with GAN-BERT and Baseline which is a vanilla fine-tuning method without any adversarial training. We experiment with two pre-trained transformer models BERT (Devlin et al., 2019) and ROBERTa (Liu et al., 2019). To understand the importance of the encapsulation of text generation and adversarial learning, we experiment with disjoint training of text generation and classifier by removing the probability of being fake from the reward function, $r_\\theta(s_t, a_t)$. We denote the method of disjoint training as D-READ in our experiments."}, {"title": "3.3 Training Details", "content": "We followed IRL for implementing text generator and reward approximator. The text generator consists of a LSTM layer with embedding size of 128 and followed by 4 linear layers with dimension of 128 along with a dropout of 0.1. We set the maximum sequence length of the generated sentences to 64. The reward approximator consists of MLPs with 3 hidden layers of 128 dimensions with a dropout of 0.2. The Classifier consists of a hidden layer of 768 dimension followed by leaky-ReLu activation function. We have used AdamW (Loshchilov and Hutter, 2018) as the optimizer with learning rate of 0.005 for G, 0.004 for R, and 5e-5 for both M and C."}, {"title": "4 Results", "content": "We have reported the accuracy for varying amount of labeled data using BERT pre-trained model in Figure 2. We observe that the accuracy increases with the increase in the amount of annotated data for all the methods. The Baseline method where no adversarial learning is used performs the worst among all the methods.\nOn the TREC-CC task, our method READ outperforms the GAN-BERT and Baseline method for all the values of labeled data. The gains are much more significant for the lower amount of labeled data, with gains of 68% and 26% over Baseline and GAN-BERT, respectively, when 2% (108 samples) labeled data is used. Similar to GAN-BERT, the gains from our method starts to diminish with the increase in the amount of labeled data.\nWe observe similar trends on the SST-5 dataset with READ outperforming all the methods in each configuration. Similar to TREC-CC task, the gains from READ starts to diminish with the increase in the amount of annotated data. When only 1% labeled data (85 samples) is used, our method provides 9% and 14% of gain over GAN-BERT and Baseline, respectively.\nTREC-CC and SST-5 datasets have only six and five classification categories. However, the TREC-CF dataset has 50 categories, making it a more challenging task than the others. The difficulty of the"}, {"title": "task is also evident from the fact that the Baseline method achieves almost 0% accuracy when less than 2% labeled data is used. We observe that the gains are more significant from READ on TREC-CF than the other datasets. We also observe that the trend of diminishing gains with the increase in amount of labeled data is not visible on TREC-CF dataset, with READ providing consistent gains for all the values of annotated data.", "content": "We have provided a similar analysis using ROBERTa in Figure 4 in the Appendix. We observe almost similar results to that of BERT, with slightly high accuracy in case of all the methods. It shows that irrespective of the choice of pre-trained transformer model, the proposed approach provides similar gains on all the datasets.\nIn Figure 2, we have reported the results for D-READ method where the text generator and classifier are independently trained, whereas READ encapsulates all the components through the reward function. We observe that D-READ provides better performance than the GAN-BERT on TREC-CF dataset, showcasing the importance of text generation instead of feature generation. However, it fails to outperform READ for all training configurations, demonstrating the importance of encapsulation of all the components."}, {"title": "4.1 Generation Quality", "content": "We hypothesize that the quality of synthetically generated text plays a big role in improving the performance of the model. To verify this, we have shown some of the generated samples by mapping it to the original text using cosine-similarity in Table 2. The generated samples are almost similar to the real text with lot of variations, showing the diversity in the generation quality."}, {"title": "4.2 Discriminative Features", "content": "We have shown the t-SNE (Van der Maaten and Hinton, 2008) visualization of the features from the last layer of the BERT model after fine-tuning Baseline and READ method on TREC-CC dataset with 1% of labeled data in Figure 3. We can see that the features learnt from Baseline are not class-discriminative and are overlapping for lot of classes, whereas the features learnt using READ are class-discriminative, with each cluster denoting a class-label. Our method is able to learn class-clusters with just 1% of labeled dataset, validating the observed gains in the previous sections."}, {"title": "5 Conclusion", "content": "In this work, we propose a novel method for improving the generalization capabilities of text classifiers when fine-tuned with limited labeled data. READ encapsulate reinforcement-based text generation and classifier through adversarial learning with the help of unlabeled data. We evaluated our method on multiple datasets and observed significant gains over the Baseline and GAN-BERT when very limited data is used. We show the importance of encapsulation through experiments and observed a significant drop in performance with disjoint training. We validated the improvements of READ through feature visualization. Our method is only evaluated in English and can be easily extended to other languages. There have been a few works (Dong et al., 2019; Li et al., 2021) proposed to improve the text generation quality by utilizing pre-trained transformer models. We plan to extend our approach by integrating these generation methods to improve performance further."}]}