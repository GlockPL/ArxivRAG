{"title": "One-shot Federated Learning Methods: A Practical Guide", "authors": ["Xiang Liu", "Zhenheng Tang", "Xia Li", "Yijun Song", "Sijie Ji", "Zemin Liu", "Bo Han", "Linshan Jiang", "Jialin Li"], "abstract": "One-shot Federated Learning (OFL) is a distributed machine learning paradigm that constrains client- server communication to a single round, addressing privacy and communication overhead issues associ- ated with multiple rounds of data exchange in tradi- tional Federated Learning (FL). OFL demonstrates the practical potential for integration with future approaches that require collaborative training mod- els, such as large language models (LLMs). How- ever, current OFL methods face two major chal- lenges: data heterogeneity and model heterogene- ity, which result in subpar performance compared to conventional FL methods. Worse still, despite numerous studies addressing these limitations, a comprehensive summary is still lacking. To address these gaps, this paper presents a systematic analy- sis of the challenges faced by OFL and thoroughly reviews the current methods. We also offer an inno- vative categorization method and analyze the trade- offs of various techniques. Additionally, we dis- cuss the most promising future directions and the technologies that should be integrated into the OFL field. This work aims to provide guidance and in- sights for future research.", "sections": [{"title": "1 Introduction", "content": "Federated Learning (FL) [McMahan et al., 2016] is an es- tablished paradigm that fosters multiple clients to collab- oratively engage in distributed machine learning, allow- ing the aggregation of local models on a server to gen- erate a global model. In practice, current implementa- tions of FL require multiple rounds of data exchange be- tween clients and the server to ensure the training of a high-accuracy global model without the need to directly utilize raw data. However, as FL is widely employed in fields such as intelligent transportation, economy, manufac- turing and healthcare [Kairouz et al., 2021], the traditional multi-round setup can still violate privacy-preserving princi- ples [Kairouz et al., 2021] and incur significant communica- tion overhead [Wang et al., 2023; Tang et al., 2024b]. To protect data privacy, researchers have proposed three main techniques to address privacy leakage in FL: Differen- tial Privacy (DP) [Dwork, 2011], Secure Multi-Party Com- putation (SMPC) [Bonawitz et al., 2017], and Homomorphic Encryption (HE) [Cheon et al., 2017]. DP-based methods in- troduce noise into the intermediate data before sharing to prevent privacy leakage, but this can reduce model accu- racy. SMPC-based methods increase network communication overhead between participants, especially when scaling to a large number of clients. HE-based models are criticized for requiring substantial computational resources. While these methods address privacy-preserving issues, they do not tackle communication overhead and can introduce additional issues. Consequently, communication between server and clients is limited to a single round, as in one-shot Federated Learn- ing (OFL) [Guha et al., 2019]. OFL methods have been specifically designed to improve accuracy in one-shot sce- narios. OFL not only reduces the burden of communication transmission but also potentially achieves even stronger se- curity due to its single-round setting when integrated with existing privacy-preserving methods. Additionally, it offers further advantages, such as alleviating the requirements for transmission synchronization [Alemdar et al., 2021]. There- fore, OFL methods have emerged as a promising approach to address these practical issues. Since 2019, a significant number of researchers, recognizing the potential of OFL, have begun to study this area and have already proposed various methods aimed at addressing specific challenges as well as enhancing global model performance on the server side [Tang et al., 2024b; Shen et al., 2025]. However, none of the current papers has effectively sum- marized the challenges within the OFL domain, and most re- searchers have provided inadequate summaries and classifica- tions of OFL-related literature. This may be due to the rapid development of OFL as a practical field, which has yet to receive a thorough overview. Additionally, because current OFL methods often employ multiple techniques simultane-"}, {"title": "2 Challenges in One-shot Federated Learning", "content": "ously, categorization becomes muddled, leading to inconsis- tent and imprecise classification approaches in the literature. Existing survey papers tend to focus on Knowl- edge Distillation [Mora et al., 2024], Label Leak- age [Liu et al., 2024b], or integration with Foundation Models [Woisetschlager and others, 2024] in FL, but none have concentrated on providing a comprehensive overview of the challenging and promising practical meth- ods in OFL. As Large Language Models (LLMs) gain prominence, maintaining data privacy during large-scale model training and reducing communication overhead through collaborative training among multiple clients becomes increasingly important [Tang et al., 2024a; Ye et al., 2024]. Especially with the development of cloud-edge collaborative frameworks [Tang et al., 2024a], OFL may demonstrate even broader applicability alongside LLMs. Thus, we present the first survey paper specifically focused on the OFL domain. Our main contributions are not limited to a standard survey; they are summarized as follows: \u2022 We propose novel taxonomies addressing the challenges faced by OFL and current methods, aiding researchers in better understanding the issues and existing approaches. \u2022 We provide a thorough and detailed overview of exist- ing OFL methods, including their primary designs, un- derlying principles, and advantages and potential draw- backs in addressing specific challenges. This enables re- searchers to gain a deeper understanding of approaches. \u2022 Beyond summarizing existing methods, we discuss our findings, highlight promising future directions, and iden- tify relevant areas, assisting researchers in integrating OFL with practical applications. The paper is organized as follows. Section 2 introduces the fundamental challenges of one-shot Federated Learning. Sec- tion 3 provides a novel taxonomy and discusses in detail the technical aspects of each category within the context of OFL. Section 4 offers a discussion that summarizes our findings and outlines future directions for the development of OFL methods. Finally, Section 5 concludes our paper."}, {"title": "2.1 Problem Formulation", "content": "In federated learning, clients collaboratively train local mod- els using their private local datasets to produce a global model for the server within a distributed fashion. The objective is to minimize the optimization problem:\n$\\min_w F(w) := \\sum_{i\\in[n]} a_i F_i(w_i)$                                                                                                                                                                                 (1)\nwhere n denotes the number of clients, $w_i$ denotes the pa- rameters of local client i, w denotes the parameters of the global model, $a_i$ denotes the proportion of client's private dataset to the entire dataset, F is the loss function. In the traditional FL setting, clients upload their parameters $w_i$ multiple times to get the final w. However, in OFL, they only upload their parameters once."}, {"title": "2.2 Proposed Taxonomy for Challenges", "content": "Besides the privacy issues, OFL methods must improve the global model's test accuracy while addressing two main chal- lenges: (1) Data Heterogeneity, where the private datasets on each client are non-iid (independent and identically dis- tributed); and (2) Model Heterogeneity, as the local models participating in FL often differ from one another."}, {"title": "Data Heterogeneity", "content": "In practice, data heterogeneity arises from the non-IID char- acteristics of the private datasets among clients, which are primarily due to three types of skew [Li et al., 2022]: quan- tity skew, feature skew and label skew. For quantity skew, each client has the same data distribu- tion but with different amounts of data. For feature skew, the classes within different clients' private datasets have differ- ent statistics. For label skew, different clients have varying proportions of data for the same class. In traditional FL, numerous methods have been proposed to handle data heterogeneity, such as Fe- dAvg [McMahan et al., 2016] (averaging parameters from clients), SCAFFOLD [Karimireddy et al., 2020] and MOON [Li et al., 2021a] (measuring the similar- ity and disparity of parameters between parties), Fed- Prox [Li et al., 2020] (adding an L2 regularization term for optimization) and VHL [Tang et al., 2022] (calibrating features via virtual data). These methods work well in non-one-shot scenarios by iteratively updating and integrat- ing the features of client models with the server round by round [Li et al., 2022]. However, in a one-shot setting with non-IID data, the dif- ferences in model data across clients can be substantial, pos- ing new challenges that traditional methods struggle to ad- dress efficiently in a single pass. This challenge is essentially an out-of-distribution (OOD) detection [Yang et al., 2024a] problem. Since models are uploaded only once, if the FL method fails to capture all the hidden dataset statistics in the client parameters effectively, parts of the data sam- ples may be OOD in the global model and remain unrec- ognized [Guha et al., 2019]. In multi-round settings, global models can be finetuned through iterative updates, but OFL methods require more robust learning capabilities to over- come the data heterogeneity challenge without sacrificing ac- curacy."}, {"title": "Model Heterogeneity", "content": "In practice, having all clients and the server share the same model as the global model often fails to meet the diverse needs of all clients, especially since FL is commonly inte- grated with different model architectures within cloud-edge collaborative frameworks or deployed on different edge de- vices, or due to privacy considerations [Zhang et al., 2024a]. Consequently, personalized FL [Smith et al., 2017] was pro- posed to address these concerns, and most of this research is based on the assumption of model homogeneity. Model heterogeneity arises from differences in the com- munication capabilities [Shah and Lau, 2021], computing re- sources [Shin et al., 2024], and model architectures of lo- cal clients [Shen et al., 2025]. Even when clients share the"}, {"title": "3 One-shot Federated Learning Techniques", "content": "Figure 1 illustrates our new taxonomy of current OFL tech- niques. These techniques aim to enhance the final global model's accuracy in light of the two challenges we previ- ously mentioned. From an innovative perspective, we com- prehensively categorize all the current methods into four main groups based on the techniques they employ: Param- eter Learning, Knowledge Distillation, Generative Mod- els, and Ensemble Methods. Besides these four techniques, many methods are hybrid methods, which employ a combi- nation of multiple techniques."}, {"title": "3.1 Parameter Learning", "content": "Methods in the parameter learning category are derived from approaches like FedAvg and tend to directly learn the statis- tical information of local clients' model parameters to deter- mine the global model's parameters."}, {"title": "Clustering Method.", "content": "k-FED [Dennis et al., 2021] employs Lloyd's k-means clustering method to first cluster the in- formation from local models and then uploads the clus- ter means to the server. This aids the server in learn- ing more local information in a one-shot scenario, albeit with increased communication costs."}, {"title": "Regularizer.", "content": "MA- Echo [Su et al., 2023] considers aggregation of local mod- els' parameters layer-wise and uses a unique method by introducing new norms. This approach helps the global model better account for differences among local models."}, {"title": "Conformal Prediction.", "content": "FedCP-QQ [Humbert et al., 2023] utilizes a conformal prediction method, demonstrating that for any distribution, a prediction set with the desired cov- erage can be computed in a single round of communica- tion, thereby improving the global model's performance."}, {"title": "Bayesian Method.", "content": "FedFisher [Jhunjhunwala et al., 2023] is an aggregation method that uses the empirical Fisher infor- mation matrix obtained by layer-wise Laplace approxima- tion. This method first leverages Bayesian techniques to better model the posteriors of local clients and then aggre- gates for the global model. FedLPA [Liu et al., 2024a] em- ploys a similar approach to FedFisher, further directly train- ing the global model's parameters to achieve results. \u03b2- PredBaye [Hasan et al., 2024] first collects parameters from the posteriors of local clients and then introduces a tunable hyper-parameter 3. This parameter interpolates between a mixture and a product of the predictive posteriors by con- sidering merging Gaussians in the predictive space, and ag- gregation is performed based on this approach."}, {"title": "Prototype Learning.", "content": "In prototype learning [Snell et al., 2017], one or more prototypes for each class are calculated using the sam- ples from the training set. These prototypes can be con- sidered as central representations summarizing and repre- senting each class. PNFM [Yurochkin et al., 2019] employs Bayesian Nonparametric Methods for neural matching based on prototype learning. FedMA [Wang et al., 2020] extends the approach from PNFM by moving beyond fully-connected networks. It constructs a shared global model in a hierarchical manner by matching and averaging hidden elements (such as channels in convolutional layers or hidden states in LSTMs) with similar feature extraction characteristics, thus expanding the methodology to CNNs and LSTMs. Since methods with parameter learning techniques are based on parameter optimization, they typically provide the- oretical proofs of convergence. As these methods require uploading local model parameters, they can introduce pri- vacy concerns. As a result, many approaches include ab- lation studies that integrate with Differential Privacy (DP). This category of methods can address the data heterogeneity challenge, potentially improving the global model's accuracy over traditional FL methods. However, due to the inherent limitations in accurately modeling the statistics, there is still a need to explore better approaches for capturing the features of local clients' parameters. Additionally, parameter learning techniques do not address the model heterogeneity challenge. Note that within parameter learning, prototype learning holds the most promise for integration with others."}, {"title": "3.2 Knowledge Distillation", "content": "Knowledge distillation is a compression technique aimed at reducing the model's size and computational demands while maintaining as much accuracy as possible. It has been shown to address OFL problems effectively and is categorized into"}, {"title": "3.3 Generative Models", "content": "Generative models are a widely used class of techniques in machine learning, designed to train synthetic samples that closely resemble the original data, thereby ensuring simi- lar data distributions. Consequently, generative models can"}, {"title": "Embedding/Feature Generation.", "content": "In Fusion Learn- ing [Kasturi et al., 2020], the distribution digest of client data and their local model parameters are sent to the server. The server regenerates dataset based on these distribution digest, integrates parameters from multiple devices to construct a global model, trains the model using the combined dataset. XorMixFL [Shin et al., 2020] assumes both the global server and clients possess labeled samples from a global class. The server collects encoded data samples (embeddings) from other devices to construct the global data samples. It employs the exclusive OR operation (XOR) to protect privacy during the encoding process. FedOCD [Liu et al., 2024c] focuses on cross-domain recommendation and applies local differen- tial privacy for security while processing the generated user embeddings. FedPFT [Beitollahi et al., 2024] utilizes a fea- ture extractor to derive the corresponding Gaussian mixture models (GMMs) to model synthetic sample features, trans- ferring per-client parametric models."}, {"title": "Generative Adversar- ial Networks.", "content": "OSGAN [Kasturi, 2023] further employs Gen- erative Adversarial Networks (GANs) as a generative model to enhance performance. Other uses [Zhang et al., 2022; Diao et al., 2023; Zeng et al., 2024; Dai et al., 2024] of Gen- erative Adversarial Networks, along with additional tech- niques, can be found in Table 1 and will be discussed in detail in Section 3.5."}, {"title": "Variational Autoencoder.", "content": "Re- searchers have further utilized Variational Autoencoders in approaches such as FedCAVE [Heinbaugh et al., 2023] and FedSD2C [Zhang et al., 2024b]."}, {"title": "Stable Diffusion.", "content": "Fed- DEO [Yang et al., 2024c] initially adopts popular diffusion models, transmitting image captions (local descriptions) to better capture the distribution of client local data. This assists the server in generating synthetic data that align more closely with the client's data distribution. Fed- BiP [Chen et al., 2024] further utilizes a better Stable Diffu- sion model, specifically the foundation model (Latent Dif- fusion Model [Rombach et al., 2022]), to improve perfor- mance. FedDISC [Yang et al., 2024b] leverages a pre-trained model CLIP [Radford et al., 2021] with prototype learning. It involves four steps: Prototype Extraction, Pseudo La- beling, Feature Processing, and Image Generation, which lead to improved outcomes."}, {"title": "Model Inversion.", "content": "Fed- MITR [Hao et al., 2025] employs model inversion techniques to obtain statistical information about some private data within local models. Additionally, it uses a vision transformer to perform token relabeling. Based on the above discussion, we find that compared to parameter learning, knowledge distillation and genera- tive models offer advantages beyond higher accuracy when given more computational resources. We observe that both techniques address data and model heterogeneity challenges. Additionally, they enhance security by not directly sharing client parameters. Since both techniques aim to statisti- cally approximate the true local datasets without exposing actual data, as local models also reflect local datasets. Thus, they enhance the global model's capability with similar ap- proaches to learning client-specific features, which enables the global model to train more effectively while mitigating out-of-distribution (OOD) issues. These two techniques also have their differences. Model distillation improves methods by distilling parameter infor- mation from clients rather than using their raw data. In con- trast, generative models enhance learning by generating syn- thetic data that capture the characteristics of clients' private datasets. Although data distillation also emphasizes the data aspect, generative models can address the inefficiencies asso- ciated with knowledge distillation methods, often leading to superior performance. As a result, numerous researchers are actively pursuing advancements in methods that better reflect local data distributions, especially through generative mod- els. This progression is evident as the focus has shifted from GANs to VAEs, then to Stable Diffusion, and finally to in- creasingly sophisticated diffusion models."}, {"title": "3.4 Ensemble Methods", "content": "Ensemble methods represent one of the most straightforward solutions to addressing OFL challenges (data heterogeneity and model heterogeneity). The most na\u00efve ensemble ap- proach involves aggregating global models obtained from lo- cal clients. However, ensemble models can actually be com- bined with a wide range of other techniques. Currently, these combinations are mainly categorized into two types: static and adaptive ensemble methods."}, {"title": "Static.", "content": "Static ensemble methods are typically used to per- form operations like averaging (DeDES [Wang et al., 2023]) or taking the maximum based on the outputs from lo- cal clients. FedTMOS [Qi et al., 2025] enhances this ap- proach by integrating a lightweight reinforcement learning model, Tsetlin Machine, with prototype learning. Each client can train a different Tsetlin Machine for each class. Due to the unique nature of the Tsetlin Machine, which employs Boolean feature representation, it is well-suited for handling sparse or high-dimensional data. This leads to improved results on the server side."}, {"title": "Adaptive.", "content": "FENS [Allouah et al., 2024] balances communication rounds and accuracy by adjusting the ensemble weights based on the similarity between global and local models. From the causality perspective, FuseFL [Tang et al., 2024b] identifies that the performance drop comes from the isolated train- ing problem. Then, FuseFL proposes a bottom-up ap- proach that trains and merges sub-models adaptively to en- hance invariant feature learning, thus alleviating spurious fitting. HPFL [Shen et al., 2025] proposes regarding the personalized modules as hot pluggable plug-ins which can be selected according to the input data during inference. The plug-ins only need to be sent to server once. Mediator [Lai et al., 2025] aggregates different finetuned LLMs that can be trained by different parties with low memory oc- cupation and adaptive routing and compression. Other methods [Guha et al., 2019; Li et al., 2021b; Zhang et al., 2022; Gong et al., 2022; Heinbaugh et al., 2023; Diao et al., 2023; Zeng et al., 2024; Dai et al., 2024] that utilize ensemble techniques are listed in Table 1 and will be discussed in Section 3.5. While ensemble methods directly leverage local models, they can be combined with other techniques to ensure privacy and improve accuracy. We also find that adaptive ensemble methods typically yield better results."}, {"title": "3.5 Hybrid Methods", "content": "As previously mentioned, current papers do not adequately classify methods addressing one-shot Federated Learning. Table 1 illustrates the challenges of classification and high- lights the effectiveness of our unique classification approach. This paper thoroughly examines all current methods of OFL from a technical contribution perspective. A single method can employ multiple techniques. The three techniques that can tackle model heterogeneity-knowledge distillation, gen- erative models, and ensemble methods, are often integrated to enhance performance and privacy. These methods can be combined in pairs or even all three together. Based on this, and following our detailed discussions of each technique, we summarize these methods, providing an in-depth analysis of how their integration enhances performance."}, {"title": "Integrating Generative Models with Ensemble Meth- ods.", "content": "FedOV [Diao et al., 2023] addresses the open-set prob- lem with an OOD perspective, where some classes may be missing on certain clients due to label skew. To handle this, it uses GAN to generate outliers with feature corruption and employs adversarial training techniques. The generated data samples serve as additional unknown classes during local training, and the results are obtained by ensemble methods from local clients. IntactOFL [Zeng et al., 2024] also em- ploys GANs to generate synthetic data samples but adopts a more flexible approach by utilizing a mixture of experts (MoE) model [Jacobs et al., 1991]. This allows for dynamic adjustment of the ensemble weights for each local client, tai-"}, {"title": "Integrating Generative Models with Distillation.", "content": "FedSD2C [Zhang et al., 2024b] begins by using a pretrained Autoencoder to extract distillate synthesis. To reduce com- munication costs and enhance the privacy of the distillates, clients perturb the distillates using the Fourier transform. To ensure that the distillates fully encompass local information, clients perform a V-information based Core-Set selection when utilizing the encoder. Finally, the server collects all the distillates and uses the pretrained autoencoder's decoder to train an accurate global model."}, {"title": "Integrating Ensemble Methods with Distillation.", "content": "One- shot FL [Guha et al., 2019], particularly in the context of semi-supervised learning, leverages distillation to re- duce the size of the global model and enhance privacy guarantees. The approach then applies average ensem- ble methods to distilled local models to achieve favorable results. FedKT [Li et al., 2021b], while using a similar approach, extends previous methods from support vector machines (SVMs). It incorporates a hierarchical knowl- edge transfer framework with a two-tier privacy aggrega- tion of teacher ensembles (PATEs) structure to improve accuracy and introduces the concept of consistent voting to strengthen the ensemble. Both of these methods uti- lize publicly available datasets to further enhance precision. FedKD [Gong et al., 2022] makes use of unlabeled, cross- domain public data and requires transferring products of these. To address privacy concerns, it incorporates a quan- tized and noisy ensemble into local models. Following the ensemble process, it applies distillation to derive the final global model. Notably, by employing a combination of both shared techniques and additional public datasets, the results demonstrate that first ensembling and then distilling yields better outcomes. This is likely because, as mentioned, dis- tillation can be inherently inefficient and may result in some loss of information. Distilling before ensembling might lead to models that are not as accurate prior to aggregation."}, {"title": "Combination with Three Techniques.", "content": "Dense [Zhang et al., 2022] uses an ensemble of local models uploaded by clients to train a GAN-type generator, which then generates synthetic data samples. The knowledge from the ensemble models is distilled into the global model, which is also trained using the synthetic data to enhance the accuracy. Co-Boosting [Dai et al., 2024] employs a similar generator and also follows the process of ensembling before distillation. However, it extends Dense by introducing a reinforcing approach. This method continuously adjusts the weights of the ensemble model's local models based on synthetic data, and through this process, it finetunes over multiple iterations. Similar to FuseFL [Tang et al., 2024b], although model parameters are uploaded only once, the method undergoes several iterations of optimization, achiev- ing high accuracy. Unlike the previous two methods, in FedCVAE [Heinbaugh et al., 2023], each local client trains an Autoencoder and transmits the local label information and decoder. The server then uses the uploaded client decoder parameters and local label distributions to train a server decoder. Ultimately, synthetic samples from the server decoders are used to train the global model. FedCAVE-KD employs knowledge distillation of local decoders to train the server decoder, while FedCAVE-Ens ensembles client decoders and trains the server decoder. To avoid repetition in Section 3.4, we address the privacy issues associated with ensemble methods here, as they often combine with other techniques. For example, using distilla- tion before ensembling can effectively enhance privacy. Con- versely, ensembling first and then distilling can achieve secu- rity by adding noise to the uploaded local models, similar to parameter learning. Therefore, regardless of how these tech- niques are combined, we find they all can ensure privacy."}, {"title": "4 Discussions and Future Directions", "content": "In this section, having discussed all the current techniques, we note that these methods have successfully implemented one-shot learning, reduced communication overhead, and en- sured data privacy. We will summarize the findings based on these methods from the perspective of improving accuracy. Following this, we will present a blueprint for future direc- tions that focuses not only on test accuracy but also considers other aspects, grounded in practical guidance."}, {"title": "4.1 Findings", "content": "Prototype learning has a bright future. Parameter learn- ing faces challenges in addressing model heterogeneity and is generally limited by the method's capacity to fully capture the weights in local parameters and discard spurious correlations caused by data heterogeneity, which can restrict accuracy. Consequently, compared to other methods, parameter learn- ing does not demonstrate significant advantages. However, fine-grained approaches such as prototype learning within pa- rameter learning show substantial potential. They enable fea- ture extraction by class, thereby enriching the global model's information. These approaches can be readily integrated with other methods, as seen with FedTMOS, which employs an ensemble method, and FedDISC, which utilizes generative model diffusion. Thus, employing prototype learning or con- ducting more granular analyses of local models or class fea- tures in OFL holds significant promise. Better Generative Model is needed. Generative models tend to perform better compared to knowledge distillation methods, primarily because they typically operate directly on the data rather than the model. Even when considering data distillation as opposed to model distillation, it becomes challenging to compare with generative methods due to the compression involved in distillation, which limits accuracy. We also observe that the academic community is striving for improved generative models. Therefore, if better generative models become available, they should be utilized in the OFL method. Of course, it is also worth experimenting with im- proved distillation methods for one-shot Federated Learning. Adaptive ensemble method shows great potential. Com- pared to static ensembles, adaptive ensembles achieve bet- ter results. Both FENSE [Allouah et al., 2024] and Co- boosting [Dai et al., 2024] consider the differences between global and local models, allowing for a refinement of ensem- ble weights for local models to enhance accuracy. However,"}, {"title": "4.2 Future Directions", "content": "In addition to the findings we have uncovered that can guide enhancements in the accuracy of OFL methods, this subsec- tion will provide a practical roadmap for the future develop- ment of one-shot Federated Learning."}, {"title": "Data-free Requirement.", "content": "XorMixFL [Shin et al., 2020] and FedCAVE [Heinbaugh et al., 2023] both require the transmission of local clients' data labels, posing a risk of data privacy leakage. On the other hand, methods like One- shot FL [Guha et al., 2019], FedKT [Li et al., 2021b], and FedKD [Gong et al., 2022] depend on addi- tional public datasets; similarly, approaches such as FedPFT [Beitollahi et al., 2024], FedBiP [Chen et al., 2024], and FedDISC [Yang et al., 2024b] utilize foundation models, leveraging extra public datasets as well. However, OFL ideally should be data-free; using additional datasets might lead the global model to learn biased information that does not align with the target dataset, such as certain industrial or biological datasets, which may ultimately reduce accuracy. Therefore, data-free operation should be one of the key objectives for OFL methods."}, {"title": "Scalability for LLMs.", "content": "Training LLMs in FL with multiple rounds requires significant communication costs because of the enormous model sizes [Ye et al., 2024; Tang et al., 2024a]. OFL has significant potential to re- duce this communication cost, thus implementing training or finetuning LLMs across different geo-distributed clients. However, most current experimental approaches for OFL methods primarily focus on models like LeNet, VGGNet, and ResNet, with datasets mostly comprising MNIST, CI- FAR, Tiny ImageNet, or simple medical datasets and etc. There is some work trying to merge different finetuned LLMs [Lai et al., 2025] only once. Besides, state-of-the- art OFL methods can accommodate model heterogene- ity [Li et al., 2021c; Shen et al., 2025], knowledge distilla- tion between heterogeneous LLMs [Gu et al., 2024] and col- laboration between LLMs [Wang et al., 2024a] can be imple- mented in OFL. In the future, the FL community could ex- plore OFL methods to enable more practical training of LLMS with different parties."}, {"title": "Practical Applications.", "content": "Based on the experimental considerations mentioned above, besides the LLM field, these methods should be applicable to more practical ap- plications. For instance, combining them with vertical federated learning (VFL) could result in one-shot VFL methods. The OFL approach can be seen in applica- tions like FedISCA [Kang et al., 2023] with biomedical data, FedD3 [Song et al., 2023] focusing on resource-constrained edge environments, and OFL-W3 [Jiang et al., 2024] inte- grated with blockchain and Web 3.0 technology. Given the one-shot nature of OFL, which eliminates the need for syn- chronization, it can also be widely utilized in cloud-edge col- laborative frameworks. Additionally, there is potential for application in emerging fields like sequential federated learn- ing [Wang et al., 2024b]. These highlight the broad applica- tion prospects of OFL."}, {"title": "Advanced Optimizations.", "content": "Although OFL significantly reduces communication overhead, integrating it with LLMs and cloud-edge collaborative frameworks presents additional challenges. These challenges arise because such models are large and often support numerous services, leading to sub- stantial parameter sizes even in a single round of interaction. Therefore, further optimization is required, such as imple- menting model compression [Hu et al., 2024] or acceleration techniques [Liu and Zeng, 2024]. In summary, while OFL methods demonstrate tremendous practical potential, improvements should not be limited to en- hancing accuracy. Consideration should also be given to the aspects we mentioned. By addressing these considerations and integrating our findings, we outline a clear direction for the future development of OFL methods."}, {"title": "5 Conclusion", "content": "In this paper, we focus on the novel distributed machine learn- ing paradigm, one-shot Federated Learning. We provide a detailed analysis of the challenges faced within one-shot Fed- erated Learning, propose an innovative taxonomy, and thor- oughly discuss the specifics of various methods based on this"}]}