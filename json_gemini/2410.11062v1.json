{"title": "CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel Pruning", "authors": ["Sjoerd Groot", "Qinyu Chen", "Jan C. van Gemert", "Chang Gao"], "abstract": "This paper presents CleanUMamba, a time-domain neural network architecture designed for real-time causal audio denoising directly applied to raw waveforms. CleanUMamba leverages a U-Net encoder-decoder structure, incorporating the Mamba state-space model in the bottleneck layer. By replacing conventional self-attention and LSTM mechanisms with Mamba, our architecture offers superior denoising performance while maintaining a constant memory footprint, enabling streaming operation. To enhance efficiency, we applied structured channel pruning, achieving an 8X reduction in model size without com-promising audio quality. Our model demonstrates strong results in the Interspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMamba achieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and 468M MACs, matching or outperforming larger models in real-time performance. Code will be available at: https://github.com/lab-emi/CleanUMamba\nIndex Terms-deep learning, speech enhancement, audio de-noising, state-space model, convolutional neural network", "sections": [{"title": "I. INTRODUCTION", "content": "Audio denoising, or speech enhancement, removes back-ground noise from speech recordings while preserving quality and intelligibility. This technology is important for appli-cations such as hearing aids, audio calls, and speech recognition systems. Traditional methods like spectral subtraction [1] and Wiener filtering [2] struggle in dynamic environments, especially when noise overlaps with speech.\nDeep neural networks (DNNs) have significantly advanced this field in recent years. Various architectures, including con-volutional neural networks (CNNs) [3]-[6], recurrent neural networks (RNNs) [7], [8], and transformers [9], [10], have been used to enhance speech. Although transformers achieve high-quality results, they are computationally expensive at longer input sequences.\nThe Mamba state space model [11] offers a promising solution for sequence modeling and time series prediction. Mamba enables parallel computation during training and re-current processing during inference without sequence length constraints, making it a strong candidate for audio denoising.\nIn this work, we introduce CleanUMamba, a neural net-work designed for real-time audio denoising that processes raw waveforms. CleanUMamba adopts the U-Net encoder-decoder architecture from [9], replacing self-attention with Mamba state-space blocks, which enables a more compact model with reduced algorithmic latency. Additionally, we implement structured pruning to reduce the model size while maintaining high performance.\nOur main contributions are:\n1) A novel Mamba-based architecture for time-domain speech enhancement with a 4ms real-time algorithmic latency.\n2) A comparative analysis of Mamba, self-attention, LSTM, and Mamba-S4 for audio denoising.\n3) An efficient, structured pruning strategy using periodic calibration of GroupTaylor importance [12]."}, {"title": "II. RELATED WORKS", "content": "Recent research has explored the application of Mamba to audio denoising, with several concurrent works emerging. Many of these studies focus on non-causal speech enhance-ment for offline settings, where the entire audio track is avail-able for processing. Notable works include SPMamba [13], DPMamba [14], Mamba-TasNet [15], and SEMamba [16], which build upon TF-Gridnet [10], Dual-path RNN [17], Conv-TasNet [4], and MP-SENet [18] respectively.\nThese approaches incorporate bidirectional Mamba implemen-tations for efficient global audio processing. TRAMBA [19] enhances a time domain U-Net Temporal FiLM [20] by integrating Mamba in the bottleneck and using self-attention for feature-wise linear modulation. This network is applied to audio super-resolution and reconstruction from bone-conducting microphone and accelerometer data. For long-term streaming applications, oSpatialNet-Mamba [21] extends SpatialNet [22] by replacing self-attention with masked self-attention, Retention, or Mamba. Operating in the time-frequency domain with 2-4 second window sizes, Mamba outperforms other variations. Zhang et al. [23] conduct an ab-lation study comparing different backbones, including Mamba and two bidirectional Mamba implementations, in the time-frequency domain.\nIn contrast to these works, our research focuses on real-time causal speech enhancement in the time domain. By applying Mamba to the U-Net architecture, we reduce computational load compared to DPMamba and Mamba-Tasnet through pro-cessing in a lower-resolution latent space."}, {"title": "III. PROPOSED METHOD", "content": "Audio denoising aims to recover the clean speech signal $x \\in R^T$ from the noisy signal $y = x + v$, where $v$ represents zero-mean noise uncorrelated with $x$. A causal model for denoising reconstructs the clean speech $\\hat{x}_t = f(y_{1:t}) \\approx x_t$ using noisy samples up to time $t$. In real-world applications, a slight look-ahead, such as a delay of 5-6 ms for hearing aids [24], [25], or up to 200 ms for video calls [26], is acceptable. This work uses 16kHz audio with CleanUMamba, achieving an algorithmic delay of 48 ms and 12 ms for 8 and 6 encoder layers, respectively."}, {"title": "B. Network Architecture", "content": "Figure 1 shows the architecture of CleanUMamba, con-sisting of an encoder-decoder structure with E layers. Each encoder layer employs a 1D convolution with a kernel size of 4 and stride of 2, followed by a ReLU activation and a 1x1 convolution with a GLU activation. These layers halve the temporal resolution and include bypass connections to their respective decoder layers. Decoder layers reverse this process using 1D transposed convolutions, doubling the temporal res-olution. The channels per layer start at H = 64 and increase up to a maximum of 768.\nIn the bottleneck, three Mamba blocks perform sequence modeling on the latent representation, which has a model dimension of D = 512. Each Mamba block is preceded by layer normalization and a residual connection, while the inner dimension is set to I = 2048, corresponding to CleanUNet's fully connected layer output [9]. The state-space model uses S = 64 channels."}, {"title": "C. Pruning Pipeline", "content": "Pruning aims to minimize the loss increase per pruned parameter. Let $L(0)$ represent the loss function of the network with parameters $\\theta$, and $\\Delta L$ denote the change in loss due to pruning. The objective is:\n$\\min_{S} \\frac{\\Delta L(S)}{|S|}$                                                                  (1)\nwhere $S$ is the set of parameters to prune, and $|S|$ is its size. Since exact loss sampling for every group is computationally infeasible, the Group Taylor importance metric [12] is used. It estimates the importance of parameter sets with both absolute and squared gradients:\n$I_s = \\sum_{s \\in S} |g_s w_s|$                                                                   (2)\n$I_s = \\sum_{s \\in S} (g_s w_s)^2$                                                                (3)\n$I_s = \\sum_{s \\in S} w_s$                                                                      (4)\nHere, $g_s$ and $w_s$ represent the gradient and weight for group $s$. Gradients are accumulated via backpropagation. The pruning pipeline (Fig. 2) follows a standard train-prune-finetune approach [27], accumulating micro-batch gradients [28]. Pruning targets a percentage of groups, selecting those with the lowest Group Taylor importance, while ensuring compatibility with Mamba's causal convolutions by pruning in multiples of 8 channels.\nThough effective, the Taylor importance metric may over-or under-penalize groups of varying sizes or depths. To address this, periodic calibration is applied by pruning 20% of groups in a layer, measuring the loss increase, and adjusting the global metric accordingly. An exponential moving average filter smooths out any noise, preventing a single outlier from disproportionately affecting the pruning process."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "We evaluated the model using several objective metrics: Perceptual Evaluation of Speech Quality (PESQ) [29], Short-Time Objective Intelligibility (STOI) [30], and the Mean Option Score (MOS) for signal distortion (CSIG), background noise intrusiveness (CBAK), and overall quality (COVL) [31]."}, {"title": "C. Training Setup", "content": "We employed a loss function combining L1 loss and multi-resolution short-time Fourier transform (STFT) loss [33]:\n$L(x, \\hat{x}) = ||x - \\hat{x}|| + STFT(x, \\hat{x})$                                                        (5)\n$STFT(x, \\hat{x}) = \\sum_{i=1}^m ( \\frac{||s(x; \\theta_i) - s(\\hat{x}; \\theta_i)||_F}{||s(x; \\theta_i)||_F} + ||\\log \\frac{s(x; \\theta_i)}{s(\\hat{x}; \\theta_i)}||_1 )$                                              (6)\nSTFT loss was calculated for FFT bins {512, 1024, 2048}, hop sizes {50, 120, 240}, and window lengths {240, 600, 1200}. Both full-band and high-frequency (4kHz-8kHz) STFT losses were tested [9].\nThe network was trained with the ADAM optimizer (learn-ing rate 0.0002, \u03b2\u2081 = 0.9, \u03b22 = 0.999) using a linear warm-up for the first 5% of training followed by cosine decay. PyTorch AMP was applied for mixed-precision training."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "In Table I, we compare the performance of Mamba in the bottleneck layer against Multi-head Attention, Mamba S4, and LSTM. All models use the same encoder-decoder architecture with E = 8 layers, starting with H = 32 channels in the first layer and H = 64 channels in subsequent layers. Each model employs a bottleneck with a model dimension of D = 64.\nFor the multi-head attention variant, 4 heads were used, each with a dimension of 16, followed by an MLP expanding to d_inner = 128, in line with the architecture in [9], but with fewer parameters. The Mamba model uses an inner dimension of d_inner = 128 and a state size S = 16, while Mamba S4 uses the same configuration but replaces the selective state space with the S4 state space [37]. The LSTM model consists of 3 layers with a hidden size of 64.\nAll models were trained on the DNS dataset for 1M iterations with a batch size of 16. The ADAMW optimizer was employed, with a weight decay of 0.1, using the full STFT loss. Autocast was disabled for Mamba S4 due to training errors."}, {"title": "B. CleanUMamba", "content": "In Table II, we present the results of training CleanUMamba with the same procedure as CleanUNet, using N = 3 bottleneck layers and encoder depths of E = 6 and E = 8. When trained with high-band loss, CleanUMamba outperforms both the same-size and larger CleanUNet models. However, under full-band loss, CleanUMamba slightly underperforms compared to CleanUNet.\nThe model with E = 6 performs marginally worse than the E = 8 version but has fewer than 75% of the parameters.\nDespite this, the number of multiply accumulates per second (MAC/s) remains comparable due to the longer input to the bottleneck layer.\nWith E = 8, most of the computations occur in the encoder and decoder, leading to a doubling of MAC/s after 18 minutes, making Mamba's fixed state size less significant. Reducing the encoder depth to E = 6 quadruples the bottleneck sequence length and reduces MAC/s in the encoder-decoder. As a result, after just 80 seconds, MAC/s with multi-head attention doubles compared to Mamba."}, {"title": "C. Different Importance Metrics without Fine-Tuning", "content": "To assess the effectiveness of different importance metrics, the CleanUMamba network trained with high loss was pruned without fine-tuning, as shown in Figure 3. At each pruning step, 24 of the least important groups were removed, with 128 audio samples used for gradient accumulation in the Taylor importance metric. The calibrated runs, denoted C10, recalibrate the importance metric every 10 pruning steps, and the exponential moving average, E5, applies a smoothing factor of 0.5.\nResults indicate that the Group Taylor importance metric effectively reflects global importance. The squared Taylor loss slightly outperforms the absolute Taylor loss at higher pruning levels. In contrast, weight magnitude performs poorly as a global importance metric, with significant performance drops after pruning the first few layers.\nWith calibration, Taylor loss initially shows better results, but stability decreases over time. Filtering the calibration improves quality per parameter, while uncalibrated Taylor loss consistently delivers superior quality per MAC/s."}, {"title": "D. Pruning with Fine-Tuning", "content": "For pruning with fine-tuning, CleanUMamba models with E = 6 and E = 8 encoder layers pre-trained using high loss were pruned using squared Taylor importance. The metric was recalibrated every 20 steps with a smoothing factor of 0.5. At each step, gradients were accumulated over 128 samples, and the model was fine-tuned on 40,960 training samples every 5 steps. The importance pruned was limited to 3e-13, with 4 channels pruned per step once this limit was reached.\nHowever, both E6 and E8 models became under-trained below 6M and 4M parameters, respectively. To address this, models at 200K, 500K, 1M, and 2M parameters were fine-tuned for an additional 100K iterations with a batch size of 16.\nFigure 4 shows the STOI scores during pruning, and Table II presents a subset of the full evaluation. At 3.22M parameters, CleanUMamba matches the performance of DEMUCS with 8X fewer parameters and nearly matches FullSubNet with 2X fewer parameters."}, {"title": "VI. CONCLUSION", "content": "In this study, we introduce CleanUMamba, a real-time speech denoising model that operates in the waveform do-main and investigate its size reduction through pruning. We evaluated the model on the Interspeech 2020 Deep Noise Sup-pression challenge and compared it to the self-attention-based CleanUNet. While both models perform similarly with an encoder depth of 8, Mamba's linear time complexity becomes more advantageous with a reduced depth of 6, achieving a 12ms latency. Our pruning pipeline further reduces model size, achieving performance comparable to LSTM-based DEMUCS with 8x fewer parameters."}]}