{"title": "Agents Thinking Fast and Slow:\nA Talker-Reasoner Architecture", "authors": ["Konstantina Christakopoulou", "Shibl Mourad", "Maja Matari\u0107"], "abstract": "Large language models have enabled agents of all kinds to interact with users\nthrough natural conversation. Consequently, agents now have two jobs: conversing\nand planning/reasoning. Their conversational responses must be informed by all\navailable information, and their actions must help to achieve goals. This dichotomy\nbetween conversing with the user and doing multi-step reasoning and planning\ncan be seen as analogous to the human systems of \"thinking fast and slow\" as\nintroduced by Kahneman [14]. Our approach is comprised of a \"Talker\" agent\n(System 1) that is fast and intuitive, and tasked with synthesizing the conversational\nresponse; and a \"Reasoner\" agent (System 2) that is slower, more deliberative, and\nmore logical, and is tasked with multi-step reasoning and planning, calling tools,\nperforming actions in the world, and thereby producing the new agent state. We\ndescribe the new Talker-Reasoner architecture and discuss its advantages, including\nmodularity and decreased latency. We ground the discussion in the context of a\nsleep coaching agent, in order to demonstrate real-world relevance.", "sections": [{"title": "Introduction", "content": "Humans have the ability to do two very different kinds of thinking. On the one hand, we can form\nsnap judgements, such as reacting to a speeding car or recognizing the emotional cues of an upset\ncoworker. On the other hand, we can solve complicated problems, like planning a vacation and doing\ncomplex multiplications. The well-known behavioral science theory proposes that two different\nsystems drive those abilities: the fast and intuitive System 1 and the slow and deliberative System 2\n[14]. Daniel Kahneman, who introduced the theory, described the two systems for the two modes\nof thinking as follows: \"System 1 operates automatically and quickly, with little or no effort and no\nsense of voluntary control. System 2 allocates attention to the effortful mental activities that demand\nit, including complex computations. It represents the conscious reasoning self that has beliefs, makes\nchoices, and decides what to think about and what to do.\"\nAlthough difficult problems might rely more on System 2 and everyday skills more on System 1,\nmost cognitive processes are a mix of both kinds of reasoning. System 1 continuously generates\nsuggestions for System 2: impressions, intuitions, intentions, and feelings. If endorsed by System 2,\nimpressions and intuitions form the basis of the explicit beliefs of System 2, and intentions turn into\nthe deliberate choices of System 2.\nMany reinforcement learning (RL) problems can also benefit from a similar dual-system approach.\nThe rapid advances in large language models (LLMs) [1, 6, 32] have enabled artificial intelligence\n(AI) agents of all kinds, from AI coding buddies, to tutors and health coaches. These agents are\nexpected to understand the complex patterns of the world via language and potentially perceive other\nheterogeneous multimodal signals, generating impressions, creating coherent patterns of ideas, and\nproducing dialog (with other modalities being actively added). This resembles the coherent-seeking\nSystem 1. On the other hand, AI agents are supposed to perform complex multi-step reasoning, and\nmake decisions that involve calling tools, actively retrieving information from external data sources,\nand solving complex problems. This is similar to the slower and more deliberative System 2."}, {"title": "Related Work", "content": "Large Language Models for Agent Planning. Inspired by the strong emergent capabilities of\nLLMs [6], such as zero-shot prompting [15], in-context learning [5], and complex reasoning [37, 43],\nresearch into LLM-driven agents is receiving a great deal of attention [36, 40, 2, 25, 21, 34, 35, 24, 4].\nThe work most relevant to this paper is on text-based agents [25, 39, 22], although a great deal of\nwork on real-world embodied agents [16] is increasingly relevant as models become truly multimodal\n[36, 2, 34, 19, 27, 30, 28]. ReAct [39] uses chain-of-thought (CoT) prompting [37] and generates\nboth reasoning traces and task-specific actions (e.g., tools to call) with LLMs. Reflexion [26] extends"}, {"title": "The Talker-Reasoner Agent Model", "content": "Before we introduce the dual-system Talker-Reasoner agent framework corresponding to the fast and\nslow thinking respectively (Section 3.2), we start with formalizing a single language-based agent\ncapable of talking and System 1 reasoning, as well as System 2 multi-step reasoning and planning\nuseful for complex problem solving (Section 3.1)."}, {"title": "Single Language-Based Agent Interacting With Humans: Synergizing Talking and\nExtracting Beliefs With Reasoning and Planning", "content": "Let us consider a language-based AI agent that can interact with users through natural language\nconversation to help them accomplish some task. The agent should be capable of multi-step reasoning\nand planning to be able to solve the task and also capable of generating a conversational response to\nthe user. This paradigm of agents reasoning/planning and conversing has become more prevalent as a\nresult of the introduction of large language models [41, 32, 1, 31, 33, 29]. We present a Reinforcement\nLearning (RL) formulation of this talking-and-reasoning paradigm. We also extend the paradigm to\ninclude explicit modeling of the beliefs the agent has about the user, such as the user's motivations,\nemotions, and goals, which guide the talking and reasoning. Figure 2 shows an overview of the\noverall language-based agent interacting with a user, which we will describe in detail in what follows.\nWe formulate the language-based agent that can reason, talk, and do explicit belief modeling in\na partially-observable RL framework [29]. The agent is continuously interacting with the world\nE. The world encompasses both the user the agent is interacting with, and the knowledge bases\n(such as the World Wide Web) that allow the agent to retrieve real-world knowledge. The agent\nonly has a partial view of the world, thus formulating beliefs $b \\in B$ about the current state of the\nworld. It can learn more about the user by interacting with them via language (future work will\nadd other modalities). Assume that L represents the language space; the agent receives from the\nuser observations $o \\in O$ which live in the language space $O \\subset L$. Observations can contain both\ninformation and feedback/rewards in natural language (e.g., \"I don't like this\", and \"Can you add\nsomething else to my plan?\"). We formalize this as $\\hat{O} = O \\cup R$, with observations $o \\in L$ and rewards\n$r \\in L$. The observations $\\hat{o} \\in \\hat{O}$ are then used to update both the agent's beliefs and the subsequent\nplanning/reasoning performed by the agent. This can be seen as a form of online policy learning via\nnatural language feedback and relates to natural-language based feedback agents [26].\nWe now focus on the agent's actions $a \\in A$. The agent can (i) formulate thoughts $\\tau \\in L$ around\nactions it can take, and (ii) decide which tools $a \\in A$ to select (e.g., APIs, engines like SEARCH,\nfunctions) to fetch external knowledge\u2014this expands the space of tasks it can accomplish. By\ncombining a series of thoughts and tools/actions, along with the results fetched via the tools, the agent\ncan create a plan $p$ for solving a problem. Furthermore, the agent can (iii) formulate beliefs about the\nuser (and potentially other aspects of the world); thus, another key action is extracting leading to\na new agent belief state. The beliefs are represented as structured language objects living in XML\nor JSON space, with $b \\in SL$ [42, 43], where structured language can be seen as a subset of the $L$\nlanguage space. The belief state could encode the agent's estimate of the user's goals, needs, thoughts,"}, {"title": "Proposed Dual-System Talker-Reasoner Agent Model", "content": "So far, we have formalized an agent that can interact with users to solve tasks via its ability to do\nmulti-step reasoning and planning, talking, and extracting beliefs about the user. However, this\ncan be hard for a single LLM to do, as there are different requirements for talking vs. multi-step\nreasoning/planning and forming beliefs. In what follows, we propose the dual-system architecture,\ninspired by the fast and slow thinking Systems 1 and 2, respectively, consisting of:"}, {"title": "The Talker (Thinking Fast) Agent", "content": "The Talker interacts with the world, including the user, and needs to understand language and the\ninteraction/conversation history, and be able to generate natural human-level language to do the\nneeded talk action. These criteria are met by implementing the Talker agent with a powerful,\nin-context learned [5] language model. Similar to System 1, the Talker strives for coherence, and\nacts as an associative machine. To ensure the coherence of the Talker and a good user experience,\ninstructions $I \\in L$ are given to the language model to follow, encoding the Talker's constitution [3].\nThe Talker also interacts with memory mem to prime its responses with relevant information $X_{mem}$,\nincluding the latest beliefs that have been formed by the Reasoner and stored in mem. At every\ninteraction with the user, the Talker takes the Talk action, and generates a conversational response,\ni.e., utterance u, conditioned on the context c and the instructions 1:\n$U_{(t+1)} \\sim Talker(u|c_{t+1}, I(\\cdot|b_{mem}); \\Phi)$ (1)"}, {"title": "The Reasoner (Thinking Slow) Agent", "content": "The Reasoner agent acts like System 2: it enables complex problem solving, deliberate belief\nforming, and choice making.\nThe Reasoner performs multi-step reasoning and planning, entailing series of calls to various in-context learned [5] or Chain-of-Thought (CoT)-prompted language models [37], and calls to different\ntools [39] or databases [17] for external knowledge fetching. This requires it to synergize reasoning\n(producing thoughts) and acting (calling tools to fetch observations), as in retrieval-augmented or\ntool-enhanced ReAct-type agents [39]. The agent can develop plans (e.g., series of tools to call) and\nreasoning traces to solve complex tasks. It typically decomposes the problem into sub-problems in a\nhierarchical fashion, and tasks each sub-problem to different modules, tools, or LLMs.\nIt also forms beliefs about the state of the world, which can combine multiple intermediate results\nof multi-step reasoning, and extract from past interaction history all interesting facts about the user\nmodel in a structured language object to be stored in mem. This aspect of deliberate belief forming is\nwhat distinguishes the Reasoner from typical ReAct-style agents, as it includes deliberate attempt\nin modeling the world/human, as described in the extract action.\nConcretely, the actions the Reasoner can take are: reason, act, and extract, each resulting in\nthoughts $\\tau \\in L$, intermediate observations as a result of tool use o fetching external knowledge, and\nbeliefs $b \\in SL$ in the form of structured language objects. Thus, the augmented space of actions\nincludes thoughts, tool actions, and belief extractions: $\\hat{A} = A \\cup T \\cup B$. Since the augmented\naction space lives in the unlimited language space, learning a policy is difficult and requires strong\nlanguage priors. Thus, we implement the Reasoner's policy via an in-context learned language\nmodel parameterized by Z. The Reasoner selects an augmented action:\n$\\hat{a} \\sim Reasoner(b, a|C_{Reasoner}; Z)$. (3)"}, {"title": "Evaluation Case Study: Sleep Coaching Agent", "content": "We instantiated and validated the Talker-Reasoner dual-agent architecture in a sleep coach use case:\nan AI language agent interacting with users to provide help with sleeping behaviors and challenges."}, {"title": "Grounding in a Real-World Scenario of AI Coaching for Sleep", "content": "We use this real-world scenario to ground the evaluation of our dual-agent architecture. We chose AI\ncoaching because it requires having a model of the user being coached, using sleep coaching expert\nknowledge to ensure scientifically-supported advice, providing a multi-step coaching plan for the user,\nand being conversational and empathetic much as a human coach would be. This instantiation allows\nus to qualitatively test the planning and reasoning capabilities of the Reasoner and the interactivity of"}, {"title": "Instantiating a Talker-Reasoner Dual-Agent Model for Sleep Coaching", "content": "Sleep Coaching Talker Agent: We encode expert knowledge about sleep obtained from clini-cal experts in a set of instructions I that describe the agent's constitution (e.g., being empathetic,\nconversational, providing accurate advice) and the desired phases of sleep coaching (understand-ing, goal-setting, and coaching-plan) with separate instructions for each: $Z_{understanding}, Z_{goal-setting},$\n$Z_{coaching-plan}$, to guide the Talker through the expert clinically-informed coaching process. We imple-mented the Talker via a Gemini 1.5 Flash [31] model, conditioned on the instructions, the context\nincluding the last user utterance, the interaction history, and the latest available belief state stored in\nmem, as in Equations 1, 2. The model's strong language prior along with set of instructions allows it\nto perform complex pattern understanding and provide thoughtful conversational responses akin to\nSystem 1, as illustrated in Section 4.3.1.\nSleep Coaching Reasoner Agent: The AI Coaching Reasoner explicitly models beliefs about\nthe user. To do so, with clinician expert input, we encoded a JSON/XML schema of the belief,\nincluding fields such as sleep concern, goals, habits, barriers, and sleep environment. The Reasoner\ninfers and updates the schema fields while it performs its multi-step reasoning/planning, thereby\ncreating/updating its belief state. We implemented a hierarchical [36] Reasoner\u2014 at every turn, it\ninfers which of the three discrete coaching phases the user is in, and employs the corresponding of\nthe three mini-Reasoners, each instantiated by a different combination of CoT-prompted Gemini\n1.5 Flash models [31] along with corresponding schemas to extract.\nTalker-Reasoner Coordination: Whether the Talker waits for the Reasoner to finish is deter-mined by the belief in the planning coaching phase the Talker waits, otherwise it does not."}, {"title": "Qualitative Results", "content": ""}, {"title": "Example Conversation", "content": "The following is an example conversation illustrating the interaction between Reasoner and Talker.\nUSER: Hey, I need your help planning re: how to create a relaxing bedtime environment.\n(BELIEF in mem) {updated-context: starting context, updated-title: Sleeping Coaching, coaching-phase: UNDERSTANDING}\nTALKER: Great, let's start working on creating a relaxing bedtime environment for you :-) What's\none thing in your bedroom that might be disrupting your sleep?\nUSER: I think noises and light can be too distracting. Can you help create a plan for me for how\nto eliminate these distractions?\n(BELIEF in mem) unchanged\nTALKER: Absolutely! Here is a plan to eliminate distractions in your bedroom: Noise:\n\u2022 Identify noise sources: Make a list of all the noise sources in your bedroom\n\u2022 Block outside noise: Use blackout curtains to block light and reduce noise; Consider noise-cancelling curtains or soundproofing panels.; Try earplugs or a white noise machine.\nLight:Remember, consistency is key. Stick to this plan every night to create a peaceful and\ndark sleep environment. Is there anything else I can help you with?\nUSER: sure, that's useful. However, I would prefer to take it more step-by-step. And also see\nresources of how to create a serene and relaxing atmosphere in my bedroom.\n(BELIEF in mem): {user context: UserCollectedContextData(primary-sleep-concern: Noise and lightdistractions in the bedroom), coaching phase: CoachingPhase(value=PLANNING)}\nREASONER's PLAN: Certainly! Let's take a more in-depth look at the first step of your plan:\n\u2022 Step: Choose a calming color palette. Description: When selecting colors for your bedroom, optfor soothing, neutral tones like soft blues, greens, or grays. These shades have been scientificallyproven to promote relaxation and tranquility."}, {"title": "Adapting planning from feedback", "content": "The following is an example of how the Reasoner agent adapted its plan based on the feedback\ncollected from user by the Talker agent.\nUSER: thank you, this is quite useful. Could you please add in my plan more steps around any\nrelaxing sounds I should add to my space?\n(BELIEF in mem): unchanged\nPLAN by REASONER: Absolutely! Here's an \"Explore Natural Sounds\" segment of your plan,\nalong with some relevant YouTube content:\n\u2022 Explore Natural Sounds: This segment is designed to help you create a calming and serene\nambience in your space using sounds of nature. By incorporating these sounds, you can promote\nrelaxation and reduce stress. Recommended YouTube videos:\n\u2022 Discover Healing Frequencies: In this segment of your plan, we'll explore the world of theurapeutic\nfrequencies, which are believed to have a profound impact on relaxation and emotional wellbeing.\nYouTube recommendations:"}, {"title": "Discussion", "content": "As mentioned in Section 3, to minimize latency, the Talker uses the latest available belief state b\nfrom memory, rather than waiting for the Reasoner to finish its thinking process. The qualitative\nresults in Section 4.3 illustrate two distinct success and failure modes of this approach:\n\"Intuitive Talker\": The asynchronous approach can be effective for tasks where the Talker is\nsufficient even if it operates with an older belief state. These are typically System 1 tasks. For\nexample, when the coaching phase is \"understanding\", the Talker can successfully carry out the\nconversation without the need for the Reasoner to finish the belief updating.\n\"Snap judgement Talker\": However, the Reasoner must update its belief state before the Talker\nproceeds in complex problem-solving scenarios e.g., when the user is asking for an explicit multi-step plan or for specific resources that require tool calling. In those cases, without waiting for the\nReasoner to finish, the Talker makes snap judgements. We can see some examples of such \"snap\njudgement Talker\" behavior when the belief extracted by the Reasoner does not yet capture the\ncorrect coaching phase, and does not fetch resources. To address this, when the Talker reads that\nthe coaching phase is \"planning\", it is instructed to wait for the Reasoner to finish. This corresponds\nto System 2 taking over and overruling the impulses of System 1.\nFinally, although there is a growing interest in AI agents performing more complex System 2\nreasoning [14], we believe that our work is the first to formalize the duality of System 1 and System\n2 reasoning that our Talker-Reasoner architecture offers."}, {"title": "Conclusions", "content": "This paper introduces the dual-system agent framework as a possible biologically-inspired architecture\nfor foundation-model driven intelligent agents. Inspired by the behavioral science principles behind\nthis framework, directions for future research include deciding when not to probe the Reasoner and\nhow to utilize it in a lower capacity most of the time, when the Talker can handle most situations.\nIdeally, given a user query, the Talker should automatically determine whether it requires System 2\nreasoning, and therefore the Reasoner, or whether it can safely proceed with its System 1 thinking.\nAnother direction is to extend the Talker-Reasoner architecture to multiple Reasoners, each writing\nbelief states to different part of the memory, for different types of reasoning."}]}