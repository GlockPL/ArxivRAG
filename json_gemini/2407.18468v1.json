{"title": "Diffusion-Driven Semantic Communication for Generative Models with Bandwidth Constraints", "authors": ["Lei Guo", "Wei Chen", "Yuxuan Sun", "Bo Ai", "Nikolaos Pappas", "Tony Quek"], "abstract": "Diffusion models have been extensively utilized in AI-generated content (AIGC) in recent years, thanks to the superior generation capabilities. Combining with semantic communications, diffusion models are used for tasks such as denoising, data reconstruction, and content generation, However, existing diffusion-based generative models do not consider the stringent bandwidth limitation, which limits its application in wireless communication. This paper introduces a diffusion-driven semantic communication framework with advanced VAE-based compression for bandwidth-constrained generative model. Our designed architecture utilizes the diffusion model, where the signal transmission process through the wireless channel acts as the forward process in diffusion. To reduce bandwidth requirements, we incorporate a downsampling module and a paired upsampling module based on a variational auto-encoder with reparameterization at the receiver to ensure that the recovered features conform to the Gaussian distribution. Furthermore, we derive the loss function for our proposed system and evaluate its performance through comprehensive experiments. Our experimental results demonstrate significant improvements in pixel-level metrics such as peak signal to noise ratio (PSNR) and semantic metrics like learned perceptual image patch similarity (LPIPS). These enhancements are more profound regarding the compression rates and SNR compared to deep joint source-channel coding (DJSCC).", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning-based semantic communication has recently demonstrated significant advantages for next-generation wireless communications, particularly in task-oriented communication systems designed to perform specific tasks or achieve particular objectives by extracting and transmitting only the relevant semantic information necessary. This approach has been applied to images [1]-[3], texts [4], and speech signals [5]. Neural networks can effectively extract useful information and handle complex data by learning the underlying features of the data. Numerous works have focused on improving communication efficiency. For example, deep joint source-channel coding (DJSCC) [6] merges source coding and channel coding into a single optimization problem with the goal of minimizing the overall transmission rate required to achieve a specific level of reliability. Compared to traditional wireless communication systems, semantic communication focuses on transmitting information in the semantic domain. Its primary goal is to ensure that the intended information of the message is accurately conveyed to the receiver.\nGenerative AI [7], [8] is becoming an essential application at the wireless edge. The generation models in generative AI have applications across various domains, including image, text, and audio generation. In image generation, these models can create new images based on given prompts or conditional inputs, effectively mapping latent features extracted by the model, to images. In wireless networks, image generation plays a crucial role in applications such as augmented reality (AR), virtual reality (VR), and mobile gaming through high-quality real-time visuals. Efficient image generation at the resource-limited devices reduces bandwidth usage and latency, vital for advanced applications like remote education and smart city surveillance. In semantic communication, the model leverages the advantages of semantic extraction and representation to transmit information at the semantic level. This process can also be seen as a conditional generation task under given channels and image features, i.e., generating corresponding images based on given conditions. Generative models over wireless networks benefit from semantic communication by focusing on transmitting essential features or information instead of raw data, which reduces data volume and enhances bandwidth efficiency. This approach also decreases latency, making it suitable for real-time applications, and improves robustness against noise and signal degradation.\nSeveral works aim to leverage the semantic extraction capabilities of the generative models for semantic communication. Human visual perception in semantic communication is explored in [9], where generative adversarial networks (GANs) capture global semantic information and local textures, producing images resembling human visual perception. Additionally, GenerativeJSCC, introduced in [10], utilizes Style-GAN [11] to enhance quality in edge cases. The lightweight GenerativeJSCC is proposed in [12], reducing the computation of the generation network while maintaining performance. A novel discrete variational auto-encoder (VAE) model is proposed in [13] for semantic communication, while an adaptive rate transmission based on VAE is further explored in [14].\nRecently, more work has focused on diffusion models for reducing the impact of wireless noise. The diffusion model [15], [16] adds Gaussian white noise during the forward process and gradually denoises the image during the backward process. This process can fit the communication system in an Additive White Gaussian Noise (AWGN) channel. For example, in [17] and [18], the diffusion model is employed to remove channel noise and improve reconstruction quality. The denoising diffusion probabilistic model (DDPM) is further applied in [19] for probabilistic constellation shaping in wireless communications. To address perception distortion in finite block lengths, diffusion is integrated into DJSCC in [20]. Moreover, some works use degraded signals based on diffusion models for semantic communication. A hybrid joint source-channel coding (JSCC) scheme is proposed in [21], where conventional digital communication with the compressed image is complemented with a generative refinement component to enhance the perceptual quality of reconstruction. Invertible Neural Networks are proposed in [22], where the signal is decomposed at the transmitter and is estimated from the degraded part using the diffusion models to recover high-quality source images under extreme conditions at the receiver. The diffusion models are used to denoise in wireless communication. However, current diffusion models overlook bandwidth constraints, focusing on denoising on the original image size or features in image processing and generation tasks.\nThere are several works exploring bandwidth-compression semantic communication. Some of them focus on VAE and diffusion models for compression. The latent variable model with a quantization-aware posterior and prior is designed in [23], [24] for lossy image compression, where a hierarchical VAE architecture is integrated. Several works focus on specific tasks using VAE compression, combining the VAE decoder and task-oriented detector into a compressed task detector. A bridge network is proposed in [25] to adopt for learning a compact representation. A VAE-based joint compression and classification model is proposed in [26], which enables learning on the latent feature space to efficiently encode/compress and effectively classify images through end-to-end training. There are also several research works that explore compression methods based on diffusion models. An additional latent variable is utilized as a condition on the reverse diffusion process in [27]. A novel diffusion-driven image compression framework with a privileged end-to-end decoder is proposed in [28], where the privileged decoder helps correct the sampling process with only a few bits to achieve better reconstruction. Stable diffusion is proposed in [16], where the latent space is applied with cross-attention layers to facilitate diffusion model training on limited computational resources. These compression works based on VAE and diffusion focus on reducing computational resources. In addition, there are other works leveraging unstructured data with graph neural network (GNN) [29], [30] to explore bandwidth compression. For example, in [31], the modulated symbols of users are treated as nodes and applied to a graph neural network to mitigate multi-user interference and reduce the bandwidth required for transmission while achieving the desired classification accuracy. Additionally, a pragmatic semantic communication framework based on GNNs is proposed in [32], where a semantic feedback level is introduced to provide information on the perceived semantic effectiveness with minimal overhead.\nThis paper introduces a diffusion-driven semantic communication framework with advanced VAE-based compression for bandwidth-constrained generative model. We design an architecture that utilizes the diffusion model, where the signal transmission process through the channel acts as the forward process in diffusion. To reduce bandwidth requirements, we incorporate a downsampling module and a paired VAE-based upsampling module with reparameterization at the receiver to ensure that the recovered features conform to the Gaussian distribution. We use the Stable Diffusion model to effectively enhance information transmission by integrating compression in the proposed bandwidth-constrained semantic communication. Our contributions can be summarized as follows:\n\u2022 We propose a communication-efficient generative semantic communication system, which incorporates the forward process of diffusion and channel noise, mapping the channel noise into the T-th forward processes in diffusion, adaptable to different signal to noise ratios (SNRs) of the wireless channel. At the receiver, we effectively utilize the diffusion model of the reverse process to remove the channel noise.\n\u2022 To reduce bandwidth requirements, we incorporate a downsampling module and a paired upsampling module to restore the signal at the receiver. Considering the potential distortion of Gaussian distribution, we introduce a VAE-based upsampling network with reparameterization at the receiver. Leveraging the reparameterization of the VAE ensures that the recovered features adhere to the Gaussian distribution necessary for the diffusion process.\n\u2022 To further enhance the feature extraction capability of the downsampling module and the recovery capability of the upsampling module, we integrate a guidance approach within this architecture that learns from the distribution of generators without bandwidth compression. Additionally, we introduce a comprehensive loss function that combines VAE-based compression and guidance. This loss function integrates contributions from both the VAE loss and the guidance loss, utilizing Kullback-Leibler (KL) divergence to effectively align the distributions of networks employing paired downsampling and upsampling modules with those lacking such modules.\n\u2022 Our experimental results demonstrate the effectiveness of the proposed architecture under bandwidth constraints. Our approach exhibits substantial improve-"}, {"title": "II. SYSTEM MODEL OF DIFFUSION-DRIVEN COMMUNICATION-EFFICIENT SEMANTIC COMMUNICATION SYSTEM", "content": "In this section, we introduce the proposed communication-efficient generative semantic communication system based on diffusion and bandwidth compression. This framework benefits from the semantic extraction capability of Stable Diffusion [16] and the efficiency of bandwidth compression to enhance the overall communication process. The proposed generative semantic communication system based on a diffusion model, is depicted in Fig. 1. Our focus is on image generation over an AWGN channel. Let $x \\in C^{3WH}$ represents the vector reshaped from the input image, where W and H denote the width and height of the image, respectively. The semantic encoder transforms the input into latent features, which can be represented as:\n$y = E(x; \\theta),$ (1)\nwhere $y \\in C^{hwc}$ is the vector reshaped from semantic feature. $E(\\cdot; \\theta)$ denotes the semantic encoder with the parameter $\\theta$. To reduce the bandwidth for transmitting semantic features, we design a low-complexity downsampling network, which can be represented as:\n$z = F_d(y; \\psi),$ (2)\nwhere $z \\in C^{kwhc}$ is the transmitted complex-valued vector reshaped from semantic feature, with $k \\in (0, 1)$ being the compression factor. $F_d(\\cdot; \\psi)$ denotes the downsampling network with the parameter $\\psi$. The compression feature is transmitted over the channel, which is expressed as:\n$\\hat{z} = z + n_o,$ (3)\nwhere $\\hat{z}$ is the vector reshaped from received feature with channel noise following Gaussian distribution with variance $\\sigma^2$, i.e., $n_o \\sim N(0, \\sigma^2 \\cdot 1)$. 0 and 1 are vectors where each element equals zero and one, respectively. The transmission process can be represented as a conditional probability distribution, which follows a Gaussian distribution, i.e., $p_c(\\hat{z} | z) \\sim N(z, \\sigma^2 \\cdot 1)$.\nAt the receiver, the compression feature is recovered by the paired upsampling network. Additionally, to ensure the recovered features follow a Gaussian distribution, we utilize the VAE-based upsampling network to reconstruct the features, which can be represented as:\n$(\\mu_\\psi, \\sigma_\\psi) = F_u(\\hat{z}, SNR; \\omega),$ (4)\n$\\hat{y} = \\mu_\\psi + \\sigma_\\psi \\epsilon_y,$ (5)\nwhere $\\mu_\\psi, \\sigma_\\psi, \\epsilon_y \\in C^{whc}, \\epsilon_y \\sim N(0, 1)$. $F_u(\\cdot; \\omega)$ denotes the upsampling network with the parameter $\\omega$. Since the reconstructed semantic feature is influenced by the noise of the wireless channel, we also incorporate the SNR as an input. We assume $\\sigma^2$ is known at the the receiver, and $SNR = \\frac{\\|z\\|_2^2}{kwch \\sigma^2}$. $Y \\sim N(\\mu_\\psi, \\sigma_\\psi^2)$ is the vector reshaped estimated semantic feature, which is denoised by the reverse process of diffusion with the diffusion parameter $\\phi$, i.e., $p_\\phi(y' | \\hat{y}, \\sigma^2, \\phi)$. The decoder transforms the reshaped vector $y'$ from diffusion into the image, which can be represented as:\n$\\hat{x} = D(y'; \\delta),$ (6)\nwhere $D(\\cdot; \\delta)$ denotes the semantic decoder with the parameter $\\delta$. In our framework, the parameters of encoder $\\theta$, diffusion $\\phi$ and decoder $\\delta$ are utilized in the same as those of the Stable Diffusion [16]."}, {"title": "III. DESIGN OF ADAPTIVE FORWARD PROCESS BASED ON DIFFUSION", "content": "In this section, we introduce the semantic encoder and decoder based on the adaptive forward process of diffusion, as shown in Fig. 2. In this design, we map the signal transmission process through the channel to the forward process of diffusion and utilize its reverse process to eliminate the noise. Additionally, this design serves as a crucial component in training the proposed framework with compression generator by providing valuable information and guidance.\nA. Channel as Part of the Forward Process\nFor diffusion, the forward process involves gradually adding increasing levels of Gaussian noise to the data with a decreasing variance schedule $\\alpha_1, \\alpha_2, ..., \\alpha_T$, which can be expressed as:\n$p (y_T | y_{T-1}) \\sim N(\\sqrt{\\alpha_T} \\cdot y_{T-1}, \\sqrt{1 - \\alpha_T} \\cdot 1),$ (8)\n$p (y_T | y_0) \\sim N(\\sqrt{\\overline{\\alpha}_T} \\cdot y_0, \\sqrt{1 - \\overline{\\alpha}_T} \\cdot 1),$ (9)\nwhere $\\overline{\\alpha}_T = \\prod_{t=1}^T \\alpha_t$, $y_0 = y$. $y_T$ can be sampled directly from the conditional probability, that is:\n$y_T = \\sqrt{\\overline{\\alpha}_T} y_0 + \\sqrt{1 - \\overline{\\alpha}_T} \\cdot \\epsilon_T,$ (10)\nwhere $\\epsilon_T \\sim N(0, 1)$. Note that $\\overline{\\alpha}_T$ is a hyperparameter that has been given in the diffusion-driven generation model. The reverse process of diffusion involves reconstructing the distribution of $y_{T-1}$ from $y_T$. The deep learning network with parameters $\\phi$, such as U-Net, is employed to predict the posterior distribution, which can be represented as:\n$p_\\phi (y_{T-1} | y_T) \\sim N (\\mu_\\phi (y_T, T), \\Sigma_\\phi (\\psi_T, T)),$ (11)\n$\\mu_\\phi (y_T, T) = \\frac{1}{\\sqrt{\\alpha_T}} (\\frac{y_T}{\\sqrt{\\overline{\\alpha}_T}} - \\frac{1 - \\alpha_T}{\\sqrt{1 - \\overline{\\alpha}_T}} \\epsilon_\\phi (y_T, T)),$ (12)\n$\\Sigma_\\phi (\\psi_T, T) = \\frac{(1 - \\overline{\\alpha}_{T-1}) (1 - \\alpha_T)}{1 - \\overline{\\alpha}_T} \\cdot 1,$ (13)\nwhere $\\epsilon_\\phi (y_T, T)$ is the predicted distribution of noise by the network given $y_T$ and T.\nAs shown in Fig. 2, when the signal y is transmitted over the channel with the Gaussian noise of variance $\\sigma^2$, it also can be represented as $\\hat{y} = y + \\sigma \\cdot \\epsilon, \\epsilon \\sim N(0, 1)$. The received signal follows a Gaussian distribution with mean y and variance $\\sigma^2 \\cdot 1$. The transmission process through the wireless channel can be transformed into a forward process of diffusion. At the receiver, the reverse process of diffusion can be used to eliminate the noise and recover the signal.\nDuring the forward process of diffusion, a fixed step of Gaussian noise is added, typically around 200 steps in stable diffusion in the LSUN dataset, corresponding to an SNR of approximately -1dB. To simulate the noise in the forward process, the received feature with channel noise might be compensated by the remaining noise required in the forward process from the receiver, when the SNR of the channel is greater than -1dB. Under such circumstances, the impact of channel noise on the signal can be reduced as much as possible. To align the noise of the forward process and"}, {"title": "IV. THE DETAILS OF THE DIFFUSION-DRIVEN SEMANTIC COMMUNICATION SYSTEM WITH VAE-BASED COMPRESSION", "content": "In this section, we present a diffusion-driven semantic communication framework enhanced by VAE-based compression. To ensure the Gaussian distribution of features, we employ VAE-based reparameterization for feature reconstruction. Additionally, we introduce guidance to focus on training the paired upsampling and downsampling networks to reduce training costs. The generator integrated with compression leverages guidance from the distribution of uncompressed generator, enabling the efficient generation of corresponding images with reduced bandwidth. Fig. 3 illustrates the framework for detailed semantic communication with compression and guidance.\nA. Compression and Reconstruction by Reparameterization\nIn this section, we introduce the details of the VAE-based compression module aimed at mitigating bandwidth requirements by reducing data volume at the transmitter. This is complemented by a corresponding reconstruction module at the receiver to recover information lost during compression. To address potential information loss caused by non-linear operations in the compression and upsampling modules, we propose a novel compression and reconstruction method through reparameterization. This method helps maintain the Gaussian distribution of the latent feature, as shown in Fig. 4. This approach ensures that the output maintains a Gaussian distribution. The upsampling module, based on VAE [33], is employed to reconstruct the transmitted information while preserving its distribution.\nFor the compression module, acting as a downsampling network, the semantic feature z with reduced data volume is transmitted. To avoid the computational burden caused by complex network, we design a low-complexity downsampling network consisting of two residual blocks to compress the semantic information. After transmission through the wireless channel, the received semantic feature is reconstructed by an upsampling network based on VAE to produce the mean features $\\mu_y$ and variance features $\\sigma_y$ of $y$. Since the variance $\\sigma_y$ is particularly affected by the noise in the channel, we incorporate the SNR of the channel as an input to the output variance network. This integration of SNR enables the network to dynamically adjust the output variance based on the noise level in the channel, thereby improving its adaptability to diverse channel conditions. Consequently, this approach enhances the fidelity of semantic reconstruction and strengthens the resilience of system to noise across varying channel conditions. The reconstruction module can be represented as:\n$\\hat{y} = F_u (\\hat{z}, SNR; \\omega),$ (18)\nThe upsampling network with parameter $\\omega$ is also designed to consist of two residual blocks to avoid the computational load. The reconstructed feature $\\hat{y}$ is expected to have a similar distribution to the features of the generator"}, {"title": "B. Training with Guidance", "content": "To learn the distribution of the uncompressed generator, we employ a method similar to distillation to ensure that the reconstructed feature closely aligns with that of the uncompressed generator. The output distribution of the $\\hat{s}$ serves as the target distribution to guide the $\\hat{y}$ of the compressed generator, making the output distribution of the $\\hat{y}$ as close as possible to minimize performance loss due to bandwidth compression. This alignment is designed through the guided objective $L_g$, which minimizes the KL divergence between $\\hat{y}$ and $\\hat{s}$. By effectively learning from the distribution of $\\hat{s}$, the compressed generator can learn its representations accordingly. $L_g$ ensures efficient compression while preserving the essential the feature distribution of $\\hat{s}$, which can be represented as:\n$L_g = D_{KL} (P_c (\\hat{s} | Y) ||P_\\omega (\\hat{y} | z)),$ (19)\nwhere $D_{KL} (\\cdot || \\cdot)$ denotes the KL divergence between the two distributions. $p_\\omega (\\hat{y} | z)$ denotes the conditional probability from the upsampling module based on reparameterization."}, {"title": "C. The Hybrid Loss from Reparameterization and Guidance", "content": "In the described system, the hybrid loss function is composed of two key components. The first component is the modeling loss, derived from the reparameterization of the VAE. This loss measures the discrepancy between the input data and its reconstruction by the VAE, considering the stochastic nature of the latent space. The second component is the guidance loss, as mentioned earlier. It aims to align the distribution of the compressed generator with that of the uncompressed generator. This loss encourages the compressed generator to learn from the distribution of the uncompressed generator, with the objective of mitigating errors introduced due to bandwidth compression during the transmission. The overall loss function for the compression diffusion model within guidance can be expressed as follows:\n$L(\\psi, \\omega, \\phi) = L_v + \\gamma L_g,$ (21)\nwhere $L_v$ represents the modeling loss function based on VAE, and $\\gamma$ is a hyperparameter that controls the balance between the two components.\nAdditionally, we regard the supplementary downsampling and upsampling networks before reparameterization as the VAE-based encoder. Since both the reverse process of diffusion and the VAE-based decoder offer posterior probability distributions, the reverse process of diffusion is utilized as the VAE-based decoder in this system. According to the VAE, the loss function $L_v$ can be expressed as:\n$L_v = \\lambda D_{KL} (q_{\\psi, \\omega} (\\hat{Y} | Y) ||P_\\phi (Y)) + E [(y'-y)^2],$ (22)\nwhere $q_{\\psi, \\omega} (\\hat{Y} | Y)$ represents a prior probability provided by the supplementary downsampling and upsampling networks acting as the VAE-based encoder. The distribution $p_{\\phi}(Y)$ follows a standard Gaussian distribution $N(0, 1)$ as [33]. The hyperparameter $\\lambda$ adjusts the balance between the first and second terms of the loss function $L_v$.\nCorollary 2: The first term of the loss function $L_v$, which is the KL divergence between two Gaussian distributions, $q_{\\psi, \\omega} (\\hat{Y} | Y) \\sim N (\\mu_\\psi, \\sigma^2)$ and $p_{\\phi}(Y) \\sim N(0, 1)$, can be expressed as:\n$D_{KL} (q_{\\psi, \\omega} (\\hat{Y} | Y) ||P_\\phi (Y)) = \\frac{1}{2} (\\mu_{\\psi}^2 + \\sigma^2 - log(\\sigma^2) - 1).$ (23)\nProof: The proof is along the lines of the proof in Appendix A and is therefore omitted..\nCorollary 3: $E [(y' - y)^2]$, which is the second term of the loss function $L_v$, can be expressed as:\n$E [(y' - y)^2] = E [(y - \\hat{y})^2] + C.$ (24)\nProof: The proof is given in Appendix B.\nIn summary, the hybrid loss function of our system can be represented as:\n$L(\\psi, \\omega, \\phi)\n= \\lambda D_{KL} (q_{\\psi, \\omega} (\\hat{Y} | Y) ||P_\\phi (Y)) + E [(y - \\hat{y})^2]\n+ \\gamma D_{KL} (P_c (\\hat{s} | Y) ||q_{\\psi, \\omega} (\\hat{Y} | Y))\n= \\frac{\\lambda}{2} (\\mu_{\\psi}^2 + \\sigma^2 - log(\\sigma^2) - 1) + E [(y - \\hat{y})^2]\n+ \\gamma log(\\frac{\\sigma^2}{\\sigma_y}) + \\frac{\\sigma_{.1}^2 + (\\mu - \\psi)^2}{2 \\sigma^2} - \\frac{1}{2}.$ (25)\nThis hybrid loss function integrates components that ensure compression and reconstruction. The term of $L_v$ addresses the reconstruction through two primary components: the KL divergence between the distributions as well as the"}, {"title": "V. RESULTS", "content": "In this section, we present experimental results to validate the effectiveness of the proposed semantic communication generation with compression and guidance. The uncompressed generator employed in our approach is the stable-diffusion-v2 [16]. Our method is evaluated on the LSUN-Churches and LSUN-Bedrooms datasets [34]. We configure the number of diffusion timesteps to $T = 1000$ with a linear schedule, set the learning rate to $1 \\times 10^{-4}$, and use the training batch size of 16. In addition, we also visualized the semantic distinctions between the generated images and real images. This helps to provide a more intuitive understanding of the effectiveness of generating semantic communication based diffusion. The simulations are conducted on the computer equipped with an Intel Xeon Silver 4110 CPU @ 2.10GHz and an NVIDIA RTX A40 GPU.\nA. The Metrics\nThe performance is evaluated in terms of details transmission and semantic transmission using multiple metrics such as peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) [35], learned perceptual image patch similarity (LPIPS) [36], Frechet Inception Distance (FID) [37] and CLIP-score [38]. The quality of transmission is traditionally evaluated using distortion measures such as the PSNR and SSIM, which focus on pixel-level evaluation. PSNR emphasizes pixel-level errors, while SSIM considers the overall structural similarity of the pixel. Both metrics focus on evaluating the details of the images.\nIn semantic communication, the evaluation of transmission quality revolves around how effectively the intended information is conveyed to the receiver. This necessitates a more comprehensive evaluation metric that considers not only pixel-level details but also the overall structure and content of the image.\nThe LPIPS metric quantifies the similarity between two images based on human perception. It considers various factors such as texture, color, and structure, providing a more accurate assessment of perceptual quality compared to PSNR and SSIM. Additionally, metrics like FID and CLIP-score are used to evaluate the quality of generated images. FID measures the distance between the feature space distributions of generated and real images, focusing on global features. On the other hand, CLIP-score measures semantic similarity using a pre-trained model, focusing on the semantic content of the images. FID is more focused on the overall structure and texture of the image, while CLIP-score prioritizes semantic similarity. Thus, using both metrics together can"}, {"title": "B. Performance of Channel Integration and Adaptation", "content": "The noise from the wireless channel is integrated into the forward process to mitigate its impact. At the receiver, the compensated noise is taken into account, utilizing the channel noise to complete the fixed 200 steps of the forward process. This suggests that integrating the channel into diffusion-driven semantic communication eliminates the influence of the channel on the system."}, {"title": "C. Performance of Compression with Different Bandwidths", "content": "In the uncompressed generator, the noise from the wireless channel is treated as a complete forward process to adapt to different SNR conditions. We also evaluate the generation performance on the LSUN Bedrooms and LSUN Churches datasets. The results are shown in Tables III and IV. The CLIP-score metrics, used for evaluating the semantic content of the images, demonstrate indicate that diffusion-driven semantic communication ensures accurate transmission of semantic information across various SNR levels. However,"}, {"title": "D. Improvement from Reparameterization and Guidance", "content": "Guidance provides auxiliary information derived from the uncompressed generator, which guides the upsampling network to learn the distribution of the uncompressed generator, directing the output of the upsampling model towards more accurate distributions. In this subsection, we analyze the effects of guidance on enhancing the performance of the generation semantic communication based on diffusion. The results in Fig. 7 show improvements for two compression rates, namely C = 14 and C = 4. Compared to DJSCC, the generation semantic communication system demonstrates improvements in two aspects. One part of the improvement stems from maintaining the Gaussian distribution of features through reparameterization, while the other part arises from the guidance provided by the uncompressed generator. In terms of pixel-level metrics such as PSNR and SSIM, the performance improvement mainly comes from reparameterization, while for semantic metrics like LPIPS and CLIP-score, the enhancement primarily comes from guidance. Therefore, overall, both reparameterization and guidance from the uncompressed generator play crucial roles in enhancing transmission performance."}, {"title": "E. Hyperparameters Settings in the Hybrid Loss", "content": "Here, our focus lies on configuring the hyperparameters within the loss function for compression and guidance, which is employed to train the diffusion model. Hyperparameters, predetermined variables set before the learning process begins and remain constant during training, play a pivotal role in balancing the diverse components of the loss function and can significantly impact the capacity of model to learn from data. We investigate various hyperparameter configurations and their implications on performance. To conserve training resources, we subset the training dataset to include only 1000 images and evaluate it on a testing dataset consisting of 300 images. Training for 20 epochs allows us to identify the optimal combination based on selected metrics.\nInitially, we set the hyperparameter $\\gamma$ to 0 to determine the optimal value of the hyperparameter $\\lambda$. Subsequently, after identifying the best value for $\\lambda$, we adjust $\\gamma$ to select the optimal combination. Ultimately selected the parameter combination of $\\gamma = 0.1$ and $\\lambda = 0.1$, which exhibited the best performance."}, {"title": "VI. CONCLUSION", "content": "In this paper, we introduced a novel diffusion-driven semantic communication framework that incorporates advanced VAE-based compression for generative model in bandwidth-constrained environments. By leveraging the diffusion model, where the signal transmission process over the wireless channel serves as the forward diffusion process, our architecture successfully eliminates Gaussian noise from channel while conserving bandwidth, enhancing its applicability in wireless communication scenarios. We integrated a downsampling module and a corresponding VAE-based upsampling module with reparameterization to ensure the recovered features conform to the Gaussian distribution, a prerequisite for the diffusion model. Furthermore, we derived the loss function with the guidance for this system based on the design of reparameterization and compression. Experimental results demonstrate the effectiveness of our approach, with improvements observed in both compression rates and SNR compared to the DJSCC baseline. Specifically, the reparameterization significantly enhances pixel-level performance, while guidance plays a crucial role in improving semantic transmission. This comprehensive approach demonstrates the potential for advancing semantic communication systems, offering enhanced reliability in generation performance."}, {"title": "APPENDIX A", "content": "PROOF OF THEOREM 1\nThe point in the feature maps $\\hat{y}$ and $\\hat{s}$ are considered as a conditional probability, represented by $p_c (\\hat{s} | y) \\sim N (y, \\sigma^2)$ and $p_\\omega (\\hat{y} | z) \\sim N (\\mu_y, \\sigma_y^2)$, respectively. The probability density functions of these two Gaussian distributions are:\n$P(x)_{x\\sim p_c(\\hat{s}|y)} = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} exp(-\\frac{(x - y)^2}{2\\sigma^2}),$ (26)\n$Q(x)_{x\\sim p_\\omega(\\hat{y}|z)} = \\frac{1}{\\sqrt{2\\pi \\sigma_y^2}} exp(-\\frac{(x - \\mu_y)^2}{2\\sigma_y^2}).$ (27)"}, {"title": "APPENDIX B", "content": "PROOF OF THEOREM 2\nTo enhance the stability of the loss function and reduce dependency on the direct outputs of the diffusion model y', we introduce an intermediary s'. This intermediary serves as a bridge between the model's output and the final loss calculation, facilitating more robust training by mitigating the direct impact of fluctuations or noise in the model's predictions. By optimizing the loss function through this intermediary, we effectively decouple the immediate influence of y', allowing for more stable gradient updates. The revised loss function can be expressed as follows:\n$E[(y'-y)^2]\n=E[(y'-\\hat{s}+\\hat{s}-y)^2]\n=E [(y'-\\hat{s})^2 + 2 (y'-\\hat{s}) & (\\hat{s}-y)+(\\hat{s}-y)^2],$ (29)\nwhere y' ~ N(\\mu_{\\phi_{\\hat{y}1}},\\Sigma_{\\phi}), \\hat{s}' ~ N(\\mu_{\\phi_{\\hat{s}1}}, \\Sigma_{\\phi}). Here, $\\hat{y}_t$ and $\\hat{s}_t$ represent the t-th step feature of $\\hat{y}$ and $\\hat{s}$ in the reverse process of diffusion, respectively. Additionally, y' and $\\hat{s}'$ denote the 0-th step feature $y_0$ and $\\hat{s}_0$, respectively. We assume that (y' - $\\hat{s}'$) and ($\\hat{s}'$ - y) are independently and identically distributed (i.i.d.). Thus, Eqn. (29) can be represented as:\n$E [(y' - y)^2]\n=E[(y'-\\hat{s}')^2] + 2E [(y'-\\hat{s}')] E [(\\hat{s}'-y)]+ E [(\\hat{s}'-y)^2]\n=E [(\\mu_{\\phi_{(\\hat{Y}1)}} - \\mu_{\\phi_{(\\hat{s}1)}})^2] + C_2 + 2E [(y'-\\hat{s}')] E [(\\mu_{\\phi_{(\\hat{Y}1)}}-y)] + C1,$ (30)\n$E [(\\hat{s}' - y)^2] = C_1$, since the parameters of the uncompressed generator are frozen. Eqn. (29) can be further expressed as:\n$E [(y' - y)^2]\n=E[(\\mu_{\\phi_{(\\hat{Y}1)}} - y)^2] + C_2\n+ 2E [(y'-\\hat{s}')] E [(\\mu_{\\phi_{(\\hat{Y}1)}}-y)] + E [(\\hat{s}'-y)^2]\n=\\frac{1}{\\alpha_1} E[(\\hat{y}' - \\hat{s}')^2] + C_3\n+ \\frac{2}{\\sqrt{\\alpha_1}} E [\\hat{y}'-\\hat{s}')] E [(\\hat{s}'-y)]+ E [(\\hat{s}'-y)^2]\n=\\frac{1}{\\alpha_1} E[(\\hat{y}' - \\hat{s}')^2] + C_4\n+ \\frac{2}{\\sqrt{\\alpha_1}} E [(\\hat{y}'-\\hat{s}')] E [(\\hat{s}'-y)]+ C1\n= E (y - \\hat{y})^2] + C."}]}