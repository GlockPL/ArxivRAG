{"title": "Humans Continue to Outperform Large Language Models in Complex Clinical Decision-Making: A Study with Medical Calculators", "authors": ["Nicholas Wan", "Qiao Jin", "Joey Chan", "Guangzhi Xiong", "Serina Applebaum", "Aidan Gilson", "Reid McMurry", "R. Andrew Taylor", "Aidong Zhang", "Qingyu Chen", "Zhiyong Lu"], "abstract": "Although large language models (LLMs) have been assessed for general medical knowledge using medical licensing exams, their ability to effectively support clinical decision-making tasks, such as selecting and using medical calculators, remains uncertain. Here, we evaluate the capability of both medical trainees and LLMs to recommend medical calculators in response to various multiple-choice clinical scenarios such as risk stratification, prognosis, and disease diagnosis. We assessed eight LLMs, including open-source, proprietary, and domain-specific models, with 1,009 question-answer pairs across 35 clinical calculators and measured human performance on a subset of 100 questions. While the highest-performing LLM, GPT-40, provided an answer accuracy of 74.3% (CI: 71.5-76.9%), human annotators, on average, outperformed LLMs with an accuracy of 79.5% (CI: 73.5-85.0%). With error analysis showing that the highest-performing LLMs continue to make mistakes in comprehension (56.6%) and calculator knowledge (8.1%), our findings emphasize that humans continue to surpass LLMs on complex clinical tasks such as calculator recommendation.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) such as GPT-4 and Med-PaLM have achieved expert-level performance on biomedical tasks like medical question-answering (QA)1\u20135. While these LLMs have been evaluated using medical licensing exam questions to quantify their general medical knowledge6-8, their capability for promoting real-world clinical decision support (CDS) and going beyond factual recall cannot be fully assessed by medical exams alone. Simultaneously, clinical calculators, i.e. evidence-based computational tools for risk assessment, prognosis, and diagnosis, play an important role in real-world clinical care delivery 10,11 and are becoming increasingly prevalent via web-based platforms like MDCalc12,13. With recent research also showing that LLM agents can engage in tool learning14,15, clinical calculators represent a set of tools that may be used to augment medical LLM agents and their capability to probabilistically reason16. However, the capability of LLMs to reason through complex clinical scenarios and appropriately select calculators for recommendation remains unknown.\nTo address this gap, we conducted the first comprehensive analysis of whether LLMs can recommend clinical calculators, with comparison to human performance, and curated MedQA-Calc, a first-of-its-kind dataset for evaluating the clinical calculator recommendation capabilities of LLMs. To construct MedQA-Calc, we identified 35 popular clinical calculators from MDCalc, excluding questionnaire calculators as questionnaires can rely on subjective inputs not found in patient reports. We then curated questions relating to these calculators using publicly available patient data from case reports in PubMed Central17; this data source, PMC-Patients, covers a broad range of clinical settings (emergency medicine, inpatient care, outpatient clinic, and"}, {"title": null, "content": "surgery) and includes patient cases from both the United States and international populations. Leveraging the case reports, we identified instances of clinical calculator usage within patient notes, truncated notes to remove evidence of calculators at the point of use, and constructed multi-choice questions using the extracted calculators as ground-truth. We curated 1,009 question-answer instances, where each instance contained the truncated patient note, a generalized question for recommending calculators, answer options, and a ground-truth answer."}, {"title": null, "content": "Figure 1 presents the overall study design. Using PMC-Patients as a data source for publicly available patient case reports from PubMed Central, we leveraged GPT-4o and few-shot learning to extract clinical calculators from patient cases (methodology and code detailed at https://github.com/ncbi-nlp/Calculator-Recommendation). We then truncated the extracted texts to remove evidence of calculator usage by implementing a fine-tuned model of GPT-40. Next, we curated questions by randomly incorporating five answer choices from our list of calculators, excluding calculator options that were similar to the ground-truth calculator. For evaluation, we focused on three procedures: (1) LLM performance, where model accuracy in answering questions was assessed; (2) Human performance, where two medical trainees answered a subset of curated questions; and (3) Error analysis, where researchers and medical trainees reviewed the types of errors made by LLMs across the questions used to evaluate human performance."}, {"title": "Results", "content": "answer these questions and recommend calculators was evaluated. c. Researchers and medical trainees reviewed LLM errors and evaluated the quality of questions.\nThe results of the evaluation are shown in Figure 2. Both LLM and human performance were evaluated in \u201cclosed-book\u201d settings as neither evaluation included the use of external tools like literature search engines or web browsing. Eight LLMs were used for evaluation, and two medical trainees were used for human evaluation. In terms of LLM performance, GPT-40 provided the best nominal stand-alone performance; although, GPT-40 performance is not significantly different from Llama-3-70B or GPT-3.5-Turbo performance. Additionally, the low performance of PMC-LLaMa suggests that our curated dataset is affected by minimal data contamination18. With human performance, two medical trainees provided the same answer choices for 69 out of 100 questions. One medical trainee (90.0%, CI: 84.2%-95.9%) significantly outperformed GPT-40 (74%, CI: 65.4%-82.6%) on the subset of questions, and one medical trainee (69.0%, CI: 59.9%-78.1%) matched performance with GPT-40. Thus, LLMs show the potential to provide calculator recommendations on par with humans; however, they have not reached human-level performance, even at the medical training level, for the clinical task of calculator selection. To further investigate errors made by LLMs, we divided LLM errors into four groups: comprehension (misunderstanding or hallucination), calculator knowledge (incorrect calculator usage), alternative choice (choosing a different calculator option that may be appropriate), and no explanation (LLMs lacking sufficient explanation). To assess human error, medical trainees reviewed their errors and"}, {"title": null, "content": "reported agreement with ground-truth answers. Reasons for disagreement included limited patient note vignettes and reasonable alternative answers."}, {"title": "Discussion", "content": "Figure 2c displays the confusion matrices of GPT-40 and medical trainees. Depending on the annotator comparison, GPT-40 correctly answered at least four out of the 100 questions that medical trainees incorrectly answered. Overall, only six out of 100 questions were answered incorrectly by both medical trainees and GPT-40. We next evaluated the performance errors. We found that high-performing models like GPT-40 most commonly made comprehension and alternate solution errors (Fig. 2d). In contrast, high-performing models made proportionally fewer errors due to incorrect calculator knowledge.\nA closer look at GPT-40 explanations during error analysis revealed that advanced models may struggle with making inferences with respect to time. For example, GPT-40 may automatically assume that a patient, who has already received a Sequential Organ Failure Assessment (SOFA) score, does not require an additional SOFA score. Moreover, GPT-40 may fail to recognize the utility of using a calculator score for diagnostic support. If a truncated patient note reveals a respiratory diagnosis, GPT-40 may report that a respiratory risk calculator is not needed as a diagnosis has already been made (LLM explanations and data are publicly available at https://github.com/ncbi-nlp/Calculator-Recommendation). Ultimately, large language models may require further integration with time-series data to provide applicable, real-world clinical decision support."}, {"title": "Limitations and Future Work", "content": "Our study design has several limitations. Firstly, though we have manually evaluated the relevance of each calculator for each instance, clinical calculator selection may not always be necessary given evidence-based practices. In addition, though we designed our study to exclude similar calculators during answer choice curation, multiple distinct medical calculators may be appropriate in clinical practice. For example, the process of truncating patient reports at the first appearance of a given medical calculator can remove mentions of other pertinent calculators within the downstream text. Moreover, the size and scope of the dataset is limited as it uses only 35 medical calculators; thus, we excluded other medical calculators that may have appeared within patient texts. Finally, the PMC-Patients dataset is composed of case reports which may deviate from clinical scenarios where clinical calculators are more commonly applied; such case reports also span multiple countries, where disease burden, documentation, and practice patterns may vary.\nIn future work, we will address these limitations by expanding the coverage of clinical calculators or medical decision-making scenarios and creating more granular patient vignettes, stratified across medical specialties. Additionally, we will consider curating open-ended clinical calculator questions to evaluate the clinical utility of LLMs in medicine. We will also measure the baseline quantitative reasoning of LLMs as done in recent work with MedCalc-Bench, a benchmarking study on the capability of LLMs to perform medical calculations19. Finally, we will further integrate our dataset with LLMs through tool learning, a key feature of language agents. With works like OpenMedCalc and AgentMD using medical calculators to augment LLMs16,20, our dataset of explanations and calculator evidence will better inform the use of language agents and medical calculators."}, {"title": "Conclusion", "content": "In summary, we present the curation and evaluation of question-answer instances relating to medical calculator usage, a complex clinical task. In addition to GPT-40 demonstrating nominally lower multi-choice accuracy as compared to the average performance of medical trainees, large language models continue to show errors in comprehension and calculator knowledge. Despite filtering similar calculators during question-answer curation, our research also identified error cases in which other calculator answers may be appropriate. This suggests that further curation and evaluations in open-answer settings are needed to test the capabilities of LLMs in medical decision-making."}, {"title": "Methods", "content": "We manually reviewed MDCalc in June 2024 to identify 35 medical calculators from MDCalc's internal \u201cPopular\u201d list of calculators, to collect calculator names, and to curate calculator codes (e.g., abbreviating \u201cGlasgow Coma Scale\u201d as \u201cgcs\u201d. While popular calculators involving only questionnaire data were excluded from the evaluation, we gathered calculators across medical domains including cardiology, neurology, pulmonology, critical care, and surgery. Next, we extracted patient summary information from PMC-Patients, a publicly available dataset of real, anonymized case reports."}, {"title": "Data Source", "content": "Using GPT-40, we extracted text evidence of calculators within each patient note, patient ID, calculator score values, calculator names, and calculator units. After data cleaning (i.e., limiting to 1000 instances per calculator, removing calculators not present in our list of 35 calculators), our dataset, MedQA-Calc, consisted of 7,768 medical calculator prediction instances."}, {"title": "Data Truncation", "content": "Then, we implemented a GPT-40 model (fine-tuned on 70 truncated patient-calculator note instances) to truncate patient notes at the appearance of any given extracted medical calculator name and value. An example of truncation is shown below:\nOriginal note: A 55-year-old African American male with a past medical history of hypertension presented to the emergency department at our hospital with a chief complaint of generalized weakness... His Wells score for pulmonary embolism (PE) was 7.5 putting him at high risk of PE.\nTruncated note: A 55-year-old African American male with a past medical history of hypertension presented to the emergency department at our hospital with a chief complaint of generalized weakness...\nAfter running notes through the truncation model, we had a total of 7,758 medical calculator question-answer instances. We then split this data into a training and test set"}, {"title": "Question Curation", "content": "of 6,614 and 1,144 instances, respectively. We curated the testing set by including all calculator types, ensuring a maximum 50 instances per calculator.\nQuestion curation\nNext, two researchers, NW and JC, manually reviewed the test instances to ensure that notes were properly truncated with respect to the original note and calculator evidence. After NW reviewed all instances, JC reviewed all validated instances. After manual review, question-answer vignettes were randomly assigned answer options to include five options. Of note, we included \u201cNone of the above\u201d as a possible answer choice in all cases, and we also created an exclusion matrix so that calculators with similar purposes (e.g., CURB-65 and Pneumonia Severity Index) were not provided alongside one another."}, {"title": "Question Evaluation", "content": "We evaluated eight LLMs (i.e., GPT-40, GPT-3.5 Turbo, Llama-3-70B, Llama-3-8B, Mixtral-8x7B, Mistral-7B, Meditron, and PMC-Llama) using our dataset of 1,009 questions. Additionally, we employed two medical trainees to review 100 instances of calculator recommendations across all 35 calculators. Medical trainees answered questions in a closed-book setting with no external resources. After receiving ground-truth answers, medical trainees then evaluated calculator instances by labeling their agreement with the question-answer vignette."}, {"title": "Error Analysis", "content": "In order to perform additional analysis of LLM errors, we had two annotators review the errors made by LLMs on the subset of 100 questions that they answered. Annotators labeled LLM errors as (1) comprehension errors, (2) calculator knowledge errors, (3) alternate solution errors, or (4) \u201cno explanation\u201d errors according to our provided annotation guidelines. Of note, \u201cno explanation\u201d errors were defined as errors made by LLMs that did not provide an explanation or rationale for answers. In cases of error type disagreement, annotators discussed the LLM error until a consensus was reached."}]}