{"title": "KAN-RCBEVDepth: An multi-modal fusion algorithm\nin object detection for autonomous driving", "authors": ["Zhihao Lai", "Chuanhao Liu", "Shihui Sheng", "Zhiqiang Zhang"], "abstract": "Accurate 3D object detection in autonomous driving is critical yet challenging\ndue to occlusions, varying object scales, and complex urban environments.\nThis paper\nintroduces the KAN-RCBEVDepth algorithm, a pioneering method designed to enhance 3D\nobject detection by fusing multimodal sensor data from cameras, LiDAR, and millimeter-\nwave radar. Our innovative Bird's Eye View (BEV)-based approach, utilizing a Transformer\narchitecture, significantly boosts detection precision and efficiency by seamlessly integrating\ndiverse data sources, improving spatial relationship handling, and optimizing computational\nprocesses. Experimental results show that the KAN-RCBEVDepth model demonstrates superior\nperformance across most detection categories, achieving higher Mean Distance AP (0.389 vs.\n0.  316, a 23% improvement), better ND Score (0.484 vs. 0.  415, a 17% improvement), and faster\nEvaluation Time (71.28s, 8% faster). These results indicate that KAN-RCBEVDepth is more\naccurate, reliable, and efficient, making it ideal for dynamic and challenging autonomous driving\nenvironments.", "sections": [{"title": "1. Introduction", "content": "Accurate 3D object detection is a critical component of autonomous driving systems, enabling\nvehicles to perceive their environment in three dimensions and accurately identify and localize\nsurrounding objects such as vehicles, pedestrians, and obstacles [1]. This capability is essential\nfor safe navigation and decision-making in complex and dynamic environments, particularly in\nurban settings where visibility can be limited, and the density of objects can be high. The ability\nto detect and track objects in 3D allows autonomous vehicles to anticipate potential hazards, plan\nsafe paths, and execute maneuvers with higher precision and confidence. In urban environments,\nwhere visibility can be limited, accurate 3D object detection is crucial for managing interactions\nand recognizing occluded objects. To overcome the limitations of individual sensors, integrating\ndata from cameras, LiDAR, and radar through sensor fusion can provide a more comprehensive\nand accurate perception of the environment. Sensor fusion leverages the strengths of each sensor\ntype to mitigate their weaknesses, enhancing the overall performance of 3D object detection\nsystems. Traditional multimodal fusion at the perception result level often limits performance,\nbut integrating modalities into a unified feature space, such as with Bird's Eye View (BEV)\nperception, can overcome these challenges. Current research focuses on how to process sparse,\nunordered point cloud data efficiently. LiDAR-based 3D object detection techniques are mainly\ndivided into voxel and pillar methods. VoxelNet [14] organizes unordered point cloud data\ninto structured voxels and uses 3D convolutional networks to extract features. CenterPoint [15]\ndetects objects without fixed-size anchor boxes by assigning center points. Voxel [16] R-CNN\nenhances the point cloud features through voxel pooling. This study introduces an advanced\nsensor fusion method that integrates data from cameras, LiDAR, and millimeter-wave radar into a\nunified BEV feature space, effectively leveraging the strengths of each sensor to address occlusion\nand scale variation issues inherent in traditional 2D perception methods. Our contributions are\nsummarized as follows:\n\u2022 Innovative Multimodal Sensor Fusion Method: This paper introduces a multimodal network\nthat integrates millimeter-wave radar and multi-view camera data directly into the Bird's\nEye View (BEV) feature map. One key component is a new multi-view 3D detector, which\ninitially utilized direct supervision from LiDAR point cloud data to enhance depth learning\nand, building on this foundation, incorporates millimeter-wave radar as an additional\nmodality, further addressing the limitations of indirect supervision in-depth prediction\nmodules and mitigating overfitting, effectively leveraging the strengths of each sensor.\n\u2022 Utilization of Kolmogorov-Arnold Network for Feature Processing: The study integrates the\nhigh interpretability and non-linear modeling capabilities of Kolmogorov-Arnold Networks\n[4] into DepthNet for processing time-series data from the camera. A novel tokenized KAN\nblock is introduced to extract informative patterns, enhancing feature representation. This\napproach optimizes feature fusion, ensuring accurate and robust 3D object detection in\ncomplex environments.\n\u2022 High-Accuracy and Robust 3D Object Detection in Complex Urban Environments: Ex-\nperimental results demonstrate that the proposed KAN-RCBEVDepth model outperforms\nexisting standards in most detection categories, achieving higher mean Average Precision\n(mAP), lower error rates, and superior overall performance metrics. It also more accurately\nidentifies and tracks objects in dynamic environments, ensuring the reliability and safety of\nautonomous driving systems in complex urban settings."}, {"title": "2. Related Works", "content": "2.  1. Camera-based 3D Object Detection Methods\nObject tracking is vital for intelligent vehicle perception. With advances in autonomous driving,\nsystems using RGB images for object detection have become crucial. Deep learning has\nenhanced speed with single-stage models. Autonomous vehicles employ LiDAR, radar, and\nRGB-Depth cameras using point clouds and depth images for detection. Deep learning methods\nsurpass traditional techniques but require large datasets and extensive training. Notable models\ninclude YOLO [2], Faster R-CNN [3], R-CNN [5], and RetinaNet [6]. Methods often detect\n2D candidate regions and then predict 3D bounding boxes using neural networks or geometric\nconstraints. Chen et al. proposed Mono3D [7], which uses a 2D detector for candidate areas,\nmanually designing shape features and location priors. Another work by Chen et al., 3DOP\n[8], describes the problem as minimizing an energy function to predict ground plane depth and\nobject dimensions, combined with a CNN for confidence scores. However, 2D object detection\nprovides limited information, offering only bounding boxes and class confidence scores.\n2.  2. LiDAR-based 3D Object Detection Methods\nUnlike images, point clouds, particularly from LiDAR, offer high-quality 3D geometric\ninformation with strong ranging capabilities and stable performance under varying lighting\nconditions, making them suitable for a wide range of detection tasks.\nCompared to voxel-based methods, pillar-based methods aim to reduce inference time during\nthe detection process. PointPillars transforms point features into bird's-eye view pseudo-images,\nutilizing only 2D convolutional layers for learning, suitable for low-latency and low-compute\nembedded systems [17]. PillarNet [18] introduces 2D sparse convolution into the BEV feature\nextraction module. F-Convnet [19] extracts pillar point cloud features and performs classification\nand location regression through fully connected layers. Experiments show that although pillar-\nbased networks achieve similar accuracy to voxel networks through 2D sparse convolution feature\nextraction, the sparsity of point clouds results in many pillars lacking fine-grained information,\nespecially in detecting small objects. Lidar sensor's high cost, low refresh rates, and limited\nresolution also restrict its widespread application.\n2.  3. Multimodal Sensor Fusion-based 3D Object Detection Methods\nEarly studies separately processed LiDAR and camera data, then fused them at the decision\nlevel, as seen in BEVDet [25]. BEV-based 3D perception, known for rich semantic and precise\nlocation information, has become vital for tasks like behavior prediction and motion planning.\nFrom 2020 to 2022, research shifted towards converting perspective view (PV) to bird's-eye view\n(BEV) for image-based 3D object detection. Zhang et al. introduced BEVerse, which extracts\nspatiotemporal BEV features from multi-view cameras for multi-task inference [10]. Tesla used\nsurround cameras for BEV object detection, enhancing visual 3D object detection accuracy [12].\nAssociation Modality Fusion focuses on finding spatial relationships among multi-modal\nsensors, primarily combining LiDAR and cameras. Approaches like Pointpainting [20] overlay\nimage segmentation on point clouds, while MVDNet [24] fuses radar and LiDAR data, beneficial\nin foggy conditions. Despite the computational cost of 2D convolution on radar points, MV3D\n[22, 23] integrates LiDAR into RGB channels for 3D region creation. Methods like AVOD [?]\nand F-PointNet [27] enhance detection by mapping 2D boxes into 3D. BEVFusion [21] innovates\nby integrating depth probabilities from image features into BEV for pseudo-3D projections.\nRecent multimodal sensor fusion approaches have taken another direction. Vora et al. from\nnuTonomy proposed the PointPainting network [28], which maps image semantic labels onto\nLiDAR point clouds, enhancing the point clouds for use with any 3D detector. Yin et al. from\nthe University of Texas proposed the MVP [29] network, which uses semantic segmentation from\n2D images to generate virtual points, enhancing point cloud density.\nTo overcome these challenges, feature-level fusion methods have been proposed. Li et al.\nintroduced a deep fusion method for LiDAR and vision sensors [31]. Qin et al. developed\nSupFusion [32], a supervised LiDAR-camera fusion technique, integrating features to enhance\ndetection accuracy and understanding of the 3D environment."}, {"title": "3. Methodology", "content": "3.  1. Overview\nThis paper utilizes data from camera sensors, millimeter-wave radar, and LiDAR. Camera sensors\nprovide rich semantic information and precise object boundaries, capturing detailed visuals of\nthe environment. Millimeter-wave radar, with its longer wavelengths, excels in maintaining\nperformance under adverse weather conditions like rain, snow, and dust. It leverages the\nDoppler effect to estimate object depth and velocity efficiently, requiring minimal computational\nresources. LiDAR offers high-precision 3D point cloud data, which is crucial for accurate depth\nperception and spatial understanding. As illustrated in Figure 1, these sensors' complementary\nstrengths-detailed visual information, reliable performance in harsh weather, and precise\ndepth data-work together to create a robust perception system capable of handling various\nenvironmental conditions.\nKAN-RCBEVDepth excels in 3D object detection by leveraging multimodal sensor data and\ndepth supervision. It integrates data from six cameras and voxelized radar point cloud data using\nthe PointPillar method. By dividing radar point clouds into pillars and encoding them into a\nsparse pseudo-image, which is then seamlessly integrated with visual data from the cameras for\ncomprehensive environmental perception. Also, inputting camera intrinsic parameters into the\nDepthNet module, expanding them through a KAN layer, and reweighting the image features\n$F^{2d}$ using a Squeeze-and-Excitation module. Camera extrinsics combined with intrinsics help\nDepthNet perceive the position of $F^{2d}$ within the vehicle's coordinate system.\nA DETR-based fusion strategy integrates multi-angle camera visuals with radar data to\nenhance object detection and tracking. LiDAR serves as the primary supervision source, with\nmillimeter-wave radar and monocular cameras providing supplementary information. The system\nemploys the PointPillarsScatter method and Voxel Feature Encoder to convert radar features\ninto Bird's-Eye View (BEV) maps, fused with camera image features in a multi-modal backbone\nnetwork. Transformation matrices and timestamps ensure temporal and spatial consistency\nacross different sensors.\nAs depicted in Figure 2, this framework integrates multimodal sensor data and directly\nsupervises the depth prediction module using real depth information from LiDAR point clouds.\nThis direct supervision enhances the consistency between predicted depth values and actual\nmeasurements, improving depth estimation accuracy and reliability over traditional methods that\nrely on detection loss for supervision. The feature extraction backbone and detection head can be\nconsidered a conventional millimeter wave radar-vision fusion 3D detector D, where the feature\nextraction backbone consists of two encoders Er and Ec. Let D = {(r1, C1), (r2, C2), ..., (rn, CN)}\nbe a given dataset with N samples, where (ri, ci) are the paired inputs of millimeter wave radar\nand camera. Previous methods input the paired (ri, ci) into encoders Er and Ec to obtain the\ncorresponding feature representations fr and fe. Then, the paired feature representations are\ninput into the detection head for fusion to obtain the fused features, which are used for the final\nprediction."}, {"title": "3.2. Camera-awareness Module", "content": "The Camera-awareness Module in the KAN-RCBEVDepth framework enhances the capture of\ndepth information by incorporating both intrinsic and extrinsic camera parameters into the\ndepth estimation network (DepthNet) [13]. The process begins with the introduction of intrinsic\ncamera parameters, which are expanded using a Kolmogorov-Arnold Networks (KAN) layer.\nKANs employ learnable B-spline functions as weights, offering non-linear transformations that\nprovide flexible control over the flow of information. This approach shifts the activation function's\nrole from neurons to the connections, enabling a more nuanced feature representation.\nFor B-splines, the function maintains the same continuity at the knots within its domain. Its\npolynomial expression can be represented by the Cox-de Boor recursive formula:\n$B_{i,o}(x) := \\begin{cases}\n1 & \\text{if } t_i < x < t_{i+1} \\\\\n0 & \\text{otherwise}\n\\end{cases}$\n$B_{i,k}(x) := \\frac{x-t_i}{t_{i+k} - t_i}B_{i,k-1}(x)+ \\frac{t_{i+k+1} - x}{t_{i+k+1} - t_{i+1}}B_{i+1,k-1}(x)$\nThe 2D features $F^{2d}$ are extracted from multiple images using a ResNet backbone. The\nextracted 2D feature representations are:\n$F^{2d} = \\{F_i^{2d} \\in R^{C_F \\times H \\times W}, i = 1,2,..., N \\}$\nwhere W, H, and CF represent the width, height, and number of channels of the features,\nrespectively.\nFollowing this, the camera's extrinsic parameters are integrated with the intrinsic parameters.\nThis integration enables the DepthNet to accurately perceive the spatial positions of objects\nwithin the vehicle's coordinate system. By accounting for the camera's position and orientation\nrelative to the vehicle, the DepthNet can better interpret the scene geometry, leading to improved\ndepth estimation."}, {"title": "3.3. Millimeter Wave Radar and Camera Feature Extraction", "content": "In the process of data fusion, particularly aligning data from different sensors to a unified\ncoordinate system, presents significant challenges. This process involves a series of coordinate\ntransformation operations: first, converting point cloud data to the radar sensor's coordinate\nsystem, then transforming it to the global coordinate system, adjusting it to the vehicle's\ncoordinate system based on the camera's timestamp, and finally converting it to the camera's\ncoordinate system. These steps ensure that all sensor data are aligned spatially and temporally,\nfacilitating subsequent environmental perception and decision-making.\nThe Pillar-Based point cloud extraction backbone network provides an efficient way to extract\npoint cloud features. Unlike traditional VoxelNet algorithms, the PointPillars method processes\npoint cloud data directly into a bird's-eye view (BEV) feature map, eliminating the need for\nfeature extraction by 3D convolutional backbone networks, thus enhancing inference speed.\nThis method includes the voxelization of point cloud data, voxel feature encoding, and feature\nprojection and extraction. In the voxel feature encoding stage, the original 4D LiDAR point cloud\ndata is extended to 9 dimensions to more comprehensively represent the point cloud features\nwithin each voxel. After voxelization and feature encoding, these features are eventually filled\ninto the corresponding positions on the BEV feature map. This process effectively integrates\nmillimeter-wave radar information with camera data into a unified BEV feature map, supporting\nsubsequent 2D backbone networks to extract high-level features."}, {"title": "3.4. Explicit Depth Supervision", "content": "In standard depth estimation frameworks, detection loss alone often fails to provide sufficient\nsupervision, especially in complex monocular depth estimation scenarios. To address this\nlimitation, we utilize LiDAR point cloud data as a supervision signal, using ground-truth depth\ndata ($D_{gt}$) to directly supervise intermediate depth predictions ($D_{pred}$). This method ensures\nthe system learns and verifies depth information accurately, enhancing prediction precision and\nreliability.\nThe explicit depth supervision process involves projecting LiDAR point clouds into the camera\ncoordinate system. We calculate the 2.5D image coordinates using the intrinsic parameters (Ki),\nand the rotation (Ri) and translation (ti) matrices for each camera view:\n$P_{img} (u_d, v_d, d) = K_i(R_iP + t_i)$.\nThese projections are then processed using minimum pooling and one-hot encoding to align\nthe point cloud with the predicted depth, resulting in the supervised depth data:\n$D_{gt} = \\phi(P_{img})$.\nWe employ Binary Cross Entropy (BCE) for depth loss correction. Given the potential for\nmisalignment due to vehicle motion and the limited receptive field of DepthNet, we introduce a\ndepth correction module. This module expands the receptive field using residual blocks and\nDeformable Convolutions, helping to align the depth ground truth with the correct spatial\nfeatures.\nTo further refine the depth estimation, we first fuse the Depth Distribution and Context\nFeatures, followed by the integration of Radar Point Cloud Pillar Features. These radar pillar\nfeatures, processed through a Voxel Feature Encoding (VFE) network, are represented as fr\nand provide crucial supplementary spatial information, particularly under challenging conditions\nwhere visual and LiDAR data may be less reliable. The combined features are then fed into\nthe Depth Refinement Module, which reshapes the feature map F3d from [CF, CD, H, W] to\n[CF \u00d7 H,CD,W] and applies 3\u00d73 convolution operations along the depth axis. This module\neffectively aggregates features across different depth planes through Pillar Pooling. By including"}, {"title": "3.5. Voxel Pooling Optimization Process", "content": "After voxelization and feature encoding, the features are filled into the corresponding positions\non the BEV feature map. First, millimeter-wave radar and camera data are voxelized to generate\nthe corresponding features. These features are then filled into the designated positions on the\nBEV (Bird's Eye View) feature map, integrating data from different sensors into a unified BEV\nfeature map. This process lays the foundation for subsequent 2D backbone networks to extract\nhigh-level features.\nDuring the feature aggregation process, a significant computational overhead is introduced\ndue to the need to sort a large number of BEV grids. The traditional prefix sum calculation is a\nsequential process, which is relatively inefficient. To address this issue, we introduced the voxel\npooling technique. The observation space (ego space) is uniformly divided into multiple grids,\neach representing a voxel in the BEV feature map. We utilize CUDA threads to parallelize the\nprocessing of these grids, accumulating the 3D features within each voxel into the BEV feature\nmap. This method leverages the parallel computing power of GPUs, significantly speeding up\ndata processing.\nIn the feature aggregation process, we employed a prefix sum trick (cumsum trick), sorting\nand accumulating features, then obtaining the final aggregated result through subtraction. This\napproach not only reduces computation time but also significantly improves efficiency, enabling\nus to quickly generate accurate aggregated features on the BEV feature map.\nBy aligning frustum features from different time frames into the current self-coordinate system,\nwe eliminate the effects of self-motion on detection results. Voxel pooling is performed on these\naligned features, and the fused BEV features are then input into subsequent detection tasks.\nThis method not only enhances the spatiotemporal consistency of features but also improves the\nsystem's robustness in dynamic environments.\nThroughout this process, the CUDA kernel is used to handle voxel pooling operations.\nSpecifically, each CUDA thread processes a frustum point and updates the feature map using\natomic addition (atomic). This ensures safe updates from multiple threads, avoiding data race\nissues and ensuring the accurate update of the feature map."}, {"title": "3.6. Detection Head Feature Fusion", "content": "The feature fusion process is implemented in the detection head, and the key is to process the\nfeatures extracted from the millimeter wave radar and camera (fr and fc) together with the data\nfrom the Depth Refinement module. After voxel pooling, the BEV feature map fbev is generated.\nThe detection head, as the final processing module in the system, receives the fused BEV feature\nmap fbev as input. The primary function of this module is to perform object detection tasks,\nfiltering the LiDAR point cloud fi and millimeter wave radar point cloud for using location prior\ninformation (ri, ci), obtained from heatmaps generated by the main 3D detection head fer across\nmultiple categories. The effective point cloud is then processed to generate 2D bounding boxes\nin the BEV perspective. These bounding boxes are matched with reference valid areas obtained\nfrom the heatmaps using IOU matching. The matched positional information is used to generate\ntensors related to radar positional coordinates and velocity information {q}, which are then\nfused with the original feature map for subsequent predictions.\nThis step uses the DETR method based on a Transformer encoder, which integrates features\nfrom different sensors through an attention mechanism. The detection head first generates a set\nof randomly initialized 3D reference points {q}, which do not rely on any specific sensor outputs\nbut exist as an abstract concept in the learning process. During the fusion process, each point in"}, {"title": "4. Experiments", "content": "In this section, we introduce our experimental setup and conduct experiments to validate our\nproposed components. We present comparisons with other leading camera-based 3D detection\nmodels.\n4.  1. Dataset\nThe dataset used is nuScenes [17], containing 1000 scenes with six images and one LiDAR point\ncloud per frame. Using ResNet50 as the model's backbone, the input image resolution is fixed\nat 256x704, with a training period of 24 epochs. Strategies such as grouped sampling (CBGS),\ndata augmentation (DA), and exponential moving average (EMA) are used to optimize training.\n4.  2. Metrics\nIn the task of 3D detection, there are several key performance metrics, including the nuScenes\ndetection score (NDS), mean Average Precision (mAP), and five true positive (TP) metrics: mean\nTranslation Error (mATE), mean Scale Error (mASE), mean Orientation Error (mAOE), mean\nVelocity Error (mAVE), mean Attribute Error (mAAE) and Normalized Detection Score (NDS)\nas the evaluation metrics for the model. When calculating mAP, it is necessary to compute the\nAverage Precision (AP) for all categories. Unlike other object detection tasks, the AP calculation\nfor the Nuscenes dataset is not based on IOU but is defined by setting a threshold for the 2D\ncenter distance on the ground plane, aimed at reducing the impact of object size and orientation\non AP. When calculating AP, recall and precision rates below 10% are ignored and replaced with\n0. Next, mAP will be calculated at matching thresholds of 0.5, 1, 2, 4 meters and across different\ncategories.\nSince the mAP metric cannot fully evaluate all aspects of the Nuscenes dataset in detection\ntasks, such as velocity and attribute estimation, NDS is introduced for evaluation. The formula\nfor calculating NDS is:\n$NDS = \\frac{1}{10} [5mAP + \\sum_{mTP \\in TP} (1 \u2013 min(1, mTP)) ]$\nwhere mTP = \u03a3\u03a4\u03a1, and TP is the average true positive rate metric for a category, which\nincludes the collection of five average true positive rate metrics above.\n4.  3. Model Training\nThe experiments in this study were conducted on a high-performance computing platform with\nthe following configuration: an NVIDIA GeForce RTX 4070 GPU and an Intel Core i7-14700KF\nCPU, featuring up to 5.3 GHz single-core turbo boost and a 12-core 24-thread high-parallel\nprocessing capability. The system memory is 32GB DDR5, ensuring ample data processing\ncapacity. All experiments were conducted on a Windows 11 operating system with an Ubuntu\n20.04 LTS environment through Windows Subsystem for Linux 2 (WSL2), facilitating the use\nof Linux-specific software. The CUDA version used was 12.1, and cuDNN version v8.7.0 was\nutilized for GPU acceleration.\nDuring the model training phase, a combined loss function was used. The input resolution\nwas set to 256x704, and the output resolution was 128x128, with ResNet-50 serving as the\nbackbone network. The model was trained for 24 epochs, utilizing Class-Balanced Grouping and\nSampling to ensure balanced data distribution. Data augmentation techniques and Exponential\nMoving Average (EMA) were employed to enhance model stability. The training data included\ninformation from two keyframes and integrated various perception data sources, such as camera\nand radar, while excluding lidar, maps, and external data from nuScenes. The configuration\nmeticulously specified detection distances, the method for calculating center distance, and true\npositive distance thresholds for accurate detection of different target classes.\n4.  4. Experimental Details\nThis experiment mainly compares the performance of two different configurations (BEVDepth\n[9] and KAN-RCBEVDepth) in detecting various objects. By evaluating the following detection\nobjects: Car, Pedestrian, Bicycle, Truck, Bus, Construction Vehicle, Trailer, Motorcycle, Traffic\nCone, and Barrier.\n4.  5. Comparative Experiments\nThe key performance metrics in Table 4 examined include mean Average Precision (mAP),\nNormalized Detection Score (NDS), mean Average Translation Error (mATE), mean Average\nScale Error (mASE), mean Average Orientation Error (mAOE), mean Average Velocity Error\n(mAVE), and mean Average Attribute Error (mAAE) on different algorithms.\nKey findings include that KAN-RCBEVDepth achieves the highest scores in mAP (0.3891)\nand NDS (0.4845), indicating superior overall detection performance and accuracy compared to\nother methods. It also records the lowest mATE (0.6044) and mASE (0.278), demonstrating\nbetter precision in estimating object positions and scales. While STS shows the lowest\nmAOE (0.45), BEVDet4D [11] excels in mAVE (0.354) and and PETRv2 reveals mAAE\n(0.187), respectively. This comprehensive assessment highlights KAN-RCBEVDepth's strengths\nin providing more accurate and reliable 3D object detection results. KAN-RCBEVDepth\noutperforms other models, such as BEVDet, BEVerse, BEVDet4D [11], and PETRv2,\nparticularly in key metrics like mAP, NDS, and various error metrics."}, {"title": "5. Conclusion", "content": "This study delves into the integration of multimodal perception technologies within autonomous\ndriving systems, with a particular emphasis on the advancement of 3D object detection\nmethodologies for smart vehicles. By harnessing a fusion of camera imagery, LiDAR, and radar\ndata, we have developed an innovative approach that employs a multimodal Bird's Eye View\n(BEV) framework. This approach not only refines the precision of object detection in intricate\nurban settings but also bolsters the efficiency of the process. KAN-RCBEVDepth stands out\nfor its adept handling of multimodal data, significantly elevating the accuracy and robustness\nof detections even in the face of occlusions and varying object scales. The findings of this\nresearch underscore the potential of cutting-edge algorithms and network architectures to enrich\nthe environmental perception and decision-making prowess of autonomous vehicles, paving the\nway for safer and more reliable smart transportation solutions."}]}