{"title": "System Message Generation for User Preferences using Open-Source Models", "authors": ["Minbyul Jeong", "Jungho Cho", "Minsoo Khang", "Dawoon Jung", "Teakgyu Hong"], "abstract": "System messages play a crucial role in interactions with large language models (LLMs), often serving as prompts to initiate conversations. Through system messages, users can assign specific roles, perform intended tasks, incorporate background information, specify various output formats and communication styles. Despite such versatility, publicly available data are often lack system messages and subject to strict license constraints in the industry field. Manual labeling of publicly available data with system messages that align with user instructions demands significant resources. In view of such challenges, our work introduces SYSGEN, a pipeline for generating system messages with better aligned assistant responses from the supervised fine-tuning dataset without system messages. Training on SYSGEN data has demonstrated substantial improvements in the alignment of model responses with system messages and user instructions, as demonstrated across various open-source models on the Multifacet benchmark, while maintaining minimal impact on other unseen benchmarks such as Open LLM Leaderboard 2. Our qualitative analysis highlights the importance of diverse system messages to ensure better adaptability across different contexts.", "sections": [{"title": "1 Introduction", "content": "System message, also known as initial prompt, serves as an initial input to start a conversation with LLMs (Openai, 2024; Cohere, 2024; PromptHub, 2025). They have been shown to greatly affect model's assistant responses by providing contexts, guidances, and directions to LLMs (Qin et al., 2024; Lee et al., 2024). For example, given a system message, we can steer the LLM's behavior to set roles, provide the additional background information, maintain consistency of generated responses, customize a format, align to user preferences, and ensure safety and ethical considerations (AlKhamissi et al., 2024; Yang et al., 2024; Dubey et al., 2024). System messages have proven capable of setting constraints such as knowledge cut-off and current date or when different model behaviors need to be tailored for optimal overall performance (Lin et al., 2024; Abdin et al., 2024).\nWhile LLMs' capabilities of utilizing the system messages is widely investigated, how to acquire these system messages is underexplored. Our preliminary analysis has shown the following limitations about system messages in datasets. Most publicly available datasets have license constraints when used in the industry field, limiting their use in post-training techniques for target tasks (Xie et al., 2020; Ouyang et al., 2022; Zhou et al., 2023; Cui et al., 2023). Additionally, most datasets either lack system messages or contain the common system messages such as \u201cYou are a helpful AI assistant.\u201d (Xu et al., 2023; Pareja et al., 2024). Lastly, labeling system messages to fit various user instruction scenarios requires substantial resources (Abdin"}, {"title": "2 Related Works", "content": "System message: utilization and evaluation. A system message is a unique component of LLMs to initiate a conversation with them. It is utilized by many proprietary models (e.g., ChatGPT (OpenAI, 2023) and Claude (Anthropic, 2024)) as well as open-source models (e.g., Mistral (AlKhamissi et al., 2024), LLaMA (Meta, 2024), Qwen (Yang et al., 2025), and DeepSeek (Guo et al., 2025)). The system messages serve the purpose of steering the LLM's generation behavior and are widely used for various functions, including imprinting the model's identity, recording the knowledge cut-off date of the training data, and providing guidelines for various tool usages (Openai, 2024; Cohere, 2024; PromptHub, 2025). Additionally, the system messages are used to guide the model in generating safe and harmless responses (Touvron et al., 2023; Lu et al., 2024; Wallace et al.).\nDespite the usefulness of system messages, there is a significant lack of data that includes system messages reflecting diverse user instructions without license constraints. Furthermore, manually labeling such data requires substantial human resources and even among publicly available datasets, it is challenging to obtain data that includes various system messages (Lin et al., 2024; Xu et al., 2024). Lee et al. (2024) provide data augmentation which reflects hierarchical dimensions of system role data with multiple aspects of evaluation benchmark. Furthermore, Qin et al. (2024) provide multi-turn benchmark to evaluate system message alignment. In line of these works, our SYSGEN pipeline ensures high-quality system messages and assistant responses by supplementing data using only open-source models without licensing concerns. Furthermore, it demonstrates that data augmentation is possible on existing SFT datasets without requiring extensive human labeling efforts."}, {"title": "3 SYSGEN: Pipeline of System and Assistant Response Generation", "content": "Our SYSGEN pipeline consists of four phases: (1) generating system messages with eight key functionalities (Sec 3.1), (2) filtering mis-specified system tags and reorganizing them (Sec 3.2), (3) verifying the key functionalities on a phrase level (Sec 3.3), (4) generating the new assistant responses using the refined system messages and original user instructions (Sec 3.4). Figure 2 depicts the overall architecture of the SYSGEN pipeline."}, {"title": "3.1 Phase 1: System Message Generation", "content": "The primary goal of our SYSGEN pipeline is to enhance existing SFT datasets by adding system messages that were not originally included. As the system messages can steer the LLM's behaviors, we focus on these messages during the development and release of the models. However, license constraints and substantial resource requirements of manually labeling the system messages inevitably arise, making it difficult to utilize most publicly available datasets. Thus, we aim to generate system messages by leveraging open-source models and data without any license issues."}, {"title": "Phrase level Annotation to System Messages", "content": "We manually classify eight functionalities that are widely used in the system messages referring to previous works (Openai, 2024; Cohere, 2024; AlKhamissi et al., 2024; Lee et al., 2024): (1) Specifies the role, profession, or identity that needs to be played (Role); (2) Specifies the content that needs to be included in the response such as an identity of the company (Content); (3) Identifies what to perform (Task); (4) Specifies the behavior to perform (Action); (5) Prefers the style of communication for responses (Style); (6) Provides additional information to be served as an assistant (Background); (7) Provides built-in methods to use (Tool); (8) Preference of what output should look like (Format).\nGiven a pair of user instructions Q and assistant responses A, we generate a system message S using the open-source LLMs M with a prompt P that includes few-shot demonstrations:\nM(S|P, Q, A) (1)\nWe provide details about the few-shot demonstrations in the Appendix D."}, {"title": "3.2 Phase 2: Filtering Process", "content": "After generating the system messages, we filter out the abnormal system messages for consistent text format. In Figure 2 (top right), we first identify and remove mis-tagged phrases. For example, we can guarantee the correctness of the phrases between these tokens only if the start and end tokens are the same (e.g., \u00abTask\u00bb). In addition, we remove invalid tags such as \u00abExample\u00bb or \u00abSystem\u00bb, which may be generated in phase 1. To ensure a consistent structure of system messages, we reorder the tags and phrases in manually defined order."}, {"title": "3.3 Phase 3: Verification of Eight Key Functionalities", "content": "In this phase, we verify whether each generated phrase is appropriate for its assigned tag. Using the LLM-as-a-judge (Zheng et al., 2023) approach with self-model feedback, we assign one of three labels for each tag: Good if the tagging is appropriate, Bad if the tagging is inappropriate, and None if the tag or phrases are missing. Phrases labeled as Bad or None are then removed from the system message to ensure accuracy and consistency. We observe that most of the data instances (up to 99%) are preserved after applying phase 3."}, {"title": "3.4 Phase 4: Assistant Response Generation", "content": "After filtering and verifying the generated system messages, they can be used alongside existing QA pairs. However, we hypothesize that if there is any potential misalignment between the human curated QA and model-generated system messages, a follow-up data alignment phase is necessary. Therefore, we generate new assistant responses A' based on a refined system messages S and the user instructions Q, ensuring better alignment with the given instructions.\nTo achieve this, we first remove the annotated tags from the system messages to guarantee that the refined messages seem natural. We provide a detailed example in Figure 2 (bottom right). Then, we use the open-source LLMs M employed in phase 1 to generate new responses A'.\nM(A'|S, Q) (2)\nIn Table 1, the new responses preserve similar content with high n-gram matching compared to the original responses, but have shown diversified formats with high semanticity and verbosity. We provide the cases in Appendix C.\nWe also use LLM-as-a-judge with GPT-40 to analyze that the new responses A' are better aligned to the user instructions than the original responses A. Figure 3 illustrates the proportion of cases where the new responses are judged to be better aligned than the original responses when given the user instructions. For simpler evaluation, we evaluated 1K randomly sampled instances from the generated datasets. Overall, our findings suggest that generating responses based on the system messages lead to better alignment with user instructions."}, {"title": "4 Experimental Settings", "content": "4.1 Training Dataset\nIn Table 2, we provide the remaining instances after processing each phase of our generated datasets. We target datasets with three conditions: (1) widely used as SFT datasets; (2) do not contain the system messages; (3) diverse domains are covered. We enumerate the selected datasets as follows: (1)"}, {"title": "5 Experiments", "content": "The primary goal of SYSGEN pipeline is to enhance the utilization of the system role while minimizing performance degradation on unseen benchmarks, thereby improving the effectiveness of supervised fine-tuning (SFT). To validate this, we evaluate how well the models trained on SYSGEN data generate appropriate assistant responses given both the system messages and user instructions, using the Multifacet (Lee et al., 2024) dataset. For models"}, {"title": "6 Analysis", "content": "6.1 What makes SYSGEN pipeline useful?\nTo assess the impact of system messages generated by SYSGEN during training, we conduct ablation studies on four different model variations:\n\u2022 No System Message: The original SFT dataset which does not contain the system message.\n\u2022 Common System Message: An SQA triplet where the common system message is inserted such as \"You are a helpful AI assistant\".\n\u2022 SYSGEN without A': An SQA triplet that includes only a system message generated by our SYSGEN pipeline.\n\u2022 SYSGEN: An SQA' triplet where both the SYSGEN-generated system message and the newly-generated answer are incorporated.\nWe measure the effectiveness of these models by analyzing score variations on the Multifacet and unseen benchmarks in Table 6.\nTraining with data that includes common system messages does not result in a significant performance difference compared to training without system messages. This led us to question: \"Would it be sufficient to include only the most suitable system messages?\". To explore this, we train models using data that contains only system messages generated by SYSGEN pipeline. As a result, we observe an improvement in Multifacet performance for both models, while the scores on the unseen benchmark remained similar. Furthermore, when both system messages and assistant responses generated by SYSGEN are used for fine-tuning, we observe performance improvements in both Multifacet evaluation and unseen benchmarks."}, {"title": "6.2 System message vs. User instruction", "content": "A key question arises that what happens if we add a message intended for the system role at the beginning of the user instruction? Could it serve as a replacement for the system role? To explore this, we conduct an experiment on a Multifacet benchmark. Specifically, we included messages that should typically be in the system role within the user instruction during inference.\nAs shown in Table 7, we observe that open-source models tend to experience score degradation when system role messages are incorporated into the user instruction. This trend suggests that adding such content can make the query itself more ambiguous to answer. Furthermore, even in models trained with our SYSGEN, this trend persists similarly to the previous work (Lee et al., 2024). Despite additional fine-tuning on system roles, scores still remain low when system messages are reflected in the user instruction. This highlights the importance of properly placing these messages in the system role to maintain performance."}, {"title": "6.3 New assistant responses align to the system messages", "content": "In Table 1, we presented that the new assistant responses exhibit similar n-gram matching, high semantic similarities, and verbosity. Therefore, it is necessary to verify whether the generated assistant responses aligned with the system messages. Figure 4 illustrates the GPT-4o results using LLM-as-a-judge approach. Through the three SYSGEN data generated by Phi-4, LLaMA, and Qwen models, we determined that all of the assistant responses are highly aligned with the system messages. Overall, the experiments and analyses reveal that our SYSGEN data were generated to effectively respond to various user instructions as system messages. In addition, we observed that the assistant responses align with the system messages and are capable of generating better aligned responses compared the original assistant responses."}, {"title": "7 Conclusion", "content": "In our study, we introduce SYSGEN, a novel pipeline to generate system messages with better aligned assistant responses from an existing SFT datasets without system messages. Using the SYSGEN data, new assistant responses maintain lexical and semantic consistency with the original responses while aligning more closely with user instructions. Our experiments reveal that various open-source models trained on SYSGEN data perform better on the Multifacet dataset while maintaining minimal performance degradation on unseen benchmarks, Open LLM Leaderboard 2. Our analysis demonstrates that diverse system messages improve the LLMs' abilities to adapt to different user instructions. Additionally, we emphasize the importance of clearly distinguishing between the system and user roles."}, {"title": "Limitations", "content": "While our SYSGEN pipeline demonstrates promising results in system messages alignment to the user instructions through Multifacet dataset. How-"}]}