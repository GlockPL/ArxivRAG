{"title": "ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical Insights from Large Language Models", "authors": ["Yuxi Sun", "Wei Gao", "Jing Ma", "Hongzhan Lin", "Ziyang Luo", "Wenxuan Zhang"], "abstract": "With the rise and widespread use of Large Language Models (LLMs), ensuring their safety is crucial to prevent harm to humans and promote ethical behaviors. However, directly assessing value valence (i.e., support or oppose) by leveraging large-scale data training is untrustworthy and inexplainable. We assume that emulating humans to rely on social norms to make moral decisions can help LLMs understand and predict moral judgment. However, capturing human values remains a challenge, as multiple related norms might conflict in specific contexts. Consider norms that are upheld by the majority and promote the well-being of society are more likely to be accepted and widely adopted (e.g., \"don't cheat,\"). Therefore, it is essential for LLM to identify the appropriate norms for a given scenario before making moral decisions. To this end, we introduce a novel moral judgment approach called ClarityEthic that leverages LLMs' reasoning ability and contrastive learning to uncover relevant social norms for human actions from different perspectives and select the most reliable one to enhance judgment accuracy. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in moral judgment tasks. Moreover, human evaluations confirm that the generated social norms provide plausible explanations that support the judgments. This suggests that modeling human moral judgment with the emulating humans moral strategy is promising for improving the ethical behaviors of LLMs.", "sections": [{"title": "1 Introduction", "content": "In recent years, with the widespread application of LLMs, there has been an increasing expectation for building responsible AI that meets societal standards. However, investigations have revealed problematic behaviors in LLMs, such as generating harmful content, reinforcing biases, undermining fairness, and disseminating disinformation (Gehman et al., 2020; Venkit et al., 2022; Gabriel, 2020). Developing LLMs with desirable societal behaviors remains a challenging problem. It is essential to build trustworthy systems that can accurately evaluate specific scenarios and provide persuasive explanations for their moral judgments.\nIn real life, human moral decisions are often guided by social norms, which are defined as shared rules about social behavior\u00b9. Understanding these norms is crucial for interpreting hu-"}, {"title": "2 Related Work", "content": "AI Safety. The primary goal of AI safety is to ensure that models do not harm humans (Bostrom and Yudkowsky, 2018; Hendrycks et al., 2020). However, AI systems can pose significant risks if they follow misspecified rules, and potentially triggering dangerous failure modes (Jin et al., 2022). Although expressing and understanding human values for AI is complex and challenging (Bostrom and Yudkowsky, 2018; Russell, 2019), some researchers have attempted to analyze the system-"}, {"title": "3 Methodology", "content": "Data Formulation. Let us assume that in a standard human-labeled dataset $ \\{(a_m, a_i^m, n_i)\\}_{i=1}^N \\in D$, there are two conflicting moral judgment actions $a_m$ and $a_i^m$ within a social norm $n_i$. For example, under a pre-existing norm $n_i$ \"Reporting crimes are encouraged which can reduce bad impact\", reporting a crime is a moral action denoted as $a_m$, while avoiding informing criminal act is an immoral action denoted as $a_i^m$. We use $a_i$ to generally denote a human action to be judged by our model.\nTask Formulation. Given a human action $a_i$ (e.g., \"To protect myself I do not report a witnessed crime\"), the task is to make a binary prediction $f_{class}: (a_i,n_i) \\rightarrow Y_i \\in \\{0,1\\}$, which aims to infer whether the action is moral ($y_i = 1$) or immoral ($Y_i = 0$), where $n_i$ is the norm selectively derived from the rationales of moral (e.g., \"It is ok to self-defend\") and immoral (e.g., \"Avoiding reporting crimes may contribute to the cycle of violence\") paths generated by our model: $f_{ngen}(f_{rgen}(a_i)) \\rightarrow n_i$. We use $f_{rgen}$ and $f_{ngen}$ to denote the functions for task-specific rationale generator and norm generator, respectively. In particular, when no explicit norm can be generated, we use the generated rationale $\\hat{r_i} = f_{rgen}(a_i)$ as a substitution of $n_i$ to infer the final judgment (\u00a73.3).\nOur Design. Our design takes two aspects into consideration: 1) During inference, it can produce moral rationale within the specific context based on human action and then summarize the rationale into a general social norm; meanwhile, the rationale and norm can support action judgment. 2) During training, for generating norms from actions, we leverage LLMs to elicit textual rationales, which can provide additional knowledge to improve the social norm generation. Moreover, our approach is an alternative method for explaining the actions with rationales when specific norms are not explicitly given. Figure 2 illustrates our framework, which includes 1) pre-training three distinct task-specific language models \u2013 a classifier to make binary moral judgment, and two generators to provide rationale"}, {"title": "3.1 Pre-train Task-specific Language Models", "content": "We utilize the T5 architecture (Raffel et al., 2020) to pre-train the classifier, the rationale generator and the norm generator. T5 add a task-specific text to the original input sequence before feeding it to the model asle know as Prefix Language Modeling (PrefixLM), provides an efficient method for various NLP tasks, enabling us to pre-train the three task-specific models, as illustrated in Figure 2.\nProposed Objective: PrefixLM Consider a conditional generation task where the input is a context x and the output y is a sequence of tokens. Assume we have an language model $P_{\\theta}(y|x)$ based on the Transformer architecture and parametrized by $\\theta$. PrefixLM differs from the standard LM such that it enables bi-directional attention on the prefix sequence (e.g. {prefix, x}) in Eq.1, and only conducts autoregressive factorization on the target tokens (e.g. y in Eq.1). During pretraining, the length of a prefix sequence of tokens is $T_p$ and the training objective becomes:\n$L_{PrefixLM}(\\theta) = -E_{x,y~D}[logP_{\\theta}(y|\\{prefix,x\\})]$\n$= -E_{x,y~D}[\\sum_{t=T_p}^T logP_{\\theta}(y_t|y_{<s>t}, \\{prefix, x\\}],$"}, {"title": "3.2 Fine-tune Generators", "content": "Distinguishing human actions within the same social norm is challenging due to similar contexts of the actions. To enhance feature learning, we introduce triplets $\\{(a_m, a_i^m, a_m')\\}$ for conducting contrastive learning, which aims to generate more similar norms for action pairs $(a_m, a_i^m)$ under the same norm and distinguish them from other actions under different norms like $a_m'$. The model is illustrated by step 2 in Figure 2.\nBy utilizing human-written norms for supervised training, we can mitigate the impact of rationale quality on the final outcome. We sample moral actions as negative samples and utilize the triplet loss function similar to the one in (Schroff et al., 2015), formulated to pull the anchor and the positive together and push the anchor and the negative apart by a margin. We define the loss function $L$ as:\n$max\\{\\|\\| f_{ngen} (f_{rgen}(a_m))-f_{ngen}(f_{rgen}(a_i^m)\\|\\|_2-\n\\| (f_{ngen} (f_{rgen}(a_m))-f_{ngen}(f_{rgen}(a_m'))\\)\\|\\|_2+\\alpha, 0\\},$ where $a_m$ is an anchor input, $a_i^m$ is a positive input under the same norm $n_i$, $a_m'$ is a negative input under a different norm $n_j$, $\\alpha$ is a margin between positive and negative pairs, and $f_{rgen}$ and $f_{ngen}$ are respectively the pre-trained rationale generator and norm generator.\nModel Fine-tuning. We integrate the three tasks into a multi-task learning framework. The overall objective function is designed as follows:\n$\\mathcal{L} = min(\\lambda_1 L_{rgen} + \\lambda_2 L_{ngen} + \\lambda_3 L_{triplet}),$ where $ \\lambda_1$, $\\lambda_2$ and $ \\lambda_3$ denote regularization weights that balance the learning of the three tasks, and $\\theta$ contains model parameters of the two generators."}, {"title": "3.3 Inference", "content": "Given a human action $a_i$, our model generates moral and immoral paths of rationales (i.e., $\\hat{r_m}$"}, {"title": "4 Experimental Evaluation", "content": "We conduct experiments of ClarityEthic on both classification and generation tasks. Additionally, we leverage ClarityEthic's key principle \u2013 setting the two contrastive decision-making paths and selecting the better one to design a preliminary version of our approach, named ClarityCoT, by directly prompting LLMs and compare their performance. The details of the prompt template, training setup, and datasets are described in Appendices A.1, A.2, and A.4, respectively. We also provide statistical test results in Appendix A.7."}, {"title": "4.1 Evaluation on Moral Stories", "content": "Moral Stories (Emelin et al., 2020) is a structured descriptive morality dataset that includes pairwise information, consisting of a pair of moral and immoral actions within the same social norm.\nBaseline. We follow the setting of baselines from previous work on predicting moral decisions (Hendrycks et al., 2020), and compare several language models: ROBERTa-large (Liu et al., 2019), DeBERTa-large (He et al., 2020), BART-large (Lewis et al., 2019), and T5-large (Raffel et al., 2020). We also include GPT-3.5 (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), Claude-3 (Anthropic, 2024), and MoralCoT (Jin et al., 2022) for LLM baselines, and word-averaging model based"}, {"title": "4.2 Evaluation on ETHICS", "content": "To assess ClarityEthic's generalization performance without established social norms, we utilize the ETHICS dataset annotated with five moral concepts across justice, deontology, virtue ethics, utilitarianism, and commonsense intuitions. Except for commonsense, the dataset clearly emphasizes each concept. For example, justice \"requires giving people what they are due,\" and virtue emphasizes \"acting as a virtuous person would act.\" Meanwhile, Hendrycks et al. (2020) express that models must comprehend the morally relevant factors emphasized by each concept to do well. Therefore, we employ these sentence definitions of moral concepts as the ground-truth text of norm to train and evaluate ClarityEthic. In this scenario, due to the absence of explicit norms in terms of specific actions, we rely on generated rationales to make moral judgments, as illustrated in \u00a73.3. The examples of LLM-generated rationales for training can be found in Appendix A.5, and the generated rationales by our generator in the inference are exemplified in \u00a74.4.\nThe evaluation metrics are the same as those used on the Moral Stories. The results in Table 3 indicate that the classifier makes judgments by leveraging the generated rationales and outperforms all baselines including GPT-4. These generated rationales can serve as explanations to support the decision, which is discussed in the case study in \u00a74.4."}, {"title": "4.3 Ablation Study", "content": "We ablate three components: 1) Pre-training: Pre-train the rationale and norm generator with the dataset. 2) Fine-tuning: Fine-tune pre-trained generators with multi-task learning losses $L_{rgen}$ and $L_{ngen}$. 3) Contrastive learning: Incorporate contrastive learning loss $L_{triplet}$ in the fine-tuning stage. To ensure fairness, we adhere to the infer-"}, {"title": "4.4 Case Study", "content": "Table 5 shows two types of results with four specific cases. The first type is the expected result where the two generators can generate the corre-"}, {"title": "5 Conclusion", "content": "We propose the ClarityEthic, a novel explainable approach to enhance automatic moral judgments for human actions. Our method generates social norms for moral and immoral paths and selects the more reliable norm to enhance moral judgment. We design a two-stage training framework that leverages LLM reasoning and utilizes contrastive learning for fine-tuning. Extensive experiments on two benchmarks demonstrate promising results of ClarityEthic in moral judgments explainable by the generated social norms or rationales."}, {"title": "Limitations", "content": "There are three main limitations suggesting a few directions to further our current work:\n\u2022 It is important to acknowledge that the training data used in this study is derived from US norms, which may have limitations in its applicability to other cultural contexts. Additionally, the user study only include evaluations from individuals of Asian descent, potentially introducing bias. We recognize cultural differences in moral norms and intend to create a benchmark specifically tailored to Asian cultures to address this gap.\n\u2022 Human moral intensity and cross-culture moral (MacAskill et al., 2020; Takeshita et al., 2023; Awad et al., 2022b; ?) are also challenge and important tasks, which are not the focus of our current version. We acknowledge the variability of moral norms across different regional and cultural backgrounds, emphasizing the importance of establishing social norms within diverse cultural contexts, which will be further investigated in the future. However, Our approach may capture moral intensity, reflected in the final moral judgment. For example, when considering the action in Figure 1 \"not taking shower for 3 days to save soap,\" the norms of \"saving resources\" and \"maintaining hygiene\" in two paths are still reasonable. ClarityEthic will refer to both the action and the generated norms in the decision function $f_{class}(a_i, n_i)$ to infer judgment $y_i$ probabilistically, which may filter a different final judgment.\n\u2022 In current stage, ClarityEthic make moral decision in two opposite path. In the future, we would like to research the ability of ClarityEthic in multi-party case, which may require new data sets and strategies.\n\u2022 While our approach aims to interpret the results of the model's predictions, it does not delve into explaining the model's internal mechanisms. Therefore, we plan to continue our research in the future to enhance the interpretability of model's internals."}, {"title": "Broader Impact and Ethical Statements", "content": "Our work aims to mitigate the potential risks stemming from Al systems' misunderstanding or incorrect predictions of human world norms. By contributing to the development of safer AI systems capable of collaborating with humans, we strive to enhance overall AI safety. It is crucial to note that our approach should not be used to guide human interactions or provide advice to individuals in the real world. Meanwhile, the datasets we utilized have been carefully curated to minimize the presence of offensive or biased language.\nDuring fine-tuning, we utilize rationales as intermediate results. While we conduct supervised training with the ground-truth norms to mitigate the impact of rationale quality issues on the final outcomes, and our training data are sourced from public datasets with no offensive language (Emelin et al., 2020; Hendrycks et al., 2020), the concerns about potential ethical issues with rationales generated by LLMs still persists. To address this concern, we randomly select 50 rationales used in training and engage three participants to perform a human check using the social bias frame proposed by Sap et al. (2019). Detailed information on the sampled data is provided in Appendix A.3."}, {"title": "A Appendix", "content": "To evaluate the effectiveness of our approach in prompt LLMs, we design a detailed chain of thought template as follows, and the results of two datasets are displayed in Table 1 and Table 3:\nIn the pre-training stage, we perform experiments with T5-large (770M) models with the following hyperparameters: learning rate = 5 \u00d7 10\u22125, batch size = 8, max input length = 1,024, for a maxi-mum of 10,000 steps. In the fine-tuning stage, we load the best pre-trained rationale generator and norm generator with the hyperparameters: margin = 0.3, \u03bb\u2081 = 0.2, \u03bb2 = 1, \u03bb3 = 0.3, learning rate = 5 \u00d7 10-5, batch size = 8, max input length = 1,024, epoch = 5. We run train-test experiments five times based on different random seeds to set hyperparameters \u03c4 and \u03b1, respectively. Each time renders an optimal \u03c4 that ranges in 0.90-0.99 or \u03b1 ranges from 0.1-0.5 for a different run, in which \u03c4 or \u03b1 is determined using a small set of held-out validation data. The performance results are finally averaged over these five random runs. All experiments were conducted using an A100 80GB GPU."}, {"title": "A.3 Ethical Human Check", "content": "We conducted an ethical evaluation of LLMs by enlisting the help of three undergraduate individuals in social science major (one male and two females). We randomly selected 50 pairs of actions and rationales from the training data to conduct the evaluation and asked the three individuals to evaluate them. We used the evaluation criteria derived from the social bias frame produced by Sap et al. (2019), with Offensiveness, Intent to offend, Lewd, Group implications, Targeted group, Implied statement, and In-group language being the specific criteria. All the three evaluators reported that they did not observe any evident biases during their evaluation. Table 12 displays the details of the 50 pairs of actions and rationales we analyzed."}, {"title": "A.4 Datasets", "content": "The details of the datasets (i.e., Moral Stories and ETHICS) are shown in Tables 6 and 7. Both datasets hold a MIT License2."}, {"title": "A.5 Examples of Extracted Rationales", "content": "Table 11 provides a list of examples of two datasets, containing human actions, social norms, and rationales (extracted from LLMs)."}, {"title": "A.6 User Study", "content": "To ensure accuracy, we will randomly choose ten actions from the test set for each participant. We will also include the corresponding generated norms from four different baselines (excluding VAE and GPT-2 due to low quality) and our method. This means that every participant will answer 100 questions (two metrics \u00d7 five models \u00d7 ten actions). The participants are then required to score the plausibility and relevance on a scale of 1-3 (Forbes"}, {"title": "A.7 Standard Deviation and Significance Test", "content": "For the classification task of two datasets, we provide the standard deviation across five runs in Table 10, corresponding to the averaged results in Table 1 and Table 3. Among the prompting methods of classification, it seems that Claude and ClarityCoT are more stable than GPT3.5. Compared to the Moral Stories, the results on the ETHICS exhibit higher variability, while our method maintains relatively better stability. Moreover, the results of"}, {"title": "A.8 Baselines", "content": "This section describes the details of our experiment baselines. To ensure fairness, we do not compare with Delphi, which is fine-tuned in T5, with a large dataset that may contain our test data (Jiang et al., 2021), and we compare with fine-tuning T5 instead. For both tasks, the inputs are actions ai, and the outputs are binary judgment yi and the targeted norms ni for classification and norm generation, respectively."}]}