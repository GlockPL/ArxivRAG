{"title": "ReLEP: A Novel Framework for Real-world Long-horizon Embodied Planning", "authors": ["Siyuan Liu", "Jiawei Du", "Sicheng Xiang", "Zibo Wang", "Dingsheng Luo"], "abstract": "Real-world long-horizon embodied planning underpins embodied AI. To accomplish long-horizon tasks, agents need to decompose abstract instructions into detailed steps. Prior works mostly rely on GPT-4V for task decomposition into predefined actions, which limits task diversity due to GPT-4V's finite understanding of larger skillsets. Therefore, we present ReLEP, a groundbreaking framework for Real world Long-horizon Embodied Planning, which can accomplish a wide range of daily tasks. At its core lies a fine-tuned large vision language model that formulates plans as sequences of skill functions according to input instruction and scene image. These functions are selected from a carefully designed skill library. ReLEP is also equipped with a Memory module for plan and status recall, and a Robot Configuration module for versatility across robot types. In addition, we propose a semi-automatic data generation pipeline to tackle dataset scarcity. Real-world off-line experiments across eight daily embodied tasks demonstrate that ReLEP is able to accomplish long-horizon embodied tasks and outperforms other state-of-the-art baseline methods.", "sections": [{"title": "Introduction", "content": "For a long period of time, robots have been the technological romance pursued by mankind. History has seen great robots created. However, none of them shows equal intelligence to robots we imagine in the movies and science fictions. The major obstacle is the difficulty to make them perceive, reason, understand and react like humans do. They alone are tin men without souls, or without embodied intelligence.\nRecently, the blossom of large foundation models points out a path for embodied reasoning and planning. Their abilities to understand semantic information and reason make them good candidates for use as robot brains. However, even with the help of large foundation models, embodied tasks are still difficult to carry out.\nLarge foundation models use language as media to interact with the environment. It is hard to ground tasks in natural language into robot action space. There are generally three ways to accomplish embodied task grounding using large foundation models. The first is to fine-tune the large foundation model to directly output control signals in text form. This method can only be used in simple control like primitive autonomous driving (Xu et al. 2023), and the performance is not guaranteed. The second approach is vision-language-action models (Brohan et al. 2023; Belkhale et al. 2024). It assigns action controls to rarely used tokens or integer tokens in the original large foundation models and name them action tokens, then trains these models to predict action tokens given language input. The third way is more cost-efficient. It uses pre-defined skills as APIs, and let the large foundation models predict a sequence of skills, then execute these skills in succession (Ahn et al. 2022; Huang et al. 2023a).\nAnother difficulty lies in task planning. Tasks like \"bring me a bottle of water\" can be difficult to carry out in an end-to-end manner. The agent need to first detect the bottle of water and then perform a sequence of actions to acquire the bottle. Finally, it needs to comprehend the meaning of giving \"me\" the water and hand the bottle to \"me\". These long-horizon tasks usually need to be broken down into multiple short-term low-level tasks in the first place. Experiments show that advanced large foundation models are already capable of decomposing simple long-chain tasks (Hu et al. 2023; Zhi et al. 2024). However, they can only perform a small range of tasks due to limited pre-defined skills. In addition, this issue cannot be resolved simply by scaling up the skill library. When the skill library scales up, large foundation models struggle to comprehend the connections between the skills and the usage of them (see Sec. 4.3).\nMost embodied planning methods are carried out in simulated environments (Shridhar et al. 2020; Huang et al. 2023a; Skreta et al. 2024) due to lack of real-world data. However, a robot must eventually preform tasks in a real environment. There are few datasets for embodied planning, not to mention most of them are simulation data. EgoCOT (Mu et al. 2024) is one of the few real-world dataset for embodied planning, but the skills in the dataset are free-form and difficult to reuse.\nIn this work, we introduce ReLEP, a universal framework for real-world long-horizon embodied planning via large vision language models, which can be applied to robots with different configurations and skill libraries. In addition, it is able to re-plan when the original plan fails or the environment changes. More importantly, ReLEP stands out for its versatility in solving a broad spectrum of tasks within real-world environments. Also, it can be easily reproduced, simplifying the process for researchers and practitioners to replicate and build upon our work. An overview of ReLEP is shown in Figure 1.\nIn the center of our framework is the brain of the agent, a fine-tuned large vision language model which is aligned to decompose long-horizon tasks into a sequence of pre-defined skill functions. The model is fine-tuned on the real-world embodied planning dataset that we constructed from scratch. In this way, the model can better understand the relations between skills from the data and makes more reasonable plans.\nWhen executing the generated plan, the framework calls the predicted skill functions in succession. The skill library that we carefully constructed is able to complete a wide range of tasks. Also, we designed an embodied question answering skill that can answer questions related to the tasks. Embodied Question Answering (Das et al. 2018; Majumdar et al. 2024) poses a challenge due to the intricate necessity for the agent to comprehensively integrate task requirements, past behaviors, scenes, semantic information and other contexts to formulate an apt response. Consequently, the ability to answer embodied questions is a manifestation of embodied intelligence. This is crucial for embodied agents, for this makes them more human instead of machine.\nTo cope with the challenge of real-world data, we put forward a method to generate open-loop dataset with images using GPT-4V (OpenAI 2023). This semi-automatic pipeline greatly simplifies the difficulty of obtaining embodied planning datasets, which is of great significance for the development of embodied agents in real-world environments.\nWe also conducted real-world off-line experiments across eight challenging long-horizon embodied tasks. We compared the performance of ReLEP with state-of-the-art methods EmbodiedGPT (Mu et al. 2024), VILA (Hu et al. 2023) as baselines. Results indicate that ReLEP outperforms all baseline methods in different ways and demonstrate the effectiveness of the proposed framework.\nThe main contributions of our work are as follows:\n\u2022 A real-world long-horizon embodied planning framework named ReLEP, which is able to perform a wide range of long-chain embodied tasks in real world and is easily reproducible. The framework can be adopted to robots with different configurations and skill libraries.\n\u2022 A semi-automatic pipeline to generate real-world embodied planning dataset with the help of GPT-4V. It can solve the current shortage of real-world data for embodied tasks and is of great significance for the development of embodied agents in real-world environments.\n\u2022 We fine-tuned a large vision language model Qwen-VL-Chat(Bai et al. 2023) as the brain of ReLEP. The model helps the embodied agent comprehend the skill library better, therefore, make more reasonable plans. We emphasize that without undergoing fine-tuning, large foundation models are inherently incapable of comprehending skills in an adept manner."}, {"title": "Related Work", "content": "The outstanding capabilities of large language models inspire researchers to build large multimodal models for multimodal instruction-following tasks. Large vision language models like BLIP (Li et al. 2022), BLIP-2 (Li et al. 2023), LLaVA (Liu et al. 2024), InstructBLIP (Dai et al. 2023), Qwen-VL (Bai et al. 2023) and GPT-4V (OpenAI 2023) have shown their capabilities in classification, detection, segmentation, captioning and visual question answering. One of the most exciting discoveries is the possibility that large vision language models can perform embodied tasks like embodied question answering and embodied planning.\nCompared to large language models in embodied tasks, large vision language models take in visual input directly instead of converting it into texts then passing it into large language models (Huang et al. 2022; Song et al. 2023; Liang et al. 2023; Wu et al. 2024). This helps the model to see the scene itself, avoiding information loss and misunderstanding (Hu et al. 2023)."}, {"title": "Embodied Planning via Large Vision Language Models", "content": "Embodied planning can be divided into two types: short-term planning and long-horizon planning. Short-term planning, or low-level planning, focuses on fundamental instructions like move and grasp. With the input natural language instruction, it outputs the trajectories of the robot's joints to complete the task. While long-horizon planning, or high-level planning, focuses on more complex tasks, which normally need to be broken down into a sequence of sub-tasks. These sub-tasks can be defined in the form of natural language or code.\nRT-1 (Brohan et al. 2022) is an end-to-end model specially designed for robots to perform real-world tasks. It processes real-time input images and outputs action tokens to control a mobile robot arm in a closed loop. RT-2 (Brohan et al. 2023) puts forward the concept of vision-language-action-models. They express the actions as text tokens by replacing the original natural language tokens on embodied tasks, and fine-tune large vision language models on both robotic trajectory data and vision-language data. RT-H (Belkhale et al. 2024) introduces a hierarchical method that predicts language motions from the natural language instruction and then predicts actions from the language motions.\nThe above Robot Transformers control the robot end-to-end, with specially designed model or fine-tuned vision-language-action-models.\nVoxPoser (Huang et al. 2023b) sees the short-term planning problem from a different perspective. It passes language instruction into an large language model to generate Python style Numpy 3D-array of constraint map, and combines RGB-D camera inputs to compose affordance map using a vision language model. These voxel maps instructs"}, {"title": "Methodology", "content": "We first introduce the formulation of real-time embodied planning problem in Sec. 3.1. Then we present the semi-automatic pipeline we used to generate real-world embodied task planning dataset in Sec. 3.2. Finally, details of our framework are discussed in Sec. 3.3."}, {"title": "Problem Formulation", "content": "Given an embodied task $E$ in natural language and an image of the scene $I$, the agent decomposes $E$ into a sequence of skill functions $P$. These skill functions belong to a set of pre-defined skill library $\\Pi = \\{s_1, s_2, ..., s_n\\}$. By executing $P$ in succession, the agent should be able to complete task $E$.\nTo complete embodied tasks in real time, the robot not only needs to make reasonable plans $P$, but also has to execute them and react. A real-time embodied planning problem can be viewed as multiple long-horizon embodied planning problems arranged in the time dimension.\nAt time $t_0$, the agent is given an embodied task $E$ in natural language and an image of the current scene $I_0$. Then the agent decomposes $E$ into a sequence of skill functions $P_0$."}, {"title": "Dataset Acquisition", "content": "The dataset we used for fine-tuning consists of two parts: long-horizon embodied planning data that we constructed (5K + 24K) and part of language-image instruction-following dataset LLaVA-Instruct-150K (15K) (Liu et al. 2024). Based on 5K initial plans, we generate 24K sequential plans for second round planning. A certain proportion of language-image instruction-following data is used to retain the generalization ability of the model.\nThe pipeline to generate embodied planning dataset shown in Figure 2 takes two steps: using images to generate tasks, then using the paired image and task to generate plan. In the end, we will obtain a set of triplets (image, task, plan) then re-form them into dialogues of the first round and sequential rounds.\nWe first collected a set of indoor object detection datasets (Jaiinsom 2022; Kipuw 2023) from the Internet. Object detection has a large foundation of image data, and the images from indoor object detection dataset are often more closely related to embodied tasks. Then we manually filter out the images whose scenes are not suitable for generating embodied tasks. After obtaining the filtered images, we use the following prompt to query GPT-4V to generate five tasks that a robot may perform in the scene of those images. Sometimes, GPT-4V may hallucinate and generate tasks related to objects that are not in the scene. We then modify or delete these tasks based on the hallucination and feasibility.\nWith the paired images and tasks we generated from the first step, we construct a simplified prompt for embodied planning to query GPT-4V to create a plan in a specified format. This simplified prompt includes our complete skill library so that the generated plans will not miss on using any skills. Studies have proven that GPT-4V is capable of generating reasonable plans according to zero-shot instructions or few-shot examples (Hu et al. 2023; Zhi et al. 2024). However, due to not fully understanding the library, GPT-4V sometimes create unreasonable plans. Finally, we manually modify unreasonable plans within the boundary of the skill library and physical constraints."}, {"title": "ReLEP", "content": "The real-world long-horizon embodied planning framework consists of five parts: a large vision language model for planning, three storage modules for prompting and an execution module for interacting.\nAt time $t_0$ an embodied task $E$ and the initial image $I_0$ are passed into the framework. With specific guidance in the Skill Library and Robot Configuration, the large vision language model generates its initial plan $P_0$. Then, the execution module performs the first step of the plan by calling the corresponding skill function. The environment changes as the first step is completed at time $t_1$. Finally, $P_0$ is recorded in to the Memory module as well as the finished steps and robot status.\nFor up coming time stamp $t_i$, the same embodied task $E$ and the current image $I_i$ are passed into the framework. This time, the model refers to all three modules of Memory, Skill Library and Robot Configuration to make $P_i$. Then the first step of $P_i$ is executed and information is recorded like in the first round. It's noteworthy that only plan $P_i$ will be shown to the model at time $t_{i+1}$ rather than all past plans.\nThe loop will break at time $t_{i+1}$ if $P_i$ only contains one step and ends with a Done state.\nThe pseudo code of the entire process is shown in Algorithm 1 for better understanding.\nThe center of the framework is a large vision language model predicting plans based on the information from other modules. As mentioned before, vision language models take the image of"}, {"title": "Experimental Setup", "content": "We designed eight off-line daily tasks in real-world environment for the embodied agents to complete. Seven of them involves complex long-horizon embodied planning and one of them is embodied question answering. These tasks are listed in Table 2. It is worth noting that even if described in different words, as long as the instructions express the same task, the agent should make the same plan.\nand CLOSE LAPTOP are designed to verify the basic Push and Pull abilities. The agent needs to determine whether to use Push or Pull and choose the correct directions. While PUSH CHAIR and UNPLUG POWER requires the agent to generalize Push and Pull to other actions."}, {"title": "Baselines and Metrics", "content": "We chose a fine-tuning method EmbodiedGPT (Mu et al. 2024) and a GPT-4V method VILA (Hu et al. 2023) as our baselines. We also tested a ReLEP whose fine-tuned model is replaced with GPT-4V, referred as GPT-4V* Although EmbodiedGPT is already fine-tuned with EgoCOT, its generated plans are still in natural language. Therefore, we cannot use these plans directly to control a robot."}, {"title": "Results and Analysis", "content": "We present the quantitative evaluation results on designed tasks in Table 2. We count the baselines' plans success as long as they are logical, although the output skill sequence may not meet the strict requirements for execution. For without in-context examples, it is difficult for the model to understand the correct usage of skills. For ReLEP, we require its plan to be executable. For simplicity and convenience, we omit the steps that would not change the environment image during testing.\nEmbodiedGPT is not designed for real-time planning, so we only tested its IPSR. By using a combination of the prompt from ReLEP and EmbodiedGPT, we asked EmbodiedGPT to write a detailed plan for each tasks. EmbodiedGPT does not use Navigate to approach the target, so it fails to complete the tasks. This may be because it was not trained on tasks involving navigation.\nGPT-4V* ignores the Previous Plan and Finished Steps provided in the prompt and repeats planning from scratch at each step, resulting in its inability to complete the tasks. Therefore, its SR is not presented. GPT-4V* plans well on the initial plan, except for the BRING WATER task. In the BRING WATER task, the fridge in the first image is closed. Seeing the image, GPT-4V* believes there is no water in the fridge and aborts most of the time without opening the fridge.\nVILA* performs well on sequential steps. This is because it provides both the initial environment image and the current image. As long as its initial plan is reasonable, the subsequent planning would be successful in most cases. In the PUSH CHAIR task, VILA* tends to overthink. It pulls and pushes the chair to avoid imagined obstacles under the table. In the UNPLUG POWER task, VILA* keeps predicting wrong directions. VILA* cannot combine existing skills to create new skills. It sometimes uses skills not provided in the library. Like in the BRING WATER task, VILA* has trouble opening the fridge door. It either predicts wrong directions or plans to ask human for help, and even try to detect water without opening the fridge.\nGPT-4V* has a higher IPSR than VILA*, showing that our framework helps to generate more reasonable plans and suppress illusions.\nReLEP achieves high success rates on all tasks, demonstrating the effectiveness of out method. Its higher IPSR than GPT-4V* and VILA* showcases ReLEP's outstanding ability in real-world long-horizon planning and its mastery of a larger skill set. Its higher SSR than GPT-4V* suggests that fine-tuning the large vision language model is necessary. It helps the model to better understand how each module works and standardizes the usage of skills.\nAccording to the response, the agent managed to realize that it is itself, a humanoid robot, in the mirror. This demonstrates the effectiveness of the implementation of EQA skill, proving that ReLEP has the ability to perform Embodied Question Answering."}, {"title": "Conclusion", "content": "We proposed a novel frame work for real-world long-horizon embodied planning. We fine-tuned a large vision language model to predict plans according to real-time environment image and a carefully designed skill library. A Memory module helps the agent to recall past plans and robot status, while a Robot Configuration module enables the framework to support different types of robots. We also propose a semi-automatic pipeline for real-world embodied planning data generation to address the problem of dataset scarcity. Real-world off-line experiments across eight daily embodied tasks are conducted. Results demonstrate the effectiveness of ReLEP and suggests that fine-tuning helps the model better understand the skill set.\nUnfortunately, we did not conduct experiments on real robots. With the help of open-set object detection methods (Liu et al. 2023; Ren et al. 2024), we successfully implemented Detect in real-world environment. However, we did not implement some of the important skills in real-world environment like Navigate, Push, Pull and Put. Our future research will focus on fully realizing a robot that works in a real environment with ReLEP."}]}