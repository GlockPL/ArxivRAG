{"title": "SLIM: Sim-to-Real Legged Instructive Manipulation via Long-Horizon Visuomotor Learning", "authors": ["Haichao Zhang", "Haonan Yu", "Le Zhao", "Andrew Choi", "Qinxun Bai", "Yiqing Yang", "Wei Xu"], "abstract": "We present a low-cost quadruped manipulation system that solves long-horizon real-world tasks, trained by reinforcement learning (RL) purely in simulation. The system comprises 1) a hierarchical design of a high-level policy for visual-mobile manipulation following instructions, and a low-level policy for quadruped movement and limb-control, 2) a progressive policy expansion approach for solving the long-horizon task together with a teacher-student framework for efficient high-level training of the high-level visuomotor policy, and 3) a suite of techniques for minimizing sim-to-real gaps.  With budget-friendly but limited reliability and performance hardware, and just one wrist-mounted RGB camera, the entire system fully trained in simulation achieves high success rates for long horizon tasks involving search, move, grasp, and drop-into, with fluid sim-to-real transfer in a wide variety of indoor and outdoor scenes and lighting conditions.Extensive real-world evaluations show that on the long horizon mobile manipulation tasks, our system achieves good performance when transferred to real both in terms of task success rate and execution efficiency. Finally, we discuss the necessity of our sim-to-real techniques for legged mobile manipulation, and show their ablation performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Until recently, robots have primarily been limited to static, highly repetitive tasks, often in industrial manufacturing. As the demand for autonomy grows, the need emerges for robots to navigate and interact with their environment \u2013 commonly referred to as mobile manipulators. Such systems typically handle primitive tasks, such as searching, grasping, and transporting objects, to achieve overarching long-horizon goals.  Several promising directions exist in training autonomous mobile manipulators: imitation learning from human demonstrations, reinforcement learning (RL) in the real world, and RL in simulation with sim-to-real transfer. Each of the directions has its pros and cons.  Imitation learning benefits from learning directly from human experts teleoperating the hardware, thus, reasonable behavior and some successes can be achieved even with a small"}, {"title": null, "content": "amount of training. However, it can be expensive to collect demonstrations to cover all the combinatorial variations of the scene, the task setup, and the hardware. Reinforcement learning (RL) can automate policy optimization so minimal human effort is needed to adapt to the environment and task variations. Furthermore, the resulting policy is often more efficient than human teleoperation. However, mastering long-horizon tasks in the real world still poses a significant challenge. Directly training an RL policy in the real world can be challenging due to hardware constraints and safety issues. Having to rely on real robots, training efficiency can be low, and scalability is often limited. Additionally, a model trained in one physical setup may not generalize to another with for example different background or floor texture.  RL in simulation solves the scalability and adaptability issues by running massively parallel simulations with randomized setups. The difficulty lies in the sim-to-real gap. Shortcomings of the design often only shows up in real world testing, at the very end of the development cycle. Making things worse, several iterations of modeling, design tweaking, policy training, and real testing are often needed to close up a particular sim-to-real gap. Given such a long and often expensive pipeline of tuning steps, RL in simulation can be slow to arrive at a working prototype, and can be expensive in terms of engineering costs and server time for simulation and training. Therefore, the hope is to develop general techniques to bridge the sim-to-real gaps, so that efforts spent on developing the training pipeline and the robotic system for some tasks can be reused for training future tasks. This work focuses on training an end-to-end RL policy in simulation, and zero-shot deploying in the real world on a relatively low-cost robotic system to solve long-horizon, intricate quadruped manipulation tasks. Weak and often faulty hardware, the complexity of legged manipulation, and the long horizon are some of the factors that would compound and significantly expand the sim-to-real gap, making this a daunting task.  To the best of our knowledge, this is the first working solution of a legged manipulator fully trained with RL in simulation, for long horizon tasks, with low cost hardware.  We use a quadruped instead of a wheel base, for it can adapt to different terrains, but manipulation is also more challenging on legs, due to the unstable legged base.  This results in a large 19 DOF (12 leg joints, 6 arm joints, and 1 gripper) system, exacerbating the already difficult long-horizon RL exploration issue. To mitigate this, we utilize a hierarchical design consisting of a high-level mobile manipulation policy and low-level quadruped controller, and also rely on a teacher-student training framework similar to prior work [29, 59].  The teacher policy has access to privileged information (e.g., structured and accurate object features) that is unavailable in the real world. To tackle the difficulties of long-horizon task learning, we also incorporate task decomposition as a form of privileged information, where boundaries between subtasks are defined. The student policy is then trained to distill the teacher policy while maximizing task rewards, but with access to only"}, {"title": null, "content": "standard sensor data such as camera feed and proprioceptive state. We also condition the student visuomotor policy on language instructions, allowing us to naturally command the robot to achieve different tasks.  We name our robotic system SLIM (Sim-to-Real Legged Instructive Manipulation). We reiterate that SLIM is a complete robotic system with fluid sim-to-real transfer. Unlike prior work [29, 52], SLIM learns all modules in simulation from scratch, without the need for any pretrained model or real-world finetuning. SLIM relies on pure RL, and thus it does not require a single demonstration trajectory. Our contributions are as follows:  1) We develop a low-cost quadruped manipulation system SLIM. To our best knowledge, SLIM is the first end-to-end system for solving long-horizon real-world mobile manipulation tasks from RL alone. 2) To address the long-horizon RL exploration challenge, we propose a hierarchical framework for training SLIM's high-level mobile manipulation policy and low-level quadruped controller, and adopt a teacher-student setup for high-level training efficiency. 3) We identify several techniques that are crucial for SLIM to achieve successful sim-to-real transfer with inexpensive hardware, as shown in real-world ablation studies. 4) We conduct an extensive set of simulated and real-world experiments, achieving higher success rate and faster task completion than baselines. In the real world, we demonstrate robust task success across a wide range of indoor and outdoor environments."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Learning-based Quadruped Locomotion", "content": "Traditionally, quadruped locomotion has been tackled through classical control methods designed to follow hand-tuned gaits on flat ground [16, 8], dynamic rigid platforms [23], discrete terrain [2], and rough terrain [11]. Though impressive, such methods require significant human engineering efforts and can be brittle to environmental changes.  More recently, there has been explosive progress in using learning-based approaches for achieving quadruped locomotion. In particular, sim-to-real reinforcement learning (RL) has arisen as a robust solution showcasing impressive feats such as parkour [60, 7, 4] and speeds of up to 3.9m/s [34]. Other works have also focused on exploiting the strengths of legged locomotion over their wheeled counterparts through utilizing the strong inherent coupling of proprioception with egocentric vision [15, 1] and active estimation of the environ-ment [36, 27, 30]. Furthermore, minimal tuning is necessary compared to classical approaches [13] with massively paral-lelized simulations having been shown capable of learning gaits in just minutes [42]. Finally, data-driven legged locomotion has shown great promise in its ability to zero-shot generalize to new morphologies [12], surfaces [35, 27, 30] and agile skills [49, 26]."}, {"title": "B. Legged Mobile Manipulation", "content": "Significant advancements in robust locomotion have fur-ther enabled researchers to push the boundaries of legged mobile manipulation. Building upon data-driven approaches, quadrupeds have been demonstrated pushing objects with their body [25], dribble around balls [26], and manipulate objects using egocentric [48] or calve mounted grippers [28]. Still, the most popular setup has been the traditional top-mounted 6DOF manipulator design [14, 17, 29, 58, 52, 33, 37, 44]. In addition to significant workplace expansion, manipulators offer other nuanced benefits, such as assisting with balance [21] and serving as an intuitive interface for collecting human demonstrations [17]. Classical approaches to solving top-mounted legged mobile manipulation consist of designing wholebody controllers [44] or even combining locomotion learning with model-based manipulation control [33]. Others simplify the need for low-level controllers by using high-level API calls (\u017c, \u03b8) provided by their respective quadruped robots to achieve graspability-aware policies [58], navigational pick-and-place [52], and language-conditioned mobile manipulation [39]. Complex tasks have also been achieved by leveraging teacher-student training setups in simulation for grasping [29] and door opening [59]. Finally, to expand the robot's workspace even further, wholebody loco-manipulation has arisen where leg joints are actuated in a way to assist manipulation [14, 29, 37].  Most similar to our work, Liu et al. [29] introduced VBC, which trains a sim-to-real visuomotor whole-body loco-manipulation policy using a high-low hierarchical model and teacher-student training setup for the object pickup task. They achieve impressive results across a diverse range of objects by leveraging whole-body manipulation, though this approach comes with several key caveats. First, VBC requires the user to manually click on the target object to perform an initial segmentation using a third-party general-purpose model. This segmentation is then used to track the object through another third-party model. Consequently, to ensure consistent segmentation, the object must always remain within the frame. This requirement inherently limits the feasible starting state distribution of VBC, as the target object must be placed within a short distance in front of the robot and in view. Finally, due to the close proximity and visibility of the object, the VBC task horizon is relatively short, ending as soon as the object is lifted. In comparison, 1) SLIM is a complete and self-contained system. All modules-vision, high-level visual loco-manipulation, low-level motor control-are trained within a single framework, resulting in both minimal latency and full autonomy. 2) SLIM is fluid and intuitive. Conditioned on language, SLIM is able to rapidly accomplish tasks in a wide variety of scenes, requiring just a user language command. 3) Finally, SLIM is practical. With SLIM, we demonstrate impressive real-world, long-horizon task completion for a multi-stage mobile search and manipulation task."}, {"title": "III. ARCHITECTURE OVERVIEW", "content": ""}, {"title": "A. Observations and Actions", "content": "Observations. The robot observation at each time step consists of three components: \n1  A tokenized instruction vector of length L. o A temporal stack of RGB images from a single wrist-mounted camera with a shape of (N, H, W, 3), where N is the stack size, and H and W are the image height and width, respectively. The environment is always partially observed by the robot. Sr A temporal stack of proprioceptive state vectors of the robot with a shape of (N, D), where D is the dimensionality of the concatenation of all joint position readings.  Privileged Observations (only available in simulation). To achieve better learning efficiency, we will first train another policy using privileged observations, and then distill it to our robot policy conditioned on the standard observations. This guidance policy has a subtask id input k in place of 1 and a lower-dimensional object observation sp in place of o:  k An integer in [1, K] indicating which subtask the teacher is currently solving. K is the maximal number of subtasks forming a long-horizon task. Sp A group of temporal object feature stacks, where each stack has a shape of (N, Qm) and 0 < m < M denotes the feature index. M is the maximal number of task related objects in the scene. Note that object features are also partially observed and their visibility is always determined by the robot's camera field of view.  The privileged information can be extracted or computed from the underlying simulator state. Actions. The robot policy outputs a triplet of actions:  aarm A vector in [\u2212z, z]6, indicating the delta changes to the arm joint positions. z is the maximally allowed joint position change for a control interval. g A target gripper position in [0, 1]. aleg The target quadruped joint positions in R12. In total, a complete action a has 19 dimensions."}, {"title": "B. Hierarchical Policy Structure", "content": "The system as a whole takes visual and instructional input and controls all joints of the quadruped and the arm, which is a daunting training task with high-dimensional input and output. We use a two-level hierarchical policy to divide the complexity of visual mobile manipulation training from legged locomotion training, similar to [29], illustrated in the left half of Fig. 2.  The high level takes visual, instructional, and proprioceptive inputs, and outputs a simple locomotion speed command and arm control command, while the low level takes proprioceptive input and outputs leg joint commands to track the locomotion"}, {"title": null, "content": "command. The assumption is that quadruped locomotion control is largely independent of the high-level task semantics.  Intermediary Action. To enable this two-level policy structure, we introduce an intermediary action as follows,  c A vector in R2, containing the target forward and angular velocities for the quadruped.  With this, a high-level policy is defined as  (1, o, Sr) \u2192 ahi (aarm, g, c) \u2208 R\u00ba,  while the low-level policy is defined as  (c, Sr) \u2192 aleg \u2208 R12.  Note that with this decomposition, the low-level policy no longer observes task specific inputs (1, 0), and has a much shorter time horizon.  Low-Level. The low-level policy is a quadruped controller that generates joint position targets for PD control to follow a task-agnostic 2D command c [35]. The low-level policy is trained in simulation using PPO [43] and transferred to real. An illustration of the training can be found in the first block in the right column of Fig. 2. It is worth noting that we randomly sample both command c and arm joint actions aarm during low-level training. Since the arm mounted on the quadruped can have varying poses during the task, the low-level command following ability must generalize to various arm configurations. That is, the quadruped base has to learn to keep balance while achieving locomotion commands regardless of the arm's"}, {"title": null, "content": "current joint positions and movements. Similar to most sim2real approaches, we also randomize the simulation environment during training. Besides widely-used domain randomization parameters as in [14], we further randomize the simulated delay of each sensor and foot softness of the robot, to better adapt to variations in real deployment. Rewards design and full details of the training setup can be found in Appendix A-B.  High-Level. The high-level policy is responsible for outputting the command c and the arm control (aarm, g), conditioned on language 1 and the current stack of RGB images o. As demonstrated in Fig. 2, the command c is forwarded to the low-level policy which then follows the received command for a number of time steps, before the high-level policy outputs the next command. For the manipulator, we opt to operate in joint space (as opposed to task space) and thus no inverse kinematics is needed. Once the low-level policy is trained, it is frozen and used as a base controller by the high-level policy. In the remainder of this paper, we will only talk about the high-level policy unless otherwise stated."}, {"title": "C. Teacher-Student Training", "content": "Given the challenges of an enormous exploration space implied by long-horizon tasks, we adopt a teacher-student learning framework [10, 29, 59] for better high-level training efficiency. The overall structure of the framework is shown in the second and third blocks of the right side of Fig. 2. As shown, the teacher is trained purely with RL from privileged, structured, and low-dimensional environment data (k, sp, Sr) that can only be obtained from the simulator. After a high"}, {"title": null, "content": "performing teacher policy is learned, it is frozen and used to guide the student's policy learning via a distillation loss. As the student will eventually be deployed in the real world, its inputs are consistent with what our robot system has. Besides policy distillation, we also use an RL objective to allow the student's behavior being shaped by the same set of task rewards used by the teacher. Later, we will show improvements of this distillation-guided RL setting, compared to pure policy distillation (Section VIII-B Distillation Only baseline.). In the next two sections, we will provide more technical details of the teacher and student."}, {"title": "IV. THE TEACHER: PROGRESSIVE POLICY EXPANSION FOR LONG-HORIZON TASK LEARNING", "content": ""}, {"title": "A. Long-Horizon Task Learning Challenges", "content": "Since we are targeting at solving long-horizon tasks via RL, we need an effective approach to train a policy to solve long-horizon tasks successfully. There are are few key challenges that must be addressed for solving the long horizon tasks: 1) Continual exploration: When learning a long-horizon task via RL, there may be a number of intermediate milestones that must be sequentially achieved. Therefore, the teacher must be able to keep exploring at its frontier despite solving an intermediate milestone stage, in pursuit of solving the full task. Without the ability of continual exploration, it is more likely to converge to a sub-optimal solution that solves the task only up to some intermediate milestones, therefore cannot solve the full task completely. 2) Loss of plasticity and catastrophic forgetting: As the teacher progresses into a task, it needs to properly cope with both the loss of capacity/plasticity [31, 32] of the network due to the evolving nature of the problem as well as the data imbalance during long-horizon RL task exploration and training, without impacting skills that have already been acquired (catastrophic forgetting)."}, {"title": "B. Progressive Policy Expansion", "content": "Given the challenges listed above, we build upon the idea of Policy Expansion (PEX) [57] originally used in the context of offline-to-online RL. Intuitively, PEX divides the full learning process into stages (e.g. offline and online stages in [57]) and uses dedicated networks for learning within each stage. This way, the trainer allocates new network capacity for continual exploration and learning of new skills without affecting any existing skill (catastrophic forgetting).  Here we generalize this idea for addressing the challenges in long-horizon task learning as mentioned in Section IV-A, by extending it to multiple stages. More concretely, we partition the full long horizon task into a sequence of stages (subtasks), and then perform a policy expansion operation for each new encountered subtask. By doing this, it can effectively combat the challenges in long-horizon task learning, keep exploring beyond the currently solved subtasks to solve the subsequent subtasks, circumvent the loss of plasticity by leveraging the newly"}, {"title": null, "content": "allocated policy network 1 for learning, and avoid catastrophic forgetting by retaining the policy networks for previous subtasks. Given these progressive multi-stage expansions, we dub this strategy Progressive Policy Expansion.  Formally, we represent the full long-horizon task T as a composition of a set of subtasks  $T = C(\\lbrace \\tau_k\\rbrace_{k=1}^K)$ (1)  where C is a composition operator, assembling the set of subtasks {\u03c4k} into the full task T. K denotes the maximum number of subtasks. We also define a task decomposional operator as follows, mapping an input state to the corresponding subtask index:  $k = D(s) \\quad k\\in \\lbrace 1,2,\\dots, K\\rbrace,$ (2)  where s = [Sr, Sp]. We use $ \\pi_{\\theta_k}(a|s)$ to denote the teacher sub-network that is responsible for solving its corresponding subtask Tk. When there is no confusion, we omit the subscript of the teacher network $ \\pi_{\\theta_k}$ as \u03c0\u03ba. Figure 3 provides graphical illustrations of the teacher sub-network structures. Each sub-network (Figure 3 left) takes low-dimensional proprioceptive and privileged observation as input and is responsible for learning to solve the corresponding subtask.  We can then solve the long horizon task T via Progressive PEX as follows. We initiate the exploration and learning with a single policy network \u03a0 = {\u03c0\u00b9}. This policy network is responsible for learning to solve the initial subtask. Whenever a new subtask is encountered, a policy expansion operation"}, {"title": "V. THE STUDENT: POLICY DISTILLATION GUIDED RL", "content": "The student needs to perceive the surrounding environment through RGB images. There is no input regarding which subtask the student is currently solving, and it has to infer this information from the inputs. Because of this, the student learns only a single policy across the entire long-horizon task (no sub-task dedicated policy).  To tackle these challenges, we train the student by distilling the teacher's multiple sub-task policies into a single task policy \u03c0stu. However, distillation will only give us a policy that works fine but not quite well. It could lead to the state drifting or out-of-distribution (OOD) issues when the policy is rolled out for a long horizon. There is also always an observation gap between the teacher and the student, which sometimes creates a difficulty in minimizing the distillation loss. Moreover, due to the differences in model architectures and inductive biases, the student might need a different way of mastering certain skills. Finally, distillation makes the student's performance upper bounded by the teacher's. Given these considerations, we choose to boost the student's policy with RL under the same set of task rewards used by the teacher (Fig. 4).  We train the student by modifying SAC [18] to incorporate the distillation loss properly. First, we use a mixed rollout strategy to generate replay data. When rolling out a new episode, with a probability of \u1e9e we will sample from the student policy, or otherwise from the teacher policy. On one hand, we wish to use the teacher to efficiently generate high-performing transitions along the long horizon. On the other hand, we want to keep exploring with the student's own policy. Second, following SACLite [54] we remove the entropy reward from policy evaluation. Then for policy improvement, we replace"}, {"title": null, "content": "the entropy term with the distillation loss, and assign a fixed weight a to it:  $A_{stu} max E_{(o,s_r,s_p,l,k) \\sim D} E_{a_i \\sim \\pi_{stu} (a|o, s_r, l)} Q ((o, s_r, l), a_i)  -KL[\\pi_{tea}(s_r, s_p)||\\pi_{stu} (-o, s_r, l)]].$  To ensure that the KL term encourages enough randomness and exploration for \"stu, we perform a policy surgery on #tea where we keep the action distribution mode unchanged but assign a fixed modal dispersion (e.g., std. for Gaussian). This makes our distillation loss the best of two worlds: imitation learning and entropy regularization.  When designing the student's representation model, we specifically take visual sim2real gap reduction into considera-tion, because once after the student is trained, it will be directly deployed in real without any finetuning. While the student could have a naive representation model that simply fuses multimodal inputs and generates a latent encoding for its policy to use, we"}, {"title": "VI. TASK IMPLEMENTATION IN SIMULATION", "content": "We use a long-horizon pick-and-place task as a concrete example in this work (c.f. Figure 2 left for an illustration of the task), and we will use this task as a concrete example for illustrating the task decomposition as well. Given a language instruction (e.g., \u201cDrop the blue cube into the green basket.\"), the robot needs to locate the target object, picking it up and then drop it into the target basket.  As mentioned previously in Section IV, SLIM employs a task decompositional approach paired with progressive PEX for training a teacher policy to solve the long horizon task. More concretely, for the long-horizon task under consideration, it can be naturally broken down into the following sequential steps (c.f. Figure 5): 1) Search: the robot should first search for the target object (the green cube) for picking up (a subtask), based on the RGB images from the gripper mounted ego-centric camera; 2) MoveTo: after finding the target object, the robot moves to the object until it is within the reach of the arm; 3) Grasp: then the robot moves the gripper towards the target object for picking it up. During the grasping pro-cess, the robot base should ideally move less compared to the MoveTo subtask to facilitate accurate grasping; 4) SearchWObj: after picking up the target object, the robot should keep it in the gripper and then search for the target basket; within this stage, there should be some coordination between arm and locomotion movement to facilitate search while avoid the cube to be dropped. 5) MoveToWObj: after finding the target basket, the robot should move towards it while keeping the object in gripper, until the target basket is within a range that is appropriate for the robot to reach for dropping the object to its gripper; 6) MoveGripperToWObj: the robot can now move the gripper towards the target basket until reaching an appropriate position on top of the basket for dropping; 7) DropInto: the robot releases its gripper and drops the cube into the basket.  In addition, we can further append an auxiliary Idle subtask, which encourage the robot to adjust its arm back to neutral position after dropping the object into the basket, facilitating subsequent deployment. This is an optional subtask that is not required for task completion. We include it in our task implementation to make the final policy behavior safe and more predictable after completing the full task. When evaluating the real world performance (e.g. Section VIII-D), we do not include this auxiliary subtask into the full task metric. We also exclude it whenever we discuss task-relevant decompositions since it is not a task related subtask.\""}, {"title": "VII. SIM-TO-REAL GAP REDUCTION TECHNIQUES", "content": "When training a policy in simulation, we would like to ensure that the policy also performs well in the real world. In other words, we must minimize the \"sim2real gaps\" as much as possible. Generally, we can divide the sim2real gaps for image-conditioned policies into two distinct categories: the dynamics and visual gaps. Here, we outline essential techniques that proved invaluable in addressing both gaps. We stress that the incorporation of such techniques is crucial to real-world performance. For brevity, we discuss a subset of the techniques employed. In-depth details for these techniques can be found in Appendix C."}, {"title": "A. Dynamics Gap Reduction", "content": "The dynamics gap is essentially caused by misaligned transition functions P(st st-1, at-1) between the simulated and real worlds. This difference in dynamics will result in the sim-trained policy experiencing covariate shift when deployed in the real world, which if large enough, can destroy policy performance. In fact, even minor covariate shifts can result in task failure if the task requires fine-grained high precision coordination, e.g., grasping a small object. Therefore, to minimize the dynamics gap, we must minimize the impact of the covariate shift when transferring to real. In particular, we use the following key techniques for addressing the issue: 1) arm joint tracking error minimization, 2) domain randomization, and 3) object perturbations."}, {"title": null, "content": "Minimizing Arm Tracking Error via PID Control. We used PID controller for the robot arm. As opposed to PD controllers which will always have gravity-induced tracking errors, the integral term of PID controllers allow for more robust pose-agnostic tracking given that the joint delta targets are sufficiently small for particular operating frequency. This allows us to greatly reduce the dynamics sim-to-real gap while avoiding system identification. More details can be found in Appendix C-A.  Arm Control Perturbation. Random structured noise is added to the arm control signal to make the learned policy to be robust w.r.t. different sources of control noises arising when transferring to real.  Arm Mount Perturbation. We randomly perturb the arm mount position and yaw to make the learned policy robust to the sim-to-realgaps that could arise due to the actual mounting accuracy, torso height variations etc..  Object Perturbations. The final strategy we employ are object perturbations. Due to the relatively deterministic nature of simulations (in contrast to the high-entropy real world), we observed that policies often converge to deterministic strategies. For example, for picking up an object, a policy may learn to always position itself so that the object has the same relative position to its base. This deterministic \u201cmemorizing\" behavior can result in task failure if the conditions for success are even slightly off. Therefore, we enforce learning robust, reactive policies by perturbing objects of interest during various stages of task execution. Empirically, this helps with both the exploration (as the probability of stuck in a local state is reduced) and the robust ness of the policy (as the state coverage is enlarged)."}, {"title": "B. Visual Gap Reduction", "content": "The second source of sim2real gap for image-conditioned policies is the visual gap. This gap is caused by an RGB distribution mismatch between simulated and real world pixels. Since we do not assume knowing the target scenes in advance, we have to ensure that the perception model is able to handle a wide range of vision scenarios. Accordingly, we apply various visual sim2real gap reduction techniques to student training, in addition to the visual information bottleneck in Section V"}, {"title": null, "content": "Image Domain Randomization. Inspired by SECANT [10], we apply pixel-level perturbations to every RGB image to increase the robustness of the perception model and the chance of its successful sim2real transfer. Specifically, we will randomly select a transformation from the list (Gaussian Noise, SaltPepper Noise, Speckle Noise, Random Brightness, Gaussian Blur) and apply it to each RGB image. We choose not to introduce higher-level image randomization/generation [53] because it might impact image semantics.  Random Textures. We spawn the robot on an empty ground and randomize the ground texture. The texture can be arbitrary and generally have no specific semantics, as long as they"}, {"title": null, "content": "provide enough visual distraction to the student. This distraction will force the student to learn to focus on the most important visual features that are related to the task at hand: objects with reasonable sizes and target colors. When deployed in real, we hope that this visual focus will get transferred and help the robot locate objects.  Random Spatial Augmentation. To make the visual represen-tation module robust, we incorporate spatial augmentations in the visual representation learning [50, 51].  Random Background Objects. To further increase the visual complexity of the surrounding environment, we randomly sample and spawn a set of background objects in the room around the robot's initial position. There are two categories of background objects: 1) primitive shapes (cuboid, cylinder, and sphere), and 2) Google scanned objects [9]. We make sure that these objects only serve the purpose of distraction and the student will not confuse them with task objects. It could collide with an background object during training with some penalty reward. When spawning an object, we also randomize its size, orientation, and texture (only for primitive shapes)."}, {"title": "VIII. PHYSICAL SYSTEM, EXPERIMENTS AND RESULTS", "content": "In this section, we briefly go over the robot system and the task used to evaluate SLIM."}, {"title": "A. Physical Robotic System", "content": "For our robotic system, we use a Unitree Gol with a top-mounted WidowX-250S manipulator. An Intel RealSense D435 camera is attached to the WidowX's wrist via a 3D-printed mount, as shown in Fig. 7, serving as the robot's sole RGB vision feed. The depth stream is not used in this work. Additionally, custom 3D-printed, elongated parallel fingers are fitted on the WidowX gripper to extend its reach. The inner surface of the fingers are padded with a thin layer of foam, which act to increase grasping friction. Overall, our entire robot system is relatively low-cost\u00b2.  All model inference is performed on a laptop with a 12th Gen Intel i9-12900H CPU, NVIDIA RTX 3070Ti laptop GPU,"}, {"title": "B. Baselines", "content": "We compare SLIM with a number of baselines as detailed below:  \u2022 No Arm Retract: removing the arm retract (raise) reward in RL training, which is used to encourage the robot's arm to raise up after grasping the object (introduced in Section VI); \u2022 No Perturb: removing arm control randomization, arm mount perturbation and object perturbation (introduced in Section VII-A) \u2022 No Visual Aug: no background objects and no random spatial augmentation based visual augmentation (intro-duced in Section VII-B) \u2022 Distillation Only: agent training without using the RL loss (introduced in Section V), keeping only the distillation and representation loss.  We also tried another baseline No Distillation, which trains the visuomotor policy directly via RL without distillation from the teacher policy (i.e. RL for visuomotor policy learning from scratch). This baseline cannot learn to solve the task at all in simulation, because of the compounded difficulties of representation learning, behavior learning and long-horizon exploration. We have therefore excluded it from the subsequent real-world experiments."}, {"title": "C. Evaluation Protocol", "content": "We use the Standard object spatial layout as shown in Figure 8 in a real-world scene Lobby. By doing so, it is easier to maintain repeatability across all methods and seeds. Later in subsequent experiments, we will evaluate the robot in other scenes or with other layouts. For task objects, we use cubes and baskets of different colors. More details on these physical objects are detailed in Appendix E-A.  For each method of each random seed, we roll out the robot's policy for 20 episodes with varied initial scene states (relative spatial positioning and object colors etc.) according to the evaluation protocol. For the detailed protocol, please refer to Appendix E-B."}, {"title": "D. Results and Analysis", "content": "We conduct experiments on the baseline and the proposed methods", "efficiency": 1, "Rate": "it is defined as the success rate upto the specified subtask starting from the beginning of the full task. 2) Episode Time: it is defined as the time completing the full task from the beginning. Since we applied a time limit of tmax = 90s in evaluation, for failed episodes, we set their episode time as tmax.  Then we calculate the mean and standard deviation across the seeds as the final aggregated metric values and the results are summarized in Table I"}]}