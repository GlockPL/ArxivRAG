{"title": "LoRA-Mini : Adaptation Matrices Decomposition and Selective Training", "authors": ["Ayush Singh", "Rajdeep Aher", "Shivank Garg"], "abstract": "The rapid advancements in large language models (LLMs) have revolutionized natural language processing, creating an increased need for efficient, task-specific fine-tuning methods. Traditional fine-tuning of LLMs involves updating a large number of parameters, which is computationally expensive and memory-intensive. Low-Rank Adaptation (LORA) has emerged as a promising solution, enabling parameter-efficient fine-tuning by reducing the number of trainable parameters. However, while LoRA reduces the number of trainable parameters, LoRA modules still create significant storage challenges. We propose LoRA-Mini, an optimized adaptation of LoRA that improves parameter efficiency by splitting low-rank matrices into four parts, with only the two inner matrices being trainable. This approach achieves upto a 20x reduction compared to standard LoRA in the number of trainable parameters while preserving performance levels comparable to standard LoRA, addressing both computational and storage efficiency in LLM fine-tuning.", "sections": [{"title": "Introduction", "content": "The rapid growth of large language models (LLMs) such as GPT-3 (Brown et al. 2020), GPT-4 (OpenAI et al. 2024), Llama (Dubey, Jauhri, and Pandey 2024), and Mistral (Jiang et al. 2023) has led to various advancements in the field of natural language processing, enabling LLMs to achieve high performance across various benchmarks such as IFEval (Zhou et al. 2023), SQUAD (Rajpurkar et al. 2016) etc. However, the computational and memory costs involved in the training of these models from scratch are substantial, posing significant challenges. Fine-tuning (Zhang et al. 2024) is a popular method to adapt pre-trained models for specific downstream tasks, but requires extensive resources when tuning all the model parameters, making it impractical for many applications. To address these limitations, researchers developed Parameter-Efficient Fine-Tuning (PEFT) (Han et al. 2024) techniques, which limit the number of adjustable parameters during fine-tuning, achieving accuracy comparable to full finetuning while significantly reducing both computational and memory costs. Among PEFT approaches, Low-Rank Adaptation(LoRA) (Hu et al. 2021) has emerged as a widely adopted technique. The technique adapts large models by adding trainable low-rank matrices to the model's layers. It builds on the observation that fine-tuning updates often have low intrinsic dimensionality. By incorporating this, LoRA achieves high efficiency without a major compromise in model performance, allowing for more scalable adaptation of LLMs to diverse tasks. These advances highlight LoRA's role in balancing efficiency and effectiveness, enabling widespread application of large-scale models in resource-constrained environments.\nOur approach builds on existing Parameter-Efficient Fine-Tuning (PEFT) methods by targeting an even lower parameter count, aiming to maintain model performance while further minimizing memory usage.\nAlthough deep networks may have a vast number of parameters, but only a small number of parameters significantly impact the learning process. Hence, we refine the intrinsic rank within the current LORA framework by decomposing the standard matrices A and B into two sub-matrices each, with only one matrix from each pair being trainable.\nOur experiments cover a range of models, including BERT (Devlin et al. 2019), ROBERTa (Liu et al. 2019), and T5 (Raffel et al. 2023), with a primary focus on performance metrics from the GLUE (Wang et al. 2019) benchmark and English-Romanian translation task from WMT16 (Bojar et al. 2016) dataset. Our results show that LoRA-Mini attains accuracy comparable to full fine-tuning approaches while significantly reducing memory requirements. Our key contributions include:\n1. Evaluating the effectiveness of selectively freezing parameters within LoRA layers while maintaining model quality.\n2. Minimizing the number of trainable parameters relative to earlier PEFT methods, while achieving performance on par with full fine-tuning and LoRA.\n3. Evaluating the scalability of our approach on diverse set of tasks and models."}, {"title": "Related Work", "content": "Parameter-efficient fine-tuning has emerged as an essential approach for adapting large language models (LLMs) without incurring high computational and memory costs. Techniques such as Adapter-based methods (Houlsby et al."}, {"title": "Methodology", "content": "LORA-Mini focuses on the introduction of selective training within decomposed matrices. We divide each LoRA matrix into a trainable and frozen part, allowing us to limit the update space and execute controlled updates, where the frozen outer matrices guide the training process. The mathematical formulation of our approach can be expressed as follows:\nLet $AB \\in R^{d\\times k}$ represent our target weight matrix where d is input and k is output dimension. We decompose the LORA matrices into:\n\u2022 Outer auxiliary matrices: $A_{aux} \\in R^{d\\times a}, B_{aux} \\in R^{b\\times k}$ (frozen)\n\u2022 Inner trainable matrices: $A_{train} \\in R^{a\\times r}, B_{train} \\in R^{r\\times b}$ (trainable)\nLet x be the input to a layer. Then the output h from the layer after applying LoRA-Mini becomes :\n$h = (W + A_{aux} \\cdot A_{train} \\cdot B_{train} \\cdot B_{aux})\\cdotX$                                                                                  (1)\nIn LoRA, given a pre-trained weight matrix $W \\in R^{d\\times k}$, the weight update $\\Delta W$ is through two matrices, $A \\in R^{d\\times r}$ and $B\\in R^{r\\times k}$, where $r < min(d, k)$. This parameterization leads to $\\Delta W = AB$, resulting in $r \\times (d + k)$ trainable parameters, reducing the training overhead.\nOur method extends this reduction further by limiting the trainable parameters to $r \\times (a + b)$, where a and b are predefined dimensional constraints, as detailed in later sections."}, {"title": "Experiments and Results", "content": "To do a robust evaluation of our approach, we broadly divided our experiment into two categories.\nFirst, we evaluate our approach on the NLU and NLI tasks using the GLUE Benchmark (Wang et al. 2019) on RoBERTa (Liu et al. 2019) and BERT (Devlin et al. 2019). We particularly chose four tasks from the GLUE Benchmark: STSB (Cer et al. 2017), COLA (Warstadt, Singh, and Bowman 2019), MRPC (Dolan and Brockett 2005), and RTE (Giampiccolo et al. 2007). The experiments were conducted using the following configurations :\n\u2022 Full fine-tuning of the entire model\n\u2022 Standard LoRA applied to dense and attention layers (rank values r = 4, 8, 16)\n\u2022 LoRA-Mini applied only to dense layers\n\u2022 LoRA-Mini applied to both dense and attention layers\nWe tested rank values of 8, 16, and 32, with a and b as 8, 16, 32, and 64. This enabled an effective comparison of LoRA-Mini's impact on feed-forward versus attention mechanisms. We did not experiment with applying our approach exclusively to attention layers, as previous research (Geva et al. 2021) has demonstrated that this would not yield significant improvements in overall accuracy. From our experiments, we observed that the configuration with a and b as 64 performs consistently well. Thus, we reported results with varying ranks and keeping a and b constant in the main paper. From Table 1 and Table 2, we can see that our approach performs consistently at par or greater than full-fine-tuning and LoRA. A comparison of rank with accuracy when LoRA-Mini is applied to both dense and attention layers of ROBERTa is shown in Figure 2.\nWe also evaluate the performance of our approach on generative tasks, with a particular focus on language translation. It is a significant benchmark for assessing model performance in real-world applications, as it involves both under-"}, {"title": "Conclusion", "content": "We present LoRA-Mini as a parameter-efficient fine-tuning approach, demonstrating significant improvements in memory efficiency and task performance. Our results indicate that LoRA-Mini can achieve comparable or superior results compared to LoRA and full fine-tuning while substantially reducing the number of trainable parameters across a wide range of tasks, making it an ideal solution for finetuning LLMs in resource-constrained environments."}, {"title": "Limitations and Future Works", "content": "Due to computational constraints, we were unable to evaluate our LoRA approach on larger tasks of GLUE benchmark as well as on larger models such as LLaMA3.1-8B (Dubey, Jauhri, and Pandey 2024), Mistral-7B (Jiang et al. 2023) etc, or on more extensive benchmarks datasets like MMLU (Hendrycks et al. 2021), Math10k (Hu et al. 2023), COMMONSENSE170K (Hu et al. 2023) etc. Expanding to these larger models and benchmarks could provide a more comprehensive understanding of our method's scalability and performance in diverse domains and model sizes.\nFuture research could explore alternative matrix decomposition methods, such as partitioning LoRA matrices into more smaller components and experimenting with various combinations of trainable and frozen sections. Another approach might involve freezing randomly selected matrices (Zhu et al. 2024). Techniques like QR decomposition or singular value decomposition (SVD) could be used to initialize these matrices, potentially enhancing performance. Furthermore, this approach could be tested to approximate feedforward networks (FFNs) (Zeng and Lee 2024). Investigating a latent representation between LoRA matrices might also increase parameter efficiency."}, {"title": "Experimental Setup Details", "content": "Models:\n\u2022 BERT 1- A bidirectional transformer model with 110 million parameters, pre-trained on masked language modeling and designed for natural language understanding tasks.\n\u2022 ROBERTa Base 2- An optimized version of BERT with 125 million parameters, featuring more extensive pre-training, improving performance on a wide range of language tasks.\n\u2022 T5-Small 3- A smaller version of the Text-To-Text Transfer Transformer (T5) with 60 million parameters, optimized for efficiency on generative tasks.\n\u2022 T5-Base 4- The base variant of T5 with 223 million parameters.\nBenchmarks and Datasets :\n\u2022 GLUE Dataset 5: GLUE, or General Language Understanding Evaluation, is a benchmark designed to assess language understanding models across diverse NLP tasks, including COLA (Corpus of Linguistic Acceptability), SST-2 (Stanford Sentiment Treebank), MRPC (Microsoft Research Paraphrase Corpus), STS-B (Semantic Textual Similarity Benchmark), QQP (Quora Question Pairs), MNLI (Multi-Genre Natural Language Inference), QNLI (Question Natural Language Inference), RTE (Recognizing Textual Entailment), and WNLI (Winograd NLI). Of these, we only train on 4 tasks, namely STSB, COLA, MRPC, and RTE.\n\u2022 WMT Dataset 6: The WMT-16 (Workshop on Machine Translation 2016) benchmark is a standardized evaluation framework for comparing translation models across multiple language pairs using reliable metrics like BLEU. In this research, we use the Romanian-to-English subset of WMT-16 to evaluate our model's translation accuracy and generalization across linguistic structures."}]}