{"title": "Automated Explanation Selection for Scientific Discovery", "authors": ["Markus Iser"], "abstract": "Automated reasoning is a key technology in the young but rapidly growing field of Explainable Artificial Intelligence (XAI). Explainability helps build trust in artificial intelligence systems beyond their mere predictive accuracy and robustness. In this paper, we propose a cycle of scientific discovery that combines machine learning with automated reasoning for the generation and the selection of explanations. We present a taxonomy of explanation selection problems that draws on insights from sociology and cognitive science. These selection criteria subsume existing notions and extend them with new properties.", "sections": [{"title": "1 Introduction", "content": "The trustworthiness of AI systems, including a sound rationale for their deployment, is paramount in safety-critical and high-risk applications such as law enforcement and criminal justice, critical infrastructure operations, biometric identification and categorization of natural persons, autonomous vehicles such as self-driving cars or drones, or healthcare diagnostics and medical devices [45]. There are also legal reasons to want intelligible AI, such as the \"European Artificial Intelligence Act\" [8]. Despite their problem-solving capabilities, ML models often suffer from opacity, making their inner workings difficult or impossible for humans to understand. Explainability contributes to building trust in artificial intelligence (AI) systems beyond their pure predictive accuracy and robustness [50].\nEfficient automated reasoning enables exact approaches to the explainability of machine learning (ML) models, and is thus a key technology of the young but rapidly growing field of explainable artificial intelligence (XAI) [34]. Since the training of an ML model is an inductive process that exploits correlations in a set of observations, i.e., training data, the epistemological status of such learned abstractions is subject to the problem of induction [29]. Therefore, any form of formal explanation extracted from an ML model can merely serve as a hypothesis and can potentially be falsified by new observations.\nDiscrimination based on gender, race, or origin is a significant challenge in many areas of contemporary society. Discrimination can be exacerbated by the use of AI systems that are trained on data sets that reflect the same biases that exist in the wider world. This becomes a major concern when AI systems are used to make decisions that affect the life of people. Formal methods for the explainability and verification of AI systems facilitate early detection of flaws in ML models and validation of ML models through coherence with prior knowledge and expectations. This facilitates the identification and prevention of discriminatory practices through the use of AI systems."}, {"title": "2 Automated Reasoning for Explainability", "content": "In recent years, considerable progress has been made in the field of automated reasoning, particularly with regard to algorithmic solutions to the propositional satisfiability problem (SAT) [9]. Automated reasoning and the SAT problem are closely related, with SAT solvers often being a central component in automated reasoning systems. Even though the complexity of SAT is challenging, the ubiquity of NP-hard problems has stimulated research to the extent that modern SAT solvers efficiently solve many SAT instances arising in practice. In addition, modern SAT solvers are suitable for use in safety-critical and high-risk applications because they produce proofs that are given as efficiently checkable sequences of rigorous mathematical reasoning steps [13].\nThe genericity of the specification of the SAT problem is a key factor for their wide range of applications, including the verification of hardwareand software, cryptanalysis, planning, and many more. Advances in SAT solving often lead to advances in related reasoning problems and their optimization variants, such as Maximum Satisfiability (MaxSAT) or Pseudo-Boolean Satisfiability (PBS) and Optimization (PBO). Such advances are also closely related to problems higher up in the polynomial hierarchy, such as Quantified Boolean Formulas (QBF). SAT solvers are also used in the backend of reasoning systems that support richer formal languages, such as SAT Modulo Theory (SMT) solvers [3].\nSAT-based methods are well suited to this task for three main reasons. First, recent advances in SAT solving show that SAT-based methods have the potential to efficiently solve the instances of the NP-hard explanation selection problems addressed in this paper. Moreover, SAT-based methods are particularly trustworthy because they produce proofs that can be efficiently checked for correctness by verified software. Finally, these methods are flexible in that they can be easily adapted to new problem instances or variants thereof by modifying the encoding or objective function of the problem instance rather than the algorithm.\nEfficiency and Trustworthiness\nConflict-driven Clause Learning (CDCL) is the most popular SAT algorithm, because its implementations are extremely efficient in practice.Essential to its success are efficient data structures, implementations, and heuristics for its main pruning techniques, which are unit propagation and clause learning [36].\nWhile solutions to the SAT problem intrinsically are meant to produce a certificate of satisfiability when possible, state-of-the-art CDCL solvers also produce certificates of unsatisfiability that can be verified in time polynomial in the length of the proof. Assuming NP \u2260 co NP, the length of the proof is worst-case exponential in the size of the input formula, but feasible in practice, as its length strongly correlates with the solving time [13]. Certificate checking is a distinct process from certificate creation and can be accomplished with formally verified software, thereby enhancing the dependability of the solver outcomes [48]. This is particularly crucial when it comes to verifying the safety properties of systems for high-risk applications.\nThe study of proof complexity also provides the theoretical background for understanding some of the practical advances and limitations of logic solvers, in particular with respect to resolution-inspired clause learning in modern SAT solvers and new proof systems for SAT solving. CDCL solver runtimes are subject to the same domain-specific exponential lower bounds as proof-lengths in the general resolution proof-system they simulate [5]. The study of more powerful proof systems, such as extended resolution, has a long tradition, and recent implementations of new practical heuristics manage to exploit them. This can be achieved by the addition of propagation redundant clauses [42], or symmetry-breaking clauses [7], or variable definitions [11] that help break lower bounds on the length of unsatisfiability proofs is a way to take advantage of more powerful proof systems.\nRecent advances have also built upon the capabilities of parallel SAT solvers, with the most efficient ones being clause-sharing portfolios of CDCL solvers.The massively parallel SAT solver Mallob builds upon a portfolio of some of the most successful sequential SAT solvers and employs a unique scalable clause-sharing strategy [46].\nThe problem of enumerating all models of a satisfiable formula, a.k.a. AllSAT, has been addressed with a variety of approaches that can be broadly classified into two categories: blocking approaches, which are based on incremental SAT solving using minimized blocking clauses [53], and non-blocking approaches, which ensure exclusive enumeration through chronological backtracking [10]. A recent approach to AllSAT outperforms existing approaches by avoiding no-good learning through the combination of chronological backtracking and implicant shrinking [47].\nThe optimization problem of propositional satisfiability, Maximum Satisfiability (MaxSAT), is defined as the problem of finding an assignment that maximizes the number of satisfied clauses. This problem has two notable variants: a partial variant, in which a subset of its clauses must be satisfied, and a weighted variant, in which the objective is to maximize a sum of clause weights. The MaxSAT algorithms that remain relevant today include model-improving approaches, also known as Linear SAT UNSAT (LSU), core-guided approaches (CG), and implicit hitting set approaches (IHS). These algorithms essentially solve a sequence of SAT instances with an incremental CDCL SAT solver [1]. LSU algorithms solve a sequence of satisfiable instances by adding bound-tightening constraints until the instance is UNSAT.Conversely, CG algorithms solve a sequence of unsatisfiable instances by relaxing the instance in each iteration until the instance is SAT.Similar to core-guided approaches, IHS algorithms solve a sequence of unsatisfiable instances, but tighten the lower bound by computing a minimum-cost hitting set of the accumulated unsatisfiable cores. Recent advances in MaxSAT solvers have been made in multiobjective optimization [27] and in incremental MaxSAT solving [39]."}, {"title": "Formal Explanation Approaches", "content": "In naturally interpretable models such as decision trees, evaluation traces can serve as direct explanations of prediction results [45].In practice, such models often have limited predictive power because they can only generate step-like decision boundaries, are mostly restricted to categorical data, and are prone to overfitting. Moreover, according to recent research in formal explanability, explanations by interpretable models are generally not concise and exhibit redundancy [26].\nHeuristic explanation approaches sample the input space around the instance to be explained and observe the effect on the output space. As a result, these approaches can be applied to any class of prediction models. Due to their probabilistic nature, such model-agnostic approaches do not provide strict guarantees of correctness or minimality of explanations. A prominent example is LIME, a sampling-based approach that locally approximates a given model by an intrinsically interpretable model, such as a decision tree [43]. A formal explainability-based analysis of the quality of the well-known Shapley scores has shown that they can make misleading predictions about the importance of features [18]. Anchors, an approach that computes sufficient conditions along with an estimate of their accuracy for a given prediction, has also been shown to give unrealistically high accuracy estimates of explanations using methods of formal explainability [44].\nExplanations by formal methods are derived from an exact formal language encoding of the learned model and a formalization of the desired properties of the sought explanations. The guarantees of formal approaches are mathematically rigorous, as they are based on mathematically sound deduction systems [20]. In recent years, the emerging formal XAI community has focused on the notions of abductive explanation (AXp) and contrastive explanation (CXp). Here, an AXp, also known as prime implicant explanation (PI-explanation) and minimally sufficient reason, denotes a minimal set of features sufficient to guarantee a given prediction. In contrast, a CXp, also known as minimal required change, denotes a minimal set of features sufficient to change a prediction [22]. There are weak and strong variants in the literature to distinguish subset minimality from cardinality minimality for both AXps and CXps. AXp and CXp computation for tree ensembles is an NP-hard problem [25].\nNevertheless, efficient formal explainers for tree ensembles have been developed, one based on recent SMT and one based on recent MaxSAT approaches. In terms of runtime, both exact approaches outperform even the heuristic explainer Anchor [24]. An important property of AXps and CXps is that they are dual to each other, i.e. contrastive explanations are minimal hitting sets of abductive explanations and vice versa [23]. Thus, AXp/CXp enumeration here is closely related to the well-studied enumeration problems of Minimum Unsatisfiable Subsets (MUS) and Minimum Correction Sets (MCS) in formal logic which are known to be -complete [35]. The state-of-the-art formal explanation enumeration algorithm for decision lists is one that exploits this duality, analogous to the generic MARCO algorithm for MUS/MCS enumeration [21, 31].\nEnsemble learning is a popular approach to improve the accuracy of ML models by combining multiple weak models into a single strong model [12]. Tree ensembles, such as random forests and gradient boosting machines, are well-known examples of ensemble learning methods in which the weak models are decision trees. Tree ensembles achieve high model accuracy with a minimal number of hyperparameters, and their generalization performance is robust even in high-dimensional feature spaces due to randomization in the training process and the combination of many decision trees with limited depth. Although tree ensembles are formed from a large number of intrinsically interpretable decision trees, they themselves are not intrinsically interpretable.\nStudying the formal explainability of tree ensembles is a relevant direction of research because tree ensembles are a common class of ML models used in many practical applications, such as diagnosis [52], or marketing [41].Moreover, they can be used to explain the predictions of any black-box or otherwise hard-to-codify prediction model, such as a large neural network, as is done in Asteryx, which combines sampling-based explanation with formal explanation. Asteryx uses sampling to locally approximate a given black-box model by a random forest trained on the samples drawn. This random forest then serves as a surrogate model for the subsequent computation of exact formal explanations [4]."}, {"title": "3 Desirable Properties of Explanations", "content": "We find that the problem of explanation selection has been under-studied, resulting in a literature that contains redundant and overlapping notions, some of which even have misleading names. To alleviate these problems, we have reviewed the literature on relevant notions of explanation in the social sciences and have identified a set of desirable properties that can be used to guide the selection of explanations.\nThere is an extensive body of research from philosophy, psychology, sociology and cognitive science on the nature of explanations and what constitutes a good, useful or acceptable explanation [37]. On the nature of explanations, we first note that an explanation is an answer to a \u201cwhy?\u201d question.While the concrete question \u201cWhy P?\u201d can be extended to \"Why P instead of not P?", "Why P instead of Q?": 40}, {"title": "Relationship with Theories of Causality", "content": "Even though explanation and causality denote two different things, explanation can be associated with the dependence theories of causality, which include Hume's theory of regularity [19], later refined by Lewis, who defined causality as a matter of counterfactuals [30]. The counterfactual theory of causality is closely related to the notion of explanation, as it is based on the idea that a cause is something that makes a difference to the effect."}, {"title": "Relationship with Theories of Abduction", "content": "Explanation selection is also closely related to the notion of abduction, which was introduced by Pierce as a third mode of reasoning in addition to deduction and induction, and is particularly useful in the context of scientific discovery. While deduction is the process of deriving a result from a fact and a rule, and induction is the process of generalizing a rule from a set of observed facts and results, abduction is to suggest a fact to explain a result with a known rule [2].\nThere is a large body of work on abductive reasoning in the context of the philosophy of logic and scientific discovery [33]. Pierce himself developed a theory of science in which abduction is the first step, i.e. a fact that explains an observation is suggested by scientific intuition, then deduction is used to evaluate the plausibility of the such determined hypothesis, and then induction is used to provide further evidence for the hypothesis through experiments.\nIn AI research, interpretations that capture abduction as a theory of \"Inference to the Best Explanation\" (IBE) are more common [28]. In the epistemological framework of Hintikka, the central concept is an interrogative model of inquiry, in which deduction plays a significant role in the validation of explanations [17]."}, {"title": "4 Evaluation of Explanations", "content": "The authors of a recent comprehensive survey of evaluation methods in Explainable Artificial Intelligence identified a total of twelve evaluation criteria for the explanations generated by XAI tools [38]. In the following, we argue that many of these criteria can be fully satisfied a priori by formal reasoning methods.\nThe criteria correctness, completeness, and consistency, are fully satisfied by SAT-based formal reasoning methods, since they produce certified results that can be validated with verified software. Exact formal approaches can also explicitly maximize the continuity and compactness of the explanations by incorporating them as objective functions in the encoding of the explanation selection problem, while at the same time preserving the correctness and completeness of the encoding.\nHowever, coherence with background knowledge, beliefs, etc. is an inappropriate criterion for evaluating explanations in the context of formal reasoning. This is because, due to the exactness of the formal method, any incoherence of explanations necessarily follows from an incoherence of the model. In the context of heuristic explainers, which are not guaranteed to be exact, it may make sense to check whether the resulting explanations are consistent with what is expected from the background knowledge. In the formal setting, however, if the exact explanation is not consistent with the background knowledge, then the model is not consistent with the background knowledge. Thus, coherence is not a criterion for evaluating the explanation, but rather a criterion for evaluating the model. Formal verification of ML models is a natural complementary application to explanation selection. Verification of ML models can be achieved by encoding expectations as additional constraints in the model encoding to allow proof by contradiction.\nSince SAT-based formal reasoning produces exact and verified results, the confidence in the generated explanations is generally high. However, the whole process still depends on the correctness and completeness of the SAT encoding of the model. This involves parsing the data structures of the ML model and generating a formal language representation of the model such that the function f\u2122 of the model M is preserved in the encoding. This means that for any input I and output O, the formula has a model if and only if fm (I) = 0, provided that the input and output vectors I and O are represented by variables in the SAT formula. The SAT encoding must be verified by additional means, such as formal proofs of the correctness and completeness of the encoding and, in practical implementations, by automated consistency tests.\nThe contrastivity criterion evaluates the possibility of specifying contrast cases. Formal encodings of ML models should therefore explicitly allow the specification of contrast cases in the encoding of the explanation selection problem.\nOther evaluation criteria such as controllability, covariate complexity, composition, and user satisfaction are not directly addressed by the formal reasoning methods themselves, but can be achieved through the design of an appropriate user interface for specfication of user constraints, and for presentation and visualization of explanations and explanation statistics."}, {"title": "5 Conclusion", "content": "We have proposed a cycle of scientific discovery that combines machine learning with automated reasoning for the generation and the selection of explanations. Our findings indicate that automated reasoning methods are well-suited for the generation of explanations for machine learning models, and that they can be used to guide the selection of explanations. We have also introduced a taxonomy of explanation selection problems that draws on insights from sociology and cognitive science. These selection criteria subsume existing notions and extend them with new properties. We discussed how the evaluation of explanations is greatly simplified through the guarantees provided by automated reasoning methods. We conclude that the application of automated reasoning methods to the selection of explanations for machine learning models represents a promising avenue for future research."}]}