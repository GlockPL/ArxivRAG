{"title": "Automated Explanation Selection for Scientific Discovery", "authors": ["Markus Iser"], "abstract": "Automated reasoning is a key technology in the young but rapidly growing field of Explainable Artificial Intelligence (XAI). Explainability helps build trust in artificial intelligence systems beyond their mere predictive accuracy and robustness. In this paper, we propose a cycle of scientific discovery that combines machine learning with automated reasoning for the generation and the selection of explanations. We present a taxonomy of explanation selection problems that draws on insights from sociology and cognitive science. These selection criteria subsume existing notions and extend them with new properties.", "sections": [{"title": "1 Introduction", "content": "The trustworthiness of AI systems, including a sound rationale for their deployment, is paramount in safety-critical and high-risk applications such as law enforcement and criminal justice, critical infrastructure operations, biometric identification and categorization of natural persons, autonomous vehicles such as self-driving cars or drones, or healthcare diagnostics and medical devices [45]. There are also legal reasons to want intelligible AI, such as the \"European Artificial Intelligence Act\" [8]. Despite their problem-solving capabilities, ML models often suffer from opacity, making their inner workings difficult or impossible for humans to understand. Explainability contributes to building trust in artificial intelligence (AI) systems beyond their pure predictive accuracy and robustness [50].\nEfficient automated reasoning enables exact approaches to the explainability of machine learning (ML) models, and is thus a key technology of the young but rapidly growing field of explainable artificial intelligence (XAI) [34]. Since the training of an ML model is an inductive process that exploits correlations in a set of observations, i.e., training data, the epistemological status of such learned abstractions is subject to the problem of induction [29]. Therefore, any form of formal explanation extracted from an ML model can merely serve as a hypothesis and can potentially be falsified by new observations.\nDiscrimination based on gender, race, or origin is a significant challenge in many areas of contemporary society. Discrimination can be exacerbated by the use of AI systems that are trained on data sets that reflect the same biases that exist in the wider world. This becomes a major concern when Al systems are used to make decisions that affect the life of people. Formal methods for the explainability and verification of AI systems facilitate early detection of flaws in ML models and validation of ML models through coherence with prior knowledge and expectations. This facilitates the identification and prevention of discriminatory practices through the use of AI systems."}, {"title": "2 Automated Reasoning for Explainability", "content": "In recent years, considerable progress has been made in the field of automated reasoning, particularly with regard to algorithmic solutions to the propositional satisfiability problem (SAT) [9]. Automated reasoning and the SAT problem are closely related, with SAT solvers often being a central component in automated reasoning systems. Even though the complexity of SAT is challenging, the ubiquity of NP-hard problems has stimulated research to the extent that modern SAT solvers efficiently solve many SAT instances arising in practice. In addition, modern SAT solvers are suitable for use in safety-critical and high-risk applications because they produce proofs that are given as efficiently checkable sequences of rigorous mathematical reasoning steps [13].\nThe genericity of the specification of the SAT problem is a key factor for their wide range of applications, including the verification of hardwareand software, cryptanalysis, planning, and many more. Advances in SAT solving often lead to advances in related reasoning problems and their optimization variants, such as Maximum Satisfiability (MaxSAT) or Pseudo-Boolean Satisfiability (PBS) and Optimization (PBO). Such advances are also closely related to problems higher up in the polynomial hierarchy, such as Quantified Boolean Formulas (QBF). SAT solvers are also used in the backend of reasoning systems that support richer formal languages, such as SAT Modulo Theory (SMT) solvers [3].\nSAT-based methods are well suited to this task for three main reasons. First, recent advances in SAT solving show that SAT-based methods have the potential to efficiently solve the instances of the NP-hard explanation selection problems addressed in this paper. Moreover, SAT-based methods are particularly trustworthy because they produce proofs that can be efficiently checked for correctness by verified software. Finally, these methods are flexible in that they can be easily adapted to new problem instances or variants thereof by modifying the encoding or objective function of the problem instance rather than the algorithm.\nEfficiency and Trustworthiness\nConflict-driven Clause Learning (CDCL) is the most popular SAT algorithm, because its implementations are extremely efficient in practice.Essential to its success are efficient data structures, implementations, and heuristics for its main pruning techniques, which are unit propagation and clause learning [36].\nWhile solutions to the SAT problem intrinsically are meant to produce a certificate of satisfiability when possible, state-of-the-art CDCL solvers also produce certificates of unsatisfiability that can be verified in time polynomial in the length of the proof. Assuming $NP \\neq co NP$, the length of the proof is worst-case exponential in the size of the input formula, but feasible in practice, as its length strongly correlates with the solving time [13]. Certificate checking is a distinct process from certificate creation and can be accomplished with formally verified software, thereby enhancing the dependability of the solver outcomes [48]. This is particularly crucial when it comes to verifying the safety properties of systems for high-risk applications."}, {"title": "Formal Explanation Approaches", "content": "In naturally interpretable models such as decision trees, evaluation traces can serve as direct explanations of prediction results [45].In practice, such models often have limited predictive power because they can only generate step-like decision boundaries, are mostly restricted to categorical data, and are prone to overfitting. Moreover, according to recent research in formal explanability, explanations by interpretable models are generally not concise and exhibit redundancy [26].\nHeuristic explanation approaches sample the input space around the instance to be explained and observe the effect on the output space. As a result, these approaches can be applied to any class of prediction models. Due to their probabilistic nature, such model-agnostic approaches do not provide strict guarantees of correctness or minimality of explanations. A prominent example is LIME, a sampling-based approach that locally approximates a given model by an intrinsically interpretable model, such as a decision tree [43]. A formal explainability-based analysis of the quality of the well-known Shapley scores has shown that they can make misleading predictions about the importance of features [18]. Anchors, an approach that computes sufficient conditions along with an estimate of their accuracy for a given prediction, has also been shown to give unrealistically high accuracy estimates of explanations using methods of formal explainability [44].\nExplanations by formal methods are derived from an exact formal language encoding of the learned model and a formalization of the desired properties of the sought explanations. The guarantees of formal approaches are mathematically rigorous, as they are based on mathematically sound deduction systems [20]. In recent years, the emerging formal XAI community has focused on the notions of abductive explanation (AXp) and contrastive explanation (CXp). Here, an AXp, also known as prime implicant explanation (PI-explanation) and minimally sufficient reason, denotes a minimal set of features sufficient to guarantee a given prediction. In contrast, a CXp, also known as minimal required change, denotes a minimal set of features sufficient to change a prediction [22]. There are weak and strong variants in the literature to distinguish subset minimality from cardinality minimality for both AXps and CXps. AXp and CXp computation for tree ensembles is an NP-hard problem [25].\nNevertheless, efficient formal explainers for tree ensembles have been developed, one based on recent SMT and one based on recent MaxSAT approaches. In terms of runtime, both exact approaches outperform even the heuristic explainer Anchor [24]. An important property of AXps and CXps is that they are dual to each other, i.e. contrastive explanations are minimal hitting sets of abductive explanations and vice versa [23]. Thus, AXp/CXp enumeration here is closely related to the well-studied enumeration problems of Minimum Unsatisfiable Subsets (MUS) and Minimum Correction Sets (MCS) in formal logic which are known to be $ \\Delta_2^p $-complete [35]. The state-of-the-art formal explanation enumeration algorithm for decision lists is one that exploits this duality, analogous to the generic MARCO algorithm for MUS/MCS enumeration [21, 31].\nEnsemble learning is a popular approach to improve the accuracy of ML models by combining multiple weak models into a single strong model [12]. Tree ensembles, such as random forests and gradient boosting machines, are well-known examples of ensemble learning methods in which the weak models are decision trees. Tree ensembles achieve high model accuracy with a minimal number of hyperparameters, and their generalization performance is robust even in high-dimensional feature spaces due to randomization in the training process and the combination of many decision trees with limited depth. Although tree ensembles are formed from a large number of intrinsically interpretable decision trees, they themselves are not intrinsically interpretable.\nStudying the formal explainability of tree ensembles is a relevant direction of research because tree ensembles are a common class of ML models used in many practical applications, such as diagnosis [52], or marketing [41].Moreover, they can be used to explain the predictions of any black-box or otherwise hard-to-codify prediction model, such as a large neural network, as is done in Asteryx, which combines sampling-based explanation with formal explanation. Asteryx uses sampling to locally approximate a given black-box model by a random forest trained on the samples drawn. This random forest then serves as a surrogate model for the subsequent computation of exact formal explanations [4]."}, {"title": "3 Desirable Properties of Explanations", "content": "We find that the problem of explanation selection has been under-studied, resulting in a literature that contains redundant and overlapping notions, some of which even have misleading names. To alleviate these problems, we have reviewed the literature on relevant notions of explanation in the social sciences and have identified a set of desirable properties that can be used to guide the selection of explanations.\nThere is an extensive body of research from philosophy, psychology, sociology and cognitive science on the nature of explanations and what constitutes a good, useful or acceptable explanation [37]. On the nature of explanations, we first note that an explanation is an answer to a \u201cwhy?\u201d question.While the concrete question \u201cWhy P?\u201d can be extended to \"Why P instead of not P?", "Why P instead of Q?": 40}, {"title": "4 Evaluation of Explanations", "content": "The authors of a recent comprehensive survey of evaluation methods in Explainable Artificial Intelligence identified a total of twelve evaluation criteria for the explanations generated by XAI tools [38]. In the following, we argue that many of these criteria can be fully satisfied a priori by formal reasoning methods.\nThe criteria correctness, completeness, and consistency, are fully satisfied by SAT-based formal reasoning methods, since they produce certified results that can be validated with verified software. Exact formal approaches can also explicitly maximize the continuity and compactness of the explanations by incorporating them as objective functions in the encoding of the explanation selection problem, while at the same time preserving the correctness and completeness of the encoding.\nHowever, coherence with background knowledge, beliefs, etc. is an inappropriate criterion for evaluating explanations in the context of formal reasoning. This is because, due to the exactness of the formal method, any incoherence of explanations necessarily follows from an incoherence of the model. In the context of heuristic explainers, which are not guaranteed to be exact, it may make sense to check whether the resulting explanations are consistent with what is expected from the background knowledge. In the formal setting, however, if the exact explanation is not consistent with the background knowledge, then the model is not consistent with the background knowledge. Thus, coherence is not a criterion for evaluating the explanation, but rather a criterion for evaluating the model. Formal verification of ML models is a natural complementary application to explanation selection. Verification of ML models can be achieved by encoding expectations as additional constraints in the model encoding to allow proof by contradiction.\nSince SAT-based formal reasoning produces exact and verified results, the confidence in the generated explanations is generally high. However, the whole process still depends on the correctness and completeness of the SAT encoding of the model. This involves parsing the data structures of the ML model and generating a formal language representation of the model such that the function $f_M$ of the model M is preserved in the encoding. This means that for any input I and output O, the formula has a model if and only if $f_M(I) = O$, provided that the input and output vectors I and O are represented by variables in the SAT formula. The SAT encoding must be verified by additional means, such as formal proofs of the correctness and completeness of the encoding and, in practical implementations, by automated consistency tests.\nThe contrastivity criterion evaluates the possibility of specifying contrast cases. Formal encodings of ML models should therefore explicitly allow the specification of contrast cases in the encoding of the explanation selection problem.\nOther evaluation criteria such as controllability, covariate complexity, composition, and user satisfaction are not directly addressed by the formal reasoning methods themselves, but can be achieved through the design of an appropriate user interface for specfication of user constraints, and for presentation and visualization of explanations and explanation statistics."}, {"title": "5 Conclusion", "content": "We have proposed a cycle of scientific discovery that combines machine learning with automated reasoning for the generation and the selection of explanations. Our findings indicate that automated reasoning methods are well-suited for the generation of explanations for machine learning models, and that they can be used to guide the selection of explanations. We have also introduced a taxonomy of explanation selection problems that draws on insights from sociology and cognitive science. These selection criteria subsume existing notions and extend them with new properties. We discussed how the evaluation of explanations is greatly simplified through the guarantees provided by automated reasoning methods. We conclude that the application of automated reasoning methods to the selection of explanations for machine learning models represents a promising avenue for future research."}]}