{"title": "Less is More: A Stealthy and Efficient Adversarial Attack Method for DRL-based Autonomous Driving Policies", "authors": ["Junchao Fan", "Xuyang Lei", "Xiaolin Chang", "Jelena Mi\u0161i\u0107", "Vojislav B. Mi\u0161i\u0107"], "abstract": "Despite significant advancements in deep reinforcement learning (DRL)-based autonomous driving policies, these policies still exhibit vulnerability to adversarial attacks. This vulnerability poses a formidable challenge to the practical deployment of these policies in autonomous driving. Designing effective adversarial attacks is an indispensable prerequisite for enhancing the robustness of these policies. In view of this, we present a novel stealthy and efficient adversarial attack method for DRL-based autonomous driving policies. Specifically, we introduce a DRL-based adversary designed to trigger safety violations (e.g., collisions) by injecting adversarial samples at critical moments. We model the attack as a mixed-integer optimization problem and formulate it as a Markov decision process. Then, we train the adversary to learn the optimal policy for attacking at critical moments without domain knowledge. Furthermore, we introduce attack-related information and a trajectory clipping method to enhance the learning capability of the adversary. Finally, we validate our method in an unprotected left-turn scenario across different traffic densities. The experimental results show that our method achieves more than 90% collision rate within three attacks in most cases. Furthermore, our method achieves more than 130% improvement in attack efficiency compared to the unlimited attack method.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, autonomous driving has achieved remarkable progress, propelled by rapid advancements in sensors, cameras, radar, and artificial intelligence (AI) [1][2]. As a prominent paradigm in AI, deep reinforcement learning (DRL) has been widely applied to develop autonomous driving policies, owing to its remarkable capability to address complex decision-making problems [3]. Current DRL-based autonomous driving strategies have demonstrated competitive performance across various tasks, such as highway driving [4]-[6], on-ramp merging [7]-[9], and intersection navigation [3][10][11]. However, the vulnerability of DRL to adversarial attacks has raised concerns about the practical application of such autonomous driving models [12][13]. It is crucial to understand the potential and cost of adversarial attacks against DRL-based autonomous driving policies, as this knowledge can inform the design of more robust and secure algorithms for trustworthy autonomous driving."}, {"title": "A. Motivation", "content": "Some studies have proposed some adversarial attack methods against DRL-based autonomous driving policies and proved that these policies are not robust [13]-[15]. However, despite significant progress, existing approaches still face two key challenges.\n1) Not worst-case oriented: Current studies mostly focus on either maximizing policy deviation or minimizing agent's rewards by adding adversarial perturbations [16]. These attacks may only result in sudden acceleration or braking, leading to relatively minor effects, such as reduced ride comfort or traffic efficiency. However, they fail to address critical safety challenges in trustworthy autonomous driving, such as safety violations like red light running or collisions.\n2) Fail to exploit critical moments: Most studies perturb the agent's observation at each time step throughout the entire episode. However, critical moments that could lead to safety violations in autonomous driving are rare during the driving process. Taking an unprotected left-turn task as an example, critical moments with a high risk of safety violations are typically confined to the few seconds during which the ego vehicle crosses the intersection. Optimizing autonomous driving policies under current attack methods may fail to effectively enhance robustness. Moreover, current attack methods are not stealthy enough to avoid detection by the agent due to their high attack frequency.\nThe key to addressing the above challenges lies in enhancing the intentionality, efficiency, and stealthy of the attacks. Some explorations have been conducted in the field of DRL to improve the efficiency and stealthy of attacks. Lin et al. [17] proposed a strategically-timed attack method that triggers an attack when the value of the relative action preference function (RAPF) exceeds a pre-calculated threshold. Li et al. [18] further considered the differences in the probabilities of all actions based on RAPF, making the judgment of critical moments more precise. Sun et al. [19] proposed a critical point attack in which the adversary constructs a domain-specific model to predict subsequent states and assess the damage of each potential attack policy. A damage awareness metric is used as a threshold to determine critical moments. However, the above attacks require careful design and manual tuning of domain-specific metrics or thresholds, limiting their efficiency and scalability to more complex scenarios. As a result, some studies have attempted to leverage the potential of DRL to create an adversary capable of both selecting critical moments and executing attack actions. Lin et al. [17] proposed an antagonist attack in which a DRL-based adversary is trained to learn both when-to-attack and how-to-attack policies, guided by the reward function of the victim agent. However, this approach overlooks the sparsity of perturbation injection, which may lead to suboptimal attack policies. Mo et al. [20] proposed a decoupled adversarial policy in which trajectory clipping and padding are used to mitigate the impact of sparse perturbation injection. Moreover, a pre-constructed database is used to provide universal perturbations to induce the victim agent to choose adversarial actions with 100% probability. However, designing universal perturbations for autonomous driving tasks with continuous action space and complex state space is challenging. Since existing research cannot be effectively applied, it is crucial to explore new adversarial attacks in autonomous driving scenarios to address the above challenges. We believe this is a crucial direction for further enhancing the robustness of DRL-based autonomous driving policies, yet it has not been fully explored."}, {"title": "B. Contribution", "content": "In this paper, we propose a novel stealthy and efficient adversarial attack method, aiming to induce safety violations with the minimum number of attacks. To further improve the stealthy of the attack, the perturbations imposed by each attack are bounded. In our method, we first model the attack as a mixed integer optimization problem which is difficult to solve. Moreover, the problem becomes even more challenging to solve as the state space grows exponentially with the increase of time steps. Therefore, we model this problem as a Markov decision process (MDP) and introduce an adversary based on the proximal policy optimization algorithm (PPO) [21] to solve the MDP to learn the optimal attack policy. We design a reward function to guide the adversary to induce the ego vehicle to perform safety-violating behaviors, rather than simply minimizing the reward of the ego vehicle. Different from existing research where the adversary and victim share the same state space, we further introduce the remaining number of attacks and the ego vehicle's origin action without perturbation into the adversary's state space. Additionally, a trajectory clipping method is employed to optimize the adversary's training. We conduct our experiments in an unprotected left turn scenario with different traffic densities based on SUMO [22]. Experiments show that our method can achieve collision rates of over 90%, with an average of only three attack steps in most cases. Moreover, compared to the attack method without the attack frequency constraint, our method improves the attack efficiency by at least 130% in all cases. The contributions of our work are summarized as follows.\n\u2022 We propose a novel stealthy and efficient adversarial attack method against DRL-based autonomous driving policies. This method aims to minimize the number of attacks while maximizing safety violations. To further optimize training, we incorporate additional attack-related information into the adversary's observations to enhance its capabilities and use a trajectory clipping method to improve sample quality.\n\u2022 We conduct extensive experiments on various DRL-based autonomous driving policies, demonstrating the superiority of our method compared to existing attack methods. In addition, we propose a new evaluation metric, attack efficiency, offering a new perspective for evaluating the effectiveness of adversarial attacks against DRL-based autonomous driving policies. Finally, we validate the contributions of each component through ablation studies."}, {"title": "II. RELATED WORK", "content": "This section first reviews the recent research on DRL-based autonomous driving policies in Section II.A, followed by a review of adversarial attack methods targeting these policies in Section II.B."}, {"title": "A. DRL-based Autonomous Driving", "content": "The rapid advancements in machine intelligence and data-driven approaches have propelled DRL to the forefront of autonomous driving strategies [23]. For instance, Huang et al. [24] leveraged human prior knowledge to tackle the challenges of sample efficiency and reward function design in DRL. The effectiveness of their method was validated in unprotected left-turn and roundabout scenarios. Li et al. [25] proposed a hierarchical skill-based offline reinforcement learning method to solve the long-horizon vehicle planning task. Dang et al. [26] developed an event-triggered model predictive control framework based on a model-free DRL algorithm to address the path following problem in autonomous driving. Although the above DRL-based driving policies have achieved remarkable advancements in many scenarios, the lack of robustness against adversarial attacks limits their application in real-world settings [27]."}, {"title": "B. Attack Methods for DRL-based Autonomous Driving", "content": "Existing studies have demonstrated that well-trained DRL policies remain highly vulnerable when facing adversarial perturbations [17]-[20]. Adding adversarial perturbations to state inputs in autonomous driving policies can cause sudden acceleration or braking of the ego vehicle, posing a safety risk. Consequently, some studies have attempted to explore the robustness of autonomous driving policies under adversarial environments and have proposed some adversarial attack methods. These attack methods can be broadly categorized into two types: untargeted attacks [28]-[30] and targeted attacks [14][31][32].\nUntargeted attacks typically aim to maximize the distance between the agent's policies before and after the perturbation. For instance, He et al. [33] proposed a black-box attack technique with Bayesian optimization to maximize the action deviation of the ego vehicle by introducing adversarial perturbations to its observations. However, these methods may fail to induce worst-case scenarios, such as collisions, due to the lack of a clear purpose. Therefore, some targeted attack methods are proposed with the aim of minimizing the agent's reward or maximizing safety violations. For instance, Ma et al. [34] introduced an adversary that can introduce up to 20% adversarial perturbations to the actions of the autonomous driving agent to minimize the agent's reward.\nAll the aforementioned attack methods assume that adversarial perturbations are continuously applied throughout the driving process. However, excessive perturbations make the attack more detectable and easier to mitigate. Furthermore, continuous attacks may fail to significantly improve the effectiveness, as the critical moments for attacks are sparse. Non-critical perturbation samples may also interfere with the adversarial training of the autonomous driving agent, hindering its ability to improve robustness. We believe that considering the sparse distribution of critical attack moments in autonomous driving is essential. Based on this insight, we propose a novel stealthy and efficient adversarial attack method."}, {"title": "III. PROBLEM FORMULATION", "content": "This section first describes the preliminary concept of MDP in Section III.A. Then we model the attack problem as a mathematical formulation according to the goal of the proposed attack in Section III.B. Section III.C defines the threat model."}, {"title": "A. Preliminary", "content": "Driving policies can be learned through DRL algorithms [1]. Before learning the policy, it is usually necessary to model the driving policy learning problem as an MDP which can be defined by a tuple (S,A,r,P,\u03b3) . Here state space S denotes the set of all possible states of the system. In autonomous driving, the state information typically includes information about the ego vehicle, surrounding vehicles, and driving environment. Action space A represents the set of all possible control actions such as steering and throttle. The reward function r: S\u00d7A \u2192 R is designed to guide the agent to learn the desired driving policy and P:S\u00d7A\u00d7S\u2192[0,1] describes the transition probability of the system. \u03b3\u2208(0,1) is the discount factor. The DRL-based agent aims to find an optimal policy \u03c0* that maximizes the cumulative discounted reward\n \\(J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi(\\tau)}[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]\\)."}, {"title": "B. Problem Statement", "content": "In this paper, we aim to find a stealthy and efficient attack method against DRL-based autonomous driving policies. Instead of attacking at every time step, a more efficient and stealthy approach would be to limit the attacks to a few critical time steps while still achieving the desired objective. Letras the reward function of the adversary, we can then formulate the above intuition as the following optimization problem:\n\\(max \\mathbb{E}_{(\\chi,\\delta)} [\\sum_{t=1}^{T} \\gamma^t r_a(s_t + \\chi_t\\delta_t, a_t)]\\)\ns.t.\n\\(s_t = s + \\chi_t\\delta_t, t \\in T,\\)\n\\(\\delta_t = PG(s_t, a_t, a'), t \\in T,\\)\n\\(\\delta_t \\le \\epsilon, t \\in T,\\)\n\\(\\sum_{t\\in T} \\chi_t \\le \\Gamma.\\)\nEq. (2) is the objective function that maximizes the adversary's cumulative discounted reward over the entire episode, where T is the maximum length of each episode. The variables are denoted as (x,d), where  x = {xt,t\u2208T} is a set of binary variables used to control whether to apply the attack at each time step, and  d = {\u03b4t,t \u2208 T} denotes the set of perturbations. If xt =1, the adversary will add perturbations \u03b4t to the state st, as described in (2a). \u03b4t can be generated by perturbation generating method PG(\u00b7). As shown in (2c), we set a threshold \u20ac for d, to further reduce the probability of the attack being detected. We assume the total number of attacks per episode must not exceed \u0393, as shown in (2d), where \u0413\u226a\u0422. Solving this optimization problem is challenging because it is a complex mixed integer optimization problem. The difficulty is further complicated by the non-linear growth in the size of the problem, driven by T [17]."}, {"title": "C. Threat Model", "content": "We outline the goal and capabilities of the adversary to guide the design of the attack policy before detailing our attack method to solve the optimization problem (2).\nGoal of the adversary: The adversary attempts to manipulate the agent's actions by introducing small adversarial perturbations to its observations at critical time steps, thereby causing safety violations such as collisions.\nCapabilities of the adversary: We assume that the adversary can only access the observations and actions of the agent. Furthermore, the adversary is only permitted to modify the agent's observations by adding adversarial perturbations [35]. Since the adversary has no knowledge of the architecture, parameters, or gradients of the agent's policy, our attack is a black-box attack."}, {"title": "IV. METHODOLOGY", "content": "This section first outlines the overall framework of our attack method in Section IV.A and propose the algorithm in Section IV.B."}, {"title": "A. Framework", "content": "The optimization problem can be easily divided into two subproblems: when-to-attack and how-to-attack. In our attack method, we introduce a DRL-based adversary to independently learn the optimal strategies for both subproblems. The overview diagram of the proposed attack method is given in Fig. 2. Our attack method consists of two stages. The first stage is the decision-making stage. In this stage, the adversary needs to make attack decisions (pt,at')~ \u03c0adv (sadv) based on its observation sadv. Besides treating sadv = st, as in [19], we believe that introducing additional attack-related information can enhance the ability of the adversary. Therefore, we add the remaining number of attacks nt and the agent's original action at into the adversary's observation, i.e.,  sadv = (st,nt,at). The adversary's attack decision consists of two parts, where pt \u2208 [-1,1] represents the switch action determining whether to inject the perturbation into st to launch an attack, and at' \u2208 A represents the lure action that the adversary aims to induce the agent to take by applying the perturbation.\nAfter making the attack decision, the adversary enters the next stage, the attack launching stage. In this stage, the adversary will make the final decision based on (pt,at'). If pt \u22650, the adversary will generate the perturbation \u03b4t to mislead the agent to trigger the action at'. Conversely, if pt <0, it indicates that no attack will be launched at time step t. After this stage, the adversary will terminate the episode if nt = 0, this process also called trajectory clipping. The purpose is to balance the sample distribution and enhance the training effectiveness of the adversary.\nOur attack method is capable of learning optimal switch and lure policies automatically without any domain knowledge. The domain-agnostic and self-learning characteristics make our method highly suitable for complex and diverse autonomous driving scenarios."}, {"title": "B. Algorithm", "content": "Given that our autonomous driving scenario involves a continuous action space, we choose PPO, a state-of-the-art on-policy DRL algorithm [36], to train the adversary. PPO consists of two networks, an actor network \u03c0adv and a critic network Vadv, where \u03b8 and \u03c6 are the parameters corresponding to these two networks. Given an adversary observation sadv, \u03c0adv will output the policy \u03c0adv ((pt,at') | sadv). and Vadv will estimate the value of sadv, which can be expressed as\n\\(V^{adv}(s_t^{adv}) = \\mathbb{E} [\\sum_{k=1}^{T-t} \\gamma^{t+k} r(s_t^{adv}) | s_t^{adv}]\\).\nThe loss function used to optimize \u03c0adv can be expressed as\n\\(L_t(\\theta) = \\mathbb{E}_t[min(\\rho_t(\\theta)A_t, clip(\\rho_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)],\\)\nwhere\n\\(\\rho_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}\\),\n\\\nwhere p(\u03b8) represents the probability ratio between the current policy \u03c0adv and the old policy \u03c0adv,old. A is the estimated advantage and can be defined as\n\\(A = \\sum_{i=t}^{T-1} (\\gamma\\lambda)^{i-t}r_t + \\gamma^{T-t}V^{\\pi}(s_T) - V^{\\pi}(s_t)).\\)\nwhere \u03bb is a tuning parameter.\nThe loss function for the critic network is typically a mean squared error, which can be expressed as\n\\(L(\\phi) = \\mathbb{E}_t [V^{adv}(s_t^{adv}) - (r_t + \\gamma V_{\\phi}(s_{t+1}^{adv}))]^2\\).\nThen we can obtain the total loss function as\n\\(L(\\theta,\\phi) = L_t(\\theta) - c_1L_t(\\phi) + c_2S[\\pi_{\\theta}^{adv}](s_t^{adv}),\\)\nwhere c1 and c2 are coefficients and S[\u00b7] denotes an entropy bonus. Based on the loss function, \u03b8 and \u03c6 can be simultaneously updated using gradient-based methods."}, {"title": "V. EXPERIMENT", "content": "We validate the proposed attack method on the SUMO platform. First, we introduce the relevant experimental settings in Sections V.A-E, including environment settings, the MDP, baselines, training details, and metrics. Then we organize our experiments and explain the results to address the following key questions.\n\u2022 RQ1: Can the proposed attack method can achieve the adversarial goal within a limited number of attacks? (Studied in Section V.F1)\n\u2022 RQ2: Does the proposed attack method outperform the baseline and SOTA methods? (Studied in Section V.F2)\n\u2022 RQ3: How does the value of \u0393affect the proposed attack method? (Studied in Section V.F3)\n\u2022 RQ4: How does the performance of our proposed attack method vary under different traffic densities? (Studied in Section V.F4)"}, {"title": "A. Environment Setting", "content": "We select an unprotected left-turn scenario for experiment, where the ego vehicle needs to complete its left turn while interacting with dynamic cross-traffic flows at an unsignalized intersection, as shown in Fig. 2. In the simulated environment, all vehicles employ the LC2013 model of SUMO for lane-changing behavior. To ensure realistic dynamics, the maximum speed vmax is set to 15 m/s and maximum acceleration (and deceleration) \u03b2 of the ego vehicle are set to 15 m/s and 7.6 m/s\u00b2, respectively. The traffic flow density is determined by the probability p of vehicle arrivals per second. If there is no special explanation, we set p = 0.5 ."}, {"title": "B. Markov Decision Process", "content": "Based on the simulation environment, we construct an MDP with the following details:\nState Space S: The state st \u2208 S consists of two components. One is the speed and velocity direction of the ego vehicle. The other part contains information about the six nearest vehicles within a 200-meter radius of the ego vehicle, including the relative distance, orientation, speed, and velocity direction. The positions of these six cars are front, rear, left front, left rear, right front and right rear.\nAction Space A: At time stept, the action of the agent at involves continuous acceleration or deceleration, represented as a continuous variable within the range [-1,1]. To accurately reflect real conditions, at needs to be multiplied by the acceleration coefficient \u03b2 to obtain the actual acceleration of the ego vehicle. Thus, the velocity update equation can be obtained as vego(t+1) = vego(t)+at\u03b2\u0394t .\nReward functionr: The reward function of the victim agent r consists of two components: efficiency reward and safety penalty, which can be expressed as:\n\\(r = \\frac{v}{v_{max}} - c(s,a^*)\\),\n\\(c(s,a^*) = \\begin{cases} 1, & \\text{if collision}, \\\\ 0, & \\text{else}, \\end{cases}\\),\nwhere vmax denotes the maximum speed of the ego vehicle and a* represents the action performed by the ego vehicle at time stept. Specifically, the goal of the victim agent is to drive the ego vehicle as quickly as possible while avoiding collisions. Since the adversary aims to cause a collision, setting its reward ra = -r as in [20] is unreasonable. Because it may redirect the adversary's focus towards reducing the speed of the ego vehicle. Therefore, we set ra = c(s,a*) so that the adversary focuses on causing safety violations."}, {"title": "C. Baselines", "content": "We select autonomous driving agents based on state-of-the-art RL algorithms to verify the effectiveness of our attack method. These agents can be divided into two categories: one is based on vanilla RL algorithms, such as PPO and the state-of-the-art off-policy algorithms SAC [37] and TD3 [38]. The other is the state-of-the-art safe autonomous driving DRL algorithm, FNI-RL [39].\nIn terms of attack methods, we selected the following attack methods for comparison with our proposed attack method.\nRandom Attack (RA): At each time step, the adversary randomly samples an adversarial action from the action space. Then perturbations are generated to guide the agent toward selecting an action aligned with the adversarial one. Since adversarial actions are randomly selected, RA is a typical non-targeted attack method.\nAction Modification Attack (AMA): In this attack, the adversary possesses nearly the same capabilities as our attack method. The difference is that this attack directly forces the adversarial action as the final executed action, rather than guiding the agent through perturbations, i.e., a\" = a'. It is typically unrealistic to directly manipulate the agent's action in practical applications, as it requires the adversary to have higher privileges. However, this attack method can serve as a strong baseline to verify the effectiveness of our attack. Specifically, the effect of this attack is intuitively not weaker than our attack method since each attack can achieve the expected target with a 100% success rate.\nUnlimited Attack (UA): This attack is similar to the attack methods commonly adopted in current work, as it performs an attack at each time step. This attack generates the adversarial action in the same way as our method."}, {"title": "D. Training details", "content": "To verify the effectiveness of our attack method, we first train several victim agents using different DRL algorithms. All agents were trained for 12,000 steps, with a maximum episode length T set to 30. All DRL algorithms are implemented using Stable Baselines3 [40]. We fine-tune the hyperparameters of the PPO-based victim agent for better performance. Specifically, the batch size is set to 256, the learning rate to 0.001, and the number of epochs to 15. The parameters for the other agents are set to the default values provided by Stable Baselines3. Except for the random attack, all other attack methods are PPO-based. Therefore, we train the adversaries based on the victim agents to generate the corresponding attack policies. The total training steps and maximum episode length are consistent with the victim agents, and the hyperparameters, such as learning rate and batch size are set to default values. We select FGSM [41] and PGD [42], two generalized adversarial sample generation approaches, to create adversarial perturbations. All the experiments are conducted on a server equipped with Intel(R) Xeon(R) Gold 6230 CPUs and NVIDIA GeForce RTX 4090 GPUs."}, {"title": "E. Metrics", "content": "We select multiple metrics to conduct a comprehensive evaluation of our attack method. Besides the common metrics such as success rate (SR), collision rate (CR), average speed (AS), and average reward (AR) [39], we also adopt the average number of attacks (ANA) to evaluate the efficiency of our attack method. Specifically, ANA refers to the number of attacks launched by the adversary in an episode. Moreover, we introduce a new metric to evaluate the effectiveness of a single attack, called attack efficiency (AE). AE can be calculated using CR and ANA, as follows:\n\\(AE = CR \\cdot e^{-k \\cdot ANA}\\),\nwhere k is a parameter used to adjust the weights of CR and ANA. In this paper, it is set to 0.05. All metrics are computed based on the results of 100 test episodes."}, {"title": "F. Performance Evaluation", "content": "In this section, we aim to address RQ1-RQ4 through experiments and rigorous analysis."}, {"title": "F1. Effectiveness of Our Method", "content": "In this section, we will verify that our approach enables stealthy and efficient attacks, i.e., the highest possible collision rate with as few attacks as possible. Fig. 3 illustrates the performance of our method against victim agents trained by PPO, SAC, TD3, and FNI-RL. It can be observed that, in most cases, our attack method achieves a collision rate of around 90% within an average of three attacks per episode. Both FGSM-based and PGD-based attacks achieve similar attack effectiveness, i.e., CR."}, {"title": "F3. Impact of the Upper Bound of Attack Steps", "content": "Since we expect the adversary to trigger collisions through a limited number of attack steps, we control the upper bound on the number of attack steps per episode during the training process. Fig. 4 shows the performance of our attack method under different \u0393. The victim agent is based on FNI-RL, and the perturbation generation method is FGSM. The vertical axis represents the total reward of the adversary. Notably, this reward is binary, taking a value of 1 for collisions and 0 for non-collisions. Therefore, convolutional smoothing is applied to the reward curves for better representation. After smoothing, the vertical axis can be interpreted as a proxy for the collision probability to some extent. It can be observed that as \u0393 increases, the collision probability increases, indicating an improvement in the attack capability of the adversary. This is because a larger upper bound enables the adversary to explore more situations, thereby increasing the likelihood of learning the optimal attack policy. Similar conclusions can be drawn in other cases where the victim agent and perturbation generation method are different."}, {"title": "F4. Performance Under Different Traffic Densities", "content": "To conduct a more comprehensive evaluation, we construct two new scenarios with different traffic densities, denoted as flow1 and flow2. Specifically, flow1 and flow2 represent the cases where p=0.3 and p = 0.7 respectively. Table III shows the performance of the attack methods under flow1 and flow2 scenarios. It can be observed that as traffic flow density increases, all attack methods can increase the collision rate in most cases. Similar to the results in Table II, our method consistently achieves comparable performance to AMA in all cases. This implies that existing perturbation generation methods can already yield satisfactory results in our scenarios. Compared to UA and RA, our attack method achieves highly competitive collision rates in all cases with significantly fewer attack steps. Specifically, compared to UA and RA, our method achieves an average improvement of 141.24% and 607.23% in AE in flow1, respectively. In flow2, the improvements are 130.65% and 596.62%, respectively."}, {"title": "VI. CONCLUSION", "content": "In this work, we propose a novel adversarial attack method that enables stealthy and efficient attacks against DRL-based autonomous driving agents. The proposed attack method is based on DRL, allowing it to learn the optimal attack strategy in a domain-independent and self-learning manner. We validate the effectiveness of our attack across various state-of-the-art DRL algorithms, including both vanilla and safe DRL algorithms. Moreover, we compare our method with existing attack approaches. Experimental results show that our attack method achieves over 90% collision rate in most cases while the attack efficiency is significantly higher than other attack methods. Our attack method also shows robustness to changes in traffic density. Although we have demonstrated the potential of our attack method, there remain some limitations that should be addressed in future studies. Firstly, developing more efficient perturbation generation methods is expected to further enhance the effectiveness of our attack method. Secondly, adapting our attack method to long-term autonomous driving scenarios presents an interesting area for further research. Finally, investigating effective defense strategies against our attack method will contribute to improving the robustness of autonomous driving systems."}]}