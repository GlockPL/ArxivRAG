{"title": "Stability Bounds for the Unfolded Forward-Backward Algorithm", "authors": ["Emilie Chouzenoux", "Cecile Della Valle", "Jean-Christophe Pesquet"], "abstract": "We consider a neural network architecture designed to solve inverse prob- lems where the degradation operator is linear and known. This architecture is constructed by unrolling a forward-backward algorithm derived from the minimization of an objective function that combines a data-fidelity term, a Tikhonov-type regularization term, and a potentially nonsmooth convex penalty. The robustness of this inversion method to input perturbations is analyzed the- oretically. Ensuring robustness complies with the principles of inverse problem theory, as it ensures both the continuity of the inversion method and the resilience to small noise a critical property given the known vulnerability of deep neural networks to adversarial perturbations. A key novelty of our work lies in exam- ining the robustness of the proposed network to perturbations in its bias, which represents the observed data in the inverse problem. Additionally, we provide numerical illustrations of the analytical Lipschitz bounds derived in our analysis.", "sections": [{"title": "1 Introduction", "content": "A large variety of inverse problems consists of inverting convolution operators such as signal/image restoration, tomography, Fredholm equation of the first kind, or inverse Laplace transform.\nLet T be a bounded linear operator from a separable Hilbert space X to a Hilbert space Y. The problem consists in finding x \u2208 X from observed data\n$y = Tx + w$ (1)\nwhere w corresponds to an additive measurement noise. The above problem is often ill-posed i.e., a solution might not exist, might not be unique, or might not depend continuously on the data.\nThe well-posedness of the inverse problem defined by (1) is retrieved by regularization. Here we consider Tikhonov-type regularization. Let $\u03c4 \u2208]0, +\u221e[$ be the regularization parameter. Solving the inverse problem (1) with such regularization, leads to the resolution of the following optimization problem:\n$minimize J(x),$ (2)\n$x \u2208 C$\nwhere\n$(\u2200x \u2208 X)  J\u03c4(x) = 1/2||Tx - y||\u00b2 + \u03c4/2||Dx||2,$ (3)\nC is a nonempty closed convex subset of X encoding some prior knowledge, e.g. some range constraint or sparsity pattern, while D acts as a derivative operator of some order r > 0. Often, we have an a priori of smoothness on the solution which justifies the use of such a derivative-based regularization. Problem (2) is actually an instance of the more general class of problems stated below, encountered in many signal/image processing tasks:\n$minimize J(x) + \u03bcg(x),$ (4)\n$x \u2208 X$\nwhere $\u03bc\u2208 [0, +\u221e[$ is an additional regularization constant and g is a proper lower- semicontinuous convex function from Hilbert space X to ] -8,+8]. Indeed, Problem (2) corresponds to the case when g is the indicator function ic of set C.\nWe focus our attention on seeking for a solution to the addressed inverse problem through nonlinear approximation techniques making use of neural networks. Thus, instead of considering the solution to the regularized problem (4), we define the solu- tion to the inverse problem (1) as the output of a neural network, whose structure is similar to a recurrent network."}, {"title": null, "content": "Namely, by setting an initial value xo, we are interested in the following m-layers neural network where m\u2208 N \\ {0}:\nInitialization:\n$bo = T*y,$ \nLayer n \u2208 {1, ...,m}:\n$Xn = prox_{\u03bb\u03b7\u03bcn}g (Xn\u22121 - An(T*T + D**Dn)xn\u22121 + Anbo) .$ (5)\nHereabove, prox, stands for the proximity operator of a lower-semicontinuous proper convex function (see Section 2) and, for every n \u2208 {1,...,m}, Dn is a bounded linear operators from X to some Hilbert space X, and An is positive real. Throughout this paper, L* denotes the adjoint of a bounded linear operator L defined on suitable Hilbert spaces. The overall structure of the network is shown in Figure 1.\nA simple choice consists in setting, for every n \u2208 {1, ..., m}, Dn = \u221aTnD where \u03c4\u03b7 is a positive constant. The constants (\u03bc\u03b7)1\u2264n\u2264m, (Tn)1<n<m, and possibly (An)1<n<m can be learned during training. Then, Model (6) can be viewed as unrolling m iterations of an optimization algorithm, so leading to Algorithm 1. If additionally, we set \u03bc\u03b7 \u2261 \u03bc and tn = T, Algorithm 1 identifies with the forward-backward algorithm applied to the variational problem (4).\nLet $\u03b3\u2265 0$ denote the modulus of strong convexity of the regularization term g (with $\u03b3 > 0$ if g is strongly convex). We have thus $g = go + \u03b3||x||\u00b2/2$ where go is a proper lower-semicontinuous convex function from X to $] - \u221e, +\u221e]$, and Model (5)"}, {"title": null, "content": "can be equivalently expressed as\nInitialization:\n$bo = T*y,$\nLayer n \u2208 {1,...,m}:\n$Xn = Rn(Wnxn\u22121 + Vnbo), $ (6)\nwhere, for every n \u2208 {1, ..., m},\n$R_n = \\frac{prox_{\u03bb\u03b7\u03bcn} g}{1+\u03bb\u03b7\u03b3n},$ (7)\n$W_n = \\frac{(1 \u2013 An (T*T + D**Dn))}{1 + \u03bb\u03b7\u03c7\u03b7},$ (8)\n$V_n = \\frac{\u03bb\u03b7}{1 + \u03bb\u03b7\u03c7\u03b7} ,$ (9)\n$\u03a7\u03b7 = \u03bc\u03b7\u03c7,$ (10)\nand 11 denotes the identity operator.\nFrom a theoretical standpoint, little is known about the theoretical properties of Model (6) in relation with the minimization of the original regularized objective function (4). The main challenges are that (i) the number of iterations m is predefined, and (ii) the parameters can vary along the iterations. However, insight can be gained by quantifying the robustness of Model (6) to perturbations on its initialization xo and on its bias bo, through an accurate estimation of its Lipschitz properties.\nThere has been a plethora of techniques developed to invert models of the form (1). Among these methods, Tikhonov-type methods are attractive from a theoretical viewpoint, especially because they provide good convergence rate as the noise level decreases, as shown in  or [11]. Optimization techniques are then classically used to solve Problem (4). However, limitations of such methods may be encountered in their implementation. Indeed, certain parameters such as gradient descent stepsizes or the regularization coefficient need to be set, as discussed in [12] or [13]. The lat- ter parameter depends on the noise level, as shown in [14], which is not always easy to estimate. In practical cases, methods such as the L-curve method, see [15], can be implemented to set the regularization parameter, but they require a large number of resolutions and therefore a significant computational cost. Moreover, incorporating constraints on the solution may be difficult in such approaches, and often reduces to projecting the resulting solution onto the desired set. These reasons justify the use of a neural network structure to avoid laborious calibration of the parameters and to easily incorporate constraints on the solution.\nThe use of neural networks for solving inverse problems has become increasingly popular, especially in the image processing community. A rich panel of approaches have been proposed, either adapted to the sparsity of the data [16, 17], or mimick- ing variational models, or iterating learned operators [20-24], or adapating"}, {"title": null, "content": "Tikhonov method [25]. The successful numerical results of the aforementioned works raise two theoretical questions: in cases when these methods are based on the itera- tions of an optimization algorithm, do they converge (in the algorithmic sense)? Are these inversion methods stable or robust?\nIn iterative approaches, a regularization operator is learned, either in the form of a proximity (or denoiser) operator as [20, 21, 24], of a regularization term [25], of a pseudodifferential operator, or of its gradient [3, 27]. Strong connections also exist with Plug-and-Play methods [22, 28, 29], where the regularization operator is a pre-trained neural network. Such strategies have in particular enabled high-quality imaging restoration or tomography inversion [3]. Here, the nonexpansiveness of the neural network is a core property to establish convergence of the algorithm, although recent works have provided convergence guarantees using alternative tools [30].\nOther recent works solve linear inverse problems by unrolling the optimization iterative process in the form of a network architecture as in [31-33]. Here the num- ber of iterations is fixed, instead of iterating until convergence, and the network is often trained in an end-to-end fashion. Since neural network frameworks offer power- ful differential programming capabilities, such architectures are also used for learning hyper-parameters in an unrolled optimization algorithm as in [34-38].\nAll of the above strategies have shown very good numerical results. However, few studies have been conducted on their theoretical properties, especially on their stabil- ity. The study of the robustness of such a structure is often empirical, based on a series of numerical tests, as performed in [38, 39]. In [25], stability properties are established at low noise regime when using neural networks regularization. A fine characterization of the convergence conditions of recurrent neural network and of their stability via the estimation of a Lipschitz constant is done in , [41]. In particular, the Lipschitz constant estimated in [41] is more accurate than in basic approaches which often rely in computing the product of the norms of the linear weight operators of each layer as in [42, 43]. Thanks to the aforementioned works, proofs of stability/convergence have been demonstrated on specific neural networks applied to classification (e.g., [44, 45]), or inverse problems (e.g., [22, 23, 34]). The analysis carried out in this article is in the line of these references.\nOur contributions in this paper are the following.\n1. We propose a neural network architecture to solve the inverse problem (1), incor- porating the proximity operator of a potentially nonsmooth convex function as its activation function. This design enables the imposition of constraints on the sought solution. A key advantage of this architecture lies in its foundation on the unrolling of a forward-backward algorithm, which ensures the network structure is both interpretable and frugal in terms of the number of parameters to be learned.\n2. We study theoretically and numerically the stability of the so-built neural network. The sensitivity analysis is conducted with respect to the observed data y, which corresponds to a bias term in each layer of Model (6). This analysis is more general than the one performed in [34], in which only the impact of the initialization was considered."}, {"title": "Outline", "content": "The outline of the paper is as follows. In Section 2, we recall the theoretical background of our work and specify the notation, which follows the framework in [46]. Section 3 introduces a class of dynamical systems with a leakage factor, providing a more general framework than the neural network defined by (6). In Section 4, we establish the stability of the corresponding neural network defined on the product space X \u00d7 X, building upon the results from [40] and [41]. By stability, we mean that the network output remains controlled with respect to both its initial input xo and the bias term T*y, ensuring that small differences or errors in these vectors are not amplified through the network. We also provide sufficient conditions for the averagedness of the network. Finally, we show how stability properties for network (6) can be deduced from these results. Section 5 presents numerical illustrations of the derived Lipschitz bounds, followed by concluding remarks in Section 6."}, {"title": "2 Notation", "content": "We introduce the elements of convex analysis we will be dealing with, in Hilbert spaces. We also cover the bits of monotone operator theory that will be needed throughout.\nLet us consider the Hilbert space X endowed with the norm || || and the scalar product (,). In the following, X shall refer to the underlying signal space. The notation || || will also refer to the operator norm of bounded operators from X onto X.\nAn operator S: X \u2192 X is nonexpansive if it is 1-Lipschitz, that is\n$(\u2200(x,y) \u2208 X \u00d7 X)  ||Sx - Sy|| \u2264 ||x - y || .$ \nMoreover, S is said to be\n1. firmly nonexpansive if\n$(\u2200(x,y) \u2208 X \u00d7 X)  ||Sx \u2013 Sy||\u00b2 + ||(1 \u2013 S)x \u2013 (1 \u2013 S)y||\u00b2 < ||x \u2212 y||2 ;$\n2. a Banach contraction if there exists \u03ba\u2208]0, 1[ such that\n$(\u2200(x,y) \u2208 X \u00d7 X)  ||Sx - Sy|| \u2264 \u043a||x \u2212 y|| . $ (11)\nIf S is a Banach contraction, then the iterates (Snx)n\u2208N converge linearly to a fixed point of S according to Picard's theorem. On the other hand, when S is nonexpansive, the convergence is no longer guaranteed. A way of recovering the convergence of the iterates is to assume that S is averaged, i.e., there exists a \u2208]0, 1[ and a nonexpansive operator R: X \u2192 X such that S = (1 - a) 1 + aR. In particular, S is a-averaged if and only if\n$(\u2200(x,y) \u2208 X \u00d7 X)  ||Sx \u2013 Sy ||\u00b2 + \\frac{1-\u03b1}{\u03b1}||(1 \u2013 S)x \u2013 (1 \u2013 S)y||2 \u2264 ||x \u2212 y||2 .$"}, {"title": null, "content": "If S has a fixed point and it is averaged, then the iterates (Snx) n\u2208N converge weakly to a fixed point. Note that S is firmly nonexpansive if and only if it is 1/2-averaged and that, if S satisfies (11) with \u043a\u2208]0, 1[, then it is (k + 1)/2-averaged.\nLet To(X) be the set of proper lower semicontinuous convex functions from X to ] - \u221e, +\u221e]. Then we define the proximity operator [46, Chapter 9] as follows.\nLet f \u2208 \u03930(X), x \u2208 X, and y > 0. Then proxyf(x) is the unique point that satisfies\n$proxy f(x) = argmin (f(y) + \\frac{1}{2\u03b3}||x-y||\u00b2 ).$\n$y \u2208 X$\nThe function proxyf: X \u2192 X is the proximity operator of yf.\nFinally, the proximity operator has the following property.\nProposition 2.1 (Proposition 12.28 of [46]). The operators proxyf and 11 proxy f are firmly nonexpansive.\nIn the proposed neural network (6), the activation operator is a proximity operator. In practice, this is the case for most activation operators, as shown in [40]. The neural network (6) is thus a cascade of firmly nonexpansive operators and linear operators. If the linear part is also nonexpansive, bounds on the effect of a perturbation of the neural network or its iterates can be established."}, {"title": "3 Considered neural network architecture", "content": "In this section, Model (6) is reformulated as a virtual network, which takes as inputs the classical ones on top of a new one, which is a bias parameter."}, {"title": "3.1 Virtual neural network with leakage factor", "content": "To facilitate our theoretical analysis, we will introduce a virtual network making use of new variables (Zn)nen in the product space X \u00d7 X. For every n \u2208 N \\ {0}, we define the n-th layer of our virtual network as follows:\n$Q_n = \\begin{pmatrix} R_n & 0\\\\ 0 & 1_1\\end{pmatrix},$ \n$\\begin{pmatrix} zn\\\\ bn \\end{pmatrix} = Q_n(U_n z_{n-1}), with  U_n = \\begin{pmatrix} W_n & V_n\\\\ 0 & \u03b7_n 1_1\\end{pmatrix}  ,$ (12)\nand Wn (resp. Vn) defined by (8) (resp. (10)). In our context, Rn is defined by (7), but all the results in our stability analysis remain valid for any firmly nonexpansive operator such as the resolvent of a maximally monotone operator. Note that, in order to gain more flexibility, we have included positive multiplicative factors (Nm)n\u22651 on"}, {"title": null, "content": "the bias. Cascading m such layers yields\nInitialization:\n$bo = T*y,$\nLayer n \u2208 {1,...,m}:\n$xn = Rn(Wnxn\u22121 + Vnbo) = prox_{\u03bb\u03b7\u03bcn}g (Xn\u22121 - An(T*T + D+ Dn)Xn\u22121 + AnNn-1\u2026\u2026Nobo), $ (13)\nwhere\n$Vn = \u03bb\u03b7\u03b7\u03b7-1\u2026 11 and m = 1. $ (14)\nWe thus see that the network defined by Model (6) is equivalent to the virtual one when all the factors \u00een are equal to one. When n \u2265 1 and nn < 1, the parameters \u03b7\u03b7 can be interpreted as a leakage factor.\nRemark 3.1. In the original forward-backward algorithm, the introduction of (Nn)n>1 amounts to introducing an error en in the gradient step, at iteration n, which is equal to\n$en = \u03bb\u03b7 (1\u03b7-1 No - 1)bo.$ (15)\nFrom known properties concerning the forward-backward algorithm [9], the standard convergence results for the algorithm are still valid provided that\n$\\sum_{n=2}^{+\\infty}  |\u03bb\u03b7 \u03b7\u03b7\u22121\u00b7\u00b7\u00b7 \u03b7\u03bf \u2212 1| < +\u221e . $ (16)"}, {"title": "3.2 Multilayer architecture", "content": "When we cascade the layers of the virtual neural network (12), the following triangular linear operator plays a prominent role:\n$U = Um... U\u2081 = \\begin{pmatrix}  W_{1,m} & V_{1,m}\\\\  0 & 7_{1,m} 1_1\\end{pmatrix}  ,$ (17)\nwhere, for every n \u2208 {1, ..., m} and i \u2208 {1, ..., n}\n$Win = \\sum_{j=i}^n \\frac{\u03bb_j}{1 + AjXj}-Ni,j-1Wj+1,n$ (18)\nand, for every i \u2208 {1, . . ., m + 1} and j \u2208 {0,...,m},\n$\\begin{cases}  W_{i,j} = W_{i,...j}  if  j\u2265 i \\\\  1 otherwise,\\end{cases}$ (19)\n$\\begin{cases}  Ni,j =  Nj \\\\  Ni  1\\end{cases}$ if j\u2265 i , (20)\notherwise."}, {"title": "4 Stability and a-averagedness", "content": "In this section, we study the stability of the proposed neural network (6). This analysis is performed by estimating the Lipschitz constant of the network, and by determining under which conditions a-averagedness is achieved."}, {"title": "4.1 Assumptions", "content": "We will make the following assumption on the degradation operator T and the regu- larization operators (Dn)1<n<m:\nOperators T*T and (DDn)1<n<m can be diagonalized in an orthonormal basis (vp)p of X.\nThe existence of such an eigenbasis is satisfied in the following cases:\n\u2022T and (Dn)1<n<m are compact operators (which guarantees the existence of their singular value decompositions), and (DDn)0<n<m commute pairwise if we set Do = T.\n\u2022 X = I is the space L\u00b2([0,T]) of square summable complex-valued functions defined on [0,T], T is a filter (i.e., circular convolutive operator) and, for every n\u2208 {1,..., m}, Dn is a single-input c-output filter that maps every signal x \u2208 X to a vector (Dn,1x, ..., Dn,cx) \u2208 Xc. In this case, the diagonalization is performed in the Fourier basis defined as\n$(Vp \u2208 Z)(vt \u2208 [0,T]) up(t) = \\frac{1}{\\sqrt{T}} exp(2\\pi i \\frac{pt}{T}).$\n\u2022 X = V is the space l\u00b2(N) of complex-valued sequences x = (xn)0<n<N\u22121 equipped with the standard Hermitian norm, T is a discrete periodic filter and, for every n\u2208 {1,..., m}, Dn is a single-input c-output periodic filter that maps every signal x \u2208 X to a vector (Dn,1x, . . ., Dn,cx) \u2208 Xc. The eigenbasis is associated with the discrete Fourier transform:\n$(\u221a(p, n) \u2208 {0,..., N \u2212 1}2) vp(n) = \\frac{1}{\\sqrt{N}} exp (2\\pi \\frac{pn}{N} ).$\n\u2022 The two previous examples extend to d-dimensional signals (d = 2 for images), defined on [0, T\u2081] \u00d7 \uff65\uff65\uff65 \u00d7 [0, Ta] or {0, . . ., N\u2081 - 1} \u00d7 \u2026\u2026\u2026 \u00d7 {0, . . ., Na \u2212 1}.\nBased on the above assumption, we define the respective nonnegative real-valued eigenvalues (BT,p)p and (BDn,p)p of T*T and DDn with n \u2208 {1,...,m}, as well as the following quantities, for every eigenspace index p and i \u2208 {1, ..., n},\n$\u03b2^{(n)}_p = \\frac{1}{1+\u03bb\u03b7\u03c7\u03b7} (1 - \u03bb\u03b7(\u03b2_{T.p} + \u03b2_{Dn.p})),$ (21)\n$\u03b2_{i,n,\u03c1} = \\prod_{j=i}^{n} \u03b2^{(j)}_p,$ (22)"}, {"title": null, "content": "$B_{i,n,p} = \\sum_{j=i}^{n-1}B_j^{(i)} ...B_{j+1}^{(n)} \\frac{AjNi,j\u22121 - \u03bb\u03b7Ni,n\u22121}{1 + \u03bb\u03b7\u03c7\u03b7} + \\frac{ \u03bb\u03b7Ni,n\u22121}{1 + \u03bb\u03b7\u03c7\u03b7} ,$ (23)\n with the convention $B_{n-1}^{(n)} = 0$. As limit cases, we have\n$B_{n,n,p} = B^{(\u03b7)}_p, B_{n,n,p} = \\frac{\u03bb\u03b7}{1 + \u03bb\u03b7\u03c7\u03b7}.$ (24)\nNote that ($fon), up)p, ($Bi,n,p, Up)p, and ($Bi,n,p, Up)p are the eigensystems of Wn, Win and Wi,n, defined by (8), (19), and (18), respectively."}, {"title": "4.2 Stability results for the virtual network", "content": "We first recall some existing results on the stability of neural networks [40, Proposition 3.6(iii)] [41, Theorem 4.2].\nLet m > 1 be an integer, let (Hi)0<i<m be non null Hilbert spaces. For every n \u2208 {1, ...,m}, let Un be a bounded linear operator from Hn\u22121 to Hn and let Qn: Hn \u2192 Hn be a firmly nonexpansive operator. Set U = Um 0\uff65\uff65\uff65 \u25cbU\u2081 and\n$0m = ||U|| +  \\sum_{k=1}^{m-1} \\sum_{1<j1<...<jk<m-1} ||Um... Ujk+1|| ||Ujk 0\u2022\u2022\u20220 Ujk\u22121+1||\u00b7\u00b7\u00b7 ||Uj1 0\uff65\uff65\uff65 \u25cb U1 ||.$ (25)\nLet S = QmUm\u00b0\uff65\uff65\uff65 \u25cbQ1U1. Then the following hold:\n1. 0m/2m-1 is a Lipschitz constant of S.\n2. Let a \u2208 [1/2,1]. If Hm = Ho and\n$||U \u2013 2m (1 \u2212 a)1|| \u2013 ||U|| + 20m \u22642ma,$ (26)\nthen S is a-averaged.\nIn light of these results, we will now analyze the properties of the virtual net- work (12) based on the spectral quantities defined at the end of Section 4.1, and the parameters (An)1<n<m, \u2169 (and possibly (tn)1\u2264n\u2264m). One of the main difficulties with respect to the case already studied by [34] is that, in our context, the involved operators (Un)1<n<m are no longer self-adjoint.\nA preliminary result will be needed:\nLet m\u2208 N \\{0} be the total number of layers. For every layer indices n\u2208 {1,...,m} and i \u2208 {1, ..., n}, the norm of Un 0\uff65\uff65\uff65 \u25cb U\u00a1 is equal to \u221aai,n with\n$a_{i,n} = \\frac{1}{2} sup_p (B_{i,n,p}+ B_{i,n,p} + \u221a(Bin,p+Bin,p)^2 +4B_{in,p}B_{in,p}).$ (27)"}, {"title": null, "content": "where p indexes the eigenspaces of T*T.\nThanks to expressions (12), (18), (19), and (20), we can calculate the norm of Un \u25cb\u2026\u2026\u2026\u25cb U\u00bf. For every z = (x, b), Un 0\uff65\uff65\uff65 \u25cb Uiz = (Wi,nx + Wi,nb, Ni,nb) and\n$||Un 0\u00b7\u00b7\u00b70Uiz||\u00b2 = ||Wi,nx + Wi,nb||2 + ni,n||6||2 = ||Wi,nx||\u00b2 + 2(Wi,nx, Winb) + ||Wi,nb||2 + ni,n||6||2 .$\nLet (Bi,n,p, Up)pen defined by (22) and (\u1e9ei,n,p, Up)pen defined by (23) be the respective eigensystems of Wi,n and Wi,n. Let us decompose (x, b) on the basis of eigenvectors (Up)p of T*T, as\n$x= \\sum_p \u03be_pUp, b = \\sum_p 5_p Up.$ (28)\nIn the following, we will assume that X is a real Hilbert space. A similar proof can be followed for complex Hilbert spaces. We have then\n$||Un 0...0 Uiz||2 = \\sum_p B_{in,p}\u03be_p + 2\\sum_p B_{i,n,p}B_{i,n,p}\u03be_p5_p + \\sum_p(B_{i,n,p}^2 + \u03b7_{i,n}^2).$\nBy definition of the operator norm,\n$||Un 0...0 U\u00bf ||2 =  sup_{||x||^2+||6||^2=1}( \\sum_p B_{in,p}\u03be_p + (\u03b7^2 + B_{i,n,p}) 5_p  + 2B_{i,n,p}B_{i,n,p} \u03be_p 5_p )$.\nNote that, for every integer p and wp = (\u03bep, \u03b6p) \u2208 R2,\n$Bin,pp + (in + Bi,n,p) 2 + 2Bi,n,pBi,n,p 5p 5p = wp Ai,n,pwp$\nwhere\n$A_{i,n,p}= \\begin{pmatrix}  B_{i,n,p} & B_{i,n,p}B_{m,p}\\\\ B_{i,n,p}B_{i,n,p} & \u03b7^2+ B_{i,n,p}\\\\ \\end{pmatrix}$.\nHence,\n$||Un 0...0 U\u00bf ||\u00b2 =  sup_{z=(Wp)p, ||2||=1} \\sum w_p^T A_{i,n,p}w_p$.\nSince Ai,n,p is a symmetric positive semidefinite matrix,\n$||Un o... Ui ||2 =  sup_{p, ||wp||=1} w_p^T A_{i,n,p}w_p = sup Vi,n,p,$ (30)\np"}, {"title": null, "content": "where, for every index p, Vi,n,p is the maximum eigenvalue of Ai,n,p. The two eigenvalues of this matrix are the roots of the characteristic polynomial\n$(\u03bd\u2208 R) det(Ai,n,p \u2013 v112) = (B_{2,n,p} - \u03bd) (B_{i,n,p} + \u03b7 - \u03bd) \u2013 B_{i,n,p}B_{i,n,\u03c1} = v\u00b2 \u2013 (B_{2,n,p} + B_{2,n,p} + \u03b7_{i,n})v + B_{2,n,p}n$.\nThe discriminant of this second-order polynomial reads\n$\u0394_{i,n,p} = (B_{i,n,p} + B_{i,n,p} + \u03b7_{i,n})\u00b2 - 4B_{i,n,p}B_{i,n} = (B_{i,n,p} - B_{i,n,p} \u2013 \u03b7_{i,n})\u00b2 + 4B_{i,n,p}B_{i,n,p} \u22650$.\nTherefore, for every integer p,\n$Vi,n,p = \\frac{1}{2} (B_{i,n,p}+B_{i,n,p} + \\sqrt{(B_{2,np} + B_{i,n,p} + \u03b7_{i,n})\u00b2 - 4B_{i,n}B_{i,n}}).$ (31)\nBy going back to (30), we have\n$||Un... Ui||2 =  sup (||wp||^2 V_{i,n,p} \u2264 a_{i,n}.\n z=(Wp)p p\n\u03a3 ||\u03c9\u03c1||2=1\nIn addition, from the definition of ai,n, for every \u20ac > 0, there exists p* such that\n$ai,ne Vi,n,p*,$\nwhich shows that\n$ai,n - \u20ac < ||Un... Ui||2 < ai,n.$\nWe deduce that\n$||Un 0...0 U\u00bf ||2 = a_{i,n}.$\nRemark 4.1. The previous result assumes that the product space X X X is equipped with the standard norm. Since we may be interested in the stability w.r.t. variations of the observed data y, and bo = T*y, it might be more insightful to consider instead the following weighted norm:\n$(\u2200z = (x, b) \u2208 X x X ) ||z|| = \\sqrt{ ||x||2 + ||6||},$ (32)\nwhere b is decomposed as in (28),\n$||b|| = \\sqrt{\\sum \\frac{C_p}{W_p}},$ (33)"}, {"title": null, "content": "and w = (w)p is such that infpWp > 0 and suppWp < +\u221e. The latter weights aim to compensate the effect of T* on the observed data. This space renormalization is possible since the operator Qn remains firmly nonexpansive after this norm change, because of its specific structure.\nThe expression of ai,n for n \u2208 {1,...,m} and i \u2208 {1,...,n} in Lemma 4.1 is then modified as follows:\n$ai,n =  \\frac{1}{2}sup_p (B_{i,n,p}+ W_pB_{i,n,p}+ \\sqrt{(B_{i,n,p}+ W_pB_{i,n,p})^2 - 4B_{i,n,p}B_{i,n,p}W_p}).$ (34)\nWe will now quantify the Lipschitz regularity of the network.\nLet m\u2208 N \\{0}. For every n \u2208 {1, ...,m} and i \u2208 {1, ..., n}, let ai,n be given by (27). Set \u03b8\u2081 = 1 and define (On)1<n<m recursively by\n$(\u2200n \u2208 {1,...,m}) \u03b8\u03b7 = \\sum_1^n\u03b8_{i-1}\u221a\u03b1_{i,\u03b7}.$\nThen 0m/2m-1 is a Lipschitz constant of the virtual network (12). In addition,\n$\\frac{\u221aa1,m}{ 2^{m-1}}\u2264 \\frac{\u03b8m}{2^{m-1}} \u2264 (\\prod_{n=1}^m an,n)^{1/2}.$ (35)\nAccording to Proposition 4.11, if Om is given by (25), then 0m/2m-1 is a Lip- schitz constant of the virtual network (12). On the other hand, it follows from [40, Lemma 3.3] that om can be calculated recursively as\n$(\u2200n \u2208 {1,...,m}) \u03b8\u03b7 = \\sum \u03b8_{i-1}||Un... U\u00bf||,$\ni=1\nwith 00 = 1. Finally, Lemma (4.1) allows us to substitute (\u221aai,n)1<i<n for (||Un 0.0 Ui||)1<i<n in the above expression.\nIn addition, according to [41, Proposition 4.3(i)],\n$\\frac{|| Um U1 ||}{2^{m-1}} \u2264 \\frac{\u04e8\u0442}{2^{m-1}} \u2264 \\prod_{n=1}^m ||Un||.$\nIt follows from Lemma (4.1), that ||Um0...0U1|| = \u221aa1,m and\n$(\u2200n \u2208 {1,...,m}) ||Un|| = \u221aan,n,$\nwhich allows us to deduce inequality (35)."}, {"title": null, "content": "We can relate the bounds provided on the Lipschitz constant in the previous proposition to simpler ones.\nAssume that X \u00d7 X is equipped with the norm defined by (32). Then\n$sup max (\\prod_{n=1}^m \u221a(B), Bim,p + Wim,p+nim) \u2264  \\frac{\u04e8\u0442}{2^{m-1}}$\n$< (sup (1) + (1+x)\u00b2 + m\u00b2))2.$ (36)\n(B), 111,pm  m\nProof. It follows from (34) that, for every n \u2208 {1, ..., m},\n$an,n \u2264 sup B_{n,n,p} + W_pB_{n,n,p} + nim p sup (B^{(\u03b7)}_p)\u00b2 +  W_p \\frac{\u03bb\u03b7}{1 + \u03bb\u03b7\u03c7\u03b7}^2 +nim  .\\\nThe upper bound in (36) then follows from the upper bound in (35).\nOn the other hand,\n$a1,m = \\frac{1}{2}sup (B1,m,p + WpB1,m,p+ni,m+\u221a(Bi,m,p-WpBi,m,p \u2013 ni"}]}