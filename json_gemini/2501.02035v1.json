{"title": "3D Cloud reconstruction through geospatially-aware\nMasked Autoencoders", "authors": ["Stella Girtsou", "Lilli Freischem", "Kyriaki-Margarita Bintsi", "William Jones", "Emmanuel Johnson", "Emiliano Diaz Salas-Porras", "Joppe Massant", "Guiseppe Castiglione", "Michael Eisinger", "Anna Jungbluth"], "abstract": "Clouds play a key role in Earth's radiation balance with complex effects that intro-\nduce large uncertainties into climate models. Real-time 3D cloud data is essential\nfor improving climate predictions. This study leverages geostationary imagery\nfrom MSG/SEVIRI and radar reflectivity measurements of cloud profiles from\nCloudSat/CPR to reconstruct 3D cloud structures. We first apply self-supervised\nlearning (SSL) methods\u2014Masked Autoencoders (MAE) and geospatially-aware\nSatMAE-on unlabelled MSG images, and then fine-tune our models on matched\nimage-profile pairs. Our approach outperforms state-of-the-art methods like U-Nets,\nand our geospatial encoding further improves prediction results, demonstrating the\npotential of SSL for cloud reconstruction.", "sections": [{"title": "1 Introduction", "content": "Clouds play a crucial role in Earth's radiation balance by reflecting sunlight (cooling) and absorbing\nheat (warming) [1]. As climate change progresses their complex interactions increase uncertainties\nin climate models. Enhanced global, real-time 3D cloud data can help reduce these uncertainties,\nimproving climate predictions and decision-making. Satellite remote sensing has transformed climate\nresearch by providing global data. The European Space Agency's recently launched EarthCARE\nmission aims to improve our understanding of cloud dynamics through imaging, radar, and lidar\ninstruments [2]. Over the last decades NASA's CloudSat mission has provided valuable measurements\nof cloud vertical profiles using radar, but is limited by infrequent revisits (every 16 days) and a narrow\nswath (1.4 km). Additionally, with its sun-synchronous orbit, CloudSat passes over the equator\nat the same local time, limiting observations to \"snapshots\" of the atmosphere at specific times\nof day. In contrast, imaging instruments take measurements across wider fields-of-view and with\nhigher temporal resolution, but they only offer a 'top-down' perspective and do not directly measure\natmospheric profiles. However, combining images in different spectral channels with overlapping\nmeasurements of atmospheric profiles allows extrapolation of vertical profiles beyond the radar track.\nBarker et al. [3, 4] developed an algorithm to extend EarthCARE profiles to 3D through intensity\npixel-matching. Recent work [5, 6, 7] has used ML-based methods (e.g. U-Nets, CGANs, Linear\nRegression, Random Forests, XGBoost) to estimate vertical cloud information from 'top-down'\nmeasurements. Notably Br\u00fcning et al. [5] trained a Res-UNet to fuse satellite images from the\nMeteosat Second Generation (MSG) Spinning Enhanced Visible and InfraRed Imager (SEVIRI) with\nCloudSat cloud profiling radar (CPR) reflectivity profiles, reconstructing 3D cloud structures. For\nall approaches, model training required precise spatial and temporal alignment between the data\nsources. Due to the limited overpasses of the radar satellites (Figure 1b), profile measurements are\nmuch sparser than available imagery (for comparison, MSG/SEVIRI generates 40 TB of image data\nper year, while CPR produces 150 GB annually). Recent advances in self-supervised learning (SSL)\nhave shown promise in pre-training models on large, unlabeled datasets, yet their application to\ncloud studies remains under-explored. In this work, we apply SSL methods\u2014Masked Autoencoders\n(MAE, [8]) and geospatially-aware Masked Autoencoders (based on SatMAE, [9])\u2014to multispectral\nMSG/SEVIRI data from 2010. We then fine-tune the pre-trained models for the task of 3D cloud\nreconstruction using matched image-profile pairs. Our results show that pre-training consistently\nimproves performance for this task, especially in complex regions like the tropical convection\nbelt. Pre-trained models with geospatial awareness (i.e., time and coordinate encoding) especially\noutperform randomly initialized networks and simpler U-Net architectures, leading to improved\nreconstruction results. The code will be made available upon acceptance."}, {"title": "2 Data & Method", "content": "Input Imagery. We use radiance data from MSG/SEVIRI in 11 spectral channels as model input.\nMSG takes measurements from a geostationary orbit, centered at 0\u00b0 longitude and latitude, and covers\na +80\u00b0 field-of-view, which we limit to \u00b145\u00b0 to avoid data quality issues. New images are captured\nevery 15 mins, offering continuous monitoring at a spatial resolution of 3 km at the sub-satellite\npoint. From the 40 TB of annually available data, our pre-training dataset comprises hourly cadence\ndata split monthly into days 1-10 (training), days 21-22 (validation), and days 25-27 (testing). We\nsplit each full-disk image into non-overlapping patches of 256 x 256 pixels, resulting in 1.2 million\n(8.6 TB) patches. To increase training speed and reduce computational load, we randomly sample a\ndifferent 10% of our patches each epoch during pre-training. With this sub-sampling, convergence\nduring pre-training remains smooth and stable.\nTarget Profiles. We use radar reflectivity profiles measured via the CPR onboard the CloudSat\nsatellite as our target. We spatially and temporally align CloudSat/CPR radar reflectivity from 2010\nwith MSG/SEVIRI images, and restrict our dataset to Cloudsat tracks with at least 20% cloud cover.\nThis results in approximately 47 thousand image-profile pairs consisting of a 11 x 256 x 256 pixels\nimage and a geo-referenced CloudSat profile with 125 vertical bins, spanning roughly 30 km into the\natmosphere. We crop 25 (10) height levels at the bottom (top) of each profile to remove measurement\nartifacts and reduce clear sky data [7, 5]. The length of each profile varies due to the intersection\nbetween CloudSat's trajectory and the SEVIRI image. Similar to pre-training, we use days 1-19,\n23-24, and 28-31 for training, days 20-22 for validation, and days 25-27 for testing. Since clouds\nevolve over minutes to hours, a temporal split based on multi-day groupings ensures minimal data\nleakage. We randomly sample 50% of all image-profile pairs per epoch.\nModels & Training. For pre-training, we use MAEs with Vision Transformer (ViT) backbones\nto tokenize MSG/SEVIRI images and learn local features and their spatial relationships through\npositional encodings. We chose MAEs for their potential to handle missing data, and ViTs for their\nself-attention mechanisms and ability to relate local and global image features - all important for\nEarth Observation satellite data. We tested two ViT variants: a base ViT-B (90M parameters) and a\nsmall ViT-S (26M), encoding 11 \u00d7 256 \u00d7 256 images with 768 and 384 hidden channels, respectively.\nImage tokenization was performed at 8x8 and 16x16 pixel resolutions, with 75% of tokens masked\nfor reconstruction using multi-layer decoders [8]. Given our geospatial data, we also adapted the"}, {"title": "3 Results", "content": "Pre-training. We experimented with different sizes of ViT backbones, ViT-base (90M) and ViT-small\n(26M), and tokenization schemes. As shown in Figure 2, smaller ViT tokens yield lower losses\nand finer-grained detail in the image reconstruction. Since the small and base MAE models exhibit\ncomparable performance, we chose the smaller ViT for its reduced computational demands.\nFine-tuning. To evaluate the benefit of self-supervised pre-training, we compare models that were\ntrained from scratch, or fine-tuned with frozen or unfrozen backbones. As shown in Figure 5 in the\nAppendix, pre-training brings substantial benefit, especially when the backbone is unfrozen. The\nunfrozen MAE outperformed all models in validation loss, achieved better PSNR and demonstrated\nfaster, smoother convergence than the U-Net."}, {"title": "Geospatial Awareness", "content": "Having demonstrated the benefit of self-supervised pre-training with ViT\nbackbones that encode small-scale cloud features, we now compare the standard MAE to our space-\nand time-aware SatMAE model. Looking at the perceptual quality of the predicted cloud profiles\n(Figure 3), the MAE and SatMAE models lead to visually sharper reconstructions than the U-Net.\nFor information, an example rendering of a predicted 3D cloud volume is shown in Figure 7 in\nthe Appendix, which shows good qualitative agreement with where clouds appear to be in the\ninput MSG/SEVIRI image. Table 1 shows that SatMAE achieves lower root-mean-square errors\n(RMSE) across the test set compared to both the U-Net and the MAE. SSIM and PSNR are similar\nacross models, with a small improvement for SatMAE. The values presented in Table 1 show the\nmean and standard deviation of the metrics across the test set, and do not represent actual model\nuncertainties. To gain more in-depth understanding on the performance of the models, we separated\nthe contributions to the RMSE by cloud type (Table 4 and Figure 8 in the Appendix). The largest\nerrors are observed for Nimbostratus clouds and deep convection, i.e. rain and storm clouds with\nhigh radar reflectivities. For all cloud types except Nimbostratus, SatMAE achieves lower errors\ncompared to both the baseline and the standard MAE. Figure 4 clearly demonstrates that the time\nand coordinate encodings positively impact performance, especially in the tropical convection belt,\nwhere cloud formations are larger and denser. Furthermore, Figure 6 in the Appendix shows that\nthe coordinate encoding provides the most significant contribution to the model performance. The\nimprovement of the MAE model over the U-net, and that of the SatMAE over the MAE model are\nboth calculated to have p-value of <0.001 showing the statistical significance of the improvement,\nwhen we take region into account, is very strong (see appendix A for details on hypothesis test)."}, {"title": "4 Conclusions", "content": "In this work, we used SSL via MAEs to pre-train models using unlabelled MSG/SEVIRI images,\nand then fine-tuned the models for 3D cloud reconstruction, using aligned pairs of MSG/SEVIRI\nand CloudSat/CPR radar reflectivity profiles. In addition to the standard MAE implementation,\nwe adapted a SatMAE model to encode the date, time, and location of the input data. Compared\nto the U-Net, the current state-of-the-art on the task, our models showed improved performance\nacross RMSE, PSNR, and perceptual quality. While the improvements are moderate on average,\nvisualization of prediction highlights that the SatMAE model achieves the lowest errors in the tropical\nconvection belt, demonstrating its superior generalization. This suggests that this learning set up\nhas potential for larger gains for harder learning tasks such as 3D-cloud-type segmentation, helping\nfurther our goal of characterizing 3D cloud properties and contributing to the narrowing of climate\nmodel uncertainties. While our current work only explores reconstruction of CloudSat tracks for one\nyear, future work will apply this method to longer time periods and ESA's EarthCARE data once\navailable. This could enable the creation of long-term 3D cloud products, crucial for advancing our\nunderstanding of complex climate feedback mechanisms."}, {"title": "A Appendix", "content": "Further Training Details. We conducted our experiments on 1-2 NVIDIA V100 GPUs (16GB)\nvia Google Cloud, with batch size 8 during pre-training and batch size 4 during fine-tuning. We\nused the Adam optimizer [10] with an initial learning rate of 0.00015, using backpropagation [11].\nTraining was optimized via the Mean Squared Error (MSE) loss, while additional metrics like the\nPeak Signal-to-Noise Ratio (PSNR) and the Structural Similarity Index Measure (SSIM) [12] were\nmonitored. Checkpointing was used to save models with the lowest validation loss. Both pre-training\nand fine-tuning ran for up to 50 epochs. Regarding training times, the pretraining duration ranged\nfrom 2 to 6 days and was largely dependent on the patch size and the scale of the ViT backbone\n(ViT-small and ViT-base, see Table 2). Smaller patch sizes led to longer training times due to the\nincreased number of tokens, while larger ViT architectures extended the duration due to the higher\nnumber of trainable parameters. For fine-tuning, training times ranged from 8 to 24 hours for the\nU-Net and MAE/SatMAE models with patch sizes of 16 and 8. With even smaller ViT patch size of\n4, the training duration increased significantly, reaching up to 5 days."}, {"title": "Hypothesis test", "content": "To show that the MAE-ViT-small 8 has superior performance to the U-net and the\nSatMAE-ViT- small 8 has superior performance to the MAE-ViT-small 8, for a random location we\nperformed hypotheses tests with p-values which we report in section 3. The null hypothesis for this\ntest is that while a given model may have a higher probability to obtain a higher RMSE for a given\nlocation, over the entire region under study, the expected probability for any given location is 0.5,\nmeaning on average, the models have equal performance. Under this null hypothesis the number\nof times model 1 is superior out of the 5,112 trials (the number observations in our test set) would\nbe distributed as binomial random variable with parameters $n = 46,752$ and $p = 0.5$ assuming the\noutcome for each location was random, in which case we could construct a simple hypothesis test\nwith a theoretical p-value. However, it is reasonable to assume that $X_i \\in {1, 2}$, that describes which\nmodel is superior at location i is correlated with $X_j \\in {1,2}$ that describes which model is superior\nat location j especially if i and j are close together. In order to avoid making specific assumption\nabout the form of the spatial correlation we construct an empirical null distribution by taking the\nfollowing steps:\n1. construct $Y_i \\in {0, 1}, i = 1, ..., 5, 112$ that determines which model is superior at location i\nin the following way $Y_i = 1_{{W_i \\lt m}}$ where:\n\u2022 $1_x$ is the indicator function\n\u2022 $W_i = z - Z$\n\u2022 $Z$ is the test rmse observed at location i for model k and,\n\u2022 m is the test observed sample median of W accross locations.\nThis ensures that the null distribution respects the null hypothesis assumption that for a\nrandom location the probability that any model is superior is 0.5, while also preserving the\nspatial correlation structure.\n2. We ideally would like to have many independent samples of $Y_i$ and then calculate, $P_i$,\nthe proportion of times each model is superior at a location i. Since we only have one\nobservation of each $Y_i$ we obtain a boostrap sample by using $Y_i$ and $Y_j$ for $j \\in NN_{1000}(i)$\nwhere $NN_{1000}(i)$ is the set of 1000 nearest neighbor locations to location i. Higher spatial\ncorrelation makes it more likely that, even though the probability that any given model is\nsuperior is 0.5, in an observed sample we observe a proportion far from the expected 0.5\n(it increases the variance). The assumption is that the magnitude of the correlation at the\nsmaller scales (1000 locations) is larger than the at larger scales (the scale of figure 4). If\nthis holds our null distribution will be conservative meaning it has more variance than the\nactual null distribution, yielding valid p-values.\nWe then calculate the probability of obtaining the observed values for the proportion of times\nthe MAE-ViT-small 8 has superior performance to the U-net (0.85) or a greater one, under the\nnull distribution. Similarly, we calculate the probability of obtaining the observed values for the\nproportion of times the SatMAE-ViT- small 8 has superior performance to the MAE-ViT-small 8\n(0.73) or greater, under the null. The probability of such extreme proportions of cases under the null\ndistribution is less than 0.001 meaning both hypothesis tests are assigned a p-value of <0.001, and\nthe null hypothesis can be rejected in both cases."}]}