{"title": "Enhancing Training Efficiency Using Packing with Flash Attention", "authors": ["Achintya Kundu", "Rhui Dih Lee", "Laura Wynter", "Raghu Kiran Ganti"], "abstract": "Padding is often used in tuning LLM models by adding special tokens to shorter training examples to match the length of the longest sequence in each batch. While this ensures uniformity for batch processing, it introduces inefficiencies by including irrelevant padding tokens in the computation and wastes GPU resources. On the other hand, the Hugging Face SFT trainer offers the option to use packing to combine multiple training examples up to the maximum sequence length. This allows for maximal utilization of GPU resources. However, without proper masking of each packed training example, attention will not be computed correctly when using SFT trainer. We enable and then analyse packing and Flash Attention with proper attention masking of each example and show the benefits of this training paradigm.", "sections": [{"title": "1 Introduction", "content": "In LLM tasks, data is represented as sequences of tokens, where each token typically corresponds to a word, character, or sub-word. These sequences form the input to the models, which aim to learn meaningful representations and capture intricate patterns within the data. However, the lengths of these sequences can vary substantially, posing a computational challenge during training.\nTo illustrate this challenge, consider a scenario where we aim to fine-tune a LLM on a corpus of text. Each sentence in the corpus may have a different length, ranging from a few words to several dozen or even hundreds. Traditional approaches would require padding shorter sequences with special tokens up to the maximum sequence length. While this ensures uniformity for processing, it introduces inefficiencies by including irrelevant padding tokens in the computation, which not only wastes GPU resources but also can dilute the model's learning signal.\nBatch-level, or dynamic, padding improves this by organizing sequences within a batch in a manner to improve computational efficiency without sacrificing learning efficacy. This is achieved by dynamically padding sequences to the same length within a batch, up to the maximum example length in each batch, allowing for parallel processing across multiple sequences. By eliminating the need for fixed-length padding across batches, Batch-level padding minimizes wasted computation on padding tokens, leading to improvements in training efficiency and batch inference throughput.\nCentral to the implementation of batch-level padding is the concept of masking. Masking mechanisms enable neural network models to selectively ignore padded regions during computation, ensuring that they do not contribute to the model's output or gradients. This enables the model to focus exclusively on the relevant parts of the input sequences, thereby preserving the integrity of the learning process"}, {"title": "2 Related Work", "content": "There are a few ways that sample packing can be enabled with Flash Attention 2 [9] and proper attention masking of each example. The Flash Attention repository itself offers a way to pack while enabling proper masking of examples with Flash Attention. See Flash Attention Closed Issue 654 [1]. Others have proposed padding-free transformers, such as [6, 7]. The padding-free transformer methods require substantial and intrusive changes however to Hugging Face transformers library. As such, these methods have seen less uptake by the community and are not currently available in the Hugging Face library. For Hugging Face SFT Trainer users, it is desirable to have a readily available solution without requiring going outside the library.\nMost works on sample packing are concerned with how to select the sequences to pack together. This problem can be formulated as a bin packing, or machine scheduling, problem from the combinatorial optimisation literature. For instance, [12] discuss one such approach for sequence selection and packing. In [8] the authors also use packing to combine multiple training examples into a single sequence, separating inputs from targets using an end-of-sequence token, and also using masking to prevent tokens from attending to others across the packed example boundaries.\nThe multi-pack sampler repository [5], used also by [3], employs a first-fit-decreasing heuristic for the bin packing problem to select sequences to put together on each gpu in a distributed computing setting. Since this method is the most widely used, we make use of it in conjunction with our Packing with PositionIDs solution to further enhance the packing performance.\nWe mention also the LengthGroupedSampler function [4] in Hugging Face Transformers library which is often used in conjunction with sample padding. We consider this method as another baseline, referred to as GroupByLength+Padding."}, {"title": "3 Packing with Position IDs", "content": "We denote the input_ids of the tokenized i-th example as a tensor of shape (Li, ):\n\\(X_i = [x_i^{(0)},...,x_i^{(L_i-1)}]\\).\nLet Xi1, Xi2,\u2026 be an ordering of the training examples. Assuming a batch size bs = 4, consider a batch consisting of examples {Xi1, Xi2, Xi3, Xi4}. Then, in a batch-level padding based approach, the batch, B, is processed as a tensor of (4, Lmax), where Lmax:= max{Li\u2081, Li2, Li3, Li4} and the required number of padding tokens are appended to each example (Lmax - Li\u2081 to Xi\u2081) to make them tensors with shape (Lmax,).\nTo avoid cross-contamination while packing examples, we propose to utilize position IDs to demarcate boundaries of individual examples in a packed sequence. We assume support for Flash Attention and the availability of position IDs which is the case for the most popular open-source LLM models.\nThen, to use the position IDs, we require arranging the data sequences accordingly. There are multiple ways to arrange the data with this solution, we discuss three below."}, {"title": "3.1 Online Mini-batch Collating", "content": "In mini-batch collating, the padding-free collator must pack the examples online, for each mini-batch, into a tensor of dimension, dimmini:\ndimmini = (1, \u2211 Li),\ni=1...4\nand then provide position IDs which then take the form:\n[[0, , (Li\u2081 - 1), 0, , (Li2 - 1), 0, , (Li3 - 1), 0, , (Li4 - 1)]]."}, {"title": "3.2 Offline Batch Collating", "content": "It is also possible to pack a full set of samples offline in a single tensor of dimension dimflat:\ndimflat = (1, bs * msl),\nwhere bs is the batch size and msl denotes the maximum sequence length allowed for individual training examples."}, {"title": "3.3 Collating with Optimised Sample Selection", "content": "It is possible to leverage the bin-packing-type sample selection algorithms such as [5] in conjunction with the solution provided by Packing with PositionIDs. In this case the samples to assign to each gpu and the position IDs for those samples are grouped per gpu. We call this Multipack+Position IDs. To contrast with the Multipack bin packing method, we introduce two simple baselines: (q) RandomPacking+PosID and (b) SortedPacking+PosID; Examples are packed on a first-come-first-serve basis from randomly ordered training data set and sorted (length-wise from long to short) data set, respectively.\nSpecifically, to enable the use of position IDs, we modify the models' _flash_attention_forward(), adding the argument position_ids and extracting the number of examples in the batch from the position_ids. When attention_mask is None in the case of number of examples > batch size, we compute cu_seq_len from position_ids and use the flash_attn_varlen_func()."}, {"title": "3.4 PaddingFreeCollator", "content": "We further provide a new off-the-shelf data collator, the PaddingFreeCollator, summarised below.\nreturn { input_ids: ..., labels: , position_ids: ... }\ninput_ids (1, \u2211 sequence_length_of_each_example) :\nConcatenate all the examples in the mini.\nlabels (1, \u2211 sequence_length_of_each_examples) :"}, {"title": "3.5 Example", "content": "Consider again the scenario with a batchsize = 4 where the sequences are as follows:\n[[10, 11, 12, 13],\n[20, 21, 22, 23, 24, 25, 26, 27],\n[30, 31, 32, 33, 34],\n[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 410]]\nThe padding-free collator returns the input IDs, labels, and the position IDs of each example after concatenating the examples together. Hence, the collator provides:\ninput_ids:\n[[10, 11, 12, 13, 20, 21, 22, 23, 24, 25, 26, 27,\n30, 31, 32, 33, 34, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 410]],\nlabels:\n[[-100, 11, 12, 13, -100, 21, 22, 23, 24, 25, 26, 27,\n-100, 31, 32, 33, 34, -100, 41, 42, 43, 44, 45, 46, 47, 48, 49, 410]],\nposition_ids:\n[[0, 1, 2, 3, 0, 1, 2, 3, 4, 5, 6, 7,\n0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]].\nThe modifications required are lightweight and are limited to providing the position IDs to Flash Attention. For details, see the repository at [2]. This solution relies, however, on the model exposing position IDs. As of the time of writing, 13 models expose position IDs and are supported by the solution. Specifically: dbrx, falcon, gemma, llama, mistral, olmo, phi, phi3, qwen2, qwen2_moe, stablelm, starcoder2 are all supported by the solution."}, {"title": "4 Experiments and Results", "content": "We consider three different datasets with different example-length characteristics: FLAN [15], OrcaMath[13], and python code data from The Stack[11]. A subset of 20K examples from each of the 3 datasets was used in the experiments. We evaluate the throughput in tokens/second, denoted Tok/s, peak memory, denoted Mem, and validation loss, denoted VLoss. All tests were performed on a single A100-80GB node having 8 GPU with FSDP.\nThe characteristics of example lengths in our 20K subsets of FLAN, OrcaMath, as well as in the python coding dataset from The Stack are shown in the histograms in Figure 1. Observe that both FLAN and OrcaMath have primarily small example lengths, which makes them amenable to significant throughput gains via packing, while the coding dataset the Stack has significantly longer examples, and hence we expect less throughput benefit from packing on that dataset."}, {"title": "4.1 Main Results", "content": "Four approaches were evaluated. The two base cases are: (i) No packing with truncate, i.e., padding, and (ii) Basic packing (as is currently available in SFT trainer, without position IDs). Then, we evaluate two variants of the proposed solution: (iii) Our solution providing packing with position IDs, in which the batch is prepared offline and flattened, and (iv) Our solution with online packing with position IDs for each mini-batch.\nWe expect that base case (i) using padding will be the slowest approach but provide optimal training loss reduction since it does not disrupt any training examples. On the other hand, basic packing without position IDs can achieve high throughput, at the expense of distorting examples through improper attention masks.\nWe demonstrate the benefits of the solution on 9 of the 13 supported models. Specifically, we test it on Llama-2-7B-fp16, mistralai/Mistral-7B-v0.1, tiiuae/falcon-7b, google/gemma-7b, microsoft/phi-2, Qwen/CodeQwen1.5-7B, bigcode/starcoder2-7b, stabilityai/stablelm-2-1_6b, Qwen/Qwen1.5-MoE-A2.7B.\nObserve that our Packing with PositionID methods improve substantially the throughput in terms of tokens per second, in many case above and beyond even basic packing that does not account for improper cross-attention (the second row in each model section).\nHowever, this maximal packing has an impact on the loss behaviour. Due to the fact that far fewer optimisation steps are taken with such maximal packing, the loss does not decrease as fast, and its effect is confirmed by the validation loss (\"VLoss\") after one epoch. Hence, as a best-of-both worlds remedy, we propose the online mini-batch approach to Packing with PositionIDs, which due to the packing of fewer examples in each pack, does not achieve the maximal throughput, however it"}, {"title": "4.2 Ablation Study on Mini-batch Size", "content": "In this ablation study we compare the effect of mini-batch size (bs) on training throughput and memory for two approaches: the standard padding vs proposed mini-batch packing. For this experiment, we fine-tune Mistral-7B[10] on 2 different training datasets: FLAN_20k, (right) OrcaMath_20k."}, {"title": "4.3 Ablation Study on Sample Selection Method", "content": "In this ablation study we consider two different models: Mistral-7B[10] and CodeLlama-Python-7B [14]. In Tables 4,5, and 6, we examine different batch sizes and sequence lengths on two of the most popular models, Mistral-7B and CodeLlama-Python-7B with several sample selection methods. We provide the results on FLAN, OrcaMath, and the Stack [11], respectively. In the tables, bs is the mini-batch size per GPU, msl stands for maximum sequence length of an example/packed sequence, and gas denotes the number gradient accumulation steps."}, {"title": "5 Conclusions", "content": "We provide a lightweight solution for packing with PositionIDs that can be readily integrated into Hugging Face transformers library and demonstrate its benefits."}]}