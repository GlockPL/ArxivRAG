{"title": "Heuristic-Free Multi-Teacher Learning", "authors": ["Huy Thong Nguyen", "En-Hung Chu", "Lenord Melvix", "Jazon Jiao", "Chunglin Wen", "Benjamin Louie"], "abstract": "We introduce Teacher2Task, a novel framework for multi-teacher learning that\neliminates the need for manual aggregation heuristics. Existing multi-teacher\nmethods typically rely on such heuristics to combine predictions from multiple\nteachers, often resulting in sub-optimal aggregated labels and the propagation\nof aggregation errors. Teacher2Task addresses these limitations by introducing\nteacher-specific input tokens and reformulating the training process. Instead of\nrelying on aggregated labels, the framework transforms the training data con-\nsisting of ground truth labels and annotations from N teachers \u2013 into N+1 distinct\ntasks: N auxiliary tasks that predict the labeling styles of the N individual teach-\ners, and one primary task that focuses on the ground truth labels. This approach,\ndrawing upon principles from multiple learning paradigms, demonstrates strong\nempirical results across a range of architectures, modalities, and tasks.", "sections": [{"title": "Introduction", "content": "Since AlexNet [1], a decade of ML development has yielded a wealth of capable \"teachers\". Humans\nas teachers, though expensive, provide near-perfect accuracy annotation. Large Language Models\n(LLMs) offer excellent zero-shot capabilities, generating high-quality \"silver\" data for many tasks.\nDomain-specific foundational models serve as specialized teachers within their domains. An ideal\nlearning framework would enable ML models to learn effectively from all useful data sources, con-\nsidering their strengths and weaknesses, unlocking the benefits of both accuracy and scalability.\nHowever, effectively leveraging multiple teachers remains an open challenge. Conflicting annota-\ntions from humans, LLMs, and domain-specific models can be difficult to reconcile, e.g., various\nteachers give conflict annotation for the same input samples. Also, directly aggregating predictions\nfrom LLMs and machine learning (ML) models as final labels can be problematic due to the in-\nherent noise in individual predictions, which can propagate and amplify in the inaccuracies after\naggregation.\nExisting multiple-teacher learning approaches typically leverage the aggregated output of an ensem-\nble of teachers [2] [3]. Most use a simple weighted average of teacher predictions, often with fixed\nor uniform weights [4][5]. More sophisticated approaches explore manually tuned weights [6] or\nlearn instance-specific teacher importance weights [7] [8]. Alternatively, some methods focus on\nselecting the \"best\" teacher for each instance, using strategies ranging from random selection [4]\nto reinforcement learning-based dynamic selection [9]. Specialized approaches, such as assigning\nteachers to distinct language pairs in multilingual neural machine translation [10], represent specific\ncases of domain-based teacher selection. However, a common limitation is the reliance on pre-\ndefined heuristics for teacher aggregation or selection, where these heuristics treat the aggregated"}, {"title": "Proposed Heuristic-Free Multi-Teacher Learning", "content": "teacher output, often noisy or sub-optimal, as the ground truth for student training. This limitation\nmotivates our exploration of heuristic-free multi-teacher learning.\nThis work introduces Teacher2Task, a novel multi-teacher learning method that departs from the\nconventional heuristic-based approaches. Our proposed method explicitly incorporates teacher-\nspecific tokens into the input, allowing the model to internally differentiate between individual\nteacher labeling styles. For each teacher, we introduce an auxiliary task: predicting the teacher's\nconfidence score across the entire input distribution. Given N teachers and ground truth labels, we\nconstruct N+1 training tasks: N auxiliary tasks focused on predicting each teacher's confidence\nscores, and one primary task focused on learning the ground truth. By jointly learning from both the\nground truth and the diverse predictions of multiple teachers, the student model learns a more robust\nand nuanced understanding of the data distribution, effectively interpolating between the ground\ntruth and diverse teacher perspectives.\nThe proposed approach offers several key advantages. First, it is highly label-efficient, as each\nteacher prediction serves as an additional training sample. Second, by explicitly encoding teacher\nidentities within the input, the method eliminates the need for manual aggregation or selection\nheuristics. Finally, it mitigates the impact of potential label inaccuracies by treating teacher con-\nfidence scores as data for auxiliary tasks rather than as absolute ground truth. Experiments across\nvarious modalities and architectures demonstrate that Teacher2Task consistently benefits from the\ninclusion of more teachers, showcasing improved performance and robustness.\nOn another perspective, we extend distillation [11] from one teacher to multiple, with a relatively\nstraightforward path to scaling to \"almost infinity\" teachers. Deep Learning has thrived on scal-\ning rules, notably data-scaling and model-scaling. In a sense, we're introducing teacher-scaling,\npotentially opening new avenues for innovation."}, {"title": "Multi-Teacher Transformation", "content": "Traditional ensemble methods [2][3] rely on aggregating predictions from multiple teacher models\nfor a given input (see Fig. 1a). However, this approach suffers from several drawbacks:(1) Imperfect\nPredictions: Individual teacher predictions and the aggregated result can both be inaccurate. (2)\nHeuristic Aggregation: Combining annotations often relies on manual and sub-optimal heuristics,\nand (3) Low Efficiency: Annotating a single sample requires running inference on all N teachers.\nProposed Method: To address these limitations, we propose Teacher2Task (Fig.1b). Instead of\ndirectly aggregating predictions, we transform the problem by incorporating teacher identity and\npredicted class as inputs to a model that predicts the teacher's confidence score.\nMulti-Teacher Input = Teacher Identity + Original Input + Predicted Output Class\nMulti-Teacher Output = Confidence Score"}, {"title": "Conceptual Illustration", "content": "The proposed algorithm, though simple, introduces several key features that address inherent chal-\nlenges in existing multi-teacher learning approaches.\nIndividualized Teacher Tasks: For each input sample annotated by a teacher, we add special\nteacher tokens to the input and train the model to predict that teacher's confidence score. This\nallows seamless integration of new teachers \u2013 each new teacher simply introduces a new auxiliary\ntask:\nTask for a new teacher: Predict the teacher's confidence score for each input\nacross the entire input distribution.\nResolving Annotation Conflicts: Traditional multi-teacher learning often relies on heuristics like\nweighted aggregation or teacher selection to resolve conflicting annotations from multiple teachers\non the same input. Our algorithm circumvents this issue. By appending a unique teacher-specific\ntoken to each input, the model learns to differentiate between teachers and their individual labeling\nstyles, implicitly resolving conflicts.\nMitigating Label Noise: Another challenge in multi-teacher learning is the potential for noisy or\ninaccurate labels, both from individual teachers and from aggregated predictions. Existing methods\nfrequently use aggregated results as pseudo-labels for the student, propagating these inaccuracies.\nOur framework, however, treats teacher predictions as targets for auxiliary confidence prediction\ntasks. First, because all neurons in ML models are fixed, there exists an absolute mathematical\nformulate that transforms <inputs, output class> to <confident score>. Second, the true, human-\nannotated ground truth labels remain the primary learning objective. This distinction, enabled by\nthe unique teacher tokens, allows the model to learn from both the ground truth and the diverse\nperspectives of multiple teachers, improving its ability to predict with human-level confidence.\nImproved Label Efficiency: Our approach also offers significant gains in label efficiency. While\naggregation methods require multiple predictions per training sample, our method generates a multi-\nteacher training sample from each individual teacher's prediction, reducing computational overhead."}, {"title": "Constructing Multi-Teacher Training Samples", "content": "We demonstrate the construction of Teacher2Task training samples using Generative LLMs, multi-\nlabel classification models, and human annotators.\nGenerative LLMs: Large language models (LLMs), prompted with specific instructions, can serve\nas distinct teachers in our Multi-Teacher Learning framework. Open-ended prompts like \"What\nis the class of input?\" yield free-text predictions with associated confidence scores. Conversely,\nprompts like \"Is input semantically relevant to class name?\" pre-define the output class, focusing on\nthe \"Yes\" confidence score. Since each unique prompt effectively creates a new \"teacher\" from an\nLLM, we prefer to predefine a fixed prompt for each LLM, consistently generating Multi-Teacher\nLearning samples (see Fig. 3(a)) from each LLM inference.\nClassification Models: For each class, we use the input, teacher name, and the model's predicted\nprobability for that class. We can generate multiple Multi-Teacher Learning samples from a single\ninference of multi-class models (See Fig. 3(b)).\nHuman Annotation: Human annotators provide relevance scores for input-class pairs. Averaging\nmultiple annotations yields the final score."}, {"title": "Model Training", "content": "Our proposed multi-teacher learning supports various architectures (see Fig. 4), including: Encoder-Only - The teacher identity can be appended directly to the input, Dual-Encoder - Teacher informa-"}, {"title": "Experiments", "content": "We experiment on the open-vocabulary classification tasks for image and video understanding.\nOur goal is NOT to compare with public benchmarks, rather we'd like to demonstrate that our\nTeacher2Task algorithm effectively integrates knowledge from diverse sources to surpass that of\noriginal teachers."}, {"title": "Teachers", "content": "Our framework's flexibility allows for learning from diverse teacher types. To maximize knowledge\nacquisition from various label sources, we utilize four main teacher categories.\nLarge Language Models (LLMs): We employ PaLI [12][13] and Gemini [14] and follow Section\n2.3 to generate multi-teacher samples for image-topic pairs. We use slightly modified prompts for\nPaLI and Gemini:\nPaLI: Is \"topic\" the primary focus of this image?\nGemini: Answer strictly with YES/NO. Does this image provide visual evidence\nfor the topic \"topic\"?\nDomain-Specific Models: The domain-specific models excel within specific domains, providing\npotentially high-quality knowledge on a subset of the OpenVocab distribution. Compared to often\nexpensive LLMs, these models offer a cost-effective alternative to generate training samples and add\nknowledge on a smaller domain distribution.\nHumans: We utilize high-quality human annotations as a key source of ground truth for our model.\nSelf-Training Teachers: We leverage self-training [15] [16] as a teacher type within our framework.\nThis allows the model to iteratively learn from its own predictions on unlabeled data."}, {"title": "Experiment Setup", "content": "Model Architectures: We utilize the compact T5/mT5 [17][18] architecture for our encoders. Im-\nages are directly converted to embedding, while videos are transformed into sequences of frame-\nlevel embedding before processing.\nDatasets: Our human-annotated dataset consists of approximately 2M image/video samples, labeled\nby trained operators to indicate the presence of specific visual evidence for given topics (e.g. evening\nwith loved ones, ancient civilizations, sriracha loaded fries, ninja warrior party, ...). We generates\n200M PaLI-labeled, 20M Gemini-labeled, and 300M domain-specific ML-labeled samples.\nAdding Teacher's Identities: Teacher2Task leverages teacher identities as input features, enabling\nthe model to learn the distinct prediction patterns of each teacher for a given input-output pair. For\ntext-based models like T5/mT5 [17][18], we prepend the teacher's identifier to the input text (e.g.,\nPaLI: input text or Gemini: input text). For non-textual models, such as a ResNet [19] for image\nclassification, a one-hot vector representing the teacher's identity can be appended to the input.\nEvaluation: To assess open-vocabulary generalization, we perform a topic-split evaluation to ensure\nthat the majority of topics present in the evaluation set are unseen during training, offering a more\nchallenging and realistic evaluation compared to conventional random splits.\nTraining: We use a learning rate of $1e^{-3}$ with a batch size of 65k throughout our experiments.\nMetrics: We focus on precision and recall as key performance indicators, employing PR-AUC\n(Precision-Recall Area Under the Curve) as our primary evaluation metric."}, {"title": "Results", "content": "This section reports experiment results of a dual-encoder configuration with a 64-dimensional em-\nbedding space, a popular choice for large-scale tagging and high-traffic retrieval systems."}, {"title": "Image Classification", "content": "Our Teacher2Task model learns image knowledge from five primary teachers: Human annotations,\nPaLI [13], Gemini [14], and 2 domain-specific models. Utilizing the specified prompts, zero-shot\nPaLI achieves 79.1% PR-AUC, while Gemini reaches 82.2% PR-AUC. Gemini, the larger model,\noutperforms PaLI, particularly in higher precision regions.\nOur Multi-Teacher Learning model, only 150M parameters, surpasses even its best ML teacher\n(Gemini) by achieving 84% PR-AUC. This highlights the benefits of combining diverse knowledge\nsources, even when some are imperfect.\nAnalyzing the PR curve reveals insightful trends (See Fig.5). At higher precision levels, our model\noutperforms Gemini due to its access to human annotations. At lower precision levels, it leverages\nthe strengths of both PaLI and Gemini, achieving an outer bound on their respective PR curves. This\nempirical results well match the conceptual illustration analyzed in Section 2.2."}, {"title": "Video Classification", "content": "Our baseline for video open-vocabulary classification, trained solely on human-annotated video-\ntopic pairs, achieves a PR-AUC of 75.6%. We demonstrate the effectiveness of our Teacher2Task\nalgorithm by scaling the number of teachers in two scenarios, both yielding metric improvements\n(see Table 1).\nImage-Teacher Learning: Leveraging cross-domain knowledge transfer, we incorporate five\nimage-based teachers (humans, LLMs and domain-specific image models), treating images as"}, {"title": "Ablation Study", "content": "Next, we run ablation study of the algorithm with various embedding sizes, model architectures, and\nmodel sizes."}, {"title": "Embedding Sizes", "content": "The PR-AUC increases for both image and video benchmarks when increasing the embedding size,\nas expected because larger embedding sizes imply better representation capabilities (see Table 2)"}, {"title": "Model Architectures", "content": "When changing model architectures from dual-encoder to encoder-only, we see the encoder-only\nconfiguration slightly outperforms the dual-encoder configuration, explained by the encoder-only\ncan be viewed as dual-encoder with approaching infinite embedding size (see Table 4)."}, {"title": "Model Sizes", "content": "Variations in model size result in minimal changes in performance. We attribute the algorithm's\nstable performance across model sizes to its distillation-like approach, which enables smaller student\nmodels to achieve comparable results to larger teacher models (see Table 4)."}, {"title": "Discussion", "content": "This section positions our Teacher2Task algorithm within the broader landscape of Deep Learning\nmethodologies, highlighting its advantages and connections to existing techniques.\nDistillation: While distillation [11] methods typically learn from a single, often stronger, teacher,\nTeacher2Task aggregates knowledge from multiple sources, including human annotations and many\ndiverse models. Our approach offers a scalable path to integrating knowledge from an \"almost\ninfinite\" number of teachers.\nEnsemble Methods: Ensemble methods [2][3] often suffer from limitations such as reliance on\nmanual aggregation heuristics, suboptimal aggregation strategies, and low annotation efficiency.\nTeacher2Task addresses these challenges by directly learning a unified model from multiple teachers\nwithin a principled framework.\nSelf-Training: Self-training [15] [16], a semi-supervised technique, iteratively trains teacher-\nstudent models on labeled and pseudo-labeled data. However, it can be susceptible to confirmation\nbias if pseudo-labels are inaccurate [20]. Teacher2Task mitigates this risk by separating out the\nsource of annotations in the inputs, so that self-training can be an additional teacher in our frame-\nwork.\nPretraining: While Self-Supervised Learning [21] [22] has been widely adopted for pretraining\nwith massive unlabeled datasets, our heuristic-free multi-teacher learning offers a compelling al-\nternative. From readily available LLMs, domain-specific models, and running those on unlabeled\ndata, we can generate billions or even trillions of multi-teacher labeled samples for effective pre-\ntraining, maximizing knowledge transfer by enabling the pretrained model to inherit insights from\nall its teachers."}, {"title": "Comparison to Multi-Teacher algorithms", "content": "This section compares our proposed method to other multi-teacher approaches, dividing into three\nmajor categories: Weighted Aggregation, Teacher Selection, or Domain Separation approaches.\nWeighted Aggregation: While uniform weights for each teacher is the most common practice\n[4][5], research has explored more sophisticated weighting approaches, such as manually tuning\nweights [6] or learning instance-level teacher importance weights [7]. However, even advanced\nweighted averaging methods suffer from drawbacks: reduced annotation efficiency (requiring mul-\ntiple teacher labels per aggregated label), increased computational overhead for label aggregation,\nand potential imperfections in the heuristically aggregated labels.\nTeacher Selection: [4] randomly select a teacher for each mini-batch, while [9] employ reinforce-\nment learning for dynamic teacher selection. This can be considered a special case of weighted\naveraging, where one teacher's weight is set to 1 and the others to 0. However, this approach still\nsuffers from increased computational overhead, potential for suboptimal teacher selection, and the\ninherent limitation of treating teacher predictions as ground truth.\nDomain Separation: [10] employ multi-teachers for multilingual neural machine translation train-\ning, where each teacher is assigned to a distinct language pair. The problem is a special case, where\nall teacher domains are rigidly separated by language pairs, removing the need for teacher's aggre-\ngation or selection. In cases of potential annotation conflicts between teachers, the method might\nrequire further heuristics for domain selection.\nOur approach addresses the challenges of existing multi-teacher methods. By treating each teacher's\nprediction as a training sample, we maximize annotation efficiency by leveraging all teacher labels.\nFurthermore, incorporating teacher identities as input features and re-framing the task as predicting\nindividual teacher labeling styles, we remove the need for weight aggregation, teacher selection, or\ndomain separation. By not viewing teacher confidence scores as ground-truth labels, our algorithm\neliminates the problem of imperfect aggregation heuristics."}, {"title": "Conclusion", "content": "Teacher2Task offers a unified and scalable approach that leverages the strengths of multiple learning\nparadigms. By utilizing confidence scores from a potentially vast number of teachers, it extends the\nconcept of distillation while eliminating the need for explicit aggregation heuristics. The approach\nfacilitates the generation of massive training datasets from unlabeled data, proving particularly ef-\nfective for training compact, yet highly knowledgeable, student models that inherit the collective\nknowledge of all original teachers."}]}