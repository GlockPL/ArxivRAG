{"title": "Deep Ensembles Secretly Perform Empirical Bayes", "authors": ["Gabriel Loaiza-Ganem", "Valentin Villecroze", "Yixin Wang"], "abstract": "Quantifying uncertainty in neural networks is a highly relevant problem which is essential\nto many applications. The two predominant paradigms to tackle this task are Bayesian\nneural networks (BNNs) and deep ensembles. Despite some similarities between these two\napproaches, they are typically surmised to lack a formal connection and are thus understood\nas fundamentally different. BNNs are often touted as more principled due to their reliance on\nthe Bayesian paradigm, whereas ensembles are perceived as more ad-hoc; yet, deep ensembles\ntend to empirically outperform BNNs, with no satisfying explanation as to why this is the\ncase. In this work we bridge this gap by showing that deep ensembles perform exact Bayesian\naveraging with a posterior obtained with an implicitly learned data-dependent prior. In\nother words deep ensembles are Bayesian, or more specifically, they implement an empirical\nBayes procedure wherein the prior is learned from the data. This perspective offers two\nmain benefits: (i) it theoretically justifies deep ensembles and thus provides an explanation\nfor their strong empirical performance; and (ii) inspection of the learned prior reveals it is\ngiven by a mixture of point masses the use of such a strong prior helps elucidate observed\nphenomena about ensembles. Overall, our work delivers a newfound understanding of deep\nensembles which is not only of interest in it of itself, but which is also likely to generate\nfuture insights that drive empirical improvements for these models.", "sections": [{"title": "1 Introduction", "content": "Deep learning has proven extremely successful at prediction (Krizhevsky et al., 2012; Szegedy et al., 2015;\nHe et al., 2016), representation learning (Mikolov et al., 2013; Chen et al., 2020; Radford et al., 2021;\nCaron et al., 2021; Oquab et al., 2024), and generative modelling (Radford et al., 2018; Brown et al., 2020;\nRamesh et al., 2021; Rombach et al., 2022; Saharia et al., 2022; Yu et al., 2022; OpenAI, 2023), yet it still\nlags behind statistical methods (Kutner et al., 2005; Rasmussen & Williams, 2005; Gelman et al., 2013) at\nuncertainty quantification (UQ) \u2013 for example, off-the-shelf neural network classifiers can be poorly calibrated\ndespite being accurate (Guo et al., 2017). The two most popular approaches for UQ in deep learning are\nBayesian neural networks (BNNs; Welling & Teh, 2011; Graves, 2011; Hern\u00e1ndez-Lobato & Adams, 2015;\nBlundell et al., 2015; Gal & Ghahramani, 2016; Ritter et al., 2018) and ensembles (Lakshminarayanan et al.,\n2017). BNNs operate by specifying a prior distribution over network parameters and then following the rules\nof Bayesian inference to obtain a posterior distribution over these parameters. The corresponding posterior\npredictive distribution - i.e. the expected predictive density over this posterior - then captures all relevant\nuncertainty. Ensembles also average predictive densities, but they do so over models trained with differ-\nent seeds. BNNs are typically thought of as the more theoretically justified of the two approaches due to\ntheir adherence to Bayesian principles, with ensembles often being considered non-Bayesian due to the fact\nthat the corresponding average is (seemingly) not performed over a posterior distribution. Nonetheless,\ndespite being considered more ad-hoc and lacking formal justification, ensembles typically outperform BNNs\n(Abdar et al., 2021)."}, {"title": "What this paper is and what it is not", "content": "In this work we establish a simple yet precise and formal\nconnection between deep ensembles and empirical Bayes, showing that ensembles can be interpreted as a\nparticular way of carrying out Bayesian inference for neural networks. This result reveals that deep ensembles\nare a type of BNN and that these two paradigms are thus not fundamentally distinct. Although we hope\nthat this new understanding of the connection between BNNs and ensembles will lead to enhanced UQ in\nthe future, our goal here is only to establish this connection and to link it to existing work."}, {"title": "2 Background", "content": "Throughout this paper we will consider a dataset $D$ along with a probabilistic model $p(D | \\theta)$ describing how\nthe observed data is assumed to depend on some parameter $\\theta \\in \\Theta$ of interest. Here $\\Theta$ are the parameters\nof a neural network, and $\\Theta$ the set of possible parameter values. For example, in the classification setting\nwith i.i.d. data, $D = \\{(x_i, y_i)\\}_{i=1}^n$, where $x_i$ represents a feature vector which is treated as a covariate, $y_i$ is a\ncategorical label, and $p(D | \\theta) = \\prod_i P(y_i | \\theta, x_i)$, where $p(y_i | \\theta, x_i)$ is the probability assigned by the neural\nnetwork classifier to $y_i$ when given $x_i$ as input. This is the most commonly studied setting for UQ and we\nwill use it throughout our work. Nonetheless, we highlight that our main result applies to any setting with\na well-defined likelihood $p(D | \\theta)$. In the rest of this section we cover the preliminaries needed for our main\nresult: BNNs, empirical Bayes, variational inference, and deep ensembles."}, {"title": "2.1 Bayesian Neural Networks", "content": "Most learning algorithms produce a single parameter estimate $\\theta^*$, for example, maximum-likelihood achieves\nthis by maximizing $\\log p(D | \\theta)$ over $\\theta \\in \\Theta$. For a query point $x_{n+1}$, the resulting predictive distribution\n$p(\\cdot | \\theta^*, x_{n+1})$ can be used not only for prediction, but it also aims to encapsulate the aleatoric uncertainty\n(i.e. irreducible uncertainty) associated with predicting the label of $x_{n+1}$.\nIn contrast with most learning algorithms, Bayesian methods aim to obtain a distribution over $\\theta$ which can\nthen be leveraged for improved prediction and UQ. At a high level, BNNs operate by first specifying a prior\ndistribution $\\pi$ on $\\Theta$ and then following the rules of Bayesian inference to obtain the corresponding posterior\ndistribution $\\pi(\\cdot | D)$, given by\n$\\pi(\\theta | D) \\propto \\pi(\\theta) p(D | \\theta)$.\nThis posterior encapsulates epistemic uncertainty, i.e. uncertainty arising from a lack of perfect knowledge\nof the underlying true data-generating process. As such, variability in $p(\\cdot | \\theta, x_{n+1})$ over values of $\\theta$ sampled\nfrom the posterior can be used to quantify epistemic uncertainty. The posterior can also be combined with"}, {"title": "2.2 Empirical Bayes", "content": "Empirical Bayes is a class of procedures where the prior is learned from $D$, rather than fixed beforehand in\nan attempt to express prior uncertainty about $\\theta$. Empirical Bayes is a sensible approach towards setting the\nprior when fixing a prior through expert knowledge is challenging, as is the case in BNNs (Wenzel et al.,\n2020); it is also principled (Carlin & Louis, 2000) and can result in superefficient estimators (James & Stein,\n1961; Efron & Morris, 1971; 1972). When the prior is learnable, we will consider a set of potential priors\n$\\Pi \\subset \\Delta(\\Theta)$, where $\\Delta(\\Theta)$ denotes the set of distributions on $\\Theta$. In this work we will focus on a particular\nempirical Bayes procedure, namely maximum marginal likelihood, which maximizes $\\log p_{\\pi}(D)$ over $\\pi \\in \\Pi$,\nwhere\n$p_{\\pi}(D) = \\mathbb{E}_{\\theta \\sim \\pi}[p(D | \\theta)]$\nis called the marginal likelihood. We will denote the optimal prior as $\\pi^*$ and the corresponding posterior as\n$\\pi^*(\\cdot | D)$. Directly optimizing the marginal likelihood is intractable, and VI can also be used towards this\ngoal. We will show in Section 3 how deep ensembles implicitly use VI to perform empirical Bayes."}, {"title": "2.3 Variational Inference", "content": "As previously mentioned, exact Bayesian inference and direct maximum marginal likelihood are both in-\ntractable; VI allows for simultaneously performing approximate Bayesian inference and marginal likelihood\nmaximization. VI starts by introducing a family of distributions $Q \\subset \\Delta(\\Theta)$. The premise of VI is to simul-\ntaneously find $\\pi^* \\in \\Pi$ through maximum marginal likelihood, along with an element $q^* \\in Q$ such that $q^*$,\noften called the variational posterior, best approximates the corresponding true posterior, i.e. $q^* \\approx \\pi^*(\\cdot | D)$.\nFormally, this is achieved by introducing the evidence lower bound (ELBO),\n$\\text{ELBO}(q, \\pi) := \\mathbb{E}_{\\theta \\sim q} [\\log p(D | \\theta)] - \\text{KL} (q || \\pi) = \\log p_{\\pi}(D) - \\text{KL} (q || \\pi(\\cdot | D))$,\nand then obtaining $q^*$ and $\\pi^*$ by maximizing the ELBO over $(q, \\pi) \\in Q \\times \\Pi$. Note that the ELBO is\ndefined through the first equality in Equation 5, which can be tractably maximized when Q is adequately\nchosen. The second equality in Equation 5 shows why the ELBO provides a sensible objective, even though\nneither $\\log p_{\\pi}(D)$ nor $\\text{KL}(q || \\pi(\\cdot | D))$ can be individually evaluated nor straightforwardly approximated:\nmaximizing the ELBO indeed promotes the concurrent maximization of the marginal likelihood and the\nmatching between the variational and true posteriors."}, {"title": "2.4 Deep Ensembles", "content": "Deep ensembles are very simple: they train $M$ models through maximum-likelihood, resulting in parameter\nvalues $\\theta_1, ..., \\theta_M$, and average the models. All these models are trained through the same stochastic gradient-\nbased procedure, for example Adam (Kingma & Ba, 2015), except optimization trajectories differ across\nmodels due to randomization. The predictive distribution implied by this procedure, which we denote as\n$p_{\\text{ens}}( \\cdot | x_{n+1})$, can then be written as\n$p_{\\text{ens}}( \\cdot | x_{n+1}):= \\frac{1}{M} \\sum_{m=1}^M p(\\cdot | \\theta_m, x_{n+1}) = \\mathbb{E}_{\\theta \\sim \\pi_{\\text{ens}}} [p(\\cdot | \\theta, x_{n+1})], \\text{ where } \\pi_{\\text{ens}}(\\theta) := \\frac{1}{M} \\sum_{m=1}^M \\delta_{\\theta_m}(\\theta)$,\nwith $\\delta_{\\theta_m}$ representing a point mass at $\\theta_m$.\nThe distribution $\\pi_{\\text{ens}}$ plays an analogous role to a Bayesian posterior in several ways: it also aims to encap-\nsulate epistemic uncertainty, and Equation 6 is highly reminiscent of the posterior predictive in Equation 2\nsince both compute an expectation of $p(\\cdot | \\theta, x_{n+1})$ over $\\theta$. However, although deep ensembles produce ac-\ncurate and well-calibrated models, they are presumed to be less principled than BNNS since $\\pi_{\\text{ens}}$ is obtained\nin a seemingly ad-hoc way, whereas the posterior is derived through the principles of Bayesian inference.\nWe also highlight that Equation 3 and Equation 6 result in very similar computations as they both average\n$p(\\cdot | \\theta, x_{n+1})$ over $M$ values of $\\theta$, but the former does so to approximate an intractable expectation while\nthe latter calculates its respective expectation exactly.\nIn summary, in spite of some similarities, BNNs and deep ensembles are surmised to be fundamentally\ndifferent. We will shortly challenge this view by showing how BNNs are linked to deep ensembles via\nempirical Bayes and VI."}, {"title": "3 How Deep Ensembles Perform Exact Bayesian Averaging via Empirical Bayes", "content": "In this section we will present our main result: that deep ensembles can be derived as implicitly performing\nmaximum marginal likelihood through VI with an arbitrarily flexible prior and flexible enough variational\nposterior. We begin by stating a simple observation which we will use during our discussion.\nObservation 1. Let $\\Theta_c$ denote the $c$-level set of the likelihood for $c > 0$, i.e. $\\Theta_c := \\{\\theta \\in \\Theta : p(D | \\theta) = c\\}$.\nAssume $\\Theta_c$ is non-empty and let $\\pi_c \\in \\Delta(\\Theta)$ be a probability distribution placing all its mass on $\\Theta_c$, i.e.\n$P_{\\theta \\sim \\pi_c} (\\theta \\in \\Theta_c) = 1$. Then, $\\pi_c = \\pi_c(\\cdot | D)$.\nProof. Recall that the support of a posterior distribution must be contained in the support of its cor-\nresponding prior, and thus the proportionality in Equation 1 holds over the support of the prior, i.e.\n$\\pi_c(\\theta | D) \\propto \\pi_c(\\theta)p(D|\\theta)$ is valid over the support of $\\pi_c$. Since by assumption the support of $\\pi_c$ is con-\ntained in $\\Theta_c$, it follows that\n$\\pi_c(\\theta | D) \\propto \\pi_c(\\theta)p(D | \\theta) = \\pi_c(\\theta)c \\propto \\pi_c(\\theta)$.\nSince two distributions can be proportional to each other only if they are equal, the equation above implies\nthat $\\pi_c = \\pi_c(\\cdot | D)$.\nProposition 1. Let $(q^*, \\pi^*) \\in Q \\times \\Pi$. Then, under Assumption 1, Assumption 2, and Assumption 3, $(q^*, \\pi^*)$\nmaximizes the ELBO in Equation 5, i.e. $\\text{ELBO}(q,\\pi) < \\text{ELBO}(q^*,\\pi^*)$ for every $(q, \\pi) \\in Q \\times \\Pi$, if and only\nif the following two properties hold:\n(A) $q^*$ places all of its mass on $\\Theta^*$, i.e. $P_{\\theta \\sim q^*} (\\theta \\in \\Theta^*) = 1$.\n(B) The prior and variational posterior match, i.e. $\\pi^* = q^*$.\nProof. We begin by proving that if (A) and (B) hold, then $(q^*, \\pi^*)$ maximizes the ELBO. Let $(q, \\pi) \\in Q \\times \\Pi\nand assume (A) and (B) hold. We have:\n$\\text{ELBO}(q, \\pi) = \\mathbb{E}_{\\theta \\sim q} [\\log p(D | \\theta)] - \\text{KL} (q || \\pi) \\leq \\mathbb{E}_{\\theta \\sim q} [\\log p(D | \\theta)] \\leq \\mathbb{E}_{\\theta \\sim q^*} [\\log p(D | \\theta)]$\n$= \\mathbb{E}_{\\theta \\sim q^*} [\\log p(D | \\theta)] - \\text{KL} (q^* || \\pi^*) = \\text{ELBO}(q^*, \\pi^*)$,\nwhere the last inequality in Equation 8 follows from (A) and the first equality in Equation 9 follows from\n(B). Thus, $(q^*, \\pi^*)$ maximizes the ELBO. The equations above also show that the maximal value achieved\nby the ELBO is $\\mathbb{E}_{\\theta \\sim q^*} [\\log p(D | \\theta)]$, which by Assumption 1 is neither -$\\infty$ nor $\\infty$.\nWe now assume that $(q^*, \\pi^*)$ maximizes the ELBO and will prove that (A) and (B) hold. We proceed by\ncontradiction and assume that (B) does not hold, meaning that $\\text{KL}(q^* || \\pi^*) > 0$. Since the maximal value\nof the ELBO is finite it follows that -$\\infty < \\mathbb{E}_{\\theta \\sim q^*} [\\log p(D | \\theta)] < \\infty$, and we then have that\n$\\text{ELBO}(q^*, \\pi^*) = \\mathbb{E}_{\\theta \\sim q^*} [\\log p(D | \\theta)] - \\text{KL} (q^* || \\pi^*) < \\mathbb{E}_{\\theta \\sim q^*} [\\log p(D | \\theta)] - \\text{KL} (q^* || q^*)$\n$= \\text{ELBO}(q^*, q^*)$.\nBy Assumption 2 $q^* \\in \\Pi$, so the above equations show that $(q^*, \\pi^*)$ does not maximize the ELBO. This is\na contradiction and thus (B) must hold. It remains to prove that (A) also holds, which we now do.\nLet $\\hat{\\pi} \\in \\Delta(\\Theta)$ be a probability distribution placing all of its mass on $\\Theta^* \\subset \\Theta$ and let $\\hat{\\pi} \\in \\Pi$, so that\nby Assumption 1, $p_{\\pi}(D) = \\mathbb{E}_{\\theta \\sim \\pi} [p(D | \\theta)] \\leq \\mathbb{E}_{\\theta \\sim \\hat{\\pi}} [p(D | \\theta)] = p_{\\hat{\\pi}}(D)$ with $0 < p_{\\hat{\\pi}}(D) < \\infty$. It follows\nthat $\\hat{\\pi}$ maximizes the marginal likelihood, and by Assumption 3, $\\hat{\\pi}(\\cdot | D) \\in Q$. We now also proceed by\ncontradiction and assume (A) does not hold, so that $p_{\\pi^*}(D) < p_{\\hat{\\pi}}(D)$. We then have two cases:"}, {"title": "4 Relationship to Existing Work", "content": "Deep ensembles Despite approaching a decade of existence, deep ensembles (Lakshminarayanan et al.,\n2017) remain a gold standard for UQ with neural networks, with most follow-up work focusing on improving\ncomputational efficiency rather than performance (Huang et al., 2017; Garipov et al., 2018; Wen et al., 2020;\nHavasi et al., 2021). As previously mentioned, ensembles are often explicitly described as non-Bayesian; our\nwork disproves this conventional understanding and provides intuitions that will hopefully lead to future\nperformance improvements. Wu & Williamson (2024) proposed one of the rare methods whose goal is to\noutperform deep ensembles at UQ. Interestingly, despite not connecting ensembles and empirical Bayes,\ntheir motivation is precisely that $\\pi_{ens}$ being a mixture of point masses is overly strong. We thus see the\ngood empirical results of Wu & Williamson (2024) as evidence supporting the intuitions developed from\nour newfound understanding of deep ensembles. We also highlight the works of Abe et al. (2022) and"}, {"title": "5 Conclusions", "content": "In this paper we introduced a simple and fresh perspective for reasoning about deep ensembles and their\nrelationship to BNNs. Our work serves as a position paper about understanding ensembles, and in contrast\nwith previous work which has viewed ensembles as either non-Bayesian or merely approximating posterior\naveraging, we showed that through the lens of empirical Bayes, ensembles are the only BNN which performs\nexact posterior averaging. Our developed understanding of ensembles is consistent with and expands upon\nexisting literature, and it is our hope that it will be useful towards improving ensembles in the future."}]}