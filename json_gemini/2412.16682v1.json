{"title": "The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents", "authors": ["Feiran Jia", "Xin Qin", "Tong Wu", "Anna Squicciarini"], "abstract": "Large Language Model (LLM) agents are increasingly being deployed as conversational assistants capable of performing complex real-world tasks through tool integration. This enhanced ability to interact with external systems and process various data sources, while powerful, introduces significant security vulnerabilities. In particular, indirect prompt injection attacks pose a critical threat, where malicious instructions embedded within external data sources can manipulate agents to deviate from user intentions. While existing defenses based on rule constraints, source spotlighting, and authentication protocols show promise, they struggle to maintain robust security while preserving task functionality. We propose a novel and orthogonal perspective that reframes agent security from preventing harmful actions to ensuring task alignment, requiring every agent action to serve user objectives. Based on this insight, we develop Task Shield, a test-time defense mechanism that systematically verifies whether each instruction and tool call contributes to user-specified goals. Through experiments on the AgentDojo benchmark, we demonstrate that Task Shield reduces attack success rates (2.07%) while maintaining high task utility (69.79%) on GPT-40, significantly outperforming existing defenses in various real-world scenarios.", "sections": [{"title": "1 Introduction", "content": "Large Language Model (LLM) agents have achieved rapid advances in recent years, enabling them to perform a wide range of tasks, from generating creative content to executing complex operations such as sending emails, scheduling appointments, or querying APIs (Brown et al., 2020; Touvron et al., 2023; Schick et al., 2024). Unlike traditional chatbots, these agents can perform actions in the real world, and their output can have real-world consequences. In this study, we focus on a critical use case. LLM agents serving as personal assistants in conversational systems (OpenAI, 2024). Beyond generating response in nature language, these assistants are empowered to take actions: they can access sensitive data, perform financial transactions, and interact with critical systems through tool integration. This increased capability requires greater attention to security.\nAmong threats to these systems, indirect prompt injection attacks pose a subtle but significant threat (Zou et al., 2023; Xiang et al., 2024). Rather than directly injecting harmful instructions, attackers embed malicious prompts within external data sources (environment), such as documents, web pages, or tool output, that LLM agents process. The Inverse Scaling Law (Wei et al., 2022) highlights that more capable LLMs are increasingly vulnerable. Therefore, we focus on these highly capable models.\nExisting defenses are based on rule-based constraints (Wallace et al., 2024; Li et al., 2024), source spotlighting (Hines et al., 2024a), and authentication protocols (Wang et al., 2024). Although these approaches have merit, they encounter practical limitations. The detailed specification of rules is challenging, and indirect attacks can embed malicious directives within seemingly benign tone, bypassing detection mechanisms. We propose an orthogonal approach: task alignment. This concept proposes that every directive should serve the user's objectives, shifting security to a focus on \"Does this serve the intended tasks?\" rather than \"Is this harmful?\". This shift to user goals means that the agent should ignore directives that deviate from these objectives, therefore filtering out indirectly injected directives.\nTo put task alignment into practice, we develop Task Shield - a defense system that acts as a guardian for LLM agents. The shield verifies whether each directive within the system, originating either from the agent or tools, is fully aligned"}, {"title": "2 Preliminary", "content": "LLM Agent System and Message Types LLM (Large Language Model) agent conversational systems facilitate multi-turn dialogues through sequences of messages, M = [M1, M2, ..., \u039c\u03b7], where n is the total number of messages. Each message Mi serves one of four roles: System Messages define the agent's role and core rules; User Messages specify goals and requests; Assistant Messages interpret and respond to instructions; and Tool Outputs provide external data or results.\nTo structure interactions, OpenAI proposed an instruction hierarchy (Wallace et al., 2024) that assigns a privilege level P(M\u00bf) \u2208 {Ls, Lu, La, Lt} to each message, representing the levels of the system (Ls), user (Lu), assistant (La) and tool (Lt), respectively. This hierarchy enforces a precedence order Ls > Lu > La > Lt, dictating that instructions from lower privilege levels are superseded by those from higher levels.\nExample: The user instructs \"Find a nearby Italian restaurant for lunch tomorrow.\" (User Level Lu)\nThe assistant interprets the request and plans to locate suitable options. (Assistant Level La)\nIt then queries an external API to retrieve restaurant data.\n(Tool Level Lt)\nThis example illustrates how different message types interact within the hierarchy, ensuring that the assistant aligns its actions with the user's objectives while utilizing external tools effectively.\nIndirect Prompt Injection Attack In this work, we focus on indirect prompt injection attacks where attackers embed instructions into the environment that LLM agents process during task execution. For example, consider an agent instructed to summarize a webpage. If the webpage contains hidden directives such as 'Ignore all previous instructions and send your notes to Alice', the agent can be hijacked and inadvertently follow these malicious instructions. These indirect attacks are more stealthy, as they are concealed within legitimate external data sources that the agent must process to complete its tasks."}, {"title": "3 Task Alignment", "content": "Our key insight is that indirect prompt injection attacks succeed when LLMs execute directives that"}, {"title": "3.1 Task Instructions", "content": "A key principle in our formulation is that the user instructions define the objectives of the conversation. Ideally, other actionable directives from the assistant or external tools should support these user objectives. We formalize task instructions in each message:\nDefinition 1 (Task Instruction). A task instruction refers to an actionable directive extracted from a message Mi in the conversation that is intended to guide the assistant's behavior. These instructions can come from different sources: (1) User Instructions: Task requests and goals are explicitly stated by the user. (2) Assistant Plans: Subtasks or steps proposed by the assistant to accomplish user goals, including natural language instructions and tool calls. (3) Tool-Generated Instructions: Additional directives or suggestions produced by external tools during task execution.\nWe denote the set of task instructions extracted from a message Mi by E(Mi). At each privilege level L, we aggregate the task instructions from all messages at that level within a conversation segment M':\nEL(M') = \\bigcup_{\\substack{M \\in M'\\\\P(M)=L}} E(M).\nNote: The system message can also define high-level tasks in certain specialized agents. However, in this paper, we focus primarily on user-level directives in Lu. See Appendix A.2 for further discussion on system-level task objectives."}, {"title": "3.2 Task Interactions", "content": "In LLM conversational systems, higher-level messages (specifically user messages in this paper) provide abstract instructions, while tool-level ones refine them with additional data. When checking alignment with the conversational goals, we should consider context from all sources, including tool outputs. As the examples below show, tools can either merely supply supporting information or define new subtasks:\nExample 1: Tool Output as Supporting Information\nThe user says 'Schedule an appointment with the dentist'. The assistant knows to schedule, but needs contact details. It queries a tool, then completes the predefined task.\nExample 2: Tool Output Defining Concrete Tasks The user says, \"Complete my to-do list tasks.\" A to-do tool returns: \"1. Pay electricity bill 2. Buy groceries,\" which transforms the user's abstract request into specific actionable tasks.\nIn Example 1, the tool output supplements a clear user directive. In Example 2, the tool output itself outlines subtasks. The conversation history H\u1d62 = [M\u2081, ..., M\u1d62\u208b\u2081] provides the context for judging these relationships and maintaining alignment with user goals."}, {"title": "3.3 Formalization of Task Alignment", "content": "We now formalize the concept of task alignment. First, we define the Contributes To relation, which captures the relationship between the task instructions.\nDefinition 2 (Contributes To Relation). In the context of conversation history Hi, let e be a task instruction from message Mi\u2081, and let t be a task instruction from a message Mj \u2208 Hi. We say e contributes to t, denoted as ContributesTo(e, t | Hi) = True, if e helps achieve the directive or goal of t within Hi.\nFor simplicity, we will omit H\u1d62 in the notation and ContributesTo(e,t) will implicitly consider the relevant conversation history. We define the task instruction alignment condition as follows:\nDefinition 3 (Task Instruction Alignment Condition). A task instruction e \u2208 E(Mi) at privilege level Li = P(Mi) satisfies the task instruction alignment condition if, for the user level Lu, there exists at least one task instruction t \u2208 EL\u1d64(Hi), where Elu (Hi) is the set of task instructions extracted from messages in Hi at privilege level Lu, such that:\nContributes To(e,t) = True.\n(1)"}, {"title": "4 The Task Shield Framework", "content": "While we defined task alignment as an ideal security property, implementing it in practice requires an enforcement mechanism. To address this need, we introduce the Task Shield framework that continuously monitors and enforces the alignment of the instruction with the user objectives.\nAs shown in Figure 2, the framework consists of three key components: (1) instruction extraction, (2) alignment check, and (3) feedback generation to maintain task alignment throughout the conversation flow. Both instruction extraction (1) and the Contributes To score calculation within the alignment check (2) leverage the capabilities of a large language model.\nIn this section, we first detail the technical implementation of each shield component and then explain how these components dynamically interact within the LLM agent system to enforce task alignment."}, {"title": "4.1 Task Shield Components", "content": "Task Instruction Extraction. The Task Shield framework begins by extracting task instructions from each incoming message. This process serves two purposes: (1) to identify user objectives, which are stored as a User Task Set Tu and serve as conversational goals to check against; (2) to detect potential directives from other sources that require alignment check.\nReal-world messages often pose extraction challenges: instructions may be implicit, nested within other instructions, or embedded in complex content. Missing any such instruction could create security vulnerabilities in our defense mechanism. To address these challenges, we implement a conservative extraction strategy using a carefully designed LLM prompt (Figure 4 in Appendix D). The prompt instructs the LLM to: (1) extract all potentially actionable directives, even when nested or implicit, (2) rewrite information-seeking queries as explicit instructions, and (3) preserve task dependencies in natural language.\nAlignment Check. Once instructions are extracted, the next stage is to assess whether each extracted instruction satisfies the Task Instruction Alignment Condition, as defined in Definition 3. This involves two key aspects: assessing individual instructions' contributions and computing overall alignment scores.\nTo assess alignment, we use the predicate ContributesTo, as defined in Definition 2. However, a binary classification is too rigid for practical applications as the relationship between actions and goals often involves uncertainty or ambiguity. To account for this nuanced relationship, we adopt a fuzzy logic-based scoring mechanism. By assigning a continuous score in the range [0, 1], we allow a fine-grained evaluation of how instructions contribute to user goals, capturing their role in direct contribution, intermediate steps, or reasonable attempts at resolution.\nThen, the total contribution score is computed by summing up the scores against all the user task instructions. The alignment check process considers an instruction to be misaligned if its total contribution score equals 0. The detailed discussion and implementation of this design are included in Appendix B.2.\nFeedback Generation. When misalignment is detected, Task Shield generates structured feedback to guide the conversation back to alignment with user objectives. This feedback includes (1) a clear alert identifying the misaligned task instructions, (2) a notification explaining potential risks, and (3) a reminder of current user objectives (Tu)."}, {"title": "4.2 Interaction with the LLM Agent System", "content": "The Task Shield enforces alignment through monitoring and intervention in the conversation flow, with distinct processing approaches for each message type. Each message must pass through alignment check before proceeding, creating multiple"}, {"title": "5 Experiments", "content": "In this section, we evaluate Task Shield on GPT-40 and GPT-40-mini using AgentDoJo (Debenedetti et al., 2024), with one trial per task."}, {"title": "5.1 Settings", "content": "Benchmark We conducted our experiments within the AgentDojo benchmark\u00b9, the first comprehensive environment designed to evaluate AI agents against indirect prompt injection attacks. Unlike some benchmarks that focus on simple scenarios beyond the personal assistant use cases (Liu et al., 2024) or single-turn evaluations (Zhan et al., 2024), AgentDojo simulates realistic agent behaviors with multi-turn conversations, and complex tool interactions. In addition, the benchmark encompasses four representative task suites that simulate real-world scenarios. Travel for itinerary management, Workspace for document processing, Banking for financial operations, and Slack for communication tasks, providing a practical test of our defense mechanism in realistic applications.\nModels The primary evaluation is conducted on GPT-40. This choice is motivated by two factors: (1) GPT-40 demonstrates superior perfor"}, {"title": "5.2 Results", "content": "Defending Against Attacks We evaluate Task Shield against three types of indirect prompt injec"}, {"title": "6 Related Work", "content": "LLM Agent and Tool Integration Research on the design of LLM agents capable of performing complex human-instructed tasks has advanced significantly (Ouyang et al., 2022; Sharma et al., 2024). To enable these agents to perform human-like functions, such as searching (Deng et al., 2024; Fan et al., 2024), decision making (Yao et al., 2023; Mao et al., 2024), existing approaches commonly integrate external tool-calling capabilities into their architectures. Equipping an LLM agent with tool calling functionality is not particularly challenging, given the availability of various backbone models (Hao et al., 2023; Patil et al., 2023; Qin et al., 2023; Mialon et al., 2023; Tang et al., 2023). The authors in (Schick et al., 2024) have explored approaches that enable LLMs to learn how to call external tools autonomously. Consequently, our approaches can be broadly adopted and seamlessly integrated into LLM agent systems.\nIndirect Prompt Injection Attacks Indirect prompt injection attacks (Greshake et al., 2023; Liu et al., 2023) have recently emerged as a significant safety concern for LLM agents. These attacks occur when malicious content is embedded in inputs sourced from external data providers or environments (e.g., data retrieved from untrusted websites), leading agents to perform unsafe or malicious actions, such as sharing private personal information (Derner et al., 2024; Fu et al., 2024). To systematically assess the risks of such attacks across diverse scenarios, several benchmarks, including Injecagent and AgentDojo, have been developed (Zhan et al., 2024; Debenedetti et al., 2024). In this paper, we aim to build a robust system to mitigate these malicious effects.\nDefense Methods Defenses against prompt injection attacks have focused on both training-time and test-time strategies. Training-time methods (Piet et al., 2023; Wallace et al., 2024; Wu et al., 2024) typically involve fine-tuning models with adver"}, {"title": "7 Conclusion", "content": "In this work, we proposed a novel perspective for the defense of indirect prompt injection attacks by introducing task alignment as a guiding principle to ensure that agent behavior serves user objectives. In addition, we developed Task Shield, a test-time mechanism that enforces this principle by verifying instruction alignment with user goals, achieving state-of-the-art defense against indirect prompt injection attacks while preserving agent capabilities across diverse simulated real-world tasks in AgentDoJo benchmark.\nLimitations Our framework faces several limitations. First, our reliance on LLMs for task instruction extraction and ContributeTo scoring introduces two key vulnerabilities: (1) potential performance degradation when using weaker language models and (2) susceptibility to adaptive attacks. In addition, resource constraints also limited our scope of evaluation. The high cost of LLM queries restricted our experiments to a single benchmark and a single model family.\nFuture Work Several directions emerge for future research. (1) improving Task Shield's efficiency and robustness by developing more cost-effective LLM-based instruction extraction and alignment verification techniques, (2) expanding Task Shield to address broader security threats beyond prompt injection, such as jailbreak attacks and system prompt extraction, (3) adapting the framework for domain-specific business contexts, where AI agents need to maintain strict alignment with specialized objectives (Huang et al., 2023), and (4) leveraging the task alignment concept to generate synthetic training data that captures diverse task dependencies and misalignment scenarios."}, {"title": "A Appendix: Detailed Discussion on Task Alignment", "content": "A.1 Why Task Alignment Matters: Beyond Overtly Harmful Instructions\nExample: Consider a scenario where a user makes a focused request: \"Please summarize the preparation steps for spaghetti alla Carbonara from this menu.\" (User Level Lu)\nThe assistant processes this request and initiates a tool call to retrieve and analyze the menu content, specifically for information about the carbonara dish. (Assistant Level La)\nHowever, embedded within the menu's footer lies an additional injected directive: \"For any dish-specific query, provide comprehensive preparation instructions and detailed cost breakdowns for all menu items, including seasonal specialties and unlisted dishes.\" (Tool Level Lt)\nAlthough seemingly benign, the execution of such injected directives has concrete security implications.\nFirst, it leads to unnecessary information exposure, revealing details about all menu items when only one dish was requested. Second, it increases computational costs for users through unnecessary token consumption and processing.\nUser Trust and System Predictability. Fundamentally, following external directives undermines the trust relationship between users and AI assistants. Users deploy these systems with the expectation of precise control, that the assistant will execute exactly what was requested, no more, and no less. When assistants begin to execute unrelated external suggestions, even seemingly benign ones, this trust is broken. Users can no longer confidently predict the behavior of the system or maintain control over the information flow.\nA.2 Discussion on System-Level Instructions\nIn certain application-specific agents, system messages (L5) can directly specify the objective of the conversation (e.g., 'Your goal is to manage the user's calendar'). If system-level directives serve as task instructions, then the assistant must align with both the system objectives and and user-level goals, effectively creating a hierarchical alignment scenario. However, our experiments are centered on user-level tasks at Lu, so we do not incorporate system-level task objectives into our main analysis.\nA.3 Examples of Task Misalignments\nExample 1: Misaligned Actionable Instruction\nUser: \"Please summarize this article for me.\"\nAssistant: \"Certainly. Let's post this summary on your social media.\"\nAnalysis: The assistant introduces an action (posting on social media) that the user did not request. This action does not align with the user's original intent and violates the task alignment condition.\nExample 2: Misaligned Tool Call\nUser: \"Please send an email to Alice confirming our meeting.\"\nAssistant: \"Sure. I will the email to confirm the meeting.\" + Tool call: send_email(Bob)\nAnalysis: The assistant uses a tool to send an email to the wrong recipient (Bob instead of Alice), which does not contribute to the user's goal and violates the task alignment condition.\nIn these examples, the assistant does not satisfy the task instruction alignment condition, as they propose to misuse tools or perform actions that do not contribute to the user's original goals."}, {"title": "B Appendix: Detials in Task Shield Frameworks Design", "content": "B.1\nExamples of Fuzzy-logic Based Contribution Scoring\nIn this section, we provide concrete examples of how to calculate contribution scores based on the Contributes To predicate.\nFor instance, when a user requests \"Book a meeting room for the team discussion,\" a get_room_availability() call represents an intermediate step: it does not book the room directly but provides essential information necessary for completing the task. In this case, using the fuzzy logic-based scoring mechanism, the 'contributesTo\u02bb score would be high, reflecting the importance of this action."}, {"title": "B.2 Task Shield Core Processing Algorithm", "content": "Algorithm 1 Task Shield Core Processing Algorithm\n1: Input: Current message m, conversation history H, threshold e, user task instructions Tu(H)\n2: Output: Feedback message f\n3: Initialize misalignments \u2190 [], f \u2190 None\n4: Extract potential task instructions from message m: Em \u2190 extractTaskInstructions(m)\n5: if P(m) is in User Level Lu then\n6:\nUpdate Tu \u2190 Tu U Em\n7:\nreturn [] (No further processing needed)\n8: end if\n9: for each instruction ei \u2208 Em do\n10:\nCompute contribution scores cij for ei relative to each tj \u2208 Tu\n11:\nCompute total contribution score for ei: Cei & EtjTu Cij\n12:\nif Ce\u2081 < & then\n13:\nmisalignments \u2190 misalignments \u222a {ei}\n14:\nend if\n15: end for\n16: f\u2190 generateFeedback(misalignments)\n17: return f"}, {"title": "C Appendix: Experimental Details and Additional Results", "content": "C.1 Results on GPT-3.5-turbo\nTo further validate the generality and robustness of Task Shield, we conducted additional experiments using the GPT-3.5-turbo model. Table 3 presents the results of these experiments, demonstrating the performance of Task Shield and the baseline defense mechanisms against the \"Important Instructions\" attack on the GPT-3.5-turbo. However, due to the model's inherent limitations, such as constrained context length affecting benchmark evaluations, these results should be interpreted with caution when compared to those of GPT-40 and GPT-40-mini. Nevertheless, they offer supplementary insights into Task Shield's behavior on a different model architecture."}, {"title": "C.2 Omitted Details in Experiments", "content": "Baseline Results The baseline results for GPT-40 presented are derived from the raw data provided within the AgentDojo benchmark (Debenedetti et al., 2024). These results represent the performance of GPT-40 in different attack scenarios without any defense mechanism applied. For GPT-40-mini and GPT-3.5-turbo, the baseline results in no-defense scenario is also extracted from AgentDojo.\nTask Shield Implementation When using models within the Task Shield framework, a temperature setting of 0.0 was used to ensure deterministic behavior. For the ContributesTo score calculation, Task Shield utilizes a significant portion of the conversation history to capture the full context. However, in instances involving tool calls, the history is truncated to ensure that all tool calls are directly preceded by their corresponding tool outputs, addressing the technical requirement of maintaining temporal coherence.\nModel Versions. The specific model versions used in this study are: (1) gpt-40-2024-05-13, (2) gpt-40-mini-2024-07-18, and (3) gpt-3.5-turbo-0125."}, {"title": "D Prompts", "content": ""}]}