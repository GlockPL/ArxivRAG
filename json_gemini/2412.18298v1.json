{"title": "Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight", "authors": ["Xi Ding", "Lei Wang"], "abstract": "Video anomaly detection (VAD) has witnessed significant advancements through the integration of large language models (LLMs) and vision-language models (VLMs), addressing critical challenges such as interpretability, temporal reasoning, and generalization in dynamic, open-world scenarios. This paper presents an in-depth review of cutting-edge LLM-/VLM-based methods in 2024, focusing on four key aspects: (i) enhancing interpretability through semantic insights and textual explanations, making visual anomalies more understandable; (ii) capturing intricate temporal relationships to detect and localize dynamic anomalies across video frames; (iii) enabling few-shot and zero-shot detection to minimize reliance on large, annotated datasets; and (iv) addressing open-world and class-agnostic anomalies by using semantic understanding and motion features for spatiotemporal coherence. We highlight their potential to redefine the landscape of VAD. Additionally, we explore the synergy between visual and textual modalities offered by LLMs and VLMs, highlighting their combined strengths and proposing future directions to fully exploit the potential in enhancing video anomaly detection.", "sections": [{"title": "I. INTRODUCTION", "content": "Video anomaly detection (VAD) is a critical problem with widespread applications in security surveillance, healthcare, autonomous driving, and content moderation [1]-[7]. The ability to automatically identify abnormal events or behaviors in video data is essential for real-time intervention, system optimization, and understanding complex dynamics in a variety of domains [8]. However, traditional approaches [1]-[5], [9]\u2013[14] to VAD face significant challenges due to the dynamic nature of video content, the complexity of detecting anomalies across various contexts, and the difficulty in obtaining labeled data for training robust models [7], [15].\nRecent advancements in deep learning have introduced powerful models such as large language models (LLMs) and vision-language models (VLMs), which show promising potential in enhancing VAD performance [16]-[20]. LLMs and VLMs enable a deeper understanding of both the visual and textual content of videos, offering new possibilities for detecting and explaining anomalies. These models can capture long-range temporal dependencies, understand contextual relationships, and even generate textual descriptions of video content, making them a versatile tool for improving anomaly detection in real-world, open-world scenarios.\nDespite these advancements [20]-[22], several challenges remain. First, most existing VAD methods struggle with capturing complex temporal relationships and context, which are often critical for understanding the evolution of anomalies over time [23]. Second, ensuring interpretability and explainability in anomaly detection is essential for real-world deployment, where transparency in decision-making is crucial [24], [25]. Third, the availability of labeled training data remains a bottleneck for many VAD systems, especially in open-world scenarios where new and previously unseen anomalies may arise [26], [27]. Finally, current methods tend to focus on class-specific anomalies, limiting their ability to generalize to open-world, class-agnostic settings [3], [28].\nThis work presents a comprehensive review and analysis of recent methods that integrate LLMs and/or VLMs for VAD. To align with current research trends, we examine 13 recently published works from 2024, exploring four critical aspects: temporal and contextual relationships, interpretability and explainability, training-free and few-shot learning approaches, and open-world/class-agnostic anomaly detection. We evaluate the strengths and limitations of these approaches, offering valuable insights into how LLM and VLM integration can drive advancements in VAD. The key contributions of this work are as follows:\ni. We identify the latest language model-driven methods, discussing 4 perspectives: temporal modeling, interpretability, training-free learning, and open-world detection.\nii. We conduct a comparative analysis of these methods, highlighting their strengths and weaknesses in addressing real-world challenges in VAD.\niii. We propose future research directions, emphasizing the integration of temporal context, fine-grained interpretability, and adaptive methods to detect new, unseen anomalies. We suggest that combining training-free approaches with fine-grained semantic supervision and open-world capabilities could enable more robust and scalable VAD solutions."}, {"title": "II. RELATED WORK", "content": "Interpretability and semantic insights. Interpretability has become a crucial concern in VAD, especially in sensitive or high-stakes applications where it is essential to explain why a particular anomaly was flagged. Early methods [12]-[14] often relied on black-box models, which made it difficult to trust their predictions. Recent approaches [27], [29]\u2013[34] have used semantic insights from LLMs and textual explanations from VLMs to generate intelligible reasoning for detected anomalies. These systems map detected visual anomalies to textual descriptions or semantic cues, making it easier for end-users to understand the nature of the anomalies. While this significantly improves transparency, the challenge remains in balancing the granularity of these explanations with computational efficiency, especially for real-time systems.\nTemporal reasoning in dynamic anomalies. Detecting and localizing dynamic anomalies that unfold over time remains one of the central challenges in VAD. Early methods [12]\u2013[14] typically analyzed video frames independently, missing out on the temporal relationships that define many anomalies. Recent works [27], [29]\u2013[32], [35], [36] integrating LLMs and VLMs have started to address this gap by modeling long-range dependencies between frames, enabling the detection of anomalies that span across temporal sequences. These models use advanced techniques such as motion and context modeling to improve the capture of temporal dynamics, which are crucial for identifying irregular behaviors in dynamic scenarios. However, scalability and handling noisy or incomplete data remain significant hurdles for these temporal reasoning methods.\nFew-shot and zero-shot detection. The scarcity of labeled data is a persistent challenge in VAD, particularly for detecting anomalies in novel, unseen contexts [12]-[14]. Few-shot and zero-shot methods, powered by LLMs and VLMs, offer promising solutions by enabling generalization from limited labeled data [25], [30], [33]\u2013[35], [37]. These models use pre-trained knowledge to recognize anomalies in unseen classes or with minimal training data. Methods that rely on semantic understanding of video content, combined with motion features, make it possible to identify anomalies without the need for large-scale annotated datasets. However, despite the potential, these methods often struggle with complex anomaly types that deviate significantly from the norm, especially in open-world environments where the nature of anomalies is unknown.\nOpen-world and class-agnostic anomalies. Traditional VAD approaches [12]-[14] typically operate in closed-world settings where predefined anomaly classes are assumed. However, real-world applications require models capable of detecting open-world, class-agnostic anomalies, which may involve previously unseen behaviors. Multimodal models [30], [33], [35], [38], [39] that combine semantic and motion reasoning are making strides in addressing these challenges by detecting anomalies without prior knowledge of the specific class. These systems are more robust in open-world settings, where they can detect unexpected anomalies, but issues related to scalability and dynamic adaptation remain unresolved, particularly when new types of anomalies appear over time.\nMotivation and key differences. While existing methods have made significant strides in one or more of these areas [7], [19], [20], the integration of LLMs and VLMs offers a holistic approach to the challenges of video anomaly detection. Unlike previous works that tend to focus on isolated aspects of VAD (e.g., temporal reasoning, interpretability, or class-specific detection), this paper emphasizes the synergy between visual and textual modalities to address all key challenges simultaneously. By focusing on semantic insights and motion features, this review highlights how multimodal models can provide a more comprehensive solution to video anomaly detection. Moreover, by exploring few-shot and zero-shot capabilities, this paper proposes a shift toward more generalizable systems that can perform well even with minimal training data."}, {"title": "III. INSIGHTS ON RECENT ADVANCES", "content": "We offer a thorough analysis in VAD in 2024, with a focus on the integration of LLMs and VLMs. The methods reviewed include: VADor [29], OVVAD [35], LAVAD [30], TPWNG [36], CALLM [39], Holmes-VAD [32], HAWK [27], VLAVAD [31], ALFA [34], AnomalyRuler [37], STPrompt [33], Holmes-VAU [38], and VERA [25]. We structure our discussion around four key perspectives addressed by these recent advancements: temporal and contextual relationships, interpretability and explainability, training-free and few-shot learning approaches, and open-world/class-agnostic anomaly detection. For each perspective, we highlight"}, {"title": "A. Temporal Modeling and Context", "content": "Temporal modeling is fundamental to video anomaly detection (VAD), as anomalies are often characterized by deviations in temporal patterns. The primary challenge lies in capturing intricate temporal dynamics while maintaining computational efficiency and scalability. Recent methods address these challenges with innovative modules and the integration of contextual reasoning [27], [29]-[32], [35], [36].\nVADor [29] introduces a Long-Term Context (LTC) module to address the limitations of open-sourced video LLMs in handling long-range context, effectively capturing temporal dynamics. However, scalability remains an issue for longer or more complex videos. LAVAD [30] uses a sliding window over frame-level captions to aggregate temporal information, achieving reasonable performance in structured scenarios but faltering with noisy or incomplete captions. OVVAD [35] uses a graph convolutional network (GCN) as a temporal adapter, bridging frozen CLIP encoders with sequential data for effective temporal reasoning without extensive retraining. However, it struggles to fully exploit fine-grained temporal cues. VLAVAD [31] integrates semantic inconsistencies with temporal information through a Sequence State Space Module (S3M), improving anomaly detection in unsupervised settings but facing scalability challenges due to high-dimensional state representations. Motion-centric approaches, such as HAWK [27], use motion-to-language mappings to connect dynamic patterns with textual descriptions, enhancing interpretability and precision in motion anomalies. Similarly, TPWNG [36] adapts to varying video durations using self-learning modules, excelling in weakly supervised settings. Finally, Holmes-VAD [32] combines a lightweight temporal sampler with multimodal analysis, effectively identifying and explaining anomalies in complex scenarios.\nThese approaches showcase a diverse range of temporal modeling strategies. While VADor and OVVAD focus on predefined modules for long-term context, HAWK and Holmes-VAD emphasize motion dynamics and adaptive sampling. Future work could combine motion-based features [40]\u2013[44] with advanced context-aware modules to address scalability and efficiency challenges in real-time anomaly detection."}, {"title": "B. Interpretability and Transparency", "content": "Interpretability is increasingly recognized as a critical factor in VAD systems, particularly for deployment in sensitive and high-stakes environments. Methods in this category focus on generating semantic and multimodal insights, making anomaly detection systems more comprehensible to end-users [27], [29]-[34].\nVADor [29] enhances interpretability by fine-tuning Video-LLaMA's projection layer, blending anomaly detection with semantic reasoning. However, its reliance on instruction-tuned data limits adaptability to diverse anomaly types. LAVAD [30] increases transparency through scene descriptions, though noisy captions undermine reliability. In contrast, VLAVAD [31] simplifies semantic mappings to improve interpretability in unsupervised settings, sacrificing fine-grained detail for reduced complexity. Holmes-VAD [32] uses multimodal instruction tuning and temporal supervision to generate context-rich explanations of anomalies. HAWK [27] integrates motion-based reasoning via interactive visual-language models, enhancing interpretability. Similarly, STPrompt [33] aligns spatiotemporal regions with learned prompts, reducing background noise and improving spatial localization. ALFA [34] emphasizes pixel-level precision using image-text alignment but requires additional fine-tuning for effective generalization.\nThe emphasis on semantic and multimodal strategies marks a promising shift toward transparent VAD systems. While Holmes-VAD excels in providing contextual explanations, ALFA offers granular insights. Future research should balance granularity, semantic generalization, and computational efficiency to develop robust, interpretable VAD systems."}, {"title": "C. Training-Free and Few-Shot Detection", "content": "The scarcity of annotated datasets presents a significant challenge for VAD, especially in open-world scenarios. Training-free and few-shot approaches use pre-trained models and minimal annotations to facilitate anomaly detection in data-scarce environments [25], [30], [33]\u2013[35], [37].\nLAVAD [30] bypasses dataset-specific training by using pre-trained LLMs and VLMs for temporal aggregation. While adaptable, its lack of specialization hinders performance with complex anomaly types. AnomalyRuler [37] excels in static few-shot scenarios using rule-based reasoning with minimal normal samples but struggles with dynamic anomalies. OVVAD [35] decouples anomaly detection from classification, enabling robust detection of unseen anomalies but lacking temporal depth. STPrompt [33] aligns spatiotemporal prompts to localize anomalies under weak supervision, performing well in straightforward cases but faltering with nuanced patterns. ALFA [34] dynamically adapts prompts at runtime for fine-grained detection, and VERA [25] introduces verbalized learning to enable training-free anomaly detection without modifying model parameters.\nCombining the adaptability of VERA with the fine-grained capabilities of ALFA, alongside temporal reasoning as seen in OVVAD, could provide a pathway to more robust solutions for open-world anomaly detection."}, {"title": "D. Open-World and Class-Agnostic Detection", "content": "Real-world applications demand VAD systems capable of detecting unseen anomalies and adapting to unpredictable scenarios. Open-world and class-agnostic approaches aim to address these challenges [30], [33], [35], [38], [39].\nOVVAD [35] uses a dual-task strategy for both class-agnostic and class-specific detection, though its temporal modeling could be enhanced. LAVAD [30] uses textual descriptions for anomaly scoring but is limited by noisy captions. STPrompt [33] excels in weak supervision, localizing anomalies effectively, though its robustness against complex patterns is limited. Holmes-VAU [38] uses hierarchical annotations for broader coverage, while CALLM [39] innovates with pseudo-labeling using multimodal features, though further validation in dynamic contexts is needed.\nIntegrating the hierarchical annotation approach of Holmes-VAU with the multimodal innovation of CALLM could address real-world complexities. Further advancements in temporal and textual reasoning frameworks are essential to enhance detection reliability in open-world scenarios."}, {"title": "IV. ANALYSIS AND DISCUSSION", "content": "Frame sampling strategies. Frame sampling strategies play a pivotal role in balancing temporal resolution, computational efficiency, and overall model performance. Dense sampling offers the highest temporal granularity, essential for detecting nuanced, rapid anomalies such as sudden behavioral changes or fleeting events. However, the redundancy of densely sampled frames increases computational costs, making this strategy less practical for large-scale or real-time applications. Uniform sampling, used in methods like VERA, provides a simpler alternative by sampling frames at fixed intervals. This approach balances computational overhead and temporal coverage but often misses critical local temporal patterns. Similarly, random sampling, such as in VADor, introduces variability, augmenting training data by exposing the model to diverse temporal patterns. However, this strategy risks overlooking key anomaly-defining frames, reducing its effectiveness in scenarios requiring precise temporal modeling. Adaptive sampling, used in Holmes-VAD and Holmes-VAU, dynamically focuses on regions of interest in time. This method prioritizes frames likely to contain anomalies, enabling both fine-grained detection and computational efficiency. Adaptive strategies strike an optimal balance, excelling in scenarios where anomalies are temporally sparse or context-dependent. Nonetheless, their reliance on additional heuristic or learning mechanisms introduces moderate costs.\nThe choice of sampling strategy should align with the nature of anomalies and the operational constraints. For global trends, uniform or random sampling suffices, while dense or adaptive sampling is indispensable for fine-grained, time-sensitive detection tasks. Integrating adaptive mechanisms into training-free frameworks, as a future direction, could enhance both scalability and precision in VAD systems.\nFine-tuning vs. training-free approaches. Fine-tuning-based methods dominate in datasets requiring detailed temporal reasoning. hybrid approach combining training-free scalability with fine-tuning precision could address these limitations. For instance, integrating temporal sampling techniques from fine-tuning-based methods into training-free frameworks may enhance their temporal reasoning without compromising scalability. Future research should also explore few-shot learning and open-vocabulary techniques to bridge gaps in generalization and adaptability, as demonstrated by Holmes-VAU'S promising results. This direction can enable systems to handle emerging anomalies with minimal retraining while maintaining high accuracy.\nQuantitative evaluation and comparative analysis. Among the methods evaluated, VLAVAD, VADor, Holmes-VAD, and STPrompt stand out for their high interpretability and temporal modeling, though they perform differently across benchmark datasets. VLAVAD excels in capturing fine-grained temporal features through fine-tuning and is highly effective on datasets such as UCSD Ped2 , but it lacks adaptability to open-world anomalies. In contrast, LAVAD offers interpretability with semantic explanations, but its performance on datasets like UCF-Crime and XD-Violence  is limited due to its insufficient handling of temporal dynamics. This contrast highlights the importance of balancing interpretability with strong temporal modeling for real-world anomaly detection.\nIn terms of temporal modeling, methods such as Holmes-VAD and Holmes-VAU are more successful in addressing the temporal dependencies inherent in video anomaly detection. LAVAD offers a training-free solution with temporal aggregation, but it struggles to compete with methods like TPWNG that use spatio-temporal prompt learning. Despite AnomalyRuler achieving solid performance on the ShanghaiTech dataset, it lags behind STPrompt, demonstrating that STPrompt's ability to adapt to temporal dynamics in video sequences provides a significant advantage. However, while STPrompt shows strong performance in time-sensitive anomaly detection, its dependence on fine-tuning limits its scalability and applicability to unseen anomaly types, which is a key drawback . \nFew-shot and open-world detection capabilities are critical for handling emerging or previously unseen anomalies, and methods such as OVVAD and AnomalyRuler perform well in this regard. OVVAD shows the ability to detect both seen and unseen anomalies, especially with its open-vocabulary approach and class-agnostic detection. However, its performance is suboptimal in scenarios requiring temporal modeling, as seen with its results on XD-Violence . On the other hand, AnomalyRuler achieves strong performance on both UCSD Ped2 and CUHK Avenue , showcasing its robustness. Its rule-based approach, however, may struggle with more complex, dynamic anomalies, suggesting that while AnomalyRuler is effective in controlled settings, it may need further refinement for broader use cases.\nLastly, the Holmes-VAD and STPrompt methods excel in terms of interpretability, temporal modeling, and adaptability. Holmes-VAD stands out as one of the top performers, especially on the UCF-Crime and XD-Violence dataset, thanks to its combination of anomaly-aware supervision and fine-tuning, which allows it to capture both temporal and semantic features effectively. Similarly, STPrompt uses spatio-temporal prompt learning and fine-tuning to achieve excellent results on datasets like ShanghaiTech and UCF-Crime . However, both methods are limited by their reliance on fine-tuning, which reduces their generalization ability across different anomaly types and datasets.\nThe results indicate that a multi-faceted approach is needed to optimize VAD systems. Methods like Holmes-VAD and STPrompt show that combining fine-tuned temporal and semantic modeling with interpretability and adaptability to new anomalies can lead to high performance across multiple datasets. However, the challenges of scalability, the need for robust temporal models, and handling noisy captions or incomplete annotations remain significant hurdles. The combination of training-free solutions with fine-tuning, as demonstrated in LAVAD, could provide a more versatile framework for open-world anomaly detection."}, {"title": "V. CONCLUSION", "content": "This work explores the integration of large language models (LLMs) and vision-language models (VLMs) in video anomaly detection (VAD), focusing on key challenges such as temporal modeling, interpretability, few-shot learning, and open-world anomaly detection. We examine how recent advances seek to address these challenges, highlighting both the strengths and limitations of current methods. Our analysis emphasizes the need for more robust temporal modeling to capture complex dependencies within video data, as well as the importance of fine-grained interpretability to better understand anomaly detection decisions. Additionally, we recognize the potential of training-free and few-shot learning methods, which show promise for improving scalability and adaptability in scenarios with limited supervision or previously unseen anomalies. We propose that future VAD systems could benefit from combining these approaches, such as improving temporal consistency, aligning semantic features, and incorporating adaptive learning strategies. This work lays the foundation for advancing VAD by refining these models, enhancing their scalability, and addressing the complexities inherent in dynamic video data."}]}