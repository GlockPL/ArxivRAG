{"title": "Inverse Flow and Consistency Models", "authors": ["Yuchen Zhang", "Jian Zhou"], "abstract": "Inverse generation problems, such as denoising without ground truth observations, is a critical challenge in many scientific inquiries and real-world applications. While recent advances in generative models like diffusion models, conditional flow matching, and consistency models achieved impressive results by casting generation as denoising problems, they cannot be directly used for inverse generation without access to clean data. Here we introduce Inverse Flow (IF), a novel framework that enables using these generative models for inverse generation problems including denoising without ground truth. Inverse Flow can be flexibly applied to nearly any continuous noise distribution and allows complex dependencies. We propose two algorithms for learning Inverse Flows, Inverse Flow Matching (IFM) and Inverse Consistency Model (ICM). Notably, to derive the computationally efficient, simulation-free inverse consistency model objective, we generalized consistency training to any forward diffusion processes or conditional flows, which have applications beyond denoising. We demonstrate the effectiveness of IF on synthetic and real datasets, outperforming prior approaches while enabling noise distributions that previous methods cannot support. Finally, we showcase applications of our techniques to fluorescence microscopy and single-cell genomics data, highlighting IF's utility in scientific problems. Overall, this work expands the applications of powerful generative models to inversion generation problems.", "sections": [{"title": "1. Introduction", "content": "Recent advances in generative modeling such as diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song & Ermon, 2020; Song et al., 2021; 2022), conditional flow matching models (Lipman et al., 2023; Tong et al., 2024), and consistency models (Song et al., 2023; Song & Dhariwal, 2023) have achieved great success by learning a mapping from a simple prior distribution to the data distribution through an Ordinary Differential Equation (ODE) or Stochastic Differential Equation (SDE). We refer to their models as continuous-time generative models. These models typically involve defining a forward process, which transforms the data distribution to the prior distribution over time, and generation is achieved through learning a reverse process that can gradually transform the prior distribution to the data distribution (Figure 1).\nDespite that those generative models are powerful tools for modeling the data distribution, they are not suitable for the inverse generation problems when the data distribution is not observed and only data transformed by a forward process is given, which is typically true for noisy real-world data measurements. Mapping from noisy data to the latent ground truth is especially important in various scientific applications when pushing the limit of measurement capabilities. This limitation necessitates the exploration of novel methodologies that can bridge the gap between generative modeling and effective denoising in the absence of clean data.\nHere we propose a new approach called Inverse Flow (IF)1, that learns a mapping from the observed noisy data distribution to the unobserved, ground truth data distribution (Figure 1), inverting the data requirement of generative models. An ODE or SDE is specified to reflect knowledge about the noise distribution. We further devised a pair of algorithms, Inverse Flow Matching (IFM) and Inverse Consistency Model (ICM) for learning inverse flows. Specifically, ICM involves a computationally efficient simulation-free objective that does not involve any ODE solver.\nA main contribution of our approach is generalizing continuous-time generative models to inverse generation problems such as denoising without ground truth. In addition, in order to develop ICM, we generalized the consistency training objective for consistency models to any forward diffusion process or conditional flow. This broadens the scope of consistency model applications and has implications beyond denoising.\nCompared to prior approaches for denoising without ground truth, IF offers the most flexibility in noise distribution, allowing almost any continuous noise distributions including those with complex dependency and transformations. IF can be seamlessly integrated with generative modeling to generate samples from the ground truth rather than the observed noisy distribution. More generally, IF models the past states of a (stochastic) dynamical system before the observed time points using the knowledge of its dynamics, which can have applications beyond denoising."}, {"title": "2. Background", "content": "Our proposed inverse flow framework is built upon continuous-time generative models such as diffusion models, conditional flow matching, and consistency models. Here we present a unified view of these methods that will help connect inverse flow with this entire family of models (Section 3).\nThese generative modeling methods are connected by their equivalence to continuous normalizing flow or neural ODE (Chen et al., 2019). They can all be considered as explicitly or implicitly learning the ODE that transforms between the prior distribution p(x1) and the data distribution p(x0)\n$\\frac{dx}{dt} = u(x)$.\n(1)\nin which $u_t(x)$ represents the vector field of the ODE. We use the convention that $t = 0$ corresponds to the data distribution and $t = 1$ corresponds to the prior distribution. Generation is realized by reversing this ODE, which makes this family of methods a natural candidate for extension toward denoising problems.\nContinuous-time generative models typically involve defining a conditional ODE or SDE that determines the $p(x_t|x_0)$ that transforms the data distribution to the prior distribution. Training these models involves learning the unconditional ODE (Eq. 1) based on $x_0$ and the conditional ODE or SDE (Lipman et al., 2023; Tong et al., 2024; Song et al., 2021) (Figure 1). The unconditional ODE can be used for generation from noise to data."}, {"title": "2.1.1. CONDITIONAL FLOW MATCHING", "content": "Conditional flow matching defines the transformation from data to prior distribution via a conditional ODE vector field $u_t(x | x_0)$. The unconditional ODE vector field $v(x)$ is learned by minimizing the objective (Lipman et al., 2023; Tong et al., 2024; Albergo & Vanden-Eijnden, 2023):\n$\\vert\\vert v_t(x_t) - u_t(x_t | x_0) \\vert\\vert$.\n(2)\nwhere $x_0$ is sampled from the data distribution, and $x_t$ is sampled from the conditional distribution $p(x_t | x_0)$ given by the conditional ODE.\nThe conditional ODE vector field $u_t(x | x_0)$ can also be stochastically approximated through sampling from both prior distribution and data distribution and using the conditional vector field $u_t(x | x_0, x_1)$ as the training target (Lipman et al., 2023; Tong et al., 2024):\n$\\vert\\vert v_t(x_t) - u_t(x_t | x_0, x_1) \\vert\\vert$.\n(3)\nThis formulation has the benefit that $u_t(x | x_0, x_1)$ can be easily chosen as any interpolation between $x_0$ and $x_1$, because this interpolation does not affect the probability density at time 0 or 1 (Lipman et al., 2023; Tong et al., 2024; Albergo & Vanden-Eijnden, 2023; Albergo et al., 2023). For example, a linear interpolation corresponds to $x_t = x_0 + t(x_1 - x_0)$ (Lipman et al., 2023; Tong et al., 2024; Liu et al., 2022). Sampling is realized by simulating the unconditional ODE with learned vector field $v_t(x)$ in the reverse direction."}, {"title": "2.1.2. CONSISTENCY MODELS", "content": "In contrast, consistency models (Song et al., 2023; Song & Dhariwal, 2023) learn consistency functions that can directly map a sample from the prior distribution to data distribution, equivalent to simulating the unconditional ODE in the reverse direction:\n$c(x_t, t) = ODE^t_{t \\rightarrow 0}(x_t)$\nwhere $x_t$ denotes $x$ at time $t$, and we use $ODE^t_{t \\rightarrow 0}(x_t)$ to denote simulating the ODE with vector field $u_t(x)$ from time $t$ to time 0 starting from $x_t$. The consistency function is trained by minimizing the consistency loss (Song et al., 2023), which measures the difference between consistency function evaluations at two adjacent time points\n$L_{CM}(\\theta) =$\\\n$\\mathbb{E}_{i, x_{t_{i+1}}, x_{t_i}} [\\vert\\vert c_{\\theta}(x_{t_{i+1}}, t_{i+1}) - stopgrad(c_{\\theta}(x_{t_i}, t_i)) \\vert\\vert]$\n(4)\nwith the boundary condition $c(x, 0) = x$. Stopgrad indicates that the term within the operator does not get optimized.\nThere are two approaches to training consistency models: one is distillation, and the other is training from scratch. In the consistency distillation objective, a pretrained diffusion model is used to obtain the unconditional ODE vector field $u_t$, and $x_{t_{i+1}}$ and $x_{t_i}$ differs by one ODE step\n$x_{t_{i+1}} \\sim p(x_{t_{i+1}} | x_0)$,\n$x_{t_{i+1}} - x_{t_i} = u_{t_{i+1}}(x_{t_{i+1}})(t_{i+1} - t_i)$\n(5)"}, {"title": "2.1.3. DIFFUSION MODELS", "content": "In diffusion models, the transformation from data to prior distribution is defined by a forward diffusion process (conditional SDE). The diffusion model training learns the score function which determines the unconditional ODE, also known as the probability flow ODE (Song et al., 2021).\nDenoising applications of diffusion models Diffusion models are inherently connected to denoising problems as the generation process is essentially a denoising process. However, existing denoising methods using diffusion models require training on ground truth data (Yue et al., 2023; Xie et al., 2023b), which is not available in inverse generation problems.\nAmbient diffusion and GSURE-diffusion Ambient Diffusion (Daras et al., 2023) and GSURE-diffusion (Kawar et al., 2024) address a related problem of learning the distribution of clean data by training on only linearly corrupted (linear transformation followed by additive Gaussian noise) data. Although those methods are designed for generation, they can be applied to denoising. Ambient Diffusion Posterior Sampling (Aali et al., 2024), further allowed using models trained with ambient diffusion on corrupted data to perform posterior sampling-based denoising for a different forward process (e.g., blurring). Consistent Diffusion Meets Tweedie (Daras et al., 2024) improves Ambient Diffusion by allowing exact sampling from clean data distribution using consistency loss with a double application of Tweedie's formula. (Rozet et al., 2024) explored the potential of expectation maximization in training diffusion models on corrupted data. However, all these methods are restricted to training on linearly corrupted data, which still limit their applications when the available data is affected by other types of noises."}, {"title": "2.2. Denoising without ground truth", "content": "Denoising without access to ground truth data requires assumptions about the noise or the signal. Most contemporary approaches are based on assumptions about the noise, as the noise distribution is generally much simpler and better understood. Because prior methods have been comprehensively reviewed (Kim & Ye, 2021; Batson & Royer, 2019; Lehtinen et al., 2018; Xie et al., 2020; Soltanayev & Chun, 2018; Metzler et al., 2020), and our approach is not directly built upon these approaches, we only present a brief overview and refer the readers to Appendix A.3 referenced literature for more detailed discussion. None of these approaches are generally applicable to any noise types."}, {"title": "3. Inverse Flow and Consistency Models", "content": "In continuous-time generative models, usually the data $x_0$ from the distribution of interest is given. In contrast, in inverse generation problems, only the transformed data $x_1$ and the conditional distribution $p(x_1|x_0)$ are given, whereas $x_0$ are unobserved. For example, $x_1$ are the noisy observations and $p(x_1|x_0)$ is the conditional noise distribution. We define the Inverse Flow (IF) problem as finding a mapping from $x_1$ to $x_0$ which allows not only recovering the unobserved data distribution $p(x_0)$ but also providing an estimate of $x_0$ from $x_1$ (Figure 1).\nFor denoising without ground truth applications, the inverse flow framework requires only the noisy data $x_1$ and the ability to sample from the noise distribution $p(x_1|x_0)$. This is thus applicable to any continuous noise and allows complex dependencies on the noise distribution, including noise that can only be sampled through a diffusion process.\nIntuitively, without access to unobserved data $x_0$, inverse flow algorithms train a continuous-time generative model using generated $x_0$ from observed data $x_1$ within the training loop (Figure 1). We demonstrated that this approach effectively recovers the unobserved distribution $p(x_0)$ and learns a mapping from $x_1$ to $x_0$."}, {"title": "3.1. Inverse Flow Matching", "content": "To solve the inverse flow problem, we first consider learning a mapping from $x_1$ to $x_0$ through an ODE with vector field $v(x)$. We propose to learn $v_{\\theta}(x)$ with the inverse flow matching (IFM) objective\n$L_{IFM}(\\theta) = \\mathbb{E} \\vert\\vert v_{\\theta}(x_t) - u_t(x_t | ODE^1_{\\theta \\rightarrow 0}(x_1)) \\vert\\vert$ \n(7)\nwhere the expectation is taken over $t$, $p(x_1)$, and $p(x_t | x_0 = ODE^1_{\\theta \\rightarrow 0}(x_1))$. This objective differs from conditional flow matching (Eq. 2) in two key aspects: using only transformed data $x_1$ rather than unobserved data $x_0$, and choosing the conditional ODE based on the conditional distribution $p(x_1|x_0)$. Specifically,\n1. Sampling from the data distribution $p(x_0)$ is replaced with sampling from $p(x_1)$ and simulating the unconditional ODE backward in time based on the vector field $v_{\\theta}$, denoted as $ODE^1_{\\theta \\rightarrow 0}(x_1)$. We refer to this distribution as the recovered data distribution $q(x_0)$.\n2. The conditional ODE vector field $u_t(x | x_0)$ is chosen to match the given conditional distribution $p(x_1 | x_0)$ at time 1.\nFor easier and more flexible application of IFM, similar to conditional flow matching (Eq. 3), an alternative form of the conditional ODE $u_t(x | x_0, x_1)$ can be used instead of $u_t(x | x_0)$. Since $x_1$ is sampled from the noise distribution $p(x_1|x_0)$, the above condition is automatically satisfied. The conditional ODE vector field can be easily chosen as any smooth interpolation between $x_0$ and $x_1$, such as $u_t(x | x_0, x_1) = x_1 - x_0$. We detailed the inverse flow matching training in Algorithm 1 with the alternative form in Appendix A.1.\nNext, we discuss the theoretical justifications of the IFM objective and the interpretation of the learned model. We show below that when the loss converges, the recovered data distribution $q(x_0)$ matches the ground truth distribution $p(x_0)$. The proof is provided in Appendix A.2.1.\nTheorem 1 Assume that the noise distribution $p(x_1 | x_0)$ satisfies the condition that, for any noisy data distribution $p(x_1)$ there exists only one probability distribution $p(x_0)$ that satisfies $p(x_1) = \\int p(x_1 | x_0)p(x_0)dx_0$, then under the condition that $L_{IFM} = 0$, we have the recovered data distribution $q(x_0) = p(x_0)$.\nMoreover, we show that with IFM the learned ODE trajectory from $x_1$ to $x_0$ can be intuitively interpreted as always pointing toward the direction of the estimated $x_0$. More formally, the learned unconditional ODE vector field can be interpreted as an expectation of the conditional ODE vector field.\nLemma 1 Given a conditional ODE vector field $u_t(x | x_0, x_1)$ that generates a conditional probability path $p(x_t | x_0, x_1)$, the unconditional probability path $p(x_t)$ can be generated by the unconditional ODE vector field $u_t(x)$, which is defined as\n$u_t(x) = \\mathbb{E}_{p(x_0, x_1|x)} [u_t(x | x_0, x_1)]$\n(8)\nThe proof is provided in Appendix A.2.1. Specifically, with the choice of $u_t(x | x_0, x_1) = x_1 - x_0$, Eq. 8 has an intuitively interpretable form\n$u_t(x) = \\mathbb{E}_{p(x_0/x)} \\Big[ \\frac{x_1 - x_0}{t} \\Big]$\n(9)\nwhich means that the unconditional ODE vector field at any time t points straight toward the expected ground truth $x_0$."}, {"title": "3.2. Simulation-free Inverse Flow with Inverse Consistency Model", "content": "IFM can be computationally expensive during training and inference because it requires solving ODE in each update. We address this limitation by introducing inverse consistency model (ICM), which learns a consistency function to directly solve the inverse flow without involving an ODE solver.\nHowever, the original consistency training formulation is specific to one type of diffusion process (Karras et al., 2022), which is only applicable to independent Gaussian noise distribution for inverse generation application. Thus, to derive inverse consistency model that is applicable to any transformation, we first generalize consistency training so that it can be applied to arbitrary transformations and thus flexible to model almost any noise distribution."}, {"title": "3.2.1. GENERALIZED CONSISTENCY TRAINING", "content": "To recall from Section 2.1.2, consistency distillation is only applicable to distilling a pretrained diffusion or conditional flow matching model. The consistency training objective allows training consistency models from scratch but only for a specific forward diffusion process, which limits its flexibility in applying to any inverse generation problem.\nGeneralized Consistency Training $\\longrightarrow$ Consistency Distillation\n---\nConditional Flow Matching $\\longrightarrow$ Flow Matching\nHere we introduce generalized consistency training (GCT), which extends consistency training to any conditional ODE or forward diffusion process (through the corresponding conditional ODE). Intuitively, generalized consistency training modified consistency distillation (Eq. 4 and Eq. 5) in the same manner as how conditional flow matching modified the flow matching objective: instead of requiring the unconditional ODE vector field $u_t(x)$ which is not available without a pretrained diffusion or conditional flow matching model, GCT only requires the user-specified conditional ODE vector field $u_t(x | x_0)$.\n$L_{GCT}(\\theta) =$\\\n$\\mathbb{E} [\\vert\\vert (c_{\\theta}(x_{t_{i+1}}, t_{i+1}) - stopgrad (c_{\\theta}(x_{t_i}, t_i))) \\vert\\vert]$,\n$x_{t_{i+1}} - x_{t_i} = u_{t_{i+1}}(x_{t_{i+1}} | x_0)(t_{i+1} - t_i)$\n(10)\nWhere the expectation is taken over $i$, $p(x_0)$, and $p(x_{t_{i+1}}|x_0)$. An alternative formulation where the conditional flow is defined by $u_{t_{i+1}}(x | x_0, x_1)$ is detailed in Appendix A.1.\nWe proved that the generalized consistency training (GCT) objective is equivalent to the consistency distillation (CD) objective (Eq. 4, Eq. 5). The proof is provided in Appendix A.2.2.\nTheorem 2 Assuming the consistency function $c_{\\theta}(x, t)$ is twice differentiable with bounded second derivatives, and $\\mathbb{E}_{p(x_0, x_1|x)} [\\vert\\vert u_t(x | x_0, x_1) \\vert\\vert] < \\infty$, up to a constant independent of $\\theta$, $L_{GCT}$ and $L_{CD}$ are equal."}, {"title": "3.2.2. INVERSE CONSISTENCY MODELS", "content": "With generalized consistency training, we can now provide the inverse consistency model (ICM) (Figure 1, Algorithm 2):\n$L_{ICM}(\\theta) = \\mathbb{E} [\\vert\\vert (c_{\\theta}(x_{t_{i+1}}, t_{i+1}) - stopgrad (c_{\\theta}(x_{t_i}, t_i))) \\vert\\vert]$,\n$x_{t_{i+1}} - x_{t_i} = u_{t_{i+1}}(x_{t_{i+1}} | x_0)(t_{i+1} - t_i)$\n(11)\nwhich is the consistency model counterpart of IFM (Eq. 7). The expectation is taken over $i$, $p(x_1)$, $p (x_{t_{i+1}} | x_0 = c_{\\theta}(x_1, 1))$. Similar to IFM, a convenient alternative form is provided in Appendix A.1.\nSince learning a consistency model is equivalent to learning a conditional flow matching model, ICM is equivalent to IFM following directly from our Theorem 2 and Theorem 1 from (Song et al., 2023).\nLemma 2 Assuming the consistency function $c_{\\theta}(x, t)$ is twice differentiable and $\\frac{\\partial c_{\\theta}(x, t)}{\\partial x}$ is almost everywhere nonzero2, when the inverse consistency loss $L_{ICM} = 0$, there exists a corresponding ODE vector field $v_{\\theta}^*(x)$ that minimized the inverse flow matching loss $L_{IFM}$ to 0.\nThe proof is provided in Appendix A.2.2. As in IFM, when the loss converges, the data distribution $q_{\\theta} (x_0)$ recovered by ICM matches the ground truth distribution $p(x_0)$, but ICM is much more computationally efficient as it is a simulation-free objective."}, {"title": "4. Experiments", "content": "We first demonstrated the performance and properties of IFM and ICM on synthetic inverse generation datasets, which include a deterministic problem of inverting Naiver-Stokes simulation and a stochastic problem of denoising a"}, {"title": "4.1. Synthetic datasets", "content": "To test the capability of inverse flow in inverting complex transformations, we first attempted the deterministic inverse generation problem of inverting the transformation by Navier-Stokes fluid dynamics simulation3. We aim to recover the earlier state of the system without providing them for training (Figure 2). Navier-Stokes equations describe the motion of fluids by modeling the relationship between fluid velocity, pressure, viscosity, and external forces. These equations are fundamental in fluid dynamics and remain mathematically challenging, particularly in understanding turbulent flows. The details of the simulation are described in Appendix A.4.2.\nTo test inverse flow algorithms on a denoising inverse generation problem, we generated a synthetic 8-gaussians dataset (Appendix A.4.2 for details). Both IFM and ICM are capable of noise removal (Figure 2). ICM achieved a similar denoising performance as IFM, even though it is much more computationally efficient due to the iterative evaluation of ODE (NFE=10) by IFM."}, {"title": "4.2. Semi-synthetic datasets", "content": "We evaluated the proposed method on images in the benchmark dataset BSDS500 (Arbel\u00e1ez et al., 2011), Kodak, and Set12 (Zhang et al., 2017). To test the model's capability to deal with various types of conditional noise distribution, we generated synthetic noisy images for three different types of noise, including correlated noise and adding noise through a diffusion process without a closed-form transition density function (Appendix A.4.3 for details). All models were trained using the BSDS500 training set and evaluated on the BSDS500 test set, Kodak, and Set12. We show additional qualitative results in Appendix A.6.\n1. Gaussian noise: we added independent Gaussian noise with fixed variance.\n2. Correlated noise: we employed convolution kernels to generate correlated Gaussian noise following the method in (M\u00e4kinen et al., 2020)\n$\\eta = \\nu * g$ \n(12)\nwhere $\\nu \\sim N(0, \\sigma^2I)$ and g is a convolution kernel.\n3. Jacobi process: we transformed the data with Jacobi process (Wright-Fisher diffusion), as an example of SDE-based transform without closed-form conditional distribution\n$dx = [a(1-x) - bx]dt + \\sqrt{sx(1-x)}dw$. (13)\nWe generated corresponding noise data by simulating the Jacobi process with s = 1 and a = b = 1. Notably, the conditional noise distribution generated by the Jacobi process does not generally has an expectation that equals the ground truth (i.e. non-centered noise), which violates the assumptions of Noise2X methods.\nOur approach outperformed alternative unsupervised methods in all three noise types, especially in correlated noise and Jacobi process (Appendix A.6, Table 4.2). This can be attributed to the fact that both Noise2X methods assumes independence of noise across different feature dimensions as well as centered-noise which were violated in corrleated noise and Jacobi process respectively."}, {"title": "4.3. Real-world datasets", "content": ""}, {"title": "4.3.1. FLUORESCENCE MICROSCOPY DATA (FMD)", "content": "Fluorescence microscopy is an important scientific application of denoising without ground truth. Experimental constraints such as phototoxicity and frame rates often limit the capability to obtain clean data. We denoised confocal microscopy images from Fluorescence Microscopy Denoising (FMD) dataset (Zhang et al., 2019). We first fitted a signal-dependent Poisson-Gaussian noise model adopted from (Liu et al., 2013) for separate channels of each noisy microscopic images (Appendix A.4.4 for details). Then denoising flow models were trained with the conditional ODE specified to be consistent with fitted noise model. Our method outperforms Noise2Self and Noise2Void, achieving superior denoising performance for mitochondria, F-actin, and nuclei in the microscopic images of BPAE cells (Figure 3)."}, {"title": "4.3.2. APPLICATION TO DENOISE SINGLE-CELL GENOMICS DATA", "content": "In recent years, the development of single-cell sequencing technologies has enabled researchers to obtain more fine-grained information on tissues and organs at the resolution of single cells. However, the low amount of sample materials per-cell introduces considerable noise in single-cell genomics data. These noises may obscure real biological signals, thereby affecting subsequent analyses.\nApplying ICM to an adult mouse brain single-cell RNA-seq dataset (Zeisel et al., 2018) and a mouse brain development single-cell RNA-seq dataset (Hochgerner et al., 2018b) (Figure 4, Appendix A.4.5 for details), we observed that the denoised data better reflects the cell types and developmental trajectories. We compared the original and denoised data by the accuracy of predicting the cell type identity of each cell based on its nearest neighbor in the top two principal components. Our methods improved the accuracy of the adult mouse brain dataset from 0.513 \u00b1 0.003 to 0.571 \u00b1 0.003, and the mouse brain development dataset from 0.647 \u00b1 0.006 to 0.736 \u00b1 0.006."}, {"title": "5. Limitation and Conclusion", "content": "We introduce Inverse Flow (IF), a generative modeling framework for inverse generation problems such as denoising without ground truth, and two methods Inverse Flow Match (IFM) and Inverse Consistency Model (ICM) to solve the inverse flow problem. Our framework connects the family of continuous-time generative models to inverse generation problems. Practically, we extended the applicability of denoising without ground truth to almost any continuous noise distributions. We demonstrated strong empirical results applying inverse flow. A limitation of inverse flow is assuming prior knowledge of the noise distribution, and future work is needed to relax this assumption. We expect inverse flow to open up possibilities to explore additional connections to the expanding family of continuous-time generative model methods, and the generalized consistency training objective will expand the application of consistency models."}, {"title": "A. Appendix", "content": "Here we provide the details of alternative objectives and corresponding algorithms of IFM and ICM which are easier and flexible to use."}, {"title": "A.1. Alternative forms of IFM and ICM", "content": "We define the alternative objective of IFM similar to conditional flow matching (Eq. 3):\n$L_{IFM}(\\theta) =$\\\n$\\mathbb{E}_{t,p(x_1),p(x_1|x_0=ODE^1_{\\theta \\rightarrow 0}(x_1)),p(x/x_0,x_1)}[\\vert\\vert v_{\\theta}(x_t) - u_t (x_t | ODE^1_{\\theta \\rightarrow 0}(x_1), 1) \\vert\\vert]$ \n(14)\nwhere $x_1$ is sampled from the conditional noise distribution. As described in Section 2.1.1 $u_t(x | x_0, x_1)$ can be easily chosen as any smooth interpolation between $x_0$ and $x_1$, such as $u_t(x | x_0, x_1) = x_1 - x_0$.\nSince ICM is based on generalized consistency training, we first provide the alternative objective of generalized consistency training\n$L_{GCT}(\\theta) = \\mathbb{E}_{i,p(x_0,x_1),p(x_{i+1}/x_0,x_1)} [\\vert\\vert c_{\\theta} (x_{t_{i+1}}, t_{i+1}) - stopgrad (c_{\\theta}(x_{t_i}, t_i)) \\vert\\vert]$,\n$x_{t_{i+1}} - x_{t_i} = u_{t_{i+1}} (x_{t_{i+1}} | x_0, x_1) (t_{i+1} - t_i)$\n(15)\nwhere the conditional flow is defined jointly by $p(x_1 | x_0)$ and $u_{t_{i+1}} (x | x_0, x_1)$.\nThen the alterntive form of ICM can be defined as\n$L_{ICM}(\\theta) = $\n$\\mathbb{E}_{i,p(x_1),p(x_1|x_0=c_{\\theta}(x_1,1)),p(x_{t_{i+1}}|x_0=c_{\\theta}(x_1,1),x_1)} [\\vert\\vert c_{\\theta}(x_{t_{i+1}}, t_{i+1}) - stopgrad (c_{\\theta} (x_{t_i},t_i)) \\vert\\vert]$,\n$x_{t_{i+1}} - x_{t_i} = u_{t_{i+1}} (x_{t_{i+1}} | x_0, x_1)(t_{i+1} - t_i)$ (16)\nwhere $u_t(x | x_0, x_1)$ can be freely defined based on any interpolation between $x_0$ and $x_1$, which is more easily applicable to any conditional noise distribution:."}, {"title": "A.1.2. ALTERNATIVE ALGORITHMS OF IFM AND ICM", "content": "Here we show the algorithms of alternative objectives of IFM (Eq. 14) and ICM (Eq. 16)."}, {"title": "A.2. Proofs", "content": ""}, {"title": "A.2.1. INVERSE FLOW MATCHING", "content": "Theorem 1: Assume that the conditional noise distribution $p(x_1 | x_0)$ satisfies the condition that", "p(x_0)$.\nProof": "nThe inferred data distribution is given by the push-forward operator (Lipman et al.", "2023)": "n$q(x_0) = [ODE^1_{\\theta \\rightarrow 0}", "of\n$[\\phi": "p(x_1) = p( \\phi^{-1}(x_0)) det \\Big[ \\frac{\\partial \\phi^{-1}(x_0)}{\\partial x_1} \\Big", "0}": "q(x_1)$\n(20)\nThen we find that\n$[ODE^1_{\\theta \\rightarrow 0}"}, {"0}": "q(x_1)$\n(21)\nBy the definition of the push-forward operator, we have\n$p \\Big( (ODE^1_{\\theta \\rightarrow 0})^{-1}(x_1) \\Big) det \\Big[ \\frac{\\partial (ODE^1_{\\theta \\rightarrow 0})^{-1}(x_1)}{\\partial x_1} \\Big"}]}