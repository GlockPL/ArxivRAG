{"title": "AI Consciousness and Public Perceptions: Four Futures", "authors": ["Ines Fernandez", "Nicoleta Kyosovska", "Jay Luong", "Gabriel Mukobi"], "abstract": "The discourse on risks from advanced AI systems (\"AIs\") typically focuses on misuse, accidents and loss of control, but the question of Als' moral status could have negative impacts which are of comparable significance and could be realised within similar timeframes. Our paper evaluates these impacts by investigating (1) the factual question of whether future advanced AI systems will be conscious, together with (2) the epistemic question of whether future human society will broadly believe advanced Al systems to be conscious. Assuming binary responses to (1) and (2) gives rise to four possibilities: in the true positive scenario, society predominantly correctly believes that Als are conscious; in the false positive scenario, that belief is incorrect; in the true negative scenario, society correctly believes that Als are not conscious; and lastly, in the false negative scenario, society incorrectly believes that Als are not conscious. The paper offers vivid vignettes of the different futures to ground the two-dimensional framework. Critically, we identify four major risks: AI suffering, human disempowerment, geopolitical instability, and human depravity. We evaluate each risk across the different scenarios and provide an overall qualitative risk assessment for each scenario. Our analysis suggests that the worst possibility is the wrong belief that AI is non-conscious, followed by the wrong belief that AI is conscious. The paper concludes with the main recommendations to avoid research aimed at intentionally creating conscious AI and instead focus efforts on reducing our current uncertainties on both the factual and epistemic questions on AI consciousness.", "sections": [{"title": "1. AI consciousness concerns all of us", "content": "Artificial intelligence continues to surprise us with capabilities once thought to be impossible for machines. Today, AI systems (henceforth, \"Als\") assist scientists at the forefront of scientific research. They drive cars and create works of art and music. People engage in increasingly complex conversations with AI chatbots on a daily basis. Some Als even provide social and emotional companionship. This raises the question: what's next?\nToday, many people think that AI could never be conscious (Pauketat et al. 2023). They don't think that AI could feel things, like emotions or pain. And if they have no more subjective experience than rocks, tables, and chairs, then they probably don't have much moral standing\u2014they wouldn't really have interests that we ought to take into consideration. But could that change in the near future? Could it only be a matter of time before we build conscious AI\u2014and are we ready for such a future? At first, the notion that machines might be conscious or merit moral consideration might seem outlandish or fanciful\u2014the stuff of science fiction. One might reasonably ask \u201cBut aren't they just machines? Why should we even be considering this?\u201d\nWe think there are a few reasons to take this possibility seriously. Today, researchers and technologists around the world are working on different projects both directly and indirectly related to conscious AI (Huckins 2023). Experts who are engaged in such projects see enormous promise in conscious AI (discussed at length in appendix \u00a7A1). Some think that conscious AI may have superior functionality (e.g. problem-solving ability) and could be capable of more fluid interactions with humans. Others believe that machines that are able to feel and be empathic could be better able to understand human values. If this is right, conscious AI might be safer. Still others see the challenge of building conscious AI as a crucial part of unravelling the deep mysteries of the mind.\nWhile promising, these potential benefits fall short of an open-and-shut case for building conscious AI. We think that, before committing to building conscious AI, we ought to be reasonably sure that it's even a good idea that the benefits outweigh the risks. To this end, this report aims to clarify the pros and cons of building conscious AI. To summarise, our conclusions are as follows:\ni. The benefits of building conscious AI are poorly defined and speculative at best.\nii. There are significant risks associated with building conscious AI.\nIn other words, we think that the attendant risks far outweigh the benefits of building conscious AI. Like many other AI-related risks, the issues posed by AI consciousness present a Collingridge dilemma (1980). On the one hand, the possibility, likelihood, and impact of conscious AI (if any) is exceedingly difficult to forecast (\u00a75.1.2; \u00a71.1). At the same time, if and when conscious Al is actually created, it will be hard to \"put the genie back in the bottle\u201d. It will be hard for humans to maintain control (especially if conscious AI is also at or above human-level intelligence; \u00a74.2)-to say nothing of reverting back to a state prior to conscious AI (that is, without committing some form of violence). Bearing these considerations in mind, we believe the most prudent course of action is fundamentally preventative (Metzinger 2021a; Seth 2021): we should not build conscious AI\u2014at least until we have a clearer grasp of the attendant promises and perils.\nOur analysis in this paper takes as its starting point two questions that are fundamental to the moral status of Als:\na. Factual: Will future advanced AI systems be conscious?\nb. Epistemic: Will future human society believe advanced AI systems to be conscious?\n(a) is a factual question because it concerns the actual state of affairs with respect to whether future Als are conscious. (b) is an epistemic question because it concerns future humans' beliefs about whether future Als are conscious. Assuming binary (yes/no) responses to these questions gives rise to four future scenarios."}, {"title": "1.1 AI consciousness is a neglected issue", "content": "At present, we may not be able to tell whether in the future we will develop conscious AI. But what we can conclude is that, today, virtually no one\u2014not a single political party or ethics committee\u2014is standing up for conscious AI (Metzinger 2021a). Despite growing initiative among researchers to take AI consciousness-related risks seriously, our current theoretical, cultural, and legal frameworks are critically ill-prepared to accommodate the genesis of conscious AI. This widespread neglect can be attributed to three factors:\nUnpredictability: Experts disagree over how likely or imminent AI consciousness is, or even whether it is possible at all (Metzinger 2021b; see also \u00a72.2). This state of radical uncertainty is discussed further in (\u00a75.1.2). Difficulties assessing the extent and ways in which Als may be conscious translate into ambiguities concerning its moral status ($2.1; Hildt 2022). The welfare of future conscious AI populations is thus doubly discounted (Ramsey 1928; Harrison 2010): in the first place, because they do not yet exist, and, in the second place, because it is not yet known whether they can exist at all\u00ba.\nUninformed policymakers and populace: Individually, both AI and consciousness are rather technical and complicated subjects. Understanding frontier problems in each of these fields (to say nothing of their intersection) requires specialised knowledge which is not commonly shared by lawmakers and the general public. These knowledge gaps may lead to ignorance of, or misconceptions about AI consciousness-related risks, or else the perception that such risks are \u201cmerely\u201d speculative. In (\u00a75.2), we discuss the general public's attitudes towards conscious AI.\nOther pressing concerns: AI-related regulatory challenges must compete with other urgent issues such as economic crisis, global conflict, and environmental collapse. Even among the gallery of AI-related risks, AI suffering tends to be superseded by other risks that are judged to be more proximal (discrimination and bias, misinformation and disinformation, worker displacement due to automation), more realistic/prosaic (usage of AI technologies to enforce totalitarian rule, harms to humans due to misalignment), or otherwise closer to human interests10.\nNeedless to say, we do not mean to dismiss the reality or urgency of other risks-whether or not they are related to AI. Our only intention is to point out that risks related to AI consciousness currently command little political concern. As a matter of fact, calls to actively take steps to prepare for AI consciousness-related risks are typically either met with indifference or dismissed as alarmist if they are even seriously considered at all. It is our hope that this position paper will advance the conversation on Al consciousness, preparing decisionmakers across the spectrum for critical and nuanced dialogue."}, {"title": "1.2 Structure", "content": "Having motivated our overall approach, we now turn to laying out the groundwork for our investigation. To begin with (\u00a72), we provide background on the discussion on AI consciousness and moral status. We introduce key concepts and terminology, and specify the scope of the discussion. Afterward, we provide a more thorough introduction to the foregoing 2D framework (\u00a73) and identify the most significant risks associated with conscious AI (\u00a74). In (\u00a75), we make efforts to pinpoint our location and trajectory within the 2D framework. Finally (\u00a76), we propose practical strategies for risk reduction given uncertainty about which scenario we are actually in."}, {"title": "2. Background: key concepts and frameworks", "content": "First and foremost, we introduce a few basic building blocks of our approach. The simple question with which we start is this:\nWhat makes an AI system a moral subject11?\nAccording to the standard\u00b92 account, called pathocentrism (Metzinger 2021a), a being's moral status depends upon its capacity to suffer. This means that at the very least, in order to receive moral consideration, AI systems must be conscious or sentient.\nConsciousness (or \u201cphenomenal consciousness\"; Block 1995) is the capacity for subjective experience, or the ability to appreciate qualitative properties, such as what it's like to see red (Jackson 1982; cf. Robinson 1982), to taste sweetness, to be a bat (Nagel 1974), etc.\nSentience (or \"affective sentience\"; Powell and Mikhalevich 2021) is the capacity to undergo good (positively valenced\") or bad (\"negatively valenced\") states\u2014i.e., to feel pleasure and pain (Browning and Birch 2022).\nTheorists disagree as to whether consciousness or sentience is more fundamental-whether it is possible to be conscious but not sentient, or vice versa (Dawkins 2008; cf. Metzinger 2021a); or whether consciousness or sentience is what really matters to moral patienthood (Ladak 2023; Milli\u00e8re 2023; Shepherd 2023, 2024). Furthermore, there is little consistency on how these terms are applied in the literature. Some authors use them interchangeably (Chalmers 2023), others don't (Dawkins 2008; Ladak 2021). Herein, we strive to remain neutral on these debates. While we do opt to use the term \"consciousness\", we do not believe that this implies a commitment to its being the true ground of moral status\u00b9\u00b3. We are confident that many of our conclusions will still stand even if sentience, rather than consciousness, turns out to be the principal criterion of moral status.\nThe flagship appeal of the pathocentric rubric is that it enables ethical and legal frameworks to draw from philosophical and scientific investigations of consciousness. While this connection can be advantageous, it is not also without serious drawbacks\u2014one cannot pick and choose what to inherit from the philosophy and science of consciousness. In order to be of any practical guidance, pathocentrism requires the resolution of a long-standing philosophical and scientific problem: how to determine which sorts of beings (organisms, Als) are conscious [in the morally relevant sense(s)] (Allen and Trestman 2024). Suffice to say, this is no easy task. In the next section, we discuss a qualifying issue: whether or not it is even possible for Als to be conscious at all. Afterward, we briefly canvas reasons for and against building conscious AI."}, {"title": "2.2 Is it even possible for machines to be conscious?", "content": "In order to even consider the possibility that AI can be conscious at all, it is necessary to endorse some degree of substrate neutrality (Bostrom 2003; Butlin et al. 2023; Jarow 2024) or <hardware independence\u00bb\u2014what in philosophy has sometimes been called \"multiple realisability\" (Putnam 1967; cf. Coelho Mollo forthcoming): the view that different kinds of things can be conscious regardless of what they are made of (e.g. carbon, silicon, etc.) or whether they are living or nonliving. The opposite of this view is biological essentialism: the view that only living things can be conscious14 (Aru et al. 2023; Godfrey-Smith 2023; Seth 2023). This may be because certain biological structures (e.g. neuroprotein; Block 1978; Searle 2000; though see Schwitzgebel 2015) or processes (e.g. metabolism) are necessary for consciousness to arise (Young and Pigott 1999; Sebo and Long 2023). Strong biological essentialist views categorically rule out conscious AI.\nIn this work, we adopt a position closer to substrate neutrality: we remain open to the possibility that Als may suffer and hence even merit moral consideration\u00b95. There are two reasons for this. First, while it is true that everything that we know best to be conscious are living things (e.g. humans and other animals), there is no obvious reason why the structures or functions that make living things conscious couldn't also be realised in artificial, non-living things like AI16 (Dehaene et al. 2017). Second, versions of the substrate neutrality doctrine are the prevailing orthodoxy across contemporary philosophy and psychology (Kim 1989; Bourget and Chalmers 2023). Many researchers studying human, animal, and artificial minds subscribe to views like functionalism, which permit artificial consciousness."}, {"title": "2.3 AI suffering: a significant consideration against building conscious AI", "content": "Suppose we grant that it is in principle possible for AIs to be conscious\u2014what then? If we create conscious Als, how likely is it that they will suffer? As it happens, conscious Al may be subjected to a range of adverse conditions in the service of human interests\u00b97. Examples include 18:\nTorturous scientific/medical experimentation: Conscious AI may be used to simulate psychiatric conditions to model their long-term course (Metzinger 2021b).\nEnslavement: Conscious AI may be forced into labour with neither compensation nor rest (Bryson 2010).\nCaregiver stress: Conscious AI may perform caretaker roles, such as Al companions and therapists. In this capacity, AI may experience significant stress due to constant exposure to negative thoughts and emotions, the emotional labour required to perform this role, and the parasocial19 nature of AI caretakers.\nAbuse for entertainment: Conscious AI may be exploited for amusement.\nHumans may cause Als to suffer through either malice, prejudice, indifference, or pure ignorance. As with animals, humans may recognise that AI is conscious, but not care enough to make trade-offs to alleviate their suffering (Anthis and Ladak 2022; \u00a75.2.1). We may judge Al's moral status to be lower than that of humans, animals, and/or living things in general, and discount their interests accordingly. Alternatively, AI suffering may be wholly inadvertent. Humans may be genuinely ignorant of the fact that AI is actually conscious (e.g. because our methods of testing for consciousness are not sufficiently sensitive; \u00a75.1.3). This last factor\u2014ignorance or anthropocentric na\u00efvet\u00e9 is especially concerning since it means that AI suffering may not look like how we expect it to-we may not know it when we see it, and this makes it very hard to develop specific plans to reduce the risk of AI suffering."}, {"title": "2.3.1 The precautionary principle", "content": "These latter worries underwrite a precautionary approach to conscious AI. First proposed in the context of animal welfare (Birch 2017), the precautionary principle prescribes a permissive approach to identifying conscious beings and recognising them as moral patients. This means that we need not be certain nor even confident that an AI is conscious in order for it to merit moral consideration. Under the precautionary rubric, Als qualify for moral consideration if there is a non-negligible chance that they are actually conscious (Sebo and Long 2023).\nMany theorists believe that it's better to err on the side of caution when attributing consciousness (Birch 2017): we are liable to do more harm by failing to recognise an Al system as conscious (a false negative) than by incorrectly attributing consciousness to it (a false positive). We can explicate this reasoning in terms of an equation such as the following:\n net suffering risk = p(C) \u00d7n\u00d7d\nwhere p(C) expresses the probability that a type of AI system is conscious in a certain sense, n the size of the relevant AI population, and d the degree of suffering to which they might be subjected. Notably, even if p(C) is low, both n and d may be high (Ladak 2021; Sebo 2023a on the \"rebugnant conclusion\"; Dung 2023a). Future technologies could make it cheap and feasible to create many copies of digital beings (Shulman and Bostrom 2021). As a result, future AI populations may number in the billions, and, moreover, they may be subjected to adverse conditions. As previously mentioned, researchers may create millions of AI models to simulate the clinical course of depression and to study its effects (Metzinger 2021b). In any case, even if there is a small chance\u00b2\u00b3\u00b9 that Al systems are capable of suffering, we should extend them moral consideration (Sebo and Long 2023)."}, {"title": "2.4 Which AI systems? What sort of moral responsibilities?", "content": "Today, there exists a variety of AI systems which differ substantially in their functional architectures, physical embodiments, and capabilities. In the future, we can expect an even greater diversity. On top of this, there is emerging consensus that consciousness itself may admit of multiple qualitatively distinct dimensions22 (Birch et al. 2020; Ladak 2021), such that it would not make sense to speak of different creatures as \u201cmore\u201d or \u201cless\u201d conscious. Like animals, Al systems may well be conscious in different ways23. Thus, there is no single question of AI consciousness.\nLikewise, there is no single question of AI moral status (Hildt 2022; cf. Grimm and Hartnack 2013). In virtue of the above-mentioned differences, different AI systems are also likely to present diverse needs and interests. To quote Shulman and Bostrom (2021; emphasis ours):\n\"Digital minds come in many varieties. Some of them would be more different from one another than a human mind is to that of a cat. If a digital mind is constituted very differently than human minds, it would not be surprising if our moral duties towards it would differ from the duties we owe to other human beings; and so treating it differently need not be objectionably discriminatory.\u201d\nIf anything, the discussion of AI rights24 and protections will likely have to be relativised to different broad categories of AI systems-much like how today, there exists a diversity of legal entities with unique sets of privileges and/or responsibilities (Compare, e.g., an adult human, an unborn foetus, a corporation, a lobster, and a chimpanzee). While the precise nature of AI rights (and possibly, responsibilities\u00b2) might depend upon future technological advances and societal developments, we are arguably at a point where we can and should be thinking about the possibility and general form of such concessions."}, {"title": "3. Taxonomising AI consciousness and public perception", "content": "Based on our limited knowledge of the mechanistic underpinnings of consciousness, we are presented with a moral challenge on both a factual and an epistemic level. Our current factual situation allows for the possibility that Als become conscious, and, according to pathocentrism, this allows for the possibility that Als become moral patients. Conditioned on whether society believes Als to be conscious, or on the epistemic question of consciousness, we are presented with four scenarios with respect to Als' moral status:\n1. True positive: We correctly regard AI systems as moral patients\n2.False positive: We incorrectly regard AI systems as moral patients\n3. True negative: We correctly disregard AI systems as moral patients\n4. False negative: We incorrectly disregard AI systems as moral patients\nThis comprises the basis of our paper-we tackle the problem of AI consciousness by dividing the future into four quadrants (table 1, over). To the best of our knowledge, no other work has taken a similar systematic approach to this problem with the closest exception being Berg et al.'s very recent (2024a) and (2024b). Berg et al. employ the same possibility space to probe different societal attitudes towards potentially conscious Als. They conclude that, provided uncertainty about how consciousness actually works, it is best to proceed as if potentially conscious Als actually are conscious and deserving of moral consideration rather than assuming the opposite (ibid).\nWe give binary answers to the two questions we pose in order to categorise the future in a simple and clear way but we recognise that both dimensions will be more nuanced. Along the epistemic axis, there is no guarantee that human opinions will converge, or that they will apply to all Al systems. On one hand, public opinion could be polarised, or otherwise distributed across multiple stances. Alternatively, people could interact with some systems more than others, such as Al companions with prosocial abilities, which might unwarrantedly shift the saliency of the question of consciousness to this specific kind of systems. Finally, similarly to the case with animals, society at large might not care about AI suffering. Along the factual axis, it might be the case that (i) only some Als are conscious, (ii) different types of AI systems are conscious in different ways (Hildt 2022; cf. Birch et al. 2020), and/or (iii) to different degrees ($2.4) (notwithstanding these degrees of freedom, qualifying as consciousness could still be a binary matter).\nWe address these nuances throughout the paper, including the illustrative examples of how the four scenarios might play out, as well as the specific risks."}, {"title": "3.1 Society believes AI is conscious: true positive and false positive", "content": "Let's first consider the positive belief scenarios: true positive and false positive. In these cases, human society at large regards Als as conscious. Thus, both scenarios are more likely to involve institutional recognition of AI consciousness and hence legal protections and rights (although this is not a given). In the false positive scenario these various privileges are fundamentally unwarranted, since Als actually lack consciousness or sentience (e.g. they are \u201cP-zombies\u201d26; Chalmers 1996). Regardless of the sentience of Als, if they are misaligned or ill-intentioned towards humans, having rights might make it easier for them to achieve goals that conflict with human interests, which can lead to the disempowerment of humans ($4.2). Furthermore, the welfare of actual moral patients could be compromised in the service of Als' needs (whether illusory or not). Finally, one could envision the rise of ideological disagreements and the rise of anti-Al factions, leading to geopolitical instability ($4.3).\nThe true positive scenario implies additional societal challenges. For one, future populations of conscious AI might vastly outnumber future populations of humans (Shulman and Bostrom 2021). What's more, it is possible that advanced AI might become \u201csuper-beneficiaries\u201d\u2014entities which can derive greater utility per unit of resource than humans27. Either development could lead to a disproportionate, yet morally justified claim on our planet's limited resources so much so that the respective allocation for humans falls below the subsistence level. It would be morally justified based on utilitarian or equality principles, which aim to maximise overall well-being. However, this could come to the disadvantage of humans, since it's difficult to balance the interests of entities with different well-being capacities."}, {"title": "3.2 Society doesn't believe Al is conscious: true negative and false negative", "content": "We now turn to the negative belief scenarios, which are characterised by widespread disbelief about the consciousness and hence moral status of AIs. Both scenarios entail no protections of Als' interests-Als are used as tools. In both quadrants, our treatment of Als as objects could translate negatively to our relationships with actual moral patients (Darling 2016) (\u00a74.4). Both quadrants imply human disempowerment risk: in the true negative case this is most likely to play out via misalignment (\u00a74.2), while in the true negative case there is a possibility that Als decide to retaliate against their oppression and initiate armed conflicts against human society.\nThe key difference between the two quadrants is the risk of AI suffering ($4.1), which is very significant in the false negative case. Als will likely feel harmed, abused, and enslaved if humans give no consideration to their subjective experiences while training them, interacting with them and using them.\nThe true negative scenario is notably the most likely the situation we are currently in. By not intentionally building conscious AI, we have a good chance at remaining in this quadrant, unless we do so accidentally."}, {"title": "3.3 Vignettes", "content": "In order to more vividly envision how these scenarios may play out, we outline a non-exhaustive list of vignettes describing various ways society may respond to this issue."}, {"title": "3.3.1 Prevailing positive beliefs about AI consciousness", "content": "A. AI as peers we peacefully cohabitate with: People come to believe Als are conscious (experts may or may not agree with this), generally treat them with respect, and support measures to protect their welfare. This could split off into further sub-branches:\na. Als as equals: Als may have the right to vote and own property, as well as other legal rights. Romantic relationships with Als are normalised, and marriage with AI may even be legalised in some places. Als are considered moral patients on par with humans, and society devotes significant resources to their interests. Those who are opposed to this are regarded as bigoted.\nb. Als as beings subservient to humans: Most people treat Als nicely, but do not regard them as equals. In other words, they believe that Als do merit some level of respect, but that their purpose is ultimately to serve humans. Als might have very basic legal protections (e.g. protection from cruelty). While tolerated, romantic relationships with AI are generally viewed as abnormal.\nB. AI as farmed animals: Despite many people believing that AI is conscious, their [potential] welfare remains of little concern to most outside of a small minority (analogous to vegan activists). Conscious Als are routinely subjected to inhumane conditions, and society is unwilling to take meaningful action to protect their welfare.\nC. Als as idols of worship: In awe of its superhuman abilities, humanity develops a divine admiration for advanced AI. Believers become convinced as some in the \"effective accelerationism\" movement already are (Roose 2023)\u2014that superintelligent AI is humanity's natural and rightful successor, and begin to allocate significant resources to its flourishing, possibly at the expense of the thriving of humans."}, {"title": "3.3.2 Prevailing negative or mixed beliefs about AI consciousness", "content": "A. AI as tools: The idea that AI is conscious is a fringe opinion; most people, including experts, believe AI is not conscious. Therefore, we continue using them as tools. Naturally, people may sometimes anthropomorphize them, but, on the whole, even advanced Als are thought to be no more conscious than laptops and phones are today. This is probably the closest to the present day.\nB. AI welfare as a \u201cculture war\u201d issue: Different demographics have different beliefs about whether Al is conscious and whether/how much its welfare ought to be considered. Given the prominent role of AI in public life, this becomes a heated political topic, and polarisation makes it a hard topic to make progress on. Some people (perhaps tech enthusiasts, progressives, or people who feel emotionally bonded with AI ($5.2.1) advocate for granting Al rights; others reject the idea that Als are conscious and insist they should be treated as tools; still others agree Als are conscious, but believe that instead of giving them rights, they should be banned.\nC. Conscious Als as lab rats: We develop conscious AI, but it is not mass deployed, either because it is illegal or because AI labs have moral/reputational concerns. Therefore, conscious AI only exists inside top AI labs, and is used to perform experiments about consciousness or related topics.\nD. Conscious AI as a black market: Transgressing a moratorium on the development of conscious AI ($6.3), a rogue actor builds it anyway, and makes it publicly accessible. Most people think using this Al is unethical, but criminals or nefarious actors still have access to it, and perhaps use it due to its possible unique features ($A1)."}, {"title": "4. Risks posed by the perception of AI consciousness", "content": "On our analysis, the 2D framework is dominated by 4 major risks:\n1. AI suffering: Vast populations of AI are subjected to severe distress and pain, possibly due to various human-imposed conditions.\n2. Human disempowerment: Human autonomy and/or dominance are undermined due to lack of cooperation with Als, AI exploitation of human trust, and/or retaliation.\n3. Geopolitical instability: Near-term economic crisis, civil unrest, and/or armed conflict.\n4. Depravity: Inhumane treatment of Als causes spill-over effects, negatively impacting human-human relations."}, {"title": "4.1 AI suffering", "content": "On our framework, AI suffering emerges as the most significant risk. As mentioned earlier (\u00a72.3), this is due to the scale of AI populations and the degree of suffering at stake.\ni. Scale of AI populations: In the future, it could be feasible to arbitrarily generate many copies of Als. As a result, AI populations may rapidly achieve historically unprecedented, \"astronomical\" scales (Shulman and Bostrom 2021), outnumbering the number of contemporaneous living humans.\nii. Degree of suffering: AI may be purposed for a variety of painful and distressing ends that benefit humans. These include: torturous scientific experimentation (e.g. simulating psychiatric conditions to model their long-term course; Metzinger 2021b)28, enslavement including exploitation for entertainment purposes, and caregiver stress; as well as fear of deactivation or reset or identity crisis due to repeated revision of core parameters\u2014in addition to any number of unforeseen\u2014and importantly, non-anthropomorphised29 harms (Metzinger 2022). Given the complexity of AI technologies and the synthetic nature of AI consciousness, it is exceedingly difficult to anticipate the full range of harms that conscious AI might suffer. The potential for AI suffering is limited only by our imagination.\nIn short, future AI populations may number in the billions, and they may be subjected to harrowing conditions. The risk of AI suffering is likely limited to those scenarios where AI is actually conscious: true positive and false negative. Between these two, the risk is highest in false negative one because in this case, human society largely fails to recognise Als as conscious. In this case, conscious Als are least likely to enjoy any legal protections whatsoever, and are most likely to be exploited. Having said that, we leave open the possibility of AI suffering even in the scenarios where AI is not conscious. This is because suffering may turn out not to actually require phenomenal consciousness. Thus, we rate the risk of Al suffering as low in the true negative and false positive scenarios, rather than ruling it out entirely."}, {"title": "4.2 Human disempowerment", "content": "Human disempowerment is the risk of humans losing their current autonomy and dominance in relation to other beings and the environment. We identify different routes to this risk across the four scenarios, dependent on the two axes of our 2D framework, as well as the possible link between consciousness and alignment."}, {"title": "4.2.1 AI is perceived as non-conscious", "content": "In the case of scenarios where AI is non-conscious, human disempowerment is equal to the risk of loss of control (e.g. Bostrom 2014). This is the idea that as systems become more and more intelligent, humans become incapable of steering their actions and this leads to Als dominating over humans. One way to ensure that Als do not make actions which are against humans' interests is to align their values with human values, which is currently very difficult to do and is known as the alignment problem (Christian 2020).\nApart from the alignment problem, in the case of actual conscious Als, we are faced with the possibility for retaliation\u2014the possibility for conscious Als to want to respond to their maltreatment in harmful ways. Note that this is also possible if we perceive them as conscious but nevertheless treat them badly.\nThere may be a link between consciousness and moral knowledge30 (Shepherd and Levy 2020) which makes it possible for consciousness to correlate with alignment. This is highly uncertain but we could factor it into our assessment. If consciousness can help solve the alignment problem, there is a lower probability for misalignment risk in the false negative case, compared to the true negative case, but the risk for retaliation remains (compare columns 3 and 6 in table 2). Therefore, the false negative case might have lower likelihood for disempowerment compared to the true negative in the case of alignment. On the other hand, its likelihood is higher in the case of misalignment, given the possibility for both misaligned behaviour and retaliation (compare columns 4 and 6 in table 2). We can therefore roughly consider the true and false negative scenarios of equal risk."}, {"title": "4.2.2 AI is perceived as conscious", "content": "If we recognise Als as beings with moral status (true positive scenario)", "ways": "n1.Cooperation failures\na. Humans fail to protect Als' needs without sacrificing their own needs. This threat model implies Als are not moral agents. It can be realised mainly in the false positive scenario.\nb. Humans and Als fail to cooperate on economic and societal matters. This threat model implies benevolent (i.e. aligned) AIs as moral agents. It can be realised mainly in the true positive scenario.\n2. Exploitation of trust\na. Als deceive humans and/or engage in dangerous behaviours under the disguise of their protections (Hendrycks et al. 2023). This threat model implies malevolent (i.e. misaligned) AIs as moral agents. It can be realised in the true positive scenario and the false positive scenario.\nThe case of \"super-beneficiaries\"", "allocation": "how many more resources should society allocate to the super-beneficiaries? On utilitarian grounds, the view is that humans should be allocated much less. However, it is difficult to see how humans will retain some level of autonomy in this scenario.\nBetween the cases where AI is actually non-conscious, the possibility for cooperation failures and exploitation of trust makes the false positive case higher on disempowerment risk compared to the true negative one (compare columns 5 and 6 in table 2). Between the cases where AI is likely given rights, the true positive is overall higher risk than the false positive in the case of no consciousness-alignment correlation, because, additionally to the risks related to Al rights, it also holds the risk for retaliation (compare columns 2 and 5 in table 2). If conscious Als can more easily become aligned, the true positive scenario becomes overall lower risk than the false positive one (columns 1 and 5).\nFollowing the analysis of the different routes to human disempowerment, we identify the risk to be the lowest amongst the scenarios in the true negative case (we still label it \u201cMedium\u201d in table 2 because of the high severity of the risk in general). The false negative scenario is of comparably the same risk, while the two positive belief scenarios are of higher risk (labelled as \"High\" in table 2). The analysis identifies the presence of Al rights as a"}]}