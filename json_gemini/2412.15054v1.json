{"title": "GIRAFE: GLOTTAL IMAGING DATASET FOR ADVANCED SEGMENTATION, ANALYSIS, AND FACILITATIVE PLAYBACKS EVALUATION", "authors": ["Gustavo Andrade-Miranda", "Konstantinos Chatzipapas", "Juli\u00e1n D. Arias-Londo\u00f1o", "Juan I. Godino-Llorente"], "abstract": "The advances in the development of Facilitative Playbacks extracted from High-Speed videoendo- scopic sequences of the vocal folds are hindered by a notable lack of publicly available datasets annotated with the semantic segmentations corresponding to the area of the glottal gap. This fact also limits the reproducibility and further exploration of existing research in this field.", "sections": [{"title": "Background", "content": "Voice is fundamental to our daily interactions, serving as the primary means of communication and enabling us to express emotions, thoughts, and cultural identities [1]. For professionals such as teachers, singers, and public speakers, maintaining a healthy voice is essential for their careers [2, 3, 4, 5]. On average, a person speaks approximately 16,000 words daily, highlighting the extensive use of our vocal folds [6]. However, excessive or inappropriate use can lead to voice disorders, such as vocal folds nodules, polyps, laryngitis, or vocal fold paralysis, affecting the quality of life and professional performance [7, 8, 9]. In this context, clinical voice assessment tools are vital for diagnosing and evaluating these disorders, as they provide a comprehensive and objective analysis of vocal function through techniques such as laryngeal endoscopic imaging, acoustic measurements, and aerodynamic evaluations [10]. Among these, laryngeal endoscopic imaging is particularly relevant to distinguish between organic and functional voice disorders, underscoring the need to visually examine the vibratory characteristics of the vocal folds to ensure an accurate diagnosis [11, 12, 13, 14, 15, 16].\nAmong the techniques based on endoscopic imaging, laryngeal high-speed videoendoscopy (HSV) -with sampling frequencies up to 4,000 fps-, is an imaging technique that has shown to be of clinical interest due to its ability to accurately capture the fast dynamics of the vocal folds motion. However, despite its reliability, this technique is not exempt from drawbacks, since it generates a vast amount of data, requires specific and costly hardware, and additional methods to postprocess the HSV sequences recorded. This fact has led to the development of a set of derived techniques that, applied to the HSV sequences, are aimed to synthesize and represent certain characteristics of the phonation process in a more manageable single image and/or time-varying sequence. These techniques are called Facilitative Playbacks (FP) [17, 18, 19], and their synthesis is usually based on a previous automatic or semi- automatic segmentation of the glottal area [20, 21, 22]. Examples of different FP are: glottal area waveforms (GAW) [1], glottovibrograms (GVG) [23], phonovibrograms (PVG) [24], and digital kymograms (DKG) [25, 26]. The utility of these FP is widely reported in the literature, being essential to evaluate certain aspects of the phonation, including, among others, the periodicity and amplitude of vibration, the presence and characteristics of the mucosal wave (MW), the characteristics of the glottal closure, the duration of the closed phase, the analysis of the symmetry of the vibration pattern, and the identification of non-vibrating instants [10, 1]. \nBased on an analysis of HSV video sequences and more specifically of certain FP extracted from them, previous studies have shown that the presence of asymmetries, transients, breaks, and irregularities in the vibration of the vocal folds is indicative of the presence of certain phonatory diseases [27, 28, 29, 30, 31]. For example, the authors in [32, 33, 34] studied several representations of the GAW, identifying a set of novel quantitative metrics to assess the regularity of the vibration of the vocal folds during phonation. Other studies analyzed acoustic signals and GAW features to establish their mathematical relationships, aiming to identify the parameters that most effectively described the correlation between the characteristics of the GAW and the corresponding acoustic signals [35].\nSimilarly, an atypical magnitude and asymmetry of the MW during the vocal folds vibration have been linked to the presence of voice disorders [12, 13, 36]. The MW visualized in the HSV sequences, and more specifically using the FP, has been used to diagnose vocal folds lesions and to evaluate treatment outcomes, including the effects of hydration and mucosal healing after phonosurgery [37, 38]. Furthermore, the appearance of the MW is recognized as a key component in the myoelastic-aerodynamic theory of phonation, which explains the mechanisms underlying self-sustained vocal fold oscillations [39, 40, 41]."}, {"title": "Data Description", "content": "GIRAFE aims to contribute with open data but also with open software to provide a robust and reliable baseline for the semantic segmentation of the glottal gap and for the evaluation of FP. To achieve this, it includes a diverse set of recordings collected at the facilities of the Otorhinolaryngology Service of the Gregorio Mara\u00f1on Hospital in Madrid, which were carefully delineated with their corresponding manual annotation masks. To validate the benchmark data and establish a baseline score, GIRAFE includes segmentation results obtained using two well-established image processing techniques: InP [22] and Loh [20]. Additionally, to keep pace with advancements in the field, two state-of- the-art DL methods were used in this paper, namely: UNet [63] and SwinUnetV2 [64], both trained using the GIRAFE dataset.\nThe repository contains raw HSV recordings in AVI format for each patient, accompanied by metadata in JSON for- mat. The glottal segmentation and FP results from all approaches, including the manual segmentation, are saved in PKL Phyton\u00ae format. The dataset also includes 760 images in PNG format with the corresponding manual annota- tions, along with the default splitting used for all DL models in JSON format. The complete workflow of GIRAFE, encompassing data collection, annotation, and validation processes, is shown in Fig.2.\nThis section is organized as follows: First, we outline the data collection strategy, specifying the number of recordings and the population involved. Next, we describe the data acquisition process and its characteristics, including the devices used and the clinical findings. We then explain the data labelling and evaluation methods, including the segmentation techniques and the use of FP. Finally, we address the ethical considerations and relevant declarations."}, {"title": "Data collection.", "content": "To achieve high temporal resolution and accurately capture the fast motion of the vocal folds, recordings were per- formed using HSV. This technique allows the diagnosis of subtle vibratory patterns and anomalies undetectable with techniques based on standard laryngeal videostroboscopy (VS) [65, 66].\nThe population selected to create GIRAFE was meticulously chosen so the dataset includes recordings showing intricate details of the vocal folds motion, which is crucial for understanding various phonatory mechanisms and pathologies. By capturing the true intra-cycle vibratory behaviour of the vocal folds, the dataset visualizes aperiodic movements and transient phonatory events, such as phonatory breaks, laryngeal spasms, and the onset and offset of phonation [67, 44, 31].\nThe HSV recordings were carried out by a specialist of the Otorhinolaryngology Service of Gregorio Mara\u00f1\u00f3n Hospi- tal. The database comprises 65 recordings from 50 patients: 30 females (60%) and 20 males (40%), with a mean age of 55.65 \u00b1 19.35 years, ranging from 25 to 101. The recordings capture a wide range of vocal fold behaviours, from normal function to various pathologies, providing a valuable resource for studying the biomechanics of voice produc- tion and disorders. This diversity enhances the dataset's applicability, making it a robust tool for training deep-learning segmentation models. All patients were native Spanish speakers and adhered to the same experimental protocol."}, {"title": "Data Acquisition and properties.", "content": "The HSV sequences were acquired using the WOLF\u00ae HRES ENDOCAM 5562 camera system and a rigid endoscope with a 70-degree angle of view. The light source was the WOLF AUTO LP 5132. The recordings exhibit varying levels of illumination, contrast, partial occlusion of the glottis, and lateral displacements of the camera, providing a comprehensive resource for robust analysis under diverse conditions. Some key features of the recordings are highlighted as follows:\n\u2022 Each sequence contains 502 frames, resulting in 32,630 images available for analysis. This extensive frame count enables thorough temporal analysis, the study of dynamic changes over time, and the evaluation of different segmentation models.\n\u2022 The videos capture a sustained vowel phonation, including, in some cases, the vocal onset, providing valuable data on phonatory behavior. This aspect is particularly important for studying the biomechanics of voice production and the initiation of phonation, which are critical for diagnosing voice disorders.\n\u2022 The sampling rate was 4,000 fps, and the spatial resolution 256 \u00d7 256 pixels. Such a high frame rate is necessary to accurately capture the fast dynamics of vocal fold motion, while spatial resolution provides sufficient detail for identifying anatomical landmarks and pathologies.\n\u2022 The distance between the camera head in the oropharynx and the vocal folds varies, reflecting real-world clinical conditions. This variability enhances the dataset's robustness for developing and testing semantic segmentation algorithms.\n\u2022 All sequences were recorded in color, enhancing the visibility of anatomical features and pathologies. Color recordings facilitate better differentiation of tissue types and more accurate identification of pathological changes.\nThe database includes voiced sounds in laryngeal mechanism M1 [5], along with a variety of vocal fold pathologies. Health status information was unavailable for 24 subjects."}, {"title": "Data labelling, segmentation, and facilitative playbacks", "content": "An expert in glottis segmentation performed the manual delineations for the dataset, which were subsequently verified by an otolaryngologist. Manual segmentation was conducted on 38 out of the 65 videos, encompassing a complete glottal cycle per patient and resulting in 760 segmented frames. These segmentations were further validated using two established image processing techniques: InP [22] and Loh [20].\nThe detailed segmentation process and FP generation are described as follows:\n\u2022 The InP method [22] proposed an automatic segmentation method based on two steps. First, the Region of In- terest (ROI) (i.e., a broad area including the glottal gap) is automatically detected by evaluating the interframe intensity variation. Second, the glottis is delineated by combining background subtraction, inpainting, and a technique based on active contours. We applied this procedure to segment the complete dataset automatically. However, in cases where no intensity variation was observed, such as in paresis or partial paralysis (27 out of 65 cases), the first step failed. In these instances, the ROI was manually delineated and then proceeded with the segmentation."}, {"title": "Data Organization", "content": "The structural organization of the dataset is shown in Fig. 5. It comprises 32.8 GB of data, including 65 HSV recordings with their respective metadata and results obtained using traditional image processing methods.\nThe dataset is divided into three directories: \\Raw_Data, \\Seg_FP-Results, and \\Training. The following sections offer a detailed overview of the contents of each directory."}, {"title": "The \\Raw_Data directory", "content": "The \\Raw_Data directory contains 65 folders (one per exploration), each containing an HSV recording. These folders are labeled with a unique ID in the format \\Raw_Data\\patientID. Each folder contains the individual patient's recordings. This directory contains several patients with multiple recordings, taken on the same day or on different dates. For example, patient 51 has two recordings from the same day (stored in \\Raw_Data\\patient51A1 and \\Raw_Data\\patient51A2) and one from a later date (saved in \\Raw_Data\\patient51B1).\nEach \\Raw_Data\\patientID folder contains two different files:\n\u2022 patientID.avi. This file contains a video file in uncompressed AVI format, which includes the original laryngeal HSV recording.\n\u2022 metadata.json. This file includes information about the patient, such as ID, age, sex, and disorder, as well as acquisition details, like camera type and video resolution. Furthermore, it specifies whether the recording is complemented with the manual segmentation (Man) and/or with one of the aforementioned automatic or semi-automatic segmentation techniques (InP or Loh). If Man or Loh is present, it is marked as \"Yes\"; otherwise, with \"No\". Additionally, all recordings are complemented, at least, with InP segmentations, calculated following a fully automatic or a semi-automatic procedure (labeled as \"A\" or \"SA\", respectively)."}, {"title": "The \\Seg_FP-Results directory", "content": "The \\Seg_FP-Results directory contains 2 folders. They are enumerated and presented next:"}, {"title": "The \\Training directory.", "content": "The \\Training directory is organized to facilitate the training and testing of DL semantic segmentation models. It is organized into two folders: \\Training\\imagesTr, containing the original frames; and \\Training\\labelsTr, which contains the corresponding manual segmentations. Each subfolder contains 760 images in uncompressed PNG format, corresponding to the same number of consecutive frames extracted from the original sequences.\nAdditionally, this directory also includes one file, which is described next:\n\u2022 training.json. This is a JSON file containing the distribution of patients to be used for the training and testing sets used to evaluate the performance of the DL models included in the repository (see usage notes)."}, {"title": "Clinical Data and Recording Details", "content": "Clinical data are stored in a spreadsheet file in XLSX format named dataset.xlsx. This file is placed in the root of the hierarchical structure of the corpus, and provides a summary of the clinical data collected for each patient, including details such as disease status, sex, date of birth, age, and recording date. Additionally, it includes information on the availability of contours delineated using manual (Man), automatic (Loh) and semiautomatic techniques (InP). This file is included to provide an initial, comprehensive overview of the dataset, enabling a clearer understanding of its structure and contents. Additional metadata, including camera type and resolution, are available for each video in the corresponding metadata.json files, located in the \\Raw_Data folder."}, {"title": "Experimental design, Material and Methods", "content": "Various subsets of the GIRAFE corpus have been extensively used in the past as an experimental basis for several research works [21, 22, 10, 1, 19].\nSpecifically, the dataset was widely used to evaluate the accuracy of different glottal segmentation methods and their suitability to synthesize the motion of the vocal folds using different FP [22]. The corpus was first used to objectively evaluate and compare traditional image processing techniques, such as background subtraction and active contours (InP [22]), region growing techniques (Loh [20]), and watershed transform [21]. In the same work, a subjective eval- uation was also carried out using the information provided by different FP to detect segmentation errors by analyzing the GVG, PVG, and GAW [22]. Specifically, the assessment aimed to identify discrepancies in vibratory patterns, segmentation errors in the anterior or posterior glottis, playback discontinuities, and inconsistencies in the duration of the open phase. The experiments demonstrated objective and subjective improvements in segmentation quality, readability, and shape similarity of the FP using the InP method.\nIn addition to the works mentioned above, an alternative validation was included in this work using two DL methods. This validation is not intended to be exhaustive but aims to provide new evidence of the corpus's usefulness and demonstrate an example of how to use the dataset (see the Usage Notes section). In this regard, a comparison is provided between the two aforementioned methods based on traditional image processing (InP and Loh) and two approaches based on DL (UNet and SwinUnetV2).\nTo ensure a fair and consistent comparison between the DL models, identical preprocessing steps were applied. Images were resized to 256 \u00d7 256, and segmentation was performed in RGB for both models. In line with BAGLS [60], data augmentation techniques were applied on the fly during training, including random rotation, scaling, flipping, Gaussian noise, Gaussian blur, and brightness and contrast adjustments using the MONAI Python package [69]. Both models were trained for 200 epochs with a batch size of 16, utilizing the Adam optimizer and the Dice loss function. The learning rate was set to 2 \u00d7 10\u22124, with a weight decay of 1 \u00d7 10\u22125. Out of the 38 manually segmented recordings, 30 were used for training, 4 for validation, and 4 for testing, each consisting of 20 frames.\nAlthough the results with InP demonstrate superior performance, both UNet and SwinUnetV2 exhibit consistent outcomes across all four test patients, comparable to those achieved with InP. As shown in Fig. 7, for Patient57A3, InP achieves the best performance with an average DICE score of 0.675, closely followed by UNet with a score of 0.646. SwinUnetV2 performs slightly worse due to the presence of additional artifacts near the vocal folds, leading to over-segmentation issues, as illustrated in the first row of Fig. 8. In contrast, for Patient61, UNet achieves the highest performance, followed by InP, with SwinUnetV2 closely trailing. The corresponding average DICE scores are 0.866, 0.825, and 0.822, respectively. The segmentations produced by all three models closely align with the ground truth, as shown in the second row of Fig. 8. For Patient63, InP once again outperforms the DL models, achieving an average DICE score of 0.711. This case is particularly challenging, as the glottal gap is split into two parts. As seen in the third row of Fig. 8, most models fail to accurately segment the dorsal region of the vocal folds. In the case of Patient64, the performance of all models is visually satisfactory, as shown in the fourth row of Fig. 8. Among them, SwinUnetV2 achieves the highest DICE score of 0.644, outperforming the other models. UNet, on the other hand, exhibits a significant drop in performance, with a DICE score of 0.495, likely due to over-segmentation issues similar to those observed in Patient57A3. Furthermore, as illustrated in the boxplot (Fig. 7), some frames achieve a DICE score of 0. This occurs when the segmentation models incorrectly predict a closed glottal gap, despite the gap being partially open, with a few visible pixels still present.\nThe previous results provide evidence of the interest of the corpus for the experimentation, as it encompasses a diverse range of pathologies and recording conditions, including varying levels of illumination, contrast, the presence of glottal chinks, and lateral displacements of the camera. This diversity ensures that the dataset is both challenging and representative, making it suitable for evaluating the robustness and generalization capabilities of segmentation models."}, {"title": "Usage Notes", "content": "As commented, the utility of the GIRAFE corpus has also been evaluated using two different state-of-the-art DL techniques, namely: UNet and SwinUnetV2.\nThe corresponding Python data management scripts can be found in the GitHub repository referenced in the Code Availability section. For practical guidance and a hands-on demonstration, the corpus includes a Jupyter notebook named Seg_FP-Results.ipynb, which delineates the process of reading images and their masks, as well as the process to follow in order to generate several FP visualizations. MATLAB scripts (\\Matlab_code) for generating FP from raw segmentations are also provided, with detailed instructions available in the Matlab_code.ipynb notebook.\nThe repository also provides source code for two basic experiments using the referenced DL architectures, available in the \\DL_code folder, as detailed in the Technical Validation section. This folder includes scripts for training (train.py) and testing (inference.py) the DL models, serving as a practical guide for working with the dataset and offering a foundation for further exploration and analysis with new methods. These experiments are not designed to maximize accuracy or improve state-of-the-art results but to demonstrate how the dataset can be used."}, {"title": "Ethics declaration", "content": "The study was approved by the Ethics Review Board of the Hospital Universitario Gregorio Mara\u00f1\u00f3n in Madrid (code 11/2015), following the guidelines of the Spanish Ethical Review Act. All participants filled out a questionnaire and provided written consent to join the study. Patients were fully informed of their rights, including their ability to withdraw from the study at any time. All participants were native Spanish speakers and followed the same experimental protocol. The clinical data was collected by the otolaryngologist who performed the procedures and was the only person in direct contact with the patients. To maintain anonymity, each patient was assigned a unique ID, separated from their hospital clinical history code. No personal information was shared with the researchers who had access to the dataset."}, {"title": "Data and Code availability", "content": "The GIRAFE dataset is available through a Zenodo repository [70].\nThe codes used for data processing, DL training and calculation of the FP are available on a dedicated GitHub\u00ae repository (https://github.com/Andrade-Miranda/GIRAFE).\nThe InP method is publicly accessible via the GitHub repository (https://github.com/Andrade-Miranda/ Glottal-Gap-Segmentation)."}, {"title": "CRediT Author Statement", "content": "G.A.M. and J.I.G. designed the experiment. G.A.M., K.C. and J.I.G. designed the methodology. G.A.M. and K.C. validated the experiment. D.P.S. collected the data. J.I.G. provided the resources. G.A.M. and J.I.G. wrote the initial draft version. G.A.M., J.D.A. and J.I.G. reviewed and edited the manuscript. G.A.M. and J.D.A. provided the software to analyse the data. J.D.A. and J.I.G. supervised. J.I.G. administered the project. J.I.G. acquired the funding. All authors have read and agreed to the published version of the manuscript."}, {"title": "Acknowledgements", "content": "This work was funded by the Ministry of Economy and Competitiveness of Spain (grants TEC-2012-38630-C04-01, DPI2017-83405-R1, PID2021-128469OB-I00 and TED2021-131688B-I00), and by Comunidad de Madrid, Spain. Universidad Polit\u00e9cnica de Madrid supports Juli\u00e1n D. Arias-Londo\u00f1o through a Mar\u00eda Zambrano UP2021-035 grant funded by European Union-NextGenerationEU.\nThe authors thank the Otorhinolaryngology Service of Hospital General Gregorio Mara\u00f1\u00f3n, Madrid, for the facilities provided."}, {"title": "Declaration of Competing interests", "content": "The authors declare that they have no competing interests."}]}