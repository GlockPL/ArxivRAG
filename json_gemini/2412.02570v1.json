{"title": "TAB-Fields: A Maximum Entropy Framework for Mission-Aware Adversarial Planning", "authors": ["Gokul Puthumanaillam", "Jae Hyuk Song", "Nurzhan Yesmagambet", "Shinkyu Park", "Melkior Ornik"], "abstract": "Autonomous agents operating in adversarial scenarios face a fundamental challenge: while they may know their adversaries' high-level objectives, such as reaching specific destinations within time constraints, the exact policies these adversaries will employ remain unknown. Traditional approaches address this challenge by treating the adversary's state as a partially observable element, leading to a formulation as a Partially Observable Markov Decision Process (POMDP). However, the induced belief-space dynamics in a POMDP require knowledge of the system's transition dynamics, which, in this case, depend on the adversary's unknown policy. Our key observation is that while an adversary's exact policy is unknown, their behavior is necessarily constrained by their mission objectives and the physical environment, allowing us to characterize the space of possible behaviors without assuming specific policies. In this paper, we develop Task-Aware Behavior Fields (TAB-Fields), a representation that captures adversary state distributions over time by computing the most unbiased probability distribution consistent with known constraints. We construct TAB-Fields by solving a constrained optimization problem that minimizes additional assumptions about adversary behavior beyond mission and environmental requirements. We integrate TAB-Fields with standard planning algorithms by introducing TAB-conditioned POMCP, an adaptation of Partially Observable Monte Carlo Planning. Through experiments in simulation with underwater robots and hardware implementations with ground robots, we demonstrate that our approach achieves superior performance compared to baselines that either assume specific adversary policies or neglect mission constraints altogether.", "sections": [{"title": "1. Introduction", "content": "Effective autonomy in adversarial settings remains a fundamental problem in autonomous systems. A core challenge in such settings lies in reasoning about the adversary's state and its future trajectories, especially when critical aspects of their behavior\u2014such as decision-making policies are unknown. This lack of knowledge is further complicated by environmental factors like obstacles, terrain constraints, and dynamic operational constraints.\nOne way to address this challenge is to treat the adversary's state as a partially observable element within a broader system. In this extended state space, the problem can be described as a Partially Observable Markov Decision Process (POMDP) . POMDPs enable reasoning about uncertainty through belief dynamics-probability distributions over possible states\u2014thereby facilitating structured decision-making. However, a fundamental obstacle arises in this context: the transition dynamics of the system depend on the adversary's unknown policy, making them inherently indeterminate. Traditional POMDP planning methods rely on a priori knowledge of transition dynamics, which is unavailable here.\nOur key observation is that while an adversary's exact policy is unknown, their behavior is necessarily constrained by their mission objectives and the physical environment. Building on this observation, we propose an alternative approach: instead of assuming a specific adversary policy, we characterize the entire space of possible adversary behaviors that satisfy known mission objectives and environmental constraints. The key idea behind our approach is grounded in the principle of maximum entropy \u2014among all probability distributions consistent with the given constraints, the one with the highest entropy offers the most unbiased and comprehensive representation of the current state of knowledge. Leveraging this principle, we construct a distribution over adversary states that encapsulates the uncertainty in their decision-making while remaining consistent with all available information.\nThis perspective shifts the focus from predicting specific adversarial behavior to reasoning about them in a mission-aware manner. Through this lens, we introduce Task-Aware Behavior Fields (TAB-Fields), a novel representation that encodes adversary state distributions over time using constrained entropy maximization. As shown in Figure 1, TAB-Fields capture the evolution of belief states, demonstrating their ability to focus the distribution on regions consistent with mission constraints. TAB-Fields enable us to directly integrate adversary behavior into the belief update and planning process without relying on explicit policy assumptions or extensive training data."}, {"title": "2. Related Work", "content": "The presented work intersects several research areas in adversarial planning, behavior prediction, and planning under uncertainty. We discuss and highlight how our method differs from prior work.\nPlanning Under Uncertainty. Planning in environments with uncertainty has been extensively studied within the framework of Partially Observable Markov Decision Processes (POMDPs) . Traditional POMDP solvers rely on known transition and observation models to perform belief updates and compute optimal policies. However, when the environment includes other agents with unknown policies such as adversaries-the transition dynamics become partially unknown, complicating standard POMDP approaches. Several works have extended POMDP frameworks to handle interactions with other agents. Interactive POMDPS model other agents by maintaining beliefs over their beliefs and policies, but this quickly becomes intractable due to the curse of dimensionality. Decentralized POMDPS consider multiple cooperative agents, but are less suited for adversarial settings.\nModeling Adversary Behavior. In surveillance and security domains, predicting adversary behavior is critical. Traditional methods often assume specific models of adversary policies, such as rational decision-makers optimizing a known utility function. However, these assumptions may not hold in practice, leading to ineffective strategies. To mitigate this, some approaches use learning-based methods to model adversary behavior from observed data. While effective when ample data is available, these methods struggle when observations are sparse. Robust planning methods consider worst-case scenarios without relying on specific adversary models . However, these can be overly conservative.\nMaximum Entropy Methods for Behavior Prediction. The principle of maximum entropy has been employed to model behavior under uncertainty with known constraints. In the context of prediction, maximum entropy methods have been used to model motion , where the goal is to predict likely paths based on environmental features and goal destinations. applies the idea to design policies for agents under temporal logic constraints by maximizing entropy in constrained MDPs. Maximum entropy inverse reinforcement learning (IRL) tackles this problem from a different perspective by recovering reward functions that explain observed behavior, without assuming specific policies. However, IRL requires observed trajectories for learning, which may not be available in adversarial settings.\nBelief Planning with Unknown Dynamics When transition models are partially unknown, belief planning becomes challenging. Methods like Robust MDPs and exploration-exploitation algorithms address uncertainty by optimizing for the worst-case scenario or learning the dynamics online. In the context of POMDPs, propose Monte Carlo POMDPs, where transition probabilities are sampled from a distribution to account for uncertainty. address model uncertainty by learning models during planning. extends this work to learn the transition probabilities in dynamic, time-varying POMDPs. Our approach avoids the need to learn the adversary's transition dynamics by directly computing the distribution over possible states using the maximum entropy principle and known mission constraints. Some works consider planning under model uncertainty using robust or risk-sensitive approaches . However, these methods typically do not leverage known constraints or objectives of other agents.\nOur work differs from these approaches by avoiding assumptions about adversary policies or the need for behavior data. Instead of learning from demonstrations like maximum entropy IRL or using nested belief hierarchies as in I-POMDPs, we leverage mission specifications and environmental constraints to compute adversary state distributions through maximum entropy principles. This enables efficient real-time planning without requiring extensive adversary modeling or becoming overly conservative like robust planning methods. By integrating these distributions directly into the POMDP framework, we maintain computational tractability while making informed predictions about adversary behavior based on known constraints."}, {"title": "Statement of Contributions", "content": "The primary contribution of this work is TAB-Fields, a novel representation that captures the distribution of possible adversary states through principled entropy maximization subject to mission and environmental constraints. We show how this representation can be effectively integrated with existing planners through TAB-conditioned POMCP, an adaptation that maintains computational tractability while leveraging our structured representation. Through comprehensive evaluation across diverse scenarios in both simulation and hardware experiments, we demonstrate that our approach significantly improves mission-constrained adversarial planning compared to existing methods."}, {"title": "3. Preliminaries", "content": "We consider an ego agent operating in a shared environment with an adversary. The adversary's mission objectives are known, but their exact policy and decision-making processes remain unknown. The environment contains obstacles and operational constraints that affect all agents' feasible actions. Additionally, certain areas provide full observability of the adversary, while in other areas, the adversary is unobservable\u2014a common scenario in surveillance missions where checkpoints or security cameras offer intermittent visibility.\nObjective. Our primary problem is to enable the ego agent to plan effectively in this environment without knowledge of the adversary's decision-making process, while maximizing its objectives encoded in the reward function. Given that the adversary's state is partially observable, we can formulate this as a POMDP. A POMDP typically enables planning through belief space dynamics. However, the transition dynamics of the adversary depend on its unknown policy, making the transition probabilities involving the adversary's state indeterminate\u2014complicating the application of traditional POMDP methods, which typically require known transition models for belief updates and planning. Instead of assuming a specific adversary policy-which could lead to brittle or exploitable behaviors-we seek an approach to reason about the space of possible adversary behaviors.\nPOMDP formulation. Formally, we define our problem as a POMDP tuple $(S, A, O,T, O, R, \\gamma)$. The joint state space S encompasses our autonomous agent, adversary, and the environment, with states defined as $s_t = (s_t^a, s_t^{adv}, s_e)$, where $s_t^a \\in S^a$ represents our agent's state, $s_t^{adv} \\in S^{adv}$ represents the adversary's state, and $s_e \\in S_e$ represents the static environment state. The action space A comprises all available actions for our autonomous agent. The observation space O is defined as $O_t = (o_t^a, o_t^{adv}, o_e)$, where $o_t^a$ is our agent's fully observable state, $o_t^{adv}$ represents potentially partial observations of the adversary, and $o_e$ represents environmental observations. The transition function"}, {"title": "4. Mission-Aware Adversary Behavior Representation", "content": "To enable effective belief updates and planning, we need a principled way to reason about the adversary's possible states and transitions that is consistent with their known mission objectives and environmental constraints, without assuming knowledge of their specific policies. This problem is closely related to the Schr\u00f6dinger bridge problem in stochastic processes , which seeks the most probable evolution of a system between two end-point distributions while minimizing deviation from a reference process.\nWe adopt the principle of maximum entropy , which states that among all probability distributions satisfying given constraints, the one with the highest entropy is the most unbiased representation of the current state of knowledge. In our context, this means we seek the distribution that satisfies all known mission and environmental constraints while making the minimum number of additional assumptions about the adversary's behavior.\nA trajectory of the adversary through the environment can be represented as a sequence of states $s_{0:T}^{adv} = (s_{0}^{adv},..., s_{T}^{adv})$, where $s_{t}^{adv}$ represents the adversary's state at time t. Let $Q(s_{0:T}^{adv})$ denote a reference probability distribution representing physically feasible transitions based on environmental constraints and dynamics. This reference process, similar to uncontrolled dynamics in KL control , assigns zero probability to infeasible paths (e.g., through obstacles) and encodes basic motion constraints. We seek a distribution $P(s_{0:T}^{adv})$ that incorporates mission constraints while remaining as close as possible to Q, thereby providing a prediction of the adversary's state evolution for use in belief updates. We formulate this as a constrained optimization problem:\n$\\min\\limits_{P} D_{KL}(P || Q) = \\sum\\limits_{s^{adv}_{0:T}} P(s^{adv}_{0:T}) \\log{\\frac{P(s^{adv}_{0:T})}{Q(s^{adv}_{0:T})}}$\nsubject to:\n$P(s^{adv}_{0}) = \\mu_0(s^{adv}_{0})$, (initial state)\n$E_P[f_M(s^{adv}_{0:T})] = c_M$, (mission constraints)\n$P(s^{adv}_t \\in C) = 0$, $\\forall t \\in [0, T]$, (environment constraints).\n(1)"}, {"title": "4.1. TAB-Conditioned Planning", "content": "Building on TAB-Fields, we now address how to effectively integrate them into the planning process. In our setting, the adversary's transition dynamics depend on their unknown policy, making standard POMDP planning approaches inapplicable. Instead of assuming a specific adversary policy, we use TAB-Fields as a surrogate for the unknown transition dynamics. The intuitive idea is to perform belief updates using TAB-Fields to predict the adversary's state evolution. Specifically, when a new observation $o_{t+1}^{adv}$ is received, we update our belief over the adversary's state as:\n$b_{t+1}(s_{t+1}^{adv}) = \\eta \\cdot O(o_{t+1}^{adv} | s_{t+1}^{adv}) \\cdot P(s_{t+1}^{adv} | s_t^{adv})\\cdot b_t(s_t^{adv})$\nwhere $P(s_{t+1}^{adv} | s_t^{adv})$ is the probability distribution provided by TAB-Fields and \u03b7 is a normalization constant. When no observations are available, the belief evolves according to TAB-Fields distribution."}, {"title": "TAB-POMCP", "content": "TAB-POMCP. While any POMDP solver could potentially be conditioned on TAB-Fields, we demonstrate our approach POMCP due to its ability to handle large state spaces efficiently and its natural integration with particle-based belief representations. In standard POMCP, particles representing possible states are propagated using known transition dynamics. Our TAB-conditioned variant instead uses TAB-Fields to guide particle propagation-during each simulation step, the next adversary state is sampled from the TAB-Field distribution. This ensures that simulated trajectories remain consistent with mission constraints and environmental limitations. The action selection process in TAB-POMCP remains unchanged, using UCT to balance exploration and exploitation. However, the value estimates now account for uncertainty in adversary behavior through the TAB-Field distributions rather than assumed transition models. When observations become available, particles are reweighted according to the observation likelihood, but unobserved adversary states continue to evolve according to the TAB-Fields. This approach maintains POMCP's computational efficiency while enabling planning without explicit adversary policy assumptions."}, {"title": "5. Experiments and Results", "content": "We evaluate our approach through a series of experiments: hardware implementations with ground robots followed by ablation studies in simulation to evaluate performance across larger state spaces.\nMotivating Scenario. Consider a mission where an autonomous ego vehicle must intercept an adversarial agent targeting critical infrastructure. Through intelligence, our agent knows the adversary's task which is defined by mission objectives and environmental constraints. However, the exact policy the adversary will use to execute this mission remains unknown. The agent can only observe the adversary's position when it passes through monitored checkpoints, similar to security cameras providing visibility at key locations.\nNote that this interception mission for the ego agent represents one instance of our framework. As described in Section 3, our approach maximizes a reward function capturing the ego agent's objectives. While we focus on interception throughout our experiments as a concrete example, other missions like adversary avoidance or surveillance are equally applicable."}, {"title": "5.1. Experimental Setup", "content": "Hardware platform. We implement both the autonomous agent and the adversary using TurtleBot3 Burger platforms, each equipped with an onboard computer and a LDS-01 Lidar. The platforms run ROS2 with a custom navigation package. Our experimental area includes markers providing precise localization at designated checkpoints. The environment includes obstacles creating restricted zones, while checkpoints are positioned to simulate critical areas which are monitored.\nEgo agent and adversary dynamics. The agents operate under differential drive dynamics with state vector (x, y, 0) representing position coordinates and heading angle. Control inputs are linear velocity $v \\in [0,0.22 m/s]$ and angular velocity $\\omega \\in [0,1.82 rad/s]$. A checkpoint-based observation model provides complete adversary state information only at designated locations, simulating security camera coverage at critical points.\nAdversary missions. Following the formulation in Section 3, missions are specified in natural language and are encoded into constraint tuples M defining goal states, temporal constraints, and additional requirements."}, {"title": "Ego agent mission", "content": "Ego agent mission. The ego agent aims to intercept the adversary before it reaches critical infrastructure. A reward of +50 is given for successful interception within 0.3m, while collisions incur a -30 penalty. A time step penalty of -1 encourages prompt action, and a control penalty of $-0.1(v^2 + \\omega^2)$ discourages abrupt movements."}, {"title": "5.2. Baselines", "content": "We evaluate TAB-conditioned POMCP against three baseline approaches representing different methods of handling adversary behavior uncertainty. (i) Standard POMCP (S-POMCP) represents the original algorithm without mission awareness, where adversary transitions are modeled as uniform random movements within physical constraints a common baseline that makes no assumptions about adversary behavior. (ii) Fixed-Policy POMCP (FP-POMCP) assumes the adversary follows a deterministic shortest-path policy to mission objectives, representing commonly used simplified models of goal-directed behavior. (iii) MLE-POMCP uses Maximum Likelihood Estimation to derive adversary transition probabilities from mission constraints and observed data, providing a data-driven comparison that attempts to learn adversary behavior patterns."}, {"title": "5.3. Results and Analysis", "content": "The performance of TAB-POMCP compared to the baseline methods is summarized in Table 1. TAB-POMCP consistently outperforms all baselines across all adversary mission types.\nImpact of conditioning policies on TAB-Fields. The comparison between TAB-POMCP and S-POMCP (refer Table 1) clearly demonstrates the advantages of incorporating mission constraints into the planning process. S-POMCP, which does not utilize TAB-Fields, exhibits inefficient belief updates, particularly in periods of no observation. This often leads to overly dispersed belief distributions, resulting in ineffective tracking and search patterns. This inefficiency is reflected in consistently higher StI across missions, highlighting the method's inability to effectively narrow down possible adversary states. In contrast, TAB-POMCP leverages mission constraints to focus\nbelief distributions on regions that align with the adversary's objectives, even in the absence of\nobservations. This enables more informed and targeted decision-making, leading to significantly\nhigher interception rates. Figure 3 illustrates this behavior through representative trajectories: while\nS-POMCP exhibits aimless or overly cautious search patterns, TAB-POMCP efficiently prioritizes\nhigh-likelihood regions, demonstrating the impact of mission-aware reasoning.\nComparison with alternative mission-aware approaches. The results in Table 1 provide key insights into mission-aware planning. As expected, both FP-POMCP and MLE-POMCP outperform S-POMCP, highlighting the value of incorporating mission specifications into the planning process. However, their limitations are evident when examined closely. FP-POMCP assumes deterministic, shortest-path behavior for the adversary, which makes it highly brittle in scenarios where the adversary deviates from such paths. This limitation is clearly illustrated in Figure 3, where FP-POMCP struggles to adapt to behaviors that does not follow shortest path, leading to significant tracking inefficiencies. MLE-POMCP, on the other hand, demonstrates better flexibility by learning adversary behavior patterns from data. However, as shown in Figure 3, its reliance on sufficient past observations results in poor early-mission performance. The method only improves as it gathers enough data to refine its belief, leaving a critical gap during the initial stages of the mission. In contrast, TAB-POMCP enables robust performance across all phases of the mission. Unlike FP-POMCP, TAB-POMCP does not assume specific behavior patterns anded by known constraints. Similarly, it avoids MLE-POMCP's reliance on extensive behavioral data, allowing it to excel even in sparse-data scenarios.\nScalability of TAB-conditioned planners. We evaluate the scalability of TAB-conditioned planners through high-fidelity underwater vehicle simulations using the BlueROV2 model. The environment simulates a subsea inspection scenario. The BlueROV2s operate in three-dimensional space with state vector (x, y, z, \u03c6, \u03b8, \u03c8) and corresponding velocities. Similar to the ground robot experiments,"}, {"title": "Missions", "content": "Missions the adversary is fully observable only when passing near underwater sensor networks (checkpoint), simulating acoustic or sonar detection zones.\nAs shown in Table 2, TAB-POMCP maintains its performance advantage over baselines across all the five mission types (M1-M5). The simulation results reveal critical insights about scaling TAB-conditioned policies to higher-dimensional spaces. First, the performance gap between TAB-POMCP and baselines widens as mission complexity increases, particularly in missions with complex temporal dependencies like M5. This suggests that the maximum entropy formulation becomes more valuable precisely when the search space expands. Second, even in the most complex scenarios with multiple interacting constraints (M3), TAB-POMCP maintains a 3-4x improvement in interception efficiency over methods that make explicit policy assumptions. The key driver behind this scalability is TAB-Fields' ability to automatically identify and exploit mission-constrained regions of the state space. Rather than maintaining beliefs over the full 6-DOF state space, TAB-POMCP effectively \u201ccollapses\u201d the belief to high-probability regions defined by mission constraints. This implicit dimensionality reduction enables efficient planning even as the raw state space grows.\nLimitations. Despite the performance benefits, TAB-Field generation incurs additional computational overhead. With efficient parallelized implementation, TAB-POMCP requires approximately 1.4x more computation time compared to standard POMCP. Additionally, while our current formulation handles static obstacles, it does not yet account for dynamic obstacles."}, {"title": "6. Conclusion", "content": "We presented Task-Aware Behavior Fields (TAB-Fields), a novel approach to reason about adversary behavior in scenarios where mission objectives are known but specific policies remain unknown. Our key contribution lies in recognizing that the maximum entropy principle can characterize the full space of possible adversary behaviors using just mission specifications and environmental constraints, eliminating the need for policy assumptions or hand-crafted rewards. By solving a constrained optimization problem that minimizes bias beyond known constraints, TAB-Fields provide a distribution over adversary states that captures all feasible behaviors consistent with mission objectives. When integrated with standard planning algorithms through TAB-conditioned POMCP, this representation enables effective decision-making in complex adversarial scenarios. Our experimental results demonstrate significant performance improvements over methods that either make specific policy assumptions or ignore mission constraints."}]}