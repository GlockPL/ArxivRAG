{"title": "BodyMetric: Evaluating the Realism of Human Bodies in Text-to-Image Generation", "authors": ["Nefeli Andreou", "Varsha Vivek", "Ying Wang", "Alex Vorobiov", "Tiffany Deng", "Raja Bala", "Larry Davis", "Betty Mohler Tesch"], "abstract": "Accurately generating images of human bodies from text remains a challenging problem for state of the art text-to-image models. Commonly observed body-related artifacts include extra or missing limbs, unrealistic poses, blurred body parts, etc. Currently, evaluation of such artifacts relies heavily on time-consuming human judgments, limiting the ability to benchmark models at scale. We address this by proposing BodyMetric, a learnable metric that predicts body realism in images. BodyMetric is trained on realism labels and multi-modal signals including 3D body representations inferred from the input image, and textual descriptions. In order to facilitate this approach, we design an annotation pipeline to collect expert ratings on human body realism leading to a new dataset for this task, namely, BodyRealism. Ablation studies support our architectural choices for BodyMetric and the importance of leveraging a 3D human body prior in capturing body-related artifacts in 2D images. In comparison to concurrent metrics which evaluate general user preference in images, BodyMetric specifically reflects body-related artifacts. We demonstrate the utility of BodyMetric through applications that were previously infeasible at scale. In particular, we use BodyMetric to benchmark the generation ability of text-to-image models to produce realistic human bodies. We also demonstrate the effectiveness of BodyMetric in ranking generated images based on the predicted realism scores.", "sections": [{"title": "1 Introduction", "content": "Advances in generative modeling over recent years have enabled impressive progress across many domains of image generation [1,2,3,4,5]. However, one area continues to present unique challenges - producing photorealistic human images directly from text. State-of-the-art generative models have excelled at artistic synthesis tasks but face additional difficulties when attempting to depict the complexity of human form and appearance to meet human standards of perceived authenticity. As seen in Fig. 1, this issue is notably manifested in the frequent generation of human figures with unrealistic body characteristics such as additional limbs, or abnormal body poses. While minor irregularities may go unnoticed in other generative domains, accurately depicting the human form poses a unique challenge. Even subtle deviations from typical human anatomy can negatively impact the perceived realism. This high standard arises from humans' deep-rooted ability to discern abnormal facial and anatomical features [6,7]."}, {"title": "2 Related Work", "content": "Image generation has drawn considerable attention in recent years, with significant improvements in the capabilities of the state-of-the-art models. Existing image generation models utilise architectures such as (Vector-Quantized) Variational Autoencoders (VAES, VQ-VAEs) [19,20], GANs [21], and normalizing flows (NFs) [22].\nRecently, diffusion models are used to generate images from text [23,24,4,25,4,26]. These models can capture complex data distributions, leading to better sampling quality compared to GANs. Diffusion models can be conditioned on a variety of control signals such as text or pose, using classifier [23] or classifier-free [24] guidance. Despite significant advances, these generative models can still fail to generate realistic humans in images."}, {"title": "2.2 Datasets", "content": "For our task, datasets of both real and generated images are valuable. Real image datasets like ImageNet [27] and MS COCO [28], contain images of humans with a diverse range of backgrounds along with textual descriptions. Additionally, several synthetic image datasets (DiffusionDB [29], OpenParti, HPSv2 [16], Pick-a-Pic [17]) are created using generative text-to-image models. BodyRealism includes a subset of text prompts from existing datasets which are used to generate images of humans. In addition, it contains a subset of in-the-wild images of humans, curated from MS COCO. In contrast, to the above datasets covering a wide range of categories, BodyRealism only contains images of humans. Similar to OpenParti, Pick-a-Pic and HPSv2 BodyRealism contains preference scores for each image. Different from these datasets, the scores in Body Realism are collected from expert annotators and are designed to reflect the realism of the human body in terms of anatomy."}, {"title": "2.3 Evaluation Metrics", "content": "Commonly adopted evaluation techniques for images can be classified to image fidelity, text-image alignment and user preference. The most reliable process to assess image quality is to train human annotators to rate the images in terms of visual quality or text alignment. However, this can be a cumbersome and time demanding process and the quality heavily depends on the expertise, background and clarity of annotation instructions.\nImage Fidelity Image fidelity metrics can be broadly characterised to those utilising low-level image features or deep features. Metrics based on low-level image features, such as SSIM [30] and PSRN, lack semantic context or may assume pixel-wise independence, thus, failing to properly capture image fidelity. Since deep visual representations obtained from large pre-trained models have been found to carry semantic knowledge [31], they were used to define metrics such as the Inception Score (IS) [11] and Fr\u00e9chet Inception Distance (FID) [10]. Yet, existing works challenge the credibility of such metrics for non-ImageNet datasets [32,33]. We argue that the diverse information encoded by visual features (background, shadows, and objects), hinders body realism scoring, and hypothesize that a targeted method focusing on the body features would likely yield superior results for this task.\nText-Image Alignment Widely adopted metrics, utilise CLIP as their backbone, since it has been found to be a good candidate for reference-free evaluation of images. In particular, CLIPScore [12] defines the text-image alignment based on the cosine similarity of the CLIP text and image embeddings without using any reference images, while RefCLIPScore [12] achieves a higher text-image correlation by utilising reference images when available. Instead of using CLIP, TIFA [13] utilises visual question answering to capture text-image relevance. In particular a large language model (LLM) is used to create question-answer pairs based on the text prompt and assumes that a good image candidate should be able to provide accurate answers to the questions using VQA. While this technique works well for coarse image features, it might struggle to capture finer details which relate to fingers and blurriness. Furthermore, since TIFA is bounded by the LLM it can be difficult to formulate questions relevant to human"}, {"title": "5", "content": "We strive for a dataset consisting of a diverse range of human poses and actions by formulating the prompts accordingly. Other aspects of human diversity (such as ethnicity, gender etc) largely depend on generative capabilities of the models that are used to produce the images. In order to train a learnable metric specifically tailored to assess anatomy-related artifacts, we associate each image with a combination of human annotations and multi-modal signals. Specifically, we tag each text-image pair with body realism annotations, and body-prior information that is leveraged to explicitly introduce the notion of human anatomy into our metric. Formally, BodyRealism Dataset is defined as a set of tuples D = (x,y,r,b) where x, y, r, b correspond respectively to text prompt, images, realism scores and 3D body representation. These are described in detail next."}, {"title": "3.1 Text-Image Pairs", "content": "Text Prompts To generate the images we define a set of text prompts curated from existing text-image datasets (e.g., ImageNet, MS COCO, TiFa, Pick-a-Pic, DiffusionDB, OpenParti). Since we are interested in generating images with humans, we filter the text prompts accordingly.\nImage Generation We utilise SOTA models such as stable diffusion variants[4,8,9] to generate images from text prompts. We incorporate negative prompts (such as \"black and white\", \"sepia\") during generation to overcome the biases of models to predominantly generate images of a particular style. While negative prompts improve the overall aesthetics of generated images, we find that they are not successful in improving body related artifacts. We show this by generating images with negative prompts related to body realism (such as \u201cdeformed body\u201d) and find that even with highly crafted prompts, most T2I models consistently produce unrealistic human bodies (see Fig. 1). Including such images in our dataset ensures that our model is trained to handle difficult/persistent body-related artifacts which cannot be resolved with prompt engineering. While we make efforts to limit the text prompts to a subset which will lead to the generation of individual humans in images, inevitably some generated images might contain more than one human or no humans at all. Thus, we use instance segmentation to filter out such images. Since human annotators are involved in the process, we remove NSFW images using the Amazon Rekognition Content Moderation [36]. Due to privacy concerns, we blur all faces in the dataset using MTCNN [37]."}, {"title": "3.2 Body Realism Annotation", "content": "The quality of annotations heavily depends on the expertise of annotators and the instructions provided to them. To ensure consistency and quality, all annotators were trained and instructed to follow a Standard Operating Procedure (SOP) including representative images (see Sup. Mat.). As illustrated in Fig. 3, images are annotated on a 1-10 scale. We opt for a 1-10 scale, instead of forcing a choice between pairwise image comparisons for multiple reasons: first, since both images could display artifacts, forcing a choice would lead to incorrect labels. Second, independent realism scores per image give us the flexibility to experiment with the formation of training pairs. Finally, a scale provides a higher margin of error especially for multiple annotators, allowing us to devise a tailored strategy for score aggregation (Alg. 1). The annotators are instructed to utilise a mental three-tiered severity scale to categorise body artifacts as: (A) scores 1-3 corresponding to highly unrealistic images, (B) scores 7-10 corresponding to realistic images; (C) corresponding to moderate artifacts. The scores in each bucket are further mapped to fine-grained descriptors and exemplar images ensuring adequate characterization of occurring body artifacts. Images with noticeable inaccuracies in the larger limbs such as arms, legs or a highly deformed body pose, are considered as severe. Images with less conspicuous errors such as those in the smaller limbs - extra/missing fingers, slightly blurred body parts - are considered as moderate. High-scores (8 or more) correspond to negligible or no artifacts in the human bodies. Annotators are instructed to label images which contain more than one human or no human at all as invalid. Since the annotation focuses purely on the realism of bodies the corresponding text is not shown to the annotator and the faces in the displayed images are blurred.\nAfter examining the collected human annotations, we devise a tailored strategy to distill them into robust singular realism scores per image. Given image y with annotations r = {r_j} for j \u2208 [1, N] provided by N = 5 annotators, we use the median and interquantile range (IQR) to filter outliers (see Sup. Mat.), ensuring high-quality body realism scores. We consider data samples as invalid when 3 or more annotators agree on the \"invalid-image\" label."}, {"title": "3.3 Human Body Prior", "content": "We ground our definition of realistic body on SMPL-X [18], a 3D body model of human body pose, hand pose, and facial expression, learnt using thousands of 3D real body scans. We obtain the SMPL-X parameters for each image using PIXIE [38]. Grounded by a parametric 3D body model, PIXIE's body reconstructions confine to the real body structure regardless of body artifacts in the image. We enhance our dataset with 3D body parameters, including 3D mesh vertices, and 3D keypoints or pose parameters. In Sec. 4, we elaborate on the ways in which we leverage the body information as a prior in BodyMetric."}, {"title": "3.4 Statistics and Analysis", "content": "After filtering out invalid images, we end up with ~30k images generated using ~2k unique text prompts describing more than 200 diverse actions. Out of 30,622 generated images, 12,107 are labelled with score less than 3, and 11,178 with score higher than 7, ensuring adequate representation across the full quality range. In addition to generated images, we include 1,705 real in-the-wild text-image pairs of humans from MS COCO, in order to offset any domain biases that the model might learn from generated images. Real images are consistently assigned a high score of 9. We do not assign a score of 10 for real images in order to account for any potential image related artifacts such as blur, obfuscation etc.\nTo our knowledge, BodyRealism is the first dataset to provide scores focusing purely on the realism of human bodies in images. We aim to periodically update the dataset and metric, as we continue with our efforts to collect more annotations covering a wider span of generative models and conditioning prompts."}, {"title": "4 BodyMetric", "content": "Given the collected BodyRealism Dataset, we train BodyMetric, a scoring function which measures the body realism in the images. We elaborate on how BodyMetric jointly leverages the multi-modal signals in BodyRealism. Realism annotations provide supervision during training to mimic human judgement. In addition to human annotations, we argue that anchoring BodyMetric on a 3D human body representation further strengthens the model's \"body-awareness\"."}, {"title": "4.1 Model Design and Training", "content": "Model Given an input image x, we infer the 3D keypoints of the SMPL-X body b\u2208 \u211d^(1\u00d7435) using a body regressor [38]. BodyMetric receives as input an image x, a text prompt y, and 3D keypoints of the SMPL-X body b, and outputs a body realism scores \u2208 \u211d. As shown in Fig.4, the image and text are encoded through the corresponding CLIP encoders, yielding text and image embeddings Etxt(x), Eimg(y). The body representation is projected to the body latent space using a Multi-Layer Perceptron (MLP), yielding body features Ebody(b). The image and body features are merged to an enhanced feature Em(\u1ef9) using another MLP. BodyMetric follows a CLIP-based architecture; the score is calculated using the inner product of embeddings, i.e.,\ns(x,y) = E_{txt}(x)\u00b7 E_{m}(\u1ef9).\nFollowing previous works, we formulate the objective of BodyMetric analogously to the reward model objective used in InstructGPT. In particular, given a prompt x, a pair of images (y1, y2}, and a preference distribution vector p over the two images, the goal is to optimize the parameters of the scoring function s by minimizing the KL-divergence between the preference p and the softmax-normalized scores of \u1ef91 and \u1ef92, i.e.,\nL_{pref} = \\sum_{i=1}^{2}p_i (log p_i - log \\hat{p_i}),"}, {"title": "4.2 Implementation Details", "content": "We train BodyMetric end-to-end on the BodyRealism dataset and initialize Eimg and Etxt from PickScore [17]. BodyMetric is trained for 4,000 steps, with a learning rate of 3e-6, a total batch size of 64, and a warmup period of 500 steps, which follows a linearly decaying learning rate; the experiment takes around 3 hours with 8 A100 GPUs. We evaluate the model's accuracy on the BodyRealism validation set in intervals of 100 steps, and keep the best-performing checkpoint. On an A100 GPU, BodyMetric runs at 0.08s per image pair with an additional 0.12s per image for PIXIE reconstructions."}, {"title": "5 Experiments", "content": "We evaluate BodyMetric's ability to select from a pair of images the one with the most realistic body. We ablate on our design choices, and perform comparisons with SOTA image evaluation metrics."}, {"title": "5.1 Preference Prediction", "content": "Evaluation Protocol We perform our experiments on the BodyRealism test set consisting of ~1k pairs, generated from an independently curated subset of 181 MS COCO text prompts, and 54 synthetic captions describing complex and diverse actions (e.g. running, sitting, dancing, pirouetting). We compare pairs of images corresponding to the same prompt, and measure the performance based on the number of correct guesses. We consider a tied outcome when |p1-p2|<t where t is the tie threshold, defined separately for each model based on the accuracy on the validation set."}, {"title": "6 Applications", "content": "We empirically validate the utility of BodyMetric within the emerging domain of text-to-image generation, and introduce the BodyRealism benchmark to spur"}, {"title": "6.1 Benchmarking Text-to-Image Models", "content": "We systematically apply BodyMetric (Eq. 1) to calculate body realism scores of SOTA text-to-image models on the BodyRealism benchmark. For each T2I model, we repeat the generation 20 times and report the mean across all samples. We use a common set of randomly negative prompts across all models. We follow the process described in Sec. 3 to blur the faces, filter out images and moderate NSFW samples. The results, as shown in Tab. 3, coincide with our"}, {"title": "6.2 Image Ranking", "content": "BodyMetric can be used to rank sets of generated images by sorting the predicted scores from highest to lowest; a higher score corresponds to higher body realism. Fig. 10 demonstrates how BodyMetric successfully ranks sets of 3 images. We carefully design the image sets so that the samples cover the three-tiered severity scale as described in Sec. 3.2."}, {"title": "7 Conclusion", "content": "We design a learnable metric to quantify human body realism related artifacts in images, a key challenge in text-to-image generation. Our metric is trained using the BodyRealism dataset, a new curated multi-modal dataset of images, text descriptions, expert annotations on body realism, and 3D SMPL-X joint keypoints. BodyMetric uses a carefully designed architecture to leverage the multi-modal signal, making it the first body-aware image evaluation metric. We demonstrate both qualitatively and quantitatively that BodyMetric better reflects the quality of generated humans, compared to existing evaluation metrics. In addition, we define a challenging benchmark for text-to-image generation utilising BodyRealism and BodyMetric. We demonstrate how BodyMetric can be used to improve"}]}