{"title": "WAVELET LATENT DIFFUSION (WALA): BILLION-PARAMETER 3D GENERATIVE MODEL WITH COMPACT WAVELET ENCODINGS", "authors": ["Aditya Sanghi", "Aliasghar Khani", "Pradyumna Reddy", "Arianna Rampini", "Derek Cheung", "Kamal Rahimi Malekshan", "Kanika Madan", "Hooman Shayani"], "abstract": "Large-scale 3D generative models require substantial computational resources yet often fall short in capturing fine details and complex geometries at high resolutions. We attribute this limitation to the inefficiency of current representations, which lack the compactness required to model the generative models effectively. To address this, we introduce a novel approach called Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into a wavelet-based, compact latent encodings. Specifically, we compress a 2563 signed distance field into a 123 \u00d7 4 latent grid, achieving an impressive 2,427\u00d7 compression ratio with minimal loss of detail. This high level of compression allows our method to efficiently train large-scale generative networks without increasing the inference time. Our models, both conditional and unconditional, contain approximately one billion parameters and successfully generate high-quality 3D shapes at 2563 resolution. Moreover, WaLa offers rapid inference, producing shapes within two to four seconds depending on the condition, despite the model's scale. We demonstrate state-of-the-art performance across multiple datasets, with significant improvements in generation quality, diversity, and computational efficiency. We open-source our code and, to the best of our knowledge, release the largest pretrained 3D generative models across different modalities.", "sections": [{"title": "INTRODUCTION", "content": "Training generative models on large-scale 3D data presents significant challenges. The cubic nature of 3D data drastically increases the number of input variables the model must handle, far exceeding the complexity found in image and natural language tasks. This complexity is further compounded by storage and streaming issues. Training such large models often requires cloud services, which makes the process expensive for high-resolution 3D datasets as these datasets take up considerable space and are slow to stream during training. Additionally, unlike other data types, 3D shapes can be represented in various ways, such as voxels, point clouds, meshes, and implicit functions. Each representation presents different trade-offs between quality and compactness. Determining which representation best balances high fidelity with compactness for efficient training and generation remains an open challenge. Finally, 3D representations often exhibit complex hierarchical structures with details at multiple scales, making it challenging for a generative model to capture both global structure and fine-grained details simultaneously.\nTo address these challenges, current state-of-the-art methods for large generative models typically employ three main strategies. The first strategy involves using low-resolution representations, such as sparse point clouds, low-polygon meshes, or coarse grids. While these approaches reduce computational complexity, they are limited in their ability to model the full distribution of 3D shapes, struggle to capture intricate details, and often lead to lossy representations. The second approach represents 3D shapes through a collection of 2D images or incorporates images into the training loss. However, this method suffers from long training times due to the need for rendering and can fail to capture internal details of 3D shapes, as it primarily focuses on external appearances. The third strategy introduces more compactness into the input representations to reduce the number of variables the generative model must handle. While these representations can be sparse , they are often irregular or discrete in nature making it challenging to be modeled via neural networks and can still be relatively large compared to image or natural language data, thus making it difficult to scale the model parameters efficiently.\nOne prominent compact input representation is wavelet-based representation, which includes Neural Wavelet , UDiFF , and wavelet-tree frameworks . These methods utilize wavelet transforms and their inverses to seamlessly convert between wavelet spaces and high-resolution truncated signed distance function (TSDF) representations. They offer several key advantages: data can be easily compressed by discarding selected coefficients with minimal loss of detail, and the interrelationships between coefficients facilitate efficient storage, streaming, and processing of large-scale 3D datasets compared to directly using TSDFs . However, despite these benefits, wavelet-based representations remain substantially large, especially when scaling up for large-scale generative models. For example, a 2563 TSDF can be represented as a wavelet-tree of size 463 \u00d7 64 , which is equivalent to a 1440 \u00d7 1440 RGB image. Scaling within this space continues to pose significant challenges.\nIn this work, we build upon the wavelet representation described above and introduce the Wavelet Latent Diffusion (WaLa) framework. This framework further compresses the wavelet representation to obtain compact latent encodings without significant information loss, thereby efficiently enabling us to scale a diffusion-based generative model within this space. Starting with a truncated signed distance function (TSDF) of a shape, we first convert it into 3D wavelet tree representation as in Hui et al. (2024). Then, we train a convolution-based VQ-VAE model with adaptive sampling loss and balanced fine-tuning to compress a 2563 TSDF into a 123 \u00d7 4 grid, achieving a remarkable 2, 427\u00d7 compression ratio while maintaining an impressive reconstruction without a significant loss of detail. For example, as shown in Table 1, an Intersection over Union (IOU) of 0.978 is achieved on the GSO dataset. Compared to other representations, this approach requires fewer input variables for the generative model while retaining high reconstruction accuracy. Consequently, the generative model does not need to model local details and can focus on capturing the global structure. Moreover, by significantly reducing the number of input variables that the generative model must handle due to this compression, we enable the training of large-scale 3D generative models with up to a billion parameters, producing highly detailed and diverse shapes. WaLa also supports controlled generation through multiple input modalities without adding significant inductive biases, making the framework"}, {"title": "RELATED WORK", "content": "Neural Shape Representations. Several representations have been explored for Deep learning for 3D data. Initially, volumetric methods using 3D convolutional networks were employed , but they were limited by resolution and efficiency. The field then advanced to multi-view CNNs that apply 2D processing to rendered views , and further explored sparse point cloud representations with networks like PointNet and its successors . Additionally, neural implicit representations for compact, continuous modeling were developed . Explicit mesh-based and boundary representations (BREP) have gained attention, enhancing both discriminative and generative capabilities in CAD-related applications . Recently, wavelet representations have become popular. Wavelet decompositions of SDF signals enable tractable modeling of high-resolution shapes. In this work, we extend the previous research by addressing the dimensional and computational hurdles of 3D generation. Our novel techniques for efficient shape processing enable high-quality 3D generation at scale, accommodating datasets with millions of shapes.\n3D Generative Models. 3D generative models have evolved rapidly, initially dominated by Generative Adversarial Networks (GANs). Subsequent advancements integrated differentiable rendering with GANs, utilizing multi-view losses for enhanced fidelity . Parallel developments explored normalizing flows  and Variational Autoencoders (VAEs) . Additionally, autoregressive models also gained traction for their sequential generation capabilities . The recent success of diffusion models in image generation has sparked a great interest in their application to 3D contexts. Most current approaches"}, {"title": "METHOD", "content": "Training generative models on large-scale 3D data is challenging because of the data's complexity and size. This has driven the creation of compact representations like neural wavelets, facilitating efficient neural network training. To represent a 3D shape with wavelets, it is first converted into a Truncated Signed Distance Function (TSDF) grid. A wavelet transform is then applied to decompose this TSDF grid into coarse coefficients (Co) and detail coefficients at various levels (D0, D1, D2). Various wavelet transforms, such as Haar, biorthogonal, or Meyer wavelets, can be employed. Most current methods utilize the biorthogonal wavelet transform . The coarse coefficients primarily capture the essential shape information, while the detail coefficients represent high-frequency details. To compress this representation, different filtering schemes can be applied to remove certain coefficients, though this involves a trade-off in reconstruction quality. In the neural wavelet representation , all detail coefficients are discarded during the training of the generative model and a regression network is used to predict the missing detail coefficients Do. In contrast, the wavelet-tree representation retains all coarse coefficients (Co), discards the third level of detail coefficients (D2), and selectively keeps the most significant coefficients from Do along with their corresponding details in D\u2081 using a subband coefficient filtering scheme. The neural wavelet representation, while modeling a smaller number of input variables, has lower reconstruction quality than the wavelet-tree representation, making latter a more attractive option."}, {"title": "STAGE 1: WAVELET VQ-VAE", "content": "Our primary objective is to compress the diffusible wavelet tree representation into a compact latent space without significant loss of fidelity, thereby facilitating the training of a generative model directly on this latent space. Decoupling compression from generation allows for efficient scaling of a large generative model within the latent space. To this end, we employ a convolution-based VQ-VAE, known for producing sharper reconstructions and mitigating issues like posterior collapse. Specifically, the encoder Enc() maps the input Wn to a latent representation Zn = Enc(Wn), which is then quantized as VQ(Zn) via a vector quantization layer and decoded by Dec(.) to reconstruct the shape W = Dec(VQ(Zn)). By integrating the vector quantization layer with the decoder, as in"}, {"title": "STAGE 2: LATENT DIFFUSION MODEL", "content": "In the second stage, we train a large-scale generative model with billions of parameters on the latent grid, either as an unconditioned model to capture the data distribution or conditioned on diverse modalities On (e.g., point clouds, voxels, images). We use a diffusion model within the Denoising Diffusion Probabilistic Models (DDPM) framework , modeling the generative process as a Markov chain with two phases.\nFirst, the forward diffusion process gradually adds Gaussian noise to the initial latent code Zn over T steps, resulting in $Z_T \\sim N(0, I)$. Then, the reverse denoising process employs a generator network \u03b8, conditioned on On, to systematically remove the noise and reconstruct $Z_0^*$. The generator predicts the original latent code $Z_0^*$ from any intermediate noisy latent codes $Z_t$ at time step t, using $f_{\\theta}(Z_t, t, O_n) \\sim Z_0^*$, and is optimized using a mean-squared error loss:\n$L = E_t [|| f_{\\theta}(Z_t, t, O_n) - Z_0^* ||_2^2]$.\nHere, $Z_t$ is obtained by adding Gaussian noise \u03f5 to $Z_0^*$ at time step t using a cosine noise schedule . The condition On is a latent set of vectors derived from various conditioning modalities, injected into the U-ViT generator  by using"}, {"title": "INFERENCE", "content": "At test time, we begin with a randomly generated noisy latent encoding ZT ~ N(0, I) and itera- tively denoise it to reconstruct the original latent code Zn through the reverse diffusion process, as described in DDPM . For conditional generation, we apply classifier-free guidance by interpolating between the unconditional and conditional denoising pre- dictions, steering the generation process toward the desired output. This approach allows for greater control over the quality-diversity trade-off. Once the final latent code Zn is obtained, we use the pre-trained decoder network of the VQ-VAE from 3.1 to generate the final 3D shape in the wavelet form. Subsequently, we apply the inverse wavelet transform to obtain the final 3D shape as an TSDF that can further be converted to a mesh using marching cubes. Notably, we can generate multiple samples for the same conditional input by using different initializations of the noisy latent grid."}, {"title": "RESULTS", "content": "EXPERIMENTAL SETUP\nDatasets. Our training data consists of over 10 million 3D shapes, assembled from 19 pub- licly available datasets, including ModelNet, ShapeNet , SMPL , Thingi10K , SMAL , COMA , House3D , ABC , Fusion 360 , 3D-FUTURE , BuildingNet , DeformingTh- ings4D , FG3D , Toys4K , ABO , Infinigen , Objaverse , and two subsets of Objaver- seXL (Thingiverse and GitHub). These individual datasets target specific object categories: for instance, CAD models (ABC and Fusion 360), furniture (ShapeNet, 3D-FUTURE, ModelNet, FG3D, ABO), human figures (SMPL and DeformingThings4D), animals (SMAL and Infinigen), plants (Infinigen), faces (COMA), and houses (BuildingNet, House3D). Additionally, Objaverse and ObjaverseXL cover a broader range of generic objects sourced from the internet, covering the aforementioned categories and other diverse objects. Following Hui et al. (2024), each of these 19 datasets was split into two parts for data preparation: 98% of the shapes were allocated for training, and the remaining 2% for testing. The final training and testing sets were created by merging the corresponding portions from each sub-dataset. Note that we use the entire testing dataset solely for autoencoder reconstruction validation. We also apply a 90-degree rotation augmentation along each axis, doing the same for the corresponding conditions (point clouds, voxels). We also create a balanced training set across these 19 datasets by sampling 10,000 shapes from each. If a dataset contains fewer than 10,000 shapes, we duplicate the data until the target size is reached.\nTraining Details. For optimization and training, we use the Adam optimizer Kingma & Ba (2014) with a learning rate of 0.0001 and a gradient clipping value of 1. For VQ-VAE training, we use a batch size of 256 with 1024 codebook embeddings of dimension 4. We train the network until convergence and then fine-tune the VQ-VAE using a more balanced dataset until it converges again. For the base generative model, we use a batch size of 64 and train it for 2 to 4 million iterations for each modality. Each generative model is trained on a single H100 GPU per condition. We train our model on six conditions: point clouds with 2, 500 points, voxels at 163 resolution, single-view RGB, multi-view RGB with 4 views, multi-view depth with 4 views, and multi-view depth with 6 views. We also fine-tune the single-view model with synthetic sketch data and single-depth data to obtain two more conditions. Additionally, we train an unconditional model beyond these. Finally,"}, {"title": "POINT CLOUD-TO-MESH", "content": "In this study, we aim to evaluate the generation of a mesh from an input point cloud containing 2,500 points. We present qualitative results of this task in the bottom right of Figure 4 and in rows 1-2 of Figure 2. To quantitatively assess WaLa's performance, we compare it against both traditional and data-driven techniques, as shown in Table 2. For the traditional approach, we benchmark against Poisson surface reconstruction, which uses heuristic methods to create smooth meshes from point clouds. For Poisson reconstruction, we need normals, so we estimate them using the five nearest neighbors via O3D (Zhou et al., 2018). After performing Poisson surface reconstruction, we remove vertices whose density values fall below the 20th percentile to avoid spurious faces. Additionally, we evaluate our method alongside data-driven generative models such as Point-E , MeshAnything , and Make-A-Shape . For Point- E , we utilize its SDF network to estimate the distance field from the point cloud. We also compare our method with MeshAnything , a recent transformer- based neural network designed for meshing point clouds. In this case, we use 2,500 input points and follow their hyperparameters and procedure. Finally, we compare against Make-A-Shape , which also generates meshes conditioned on point clouds and has its model open-sourced.\nThe quantitative results in Table 2 demonstrate that our method significantly outperforms existing point cloud to mesh generation techniques on both the GSO and MAS validation datasets. These results are despite us not needing normals as in Poisson reconstruction and MeshAnything. Our method can also scale well with data compared to methods like MeshAnything which do not scale well with large face counts. Moreover, our method does not require many surface points to recon- struct a 3D shape, whereas methods like MeshAnything require 8k points (as mentioned in their work) and Point-E requires 4k points. Qualitatively, our method also outperforms the baselines, as shown in the bottom right of Figure 4, and creates smoother shapes with complex geometry, as demonstrated in rows 1-2 of Figure 2."}, {"title": "VOXEL-TO-MESH", "content": "In this experiment, we evaluate our proposed method, WaLa, against several baseline approaches for generating 3D shapes from low-resolution voxels with a resolution of 163. Quantitative results are presented in Table 3, while qualitative comparisons are illustrated in the bottom left of Fig- ure 4 and in rows 3 and 4 of Figure 2. We evaluate using the GSO and MAS datasets. As detailed in Table 3, WaLa is benchmarked against traditional upsampling techniques (nearest neighbor and trilinear interpolation) and a data-driven approach, Make-A-Shape . For the tra- ditional upsampling baselines, we apply nearest neighbor and trilinear interpolation methods to the"}, {"title": "IMAGE-TO-MESH", "content": "In this section, we compare WaLa with other state-of-the-art image-to-3D generative models, focus- ing on both single-view and multi-view scenarios. In the single-view setting, our model generates 3D shapes from a single input image or depth map. For multi-view generation, we utilize four RGB images or four to six depth images along with their corresponding camera parameters. This ap- proach allows us to evaluate the model's performance under varying conditions, demonstrating the versatility and effectiveness of our generative model in different image-to-3D generation contexts. Qualitative results for single-view RGB are shown in the top left of Figure 4 and in rows 5\u20136 of Fig- ure 2. Conversely, qualitative results for multi-view RGB are displayed in the top right of Figure 4 and in rows 7-8 of Figure 2. Additional details and results can be found in the appendix and on our website. Our quantitative results, which assess both quality and inference time on the GSO and MAS validation datasets, are presented in Table 4, with the Image-to-3D task results at the top and the multiview-to-3D task at the bottom. We attempted to perform an extensive comparison; how- ever, this proved challenging as many methods are not available as open-source implementations or utilize subsets of the GSO dataset for which the sample lists are not publicly available . Consequently, we chose to use the entire GSO dataset and run open-source models whose code is available for both GSO and MAS.\nAs demonstrated in Table 4, our method consistently outperforms other 3D generation techniques across both tasks. For the single image-to-3D task, our base RGB model surpasses all baseline"}, {"title": "CONCLUSION", "content": "In this work, we introduce Wavelet Latent Diffusion (WaLa), a novel approach to 3D generation that tackles the challenges of high-dimensional data representation and computational efficiency. Our method compresses 3D shapes into a wavelet-based latent space, enabling highly efficient compres- sion while preserving intricate details. WaLa marks a significant leap forward in 3D shape gener- ation, with our billion-parameter model capable of generating high-quality shapes in just 2-4 sec- onds, outperforming current state-of-the-art methods. Its versatility allows it to handle diverse input modalities, including single and multi-view images, voxels, point clouds, depth maps, sketches, and text descriptions, making it adaptable to a wide range of 3D modeling tasks. We believe WaLa sets a new benchmark in 3D generative modeling by combining efficiency, speed, and flexibility. Finally, we release our code and model across multiple modalities to promote further research and support reproducibility within the community."}, {"title": "ADDITIONAL RESULTS AND DETAILS", "content": "For more visual results and detailed information about our model, please visit https: //autodeskailab.github.io/WaLaProject. The code is available at https:// github.com/AutodeskAILab/WaLa."}, {"title": "ARCHITECTURE DETAILS", "content": "In the first stage, we train a convolution-based VQ-VAE using a codebook size of 1024 with a di- mension of 4. We downsample the input wavelet tree representation to a 123 \u00d7 4 latent space. Our generative model operates within this latent space by utilizing the U-ViT architecture , incorporating two notable modifications. Firstly, we do not perform any additional downsampling since our latent space is already quite small. Instead, the model comprises multi- ple ResNet blocks followed by attention blocks, and then more ResNet blocks at the end, with a skip connection from the initial ResNet block. The attention blocks include both self-attention and cross-attention mechanisms, as described in . Secondly, we modulate the layer normalization parameters in both the ResNet and attention layers, following the approach detailed in . This tailored architecture enables our generative model to effectively operate within the compact latent space, enhancing both performance and efficiency.\nIn this section, we describe the details of the various conditions utilized in our model:\n1. Point Cloud Model: During training, we randomly select 2,500 points from the pre- computed point cloud dataset, which was generated from our large-scale dataset comprising 10 million shapes. These points are encoded into feature vectors using the PointNet encoder Qi et al. (2017a). To aggregate these feature vectors into condition latent vectors, we apply attention pooling as described in Lee et al. (2019). This process converts the individual points into a latent set vector. Finally, we pass this latent set vector through additional Multi-Layer Perceptron (MLP) layers to obtain the final condition latent vectors.\n2. Voxel 163 Model: For voxel-based conditions, we employ a ResNet-based convolutional encoder to process the 163 voxel grid. After applying multiple ResNet layers, the voxel volume is downsampled to reduce its dimensionality to 83. This downsampled volume is then processed with additional ResNet layers, ultimately resulting in the conditional latent vectors. This approach leverages the spatial hierarchy captured by the ResNet architecture to effectively encode volumetric data.\n3. Single View Image Model: Our dataset consists of a predetermined set of views for each object. During training, we randomly select one view from this set. The selected view is then processed by the DINO v2 encoder (Oquab et al., 2023) to extract feature representa- tions. The encoder's output serves as the conditional latent vectors, encapsulating the visual information from the single view. It is important to note that we do not train the DINO v2 encoder; instead, we freeze its weights and utilize only the conditional latent vectors.\n4. Single View Depth Model: We begin by selecting a checkpoint from a pre-trained Single View Image Model once it has converged and initialize the depth conditioned generative model using the same architecture described in the single-view section. We then fine-tune the model using pre-computed depth data. Throughout this process, we utilize the DINO v2 encoder (Oquab et al., 2023) to obtain the conditional latent vectors while keeping the encoder's weights frozen.\n5. Sketch Model: We initialize the model using the architecture described in the single-view section. After the base model converges, we fine-tune it with sketch data. This fine-tuning process involves training the model on sketch representations to adapt the latent vectors, enabling them to capture the abstract and simplified features characteristic of sketches. As in previous cases, the DINO v2 encoder (Oquab et al., 2023) remains frozen. Further details about the sketch data are provided in Appendix D.\n6. Multi-View Image/Depth Model: For multi-view scenarios, we select four viewpoints for the multi-view RGB image model and use configurations with four and six views for the multi-view depth model. These views are carefully chosen from pre-defined angles to en- sure comprehensive coverage of the object. Each view is independently processed through"}, {"title": "ABLATION STUDIES", "content": "VQ-VAE ADAPTIVE SAMPLING LOSS ANALYSIS\nIn this section, we evaluate the importance of adaptive sampling loss by training two autoencoder models for up to 200,000 iterations: one incorporating the adaptive sampling loss and one without it. The results are presented in the first two rows of Table 5. We use Intersection over Union (IoU) and Mean Squared Error (MSE) to measure the average reconstruction quality across all data points. Additionally, we introduce D-IoU and D-MSE metrics, which assess the average reconstruction performance by weighting each dataset equally. This approach ensures that any data imbalance is appropriately addressed during evaluation.\nAs shown in the table, even after approximately 200,000 iterations, the model utilizing adaptive sam- pling loss significantly outperforms the one without it. Specifically, the adaptive sampling loss leads to higher IoU and lower MSE values, indicating more accurate and reliable reconstructions. These results clearly demonstrate the substantial benefits of using adaptive sampling loss in enhancing the performance and robustness of autoencoder models.\nVQ-VAE ANALYSIS AND FINETUNING ANALYSIS\nIn this section, we examine the benefits of performing balanced fine-tuning, as described in the main section of the paper. We conduct an ablation study to determine the optimal amount of finetuning data required per dataset to achieve the best results. The results are presented in the rows following the first two in Table 6, utilizing the metrics described above.\nOur observations indicate that even a small amount of fine-tuning data improves the IoU and MSE. Specifically, incorporating as few as 2,500 samples per dataset leads to noticeable enhancements in reconstruction accuracy. However, we found that increasing the finetuning data to 10,000 samples per dataset provides optimal performance. At this level, both IOU and Mean Squared Error (MSE) metrics reach their best values, demonstrating the effectiveness of balanced fine-tuning in enhancing model performance.\nMoreover, the D-IoU and D-MSE metrics confirm that using 10,000 samples per dataset effectively mitigates data imbalance to a certain degree. Based on these findings, all subsequent results in this study are based on using 10,000 finetuning samples per dataset. We believe that an interesting area for future work is to improve data curation to further enhance reconstruction accuracy.\nARCHITECTURE ANALYSIS OF GENERATIVE MODEL\nIn this section, we conduct an extensive study on the architectural design choices of the generative model. Given the high computational cost of training large-scale generative models, we implement early stopping after 400,000 iterations. The results are presented in Table 6. First, we examine the importance of the hidden dimension in the attention layer. It is clearly observed that increasing the dimension enhances performance. A similar trend is noted when additional layers of attention"}]}