{"title": "FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions", "authors": ["Anuroop Sriram", "Ricky T. Q. Chen", "Benjamin Kurt Miller", "Brandon M. Wood"], "abstract": "Material discovery is a critical area of research with the potential to revolutionize various fields, including carbon capture, renewable energy, and electronics. However, the immense scale of the chemical space makes it challenging to explore all possible materials experimentally. In this paper, we introduce FlowLLM, a novel generative model that combines large language models (LLMs) and Riemannian flow matching (RFM) to design novel crystalline materials. FlowLLM first fine-tunes an LLM to learn an effective base distribution of meta-stable crystals in a text representation. After converting to a graph representation, the RFM model takes samples from the LLM and iteratively refines the coordinates and lattice parameters. Our approach significantly outperforms state-of-the-art methods, increasing the generation rate of stable materials by over three times and increasing the rate for stable, unique, and novel crystals by ~ 50% a huge improvement on a difficult problem. Additionally, the crystals generated by FlowLLM are much closer to their relaxed state when compared with another leading model, significantly reducing post-hoc computational cost.", "sections": [{"title": "Introduction", "content": "Material discovery holds transformative potential across numerous industries including carbon capture[38], batteries[28], photovoltaics[9], and energy storage[1]. However, the vastness of the chemical space has hindered experimental synthesis of the majority of possible materials. Generative models offer a promising avenue for exploring this untapped potential.\nGenerating crystalline materials is particularly challenging as it involves simultaneously generating both discrete (atomic types) and continuous values (atomic positions and lattice geometry). While existing approaches, namely autoregressive large language models (LLMs)[11, 6] and denoising models, e.g., denoising diffusion and flow matching [47, 16, 49, 48, 30, 26, 17], have demonstrated success, they exhibit complementary strengths and weaknesses. LLMs excel at modeling discrete values, but they can struggle with continuous values due to their reliance on finite precision representations. Conversely, denoising models more effectively handle continuous values and can easily ensure equivariances, but they face challenges with discrete elements.\nLLMs also offer the distinct advantage of natural language prompting, enabling versatile and intuitive conditional generation. This capability is further enhanced by training LLMs on vast corpora of chemistry text, equipping them with valuable prior knowledge to generate chemically valid outputs. Queries like \"Generate materials with a high bandgap and thermal stability\" or \"Propose a novel perovskite structure for efficient solar energy conversion\u201d can be directly integrated into the LLM"}, {"title": "Related Work", "content": "In the past, computational materials discovery relied on generating numerous candidate materials through random atomic substitutions in known materials[42], followed by computationally expensive quantum mechanical screening[19] to assess stability. Genetic algorithms[8, 33], and machine learning models trained to predict energies[37, 25] have accelerated this process, but the fundamental bottleneck of brute force search remains.\nRecent research has focused on generative models that directly produce stable materials, bypassing brute-force search. Diffusion models, either combined with Variational Autoencoders (VAEs) for partial variable prediction[47] or jointly diffusing all variables[16, 48, 49] have shown promise. Additionally, Riemannian Flow Matching[26], Normalizing Flows [45], and Variational Autoencoders[34] have also been adapted for material generation.\nA parallel line of work utilizes autoregressive Large Language Models (LLMs) for material generation [6, 11], representing materials as a sequence of discretized tokens. Pretraining these models on natural language imbues them with powerful prior knowledge not attainable by other approaches."}, {"title": "Preliminaries", "content": "Our approach models probability distributions over crystal lattices, defined as periodic arrangements of atoms in three-dimensional space. A crystal lattice is created by tiling a fundamental unit cell, where the unit cell contains a specific atomic configuration, forming the entire lattice when repeated. In this section, we present a high-level overview of crystal representations, building up to explain our model in section 4. Background details for the crystal representation are in appendix A.\nCrystal representation In the paper, we represent an $n \\in \\mathbb{N}$ atom crystal in a product space: $c := (a, f,l) \\in \\mathcal{C}$, indicating the atom types, positions and unit cell geometry, respectively [47, 26]. The atom types are represented by a matrix of categorical vectors: $a := [a^1, ..., a^n]$, where $a^i \\in \\mathcal{A}$. The atomic coordinates are represented using fractional coordinates within the unit cell, $f := [f^1,..., f^n]$, where $f^i \\in \\mathcal{F} = \\mathbb{T}^3$ with $\\mathbb{T}$ denoting the unitary length, flat torus manifold, i.e., the fractional coordinates satisfy periodic boundary conditions; that is, the atoms \"wrap around\" the unit cell. The unit cell geometry is defined using lattice parameters $l \\in \\mathcal{L}$, where $\\mathcal{L}$ is the space formed by a 6-tuple of three side lengths $(a, b, c) \\in \\mathbb{R}^+$ (\u00c5, i.e. Angstrom) and three internal angles $(\\alpha, \\beta, \\gamma) \\in [60^\\circ, 120^\\circ]$. This representation is not unique as the same crystal can be produced by different choices of unit cell. To make the representation unique, we select the minimum-volume unit cell and employ Niggli reduction [10] that uniquely determines the unit cell parameters.\nEquivariance & Invariance Given a group $G$ with $g \\cdot$ denoting a group action for some $g \\in G$, a function $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ is called G-equivariant if $\\forall x \\in \\mathcal{X},\\forall g \\in G, f(g \\cdot x) = g \\cdot f(x)$, while it is called G-invariant if $\\forall x \\in \\mathcal{X},\\forall g \\in G, f(g \\cdot x) = f(x)$. Since a crystal is not uniquely defined by any particular representation $e$ but an infinite set, we know that the data distribution has a G-invariant density, where G represents symmetries of a crystal.\nSymmetries of crystals Concretely, our crystal representation exhibits multiple symmetries that we detail here. The symmetric group $S_n$ on $n$ atoms permutes the atom indices: $\\sigma \\cdot c = ([a_{\\sigma(1)},...,a_{\\sigma(n)}], [f_{\\sigma(1)}, ..., f_{\\sigma(n)}],l)$. The special Euclidean group $SE(3)$ consists of orientation preserving rigid rotations and translations: $(Q,T)$ where $Q \\in SO(3)$ and $T \\in [-1,1]^{3\\times1}$ denote 3D rotations and translations respectively. This element transforms the crystal as: $(Q, T) \\cdot c = (a, f + T - \\lfloor f+T \\rfloor, l)$. We emphasize that the representation $c$ is completely invariant w.r.t. $Q$ because lattice parameters do not contain orientation information. Since these represent symmetries fundamental to crystals, the data distribution $q(c)$ is invariant to these group operations."}, {"title": "Method", "content": "Our goal is to fit a parametric generative model $p(c; \\theta)$ to approximate the distribution of known meta-stable materials $q(c)$ using a dataset of samples. The distributions $p$ and $q$ are defined on the Riemannian manifold $\\mathcal{C}$. Our FlowLLM model generates samples from the parametric distribution"}, {"title": "Large Language Model (PLLM) for Crystals", "content": "LLMs define a distribution over sequences through an autoregressive decomposition, $\\Pi_{t=1}^n P(w_{t+1}|w_{0:t})$, where each $p(w_{t+1}|w_{0:t})$ follows a categorical distribution conditioned on all previous tokens ($w_{0:t}$) in the sequence. Our LLM model closely follows Gruver et al. [11].\nTokenization Language models interact with strings in text datasets after the string is converted into a sequence of tokens. The choice of tokenizer can have a large impact on the performance of the language model. In terms of tokens, we represent a crystal $c$ using fixed precision numbers - two decimal places for fractional coordinates, and one for lattice lengths. Angles are represented as integers. Atom types are represented as discrete tokens. We use LLaMA-2 models [41] for our LLM architecture since these models break numbers into a sequence of digits, which has been shown to dramatically improve performance on arithmetic tasks [23].\nTraining We rely on the extensive pretraining of LLaMA-2 models to instill useful biases over numerical operations. To train $P_{LLM}$, we fine-tune a pre-trained LLaMA-2 model on a dataset of crystal structures represented as strings along with a prompt indicating that the model should generate bulk materials by writing the lattice in lengths and angles along with atom types and coordinates. An example of such a representation along with a prompt is shown in figure 2.\nThe flexibility of LLMs allows us to optionally include different kinds of conditional information in the prompt such as the chemical formula. We can also solve other tasks such as infilling by making changes to the prompt. For this hypothetical conditional generation, the prompt could include a desired"}, {"title": "Riemannian Flow Matching (PRFM) for Crystals", "content": "Riemannian Flow Matching RFM produces a Continuous Normalizing Flow [3], i.e., a continuous, parametric, diffeomorphism between the LLM base distribution $p_0 := P_{LLM}$ and an approximation to our target distribution $p_1 \\approx q$. To model $P_{RFM} := p_1$, we fit a time-dependent vector field $v_{\\theta_1}$ that has been adapted to crystals and is implemented using a neural network with parameters $\\theta_1$. Continuous Normalizing Flows are computationally expensive to train using maximum likelihood, but an alternative objective called Conditional Flow Matching [22] is more stable and scales better. The objective was generalized to Riemannian manifolds [2], and specifically to labeled point clouds with periodic boundary conditions, i.e. crystals, by Miller et al. [26].\nConcretely, each point $c \\in \\mathcal{C}$ has an associated tangent space $T_c \\mathcal{C}$ with an inner product $\\langle u, v \\rangle$ for $u, v \\in T_c \\mathcal{C}$, enabling the definition of distances, volumes, angles, and minimum length curves (geodesics). The geodesics for any $C$ that we consider can be written in closed form using the exponential and logarithmic maps. The geodesic connecting $c_0, c_1 \\in \\mathcal{C}$ at time $t\\in [0, 1]$ is\n$c_t := \\exp_{c_0} (t \\log_{c_0} (c_1)),$ (3)\nwhere $\\exp$ and $\\log$ are the exponential and logarithm maps for the manifold $\\mathcal{C}$. These geodesics help define the supervision signal used to train RFM."}, {"title": "Consequences of using an LLM as the base distribution", "content": "Model symmetries Just like the LLM, the orientation-invariant representation of the unit cell leads to global rotation invariance. However, permutation and translation symmetries are not so simple. If the parameterization of the RFM velocity field is G-equivariant, and the base distribution is G-invariant, then the model density is G-invariant [18]. We use graph neural networks [36, 40, 27, 44, 7, 21, 31, 51], and additional projections [26], to ensure that the RFM velocity predictions are G-equivariant to both permutation and translation. However, we will generally not recover a translation"}, {"title": "Experiments", "content": "We trained our model on the widely used MP-20 dataset\u00b9 of inorganic crystalline materials[47]. MP-20 comprises 45,231 materials, a subset of the Materials Project[15] containing up to 20 atoms known to be metastable (see section 5.2).\nWe first train our LLM independently using the various prompting strategies described in Section 4. Unless otherwise specified, we employed a pretrained LLaMA-2 70B model [41] for all experiments, that was fine-tuned with the Low-Rank Adapters (LoRA) method [14] using PyTorch[32] and Transformers[46].\nNext, we trained the RFM model using the fine-tuned LLM (with frozen weights) as the base distribution and the MP-20 dataset as the target distribution. For computational efficiency, we sampled a large number ($N_{tr}$) of examples from the base distribution in advance, and used the same set for all of our training runs. To create this set, we sampled $N_{tr}$ materials, with replacement from MP-20, and queried the LLM with a prompt conditioned on the chemical formula of each of these materials. This results in a set of $N_{tr}$ pairs, {($c_0, c_1$)}, of LLM generated materials and ground truth materials that constitutes the training set for the RFM model. We list the hyperparameter values used in our experiments in appendix C.\nTo generate new samples, we first generate a material from the LLM using an unconditional query. We then perform an integration with the RFM model, starting from this LLM-generated material. During sampling, we can adjust hyperparameters such as temperature $\\tau$, nucleus probability $P$, and the number of integration steps to achieve different trade-offs between diversity, accuracy, and efficiency."}, {"title": "Metrics", "content": "Our primary metrics are Stability Rate, the percentage of generated materials that are thermodynamically stable, a key indicator of synthesizability, and the S.U.N. rate, the percentage of materials that"}, {"title": "Discussion", "content": "The discovery of novel, stable materials holds the potential to help revolutionize numerous industries, but progress has been slow due to the high computational costs involved. Widely used random structure search methods[33] yield less than a 1% success rate in identifying stable materials. Given the substantial cost of validating generated structures using density functional theory, improving this rate is of paramount importance.\nRecent breakthroughs with denoising models[16, 26] and large language models[11] have increased the stability rate to ~ 5%, a significant improvement over traditional approaches. In this work, we propose a novel generative model which harnesses the strengths of both paradigms to further increase this number by over 3\u00d7, representing a major advancement in the field.\nLimitations While FlowLLM excels at generating stable materials, a key limitation is its lack of end-to-end differentiability. This hinders its direct application to inverse design, where generative models are optimized to generate material with specific properties, as explored in prior work using denoising models[49, 47]. Future research could investigate extending FlowLLM for inverse design.\nBroader impact This work can accelerate the discovery of new materials for renewable energy, electronics, and carbon capture, ultimately benefiting society by enabling more efficient and sus- tainable technologies. However, the adoption of generative models also raises concerns, such as the creation of harmful substances and access inequalities."}, {"title": "Crystal Representations Details", "content": "Atomic types The representation of atomic number is dependent on the model processing the data. In the LLM, the name of the element can be written into the text representation directly. This can be a string or single token, depending on LLaMA-2's tokenization. In the RFM framework, we applied a one-hot representation.\nUnit cell geometry Throughout the paper and in our implementation, we represent the unit cell using lengths and angles; however, there is another representation relevant for defining the fractional coordinates and better expressing crystal symmetries. The unit cell can be defined by a matrix of Cartesian column vectors $\\hat{l} := [l_1,l_2,l_3] \\in \\hat{\\mathcal{C}} = \\mathbb{R}^{3\\times3}$. This representation has strictly more information than $l$, since it also defines the orientation of the unit cell. This orientation is irrelevant in our paper, since we want rotation invariance. That's why we choose $l$ in the first place.\nFractional coordinates Now that we have the representation $l$, we can define fractional coordinates. Recall, atomic positions are typically represented using Cartesian coordinates $x := [x^1,...,x^n] \\in \\mathbb{R}^{3\\times n}$ with coordinates in the rows and atoms in the columns. The Fractional coordinate representation is defined $f := \\hat{l}^{-1}x = [f^1,..., f^n] \\in \\mathcal{F} = [0,1)^{3\\times n}$."}, {"title": "Graph Neural network in the RFM Model", "content": "In this section, we describe the graph neural network used in our RFM model. Our GNN model is inspired by the GNNs used in FlowMM[26] and DiffCSP[16], which in turn adapted the EGNN [36] model for fractional coordinates,\n$h_i^{(0)} = h^{(0)} (a^i)$ (10)\n$m_{ij}^{(s)} = \\varphi_m (h_i^{(s-1)}, h_j^{(s-1)}, l, SinusoidalEmbedding(f_i - f_j))$, (11)\n$m_i^{(s)} = \\sum_{j=1}^N m_{ij}^{(s)}$ (12)\n$h_i^{(s)} = h_i^{(s-1)} + \\varphi_h (h_i^{(s-1)}, m_i^{(s)})$, (13)\n$f_i = \\varphi_f (h_{\\max}^{(s)})$ (14)\n$\\hat{l} = \\varphi_{\\hat{l}} (\\sum_{i=1}^N h_{\\max}^{(s)})$ (15)\nwhere $m_{ij}^{(s)}, m_i^{(s)}, m_\\hat{l}$ represent messages at layer $s$ between nodes $i$ and $j$, $h_i^{(s)}$ represents hidden representation of node $j$ at layer $s$; $\\varphi_m, \\varphi_h, \\varphi_{\\hat{l}}, \\varphi_f, \\varphi_h^{(0)}$ represent parametric functions with all parameters noted together as $\\theta$. Finally, we define\n$SinusoidalEmbedding(x) := (sin(2\\pi kx), cos(2\\pi kx))_{k=0,..,n_{freq}}$ (16)\nwhere $n_{freq}$ is a hyperparameter. We standardized the $l$ input to the network with z-scoring. We also standardized the outputs for predicted tangent vectors $f, \\dot{l}$. Models were trained using the AdamW optimizer [24]."}]}