{"title": "Relational Programming with Foundation Models", "authors": ["Ziyang Li", "Jiani Huang", "Jason Liu", "Felix Zhu", "Eric Zhao", "William Dodds", "Neelay Velingker", "Rajeev Alur", "Mayur Naik"], "abstract": "Foundation models have vast potential to enable diverse AI applications. The powerful yet incomplete nature of these models has spurred a wide range of mechanisms to augment them with capabilities such as in-context learning, information retrieval, and code interpreting. We propose VIEIRA, a declarative framework that unifies these mechanisms in a general solution for programming with foundation models. VIEIRA follows a probabilistic relational paradigm and treats foundation models as stateless functions with relational inputs and outputs. It supports neuro-symbolic applications by enabling the seamless combination of such models with logic programs, as well as complex, multi-modal applications by streamlining the composition of diverse sub-models. We implement VIEIRA by extending the SCALLOP compiler with a foreign interface that supports foundation models as plugins. We implement plugins for 12 foundation models including GPT, CLIP, and SAM. We evaluate VIEIRA on 9 challenging tasks that span language, vision, and structured and vector databases. Our evaluation shows that programs in VIEIRA are concise, can incorporate modern foundation models, and have comparable or better accuracy than competitive baselines.", "sections": [{"title": "Introduction", "content": "Foundation models are deep neural models that are trained on a very large corpus of data and can be adapted to a wide range of downstream tasks (Bommasani et al. 2021). Exemplars of foundation models include language models (LMs) like GPT (Bubeck et al. 2023), vision models like Segment Anything (Kirillov et al. 2023), and multi-modal models like CLIP (Radford et al. 2021). While foundation models are a fundamental building block, they are inadequate for programming AI applications end-to-end. For example, LMs hallucinate and produce nonfactual claims or incorrect reasoning chains (McKenna et al. 2023). Furthermore, they lack the ability to reliably incorporate structured data, which is the dominant form of data in modern databases. Finally, composing different data modalities in custom or complex patterns remains an open problem, despite the advent of multi-modal foundation models such as ViLT (Radford et al. 2021) for visual question answering.\nVarious mechanisms have been proposed to augment foundation models to overcome these limitations. For example, PAL (Gao et al. 2023), WebGPT (Nakano et al. 2021), and Toolformer (Schick et al. 2023) connect LMs with search engines and external tools, expanding their information retrieval and structural reasoning capabilities. LMQL (Beurer-Kellner, Fischer, and Vechev 2022) generalizes pure text prompting in LMs to incorporate scripting. In the domain of computer vision (CV), neuro-symbolic visual reasoning frameworks such as VISPROG (Gupta and Kembhavi 2022) compose diverse vision models with LMs and image processing subroutines. Despite these advances, programmers lack a general solution that systematically incorporates these methods into a single unified framework.\nIn this paper, we propose VIEIRA, a declarative framework for programming with foundation models. VIEIRA follows a (probabilistic) relational paradigm due to its theoretical and practical versatility. Structured data is commonly stored in relational databases. Relations can also represent structures such as scene graphs in vision and abstract syntax trees in natural and formal languages. Moreover, extensions"}, {"title": "Related Work", "content": "Neuro-symbolic methods. These methods combine the complementary benefits of neural learning and symbolic reasoning. They include domain-specific solutions (Yi et al. 2018; Mao et al. 2019; Li et al. 2020; Wang et al. 2019; Xu et al. 2022; Chen et al. 2020; Minervini et al. 2020) as well as general programming frameworks, such as DeepProbLog (Manhaeve et al. 2018) and SCALLOP (Li, Huang, and Naik 2023). These methods typically concern training or fine-tuning neural models in the presence of logical programs, whereas we target building applications atop foundation models with zero-shot or few-shot examples. Another recent work, the STAR framework (Rajasekharan et al. 2023) also connects a language model (neural) to an answer set programming reasoner (symbolic). It is conceptu-"}, {"title": "Language", "content": "VIEIRA employs a declarative logic programming language based on Datalog (Abiteboul, Hull, and Vianu 1994). In this section, we present the core language and its foreign interface for incorporating diverse foundation models."}, {"title": "Core Language", "content": "Relations and data types. The fundamental data type in VIEIRA is set-valued relations comprising tuples of statically-typed primitive values. Besides the standard primitive types such as integers (e.g. 132) and string (String), VIEIRA introduces two additional types for seamless integration of foundation models: Tensor and Algebraic Data Types (ADTs). For example, we can declare a relation named image to store tuples of image IDs and image Tensors:\nThe contents of this relation can be specified via a set of tuples using the built-in foreign function $load_image:\nADTs in VIEIRA enable the specification of domain specific languages (DSLs) to bridge structured and unstructured data. For example, the following DSL for visual question answering (VQA) describes queries to retrieve scene objects, count objects, and check the existence of objects:"}, {"title": "Logical reasoning", "content": "Being based on Datalog, VIEIRA supports defining Horn rules, thereby allowing logical reasoning constructs such as conjunction, disjunction, recursion, stratified negation, and aggregation. Recursion is particularly useful for inductively defining the semantics of a DSL.\nFor example, a (partial) semantics for the above DSL is defined as follows, where eval_o and eval_n are recursively defined to evaluate objects and numbers, respectively:"}, {"title": "Probabilistic soft logic", "content": "Tuples can be tagged with probabilities. The example below shows hard-coded probabilities, suggesting that the entity is more likely a dog than a cat:\nSoft-logic operations produce probabilities as well. For instance, the soft-eq operator (=) on Tensors derives cosine-similarity between tensors, enabling features like soft-join and applications like semantic search. In the following example, we compute similarity scores between distinct documents by performing soft-join on their embeddings:\nNotice that in the above rule, a join on a tensor value v is desugared into a soft-eq on two individual variables (denoted v1 and v2). Internally, with the provenance framework pro- vided by SCALLOP (Li, Huang, and Naik 2023), we use the top-k-proofs semiring (Huang et al. 2021) for scalable probabilistic reasoning, thus enabling features such as ranking and uncertainty estimation."}, {"title": "Foreign Interface", "content": "In order to incorporate foundation models, we design a foreign interface with two main programming constructs, called foreign predicate and foreign attribute. They can be defined externally in languages like Python and imported into VIEIRA for application.\nForeign Predicate (FP). Foreign predicates can be used in rules just like other relations. However, instead of grounding relational facts from a table, FPs ground facts by invoking external functions. The syntax for defining FPs is as follows:\nIn addition to the type, each argument is specified either as a bounded argument (using the keyword bound) or a free"}, {"title": "Foundation Models", "content": "VIEIRA provides an extensible plugin framework that adapts to the evolving landscape of foundation models. In this work, we have implemented 7 plugins, covering 12 foundation models, all through the foreign interface. Our design principle for the interface is three-fold: simplicity, configurability, and compositionality. In this section, we present several representative predicates and attributes which substantially support the applicability of VIEIRA to diverse machine learning tasks.\nText completion. In VIEIRA, language models like GPT (OpenAI 2023) and LLaMA (Touvron et al. 2023) can be used as basic foreign predicates for text completion:\nIn this case, gpt is an arity-2 FP that takes in a String as the prompt and produces a String as the response. It uses the model gpt-3.5-turbo by default. To make the interface more relational and structural, we provide an FA:"}, {"title": "Semantic parsing", "content": "One can directly configure language models to perform semantic parsing. For instance, the semantic parser for the simple Query DSL (partially defined in the Language section) can be declared as follows:\nInternally, the language model is expected to generate a fully structured Query in its string form. Then, VIEIRA attempts to parse the string to construct actual ADT values. In practice, the success of semantic parsing depends heavily on the design of the DSL, involving factors like intuitiveness (e.g., names and arguments of ADT variants) and complexity (e.g., number of possible ADT variants)."}, {"title": "Relational data extraction", "content": "Structural relational knowledge available in free-form textual data can be extracted by language models. We introduce a foreign attribute @gpt_extract_relation for this purpose. For instance, the following declared predicate takes in a context and produces (subject, object, relation) triplets:\nThis attribute differs from the text completion attribute in that it can extract an arbitrary number of facts. The underlying implementation prompts LMs to respond with JSON-formatted strings, allowing structured facts to be parsed."}, {"title": "Language models for textual embedding", "content": "Textual embeddings are useful in performing tasks such as information retrieval. The following example declares an FP encapsulating a cross-encoder (Nogueira and Cho 2019):\nIn the last line, we compute the cosine-similarity of the encoded embeddings using a soft-join on the variable e. As a result, we obtain a probabilistic fact like 0.9::sim() whose probability encodes the cosine-similarity between the textual embeddings of \"cat\" and \"neko\"."}, {"title": "Image classification models", "content": "Image-text alignment models, such as CLIP (Radford et al. 2021), can naturally be used as zero-shot image classification models. Fig. 1b shows an example usage of the @clip attribute. We also note that dynamically-generated classification labels can be provided to CLIP via a bounded argument in the predicate."}, {"title": "Image segmentation models", "content": "OWL-ViT (Minderer et al. 2022), Segment Anything Model (SAM) (Kirillov et al. 2023), and DSFD (Li et al. 2018) are included in VIEIRA as image segmentation (IS) and object localization (LOC) models. IS and LOC models can provide many outputs, such as bounding boxes, classified labels, masks, and cropped images. For instance, the OWL-ViT model can be used and configured as follows:\nHere, the find_obj predicate takes in an image, and finds image segments containing \u201chuman face\" or \"rocket\". According to the names of the arguments, the model extracts 3 values per segment: ID, label, and cropped image. Note that each produced fact will be associated with a probability, representing the confidence from the model."}, {"title": "Image generation models", "content": "Visual generative models such as Stable Diffusion (Rombach et al. 2022) and DALL-E (Ramesh et al. 2021) can be regarded as relations as well. The following example shows the declaration of the gen_image predicate, which encapsulates a diffusion model:\nAs can be seen from the signature, it takes in a String text as input and produces a Tensor image as output. Optional arguments such as the desired image resolution and the number of inference steps can be supplied to dictate the granularity of the generated image."}, {"title": "Tasks and Solutions", "content": "We apply VIEIRA to solve 9 benchmark tasks depicted in Fig. 3. Table 1 summarizes the datasets, evaluation metrics, and the foundation models used in our solutions. We elaborate upon the evaluation settings and our solutions below.\nDate reasoning (DR). In this task adapted from BIG-bench (Srivastava et al. 2023), the model is given a context and asked to compute a date. The questions test the model's temporal and numerical reasoning skills, as well as its grasp of common knowledge. Unlike BIG-bench where multiple-choice answers are given, we require the model to directly produce its answer in MM/DD/YYYY form.\nOur solution leverages GPT-4 (5-shot\u00b9) for extracting 3 relations: mentioned dates, duration between date labels, and the target date label. From here, our relational program iterates through durations to compute dates for all date labels. Lastly, the date of the target label is returned as the output.\nTracking shuffled objects (TSO). In this task from BIG-bench, a textual description of pairwise object swaps among people is given, and the model needs to track and derive which object is in a specified person's possession at the end."}, {"title": "RQ1: Programmability", "content": "While a user study for VIEIRA's programmability is out of scope in this paper, we qualitatively evaluate its programmability on three aspects. First, we summarize the lines-of-code (LoC) for each of our solutions in Table 2. The programs"}, {"title": "RQ2: Baselines and Comparisons", "content": "We compare the performance of our solutions to existing baselines under the no-training setting. In particular, our solutions achieve better performance than comparable baselines on 6 out of 8 studied datasets with baselines. Below, we classify the tasks into 4 categories and discuss the respective performance and comparisons.\nNatural language reasoning. For the tasks of DR, TSO, CLUTRR, and GSM8K, we pick a generic baseline of GPT-"}, {"title": "Compositional multi-modal reasoning", "content": "For compositional multi-modal reasoning, we pick tasks of CLEVR and GQA. We choose two compositional VQA datasets, GQA (Hudson and Manning 2019) and CLEVR (Johnson et al. 2016). In this task, the model is given an image and a question, and needs to answer the question. For GQA, the majority of questions expect yes/no answers, while CLEVR's questions demand features like counting and spatial reasoning. We randomly sample 184 and 480 The images and questions in GQA are collected from real life while that of CLEVR are synthetic."}, {"title": "Visual object tagging", "content": "For VQAR, we consider the top 50 object bounding boxes returned by OWL-ViT. Our relational knowledge base is from (Huang et al. 2021). When querying ViLT, we take the top response from a score threshold of 0.9. We manually score semantic correctness by finding the percentage of objects returned that match the query. Object bounding boxes are considered correct if they contained any part of an entity matching the query.\nFor OFCP, we curated 50 examples featuring groups of notable celebrities and politicians from Wikimedia Commons and other Internet sources, and manually assigned descriptive filenames to each image. We obtain the set of possible names by prompting GPT-4 with the filename. We enlarge the face bounding boxes returned by DSFD by a factor of 1.3 before querying CLIP. We tag each face with its most probable name from CLIP, but if the probability is below the 0.8 threshold, then the face is tagged \"unknown\". The ground truth of relevant faces and their names were manually assigned based on the filename description. The ground truth label for non-relevant faces is \"unknown\". All faces judged to be in the foreground of an image, as well as any additional faces not tagged with \"unknown\", are counted for semantic correctness."}, {"title": "Image generation and editing", "content": "We manually wrote 20 prompts image generation and their editing sequences. Each prompt includes one image generation prompt and two consequent image editions. Our domain-specific language for image generation and editing supports 5 operations: Background, ReplaceObject, RefineObject, NotObject,\nReweightObject. We use the GPT-4 model to convert the natural language prompts into programmatic queries with 4 shot examples. There are 2 cases among 20 that fail to convert the natural language into executable programs, as the Replace operation requires to have the same token length of the original input text and the updated text, while the GPT-4 model fails to capture the requirement through the few-shot examples."}, {"title": "Qualitative Studies", "content": "We present exemplars for face tagging in Figure 9, object tagging in Figure 10, image editing in Figure 11, and image generation and editing in Figure 12."}, {"title": "Full VIEIRA Language", "content": "We present the full surface syntax of the VIEIRA language in Fig. 7."}, {"title": "Detailed Example", "content": "In this section, we describe the VIEIRA program for one of our benchmark applications, CLEVR (Johnson et al. 2016). We decompose this application into three sub-tasks: 1) extracting a structured scene graph from the input image, 2) extracting an executable query program from the input natural language (NL) question, and 3) combining both to answer the question based on the scene graph. We next describe how we solve each of these sub-tasks. For illustration, we use the example image and question shown in Fig. 8."}, {"title": "Image to structured scene graph", "content": "To convert image to structured scene graph, we use two vision models, namely OWL-ViT (Minderer et al. 2022) and CLIP (Radford et al. 2021). We use OWL-ViT for obtaining object segments and CLIP models for classifying object properties. The goal is to construct scene graph which contains the following information: the shape, color, material, and size for each object, and the spatial relationships between each pair of objects.\nOur object detection predicate is defined as follows:\nWe are using the @owl_vit foreign attribute to decorate a predicate vit_segment_image. Here, the image has one bounded argument which is the input image, and it produces image segments represented by 5 tuples, containing segment id (id), segmented image (cropped_image), the area of segment (area), the center x coordinate (bbox_center_x), and the bottom y coordinate (bbox_bottom_y). Specifically, segmented images can be passed to downstream image classifiers, the area is used to classify whether the object is big or small, and the coordinates are used to determine spatial relationships between objects.\nNote that the arguments we pass to @owl_vit contain expected labels of cube, sphere, and cylinder. Because OWL-ViT does not perform well at classifying given geometric objects by shape, we do not use it to query the labels associated with each object. Rather, these labels identify the image segments the model extracts from the base image.\nWe set expand_crop_region to be 10, which expands the cropped images by the given factor. Since the bounding boxes of the objects are tight, enlarging the crop region can help subsequent classifiers to better see the object. With the limit set to 10, OWL-ViT only generates 10 image segments. Lastly, we set flatten_probability to be true. Again, OWL-ViT is not trained on CLEVR, so it produces very low confidence scores on all recognized objects. In order to not let the scores affect downstream computation, we overwrite the probability to 1 for all objects.\nWe load the image specified by the image directory path using the foreign function $load_image, and then segment the image using the segment_image predicate:\nWe next define the shape classifier. For this, we repurpose @clip to classify each object segment with a label from three possible shapes: cube, sphere, and cylinder. In order to interface with CLIP, we create a prompt \"a {{}} shaped object\". Each label is filled into the prompt, producing short phrases like \"a cube shaped object\". Then, the three prompts are passed to CLIP along with the object image, and facts with labels are returned with probabilities.\nThe classifiers for color and material are done similarly:\nFrom here, we just invoke the previous We continue to dis- cuss how do we obtain the size and spatial relationships. In order to obtain the size (small or large) of each object, we use a probabilistic rule for specifying that:\nFinally, the spatial relationship (left, right, front, and behind) is derived from object coordinates.\nCombining everything together, we have produced the relationships color, shape, material, size, and relate, forming the scene graph of the image."}, {"title": "NL question to programmatic query", "content": "We use the GPT-4 model (OpenAI 2023) for converting a natural language question into a programmatic query. The first step is defining the domain specific language (DSL) for querying the CLEVR dataset:\nNotice that the DSL is represented by the user-defined algebraic data type (ADT) Query, which contains constructs for getting objects, counting objects, checking existence of objects, and even comparing counts obtained from evaluating multiple queries. We then create the semantic parser for the DSL by configuring the GPT-4 model:\nOther than the model argument which is used to specify the OpenAI model to call, we also pass 3 additional arguments to gpt_semantic_parse: header, prompt, and examples. These arguments construct the first part of the prompt that we pass to GPT-4. Assuming the actual question (\u201cIs there an object to the left of the cube?\") is passed to the foreign predicate parse_expr as the first argument s, the entire prompt becomes:\nThen, GPT-4 is prompted to produce the query, which is parsed back into our ADT Query:"}, {"title": "Putting it all together", "content": "The last part which brings everything together is the semantics of our Query DSL. The semantics is inductively defined on the Query data structure. We start from defining the variants which return a set of objects. For this, we use the eval_obj binary relation to connect queries with their evaluated object IDs:\nWe next define the semantics for queries which evaluate to a boolean, producing the eval_bool relation:\nWe finally define the semantics for queries which evaluate to a number, producing the eval_num relation:\nTo connect everything together, we apply the eval_* relation on the parsed expression to get the evaluated result:"}, {"title": "A concrete example", "content": "We illustrate a concrete example in Fig. 8."}, {"title": "Experimental Details", "content": "Our experiments are conducted on a machine with two 20-core Intel Xeon CPUs, four GeForce RTX 2080 Ti GPUs, and 768 GB RAM. Note that our experiments do not involve training and therefore do not require high-end computation resources. In this section we elaborate on the foundation models that we used in our experiments and the setup for individual tasks."}, {"title": "Model setup", "content": "GPT. The default GPT model we use is gpt-4. Depending on the task, there are a few variations we have used which include gpt-3-turbo, text-embedding-ada-002. We set the model temperature to 0 as the default value, and we cache the intermediate result locally for expense-saving purposes. For chain-of-thought (CoT) prompting, we adopt the zero-shot technique introduced by (Kojima et al. 2022). Questions that encounter an API server error are manually re-queried. All experiments are performed from June to August 2023.\nOWL-VIT. We use the OWLVITProcessor and OwlViT-ForObjectDetection models from Hugging Face. We load the pretrained checkpoint google/owlvit-base- patch32. We set the processor's score_threshold to 0.1, score_multiplier to 1.0, and expand_crop_region to 0.0."}, {"title": "Date reasoning", "content": "The DR dataset is adapted from BIG-bench's date understanding task, with 28 of the original 369 questions being corrected for wrong target answers. We solve this task by decomposing it into two sub-tasks: extracting structured information using an LM, followed by logical reasoning over the structured information using relational rules and date arithmetic.\nFor the first sub-task, we leverage GPT-4 with 5-shot prompting for extracting the following three relations from the given context:\nSee Table 7 for an example set of extracted relations. The shots for gpt_extract_relation are manually composed to be similar to questions in the dataset. For this task specifically, we configure gpt_extract_relation to use zero-shot CoT (Kojima et al. 2022) when extracting relationship, which improves accuracy by over 10%."}, {"title": "Tracking shuffled objects", "content": "The TSO dataset is randomly sampled from a combined dataset of subtasks corresponding to n = 3, 5, 7 objects from BIG-bench's tracking shuffled objects task. Specifically, our random sample contains 32 questions where n = 3, 59 questions where n = 5, and 59 questions where n = 7. Our solution relies on GPT-4 with single-shot prompting for extracting three relations:\nSee Table 8 for an example set of extracted relations. We prompt gpt_extract_relation with one shot based on a question from the BIG-bench task but not from our sampled dataset. Our reasoning program then iterates through all the swaps starting from the initial possessions and retrieves the last possessed object associated with the target.\nWe conjecture that the exemplary performance of our model on TSO is due to the highly consistent syntactic structure of the NL inputs, facilitating relation extraction under a one-shot setting."}, {"title": "Kinship reasoning", "content": "We use the CLUTRR (Sinha et al. 2019) dataset to perform the kinship reasoning task in NLP. Each data point in this task contains a story that indicates the kinship between characters, and the aim is to infer the relationship between two specified characters. We use all the 1,146 data points in the test dataset of 089907f8.\nThe intermediate symbolic representation we use GPT-4 model to extract is a kinship graph, whose edge is composed of 20 different relationships, including \"father\u201d, \u201cmother\u201d, \"uncle\", \u201cniece\u201d. For prompting the GPT-4 model, we first ask the GPT model to yield us all the kinships relations"}, {"title": "Math reasoning", "content": "This task is drawn from the GSM8K dataset of arithmetic word problems (Cobbe et al. 2021). Both our math and date reasoning datasets have previously served as benchmarks for LLM performance under chain-of- thought prompting (Wei et al. 2023; Kojima et al. 2022). The questions involve grade school math word problems created by human problem writers, and the model is asked to produce a number as the result. Since the output can be frac-"}]}