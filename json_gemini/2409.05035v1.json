{"title": "DEEP GENERIC REPRESENTATIONS FOR DOMAIN-GENERALIZED ANOMALOUS SOUND DETECTION", "authors": ["Phurich Saengthong", "Takahiro Shinozaki"], "abstract": "Developing a reliable anomalous sound detection (ASD) system requires robustness to noise, adaptation to domain shifts, and effective performance with limited training data. Current leading methods rely on extensive labeled data for each target machine type to train feature extractors using Outlier-Exposure (OE) techniques, yet their performance on the target domain remains sub-optimal. In this paper, we present Gen-Rep, which utilizes generic feature representations from a robust, large-scale pre-trained feature extractor combined with KNN for domain-generalized ASD, without the need for fine-tuning. GenRep incorporates MemMixup, a simple approach for augmenting the target memory bank using nearest source samples, paired with a domain normalization technique to address the imbalance between source and target domains. GenRep outperforms the best OE-based approach without a need for labeled data with an Official Score of 73.79% on the DCASE2023T2 Eval set and demonstrates robustness under limited data scenarios. The code is available open-source 1.", "sections": [{"title": "1. INTRODUCTION", "content": "Developing a reliable anomalous sound detection (ASD) system for machine condition monitoring requires robustness to noisy inputs, adaptability to domain shifts (e.g., changes in machine states due to temperature or background noise), and effective performance with limited training data from new installations [1]. Key ASD approaches include unsupervised methods using generative models, such as reconstruction-based methods with autoencoders (AE) [2][3][4][5][6], and distribution-based methods using auto-regressive models to predict probability density functions [7]. Self-supervised methods often assume labeled data for training a model and typically involve classification-based methods [8][9].\nClassification-based methods often achieve the best performance, with a focus on enhancing feature extractors via Outlier-Exposure (OE) techniques [10][8][9]. These techniques involve training on auxiliary tasks that distinguish between target classes based on machine types and states using cross-entropy loss, allowing the model to learn robust representations of target distributions. Anomaly detection is then performed by assigning classification scores [8] or measuring distances between feature embeddings of known normal and unknown inputs from the pre-trained extractor [9], which can also be considered as a distance-based method.\nHowever, these methods face two key challenges [1][2]: (1) they require sufficient normal data for training on both source and target domains, along with careful hyperparameter tuning to prevent overfitting and the capture of irrelevant noise; and (2) they necessitate extensive labeling for machine types and section IDs on normal and anomalous data, which can be impractical or infeasible in real-world applications. This raises the quesiton: How can robust feature representations be obtained without these challenges?\nRecent advances in large-scale pre-trained speech and audio models have shown success in general audio tasks by learning robust, transferable features. To address domain-generalized ASD challenges, we introduce GenRep, which leverages generic representations from BEATS [11] combined with a distance-based approach, k-nearest neighbors (kNN). GenRep incorporates MemMixup to balance source and target memory banks by augmenting target features with their nearest source features, and applies Domain Normalization (DN) using Z-score normalization to standardize score distributions across domains.\nOur contributions include: (1) demonstrating the effectiveness of robust, general-purpose feature representations using BEATS [11] without fine-tuning for ASD; (2) evaluating performance across BEATs' pre-training methods and layers, proposing an effective pooling strategy, and mitigating domain shift with introduced MemMixup and DN; (3) achieving state-of-the-art performance on domain-shift settings with an Official Score of 73.79% on the DCASE2023T2 Eval set [12], while also demonstrating robustness under limited data scenarios on the DCASE2020T2 Dev set [13][14].\nRelated work. Our work relates to both reconstruction-based and classification-based anomaly detection methods. Reconstruction-based approaches, like autoencoders (AE)"}, {"title": "2. METHOD", "content": "2.1. Feature extraction and pooling\nTo obtain feature embeddings for a memory bank, GenRep utilizes a pre-trained model on AudioSet, such as BEATS [11], which is based on the ViT [19] architecture with multiple Transformer [17] layers. The pre-trained model choice is flexible, allowing for unsupervised, semi-supervised, or supervised approaches [11]. Given a set of input spectrograms \\(X_{train} = {x_1,x_2,...,x_N}\\), the model maps each input to a patch embedding \\(x_i \\in R^{T \\times F \\times C}\\), where T, F, and C represent time frames, frequency bins, and channels, respectively. The temporal and spectral dimensions are then flattened into a sequence \\(x_i \\in R^{TF \\times C}\\) and processed by Transformer layers to produce embeddings \\(f_i = F(x_i)\\) with shape \\(f_i \\in R^{TF \\times C}\\). While pooling over the entire spatial dimension TF [11] is an option, it can lead to sub-optimal performance as spectral components are not directly used in kNN-based anomaly scoring. Instead, we apply mean-pooling over the temporal dimension only, preserving spectral components by reshaping the embedding to \\(f_i \\in R^{T \\times F \\times C}\\) and then applying:\n\\[f_i = \\frac{1}{T} \\sum_{t=1}^{T} f_{it}\\]\nThis results in a final embedding flattened into \\(f_i \\in R^{FC}\\).\n2.2. Memory Mixing up for handling domain shift\nWhen the target feature set \\(F_T\\) is much smaller than the source set \\(F_S\\), the proximity of source features to target test samples can introduce bias in anomaly detection performance. To address this imbalance, we propose MemMixup, a method that augments target features in the target memory bank by interpolating between each target feature and its nearest K source features. Unlike Mixup [20], which augments samples during training to enhance overall model robustness, MemMixup specifically aims to increase the target features in a target memory bank by generating new features that lie between the source and target distributions, with a stronger"}, {"title": "2.3. Anomaly detection", "content": "Using the features in the source and target memory banks, we determine if a test sample y is anomalous by computing its kNN distance separately from both the source and target memory banks. The anomaly score \\(d(y)\\) is defined as:\n\\[d(y) = \\frac{1}{K_n} \\sum_{f \\in N_{K_n}(f_y)} || f - f_y ||^2,\\]\nwhere f represents either the source features \\(f_s\\) or the target features \\(f_t\\), depending on whether we are calculating the distance from the source or target memory bank. Specifically, \\(N_{K_n}(f_y)\\) denotes the \\(K_n\\) nearest normal features f to the test sample \\(f_y\\) within the corresponding memory bank.\nIn domain shift scenarios, aligning source and target distances is essential to prevent performance degradation due to score discrepancies. To address this, we apply DN by separating source and target scores from their respective memory banks and normalizing them using Z-score. The Z-scores are computed as \\(Z-score(d_s) = \\frac{d_s - \\mu_s}{\\sigma_s}\\) and \\(Z-score(d_t) = \\frac{d_t - \\mu_t}{\\sigma_t}\\), where \\(\\mu_s, \\sigma_s\\) and \\(\\mu_t, \\sigma_t\\) are the means and standard deviations of the source and target scores, respectively. We then determine the closest domain sample for each test sample by finding the minimum distance between the normalized scores from both domains:\n\\[score = arg \\min (Z-score(d_s(y)), Z-score(d_t(y))).\\]\nThis approach aims to align the score distributions, which may help reduce the impact of domain shift and potentially improve the overall performance of ASD task."}, {"title": "3. EXPERIMENTS", "content": "3.1. Setup\nDatasets and Metrics. We use the DCASE2023T2 Eval set for domain-shift settings and the DCASE2020T2 Dev set for low-shot settings. Both datasets include machine sounds recorded in labs and mixed with real factory noise. The DCASE2023T2 Eval set [1] is designed for domain-generalized ASD, with 990 source and 10 target samples per machine type, featuring sounds from seven ToyAdmos2+ machines [21]. Recordings are single-channel, 6-18 seconds long at 16 kHz. The DCASE2020T2 Dev set [2] includes sounds from six machine types sourced from ToyADMOS [13] and MIMII [14] datasets, with 10-second recordings at 16 kHz. For domain-shift, results are based on AUC, partial AUC (PAUC), and the Official Score using the harmonic mean of source AUC, target AUC, and mixed pAUC across all machine types [1]. For low-shot, results are reported as the arithmetic mean of AUC and pAUC scores across all machine types.\nImplementation details. We use the default BEATS [11]"}, {"title": "3.2. Robustness to domain shift", "content": "3.2.1. Effect of feature representation in each layer\nFigure 1 compares the performance of each layer's representation without applying MemMixup from BEATSiter3, BEATSiter3+, and their fine-tuned versions, corresponding to unsupervised, semi-supervised, and supervised models, respectively. First, we observe that the best performances of the supervised fine-tuned models are higher than those of the unsupervised and semi-supervised pre-trained models. Second, the performance of the semi-supervised and supervised models declines from layer 6 to layer 12, while it remains stable for the unsupervised model. This suggests that the features from the deeper layers, particularly the last layer, of models trained with labels from AudioSet classes may be biased towards those classes. Finally, we find that layer 4 is the optimal layer for the supervised BEATSiter3+, achieving an Official Score of 72.96, whereas layer 5 is optimal for the other models, with official scores exceeding 72. Based on these results, we will focus on utilizing the supervised BEATsiter3+ (referred to as BEATSiter3+ ft), as it consistently demonstrates the best performance.\n3.2.2. Comparison to state-of-the-art\nThe results for domain-generalized ASD on DCASE2023T2 Eval set are shown in Table 1. We report results for Gen-"}, {"title": "3.3. Ablation studies", "content": "Table 2 highlights the impact of DN and MemMixup (\\(K_s = 990\\)), in mitigating domain shift using BEATSiter3+ and BEATSiter3+ ft. The results show that these techniques balance performance between source and target domains. For BEATSiter3+ ft, DN alone boosts the target domain AUC by 16.61%, while MemMixup alone improves it by 8.09%. Although these methods enhance the target memory bank, they can reduce the source domain AUC due to test source samples becoming closer to the target memory bank. DN generally causes a larger drop in source AUC but yields a higher target AUC, improving the official score. Combining DN with MemMixup achieves the best performance across both domains, resulting in the highest official score.\nTo evaluate whether the temporal pooling is the most effective pooling method, we compare it with the spatial pooling and the spectral pooling, on the DCASE2020T2 Dev set and the DCASE2023T2 Eval set. As shown in Table 3, tem-"}, {"title": "3.4. Robustness to limited data", "content": "We evaluate GenRep in a low-shot setting with training samples ranging from 4 (0.12% of the dataset) to 200 (5.96%), using five different random seeds for the training split (except for the full-shot setting), and report the average scores. Note that MemMixup is not applied in this setting. Table 4 shows the results of GenRep compared to unsupervised generative approaches. Our observations are as follows: (1) With about 6% of the training data (200-shot), GenRep matches the performance of the AE baseline [2]; (2) GenRep in half-shot and full-shot settings is competitive with advanced AE approaches [3][4][5]. Furthermore, GenRep can leverage section ID labels for domain normalization (DN), improving performance over the label-free method by up to 2.1% (full-shot) in AUC scores."}, {"title": "4. CONCLUSION", "content": "In conclusion, our method, GenRep, demonstrates strong performance and generalizability across both source and target domains, as well as in limited data scenarios. By leveraging BEATS [11] as a pre-trained feature extractor without fine-tuning, along with techniques such as temporal pooling, MemMixup, and Domain Normalization, GenRep effectively enhances domain-generalized ASD. This approach addresses key challenges, including domain shifts and noisy inputs, without the need for extensive labeled data or complex tuning, highlighting a promising direction for developing efficient and adaptable ASD systems."}]}