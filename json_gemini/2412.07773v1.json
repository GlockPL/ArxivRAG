{"title": "Mobile-TeleVision: Predictive Motion Priors for Humanoid Whole-Body Control", "authors": ["Chenhao Lu", "Xuxin Cheng", "Jialong Li", "Shiqi Yang", "Mazeyu Ji", "Chengjing Yuan", "Ge Yang", "Sha Yi", "Xiaolong Wang"], "abstract": "Humanoid robots require both robust lower-body locomotion and precise upper-body manipulation. While recent Reinforcement Learning (RL) approaches provide whole-body loco-manipulation policies, they lack precise manipulation with high DoF arms. In this paper, we propose decoupling upper-body control from locomotion, using inverse kinematics (IK) and motion retargeting for precise manipulation, while RL focuses on robust lower-body locomotion. We introduce PMP (Predictive Motion Priors), trained with Conditional Variational Autoencoder (CVAE) to effectively represent upper-body motions. The locomotion policy is trained conditioned on this upper-body motion representation, ensuring that the system remains robust with both manipulation and locomotion. We show that CVAE features are crucial for stability and robustness, and significantly outperforms RL-based whole-body control in precise manipulation. With precise upper-body motion and robust lower-body locomotion control, operators can remotely control the humanoid to walk around and explore different environments, while performing diverse manipulation tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Our ability to precisely grasp objects while being on the move is essential to our daily lives. Looking at humanoid control, what we demand of its arms is quite different from those of the legs: We want the arms to be versatile enough to apply variable forces, to hold objects at a fixed position and to move rapidly, such as when catching a ball. Meanwhile, its legs must also maintain balance. It should be able to walk while carrying a box, or to run while it dribbles. Current state-of-the-art humanoid whole-body control is unable to do this. The key missing piece is a way to integrate expressive and load-bearing movements of the upper body with control of the legs that fully captures common ranges of motion.\nRecent advances in whole-body humanoid control using Reinforcement Learning (RL) have demonstrated the ability of robots to robustly execute a wide range of motions [1]\u2013[3] from large-scale human motion dataset [4] or teleoperation [5], [6]. RL commonly exposes the robot in simulation to a variety of target motions and failure modes. Such setups are able to produce performant policies that can directly deploy in the real world, but at the expense of expressiveness, due to Reinforcement Learning's tendency to seek and overfit to particular modes. RL policy is also not well-suited for high DoF positional and orientation control [7], [8] and can output unpredictable actions for arms, especially in dynamic walking scenarios. It brings additional complexity to training the policy as well. These experimental results share a consensus with our observations that different mechanisms are required to control arms and legs.\nThe upper body's movements can be efficiently computed using inverse kinematics (IK), without needing to address complex balance challenges. Based on this, we propose removing the upper-body tracking during RL training, leaving the RL controller responsible solely for robust walking based on velocity commands. This approach reduces exploration and training costs while ensuring high precision for the upper body using direct joint position control.\nWhile decoupling upper and lower bodies brings benefits to the manipulation precision, prior studies [2], [3], [8] also show that fully decoupling the control of the upper and lower body may lead to instability or loss of balance, as it fails to account for the interaction between them. The goal of this work is to develop an integrated system that combines the precision and expressiveness of classical motion planning for the upper body, with the real-world robustness of locomotion control trained via deep reinforcement learning, in a single, whole-body control system. We do so by treating this as a representation learning problem, that starts with learning Predictive Motion Priors (PMP) of the upper body movement using a conditional variational autoencoder (CVAE) to encode upper-body motions as the observation of the policy. Specifically, we first train a CVAE on a motion dataset. Conditioned on past motion frames, the decoder of the CVAE predicts future motion frames with a randomly sampled latent vector. Next, we train an RL locomotion policy that observes the latent vector as an observation. The output of the RL policy only involves motors for the lower body and the upper body motor angles can be set in a general way such as IK or motion retargeting, resulting in higher precision.\nThis decoupled structure is particularly well-suited for teleoperation systems designed for loco-manipulation tasks, where the operator can control the robot's mobility using simple velocity commands while simultaneously performing precise arm and hand operations. By leveraging inverse kinematics (IK) and hand re-targeting, the teleoperator can efficiently perform complex manipulation tasks without the need to focus on balancing or lower-body coordination. As shown in Figure 1, this enables us to perform fine, dynamic, and load-bearing manipulation tasks more efficiently, even when the robot is on the move. We compare our method and previous approaches in detail in Table I.\nTo test the generalizability of our method, we experiment with both the Fourier GR1 and the Unitree H1 robots in simulation, and evaluate the Unitree controller in the real world. We test its robustness by performing large motions in"}, {"title": "II. PROBLEM FORMULATION", "content": "We focus on providing a lower-body controller that tracks a given body velocity while maintaining stability when the upper body performs dynamic actions.\nWe consider this problem as a goal-conditioned motor policy \u03c0: G \u00d7 S \u2192 A. The goal space G includes the target behavior of both the root and the upper body. S consists of proprioception and auxiliary information obtained from observations. The auxiliary information in our work is the motion prior, which we will go into detail in the next section. A is the action space, which contains the PD torque control input for each joint.\na) Goal Space for Locomotion Control and Upper Body Motion: The goal space G contains two parts: the locomotion goal $G_m = (v, r, p, y, h)$ for the lower body, and the upper body motion goal $G_u = (q_{\\text{upper}})$. The base locomotion is to track the given linear velocity $v \\in \\mathbb{R}^3$, orientation described by Euler angles roll r, pitch p, and yaw y, and the base height h. $q_{\\text{upper}}$ is the target position of upper body joints including the waist, shoulders, elbows, wrists, and hands. For the target platforms of this paper, H1 (with dexterous hands) and GR1 contain 27 and 19 upper-body joints, respectively.\nb) Decoupled control policy: We decouple the control policy by $\\pi = [\\pi_{\\text{upper}}, \\pi_{\\text{lower}}]$. $\\pi_{\\text{upper}}$ is the open loop control, directly outputting the upper body motion goal $q_{\\text{upper}}$. $\\pi_{\\text{lower}}$ is trained by RL. It observes proprioception $s_t = [q_t, \\dot{q_t}, \\omega_t, g_t, a_{t-1}]$ and the locomotion command where $q_t, \\dot{q_t}$ is current joint positions and velocities, $\\omega_t \\in \\mathbb{R}^3$ is current base angular velocity, $g_t$ is current projected gravity, $a_{t-1} \\in \\mathbb{R}^{12}$ is the previous action. Both GR1 and H1 have 12 joints in the lower body. Our method takes auxiliary information as input including gait periodic signals and motion prior."}, {"title": "III. METHOD", "content": "As illustrated in Figure 2, our training pipeline consists of three stages. The first stage involves preprocessing a given human motion dataset, where we apply a local rotation mapping to convert the local rotation matrices into joint angles on the robots. We follow the data filtering and retargeting process described in ExBody [2] to get a retargeted\nA. Motion Prior Training\nSince our control policy decouples the upper and lower body, the robustness of the lower body control becomes critical when the upper body controller has separate policies. The key to addressing this challenge lies in informing the lower body control with prior knowledge of upper body movements. Specifically, we estimate the future motions of the upper body from previous motions based on the retargeted dataset, and train a representation to be incorporated in the state space of the lower body control. We define this representation as Predictive Motion Priors (PMP).\nConditional variational autoencoder (CVAE) has been shown as an effective motion representation [9], [10]. A CVAE architecture contains a prior model R, an encoder E, and a decoder D. Given an upper-body motion sequence within a time frame of t - W tot + W, where W is a time window, the prior model R calculates a prior distribution for a latent vector $z_t$. It allows us to generate the essential future upper-body motion from $z_t \\in \\mathbb{R}^H$. $z_t$ is the desired motion prior for our policy.\nFormally, we model the encoder and prior distribution as diagonal Gaussian:\nR(\\cdot|M) = N(\\cdot|\\mu_{\\phi}(M), \\sigma_{\\phi}(M)) (1)\nE(\\cdot|M, M^{upper}) = N(\\cdot|\\mu_{\\theta}(M, M^{upper}), \\sigma_{\\theta}(M, M^{upper})), (2)\nwhere $M_t^{lower} = q_{t-\\text{W}:t-1}$ and $M_t^{upper} = q_{t:t+W-1}$ are two consecutive motion sequence.\nDuring training, (M, $M^{upper}$) are sampled from the dataset M. The objective in the evidence lower bound is as follows:\n$\\log P(M^{upper}|M^{lower}) \\geq \\mathbb{E}_{z_t \\sim E} [D(M^{upper}|z_t, M^{lower})]\n- KL (E(z_t|M, M^{upper}) || R(z_t|M)).$\nIn the specific implementation, the structures of R, E, and D are three-layer MLPs, and the size H of $z_t$ is 64. The decoder D is responsible for reconstructing the future motion sequence $M^{upper}$ from the latent vector $z_t$. The decoder learns to map $z_t$ back into the space of motion sequences, aiming to generate a future motion sequence $M^{upper}$ that closely resembles the original future motion sequence. The window length W corresponds to a real time of 1s. Due to different control frequencies, W is 50 for H1 and 100 for GR1. In the next stage, we use the prior model R to calculate motion prior $z_t$, which is as state input for the control policy.\nB. Decoupling Policy Training\nWe use PPO [11] to train the lower-body policy. During training, the environment will randomly sample a motion sequence $M_t^{upper} = q_{t:t+W-1}^{upper}$ from the retargeted motion dataset M at the beginning of a new episode. At step t, $q_{t}^{upper}$ will be directly set as the PD target of the humanoid's upper body. To overcome the exploration burden for RL with hard motions, we introduce an adaptive difficulty factor $a_i$ for each motion i, adjusting the amplitude of the upper body motions in a curriculum schedule. Given a sequence of upper-body motion i, denote the target joint position to be $q_{target}^{upper}$. For the PD controller of the joint position during the curriculum, we follow\n$q_{curriculum}^{upper} = q_{o} + a_i (q_{target}^{upper} - q_{o})$ (3)\nwhere $q_{o}$ is the default joint position for the robot.\nThe difficulty factor $a_i$ changes when the environment is reset with the following rule:\n$A_i \\leftarrow\n\\begin{cases}\na_i +0.05 & \\text{survival time} \\geq 90\\% \\text{total time}\n-0.01 & \\text{otherwise}\n\\end{cases}$(4)"}, {"title": "IV. EXPERIMENTS", "content": "We use the Unitree H1 robot for our real-world experiments and an additional Fourier GR-1 robot for simulation experiments. The details of the H1 robot are shown in Figure 4 right. We aim to answer the following questions by both simulation and real-world experiments:\n1) How well does PMP perform compared to those methods in learning-based upper-body control?\n2) How does the tracking precision and the stability of PMP perform when a disturbance occurs?\n3) How does PMP perform in real-world scenarios requiring both robust locomotion and precise manipulation?\nA. Simulation Results\nTo answer Q1 in simulation, the performance of PMP is evaluated by comparing it with the following baseline:\n\u2022 Exbody: This baseline's upper body control policy is RL-trained with tracking rewards for upper body target keypoints and joint positions. The code for H1 is sourced from the Exbody's [2] codebase. The code for GR1 is sourced from our own reimplementation.\n\u2022 Exbody (Whole): This baseline's upper body control policy is RL-trained with tracking rewards for whole-body target keypoints and joint positions. The H1 code is sourced from Exbody's codebase, and similar methods include HumanoidPlus [1] and OmniH2O [3] in terms of reward settings and RL training.\n\u2022 PMP without motion prior: This baseline uses decoupled policy but without motion prior.\n\u2022 PMP without motion curriculum: This is an ablation study on motion curriculum. In this baseline, the a in the motion curriculum is always set to 1.\nOur metrics are as follows:\n1) Precision: upper joint position error $E_{jpe}^{upper}$, upper key point position error $E_{kpe}^{upper}$.\n2) Smoothness: upper joint acceleration $E_{acc}^{upper}$, upper action difference $E_{action}^{upper}$.\n3) Locomotion: command linear velocity error $E_{vel}$, command angular velocity error $E_{ang}$.\n4) Stability: lower joint acceleration $E_{lower}^{lower}$, lower action difference $E_{action}^{lower}$, projected gravity $E_{g}$.\nTo answer Q2, we conduct robustness tests under perturbations such as pushing the robot and varying the playback speed of upper-body motions. We assess precision by the average upper DoF error $E_{jpe}^{upper}$ and stability by the average projected gravity $E_{g}$.\nB. Teleoperation Setup\nTo test the performance of our method in the real world, we set up a real-world teleoperation system with the Unitree H1 robot. The teleoperation of the active neck and upper body follows the pipeline described in [5].\nC. Real-World Results\nTo answer Q3, we conduct experiments to evaluate the locomotion robustness and manipulation precision."}, {"title": "V. RELATED WORK", "content": "Whole-Body Control for Humanoid Robots. For humanoid robots to be useful, they need to be both robust in mobility and precise in manipulation. This is previously primarily achieved by dynamics modeling and control [14]\u2013[27]. More recently, deep reinforcement learning methods have shown promise in learning complex locomotion skills for legged robots [8], [28]\u2013[46]. Whole-body loco-manipulation from high-dimensional sensory inputs has been studied for quadrupeds [8], [47]\u2013[49] and humanoids [1]\u2013[3], [32], [50], [51]. In [1]\u2013[3], authors train whole-body policies that output the actions for the entire body for robustness. [2] decouples the upper and lower bodies' objectives to compensate for morphological differences between humans and robots. [1] trains one transformer for whole-body control of the humanoid robot and another transformer for imitation learning. [3] trains goal-conditioned policies for various downstream tasks. However, these works focus on whole-body control with smaller arm DoFs (4 or 5) and do not study how the precision of the upper body affects manipulation.\nLoco-Manipulation. Quadrupedal robot can either use their legs for manipulation [48], [52]\u2013[55], or with an additional mounted manipulator [7], [8], [49], [56], [57]. Humanoid robots naturally have two arms for manipulation and two legs for locomotion. [58] uses hierarchical visual-motor policy with implicit-hierarchical whole-body control to do fine manipulation. However, no robust walking is shown in the real world and the focus is on static manipulation. [59] studies how to complete box loco-manipulation task with sim-to-real techniques and lacks the ability to generalize to a wide variety of tasks.\nMotion Representation Learning. Human and robot motions are in high-dimensional temporal-spatial space. A good representation of motion facilitates motion understanding, generation, and imitation. Adversarial imitation methods such as [60]\u2013[63] work well with small motion datasets but suffer from mode collapse as the motions scale. [64]\u2013[67] learn reusable motion representations via conditional variational encoders in simulation to generate novel motions. However, how CVAE can coordinate whole-body movements for real humanoid robots has not been well studied."}, {"title": "VI. DISCUSSION AND LIMITATION", "content": "This paper introduces a novel whole-body controller for humanoid robots, which separately models the upper body with IK and retargeting, and the lower body with RL controllers. To bridge upper and lower bodies for robust control, we introduce Predictive Motion Priors to encode upper-body information for lower-body RL training. We show superior performance in both simulation and real-robot whole-body control, as well as teleoperation experiments.\nWhile our method allows the humanoid robot to conduct a variety of loco-manipulation tasks, the separation of upper-body and lower-body controls limits the robot from certain agile tasks. However, the current humanoid hardware still has too few degrees of freedom compared to humans, which makes RL tracking agile whole-body motion very challenging from the practical perspective. The whole-body policy accepts various input commands such as forward/lateral, and yaw velocity, bringing burden for the human teleoperator since the arm and hand movements are mapped to control the robot's arms and hands. We need a better human-robot interaction interface to address these challenges."}]}