{"title": "EBES: EASY BENCHMARKING FOR EVENT SEQUENCES", "authors": ["Dmitry Osin", "Igor Udovichenko", "Viktor Moskvoretskii", "Egor Shvetsov", "Evgeny Burnaev"], "abstract": "Event sequences, characterized by irregular sampling intervals and a mix of categorical and numerical features, are common data structures in various real-world domains such as healthcare, finance, and user interaction logs. Despite advances in temporal data modeling techniques, there is no standardized benchmarks for evaluating their performance on event sequences. This complicates result comparison across different papers due to varying evaluation protocols, potentially misleading progress in this field. We introduce EBES, a comprehensive benchmarking tool with standardized evaluation scenarios and protocols, focusing on regression and classification problems with sequence-level targets. Our library \u00b9 simplifies benchmarking, dataset addition, and method integration through a unified interface. It includes a novel synthetic dataset and provides preprocessed real-world datasets, including the largest publicly available banking dataset. Our results provide an in-depth analysis of datasets, identifying some as unsuitable for model comparison. We investigate the importance of modeling temporal and sequential components, as well as the robustness and scaling properties of the models. These findings highlight potential directions for future research. Our benchmark aim is to facilitate reproducible research, expediting progress and increasing real-world impacts.", "sections": [{"title": "Introduction", "content": "The world we live in is constantly changing [26]. We continuously collect and analyze data to understand and navigate this dynamic environment. This ongoing data collection helps capture the evolving nature of reality and can be captured in sequential datasets, which can be further analyzed or used for modeling.\nVarious types of sequential data are usually approached differently based on their characteristics. One prevalent form of sequential data is time series, regular measurements of some processes. The uniformity of these intervals enables researchers to apply a wide range of developed techniques [16]. Measurements of some processes that are taken or observed at non-uniform time intervals lead to irregularly sampled time series (ISTS). Fewer methods exist specifically for ISTS [16], and modeling them brings new challenges [27]. However, modeling them has a considerable importance since they naturally occur in many real-world areas: ecology [10], astronomy [37], climate [38], biology [16], medicine [18, 25, 35], geology [17] and finance [5].\nAnother widely explored temporal data type is a stream of discrete events. Intervals between events are random, and modeling the distribution of inter-event intervals is an essential task with many applications. Temporal point process (TPP) model is commonly employed to model streams of discrete events [14, 30, 33, 24, 39, 51, 54, 53, 42].\nIn this work, we focus on another type of sequential data, event sequences (EvS), which are sequences of observations made at irregular times characterized by numerical and categorical features. EvS can be viewed as a generalization of both ISTS and streams of discrete events. Examples of various types of EvS are illustrated in Figure 1. Many modeling tasks naturally arise when dealing with sequential data, including whole sequence classification and regression [40], extrapolation or forecasting [13], missing data imputation [36], point-wise classification [21], and predicting the next event's time and type [48]. Some of these tasks assume either a continuous or discrete nature of the data, which may not be known given a raw dataset. For instance, predicting the time of the next event is not reasonable when dealing with measurements from a continuous process. However, we can perform an assessment of the entire sequence regardless of the assumptions about the nature of the data.\nAs a task we consider the whole EvS classification and regression task, which we refer to EvS assessment. We emphasize the crucial role of EvS classification and regression in medicine [40], churn prediction [23], e-commerce [52], fraud detection [46] and more.\nOur contributions are as follows:\n\u2022 We introduce EBES, a comprehensive benchmarking framework designed for EvS assessment. EBES features unified interfaces for datasets, models, and experimental protocols, facilitating future research in EvS assessment. Our library is publicly available.\n\u2022 We design a benchmark protocol that considers both model and dataset analysis. Our evaluation includes various scenarios, including some specific to EvS, highlighting important properties of both the datasets and models.\n\u2022 Using EBES, we evaluated various methods on established datasets through a multi-phase evaluation protocol. This approach ensures a fair and consistent comparison across different methods. All results are tested for statistical significance. As a result of our analysis, we provide recommendations for future research. These recommendations include possible pitfalls related to dataset usage and model evaluation."}, {"title": "Benchmark goals and approaches", "content": "Numerous methods have been proposed for EvS modeling and related problems. However, most of these methods lack rigorous evaluation, and there is currently no established benchmark for this domain. Benchmarking machine learning algorithms involves two main components: benchmark design and datasets, each presenting its challenges and goals. Below, we describe how we address each challenge in the context of EvS."}, {"title": "Datasets", "content": "We have chosen three commonly used datasets based on previous studies [41, 44, 4, 32], one recent and one of the largest event sequence datasets MBD [15], two medical datasets, and one synthetic pendulum dataset to validate the importance of time and how models capture the sequential properties of the data. \nData Quality One of the primary challenges in benchmarking is ensuring that the datasets used are high quality and accurately represent the problem domain. Poor data quality can lead to misleading benchmark results.\nTo address data quality, we employ two strategies:\n\u2022 Synthetic Dataset Development: We create a synthetic Pendulum dataset, particularly useful for evaluating time- sensitive methods; dataset creation is described in Appendix C.\n\u2022 Dataset Analysis: We analyze the correlation of model performance with Monte Carlo cross-validation. Specifically, we consider the relationship between metrics across various folds and the holdout test set.\nDiversity of Datasets. Datasets with similar structures but different domains can vary greatly. For example, financial transactions differ significantly from medical records. Additionally, datasets can vary in complexity and difficulty. Our work includes a diverse range: two medical, three banking, one retail, and one synthetic dataset.\nVolume of Data. Large datasets enable models to capture the complexity and nuances of real-world phenomena, leading to more accurate and reliable predictions. Moreover, different algorithms scale differently as the data grows. To address this challenge, we included datasets of various sizes.\nOpen Access to Data. It is crucial that data is available to researchers worldwide for reproducibility, collaboration, and innovation. While many event-sequence datasets exist, we focus on open-access ones and welcome contributions from other domains to enhance our collection. For example, astronomical observations [6] are event sequences but are not openly accessible."}, {"title": "Benchmark Design", "content": "Creating effective benchmarks is a complex task, which involves designing tests that accurately reflect the capabilities of machine learning models across different scenarios:\nModel evaluation. Hyperparameters are a fundamental aspect of machine learning that directly impacts model performance. However, the procedure of hyperparameters tuning is rarely described. Therefore, this becomes a source of non-reproducibility [3, 20]. Moreover, manual hyperparameter tuning can lead to the leakage of the test set into the training procedure and performance [28], and testing different hyperparameter values is necessary to find a model that generalizes well [20].\nIn our procedure, we first conduct an extensive hyperparameter search. Randomness can destabilize models, causing large variances in results across training runs. Ignoring this sensitivity can create a false perception of research progress [34]. Therefore, after determining the optimal hyperparameters, we perform Monte Carlo cross-validation [47] with 20 seeds to evaluate the final model.\nScalability As datasets grow larger, machine learning algorithms scale differently. Large datasets enable models to capture real-world nuances, improving prediction accuracy. To address this, we study the scaling properties of various event sequence assessment algorithms.\nImportance of Time and Sequence Order It is possible to perform EvS assessment while disregarding the temporal and sequential nature of the data. To evaluate the importance of each component, we designed two stress tests for event sequences: rearranging the sequence order and replacing time components with noise. This analysis provides significant insights and highlights future research directions.\nModel Granularity As AI systems grow more complex, assessing which components contribute to success becomes challenging. In our work, we evaluate different components, such as various aggregation approaches along the temporal dimension, batch normalization, and the impact of adding time as a separate feature on overall model performance."}, {"title": "Benchmark Accessibility and Maintenance", "content": "The rapid evolution of machine learning makes keeping benchmarks up-to-date challenging. Benchmarks must reflect the latest advancements, incorporate new data and algorithms, and be maintained over time. Our work focuses on developing an easy-to-use plug-and-play codebase to facilitate collaboration and research. The library's interface structure enforces the independence of implementing new datasets, methods, and experiments, making adding and testing new components easy. We are committed to maintaining this benchmark and encourage contributions from researchers and practitioners to support reproducible research."}, {"title": "Models", "content": "We have carefully selected a diverse set of popular models and approaches that have been previously applied for EvS assessment. Some of the models, such as MLP, are included as baseline solutions, some are commonly used for sequential data GRU [9], Mamba [19], Transformer [45]. The following models were explicitly designed to handle the unique challenges associated with EvS: mTAND [41], PrimeNet [8] and CoLES [4]."}, {"title": "Benchmarking Methods", "content": "In our work we aim to perform as little preprocessing as possible to preserve the originality of the data in order to prevent data preproccessing from affecting model evaluation. For ease of extensibility, we convert all datasets into a single format and release scripts that perform the conversion.\nOur data preprocessing includes:\n\u2022 Applying a logarithm to fat-tailed variables, which are selected manually;\n\u2022 Rescaling time points to make the time range of all sequences to fall in [0, 1];\n\u2022 For missing values, we propagate them forward for the PhysioNet, MIMIC-III, and Pendulum datasets based on results in [7], and impute with constants for others.\n\u2022 We encode categorical features using embedding layer and treat missing values as additional categories."}, {"title": "Model evaluation and HPO", "content": "Hyperparameter optimization (HPO) and Monte Carlo cross-validation are at the core of our benchmark design, as they enable us to evaluate numerous design choices and hyperparameters, and to fairly compare models. Furthermore, we derive important insights from multiple HPO runs. Our evaluation procedure is twofold:\n\u2022 HPO step, here we perform hyperparameter optimization for all the models for each dataset. After obtaining the set of best hyperparameters (BHP), we use them for the next step.\n\u2022 Final evaluation, during this step we train models with BHP 20 times using different seeds and random train and train-val splits. Final metrics are reported as average with standard deviation over 20 runs on test sets after models were trained from scratch."}, {"title": "Experiments and Results", "content": "In this section, we address the main question of the benchmark: Which model performs the best? The results are presented in Table 2, where methods are sorted from top to least performing. Along with the mean performance we report method's rank as a superscript. We performed pairwise Mann-Whitney U test [29] with Holm-Bonferroni correction [22], methods with no significant performance difference (p > 0.01) share the same superscript. All top three performing methods are based on GRU with different pre-training strategies. Mamba and Transformer comes next in rating, suggesting that this architectures are less suitable for EvS. mTAND [41] demonstrated best performance on Pendulum dataset, this result can be attributed to the mTAND architecture, which is specifically designed to model the time component.\nThe MLP performs relatively well, typically within 5% of the top-performing method on all real-world datasets. This suggests that EvS assessment can be effectively carried out using aggregated statistics along temporal dimensions, a practice commonly employed in industrial applications with boosting models [1]. The difference in performance between MLP and mTAND on the Pendulum dataset further supports this idea, since we can not apply such aggregation approach to this dataset.\nWe see that, all methods show close results on the PhysioNet2012 dataset, based on ranks. This raises questions about its suitability for evaluating models for EvS assessment task.\nTrain-Val-Test splits For both steps we utilize data splits as follows: train - for training models, train-val - for early stopping procedure, we stop training if the model performance does not improve after several epochs and exceeds patience limit, hpo-val - a subset to evaluate the model to update HPO sampler, it does not present in Final evaluation step. Both train-val and hpo-val take 15% from the initial train dataset. See Figure 2 for clarification. For each split we apply on-target stratification. The number of patience steps is different for each dataset due to computational constrains.\nFor datasets, which do not have commonly accepted test sets, we cut 20% as our fixed test set. For HPO we use Optuna [2] Tree-structured Parzen Estimator (TPE)."}, {"title": "Dataset Analysis", "content": "In this section, we analyze datasets based on data from the HPO step and Final evaluation phases, exploring relationships between metrics from different data subsets. \nDuring the HPO step, we observe overfitting for most datasets, as train metrics increase while train-val metrics plateau. This supports the use of early stopping.\nMetrics of hpo-val and test subsets are strongly correlated unless the test set is sampled out-of-time, as seen for the Taobao dataset. Here, hpo-val and test metrics lack a clear linear trend, but train-val and hpo-val metrics do, suggesting a distribution shift in the test set.\nFor most datasets, in the Final evaluation phase validation and test set metrics exhibit a linear trend, except for PhysioNet2012, where different validation metrics attribute to similar test metrics. This supports our observations in Section 4.1, where results for most models are not statistically distinguishable for most methods on PhysioNet2012."}, {"title": "Data Scaling Results", "content": "To study the scaling properties of various models, we evaluated each model trained with different numbers of sequences. We focused on two biggest real-world dataset in our benchmark: Retail and MBD. We sampled different subsets, each containing progressively more data. Each model was trained from scratch on different-sized subsets with Monte Carlo cross-validation using three random seeds.\nA common approach is to estimate model performance with a fixed data size. However, as seen in Figure 4, while all models improve with the growth of the data, their ranking does not stay the same, except for CoLES on the Retail Dataset, where it demonstrates superior performance. With some data size, even MLP becomes a top performer. Most models, except for MLP, mTAND, and PrimeNet, converge to similar performance on the MBD dataset given a large data size. It is worth noting that for each dataset, we used the BHP found for each model when the dataset was at its full size."}, {"title": "Assessing Architecture Design Choices", "content": "Although our models exhibit a diverse range of architectures, there are several common design choices among them. We evaluated the impact of these choices as part of our HPO procedure.\nWe observe that some design choices depend more on the dataset than on the method, highlighting the importance of HPO for fair evaluation. First, we study the effect of different aggregation approaches along the temporal dimension on overall performance. We focus on two approaches: mean across all hidden states and the last sequence state. The best aggregation strategy depends more on the dataset than on the method. Similarly, batch normalization for numerical features improves performance for almost all methods and datasets, except for Pendulum. Finally, we evaluate the importance of hyperparameters according to HPO. There is no clear winner except for the learning rate, which is often the most important hyperparameter across all HPO runs."}, {"title": "Importance of Sequence Order", "content": "One aspect of EvS is the order of events in a sequence. To examine its importance, we conducted two experiments: 1) We took models trained on regular data and evaluated them on test sequences with permuted order, keeping the time component unchanged. 2) We removed the time component and retrained the models on sequences with permuted order, then evaluated them on permuted test sequences.\nTesting on Permuted Sequences We evaluated pre-trained models from the Final evaluation step on perturbed sequences. Missing values were filled prior to shuffling, and time was added as a numerical feature before shuffling. For all runs, the last events were kept in their original positions, as some models use the last hidden state in the aggregation step.\nResults are presented in Table 3. The Transformer model experienced a significantly small drop due to its attention mechanism. The MLP model did not experience any drop at all because sequence order is inherently not important for aggregation. We observed that while performance dropped for other models, the drop was statistically significant (p < 0.01) but less than expected for all real-world datasets. Additionally, the MBD dataset did not experience a significant drop with most methods, suggesting that models do not rely on the order of sequences to make predictions. This indicates that while sequence order is important, it is not as critical for EvS assessment of real-world datasets as initially thought. However, we observed that models' performance degraded on the pendulum dataset, indicating that the evaluated models can capture the sequential nature of the data.\nTraining on Permuted Sequences The second experiment further analyzed datasets to determine if sequential order is important or if sequences can be treated as a \"bag of words.\"\nWe selected the GRU with BHP for each dataset, removed the time component from its architecture, and trained it from scratch with both training and test sequences permuted. The results are in Table 4. We observed that for some real-world datasets, the performance drop was not statistically significant. We speculate that such permutation could even serve as a form of data augmentation, since in some cases mean metrics increased with permutation. Notably, after retraining on permuted sequences, we observed a significant drop on the MBD dataset. At first, this seems to contradict the results from the previous section. However, upon considering that the time component was also removed, we conclude that in the MBD dataset, time component is crucial while the order is not.\nFrom both experiments, we conclude that sequence order is important for EvS assessment, but it is less critical than expected for real-world datasets and varies from dataset to dataset."}, {"title": "Importance of Time", "content": "Next, we evaluate the role of time in EvS. Similarly to the previous section, we perform two experiments: 1) using random time-steps on pre-trained models during testing, and 2) adding or removing time as an extra feature to train the models.\nIncorporation of Event Time Information into Models To evaluate the importance of time, we follow a simple procedure. First, we note that time is rescaled during preprocessing. After that, there are three options to incorporate it into the model, all of which are searchable during hyperparameter optimization (HPO): No time - Do not use time at all; Time delta - Compute the time difference from the previous step and concatenate it as a feature; Absolute time - Concatenate the rescaled time as a feature.\nThe results in Table 5 indicate that time significantly improves performance, if added, to three datasets: MBD, MIMIC-III, and Pendulum. Surprisingly, it is important for almost all datasets if we use the Transformer. However, we cannot make the claim for other methods and datasets that the time is not important, as there are various other ways to incorporate it into models that may show statistically significant improvements, but we did not explore them.\nRandom Timestamps In our work, two methods are specifically designed to model the time component: mTAND [41] and PrimeNet [8]. We evaluated them on test data with noisy timestamps, where the original timestamps were replaced with random values sorted in ascending order. The results are presented in Table 6. While time is important for these models on the synthetic Pendulum dataset, it did not contribute significantly to the other datasets.\nFrom the observations above, we first see that time is important and contributes to EvS assessment. Secondly, we observe that methods specifically designed to work with time do not effectively capture temporal dependencies on real-world datasets. This emphasizes the importance of developing or testing new methods on EvS that can model the time component on real-world datasets."}, {"title": "Related Work", "content": "The UCR Time Series Archive, widely used for time-series classification, is limited to univariate time series, offering 128 datasets for algorithm evaluation [12]. Despite its extensive use, this benchmark does not address the complexity of event sequence data, crucial for many real-world applications. The torchtime package [11] extends the utility of UEA & UCR datasets by providing reproducible implementations for PyTorch, simplifying data access and ensuring fair model comparisons, it is still primarily focuses on time series classification. EasyTPP [48] is a new benchmark targeting streams of discrete events, offering a centralized repository for evaluating TPP models. It emphasizes reproducible research through a standardized benchmarking framework and provides various research assets. However, EasyTPP cannot be extended to handle general EvS, as event sequences generally cannot be modeled using TPP. The sequence of card transactions made by a client is a good example of EvS. Each transaction is characterized by attributes such as transaction amount and merchant category code, making them unfit for time series or discrete event streams categories. Authors in [5, 50, 4] evaluate several representation learning approaches on event sequences. In [5], the authors propose a protocol for evaluating obtained representations on a set of downstream tasks."}, {"title": "Conclusion", "content": "In this work, we presented EBES, an open and comprehensive benchmark for the standardized and transparent comparison of event sequence models. The benchmark includes a diverse range of datasets and models. Additionally, it provides a user-friendly interface and a rich library, allowing for the easy integration of new datasets and the implementation of new models. With these features, EBES has the potential to facilitate future research in event sequence modeling significantly.\nWe emphasize the importance of HPO and cross-validation for fair model evaluation. Moreover, we recommend performing several runs to validate if the model performance is statistically significant, especially on small datasets. This is also supported by scaling experiments, where model rankings tend to change significantly on smaller data sizes and slowly converge to the same point as the data size grows while the standard deviation decreases."}, {"title": "Appendix", "content": ""}, {"title": "Models description", "content": "GRU We have chosen to use the GRU as one of our base models due to its proven effectiveness in encoding time- ordered sequences [4, 36, 43, 49, 44]. In recent study on neural architecture search [44]), authors demonstrated that architectures with RNN blocks tend to exhibit higher performance on average on EvS assesment task.\nMLP The models applied 3 linear layers with the ReLU nonlinearity and dropouts in between to the aggregated embeddings obtained right after the preprocessing block. So effectively the model is just a basic MLP applied to aggregations. Models for EvS handles the sequential nature of data in a special way, ofthen considering the exact time intervals between the events, so we were interested in the performance of the model, that consciously discards the sequential nature of data.\nMamba Mamba [19] is a recent state-space model (SSM) that has been designed for efficient handling of complex, long sequences. It incorporates selective state spaces to deliver top-notch performance across different modalities, including language, audio, and genomics, outperforming Transformers in some scenarios. For the best of our knowledge Mamba has not been applied to EvS assessment previously, however, we believe that type of models worth of investigating.\nmTAND Authors in [41] proposed an architecture which learns an embedding of continuous-time values and utilizes an attention mechanism to produce a fixed-length representation of a time series. This procedure is specifically designed to deal with ISTS and has been shown to outperform numerous ordinary differential equations-based models such as Latent ODE and ODE-RNN [36].\nCOLES The contrastive pretraining method for sequential data was proposed by [4]. We specifically focus on this method due to its superior performance compared to other contrastive approaches demonstrated in the work. CoLES learns to encode a sequence into a latent vector by bringing sub-sequences of the same sequence closer in the embedding space while pushing sub-sequences from different sequences further apart.\nPrimeNet The method proposed in [8] also, falls under the category of self-supervised. It utilizes time-sensitive contrastive pretraining and enhances pretraining procedure with data reconstruction tasks to facilitate the usage of unlabeled data. Authors modify mTAN architecture by replacing an RNN block with Feature-Feature Attention.\nMLEM The Multimodal Learning Event Model [31] is a recently proposed method for Event Sequences that unifies contrastive learning with generative modeling. It treats generative pre-training and contrastive learning as distinct modalities. First, a contrastive encoder is trained, followed by an encoder-decoder that learns latent states using reconstruction loss while aligning with contrastive embeddings to enhance the embedding information."}, {"title": "Datasets Description", "content": "PhysioNet2012 dataset\u00b2 was first intruduced in [18]. It includes multivariate time series data with 37 variables gathered from intensive-care unit (ICU) records. Each record contains measurements taken at irregular intervals during the first 48 hours of ICU admission. We used set-a as a train set and set-b as a test set. Both sets contain 4000 labeled sequences.\nMIMIC-III dataset\u00b3 [25] consists of multivariate time series data featuring sparse and irregularly sampled physio- logical signals, collected at Beth Israel Deaconess Medical Center from 2001 to 2012. While we aimed to follow the general pipeline outlined in [40], we made several modifications to enhance the accuracy and reproducibility of our approach. Importantly, we did not alter the original problem statement: we excluded series that last less than 48 hours and used the first 48 hours of observations from the remaining series to predict in-hospital mortality. These adjustments were necessary to address certain issues and improve the overall robustness of our analysis.\nAge dataset consists of 44M anonymized credit card transactions representing 50K individuals. The target is to predict the age group of a cardholder that made the transactions. The multiclass target label is known only for 30K records, and within this subset the labels are balanced. Each transaction includes the date, type, and amount being charged. The dataset was first introduced in scientific literature in work [4].\nRetail dataset comprises 45.8M retail purchases from 400K clients, with the aim of predicting a client's age group based on their purchase history. Each purchase record includes details such as time, item category, the cose, and loyalty program points received. The age group information is available for all clients, and the distribution of these groups is balanced across the dataset. The dataset was first introduced in scientific literature in work [4].\nMBD is a multimodal banking dataset introduced in [15]. The dataset contains an industrial-scale number of sequences, with data from more than 1.5 million clients. Each client corresponds to a sequence of events. This multi-modal dataset includes card transactions, geo-position events, and embeddings of dialogs with technical support. The goal is to predict the purchases of four banking products in each month, given the historical data from the previous month. For our analysis, we use only card transactions.\nSince we focused on the event sequence assessment task, we restricted our setup as follows. To predict the purchases, we use transactions from the preceding month. For example, we use a sequence from June to predict a label by the last day of July. We did not use out-of-time validation, as the labeled time span of the data is less than a year. The authors of the dataset split the data into 5 folds (0-4), we use fold 4 as the test fold.\nPendulum Inspired by [32] we created a pendulum dataset to evaluate time-dependent models. The dataset simulates damped pendulum motion with varying lengths. Observation times are sampled irregularly using a Hawkes process, emphasizing the importance of accurate event timing for real-world applications. Each sequence in the dataset consists of events represented by time and two normalized coordinates (x, y), with some values randomly dropped. The goal is to predict the damping factor. We publish the reproducible code to generate the dataset.\nTo model the Hawkes process, we consider the following intensity function \u5165(t) that is given by (1).\n$\\x(t) = \\mu + \\sum_{t_i<t} ae^{-\\beta(t-t_i)}$ (1)\nWe used following parameters for the Hawkes process:\n\u2022 $\\mu$ is the base intensity;\n\u2022 a is the excitation factor, was chosen to be 0.5;\n\u2022 \u1e9e is the decay factor, was set to 1.\n\u2022 $t_i$ are the times of previous events before time t.\nThe time points are sampled within the interval [0, end time], where the end time is sampled from a uniform distribution U(3, 5). To maintain an approximately constant number of points (30) per sequence, we adjust the base intensity \u00b5 as follows:\n$\\mu = 30 \\times \\frac{1- \\alpha}{end\\ time -1}$\nThis ensures each sequence has a dynamic time interval but approximately the same number of points, preventing the model from learning the timestamp distribution without using timestamp data.\nTo model the pendulum we consider the second-order differential equation:\n$\\theta'' + (\\frac{b}{m})\\theta' + (\\frac{g}{L})sin(\\theta) = 0$ (2)\nwhere,\n\u2022 0\" is the Angular Acceleration,\n\u2022 \u03b8' is the Angular Velocity,\n\u2022 0 is the Angular Displacement,\n\u2022 b is the Damping Factor,\n\u2022 g = 9.81 m/s\u00b2 is the acceleration due to gravity,\n\u2022 L is the Length of pendulum,\n\u2022 m is the Mass of bob in kg.\nTo convert this second-order differential equation into two first-order differential equations, we let $\\theta_1 = \\theta$ and $\\theta_2 = \\theta'$, which gives us:\n$\\dot{\\theta_2} = \\theta'' = -(\\frac{b}{m})\\theta_2 -(\\frac{g}{L})sin(\\theta_1)$ (3)\n$\\dot{\\theta_1} = \\theta_2$ (4)\nThus, the first-order differential equations for the pendulum simulation are:\n$\\dot{\\theta_2} = -(\\frac{b}{m})\\theta_2 -(\\frac{g}{L})sin(\\theta_1)$ (5)\n$\\dot{\\theta_1} = \\theta_2$ (6)\nIn our simulations, the damping factor b is sampled from a uniform distribution U(1, 3), and the mass of the bob m = 1. The length L of the pendulum is taken from a uniform distribution U (0.5, 10), representing a range of possible lengths from 0.5 to 10 meters. The initial angular displacement @ is sampled from a uniform distribution U(0, 2\u03c0), and the initial angular velocity \u03b8' is sampled from a uniform distribution U(\u2212\u03c0, \u03c0), providing a range of initial conditions in radians and radians per second, respectively.\nOur primary objective is to predict the damping factor b, using the normalized coordinates x and y on the plane. These coordinates are scaled with respect to the pendulum's length, such that the trajectory of the pendulum is represented in a unitless fashion. This normalization allows us to abstract the pendulum's motion from its actual physical dimensions and instead focus on the pattern of movement. Additionally, we randomly drop 10% of values for both coordinates."}, {"title": "HPO details", "content": "Hyperparameter Optimization (HPO) is a critical step in the development and evaluation of machine learning models. It involves systematically searching for the optimal set of hyperparameters that maximize model performance. In this section, we outline our main evaluation methodology and HPO process, which is detailed in Algorithm 1.\nOur approach includes two main steps: the HPO step and the final evaluation step. In the HPO step, we use the Tree-structured Parzen Estimator (TPE) to efficiently search the hyperparameter space. We split the training dataset into three subsets: train (70%), train-val (15%), and hpo-val (15%). The model is trained on the train set, and its performance is evaluated on the train-val set to determine when to stop training. The hpo-val set is used to update the TPE sampler and guide the selection of hyperparameters.\nAfter the HPO step, we proceed to the final evaluation step. Here, we use the best hyperparameters (BHP) identified in the HPO step to train and evaluate the model multiple times with different random seeds. This ensures that our results are robust and not dependent on a particular random initialization. The training dataset is split into train (85%) and train-val (15%) sets, and the model is trained until performance on the train-val set stops improving or until the training budget is exhausted. Finally, we evaluate the model on the test set and report the mean and standard deviation of the test metrics."}, {"title": "Subsets metric relationships", "content": ""}, {"title": "HPO analisys", "content": "This section presents a comprehensive evaluation of different aggregation and normalization approaches, as well as the importance of learning rates, for various models across multiple datasets.\nTable 7 compares two aggregation methods: using the last hidden state and the mean of all hidden states. The results indicate that the choice of aggregation method can significantly impact model performance. For instance, in the Age dataset, the mean hidden state approach improves performance for models like GRU and Mamba, while the last hidden state approach is more effective for mTAND. Similarly, Table 8 evaluates the impact of batch normalization on input features. The results show that batch normalization can enhance model performance in many cases.\nAdditionally, Table 9 ranks the importance of learning rate hyperparameter for different models and datasets using Optuna. The rankings highlight that the learning rate is a critical hyperparameter, with its importance varying across different dataset and model combinations. For example, the learning rate is ranked highest for Mamba across all datasets, indicating its significant impact on model performance. These findings provide valuable insights into the optimal configuration of models for different datasets and can guide future research in hyperparameter optimization."}]}