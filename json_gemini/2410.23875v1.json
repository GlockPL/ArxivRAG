{"title": "Plan-on-Graph: Self-Correcting Adaptive Planning of\nLarge Language Model on Knowledge Graphs", "authors": ["Liyi Chen", "Panrong Tong", "Zhongming Jin", "Ying Sun", "Jieping Ye", "Hui Xiong"], "abstract": "Large Language Models (LLMs) have shown remarkable reasoning capabilities\non complex tasks, but they still suffer from out-of-date knowledge, hallucinations,\nand opaque decision-making. In contrast, Knowledge Graphs (KGs) can provide\nexplicit and editable knowledge for LLMs to alleviate these issues. Existing\nparadigm of KG-augmented LLM manually predefines the breadth of exploration\nspace and requires flawless navigation in KGs. However, this paradigm cannot\nadaptively explore reasoning paths in KGs based on the question semantics and\nself-correct erroneous reasoning paths, resulting in a bottleneck in efficiency and\neffect. To address these limitations, we propose a novel self-correcting adaptive\nplanning paradigm for KG-augmented LLM named Plan-on-Graph (PoG), which\nfirst decomposes the question into several sub-objectives and then repeats the\nprocess of adaptively exploring reasoning paths, updating memory, and reflecting\non the need to self-correct erroneous reasoning paths until arriving at the answer.\nSpecifically, three important mechanisms of Guidance, Memory, and Reflection\nare designed to work together, to guarantee the adaptive breadth of self-correcting\nplanning for graph reasoning. Finally, extensive experiments on three real-world\ndatasets demonstrate the effectiveness and efficiency of PoG.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have manifested outstanding performance in various natural\nlanguage processing and data science tasks, such as question answering [42, 25, 62], text gen-\neration [18, 13, 8, 15], recommender systems [60, 59, 51, 44], and domain-specific applica-\ntions [46, 45, 14, 50, 36]. They leverage advanced deep learning techniques and immense amounts of\npre-existing text data to understand and generate human language with impressive fluency and coher-\nence. Despite their success in numerous applications, LLMs still suffer from out-of-date knowledge,\nhallucinations, and opaque decision-making, highlighting the ongoing need for further investigation\nin this rapidly evolving field.\nIntuitively, as large-scale structural knowledge bases, Knowledge Graphs (KGs) [5, 1, 12, 43] provide\nexplicit and editable depictions of massive real-world knowledge, which have the potential to be a\npromising complement to the drawbacks of LLMs. Previous studies manage to integrate KGs into\nLLM pre-training [61, 47] or fine-tuning [53, 27] stage. However, these methods mainly compress\nstructured knowledge in KGs into LLMs' parameters in a black-box fashion and still cannot fully"}, {"title": "2 Preliminary", "content": "Knowledge Graph (KG) stores massive factual knowledge in the form of a set of triplets: G =\n{(e, r, e') | e, e' \u2208 E,r \u2208 R}, where E and R denote the set of entities and relations, respectively.\nRelation Paths are a sequence of relations: $z {1, 2, ..., r_i}$, where $r_i \u2208 R$ denotes the i-th\nrelation in the path and I denotes the length of the path.\nReasoning Paths are the instances of a relation path z in the KG: $p_z = e_o \u2192 r_1e_1 \u2192 r_2e_2 \u2192\n... \u2192 r_ie_i$, where $e_i \u2208 E$ denotes the i-th entity and $r_i$ denotes the i-th relation in the relation path z.\nKnowledge Graph Question Answering (KGQA) is the task of answering natural language ques-\ntions based on a set of facts over the KG. Given a question q, a knowledge graph G, and topic entities\n$T_q$ mentioned in q, the target of KGQA is to generate answers $A_q$ to the question q. Following\nprevious studies [35], we assume any entity $e_q \u2208 T_q$ mentioned in q and answers $a_q \u2208 A_q$ are labeled\nand linked to the corresponding entities in G, i.e., $T_q, A_q \u2286 E$."}, {"title": "3 Methodology", "content": "In this section, we introduce the technical details of the novel self-correcting adaptive planning\nparadigm for KG-augmented LLM named Plan-on-Graph (PoG). As illustrated in Figure 2, PoG\nconsists of four key components: Task Decomposition, Path Exploration, Memory Updating, and\nEvaluation. PoG first decomposes the question into several sub-objectives as guidance of planning\nexploration and then repeats the process of adaptively exploring reasoning paths to access relevant\nKG data, updating memory to provide historical retrieval and reasoning information for reflection,\nand reflecting on the need to self-correct reasoning paths until arriving at the answer."}, {"title": "3.1 Task Decomposition", "content": "To harness conditions in the question to better guide the adaptive exploration process, PoG decom-\nposes the task of answering the question into multiple sub-objectives containing conditions through\nsemantic analysis of the LLM. Sub-objectives serve as guidance for path exploration, benefiting\nthe identification of relevant paths to each condition outlined in the question with flexible explo-\nration breadth. Specifically, we prompt the LLM to decompose the original question q into a list of\nsub-objectives for KG retrieval and reasoning. The prompt is shown in Appendix A.1. The list of\nsub-objectives can be denoted as O = {01, 02, 03, ...}. It is important to note that sub-objectives in O\nmay refer to the results obtained from other sub-objectives in O, allowing for interdependencies in\nthe reasoning process."}, {"title": "3.2 Path Exploration", "content": "We access relevant information from the KG by exploring reasoning paths in the KG. On the initiation\nof path exploration, we localize the initial entities of reasoning paths, which correspond to the topic\nentities mentioned in the given question. Similar to prior research [19, 35], topic entities have been\npre-identified and are part of the annotated datasets. Specifically, when presented with a question q, we\nuse topic entities to serve as the initial elements of the reasoning paths, $E^0 = T_q = {e_q^1, e_q^2, ..., e_q^{N_0}}$,\nwhere $N_0$ is the number of topic entities.\nIn the subsequent iterations, we continue exploring reasoning paths most relevant to the question and\nsuspend other reasoning paths. Taking the D-th iteration as an example, before the iteration starts, each\nreasoning path $p_n \u2208 P$ consists of $D_{p_n}$ ($D_{p_n} \u2264 D \u2013 1$) triplets, i.e., $P_n = {(e_n^{d,n},r_{a_n}^n,e_{a_n}^{d,n})}_{d=1}^{D_{p_n}}$,\nwhere $e_n^{d,n}$ and $e_{a_n}^{d,n}$ denote subject and object entities, $r_{a_n}^{d,n}$ is a specific relation between them,\n$(e^{d,n},r^{d,n}, e^{d,n})$ and $(e^{d+1}, r^{d+1}, e^{d+1})$ are linked to each other. It is noted that the length of each\nreasoning path may vary, because in the D-th iteration, we only continue exploring the reasoning\npaths most semantically relevant to the question, which are identified in the D \u2013 1-th iteration. The\nsets of tail entities and relations to be explored are denoted as $E^{D-1} = {e_1^{D-1}, e_2^{D-1},...,e_{N_{D-1}}^{D-1}}$\nand $R^{D-1} = {r_1^{D-1}, r_2^{D-1},...,r_{N_{D-1}}^{D-1}}$, respectively, where $N_{D-1}$ is the length of $E^{D-1}$ and"}, {"title": "3.3 Memory Updating", "content": "The information stored in memory provides historical retrieval and reasoning information for reflec-\ntion. After a two-step exploration, we dynamically update the searched subgraph $G_{sub}$, reasoning\npaths P, and sub-objective status S in memory based on the ongoing reasoning process.\nSubgraph. The subgraph includes all retrieved relations and entities from the KG. We update the\nsubgraph in memory, which can be utilized during later reflection to determine which entity to\nbacktrack to for self-correction. In the D-th iteration, the searched subgraph $G_{sub}$ is updated by\nadding the retrieved candidate relation set $R_{cand}^D$ and candidate entity set $E_{cand}^D$.\nReasoning Paths. In order to ensure that the LLM can understand relationships between entities for\nbetter reasoning and allow for path correction in reflection stage, we update reasoning paths P to\npreserve the semantic structure within the KG.\nSub-Objective Status. The LLM may forget partial conditions in the reasoning process. Sub-\nobjectives obtained by decomposing the question can help the LLM remember multiple conditions\nin the question. The status of sub-objectives contains the current known information related to\nthe sub-objectives, which can aid the LLM in remembering the known information of each condition and\ndetermining whether to correct the exploration direction in reflection stage. Hence, we leverage the\nLLM to update the currently known information relevant to sub-objectives into sub-objective status\n$S = {s_1, s_2, s_3, ...}$, $|S| = |O|$, based on the semantic information of the question q, sub-objectives\nO, historical sub-objective status, and reasoning paths P, along with the LLM's own knowledge. The\nprompt is shown in Appendix A.3."}, {"title": "3.4 Evaluation", "content": "After the path exploration and memory updating, PoG prompts the LLM to reason whether the current\nacquired information, including sub-objective states and reasoning paths recorded in memory, is\nsufficient to infer an answer. The prompt is shown in Appendix A.4.1. If the LLM determines that the"}, {"title": "4 Experiments", "content": "4.1 Experimental Setups\n4.1.1 Datasets & Evaluation Metrics\nTo demonstrate the effectiveness of PoG on complex reasoning over knowledge graphs, we adopt three\nrepresentative multi-hop KGQA datasets: CWQ [37], WebQSP [56], and GrailQA [17]. All three\ndatasets rely on the external knowledge graph from Freebase [5]. For the large dataset GrailQA, we\nutilize the same testing samples as those in ToG [35] to improve computational efficiency. Following\nprior research [23, 19, 35], we use exact match accuracy (Hits@1) as the evaluation metric.\n4.1.2 Comparison Methods\nDue to variations in the performance of the method across different datasets, we select prior state-of-\nthe-art (SOTA) approaches as baselines for each dataset. They can be categorized into two groups:\n(1) LLM-only methods, including standard prompting (IO prompt) [6], Chain-of-Thought prompting\n(CoT) [49], and Self-Consistency (SC) [48].\n(2) KG-augmented LLM methods, including\nfine-tuned and prompting methods. For CWQ\nand WebQSP, we utilize UniKGQA [20],\nTIARA [34], RE-KBQA [7], DeCAF [57],\nand RoG [27] as fine-tuned baselines and KD-\nCoT [41], KB-BINDER [23], StructGPT [19],\nInteractive KBQA [52], and ToG [35] as prompt-\ning baselines. For GrailQA, we utilize RnG-\nKBQA [55], TIARA [34], FC-KBQA [58],\nPangu [16], FlexKBQA [26], and GAIN [33] as\nfined baselines and KB-BINDER [23] and\nToG [35] as prompting baselines. The descrip-\ntions of baselines are presented in Appendix D."}, {"title": "4.2 Performance Comparison", "content": "We compare PoG with the SOTA baselines to\ndemonstrate its effectiveness for KG-augmented\nLLM. Table 1 and Table 2 present the experi-\nmental results on CWQ, WebQSP, and GrailQA\ndatasets. Overall, PoG achieves the best per-\nformance across all three datasets. Specifically,\nwe can make the following observations. First,\ncompared to all prompting KG-augmented LLM\nbaselines, PoG shows superior performance ad-\nvantages. Regardless of whether GPT-3.5 or"}, {"title": "4.3 Ablation Study", "content": "In order to assess the effectiveness of each mech-\nanism and adaptive exploration in PoG, we con-\nduct the ablation study to remove them on three\ndatasets, respectively. Specifically, w/o Guid-\nance refers to the variant where entire task de-\ncomposition as guidance is removed. w/o Mem-\nory indicates the variant without the memory\nmechanism. w/o Reflection refers to the variant"}, {"title": "4.4 Efficiency Study", "content": "We study the efficiency of PoG and the SOTA prompting KG-augmented LLM baseline, ToG. Table 4\npresents the average LLM call, token consumption, and time required by both methods to answer\na question across three datasets. In all datasets, PoG demonstrates clear advantages over ToG in\nterms of all metrics. For average number of LLM calls, PoG consistently requires fewer calls to\nthe LLM, and reduces it by at least 40.8%. This highlights PoG's ability to reason more efficiently\nwith fewer LLM interactions. Regarding token consumption, PoG exhibits a notable advantage\nin both input and output token usage. On CWQ, compared to ToG's input tokens, PoG shows a\nreduction of approximately 4.6% in input token consumption. As for output tokens, PoG produces\njust 353.159 output tokens, representing a substantial decrease of roughly 76.2%. This indicates\nthe effectiveness of PoG in reducing the overall token consumption during the reasoning process.\nMost importantly, PoG achieves superior time efficiency compared to ToG. On CWQ and GrailQA,\nPoG presents a speedup of over 4 times. ToG predefines the breadth of exploration, leading to the\nexploration of many irrelevant paths. Additionally, ToG lacks a self-correction mechanism, and when\nthere is insufficient information to answer a question, it can only extend the current reasoning paths,\nsacrificing a lot of efficiency on irrelevant explorations. By contrast, the efficiency advantages of\nPoG can be attributed to its adaptive exploration and self-correction of reasoning paths based on\nthe semantics of the question. The adaptive breadth reduces unnecessary exploration, and effective\ncorrection avoids the extending of wrong current paths."}, {"title": "4.5 Case Study", "content": "Figure 3 shows a typical case from the testing results on CWQ dataset. We compare the results of PoG,\nToG, and CoT in answering the question \"Who is in control of the place where the movie 'The Naked\nand the Dead' takes place?\". The underlying LLMs they used are all based on GPT-3.5. PoG initially\nidentifies a flexible number of relations related to the topic entities. Specifically, for \u201cThe Naked and\nthe Dead\", PoG successfully discovers that the movie takes place in Panama, while for \u201cPresident of\nPanama\", the LLM thinks that only the relation \u201cgovernment.government_office_or_title.jurisdiction\u201d\nis relevant. Upon retrieval, no information is found regarding the person in control of Panama.\nThis triggers reflection as PoG realizes that it lacks sufficient information. With the memory,\nPoG refers to the sub-objective status and recognizes that it already knows the movie location\n(Panama) for sub-objective #1 but is unaware of the person in control of Panama for sub-objective\n#2. Based on the current reasoning paths, PoG makes a decision to execute self-correction and\nreturns to exploring the relation not previously explored for \"President of Panama\". Due to the task\ndecomposition, during the self-correction process, it becomes easier to identify the correct relation\n\u201cgovernment.government_office_or_title.office_holders\u201d according to the sub-objectives. Through\nthe guidance, memory, and reflection mechanisms, PoG successfully finds the correct answer, \u201cJuan\nCarlos Varela\". In contrast, ToG fails to identify the most relevant relation concerning \u201cPresident of\nPanama\" and continues exploring incorrect paths. This consumes a significant amount of time and\nultimately leads to an erroneous answer due to the hallucination. CoT refuses to answer directly since\nthe LLM realizes its lack of knowledge regarding the answer and requires additional information to\nbe provided. From this analysis, it is evident that PoG outperforms ToG and CoT. PoG successfully"}, {"title": "5 Related Work", "content": "LLM Reasoning. To encourage LLMs to engage in reasoning rather than simply providing answers\ndirectly, many researchers instruct LLMs to generate the process of thinking in their outputs [49,\n22, 63]. In the early stages, Chain of Thought (CoT) [49] was designed to provide a few examples\nof intermediate natural language reasoning steps as the prompt. After that, several variants of\nCoT reasoning with different forms like Tree-of-Thought [54], Graph-of-Thought [4], Memory of\nThought [24], and Skeleton-of-Thought [30] were proposed to enhance the thinking process. However,\nLLMs may make mistakes during the reasoning process. Hence, many works [32, 21, 28, 29]\ndesigned self-correction mechanisms based on feedback to rectify flawed reasoning and ensure\naccuracy. Additionally, large efforts were dedicated to guiding LLMs in understanding complex\ngraph structures [39, 38] and improving their graph reasoning across different graph tasks [11, 10, 9].\nHowever, it is still an open issue to address the outdated knowledge, hallucinations, and opaque\ndecision-making for LLM reasoning.\nKG-Augmented LLM. Despite the pre-training of LLMs on massive corpora, they still suffer from\nlimitations such as outdated knowledge, hallucinations, and opaque decision-making. An effective\napproach to address these limitations is to leverage KGs for explicit and editable knowledge provision\nto LLMs. Previous studies integrated KGs into LLM pre-training [61, 47] or fine-tuning [53, 27]\nstage, but they merely inject structured knowledge into LLMs' parameters and still leave these\nlimitations unexplored. Therefore, several works [3, 2, 40] first retrieved information from KGs and\nthen directly fed explicit knowledge into LLMs. In this way, LLMs do not involve the graph reasoning\nprocess and cannot provide potential insights. Then, a novel KG-augmented LLM paradigm [19, 35]"}, {"title": "6 Conclusion", "content": "In this paper, we proposed a novel self-correcting adaptive planning paradigm for KG-augmented\nLLM named Plan-on-Graph (PoG). To the best of our knowledge, we were the first to incorporate a\nreflection mechanism for self-correction and adaptive KG exploration into KG-augmented LLMs,\neffectively augmenting LLM's reasoning ability and efficiency. PoG first decomposed the question\ninto several sub-objectives, and then repeated the process of exploring reasoning paths, updating\nmemory, and reflecting on the need to self-correct reasoning paths until arriving at the answer. To\nbe specific, three important mechanisms were designed to work together to guarantee the adaptive\nbreadth of self-correcting planning for graph reasoning, i.e., Guidance, Memory, and Reflection.\nFinally, extensive experiments on three real-world KGQA datasets validated not only the effectiveness\nbut also the efficiency of the proposed PoG."}, {"title": "H Broader Impact & Limitation", "content": "In the current research landscape, PoG carries a significant broader impact, primarily reflected in its\nenhancement of complex reasoning capabilities for KG-augmented LLM. By innovatively integrating\nguidance, memory, and reflection mechanisms, PoG not only strengthens the model's flexibility\nand accuracy when facing complex queries but also enhances its ability to self-correct erroneous\nreasoning paths. This self-correcting adaptive planning paradigm enables the model to backtrack\nand adjust reasoning directions when faced with invalid initial assumptions or impasses, resulting\nin an optimal solution search. Additionally, the broader impact of PoG is manifested in several\nother aspects: (1) Improving Efficiency and Effectiveness in Problem-Solving: By dynamically\nadjusting exploration breadth and employing self-correction mechanisms, PoG can more efficiently\nhandle complex questions and provide more accurate answers, significantly enhancing the overall\nperformance of KGQA systems. (2) Enhancing the Robustness and Adaptability of LLMs: Through\nits memory mechanism, which records and tracks the completion status and reasoning paths of each\nsub-objective, PoG enables the LLM to more robustly deal with the uncertainty and complexity of\nquestions, making it more precise and reliable across a wide range of applications. (3) Fostering\nInnovation in the Field of Artificial Intelligence: PoG's integration of meta-cognitive capabilities into"}]}