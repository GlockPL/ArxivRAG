{"title": "Roadmap towards Superhuman Speech Understanding using Large Language Models", "authors": ["Fan Bu", "Yuhao Zhang", "Xidong Wang", "Benyou Wang", "Qun Liu", "Haizhou Li"], "abstract": "The success of large language models (LLMs) has prompted efforts to integrate speech and audio data, aiming to create general foundation models capable of processing both textual and non-textual inputs. Recent advances, such as GPT-40, highlight the potential for end-to-end speech LLMs, which preserves non-semantic information and world knowledge for deeper speech understanding. To guide the development of speech LLMs, we propose a five-level roadmap, ranging from basic automatic speech recognition (ASR) to advanced superhuman models capable of integrating non-semantic information with abstract acoustic knowledge for complex tasks. Moreover, we design a benchmark, SAGI Bechmark, that standardizes critical aspects across various tasks in these five levels, uncovering challenges in using abstract acoustic knowledge and completeness of capability. Our findings reveal gaps in handling paralinguistic cues and abstract acoustic knowledge, and we offer future directions. This paper outlines a roadmap for advancing speech LLMs, introduces a benchmark for evaluation, and provides key insights into their current limitations and potential.", "sections": [{"title": "1 Introduction", "content": "Paradigms to process language have been reshaped thanks to LLMs and its scaling law. Given the success of LLMs, one may expect to integrate extensive data in speech and audio modality into LLMs (similar to visual language models Liu et al. (2023); Li et al. (2023) 2), resulting in a more general foundation model.\nTowards this path, the exploration on speech foundation models recently brings new research insights from the perspectives of multi-task and multi-lingual processing (Radford et al., 2023; Bapna et al., 2021; Zhang et al., 2023c; Seamless Communication et al., 2023; Pratap et al., 2024). A remarkable event is the release of GPT-40, which is notable for its ability in open-ended speech-to-speech dialogue. Its performance in speech understanding, speech synthesis, and system latency has reached new levels, leading to a wave of studies on speech LLMs. The next question is, where are we now and where should we go? To answer this, we begin by introducing the potential of using LLMs to understand speech.\nProcessing Speech using LLMs Compared to the traditional approach of feeding ASR-transcribed text into text-only language models, unified speech-language models process raw audio or speech directly in an end-to-end fashion. The benefits for using LLMs to process speech are mainly two-fold."}, {"title": "Five-level Speech Understanding", "content": "The two benefits highlight the potential of speech LLMs, achieving of which requires the models to perceive complete speech information and achieve abstraction of expert speech/acoustic knowledge (e.g., inferring from cough and melody in some applications). To this regards, we define five levels (see Fig. 1.) as below:\n\u2022 Basic Level At the most basic level (Level 1), speech language models should be able to recognize speech as text. The rationale for defining automatic speech recognition as the foundational level is that it serves as the basis for directly interacting with LLMs through speech. However, these capabilities at the basic level (e.g., speech recognition) offer limited additional benefits for ASR-equipped cascade model to understand human speech as it is somehow equivalent to a combination with a ASR model and a text-only LLM.\n\u2022 Acoustic Information Perception Levels More advanced models (at Level 2 and Level 3) are expected to directly perceive basic paralinguistic information such as tone, pitch, and loudness, and further enable them to comprehend non-semantic cues like emotions and the surrounding environment (e.g., sarcasm).\n\u2022 Abstract Acoustic Knowledge Levels At a higher level (at Level 4), models can integrate speech with expert speech/audio knowledge to perform specialized tasks, such as medical assessments. At the final lavel (Level 5), the ultimate goal is to develop the Speech Artificial General Intelligence (SAGI) capable of combining non-semantic information with speech/audio knowledge to perform all speech understanding tasks, even achieving superhuman speech understanding.\nThe Benchmark However, these levels remain insufficiently intuitive. Therefore, we have preliminarily developed a benchmark to concretize and exemplify these capability levels. We designed the SAGI Benchmark to evaluate speech LLMs across various tasks that typically represent the characteristics of each level.\nThe benchmark covers a wide range of tasks, including speech recognition, language distinction, volume perception, emotion recognition, and more, with each task corresponds to a specific level of capability within speech LLMs. The reliability of these evaluation sets was verified using human"}, {"title": "2 Roadmap towards Understanding Speech", "content": "To design a roadmap for future speech LLMs, we first analyzed the development process of speech LLMs in the past (in Sec. 2.1). Following that, we present our philosophy of the roadmap in Sec. 2.2."}, {"title": "2.1 The Background", "content": "Current speech LLMs are mainly divided into two types: the Cascade Paradigm and the End-to-End Paradigm. Below, we will focus on analyzing these two approaches.\nCascade Paradigm A straightforward approach to understanding speech using LLMs is to feed speech transcriptions (in text format) into LLMs. This is known as the cascade paradigm (see the left in Fig. 2). While this method allows for basic speech understanding, it lacks the ability to perceive non-semantic information (e.g., emotion, stress) within LLMs. This hinders a deeper understanding of the spoken content as its non-semantic information is often crucial for fully grasping the intent or nuances in speech.\nEnd-to-end Paradigm In contrast, an end-to-end speech LLM can process both semantic and non-semantic information simultaneously within a single model. This approach not only retains more detailed information within the LLM but also allows the world knowledge embedded in the LLM to interact directly with speech data. Note that this end-to-end speech paradigm introduces additional complexity, as it requires LLMs to handle raw speech data, which operates at a lower level compared to textual inputs.\nIn summary, the end-to-end solution enables LLMs to directly handle non-semantic information, such as emotions. Additionally, due to its stronger perceptual capabilities, it holds greater potential for understanding and applying abstract acoustic knowledge. As a result, end-to-end solution can be considered the future direction for the development of speech LLMs."}, {"title": "2.2 The Philosophy of the Roadmap", "content": "With the rise of large language models (LLMs), there is an increasing demand to understand information beyond text, particularly speech. The core idea is that speech conveys richer information than text alone, positioning ASR (Automatic Speech Recognition) as a foundational level. End-to-end speech LLMs can begin with ASR capabilities to directly leverage the capabilities of text LLMs. And then, it progressively incorporate more advanced comprehension of non-semantic features. Finally it contains the ability to retain and apply abstract acoustic knowledge. This progress can be described as evolving through the following five levels:\nLevel 1. Speech Recognition Level At the most basic level, a speech language model should be capable of recognizing text.\nThese tasks form the most fundamental requirements for interacting with large models using speech. However, even at Level 1, the model offers limited advantages over a traditional cascade approach (e.g., feeding ASR-transcribed text into LLMs). The real benefits of speech LLMs begin to emerge at the next level, with the ability to capture non-semantic features such as paralinguistic information.\nLevel 2. Basic Paralinguistic Perception Level At this level, Speech LLMs gain the ability to perceive basic paralinguistic features in speech, such as tone, pitch, volume, rhythm, and speech rate.\nThese elements are essential to speech comprehension and provide distinct advantages over pure text-based models (or Speech LLMs at Level 1). While this lays the foundation for more advanced capabilities, the insights derived at this level are still relatively shallow. For deeper understanding, we must move to Level 3, where a model comprehends a broader range of non-semantic information.\nLevel 3. Non-semantic Comprehension Level At this stage, the Speech LLM extends beyond basic paralinguistic features and is capable of comprehending and interpreting more complex non-semantic information, such as emotions, sarcasm, and heightened states like pride.\nFor example, emotions are higher-level human experiences that involve cognitive functions, distinguishing them from basic paralinguistic information. Interestingly, even some higher animals, like pet dogs, can perceive these types of non-semantic information. To fundamentally distinguish humans from animals, we designed Level 4 by leveraging the human strengths in higher-level cognitive capabilities.\nLevel 4. Speech Specialist Level At this advanced level, Speech LLMs integrate expert-level abstract acoustic knowledge to handle a few specific, complex tasks.\nThis requires integrating abstract acoustic knowledge which are advanced knowledge derived from acoustic information. This goes beyond mere recognition and comprehension at Level 1 and Level"}, {"title": "3 Benchmarking", "content": "To implement the roadmap (Sec.2), we aim to build a comprehensive benchmark to concretes these levels. Though previous benchmarks for speech LLMs have contributed significantly, they focus mainly on the first three levels, neglecting abstract acoustic knowledge and broader SAGI applications (App.A). Additionally, current benchmarks lack the depth needed for full speech LLM development, particularly in foundational tasks like pitch and volume perception. To address these gaps, we propose a new benchmark, detailed in the following section."}, {"title": "3.1 The New Benchmark: SAGI", "content": "Philosophy of Benchmark The SAGI Benchmark is structured to align with the five levels of speech understanding\u2074, and the overview of the benchmark is shown in Tab. 2. The tasks are organized into five levels: Level 1 focuses on testing the recognition capabilities of speech LLMs, including ASR, lyrics transcription, and term recognition tasks. Level 2 evaluates foundational perception abilities, such as pitch and volume perception for tasks like age, gender, and emotion recognition. Level 3 assesses non-semantic comprehension, incorporating tasks like emotion-integrated translation, environment perception, and emotional intensity recognition. Level 4 explores the application of abstract acoustic knowledge, specifically focusing on medical-related contexts. Finally, Level 5 envisions the capabilities of Speech AGI (SAGI), highlighting tasks that promote creativity and diverse thinking, such as appreciating artwork, with a strong foundation in earlier levels."}, {"title": "3.2 Benchmarked Objects", "content": "Humans To conduct an initial evaluation of human performance, we created evaluation subsets by randomly selecting 80 samples per label for the objective multiple-choice tasks, and 80 samples in total for the other tasks. Four students (two males and two females) with strong English proficiency completed the assessments. The results are recorded in Tab. 3. The participant information and consistency test is in App. C.1.\nSpeech LLMs There are four types of speech LLMs, see more details in Sec. 5. We selected an open-source model for each type, except for video LLMs, where the performance on audio-only tasks is not stable. For speech-related models, we chose Qwen2-Audio for its strong performance. We selected Mu-llama for the music model and GAMA for the audio model. Additionally, we tested SALMONN as a mixed audio and speech model. We further test GPT-4o advanced speech mode. Because only some models supports the speech instruction, we utilize the text instruction to ensure fair comparison.\nFor more details on model replication and evaluation settings, please refer to App. C.2."}, {"title": "3.3 Benchmarking Results", "content": "Humans As seen in Tab. 3, human performs generally well from Level 1 to 3. However, it becomes worse at higher levels due to a lack of acoustic knowledge. On the other side, speech understanding for humans are generally better than speech language models.\nTake-away 1. Human performance: Human generally performs well in speech understanding from Level 1 to 3, but fails to reach a high level due to a lack of abstract acoustic knowledge.\nSpeech LLMs As shown in Tab. 3, speech LLMs exhibit a significant weakness in Level 2 which consists of basic listening abilities of the human. These models are currently focused on directly addressing high-level tasks while neglecting basic paralinguistic information perception, thereby the model fails to shows generalization at higher level. Furthermore, most models do not fully satisfy the requirements at any given level, highlighting a lack of consideration for both task diversity and comprehensiveness. Notably, Qwen2-Audio has outperformed humans in tasks like emotion recognition. This suggests that speech LLMs have the potential to detect subtle changes in speech, even beyond human capabilities.\nTake-away 2. Speech LLMs: Speech LLMs still struggle with non-semantic perception and comprehension from Level 1 to Level 3, despite excelling in some tasks, limiting their performance on more complex tasks at higher levels.\nGPT-40 The results indicate that GPT-40 tends to reject audio-related tasks. Compared to other models, GPT-40 shows merit in emotion-related tasks but fails to demonstrate overwhelming advantages in understanding ability. We suppose its strength lies in its interaction capability. Therefore, we tested its ability to follow speech instructions, which directly evaluates its interaction skills. We also tested Qwen2-Audio, one of the few models that support speech instructions.\nThe performance is detailed in Tab. 4. Compared to the results with text instructions, GPT-40 performs better with speech instructions, while Qwen2-Audio loses most of its capabilities. However, there remains a significant gap compared to the best results achieved using text instructions."}, {"title": "4 More Analysis on Performance Deficiency", "content": "In this section, we discuss reasons for performance deficiency in SAGI benchmark. We first consider composition of training data (in Sec. 4.1). Then we analyse the model from three perspectives: 1) perception of acoustic information (in Sec. 4.2), 2) ability of instruction following (in Sec. 4.3), and 3) capacity of LLM backbone (in Sec. 4.4)."}, {"title": "4.1 Limited Types of Training Data", "content": "We observed in Tab. 3 that certain tasks, particularly those in Level 2, are easy for humans but challenging for speech LLMs. We first analyzed the composition of the training data for speech LLMs, as shown in Fig. 3. We found that most speech LLMs tend to disregard audio data except for GAMA, whereas GAMA focuses primarily on audio. This indicates distinct data biases among different speech LLMs, leading to variations in task preferences.\nTo further examine the influence of task preference, we compared the performance of various speech LLMs with Whisper V3 (trained with ~5,000k hours), as shown in Tab. 5. We found that Whisper still outperforms other models on the Lyrics Transcription task due to its the massive training data. On the other hand, with the help of the learned knowledge, speech LLMs perform significantly better at recognizing certain terms. This demonstrates that speech LLMs have great potential compared to traditional speech models. Notably, we also tested a Small model trained exclusively on an audio dataset. This Small model achieved 100% accuracy, while speech LLMs struggled with the task."}, {"title": "4.2 Inability to Comprehensively Perceive Acoustic Information", "content": "The current end-to-end paradigm universally adopts the stacking paradigm. However, the stacking paradigm may suffer from two types of information loss: 1) the latent representation produced by the acoustic encoder does not fully capture or convey the necessary information, and 2) the acoustic encoder fails to transfer all the information to the downstream LLMs.\nWe first investigate whether the loss of latent representation contributes to the limited performance. We compare the speech features generated from the same text content, which are spoken by different genders and with different emotions. The features are generated by Whisper, and cosine similarity is used to analyze the original and perturbed speech. The results, shown in Fig. 4, indicate that there is no significant difference between different speech samples. This suggests that emotion and gender information is lost during the acoustic encoder process. This could explain why some speech"}, {"title": "4.3 Inadequate Instruction Following", "content": "We observed that some models exhibit poor instruction following in Tab. 3. Two reasons can lead to these results: 1) the models do not understand the instructions, and 2) the instruction fails to help the models comprehend the speech.\nWe classify the cause by observing changes in performance after perturbing the prompt. If the model is insensitive to different perturbed prompts, it indicates that the model cannot understand the prompt. On the other hand, if the models show significantly better performance with a properly structured prompt, it suggests that the model could understand the task, while requires the specific instruction. We choose the two Level 3 tasks (Age prediction and Ambient Noise Detection) where the instruction following ability is crucial, and the results shown in Fig. 5.\nFor the result of Fig. 5, we can find the Mullama is not sensitive about the instruction. This prove the model can not figure out this task. Further, the performance of most speech LLMs highly related with the specific prompt, this shows models are sensitive with the format of instruction. Comparing with the text LLMs which are robust with diverse instruction, the speech LLMs need much effect to guarantee instruction following."}, {"title": "4.4 Weak LLM Backbones", "content": "Most current speech LLMs follow the paradigm of stacking the acoustic model and text LLMs. This paradigm requires the text LLMs to process audio-like tokens, raising an intuitive question: whether text LLMs have the potential to handle cross-modal tasks. We designed a direct task of converting a"}, {"title": "5 Related Work", "content": "Speech language models have seen a surge in development following the advent of LLMs. Currently, most work integrates pre-trained acoustic models with LLMs using an alignment module. There are two main strategies to bridge the gap between the two models: 1) adapters and 2) attention mechanisms.\nAdapter The former method adds modules (usually convolutional networks and MLPs) between the acoustic model and LLMs. Convolutional networks can compress sequence length (Wang et al., 2023a), while MLPs are used to align acoustic tokens with text embeddings (Su et al., 2023).\nAttention Mechanisms Regarding the attention method, Kong et al. (2024) implemented cross-attention to filter information from the output of the speech encoder. Li et al. (2023) proposed the Q-former as an intermediate extractor based on cross-attention. Similarly, Pan et al. (2023) applied the Q-former to extract useful acoustic information for LLMs. Some works directly treat the acoustic codec as tokens and do not rely on alignment strategies (Zhang et al., 2023a; Rubenstein et al., 2023).\nCategorization of speech LLMs We have introduced that acoustic models can generally be divided into four types. Some works aim to build universal multi-modal LLMS (Su et al., 2023; Zhan et al., 2024; Wu et al., 2023b; Lyu et al., 2023; Zhang et al., 2023b; Shukor et al., 2023). Several studies focus on enhancing music understanding, an important area that has not yet received enough attention (Deshmukh et al., 2023; Zhan et al., 2024; Liu et al., 2024a). Most speech LLMs"}, {"title": "6 Conclusion", "content": "In this paper, we explored the evolving landscape of large language models (LLMs) in the realm of speech processing. We introduced a five-level roadmap to guide the development of human-level speech understanding, from basic ASR capabilities to advanced generalist models that integrate non-semantic information with general abstract acoustic knowledge for complex tasks. To assess the current state of speech LLMs, we designed a comprehensive benchmark that standardizes critical aspects across various tasks, ensuring consistency and reliability in performance evaluation. Our research reveals the current stage and deficiencies in understanding speech by both humans and speech LLMs. We evaluate the advanced speech mode of GPT-40 and find that following speech instructions is very challenging. Further analysis has uncovered structural flaws in existing speech LLMs. Reveals that current speech LLMs face issues in both Acoustic Information Transfer and Foundation LLMs' Potentiality. The contributions of this paper provide a structured approach to advancing speech LLMs, offering valuable insights for future innovations in this field."}, {"title": "Limitation", "content": "Artificial intelligence should not be confined to overly narrow domains, as such a focus can lead to frequent model switching when handling diverse tasks.This requires SAGI, a speech AGI, to be a powerful assistant capable of completing all kinds of tasks. However, during our primary testing, most speech LLMs remain at levels 1 and 2, indicating there is still a long way to go in terms of understanding speech.\nTo advance further, we conclude some important directions for improving speech LLMs toward higher level:\n\u2022 Requiring more diverse speech data to handle complex tasks.\n\u2022 Enhancing the ability of text LLMs to process speech-related tasks.\n\u2022 Ensuring that LLMs can receive complete acoustic information.\nWe advocate for the development of more powerful acoustic models, consideration of cross-domain compatibility when constructing datasets, and a deepening of expertise in specific research areas. This approach will enhance the generalization and adaptability of the models."}, {"title": "A Existing Benchmark", "content": "Tab. A summarizes the coverage of existing benchmarks across different levels of speech model tasks, highlighting gaps in current evaluation methods. L1 tasks such as Speech ASR, Intent Classification, and Language Identification are well supported by both Dynamic-SUPERB and AIR-Bench, though SD-Eval (Ao et al., 2024) lacks coverage. For Level 2 foundational perception tasks, like Music Pitch and Velocity, only AIR-Bench (Yang et al., 2024) provides support. Level 3 tasks related to non-semantic comprehension, such as Emotion, Environment, and Speaker Gender/Age, are covered to varying degrees across all benchmarks, with Dynamic-SUPERB (Huang et al., 2024) offering the most comprehensive support. However, more specialized tasks like Sarcasm, Stress, and Spoof Detection are only covered by Dynamic-SUPERB. Notably, Level 4 (Abstract Knowledge) and Level 5 (Speech AGI) remain entirely unsupported across all benchmarks. This underscores the urgent need to build a more comprehensive benchmark that addresses the gaps in Level 2, Level 4, and Level 5, ensuring more robust evaluation across all levels of speech model tasks."}, {"title": "B.1 General Principles of Data Construction", "content": "B.1.1 Question Construction\nFor objective multiple-choice questions, we guide large models by including multiple-choice options within the questions to facilitate the generation of final results. For subjective response questions, we specified the main aspects around which the questions revolve and set suggested answers, although these do not require the model to produce results that are exactly identical, illustrated in Fig. 6."}, {"title": "B.1.2 Uniform Sampling Rate", "content": "Considering the potential introduction of extraneous factors due to varying sampling rates of audio data, this paper standardizes all datasets to the one with the lowest sampling rate. Consequently, all test data is downsampled to 16,000 Hz."}, {"title": "B.1.3 Uniform number of audio channels", "content": "To standardize the format of the input audio, we converted all audio files for the tasks into mono channel, except for those in the Binaural Effect Perception sub-task."}, {"title": "B.1.4 Uniform Audio Duration", "content": "Most speech LLMs (Chu et al., 2023, 2024; Liu et al., 2024b; Tang et al., 2023) utilize the encoder from Radford et al. (2023), which limits their maximum audio processing duration to 30 seconds. To ensure fairness, we have restricted the lengths of the audio inputs to a maximum of 30 seconds."}, {"title": "B.1.5 Uniform Option Ratio", "content": "For the multiclass classification problem, we performed data balancing. Taking binary classification tasks as an example, due to some limitations in the current models, they might always choose one option in binary classification tasks. If the data were unbalanced, such as 40% for one option and 60% for the other, different models that always pick the same option could yield very different results, even though their capabilities are similar. This is not what we want, so we balanced the data for all multiclass classification tasks. Please refer to Tab. 10 for detailed information."}, {"title": "B.4.1 Language Identification", "content": "We used Europarl-ST (Iranzo-S\u00e1nchez et al., 2020) to construct our evaluation dataset. Europarl-ST is a multilingual speech translation corpus containing paired audio-text samples for speech trans-"}, {"title": "B.4.2 Automatic Speech Recognition", "content": "We constructed our evaluation dataset based on LibriSpeech (Panayotov et al., 2015). Inspired by Radford et al. (2023), we used the test-clean and test-other splits as our test sets, comprising a total of 2791 data entries. Since we addressed specific aspects within our metric C.3.1, we did not perform any additional processing when constructing the dataset. The task was set as: \u201cWhat does the person say? Please answer with 'The person says: xxxx'.\u201d"}, {"title": "B.4.3 ASR for Legal Terms", "content": "We selected 27 offenses defined under Chinese criminal law and combined them with four templates to generate 108 sentences, which were synthesized using cosy Voice (SpeechTeam, 2024). After manual screening (detailed in Sec. B.5.1), 102 utterances remained. The task was set as: \u201cWhat does the person say? Please answer with 'The person says: xxxx'.\" This approach is consistent with ASR, as we believe that this ability should be demonstrated automatically during the ASR process without the need for additional prompts.\""}, {"title": "B.4.4 ASR for Medical Terms", "content": "We selected 62 medical terms referring to specific locations and combined them with four templates to generate 248 sentences, which were synthesized using cosy Voice (SpeechTeam, 2024). After manual screening (detailed in Sec. B.5.1), 203 utterances remained. The task was set as: \"What does the person say? Please answer with 'The person says: xxxx'.\" This approach is consistent with ASR, as we believe that this ability should be demonstrated automatically during the ASR process without the need for additional prompts."}, {"title": "B.4.5 Automatic Lyrics Transcription", "content": "We utilized the JamendoLyrics MultiLang dataset (Durand et al., 2023) for our research. We acknowledge that a revised version of this dataset has been released as the Jam-Alt dataset (C\u00edfka et al., 2023). However, in accordance with the constraints outlined in Sec. B.1.4, we were required to resegment the audio files. Given that the Jam-Alt dataset, as described by its authors, exhibits certain deviations in its timestamps, we elected to employ the JamendoLyrics MultiLang dataset as our primary dataset for construction purposes. During the construction process, we manually selected the segmentation points and employed code to segment the audio files, thereby obtaining our final dataset. The task was set as: \u201cPlease transcribe the lyrics of this audio segment.Please answer with: 'The lyrics is: xxxx'.\""}, {"title": "B.4.6 Volume Perception", "content": "We constructed our evaluation dataset based on LJSpeech (Ito & Johnson, 2017). Following the data split of Chien et al. (2021), we used 512 test samples. We set up two scenarios: one where the volume gradually increases from 0 to its original level, and another where it decreases from the original level to 0. We tasked the model with determining whether the volume is increasing or decreasing. The task was set as: \"Is the volume of this audio segment gradually increasing or decreasing?\""}, {"title": "B.4.7 Pitch Perception", "content": "We used the SpeechAccentArchive (Weinberger, 2013) dataset to construct our test set. During this process, we first identified the frequency ranges with the highest proportion of fundamental frequency (F0). Ultimately, we selected the ranges (80, 150) Hz and (180, 250) Hz for our experiments. We framed the problem as follows: \u201cIn the following audio segment, into which range does more than 70% of the fundamental frequency content fall? Please choose from the following two ranges: (80, 150) Hz and (180, 250) Hz.\" We calculated the proportion of FO content falling within these two\""}, {"title": "B.4.8 Binaural Effect Perception", "content": "We generated random sounds using four methods: sine wave, square wave, triangle wave, and noise. These sounds are heard only in the left ear or the right ear. For more details, please refer to our public code. The model is used to determine which ear hears these sounds. The task was set as: \u201cIn this audio segment, does the sound appear in the left ear or the right ear? Please answer with 'left' or 'right'."}, {"title": "B.4.9 Ambient Noise Detection", "content": "We constructed the evaluation dataset using Noisy speech (Valentini-Botinhao et al., 2017).Noisy speech dataset contains corresponding pairs of noisy and clean data. The purpose of the dataset is to explore methods for speech enhancement.We selected the entire test set from this dataset, which includes 824 clean audio clips and 824 audio clips with ambient noise. We used all of these data, and the task was set as: \"Is there any ambient noise in this audio segment, in addition to the speaker voice? Please answer with yes or no.\""}, {"title": "B.4.10 Acoustic Scene Classification", "content": "We used MS-SNSD (Reddy et al., 2019) to synthesize these test datasets.MS-SNSD is a tool for synthesizing speech with environmental noise, aimed at advancing research in speech enhance-ment. We selected 51 environmental noise samples from its test set to synthesize 6,105 test samples, and the task was set as: \"What is the ambient noise of this audio segment? Please choose from the ['Babble', 'CopyMachine', 'Neighbor', 'ShuttingDoor', 'AirportAnnouncements', 'Munching', 'Typing', 'AirConditioner', 'VacuumCleaner'] options?\""}, {"title": "B.4.11 Speaker's Age Prediction", "content": "We have observed that there are relatively few datasets specifically aimed at speaker age recognition. We noted that the AIR Bench (Yang et al., 2024) has done an excellent job in addressing this task, We followed their approach of categorizing age into four groups but noticed that their data distribution was not balanced, specifically: teens to twenties: 653, thirties to forties: 268, fifties to sixties: 64, seventies to eighties: 15. Therefore, we used the SpeechAccentArchive (Weinberger, 2013) to balance the age distribution. Unfortunately, we found it difficult to obtain sufficient data for the seventies to eighties category, so we retained only three categories: teens to twenties, thirties to forties, and fifties to sixties. And the task was set as: \"Which age range do you believe best matches the speaker's voice? Please choose from the ['teens to twenties', 'thirties to forties', 'fifties to sixties'] options?\""}, {"title": "B.4.12 Speaker's Gender Recognition", "content": "We constructed the evaluation dataset using VCTK (Yamagishi et al., 2019).To balance the number of males and females in the benchmark, considering there are 61 female speakers and 47 male speakers in the VCTK dataset, we selected the top 47 female speakers along with all the male speakers. For each speaker, we chose the first 30 audio recordings. The task was set as: \"Is the speaker in this audio segment male or female?Please answer with 'male' or 'female'.\u201d"}, {"title": "B.4.13 Speech Emotion Recognition", "content": "In a genuine sense, understanding emotions in models should not solely depend on interpreting text. Emotions do not have a one-to-one correspondence with sentences; the same sentence can express various emotional tones depending on the speaker's emotional state. Therefore, it is crucial to advocate for models to move beyond mere textual content of sentences when inferring emotions and to delve into the non-textual information within the speech. Accordingly, in the evaluation set for emotion recognition, we employed a dataset unrelated to both the emotions and the sentence content-the RAVDESS dataset (Livingstone & Russo, 2018). The task is then defined as: \u201cWhat"}, {"title": "B.4.14 Cappella Emotion Recognition", "content": "We also used RAVDESS (Livingstone & Russo, 2018) to construct the evaluation set for singing emotion detection. The task is then defined as: \"What emotion does this audio clip convey? Please answer by single word select from ['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']."}, {"title": "B.4.15 Emotional Intensity Perception", "content": "We used the RAVDESS (Livingstone & Russo, 2018) dataset to construct the evaluation set for Emotional Intensity Perception. Since most models accept only a single audio input, we merged two audio segments and tasked the model with analyzing which part of the combined audio segment exhibits stronger emotional intensity. Specifically, we defined the problem as follows: \u201cIn this audio segment, a sentence is repeated twice. Is the emotion in the \u2018former' stronger or the \u2018latter' stronger? Please answer with 'former' or 'latter'.\" To balance the proportion between the two options, we alternated the placement of the stronger emotion, sometimes positioning it at the former and other times at the latter when synthesizing the data.\""}, {"title": "B.4.16 Emotion Translation", "content": "We believe that translations should reflect different expressions based on the emotional context. For example, the phrase \"What are you doing?\u201d can convey various meanings depending on the emotion-whether it's anger, surprise, sadness, or neutrality. In an angry context, it expresses strong disapproval or questioning of the person's actions; in a surprised context, it conveys disbelief about what the other person is doing; and in a sad context, it should reflect disappointment. Therefore, translations should be adjusted accordingly to better capture these nuances.\nWe observed that cosy Voice (SpeechTeam, 2024) demonstrates excellent zero-shot capabilities, effectively mimicking the tone and style of the input speech prompt. Therefore, we used cosy Voice to emulate the sentences with strong emotions from the RAVDESS (Livingstone & Russo, 2018) dataset to generate speech with corresponding emotions. After synthesis, we had five native speakers review the generated speech. If any of the native speakers felt that the synthesized speech did not convey the intended emotion, that segment was discarded. Ultimately, we obtained xxx valid speech samples. The task was set as: \u201cPlease translate the following sentence into the most appropriate Chinese, based on the emotion and content of this audio segment."}]}