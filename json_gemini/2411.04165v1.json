{"title": "BIO-XLSTM: GENERATIVE MODELING, REPRESENTATION AND IN-CONTEXT LEARNING OF BIOLOGICAL AND CHEMICAL SEQUENCES", "authors": ["Niklas Schmidinger", "Lisa Schneckenreiter", "Philipp Seidl", "Johannes Schimunek", "Pieter-Jan Hoedt", "Johannes Brandstetter", "Andreas Mayr", "Sohvi Luukkonen", "Sepp Hochreiter", "G\u00fcnter Klambauer"], "abstract": "Language models for biological and chemical sequences enable crucial applications such as drug discovery, protein engineering, and precision medicine. Currently, these language models are predominantly based on Transformer architectures. While Transformers have yielded impressive results, their quadratic runtime dependency on the sequence length complicates their use for long genomic sequences and in-context learning on proteins and chemical sequences. Recently, the recurrent XLSTM architecture has been shown to perform favorably compared to Transformers and modern state-space model (SSM) architectures in the natural language domain. Similar to SSMs, xLSTMs have a linear runtime dependency on the sequence length and allow for constant-memory decoding at inference time, which makes them prime candidates for modeling long-range dependencies in biological and chemical sequences. In this work, we tailor XLSTM towards these domains and propose a suite of architectural variants called Bio-xLSTM. Extensive experiments in three large domains, genomics, proteins, and chemistry, were performed to assess xLSTM's ability to model biological and chemical sequences. The results show that models based on Bio-xLSTM a) can serve as proficient generative models for DNA, protein, and chemical sequences, b) learn rich representations for those modalities, and c) can perform in-context learning for proteins and small molecules.", "sections": [{"title": "INTRODUCTION", "content": "Accurate computational models for biological sequences are essential for translating data into actionable insights in modern biology. Biological sequences like DNA, RNA, and proteins are central to molecular biology, genomics, and drug discovery. Major projects like the Human Genome Project (Lander et al., 2001) and the 1000 Genomes Project (1000 Genomes Project Consortium, 2010) have driven large-scale data collection efforts. Modeling these sequences is key to advancing life sciences (Benegas et al., 2023; Karollus et al., 2024), interacting with biological systems (Hopf et al., 2017; Riesselman et al., 2018; Yang et al., 2019) or predicting phenotypes from genetic variants (Ashley, 2016; Brandes et al., 2023; Acosta et al., 2022). Similar efforts exist for protein sequences (The UniProt Consortium, 2023) and small molecules (Kim et al., 2023; Zdrazil et al., 2023), used for tasks like protein engineering (Arnold, 2018; Yang et al., 2019), predicting 3D structures (Jumper et al., 2021), and drug discovery (Zhavoronkov et al., 2019). Large language models (LLMs) (Brown et al., 2020; Bubeck et al., 2023) have emerged as prime candidates for modeling biological sequences and serving as foundation models for molecular biology and chemistry (Ji et al., 2021; Schiff et al., 2024; Nguyen et al., 2023; Rives et al., 2021; Lin et al., 2023).\nLarge language models for biological sequences must handle long sequences and incorporate context. The rise of LLMs (Radford et al., 2018; Brown et al., 2020; Bubeck et al., 2023) has"}, {"title": "BACKGROUND AND NOTATION", "content": "XLSTM (Beck et al., 2024) consists of two types of layers: sLSTM (see Section 2.1) and mLSTM (see Section 2.2) which are the main components within block structures (see Section 2.3) of its multi-layer architectures. We consider a series of input vectors $x_t \\in \\mathbb{R}^D$ given at a certain time step $t \\in \\{1, ...,T\\}$. $X = X_{1:T} = (x_1,x_2, ..., x_T) \\in \\mathbb{R}^{D\\times T}$ denotes the matrix of stacked input vectors from all time steps. Both sLSTM and mLSTM are recurrent neural networks, which either map a state $(h_{t-1}, C_{t-1}, n_{t-1})$ to a successor state $(h_t, c_t, n_t)$ given an input $x_{t-1}$ (sLSTM) or a state $(h_{t-1}, C_{t-1}, n_{t-1})$ to a successor state $(h_t, C_t, n_t)$ given an input $x_{t-1}$ (mLSTM). Here, $h_t\\in \\mathbb{R}^d$ denotes a hidden state, $c_t \\in \\mathbb{R}^d$ and $C_t \\in \\mathbb{R}^{d\\times d}$ denote cell states responsible for long-term memory and, $n_t \\in \\mathbb{R}^d$ denotes a normalizer state. sLSTM and mLSTM utilize several adjustable weight matrices and bias vectors (detailed equations below) and employ input-, output-, and forget-gates, activated by exponential (exp) or the sigmoid functions ($\\sigma$). For cell inputs in sLSTM, the hyperbolic tangent function (tanh, abbreviated as $\\varphi$) is used as an activation function."}, {"title": "SLSTM", "content": "The forward pass of sLSTM in the vectorized version is defined as follows:\n$c_t = f_t \\odot C_{t-1} + i_t z_t$ \t cell state (1)\n$n_t = f_t n_{t-1} + i_t$ \t normalizer state (2)\n$h_t = \\varphi(c_t) n_t$ \t hidden state (3)\n$z_t = W_z x_t + R_z h_{t-1} + b_z$ \t cell input (4)\n$i_t = \\exp(\\tilde{i}_t)$ , \t $ \\tilde{i}_t = W_i x_t + R_i h_{t-1} + b_i $\t input gate (5)\n$f_t = \\exp(\\tilde{f}_t)$ , \t OR $\\sigma(\\tilde{f}_t)$, \t$\\tilde{f}_t = W_f x_t + R_f h_{t-1} + b_f$ \t forget gate (6)\n$o_t = \\sigma(\\tilde{o}_t)$ , \t$\\tilde{o}_t = W_o x_t + R_o h_{t-1} + b_o$ \t output gate, (7)\nwhere $i_t, o_t, f_t \\in \\mathbb{R}^d$ are the input, output and forget gate, respectively, $W_z, W_i, W_f, W_o \\in \\mathbb{R}^{d\\times D}$, $R_z, R_i, R_f, R_o \\in \\mathbb{R}^{d\\times d}$, and $b_z, b_i, b_f, b_o \\in \\mathbb{R}^d$ are trainable weight matrices and biases."}, {"title": "MLSTM", "content": "The forward pass of the mLSTM is defined as follows:\n$C_t = f_t C_{t-1} + i_t v_t k_t$ \t cell state (8)\n$n_t = f_t n_{t-1} + i_t k_t$ \t normalizer state (9)\n$h_t = o_t \\tilde{h}_t$ , \t $\\tilde{h}_t = C_t q_t / \\max \\{|n_t q_t|,1\\}$ \t hidden state (10)\n$q_t = W_q x_t + b_q$ \t query input (11)\n$k_t = \\frac{1}{\\sqrt{d}} W_k x_t + b_k$ \t key input (12)\n$v_t = W_v x_t + b_v$ \t value input (13)\n$i_t = \\exp(\\tilde{i}_t)$ , \t$\\tilde{i}_t = w_i^T x_t + b_i$ \t input gate (14)\n$f_t = \\sigma(\\tilde{f}_t)$ OR $\\exp(\\tilde{f}_t)$, \t $\\tilde{f}_t = w_f^T x_t + b_f$ \t forget gate (15)\n$o_t = \\sigma(\\tilde{o}_t)$ \t$\\tilde{o}_t = W_o x_t + b_o$ \t output gate (16)"}, {"title": "BLOCK STRUCTURES", "content": "The sLSTM and mLSTM layers are integrated into larger residual backbones (Srivastava et al., 2015; He et al., 2016), which incorporate layer normalization (Ba et al., 2016), pre- or post-up projection layers (Vaswani et al., 2017; Dao, 2024), with short causal convolutions and group normalization (Wu and He, 2020). For Bio-xLSTM we retain these basic building blocks but adjust them to the respective domains.\nThe entire architecture, including all layers, normalization, blocks, and other components, defines a mapping from an input sequence of length t to an output sequence. This mapping is denoted as $xLSTM : \\mathbb{R}^{D\\times t} \\rightarrow \\mathbb{R}^{D\\times t}$, where xLSTM transforms the stacked inputs up to time step t, i.e., $X_{1:t} := (x_1, x_2,...,x_t) \\in \\mathbb{R}^{D\\times t}$, to the corresponding stacked outputs of sequence length t, i.e., $Y_{1:t}: (y_1, y_2,..., y_t) \\in \\mathbb{R}^{D\\times t}$. The i-th sequence element is denoted with the subscript i, e.g. the i-th element from $X_{1:t}$ would be $(X_{1:t})_i$. Similarly to the mapping xLSTM, we also define mappings for the sequence-wise input-/output behaviour of layers themselves for an sLSTM layer (SLSTM : $\\mathbb{R}^{D\\times t} \\rightarrow \\mathbb{R}^{D\\times t}$) or an mLSTM layer (mLSTM : $\\mathbb{R}^{D\\times t} \\rightarrow \\mathbb{R}^{D\\times t}$). If the specific parameters used for the mapping are unclear, we will denote this by including a second argument in the function, separated by a semicolon."}, {"title": "MODES OF OPERATION: PARALLEL, CHUNKWISE, AND RECURRENT", "content": "The recurrent forms of sLSTM and mLSTM, introduced in Sections 2.1 and 2.2, provide efficient, constant-memory decoding during inference. This eliminates the need for expensive key-value caching, which represents a major challenge for Transformer models in long-range settings. Like Transformers, mLSTM allows for parallelization across the sequence length which significantly speeds up training. Additionally, similar to linear attention variants (Katharopoulos et al., 2020; Yang et al., 2024), mLSTM supports chunkwise parallel processing, blending recurrent and parallel modes (Beck et al., 2025). This approach is especially advantageous for long-sequence training and prompt encoding."}, {"title": "BIO-XLSTM: LONGE-RANGE MODELING OF BIOLOGICAL AND CHEMICAL SEQUENCES", "content": "Bio-xLSTM introduces three xLSTM-based architectural variants tailored specifically to DNA (Section 3.4), proteins (Section 3.5) and small molecules (Section 3.6). For these application domains, we extend XLSTM from causal language modeling (CLM) (Section 3.1) to new modeling approaches such as fill-in the middle (FIM), in-context learning (ICL) and masked language modeling (MLM) (Section 3.2)."}, {"title": "CAUSAL LANGUAGE MODELING AND NEXT-TOKEN PREDICTION", "content": "Causal language modeling (CLM) uses the\nCLM loss: $\\mathcal{L}_{CLM} = \\mathbb{E}_{x \\sim p_x} \\mathbb{E}_{t \\sim [[1,T-1]]} \\text{CE} \\big(x_{t+1}, \\text{xLSTM}(X_{1:t})_t\\big),$ (17)\nwhere CE is the cross-entropy loss (with logits), px is the data distribution, and $[[1, T \u2013 1]]$ is the discrete uniform distribution from 1 to $T \u2013 1$. The objective measures how well a particular sequence token $x_{t+1}$ can be predicted based on the previous tokens $X_{1:t}$ by the model xLSTM : $\\mathbb{R}^{D\\times t} \\rightarrow \\mathbb{R}^{D\\times t}$. Therefore, this type of modeling is sometimes also called next token prediction (NTP), uni-directional modeling or autoregressive (AR) modeling and the loss is also called NTP loss."}, {"title": "MASKED LANGUAGE MODELING (MLM)", "content": "Bio-xLSTM extends xLSTM to masked modeling of biological sequences, for which the typical de-masking or de-noising objective (Vincent et al., 2010; Devlin et al., 2019) is used, concretely the\nMLM loss: $ \\mathcal{L}_{MLM} = \\mathbb{E}_{x \\sim p_x} \\mathbb{E}_{t \\sim [[1,T]]} \\mathbb{E}_{M \\sim p_M} CE \\big(x_t, \\text{xLSTM}(X \\odot M)_t \\big), $ (18)\nwhere $M \\in \\{0,1\\}^{D \\times T}$ is a random matrix with binary entries which are usually drawn from a Bernoulli distribution pm, and $\\odot$ is element-wise multiplication. The objective measures how well the original sequence $X$ can be reconstructed from a noisy version $X \\odot M$ by the model xLSTM : $\\mathbb{R}^{D\\times T} \\rightarrow \\mathbb{R}^{D\\times T}$. This modeling paradigm has also been called bidirectional modeling. It has been highly successful in learning representations of proteins at evolutionary scale (Rives et al., 2021), which has powered many subsequent applications such as protein engineering and machine-learning guided directed evolution (Yang et al., 2019). For details on how xLSTM is extended to the MLM setting, we refer to Appendix Section B.3."}, {"title": "REVERSE COMPLEMENT (RC) EQUIVARIANCE", "content": "We develop an xLSTM block that is equivariant to the RC of an input sequence, a property particularly relevant to DNA-based applications. In double-helix DNA structures, both strands are semantically equivalent, with one strand being the RC of the other. The RC strand is oriented in the opposite direction of the forward strand, with base pairs converted from A to T and C to G. Shrikumar et al. (2017) show that a data-driven approach to learn the equivalence between RC sequences can fail. Therefore, Schiff et al. (2024) propose to enforce RC-equivariance by design, making use of two different inductive biases, post-hoc conjoining (PH) (Zhou et al., 2022) and parameter sharing (PS), in the architecture. In PH architectures, the backbone is trained to handle both DNA sequences and their RCs by applying RC augmentations during pre-training. For downstream tasks, PH architectures are applied to both the original sequence and its RC, and their outputs are summed to reach overall RC invariance. In contrast, PS architectures integrate RC-equivariant xLSTM blocks with equivariant word embeddings and language model heads similar to Schiff et al. (2024). For additional details, see Appendix Section C.4."}, {"title": "DNA-XLSTM", "content": "For the DNA domain, we propose the DNA-xLSTM architecture to enhance sequence modeling capabilities, particularly for varying context lengths. We introduce three model configurations based on DNA-xLSTM: two sLSTM-based configurations trained with a context window of 1,024 tokens (DNA-xLSTM-500k and DNA-xLSTM-2M), and an mLSTM-based configuration trained"}, {"title": "PROT-XLSTM", "content": "For the protein domain, we propose the architectural variant Prot-xLSTM to address the complexities of protein sequence data, particularly in capturing long-range dependencies to enable homology-conditioned modeling. We introduce two configurations: Prot-xLSTM-26M and Prot-xLSTM-102M, with 26M and 102M parameters, respectively. Both configurations consist of 16 mLSTM blocks, with embedding dimensions of 512 for Prot-xLSTM-26M and 1,024 for Prot- xLSTM-102M and maintaining a consistent 2:1 projection ratio across both configurations, and are trained with increasing context sizes ranging from 2,048 to 262,144 tokens. To effectively manage the wide range of protein sequence lengths and context sizes, RoPEs are implemented for Prot-xLSTM. The according Prot- XLSTM models are trained with CLM using a FIM strategy on non-aligned homologous sequences, enabling them to perform ICL at inference time in two modes: a) generative and b) inpainting. Both approaches can be used for protein design, with the latter also suited for residue-based predictions, such as mutant fitness estimation. Prot-xLSTM shows better performance than similarly configured Mamba- and Transformer-based models and shows promising results for homology-conditioned sequence generation (see Section 4.2)."}, {"title": "CHEM-XLSTM", "content": "For the chemical sequence domain, we introduce Chem-xLSTM to enhance generative modeling of SMILES strings (Weininger, 1988), enabling domain-conditioned generation of small molecules without fine-tuning. We introduce two models: an unconditional generative model trained with a context length of 100 tokens (Chem-xLSTM-15M) and a domain-conditioned model trained with a 4,096-token context for in-context learning tasks (Chem-xLSTM-15M-icl). The latter can generate molecules within a specific domain without fine-tuning only based on examples provided as context, a highly sought-after capability in drug discovery. Both models are configured to have 15M parameters, consist of 9 mLSTM blocks with an embedding dimension of 512 and a 1.3:1 projection ratio. The models have been benchmarked against other generative models for SMILES and at their ICL capabilities (see Section 4.3)."}, {"title": "EXPERIMENTS AND RESULTS", "content": ""}, {"title": "DNA SEQUENCES", "content": "For the DNA-XLSTM experiments, we followed the experimental protocol outlined in Schiff et al. (2024) and Nguyen et al. (2023) for both pre-training and downstream adaptation.\nPre-training. The training data for both the CLM and MLM tasks was sourced from the human reference genome (Church et al., 2011), with context lengths set to 1,024 and 32k tokens. Our baseline models included HyenaDNA (Nguyen et al., 2023) and Caduceus (Schiff et al., 2024), which is based on the Mamba architecture. Additionally, we trained Transformer++ baselines, building on the Llama architecture (Touvron et al., 2023). Similar to Caduceus, we experimented with both PH- and PS-equivariant XLSTM configurations, benchmarking them against the corresponding Mamba baselines. All models that did not use PS-equivariance were trained with RC augmentation. Hyperparameters"}, {"title": "PROTEIN SEQUENCES", "content": "We followed the experimental protocols from Sgarbossa et al. (2024) for protein sequences.\nHomology-aware training. Training data was sourced from the filtered OpenProteinSet (Ahdritz et al., 2023), consisting of 270k UniClust30 clusters (508M sequences, 110B residues). Using the ProtMamba pipeline, we constructed homology-aware, alignment-free inputs by concatenating unaligned homologous sequences and mask patches for training with the FIM strategy. We trained two xLSTM-based models: Prot-xLSTM-26M and Prot-xLSTM-102M. For comparison, we also trained a smaller ProtMamba (ProtMamba-28M) and Transformer-based (Prot-Transformer++-26M) (Touvron et al., 2023) model and used the ProtMamba Long Foundation (ProtMamba-107M) provided by Sgarbossa et al. (2024). The initial training followed a context length scheduling strategy, with models gradually increasing context from $2^{11}$ to $2^{17}$ tokens. Finally, Prot-xLSTM-102M was further trained with $T = 2^{18}$.2 We evaluated the models using negative log-likelihood and perplexity, calculated for different parts of the concatenated-FIM sequences."}, {"title": "CHEMICAL SEQUENCES", "content": "Unconditional molecule generation aims to produce valid small organic molecules without imposing specific constraints, such as being from a particular molecular domain. Following the setup of \u00d6z\u00e7elik et al. (2024), we trained models to generate SMILES strings using a CLM approach on a dataset derived from ChEMBL with a context length of 100 tokens. We compared our Chem-xLSTM architecture with several architectures, including LSTM, GPT, S4, and Mamba, where all models contain approximately 15 million parameters. The evaluation focused on two primary metrics: perplexity and Fr\u00e9chet ChemNet Distance (FCD) (Preuer et al., 2018). Chem-xLSTM achieved the lowest FCD of 0.13 and a competitive perplexity score of 1.68, indicating its strong ability to generate realistic chemical structures (see Table 4). All models produced valid, unique, and novel molecules,"}, {"title": "COMPUTE DEMAND AND RESOURCES.", "content": "The experiments were conducted on multiple GPU servers with A100 GPUs. Model training was performed in both single-node and multi-node setups, utilizing 1\u20138 A100 GPUs per node. Prot-"}, {"title": "DISCUSSION", "content": "In this work, we demonstrated the potential of Bio-xLSTM variants as prime candidates to model biological and chemical sequences. We have provided clarity in two key areas: a) how to tailor XLSTM for biological and chemical sequences, and b) comparing xLSTM-based models to other domain-specific LLMs, showcasing their robust performance in DNA, protein, and chemical sequence modeling tasks. Despite certain limitations, DNA-xLSTM showed strong performance in DNA sequence modeling, excelling in both masked and causal language tasks across different context sizes. In protein modeling, Prot-xLSTM proved particularly effective at handling long-range dependencies, positioning it as a promising tool for generating homologous proteins. In small molecule modeling, Chem-xLSTM achieved the best FCD scores for unconditional generation and demonstrated strong ICL capabilities. Our findings underscore the potential of xLSTM as a prime candidate for foundational models in molecular biology. The models we have introduced, trained, and made available can be used for example to generate rich learned representations for DNA sequences and homology- and chemical domain-conditioned generation of proteins and molecules without the need for fine-tuning.\nWhile Bio-xLSTM has shown strong performance across DNA, protein, and chemical sequence modeling, it has several limitations. The manual hyperparameter selection process, which was due to limited computational resources, may prevent optimal model configurations. We will explore the hyperparameter space further in the future, which might yield even better models. For DNA, the reliance on character-level tokenization might also restrict the performance and scaling to larger context sizes. Also for proteins, amino acid level tokenization without explicit structural information might limit it's performance. The DNA-xLSTM, Prot-xLSTM, and Chem-xLSTM models are currently constrained by the training dataset and their generalizability across organisms and chemical domains needs further exploration. Across all three domains, the training datasets contain biases \u2013 whether it is population biases in the genomic data, sequence distribution biases in protein datasets, or chemical exploration biases in molecular datasets. These biases could influence the model's predictions and limit its generalizability in real-world applications. In line with many works, we consider the perplexity metric, for example, next token perplexity, or the related cross-entropy losses as a proxy for performance on downstream tasks. However, this metric might not capture the capacities of biological and chemical language models appropriately. Future work could address these limitations by expanding the training datasets and downstream evaluations of Bio-xLSTM. Finally, assessing Bio-xLSTM's performance in parameter regimes beyond the billion scale remains an open question."}, {"title": "ETHICS STATEMENT", "content": "The development of our large language model for biological sequences, including DNA, proteins, and small molecules, has the potential to significantly advance biomedical research and therapeutics. In creating this model, we have taken care to train exclusively on publicly available data, such as the human reference genome, OpenProteinSet, and publicly available small molecule databases. Note that no human subjects are involved in the studies, since the human reference genome does not represent a particular individual but a type of average human genome. As common with machine learning methods, potential danger lies in the possibility that users rely too much on our new approach and use it without reflecting on the outcomes. However, the full pipeline, in which our method would be used, includes wet lab tests after its application, to verify and investigate the results, which decreases the danger of misuse or overly relying on the predictions. To further mitigate the risk of misuse, we provide model limitations and mention potential biases. Users are encouraged to approach the model's predictions critically and consider them as one component of a broader decision-making process."}, {"title": "XLSTM ARCHITECTURE DETAILS", "content": ""}, {"title": "XLSTM AND BIO-XLSTM BLOCKS", "content": "Beck et al. (2024) suggested xLSTM blocks, which are residual (Srivastava et al., 2015; He et al., 2016) block modules, into which the sLSTM and mLSTM layers can be integrated. The two basic blocks can in principle be characterized by either applying post-sLSTM/mLSTM up- and down-projections (similar to Vaswani et al. (2017)) or by applying pre-sLSTM/mLSTM up-projections and post-sLSTM/mLSTM down-projections (similar to Dao (2024)). An sLSTM block integrates the sLSTM layer into the up- and down-projection block, while the mLSTM block integrates the mLSTM layer into the pre-up-projection and post-down-projection block. The two basic xLSTM blocks also make use of neural network modules like layer normalization (Ba et al., 2016), short causal convolutions, and, group normalization (Wu and He, 2020). For the exact architecture of the blocks, we refer to Beck et al. (2024, Sec.2.4). An xLSTM architecture is constructed by residually stacking the suggested xLSTM blocks. For that, the most commonly used pre-LayerNorm residual backbone is used.\nFor Bio-xLSTM we keep the basic xLSTM building blocks and the basic xLSTM architecture template, but adjust them to the respective domains."}, {"title": "MODES OF OPERATION: PARALLEL, CHUNKWISE OR RECURRENT", "content": "Similar to linear attention variants (Katharopoulos et al., 2020; Yang et al., 2024), the mLSTM has three possible formulations: parallel, recurrent or chunkwise. The presentation in section 2.2 (and Beck et al., 2024) focuses on the recurrent form:\n$C_t = \\sigma(f_t) C_{t-1} + \\exp(i_t) v_t k_t$ (9)\n$n_t = \\sigma(f_t) n_{t-1} + \\exp(i_t) k_t$ (10)\n$h_t = \\sigma(\\tilde{o}_t) \\frac{C_t q_t}{\\max(|n_t q_t|, 1)}$\t(11)\nThis form is especially useful for inference when samples arrive one time-step at a time."}, {"title": "EFFICIENT BIDIRECTIONAL MODELING FOR WEIGHT-TIED LAYERS OF BIO-XLSTM", "content": "Bidirectional modeling is often required to learn representations of biological and chemical sequences, for example with the MLM paradigm. The default approach for bidirectional modeling would be to use an mLSTM layer on the usual sequence $X_{1:T} = (x_1,x_2,...,x_T)$ and then applying a weight-tied layer on the reversed sequence $X_{T:1} = (x_T,x_{T-1},...,x_1)$ and subsequently summing those outputs:\n$H^+ = \\text{mLSTM}(X_{1:T}; w)$ (19)\n$H^- = \\text{mLSTM}(X_{T:1}; w)$ (20)\n$H = H^+ + H_{T:1},$ (21)"}, {"title": "DNA-XLSTM: DETAILS AND ADDITIONAL RESULTS", "content": "In this section, we provide further details regarding the architecture, training setup, and evaluation metrics for the DNA-XLSTM models."}, {"title": "PRE-TRAINING", "content": "Experimental setup. We followed the experimental protocol established in Schiff et al. (2024) and Nguyen et al. (2023). The human reference genome (Church et al., 2011) was used as the training dataset for two main tasks: a) causal language modeling (CLM) and b) masked language modeling (MLM). We employed context lengths of 1,024 and 32,000 tokens for these tasks.\nTo ensure a fair comparison with previous methods, such as Schiff et al. (2024), we used character- or base pair-level tokenization, training models with parameter sizes ranging from 500k to 4M. This experimental setup enabled us to evaluate model performance for both a) generative modeling of DNA sequences and b) learning rich DNA sequence representations-core tasks in this domain.\nMethods and hyperparameters. In our pre-training experiments, we compared several architectures: a Transformer variant based on the Llama architecture, referred to as Transformer++ (Touvron et al., 2023), DNA-xLSTM, HyenaDNA (Nguyen et al., 2023), and DNA-Mamba (also known as Caduceus) (Schiff et al., 2024). Each architecture was trained under both CLM and MLM settings. Additionally, we assessed two types of reverse-complement (RC) equivariant models when applicable: DNA-Mamba-PH and DNA-Mamba-PS, as well as DNA-xLSTM-PH and DNA-xLSTM-PS. For non-equivariant models, reverse-complement augmentation was applied, following the approach described in Schiff et al. (2024). Further details on RC-equivariant modeling can be found in Section C.4. The hyperparameters for DNA-xLSTM and Transformer++ were optimized using a validation set, with the final configurations reported in Appendix Tables A4 and A3.\nMetrics. We report cross-entropy loss on a held-out test set for both CLM and MLM pre-training experiments.\nResults. Our experiments show that the sLSTM-based DNA-xLSTM-2M model, trained with a context size of 1,024 and reverse-complement augmentation, outperforms DNA-Mamba (Schiff et al., 2024), HyenaDNA (Nguyen et al., 2023), and Transformer++ across both CLM and MLM tasks. Notably, the performance gap between DNA-xLSTM and the baseline models increases in the MLM setting. See Figure 2.\nWe further enhanced DNA-xLSTM-500k and DNA-xLSTM-2M models by incorporating reverse-complement equivariance via parameter sharing. For smaller models, we achieved MLM losses comparable to DNA-Mamba-PS, with a significant improvement over DNA-Mamba-PS as model size scaled to 2M parameters (Figure A3). Additionally, we pre-trained a long-range DNA-xLSTM model based on mLSTM, with a context size of 32k, using both CLM and MLM objectives. This model achieved the lowest cross-entropy loss in both tasks, outperforming Transformers and HyenaDNA, while performing comparably to Mamba (Figure A2)."}, {"title": "DOWNSTREAM TASKS", "content": "Experimental setup. Two sets of downstream tasks were used for evaluating the learned representations: the Genomic benchmark (Gre\u0161ov\u00e1 et al., 2023) and the Nucleotide Transformers Tasks (Dalla-Torre et al., 2023), which is a collection of 18 datasets derived from five peer-reviewed studies (Phaml et al., 2005; Oubounyt et al., 2019; Wang et al., 2019a; Scalzitti et al., 2021; Geng et al., 2022). These classification tasks were selected to determine how rich the learned representations of the architectures are. To extract representations from the pre-trained XLSTM-DNA models, we perform average pooling on the activations from the final xLSTM block. For each downstream dataset, these representations served as inputs to a task-specific classification head that were jointly fine-tuned with the pre-trained model parameters.\nMethods and hyperparameters. For Nucleaotide Transformer tasks, we compared HyenaDNA, DNA-Mamba, and xLSTM-based models pre-trained with 2M parameters. For Genomic benchmark tasks, we compare the smaller xLSTM-500k against Mamba. In both settings, models were pre-trained with a context size of 1,024."}, {"title": "ARCHITECTURE AND HYPERPARAMETERS", "content": "The hyperparameters and composition of the DNA-xLSTM and DNA-Transformer++ models for pre-training with context size 1k and 32k are reported in Tables A4 and A3. The hyperparameters were selected on a separate validation set using manual hyperparameter selection due to limited computational resources."}, {"title": "REVERSE-COMPLEMENT INVARIANCE", "content": "We develop an xLSTM version that is invariant to the RC of an input sequence which is relevant for DNA applications following Schiff et al. (2024). In double-helix DNA structures, both strands are semantically equivalent, as one strand is the RC of the other. Given a strand,, its RC, \u25a1, is oriented in the opposite"}]}