{"title": "SepsisLab: Early Sepsis Prediction with Uncertainty Quantification and Active Sensing", "authors": ["Changchang Yin", "Pin-Yu Chen", "Bingsheng Yao", "Dakuo Wang", "Jeffrey Caterino", "Ping Zhang"], "abstract": "Sepsis is the leading cause of in-hospital mortality in the USA. Early sepsis onset prediction and diagnosis could significantly improve the survival of sepsis patients. Existing predictive models are usually trained on high-quality data with few missing information, while missing values widely exist in real-world clinical scenarios (especially in the first hours of admissions to the hospital), which causes a significant decrease in accuracy and an increase in uncertainty for the predictive models. The common method to handle missing values is imputation, which replaces the unavailable variables with estimates from the observed data. The uncertainty of imputation results can be propagated to the sepsis prediction outputs, which have not been studied in existing works on either sepsis prediction or uncertainty quantification. In this study, we first define such propagated uncertainty as the variance of prediction output and then introduce uncertainty propagation methods to quantify the propagated uncertainty. Moreover, for the potential high-risk patients with low confidence due to limited observations, we propose a robust active sensing algorithm to increase confidence by actively recommending clinicians to observe the most informative variables. We validate the proposed models in both publicly available data (i.e., MIMIC-III and AmsterdamUMCdb) and proprietary data in The Ohio State University Wexner Medical Center (OSUWMC). The experimental results show that the propagated uncertainty is dominant at the beginning of admissions to hospitals and the proposed algorithm outperforms state-of-the-art active sensing methods. Finally, we implement a SepsisLab system for early sepsis prediction and active sensing based on our pre-trained models. Clinicians and potential sepsis patients can benefit from the system in early prediction and diagnosis of sepsis.", "sections": [{"title": "1 INTRODUCTION", "content": "Sepsis, defined as life-threatening organ dysfunction in response to infection, contributes to up to half of all hospital deaths and is associated with more than $24 billion in annual costs in the United States [13]. Existing studies [14] have shown that a sepsis patient may benefit from a 4% higher chance of survival if they are diagnosed 1 hour earlier, so developing an early sepsis onset prediction system can significantly improve clinical outcomes.\nExisting machine-learning-based predictive models [7, 10, 21, 38] are usually trained on high-quality data with few missing information, while missing values widely exist in emergency department (ED) and emergency medical services (EMS) settings, which would cause most existing sepsis prediction models to suffer from performance decline and high uncertainty. In addition, existing studies [22, 30] have shown that for sepsis cases, most patients have already progressed into sepsis before the admissions to hospitals or during the first hours of admissions. Thus it is critical to develop accurate sepsis prediction systems that can handle high missing-rate settings (e.g., cold-start setting with only several limited vital signs).\nA common method to handle missing variables is imputation, in which missing values are replaced by estimates from the observed data. To use the existing methods, we will need data imputations, which come with a new problem for the downstream sepsis prediction tasks: the uncertainty of imputation results can propagate to the sepsis prediction models. Especially for deep learning models, a small perturbation in the input variables might cause a significant"}, {"title": "2 RELATED WORK", "content": "In this section, we briefly review the existing studies related to sepsis prediction systems, uncertainty qualification and active sensing."}, {"title": "2.1 Sepsis Prediction Systems", "content": "Sepsis is a heterogeneous clinical syndrome that is the leading cause of mortality in hospital intensive care units (ICUs) [24, 33]. Early prediction and diagnosis may allow for timely treatment and lead to more targeted clinical interventions. Screening tools have been used clinically to recognize sepsis, including qSOFA [25], MEWS [27], NEWS [26], and SIRS [3]. However, those tools were designed to screen existing symptoms as opposed to explicitly early predicting sepsis before its onset, and their efficacy in sepsis diagnosis is limited. With recent advances, deep learning methods have shown great potential for accurate sepsis prediction [7, 10, 21, 38]. Although the methods achieved superior performance, they face a critical limitation: the models need to take the complete observation of a list of variables (including vital signs and lab tests), while lots of variables are missing in real-world data (especially in the first hours of admissions). Existing studies [7, 10, 38] usually impute the missing values before the prediction, which raises a new problem that the sepsis prediction models will heavily rely on the imputation methods. The imputation uncertainty would also be propagated to downstream prediction models. Thus it is necessary to quantify the propagated uncertainty, especially for high-stakes sepsis prediction tasks."}, {"title": "2.2 Uncertainty Qualification", "content": "Understanding what a model does not know is a critical part of many machine learning systems. Despite the superior performance deep learning models have achieved in various domain, they are usually over-confident about the predictions, which could limit their applications to real-world risk-sensitive settings (e.g., in healthcare). Uncertainty quantification methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes [1]. Existing uncertainty qualification work [4, 6, 11, 23] has widely studied epistemic uncertainty and"}, {"title": "2.3 Active Sensing", "content": "Active sensing aims to improve the target tasks' performance by actively selecting most informative variables with the minimal cost. Yu et. al [37] propose to select the informative variables based on mutual information and predictive variance. However, the model is based on Bayesian co-training framework, the prediction ability of which is not as good as deep neural networks when handling large-scale time serial data. Yoon et. al [36] attempt to solve the active sensing problem by proposing an RNN-based model (i.e., Deep Sensing). The Deep Sensing framework involves learning 3 different networks: an interpolation network, a prediction network and an error estimation network. Each network is separately optimized for its own objective and then combined together after training to be used for active sensing. Jarrett et. al [8] propose an Inverse Active Sensing (IAS) to require negotiating (subjective) trade-off between accuracy, speediness, and cost of information. Yoon et. al [35] propose an RL-based framework (Active Sensing using Actor-Critic models, ASAC) to directly optimize the predictive power after active sensing. Although the methods achieved superior performance in the target prediction tasks, they failed to measure the uncertainty of both missing values and model output risks, which limit their application in high-stakes clinical settings.\nIn this study, we aim to develop an accurate sepsis prediction system with propagated uncertainty quantification and incorporate active sensing algorithms to reduce the propagated uncertainty."}, {"title": "3 METHODOLOGY", "content": "In this section, we present the proposed sepsis prediction system SepsisLab, including a missing value imputation model, an early sepsis prediction model, and an active sensing algorithm."}, {"title": "3.1 Notation and Problem Statement", "content": "In this study, we aim to predict sepsis onset with limited clinical variables observed. We consider the following setup. A patient has a sequence of clinical variables (i.e., lab test data and vital sign data) with timestamps. Let \\(Z \\in {\\mathbb{R} \\cup *}^{n \\times k}\\) denote the observations of variables, where * represents missing values, \\(n\\) denotes the number of collections of observations and \\(k\\) denotes the number of unique clinical variables. \\(T \\in \\mathbb{R}^{n}\\) denotes the observation timestamps. \\(Y \\in {0, 1}^{n}\\) denotes the ground truth of whether the patient will progress to sepsis in the coming hours. Following [10, 38], we set the prediction window as 4 hours. Due to the existence of missing values, we impute the missing values first and use \\(X \\in \\mathbb{R}^{n \\times k}\\) to denote the imputed results.\nGiven a loss function \\(\\mathcal{L}\\) and a distribution over pairs (X, Y), the goal is to find a function \\(f\\) that minimize the expected loss:\n\\[f^* = \\arg \\min_{f} E[\\mathcal{L}(f(X), Y)] \\tag{1}\\]\nWe list the important notations in Table 1."}, {"title": "3.2 Missing Value Imputation", "content": "We assume the missing values follow the Gaussian distributions and impute the missing values by estimating the distribution of variables (i.e., the mean and covariance). Figure 2(A) shows the framework of our imputation model.\nFollowing [31], we first use mean-imputation to preprocess the observational data Z and send the embedding of Z to LSTM to model the patient's health states.\nEmbedding layer. In the \\(i\\)-th collection, we have observational values \\(Z_i\\), observation time \\(T_i\\). We use a fully connected layer to embed the observed variable in the collection:\n\\[e_i = w_e [Z_i; e] + b_e, \\tag{2}\\]\nwhere [;] denotes concatenation operation. \\(w_e \\in \\mathbb{R}^{(k+2d) \\times d}\\) and \\(b_e \\in \\mathbb{R}^d\\) are learnable variables. \\(e \\in \\mathbb{R}^{2d}\\) denotes the time embedding and is computed as follows:\n\\[e_{i,j} = \\sin(\\frac{j}{T_{\\max} * d} T_{i,j}), e_{i, d+j} = \\cos(\\frac{j}{T_{\\max} * d} T_{i,j}),\\tag{3}\\]\nwhere \\(0 \\leq j < d\\), and \\(T_{\\max}\\) denotes the max value of T.\nTime-aware LSTM encoder. Given the embedding vectors \\([e_1, e_2, ..., e_n]\\), we use LSTM to model the patients' states:\n\\[S_1, S_2, ..., S_T = LSTM(e_1, e_2, ..., e_n) \\tag{4}\\]\nMissing value distribution estimation. A fully connected layers is used to generate the parameters of the missing value distribution:\n\\[\\mu_i = w_{\\mu}s_i + b_{\\mu}, \\sigma_i = ReLU(w_{\\sigma}s_i + b_{\\sigma}),\\tag{5}\\]\nwhere \\(w_{\\mu}, w_{\\sigma} \\in \\mathbb{R}^k\\) and \\(b_{\\mu}, b_{\\sigma} \\in \\mathbb{R}\\) are learnable variables.\nWe train the imputation model with the mean square error loss function:\n\\[\\mathcal{L}_{imp} (Z, M, \\mu) = \\sum_{i=1}^n \\sum_{j=1}^k M_{ij} (\\mu_{ij} - Z_{i,j})^2, \\tag{6}\\]\nwhere \\(M \\in {0,1}^{n \\times k}\\) denotes the indices of masked variables. \\(M_{i, j}\\) is 1 if the \\(j\\)-th variable in \\(i\\)-th collection is observed and masked; otherwise, 0. Replacing the missed values * with the estimates \\(\\mu\\), the observed variables Z become \\(X \\in \\mathbb{R}^{T \\times n}\\).\nAfter the imputation model is well-trained with Equation 6, we further learn to estimate the standard deviation \\(\\sigma\\) by finetuning \\(w_\\sigma\\) and \\(b_\\sigma\\) and fixing other parameters. We minimize the following loglikelihood loss:\n\\[\\mathcal{L}_{\\sigma} (Z, M, \\mu, \\sigma) = \\sum_{i=1}^n \\sum_{j=1}^k M_{i,j} [\\frac{(\\mu_{i,j} - Z_{i,j})^2}{2\\sigma_j^2} + \\frac{\\log \\sigma_j^2}{2} ]\\tag{7}\\]"}, {"title": "3.3 Sepsis Prediction Model", "content": "With the imputed results to replace the missing values, we continue to predict whether the patients will suffer from sepsis in the coming hours. The framework of sepsis prediction model is shown in Figure 2(B).\nSimilar to Equation 8 in the imputation model, we use the same embedding layers in the imputation model.\n\\[e_i = w_e [X_i; e] + b_e, \\tag{8}\\]\nwhere the time embedding \\(e\\) is the same as in Equation 3.\nThen we use LSTM [5] to model the patient's health states. A fully connected layer and a Sigmoid layer is followed to generate the sepsis risks:\n\\[P_i = Sigmoid(w_s h_i + b_s), \\text{ where } t = 1, 2, ..., T \\tag{9}\\]\n\\[h_1, h_2, ..., h_n = LSTM(e_1, e_2, ..., e_n), \\tag{10}\\]\nwhere \\(w_s \\in \\mathbb{R}^d\\) and \\(b_s \\in \\mathbb{R}\\) are learnable parameters.\nThe model is trained by minimizing the binary cross-entropy loss:\n\\[\\mathcal{L}_{cls} (P, Y) = \\sum_{i=1}^n -y_i \\log(p_i) - (1 - y_i) \\log(1 - p_i) \\tag{11}\\]"}, {"title": "3.4 Sources of Uncertainty", "content": "When applying deep learning methods to high-stakes sepsis prediction tasks, the lack of uncertainty quantification will make the models less reliable. In this subsection, we investigate two main sources of uncertainty.\nUncertainty from the model parameters. Existing uncertainty qualification work [4, 6, 11, 23] has widely studied epistemic uncertainty, which accounts for uncertainty in the model parameters, especially for the huge amount of parameters in deep learning models. Following [11], we use drop-out during the test phase and run the inference many times to quantify such kind of uncertainty.\nUncertainty from missing values. Superior risk prediction models in the healthcare domain heavily rely on high-quality complete input. However, missing values (e.g., vital signs and lab test results) widely exist in real-world clinical settings. Most risk prediction methods [7, 10, 21, 38] first impute the missing values and then make predictions based on the imputed values. The accuracy"}, {"title": "3.5 Uncertainty Definition", "content": "We use the variance of prediction models' output to define the two kinds of uncertainty mentioned above. Patients' data X contains a sequence of collections of variables. We can use all the observations until the current collections to make predictions. When applying active sensing algorithms to reduce the propagated uncertainty with additional observations, we can only request the variables in the current collection.\nIn the active sensing task, we only focus on the uncertainty related to the latest collection. In the following subsections, for simplicity, at a given time \\(T_i\\), we use \\(x\\) to represent the \\(i\\)-th observation (i.e., \\(X_i\\)), and use \\(f_w(x)\\) rather than \\(f_w(X)\\) to denote the predicted risk, where \\(w\\) means all the learnable parameters in the sepsis prediction model.\nWe assume the input variables \\(x \\in \\mathbb{R}^k\\) and model parameters \\(w\\) follow Gaussian distributions \\(\\mathcal{N} (\\mu_x, \\sigma_x)\\) and \\(\\mathcal{N} (\\mu_w, \\sigma_w)\\). \\(\\mu_x \\in \\mathbb{R}^k\\) and \\(\\sigma_x \\in \\mathbb{R}^k\\) can be estimated with Equation 5. Let \\(y\\) denote the sepsis prediction label for the patient at current time.\nFollowing existing studies [11], we define the uncertainty of predicted risk as the variance of model outcomes:\n\\[U = \\mathbb{E}_{x,w}[(f_w(x) - y)^2] = U_x + U_w \\tag{12}\\]\nwhere \\(U_x = \\mathbb{E}_{x,w}[(f_w(x) - \\mathbb{E}_{yw})^2],\\)\n\\(U_w = \\mathbb{E}_w[(\\mathbb{E}_{yw} - y)^2],\\)\n\\(\\mathbb{E}_{yw} = \\mathbb{E}_x[f_w(x)],\\)\n\\(y = \\mathbb{E}_{x,w}[f_w(x)].\\)\nwhere \\(p()\\) denotes the density function.\nWe split the uncertainty into two terms. The second term \\(U_w\\) is caused by the model uncertainty from the model parameters, so we just focus on the first term \\(U_x\\) when actively selecting unobserved variables.\nWhen the model parameter \\(w\\) is fixed, we can estimate the propagated uncertainty as:\n\\[U_x = U(w) = \\mathbb{E}[(f_w(x) - \\mathbb{E}_x f_w(x))^2] \\tag{13}\\]"}, {"title": "3.6 Propagated Uncertainty Quantification", "content": "3.6.1 Propagated Uncertainty for Linear Target Prediction. When the sepsis risk prediction function is a linear function, \\(f_w(x) = \\sum_j w_jx_j\\), following [12], we compute the uncertainty in Equation 13 as:\n\\[U(w) = \\sum_i w_i^2 \\sigma_{x_i}^2 + \\sum_{i} \\sum_{j \\neq i} w_iw_j \\rho_{ij} \\sigma_{x_i} \\sigma_{x_j}, \\tag{14}\\]\nwhere \\(\\rho_{ij}\\) denotes the correlation between \\(i\\)-th and \\(j\\)-th variable. It is easy to compute the propagated uncertainty for linear function based on Equation 14 for linear function. The calculation details\nfor Equation 14 can be found in subsection A.1 in supplementary materials.\nThe propagated uncertainty reduction after observing \\(i\\)-th variable is:\n\\[U(w)^{(i)} = \\sum_{j \\neq i} w_j^2 \\sigma_{x_j}^2 + \\sum_{i \\neq j} \\sum_{j} w_i w_j \\rho_{ij} \\sigma_{x_i} \\sigma_{x_j} \\tag{15}\\]\n3.6.2 Propagated Uncertainty for Non-Linear Target Prediction. For the non-linear sepsis prediction function, we use the Taylor expansion as approximate function:\n\\[f_w(x + \\delta) = f_w(x) + \\delta^T \\nabla_x f_w(x) \\tag{16}\\]\nWe can use the uncertain propagation in Equation 14 as the ap-proximation of the uncertainty of non-linear function \\(f_w\\). However, the propagated uncertainty estimation for non-linear functions are biased on account of using a truncated series expansion. The extent of this bias depends on the nature of the function.\nThe absolute difference between the two values \\(f_w(x + \\delta)\\) and \\(f_w(x + \\delta)\\) is:\n\\[g(\\delta,x) = |f_w(x + \\delta) - f_w(x) + \\delta^T \\nabla_x f_w(x)| \\tag{17}\\]\nWhen \\(g(\\delta, x)\\) is small enough in the neighborhood near \\(\\mu_x\\) (i.e., \\(f_w\\) is locally linear), the propagated uncertainty in Equation 14 is still accurate and able to guide the active sensing."}, {"title": "3.7 Robust Active Sensing", "content": "3.7.1 Adversarial Training for Local Linearity. Existing studies [18, 19] have shown that adversarial training can encourage the local linearity of the learned functions. In this study, we adopt adversarial training to make the target prediction function locally linear in a neighborhood near the mean value of input \\(x\\).\n\\[\\mathcal{L}_{adv} = \\min_W \\max_{\\delta} g(\\delta, \\mu_x), \\text{ where } -2\\sigma_x < \\delta < 2\\sigma_x \\tag{18}\\]\nThe risk prediction model is trained with a weighted sum of classification loss and adversarial loss:\n\\[\\mathcal{L} = \\alpha \\mathcal{L}_{cls} + (1 - \\alpha) \\mathcal{L}_{adv} \\tag{19}\\]\nwhere \\(0 < \\alpha < 1\\) is a hyper-parameter.\nWe consider the quantity:\n\\[\\gamma(\\sigma, x) = \\max_{-2\\sigma \\leq \\delta \\leq 2\\sigma} |f_w(x + \\delta) - f_w(x) - \\delta^T \\nabla_x f_w(x)| \\tag{20}\\]\nto be a measure of how linear the surface is within a neighborhood near \\(x\\). We call this quantity the local linearity measure. The missing variables follow Gaussian distribution, so \\(\\delta\\) lies within two standard deviations with more than probability 95%. The uncertainty estimation error would be less than \\(\\gamma(\\sigma_x, x)\\) with probability more than 95%.\nAlgorithm 1 describes the training process of the sepsis prediction model.\n3.7.2 Active Sensing. The approximate uncertainty of the risk prediction outcome is defined as:\n\\[U_x^{(i)} = \\mathbb{E}_{x, w} [(\\sum_j x_i x_j \\rho_{ij} \\sigma_{x_i} \\sigma_{x_j}) + (\\sum_i \\sum_{j \\neq i} x_i x_j \\rho_{ij} \\sigma_{x_i} \\sigma_{x_j})] \\tag{21}\\]\nThe propagated uncertainty reduction after observing \\(i\\)-th variable is:\n\\[U_x^{(w)} = \\sum_{j \\neq i} x_j^2 \\sigma_{x_j}^2 + \\sum_{i \\neq j} \\sum_{j} x_i x_j \\rho_{ij} \\sigma_{x_i} \\sigma_{x_j} \\tag{22}\\]\nConsidering the distribution of \\(w\\), we use Monte-Carlo dropout to sample model parameters and use the average uncertainty of \\(U_x^{(w)} (i)\\) to approximately compute \\(U_x(i)\\):\n\\[U_x(i) = \\mathbb{E}_w[U_x^{(w)} (i)] \\tag{23}\\]\nWe can select the unobserved variables based on the maximal uncertainty criterion.\n\\[i^* = \\arg \\max_{i} U_x (i), \\tag{24}\\]\nwhere \\(i^*\\) is the best variable to observe. Figure 2(D) shows the workflow of propagated uncertainty quantification and active sensing methods."}, {"title": "4 EXPERIMENT SETUP", "content": "To demonstrate the effectiveness of the proposed method, we conducted experiments on real-world datasets."}, {"title": "4.1 Datasets", "content": "Datasets. We validate our system on two publicly available datasets (MIMIC-III\u00b2 and AmsterdamUMCdb\u00b3) and one proprietary dataset extracted from OSUWMC4. We first extracted all the sepsis patients with sepsis-3 criteria [24] in the datasets. For each sepsis patient, we select 1 control patient with the same demographics (i.e., age and gender). We extracted 26 vital signs and lab tests from the datasets. A detailed list of clinical variables can be found in supplementary materials. The statistics of the three datasets are displayed in Table 2.\nVariables Used for Sepsis Prediction. Following [33], we use following variables to model sepsis patients' health states: heart rate, Respratory, Temperature, Spo2, SysBP, DiasBP, MeanBP, Glucose, Bicarbonate, WBC, Bands, C-Reactive, BUN, GCS, Urineoutput,"}, {"title": "4.2 Setup", "content": "We mimic the cold-start environment where only vital signs are immediately available, while all the lab tests can be observed after the assignment. Figure 3 displays the setting of the experiments. After the patients arrive at the hospital, we start to predict whether the patients will suffer from sepsis in 4 hours. We run the prediction process hourly until the patients have been diagnosed with sepsis or discharged. When the model's output has a high uncertainty due to the limited observations, the active sensing algorithms can select the missing lab tests to observe. Based on the lab testing turn-around times policy of OSUWMC, most lab results will be available in less than 30~60 min5 (or even sooner for sepsis patients with high priority), so the observation results for the selected lab items can be used in the same hour to update the predicted sepsis risk. Note that when active sensing algorithms select some variables that are not collected at the corresponding time, we use the estimates from other observed variables as the active observation results."}, {"title": "4.3 Methods for Comparison", "content": "We compare the proposed model with following methods:\n\u2022 Random sensing: We randomly select the masked values to observe for random sensing.\n\u2022 Active sensing MI [37]: The method selects the most informative variables based on the mutual information."}, {"title": "3.7.2 Active Sensing.", "content": "The approximate uncertainty of the risk pre-diction outcome is defined as:\n\\[U_x^{(w)} = \\mathbb{E}_{x, w} [(\\sum_j x_i x_j \\rho_{ij} \\sigma_{x_i} \\sigma_{x_j}) + (\\sum_i \\sum_{j \\neq i} x_i x_j \\rho_{ij} \\sigma_{x_i} \\sigma_{x_j})] \\tag{21}\\]\nThe propagated uncertainty reduction after observing \\(i\\)-th variable is:\n\\[U_x^{(w)} = \\sum_{j \\neq i} x_j^2 \\sigma_{x_j}^2 + \\sum_{i \\neq j} \\sum_{j} x_i x_j \\rho_{ij} \\sigma_{x_i} \\sigma_{x_j} \\tag{22}\\]\nConsidering the distribution of \\(w\\), we use Monte-Carlo dropout to sample model parameters and use the average uncertainty of \\(U_x^{(w)} (i)\\) to approximately compute \\(U_x(i)\\):\n\\[U_x(i) = \\mathbb{E}_w[U_x^{(w)} (i)] \\tag{23}\\]\nWe can select the unobserved variables based on the maximal uncertainty criterion.\n\\[i^* = \\arg \\max_{i} U_x (i), \\tag{24}\\]\nwhere \\(i^*\\) is the best variable to observe. Figure 2(D) shows the workflow of propagated uncertainty quantification and active sensing methods."}, {"title": "5 RESULTS", "content": "We now report the performance of SepsisLab in the three datasets. We focus on answering the following research questions by our experimental results:\n\u2022 Q1: How does the model uncertainty affect the sepsis prediction performance?\n\u2022 Q2: How does the active sensing algorithm reduce the propagated uncertainty?\n\u2022 Q3: How does the active sensing algorithm improve the sepsis prediction performance?"}, {"title": "5.1 Q1: How does the model uncertainty affect the sepsis prediction performance?", "content": "The existence of uncertainty makes Al models less reliable and less accurate when applying the models to real-world high-stakes scenarios. In this subsection, we aim to show how the model uncertainty affects sepsis prediction performance by analyzing the relation between uncertainty and prediction performance."}, {"title": "5.1.1 Prediction Performance over Uncertainty Scales.", "content": "We compute the uncertainty of the sepsis onset prediction model's output with Equation 12 and split the patients into 6 sets with different uncertainty scales. Then we calculate the sepsis onset prediction performance on AUROC inside each set. Figure 4 displays the model performance over the different uncertainty scales in the three datasets. We conducted experiments in two settings. In active sensing setting, we compute the AUROC after active sensing algorithms are used. In the observed data setting, we directly run the data in the observed data (including all the recorded vital signs and lab tests) and compute the AUROC. The results in both settings show that when uncertainty is higher, the model performance becomes less"}, {"title": "5.1.2 Uncertainty Scales over Time.", "content": "We quantify the model uncertainty at different times from admissions. Figure 5 displays the average uncertainty scales.\nFigure 5 shows that in the first 15 hours, propagated uncertainty is dominant in sepsis onset risk prediction models. We speculate the reason is that at the beginning most variables have not been observed and the missing values cause the main uncertainty, which is consistent with our clinical experts' experience. With more variables collected, the propagated uncertainty decreases a lot after 15 hours of the admissions.\nBecause the missing variables can cause high uncertainty during the first hours, it is critical to quantify the propagated uncertainty when applying risk prediction models to high missing-rate settings."}, {"title": "5.2 Q2: How does the active sensing algorithm reduce the propagated uncertainty?", "content": "Based on the estimated uncertainty, we propose active sensing algorithms to further reduce the prediction uncertainty by recommending clinicians collect more unobserved variables. We conduct experiments to show whether uncertainty can be significantly reduced with minimal additional variables observed."}, {"title": "5.2.1 Uncertainty with Different Active Sensing Ratio.", "content": "Figure 6 displays the average uncertainties for sepsis prediction results with different active sensing ratios. The results show that with more missing variables observed, the uncertainty on the predicted sepsis risks are significantly reduced. Besides, all the versions of the proposed RAS reduce more uncertainty than the baselines, which demonstrates the effectiveness of the proposed active sensing algorithms on uncertainty reduction."}, {"title": "5.2.2 Uncertainty Quantification Efficiency.", "content": "We also investigate the time cost for uncertainty quantification during the inference phase. Figure 7 displays the inference time cost for uncertainty quantification. The results show that RAS can achieve much less time than the baselines, which makes the SepsisLab system work more efficiently during the active sensing phase."}, {"title": "5.3 Q3: How does the active sensing algorithm improve the sepsis prediction performance?", "content": "The goal of SepsisLab is to accurately predict the sepsis so as to provide reliable decision-making support to clinicians. We conduct experiments to show sepsis prediction performance improvement with the active sensing algorithms."}, {"title": "5.3.1 Sepsis onset Prediction Results.", "content": "Table 3 displays the risk prediction performance with different active sensing ratios (i.e., 2%-8%). With additional variables observed, all the methods can achieve more accurate prediction performance for sepsis onset. Moreover, all the active sensing algorithms outperform the random sensing baseline with the same observation rate, which demonstrates that active sensing can improve downstream tasks' performance. Among the active sensing algorithms, the proposed RAS achieved the best performance with different active sensing ratios, which demonstrate the effectiveness of the proposed model."}, {"title": "5.3.2 Ablation Study.", "content": "We have three versions of the framework. RASN directly uses the gradient to estimate propagated uncertainty. RASL uses a linear regularization term to make the model locally smooth, while RAS uses adversarial training. For RASL and RAS versions, the additional terms change the loss functions. We conduct experiments to show whether the additional terms can improve model training. We train the three versions of models independently and test them on all the observed data (without active sensing). As Table 4 shows, RASL and RAS outperform RASN, which demonstrates local linearity can further improve prediction performance. With adversarial training, RAS can achieve better local linearity than RASL and thus perform the best, which also explains why the RAS outperforms better than Monte-Carlo sampling in Table 3 in the active sensing."}, {"title": "5.3.3 Hyper-parameter Optimization.", "content": "The proposed RAS have four important hyper-parameter: weight \\(\\alpha\\) in Equation 19, step size \\(s_{adv}\\), step \\(n_{adv}\\), learning rate lr in Algorithm 1. We use grid-search to find the best parameter (with active sensing ratio equal to 8%). Table 5 displays the searching space and the optimal values used in the training process."}, {"title": "6 DEPLOYMENT", "content": "Based on the sepsis prediction model and active sensing algorithm, we implement a system SepsisLab. Figure 8 and Figure 9 shows how the system is deploed in the Epic EHR Systems at OSUWMC.\nSepsisLab starts to collect patients' data after the patients arrive hospital and automatically predicts sepsis risks hourly. Figure 8(A) displays a list of patients with different sepsis risk prediction scores, colored from no risk as Green, to medium risk as Yellow, to high risk as Red. When picking a patient's data, Figure 8(B) shows the patient's demographics and the dashboard that includes the patient's vital signs, lab test results, and medical history, which are helpful for clinicians to understand the patient's health states. Figure 8(C) shows the patient's sepsis risk (solid line) and uncertainty range"}, {"title": "7 CONCLUSION", "content": "In this work, we study a real-world problem that how to accurately predict sepsis with limited variables available. Missing values widely exist in clinical data and can cause inaccurate prediction and high uncertainty for the sepsis prediction models. To the best of our knowledge, it is the first work that studies the model uncertainty caused by missing values. We define a new term propagated uncertainty to describe the uncertainty, which is the downstream models' uncertainty propagated from the uncertain input (i.e., imputation results). We further propose uncertainty propagation methods to quantify the propagated uncertainty. Based on the uncertainty quantification, we propose a robust active sensing algorithm to reduce the uncertainty by actively recommending clinicians to observe the most informative variables. The experimental results on real-world datasets show that the introduced propagated uncertainty is dominant at the beginning of patients' admissions to the hospital due to the very limited variables and the proposed active sensing algorithm can significantly reduce the propagated uncertainty and thus improve the sepsis prediction performance. Finally, we design a SepsisLab system for deployment to integrate into clinicians' workflow, which paves the way for human-AI collaboration and early intervention for sepsis management."}, {"title": "8 ACKNOWLEDGMENTS", "content": "This work was funded in part by the National Science Foundation under award number IIS-2145625, by the National Institutes of Health under award number R01GM141279 and R01AI188576, and by The Ohio State University President's Research Excellence Accelerator Grant."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 Uncertainty Propagation for Linear Function", "content": "For a linear function \\(f_w(x) = \\sum_i W_i x_i (1 \\leq i \\leq n)\\), the uncertainty is defined as the variance:\n\\[Var(f_w(x)) = \\mathbb{E}_x[(f_w(x) - \\mathbb{E} f_w(x))^2]\\]\n\\[= \\mathbb{E}_x[(\\sum_i W_i x_i - \\mathbb{E} \\sum_i W_i x_i)^2]\\]\n\\[= \\mathbb{E}_x[\\sum_i (W_i x_i - \\mathbb{E} W_i x_i)]^2\\]\n\\[=\\mathbb{E} \\int [\\sum_i (W_i x_i - \\mathbb{E} W_i x_i)][\\sum_j (W_j x_j - \\mathbb{E} W_j x_j)] dx\\]\n\\[=  \\int \\sum_i \\sum_j \\mathbb{E}(W_i x_i - \\mathbb{E} W_i x_i)(W_j x_j - \\mathbb{E} W_j x_j) dx\\]\n\\[=  \\int \\sum_{i \\ne j} \\sum_j \\mathbb{E}(W_i x_i - \\mathbb{E} W_i x_i)(W_i x_i - \\mathbb{E} W_i x_i) \\]"}, {"title": "A.2 Missing Rates of Clinical Variables", "content": "We display the missing rates of lab test variables in Table 7."}, {"title": "A.3 Model Performance with different backbones", "content": "Our model is applicable to various models, including LSTM, GRU, and fully-connected networks (FC). LSTM has shown superior performance in modeling clinical time series data in multiple tasks, including missing value imputation [32, 34], clinical prediction [16], and patient subtyping [2], so we choose LSTM as the model backbone. We also conducted more experiments with different backbones as shown in Table 6. The experimental results show that the proposed model can significantly improve the prediction performance for all the backbones by recommending the most informative variables for observation."}]}