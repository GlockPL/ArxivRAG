{"title": "HumanVLM: Foundation for Human-Scene Vision-Language Model", "authors": ["Dawei Dai", "Xu Long", "Li Yutang", "Zhang Yuanhui", "Shuyin Xia"], "abstract": "Human-scene vision-language tasks are increasingly prevalent in diverse social applications, yet recent advancements predominantly rely on models specifically tailored to individual tasks. Emerging research indicates that large vision-language models (VLMs) can enhance performance across various downstream vision-language understanding tasks. However, general-domain models often underperform in specialized fields. This study introduces a domain-specific Large Vision-Language Model, Human-Scene Vision-Language Model (HumanVLM), designed to provide a foundation for human-scene Vision-Language tasks. Specifically, (1) we create a large-scale human-scene multimodal image-text dataset (HumanCaption-10M) sourced from the Internet to facilitate domain-specific alignment; (2) develop a captioning approach for human-centered images, capturing human faces, bodies, and backgrounds, and construct a high-quality Human-Scene image-text dataset (HumanCaptionHQ, about 311k pairs) that contain as much detailed information as possible about human; (3) Using HumanCaption-10M and Human-CaptionHQ, we train a HumanVLM. In the experiments, we then evaluate our HumanVLM across varous downstream tasks, where it demonstrates superior overall performance among multimodal models of comparable scale, particularly excelling in human-related tasks and significantly outperforming similar models, including Qwen2VL and ChatGPT-4o (as shown in Figure 1). HumanVLM, alongside the data introduced, will stimulate the research in human-around fields. All codes, data and model checkpoints are available at: https://github.com/ddw2AIGROUP2CQUPT/HumanVLM; https://huggingface.co/OpenFace-CQUPT", "sections": [{"title": "1. Introduction", "content": "Human-scene vision and language tasks are now integral components in a variety of applications, including social media analysis[1, 2], customer service[3, 4, 5], safety monitoring[2], education[6], and entertainment[7]. These tasks are essential for developing systems capable of understanding and interacting with humans in more natural and effective ways. Despite significant progress, recent advancements[8, 9] in the field have been largely propelled by models tailored to specific tasks. While this specialization can lead to notable performance improvements, it also presents substantial drawbacks. For example, building and optimizing these task-specific models require significant time, computational resources, and specialized expertise. Additionally, these task-specific models are often highly proficient only within their designated applications, lacking versatility and adaptability, which renders them less efficient when applied to diverse tasks.\nTo address these limitations, researchers are increasingly exploring generalized approaches, such as multi-task learning[10] and universal representation learning[11, 12], which aim to create models capable of efficiently and robustly managing a wide array of tasks. Recent studies[13, 14, 15] have shown"}, {"title": "2. Related Work", "content": "Human-scene image processing encompasses a range of tasks, from basic image processing to advanced artificial intelligence applications. Tasks focused on facial identity, location, expression[1], and feature recognition[22] are commonly applied in security monitoring, identity verification[23], and human-computer interaction[6], as well as in social media and entertainment. Body posture[24] and gesture recognition[7] are utilized in sports analysis, gaming, entertainment, and human-computer interaction. Behavior recognition[25], which involves analyzing human activities and actions within images, is widely used in security monitoring, smart retail, and health monitoring. Human and clothing segmentation finds[5, 26] applications in virtual try-on technology, entertainment, and special effects. Image beautification and enhancement are keys in photo editing, social media, advertising, and commercial photography.\nDeep learning models play a pivotal role in these human-centric tasks. For instance, in facial recognition, CNN-based models such as VGGFace[27], Facenet[28], and DeepFace[29] are widely adopted. For pose estimation, models like OpenPose[30] and PoseNet[31] are commonly employed. U-Net[32] and the YOLO[33] series are extensively used for human detection and segmentation, while GAN[34], SRCNN[35], and ESRGAN[36] models are highly effective in image beautification and enhancement. Most of these models rely on CNN-based models. Recently, deep learning models based on Vision Transformers (ViT[37]) have also gained popularity in human-scene tasks. However, these models are typically task-specific, with each designed to perform a distinct function. Due to the complex and evolving nature of application scenarios, this proliferation of specialized models lacks generalizability, resulting in significant resource inefficiencies."}, {"title": "2.1. Human-Centric Tasks", "content": "Human-scene image processing encompasses a range of tasks, from basic image processing to advanced artificial intelligence applications. Tasks focused on facial identity, location, expression[1], and feature recognition[22] are commonly applied in security monitoring, identity verification[23], and human-computer interaction[6], as well as in social media and entertainment. Body posture[24] and gesture recognition[7] are utilized in sports analysis, gaming, entertainment, and human-computer interaction. Behavior recognition[25], which involves analyzing human activities and actions within images, is widely used in security monitoring, smart retail, and health monitoring. Human and clothing segmentation finds[5, 26] applications in virtual try-on technology, entertainment, and special effects. Image beautification and enhancement are keys in photo editing, social media, advertising, and commercial photography.\nDeep learning models play a pivotal role in these human-centric tasks. For instance, in facial recognition, CNN-based models such as VGGFace[27], Facenet [28], and DeepFace[29] are widely adopted. For pose estimation, models like OpenPose[30] and PoseNet[31] are commonly employed. U-Net[32] and the YOLO[33] series are extensively used for human detection and segmentation, while GAN[34], SRCNN[35], and ESRGAN[36] models are highly effective in image beautification and enhancement. Most of these models rely on CNN-based models. Recently, deep learning models based on Vision Transformers (ViT[37]) have also gained popularity in human-scene tasks. However, these models are typically task-specific, with each designed to perform a distinct function. Due to the complex and evolving nature of application scenarios, this proliferation of specialized models lacks generalizability, resulting in significant resource inefficiencies."}, {"title": "2.2. Multimodal Image-Text Dataset", "content": "Single-modal datasets comprising images and labels have played a pivotal role in many areas of research, such as the CIFAR-10/100[38] and ImageNet[39] datasets. These datasets contain a large number of images collected from the web. In contrast, multimodal image-text datasets consist of images paired with corresponding descriptive text. With recent advancements in large-scale VLMs, high-quality multimodal image-text datasets are increasingly essential for a range of applications. Below is a summary of some notable multimodal image-text datasets.\nFlickr30k[40] dataset includes approximately 31,000 facial images collected from Flickr, each annotated with five reference sentences created by human annotators. However, these images often feature complex backgrounds, and the associated text does not naturally capture facial features. While MM-CelebA[41] and CelebA-Dialog[42] contain multiple pairs of human-labeled face descriptions, their sample sizes are insufficient for training large models. The LAION-Face[43] dataset, a subset of LAION-400M[44], is currently the largest human-related image-text dataset, containing approximately 50 million image-text pairs. However, the text in this dataset is directly extracted from the internet and often exhibits a weak correlation with the images.\nDue to the lack of large-scale and high-quality human-related image-text datasets, researchers often first train a model (such as ResNet[45], VIT, and CLIP[46]) on the general large-scale datasets such as LAION-5B[47], CC[48], ImageNet22K[49], and COCO[50] as pre-trained modules. Subsequently, they fine-tune the pre-trained models on a smaller-scale dataset for specific human-related tasks. However, these pre-trained models often demonstrate limited generalization capabilities when applied to human-related tasks. Overall, various limitations emphasize the urgent need for a large-scale, high-quality multimodal human-related dataset that provides natural language descriptions of image content to support more complex human-related tasks."}, {"title": "2.3. Various Vision-Language Applications", "content": "Liu at el. introduced an end-to-end trained large vision-language assistant (LLaVA[51]) on instruction-following data for general purpose visual and language understanding, which gained widespread attention upon release. Subsequent research has further enhanced LLaVA's performance. For instance, LLaVA-OneVision[52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual\nscenarios. LLaVA-Interactive[53] serves as a comprehensive demonstration platform, incorporating features such as image chatting, segmentation, and generation and editing capabilities, significantly expanding LLaVA's original functionalities. MoE-LLaVA[54], a sparse LVLM architecture based on Mixture of Experts (MoE), was developed to tackle performance degradation in multimodal sparse learning. MG-LLaVA[55] enhanced the model's visual processing capabilities by introducing multi-granularity visual streams, allowing it to handle features at various resolutions and object centers.\nLLaVA has set new standards for efficiency and effectiveness in multimodal learning and has quickly been adapted across various domains. For example, LLaVA-based models, including LLaVA-Med[18], PathChat[19], QUILT-LLaVA[56], PA-LLaVA[57], have been designed for medical image understanding, where they outperform traditional methods. Zheng et al. [58] developed the first large-scale open-source dataset, MMTab, to address the multimodal table understanding problem and trained a multifunctional table-format LLM called Table-LLaVA. In the power sector, Wang et al.[21] proposed Power-LLaVA, a large vision-language assistant designed for reliable inspection of power transmission lines, showcasing strong capabilities in this field. In the food domain, Fnu Mohbat et al. [20] introduced LLaVA-Chef, trained on a carefully selected recipe dataset, enabling it to recognize ingredients and generate detailed recipes. In this study, we aim to construct a unified multimodal Vision-Language Model for human-scene tasks."}, {"title": "3. Constructing Human-Scene Image-Text Data", "content": "To construct a large-scale image-text dataset of human scenes, we use LAION-Face[43] as the raw data and primarily construct two image-text pair datasets, HumanCaption-10M and HumanCaptionHQ, on which we train HumanVLM for human-scene image understanding. The approach involves collecting as many images with people as possible and designing a suitable algorithm to generate detailed text descriptions[62]. Most VLMs can generate only an overall description of an image. In this study, we first generate the captions for facial features and the broader image separately and then integrate them to produce a comprehensive description of human-scene images."}, {"title": "3.1. Overview", "content": "To construct a large-scale image-text dataset of human scenes, we use LAION-Face[43] as the raw data and primarily construct two image-text pair datasets, HumanCaption-10M and HumanCaptionHQ, on which we train HumanVLM for human-scene image understanding. The approach involves collecting as many images with people as possible and designing a suitable algorithm to generate detailed text descriptions[62]. Most VLMs can generate only an overall description of an image. In this study, we first generate the captions for facial features and the broader image separately and then integrate them to produce a comprehensive description of human-scene images."}, {"title": "3.2. Human-Scene Image Collection", "content": "Specifically, we accessed the LAION-Face[43] dataset, which contains over 50M image-text pairs obtained through web crawling, as our source of raw image data. LAION-Face is of a considerable scale, and its image distribution closely resembles real-world. Moreover, using this a dataset offers significant cost savings compared to manual collection. Since, there were limitations stemming from link expiration and network issues, we could only access about 75% images of the LAION-Face.\nDespite its name, LAION-Face[43] is not strictly a facial image dataset; rather, it is an human-scene image-text dataset that includes human with low text-image correlation. Thus, we needed to select the high-quality human-scene images from LAION-Face and re-label them. First, we employed RetinaFace model[63] to filter images with faces. To ensure high-quality human-scene images, we retained only images with facial regions at resolutions exceeding 128 \u00d7 128 pixels and confidence scores above 0.98."}, {"title": "Selecting Human-Scene Images.", "content": "Despite its name, LAION-Face[43] is not strictly a facial image dataset; rather, it is an human-scene image-text dataset that includes human with low text-image correlation. Thus, we needed to select the high-quality human-scene images from LAION-Face and re-label them. First, we employed RetinaFace model[63] to filter images with faces. To ensure high-quality human-scene images, we retained only images with facial regions at resolutions exceeding 128 \u00d7 128 pixels and confidence scores above 0.98."}, {"title": "3.3. Facial Attributes Annotation", "content": "Facial attributes are essential for accurately describing the appearance of a person. We utilized 40 appearance attributes for facial feature annotation[64], which is widely used to describe a face. Considering the efficiency and accuracy, we employed an open-source algorithm[65] to predict facial attributes for each image. To enhance annotation reliability, we retained labels predicted with a probability greater than 0.85. Additionally, to generate more accurate natural language descriptions, we retained samples with more than five valid predicted labels, ultimately refining the dataset to 10 million human-scene images."}, {"title": "3.4. Caption Generation", "content": "Since the image-text pairs in the LAION-Face dataset were obtained through subtitle crawling, the accompanying text shows a weak correlation with the actual image content. Our goal is to generate captions that accurately describe image content, particularly focusing on people within the images. Traditional automatic text generation methods, limited by grammatical templates, often lack the diversity, complexity, and naturalness required for descriptive sentences. However, recent advancements in LLMs [66, 67, 68] have enabled the generation of text with high diversity and naturalness.\nFor human-scene images, most VLMs in the general domain may not generate captions that emphasize facial features. In this study, we first generate two independent captions (facial region and global region) for each human-scene image, and then employed the method of grammar concatenation to combine the two independent captions, generating the final captions."}, {"title": "Facial Caption:", "content": "To ensure the production of high-quality descriptive text using LLMs, the initial raw text generated via grammatical templates is critical. Here, we employ the probabilistic context-free grammar (PCFG[62]) algorithm to create raw text as multiple short sentences, each structured around different attributes. The performance of the LLM itself may impact the quality of the generated captions. After researching open-source LLMs based on their parameter configurations and average scores in English language proficiency, we selected the Qwen-7B-Chat model[66] for optimal results."}, {"title": "Global Caption:", "content": "Considering the efficiency, we directly employed Qwen-VL[69] to generate the large-scale caption for whole images, thereby constructing over 10M human-scene image-text pairs (HumanCaption-10M). Considering the capability of vision understanding, detailed descriptions of entire images using GPT4V[70] are valuable. Balancing efficiency and value, we also employed GPT4V to generate the high-quality caption for 311K human-scene image-text pairs selected from HumanCaption-10M."}, {"title": "3.5. Post-Processing", "content": "The construction of HumanCaption-10M was fully automated, due to the inherent limitations of the model, it leds to some biases or erroneous outputs (e.g., blank responses). Consequently, we implemented a automatic approach for automatic cleaning."}, {"title": "Word Frequency Statistics:", "content": "Through word frequency statistics, we remove the image-text pairs with particularly short text annotations, which were usually due to blank or incomplete model outputs."}, {"title": "Random Sampling Inspection:", "content": "We conducted multiple rounds of random sampling inspection on the HumanCaption-10M dataset to identify and remove refusal responses. Such responses typically result from the multimodal model's safety mechanisms, which may reject generating descriptions if potentially sensitive content is detected."}, {"title": "4. Statistical Analysis for HumanCaption-10M/HQ", "content": "We employed two general no-reference image quality assessment methods, BRISQUE[71] and CLIPIQA[72], to evaluate our HumanCaption-10M and HumanCaptionHQ. BRISQUE method evaluates image quality by calculating the local normalized brightness coefficient of the pixels, where lower scores indicate better image quality. CLIPIQA method calculates the cosine similarity between the given image and predefined prompts, with higher scores indicating better image quality. As shown in Figure 4, we conducted a comparison across some popular image-text datasets: CelebA-Dialog[42], MM-CelebA[41], CelebV-Text[61] (randomly selecting 10 frames from each video to evaluate their quality), FaceCaption-15M[73] and LAION-Face[43]. Based on the results , it is evident that the image quality score distribution of our HumanCaption-10M/HQ datasets are comparable to high-quality small-scale datasets, though it falls slightly behind MM-CelebA according to BRISQUE and CLIPIQA evaluations."}, {"title": "4.1. Image Quality Comparisons", "content": "We employed two general no-reference image quality assessment methods, BRISQUE[71] and CLIPIQA[72], to evaluate our HumanCaption-10M and HumanCaptionHQ. BRISQUE method evaluates image quality by calculating the local normalized brightness coefficient of the pixels, where lower scores indicate better image quality. CLIPIQA method calculates the cosine similarity between the given image and predefined prompts, with higher scores indicating better image quality. As shown in Figure 4, we conducted a comparison across some popular image-text datasets: CelebA-Dialog[42], MM-CelebA[41], CelebV-Text[61] (randomly selecting 10 frames from each video to evaluate their quality), FaceCaption-15M[73] and LAION-Face[43]. Based on the results , it is evident that the image quality score distribution of our HumanCaption-10M/HQ datasets are comparable to high-quality small-scale datasets, though it falls slightly behind MM-CelebA according to BRISQUE and CLIPIQA evaluations."}, {"title": "4.2. Text Comparison", "content": "Compared to the LAION-Face dataset, our primary contribution lies in re-generating detailed descriptions for the images. As shown in Figure 5, the text within HumanCaptionHQ is more extensive and detailed than in HumanCaption-10M with both exhibiting significantly higher quality than other datasets. Specifically, the average text lengths for"}, {"title": "4.3. Manual Evaluation", "content": "We utilized both GPT4V and manual evaluation to assess the quality of our HumanCaption-HQ dataset. The specific steps were as follows: (1) We randomly selected 100 human-scene image-text pairs from the COCO dataset and identified the corresponding image-text pairs in ShareGPT4V[75]; (2) Using both GPT4V and our text generation methods, we generated captions for these 100 images; (3) We invited 10 volunteers to rate the descriptions with focusing on human (win, tie and lose). Each volunteer was tasked with choosing the best description for each image. Additionally, we also compared the generated results with Qwen2-VL. As shown in Figure 6, average scores of manual rating demonstrate that our text can better describe the detailed information of the people in the image."}, {"title": "5. Training Human Vision-Language Model (HumanVLM)", "content": "As illustrated in Figure 7, our HumanVLM comprises a vision encoder to extract the features for human-scene images, a connector module that maps the image tokens to a specified number and dimension, and a LLM to output the responses. For the HumanVLM, we first obtain the initial representation of the input image using the vision encoder of SigLIP [76] model. This visual representation is then processed through a learnable connector module, combined with tokenized textual queries and input into an LLM to generate the desired response."}, {"title": "Model Architecture.", "content": "As illustrated in Figure 7, our HumanVLM comprises a vision encoder to extract the features for human-scene images, a connector module that maps the image tokens to a specified number and dimension, and a LLM to output the responses. For the HumanVLM, we first obtain the initial representation of the input image using the vision encoder of SigLIP [76] model. This visual representation is then processed through a learnable connector module, combined with tokenized textual queries and input into an LLM to generate the desired response."}, {"title": "5.1. Two-stage Learning for HumanVLM", "content": "This training stage aligns human-scene images with their corresponding text for the LLM. Specifically, HumanVLM is trained to generate comprehensive descriptions of images, establishing a foundation for the subsequent instruction-learning stage. During the training, we freeze the visual encoder and update only the connector, with employing the LM[77] loss to optimize the connector in this phase. The unidirectional Language Modeling (LM) trains the model to directly maximize the likelihood of the sequence x under the forward autoregressive factorization."}, {"title": "Domain-specific Alignment for LLM.", "content": "This training stage aligns human-scene images with their corresponding text for the LLM. Specifically, HumanVLM is trained to generate comprehensive descriptions of images, establishing a foundation for the subsequent instruction-learning stage. During the training, we freeze the visual encoder and update only the connector, with employing the LM[77] loss to optimize the connector in this phase. The unidirectional Language Modeling (LM) trains the model to directly maximize the likelihood of the sequence x under the forward autoregressive factorization."}, {"title": "Instruction-Learning.", "content": "This stage enhances the model's ability to respond accurately to various types of instructions. As shown in Figure 8, we prepare a high-quality multimodal instruction-following data, combining general domain and human-scene image-text pairs, including image-caption data, VQA data, visual grounding, and facial attribute annotations."}, {"title": "6. Experiments", "content": "We trained of HumanVLM using the Xtuner\u00b9 toolkit on 16 \u00d7 NVIDIA A100 GPUs. Our training process is divided into two stages: alignment phase and instruction fine-tuning phase. (1) For the first stage: we set the gradient accumulation steps to 4, and the batch size was set to 16 \u00d7 8 \u00d7 4; Learning rate was linearly increased from zero to 1e-3 and then gradually decayed to 0 using the cosine annealing strategy. This phase of training lasted for 1 epoch. (2) For the sceond stage, the batch size was 16 \u00d7 2 \u00d7 8; Learning rate of the connector module was linearly increased 5e-5 and then cosine decayed to 1e-6; Meanwhile, the learning rate of the LLM's LoRA gradually increased to 2e-4 and finally also cosine decayed to 1e-6; This training was also conducted for 1 epoch. AdamW optimizer and mixed precision are employed to improve computational efficiency and save memory."}, {"title": "6.1. Implementation details", "content": "We trained of HumanVLM using the Xtuner\u00b9 toolkit on 16 \u00d7 NVIDIA A100 GPUs. Our training process is divided into two stages: alignment phase and instruction fine-tuning phase. (1) For the first stage: we set the gradient accumulation steps to 4, and the batch size was set to 16 \u00d7 8 \u00d7 4; Learning rate was linearly increased from zero to 1e-3 and then gradually decayed to 0 using the cosine annealing strategy. This phase of training lasted for 1 epoch. (2) For the sceond stage, the batch size was 16 \u00d7 2 \u00d7 8; Learning rate of the connector module was linearly increased 5e-5 and then cosine decayed to 1e-6; Meanwhile, the learning rate of the LLM's LoRA gradually increased to 2e-4 and finally also cosine decayed to 1e-6; This training was also conducted for 1 epoch. AdamW optimizer and mixed precision are employed to improve computational efficiency and save memory."}, {"title": "6.2. Comparisons on General Domain", "content": "Although we used a large amount of human-scene image-text data in training our HumanVLM, human-scene image understanding cannot be fully separated from context; thus, we also incorporated a certain amount of general domain data. This combination endows the HumanVLM with a degree of general understanding capability. As shown in Table 4, compared to general domain VLMs of similar scale (LLaVA-based), our HumanVLM also exhibits competitive performance in general domain image understanding. This improvement is primarily due to the following factors: (1) the large-scale HumanCaption-10M dataset includes a variety of general scenes, and we supplemented it with general domain data in the second training stage; (2) the advanced SigLIP[76] encoder strengthens the visual feature representation."}, {"title": "6.3. Comparisons on Human-Scene Tasks", "content": "To evaluate the ability of VLMs to interpret detailed content in human-scene images, we constructed 3,950 image-caption pairs from the Human-CaptionHQ as test data. We employed GPT4o to assess the quality of the captions generated by various models, with a focus on the completeness of entity nouns and semantic similarity. Higher scores indicate better overall performance. As shown in Table 5, our proposed HumanVLM generates higher-quality image descriptions than other models, even surpassing GPT4o. Illustrations in Figure 9 demonstrate that HumanVLM can provide more detailed image descriptions, highlighting its capacity to deeply understand image content\u2014a foundational element for other tasks.\nWe designed the prompt for GPT4-o to evaluate caption as follows:"}, {"title": "6.3.1. Caption Generation", "content": "To evaluate the ability of VLMs to interpret detailed content in human-scene images, we constructed 3,950 image-caption pairs from the Human-CaptionHQ as test data. We employed GPT4o to assess the quality of the captions generated by various models, with a focus on the completeness of entity nouns and semantic similarity. Higher scores indicate better overall performance. As shown in Table 5, our proposed HumanVLM generates higher-quality image descriptions than other models, even surpassing GPT4o. Illustrations in Figure 9 demonstrate that HumanVLM can provide more detailed image descriptions, highlighting its capacity to deeply understand image content\u2014a foundational element for other tasks.\nWe designed the prompt for GPT4-o to evaluate caption as follows:"}, {"title": "6.3.2. VQA Test", "content": "In addition to caption generation, we also evaluated the VQA capability. VQA tasks are characterized by more open-ended prompts. For this evaluation, we divided VQA task into open-set and closed-set categories. During instruction learning, we used only general-domain VQA datasets. For this test, we selected 5,000 human-scene images and created 3\u20135 correct question-answer (QA) pairs per image using GPT4, totaling 18,312 QA pairs. For closed-set VQA, the prompt consisted of a question and four answer options, with accuracy used to measure performance. For open-set VQA, where only the question is provided, performance was measured based on the similarity score between the generated answer and the correct text."}, {"title": "6.3.3. Face Attributes Recognition & Visual Grounding", "content": "In contrast to the open questions in VQA tasks, the queries in these tasks can be regarded as instructions, enabling direct mapping to target objects in the visual content. The Face Attributes Recognition task involves predicting various attributes of a given facial image, such as gender and hairstyle, making it a multilabel classification task. This capability is widely applicable in fields like recommendation systems and security monitoring. To assess the effectiveness of HumanVLM on this task, we conducted evaluations using our self-constructed test data FaceC, and public datasets CelebA [64] and LFWA[88]. As shown in Table 7, HumanVLM significantly outperforms all listed models on both supervised tasks (FaceCaption and CelebA) and the zero-shot task (LFWA). For the Visual Grounding task, we conducted evaluations on public human-scene data selected from the Ref-coco_testA and Refcoco_testA datasets. HumanVLM demonstrates superior performance compared to all other models."}, {"title": "6.4. Ablation experiment", "content": "To validate the effectiveness of our HumanCaption-10M and HumanCaptionHQ, we trained the following models and validated their performance on image caption generation and VQA tasks within Human-Scene contexts:\nIn the first stage, we used HumanCaption-10M for domain alignment. During the second-stage instruction learning, we replaced HumanCaptionHQ with an equivalent number of samples that randomly selected from the HumanCaption-10M, keeping other data unchanged.\nThe former uses the same second-stage data as HumanVLM for instruction fine-tuning of LLaVA-llama3, while the latter replaces HumanCaptionHQ with an equivalent number of samples from HumanCaption-10M during the second stage, with all other data remaining unchanged. Instruction fine-tuning is performed on the LLaVA-llama3 model.\nFrom the results in Figure 11 and Table 8, the main observations are as follows:"}, {"title": "HumanVLM-NoHQ:", "content": "In the first stage, we used HumanCaption-10M for domain alignment. During the second-stage instruction learning, we replaced HumanCaptionHQ with an equivalent number of samples that randomly selected from the HumanCaption-10M, keeping other data unchanged."}, {"title": "7. Conclusions", "content": "Human-scene image understanding is widely applicable across various social contexts, with large VLMs increasingly demonstrating enhanced performance in a range of downstream tasks. However, there remains a shortage of large-scale high-qulity image-text datasets specifically related to human-scene. Consequently, common approaches often involve either retraining specialized models or fine-tuning pre-trained general domain models. The latter approach has limitations in cross-domain generalization capabilities, underscoring the need for constructing specialized image-text datasets and domain-specific pre-trained models to advance this field.\nIn this work, we constructed a series of human-scene multimodal datasets and trained a domain-specific large language-vision model, HumanVLM, aimed at establishing a unified multimodal language-vision model for human-related tasks. Experimental results indicate that our HumanVLM achieves the best overall performance among multimodal models of similar scale in a range of human-related tasks. We believe that HumanVLM, alongside the HumanCaption-10M/HQ datasets introduced, will stimulate further research in human-around fields."}]}