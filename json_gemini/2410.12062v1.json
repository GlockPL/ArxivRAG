{"title": "MFC-EQ: Mean-Field Control with Envelope Q-learning for Moving Decentralized Agents in Formation", "authors": ["Qiushi Lin", "Hang Ma"], "abstract": "We study a decentralized version of Moving Agents in Formation (MAIF), a variant of Multi-Agent Path Finding aiming to plan collision-free paths for multiple agents with the dual objectives of reaching their goals quickly while maintaining a desired formation. The agents must balance these objectives under conditions of partial observation and limited communication. The formation maintenance depends on the joint state of all agents, whose dimensionality increases exponentially with the number of agents, rendering the learning process intractable. Additionally, learning a single policy that can accommodate different linear preferences for these two objectives presents a significant challenge. In this paper, we propose Mean-Field Control with Envelop Q-learning (MFC-EQ), a scalable and adaptable learning framework for this bi-objective multi-agent problem. We approximate the dynamics of all agents using mean-field theory while learning a universal preference-agnostic policy through envelop Q-learning. Our empirical evaluation of MFC-EQ across numerous instances shows that it outperforms state-of-the-art centralized MAIF baselines. Furthermore, MFC-EQ effectively handles more complex scenarios where the desired formation changes dynamically-a challenge that existing MAiF planners cannot address.", "sections": [{"title": "I. INTRODUCTION", "content": "Multi-Agent Path Finding (MAPF) [1], [2] is a well-studied problem in various multi-agent systems, aiming to find collision-free paths for agents in a shared environment. Its applications include warehouse management [3], airport surface operations [4], video games [5], and other multi-agent systems [6]. Many of these applications require agents to adhere closely to a designated formation to accomplish collaborative tasks or ensure an efficient communication network. For example, in warehouse logistics, multiple robots or vehicles must collaborate to transport large objects, where maintaining a specific formation is critical for optimizing transport efficiency or ensuring reliable communication. Similarly, in video gaming or military strategy simulations, game characters or army personnel must move in formations to safeguard vulnerable members.\nTo tackle this challenge, [7] formalized the bi-objective problem of Moving Agents in Formation (MAiF), which combines these two tasks, and proposed a centralized MAiF planner based on a leader-follower scheme and a search-based MAPF algorithm. However, existing MAiF planners are designed for centralized settings and are not applicable in practical scenarios where agents lack full observation of the environment. Additionally, centralized MAiF planners face significant computational challenges as the number of agents increases, making them unsuitable for real-time planning. Moreover, the only scalable MAiF planner, SWARM-MAPF [7], lacks the flexibility to adjust to specific preferences between the two objectives, as it balances them only by setting a makespan allowance between two sets of heuristically determined waypoints, without guaranteeing optimization toward a targeted preference. We propose a novel approach for learning a general MAiF solver suitable for decentralized settings that can directly adapt to various preferences between the two objectives.\nIn the MAPF literature, reinforcement learning and imitation learning have been explored to solve MAPF in decentralized settings [8], [9], [10]. However, most learning-based MAPF solvers learn one homogeneous policy for any set of agents that treats nearby agents as part of the environment. This learning scheme does not seamlessly translate to decentralized MAiF. Unlike MAPF where the joint action cost can be directly decomposed into action costs of individual agents, formation maintenance in MAiF depends on the joint state of all agents at any given time. Each agent must not only avoid colliding with others but also coordinate with them to maintain proximity to the desired formation. The dimensionality of the joint state space grows exponentially with the number of agents, which hinders scalability. Additionally, trading off the two objectives under partial observation and limited communication further complicates this task.\nIn this paper, we formalize decentralized MAiF as a bi-objective multi-agent reinforcement learning problem. The major contributions of our work are as follows: We design a practical learning formalization for MAiF, including specifications for observations, actions, rewards, and inter-agent communication. To address the aforementioned challenges of MAIF, we propose a novel approach called MEAN FIELD CONTROL WITH ENVELOP Q-LEARNING (MFC-EQ), a multi-agent reinforcement learning technique that optimizes toward any linear combination of two objectives for any number of agents, ensuring a stable and efficient learning process. MFC-EQ leverages mean-field control to approximate the collective dynamics of the agents, treating the interaction of each agent within the formation as influenced by the collective effect of others. This design choice facilitates seamless scalability to large-scale instances. Furthermore, MFC-EQ extends envelope Q-learning to a multi-agent setting, enabling the learning of a universal preference-agnostic model adaptable to any linear combination of the two objectives. To evaluate our method empirically, we extensively test MFC-EQ across various MAiF instances."}, {"title": "II. PROBLEM DEFINITION", "content": "In this section, we first describe the standard MAiF for-mulation using terminology that facilitates the presentation of our learning approach. We then discuss how MAiF can be extended to a partially observable environment, which is a more practical problem setting. Finally, we define relevant concepts and outline the bi-objective optimization problem.\nStandard Formulation of MAiF\nIn the standard formulation, an MAiF instance is defined on an undirected graph G = (V, E) in a d-dimensional Cartesian system. Each location in V can be identified by its coordinates v = (v1,...,vd) \u2208 Rd. In this paper, superscripts represent agents' indices and boldface denotes multi-dimensional vectors. We define a set of M agents I = {ai|i \u2208 [M]}, where [M] = {1, ..., M}. Each agent has a unique start location si \u2208 V and goal location gi \u2208 V. Time is discretized, and at each time step, an agent can either wait at its current location or move from v to v', provided (v, v') \u2208 E. We consider two types of collision between agents ai and aj at time step t: A vertex collision (ai, aj, v,t) occurs when they occupy the same location v, and an edge collision (ai, aj, u, v,t) occurs when ai moves from u to v while aj moves in the opposite direction.\nThe MAiF problem aims to find a set of M collision-free paths \u03a0 = {\u03a0i|i \u2208 [M]} as a solution, where \u03a0i = (pi,..., pri) represents agent i's trajectory. Each solution is evaluated based on two objective functions, makespan and formation deviation. The makespan is defined as T = max1\u2264i\u2264M Ti, representing the longest path length among all agents. The formation at time t can be represented as an M-tuple, l(t) = (p1(t),...,pM(t)). The desired formation corresponds to a combination of all agents' goal locations, lg = (g1,...,gM). Following the definition in [7], the formation deviation between any two formations l = (u1, ..., uM) and l' = (v1, ..., vM) indicates the least effort required to transform l into l', defined as:\n$F (l, l') := \\min_{\\Delta} \\sum_{i=1}^{M} ||u^{i} - (v^{i} + \\Delta)||_{1}$ \n$=\\sum_{i=1}^{M} \\sum_{j=1}^{d} |(u_{j}^{i} - v_{j}^{i}) - \\Delta_{j}|$, (1)\n$:=\\sum_{i=1}^{M}F^{i}(l,l')$\nwhere j indexes the dimension for each vector and \u0394j = median(({ui \u2013 vi}[M]) is the median of the differences between the coordinates in the j-th dimension across all agents. The term Fi(l,l') denotes the component related"}, {"title": "B. Partially Observable Environment", "content": "In this paper, we consider a more practical problem setting where, instead of having full knowledge of the environment, each agent can only observe part of its surroundings. We first introduce the standard decentralized partially observ-able Markov Decision Process (Dec-POMDP) [11]. A Dec-POMDP is represented by a 7-tuple (S, A, Ps, O, Po, R, \u03b3), where S is the global state space. A = \u03a0Mi=1 Ai and O = \u03a0Mi=1 Si, where Ai and Si are agent i's action and observation space. Ps : S \u00d7 A \u2192 S describes the state-transition function, and Po : S \u00d7 A \u2192 O is the observation-transition function. R is the reward function with the discount factor \u03b3. We adopt the standard Dec-POMDP framework to model our decentralized MAiF problem, assuming both the observation-transition and state-transition functions are deterministic. We also assume each agent can take an action based solely on its local observation and limited communi-cation with others. Following existing learning-based MAPF literature [8], [10], we formalize this problem on a 2D 4-neighbor grid, though our method can be easily generalized to other settings. Partial observability restricts each agent's perception to a L \u00d7 L square area centered on the agent, defined as its FOV.\nBi-Objective Optimization\nWe now define the goal of this bi-objective optimization problem. Each MAiF solution is evaluated as r = (v,w), where v denotes its makespan and w denotes its average formation deviation per agent. We first define dominance:"}, {"title": "III. RELATED WORK", "content": "This section reviews related work on decentralized MAPF and MAiF, mean-field reinforcement learning, and multi-objective reinforcement learning.\nLearning-Based MAPF and MAiF Solvers\nRecently, reinforcement learning has been introduced to solve decentralized variants of MAPF [8], [10], [12]. These methods are designed to learn a decentralized model that can be generalized across different MAPF instances. Traditional centralized MAPF planners usually require full observation of the environment and must plan paths from scratch for each instance. In contrast, well-trained learning-based mod-els offer the advantage of being applicable to any MAPF instances, regardless of the number of agents or the size of the environment. For decentralized MAiF, [13] explored a similar setup to our work and proposed a hierarchical reinforcement learning scheme to divide the bi-objective task into unrelated sub-tasks. However, the hierarchical learning structure hinders the learned model's ability to adapt to different preference weights between the two objectives. Additionally, the simplistic network design struggles to scale to large numbers of agents. [13] also employs a different definition of formation deviation, and for that reason, we do not compare their results with our method in Section V.\nMean-Field Reinforcement Learning\nInspired by mean-field theory [14] from physics, mean-field reinforcement learning has been proposed in [15] to estimate the dynamics within an entire group of agents by modeling the interaction between each agent and the mean effect of all other agents as a whole. Since the dimensionality of the mean effect is independent of the number of agents, this method avoids the curse of dimensionality, providing a general framework for large-scale multi-agent tasks. This approach has been extended to partially observable stochastic settings [16], where certain distributions are used to sample agents' actions without the necessity of observing them. However, the sampling process is suited only for stochastic games, making it inapplicable to our task.\nMulti-Objective Reinforcement Learning\nMulti-objective reinforcement learning methods can be categorized into three major types. Single-policy meth-ods [17], [18] convert a multi-objective problem into a single-objective optimization using linear or non-linear func-tions, but these methods cannot handle unknown preferences."}, {"title": "IV. MFC-EQ", "content": "This section presents the design of our learning frame-work for decentralized MAiF. We first outline the learning environment, including agents' observation, communication, action, and reward functions. We then elaborate on the bi-objective multi-agent learning process based on mean-field theory and envelope Q-learning.\nEnvironment and Model Design\n1) Observation: As with most research in the MAPF community [1], we study our problem in a 2D 4-neighbor grid environment. To mimic many real-world robotics ap-plications where robots have limited visibility and sensing range, each agent in our grid world can observe only its field of view (FOV), represented by its surrounding L \u00d7 L area. Each agent's observation is represented by multi-channel feature maps. The first few channels indicate obstacles and other neighboring agents' positions. Inspired by some de-centralized MAPF solvers [9], [10], the rest of the channels encompass heuristic information, where each cell in the FOV is assigned a value proportional to the short-path distance from that cell to the agent's goal.\n2) Action: Agents can move to their cardinally adjacent cells at each time step. The action taken by agent i at time step t, denoted by $a_{t}^{i}$ \u2208 R5, is a 5-dimensional one-hot vector where each dimension represents one of the actions: {up, down, left.right, wait}. The first four actions move the agent to a new cell, shifting its observation accordingly. The wait action keeps the agent in its current cell, which is especially crucial for formation control, allowing other agents to catch up and reduce formation deviation.\n3) Multi-Agent Communication: To maintain the desired formation, agents need to not only communicate with nearby agents within their FOVs but also with those outside. We specifically design communication messages to convey crit-ical information under low communication bandwidth.\nIn many real-world robotics applications, each agent can only access pairwise relative positions between itself and other agents. Assume that the current formation at time stept is lp = <p1,...,pM) and the desired formation is lg = (g1,...,gM). The relative position between agent i and jis defined as $p^{i,j}= p^{j}- p^{i}$ (resp., $g^{i,j}$). Agent i receives {$p^{i,j}$}j\u2208[M] in real-time and knows the relative positions in the goal formation, {$g^{i,1}$}j\u2208[M], pre-calculated"}, {"title": "B. Mean-Field and Envelop Optimality", "content": "In the following, we discuss the learning algorithm in detail. Learning multiple policy networks, \u03c0 = [\u03c01, ..., \u03c0\u039c], for this bi-objective multi-agent task is highly challenging. To simplify this, we make some common assumptions.\n1) Mean-Field Approximation: The goal of MAiF is to minimize the makespan and formation deviation. With our specifically designed reward function, the return-the discounted sum of all rewards from the initial to the goal joint state, $\\sum_{t} \\sum_{i} v_{t}r(s_{t}, a_{t})$\u2014reflects the true values of the two objectives. Therefore, the learning goal is to find a set of policies that maximize the general sum of Q-values arg max\u03c01,...,\u03c0\u039c \u03a3j=1 Q\u03c0j(s, a) given the lin-ear preference w. However, the dimensionality of s and a grows exponentially with the number of agents, rendering efficient learning infeasible. To address this, we introduce mean-field reinforcement learning. Similar to [15], we first"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "We proposed MFC-EQ, a general Q-learning framework for solving decentralized MAiF under conditions of partial observation and limited communication. MFC-EQ leverages mean-field approximation to simplify complex multi-agent interactions and employs envelop Q-learning to adapt to various preferences in this bi-objective task. Empirical results demonstrate that MFC-EQ outperforms existing centralized baselines in most scenarios and is particularly versatile"}]}