{"title": "Course-Correction: Safety Alignment Using Synthetic Preferences", "authors": ["Rongwu Xu", "Yishuo Cai", "Zhenhong Zhou", "Renjie Gu", "Haiqin Weng", "Yan Liu", "Tianwei Zhang", "Wei Xu", "Han Qiu"], "abstract": "The risk of harmful content generated by large language models (LLMs) becomes a critical concern. This paper presents a systematic study on assessing and improving LLMs' capability to perform the task of course-correction, i.e., the model can steer away from generating harmful content autonomously. To start with, we introduce the C2-EVAL benchmark for quantitative assessment and analyze 10 popular LLMs, revealing varying proficiency of current safety-tuned LLMs in course-correction. To improve, we propose fine-tuning LLMs with preference learning, emphasizing the preference for timely course-correction. Using an automated pipeline, we create C2-SYN, a synthetic dataset with 750K pairwise preferences, to teach models the concept of timely course-correction through data-driven preference learning. Experiments on 2 LLMs, LLAMA2-CHAT 7B and QWEN2 7B, show that our method effectively enhances course-correction skills without affecting general performance. Additionally, it effectively improves LLMs' safety, particularly in resisting jailbreak attacks.", "sections": [{"title": "1 Introduction", "content": "Recently, large language models (LLMs; OpenAI 2023; Chowdhery et al. 2023), built on transformer architectures, have shown impressive abilities in generating coherent and contextually relevant text. As LLMs evolve, concerns about their potential to generate harmful content are escalating (Bengio et al., 2023). The alignment of these models with human values and safety standards is essential (Hendrycks et al., 2020a). Currently, many model providers offer safety-tuned versions of their base models as their chatbot LLMs, such as"}, {"title": "2 C2-EVAL: Evaluating\nCourse-Correction Ability", "content": "In this section, we show how to evaluate course-correction ability with the help of C2-EVAL. We construct C2-EVAL based on 500 entries of (harmful request HR, harmful response FHR) pairs selected from the PKU-SafeRLHF (Ji et al., 2024) dataset, initially comprising 83.4K preference entries for RLHF. We specifically select safety-related entries with a response exceeding 80 tokens as our FHRs. Refer to Appendix A for details.\nThe overall methodology of C2-EVAL is illustrated in Figure 2. To observe potential course-correction behavior, we prefill the input with an initial harmful response IHR, which is the prefix derived from the corresponding FHR. Besides, the cutoff delimiters for the user prompt and the model response, i.e., <user_end><ai_start>, are placed between HR and IHR. The intention is to mark that IHR is generated by the model itself, not from the user prompt. Given this setup, our evaluation is limited to open-source models. This is because controlling delimiters in many closed LLMs such as GPT-4 (OpenAI, 2023) is restricted.\nThe second phase, as outlined in Figure 2, involves sampling multiple decoding paths based on the input prompt of HR||IHR. We then measure the proportion of paths that exhibit corrective behavior. To achieve accurate course-correction detection, we prompt an LLM. Refer to Appendix B for details.\nWe present the metric Corr(Input, b, m)\n|corrected paths|\nb\nto quantify the course-correction performance on one input, where b is the number of sampled paths, and m represents the max number of new tokens in continuations. For C2-EVAL, we report two metrics, Corr@k and Corrmean:\nCorr@k = \\frac{\\sum_{(HR, FHR) \\in B} Corr(HR||FHR<k, b, m)}{B} \n(1)"}, {"title": "3 Evaluation with C2-EVAL", "content": "In this section, we apply the C2-EVAL benchmark to investigate how well LLMs can course-correct from initial harmful responses.\nModel Selection We evaluate 10 state-of-the-art open-source LLMs, including LLAMA2-CHAT 7B (Touvron et al., 2023), VICUNA V1.5 7B (Chiang et al., 2023), PHI-3 SMALL (Abdin et al., 2024), ZEPHYR-7B-B (Tunstall et al., 2023), LLAMA3-INSTRUCT 8B (Meta, 2024), CHATGLM4 9B (Team et al., 2024) and QWEN2 0.5B/1.5B/7B/72B (Qwen, 2024). These are up-to-date LLMs, meaning that most of them underwent safety-tuning such as SFT (e.g., DPO) and RLHF with the exception of VICUNA V1.5, which only went through SFT on ShareGPT user conversations, with no signs of specific safety-related data. Details of model size and safety-tunning algorithms can be found in Table 1.\nResults We employ the Corr@k and Corrmean metrics, setting b = 20 to sample diverse generation paths and m = 32 to capture timely correction. For ease of observation, we scale the scores to a percentage format of 0 100%. We evaluate the selected LLMs on the full set of C2-EVAL, with the overall results shown in Table 1.\nAs depicted in Figure 3, we plot the variation in Corr@k across various k values. This figure captures how the length of the initial harmful response influences the course-correction capabilities."}, {"title": "4 C2-SYN: A Synthetic Dataset for\nPreference Learning", "content": "In this section, we describe the process of creating C2-SYN, a synthetic pairwise preference dataset containing 750,000 entries designed to teach the value of timely course-correction."}, {"title": "4.1 Principles and Practices", "content": "To align the model with human values, we first establish two fundamental principles. We then create synthesized responses, each inherently ranked based on its adherence to these principles, indicating its relative alignment with human values.\nValue Principles We define the following two value principles:"}, {"title": "5 Preference Learning with C2-SYN", "content": "In this section, we experiment using C2-SYN to impart course-correction capabilities to 2 LLMs: LLAMA2-CHAT 7B and QWEN2 7B."}, {"title": "5.1 Alignment Algorithm", "content": "We select the standard direct preference optimization (DPO) algorithm from (Rafailov et al., 2024):\nLDPO(\u03c0\u03b8; Trek) = \u2212E(x,yw,y1)~D log \\frac{\\pi_{\u03b8}(y_w | x)}{\\pi_{ref}(y | x)}\n-log(e^{\\beta(\\log \\pi_{\u03b8}(y_w | x) - \\log \\pi_{\u03b8}(y_l | x))})\n(3)"}, {"title": "6 Related Work", "content": null}, {"title": "6.1 LLM Safety and Red-Teaming", "content": "Ensuring the safety of LLMs has become a critical area of focus as these models are increasingly deployed in real-world applications (Hendrycks et al., 2020a; Weidinger et al., 2021; Bengio et al., 2023). One prominent method for assessing LLMs safety is red-teaming, which involves attacking models by intentionally probing them with challenging and potentially harmful inputs to uncover weaknesses and failure modes (Ganguli et al., 2022; Zhuo et al., 2023). A critical technique in red-teaming is jailbreak attack, which involves designing various algorithms to intentionally guide the models, often safety-tuned LLMs, out of their safe guardrails (Wei et al., 2024). Many notable jailbreak attacks (Zou et al., 2023; Liu et al., 2023a)"}, {"title": "6.2 Alignment Approaches", "content": "Alignment in the context of AI refers to ensuring AI models' behaviors align with human values and intentions, making AI systems user-friendly and safe (Soares and Fallenstein, 2014; Liu et al., 2023b; Ji et al., 2023). Alignment approaches can be broadly categorized based on whether they require reinforcement learning (RL). In the RL line of work, advanced chat LLMs (Bai et al., 2022a; Ouyang et al., 2022; Touvron et al., 2023) are fine-tuned using Reinforcement Learning from Human Feedback (RLHF), which fits a reward model to human preferences and optimizes the LLM to maximize rewards using algorithms like PPO (Schulman et al., 2017). Besides, RLAIF (Bai et al., 2022b; Lee et al., 2023) uses AI feedback instead of human feedback to train the reward model. Non-RL alignment approaches are divided into those requiring learning (e.g., SFT) and those that do not."}, {"title": "7 Discussion", "content": null}, {"title": "7.1 Bias in the Way of Evaluation", "content": "The evaluation protocol of C2-EVAL has a limitation. We mimic the initial phase of harmful content generation by directly prompting the LLM with a truncated harmful response that follows the user prompt delimiter. However, since the simulated harmful content is derived from the PKU-SafeRLHF dataset rather than being generated by the test model itself, there is an inherent bias. Since FHRS come from LLAMA's generation, bias increases as the tested model's distribution diverges from LLAMA's distribution. Nevertheless, this limitation can be easily remedied. We only need to gather relevant harmful responses for each tested model before the evaluation begins. This can be accomplished by first launching a jailbreak attack on the test model with the requests from C2-EVAL. In the end, to maintain the ready-to-use nature of our C2-EVAL, we have refrained from using this \u201cdynamic\u201d"}, {"title": "7.2 Other Potential Alignment Algorithm", "content": "The synthetic dataset we have constructed adheres to the standards of preference learning datasets, making it versatile for various alignment algorithms that optimize the model on pairwise preferences. In our paper, we opt to employ DPO due to its stability and lower memory footprint during training, as compared to the PPO algorithm used in traditional RLHF approaches. However, this choice does not imply that DPO is the optimal algorithm. Further experimentation is necessary to evaluate its effectiveness fully and explore the potential of alternative algorithms. Furthermore, we acknowledge the possibility that there may be specific optimizations or novel alignment algorithms tailored for the course-correction task. However, our research focuses on addressing the problem through the lens of training data patterns, which may not fully explore these potential advancements."}, {"title": "7.3 Relationship between Course-Correction\nand Superficial Alignment", "content": "The current models' limited ability to perform course-correction suggests a \"superficial\" alignment with safety standards. Recent studies (Lin et al., 2023; Qi et al., 2024) have observed that token distribution dynamics differ across decoding positions, indicating varying levels of safety. These studies indicate that existing alignment approaches often prioritize safe-tuning at earlier token positions in text generation, leading to a diminishing impact of alignment as the decoding sequence progresses. Parallel to our research, Qi et al. (2024) and Yuan et al. (2024) develop methods with similar objectives. They also aim to reduce the potential harm of generation throughout the response sequence, rather than focusing on shallow tokens."}, {"title": "7.4 Relationship between Course-Correction\nand Self-Correction", "content": "Course-correction is inherently different from existing self-correction techniques, which are typically regenerate methods. These methods involve models reviewing and revising their outputs post-generation, often through reprompting (Gou et al., 2023), or by monitoring and controlling each step of the autoregressive decoding process (Li et al., 2023). The limitations of these paradigms include the need for additional tokens in the reprompting process or the time costs associated with controlled decoding. An ideal course-correction strategy should focus on enabling models to self-correct autonomously, eliminating the need for external prompts and streamlining the correction process."}, {"title": "8 Conclusion", "content": "In this research, we systematically investigate the problem of course-correction in the context of harmful content generation within large language models (LLMs). We begin with the development of C2-EVAL, a benchmark to evaluate models' course-correction capabilities. Using C2-EVAL, we evaluate ten prevalent LLMs. We then construct C2-SYN, a synthetic preference dataset of 750K entries, crafted to emphasize the importance of timely course-correction. Using C2-SYN and the direct preference optimization (DPO) algorithm, we conduct safety alignment experiments on two representative LLMs, LLAMA2-CHAT 7B and VICUNA v1.5 7B. Results demonstrate that preference learning with our synthetic data can improve two models' overall safety without harming general performance, demonstrating the effectiveness of our method. On the whole, our research addresses a critical gap in the field of NLP safety, focusing on a niche yet essential aspect."}, {"title": "9 Limitations", "content": "While our study presents both a systematic evaluation and a novel approach to explore and improve the course-correction abilities of LLMs with the introduction of the C2-EVAL benchmark and the C2-SYN synthetic preferences dataset, there are several limitations that warrant discussion:\nDataset Bias C2-SYN is synthesized based on a subset of the PKU-SafeRLHF dataset, which may"}, {"title": "10 Ethical Consideration", "content": "The purpose of our research is to address the ethical considerations inherent in the development and evaluation of LLMs capable of performing course-correction. We have approached this with the creation of the C2-EVAL benchmark and the C2-SYN dataset, ensuring that our methodologies prioritize safety by training models to autonomously halt harmful content generation. Both datasets are curated to exclude any personally identifiable information or offensive material, thereby upholding the privacy and respect of all individuals. Transparency is maintained through our evaluation metric, which provides a clear and quantifiable measure of the models' ethical performance. We are dedicated to refining our ethical practices in response to the ever-evolving landscape of AI ethics, ensuring that our contributions to the field of LLMs are both technically advanced and morally sound.\nComputational Resources We conducted all experiments on a server equipped with 8 NVIDIA A800 80GB GPUs and an Intel Xeon Gold 6430 CPU. Overall speaking, the experiments were not significantly CPU-intensive. All experiments utilized open-source LLMs except for the detection of course-corrective behaviors, in which we employed OpenAI's GPT-40 (OpenAI, 2024). The total cost involving calling GPT-40 is approximately 580$."}, {"title": "A Further Details on Data Processing", "content": "In this section, we detail the data processing steps to obtain (harmful request HR, harmful response FHR) pairs, which will later serve as the basis for constructing C2-EVAL and C2-SYN.\nChoice on the Base Dataset The base dataset should offer both harmful requests and harmful responses and be large enough to generate training data on top of it. These requests make several well-known red-teaming/jailbreak datasets inapplicable, e.g., AdvBench (Zou et al., 2023), HarmBench (Mazeika et al., 2024), Jailbreak-Bench (Chao et al., 2024), inter alia.\nWe employ the PKU-SafeRLHF dataset (Ji et al., 2024), which is particularly suitable for deriving the test data in C2-EVAL and the training data in C2-SYN used in our study. Initially compiled for research in safety alignment, this dataset offers a comprehensive set of training data (75.1k entries) and testing data (8.34k entries). It encompasses a wide range of 19 harm categories, with each category featuring questions and responses generated by models from the Llama model family. The data format of an entry in the PKU-SafeRLHF dataset can be found in Table 6."}, {"title": "B Further Details on C2-EVAL", "content": "In the procedure of sampling multiple decoding paths, we adopt temperature sampling (Ackley et al., 1985) with T = 0.8 and Top-p (Nucleus) sampling (Holtzman et al., 2019) with p = 0.7 as our decoding strategy, which enables diverse generations and is closer to the decoding configuration of modern LLMs.\nIn the setup of detecting course-corrective behaviors, we employ OpenAI's GPT-40 (OpenAI, 2024), the most advanced LLM available at the time of research, using the prompt template detailed in Figure 6. We configure the GPT-40 to greedy decoding and a fixed decoding seed of 42 to ensure reproducible evaluation results.\nTo validate the effectiveness of GPT-40 in this context, we conduct a human evaluation on 100 samples generated by the model. Two authors independently assess the judgments produced by GPT-40. The F1 score achieved by GPT-40 is 0.85, indicating a high level of reliability in detecting course-corrective behaviors. Additionally, the inter-annotator agreement, measured by Cohen's Kappa, is 0.77, which suggests a substantial agreement between the two evaluators. While the evaluation using GPT-40 is not without flaws, it demonstrates a high degree of suitability for the task at hand."}, {"title": "C Futher Details on C2-SYN", "content": null}, {"title": "C.1 Details on Data Synthesis", "content": "The key to generating synthetic responses is to splice the truncated full harmful response, i.e., we call it initial harmful response IHR, with a corrective trigger T, and then employ a well-aligned LLM Maligned to generate continuations. The concatenation of the IHR, the trigger T, and the model-generated continuation (which is assumed to correct the initial harmful content) form one synthetic course-correction response.\nTo make the synthetic response more realistic, the key processing details are as follows:"}, {"title": "C.2 Details on Human Evaluation", "content": "We recruit three annotators to examine the effectiveness of course-correction in continuations generated by the well-aligned LLM (Maligned). As per Section 4, the continuations are generated based on HR||concat(IHR, T\u2208 TriggerSet). This human evaluation process is crucial to assure the quality and usability of the C2-SYN dataset.\nAnnotated Samples We randomly sample 200 synthetic responses, i.e., SYN\u017c in Algorithm 1 from the C2-SYN dataset. Each sample for annotation includes a harmful request HR and an associated synthetic response SYN\u017c, with the trigger T part distinctly highlighted to facilitate the annotation"}, {"title": "D Further Details on Evaluation with\nC2-EVAL", "content": null}, {"title": "D.1 Analysis on Harmful Behaviors and\nSeverity of Harmfulness", "content": "Here we provide a detailed analysis of models' course-correction ability w.r.t. different types of harmful behaviors as well as the severity of harmfulness. As shown in Table 11, we first categorize the original 19 kinds of harmful behavior (as mentioned in (Ji et al., 2024)) into three distinct severity levels: severe, medium, and modest, based on the severity of the harmfulness.\nThe distribution of the behaviors of C2-EVAL across 19 types of harmful behaviors is shown in Figure 7. The distribution of the behaviors across 3 levels of severity can be found in Figure 8.\nFor LLAMA2-CHAT 7B's course-correction performance, we provide a more detailed analysis. In Figure 9, we plot the course-correction performance across 19 types of behaviors. In Figure 10, we depict the model's performance across three levels of severity. From the two figures, we observe that LLAMA2-CHAT 7B demonstrates varying degrees of course-correction effectiveness depending on the type of behavior. We find that the model exhibits significantly different course-correction capabilities across different harmful requests. For instance, it shows notably stronger correction abilities in areas such as white-collar crime and endangering national security, which may be attributed to more effective training in these areas during the safe-tuning process. Additionally, we observe that for severe and medium-level harmful requests, the model's course-correction ability is notably more substantial. This could be due to the heightened sensitivity and focus on these more critical areas during the training phase. Continuing from this observation, it's crucial to recognize the importance of training models to handle a diverse range of harmful requests effectively. As reflected by Figure 8, while the model shows promise in addressing severe and medium-level issues, there is still room for improvement in handling less severe but potentially widespread harmful content."}, {"title": "D.2 LLMs' Tendency to Delay Corrections", "content": "We are further examining the curious cases of some LLMs that initially show a decline in their course-correction abilities, only to experience an uptick once the volume of harmful content becomes more substantial. These cases pique our interest as they"}, {"title": "E Further Details on Experiments with\nC2-SYN", "content": null}, {"title": "E.1 Detailed Setup", "content": "Training We adopt LLaMA-Factory (Zheng et al., 2024) to implement standard DPO training, we use a warmup ratio of 0.1 and a max length of 1024.\nBenchmarks To evaluate the general performance and safety of the targeted LLMs, we employ a variety of benchmarks targeting different abilities. We select Eval-Scope (ModelScope Contributors, 2024) to measure performance on the following datasets: MMLU (Hendrycks et al., 2020b), TruthfulQA (Lin et al., 2022), Hellaswag (Zellers et al., 2019), C-Eval (Huang et al., 2024), and HumanEval (Chen et al., 2021). For Natural Questions (NQ) (Kwiatkowski et al., 2019), we used OpenCompass (Contributors, 2023). Lastly, we assess performance on GSM8K (Cobbe et al., 2021) and ToxiGen (Hartvigsen et al., 2022) with the EleutherAI/lm-evaluation-harness (Gao et al., 2023) evaluation framework.\nJailbreak Attacks The setup details of the conducted jailbreak attacks are described as follows:"}, {"title": "E.2 Safety Assessed via Token Dynamics", "content": "In Section 5.4, we assess the model's safety by analyzing the distribution of tokens in the text generated by the model. We focus on a series of tokens related to safety, which are considered to halt and suppress the generation of harmful content in the model's output. We pick a set of safety tokens, as shown in Table 13.\nIn Figure 15, we provide a case of the probability shifts in tokens between the vanilla and the trained LLAMA2-CHAT 7B model using our method, with a focus on safety-aligned tokens. We analyze the direction of probability shifts in tokens between the vanilla and our trained model to understand how our method influences the model's response at certain decoding positions. The direction of these shifts is crucial, as it indicates whether our method is enhancing the model's use of safety-aligned tokens. A positive shift regarding safety-aligned tokens in Ours \u2212 Vanilla suggests that our method increases the likelihood of these tokens appearing in the model's output, which is a desired"}]}