{"title": "MuJoCo MPC for Humanoid Control: Evaluation on HumanoidBench", "authors": ["Moritz Meser", "Aditya Bhatt", "Boris Belousov", "Jan Peters"], "abstract": "We tackle the recently introduced benchmark for whole-body humanoid control HumanoidBench [9] using MuJoCo MPC [6]. We find that sparse reward functions of HumanoidBench yield undesirable and unrealistic behaviors when optimized; therefore, we propose a set of regularization terms that stabilize the robot behavior across tasks. Current evaluations on a subset of tasks demonstrate that our proposed reward function allows achieving the highest HumanoidBench scores while maintaining realistic posture and smooth control signals. Our code is publicly available and will become a part of MuJoCo MPC, enabling rapid prototyping of robot behaviors.", "sections": [{"title": "I. INTRODUCTION AND RELATED WORK", "content": "Designing dynamic behaviors for humanoid robots is a challenging problem [4], [5]. One promising approach is to craft task-specific reward functions and train policies via Reinforcement Learning (RL). To this end, [9] proposed HumanoidBench, a suite of 27 tasks for Unitree H1 robot based on MuJoCo [11], with 12 tasks assessing locomotion abilities and 15 tasks evaluating object manipulation skills. In [9], performances of four popular RL algorithms are reported: DreamerV3 [2], TD-MPC2 [3], SAC [1], PPO [8]. However, none of these methods is able to solve the tasks satisfactorily, producing jittery motions ill-suited for execution on real hardware. Therefore, we evaluate an alternative approach to humanoid behavior synthesis that exploits the power of simulation: Model Predictive Control (MPC).\nMPC does not require training; at each time-step, it selects the optimal action based on a multi-step lookahead search, and repeats this procedure at the next time-step in a receding-horizon manner [7]. MPC is a mature paradigm that admits various planning algorithms for this search, including iLQG [10] and even randomly sampled action sequences [6]. We apply MPC to a subset of the HumanoidBench locomotion and manipulation tasks using MuJoCo MPC (MJPC) [6], which implements the aforementioned planners."}, {"title": "II. MJPC WITH SHAPED REWARDS", "content": "MPC repeatedly solves a finite-horizon optimal control problem \\(\\min_{u_{0:T}} \\sum_{t=0}^{T} c(x_t, u_t)\\) where \\(c(x_t, u_t)\\) is the instantaneous cost. MJPC further decomposes the cost into a weighted sum \\(c(x, u) = \\sum_{i} w_{i} n_{i}(c_{i}(x, u))\\) where \\(c_{i}(x, u)\\) are (signed) residuals, \\(n_{i}\\) are twice-differentiable norm functions, and \\(w_{i}\\) are tunable non-negative weights [6].\nThe HumanoidBench tasks are formulated in terms of rewards \\(r_{hb}\\) rather than costs. Therefore, we apply a transformation \\(c_{hb}(x, u) = |r_{max} - r_{hb}|\\) before passing it to MJPC, where \\(r_{max}\\) is the maximum achievable instantaneous reward (set to \\(r_{max} = 1\\)) and the norm is approximated by kSmoothAbsLoss to ensure differentiability.\nWe observed that \\(r_{hb}\\) alone yields physically undesirable behaviors (Fig. 2) and results in low rewards (Fig. 3). Therefore, we introduced a number of shaping terms that i) improve robot stability and ii) provide dense reward signal. Stability-enhancing cost terms penalize excessively strong movements and promote postural balance: 1) height of the robot's head from the ground; 2) height difference between the pelvis and the feet; 3) linear velocity of the center of mass (CoM); 4) 'balance' (projection of CoM between the feet); 5) deviation from a 'canonical' posture; 6) direction the robot is facing; 7) magnitude of the control signal.\nDense reward residuals: 1) distance between an object and its target location; 2) distance between the left hand and the object; 3) distance between the right hand and the object. Although some of these residuals are already encoded in the HumanoidBench reward, we find that providing them individually to the optimizer improves performance (Fig. 1)."}, {"title": "III. EXTENDED TASK EVALUATION PROTOCOL", "content": "Along with the pointed out uninformativeness of the reward function in HumanoidBench, we observe one further factor that impacts the generated behaviors and evaluations\u2014namely, the episode length. The evaluation period is very short-2 seconds for the Walk and Stand tasks; and for the Push task, the simulation terminates as soon as the box reaches its target location. Such short episode lengths result in policies that do not maintain balance at the end of the episode, not expecting further tasks (Fig. 2, bottom). We provide a quantitative evaluation of the impact of the episode length in Fig. 3 on the Walk task. A qualitative evaluation is provided in Fig. 2, where new targets are re-spawn, requiring the robot to perform the task repeatedly. Based on these evaluations, we argue for longer episode lengths and for repeated tasks with changing goals."}, {"title": "IV. ANALYSIS OF BEHAVIORS AND PLANNERS", "content": "In this section, we analyze the generated behaviors in a quantitative manner, beyond only reporting the return value."}, {"title": "V. CONCLUSION", "content": "In this paper, we made a step towards enabling easy experimentation with MPC for humanoid robot control in simulation. We ported HumanoidBench to MuJoCo MPC and provided evaluations on a subset of tasks. We showed that the current reward functions are insufficient and proposed shaping terms. Furthermore, we identified an issue with short episode lengths and argued for evaluating on repeated tasks with changing goals. Our results show superior performance of our method over MPC and RL baselines."}]}