{"title": "VARIATIONAL AUTOENCODER-BASED NEURAL NETWORK\nMODEL COMPRESSION", "authors": ["Liang Cheng", "Peiyuan Guan", "Amir Taherkordi", "Lei Liu", "Dapeng Lan"], "abstract": "Variational Autoencoders (VAEs), as a form of deep generative model, have been widely used\nin recent years, and shown great great peformance in a number of different domains, including\nimage generation and anomaly detection, etc.. This paper aims to explore neural network model\ncompression method based on VAE. The experiment uses different neural network models for MNIST\nrecognition as compression targets, including Feedforward Neural Network (FNN), Convolutional\nNeural Network (CNN), Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM).\nThese models are the most basic models in deep learning, and other more complex and advanced\nmodels are based on them or inherit their features and evolve. In the experiment, the first step is to\ntrain the models mentioned above, each trained model will have different accuracy and number of\ntotal parameters. And then the variants of parameters for each model are processed as training data in\nVAEs separately, and the trained VAEs are tested by the true model parameters. The experimental\nresults show that using the latent space as a representation of the model compression can improve the\ncompression rate compared to some traditional methods such as pruning and quantization, meanwhile\nthe accuracy is not greatly affected using the model parameters reconstructed based on the latent\nspace. In the future, a variety of different large-scale deep learning models will be used more widely,\nso exploring different ways to save time and space on saving or transferring models will become\nnecessary, and the use of VAE in this paper can provide a basis for these further explorations.", "sections": [{"title": "1 Introduction", "content": "In the last few years, the development of neural networks has been more rapid than ever before, with new models\nbecoming more accurate and more complex. A significant portion of the currently popular models are based on\nsome basic models, such as VGGNet[13] based on CNN[7] and LSTM[6] based on RNN[8], while some more novel\nmodels such as Generative Pre-trained Transformer (GPT)[4] and Vision Transformers (ViT)[5] are developed on top of\nTransformer[10]. Transformer is not directly based on traditional neural networks, but it is still influenced by some of\nthe basic models, such as RNN and FNN[9], and improves on their limitations. As the model becomes increasingly\nlarger, the amount of model parameters also increases significantly. For example, VGGNet, as a very deep convolutional\nneural network, has as many as 138M parameters for its entire network, and some of its variants even contain much\nmore parameters if more convolutional layers are stacked to improve the model accuracy."}, {"title": "2 Methods", "content": "VAE is a deep generative model improved from Autoencoder, with modification on how to operate the latent space. The\ngeneral idea behind Autoencoder is setting encoder and decoder to be neural network and using an iterative optimization\nprocess to learn the best encoding-decoding scheme. Hence, the encoder would receive input data and decoder would\noutput the reconstructed input in each iteration, the original data and reconstructed data are compared and the errors\nare backpropagated through the architecture in order to update the weights of the network. Since the loss function in\ntraining Autoencoder only aims to minimize the reconstruction loss between input and output, it does not consider\nfurther for the latent space, but simply ensures that the information in the compressed representation is sufficient for\ndecoding. As a deterministic model, Autoencoder encodes inputs as individual points in the latent space without any\ninterpretation of uncertainty or probabilistic. And the latent space also does not have any regularization, so the structure\nof the latent space can be arbitrary and may not follow any meaningful distribution. In order to improve the flexibility\nand scalability, VAE learns probabilistic representations of data, rather than a deterministic one, and generates new\nsamples.\nVAE assumes that the observed data x is generated from latent variables z, which can be described as drawing a latent\nvariable z from a prior distribution p(z) and generating data x from the conditional distribution p(x|z). Hence, the\ngoal is to model the joint distribution p(x, z) = p(z)p(x|z). However, the marginal likelihood p(x) = \\int p(x,z)dz\nis often complex and intractable because it requires integrating over all possible values of z, which makes it difficult\nto directly optimize the likelihood p(x). To bypass this intractability, VAE uses variational inference[15]. Instead of\ndirectly computing true posterior distribution p(z|x), VAE approximates it with a variational distribution q(z|x). This\nvariational distribution can be any distribution but often as a Gaussian distribution. The idea is to make q(z|x) as close\nas possible to the true posterior distribution p(z|x)."}, {"title": "", "content": "The objective of a VAE is to maximize the Evidence Lower Bound (ELBO), which is a lower bound on the log-likelihood\nof the observed data. The ELBO is derived as follows:\nlog p(x) = log \\int p(x, z)dz = log \\int \\frac{p(x, z)q(zx)}{q(zx)} dz.\nBy applying Jensen's inequality:\nlog p(x) \\geq E_{q(z/x)} \\left[log \\frac{p(x, z)}{q(zx)}\\right] = E_{q(z|x)} [log p(x|z)] \u2013 KL(q(z|x)||p(z)).\nKL(q(z|x)||p(z) above is the Kullback-Leibler (KL) divergence between the approximate posterior and the prior, which\nacts as a regularizer. The ELBO is thus given by:\nELBO = E_{q(zix)} [log p(x|z)] - KL(q(z|x)||p(z)).\nMaximizing the ELBO is equivalent to minimizing the reconstruction error (first term) and the KL divergence between\nthe approximate and true posterior (second term).\nTo optimize the ELBO using gradient-based methods, backpropagating through the stochastic sampling of z is required.\nThe reparameterization trick enables this by expressing z as a deterministic function of a parameter \u03bc(x), \u03c3(x), and a\nrandom variable e drawn from a simple distribution (like a standard normal distribution):\nz = \u03bc(x) + \u03c3(\u03c7) \u03b5, where \u03b5~ N(0, 1).\nThis reparameterization allows gradients to be propagated through \u03bc and o, enabling efficient optimization."}, {"title": "3 Data", "content": "There are two different kinds of data used in this project. The first is MNIST dataset, which is a handwritten digits\ndatabase has a training set of 60,000 examples and a test set of 10,000 examples. This dataset is used to train the basic\nmodels used for the model compression experiment, including FNN, CNN, RNN, and LSTM. The parameter sets from\nthese four trained neural networks are the data used for training VAE, which is also the model compression experiment.\nEven though these basic models are not large models, it can still be time-consuming to train enough of them and thus\nhave enough model parameters VAE training. Training data and validation data are generated from these true model\nparameters by randomly adding noise to the different positions. For each set of true model parameters, 80 test data and\n20 validation data are generated, while the true model parameters are used as test data.\nThe size of the parameter set varies from model to model. Although each model is used to recognize handwritten digits\nand the model structure is not complex, the number of parameters for each model varies due to the overall network\nstructure as well as the layers used are different in the different networks."}, {"title": "4 Experiments & Results", "content": "As mentioned above, the size of the parameter set for each model is different, and it's also relatively large compared\nto the size of most data commonly used with VAE (e.g., images or time series), so we trained VAE for each neural\nnetwork separately. The first step in data preprocessing is to put the parameters of the different layers in the model into\na one-dimensional sequence. Since the number of parameters contained in each layer of the neural network may vary,\nflattening the parameter set allows the VAE to process more easily. After getting the flattened data, the next step is to"}, {"title": "4.2 Experiments", "content": "In this research, we conducted two aspects of the experiment, first exploring the degree of model compression using\nCNNs, and then exploring the usability of this compression method using different neural network models. Although\ndifferent data were used in the experiment, the environmental settings of the experiment as well as the training conditions\nof the VAE were maintained. All model training and tests were performed on a MacOS 14.3 platform consisting of\nthe Apple M3 Pro CPU and 18 GB RAM. The code was implemented in PyTorch. The loss function used in VAE\ntraining is introduce in Equation 3. The total number of parameter set of each neural network used in this work is 101,\nof which 1 real parameter set was used as test data and the remaining 100 parameter sets were generated based on the\nreal parameters and were divided into training and validation sets in a ratio of 4:1. A limit of 500 epochs was set for\ntraining, subject to early stopping to avoid model overfitting.\nIn the first part of experiment, we began with compression rate around 20x, which downscales the chuck size from\n2048 to 128. Then we gradually reduced the size of the latent space from 128 to 64 in each VAE training, and the test\naccuracy almost kept consistent during the whole process. Therefore, we decided to use 64 as the size of latent space in\nthe second part of experiment. Next, we extract the parameter sets from the trained FNN, CNN, RNN, and LSTM for\ntraining VAE, and compare their performance using the original parameters as well as those reconstructed through VAE\nin the end."}, {"title": "4.3 Results", "content": "The training process of VAEs using input data from different neural networks is shown on Figure 2. As can be seen\nfrom it, there is no overfitting of the model within 500 epochs of training, even though the efficiency of learning slows\ndown a lot after 100 to 150 epochs of training. Moreover, the learning process is not the same in the case of using\ndifferent model parameters as inputs. The parameter sets from different models have values in the range of [-1, 1], and\nthe biggest difference between them is the size. However, when using the parameters from FNN as input, the VAE\nclearly converges faster than other cases, but the FNN parameter set is the second largest of the four different parameter\nsets. This may indicate that the size of the parameter set is not the main factor affecting the learning process in VAE, but\nthe intrinsic connection or parameter distribution of the different parameters within the parameter set is also something\nthat VAE learns when compressing data. FNN uses only fully connected layers, and the neuron operations controlled by\nits parameters will be simpler than other layers such as convolutional layer, recurrent layer, etc., which possibly make\nits parameters easier to be learned by VAE.\nDespite the fact that the parameter set was compressed to about one-thirtieth of its original size, these reconstructed\nparameter sets did not significantly affect the accuracy of the respective models after reconstruction through VAE's\ndecoder. As it can be observed from Table 2, the accuracy of each neural network on the MNIST test set after using\nthe new parameter set is not much different from that when using the original parameter set, with an accuracy loss\nof about 1% regardless of what the original accuracy was."}, {"title": "5 Conclusion & Future Work", "content": "Based on the size comparison of the latent space and the input in VAE, we can see that the use of VAE is able to\ncompress the size of the neural network parameter set to one-thirtieth of the original one, and the compression rate\nis higher than many currently popular methods. And in the MNIST test, the model using the original parameters and\nthe model using the parameters reconstructed from latent space through VAE have basically the same accuracy. This\nindicates that the 30x compression rate is not the bottleneck of the method, and that there is still room for improvement\nin the compression rate if the accuracy is slightly sacrificed or even if the current accuracy is continued to be preserved.\nThese optimistic results suggest that VAE-based model compression methods are worthy of more in-depth exploration\nin the future.\nFurther research on the method can be done in two directions, namely the compression of large models and the\nexploration of the model structure of VAE itself. The models used in this research are all basic models, but the\nlayers they use are similar to those in some of the large models in some aspects, such as parameter distribution and\ncomputational principles, so the compression of the large models should also be able to achieve good results, but the\ncorresponding VAE model structure should also become more complex, because the number of parameters in the large\nmodels will be much more than that in the basic models. In the comparison of the training process, we can also see that\nthe convergence speed of VAE is not the same for using data from neural networks with different layers, which could be\nrelated to the different parameter distributions in different neural networks, and it is also worthy of further study. As\nfor the VAE itself, only the fully connected layer and the most basic loss function in VAE are used in this experiment.\nTherefore, trying different types or more complex VAEs will also help to improve the compression efficiency."}]}