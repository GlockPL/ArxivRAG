{"title": "A Nested Model for AI Design and Validation", "authors": ["Akshat Dubey", "Zewen Yang", "Georges Hattab"], "abstract": "The burgeoning field of artificial intelligence (AI) has yet to fully permeate real-world applications, largely due to issues of trust, transparency, and concerns about fairness and discrimination. Despite the increasing need for new and revised regulations to address the ethical and legal risks of using AI, there is a mismatch between regulatory science and AI, hindering the creation of a consistent framework. This highlights the need for guidance and regulation, especially as new AI legislation emerges. To bridge this gap, we propose a five-layer nested model for AI design and validation. This model is designed to address the challenges faced by AI practitioners and streamline the design and validation of AI applications and workflows, thereby improving fairness, trust, and AI adoption. This model not only parallels regulations and addresses the daily challenges faced by AI practitioners, but also provides prescriptive guidance for determining appropriate evaluation approaches by identifying threats to validity unique to each layer. We also provide three recommendations motivated by this model: authors should distinguish between layers when claiming contributions to clarify the specific areas in which the contribution is made and to avoid confusion, authors should explicitly state upstream assumptions to ensure that the context and limitations of their AI system are clearly understood, AI venues should promote thorough testing and validation of AI systems and their compliance with regulatory requirements.", "sections": [{"title": "I. INTRODUCTION", "content": "While artificial intelligence (AI) has grown tremendously in recent years, it has yet to reach its true potential in real-world use cases. This is due in part to a lack of trust and transparency, as well as fairness and fear of discrimination [1]. Al has unique strengths and weaknesses, so there will always be a need to develop new regulations and change old ones. But with the benefits of AI come significant ethical and legal risks. As a result, there is an urgent need to address not only the regulatory policies that will facilitate the implementation of AI in real-world use cases, but also how practitioners design and validate AI applications and workflows. Several regulatory bodies from different countries are stepping in to establish a set of regulations for the implementation of AI in real-world applications. Researchers have shown an increased interest in unifying regulatory science and AI, but there is no framework for the intersection of the two.\nSeveral research efforts from the fields of Human-Computer Interaction (HCI) and Explainable AI (XAI) have highlighted current bottlenecks in involving humans in understanding, decision making, validation, etc., or having humans in the loop for AI applications and workflows. There is an influential work by Liao et al.on the intersection of HCI and XAI in the form of XAI-Question Bank (XAI-QB). XAI-QB aims to solve the challenge of achieving full explicability through algorithm-informed prototypical questions [2]. While the need for guidelines and regulatory frameworks has been addressed by Lennerz et al., one of the problems is how AI is implemented and how results are communicated, which further hinders AI adoption [3]. In fact, the authors specifically mention that regulatory concepts are necessary for AI researchers, as these concepts allow risk and safety concerns to be addressed and understood by the regulations proposed by the US and Europe. At the moment, however, the fields of regulatory science and AI are completely separate, with no overlap in sight. A growing trend has been observed in discussions that express an urgent need for work at the intersection of regulatory science and AI [4]\u2013[11]. When it comes to regulating AI, many regulatory bodies are stepping in and making it mandatory to comply with laws that require explanations or interpretations to be given to users when confronted with algorithmic output [12]. The intersection of artificial intelligence (AI) and regulation requires massive work more than ever due to the emergence of new Al laws [13]-[16]. There is a growing trend of discussions that express an urgent need for the aforementioned intersection [17]-[19]. In response to the growing need for updated regulations to address ethical and legal risks that face obstacles due to a misalignment between regulatory science and AI, we have presented a structured, five-layer, nested model for the design and validation of AI that serves as a systematic guide for the assessment and validation of AI applications and workflows. It facilitates the identification of appropriate evaluation methodologies by identifying unique threats within each layer, thereby mitigating the inherent tensions between technological innovation and regulatory imperatives. In addition, the proposed model addresses concerns about fairness, trust, and the alignment of AI models with existing regulations. To address the challenge of bridging the gap between AI practitioners and regulators, two preliminary case studies include researchers who aim to develop reliable, wise, and trustworthy human-centered AI through ethical and theoretical guidelines with management strategies or software engineering practices. These include audit trails to enable analysis of failures, software engineering workflows, verification and validation testing, bias testing to enhance fairness, and even explainable user interfaces [20]\u2013[22]. The proposed model for AI design and validation is the first of its kind, inspired by XAI-QB, which we extend to include the prototypical questions from a regulatory and domain perspective. In parallel with the regulations, it addresses the issues that AI practitioners face on a daily basis. The nested"}, {"title": "II. REGULATION OF AI", "content": "Calls for appropriate regulation of AI have grown as awareness of its risks has increased. This regulation aims to ensure that AI is legal, ethical, and robust, while minimizing potential harm and increasing legal certainty. Efforts are underway to establish global regulatory standards for AI, potentially leading to harmonization. Collaboration between government, industry, and civil society is essential for the responsible use of AI. Regulation is needed to protect consumers and society, provide a reliable framework for businesses, and understand the ethical and societal implications of AI. The complexity and risks associated with AI underscore the urgency of establishing best practices and a comprehensive framework for AI regulation that takes into account ethical, legal, and societal impacts [23], [24].\nThe EU has taken a leading role in developing AI regulations, with a diverse regulatory landscape and a focus on protecting fundamental rights. The General Data Protection Regulation (GDPR) includes provisions that address the legitimacy of algorithmic decision-making, emphasizing the right to human intervention and meaningful information for individuals. The European Commission's Ethics Guidelines for Trustworthy AI and the proposed AI Act aim to ensure the responsible and transparent use of AI, introducing principles such as human agency, technical robustness, privacy, transparency, and account-ability. These efforts aim to create a framework for trustworthy AI that benefits society and the environment [25]-[28].\nIn the United States, AI regulation has relied primarily on self-regulation by industry stakeholders, leading to criticism for a lack of rigorous regulatory oversight. The Defense Advanced Research Projects Agency (DARPA) has initiated the first research program in XAI to address the challenge of opaque yet effective AI systems, with the goal of developing machine learning techniques for more explainable models. The National Institute of Standards and Technology (NIST) has also emphasized the importance of explainability, proposing principles to ensure that AI systems provide understandable ex-planations for their results. While the US regulatory landscape is focused on promoting innovation, the EU's emphasis on XAI and ongoing policy discussions in the US and UK reflect a growing recognition of the importance of explainability in AI systems [26], [29]\u2013[31]."}, {"title": "III. NESTED MODEL FOR AI DESIGN AND VALIDATION", "content": "The nested model consists of five layers, namely the regulation, the domain, the data, the model, and the prediction. We have expanded regulatory and domain questions from the XAI-QB. In addition, we have grouped the prototypical questions from the XAI-QB based on the layer of the nested model of AI design and validation that they need to address. The five-layer nested model for AI design and validation, including but not limited to its regulatory and domain-aware layers and questions, are our major contributions. The nested model for AI provides much-needed overlap and grounding, facilitating design and validation of AI applications and workflows, increasing fairness, trust, and AI adoption.\nA. Motivation\nThe EU is developing AI regulations with key requirements that align with the principles of Explainable AI (XAI), but addressing both ethical and technical requirements may require a focus on XAI and human-computer interaction (HCI) to ensure that Al upholds broader values such as accountability, human rights, and sustainable innovation. At the intersection of XAI and HCI, the XAI-QB serves as a valuable tool for understanding user requirements for XAI. It provides a set of algorithm informed questions to achieve user-centered explainability in AI applications. To address the perspective of domain and regulatory authorities, and inspired by both Munzner's nested model for visualization and the XAI-QB, we propose a five-layer nested model for the design and validation of AI applications and workflows.\nB. Our Work\nOur work has taken full advantage of the state of the art and the effectiveness of the nested model. We extended the XAI-QB to include regulatory and domain-layer questions, aiming to help organizations address the needs of regulators when implementing AI. This user-centric approach provides a hierarchical guide for stakeholders to elicit end-user and regulatory needs, while also highlighting technical barriers and emphasizing the importance of a human-centered approach to regulation using XAI. The nested model supports specifying requirements for building AI applications and identifies opportunities for collaboration between the HCI and AI communities, industry practitioners, and academics to advance the field of AI. Figure 1 shows the nested five-layer model for AI design and validation.\nOur nested model is divided into five distinct layers. The model can be accompanied by questions. These questions should be answered at each layer. Although these questions can be modified or extended depending on the use case and user-specific requirements, they help to satisfy regulatory requirements.\nThe regulation layer is responsible for making the AI workflow compliant. At this layer, we categorize the rules into ethical and technical regulations. Ethical rules should be specified and addressed at this layer. This layer prohibits access to subsequent layers if the ethical rules are not addressed. Once the ethical rules have been addressed with the appropriate infrastructure and various methodologies, the user should proceed to subsequent layers to address the technical rules.\n1) Which regulation should be specified for this AI work-flow?\n2) Which country has specified this regulation?\n3) Can you list out the key requirements set up by the regulation?\n4) Can you categorize the guidelines into ethical and technical ones?\n5) Who are the stakeholders involved in this AI workflow?\nThe domain layer ensures that any process taking place in the nested model is within the scope of that domain. This nested model will allow for explainability at the domain layer. Different domains, in different settings, may have different answers to the XAI questions. At this layer, the domain expert is solely responsible for achieving certain goals related to the domain layer. Some of the goals may be to list the needs and requirements. After that, the domain expert should decide on the appropriate model validation metrics.\n1) What are the specific requirements within the domain?\n2) Does the domain encompass high-stakes areas such as healthcare or finance?\n3) What are the potential risks associated with the domain?\n4) Are there any pre-existing assumptions that are neces-sary?\n5) Is feedback from domain experts a requirement for this process?\nThe data layer aims to explain the data by summarizing and analyzing the data and providing insights into the data. This layer helps the user understand the data, the biases involved, how to mitigate the biases, the distribution, the limitations of the data, and what domain knowledge is contained in the data. This layer involves both the ML practitioner and the domain expert. The domain expert should list the limitations of the data, and both the ML practitioner and the domain expert should decide whether the data can be used for the specific application, The domain expert should provide the prior domain knowledge in the form of knowledge graphs to the ML practitioner, who will then incorporate this domain knowledge during training of the ML model. The data should be represented visually in the simplest way possible by the ML practitioner, so that the domain expert has a clear understanding of the data, which may help them to list the limitations or drawbacks of the data or the bias in the data, the ML practitioner will then mitigate the bias. One of the easy ways to detect & mitigate the bias in the data is to use the capabilities of IBM's AI Fairness 360 (AIF360) library [32]. This library can be used to remove the bias from both the data and the model. The data layer has the associated XAI questions.\n1) What type of information is contained within the data?\n2) What inferences can be drawn from this data?\n3) Which aspects of the data are the most significant?\n4) How is the information distributed within the data?\n5) Is it feasible to enhance the model's performance by reducing the number of dimensions?\n6) Could the use of data summarization techniques provide a more effective explanation?\nThe model layer aims to explain the inner workings of the model, the parameters involved and their meaning, the interpretability at the model layer, and which model maintains the balance between performance and interpretability. This layer involves an ML practitioner. One of the most common baselines to get started with model-layer explainability might be to answer whether the interpretable models can be used instead of black-box models. If the interpretable model can be used, then the hyper-parameter analysis should be done thoroughly to get the best result. There is often a trade-off between interpretability and performance. If performance is the most important goal at this layer, then post hoc methods can be used to achieve interpretability; otherwise, the interpretable models should undergo hyper-parameter tuning to get the best results without black box models.\n1) What attributes render a parameter, objective, or action important to the system?\n2) At what point did the system assess a parameter, objective, or action, and when was it disregarded by the model?\n3) What are the repercussions of altering a decision or modifying a parameter?\n4) How was a specific action executed by the system?\n5) How are these model parameters, objectives, or actions interconnected?\n6) What elements does the system consider (or exclude) when making a decision?\n7) What methods does the system employ or avoid to accomplish a goal or inference?\nThe prediction layer aims to explain the reason for a particular prediction, how certain inputs affect the prediction, whether the reason is sufficient for the conclusion or decision, what variables are involved behind the prediction, and how the prediction changes under certain considerations or criteria. At this layer, the domain expert relies on the ML practitioner for results and a deeper understanding of the predictions. This layer would help answer the prediction-layer questions.\n1) What factors contribute to the importance of a parameter, objective, or action within the system?\n2) When was a parameter, objective, or action evaluated by the system, and when was it rejected by the model?\n3) What are the consequences of changing a decision or adjusting a parameter?\n4) How was a specific action carried out by the system?\n5) How are the parameters, objectives, or actions within the model interconnected?\n6) What elements does the system take into account (or disregard) when making a decision?\n7) What methods does the system utilize or avoid to achieve a specific goal or inference?"}, {"title": "C. Addressing the question \"How the nested model solves the problem?\"", "content": "1) Once the regulations are followed or the key requirements have been introduced, only then can the next layer, i.e., the domain layer, be entered.\n2) The nested model prohibits entering further sub-layers until the goal of the previous layers has been achieved. For example, after accomplishing the specific goal of the domain layer, which is to define applications and requirements, the user can proceed to the data layer to accomplish certain goals within the scope of the data layer.\n3) The nested model is built on the foundation of the XAI-QB, which allows the user to answer specific questions that lead to a clear understanding of the rules, goal, needs, requirements, solutions, and conclusions.\n4) nested model conforms to regulatory guidelines. This leads to solving certain challenges that arise when using Al workflows. This enables AI to be compliant, trustworthy, accountable, non-discriminatory, appropriate for human equality, robust & secure, and transparent AI that operates under human agency & oversight.\n5) The needs of regulators are addressed in the form of prototypical questions. It uses the human-computer interaction approach to eliminate the threat posed by the lack of human agency and oversight.\n6) The nested model can be used as a guidance tool to support the need for specification work to create AI applications that meet the key requirements of regulatory authorities.\n7) The nested model addresses \"transparency and explain-ability\" from both the regulatory and AI perspectives.\n8) The nested model is a prescriptive guideline that bridges the gap between AI regulations and AI."}, {"title": "D. Outcomes of the Nested Model for AI", "content": "The outcomes of the Nested Model are as follows:\n\u2022 It helps define a common baseline for the adoption of AI in real-life use cases.\n\u2022 It acts as a common intersection between regulatory authorities (and their regulations), XAI, and AI practitioners by bringing these divergent fields together.\n\u2022 Reduces the chances of AI implosion by addressing the issues and potential threats with each layer. Mitigates the problems of non-transparency, unfairness, and discrimina-tion.\n\u2022 Evaluates the AI workflow, not only through evaluation metrics, but also through key requirements of the regula-tions."}, {"title": "IV. IMPLEMENTATION", "content": "To implement the nested model for the design and validation of AI workflows for AI governance we need to define regulations into ethical and technical. Modern regulations are a combination of both ethical and technical implementations. Ethical regulations and technical regulations in the context of AI can be distinguished based on their focus and objectives.\nEthical regulations are often concerned with guiding the moral principles and values associated with AI development and deployment, while technical regulations focus on specific technical aspects and requirements to ensure the responsible and safe use of Al systems [33].\n\u2022 Ethical Requirements: Focus: Ethical regulations are primarily concerned with promoting values, principles, and norms associated with responsible AI development and usage. This may include considerations of fairness, transparency, accountability, privacy, and the broader societal impact of AI technologies. Objective: The primary goal is to ensure that AI systems align with ethical standards and do not cause harm or violate fundamental human rights. Ethical guidelines provide a framework for developers and organizations to make morally sound decisions throughout the AI life cycle. The ethical key-requirements could be solved with methodologies listed by Al Alhamed et al. [22].\n\u2022 Technical Requirements: Focus: Technical regulations, on the other hand, are more specific and detail-oriented. They focus on the technical aspects of Al systems, such as algorithms, data quality, safety measures, and other technical requirements. Objective: The objective of technical regulations is to set standards and requirements that AI developers must follow to ensure the robustness, security, and reliability of AI systems. These regulations are often designed to prevent technical issues, biases, and potential risks associated with AI deployment.\nUsing the above discussion as a reference, we define steps to implement the nested model, referencing questions at each layer as needed.\nSteps to follow:\n1) Define the regulations and key requirements at the layer of the regulation.\n2) Categorize the key requirements into ethical and technical requirements.\n3) At the layer of the regulation, address the ethical requirements first.\n4) After proceeding into the domain layer, the domain expert will list down the domain-specific requirements, which will act as a reference for AI practitioners.\n5) Map all the technical key requirements from the reg-ulations to the sub-layer, namely, data, domain, and prediction.\n6) Address the specified technical key requirements at each sub-layer using appropriate methodologies.\nA. Use Case: Europe Union Requirements for Trustworthy AI\nListing out the steps:\n1) Define the regulations and key requirements at the layer of the regulation : Based on fundamental rights and ethi-cal principles, the Guidelines list seven key requirements that AI systems should meet to be trustworthy:\n\u2022 Human agency and oversight\n\u2022 Technical robustness and safety\n\u2022 Privacy and data governance\n\u2022 Transparency\n\u2022 Diversity, non-discrimination, and fairness\n\u2022 Societal and environmental well-being\n\u2022 Accountability\n2) Categorize the key requirements into ethical and technical requirements: After the ethical and technical requirements are satisfied, they will result in accountability.\n\u2022 Ethical requirements: Privacy and data governance, societal and environmental well-being, safety.\n\u2022 Technical requirements: Human agency and over-sight, technical robustness, transparency, diversity, non-discrimination, fairness\n3) Address the ethical requirements.\n4) Domain experts list down the domain-specific require-ments.\n5) Map the technical requirements with the sub-layers namely Data, Model, and Prediction.\n6) Address the specific technical requirements at the specific sub-layer using appropriate methodologies. When all the key requirements (ethical and technical) are achieved, it results into accountability."}, {"title": "V. EXAMPLES", "content": "A. AI Model by Google LLC to detect retinopathy.\nDiabetic retinopathy is a disease of the retina caused by diabetes that leads to vision impairment or loss. During the development of the workflow the ethical and technical requirements were neither listed nor addressed. Google's deep learning model for the detection of diabetic retinopathy failed for several reasons. The model had been trained on high-quality, high-resolution eye scans, but in real-life clinics, the images captured by nurses differed in quality and lighting conditions, leading to a significant disparity between the training data and real-life data. The model was approved by the Food and Drug Administration (FDA) of the United States. Additionally, the model was not validated on real-life data, which could have been verified by domain experts. This discrepancy between training data and real-life conditions, as well as the lack of validation of real-life data, contributed to the model's failure to accurately detect diabetic retinopathy in a clinical setting [34].\nB. Zillow Group, Inc. House Price Forecasting Model\nZillow suffered a significant loss of over $500 million in its home-flipping business due to the failure of its non-transparent proprietary forecasting algorithm [35]. The loss was due to the algorithm's inability to accurately predict home prices, resulting in over-payment for homes and financial volatility. This raised concerns about the reliability of AI models in critical business decisions, prompting a reevaluation of the use of AI in high-stakes operations and highlighting the importance of considering potential limitations and risks. The failure underscores the need for reliable and transparent AI algorithms, and emphasizes thorough evaluation and risk assessment when integrating AI into decision-making processes. The event serves as a cautionary tale for organizations relying on AI for critical business strategies, highlighting the potential consequences of inadequate algorithm performance and the need to maintain a critical perspective on AI's capabilities and limitations.\nC. The Feature Cloud Platform for Federated Learning\nThis platform takes advantage of federated learning and is an impressive work by Holzinger et al. [36] presents a comprehensive exploration of the integration of domain knowl-edge graphs into deep learning for improved interpretability and explainability using Graph Neural Networks (GNNs). Federated Learning (FL) protects privacy by transmitting only model updates, reducing the risk of data breaches. Its decentralized approach enhances data security and compliance. FL promotes collaboration between devices, improving generalization with diverse data sets. In addition, it improves the learning algorithm by incorporating explanations and conceptual knowledge for better interpretability [37]. The authors focus on using a protein-protein interaction (PPI) network to enrich deep neural networks for classification, enabling the detection of disease sub-networks using explainable AI. This work addresses the potential threats at each layer of the nested model and validates them accordingly.\n1) Regulations: GDPR and Europe Union Requirements for Trustworthy AI.\n\u2022 Ethical Requirements: Privacy and data governance, societal and environmental well-being, safety.\n\u2022 Technical Requirements: Human agency and over-sight, technical robustness, transparency, diversity, non-discrimination, fairness\n2) Addressing Ethical Requirements: Privacy, data governance, and safety are addressed through federated learning approach. This use case accelerates biomedical research, which in turn benefits humanity and enables societal well-being. In addition, by decentralizing the training process, federated learning has the potential to be more environmentally friendly, making it a promising approach for reducing the carbon footprint of machine learning model training [38].\n3) Addressing the technical requirements: The framework included an expert-in-the-loop approach to develop AI workflows under the supervision of a domain expert, allowing for human agency and oversight. The do-main expert evaluates data for biases related to non-discrimination and diversity. Knowledge graphs are implemented to incorporate prior domain knowledge. The potential threat at the data layer is validated. At the model layer, models are evaluated for fairness and technical robustness through continuous feedback from the domain expert. Open source models are preferred for training. Finally, the XAI is integrated to make the AI process transparent and trustworthy.\nIn addition, this work also answers the extended XAI-QB in a summarized fashion. The questions answered are \"What was not done and why?\", \"What problems were solved?\", \"What was difficult?\", \"What did we learn?\", and \"What are the limitations and future results?\"\nAfter validating this work against our nested model, we concluded that this work presents an appropriate paper at the intersection of HCI and AI that meets the key requirements"}, {"title": "VI. DISCUSSION", "content": "We have discussed the potential threats and their validation methodologies, but we assume that the potential threats are a non-exhaustive list. There can be many potential threats and their validation methodologies that may exist at the given layer, and these potential threats and validation methodologies should be mapped to one of the layers of the nested model for Al design and validation.\nConsidering the need of the domain expert as an expert-in-the-loop, it is also possible that a single domain expert may not be able to list all the domain-specific requirements, model validation strategies, visualization techniques, etc., so we recommend multiple iterations of the nested model for Al design and validation. In addition, the XAI-QB should be kept in mind and the specific sub-type of the topic can be extended according to the requirements. For example, if a domain-specific data visualization is desired, then this question should be added to the XAI-QB in the data subtopic and then the appropriate methods should be used at the data level to address this question. The advantage of this framework is that it allows users to evaluate the AI workflow in human terms in collaboration with XAI-QB, which is an algorithmic question bank.\nIn addition, there may be questions such as \"Sometimes we don't know the problem with the data until we start training\". One answer is that we are still at the data level because the issue with the data is still there in the training of the model. After the training of the model, it reflects that there is a problem with the data, then technically you are using the ML model to analyze the data for more potential threats, this will act as a methodology for identification of unknown or more potential threats. Accordingly, a question should be added to the X\u0391\u0399-QB under the data subtopic that asks, \"How do you identify unknown potential threats with the data that are not identifiable by normal bias identification methods?\u201d The possibility of unstated biases is an open question, and some may be implicit threats.\nThe process of navigating the nested model for AI enables validation. We name this process as \"eliminating the potential threats of one layer through the functionality of the next layer\". Successfully navigating through each layer of the nested model layer for AI means that one has successfully resolved the threats present at each layer.\nAnother important point is that the regulations don't clearly differentiate between the terms used for ethical and technical key requirements. For example, technical robustness could mean both robustness in terms of infrastructure and robustness in terms of AI or decision-making. So we assume that the requirements are specified from both technical and ethical perspectives. Our nested model for AI design and validation is the only way to design the AI workflow and meet the listed key requirements according to the regulations. Our model addresses the regulations through XAI-QB and HCI, allowing AI to be evaluated in human terms.\nWhile we have not listed the full list of algorithms or methods to address every possible potential threat, we argue that the nested model for AI is sufficient to warrant a rethink at each step of the conceptualization, design, and validation steps. The speed at which AI regulations will impact infrastructure, and the need to quickly comply with regulatory requirements, justifies the regulatory layer as an overarching gateway to validate AI systems and prepare them for the real world. The threats to validity may vary depending on the choice of algorithms, and the potential threats may vary from use case to use case; our advice is to iterate the nested model for Al as often as necessary.\nOne could argue that questioning is not sufficient. However, formulating and asking the right questions brings up potential problems and assumptions from regulation all the way to predictions of an AI system. If a certain question or questions cannot be answered truthfully, or without reasonable doubt, the question(s) shall remain open to show transparency and potential pitfalls along every subsequent layer of the nested model for AI design and validation. Naturally certain questions require a hard limit, so to not proceeded to subsequent layers. This validation is meant to resolve any downstream problems when developing AI systems."}, {"title": "VII. CONCLUSION", "content": "In conclusion, our work is the first to bring together the disparate fields of AI and regulation. Our work establishes a common baseline for designing and validating AI under the umbrella of regulation, taking into account the complexity of the domain and the need for domain experts. Through this work, we are addressing discussions around the need to work at the intersection of AI and regulation to increase adoption in high-stakes domains such as healthcare. Adopting the nested model for AI design and validation will help realize the full potential of today's state-of-the-art AI techniques in a complex, vast, but interconnected and globalized world."}]}