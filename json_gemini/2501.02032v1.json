{"title": "Dynamic Feature Fusion: Combining Global Graph Structures and Local Semantics for Blockchain Fraud Detection", "authors": ["Zhang Sheng", "Liangliang Song", "Yanbin Wang"], "abstract": "The advent of blockchain technology has facilitated the widespread adoption of smart contracts in the financial sector. However, current fraud detection methodologies exhibit limitations in capturing both global structural patterns within transaction networks and local semantic relationships embedded in transaction data. Most existing models focus on either structural information or semantic features individually, leading to suboptimal performance in detecting complex fraud patterns.In this paper, we propose a dynamic feature fusion model that combines graph-based representation learning and semantic feature extraction for blockchain fraud detection. Specifically, we construct global graph representations to model account relationships and extract local contextual features from transaction data. A dynamic multimodal fusion mechanism is introduced to adaptively integrate these features, enabling the model to capture both structural and semantic fraud patterns effectively. We further develop a comprehensive data processing pipeline, including graph construction, temporal feature enhancement, and text preprocessing. Experimental results on large-scale real-world blockchain datasets demonstrate that our method outperforms existing benchmarks across accuracy, F1 score, and recall metrics. This work highlights the importance of integrating structural relationships and semantic similarities for robust fraud detection and offers a scalable solution for securing blockchain systems.", "sections": [{"title": "I. INTRODUCTION", "content": "BLOCKCHAIN technology has developed rapidly in recent years and has triggered far-reaching changes in several fields, especially in the financial industry [1]. However, as the popularity of blockchain applications grows, so does the significant increase in fraudulent behaviors it has brought about, with serious implications for society [2]. Blockchain technology, due to its decentralization and transparency, has become a tool for unscrupulous individuals to exploit, although it provides greater security and efficiency in financial transactions [3]. For example, the application of blockchain technology in the supply chain is seen as an effective means to enhance transparency and traceability, but it also faces a crisis of social trust due to fraudulent behavior [4]. In addition, the increase in fraudulent and illegal activities poses new challenges to the global economy as blockchain expands and its applications grow, especially in high-risk financial transactions [5]. Therefore, despite its enormous potential, blockchain technology comes with social and regulatory issues that need to be addressed to ensure its safe and sustainable development [6].\nAs one of the most widely used blockchain platforms [7], Ethereum's fraud detection methods are constantly evolving to cope with a variety of complex frauds. Currently, commonly used methods include Graph Neural Networks (GNNs), deep learning models, and machine learning-based classification algorithms. These methods excel at handling complex transactional network relationships and identifying suspicious accounts. For example, GNNs can effectively capture structured information in transaction networks [8], while deep learning models based on Long Short-Term Memory (LSTM) networks excel at processing time series patterns in transaction data [9]. However, current research has some limitations, especially in the following two aspects:\n1) Local semantic similarity information: In blockchain, there is usually some correlation between the transaction data of an account [10]. For example, a phishing account may make a large number of transactions in a short period of time, where the transactions before and after an account in a certain time period have specific semantic correlations. However, it is difficult for existing methods to fully capture and utilise this local semantic similarity information, which may lead to insufficient accuracy in fraud detection.\n2) Global transaction account network information: Fraudulent and normal accounts often imply important global information in the transaction network graph in the blockchain [11]. Although some graph models are able to recognise these patterns, effectively combining them with local semantic similarity information remains a challenge. This lack of combination limits the performance of existing methods in complex fraud detection.\nIntegrating useful information is a highly promising direction to address the above issues [12]. In this study, we propose a deep learning framework with multimodal fusion for fraud detection in blockchain transaction data. Compared with traditional methods, the proposed approach effectively captures both global structural relationships in transaction networks and local semantic patterns embedded in transaction records, achieving higher accuracy and robustness in detecting complex fraud behaviors.\nSpecifically, we first construct a global account interaction graph to represent the relationships between blockchain transaction accounts. Each node in the graph corresponds to an account, while the edges capture the transaction behaviors, such as frequency, transaction value, and temporal patterns. To extract meaningful structural features from this graph, we employ graph-based representation learning, which aggregates information from neighboring accounts to capture both direct and indirect relationships within the transaction network. This step enables the model to uncover global interaction patterns that are indicative of fraudulent behaviors.\nSimultaneously, we process the semantic information embedded in transaction data using a pre-trained text representation model. The model converts textual descriptions, such as transaction amounts, smart contract details, and other metadata, into high-dimensional feature vectors. This process allows the model to identify local contextual relationships, such as recurring transaction patterns or anomalous textual characteristics associated with suspicious accounts.\nTo effectively leverage both structural and semantic insights, we propose a dynamic feature fusion mechanism that adaptively integrates these two feature spaces. The mechanism learns to balance global network structures and local transaction semantics based on their relative importance for each transaction, enabling the model to detect subtle and complex fraud patterns with high accuracy.\nBy combining these complementary perspectives\u2014global structural relationships and local semantic features\u2014our approach significantly improves the robustness and precision of fraud detection. Experimental results on real-world blockchain datasets demonstrate that the proposed ETH-GBERT model achieves state-of-the-art performance. Specifically, on the Multigraph dataset, the model achieved an F1 score of 94.71%, significantly outperforming the best-performing baseline (Role2Vec, F1 score of 74.13%), with an improvement of 20.58%. On the Transaction Network dataset, ETH-GBERT achieved an F1 score of 86.16%, representing a substantial enhancement over the next best model (Role2Vec, F1 score of 71.39%). On the B4E dataset, the model demonstrated superior recall (89.57%) and precision (90.84%), far surpassing other baseline models, whose F1 scores did not exceed 74.25%. These results highlight the model's effectiveness in capturing complex fraud patterns, its robustness in handling imbalanced data distributions, and its ability to integrate structural and semantic features dynamically. The corresponding code link is available at https://github.com/dcszhang/Dynamic_Feature\nThe main contributions of this study are as follows:\n1) A dynamic multimodal fusion model is proposed, which innovatively combines graph structure information with text semantic similarity information to enhance the fraud detection performance in blockchain smart contracts.\n2) A complete set of data processing flow is developed, including the extraction of transaction data, the generation of adjacency matrix, and the processing of text representation based on BERT, which provides a useful reference for other blockchain applications.\n3) The effectiveness of the proposed method is verified through experiments, and the results show that the method performs well in detecting complex frauds and significantly outperforms existing benchmark models."}, {"title": "II. RELATED WORK", "content": "In recent years, with the rapid development of blockchain technology, the frequent occurrence of fraud in blockchain networks has become a global challenge. Researchers and developers have developed a variety of fraud detection methods to address these challenges and ensure the security and reliability of blockchain systems [13]."}, {"title": "A. Graph-based Fraud Detection", "content": "In blockchain networks, transaction data usually has a complex relational structure, and graph-based models can effectively capture these complex relationships and excel in fraud detection. Especially in blockchain platforms like Ethereum, Graph Neural Networks (GNNs) are widely used to detect fraud. For example, Tan [14] proposed a model based on Graph Convolutional Networks (GCNs) for detecting fraud from Ethereum transaction records. They classified addresses as legitimate or fraudulent by constructing a transaction network and extracting node features. In addition, Kanezashi [15] investigated the application of Heterogeneous GNNs (Heterogeneous Graph Neural Networks) in Ethereum transaction networks, focusing on handling large-scale networks and the label imbalance problem. Li [16] also proposed a phishing detection framework called PDGNN, based on the Chebyshev-GCN, which can detect fraud in Ethereum transaction networks by extracting transaction subgraphs and training a classification model, effectively distinguishing normal accounts from phishing accounts in large-scale Ethereum networks. Wang [17] proposed the Transaction SubGraph Network (TSGN) framework to enhance phishing detection in Ethereum by constructing transaction subgraphs that capture essential features of transaction flows. Hou [18] proposed an Ethereum phishing detection method based on GCN and Conditional Random Field (CRF). This method first utilizes DeepWalk to generate initial features for each account node in the transaction graph, then employs GCN to learn graph-structured representations, capturing the transactional relationships between accounts. To enhance classification performance, a CRF layer further encourages similar nodes to learn similar representations."}, {"title": "B. Fraud Detection Based on Time Series Data", "content": "Time series data analysis plays an important role in blockchain fraud detection, especially in processing transaction records and detecting abnormal behaviours. Ethereum, as one of the major blockchain platforms, contains a large amount of time-series information, such as transaction time, frequency, and Value fluctuations, which can be used to identify potential fraudulent behaviours. Hu [19] investigated the application of time-series analysis methods based on the Long Short-Term Memory (LSTM) network in Ethereum smart contracts. Another study by Farrugia [20] proposed the use of the XGBoost model combined with time series features for illegal account detection in Ethereum. The study highlighted the importance of time series features, such as time intervals, in identifying illegal accounts by extracting key time series features and combining them with a machine learning model. Pan [21] proposed a system called EtherShield, which combines time interval analysis and contract code features to detect malicious behaviour on the Ethereum blockchain."}, {"title": "C. Hybrid Methods", "content": "Hybrid methods integrate various types of information, such as graph data, time-series data, and semantic information, achieving higher detection accuracy and robustness, effectively identifying complex and dynamic fraud patterns in Ethereum malicious transaction detection. Li [22] proposed the Temporal Transaction Aggregation Graph Network (TTAGN) for Ethereum phishing detection, utilizing temporal transaction data to improve accuracy. TTAGN combines temporal edge representation, edge-to-node aggregation, and structural enhancement to capture transaction patterns and network structure, outperforming existing methods on real-world datasets. Wen [23] proposed a hybrid feature fusion model named LBPS for phishing detection on Ethereum, combining LSTM-FCN and BP neural networks. This model integrates features extracted through manual feature engineering and transaction records analysis, using BP neural networks to capture hidden relationships between features and LSTM-FCN networks to extract temporal features from transaction data. Chen [24] proposed the DA-HGNN model, a hybrid graph neural network with data augmentation for Ethereum phishing detection. This model utilizes data augmentation to address sample imbalance, integrates Conv1D and GRU-MHA to extract temporal features, and employs SAGEConv to capture structural features from the transaction graph."}, {"title": "III. METHODOLOGY", "content": "In this chapter, we describe in detail a dynamic multimodal fusion approach for blockchain transaction data fraud detection. The proposed method integrates graph-based representation learning to capture global relationships within transaction networks and semantic feature extraction to identify local contextual patterns from transaction records. By leveraging a dynamic feature fusion mechanism, the model effectively combines structural and semantic information to enhance its ability to detect complex fraud behaviors, as illustrated in Figure 1.This chapter includes the detailed steps of our approach, starting with data generation and preprocessing, followed by a comprehensive explanation of the model architecture and the training process used to optimize performance."}, {"title": "A. Data generation and pre-processing", "content": "In the processing of blockchain transaction datasets, each transaction record typically contains several fields, such as tag, from_address (sender address), to_address (recipient address), Value (transaction Value), and timestamp (transaction timestamp). These fields describe the transaction behavior, the time it occurred, and the parties involved. To more effectively analyze and model transaction relationships, we need to properly classify and reorganize the transaction data.\nSpecifically, we classify all transaction data by sender and recipient addresses, constructing a transaction record structure based on accounts. This classification step not only simplifies transaction storage and access but also lays the foundation for subsequent graph structure construction and local semantic analysis.\nEach transaction contains two account addresses, the sender (from_address) and the recipient (to_address). We classify transactions based on the sender's address (from_address), treating it as the transaction record of an account. Each transaction is labeled as an \"outgoing\" transaction, with the field in_out = 1. Similarly, when an account is the recipient, the transaction is labeled as an \"incoming\" transaction, with the field in_out = 0.\nThe classified transaction records are stored in a dictionary accounts, where the keys are account addresses and the values are lists of all transaction records for that account. Each list associated with an account contains all outgoing and incoming transactions related to that account. In this way, by separating and indexing transaction records by account, we can quickly retrieve the transaction history of any account, especially when analyzing account behavior patterns or transaction frequency."}, {"title": "1) Time Aggregation Feature Enhancement", "content": "To improve the information expression capability of transaction data in the time dimension, we particularly focus on the time aggregation characteristics of transactions during the data generation and preprocessing stages. By enhancing the time aggregation features, we can effectively capture some potential abnormal account behaviors, especially those accounts that engage in a large number of fund transactions within a short period [25]. These behaviors are often typical characteristics of phishing accounts, so analyzing and utilizing information in the time dimension is crucial for accurately detecting fraudulent activities.\nWhen processing the transaction data of each account, we first sort the transaction records based on the timestamp. The purpose of sorting is to ensure that the subsequent time difference calculations reflect the actual order of the transactions, providing foundational support for time aggregation features. By sorting the transactions in chronological order, we can capture the flow of funds in an account over a specific period and further analyze the frequency and density of its transaction behavior.\nTo quantify the degree of frequent transactions in a short time, we introduce the n-gram time difference feature. Specifically, the n-gram time difference measures the compactness of transaction times by calculating the time difference between a transaction and the previous $n-1$ transactions. We calculate the time differences for 2-gram to 5-gram, with the formula as follows:\n$\\Delta T_n = T_i - T_{i-(n-1)}$\nwhere $t_i$ denotes the timestamp of the $i$th transaction and $t_{i-(n-1)}$ denotes the timestamp of the $i\u2212(n-1)$th transaction for the account. If the number of transactions is not sufficient to calculate the n-gram, the time difference is set to 0.\nThe n-gram time difference feature allows us to capture patterns of frequent trading over short periods. For example, if an account makes multiple inbound and outbound trades within a few minutes, the n-gram time difference will be significantly smaller, and this temporal aggregation reflects the account's high frequency of trades over a short period, which is often closely associated with phishing behavior."}, {"title": "2) Graph Data Generation", "content": "To effectively capture the inter-account relationships in blockchain transaction data, we first construct a graph-based data structure to represent the transaction network. In this section, we use an adjacency matrix A to quantify the connection weights between accounts in the transaction network. The process of generating this graph representation involves the following steps:"}, {"title": "1) Creating the Zero Matrix", "content": "We first create a n x n zero matrix A, where n denotes the number of unique account addresses. This adjacency matrix is used to store the connection weights between different accounts. The elements of the matrix $A[i, j]$ denote the transaction weights between account i and account j.\n$A = O_{n \\times n}$"}, {"title": "2) Traversing Transaction Records", "content": "In order to populate the elements of the adjacency matrix, we need to iterate through all the transaction records $T_k$, where each transaction $T_k$ contains a sender from_addressk and a receiver to_addressk. We use an 'address_to_index' dictionary to map these account addresses to indices in the adjacency matrix.\n\u2022\tThe sender address is mapped as from_idx\n\u2022\tThe receiver address is mapped as to_idx\nThe formulaic representation is as follows:\nfrom_idx = address_to_index(from_addressk)\nto_idx = address_to_index(to_addressk)"}, {"title": "3) Calculating Transaction Weights", "content": "The weight of each transaction $w_k$ reflects the transaction Value and the complexity of the transaction behavior. In order to capture more temporal features, we introduce a weight calculation method based on the n-gram time difference. The weight of a transaction $w_k$ is obtained by weighted summation based on the n-gram time difference of the transaction. The formula for calculating the weights is as follows:\n$w_k = \\sum_{n=1}^N a_n \\cdot \\Delta t_{n,k}$\nAmong them:\n\u2022\t$a_n$ denotes the weight coefficient for each n-gram time difference;\n\u2022\t$\\Delta t_{n,k}$ denotes the n-gram time difference of the transaction $T_k$.\nIn addition, the transaction Value Valuek is also an important component of the weights, which we combine with the n-gram time difference to further adjust the weights of the transactions:\n$w_k = Value_k \\sum_{n=1}^N a_n \\Delta t_{n,k}$"}, {"title": "4) Populating the Adjacency Matrix", "content": "Once the weights of the transactions we are computed, we accumulate them to the corresponding positions in the adjacency matrix A[from_idx, to_idx]. This accumulation operation allows the weights of multiple transactions to be superimposed, reflecting the strength of the connections between accounts:\nA[from_idx, to_idx] += wk\nThis operation ensures that when multiple transactions occur between two accounts, the corresponding weights are accumulated in the appropriate elements of the adjacency matrix. This accumulation process effectively reflects both the frequency of transactions and the aggregate transaction values between accounts. The resulting adjacency matrix A serves as the input for graph-based representation learning, enabling the model to capture and analyze the global structural relationships within the transaction network."}, {"title": "3) Text Transaction Data Generation", "content": "In the transaction records of each account, the from_address, to_address and timestamp fields record the address information and timestamp of the account. Although these fields are important for transaction classification and temporal feature enhancement, they are not needed in text analysis, so we delete these fields before generating text data to simplify the data structure and retain key information such as transaction Value and label.\nTo prevent the model from relying too much on the sequential information of transactions in text processing, we randomly rearrange the transaction list for each account. This operation disrupts the backward and forward order of transactions, allowing the model to focus on the content features of transactions rather than time-dependent information, thus avoiding possible noise interference.\nFor example, the list of trades for account A is [T1, T2, T3] before disruption, and after random disruption may become [T2, T1, T3].\nNext, we tag each account with an overall tag. An account is labelled as fraudulent whenever there is a transaction in the account with tag = 1, i.e., the account is labelled with a tag of 1. This tag is given to the first transaction record of the account. To simplify the transaction logging, the tag information for the rest of the transactions is deleted and only the tag of the first transaction is retained. This is because even if only one transaction in the account is related to fraud, the account itself may be potentially risky and may even be used for wider fraudulent activity. Typically, phishing accounts tend to mask their malicious behaviour by disguising multiple normal transactions. Therefore, in order to ensure the security and effectiveness of fraud detection, we have adopted more stringent criteria to ensure that the model can identify potentially high-risk accounts and prevent them from engaging in further illegal transactions. This labelling approach can help the model learn the risk characteristics of the accounts more accurately and improve the overall detection effectiveness.\nWhen generating text data, we process the transaction records of each account and convert them into a single line of descriptive text. The key fields of each transaction (e.g., label tag, transaction value, etc.) are combined to create a compact textual representation that encapsulates the transaction information for the corresponding account. This step produces the raw text corpus, which serves as input for subsequent semantic feature extraction through a pre-trained text representation model.\nFor example, a transaction record might be converted into the following form:\nAccount A: tag = 1, Value = 5.06854256, ..."}, {"title": "4) Text Data Cleaning", "content": "After generating the textual transaction data, further pre-processing steps are applied to ensure compatibility with the input format required for the downstream semantic representation model. These steps include reading the generated TSV files, tokenizing the text into subword units, and transforming it into a format suitable for deep learning-based training.\nWe first read the generated Train.tsv and dev.tsv files, which contain the processed training set and validation set data. To ensure that the models are exposed to diverse data distributions during training, we randomly disrupt the data order to avoid overfitting the models to a specific data order. In addition, the test set data was read from test.tsv and similarly randomly disrupted.\nAfter reading and shuffling the data, the training, validation, and test sets were combined into a unified data frame. From this, two key columns were extracted: the transaction text description (corpus) and the account label (y). The transaction text description captures the account's transaction behavior, while the label indicates whether the account is associated with fraudulent activity. This operation produces the input corpus and the corresponding supervisory signals (labels) required for subsequent semantic feature extraction and model training.\nThis was followed by a tokenization process to segment each document into a series of tokens (sub-word units), which were then normalized and encoded as necessary. This step ensures that the transaction text is transformed into a format suitable for semantic representation models, resulting in sequences of token IDs. These token IDs serve as inputs to the embedding layer of the text processing model for subsequent training. To ensure robustness, the order of documents is intentionally shuffled, exposing the model to unordered and varied inputs during training. Additionally, the labeled data y is aligned with the tokenized sentences and used as supervisory signals for the supervised learning process.\nThe dataset generated in the above steps contains global transactional relationships and local transactional semantic information, providing multimodal input for subsequent model training."}, {"title": "B. ETH-GBERT Model Architecture", "content": "To address the challenge of detecting complex fraudulent activities in blockchain transactions, we propose the ETH-GBERT Model, a deep learning framework designed to simultaneously capture global structural relationships and local semantic similarities. While transaction networks contain rich global patterns that reflect account interactions, transaction records hold local contextual details that can signal fraudulent behaviors. Existing methods often focus on one aspect, failing to leverage the complementary strengths of both.\nIn this study, we adopt Graph Convolutional Networks (GCNs) to capture the global transaction relationships embedded in account interaction graphs. GCNs are particularly suited for extracting structural features from graph-based data, making them ideal for modeling the relationships in blockchain transaction networks. Simultaneously, we use a pre-trained BERT model to analyze the local semantic features present in transaction text data, effectively capturing the contextual meaning and subtle patterns in transaction details.\nBy integrating these two components through a multimodal fusion mechanism, the ETH-GBERT Model combines insights from both global structural features and local semantic representations to enhance fraud detection performance. The following sections provide a detailed explanation of the architecture and design of the ETH-GBERT Model components."}, {"title": "1) Model Architecture", "content": "The ETH-GBERT Model integrates two core modules: GCN for dealing with trading account graphs and BERT for dealing with trading text data. Overall, it can be divided into the following parts:\n1) Graph-Based Representation Module: Primarily captures global relationships within the transaction network. Through the GCN layers, the relationships between transaction accounts are convoluted, generating node embeddings (account embeddings) with global semantic information.\n2) Semantic Feature Extraction Module: Extracts local semantic information from transaction text data. The BERT model deeply represents the transaction records for each account and generates high-dimensional text embeddings.\n3) Multimodal Fusion: The GCN-generated global account embeddings and BERT-produced local text embeddings are fused, forming a multimodal embed vector. This fusion enables the model to take advantage of both the transaction network structure and the text features for fraud detection.\n4) Classifier: The fused embedding vector is passed through a fully connected layer for classification, outputting predictions to determine whether the account is related to fraudulent behavior."}, {"title": "2) Graph-Based Representation Module Design", "content": "Adjacency Matrix Input. The input to the GCN module is the adjacency matrix A of the transaction account graph, where the element A[i, j] represents the transaction weight between the account i and the account j. This adjacency matrix is obtained from the graph data generation steps described earlier, incorporating transaction amounts and time features.\nGraph Convolution Layer (GCN Layer). In the GCN module [26], the transaction account graph undergoes feature extraction through multiple graph convolution layers. The convolution operation in each layer is represented by the following formula:\n$H^{(l+1)} = \\sigma (\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} H^{(l)} W^{(l)})$\nwhere:\n\u2022\t$H^{(l)}$ represents the node feature matrix at the l-th layer (account embedding matrix), and the initial $H^{(0)}$ is the initial feature of the transaction accounts;\n\u2022\t$\\tilde{A} = A + I$ is the adjacency matrix with self-loops;\n\u2022\t$\\tilde{D}$ is the degree matrix of the adjacency matrix;\n\u2022\t$W^{(l)}$ is the weight matrix at the l-th layer;\n\u2022\t$\\sigma$ is a non-linear activation function, such as ReLU. Through multiple convolution operations, the model aggregates the global information of the transaction network layer by layer, eventually generating node embeddings with global transaction relationships."}, {"title": "3) Semantic Feature Extraction Module Design", "content": "Text Input and Initial Embeddings. The input to the BERT module is the transaction text data. After being cleaned and tokenized, the text data is converted into token sequences. These token sequences are embedded using BERT's Word Embedding, Position Embedding, and Token Type Embedding layers [27]:\n$E_{BERT} = E_{word} + E_{position} + E_{token\\_type}$\nwhere:\n\u2022\t$E_{word}$ represents word embeddings;\n\u2022\t$E_{position}$ represents position embeddings to capture the positional relationships of words in the sequence;\n\u2022\t$E_{token\\_type}$ represents token type embeddings to differentiate between different sentences or paragraphs.\nBERT Encoding Layer. The embedded text is passed through BERT's multi-layer Transformer encoder, generating higher-level text representations. BERT's self-attention mechanism effectively captures the dependencies between different words in the transaction text, thus extracting local semantic similarity information.\nThe encoding operation of the Transformer [28] can be represented as follows:\n$H_{fusion} = Transformer(E_{Fused})$\nwhere:\n\u2022\t$E_{Fused}$ is calculated as presented in Section III-B4.\n\u2022\t$H_{fusion}$ is the text embedding generated by BERT."}, {"title": "4) Multimodal Fusion", "content": "In the multimodal fusion stage of the model, we introduce a dynamic feature fusion mechanism inspired by DynMM [29], which adaptively determines the contributions of BERT and GCN embeddings for each input instance.\nFusion Strategy. Our approach employs a gating network G(x) to generate instance-specific fusion weights. This allows the model to dynamically decide how much information to extract from BERT embeddings and BERT-enhanced GCN embeddings. Specifically, three fusion strategies are considered:\n\u2022\tBERT-only embeddings $E_{BERT}$: Using textual information exclusively for prediction.\n\u2022\tGCN-enhanced BERT embeddings $E_{GCN\\_Enhanced}$: GCN embeddings that integrate structural graph information and are enhanced with contextual features from BERT.\n\u2022\tA weighted combination of BERT and GCN embeddings:\n$E_{Fusion} = \\alpha E_{BERT} + (1 - \\alpha) \\cdot E_{GCN\\_Enhanced}$ where $\\alpha$ is a learnable parameter initialized to 0.5.\nDynamic Weight Calculation. The gating network G(x) takes as input the concatenated features [$E_{BERT}, E_{GCN\\_Enhanced}$] and outputs fusion weights $g = [g_1, g_2, g_3]$ corresponding to the three fusion strategies:\n$g_i = \\frac{\\exp \\left(\\left(\\log G(x)_i + b_i\\right) / \\tau\\right)}{\\sum_{j=1}^3 \\exp \\left(\\left(\\log G(x)_j + b_j\\right) / \\tau\\right)}, \\quad i \\in \\{1,2,3\\}$, where $b \\sim Gumbel(0,1)$ is Gumbel noise, and $\\tau$ is the temperature parameter controlling the smoothness of the output. When $\\tau$ is large, the output weights $g_1, g_2, g_3$ are smooth and close to a soft probability distribution, enabling a soft fusion of the three strategies. As $\\tau$ decreases, the output distribution becomes sharper, and when combined with the hard option, the weights are discretized into a one-hot vector through the straight-through estimator, effectively selecting a single fusion strategy while maintaining differentiability.\nTo handle varying task complexities and data characteristics, the gating network G(x) can be implemented using different architectures, such as Multi-Layer Perceptrons (MLPs), Transformer layers, or convolutional networks.\nIn this work, we implement the gating network as a Multi-Layer Perceptron (MLP), consisting of two fully connected layers with a ReLU activation function.\nThe final fused embedding $E_{Fused}$ is obtained as:\n$E_{Fused} = g_1 E_{BERT} + g_2 E_{GCN\\_Enhanced} + g_3 \\cdot E_{Fusion}$.\nAdaptive Fusion Mechanism. This dynamic fusion mechanism enables the model to adapt its computational resources and fusion strategy based on the input complexity:\n\u2022\tFor easy inputs, the gating network assigns higher weights to simpler strategies such as $E_{BERT}$ or $E_{GCN\\_Enhanced}$, reducing computational costs.\n\u2022\tFor complex inputs, the gating network increases the contribution of the weighted combination $E_{Fusion}$, allowing the model to effectively integrate information from both modalities.\nCompared to traditional static fusion approaches, our method fully and efficiently leverages the complementary strengths of BERT and GCN embeddings. By dynamically adjusting the fusion strategy for each input, the proposed approach achieves a better balance between computational efficiency and representation power."}, {"title": "5) Classifier Design", "content": "The fused multimodal embedding vector Hfusion is input into a fully connected layer for the classification task. Through the Softmax layer, the model outputs the probability of whether an account is related to fraudulent behavior:\n$y = Softmax(W_{fusion} H_{fusion} + b_{fusion})$ where Wfusion and bfusion are the weight matrix and bias vector of the classifier, respectively.\nETH-GBERT Model enhances the joint learning of global relationships and local semantic information in blockchain transactions through the fusion of GCN and BERT embeddings. Through multimodal fusion, the model improves its ability to detect complex fraudulent behaviors effectively."}, {"title": "IV. VALIDATION", "content": "As shown in Table 1, we use three datasets as follows.\n1) Ethereum Phishing Transaction Network: This dataset is publicly available and is provided by Chen et al. (2021). The dataset is obtained by performing second-order breadth-first search (BFS) from known phishing nodes over a large-scale Ethernet transaction network. The dataset contains 2,973,489 nodes, 13,551,303 edges, and 1,165 phishing nodes [30].\n2) First-order Transaction Network of Phishing Nodes: This dataset was collected by Wu et al. (2022) through Ethernet nodes. It includes 1,259 phishing accounts and an equal number of normal accounts. The first-order neighbours of each account and the transaction edges between them are also included in the dataset, and the subnetwork contains about 60,000 nodes and 200,000 transaction edges [31].\n3) BERT4ETH: We use the BERT4ETH dataset provided by Hu et al. (2023), which is generated from sequences of Ether transactions and contains subsets of phishing accounts, de-anonymised data (ENS and Tornado Cash), and ERC-20 token logs.The BERT4ETH dataset is able to capture multi-hop relationships between trading accounts and is is suitable for phishing account detection and account de-anonymisation tasks. This dataset is an important component of our experiments and helps to further evaluate the performance of the model [32]."}, {"title": "B. Baseline", "content": "In this experiment, we selected three common categories of baseline models for comparison:\n1) Graph embedding methods based on random walks, including DeepWalk [33], Trans2Vec [31], Dif2Vec [34], and Role2Vec [35], [36];\n2) Graph neural network(GNN) models, including GCN [26], GraphSAGE [37], and GAT [38];\n3) BERT4ETH, a model designed specifically for fraud detection on Ethereum [32].\nDeepWalk generates node sequences through random walks on the graph and employs the skip-gram model to learn low-dimensional representations of nodes. Trans2Vec builds on DeepWalk by incorporating transaction heterogeneity and temporal features, designed specifically for detecting phishing accounts in the Ethereum network. Dif2Vec adjusts the sampling probabilities of nodes during random walks to enhance the diversity of embeddings by increasing the sampling of low-degree nodes. Role2Vec learns structural roles of nodes rather than focusing solely on proximity relationships, generating more generalizable embeddings.\nRegarding GNN-based models, GCN aggregates the features of neighboring nodes via convolution operations to learn node representations, making it suitable for tasks such as node classification. GraphSAGE generates new node embeddings by sampling and aggregating the features of neighboring nodes, which enables it to handle large-scale graph data. GAT introduces an attention mechanism, dynamically assigning weights to each node's neighbors to aggregate node information more effectively."}, {"title": "V. PREPROCESSING AND TRAINING SETTINGS", "content": "In this section, we describe the ETH-GBERT preprocessing setup, initial parameters, loss function, and evaluation metrics used in our experiment."}, {"title": "A. Data Loading and Preprocessing", "content": "Before training, the dataset was split into training, validation, and test sets, accounting for 80%, 10%, and 10% of the total data, respectively. We used PyTorch's DataLoader to load the data in mini-batches, with shuffling applied during the training process. The training set is used to update model parameters, the validation set evaluates the model's generalization ability, and the test set is used for final performance evaluation."}, {"title": "B. Hyperparameter Settings", "content": "The following hyperparameters were set during the model training:\n\u2022\tLearning rate: The initial learning rate was set to $8 \\times 10^{-6}$, and a learning rate scheduler was employed to adjust the learning rate dynamically.\n\u2022\tRegularization coefficient: L2 regularization was applied with a coefficient of $\\lambda = 0.001$ to prevent overfitting.\n\u2022\tBatch size and gradient accumulation: The batch size was set to 32. We adopted gradient accumulation to save memory, updating the model's parameters after every 2 mini-batches.\n\u2022\tEpochs: The total number of training epochs was set to 40 for initial validation of the model's effectiveness."}, {"title": "C. Loss Function and Optimizer", "content": "We used the cross-entropy loss function for the classification task [39", "as": "n$L = -\\frac{1}{N} \\sum_{i=1}^{N}\\left(y_i \\log (p_i) + (1 - y_i) \\log (1 - p_i)\\right)$ where"}]}