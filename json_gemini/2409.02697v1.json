{"title": "Decision Transformer for Enhancing Neural Local Search on the Job Shop Scheduling Problem", "authors": ["Constantin Waubert de Puiseau", "Fabian Wolz", "Merlin Montag", "Jannik Peters", "Hasan Tercan", "Tobias Meisen"], "abstract": "The job shop scheduling problem (JSSP) and its solution algorithms have been of enduring interest in both academia and industry for decades. In recent years, machine learning (ML) is playing an increasingly important role in advancing existing and building new heuristic solutions for the JSSP, aiming to find better solutions in shorter computation times. In this paper we build on top of a state-of-the-art deep reinforcement learning (DRL) agent, called Neural Local Search (NLS), which can efficiently and effectively control a large local neighborhood search on the JSSP. In particular, we develop a method for training the decision transformer (DT) algorithm on search trajectories taken by a trained NLS agent to further improve upon the learned decision-making sequences. Our experiments show that the DT successfully learns local search strategies that are different and, in many cases, more effective than those of the NLS agent itself. In terms of the tradeoff between solution quality and acceptable computational time needed for the search, the DT is particularly superior in application scenarios where longer computational times are acceptable. In this case, it makes up for the longer inference times required per search step, which are caused by the larger neural network architecture, through better quality decisions per step. Thereby, the DT achieves state-of-the-art results for solving the JSSP with ML-enhanced search.", "sections": [{"title": "I. INTRODUCTION", "content": "Effective and efficient scheduling presents an ongoing challenge that is critical for the success in many sectors, from manufacturing [1, 2] to cloud computation [3]. Scheduling, broadly, deals with the allocation of resources to tasks over time with the goal of optimizing a given objective [4]. The job shop scheduling problem (JSSP) is an abstracted combinatorial scheduling problem that underlies many real-world problems and has been extensively studied in the literature. To this day, new solution methods for scheduling problems are developed and tested on the JSSP due to its interesting properties and the availability of popular public datasets that allow for rigorous benchmarking [5-7].\nIn recent years, machine learning has emerged as a promising technique for new solution methods for scheduling problems by enhancing or replacing heuristic decisions in existing dispatching rules [8\u201312], metaheuristics and search heuristics [13-16], and optimal solvers [17] to varying degrees. A notable advancement in this area is the integration of deep reinforcement learning (DRL) with local search algorithms. For instance, Falkner et al. [18] proposed an approach wherein a DRL agent is trained to learn three critical components of a local search algorithm: solution acceptance, neighborhood operators, and perturbation decisions. This ML-enhanced search significantly outperformed state-of-the-art search methods. Meanwhile, transformer architectures [18] are nowadays deployed at scale and achieve breakthrough results in various domains, most publicly known in natural language processing [19], by capturing and processing important features in sequential data. Aiming at harnessing the strength of sequential data processing of transformers, decision transformers (DTs) have recently been introduced for learning strategies in sequential decision-making processes from simulation data [20].\nIn this work, we integrate the DT with the NLS approach by learning from data generated during the local search process of trained NLS models. Using the DT in combination with NLS comes with two potential benefits over the original NLS algorithm: First, by considering a history of past actions, the DT can learn how previous decisions have influenced the current problem instance and adjusts its strategy accordingly. Second, the DT is trained to align its actions with a manually set reward prior. This can lead to performance improvements over teacher algorithms \u2013 in our case, the already highly competitive NLS models.\nThe main contributions of this paper are:\n\u2022 The introduction of a decision transformer approach to boost the performance of a learned local search heuristic on the JSSP.\n\u2022 An experimental analysis of both the performance and the learned behavior of the model with respect to the teacher models.\nThe remainder of this work is structured as follows: first, the JSSP and general solution approaches are introduced in section II. Here, we also provide a brief introduction to DRL, DTs, and the neural neighborhood search algorithm that jointly build the basis of our work. Next, related work is discussed in section III before our methods are detailed in section IV. We present the achieved results in section V, and a detailed comparison between the teacher and DT in terms of practical implications and learned behavior is provided in section VI. Section VII gives a conclusion and an outlook for future work."}, {"title": "II. PRELIMINARIES", "content": "The classical, abstract version of the JSSP considered in this paper deals with the allocation of a set of i jobs Ji, each comprising j operations Oj with processing times Pij, to m machines Mm over time, with the objective of minimizing the maximum completion time Cmax - that is, the duration from the initiation of the first operation to the completion of the last one, commonly referred to as the makespan. Operations within each job underlie precedence and no-overlap constraints, such that any job visits each machine exactly once in a predefined order. Accordingly, the number of operations j and the number of machines m is equal in this setup. In addition, each machine may only process one job at a time. In terms of notation, we refer to problem instances with j jobs and m machines as instances of size j x m. For example, a 20x15 problem instance consists of 20 jobs, each with 15 operations on 15 machines in total. The described JSSP problem is NP-hard, making enumerative algorithms for finding optimal solutions to large instance sizes impractical in real-world applications."}, {"title": "B. Solution Methods for Job Shop Scheduling Problems", "content": "Solutions algorithms for the JSSP have been under active development and research for decades in the operations research domain. These algorithms are designed to achieve a balance among minimizing the objective (e.g., makespan), ensuring rapid execution times, and requiring minimal implementation effort.\nProvenly optimal solutions can be found by means of constraint programming solvers such as the CP-SAT solver by Google OR-Tools [21]. However, such solvers become impractical for large JSSP instances due to the exponential growth of the solution space that must be explored.\nA common category of solution methods in practice are priority dispatching rules (PDRs), in which dispatching and ordering decisions are governed by job prioritization rules such as \u201cshortest processing time first\u201d. A representative collection of such rules has been studied by Haupt et al. [22]. Although PDRs are straightforward to understand and implement in industrial settings, they often fail to produce competitive makespans when compared to other solution methods.\nAnother solution category comprises a variety of (meta-) heuristics, such as the shifting bottleneck heuristic [23], genetic algorithms [24] and heuristic search algorithms [25, 26]. Depending on the problem size and considered constraints, tailored metaheuristics have long been the de-facto state-of-the-art.\nWith the advent of more powerful machine learning algorithms, particularly deep learning techniques, the aforementioned solution methods have increasingly been either enhanced or partially replaced by data-driven approaches in recent years. Further details and significant developments related to this work are discussed in Section III, \"Related Work.\""}, {"title": "C. Deep Reinforcement Learning", "content": "DRL is a machine learning paradigm, in which an agent learns a parameterized policy, approximated by a deep neural network, through interaction with an environment. In each timestep t, the agent receives a representation St of the state of the environment, takes an action At which alters the state of the environment and receives a feedback signal Rt, called reward. The training process aims at maximizing the cumulated reward across an episode, \\(\\sum r_t\\), i.e., from the first interaction a0 to to the final interaction tfinal. A common way to learn a policy is using the Q-Learning algorithm. Q-values, or state-action-values, are assigned to each state-action pair and represent the total cumulative reward that can be achieved by taking action At in state St and subsequently following the learned policy. Once Q-values have been learned from experience, the optimal policy is derived by iteratively estimating the Q-values for all possible actions in each state of the episode and selecting the action that corresponds to the highest predicted Q-value.\nDRL can be applied to any sequential decision-making process that adheres to the Markov property, meaning that the state of the environment at any given time is independent of prior states. [27]"}, {"title": "D. Decision Transformers", "content": "The DT is an architecture for offline reinforcement learning, in which Markov decision processes are abstracted as sequence-modelling problems [20]. The DT has already been successfully applied in various domains such as active object detection in robotics [28], recommender systems [29] and chip placement optimization [30]. In traditional RL, models output (or evaluate) actions A given a representation of the current problem state S and receive a reward r in response. Hence, these sequences may be represented as lists of collected state-action-reward tuples, [{So, A0, ro}, {S1, A1, 11} ... {Sfinal, Afinal, Ifinal}]. In contrast, DTs are trained to output actions by considering as input the sequence of the last K states, actions, and the corresponding return-to-go values R. In this context, K is referred to as the context length. The return-to-go at a given time step t, denoted as Rt, is defined as the sum of the remaining rewards: \\(R_t = r_t + r_{t+1} + \\dots + r_{T-1} + r_T\\). At each time step t, the DT uses the sequence of past states, actions, and returns-to-go, along with the current state St and return-to-go Rt. This is formally expressed as follows:\n\n\n\n\\(A_t = DT(R_{t-k}, S_{t-k}, A_{t-k}, \\dots, R_t, S_t)\\).\n\n\n\n(1)\nIn other words, the DT learns to generate action sequences that aim to achieve the manually set target R (i.e., the prior) by minimizing the difference between R and the cumulated sum of rewards, instead of minimizing the cumulated sum or rewards directly."}, {"title": "E. Neural Local Search", "content": "NLS is a recent approach to control a local neighborhood search heuristic on the JSSP with a DRL agent [31]. The underlying local search (LS) heuristic first constructs an initial solution and then takes iterative steps to improve upon it. In each iteration, the heuristic:\n\u2022 accepts or declines the solution of the last iteration,\n\u2022 chooses a new neighborhood operation that defines the next neighborhood, i.e. the set of solutions to integrate in the search,\n\u2022 or chooses a perturbation operator to jump to a new area in the search space in which to continue the search.\nFalkner et al. [31] translate the heuristic decisions into actions that may be taken by a DRL agent, experimenting with three different action spaces that are defined by the following three sets of actions:\n1. Acceptance decisions: the decision whether the last LS step is accepted or not:\n\\(A_A := {0,1}\\)\n2. Acceptance-Neighborhood decision: tuple representing the above acceptance decision and the choice between four different neighborhood operations in the set \u03a6:\n\\(A_{AN} := {0,1} x \\Phi\\)\n3. Acceptance-Neighborhood-Perturbation decision: tuple representing the acceptance and neighborhood decisions plus a perturbation decision from the perturbation operator set \u03a8:\n\\(A_{ANP} := {0,1}x{\\Phi\\cup\\Psi}\\)\nThe neighborhood operators comprise: the Critical Time (CT) operator, which swaps adjacent nodes in critical blocks [32], the Critical End Time (CET) [33] operator, which swaps nodes at the start or end of a critical block, the Extended Critical End Time (ECET) [34] operator, which swaps nodes at both the start and end of a critical block, and the Critical End Improvement (CEI) [35] operator, which shifts a node within a critical block to a new position within the same block.\nAll actions are taken based on states that are derived from a learned representation consisting of the current problem instance to be solved and its current solution. Specifically, the authors employ an encoder-decoder architecture depicted in Fig. 2. The encoder generates a representation of the problem instance and its current solution using a graph neural network, which is then enriched with the following explicit state features: the current makespan, the best makespan found so far, the last acceptance decision, the last applied operator, the current time step, the number of consecutive steps without improvement, and the number of perturbations applied so far. Utilizing this enriched representation, the decoder computes Q-values that guide the selection of the next action. These output Q-values are learned with Double Deep Q-Learning [36] aiming to maximize the total improvement in makespan between the initial and final solution across the episode."}, {"title": "III. RELATED WORK", "content": "A plethora of deep learning based solution methods to the JSSP have been proposed in recent years [11, 37-42], many of which serve as learned construction heuristics. Construction heuristics iteratively generate a solution by starting with an empty schedule and, in each iteration, determining the next unscheduled operation of a job to be added to the schedule. Learned construction heuristics have shown very promising results for the JSSP, showcasing that deep learning models are capable of capturing the structure of the JSSP effectively. In this study, we refrain from applying the DT to construction heuristics for two reasons: firstly, although they perform well on the vanilla JSSP, the transfer to scheduling scenarios with additional constraints remains an open problem, where search heuristics are still dominant [14, 41]. Secondly, one of the advantages of the DT is that it can effectively take past search actions on the same instance into consideration. This strength cannot be leveraged for construction heuristics, in which all relevant information of past actions is summarized in the partially solved schedule, which cannot be altered retrospectively."}, {"title": "B. Learned Heuristic Search", "content": "In contrast to construction heuristics, in learned heuristic search, certain decision rules of search algorithms are replaced by neural network inferences. Typically, the search algorithms aim at improving an initial solution. Though the reported results on the JSSP are slightly worse than those of learned construction heuristics, learned heuristic search algorithms are more easily adaptable to problems with more real world constraints and objectives, because their action spaces remain constant [14]. This property makes them worth investigating, considering that most real-world use-cases are subject to multiple constraints."}, {"title": "C. Imitation Learning on the JSSP", "content": "In imitation learning scenarios, \u201cstudent\u201d-models learn to imitate the behavior in sequential decision making of a \"teacher\" in a supervised manner by following labels generated by the teacher instead of rewards. In the JSSP, teacher algorithms can range from dispatching rules to exact algorithms. The goal of this method is to imitate the behavior of the teacher and then either surpass it (e.g., a dispatching rule) or achieve faster computational inference times (e.g., compared to an exact algorithm). For example, Ingimundardottir and Runarsson trained a linear machine learning model on data generated with dispatching rules [43]. Rinciog et al., for a related scheduling problem, pre-trained a neural network on the earliest-due-date dispatching rule before fine-tuning it with DRL [44]. Although these approaches showed some success of imitation learning, the results are far from competitive in terms of the absolute results compared to other algorithms due to their weak teacher algorithms.\nIn contrast, Lee and Kim train a graph neural network on optimal solutions, but also do not reach a competitive performance [45]. Hypothesizing that optimal solutions provide noisy data from which it is difficult to learn, Corsini et al. [38] and Pirnay et al. [39] have instead very recently suggested to train a student model on its own most successful solutions generated during sampling. All imitation learning approaches mentioned here are either construction heuristics or online dispatchers. To the best of our knowledge, our study, by training a DT, is the first to address imitation learning for a learned heuristic search method."}, {"title": "IV. METHODS", "content": "In the following, we describe how the DT and NLS are conceptually combined into one trainable algorithm for the JSSP and which specific design choices were taken. The descriptions of our method are structured along the three necessary steps \u201cDataset Generation\u201d, \u201cDT-Training\u201d and \u201cDT-Testing\", which are conceptually visualized in Fig. 3."}, {"title": "A. Dataset Generation", "content": "Training Instances To train the DTs in a supervised manner, we must create suitable labeled datasets. To this end, we randomly generate 1000 problem instances for each problem size in the Taillard benchmark dataset according to the same reported specifications [5]. We then collect the state-action-reward tuples from local search trajectories on these instances with 100 and 200 search iterations generated by the interaction of the teacher NLS models with the NLS environment. 100 and 200 are in the range of iteration steps investigated in the original publication [31]. The collected information form preliminary raw labeled datasets.\nTeacher Models The teacher models are pretrained NLS checkpoint models, available online in the original repository [46], for each problem size. We do this such that the DTs can learn from the successful learned solution strategies of the teachers. Note that the input to the DT, unlike that for the teacher models, is not only the state but also the return-to-go. We therefore expect the DT to be able to surpass the teacher models' performance during inference by giving a suitable (challenging) return-to-go prior. A small value would indicate that the DT should act to achieve a small makespan, while a large value should have the opposite effect.\nWithin the original NLS models which are suitable as teachers, specific action sets were most successful for different problem sizes."}, {"title": "B. DT-Training", "content": "Training Procedure The DT model is trained for 500 epochs to minimize the categorical cross entropy loss between the predicted action apred and the label action a. Preliminary experiments with varying hyperparameters on the 15x15 JSSP indicated that a context length K between 50 and 200 was most effective. To balance effectiveness and model size K = 50 is chosen. The Adam optimizer [48] is used in combination with cosine learning-rate-decay to smoothly reduce the overall learning rate, with periodical increases, back to the initial value of 6e-2. Unlike the NLS model, the DT generates an action based on the last K state-action-reward tuples. Given that the dataset contains only single tuples, a buffer is used that returns the last K tuples depending on the current iteration step. For the iteration steps that are smaller than the context length K, the remaining tokens are zero-padded and masked within the self-attention mechanism. We test our model every 33 epochs on 30 JSSP instances of the respective size, generated with a different random seed than the training and evaluation instances, and save the weights whenever the achieved test makespan improved.\nModel Architecture and Hyperparameters Our architecture combines components of the original NLS model and our DT implementation. We use the learned latent space representation generated by the aggregator component in the NLS architecture as state embeddings. The original pre-trained checkpoint model weights, which were also used during data generation, were taken from [46].\nOur DT transformer implementation is based on the minGPT model by Karpathy [49], from which most hyperparameters were adopted (see Table 2). The DT transformer replaces the decoder part of the original models to generate the action distribution in the output layer, as shown in Fig. 4. State embeddings, actions and rewards are all projected to the same embedding dimension of the tokens through linear layers. The resulting sequence of tokens is then fed into the first multi-head attention layer of the transformer. In contrast to the original implementation for textual data, our positional embedding is not created in relation to the block size but considers the maximum trajectory length of the problem, i.e., the number of iteration steps."}, {"title": "C. DT-Testing", "content": "Augmented Return-to-Go As described earlier, the original NLS environment returns the clipped relative makespan improvement before and after the action is executed as reward signal, starting at step zero with an initially created schedule.\nThis presents a challenge during inference with the DT that requires a return-to-go value as input. Ideally, in step zero, the return-to-go Ro would be the difference between the makespan of the initial schedule minit (obtained from the FDD/MWKR dispatching rule) and the makespan of the optimal solution moptimal: \\(R_0 = m_{init} - m_{optimal}\\). However, moptimal is usually unknown prior to solving it. Therefore, instead, we calculate a lower bound makespan mb, defined as the maximum sum of processing times p of operations Om that require the same machine:\n\n\n\\(m_b = \\max_{machine \\in M} \\sum_{o \\in O_M} p(o)\\)\n\n\nNote that \\(m_b \\leq m_{optimal}\\) in all cases since the mb solution practically ignores precedence constraints within jobs. We then define our initial return-to-go as Ro = minit - m and reduce it in each step by the given reward. Since this Ro is an optimistic approximation, it forces the DT to take the best possible actions."}, {"title": "V. NUMERICAL RESULTS", "content": "A. Results on Taillard Benchmark\nIt is evident that the DTs can outperform the NLS teacher models in almost all problem sizes and in some cases by significant margins. On average, using DT100 leads to 1.11%p, 1.23%p and 1.15%p better optimality gaps than NLSANP, NLSAN, NLSA, respectively. Note that for the JSSP, such improvements are non-trivial. While DT and DT100 perform similarly well on average, each taken alone does not consistently outperform the NLS models. In practice, this implies that both variants should be tried to achieve the best final result for a problem size."}, {"title": "B. Results on Own Test Instances", "content": "To validate our observations on a larger set of instances, the results on 100 randomly generated JSSP instances are reported in TABLE 4. Indeed, the results on this larger test dataset confirm a similar trend that the DTs outperform NLS on average. However, compared to the results on the benchmark instances, they do so by a smaller margin and not as consistently. This indicates that the performance of each DT and DT100 models varies between different problem instances."}, {"title": "VI. STUDENT-TEACHER COMPARISON", "content": "In this section, we examine the learning behavior of the DT models in detail. Recall that they differ from their respective NLS teachers in two noteworthy ways.\nFirstly, DT models can base decision on a context of up to 50 previously taken actions, states and returns-to-go. This can enable them to leverage knowledge about the influence of past decisions of their local search on the same problem instance. In fact, in preliminary experiments, we observed performance improvements only when contexts lengths of 50 steps or larger were used. On the downside, to take the context into account, a much larger neural network architecture is used which leads to increased inference times compared to the original NLS architectures. The second major difference to DRL models aiming at minimizing the cumulative rewards is that DT models have an artificial return-to-go prior which the models are trained to match, and which can be used to push them towards shorter makespans than those observed during training.\nThese differences lead to the following three questions we aim to answer in the following analysis to deepen our understanding of the proposed method:\n1. What are the practical implications of using the comparatively larger and slower DT models during inference with respect to performance?\n2. Is there a correlation between relatively better performance of DT models in comparison with their teacher models and a greater deviation in learned behavior?\n3. Is there an optimal return-to-go to achieve the best performance? The results in Table 1 and Table 2 indicate a better performance of the model when the same number of local search steps is performed. However, inferences of the DT take longer to compute than those of the teacher models on the same hardware."}, {"title": "VII. CONCLUDING DISCUSSION AND OUTLOOK", "content": "In conclusion, we have shown that the DT can be leveraged to increase the performance of state-of-the-art NLS models on the JSSP by learning to take more effective decisions during the search. However, the benefit of using the DT varies for different problem sizes and parameterizations of the approach. We did not observe any trend that allows us to predict how much better a DT will perform a priori. From a practical perspective, this means that the benefit of using the DT may not always be given when a short search time limit applies. Upon closer examination of why the DT models outperform the NLS models, we found that their main advantage lies in the broader context of previous search steps that the DT effectively leverages through the transformer architecture. The returns-to-go on the other hand, seem to play only a minor role in reaching superior performance.\nThese observations motivate future work, in which we aim to push the DT towards considering the return-to-go through forced variation in the quality of solutions generated by the teacher NLS models. This could be achieved by learning from different teachers at the same time or by curriculum learning, which varies the difficulty the algorithm has with solving certain instances well, as done in [50]."}]}