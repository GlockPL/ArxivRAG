{"title": "Rank Also Matters: Hierarchical Configuration for Mixture of Adapter Experts in LLM Fine-Tuning", "authors": ["Peizhuang Cong", "Wenpu Liu", "Wenhan Yu", "Haochen Zhao", "Tong Yang"], "abstract": "Large language models (LLMs) have demonstrated remarkable success across various tasks, accompanied by a continuous increase in their parameter size. Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), address the challenges of fine-tuning LLMs by significantly reducing the number of trainable parameters. Recent studies have integrated LoRA with Mixture of Experts (MoE) architectures, leveraging multiple adapter experts and gating mechanisms to further improve fine-tuning performance. However, existing approaches primarily focus on adjusting the number of adapter experts per layer to optimize the size of introduced trainable parameters, while neglecting a critical factor of adapters' rank.\nTo this end, we propose a hierarchical configuration scheme for adapter experts, HILO, which flexibly adjusts the number and rank values of adapter experts across layers, matching the varying representational complexity cross model layers. Extensive experiments on multiple benchmark tasks demonstrate that HILO outperforms existing methods in accuracy while introducing fewer trainable parameters, providing an efficient and practical solution for fine-tuning LLMs.", "sections": [{"title": "1 Introduction", "content": "In recent years, the rapid advancement of the Large Langue Model (LLM) has demonstrated impressive performance on various tasks, e.g., natural language processing, computer vision, and multimodal applications [Liu et al., 2024b; Chang et al., 2024]. However, as the parameter size of large models continues to scale up, substantial computational resources are required for model fine-tuning. In this context, parameter-efficient fine-tuning (PEFT) methods have emerged [Han et al., 2024], significantly reducing the number of trainable parameters while maintaining performance. PEFT techniques include adapter tuning, prompt tuning, prefix tuning, and Low-Rank Adaptation (LoRA) [Houlsby et al., 2019; Lester et al., 2021; Li and Liang, 2021; Hu et al., 2022].\nAmong these, LoRA has gained considerable attention for its innovation of injecting low-rank matrices (referred to as adapters) into the model's weight matrices, enabling efficient fine-tuning for downstream tasks without introducing trainable parameters size substantially.\nBased on LoRA, several studies have further optimized it by incorporating the architecture of Mixture of Experts (MoE) [Liu et al., 2024a; Zeng et al., 2024; Dou et al., 2024]. MoE achieves superior computational efficiency and model capacity by employing multiple feed-forward networks (referred to as experts) to decompose complex problems into smaller subproblems, which will be processed by a subset of experts selected by a gating network [Zhou et al., 2022]. The integration of LoRA and MoE termed the mixture of adapter experts, which involves introducing multiple adapters for model's weight matrices and activating a subset of them selectively via a gating network, resulting in an efficient and scalable fine-tuning solution.\nThe vanilla approach to constructing a mixture of adapter experts model involves assigning a fixed number of adapters to specific weight matrices of each layer, which neglects the variability and complexity across the layers of the pre-trained model. Then, some studies try to allocate a more suitable number of experts based on characteristic per model layer, aiming to further reduce trainable parameters while minimizing accuracy loss. For example, [Gao et al., 2024] assigns fewer experts to shallow layers and more to deeper layers, [Qing et al., 2024] adjusts the number of experts based on the training quality of model networks. However, all existing studies set a consistent rank for all adapters, which limits the flexibility and adaptability of adapter experts and results in unnecessary parameters across layers. In other words, as shown in Figure 1, beyond the number of experts focused in existing studies, the rank configuration is also a critical factor in determining the size of introduced trainable parameters and influencing the feature learning capacity per adapter.\nTo this end, we present HILO, a hierarchical scheme for adapter experts configuration scheme, which aims to enhance the efficiency of mixture of adapter experts. Unlike existing studies that focus solely on the number of experts, HILO holistically adjusts layer configurations by jointly considering the effects of expert numbers and ranks, aligning with the varying representational complexity of model layers in two dimensions. Extensive experimental results demonstrate that HILO surpasses existing methods in accuracy while simultaneously achieving reductions in trainable parameters for fine-tune training and active parameters for inference.\nThe main contributions of this paper are as follows:\n\u2022 We reveal that existing mixture-of-adapter-experts architectures focus solely on the number of adapter experts and experimentally demonstrate that the rank of adapters is also a critical factor for improving performance.\n\u2022 We propose a hierarchical configuration scheme, HILO, which allocates different numbers and ranks to adapter experts across layers, enhancing the efficiency and performance of fine-tuning.\n\u2022 We conduct extensive experiments to validate that HILO achieves higher model accuracy while reducing trainable and active parameters compared to existing methods."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Parameter Efficient Fine-Tuning", "content": "The increasing size of LLM parameters motivates the development of parameter efficient fine-tuning [Han et al., 2024]. Unlike full-parameter fine-tuning, PEFT methods selectively adjust or introduce a small number of trainable parameters without modifying the entire parameter set. Adapter tuning [Houlsby et al., 2019], prompt tuning [Lester et al., 2021], prefix tuning [Li and Liang, 2021], and LoRA [Hu et al., 2022] are some representative PEFT schemes, and LoRA is one of the most widely employed strategies. It integrates the low-rank decomposition matrices into the weight update of the model and greatly reduces the number of trainable parameters. Specifically, the modified parameter matrix w' \u2208 Rnxm is computed as w' = w + \u0391\u0392, where w\u2208 Rnxm is the original parameter matrix, A \u2208 Rn\u00d7r and B \u2208 Rr\u00d7m are the low-rank decomposition matrices, and r is much smaller than n, i.e., r < min(n,m).\nBased on the basic LoRA, several optimizations have been proposed to align with the specific characteristics of LLMs and implementation requirements. The authors in [Dettmers et al., 2024] further minimize parameter overhead by quantizing adapter parameters. LoRA+ [Hayou et al., 2024] applies differentiated learning rates based on the initial values of the adapter matrices A and B, thereby improving fine-tuning performance in certain scenarios. Unlike conventional approaches, which typically initialize A with random values sampled from a normal distribution and B with zeros, VeRA [Kopiczko et al., 2024] initializes all of them with a normal distributed random values but freezes them, and adds a trainable vector for each of A and B. It can further reduce the trainable parameters involved in fine-tuning with slight accuracy sacrifice."}, {"title": "2.2 Mixture of Experts", "content": "The mixture of experts architecture was first proposed by [Jacobs et al., 1991], introducing an assignment mechanism based on input data to distribute tasks across multiple expert modules, thereby achieving efficient task specialization and model capacity utilization. With the growing popularity of the Transformer [Vaswani, 2017], many studies revisited MoE by applying it to the corresponding Feed-Forward Network (FFN) layers, extending these layers into multiple expert networks. However, a prominent feature of the widely adopted MoE in transformer-based LLMs is the use of a sparse gating mechanism, which selects only a subset of experts for token processing, enabling LLMs to scale to an extreme scale.\nGShard [Lepikhin et al., 2021] and Switch Transformer [Fedus et al., 2022] are pioneers that employ learnable top-2 or top-1 expert selection strategies, and DeepSeek [Dai et al., 2024] implements a shared expert isolation scheme. Hash-Layer [Roller et al., 2021] uses a hashing-based way to select experts for tokens, improving the stability of model training. [Huang et al., 2024; Zhou et al., 2022; Yang et al., 2024b] allow different numbers of experts to be assigned for different tokens, enhancing model flexibility. Besides studies on architectures and training strategies of MoE, recent years have also witnessed the emergence of many MoE-based multimodal models [Riquelme et al., 2021; Mustafa et al., 2022; Du et al., 2022]."}, {"title": "2.3 LORA Meets MoE", "content": "Based on basic LoRA technology, some studies have further introduced the MoE architecture, where each weight matrix's LORA adapter is no longer a single module but are a set of adapters controlled by a gating network [Yang et al., 2024a; Wu et al., 2024a]. Such the approach enhances the capability of the fine-tuned model while maintaining scalability [Li et al., 2024; Wu et al., 2024b].\nMoELORA [Liu et al., 2024a] combines the strengths of these two techniques to achieve an efficient multi-task fine-tuning framework. AdaMoE [Zeng et al., 2024] ingeniously introduces additional null expert networks, enabling a learnable and dynamic selection of the experts. LoRAMOE [Dou et al., 2024] classifies all experts of each layer into two categories, assigning them to process either world knowledge or fine-tuning knowledge, thereby mitigating the issue of world knowledge forgetting of the fine-tuning. MoLA [Gao et al., 2024] implements a layer-wise expert allocation strategy and demonstrates through experimental results that deeper networks require more experts than shallower ones."}, {"title": "3 Motivation", "content": ""}, {"title": "3.1 Observation and Analysis", "content": "Without loss of generality, in the case of a particular layer of the model's network, LoRA-based fine-tuning involves adding the two output vectors of the original network and the LORA adapter, the resulting vector will be the final output of this layer and subsequently fed into the next layer for further computation, i.e., y = wx + \u0394\u03c9x, where w is the original weight matrix and \u0394\u03c9 is the adapter. Similarly, in the mixture of adapter experts architecture, the output vectors of multiple adapter experts are performed weighted summation according to the activation states, which will be added to the output vector of the original network.\nThe outputs of original networks and adapter experts are tracked during model inference for further investigation. The model configuration is as followings: using Llama 2-7B [Touvron et al., 2023] as the base model, allocating each network layer with 8 adapter experts of all ranks 8, employing the Top-2 activation strategy, and fine-tuned on the ScienceQA dataset [Lu et al., 2022]. When inputting a randomly selected subset of the test samples into the model, the distributions of the output values of Feed-Forward Neural Network (FFN) layers and corresponding adapter expert layers are presented in Figure 2.\nThe following trends be observed: 1) the output of the original network exceeds the output of its adapter experts by more than one or two orders of magnitude in the range with the highest concentration of values within the same network layer; 2) for the adapter experts' outputs, shallow layers exhibit a higher proportion of small values close to zero compared to deep layers. These trends occur because the shallow layers of the model typically perform general feature extraction, while the deeper layers are responsible for learning specialized features. For a new fine-tuning task, the original networks in the shallow layers are capable of performing basic feature extraction to a certain extent, thereby minimizing the need for fine-tuning these layers. Therefore, with the same size of trainable parameters, allocating fewer adapter experts to the shallow layer and more adapter experts to the deep layer, rather than equally, is a straightforward and effective allocation strategy to maximize the effectiveness of such parameters. The output values distribution of MoLA (i.e., assigning fewer adapters to shallow layers) under the same fine-tuning settings, except for the adapter allocations, are present in Figure 3. Compared to the equal allocation of experts, such optimized allocation way reduces the proportion of small output values from shallow-layer experts.\nHowever, in MoE architectures, assuming all other configurations remain unchanged, the number of experts within a reasonable range is generally expected to exhibit a positive correlation with the capability of this MoE layer. From this perspective, strategies that directly reduce the number of adapter experts fail to consider the influence of the individual expert's capacity. On the one hand, the size of trainable parameters is not only proportional to the number of experts but also to the rank of the adapter experts. The rank determines the parameter size of each adapter expert, which directly impacts its fitting ability. On the other hand, when the number of experts is fixed, there is an upper limit to improving the fitting ability of this MoE layer by simply increasing the parameter size of experts. Specifically, once an individual expert's fitting ability exceeds the layer's requirements, further increasing its parameter size provides diminishing returns about model capability. In summary, the number of experts and the capacity of individual experts jointly influence the overall performance of the MoE layer."}, {"title": "3.2 Key Idea", "content": "Given the characteristic of the shallower to deeper layers of the model, there are requirement differences cross layers in the capability of individual adapter experts for fine-tuning. It exhibits incremental requirements for adapter experts from shallow to deep layers in fitting capability. And the parameters size, which decided by the rank of adapter, scales to the fitting capability.\nTherefore, inspired by the analysis above, this paper proposes a hierarchical rank setting scheme to adapter experts for efficient LLM fine-tuning, HILO. When maintaining the same trainable parameter size with MoLA, the proportions of output values smaller than 10-3 in the shallow layers (layers 1 to 8) of HILO are presented in Figure 3. It indicates that HILO can further reduce the proportion of such small values than MoLA.\nIt notes that the final goal of model fine-tuning is not to minimize such proportions but to improve model accuracy or reduce trainable parameter size. We aim to provide an analytical perspective by this data analysis. The details and outperformances of HILO will be presented in Section 4 and 5, respectively."}, {"title": "4 HILO Design", "content": "In this section, we first describe the extension of the basic LORA method to the MoE architecture, and then present the adapter experts configuration about allocation and activation of experts. Finally, we introduce a simple and efficient rank-setting strategy, which can further reducing parameters while improving accuracy of the fine-tuned model."}, {"title": "4.1 Mixture of Adapter Experts", "content": "Extending LoRA to MoE architectures is to transform the original single adapter, which comprises one pair of low rank matrices A and B, into multiple adapter experts. Suppose there is a model with N Transformer layers and each layer i has Ei adapter experts, denoted by $e_i^j$, where $j\\in E_i$. In line with the basic LoRA initialization scheme, each adapter expert has its matrix A initialized with a random Gaussian distribution and B with zeros. Moreover, a gate network $G_i$ is introduced for layer i. It is responsible for analyzing the features of each input token x and computing activation probabilities $P_i^j$ for all adapter expert $e_i^j$ via a softmax function. Following the MoE computation procedure with a Top-K activation policy, for a given token x, the adapter experts $e_i^k, k \\in K$ with highest K probabilities are activated. Then, the output of this mixture of adapter experts layer can be expressed as:\nOutpute = $\\Sigma_{k \\in K} P_i^k w_i^k x$, $P_i^k = \\frac{P_i^k}{\\Sigma_{k' \\in K} P_i^{k'}}$\nwhere $w_i^k$ represents the parameter matrix of the expert $e_i^k$. Assuming Wi is the original weight matrix of layer i, the final output of this layer yields:\nOutput = $W_i x + Output_e$.\nSo far, it is possible to select a subset of adapter experts dynamically, enhancing both the capacity and adaptation flexibility of LoRA. Meanwhile, Outpute is fed into the next layer as the input."}, {"title": "4.2 Adapter Experts Setting", "content": "In this subsection, we present the experts allocation strategies and activation policies, which determine sizes of trainable and active parameters, respectively, thereby affecting the model's efficiency."}, {"title": "Dynamic Experts Allocation", "content": "The vanilla allocation strategy assigns the same number of adapter experts to each layer. However, as mentioned earlier, such an approach overlooks the differences across layers, resulting in inefficiencies such as introducing unnecessary trainable parameters or failing to utilize them fully. Two proven flexible expert allocation strategies are presented here [Gao et al., 2024; Qing et al., 2024]. First, based on the varying complexity of processing tasks across different layers, experts can be allocated incrementally from shallow to deep layers. This intuitive method demonstrates performance improvements compared to a fixed expert allocation. Second, the Heavy-Tailed Self-Regularization theory can be used to evaluate the training quality of the original model layers, guiding a flexible and adaptive allocation of experts to each layer."}, {"title": "Activation of Experts", "content": "For expert activation, Top-K is a feasible and simple manner. However, activating the fixed number of experts neglects the processing complexity among different tokens, which may result in either redundancy or insufficiency in processing for certain tokens.\nComparatively, learning-based policies enable the activation of a variable number of experts. The Top-P series methods exemplify this policy by activating experts for each token, either by selecting in order of decreasing probability until the cumulative probability exceeds a predefined threshold P, or by directly selecting all experts whose probabilities surpass the threshold. P is predefined, and the loss function incorporates constraints on the number of activated experts. Additionally, another implementation involves inserting several placeholder adapters without parameters into the adapter expert layer [Zeng et al., 2024]. These placeholders do not perform computations on the token but allow the model to activate a varying number of true experts for each token while still adhering to the Top-K policy. For instance, if n of the selected experts are placeholder adapters, then, only K-n true experts are actually activated. During training, the model learns to activate the appropriate number of experts dynamically for each token.\nThese above two kinds of mainstream policies for expert activation will be evaluated in comparative experiments."}, {"title": "4.3 Hierarchical Rank-setting", "content": "In the mixture of adapters experts architecture, the number of adapter experts per layer and their rank are critical factors influencing model performance and trainable parameter size. The rank of an adapter expert directly determines its representational ability and parameter size, while the current layer-wise expert's allocation strategy leaves room for further optimization. That is, the rank of shallow-layer adapter experts can be set lower than that of deeper layers, which further satisfies diverse representational requirements and improves parameter utilization.\nFormally, with the layer as the rank setting granularity, the number of adapter experts in each layer can be expressed as:\n$r_i = r_{min} + \\lceil \\frac{i}{n} \\rceil \\frac{r_{max} - r_{min}}{\\lfloor \\frac{i}{n} \\rfloor + 1}$\nwhere $\\lceil \\cdot \\rceil$ and $\\lfloor \\cdot \\rfloor$ respectively represent the rounding up and rounding down operations; rmin and rmax respectively denote the minimum and maximum rank values. Typically, rmin is set to 2, depending on the model's scale, and rmax can take values such as 8, 16, 32, or higher. n indicates the total layers of the fine-tuned model. Optionally, it is feasible to set same rank for for every l layers.\nMoreover, considering the computational characteristics of hardware devices, rank values can be flexibly configured as multiples or powers of 2 based on the formula in practical implementation, rather than strictly adhering to it."}, {"title": "5 Evaluation", "content": "In this section, we demonstrate the outperformance of HILO over various benchmarks, mainly in reducing trainable or active parameter size and enhancing fined-tuned model performance through its hierarchical rank setting."}, {"title": "5.1 Experiment Setup", "content": "Model and Dataset. The Llama 2-7B [Touvron et al., 2023] is selected as the base model of fine-tuning due to its high deployment and popularity within the LLM community. To evaluate the performances of HILO and comparison methods, two distinct types of tasks are employed using widely recognized datasets. The first type of task involves commonsense reasoning and includes the following dataset: ScienceQA (SQA) [Lu et al., 2022], CommonsenseQA (CQA) [Talmor et al., 2019], and OpenBookQA (OQA) [Mihaylov et al., 2018]. The second type of task concentrates on semantic understanding, which uses three datasets from the renowned GLUE Benchmark [Wang et al., 2019]: the Microsoft Research Paraphrase Corpus (MRPC), the Recognizing Textual Entailment (RTE), and the Corpus of Linguistic Acceptability (COLA).\nBaselines. The pre-trained Llama2-7B model and vanilla framework of the mixture of adapter experts are selected as comparison methods. Specially, the Llama 2-7B model is evaluated using prompt engineering tailored for each dataset, and the vanilla method allocates 8 experts with a rank of 8 for each weight matrix of every Transformer layer.\nBased on the vanilla method, the optimization schemes of adapter experts of existing studies can be divided into three categories:\n(a) The first one is the manual-based expert allocation represented by MoLA [Gao et al., 2024]. This study indicates that in Llama 2-7B-based fine-tuning, introducing the same size of trainable parameters, allocating 2, 4, 6, and 8 experts in groups of every 8 layers from shallow to deep layers is the performance-optimal allocation. To facilitate the description, this allocation strategy is denoted as \"\u25bd\" in this paper, which serves as a comparison method.\n(b) The second one is AlphaLoRA [Qing et al., 2024], which allocates experts based on the training quality of the original model. When introducing the same size of trainable parameters as MoLA, it has a layer-wise allocation strategy\u00b9 denoted as \u201c\u2021\u201d in this paper, which is another comparison method. Both of these approaches strive to improve the performance of the fine-tuned model with the limited introduced trainable parameters.\n(c) Additionally, another category of methods improves model efficiency by flexibly adjusting the number of activated experts to reduce the size of activated parameters. For example, AdaMoE [Zeng et al., 2024] achieves this by introducing placeholder experts that neither introduce trainable parameters nor incur activation overhead. In the experiments conducted in this paper, a 1:1 ratio of real experts to placeholder experts is set for each layer, which is also employed as a baseline for comparison.\nHILO is integrated into these aforementioned comparison methods. Except for the rank, which follows HILO, all other settings remain unchanged with corresponding comparison method. The effectiveness of the proposed scheme can be validated by performance improvements in terms of trainable parameters, active parameters, and accuracy."}, {"title": "5.2 Experiment Results", "content": "The trainable parameters introduced by vanilla, which mainly involves all adapters with the rank of 8, is defined as a unit of 1. The active parameters that vanilla method activates 2 adapters with rank of 8 for each token is defined as 2. Other methods are scaled proportionally based on the number of introduced or activated adapters and corresponding rank values. All experiments performed on 4 NVIDIA 4090 GPUs.\nExcept for the pre-trained Llama 2 and Vanilla method, the trainable and active parameters of MoLA and AlphaLoRA are constant, and the active parameters corresponding to AdaMoE are dynamic. To this end, the comparison experiments are conducted respectively, and the specific comparison results are as follows.\n(1) HILO vs. MoLA & AlphaLoRA. MoLA reduces the total number of adapters in the model while keeping the adapter rank unchanged, thereby reducing the trainable parameters to 0.63 compared to Vanilla. To ensure fairness in the comparison, the trainable parameters introduced by AlphaLoRA are kept consistent with those of MoLA. Furthermore, since MoLA and AlphaLoRA also adopt the Top-2 activation scheme, their active parameters remain the same as Vanilla's.\nBased on this, HILO reduces the rank values progressively from deep to shallow layers, grouped every eight layers, to 8, 4, 6, and 2. To ensure fairness of comparison, the total trainable parameter size of HILO is kept consistent with MoLA and AlphaLoRA by proportionally increasing the number of adapter experts from shallow to deep layers. Compared to MOLA, HILO assigns eight experts per layer, while compared to AlphaLoRA, the number of experts per layer in HILO is calculated as Rank\ud835\udc56/8*AlphaLoRA\ud835\udc56, where Rank\ud835\udc56 represents the rank value of adapters in layer i, and AlphaLoRA\ud835\udc56 represents the number of adapter experts in layer i of AlphaLoRA, denoted as '\u2021\u2021' in this paper. In terms of active parameters, although HILO adopts Top-2 activation scheme, the decreasing rank result in proportionally fewer active parameters in the shallow layers. Consequently, the active parameters of HILO are reduced from 2, as in Vanilla, MoLA, and AlphaLoRA, to 1.25, representing a 37.50% reduction.\nIn terms of accuracy, since Llama 2 is not fine-tuned on new datasets, it demonstrates relatively low performance. The Vanilla achieves higher accuracy by increasing the size of trainable and active parameters as the cost. However, HILO, with a smaller parameter size, still outperforms the Vanilla method on two datasets. Comparing MoLA and AlphaLoRA, as shown in Table 1, HILO is lower (0.08%\u2193) than MOLA only on one of six datasets (CQA) while achieving a maximum improvement of 2.00% on the OQA. Moreover, HILO\u2021\u2021 outperforms AlphaLoRA across all datasets, with the highest improvement of 1.97% on the MRPC dataset and an average improvement of 1.01%.\nThe experimental results demonstrate that HILO\u2021\u2021 achieves higher model accuracy compared to MoLA and AlphaLoRA while introducing the same size of trainable parameters and the lower size of active parameters.\n(2) HILO vs. AdaMoE. To compare with AdaMoE, HILO reduces the rank values from deep to shallow layers, grouped every 8 layers, to 8, 4, 6, and 2, while keeping the number of experts constant. In contrast, the activations of real and placeholder adapter experts are learned during model fine-tuning. Similarly, the trainable parameter size of AdaMoE is 1, whereas HILO's is 0.63.\nIn addition, we tracked the expert activation during validation. As shown in Table 1, the average active parameter of AdaMoE across datasets is 1.04, while HILO's is 0.65, representing a 37.5% reduction. For accuracy, HILO outperforms AdaMoE across all datasets, with the highest improvement of 3.26% on RTE dataset and an average improvement of 1.04%. The experimental results exhibit that HILO achieves higher model accuracy with fewer both trainable and active parameters compared to AdaMoE.\nTo further evaluate the performance of HILO, we conducted another comparison with MoLA across all datasets. In this experiment, it does not constrain the introduced trainable parameters to be the same size. Instead, HILO configures both the number of adapter experts and values of adapters' ranks as V. With this setup, the total size of trainable parameters in HILO is reduced to 0.39, indicating a 37.50% decrease compared to MoLA. Similarly, the size of active parameters in HILO is reduced to 1.25, reflecting a 37.50% reduction from MoLA's 2.\nIn terms of accuracy, as shown in Figure 5, HILO outperforms MOLA on the CQA, OQA, and COLA datasets and achieves higher average accuracy across all datasets. These results further demonstrate that HILO can achieve higher fine-tuning accuracy with fewer trainable and active parameters compared to MOLA.\nWe integrate MoLA and AdaMoE as a new method, denoted by Mix, which combines layer-wise expert allocation and learning-based adapter experts activation. On this basis, we introduce HILO as a comparative method based on Mix, with experiments conducted on the SQA dataset. Under the condition that trainable parameters are unaffected by the activation scheme, the trainable parameters of Mix and HILO are 0.63 and 0.39, respectively, with HILO achieving a 37.50% reduction.\nAccording to statistical analysis, when the adapter experts activation is the Top-2 scheme, the active parameters of Mix and HILO are 1.03 and 0.77, respectively, representing a 25.24% reduction. Although the Top-V activation is not practically applied, we conducted validation and evaluation under this setting, where Mix and HILO exhibit active parameters of 1.65 and 1.23, respectively, resulting in a 25.45% reduction.\nIn terms of accuracy, as shown in Table 3, under the de facto widely employed Top-2 activation scheme, HILO outperforms Mix with a 1.30% improvement. Under the Top-V, HILO 's accuracy is 0.32% lower than Mix, but it still achieves higher accuracy than Vanilla, MoLA, and AlphaLoRA, while reducing both trainable and active parameters simultaneously.\nExploratory Studies\nHILO is evaluated by various rank configurations, specifically grouping every eight layers with ranks set as 2448, 2288, 2468, 2488, and 2888 from shallow to deep layers, with the corresponding trainable parameters gradually increasing. The parameter size of 2468 configuration is assigned unit 1. The accuracy results for these five configurations across six datasets are shown in Figure 6. It can be observed that the configuration of 2468 (green) achieves the best balance between parameter size and accuracy. This configuration is also adopted in the above experiments."}, {"title": "6 Conclusion", "content": "This paper proposes HILO, a hierarchical configuration scheme for the mixture of adapter experts in LLM fine-tuning, enabling flexible adjustments to both the number and rank of adapter experts to better align with the varying representational complexity across model layers. Extensive experimental results demonstrate that HILO outperforms existing methods in accuracy while achieving reductions in trainable and active parameters across diverse datasets."}]}