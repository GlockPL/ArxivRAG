{"title": "ADAPTIVELY CONTROLLABLE DIFFUSION MODEL FOR EFFICIENT CONDITIONAL IMAGE GENERATION", "authors": ["Yucheng Xing", "Xiaodong Liu", "Xin Wang"], "abstract": "With the development of artificial intelligence, more and more attention has been put onto generative models, which represent the creativity, a very important aspect of intelligence. In recent years, diffusion models have been studied and proven to be more reasonable and effective than previous methods. However, common diffusion frameworks suffer from controllability problems. Although extra conditions have been considered by some work to guide the diffusion process for a specific target generation, it only controls the generation result but not its process. In this work, we propose a new adaptive framework, Adaptively Controllable Diffusion (AC-Diff) Model, to automatically and fully control the generation process, including not only the type of generation result but also the length and parameters of the generation process. Both inputs and conditions will be first fed into a Conditional Time-Step (CTS) Module to determine the number of steps needed for a generation. Then according to the length of the process, the diffusion rate parameters will be estimated through our Adaptive Hybrid Noise Schedule (AHNS) Module. We further train the network with the corresponding adaptive sampling mechanism to learn how to adjust itself according to the conditions for the overall performance improvement. To enable its practical applications, AC-Diff is expected to largely reduce the average number of generation steps and execution time while maintaining the same performance as done in the literature diffusion models.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence generated content (AIGC) has been an important and popular topic in the machine learning community. On the one hand, the creativity shown by the generative models is a critical indicator to measure the level of artificial intelligence. On the other hand, generated contents could help improve the performance of other machine learning tasks by augmenting data for model training. Starting from auto-regressive model [1], many branches of generative models have been studied, such as Normalizing Flows [2], VAE [3], and GAN [4]. Particularly, diffusion models [5] have been explored recently and proven to outperform all other generative methods with their more stable training processes.\nDespite the potential, diffusion models suffer from three major limitations that prevent their practical applications. First, traditional diffusion models are unconditional with an uncontrollable generation process, thus their generation results are almost random without constraints. Second, the pre-calculated diffusion rate parameters (i.e. the noise schedule of the diffusion process) are fixed and kept the same for all kinds of generation tasks, so the variety of generated images only depends on the sampling randomness but not to meet different generation demands. Last and most importantly, the number of generation steps is pre-defined and kept the same regardless of the contents to be generated. Such a setting will not only bring unnecessary diffusion steps for simple contents but also leave complex contents not fully generated. To understand these constraints, we can compare the generation process to hand-carving. In the latter case, the outline is first generated and the details are then gradually added, and the number of steps required to generate an apple must be much fewer than that needed to produce a fully fledged bird."}, {"title": "2 Related Work", "content": "2.1 Conditional Diffusion Model\nIn traditional unconditional diffusion models, the generation results are stochastically sampled from the distribution learnt from the training dataset step-by-step, which are lack of outside control. On the contrary, many conditional diffusion models are explored recently to trade diversity for fidelity, producing high quality samples related to some specific objects, rather than covering the whole distribution. Common conditions used to control the generation process include category labels [6, 7, 22], texts [8, 9, 10, 23, 24, 25, 26] and images [10, 11, 27, 28, 29, 30]. The control methods can be divided into two groups: 1) Guidance-based, where a separate model is trained to take the intermediate images generated by the diffusion model as input to calculate the similarity with given conditions, and the gradients work as the score to revise the distribution for the next step sampling; 2) Directly combining the condition by embedding it into the generated results in the latent space. In the first group, Dhariwal and Nichol [6] modify the unconditional diffusion model with additional classifier for image synthesis, based on which Ho and Salimans [7] propose classifier-free guidance where the same diffusion model is only trained with score estimators with and without labels but not using the extra classifiers. To deal with limited labels during conditional training, You et al. [22] propose to alternatively train the classifier and the diffusion model. CLIP [31] model is commonly used in [8, 9, 10] to compare the intermediate outputs of the diffusion model with given text conditions to provide the guidance for the generation process. Liu et al. [10] also train an image encoder to take the image conditions into consideration. The same idea is utilized by Voynov et al. [11] where the edge map estimator is trained to let the given sketch image to guide the generation. A conditional diffusive estimator is used in [28] to diffuse both conditions and generated images and match them. Instead of training, Yu et al. [27] propose a training-free framework where an arbitrary pre-trained model is used to extract features from both generated images and conditions to calculate an energy score. As the second group of methods, in order to reduce the computational complexity, Rombach et al. [32] first propose to apply the diffusion process to latent features, and extra conditions are encoded and integrated with them through cross-attention mechanism, which is also widely used in [9, 23, 24]. The conditional embedding can also be directly concatenated with generated"}, {"title": "2.2 Efficient Diffusion Sampling", "content": "No matter score-based models [35] or diffusion-based models [5], iterative generative models add noise that the data distribution gradually collapses into a simple noise distribution and reverse this process to generate high quality samples. Although they show impressive performance, these models suffer from high computational cost, typically requiring hundreds or thousands iterations for each sample, which prevents them from being used in practical applications. To speed up the diffusion models, intuitive ways include sub-sampling [14, 15, 16, 17, 18] and early-stop [19, 20, 21]. On the one hand, it's believed in previous methods that there is a decoupling between the schedules applied for training and for inference, so the number of steps used during the inference phase can be different from those in training. Specifically, Nichol and Dhariwal [14] adopt even distance sub-sampling of the total steps with learnt noise variance as a combination of the upper and lower bounds for each step, while Watson et al. [15] use dynamic programming algorithm to select the sub-sequence of steps to optimize the evidence lower bound. Song et al. [16] generalize traditional DDPMs [5] through a non-Markovian process to realize the jump among diffusion steps, which is further expanded by Watson et al. [17]. In San-Roman et al.'s work [18], the output after each step is utilized to estimate the amount of noise in data, so that fewer steps are scheduled in the generation process. On the other hand, since only the first few steps of the diffusion process contribute the most, some methods try to truncate the process and start the generation from a non-Gaussian distribution. In [20], Lyu et al.use an extra generative model, such as VAE [3], to first obtain an initial noisy image from the standard Gaussian distribution, and a normal diffusion process is applied on top of the intermediate image to get the final result. Zheng et al. [21] further omit the extra generative model and only use the same diffusion model to generate the initial image from total noise in one step, which is followed by a denoising process as usual. However, we notice that no matter sub-sampling or early-stop, these methods keep the total number of steps in the generation process the same for all target images. The possible speed-up brought by the difference among generation contents is neglected, while a uniform number of diffusion steps may also compromise the generation quality for complex images if the steps are blindly reduced. Actually, according to Song et al.'s research [36], the diffusion process can be regarded as a discretization of a continuous stochastic process, such that generation results can be obtained by solving the corresponding stochastic differential equations (SDEs). Meanwhile, they propose that it can be accelerated by solving probability flow ordinary differential equations (PF-ODEs) which have the same marginal distribution with SDEs. Therefore, a branch of works [37, 38, 39, 40, 41, 42, 43] start focusing on finding out much more efficient numerical solvers. The studies in [39, 40] conduct step adjustments according to the error-check among solvers of different orders after each iteration. These adaption strategies are different from ours since their step sizes are adapted based on the step-wise checking and decision, which require too much extra calculation, while our adaption is sample-wise and only needs the extra calculation once at the very beginning of the diffusion process. Furthermore, distillation technique is used in [44, 45, 46] to train a student model using fewer sampling steps, such that multiple original diffusion steps can be integrated into one step in it. The additional training needed in these methods incurs a high cost."}, {"title": "3 Methodology", "content": "The framework of the proposed AC-Diff Model is shown in Fig. 1. Specifically, different from a common conditional diffusion model, it contains an extra Conditional Time-Step (CTS) calculator and a corresponding Adaptive Hybrid Noise Schedule (AHNS) module. In the remaining of this section, we will discuss each module in details."}, {"title": "3.1 Conditional Time-Step (CTS) Calculation", "content": "In most previous diffusion-based generation methods, the diffusion algorithms progressively sample from noise distribution to generate images from complete noise. The whole process usually takes T iterations or time steps (Fig. 2 (a)), with T pre-defined and fixed regardless of the contents to be generated. Intuitively, the length of the generation process should adapt to inputs and conditions according to the complexity of contents to generate, as in Fig. 2 (b)."}, {"title": "3.2 Adaptive Hybrid Noise Schedule (AHNS)", "content": "In common diffusion schemes, the noise schedule, controlled by a sequence of noise rate parameters {\u03b21, ..., \u03b2\u0442}, is created manually and fixed. Some literature studies decouple the training and generation, and try to down-sample the number of diffusion steps in the reverse path to accelerate the generation process by choosing a subset of parameters {\u00dft; i \u2208 [1, K]&{t1,...,tk} \u2286 {1, ...,T}}. The effectiveness of these schemes is built on the assumption that any clean image will become a pure Gaussian distribution in the forward diffusion process by gradually adding noise according to the schedule {1, ..., \u1e9eT} in T iterations, but the reverse generation process might only contain a subset of these T steps.\nIn our framework, instead of using a fixed T, we believe that the length of the forward diffusion process Tcond taken to convert an input to a total noise should be conditioned on the complexity of the input, which is described by text prompts and image conditions. In other words, previous fixed-T assumption is too strict and can be replaced by an adaptive one. Therefore, rather than just making a sub-sampling among {1, ..., \u03b2\u03c4}, we propose to also adjust the noise schedule {1, ..., \u03b2'. } during the training for better adapting to the conditional process length. Specifically, as shown in Fig. 4, our AHNS module includes two parts:"}, {"title": "3.3 Training Phase \u2013 Forward Diffusion Process", "content": "Previously, in the training phase, given a noise et, the goal of the model is to estimate the added noise ee(xt, t) so as to minimize the objective\n$E_{t \\in [1,T]}||e_t - E_\\theta(x_t, t)||^2$,\nwhere xt is the noisy version from the clean image 20 by adding the noise according to the predefined diffusion rate parameter as\n$x_t = \\sqrt{\\alpha_t}x_0 + \\sqrt{1 - \\alpha_t}e_t$,\nand &\u2081 = \u03a0=1(1 \u2013 \u03b2\uff61). The noise estimator \u20ac\u0189(\u00b7) in Eq. (10) is usually implemented by a U-Net [52], whose encoder converts the noisy image xt into a latent feature fx,t, and the decoder turns the combination of this feature as well as the time embedding ft, i.e. fx,t + ft, back to the estimated noise. Although many other works also use conditions to control the diffusion models, they only integrate them during the generation phase, while the models are still unconditionally trained, which will compromise the models' capability of learning how to provide guides based on the extra conditions. Therefore, in our model, we also include the prompts and conditions in the training phase. Besides the time embedding ft, we add the prompts embedding fp and condition embedding fa as extra channels into the hidden feature, i.e. fx,t + ft + fp + fa, before throwing it back to the U-Net decoder.\nIn real implementation, although the noise is gradually added during the diffusion process, it is proved to be equal to a one-step operation in Eq. (11). Therefore, from Eq. (10), during the training of previous diffusion models, an arbitrary t\u2208 [1, T] is sampled and an accumulated noise is added according to Eq. (11), then the model is supposed to estimate this noise. In our design, since the total length of the diffusion process Tcond is dynamic, to better adapt to the change of diffusion steps, we also make some modification to the above training process. Specifically, for each target image xo with prompts Cp and condition cd, we first calculate the dynamic Tcond(Cp, Cd) as Eq. (5). In order to make our trained model more consistent with the dynamic diffusion process in the generation phase, we sample the t from the range [1, Tcond] that is adaptive to different target images to be generated. Besides, we make the reschedule for the noise parameters {\u03b21, \u2026, \u1e9e'Teona} accordingly as introduced in Sec. 3.2.\nIn summary, the training process can be expressed by Algorithm. 1 and the training objective in our model can be rewritten as\n$E_{t\\in[1,T_{cond}(c_p,C_d)]}||e_t - \\epsilon_\\theta(x_t, t, c_p, c_d)||^2$,\nwhere\n$x_t = \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1 - \\bar{\\alpha_t}}e$,\n$\\bar{\\alpha}_t = \\prod_{s=1}^t (1 - \\beta_s)$, \n Algorithm 1 Forward Diffusion Process\n1: given Xo, Cp, Cd\n2: repeat\n3: Tcond Eq. (5)\n4: {\u03b21,..., BTcond} \u2190 Eq. (7) \u2013 Eq. (9)\n5: t~{1, ..., Tcond}\n6: \u20ac ~ N(0, 1)\n7: Take gradient descent step on\n8: until converged"}, {"title": "3.4 Generation Phase - Reverse Diffusion Process", "content": "During the generation process, the model is given the priors of what is going to be generated and how it should look like through text descriptions and additional conditions, such as sketch, input by users. Our model first estimates the adaptive length Tcond of the reverse diffusion process needed for generating the required high-quality image, which is controlled by prompts Cp and conditions ca. Given an initial state Tcond ~ N(0, I), the generation process takes a sample in each step from the denoised output, which can be also regarded as a Gaussian distribution, i.e.\n$x_{t-1} \\sim N(\\mu_\\theta(x_t, t, c_p, c_d), \\beta'_tI)$,\nwhere\n$\\mu_\\theta(x_t, t, c_p, c_d) = \\frac{1}{\\sqrt{\\alpha'_t}}(x_t - \\frac{\\beta'_t}{\\sqrt{1 - \\bar{\\alpha'_t}}}\\epsilon_\\theta(x_t, t, c_p, c_d))$,\n$\\alpha'_t = 1 - \\beta'_t$, \nWe can also simplify the Eq. (14) - (15) by computing\n$x_{t-1} = \\frac{1}{\\sqrt{\\alpha'_t}}(x_t - \\frac{\\beta'_t}{\\sqrt{1 - \\bar{\\alpha'_t}}}\\epsilon_\\theta(x_t, t, c_p, c_d)) + \\sqrt{\\beta'_t}z$,\nwhere z ~ N(0, I). The whole process can be summarized as Algorithm. 2.\nAlgorithm 2 Reverse Diffusion Process\n1: given Cp, Cd\n2: Tcond Eq. (5)\n3: {1, ..., BTcond} \u2190 Eq. (7) \u2013 Eq. (9)\n4: XTcond ~ N(0, 1)\n5: for t = Tcond, ..., 1 do\n6: z ~ N(0, I) if t > 1 else z = 0\n7: Xt-1 Eq. (16)\n8: end for\n9: return xO"}, {"title": "4 Experiment", "content": "4.1 Experimental Setup\n4.1.1 Datasets\n\u2022 Cifar-10 [53]: Cifar-10 contains 60000 real images of 10 categories, each one of them is a 32 \u00d7 32 color image. The dataset is pre-divided into the training and testing set with the ratio as 5 : 1. In our experiments, we take the category name of each image as the text prompt, and the corresponding edge map as the additional condition to control the generation. The generation evaluation is conducted on the testing set of it.\n4.1.2 Evaluation Metrics\nThe evaluation of our model includes two aspects: 1) generated image quality, which can be categorized into the following metrics: Fr\u00e9chet Inception Distance (FID, \u2193) [54], CLIP Score (C-Score, \u2191) [55] and CLIP Aesthetic Score (C-Aes., \u2191) [56]. 2) generation efficiency, including Average Diffusion Time-Steps (#Steps, \u2193) and Average Execution Time (Time,\u2193).\n4.1.3 Implementation Details\nIn our implementation, we assume that the text prompts mainly constrain the target category to be generated, and the input condition provides a spatial clue about an expected image. To realize these functions, the text prompts we used are category names and image conditions are edge maps to mimic the sketch given by users. To embed these two inputs, we utilize the text and image encoders of a pre-trained CLIP-ViT-B/32 model. The other newly-added modules, including the cross-model fusion in CTS and the combination parameter learner in AHNS, are implemented by two-layer MLP models with sigmoid activation. Besides our AC-Diff, all the models applied in experiments are trained from scratch with 500k iterations using the batch-size 96 and also tested on a single RTX-4090 GPU.\n4.2 Overall Performance\nThe overall comparison among different diffusion-based generative models is given in Table. 1. Since our model is built based on DDPM [16], we also include the unconditional diffusion models, DDPM [5] and DDIM, into our comparison as a reference. In order to add conditional control to these models, different from conventional studies, the extra text prompts and image conditions are not only fed into the pre-trained unconditional models during the generation process"}, {"title": "4.3 Ablation Study", "content": "In the remaining of this section, we will discuss the effectiveness of each component in our model according to the experimental performance, including conditional training, dynamic time-step and adaptive noise rescheduling.\n\u2022 Conditional Training: In Table. 1, by comparing the performance among DDPM, DDIM and their variants DDPM (cond r.), DDIM (cond r.), we find that using a pre-trained unconditional model and only directly adding extra conditions during generation, as commonly did in most previous conditional models, is not a good idea. The slight increase of CLIP Score, which represents the correlations among generated images and given conditions, is not comparable to the large decrease of the realism of image generated, reflecting in the reduction of FID and CLIP Aesthetic Score. The most important reason is that these models don't learn how to utilize the given conditions in their training phase. Instead, after we also integrate extra prompts and conditions into the training phase, and teach them how to be controlled by these clues, as did in DDPM* (cond f.&r.) and DDIM* (cond f.&r.), the models improve both the condition-correlations and the image qualities.\n\u2022 Dynamic Time-Step: Through experiments, our AC-Diff can use much fewer diffusion steps to generate images with the same level qualities. It proves that a fixed large number of steps is not always necessary. Besides, we find that blindly reducing the generation steps to the same number for all kinds of images will hurt the quality of them. Instead, adaptively deciding the steps is much more effective. To better illustrate such an adaption, we list the category-level average steps required for generation in Fig. 5.\n\u2022 Adaptive Noise Rescheduling: The generation process of any diffusion model starts from a total Gaussian noise each time, and gradually removes noise at each step. Since the total number of steps we used for generation are adaptive, the noise ratios cannot be the same for different generation tasks. Intuitively, the fewer the number of steps taken, the more noise should be removed at each step. To evaluate our design, we also compare the performance of AC-Diff models with different noise scheduling, including 1) the fully-adaptive noise ratios and 2) directly downsamping from pre-calculated noise ratios. The results are shown in Table. 2, and the fully-adaptive one outperforms the other."}, {"title": "4.4 Qualitative Study", "content": "To better illustrate the generation performance, we give some examples of images generated by our method in Fig. 6. In general, the quality of adaptively generated images is generally satisfactory with recognizable contents."}, {"title": "5 Conclusion", "content": "In this paper, we propose a new adaptive diffusion framework. Different from previous methods which commonly follow a fixed diffusion process, our model first estimates the number of diffusion steps needed according to the input prompts and additional conditions to estimate the complexity of the image to generate, based on which the number of steps needed for the diffusion is determined. In addition, we replace the commonly used pre-defined noise schedule, which is parameterized by diffusion rates, with a hybrid re-schedule module. By doing so, our model can largely reduce the computation when it is unnecessary while adding extra diffusion steps for a complex generation task to keep its performance. Through experiments, our method is proved to be able to keep the same generation quality with much less average calculation and execution time than those from the literature work. In the future, we will also apply our method to more complicated real-world data to test its effectiveness."}]}