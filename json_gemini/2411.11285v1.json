{"title": "Zero-Shot Automatic Annotation and Instance Segmentation using LLM-Generated Datasets: Eliminating Field Imaging and Manual Annotation for Deep Learning Model Development", "authors": ["Ranjan Sapkota", "Achyut Paudel", "Manoj Karkee"], "abstract": "Currently, deep learning-based instance segmentation for various applications (e.g., Agriculture) is predominantly performed using a labor-intensive process involving extensive field data collection using sophisticated sensors, followed by careful manual annotation of images, presenting significant logistical and financial challenges to researchers and organizations. The process also slows down the model development and training process. In this study, we presented a novel method for deep learning-based instance segmentation of apples in commercial orchards that eliminates the need for labor-intensive field data collection and manual annotation. Utilizing a Large Language Model (LLM), we synthetically generated orchard images and automatically annotated them using the Segment Anything Model (SAM) integrated with a YOLO11 base model. This method significantly reduces reliance on physical sensors and manual data processing, presenting a major advancement in \"Agricultural AI\". The synthetic, auto-annotated dataset was used to train the YOLO11 model for Apple instance segmentation, which was then validated on real orchard images. The results showed that the automatically generated annotations achieved a Dice Coefficient of 0.9513 and an IoU of 0.9303, validating the accuracy and overlap of the mask annotations. All YOLO11 configurations, trained solely on these synthetic datasets with automated annotations, accurately recognized and delineated apples, highlighting the method's efficacy. Specifically, the YOLO11m-seg configuration achieved a mask precision of 0.902 and a mask mAP@50 of 0.833 on test images collected from a commercial orchard. Additionally, the YOLO111-seg configuration outperformed other models in validation on 40 LLM-generated images, achieving the highest mask precision and mAP@50 metrics. In terms of computational efficiency, the YOLO11n-seg model achieved the fastest inference speed (compared to all tested configurations) at 3.8 ms among all tested configurations. These results confirm the potential of using synthetic datasets and zero-shot learning to train robust instance segmentation models, enhancing AI deployments in agriculture with improved scalability and efficiency. This method offers a viable alternative to conventional instance segmentation techniques, reducing the need for sensors, and extensive field image collection and labor-intensive manual annotation efforts while maintaining high accuracy in commercial orchard environments.", "sections": [{"title": "I. INTRODUCTION", "content": "Instance segmentation, a crucial computer vision technique that integrates object detection and semantic segmentation, provides a foundation for various automated or robotic solutions in a wide range of industries including Manufacturing, Transportation, Defense, Healthcare and Agriculture [1]. For example, advancement in medical imaging analysis [2], surgical planning with accurate organ mapping, disease diagnostics through detailed examination of skin lesions, blood cells, pathology samples, and dental X-ray analysis [3]\u2013[5] have been enabled by precise instance segmentation. In the transportation systems, precise instance segmentation is a key for smarter traffic management through vehicle and pedestrian monitoring, which is a key for ensuring safety, and enhancing efficiency [6]\u2013[8]. Other example applications in transportation include automating parking space allocations, improving pedestrian safety [9], [10] and optimizing railway [11], [12], and airport operations [13]\u2013[15].\nLikewise, instance segmentation has been a key in advancing retail operations through enhanced customer engagement and operational efficiency [16]. It enables real-time monitoring of product placements and inventory [17], [18], optimizes store layouts through customer interaction analysis [19], [20], and improves theft detection [21], [22], thereby significantly enhancing both the shopping experience and store management. Furthermore, instance segmentation techniques are crucial for improving quality control and operational efficiency [23], [24] in manufacturing. The techniques can be used to facilitate verification of component assembly [25]\u2013[27], ensure packaging integrity [23], [28], detect surface defects [29]\u2013[31], guide robotic operations for better navigation and task execution [32], [33], and refine precision machining practices [34], [35], all of which contribute substantially to product quality while minimizing waste and operational costs. In addition, instance segmentation is instrumental in security surveillance, as it facilitates the accurate identification and tracking of individuals or objects over space and time [36], [37]. Instance Segmentation aids in the precise identification and tracking of individuals in crowded environments [38], [39] as well, which helps detecting unauthorized activities through anomaly detection [40], and automating vehicle recognition for traffic surveillance [11], [41].\nIn agriculture, precise instance segmentation techniques are"}, {"title": "2", "content": "essential to advance automated and robotic operations for various field operations to achieve high productivity and produce quality in a sustainable manner. Instance segmentation can provide detailed and precise representation and localization of plant structures, which facilitates the detailed analysis of plant growth and health, impacting everything from yield estimation to disease management to robotic crop management [1]. Zhang et al. 2020 ([42], [43]) highlight its utility in robust crop monitoring, which is essential for developing targeted interventions to control pests and manage water and nutrients. Similarly, Champ et al. (2020) [44] emphasized the importance of precise instance segmentation to innovate robotic solutions for various agricultural practices such as the thinning of immature fruits, and pollinating crop flowers showcasing its applicability in real-world agricultural operations. Likewise, past studies such as [44] have shown how convolutional neural networks (CNNs) can help reliably distinguish crops and weeds, offering a viable alternative to traditional herbicide broadcasting through targeted weed management.\nIn recent years, various instance segmentation techniques have been used in developing novel solutions to address agricultural challenges. [45] demonstrated the capability of deep learning-based instance segmentation to manage high variability in orchard images and to improve operational efficiencies in orchard environments. [46] utilized instance segmentation to enhance cucumber detection in greenhouses, while [47] and [48] developed systems for real-time root crop yield estimation and efficient strawberry picking, respectively. Additionally, [49] and [50] improved weed management and center pivot irrigation systems detection using instance segmentation in remote sensing data. Furthermore, [51] introduced a model called FoveaMask for robust green fruit segmentation in instance segmentation. These studies demonstrate the broad applicability and effectiveness of instance segmentation technologies in diverse agricultural settings. Concluding these advancements, [52] reviewed deep learning approaches in instance segmentation, emphasizing their accuracy and robustness in crop stress and growth monitoring.\nInstance segmentation often faces significant challenges due to the limited availability of labeled data, which typically requires extensive and labor-intensive data preparation and manual annotation. Addressing this challenge, few-shot learning has gained prominence as a method that trains models to perform tasks with a minimal amount of labeled examples, marking a shift from conventional methods. Few-shot learning enables the model to learn from a small number of training samples, reducing the dependency on extensive datasets and mitigating the need for laborious manual annotation. [53] introduced the Fully Guided Network (FGN), a novel architecture that combines few-shot learning with Mask R-CNN. FGN employs tailored guidance mechanisms within Mask R-CNN to optimize inter-class generalization, significantly outperforming previous state-of-the-art methods by leveraging a support set. Additionally, [54] developed an incremental few-shot instance segmentation model, iMTFA, which utilizes discriminative embeddings to add new classes efficiently without the need for retraining, while also minimizing memory utilization by storing embeddings rather than images. This method"}, {"title": "2", "content": "greatly enhances scalability and adaptability. Moreover, [55] proposed Reference Twice (RefT), a unified transformer-based framework that prevents overfitting and boosts the few-shot instance segmentation process through simple cross-attention mechanisms that connect support and query features. Their approach achieved substantial performance improvement across various settings, deminstrating the effectiveness of their class-enhanced base knowledge distillation loss in bridging the gap between resource-heavy traditional methods and more efficient, scalable machine learning techniques.\nRecent studies have demonstrated significant strides in enhancing model adaptability with limited data through few-shot learning which is a technique that trains models to predict accurately with minimal labeled data, a shift from traditional methods that need large datasets [56], [57]. This technique uses only a few examples per class, making it ideal where collecting extensive data is impractical [58]. It includes approaches like Siamese networks for sample comparison, meta-learning for quick task adaptation, and transfer learning to fine-tune pre-trained models on small datasets. Recent advancements have notably enhanced model adaptability with limited data through these methods. [59] addressed automated cellular instance segmentation by utilizing specialized contrastive losses that effectively leverage a minimal number of annotations. This approach significantly mitigated covariate shift effects, demonstrating that few-shot learning can produce competitive results even with limited data, reducing the dependence on extensive datasets and computational resources. Similarly, [60] developed the Dynamic Transformer Network (DTN), which uses Dynamic Queries conditioned on reference images to directly segment target instances, eliminating the need for dense proposal generation and extensive post-processing. This end-to-end method improves optimization and generalization in few-shot instance segmentation, with a Semantic-induced Transformer Decoder that effectively minimizes background noise, leading to significant improvements when tested on the COCO-20 dataset. Furthermore, [61] extended the Mask-RCNN framework with iFS-RCNN for incremental few-shot instance segmentation, introducing a probit-based classifier and an uncertainty-guided bounding box predictor. This novel approach uses Bayesian learning to handle scarce data for new classes and incorporates advanced loss functions.\nFurther advancing these models, various innovative methodologies have been developed in recent years for zero-shot learning approach [62]\u2013[64], which performs instance segmentation without reliance on labeled data for unseen classes. [65] proposed a comprehensive approach for Zero-Shot Instance Segmentation (ZSI), which is an emerging area in deep learning that aims to segment and identify object instances in images without having seen examples of those specific object classes during training. Their approach integrates a Zero-shot Detector, Semantic Mask Head, Background Aware RPN, and a Synchronized Background Strategy, establishing a new benchmark for ZSI on the MS-COCO dataset with promising results. Meanwhile, [66] introduced Zero-shot Unsupervised Transfer Instance Segmentation (ZUTIS), a framework that operates without instance-level annotations and achieves good"}, {"title": "II. METHODS", "content": "The proposed method of developing and evaluating an instance segmentation method in commercial apple orchards utilizing synthetically generated and automatically labeled images is depicted in Figure 2a. The method began with utilizing the realistic apple orchard images generated by the LLM (DALL.E), a multi-modal LLM, recognized for producing high-fidelity visual content. Subsequently, these images underwent a zero-shot learning with a YOLO11 object detection model trained only on COCO benchmark dataset that identified apples by creating bounding boxes around them. An image processing technique was then developed to estimate apple boundaries using the Segment Anything model [74] for semantic segmentation within the bounding boxes created by zero-shot YOLO11 model. Apple boundary annotations generated in this process were automatically saved in .txt format, forming a foundational dataset for further model training. This dataset was then used to train the YOLO11 instance segmentation model, specifically tailored to improve the accuracy of apple segmentation. To validate the model's performance in practical orchard environments, it was tested against real-world images from commercial orchards, captured using a machine vision system. Furthermore, for a comprehensive comparison of automatically generated masks against manually annotated ground truths, the LLM-generated images were subjected to both manual and automatic annotation using the proposed method. An additional dataset collected by a machine vision sensor from a commercial apple orchard was similarly annotated using both manual and automated processes and compared to assess the performance of the proposed method.\nIn summary, the methodology employed in this research included five sequential stages: 1) Generation of Synthetic Images Using Large Language Models (LLMs), 2) Zero-Shot Detection using YOLO11, 3) Automated Mask Annotation Generation using SAMv2, 4) Training YOLO11 Instance Segmentation Models, and 5) Performance Evaluation and Validation in a Commercial Apple Orchard."}, {"title": "A. Study Site and Data Acquisition", "content": "The study site for the validation of the deep learning model developed for instance segmentation of apples was a commercial 'Scifresh' apple orchard located in Prosser, Washington State, USA. The data acquisition occurred in October 2024, during the peak harvest season, as depicted in Figure 2b. This orchard was characterized by tree rows spacing of 10 feet and individual tree spacing of 3 feet (Figure 2b). Although no imaging was utilized during the development phase of the model, validation was conducted on-site as shown in Figure 2b. Images were captured using a machine vision camera (Microsoft Azure Kinect DK 2d), which was mounted on a robotic platform, detailed in Figure 2c. The robotic platform comprised a Universal Robotic arm Ur5e affixed to a ground robot Warthog, provided by Clearpath Robotics of Ontario, Canada. The study site for the validation of the deep learning model developed for instance segmentation of apples was a commercial 'Scifresh' apple orchard located in Prosser, Washington State, USA. The data acquisition occurred in October 2024, during the peak harvest season, as depicted in Figure 2b. This orchard was characterized by tree rows spacing of 10 feet(3 meters approx) and individual tree spacing of 3 feet (Figure 2b). Although no imaging was utilized during the development phase of the model, validation was conducted on-site as shown in Figure 2b. Images were captured using a machine vision camera (Microsoft Azure Kinect DK 2d), which was mounted on a robotic platform, detailed in Figure 2c comprised of a Robotic arm (Ur5e, Universal Robotics)"}, {"title": "B. Generation of Synthetic Images Using Large Language Model", "content": "In this study, the synthetic image generation process utilized the DALLE model to create realistic orchard scenarios with text prompts without physical data collection, as detailed in our previous study [69]. A total of 524 images from our previous study [69] (as shown in right side of Figure 1) depicting various states and formations of apples were generated based on succinct, precise textual prompts, and each image measured 1024 by 1024 pixels. These prompts efficiently guided the DALL.E model to produce visually accurate orchard scenes, and the images generated were rigorously reviewed to ensure realism and relevance for subsequent analysis. The LLM-generated dataset can be found in github link: https://github.com/ranzosap/Synthetic-Meets-Authentic."}, {"title": "C. Zero-Shot Detection using YOLO11", "content": "The model was applied to detect apples without prior exposure to a specific apple dataset, employing a method known as zero-shot detection [75], [76]. This machine learning approach enables a model to recognize and categorize objects that it"}, {"title": "D. Automatic Generation of Mask Annotation using Segment Anything Model (SAMv2)", "content": "As a foundational model, SAM [74] uses masked autoencoders for robust image encoding, coupled with a versatile prompt encoder that handles various input types such as points, boxes, masks, and text [80], [81]. This model is distinctive for its ability to generate precise segmentation masks based on minimal input, demonstrating a powerful zero-shot capability that requires no prior training on specific objects [82], [83]. SAM's architecture is built around several core components: a high-capacity image encoder that uses a vision transformer to process images into dense embeddings, and a prompt encoder that interprets sparse and dense prompts [74]. The image encoder is particularly notable for its use of a ViT-H/16 vision transformer, which manages to efficiently downscale images to a more manageable size while retaining critical visual information. The prompt encoder handles a range of input types, converting them into formats that can be effectively used to modify image embeddings. The mask decoder is where SAM's innovative segmentation capabilities are realized [82], [84]. This component integrates image and prompt embeddings to produce final segmentation masks. It utilizes a Transformer-based decoder architecture that includes self-attention and cross-attention mechanisms, allowing it to refine the segmentation output in response to the input prompts. To train such a sophisticated model, the creation of a suitable dataset was a considerable challenge. The development team employed a novel model-in-the-loop annotation system that evolved from assisted-manual annotations to fully automated mask generation. This approach not only streamlined the dataset creation process but also ensured that SAM could be trained on a vast array of images with corresponding high-quality masks, facilitating its zero-shot transfer capabilities.\nIn this study SAMv2 was used to generate precise mask annotations for instance segmentation of apples using the apple bounding boxes identified by zero shot YOLO11 (Figure 3a) (Figure 3b).\nFollowing the zero-shot detection by the YOLO11 model, the methodology for automatic annotation of apples was executed using the SAMv2. Initially, the YOLO11 model,"}, {"title": "E. Evaluation of Automatic Annotation Method", "content": "As discussed before, the mask annotation method was developed using YOLO and SAM models. Each image was first subjected to object detection via the YOLO model, which identified potential apple regions (bounding boxes) with a minimum confidence threshold of 0.3. These bounding boxes served as inputs for the SAM2 model, which then estimated segmentation masks within the defined regions. The taken for each step of the preprocessing, inference, and postprocessing steps were recorded for assessing the computational efficiency of the model.\n1) Metrics and Statistical Evaluation: The quantitative evaluation involved computing specific metrics across the dataset to gauge the model's performance. These metrics included were the average number of detections per image and the mean confidence of detections, defined as:\n$\\begin{equation}\nN_d = \\frac{1}{n}\\sum_{i=1}^{n} count(detections_i)\n\\end{equation}$\n$\\begin{equation}\nC_{avg} = \\frac{1}{n}\\sum_{i=1}^{n} mean(confidence_{detections_i})\n\\end{equation}$\nwhere $n$ is the total number of images processed, $detections_i$ denotes the detections in the $i^{th}$ image, and $confidence_{detections_i}$ is the confidence level associated with each detection. These metrics provide insight into the model's detection and segmentation capabilities within LLM-generated orchard environments. A high average confidence indicates robust detection capabilities, while the number of detections"}, {"title": "7", "content": "reflects the model's precision and effectiveness in segmenting apple instances.\nAdditionally, the performance of the automatic annotation technique was conducted on a set of test images which were both manually and automatically annotated for comparing those masks.\nInitially, a random selection of 40 images from a set of 501 LLM-generated images was annotated manually. This annotation process involved delineating the mask for each apple using a manual annotation tool available in Roboflow (Roboflow, IOWA, USA). Concurrently, the same images were subjected to the proposed automatic annotation process and compared with the manual annotation. Based on the manual and automatic mask annotations, Average Precision, Average Recall, Average F1-Score, Average Dice Coefficient, and Average IoU were calculated as follows.\n\u2022 Average Precision, Average Recall, and Average F1-Score were derived from the counts of True Positives (TP), False Positives (FP), and False Negatives (FN) across all images, calculated by:\nAverage Precision = $\\frac{1}{N}\\sum_{i=1}^{N} \\frac{TP_i}{TP_i + FP_i}$  (4)\nAverage Recall = $\\frac{1}{N}\\sum_{i=1}^{N} \\frac{TP_i}{|TP_i + FN_i|}$ (5)\nAverage F1-Score = $2 \\times \\frac{(\\frac{1}{N} \\sum_{i=1}^{N} \\frac{TP_i}{TP_i+FP_i}) \\times (\\frac{1}{N}\\sum_{i=1}^{N} \\frac{TP_i}{TP_i+FN_i})}{\\frac{1}{N} \\sum_{i=1}^{N} \\frac{TP_i}{TP_i+FP_i} + \\frac{1}{N}\\sum_{i=1}^{N} \\frac{TP_i}{TP_i+FN_i}}$ (6)\nwhere N is the total number of images in the dataset.\n\u2022 Average Dice Coefficient and Average IoU were calculated by averaging the respective metrics for each pair of predicted and ground truth masks across all images:\nAverage Dice Coefficient = $\\frac{1}{N} \\sum_{i=1}^{N} \\frac{2 \\times TP_i}{2 \\times |TP_i| + |FP_i| + |FN_i|}$ (7)\nAverage IoU = $\\frac{1}{N} \\sum_{i=1}^{N} \\frac{A_i \\cap B_i}{A_i \\cup B_i}$ (8)\nwhere $A_i$ and $B_i$ represent the automatic and manual mask areas for the $i^{th}$ image, respectively.\nThese metrics together provided a comprehensive assessment of the model's ability to perform instance segmentation without manual intervention, emphasizing the reduction in time and labor typically required for dataset collection and preparation in deep learning applications."}, {"title": "F. Training the YOLO11 Instance Segmentation Model", "content": "Once mask annotations were created automatically as discussed above, various configurations of YOLO11 instance segmentation models (Models used in recent studies [45], [69], [85], [86]) were fine-tuned for apple instance segmentation using the automatically generated annotations as shown in Figure 4. The five model configurations used were: YOLO11n,"}, {"title": "G. Performance Evaluation of YOLO11 Instance Segmentation", "content": "The efficacy of instance segmentation was assessed by calculating Mask Precision, Recall, and mean Average Precision (mAP) at a 50% Intersection over Union (IoU) threshold (mAP@50). These metrics were derived based on the overlap between the predicted masks and the ground truth annotations. Mask Precision, which quantifies the proportion of correctly identified positive predictions, is defined as:\nMask Precision =$\\frac{TP}{TP + FP}$ (9)\nMask Recall measures the ability to detect all relevant instances:\nMask Recall = $\\frac{TP}{TP + FN}$ (10)\nmAP@50 represents the mean AP at the 50% IoU threshold, providing a balanced view of precision and recall across different decision thresholds.\nAdditional performance metrics assessed included image processing speeds for preprocessing, inference, and post-processing stages, as well as training time. The architectural complexity of each model was also assessed in terms pf the number of convolutional layers, the total number of parameters, and the computational load in gigaflops (Giga Floating Point Operations per Second or GFLOPs). The architectural complexity and computational efficiency of the YOLO11 was assessed by examining three critical aspects: the number of parameters, GFLOPs, and the count of convolutional layers utilized in each configuration. These factors are essential"}, {"title": "9", "content": "indicators of a model's potential performance and operational demands.\nParameters, representing the total count of trainable elements within the model, were evaluated to understand the models' complexity and memory requirements:\nThe model parameters are defined as:\nParametersModel = Trainable weights + biases (11)\nGFLOPS, representing the computational load during the inference phase, are calculated to provide insights into the model's efficiency and speed:\nGFLOPS =$\\frac{Total floating-point ops.}{10^9}$per image (12)\nThe architectural depth, indicated by the number of convolutional layers which influences feature extraction capabilities, is given by:\nLayersConv = Total conv. layers (13)"}, {"title": "III. RESULTS", "content": "The result of the automatic mask annotation method is presented in Figure 5, where panel Figure 5(a) shows an example apple orchard image generated by the LLM. Panel 5(b) illustrates the effectiveness of automated mask annotation following the integration of zero-shot YOLO11 model and SAMv2. The LLM-generated images show a high degree of realism, underlining the potential of our method for generating training dataset for instance segmentation models in agricultural settings. The fusion of YOLO11's zero-shot detection capabilities with SAM's precise mask-generation techniques led to promising outcomes in automated image annotation. Qualitatively, our approach successfully identified and annotated both fully and partially visible apples within the complex orchard scene, a task that would otherwise demand substantial manual labor.\nAs mentioned in the methods section, the quantitative performance of the automatic mask annotation method was evaluated using manual annotations of apples in 40 synthetic and 42 real orchard images. Figure 5(c) highlighted the effectiveness of our approach for training deep learning models for instance segmentation. With the synthetic (LLM Generated) Dataset, the proposed method achieved high Dice Coefficients and IoU values of 0.88 and 0.86, respectively, demonstrating strong overlap and accuracy of mask annotations. For orchard images (Real Field Images) collected with a Microsoft Azure Kinect camera, the method reached an even higher precision of 0.91, although the recall was lower at 0.61. These metrics demonstrated the potential of our automatic annotation approach, across both synthetic (LLM-generated Images) and complex real-world environments.\nQualitatively, the automated mask annotation process detected and masked most of the apples as shown in Figure 5 (b). On average, the model successfully detected and masked 8 objects (apples) per image with a high average confidence of 0.80. The computational time (particularly the inference time) per image increased with the increasing number of target objects in those images. On average, the model took 4.5 ms, 1,986.4 ms (1.9 seconds), and 1.1 ms per image respectively for pre-processing, inference, and post-processing respectively."}, {"title": "A. Performance of YOLO11 Instance Segmentation Model against Automatic Annotation", "content": "Figure 6 shows the precision-recall curves of the five configurations of the YOLO11 instance segmentation models over the LLM-generated synthetic dataset. Each sub-figure from (a) to (e) corresponds to different model configurations (YOLO11n-seg, YOLO11s-seg, YOLO11m-seg, YOLO111-seg, and YOLO11x-seg), showcasing mean Average Precision (mAP@0.5) values close to 0.92 across all classes. Table I shows the detailed evaluation of box and mask metrics for the detection and instance segmentation of apples in the LLM-generated dataset. Among the trained models, the YOLO111-seg model achieved the highest performance (See table I) on the synthetic test dataset (40 LLM-generated images), with a mask precision of 0.93 and overall mask mAP@50 of 0.92.\nThese results illustrate the efficacy of each YOLO11 configuration in handling instance segmentation on synthetic datasets of apple orchards. The YOLO111-seg configuration stands out with the highest performance, but the other models also show commendable capabilities, each achieving a mean Average Precision (mAP@0.5) close to 0.92. This indicates a robust ability to accurately detect and segment apples across various model complexities. The nearly uniform high performance across these configurations underscores their practical applicability in agricultural settings, offering effective solutions even with limited computational resources. These results underscore the potential for deploying these models in real agricultural operations to facilitate tasks like fruit counting and disease identification without the need for extensive manual intervention. By leveraging synthetic datasets of apple orchards, agricultural producers can reduce both costs and complexity, making advanced AI technologies more accessible and feasible for widespread use in challenging and resource-sensitive environments.\nFigure 7 provides an example of qualitative performance of YOLO11n-seg instance segmentation on highly realistic images generated by the DALL.E model. The top section of the figure displays LLM-generated images depicting varying orchard scenarios, while the bottom section shows the outcomes of YOLO11 instance segmentation applied to these images. The qualitative successes demonstrated in Figure 7 highlight the YOLO11n-seg model's robust capacity to accurately identify and segment apples within complex and varied orchard scenarios. For instance, as seen in 7a, the model proficiently detected and segmented apples positioned in the background, confirming its effectiveness in handling distant objects. Moreover, Figure 7b showcases the model's capability to consistently detect and segment nearly all visible apples, even amidst the challenging conditions of a densely populated orchard scene. Figure 7(a) (highlighted in red dotted line), an apple-like region in a sample LLM-generated image was not accurately identified by the YOLO11 model, though the model successfully identified and segmented other apples in the background. This example illustrates both the capabilities and limitations of applying advanced instance segmentation techniques based on automatically annotated synthetic datasets.\nAlso, Figure 7 b demonstrates a challenging scenario where"}, {"title": "C. Field Validation of YOLO11 Instance Segmentation Model using Azure Camera Images", "content": "To assess the model's capability to be applied to actual orchard environments, a field evaluation was performed in a commercial apple orchard using an Azure machine vision camera as shown in Figure 9. A dataset of 42 test images was collected using the camera installed on a robotic platform as discussed in methods. These images were manually annotated to delineate all the apples, and were compared against the apple masks segmented by the models trained exclusively on LLM-generated, automatically annotated dataset. The detailed metrics calculation based on the field validation experiment are presented in Table III. Among the five YOLO11 configurations tested, YOLO11m-seg demonstrated superior performance, particularly in mask metrics, where it achieved a mask precision of 0.902 and a mask mAP@50 of 0.833 (Table III). This model configuration (as well as other configurations) showed good adaptability to real orchard environments, effectively delineating apple instances in images collected by a machine vision camera despite the inherent challenges posed by natural orchard environments. The reasonable high precision and recall demonsrated the potential of YOLO11m-seg for practical agricultural applications, confirming its efficacy in handling complex visual data enabling automated agricultural monitoring and interventions.\nFigure 9 presents the sample results of the YOLO11 instance segmentation models on orchard images acquired using a Microsoft Azure camera. Figure 9 is composed of four layers to provide a comprehensive view of the model's application in real-world settings. The topmost layer captures the original orchard imagery, succeeded by the YOLO11n model's detection and segmentation outcomes, visually demonstrated using automatically generated images and annotations. Subsequent layers include a heatmap depicting segmentation intensity and a binary image that delineates the segmented apples, illustrating the model's effectiveness in complex environments. This structured display highlights the model's proficiency in identifying distant apples despite their size or relative distance. Yet, challenges persist, as indicated by the misidentification of shadow-cast foliage as apples in red-circled areas, suggesting areas for future refinement to improve accuracy under varied orchard conditions.\nFigure 9b vividly demonstrates the YOLO11 instance segmentation model's effectiveness within a commercial orchard setting. In the yellow dotted regions, the model excelled by accurately detecting and segmenting apples that were only"}, {"title": "IV. DISCUSSION", "content": "In 11, the outcomes of applying our zero-shot learning-based instance segmentation model to commercial apple orchards are illustrated, with images captured via a robotic platform equipped with a Microsoft Azure machine vision camera. This visual representation showcases different examples of field-level instance segmentation, performed by a model trained exclusively on synthetic datasets generated by LLM and automatically annotated through SAM-YOLO11 fusion. This approach marks a significant departure from traditional methods that rely heavily on sensor-based field data collection and manual annotations.\nThe images in the figure, annotated with red and yellow arrows, highlight the model's capabilities and limitations. Red arrows indicate areas where the model failed to segment occluded apples due to complex environmental conditions, suggesting a need for richer datasets. This study, limited to 501 synthetic images, highlights the potential for expanding data generation using LLM capabilities, potentially integrating multimodal inputs such as textual prompts and voice. The yellow arrows denote false segmentations where canopy foliage was incorrectly identified as apples, underscoring the importance of generating more realistic training images to improve model accuracy.\nOur approach contrasts sharply with recent advancements in few-shot and zero-shot learning that still require some form of initial data handling or manual intervention. For instance, studies like those by [59] and [60] have made significant strides in using few-shot learning to reduce data dependence, but they still rely on limited data collection and manual annotations. Similarly, zero-shot models like those proposed by [65] and [66] eliminate the need for labeled data for unseen classes but have not been applied to completely synthetic datasets for training, as in our study.\nThis research extends the boundary of what is possible with zero-shot learning in agriculture by demonstrating that robust instance segmentation models can be developed and deployed without any field data collection or manual labeling, setting a new standard for scalability and adaptability in agricultural AI applications. The integration of LLM-generated datasets and zero-shot learning significantly reduces the time, cost, and logistical challenges typically associated with model training and deployment, offering a novel and effective solution for real-world agricultural monitoring and intervention tasks.\nOur study's uniqueness and strength lie in its complete independence from physical data collection and manual annotation processes, distinguishing it from the incremental improvements reported in existing literature. While the works of [61], [65], and others represent critical steps towards reducing labor in model training, they have not achieved the level of autonomy in model preparation and deployment that this research has demonstrated. By leveraging synthetic data and automatic annotations, our model not only simplifies the preparatory phases of machine learning but also enhances the feasibility of deploying advanced AI solutions across varied and resource-constrained agricultural settings, embodying the next level of innovation in agricultural technology.\nAdditionally Our approach introduces a novel paradigm that transcends traditional data augmentation techniques commonly employed in training deep learning models. Unlike simple geometric or photometric transformations [87]\u2013[89], synthetic data generation through LLMs offers a more expansive and diverse set of training scenarios, potentially covering a broader range of object appearances and conditions not typically available in existing datasets [69], [90]. This method not only enhances the robustness of the model against varied real-world conditions but also reduces the bias inherent in limited dataset compositions.\nFurthermore, the capability to quickly adapt models to new agricultural challenges without the prerequisite of localized data collection presents strategic advantages [91], [92], especially in emerging markets and in scenarios demanding rapid responses to agricultural threats [93]. This flexibility could dramatically shorten response times in managing outbreaks of pests or crop diseases, which is crucial for preventing widespread damage in vulnerable regions [94], [95]. Furthermore, the application of zero-shot learning models in these contexts underscores the potential for AI to deliver high-impact solutions with minimal logistical overhead, thereby democratizing access to cutting-edge technology across varying economic landscapes."}, {"title": "V. CONCLUSION AND FUTURE", "content": "This study successfully developed a robust deep learning model for instance segmentation in commercial orchard environments without the traditional reliance on sensor data collection or manual annotations. Through the integration of a Large Language Model (LLM) with the Segment Anything Model (SAM) and a zero-shot YOLO11 base model, we generated and annotated synthetic images of apple orchards. This innovative approach facilitated the training of the YOLO11 model, which demonstrated high accuracy when validated against a manually annotated commercial orchard dataset. The results underscored the potential of synthetic datasets and zero-shot learning to significantly enhance the scalability and efficiency of AI deployments in agriculture. Here are the major"}]}