{"title": "Hidformer: Transformer-Style Neural Network\nin Stock Price Forecasting", "authors": ["Kamil \u0141. Szyd\u0142owski", "Jaros\u0142aw A. Chudziak"], "abstract": "This paper investigates the application of Transformer-based neural networks to stock price forecasting, with a special focus on the intersection of machine learning techniques and financial market analy-sis. The evolution of Transformer models, from their inception to their adaptation for time series analysis in financial contexts, is reviewed and discussed. Central to our study is the exploration of the Hidformer model, which is currently recognized for its promising performance in time se-ries prediction. The primary aim of this paper is to determine whether Hidformer will also prove itself in the task of stock price prediction. This slightly modified model serves as the framework for our experiments, integrating the principles of technical analysis with advanced machine learning concepts to enhance stock price prediction accuracy. We con-duct an evaluation of the Hidformer model's performance, using a set of criteria to determine its efficacy. Our findings offer additional insights into the practical application of Transformer architectures in financial time series forecasting, highlighting their potential to improve algorith-mic trading strategies, including human decision making.", "sections": [{"title": "Introduction", "content": "The stock market, commonly perceived as a venue for investment and potential profit (or loss) by the average person, is underpinned by the intricate field of mar-ket analysis. This discipline is dedicated to forecasting financial market behaviors using diverse methodologies. Some analysts rely on economic fundamentals to evaluate companies, while others interpret market trends directly from charts, suggesting that these visuals encapsulate all necessary information [10].\nIt should be noted that the topic of stock market price prediction has consis-tently maintained its popularity in the field of artificial intelligence research over years. The most popular proposals in the literature in recent years are artificial neural networks - especially models and gates based on the principle of recur-sion, but also others: both traditional ones (including convolutional networks) and highly specialized architectures, which with the help of knowledge transfer can achieve high results in this field as well [3, 5, 11]. So far, analyses have shown that hybrid projects have the potential to achieve better results than mono so-lutions [5, 11]. Currently, however, complex models such as the Transformer are also being studied for this purpose [13].\nThis article aims to evaluate Hidformer's [8] effectiveness in stock price fore-casting as a modification of the Transformer model [12] for time series prediction. Our experiments demonstrate that Hidformer either outperforms or matches ex-isting basic models based on standard metrics and visual analytics. In the first place we will briefly outline the concept of technical analysis of stock markets (Section 2), and then we will provide an overview of Transformer neural net-works, moving from the original model to ideas focused on time series processing, including stock market prediction (Section 3). In particular, we will describe the Hidformer model (Section 4), currently considered as the very promising NN ar-chitecture in time series prediction, which will serve as the basis for experiments (Section 5). And finally, we will discuss results of the conducted experiments, outlining the scope for future research as well (Section 6)."}, {"title": "Analysis of Financial Stock Markets", "content": "Market dynamics analysis aims to forecast the behavior of financial markets [10]. This section recalls the main notions and elements of technical analysis, as well as the assumptions of basic deep learning techniques in this field."}, {"title": "Preliminaries", "content": "Technical analysis studies market behaviors, primarily using charts, to predict future price trends. This approach relies on the interpretation of price and volume data. By examining historical price movements and trading volumes, analysts identify patterns and trends that suggest future market behavior.\nThe justification for using technical analysis as a tool for market analysts is based on three premises [10]. First, the market discounts everything, mean-ing that all factors affecting market prices are reflected in those prices. Second, prices move in trends, indicating that an existing trend will continue until it reverses. And third, history repeats itself, based on the assumption that historical patterns, such as the head and shoulders formation [10], are believed to reoccur and can predict future market behavior. However, we are aware of the computational and economic limitations of these assumptions [1].\nTechnical analysis can be applied using various valuation frequencies, includ-ing high-frequency valuations, which analyze market movements in fractions of a second; inter-day valuations, which consider closing prices over multiple days; and intra-day valuations, which examine price movements within a single trad-ing day. For our study, we assume a set of parameters based on daily valuations, which provide a balance between capturing market trends and managing data complexity [11]."}, {"title": "Market analysis with deep learning neural networks", "content": "In recent years, with the intensive development of deep learning methods and algorithms, many solutions have been proposed for stock price prediction and algorithmic trading. Among the most widely used and discussed were [2, 5]:\n1. Convolutional Neural Networks (CNN). In the context of stock prices, they can be used as analyzers of sequential data [5, 6]. In other words, if there are distinctive raw features in the data, the network is capable of extracting standard formations that can be the basis of a strategy.\n2. Recurrent Neural Networks (RNN). They process individual input data elements and their connections with previous elements. In this way, they retain information about the context [4, 6]. As a result, they are potentially a good choice for predicting stock data, as changes in quotes over a given time are often related to previous trends [5, 6].\n3. Long Short-Term Memory (LSTM). Given that stock price prediction might be represented as a nonlinear process, it seems that the LSTM gate, which is one of the specialized recurrent architectures, is capable of effectively uncovering correlations between nonlinear time series and the prediction target [5].\n4. Deep Neural Networks (DNN). Discerning their proficiency in handling nonlinear problems, it must be emphasized that these models might also be used for stock price prediction [5].\n5. Transformers: Recently, transformers have emerged as a powerful architec-ture for stock price prediction due to their ability to handle sequential data efficiently and capture long-term dependencies [13]."}, {"title": "Related work", "content": "After the innovative Transformer neural network model [12] was presented in 2017, it was acknowledged that this model represented a significant advancement in the use of the attention mechanism. Previously used recurrent architectures exhibited major limitations when working with long sequences. The Transformer architecture solved these issues by introducing self-attention, multi-head atten-tion, and positional encoding, which improved the model's ability to encode and process sequential data efficiently.\nAmong the applications explored by the authors were machine translation and syntactic analysis of natural language. However, they also recognized that this model was worth testing in multimodal sequences (images, sound, and video). The key to the Transformer's success turns out to be its ability to process sequential data, which is of particular significance in our considerations."}, {"title": "Transformer model in time series prediction", "content": "The initial effort to adapt the Transformer model for time series analysis occurred in 2019 [7]. Despite identifying the primary limitations of the architecture, this pioneering work also proposed solutions to address these challenges:\n1. Locality-agnostics. The point-wise dot-product self-attention turned out to be insensitive to the local context of a given point and was also very prone to single anomalies - outliers as a result of noise. These issues were resolved by the mechanism of convolutional self-attention, which considered several local values.\n2. Memory bottleneck. Due to the fact that memory complexity grows quadrat-ically, it became apparent that there was a lack of capability to model long, much longer than sentences, sequences. However, even in this context, a correction was introduced by replacing full self-attention with sparse self-attention, which only considered the most valuable elements of the sequence.\nIndependently of the aforementioned studies, in 2021, an attempt was made to adapt the original concept of the Transformer for time series prediction. In particular, the authors decided to [9]:"}, {"title": "Transformer in predicting stock prices", "content": "In 2022, an attempt was made to apply the Informer model in the task of predicting stock prices [13]. The authors highlighted the Informer's advantages of the multi-head attention mechanism. Its effectiveness was particularly evident in efficiently capturing relevant information and filtering out irrelevant noise, which are especially abundant in financial time series, and in the strong ability to extract key features, leading to better forecasting performance.\nIn the research, its performance was compared with traditional deep learning models (CNN, RNN, and LSTM), where Informer demonstrated higher accuracy significantly in predicting the first future quote in all back-testing experiments on the main stock market indices worldwide, including CSI 300, S&P 500, Hang Seng Index, and Nikkei 225, what enabled investors to gain excess earnings."}, {"title": "Hidformer solution for stock price prediction", "content": "Below, we present an idea on how to use the Hidformer artificial neural network to predict future stock prices based solely on their past quotations. Expand-ing the experiments presented previous on the Weather, Traffic, and Electricity datasets, we propose a solution similar to what was done with the Informer in finance, but this time we enable the model to predict more than one (more than the only next) quotation of an instrument, which is particularly important in the attempt to determine the trend in human decision making."}, {"title": "Stock prices - data and feature engineering", "content": "In finance, stocks are categorized into several industries [14]. As a historical price dataset we use few-year price movements of stocks from the only one particular sector since, according to our experience, stock prices in different industries are characterized by different formations, which could even lead to an attempt to induce contradictory patterns when using machine learning. These observed data for each index is a six-dimensional time series of daily open, high, low, close, adjusted close prices and volumes.\nWe normalize the dataset as follows:\nXt = $\\frac{X - X_{min}}{X_{max} - X_{min}}$ (1)\nwhere it is the normalized price at time t and $X_{max}$ ($X_{min}$) is the maximum (the minimum) value in the entire sample regarding all dimensions of the prices. With this approach, the values in the sample are in the range [0, 1], taking into account the dependencies between the components of the daily valuation of the security and the volume separately.\nIn each prediction, we use the previous $T_x$ prices and volumes to predict the close price on the next $T_y$ trading days, utilizing a moving window approach."}, {"title": "Proposed model", "content": "As mentioned above, we adopt the Hidformer model [8], for the task of stock market prediction. With respect to the original contribution, the architecture is as in the canonical Transformer besides the following improvements [8]:\n1. Two-Tower architecture. The input sequence is processed through two encoders, one operating in the time domain and the other working from the frequency domain perspective.\n2. Segment-and-merge approach. This segmentation involves creating sub-sequences from the original series with potential overlap or a step. These subsequences are merged after each encoder block, allowing for better cap-ture of local features.\n3. Recursive and linear attention. The classic multi-head attention is re-placed with a recursive version in the time encoder and a linear model in the frequency encoder."}, {"title": "MLP-Type decoder", "content": "The long time series prediction occurs in one opera-tion as introduced earlier [16].\nThus, the Hidformer model operates as follows (see Fig. 3). First, the token segmentation layer generates $N_T$ tokens. Then all six-dimensional tokens are flatten to $N_E$-dimensional embeddings which are processed by the two towers. Each of them consists of $N_B$ blocks. After merging, the output of each block is the input of the next block. However, at the output of the tower there are concatenated outputs of each block. The outputs of the time and frequency tower are concatenated once again and fed into the decoder, which is actually a multi-layer perceptron with $N_D$ layers."}, {"title": "Experiments", "content": "In this section, we conduct experiments with the aim of answering whether the Hidformer model is capable of predicting future stock index values, which we verify by comparing its output with the outcome of classic artificial neural networks (CNN, RNN, LSTM and DNN), and confirm through a Mann-Whitney U test. Additionally, we present a visual analysis."}, {"title": "Dataset selection", "content": "As a historical price dataset we selected the 43-year price movements from 12/12/1980 to 29/12/2023 of 6 stock indices (see Table 1) which are part of the Consumer Goods sector. We used Yahoo Finance as the data source. The training set included the first 95% data (from 12/12/1980 to 31/12/2021), which was used to train model parameters. The last 5% data (from 31/12/2021 to 31/12/2023) as the validation set was used to evaluate the performance of the models."}, {"title": "Hyper-parameters setting", "content": "In each prediction, we use the previous $T_x$ = 128 prices and volumes to predict the close price on the next $T_y$ = 128 trading days. The batch size for the mini-batch training is set as 64. The Adam optimizer with a learning rate of 0.0001 is used for training models. The number of epochs was set as 100 to guarantee the convergence of the training process. To validate, we selected the model chosen as the best during training. The loss function was weighted MSE loss (weights from $T_y$ = 128 to 1).\nIn the Hidformer model we set $N_T$ = 4 tokens which are flatten to $N_E$ = 1-dimensional embeddings. Each tower consists of $N_B$ = 3 blocks. The decoder contains $N_D$ = 2 layers."}, {"title": "Evaluation criteria", "content": "Below we propose criteria to evaluate the performance of the models from two perspectives, having regard to the fact that the effectiveness of a stock price predictor should be measured not only by the difference between the predicted values and the true data, but also by its usefulness in trading [13].\nPrediction accuracy For prediction accuracy, we compare the network output with the ground truth in the validation set and calculate the three common indicators: Mean Absolute Error (MAE), Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE). The lower values of them, the performance is better.\nNet value In order to evaluate usefulness of the model in trading, we adopt a simple strategy known in the literature [13]. If the predicted close price $\\hat{y}_{t+1}$ is larger than the latest observed close price $y_t$, we long one position index (buy one unit of the index); otherwise, we short one position index (sell one unit of the index). Thus, the return at time t + 1 is expressed as follows ($Y_{t+1}$ is the true next close price):\n$R_{t+1} = ln \\frac{Y_{t+1}}{Y_t} \u00d7 sign(\\hat{y}_{t+1} - Y_t)$ (2)"}, {"title": "Key results and analysis", "content": "Considering the uncertainty of deep learning methods, we implement 5 inde-pendent runs of learning and validation. Based on this, we present means and standard errors of the aforementioned indicators.\nPrediction accuracy The reported averaged results for all listings from the validation set are presented in Table 2 (the lower the value, the better).\nNet value Below we present the results of the given trading strategy separately for each index both from the perspective of the average 1-day return (Table 3) and the 2-year trading backtest (Table 4). The higher the value, the better."}, {"title": "Visual analytics", "content": "In our study, we further review our findings through a detailed visualization of the predictions generated by our Hidformer model for different datasets. As depicted in on sample Figure 4, Figure 5 and Figure 6 the Hidformer demon-strates a robust capability to accurately discern different directions of market trends. This ability is of importance for augmenting algorithmic strategies that assist in human investment decision-making processes. It may underscore the Hidformer model's potential as a valuable tool for investors seeking to navigate the complexities of the stock market through informed, data-driven strategies."}, {"title": "Conclusion and future work", "content": "The problem of forecasting stock prices has a particular resonance in the do-main of artificial neural networks, where Transformer-based architectures, have been increasingly recognized for their potential in time series prediction. Our research enriches this field of study by empirically validating the capabilities of an extended Hidformer model. We have verified the Hidformer adaptability and its promising application in financial time series forecasting, highlighting its capacity for enhancing decision-making process in daily trading operations.\nLooking ahead, deeper exploration of the Hidformer model across diverse datasets and market conditions could yield richer insights into its robustness and efficiency. This would involve using two-dimensional time and frequency towers to process stock prices and volumes separately, with decoders for each sector (or a one-hot vector conditioning one decoder). This approach accommo-dates daily stock trading specifics, enabling encoders to learn various formations while minimizing the risk of decoders learning contradictory patterns across in-dustries. Comparing the proposed method with the Informer model [13, 16] and the reinforcement learning trading strategy [15] would also be worthwhile."}]}