{"title": "Addressing Domain Shift via Imbalance-Aware Domain Adaptation in Embryo Development Assessment", "authors": ["Lei Li", "Xinglin Zhang", "Jun Liang", "Tao Chen"], "abstract": "Deep learning models in medical imaging face dual challenges: domain shift, where models perform poorly when deployed in settings different from their training environment, and class imbalance, where certain disease conditions are naturally underrepresented. We present Imbalance-Aware Domain Adaptation (IADA), a novel framework that simultaneously tackles both challenges through three key components: (1) adaptive feature learning with class-specific attention mechanisms, (2) balanced domain alignment with dynamic weighting, and (3) adaptive threshold optimization. Our theoretical analysis establishes convergence guarantees and complexity bounds. Through extensive experiments on embryo development assessment across four imaging modalities, IADA demonstrates significant improvements over existing methods, achieving up to 25.19% higher accuracy while maintaining balanced performance across classes. In challenging scenarios with low-quality imaging systems, IADA shows robust generalization with AUC improvements of up to 12.56%. These results demonstrate IADA's potential for developing reliable and equitable medical imaging systems for diverse clinical settings. The code is made public available at https://github.com/yinghemedical/imbalance-aware_domain_adaptation.", "sections": [{"title": "1 Introduction", "content": "Deep learning has revolutionized medical image analysis, demonstrating remarkable potential for automating complex diagnostic tasks [1]. However, two critical challenges emerge when deploying these systems in real-world clinical settings: domain shift and class imbalance. While these challenges have been studied independently [2,3], their interaction - particularly in medical imaging contexts - remains understudied and poses significant barriers to clinical adoption.\nDomain shift occurs when deep learning models trained on data from one medical context show degraded performance when deployed in different settings [4]. This challenge is particularly evident in embryo development assessment, where models trained on high-end clinical time-lapse imaging systems often perform poorly when applied to data from portable microscopes or smartphone-based systems [5]. As illustrated in Fig. 1, the transition from high-end to low-end imaging systems introduces significant variations in image quality, complicating the model's ability to maintain consistent performance.\nThis challenge is compounded by inherent class imbalance in medical data [6,7]. In embryo assessment, the distribution of developmental stages varies naturally and differs significantly across imaging modalities [8]. For instance, our analysis reveals that blastocyst-stage embryos represent 28.9% of samples in clinical time-lapse systems (ED4) but comprise 81.2% in portable microscope data (ED2). This disparity reflects both biological realities and differences in clinical protocols across settings [9].\nThe intersection of these challenges creates a complex problem. Existing approaches have addressed them separately - domain adaptation techniques focus on bridging domain gaps without considering class distributions [10, 11], while methods for handling imbalanced data typically assume consistent domain characteristics [12]. However, as Fig. 1 demonstrates, in real-world medical applications [13], these challenges are inherently intertwined. The varying proportions of blastocyst versus non-blastocyst stages across different imaging modalities suggest that effective solutions must simultaneously address both domain shift and class imbalance.\nIn this paper, we present Imbalance-Aware Domain Adaptation (IADA), a novel framework that simultaneously tackles both challenges through three key innovations: (1) adaptive feature learning with class-specific attention mechanisms, (2) balanced domain alignment with dynamic weighting, and (3) adaptive threshold optimization. Through theoretical analysis and extensive experimentation on embryo development assessment across four distinct imaging modalities, we demonstrate IADA's effectiveness in maintaining robust performance despite varying image quality and class distributions. Our results show up to 25.19% improvement in accuracy over existing methods while maintaining balanced performance across classes, suggesting IADA's potential for developing more reliable and equitable medical imaging systems for diverse clinical settings [14]."}, {"title": "2 Related Work", "content": "Domain shift arises when there is a discrepancy between the distribution of training and testing datasets, leading to a decline in model performance. This phenomenon has been widely studied across multiple domains, including natural images, speech processing, and medical diagnostics [10, 15, 16].\nDomain shift can occur due to various factors such as differences in imaging devices, acquisition protocols, environmental conditions, or population demographics. Techniques to combat domain shift include domain adaptation, domain generalization, and adversarial training. These approaches seek to align feature distributions across domains or make models robust to distributional variations [3,10]. For instance, adversarial learning frameworks such as Gradient Reversal Layer (GRL) have demonstrated success in reducing the discrepancy between source and target domains [3].\nDomain shift is especially problematic in medical image analysis due to the heterogeneity of medical data. Variations in imaging modalities (e.g., MRI, CT, ultrasound), equipment manufacturers, and clinical protocols exacerbate the issue. Moreover, medical datasets are often small and suffer from class imbalances, making models more susceptible to overfitting and domain bias [1,17]. In [4], Kanakasabapathy et al. develop Adaptive Adversarial Neural Networks to address domain shift in lossy and domain-shifted datasets of medical images. Their work highlights the importance of domain alignment techniques in preserving diagnostic accuracy. Other studies have explored self-supervised learning, meta-learning, and generative adversarial networks (GANs) to mitigate domain discrepancies [18-20].\nEmbryo research stands out as a particularly critical area of medical imaging due to its implications for assisted reproductive technologies (ART). Accurate assessment of embryo quality is essential for optimizing implantation success rates and improving patient outcomes. However, this field faces unique challenges due to the sensitivity of embryo imaging, variations in clinical settings, and limited availability of annotated datasets [21-23]."}, {"title": "2.1 Domain Shift", "content": ""}, {"title": "2.2 Learning with Imbalanced Data", "content": "Handling imbalanced data is a persistent challenge in medical applications, where minority classes often represent critical conditions requiring accurate predictions. Data-level methods aim to address imbalance by modifying the dataset. Oversampling techniques, such as Synthetic Minority Over-sampling Technique (SMOTE) [6], generate synthetic minority class samples to balance the class distribution. Adaptive Synthetic Sampling (ADASYN) [12] extends SMOTE by focusing on hard-to-learn instances, improving model performance on challenging samples. Alternatively, undersampling techniques, such as random undersampling and Cluster Centroids [29], reduce the majority class size but may risk losing valuable information.\nAlgorithmic solutions incorporate imbalance-handling mechanisms directly into the learning process. Cost-sensitive learning (CSL) [30] assigns higher misclassification costs to minority class instances, guiding the model toward improved performance on these classes. Ensemble methods, such as boosting and bagging, are also effective for imbalanced data by combining diverse models to mitigate bias [31].\nHybrid methods combine oversampling and undersampling techniques to refine the dataset further. For instance, SMOTE with Tomek links [32] generates synthetic samples while removing overlapping instances, creating a more balanced and cleaner dataset.\nRecent research has introduced advanced techniques for imbalanced data. Density-Aware Personalized Training [33] decouples feature extraction and classification, using density-aware loss and learnable cost matrices. Federated Learning for Class-Imbalanced Medical Image Classification (FedIIC) [34] addresses imbalance in decentralized settings, employing contrastive learning and dynamic margins. Progressive Class-Center Triplet Loss [35] uses a two-stage training approach to separate and compact class distributions.\nIn medical diagnostics, methods like SMOTE and cost-sensitive learning have been crucial for identifying rare diseases with low prevalence rates [36]. In medical imaging, approaches such as FedIIC have improved the classification of rare abnormalities [34]."}, {"title": "3 Problem Formulation", "content": "Medical domain adaptation presents a dual challenge: addressing both domain shift and class imbalance simultaneously. In healthcare applications, models trained on data from one medical context (source domain) often show degraded performance when deployed in different settings (target domain), particularly when dealing with imbalanced disease distributions. This challenge is compounded by various factors including differences in patient populations, imaging equipment, clinical protocols, and institutional practices.\nLet us first formalize the problem setup. We denote the source domain labeled dataset as $D_s={\\{(x_i,y_i)\\}}_{i=1}^{n_s}$, where:\n\u2022 $x \\in \\mathcal{X} \\subset \\mathbb{R}^d$ represents d-dimensional input features (e.g., medical images, clinical measurements)\n\u2022 $y \\in \\mathcal{Y} = \\{1, ..., C\\}$ denotes the corresponding class labels (e.g., disease diagnoses)\n\u2022$n_s$ is the total number of source domain samples\nSimilarly, we define the target domain dataset as $D_t = \\{x_i\\}_{i=n_s+1}^{n_t}$, which is typically unlabeled in real-world medical scenarios. The fundamental domain shift can manifest in three distinct ways:\n\u2022 Covariate shift: $P(X^s) \\neq P(X^t)$, indicating differences in feature distributions\n\u2022 Label shift: $P(Y^s) \\neq P(Y^t)$, reflecting varying disease prevalence\n\u2022 Concept shift: $P(Y|X^s) \\neq P(Y|X^t)$, suggesting different feature-disease relationships"}, {"title": "4 Imbalance-Aware Domain Adaptation", "content": "To address the challenges of medical domain shift while accounting for inherent class imbalances in medical data, we propose Imbalance-Aware Domain Adaptation (IADA). Our framework integrates three key innovations: adaptive feature learning that captures class-specific characteristics, balanced domain alignment that ensures fair representation across classes, and dynamic threshold optimization that adapts to varying class distributions. By designing an end-to-end training process, our method simultaneously optimizes both domain adaptation and class balance objectives."}, {"title": "4.1 Imbalance-Aware Feature", "content": "At the core of our approach lies a carefully designed feature extractor $F_c : \\mathcal{X} \\rightarrow \\mathcal{Z}$ that addresses class imbalance through a novel attention mechanism. This feature extraction process consists of two main stages that work in concert to produce robust, class-aware representations:\n1. Base Feature Extraction: The first stage begins with extracting class-specific features. For each input sample $x_i$, we process it through a sophisticated pipeline:\n$f_c(x_i) = h_c(g(x_i))$ (1)\nHere, $g(\\cdot)$ serves as a shared backbone network that captures general features, while $h_c(\\cdot)$ represents class-specific adaptation layers that fine-tune these features for each class's unique characteristics.\n2. Attention Mechanism: Building upon these base features, we introduce an attention mechanism that dynamically weights the importance of different class-specific features. The attention weights $a_c(x_i)$ are computed through a softmax operation:\n$a_c(x_i) = \\frac{\\exp(w_g(x_i))}{\\sum_{k=1}^C \\exp(w_g(x_i))}$ (2)\nIn this formulation, we represents learnable attention vectors that help the model focus on the most relevant features for each class.\nTo combine these components into a final representation, we compute a weighted sum of the class-specific features:\n$z_i = F_c(X_i) = \\sum_{c=1}^C a_c(x_i) \\cdot f_c(x_i)$ (3)\nThis carefully crafted architecture provides several key benefits:\n\u2022 It ensures features capture class-specific nuances through dedicated extractors\n\u2022 It gives minority classes fair representation through targeted attention mechanisms\n\u2022 It maintains flexibility to handle varying class distributions across different domains"}, {"title": "4.2 Adversarial Domain Alignment", "content": "To bridge the gap between source and target domains while maintaining class balance, we employ an advanced adversarial framework with several key innovations:\n1. Domain Discriminator: We implement a sophisticated discriminator $D_d : \\mathcal{Z} \\rightarrow [0,1]$ as a multi-layer neural network. This discriminator incorporates three crucial components:\n\u2022 A gradient reversal layer that enables adversarial training\n\u2022 Class-balanced batch sampling to ensure fair representation\n\u2022 Instance weighting that accounts for varying class frequencies"}, {"title": "4.3 Imbalance-Aware Classification", "content": "The final component of our framework is an adaptive classification module $C_y : \\mathcal{Z} \\rightarrow \\mathcal{Y}$ that dynamically adjusts to class imbalance through three mechanisms:\n1. Adaptive Thresholds: To account for varying class distributions, we compute class-specific thresholds that adapt to class frequencies:\n$T_c = \\beta \\log(\\frac{n}{\\min_c n_c}) + \\gamma$ (7)\nHere, $\\beta$ and $\\gamma$ are learnable parameters that allow the thresholds to adapt during training, with $n_c$ representing the number of samples in class $c$ in the source domain.\n2. Classification Decision: Using these adaptive thresholds, we make the final classification decision through:\n$\\hat{y} = \\arg \\max \\{C_y(z)_c - T_c\\}$ (8)\nThis formulation ensures that minority classes receive fair consideration by adjusting decision boundaries based on class frequencies.\n3. Confidence Calibration: To ensure reliable probability estimates, we incorporate temperature scaling:\n$p(y|z) = \\text{softmax}(C_y(z)/T)$ (9)\nThe temperature parameter $T$ is learned during training to optimize probability calibration."}, {"title": "4.4 Training Objective", "content": "To bring all components together into a cohesive framework, we formulate a comprehensive training objective:\n$\\min_{\\theta,\\psi} \\max_{\\varphi} \\mathcal{L}_{cls}(\\theta,\\psi) - \\lambda_{adv}\\mathcal{L}_{adv}(\\theta, \\varphi) + \\lambda_{reg}\\mathcal{R}(\\theta,\\psi)$ (10)\nThis objective consists of three carefully designed components:\n1. Classification Loss $\\mathcal{L}_{cls}$: We employ a weighted focal loss to address class imbalance:\n$\\mathcal{L}_{cls} = - \\frac{1}{n_s} \\sum_{i=1}^{n_s} w(y_i) (1 - p_{y_i}) \\log(p_{y_i})$ (11)\nThe focal loss term $(1 - p_{y_i})$ helps focus training on hard examples, while class weights $w(y)$ balance the contribution of different classes.\n2. Regularization Term $\\mathcal{R}$: To prevent overfitting and ensure robust feature learning, we combine multiple regularization strategies:\n$\\mathcal{R} = \\lambda_1||\\theta||_2 + \\lambda_2\\mathcal{L}_{cons} + \\lambda_3\\mathcal{L}_{div}$ (12)\nThis includes L2 regularization $(||\\theta||_2)$, consistency regularization $(\\mathcal{L}_{cons})$ across augmented samples, and feature diversity promotion $(\\mathcal{L}_{div})$."}, {"title": "3. Adversarial Term", "content": "To ensure stable training, we implement a warming-up schedule for the adversarial weight:\n$\\lambda_{adv} = \\lambda_0 \\cdot \\min(1, \\frac{t}{\\tau})$ (13)\nThis gradual increase in adversarial strength, controlled by the current iteration $t$ and warming-up period $\\tau$, allows the model to first learn good features before focusing on domain alignment."}, {"title": "5 Theoretical Analysis", "content": "In this section, we provide theoretical analyses, including generalization, convergence rate, and algorithmic complexity. First, let's establish some key assumptions and definitions:\nTheorem 5.1 (Generalization Bound with Class Imbalance). Let $h \\in \\mathcal{H}$ be a hypothesis with expected errors $\\epsilon_s(h)$ and $\\epsilon_t(h)$ on the source and target domains respectively. For any $\\delta > 0$, with probability at least $1 - \\delta$, the following bound holds:\n$\\epsilon_t(h) \\leq \\epsilon_s(h) + \\sum_{i=1}^C |\\pi_i^s - \\pi_i^t| + \\sum_{i=1}^C \\min(\\pi_i^s, \\pi_i^t) d_i(\\mathcal{H}) + \\lambda$ (14)\nwhere $\\lambda$ represents the combined error of the ideal joint hypothesis.\nProof. The proof follows a structured approach through the decomposition and bounding of error terms. We begin by expressing the target error using class-conditional distributions: $\\epsilon_t(h) = \\sum_{i=1}^C \\pi_i^t \\epsilon_{t,i}(h)$, where $\\epsilon_{t,i}(h)$ represents the error for class $i$ in the target domain. Next, we establish that the difference between source and target errors for each class is bounded by the domain discrepancy: $|\\epsilon_{t,i}(h) - \\epsilon_{s,i}(h)| \\leq d_i(\\mathcal{H})$.\nApplying the triangle inequality and leveraging the class proportions, we can derive:\n$\\epsilon_t(h) - \\epsilon_s(h) \\leq \\sum_{i=1}^C |\\pi_i^t \\epsilon_{t,i}(h) - \\pi_i^s \\epsilon_{s,i}(h)|$\n$\\lt \\sum_{i=1}^C |\\pi_i^t - \\pi_i^s| \\epsilon_{t,i}(h) + \\sum_{i=1}^C \\min(\\pi_i^s, \\pi_i^t) d_i(\\mathcal{H})$\nThe final bound is obtained by incorporating the ideal joint hypothesis error $\\lambda$.\nRemark 5.2. Theorem 5.1 provides several key insights regarding domain adaptation under class imbalance. First, the bound explicitly captures how differences in class proportions between domains affect generalization through the term $\\sum_{i=1}^C |\\pi_i^s - \\pi_i^t|$. Second, the domain discrepancy's impact is modulated by the minimum class proportion through $\\min(\\pi_i^s, \\pi_i^t)$, suggesting that rare classes have less influence on the overall bound. Third, the additive nature of the bound indicates that both class imbalance and domain shift contribute independently to the generalization gap.\nCorollary 5.3 (Balanced Domain Case). In the special case where domains are perfectly balanced (i.e., $\\pi_i^s = \\pi_i^t$ for all $i$), the generalization bound simplifies to:\n$\\epsilon_t(h) \\leq \\epsilon_s(h) + \\sum_{i=1}^C \\pi_i d_i(\\mathcal{H}) + \\lambda$\nRemark 5.4. Corollary 5.3 demonstrates the elegance of the bound under balanced conditions. The removal of the class proportion difference term $\\sum_{i=1}^C |\\pi_i^s - \\pi_i^t|$ reflects the simplified learning scenario when source and target domains share identical class distributions. Moreover, the weighting of domain discrepancies by class proportions in the simplified bound suggests that even in balanced scenarios, the impact of domain shift remains class-dependent. This provides theoretical justification for maintaining class-specific adaptation mechanisms even when domains are balanced.\nThe generalization bound explicitly depends on the difference in class proportions between domains, while the impact of domain shift is weighted by the minimum class proportion across domains. In balanced domains, the bound elegantly simplifies to a weighted sum of class-conditional discrepancies. Notably, classes with larger proportion differences contribute more significantly to the domain gap."}, {"title": "Assumption 5.5 (Smoothness and Convexity)", "content": "The loss function $\\mathcal{L}$ is assumed to be $\\beta$-smooth and $\\mu$-strongly convex. Furthermore, the gradients are bounded such that $|\\|\\nabla \\mathcal{L}(w)\\|| \\leq G$ for some constant $G$. Additionally, the class proportions are constrained to satisfy $\\pi_i^s, \\pi_i^t \\in (0,1)$ with the normalization condition $\\sum_{i=1}^C \\pi_i^s = \\sum_{i=1}^C \\pi_i^t = 1$.\nThe assumption 5.5 is standard in optimization theory and ensures the convergence of gradient-based methods. The smoothness and strong convexity conditions provide upper and lower quadratic bounds on the loss function, while the gradient bound prevents excessive parameter updates. The class proportion constraints ensure proper probability distributions across domains."}, {"title": "Lemma 5.6 (Class-weighted Gradient Bound)", "content": "For any iteration $t$, the expected gradient norm satisfies:\n$\\mathbb{E}[||\\nabla \\mathcal{L}_t(W_t)||^2] \\leq \\sum_{i=1}^C \\max(\\pi_i^s, \\pi_i^t) G^2$\nProof. Using Jensen's inequality and the gradient bound:\n$\\mathbb{E}[||\\nabla \\mathcal{L}_t(W_t)||^2] = \\mathbb{E}[|| \\sum_{i=1}^C (\\pi_i^s \\nabla \\mathcal{L} + \\pi_i^t \\nabla \\mathcal{L}) ||^2]$\n$\\leq \\mathbb{E}[\\sum_{i=1}^C \\max(\\pi_i^s, \\pi_i^t) (||\\nabla \\mathcal{L}||^2 + ||\\nabla \\mathcal{L}||^2)]$\n$\\leq \\sum_{i=1}^C \\max(\\pi_i^s, \\pi_i^t) G^2$"}, {"title": "Theorem 5.7 (Convergence Rate with Class Imbalance)", "content": "Let $w_t$ be the parameters at iteration $t$ using learning rate $\\eta_t = \\frac{\\alpha}{(\\mu t + 1)}$ where $\\gamma = \\max\\{0, \\frac{1}{\\beta}\\}$. Then:\n$\\mathbb{E}[\\mathcal{L}(w_t) - \\mathcal{L}(w^*)] \\leq \\frac{2\\beta \\Delta_0}{(\\mu t + 4\\beta)} + \\frac{C_\\pi G^2}{2\\mu^2 t}$\nwhere $\\Delta_0 = ||w_0 - w^*||^2$ and $C_\\pi = \\sum_{i=1}^C \\max(\\pi_i^s, \\pi_i^t)$ is the class proportion factor.\nProof. We begin with the strong convexity condition:\n$\\mathcal{L}(w_t) - \\mathcal{L}(w^*) \\leq <\\nabla \\mathcal{L}(w_t), w_t - w^*> - \\frac{\\mu}{2} ||w_t - w^*||^2$\nUsing the update rule $w_{t+1} = w_t - \\eta_t \\nabla \\mathcal{L}(w_t)$, we can derive:\n$||w_{t+1} - w^*||^2 = ||w_t - \\eta_t \\nabla \\mathcal{L}(w_t) - w^*||^2$\n$= ||w_t - w^*||^2 - 2\\eta_t <\\nabla \\mathcal{L}(w_t), w_t - w^*> + \\eta_t^2 ||\\nabla \\mathcal{L}(w_t)||^2$\nTaking expectation and applying Lemma 1:\n$\\mathbb{E}[||w_{t+1} - w^*||^2] \\leq (1 - \\mu \\eta_t) \\mathbb{E}[||w_t - w^*||^2] + \\eta_t^2 C_\\pi G^2$\nWith the chosen learning rate and telescoping the sum:\n$\\mathbb{E}[||w_t - w^*||^2] \\leq \\frac{4\\Delta_0}{(\\mu t + 4\\beta)} + \\frac{2 C_\\pi G^2}{\\mu^2 t}$\nThe final result follows from strong convexity."}, {"title": "Remark 5.8", "content": "Theorem 5.7 reveals that convergence is governed by two competing terms: a first-order term that depends on the initial distance to the optimum and decays as $O(1/t)$, and a second-order term affected by class proportions through $C_\\pi$. This decomposition suggests that class imbalance primarily impacts the later stages of optimization when the second term becomes dominant."}, {"title": "Corollary 5.9 (Balanced Domain Convergence)", "content": "When the domains are balanced $(\\pi_i^s = \\pi_i^t$ for all $i$), the convergence rate simplifies to:\n$\\mathbb{E}[\\mathcal{L}(w_t) - \\mathcal{L}(w^*)] \\leq \\frac{2\\beta \\Delta_0}{(\\mu t + 4\\beta)} + \\frac{G^2}{2\\mu^2 t}$"}, {"title": "Remark 5.10", "content": "Corollary 5.9 demonstrates that domain balance leads to optimal convergence rates. In this case, the class proportion factor $C_\\pi$ reduces to unity, resulting in the standard convergence rate for strongly convex optimization. This suggests that maintaining balanced domains not only improves generalization but also accelerates optimization.\nThe analysis suggests using adaptive learning rates that vary with class proportions, with larger rates for minority classes and smaller rates for majority classes to ensure stability. Furthermore, the results support the use of class-specific attention mechanisms, adaptive thresholds, and balanced batch sampling strategies to mitigate the impact of domain and class imbalance on convergence."}, {"title": "Definition 5.11 (Class-specific Sample Sizes)", "content": "For source and target domains, we define the number of samples in class $i$ as $n_i^s = n_s \\pi_i^s$ and $n_i^t = n_t \\pi_i^t$ respectively, where $n_s$ represents the total number of source samples, $n_t$ represents the total number of target samples, and $\\pi_i^s, \\pi_i^t$ denote the corresponding class proportions."}, {"title": "Theorem 5.12 (Time Complexity)", "content": "The overall time complexity for one training epoch is:\n$T(n_s, n_t) = O(C(\\max\\{\\pi_i^s, \\pi_i^t\\})(n_s + n_t)d + C^2 \\log C)$ \nwhere $C$ represents the number of classes and $d$ represents the feature dimension.\nProof. The analysis encompasses multiple components of the algorithm. The feature extraction process requires $O(d)$ operations per sample, resulting in a total cost of $O((n_s + n_t)d)$. The class-specific attention mechanism involves computing attention weights at $O(Cd)$ per sample and performing weighted aggregation at $O(C)$ per sample, yielding a total cost of $O(C(n_s + n_t)d)$.\nThe class-balanced batch sampling requires maintaining binary search trees for each class at $O(C \\log(\\max\\{n_i^s, n_i^t\\}))$ and performing per-class sampling at $O(\\log(\\max\\{n_i^s, n_i^t\\}))$, resulting in a total cost of $O(C \\log(n_s \\max_i\\{\\pi_i^s\\}))$.\nFinally, adaptive threshold computation involves sorting class frequencies at $O(C \\log C)$ and updating thresholds at $O(C)$, contributing $O(C \\log C)$ to the total complexity. The summation of these components yields the stated complexity bound."}, {"title": "Remark 5.13", "content": "Complexity analysis 5.12 reveals two major components: a linear term scaling with sample size and feature dimension, modulated by class imbalance through $\\max_i\\{\\pi_i^s, \\pi_i^t\\}$, and a class-dependent term reflecting the overhead of maintaining class-specific structures. This decomposition highlights how class imbalance affects computational efficiency."}, {"title": "Lemma 5.14 (Space Complexity)", "content": "The space complexity of the algorithm is:\n$S(n_s, n_t) = O(\\sum_{i=1}^C (\\pi_i^s n_s + \\pi_i^t n_t)d + C^2)$\nProof. The space requirements arise from several components. Feature storage demands $O(\\sum_{i=1}^C \\pi_i^s n_s d)$ for the source domain and $O(\\sum_{i=1}^C \\pi_i^t n_t d)$ for the target domain. The attention mechanism requires $O(Cd)$ for weight matrices and $O(C)$ per sample for attention scores. Additional space is needed for class-specific data structures, including binary search trees at $O(C)$ and class statistics at $O(C^2)$. The combination of these requirements establishes the stated space complexity."}, {"title": "Theorem 5.15 (Computational Trade-offs)", "content": "Given a computational budget $B$, the optimal batch size $b_i$ for class $i$ is:\n$b_i = B \\frac{\\min(\\pi_i^s, \\pi_i^t)}{\\sum_{i=1}^C \\min(\\pi_i^s, \\pi_i^t)}$\nProof. The optimization problem is formulated as minimizing the sum of inverse batch sizes subject to a total budget constraint: $\\min_{b_i} \\sum_{i=1}^C \\frac{1}{b_i}$ subject to $\\sum_{i=1}^C b_i = B$. Using Lagrange multipliers, we form $\\mathcal{L} = \\sum_{i=1}^C \\frac{1}{b_i} + \\lambda (\\sum_{i=1}^C b_i - B)$. Taking derivatives and solving the resulting system of equations yields the optimal batch size allocation."}, {"title": "Remark 5.16", "content": "Theorem 5.15 establishes the optimal allocation of computational resources across classes. The square root dependence on class proportions represents a balance between processing efficiency and class representation, ensuring that minority classes receive sufficient attention while maintaining computational efficiency."}, {"title": "Corollary 5.17 (Balanced Case Complexity)", "content": "When domains achieve perfect balance with $\\pi_i^s = \\pi_i^t = \\frac{1}{C}$, the time complexity reduces to:\n$T_{balanced}(n_s, n_t) = O(\\frac{n_s + n_t}{C} d + C \\log C)$"}, {"title": "Remark 5.18", "content": "The balanced case reveals the optimal efficiency achievable by the algorithm. The reduction in complexity compared to the imbalanced case demonstrates the computational advantages of maintaining balanced class distributions, providing additional motivation for class balancing strategies beyond their statistical benefits."}, {"title": "6 Experiments & Analysis", "content": "Dataset. To evaluate our proposed Imbalance-Aware Domain Adaptation framework, we conduct extensive experiments using medical imaging data collected across multiple imaging modalities with inherent class imbalances following the experimental protocol in [4]. Our primary evaluation focuses on embryo development assessment, where we utilize images captured from four distinct imaging systems representing varying levels of quality and accessibility. The source domain (ED4) consists of 1,698 embryo images captured using a clinical time-lapse imaging system (Vitrolife Embryoscope), exhibiting a natural class imbalance with 491 blastocyst (28.9%) and 1,207 non-blastocyst (71.1%) images. This distribution reflects real-world clinical scenarios where certain developmental stages are naturally less frequent.\nFor target domains, we incorporate three additional imaging modalities with varying class distributions. ED3 comprises 258 images collected using clinical microscopes with a 45.3% blastocyst ratio, ED2 contains 69 images from a portable microscope with an 81.2% blastocyst ratio, and ED1 includes 296 images from a smartphone-based system with a 66.6% blastocyst ratio. These varying proportions across domains enable us to evaluate our framework's robustness to both domain shift and class imbalance simultaneously. We employ a stratified sampling strategy for data organization, where ED4 is divided into training (60%), validation (20%), and testing (20%) sets while maintaining the original class distributions. The target domain datasets are reserved entirely for testing to evaluate domain adaptation performance under different imbalance scenarios.\nModels. Our experimental evaluation implements three state-of-the-art convolutional neural network architectures, each modified to incorporate our proposed imbalance-aware components. ResNet-50 [37] serves as our primary backbone architecture, enhanced with class-specific attention modules and our proposed class-balanced batch sampling strategy. The network's 50-layer architecture with residual connections provides a strong foundation for feature learning, while our modifications enable it to better handle class imbalance during domain adaptation.\nWe also implement Inception v3 [38], which naturally handles multi-scale features through its parallel convolution paths with varying receptive fields. We augment this architecture with our adaptive thresholding mechanism to account for class-specific feature distributions and include auxiliary classifiers during training to improve gradient flow for minority classes. The third architecture, Xception [39], employs depthwise separable convolutions and is enhanced with our class-weighted attention mechanism. We modify its structure to incorporate class-specific feature extractors and balanced domain alignment modules.\nEach architecture incorporates four key imbalance-aware components: a class-specific attention mechanism following Equation (2) in the original paper, adaptive thresholding for varying class distributions as defined in Equation (7), class-balanced batch sampling, and domain alignment with class-specific weights according to Equation (4). These modifications work in concert to address the challenges of both domain shift and class imbalance.\nTraining Protocol and Implementation Details. Our training process follows the experimental protocol in [4]. The learning rate is set to 0.001 for all the experiments, while using a batch size of 2. We use weight decay of 5e-4 for regularization and run the training process with 50,000 iterations. The key hyperparameters of regularization coefficient $\\lambda_{reg}$ and adversarial coefficient $\\lambda_{adv}$ are selected according to their performance in a line search. More details can be found in Section 6.3."}, {"title": "6.1 Experimental Set-Up", "content": ""}, {"title": "6.2 Performance", "content": "Our proposed approach demonstrates consistent improvements over MD-Net across different architectures and domain adaptation scenarios. The comparative analysis reveals several key insights supported by experimental results.\nAs shown in Table 1, our method shows superior performance in maintaining source domain performance while achieving better adaptation. This is evidenced by the ED4 to ED4 scenario, where our proposed method with ResNet-50 achieves an accuracy of 0.9191 compared to MD-Net's 0.8908, representing a 2.83% improvement. This trend is consistent across other architectures, with Xception-based models showing accuracy improvements from 0.9205 to 0.9299, indicating better feature learning capabilities while handling class imbalance.\nThe most significant improvements are observed in challenging domain adaptation scenarios, particularly from ED4 to ED3. In this setting, our proposed method with ResNet-50 achieves a remarkable accuracy of 0.9457 compared to MD-Net's 0.6938, representing a dramatic 25.19% improvement. This substantial gain is further supported by improved F1-scores (0.9375 vs 0.5269) and AUC values (0.9852 vs 0.8"}]}