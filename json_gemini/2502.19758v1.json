{"title": "Learning with Exact Invariances in Polynomial Time", "authors": ["Ashkan Soleymani", "Behrooz Tahmasebi", "Stefanie Jegelka", "Patrick Jaillet"], "abstract": "We study the statistical-computational trade-offs for learning with exact invariances (or symmetries) using kernel regression. Traditional methods, such as data augmentation, group averaging, canonicalization, and frame-averaging, either fail to provide a polynomial-time solution or are not applicable in the kernel setting. However, with oracle access to the geometric properties of the input space, we propose a polynomial-time algorithm that learns a classifier with exact invariances. Moreover, our approach achieves the same excess population risk (or generalization error) as the original kernel regression problem. To the best of our knowledge, this is the first polynomial-time algorithm to achieve exact (not approximate) invariances in this context. Our proof leverages tools from differential geometry, spectral theory, and optimization. A key result in our development is a new reformulation of the problem of learning under invariances as optimizing an infinite number of linearly constrained convex quadratic programs, which may be of independent interest.", "sections": [{"title": "1. Introduction", "content": "While humans can readily observe symmetries or invariances in systems, it is generally challenging for machines to detect and exploit these properties from data. The objective of machine learning with invariances is to develop approaches that enable models to be trained and utilized under the symmetries inherent in the data. This framework is broadly applicable across various domains in the natural sciences and physics, including atomistic systems (Grisafi et al., 2018), molecular wavefunctions and electronic densities (Unke et al., 2021), interatomic potentials (Batzner et al., 2022), and beyond (Batzner et al., 2023). While many applications involve Euclidean symmetries (Smidt, 2021), the scope of such methods extends well beyond them to other geometries (Bronstein et al., 2017).\nLearning with invariances has a longstanding history in machine learning (Hinton, 1987; Kondor, 2008). In recent years, there has been significant interest in the development and analysis of learning methods that account for various types of invariances. This surge in interest is strongly motivated by many models showing considerable success in practice. Empirical evidence suggests the existence of algorithms that can effectively learn under invariances while exhibiting strong generalization and computational efficiency. However, from a theoretical perspective, much of the focus has been on the expressive power of models, generalization bounds, and sample complexity. There remains a relative lack of understanding regarding the statistical-computational trade-offs in learning under invariances, even in foundational settings such as kernel regression.\nSymmetries can be incorporated into learning in multiple ways. An immediate solution for learning with invariances seems to be data augmentation over the elements of the group. Moreover, some approaches to learning with invariances rely on group averaging, a technique that involves summing over group elements. However, the typically large size of the group can make both of these approaches computationally prohibitive, even super-exponential in the dimension of input data. Alternative approaches, such as canonicalization and frame averaging, also suffer from issues like discontinuities and scalability challenges (Dym et al., 2024).\nThis paper seeks to address the following question:\nCan we obtain an invariant estimator for learning with invariances that achieves both strong generalization and computational efficiency?\nThe first contribution of this work is a detailed study of the problem of learning with invariances in the context of kernel methods. Kernels, which have been among popular learning approaches, offer both statistical and computational efficiency (Scholkopf & Smola, 2018). We argue that while group averaging fails to produce exactly invariant estimators within a computationally efficient time frame, alternative algorithms can generate invariant estimators for the kernel regression problem in time that is polylogarithmic in the size of the group. In other words, we demonstrate that it is"}, {"title": "2. Related Work", "content": "Generalization bounds and sample complexity for learning with invariances have been extensively studied, particularly in the context of invariant kernels. Works such as Elesedy (2021), Bietti et al. (2021), Tahmasebi & Jegelka (2023), and Mei et al. (2021) provide insights into this area. Additionally, studies on equivariant kernels (Elesedy & Zaidi, 2021; Petrache & Trivedi, 2023) further our understanding of how equivariances affect learning. PAC-Bayesian methods have also been applied to derive generalization bounds under equivariances (Behboodi et al., 2022). More recently, Kiani et al. (2024) explored the complexity of learning under symmetry constraints for gradient-based algorithms. For studies on the optimization of kernels under invariances, see Teo et al. (2007).\nA variety of methods have been proposed to enhance the performance of kernel-based learning models. One prominent approach is the use of random feature models (Rahimi & Recht, 2007), which approximate kernels using randomly selected features. Low-rank kernel approximation techniques, such as the Nystr\u00f6m method (Williams & Seeger, 2000; Drineas et al., 2005), have also been proposed to reduce the computational complexity of kernel methods; see also Bach (2013); Cesa-Bianchi et al. (2015). Divide-and-conquer algorithms offer another pontential avenue for kernel approximation (Zhang et al., 2013). Additionally, the impact of kernel approximation on learning accuracy is well-documented in Cortes et al. (2010).\nOur work focuses on learning with invariances, which differs significantly from the tasks of learning invariances or measuring them in neural networks. For example, Benton et al. (2020) address how neural networks can learn invariances, while Goodfellow et al. (2009) study methods to measure the degree of invariance in network architectures.\nInvariance in kernel methods is not limited to group averaging. Other approaches such as frame averaging (Puny et al., 2022), canonicalization (Kaba et al., 2023; Ma et al., 2024), random projections (Dym & Gortler, 2024), and parameter sharing (Ravanbakhsh et al., 2017) have also been proposed to construct invariant function classes. However, canonicalization and frame averaging face challenges, particularly concerning continuity, which has been addressed in recent works like Dym et al. (2024).\nIn specialized tasks such as graphs, image, and pointcloud data, Graph Neural Networks (GNNs) (Scarselli et al., 2008; Xu et al., 2019), Convolutional Neural Networks (CNNs) (Krizhevsky et al., 2012; Li et al., 2021), and Pointnet (Qi et al., 2017a;b) have demonstrated the effectiveness of leveraging symmetries. Symmetries have also been successfully integrated into generative models (Bilo\u0161 & G\u00fcnnemann, 2021; Niu et al., 2020; K\u00f6hler et al., 2020). For a broader"}, {"title": "3. Background and Problem Statement", "content": "Notation. We begin by establishing some frequently used notation. Let M be a smooth, compact, and boundaryless d-dimensional Riemannian manifold. The uniform distribution over the manifold is the normalized volume element corresponding to its metric. We denote the space of square-integrable functions over M by $L^2(M)$ and the space of continuous functions by C'(M). Furthermore, $H^s(M)$ represents the Sobolev space of functions on M with parameter s, defined as the set of functions with square-integrable derivatives up to order s. Larger values of s correspond to greater smoothness, and it holds that $H^s(M) \\subseteq C(M)$ if and only if $s > d/2$, a condition we will assume throughout this paper. For each $n \\in \\mathbb{N}$, we define $[n] := \\{1, 2, ..., n\\}$. We use log to denote the logarithm with base 2. We refer to Appendices B.1 and B.2 for a quick review of Riemannian manifolds.\nProblem statement. We consider a general learning setup on a smooth, compact, and boundaryless Riemannian manifold M of dimension d. Our objective is to identify an estimator $f \\in \\mathcal{F}$ from a feasible space of estimators $\\mathcal{F} \\subset L^2(M)$, based on n independent and uniformly\u00b9 distributed labeled samples $S = \\{(x_i, y_i) : i \\in [n] \\} \\subseteq (M \\times \\mathbb{R})^n$ drawn from the manifold. Here, the labels $y_i$ for $i \\in [n]$ are produced based on the (unknown) ground truth regression function $f^* \\in C(M)$, meaning that $y_i = f^*(x_i) + e_i$, for each $i \\in [n]$, where $e_i, i \\in [n]$, is a sequence of independent zero-mean random variables with variance bounded by $\\sigma^2$. The population risk (or generalization error) of an estimator $f \\in L^2(M)$, which quantifies the quality of the estimation, is defined as:\n$\\mathcal{R}(f) := \\mathbb{E} [||f - f^*||^2_{L^2(M)}]$,\nwhere the expectation is taken over the randomness of the data and labels.\nGiven a dataset of size n, finding estimators with minimal population risk can be quite complex, often requiring the resolution of non-convex optimization objectives. However, in scenarios where $f^* \\in \\mathcal{H}$, with $\\mathcal{H} \\subseteq L^2(M)$ being a Reproducing Kernel Hilbert Space (RKHS), it is feasible to compute kernel-based estimators with low risk efficiently. Specifically, the Kernel Ridge Regression (KRR) estimator for the RKHS $\\mathcal{H} = H^s(M)$, denoted as $f_{KRR}$, achieves a population risk of $\\mathcal{R}(f_{KRR}) = O(n^{-s/(s+d/2)})$ while being computable in time $O(n^3)$, assuming access to an"}, {"title": "4. Main Result", "content": "In this section, we address the question raised in the previous section by presenting the primary result of the paper, which is encapsulated in the following theorem.\nTheorem 1 (Learning with exact invariances in polynomial time). Consider the problem of learning with invariances with respect to a finite group G using a labeled dataset of size n sampled from a manifold of dimension d. Assume that the optimal regression function belongs to the Sobolev space of functions of order s, i.e., $f^* \\in H^s(M)$ for some $s > d/2$ and let $\u03b1 := 2s/d$. Then, there exists an algorithm that, given the data, produces an exactly invariant estimator $f$ such that:\n\u2022 It runs in time $O(\\log^3(|G|)n^{3/(1+\u03b1)} + n^{(2+\u03b1)/(1+\u03b1)})$;\n\u2022 It achieves an excess population risk (or generalization error) of $\\mathcal{R}(f) = O(n^{-s/(s+d/2)})$;\n\u2022 It requires $O(\\log(|G|)n^{2/(1+\u03b1)}+n^{(2+\u03b1)/(1+\u03b1)})$ oracle calls to construct the estimator;\n\u2022 For any $x \\in M$, the estimator $f(x)$ can be computed in time $O(n^{1/(1+\u03b1)})$ using $O(n^{1/(1+\u03b1)})$ oracle calls.\nThe full proof of Theorem 1 is presented in Appendix C.3, while a detailed proof sketch is provided in Section 5, and the algorithm is outlined in Algorithm 1.\nLet us interpret the above theorem. Note that without any invariances, the Kernel Ridge Regression (KRR) estimator (details are given in Appendix B.9) provides an estimator $f_{KRR}"}, {"title": "5. Algorithm and Proof Sketch", "content": "In this section, we provide a proof sketch for Theorem 1, introducing several new notations and concepts necessary for achieving the reduction in time complexity.\nWe begin with the most natural optimization program for obtaining an estimator: the Empirical Risk Minimization (ERM), which proposes the following estimator:\n$\\displaystyle f_{\\text{ERM}} := \\arg \\min_{f \\in H^s(M)} \\Big\\{ \\sum_{i=1}^n (f(x_i) - Y_i)^2 \\Big\\}$\nwhere $S = \\{(x_i, Y_i) : i \\in [n] \\} \\subseteq (M \\times \\mathbb{R})^n$ denotes the sampled (labeled) dataset.\nHowever, as discussed, this method does not necessarily produce an estimator that is exactly invariant. A natural idea is to introduce group invariances as constraints into the above optimization, leading to the following constrained"}, {"title": "A. Discussion and Future Directions", "content": "We initiated the study on computational-statistical trade-offs in learning with exact invariances. We designed an algorithm that shows achieving the desirable population risk (the same as kernel regression without invariances) in poly(n, d, log(|G|)) time for the task of kernel regression with invariances on general manifolds. We note that, for simplicity, we have focused on boundaryless manifolds and isometric group actions. However, using standard techniques, the theory can be extended to more general cases as well\u2075. While the proposed spectral algorithm is computationally efficient, it does not offer any improvement in sample complexity over the baseline $\\mathcal{R}(f) = O(n^{-s/(s+d/2)})$. It has been observed that without computational constraints, better convergence rates are possible for learning with invariances (Tahmasebi & Jegelka, 2023), which are minimax optimal. Thus, it remains open whether those improved rates are achievable in poly(n, d, log(|G|)) time.\nWe note that the oracle access we assumed is primarily motivated by the case of the sphere, where polynomials can be evaluated, multiplied, composed by group elements, and integrated efficiently when they are of relatively low degree. We believe this is the most natural oracle access for this problem, as it aligns well with applications involving polynomials. An interesting future work could be to investigate the statistical-computational trade-offs using alternative oracles, e.g., similar to the kernel trick, how to design computationally efficient algorithms that have only access to the inner product of the RKHS. Another interesting future direction is to find whether random feature models as approximations for kernels can significantly improve the statistical-computational trade-off of learning with invariances. At present, our theory does not apply to random feature models.\nWe also observe that the spectral algorithm used in this paper does not employ the kernel trick, as it requires access to the entire set of features, rather than just their inner products. An interesting question is whether it is possible to utilize kernel tricks and find an alternative (polynomial-time) algorithm for learning under invariances. This approach could potentially improve the statistical efficiency of the spectral algorithm. In the end, we would like to note that capturing computational-statistical trade-offs in other estimation problems with invariances such as density estimation (Chen et al., 2023; Tahmasebi & Jegelka, 2024) could serve as a compelling avenue for future research."}, {"title": "B. Background", "content": "B.1. Riemannian Manifolds\nIn this section, we review some fundamental definitions from differential geometry and refer the reader to Lee (2006); Petersen (2006); Lee (2012) for further details.\nDefinition B.1 (Manifold). A topological manifold M of dimension dim(M) is a completely separable Hausdorff space that is locally homeomorphic to an open subset of Euclidean space of the same dimension, specifically $\\mathbb{R}^{\\text{dim}(M)}$. More formally, for each point $x \\in M$, there exists an open neighborhood $U \\subseteq M$ and a homeomorphism $\u222e : U \u2192 \u00db$, where $\u00db \\subset \\mathbb{R}^{\\text{dim}(M)}$.\nThe value dim(M) is referred to as the dimension of the manifold. Examples of manifolds include tori, spheres, $\\mathbb{R}^d$, and graphs of continuous functions. Manifolds with boundaries differ from boundaryless manifolds in that they may have neighborhoods that locally resemble open subsets of closed dim(M)-dimensional upper half-spaces, denoted as $[\\mathbb{H}^{\\text{dim}(M)} \\subset \\mathbb{R}^{\\text{dim}(M)}$, defined as follows:\n$\\mathbb{H}^d = \\{(x_1, x_2, ..., x_d) \\in \\mathbb{R}^d \\vert x_d \\geq 0\\}.$\nDefinition B.2 (Local Coordinates). Given a chart (U, $\u222e$)\u2014a pair consisting of a local neighborhood U and the corresponding homeomorphism $\u222e : U \u2192 \u00db$\u2014on a manifold M with dimension d, we define local coordinates $(x^1, x^2, ..., x^d)$ such that\n$\u222e(p) = (x^1(p), x^2(p),...,x^d(p))$,\nfor each point $p \\in U$.\nDefinition B.3 (Tangent Space). At each point $x \\in M$, the tangent space $T_xM$ is defined as the vector space formed by the tangent vectors to the manifold M at x. A tangent vector $v \\in T_xM$ can be represented as the derivative of a smooth"}]}