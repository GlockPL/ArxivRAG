{"title": "Vocabulary Expansion of Chat Models with Unlabeled Target Language Data", "authors": ["Atsuki Yamaguchi", "Terufumi Morishita", "Aline Villavicencio", "Nikolaos Aletras"], "abstract": "Chat models (i.e. language models trained to follow instructions through conversation with humans) outperform base models (i.e. trained solely on unlabeled data) in both conversation and general task-solving abilities. These models are generally English-centric and require further adaptation for languages that are under-represented in or absent from their training data. A common technique for adapting base models is to extend the model's vocabulary with target language tokens, i.e. vocabulary expansion (VE), and then continually pre-train it on language-specific data. Using chat data is ideal for chat model adaptation, but often, either this does not exist or is costly to construct. Alternatively, adapting chat models with unlabeled data is a possible solution, but it could result in catastrophic forgetting. In this paper, we investigate the impact of using unlabeled target language data for VE on chat models for the first time. We first show that off-the-shelf VE generally performs well across target language tasks and models in 71% of cases, though it underperforms in scenarios where source chat models are already strong. To further improve adapted models, we propose post-hoc techniques that inject information from the source model without requiring any further training. Experiments reveal the effectiveness of our methods, helping the adapted models to achieve performance improvements in 87% of cases.", "sections": [{"title": "Introduction", "content": "Chat models such as GPT-40 (OpenAI et al., 2024), Llama 3.1 (Dubey et al., 2024), Qwen2.5 (Yang et al., 2024) and Gemma 2 (Gemma Team et al., 2024) are large language models (LLMs) trained to follow instructions through conversation with humans (Xu et al., 2023). They consistently outperform their base models (i.e. LLMs trained solely on unlabeled generic corpora) in chat capabilities and even general task-solving abilities (Qwen Team, 2024; Gemma Team et al., 2024; Groeneveld et al., 2024). However, chat models often perform poorly in languages underrepresented in their training data. Tokenizing text from such languages often results in overfragmentation due to the reliance on English-centric data and vocabulary. Overfragmentation leads to more inference steps in a target language than required, resulting in slower inference (Ahia et al., 2023; Petrov et al., 2023; Ali et al., 2024). A popular approach to address these challenges is vocabulary expansion (VE) followed by continual pre-training (CPT) on unlabeled target language data (henceforth, VE) (Cui et al., 2024; Fujii et al., 2024; Choi et al., 2024; Tejaswi et al., 2024; Mundra et al., 2024, inter alia). Tokens from a target language are added to the vocabulary of a source tokenizer, while the token embedding and language modeling head weight matrices are expanded to support the new tokens. CPT on target language data helps to learn the embeddings of the new tokens and adapts the weights of the source model to the target language. While adapting base models with VE is quite common (Cui et al., 2024; Fujii et al., 2024; Choi et al., 2024; Tejaswi et al., 2024; Mundra et al., 2024, inter alia), the extent to which this applies to chat models remains an open question. Using chat data in the target language is ideal for chat model adaptation. However, either it does not exist, or it is challenging to produce for underrepresented languages, limiting this approach. Alternatively, we could adapt chat models using unlabeled data from the target language. This approach is straightforward, as unlabeled data can be readily obtained from the web at minimal cost. Nevertheless, further training a chat model on non-chat data data could result in catastrophic forgetting.\nIn this paper, we investigate the impact of adapting chat models using unlabeled target language data. We first investigate whether off-the-shelf VE is effective for chat models by adapting various state-of-the-art chat models on seven typologically diverse languages. To the best of our knowledge, this constitutes the first such study. We evaluate the performance of the adapted chat models on various benchmarks, including summarization, machine translation, and text classification, both in each target language and English. We find that off-the-shelf VE generally performs well across target language tasks and models in 71% of cases with up to a 5.7x inference speedup. However, its performance degrades when source chat models are already strong in a target language task.\nTo further improve adapted models, we propose post-hoc techniques applied after CPT that inject information from the source model to elicit chat capabilities, i.e. using the source chat template to format the input of the target model, reusing the weights of special tokens from the source chat model, and merging the weights of the source and target models. Through experiments across three chat LLMs and seven languages, we demonstrate the following:\n\u2022 Our proposed post-hoc techniques serve as powerful tools to make adapted chat models more effective, achieving target language performance improvements in 87% of cases with up to 6.0x faster inference over the source chat models.\n\u2022 While base models are often considered the standard source for adaptation in previous VE work, we confirm that adapted chat models with the post-hoc techniques outperform adapted base counterparts in 88.9% of the settings. This suggests that chat models can be a powerful alternative."}, {"title": "Methodology", "content": "2.1 Vocabulary Expansion (VE)\nGiven a source chat model ($\\mathcal{M}_{Chat}$), its corresponding tokenizer, vocabulary $V_{Chat}$, and unlabeled target language data $D$, the aim is to obtain an adapted model $\\mathcal{M}_{chat}$ with the same architecture as $\\mathcal{M}_{Chat}$, except for its embedding and language modeling head. These weight matrices are expanded such that they can support additional target vocabulary $V_{new}$. $\\mathcal{M}_{chat}$ uses the adapted tokenizer, whose vocabulary $V_{\\mathcal{C}hat'}$ is composed of $V_{Chat} \\cup V_{new}$.\nFirst, an auxiliary target language tokenizer, whose vocabulary is $V_{aux}$, is trained on $D$. Then, the top-$k \\in \\mathbb{N}$ frequent tokens in $V_{aux}$ that are not in $V_{Chat}$ are selected to form $V_{new}$. The adapted tokenizer for $\\mathcal{M}_{chat}$ is constructed using the tokenizer of $\\mathcal{M}_{Chat}$, $V_{Chat}$, and $V_{new}$. $\\mathcal{M}_{Chat'}$ is then initialized with the weights of $\\mathcal{M}_{Chat}$. The embeddings and language modeling head are expanded to support $V_{new}$, with newly added weights initialized according to \u00a72.2. Subsequently, $\\mathcal{M}_{chat}$ is trained on $D$ using a causal language modeling objective (\u00a72.3). Finally, $\\mathcal{M}_{chat}$ might undergo post-processing as described in \u00a72.4.\n2.2 Embedding and Language Modeling Head Initialization\nEffective initialization of new embeddings and language modeling head weights is critical for training efficiency and task performance (Minixhofer et al., 2022; Dobler and de Melo, 2023; Downey et al., 2023; Liu et al., 2024; Yamaguchi et al., 2024a). We employ mean initialization (Yao et al., 2021), a popular and simple, yet effective method in VE (Fujii et al., 2024; Tejaswi et al., 2024; Mundra et al., 2024). The weight of each new token in $V_{new}$ is initialized as the average embedding or language modeling head weight of their corresponding source tokens, obtained using the tokenizer of $\\mathcal{M}_{Chat}$.\n2.3 Continual Pre-training (CPT)\nCPT on $D$ is an integral part of VE to improve the alignment of the newly initialized embeddings and language modeling head weights. Following Remy et al. (2024), we train the embedding, language modeling head, and the top and bottom two layers of $\\mathcal{M}_{chat}$. This approach aims to calibrate only the parts closely related to the encoding and decoding of the target language (Wendler et al., 2024; Tang et al., 2024; Zhao et al., 2024b), minimizing changes to the source model while allowing cost-"}, {"title": "Post-hoc Techniques for Improved Task Performance", "content": "As we continue pre-training $\\mathcal{M}_{chat}$ on unlabeled data $D$, it can substantially lose its original capabilities. However, these may still be retained in its parametric knowledge to some extent, even after CPT. We hypothesize that eliciting them should help improve task performance. To this end, we propose three approaches to helping $\\mathcal{M}_{chat}$ leverage or improve its instruction-following ability. We test all approaches in combination as our primary method but also evaluate Merge, Tmpl+Merge, and Copy+Merge independently for analysis.\nChat Template (Tmpl). One key difference between chat and base models is that the former use a chat template. This includes specific roles (e.g. user or assistant) and has a placeholder for message text in the prompt (see Appendix A for details). Motivated by this, our first approach is to append the default chat template of $\\mathcal{M}_{Chat}$ to a task-specific prompt template when using $\\mathcal{M}_{chat}$ for inference (see Table 2 for task-specific prompts). Note that we do not make any changes to the task-specific prompt to make a fair comparison between models with and without a chat template.\nCopying Special Token and Language Modeling Head Weights (Copy). Special tokens used in a chat template (e.g. <start_of_turn> in Gemma 2 to represent the start of a turn) should play a critical role in activating the instruction-following ability of a chat model (see Appendix A for a full list of special tokens). Our hypothesis is that retaining the corresponding original weights of the embeddings and language modeling head from $\\mathcal{M}_{Chat}$ is more likely to help $\\mathcal{M}_{chat}$ with instruction-following. To this end, we copy all the special token weights from $\\mathcal{M}_{Chat}$ to $\\mathcal{M}_{chat}$.\nModel Merging (Merge). Model merging is a popular method to combine two or more seed models into one by integrating various distinct parametric knowledge from each one without incurring any additional training costs (Wortsman et al., 2022; Yadav et al., 2023; Yu et al., 2024; Goddard et al., 2024). We hypothesize that merging $\\mathcal{M}_{Chat}$ and $\\mathcal{M}_{chat}$ can help restore the instruction-following ability of $\\mathcal{M}_{chat}$ while maintaining the target language performance of $\\mathcal{M}_{chat}$. To verify this, we"}, {"title": "Experimental Setup", "content": "We use three state-of-the-art chat models as source:\n(1) Qwen2.5 7B (Yang et al., 2024); (2) Llama 3.1 8B (Dubey et al., 2024); and (3) Gemma 2 9B (Gemma Team et al., 2024), across experiments.\n3.2 Target Languages and Adaptation Data\nWe experiment with the following seven typologically diverse languages, assuming that they are underrepresented compared to English in the pre-training data or entirely absent: Amharic (Afroasiatic), Bengali (Indo-European), Burmese (Sino-Tibetan), Gujarati (Indo-European), Sinhala (Indo-European), Tamil (Dravidian), and Telugu (Dravidian). We select these languages due to the availability of evaluation data with the same task formulation across languages. The ratio of training data for each source chat model has not been explicitly disclosed (Appendix C.3). Note that we do not consider Latin script target languages as they are less likely to suffer from overfragmentation and usually benefit less from VE (Yamaguchi et al., 2024b; Tejaswi et al., 2024).\nFor CPT, we use MADLAD-400 (Kudugunta et al., 2023), which consists of highly-filtered document-level samples sourced from Common-Crawl, and randomly sample 250K language-specific documents for each language.\n3.3 Baselines\nOur first two baselines are the off-the-shelf chat models ($\\mathcal{M}_{Chat}$) and their corresponding base models ($\\mathcal{M}_{Base}$) presented in \u00a73.1 without target language adaptation. Next, we adapt $\\mathcal{M}_{Chat}$ and $\\mathcal{M}_{Base}$ using standard VE followed by CPT ($\\mathcal{M}_{Chat^{\\prime}}$ and $\\mathcal{M}_{Base^{\\prime}}$) as described in \u00a72.1-\u00a72.3. For reference, we also experiment with adapting $\\mathcal{M}_{Chat}$ and $\\mathcal{M}_{Base}$ using only CPT on the same target language data $D$, without VE ($\\mathcal{M}_{Chat \\setminus VE}$ and $\\mathcal{M}_{Base \\setminus VE}$)."}, {"title": "Tasks", "content": "Target Language. To assess the efficacy of our post-hoc techniques against the baselines in a target language, we evaluate adapted models both in generative and discriminative tasks. For generative tasks, we use summarization (SUM) using XL-SUM (Hasan et al., 2021) and English-to-target machine translation (MT) using FLORES-200 (NLLB Team et al., 2022). For a discriminative task, we employ multiple-choice text classification (MC) using Belebele (Bandarkar et al., 2024a).\nSource Language (English). We also assess the extent to which the adapted models retain their general task-solving abilities in English SUM, target-to-English MT, and English MC using the same datasets as those employed for target languages. We further use two general English language understanding benchmarks: MMLU (Hendrycks et al., 2021) and BBH (Suzgun et al., 2023), and an instruction-following ability evaluation benchmark, IFEVAL (Zhou et al., 2023).\nFollowing Ahia et al. (2023), we use 500 random samples for generative tasks: SUM and MT. The rest use the full test samples for evaluation.\n3.5 Evaluation Metrics\nTask Performance. We use the standard metrics for each task: accuracy for MC, MMLU, and IFEVAL (strict prompt), and exact match for BBH. For SUM and MT, we primarily use chrF (Popovi\u0107, 2015). We report average zero-shot performance across three different runs for generative tasks: SUM and MT throughout the experiments and analysis. For the remaining tasks, we report single-run zero-shot performance for IFEVAL, three-shot performance for MC and BBH, five-shot performance for MMLU, as these tasks are deterministically evaluated with temperature set to zero.\nInference Efficiency. VE offers inference speedups in a target language compared to source models (Tejaswi et al., 2024; Mundra et al., 2024; Yamaguchi et al., 2024b). To quantify this, we measure the number of tokens generated per second (tokens/s) (Hong et al., 2024)."}, {"title": "Implementation Details", "content": "We set the vocabulary size of the auxiliary target language tokenizer |$V_{aux}$| to 50K across languages and the number of new target tokens k to 10K.\nWe train each model for 500M tokens with a batch size of 32, a maximum learning rate of 5e-5, and a sequence length of 512. We detail our experimental setup in Appendix A.4"}, {"title": "Task Performance", "content": "Figure 2 shows the aggregated mean performance across seven languages for all source and adapted target models in target language (left) and English (right) tasks. Note that all color boxes and dots in the remainder of the paper reference Figure 2 for their meaning.\n4.1 Chat: vs. Adapted\nFirst, we analyze the impact of adapting chat models using VE on unlabeled target language data.\nTarget Language. We observe that adapted chat models $\\mathcal{M}_{Chat^{\\prime}}$ outperform their source models $\\mathcal{M}_{Chat}$ (red vertical dotted lines :) in all three target language tasks for Qwen2.5 and Llama 3.1, showing that off-the-shelf VE can benefit even chat models. Specifically, adapted Qwen2.5 models exhibit substantial average gains of up to 10, 12, and 25 points in SUM, MT, and MC, respectively. Adapted Llama 3.1 models also show great improvements in SUM with an average gain of 11 points. Further, adapted Gemma 2 models show better target language MC performance by an average of 5 points and are competitive with $\\mathcal{M}_{Chat}$ in SUM. However, they underperform in MT, with an average decrease of 6.9 points. Notably, Gujarati and Telugu see substantial drops of 11.7 and 15.5 points, respectively (Table 7). This decline may be attributed to the fact that $\\mathcal{M}_{Chat}$ exhibits relatively high performance (over 40.0 points) in languages where $\\mathcal{M}_{Chat}$ leads to a decrease. This suggests that VE itself may not always improve target language task performance when the $\\mathcal{M}_{Chat}$ performance is already strong. We conduct an additional analysis by language in Appendix C.3. We assume that the extent to which off-the-shelf VE works is closely tied to the degree to which the source model has been trained on that"}, {"title": "Post-hoc Techniques", "content": "For clarity, we consider the combination of all the proposed techniques in \u00a72.4 (Tmpl+Copy+Merge; $\\mathcal{M}_{Chat}^{Tmpl+Copy+Merge}$ as our primary method. We also evaluate Merge $\\mathcal{M}_{Chat}^{Merge}$, Tmpl+Merge $\\mathcal{M}_{Chat}^{Tmpl+Merge}$ and Copy+Merge $\\mathcal{M}_{Chat}^{Copy+Merge}$ to investigate the effect of each approach.\nTarget Language. We examine the target language results of $\\mathcal{M}_{Chat}^{Tmpl+Copy+Merge}$. Overall, $\\mathcal{M}_{Chat}^{Tmpl+Copy+Merge}$ further improves $\\mathcal{M}_{chat}$, outperforming $\\mathcal{M}_{chat}$ across all tasks and all models. It further improves performance over $\\mathcal{M}_{Chat}$ in generative tasks (i.e. SUM and MT) across all settings on average. In particular, Gemma 2 improves 17 points over $\\mathcal{M}_{chat}$ in MT. However, it underperforms $\\mathcal{M}_{Chat}$ in MC (i.e. a discriminative task) by 4.9 and 6.8 points on average for Llama 3.1 and Qwen2.5, respectively. We hypothesize that the impact of the post-hoc techniques may be more prominent in generative tasks because they tend to eliminate repetitive generation (see \u00a76 for an example).\nSource Language (English). In English tasks, $\\mathcal{M}_{Chat}^{Tmpl+Copy+Merge}$ performs comparably to $\\mathcal{M}_{chat}$ with less than a 3% degradation in 5 out of 6 tasks for Gemma 2 and Qwen2.5. This is an increase of 3 for Gemma 2 and 2 for Qwen2.5 from $\\mathcal{M}_{chat}$."}, {"title": "Base vs. Chat", "content": "Open-weight models such as Llama 3.1, Gemma 2, and Qwen2.5 are available in both base and chat variants. It is often observed that $\\mathcal{M}_{Chat}$ outperforms $\\mathcal{M}_{Base}$ on English benchmarks (Qwen Team, 2024; Gemma Team et al., 2024). However, it is an open question whether adapted chat models with the best possible configuration outperform adapted base models. To address this question, we compare adapted base models with adapted chat models by training base models to each of the seven target languages, following the same recipe used for chat models. We also evaluate Merge with $\\mathcal{M}_{Base}$ for reference.\nOverall, adapted chat models with our three post-hoc techniques $\\mathcal{M}_{Chat}^{Tmpl+Copy+Merge}$ outperform adapted base models $\\mathcal{M}_{Base^{\\prime}}$ and $\\mathcal{M}_{Base}^{Merge}$ in 25 and 24 out of 27 settings, respectively. This indicates that adapted chat models perform better than the adapted base counterparts. These results clearly suggest the potential of leveraging chat models for target language adaptation.\nTakeaways:\n\u2022 VE on chat models enhances target language task performance but it is less effective when they already perform well in a target task. (\u00a74.1)\n\u2022 The combination of our post-hoc techniques substantially improves target chat models, primarily for target generative tasks and English tasks. (\u00a74.2)\n\u2022 Adapted chat models with the post-hoc techniques generally outperform their base counterparts. (\u00a74.3)"}, {"title": "Inference Efficiency", "content": "VE offers inference speedups in target language tasks as it reduces text overfragmentation (\u00a73.5). Here, we examine the inference efficiency of each VE approach including baselines. Figure 3 shows the aggregated mean inference speedups across seven languages in three target tasks.\nChat vs. Adapted First, $\\mathcal{M}_{Chat^{\\prime}}$ shows substantial inference speedups, ranging from 1.9x (Gemma 2 in MT) to 5.7x (Llama 3.1 in SUM) across tasks and models. We further observe that adapted Llama 3.1 models show the largest speedups across tasks, followed by Qwen2.5 and Gemma 2. This exactly coincides with the vocabulary size of each model: Llama 3.1 for 128K, 152K for Qwen2.5, and 256K for Gemma 2, indicating that the degree to which an adapted chat model can benefit from VE in inference speedups is correlated with its vocabulary size.\nPost-hoc Techniques. The post-hoc techniques (e.g. Copy and Merge) may increase the frequency"}, {"title": "Qualitative Analysis", "content": "We further conduct a qualitative analysis using examples from English-to-target MT, to better understand the behavior of chat-based models. Table 1 presents the text generated by different models using Qwen2.5 and Gemma 2. More examples are available in Appendix C.2.\nChat vs. Adapted. $\\mathcal{M}_{chat}$ generates a correct translation irrespective of source models, albeit with repetitions. In contrast, $\\mathcal{M}_{Chat^{\\prime}}$ using Qwen2.5 fails to produce the correct translation, while Gemma 2 generates a similar sentence to the gold translation but with three key differences: (1) \u201cto stop\" \u2192 \"to cancel\"; (2) \"traffic\" \u2192 \"trade\"; and (3) the spelling of \"U-boats.\"\nPost-hoc Techniques. $\\mathcal{M}_{Chat}^{Tmpl+Copy+Merge}$ using Qwen2.5 generates a slightly different translation for the \"the pencil\" from $\\mathcal{M}_{chat}$ while eliminating repetitions. Gemma 2 demonstrates a similar behavior, generating a different translation of \u201cthis traffic\u201d without repetition."}, {"title": "Related Work", "content": "Vocabulary Transfer for Target Language Adaptation of LLMs. To adapt LLMs across languages and remedy overfragmentation (Ahia et al., 2023; Petrov et al., 2023; Ali et al., 2024), previous studies proposed various strategies. For example, adopting the vocabulary of a model for one or more target languages, using vocabulary replacement (Ostendorff and Rehm, 2023; Da Dalt et al., 2024; Remy et al., 2024; Yamaguchi et al., 2024a; Dobler and de Melo, 2024; Cahyawijaya et al., 2024) (i.e. replacing the entire vocabulary with a target one); vocabulary expansion (Balachandran, 2023; Larcher et al., 2023; Pipatanakul et al., 2023; Lin et al., 2024; Cui et al., 2024; Kim et al., 2024; Fujii et al., 2024; Choi et al., 2024; Nguyen et al., 2024; Tejaswi et al., 2024; Mundra et al., 2024); a hypernetwork for tokenizer transfer (Minixhofer et al., 2024); token swapping (Csaki et al., 2023), or adapters (Han et al., 2024).\nVE is a standard approach for cross-lingual vocabulary transfer of LLMs. However, previous work often targets base models as source for adaptation. The most relevant work to ours (Toraman, 2024; Zhao et al., 2024a) applies VE to base models using language-specific instruction data. We instead apply VE on unlabeled data using chat models as source and investigate its effectiveness.\nModel Merging for Language Adaptation. Model merging has recently been actively studied for target language adaptation to get the most out of multiple models without additional training. Specifically, Tao et al. (2024) and Bandarkar et al. (2024b) build task-solving LLMs tailored for a target language using model merging. Aakanksha et al. (2024) aim to build strong and safe multilingual models through model merging, demonstrating its superiority over data mixing. Alexandrov et al. (2024) iteratively merge models during target language adaptation to effectively mitigate catastrophic forgetting. Our work is the first to explore the combination of VE and model merging for effective target language adaptation."}, {"title": "Conclusion", "content": "In this paper, we investigated the impact of adapting chat models to a target language using VE with unlabeled data. We show that VE generally performs well across target language tasks and models in 71% of cases with up to 5.7x inference speedups, though it underperforms in scenarios where source chat models are already strong. To make it more effective when using chat models as source, we proposed three post-hoc techniques that inject information from the source chat model without requiring any training. Extensive experiments demonstrated that: (1) the post-hoc techniques effectively enhance task performance, achieving performance improvements in 87% of cases over the source with up to 6.0x faster inference; and finally, (2) chat models are a better source for target language adaptation than their base counterparts."}, {"title": "Limitations", "content": "Vocabulary Initialization Methods. We use mean initialization as the most popular, simple, yet effective vocabulary initialization method (Yao et al., 2021). However, many other vocabulary initialization methods exist, including W2CV (Mundra et al., 2024), OFA (Liu et al., 2024), WECHSEL (Minixhofer et al., 2022), FOCUS (Dobler and de Melo, 2023), Align (Yamaguchi et al., 2024b), CLP (Ostendorff and Rehm, 2023), and inter alia. Although a different initialization method can lead to different task performance, this should not greatly affect the efficacy of our proposed post-hoc methods since are independent of the choice of vocabulary initialization.\nContinual Pre-training Methods. This paper uses a continual pre-training method proposed by Remy et al. (2024), which tunes the top and bottom two layers of a model and its embedding and language modeling head, for efficient and effective target language adaptation. Nonetheless, different continual pre-training methods, including adapter-based training (e.g. LoRA) and full fine-tuning. It would be interesting to extensively investigate the effect of different training methods for future work, but this is beyond the scope of this paper.\nModel Merging Methods. We experiment with linear and SLERP merging as representative model merging methods for simplicity. More recent methods like TIES (Yadav et al., 2023) and DARE-TIES (Yu et al., 2024) might perform even better in our post-hoc method. Given the resource constraints, we leave this investigation for future work.\nModel Sizes. Our experiments consider LLMs of 7B, 8B, and 9B parameters, following previous work on cross-lingual vocabulary expansion that adapted models of similar sizes (Tejaswi et al., 2024; Mundra et al., 2024). Experimenting with larger models would be valuable in future studies. Limited access to computational resources did not allow us to experiment with larger LLMs.\nLanguages. This paper covers seven typologically diverse languages, following previous work on cross-lingual vocabulary adaptation that has also tested a similar number of languages. For instance, Minixhofer et al. (2022) tested eight languages. Experimenting with more languages is an interesting avenue for future work but is out of the scope of this paper, given our limited computing capacity."}, {"title": "Ethics Statement", "content": "This study does not evaluate the safety aspect of the adapted models. Any model trained with our proposed methods should be used with caution."}, {"title": "Appendix", "content": "A Experimental Setup\nA.1 Chat Template and Special Tokens\nModel-specific chat templates and special tokens are accessible via the following links:\n\u2022 Qwen2.5: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/tokenizer_config.json\n\u2022 Llama 3.1: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/blob/main/tokenizer_config.json\n\u2022 Gemma 2: https://huggingface.co/google/gemma-2-2b-it/blob/main/tokenizer_config.json\nBelow are excerpts from the chat templates of each model with placeholders for a prompt and output:\nQwen2.5.\n<|im_start|>system\nYou are Qwen, created by Alibaba Cloud.\nYou are a helpful assistant.\n<|im_end|>\n<|im_start|>user\n{prompt}\n<|im_end|>\n<|im_start|>assistant\n{output}\nLlama 3.1.\n<|begin_of_text|>\n<|start_header_id|>system\n<|end_header_id|>\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n<|eot_id|>\n<|start_header_id|>user\n<|end_header_id|>\n{prompt}\n<|eot_id|>\n<|start_header_id|>assistant\n<|end_header_id|>\n{output}\nGemma 2.\n<bos><start_of_turn>user\n{prompt}\n<end_of_turn>\n<start_of_turn>model\n{output}"}, {"title": "Prompt Template", "content": "We translate the English prompt templates provided by Ahia et al. (2023) for SUM with a machine translation API, following Yong et al. (2023). For MT and MC, we formulate a task-specific English prompt, followed by machine translation for each language. For the remaining tasks, we use the default templates provided in lm-evaluation-harness (Gao et al., 2023). Table 2 shows the prompt templates used in our evaluation.\nA.3 Implementation Details\nTokenizer Training. For Gemma 2, we train tokenizers using SentencePiece (Kudo and Richardson, 2018) and convert them into the Hugging Face Tokenizers (Moi and Patry, 2023) format. For Qwen2.5 and Llama 3.1, we train tokenizers using Hugging Face Tokenizers.\nPreprocessing. We preprocess datasets with Hugging Face Datasets (Lhoest et al., 2021).\nContinual Pre-training. We implement our models using PyTorch (Ansel et al., 2024) and Hugging Face Transformers (Wolf et al., 2020). Table 3 lists the hyperparameters in CPT."}]}