{"title": "Inference-Time Language Model Alignment via Integrated Value Guidance", "authors": ["Zhixuan Liu", "Zhanhui Zhou", "Yuanfu Wang", "Chao Yang", "Yu Qiao"], "abstract": "Large language models are typically fine-tuned to align with human preferences, but tuning large models is computationally intensive and complex. In this work, we introduce Integrated Value Guidance (IVG), a method that uses implicit and explicit value functions to guide language model decoding at token and chunk-level respectively, efficiently aligning large language models purely at inference time. This approach circumvents the complexities of direct fine-tuning and outperforms traditional methods. Empirically, we demonstrate the versatility of IVG across various tasks. In controlled sentiment generation and summarization tasks, our method significantly improves the alignment of large models using inference-time guidance from gpt2-based value functions. Moreover, in a more challenging instruction-following benchmark AlpacaEval 2.0, we show that both specifically tuned and off-the-shelf value functions greatly improve the length-controlled win rates of large models against gpt-4-turbo (e.g., 19.51% \u2192 26.51% for Mistral-7B-Instruct-v0.2 and 25.58% \u2192 33.75% for Mixtral-8x7B-Instruct-v0.1 with Tulu guidance).", "sections": [{"title": "1 Introduction", "content": "Learning-based algorithms have become the standard for aligning large language models (LLMs) with human preferences, as evidenced by numerous studies (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Rafailov et al., 2024b; Azar et al., 2024). Despite their success, fine-tuning LLMs is notably resource-intensive and poses implementation challenges (Rafailov et al., 2024b). These challenges have catalyzed the development of inference-time alignment methods that maintain LLMs in a frozen state and guide their decoding during testing (Mitchell et al., 2023; Liu et al., 2024; Mudgal et al., 2023; Kim et al., 2023; Huang et al., 2024; Gao et al., 2023; Beirami et al., 2024).\nValue functions, which assess the quality or alignment of generated text with desired criteria, have proven effective for inference-time alignment in two primary forms: (1) implicit value functions, represented by the log-probability differences between fine-tuned and base models (Rafailov et al., 2024a; Mitchell et al., 2023; Liu et al., 2024; Zhou et al., 2024a; Liu et al., 2021), and (2) explicit value functions, developed through direct training (Mudgal et al., 2023; Yang and Klein, 2021). Our empirical analysis reveals a significant performance discrepancy between these functions at different granularity levels of inference alignment (Section 5): explicit value functions excel at chunk-level evaluation, whereas implicit value functions are more effective at the token-level manipulation.\nRecognizing this performance discrepancy, we introduce a novel algorithm called Integrated Value Guidance (IVG) to harness the strengths of both value function types. IVG combines the strengths of implicit and explicit value functions by applying implicit value functions to token-level sampling and explicit value functions to chunk-level beam search (Mudgal et al., 2023; Zhou et al., 2024c). IVG offers two significant advancements: (1) It integrates the distinctive performances of the two value function types across different granular strategies, thereby validating our theoretical models through empirical tests. (2) It introduces a robust inference-time alignment method, outperforming similar existing techniques in various evaluations. Figure 2 illustrates the IVG method.\nEmpirically, IVG demonstrates its versatility in tasks such as controlled-sentiment generation (Maas et al., 2011a) and summarization (Stiennon et al., 2020), where using small models like gpt2 with 124M parameters is effective in guiding larger models from the GPT-2 series (Radford et al., 2019) to achieve competitive results. Further, in challenging instruction-following benchmarks such as AlpacaEval 2.0 (Dubois et al., 2024), both open-source models (e.g., Tulu guidance) and our fully trained models (e.g., Ultra guidance) significantly enhance the length-controlled win rates of larger models against competitors like gpt-4-turbo (e.g., Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) from 19.51 to 26.51 and Mixtral-8x7B-Instruct-v0.1 (Mistral AI team, 2023) from 25.58 to 33.75)."}, {"title": "2 Related Work", "content": "Large unsupervised language models, trained on vast internet-scale datasets, have demonstrated remarkable capabilities (Chowdhery et al., 2023; Brown et al., 2020; Touvron et al., 2023a). Nonetheless, aligning these models with human values remains challenging. Traditionally, alignment is achieved through fine-tuning based on human evaluations of model-generated responses (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Rafailov et al., 2024b; Touvron et al., 2023b; AI@Meta, 2024; Bai et al., 2022a,b). While effective, this method demands significant computational and engineering resources. Moreover, the diversity of human values complicates the creation of universally aligned models (Ouyang et al., 2022; Zhou et al., 2024b; Mudgal et al., 2023; Rame et al., 2024; Jang et al., 2023; Wang et al., 2024).\nIn response to these challenges, we propose an inference-time alignment approach that freezes pre-trained models while modulating their outputs through a decoding phase managed by smaller, specialized models. This strategy minimizes the need for extensive retraining and adapts more readily to individual preferences.\nThe conceptual framework for aligning language models at inference time is rooted in the use of value functions. Implicit value functions, as described by Rafailov et al. (2024a), proposed a token-level Markov Decision Process to adjust language model outputs based on the log-likelihood differences between fine-tuned and base models. Mudgal et al. (2023) introduced a method that leverages explicit value functions trained through a KL-regularized reinforcement learning objective, which acts as a prefix scorer. Our empirical findings, detailed in Section 5, underscore that while implicit value functions excel at refining token-level nuances, explicit value functions provide superior sequence-level contextual understanding.\nThese insights motivate our method, which integrates these value functions to enhance model alignment with human preferences during the decoding phase of model output generation."}, {"title": "3 Preliminaries", "content": "In this section, we introduce the mathematical formulation of aligning large language models (LLMs) with human preferences and then describe the implicit and explicit value functions used in our approach."}, {"title": "3.1 Large Language Model Alignment with Human Preferences", "content": "Aligning large language models is commonly formulated as a Kullback-Leibler (KL)-constrained optimization problem (Ziegler et al., 2019):\narg max Exp(x),y~\u03c0(y|x) [r(x, y)\n\u03c0\n\u2013 DKL (\u03c0(y | x)||Tref (y | x))], (1)\nwhere p(x) denotes the distribution of prompts, y denotes the responses generated by the language model, r is the preference reward function induced from a preference datasets D = {(x, yw, y1)}, and DKL denotes the KL-divergence that constrains deviations from a reference model Tref."}, {"title": "3.2 Implicit and Explicit Value Functions", "content": "The value function estimates the expected terminal reward r(x, y) when following the policy \u03c0 from a given state (x, y\u2264t):\nV(x, y\u2264t) = Ey~\u03c0(x,yst) [r(x, y)]. (2)"}, {"title": "4 Method", "content": "In this section, we introduce our proposed method, Integrated Value Guidance (IVG). First, we discuss how to utilize the value function in two distinct ways: token-wise sampling (Mudgal et al., 2023) and chunk-level beam search (Zhou et al., 2024c).\nNext, we explain how to train the implicit and explicit value functions on the preference dataset. Finally, we present the overall inference-time alignment process and analyze the computational efficiency of our method."}, {"title": "4.1 Value Function Guided Sampling and Search Strategies", "content": "Given a value function, we can guide the sampling and search strategies in two ways: token-wise sampling and chunk-level beam search."}, {"title": "4.1.1 Token-wise Sampling", "content": "In the token-wise sampling strategy, we use the value function to adjust the sampling distribution of the next token. Specifically, we sample the next token yt according to the following distribution:\n\u03c0(yt | x, y\u2264t-1) \u221d\nbase(yt | x, y\u2264t\u22121) exp(\u1e9e(V(x, y\u2264t)- (5)\n\u03b2(V(x, y\u2264t-1))),\nwhere base (yt | x, y<t\u22121) is the base distribution of the next token, and \u1e9e is a hyperparameter that controls the strength of the value function guidance. For the implicit value function, we have:\n\u03c0\u03af(yt | x, y<t\u22121) \u221d\n\u03c0*(yt | x, y\u2264t-1)\nTref(yt | x, y\u2264t\u22121)\n\u03b2\n(6)\nbase (\u0423 | \u0425, \u0423\u2264-1) (\nx,\nFor the explicit value function, we have:\n\u03c0\u03b5(yt | x, y\u2264t-1) \u221d\n(7)\nbase(yt | x, y\u2264\u22121) exp (\u1e9eVe(x, y\u2264t)) ."}, {"title": "4.1.2 Chunk-level Beam Search", "content": "For search-based generation, the chunk-level beam search strategy is effective for value function-guided sampling. Previous research (Zhou et al., 2024c) has shown that chunk-level beam search outperforms best-of-N (BoN) sampling and requires an effective value function to rank candidate sequences. Specifically, we rank candidate sequences according to the following score:\nr(y<t|x) x V*(x, y\u2264t) \u2013 V*(x), (8)\nwhere r(y<t|x) is the score of the candidate sequence y<t, and V*(x,y<t) and V*(x) are the expected value of the candidate sequence and the prefix, respectively.\nFor the implicit value function, we have:\nri(y<t x) = log\n\u03c0*(y\u2264t x)\nTref(y<tx)\nFor the explicit value function, we have:\nre(y<t|x) = Vo(x, y\u2264t)."}, {"title": "4.2 Training the Implicit and Explicit Value Functions", "content": "There are various methods to train the implicit and explicit value functions on the preference dataset. For the implicit value function, we derive it from the difference in log probabilities between the tuned and untuned models, regardless of how the model was trained. For the explicit value function, we can employ any reinforcement learning algorithm.\nIn this work, we use Direct Preference Optimization (DPO) (Rafailov et al., 2024b) to train the implicit value function and FUDGE (Mudgal et al., 2023; Yang and Klein, 2021) to train the explicit value function.\nFor the implicit value function, we have (Rafailov et al., 2024b):\nlf(x, yw, y\u00b2) = - logo (Blog\n\u03c0\u03bf(\u03b3\u03c9 | x)\nTref(yw | x)\n-\u03b2log\n\u03c0\u03bf(\u03b3\u0384 | x)\nTref(ylx)\n(11)\nFor the explicit value function, we have (Mudgal et al., 2023):\nlf(x,y; 0) = \u2211 (Vo(x, y\u2264t) \u2013 r(x, y))2. (12)\n2\nt\u2208 [y]"}, {"title": "4.3 Integrated Value Guidance", "content": "The token-wise sampling and chunk-level beam search strategies can be combined to enhance the alignment of large language models with human preferences. Specifically, token-wise sampling adjusts the sampling distribution of the next token using the implicit value function, while chunk-level beam search ranks candidate sequences using the explicit value function.\nThe IVG algorithm is illustrated in Figure 2. The key insight is that by applying the implicit value function at the token level and the explicit value function at the chunk level, we effectively leverage the strengths of both. Compared to Weak-to-Strong Search (Zhou et al., 2024c), we sample tokens from a policy adjusted by the implicit value function rather than the base policy and use the explicit value function to rank candidate sequences. Empirically, we find that this combination leads to better alignment with human preferences. We demonstrate the effectiveness of IVG in the following sections."}, {"title": "4.4 Implementation and Complexity", "content": "We analyze the implementation efficiency and computational complexity of the IVG method. At each time step, the main components contributing to the time complexity of IVG include: (1) The base model performs a forward pass to compute the probability distribution of the next token based on the given context. (2) The implicit value functions (including \u03c0* and ref) perform forward passes to compute the probability distributions for the next token and calculate their difference. This difference is then combined with the base model's probability distribution to obtain the final next token distribution. (3) When the current chunk reaches the length L, we compute the value of all candidate sequences using the explicit value function Ve, and select the top-W sequences.\nConsider the complexity of a single decoding step with a context of length t tokens. The complexity is:\n\u2022 If the current chunk length \u2260 L:\nT(t) = Tbase(t) + T\u201e* (t) + \u03a4\u03c0\u03c0 Tref (t).\n\u2022 If the current chunk length = L:\nT(t) = Tbase(t) + T\u201e* (t) + Trref(t) + Tv(t).\nTo simplify, consider the case where L = 1. The total time complexity becomes:\nT(t) = Tbase(t) + T\u201e*(t) + Tref(t) + Tvo (t).\nHere, Tbase(t) and other terms represent the inference time of the respective models for a sequence of length t. Initially, due to pairwise attention computations, T(t) = O(t\u00b2). However, during generation, we can cache previous computations, reducing the complexity to O(t). Therefore, the per-step inference complexity is:\nT(t) = O(t) \u00d7 (Cbase + C\u03c0* + Cref + Cve),\nwhere Cbase, \u0421\u03c0*, Chref, and CV, are constants representing the computational costs of each model. In summary, while IVG does not change the asymptotic time complexity of the generation process, it introduces additional computational overhead due to multiple forward passes and increased memory usage. We will empirically evaluate the computational complexity of different methods in the experimental section."}, {"title": "5 Experiments", "content": "In this section, we empirically evaluate the ability of the proposed IVG to align large language models with human preferences using only inference-time guidance from small language models. First, in controlled-sentiment generation (Maas et al., 2011a) and summarization (Stiennon et al., 2020), we tune gpt2 to model the desired behaviors in each task to get the implicit value function and train the explicit value function based on the same base model. Then, we use the trained implicit and explicit value functions to steer larger models of various scales (Section 5.1).\nNext, in a more difficult instruction-following benchmark, AlpacaEval 2.0 (Dubois et al., 2024), in addition to tuning small models, we reuse the off-the-shelf open-source 7B models and their untuned versions as the implicit value function and train the explicit value function by one of the best-performance sequence-wise reward models evaluated by RewardBench (Lambert et al., 2024a). We then use them to steer a series of large models.\nBaselines. Considering that some existing methods could be represented as special cases of the combinations of implicit and explicit value functions for token-wise sampling and chunk-level beam search, we compare the proposed IVG and different combinations of implicit and explicit value functions with the following baselines: (1) Base: the base model without any value function guidance; (2) Best-of-N Sampling (BoN): BoNi uses r = log \u03c0*(y | x) -log #ref(y | x) as rewards and BoNe uses r = Vo(x, y) as rewards to select the highest-scoring responses among the N independent response from the frozen base language model; (3) FT: fine-tuning the base model on the preference dataset.\nNote that many existing methods can be represented by the framework: Emulator Fine-Tuning (EFT) (Mitchell et al., 2023) can be viewed as applying only the implicit value function in token-wise sampling. Weak-to-strong search can be viewed as applying only the implicit value function in chunk-level beam search. We use the EFTi, EFTe, CBSi and CBSe to represent the corresponding combination. For example, EFTi denotes applying implicit value function for token-wise sampling, and CBSe denotes applying explicit value function for chunk-level beam search."}, {"title": "5.1 Controlled-Sentiment Generation & Summarization", "content": "Setup. For these two tasks, we follow the synthetic setups from (Gao et al., 2023; Lightman et al., 2023; Rafailov et al., 2024b), assuming access to a gold reward model rgold. For controlled-sentiment generation, rgold encourages positive continuations of movie reviews, while for summarization, it encourages high-quality summaries of Reddit posts. We generate synthetic preference datasets D = {(x, y, y');}1 from gold with\np(y1 > y2 | x) = \u03c3(rgold(x, y\u00b9)-rgold(x, y\u00b2)) to\nmimic human feedback (Bradley and Terry, 1952).\nTo obtain the implicit value function, we optimize gpt2 (124M parameters) using the standard DPO pipeline (Rafailov et al., 2024b): (1) we first obtain the reference model ref through supervised fine-tuning on both chosen and rejected responses from the synthetic preference dataset, then (2) we apply DPO on the synthetic preference dataset with Tref as the reference policy to obtain the optimal language model \u03c0*.\nTo obtain the explicit value function, we train a prefix scorer Ve using the FUDGE(Mudgal et al., 2023; Yang and Klein, 2021) algorithm on the synthetic preference dataset. (1) we first train the sequence-wise reward model r(x, y) on the synthetic preference dataset, then (2) we apply the FUDGE algorithm to train the prefix scorer V\u0259 with the sequence-wise reward model r as the reward function.\nGiven the implicit and explicit value functions, we use them to steer the large pre-trained language models without additional training. Since token-wise sampling only supports steering the model sharing the same vocabulary as the base model, here we only study on the gpt2 family models with different scales: gpt2-mudium (345M parameters), gpt2-large (774M parameters) and gpt2-x1 (1.5B parameters). Eventually, since we have access to the gold reward model, responses can be fairly evaluated on the test split of prompts using this gold reward model.\nResults. Figure 3 demonstrates IVG's outstanding performance in both controlled-sentiment generation and summarization tasks. We find that IVG achieves the best performance among all the baselines, showing the effectiveness of the proposed method. To assess the effectiveness of IVG, we examined how different combinations of implicit and explicit value functions perform in two tasks, as depicted in Figure 4. In chunk-level beam search, the explicit value function significantly enhances performance in both tasks, whereas the implicit value function shows lesser improvements. Conversely, in token-wise sampling, the implicit value function notably boosts performance in the controlled-sentiment generation task, but the explicit value function has a minimal impact. This distinction likely arises because the controlled-sentiment generation task primarily requires adjustments at the token level (e.g., \"dislike\" \u2192 \"like\"), whereas summarization demands a focus on broader contextual information. The results suggest that the explicit value function excels in chunk-level beam search, while the implicit value function performs better in token-wise sampling. By integrating both value functions, IVG achieves superior performance in both tasks."}, {"title": "5.2 Instruction-Following", "content": "Setup. We evaluate IVG on AlpacaEval 2.0 (Dubois et al., 2024), a single-turn instruction-following benchmark comprising 805 prompts from various open-source datasets. Here, unlike steering pre-trained models (e.g., Llama-2-7b), we utilize instruction-tuned models (e.g., Llama-2-7b-chat) due to their need for additional alignment as per (Ji et al., 2024).\nFor smaller models, we adopt two strategies to derive implicit and explicit value functions: (1) Tulu guidance: Utilizing tulu-2-dpo-7b and its baseline tulu-2-7b for the implicit function, and training FsfairX-LLaMA3-RM-v0.1 on the UltraFeedback dataset for the explicit function. (2) Ultra guidance: Fine-tuning Llama-2-7b via Direct Preference Optimization (DPO) (Rafailov et al., 2024b) on UltraFeedback for both implicit and explicit functions. All the models use the Llama-2 tokenizer.\nTarget instruction-tuned models include Llama-2-7b-chat-hf, Llama-2-70b-chat-hf, Mistral-7B-Instruct-v0.2 and Mixtral-8x7B-Instruct-v0.1. To manage computational costs, we refrained from directly fine-tuning large models. We explored various valid combinations of implicit and explicit value functions, such as CBSe for chunk-level beam search with explicit value function. Language model responses are evaluated by their length-controlled win rates (LC WR) against gpt-4-turbo, with gpt-4-turbo serving as the judge.\nResults. Figure 5 demonstrates that consistently performs well. Notably, applying an explicit value function to chunk-level beam search significantly enhances outcomes, whereas an implicit value function improves results when applied to token-wise sampling, albeit less effectively for larger models such as Mixtral-8x7BInstruct-v0.1. These findings confirm the theoretical trade-offs between explicit and implicit value functions.\nAblation. We present an ablation study evaluating the inference speed of various methods applied to instruction-following tasks using a single sample. We used TuluGuidance to guide the Llama-2-70B-chat-hf, and the results are illustrated in Figure 6. Our empirical findings corroborate the theoretical analysis. The additional time overhead associated with IVG is primarily due to the extra inference steps required by both the implicit and explicit reward models. Notably, the implicit reward model incurs significantly higher inference costs relative to the explicit model, which can be attributed to the markedly higher forward pass frequency inherent to the implicit approach. The Chunk-level Beam Search (CBS) with hyperparameters W = 2, K = 2, and L = 30 demonstrates a substantial efficiency advantage over Emulator Fine-Tuning (EFT). CBS achieves superior optimization results while incurring only few additional time costs. In contrast, EFT, despite its higher time overhead, yields only modest improvements across certain models. Therefore, in practice, employing Chunk-Level Beam Search alone may represents a more efficient choice."}, {"title": "6 Discussion", "content": "We have presented Integrated Value Guidance (IVG), a method that combines implicit and explicit value functions, applied to token-wise sampling and chunk-level beam search. We conducted experiments on synthetic tasks and instruction-following task and found that IVG achieved the best performance in both tasks. We also explored the performance of different combinations of implicit and explicit value functions in the two tasks and found that the explicit value function applied to chunk-level beam search can significantly improve the results, while the implicit value function applied to token-wise sampling can improve the results. IVG combines the advantages of both, so it achieves the best performance in both tasks."}, {"title": "Limitations", "content": "Our work primarily focuses on enhancing the alignment capabilities of large language models through the integration of implicit and explicit value functions. This approach introduces several complex questions that extend beyond the current scope of our research:\n1. Our methodologies have been limited to using the DPO (Rafailov et al., 2024b) and FUDGE (Mudgal et al., 2023; Yang and Klein, 2021) algorithms for training and deriving the implicit and explicit value functions. It remains unclear whether incorporating other large model alignment strategies or offline reinforcement learning algorithms for token-wise sampling and chunk-level beam search might influence our findings. This aspect warrants additional experimental investigation.\n2. Although we have detailed the empirical outcomes associated with the Implicit and Explicit Value Functions, a theoretical framework that explicates these results is conspicuously absent. Developing a theoretical understanding to underpin these empirical findings is an essential next step for further research."}, {"title": "7 Acknowledgement", "content": "This work is supported in part by the National Key R&D Program of China (NO.2022ZD0160102)."}]}