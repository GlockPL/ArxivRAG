{"title": "REGMIX: Data Mixture as Regression for Language Model Pre-training", "authors": ["Qian Liu", "Xiaosen Zheng", "Niklas Muennighoff", "Guangtao Zeng", "Longxu Dou", "Tianyu Pang", "Jing Jiang", "Min Lin"], "abstract": "The data mixture for large language model pre-training significantly impacts per-formance, yet how to determine an effective mixture remains unclear. We proposeREGMIX to automatically identify a high-performing data mixture by formulatingit as a regression task. REGMIX involves training a set of small models withdiverse data mixtures and fitting a regression model to predict their performancegiven their respective mixtures. With the fitted regression model, we simulate thetop-ranked mixture and use it to train a large-scale model with orders of magnitudemore compute. To empirically validate REGMIX, we train 512 models with 1Mparameters for 1B tokens of different mixtures to fit the regression model and findthe optimal mixture. Using this mixture we train a 1B parameter model for 25Btokens (i.e. 1000\u00d7 larger and 25\u00d7 longer) which we find performs best among 64candidate 1B parameter models with other mixtures. Further, our method demon-strates superior performance compared to human selection and achieves results thatmatch or surpass DoReMi, while utilizing only 10% of the compute budget. Ourexperiments also show that (1) Data mixtures significantly impact performancewith single-task performance variations of up to 14.6%; (2) Web corpora ratherthan data perceived as high-quality like Wikipedia have the strongest positivecorrelation with downstream performance; (3) Domains interact in complex waysoften contradicting common sense, thus automatic approaches like REGMIX areneeded; (4) Data mixture effects transcend scaling laws, and our approach cap-tures the complexity by considering all domains together. Our code is available athttps://github.com/sail-sg/regmix.", "sections": [{"title": "1 Introduction", "content": "The availability of large-scale public datasets has been a key factor enabling the creation of large\nlanguage models (LLMs). Most data is available on the Internet and includes academic papers (e.g.\narXiv), books (e.g. Project Gutenberg), and code (e.g. GitHub). For the creation of one of the first\nLLMS, GPT-3 [7], the authors had already recognized the importance of selecting the best data for\ntraining, and thus they decided to upsample Wikipedia due to its perceived high quality. However,\nsuch manual data selection is not scalable and may lead to a suboptimal selection [3]. As the size and\ndiversity of data used for LLM pre-training continue to grow, determining the optimal data mixture\nbecomes increasingly challenging. It gives rise to the critical research question: How can we select\nthe optimal data mixture in a scalable and efficient manner?\nPrior work [64, 16, 2] employs small-scale models (\u201cproxy models\") to predict the domain weights\nfor large-scale language models. These works train proxy models with a substantial number of"}, {"title": "2 Related work", "content": "Data selection and mixture is concerned with curating data to optimize some goals, usually model\nperformance [29, 3]. Prior methods can be categorized into: (1) Token-level selection is the most"}, {"title": "3 REGMIX: Data mixture as regression", "content": "As illustrated in Figure 2, our method involves four key steps: (1) Generate random data mixtures\nand train small-scale proxy models on these mixtures. (2) Fit a linear regression model using the"}, {"title": "3.1 Train small-scale proxy models", "content": "The first step is to train a set of small-scale proxy models on multiple different data mixtures. To\nreduce the required runs, we aim to select a diverse range of data mixtures that cover extreme weights\nfrom 0% to 100% for each domain. We achieve this by using a Dirichlet distribution based on the\ntoken distribution, which allows us to sample a wide range of values and expose the regression\nmodels to various extremes. Simultaneously, basing the distribution on the token distribution ensures\nthat the overall data mixture statistically reflects the availability of data. For example, this prevents\nany single domain with a token count below 1% from being overly emphasized, which is not feasible\nfor large-scale training since there are not enough available tokens from that domain. In practice, we\nmultiply the token distribution by a value from 0.1 to 5.0 to construct various sparse and near-uniform\ndistributions, then use these distribution vectors as the Dirichlet distribution hyperparameter \u03b1.\nAfter training small-scale proxy models for a few steps, we can obtain several well-trained small\nmodels. For example, in our main experiment, each proxy model contains 1M parameters and is\ntrained on 1B tokens. We can then choose to evaluate these trained models on domains or benchmarks\nto get the target value we want to optimize. Generally, the target value can be the loss on a domain,\nas shown in Figure 2 for the StackExchange domain. Once we have obtained these target values, we\ncan use the data mixture as features and the target values as labels to fit a regression model."}, {"title": "3.2 Fit a regression model", "content": "The second step is to fit a regression model using the data mixture as features, and the target value\nas labels. The regression task is a conventional supervised learning task that involves predicting a\ncontinuous target variable y based on input features X = (x\u2081, x\u2082,..., x\u2099). The goal is to find a\nfunction f that best maps the input features to the target variable, such that y = f(X) + \u03f5, where \u03f5\nrepresents the error or noise in the data. In the context of this paper, the input features X correspond\nto the domain weights of the data mixture, and the target variable y is the value we want to optimize.\nUsing this data, we train regression models that learn a function to predict the target value based on\narbitrary data mixtures without requiring further training.\nLinear regression. The linear regression model is widely used in regression. It assumes a linear\nrelationship between the input features and the target variable, which can be represented as:\ny = w\u2080 + w\u2081x\u2081 + ... + w\u2099x\u2099 + \u03f5                                                                                                                                                             (1)\nwhere w\u2080 is the intercept, and w = (\u03c9\u2081, . . ., \u03c9\u2099) are the coefficients associated with the respective\ninput features x\u2081,...,x\u2099. The coefficients are typically estimated using techniques such as\nordinary least squares, aiming to minimize the sum of squared residuals between the predicted and"}, {"title": "3.3 Simulate and predict", "content": "Once we have trained the regression model, we can efficiently explore the entire space of possible\ndata mixtures. By using the trained model to predict the target value for each potential data mixture,\nwe can quickly identify the input that yields the best target value. This simulation-based optimization\nis relatively cheap, as both the simulation and the regression prediction are computationally fast. For\nexample, running prediction for 1,000,000 data mixtures takes less than 10 CPU seconds."}, {"title": "3.4 Large-scale model training", "content": "After identifying the best data mixture with simulation, we generalize the top-ranked data mixture to\na large-scale model training with many more tokens. As shown in Figure 2, we directly use the best\ndata mixture for training the larger model. In practice, to increase the robustness of our regression pre-diction, we select the top 100 mixtures and average them as the data mixture for large-scale training."}, {"title": "4 Evaluating on regression prediction", "content": "In this section, we evaluate the ability of REGMIX to predict the effect of unseen data mixtures. First,\nwe fit the regression model using training artifacts of small (i.e., 1M parameter) models and evaluate\nthe loss prediction performance on small models. Then, to verify our rank invariance hypothesis, we\ntest the learned regression on predicting the rank across model sizes and the number of tokens."}, {"title": "4.1 Experimental setup", "content": "Datasets and models. We conduct our experiments using the domains of the Pile dataset [18]\ndepicted in Table 1. Due to copyright concerns, we utilize the 17 subsets available on HuggingFace 5\nthat do not violate copyright issues. We consider both linear regression and LightGBM regression\nmodels, where the target variable y is set to be the validation loss of the Pile-CC domain.\nTraining and evaluation. The regression model is fitted using the training artifacts of 512\u00d7 1M\nmodels with 1B tokens, and evaluated on 256\u00d7 unseen data mixtures for 1M, 60M models (each\ntrained with 1B tokens) and 64\u00d7 unseen data mixtures for 1B models (each trained with 25B tokens).\nEvaluation metrics. We use three different metrics to benchmark our regression models: (1)\nSpearman Rank Correlation (\u03c1) is a non-parametric measure of the strength and direction of the\nassociation between two ranked variables. (2) Pearson's r is a measure of the linear relationship"}, {"title": "4.2 Experimental results", "content": "High correlation across model sizes. As shown in Table 2, the LightGBM model demonstrates\nsuperior performance over linear regression models across all three metrics, with its advantage becom-ing increasingly pronounced when evaluating on larger models with more training tokens. Meanwhile,\nthe fact that 1M models trained with 1B tokens can achieve such a high correlation of 97.12% on\nunseen mixtures of 1B models with 25B tokens directly validates our rank invariance hypothesis.\nProxy model count outweighs training token count. Given the same FLOPs budget for small-scale training, we can either increase the token count (i.e., the number of training tokens) or the\nnumber of proxy models. Therefore, we study which approach would yield better performance. As\nshown in Figure 3, increasing the training tokens of the proxy models saturates after approximately\n0.25B tokens. In contrast, increasing the number of proxy models consistently enhances performance,\nparticularly for the LightGBM model. Notably, the performance of 512 models trained on 0.2B tokens\nsurpasses that of 128 models trained on 0.8B tokens, indicating that increasing the number of proxy\nmodels is more effective than increasing the training token count beyond a certain token threshold."}, {"title": "5 Evaluating on downstream tasks", "content": "In this section, we apply our method to demonstrate its effectiveness on realistic downstream tasks.\nFor evaluation, we exclude specific benchmarks that exhibit large performance variance (e.g., RTE)\naccording to the performance traces reported in previous work [36] and our observations during\npre-training. Ultimately, we select the following benchmarks as our downstream tasks: Social\nIQA [51], HellaSwag [70], PiQA [5], OpenBookQA [39], Lambada [43], SciQ [60], ARC Easy [11],\nCOPA [52], RACE [30], LogiQA [32], QQP [59], WinoGrande [50], and MultiRC [27]. These\nbenchmarks cover a diverse range of tasks, enabling a comprehensive evaluation of the real-world\nimpact of REGMIX. For each benchmark, we use normalized accuracy as the evaluation metric if\nprovided by lm-eval-harness [19] else we use regular accuracy."}, {"title": "5.1 Data mixture significantly impacts downstream performance", "content": "Initially, we train 64 models, each with 1B parameters, using different data mixtures. Every model is\ntrained on 25B tokens from the Pile dataset [18], with tokens allocated based on their corresponding\ndomain weights. Table 3 presents the performance of the worst and best models on each downstream\ntask. The reported performance is the average from 0-shot to 5-shot evaluations, scored using the\nlm-eval-harness evaluation framework [19, 4]. We find that the data mixture significantly impacts\ndownstream performances, with the largest performance \u0394 reaching 14.6 on the Lambada task. This\nunderscores the importance of studying the optimal data mixture."}, {"title": "5.2 Web corpora benefits downstream performance the most", "content": "Next, we visualize the correlation between the validation losses of our 64 1B models across different\ndomains and their performance on various downstream tasks in Figure 4 (a). Prior to visualization, we\nhypothesized that the validation loss on the Wikipedia (en) subset would exhibit a strong correlation\nwith most downstream tasks, as it is a high-quality dataset, and many downstream tasks are derived\nfrom Wikipedia text. Similarly, previous work often takes WikiText [38] as a standard benchmark to\nindicate language model performance.\nHowever, surprisingly, the validation loss on the Pile-CC dataset shows the strongest correlation with\nmost downstream tasks. For instance, the correlation coefficient between the HellaSwag task and the\nPile-CC validation loss is remarkably close to 1.0. This unexpected result challenges the conventional\nassumption that WikiText is the most representative dataset for evaluating LLMs. Furthermore, this\nresult aligns with the findings of previous studies [17, 24], which discovered that the validation loss\non the web dataset closely relates to downstream performance.\nMoreover, we analyze the correlation between the loss of models on the C4100Domain validation\nset [34], which is taken from the C4 dataset [47] and supposed to share a similar distribution as\nPile-CC since they are all derived from the CommonCrawl corpus. Since CommonCrawl is a\ncollection of diverse domains, we would expect the correlation between the loss of each domain and"}, {"title": "5.3 Data mixture by REGMIX improves downstream performance", "content": "Previous work has shown that the data mixture method can accelerate LLM pre-training by achieving\na smaller validation loss (or perplexity) using less training tokens [64]. However, a key question is\nwhich validation loss should be optimized? The most intuitive approach, which is also adopted by\nprevious work, is to minimize the loss across all domains. However, based on our study of 1M training\nlogs, we found this to be nearly impossible to achieve in practice. None of the data mixtures were\nable to surpass the human selection on all domain validation losses simultaneously. This suggests that\na naive approach of minimizing the loss across all domains is likely infeasible. Therefore, we choose\nto optimize the Pile-CC validation loss to achieve general performance improvement on downstream\ntasks since it shows the highest correlation with downstream performance.\nWe implement two approaches to determine the data mixture. The first approach relies on human\nintuition. Since Pile-CC and its own distribution should be the closest match, we hypothesized that\npre-training solely on Pile-CC might yield better performance than baselines. The second approach\nleverages REGMIX, using the Pile-CC validation loss as the target variable. We employed LightGBM\nto predict the data mixture which can minimize the Pile-CC validation loss.\nWe compare the performance of our proposed approaches to strong baselines, including selection\ndone by humans for the Pile [18], and DoReMi [64]. For DoReMi we obtain the data mixture directly\nfrom their reported best domain weights and re-normalize it across the available 17 domains. This\nmay result in sub-optimal performance for DoReMi compared to the originally reported results. As\nshown in Table 4, both Pile-CC Only and REGMIX demonstrate strong performance compared to the\nbaselines. On the widely used HellaSwag benchmark, REGMIX shows an improvement of 6.8 over\nHuman selection. Additionally, REGMIX beats all other three methods on the task performance in 8"}, {"title": "5.4 Domain interactions are challenging for humans to understand", "content": "To understand the impact of different domains on each other, we visualize the coefficients (w) of the\nlinear regression model in Figure 6. The visualization provides insights into how the various data\ndomains contribute to the others, revealing complex interactions among them. We also display code\ncorrelation diagrams for each 1M code model trained on The Stack dataset [28]. Surprisingly, both\nthe domain interaction visualization and the code correlation diagrams display complex relationships\nthat are difficult for human experts to fully comprehend. For example, the PhilPapers domain in\nthe Pile dataset appears to provide gains for all other domains under the linear regression modeling,\nwhich is a non-obvious finding that challenges intuitive human understanding. These visualizations\nhighlight the inherent complexity in determining the optimal data mixture, underscoring the value of\nour automated REGMIX approach in efficiently identifying high-performing mixtures, rather than\nrelying solely on human intuition."}, {"title": "5.5 Data mixture effects transcend scaling laws", "content": "Recent research [68, 20] has demonstrated the feasibility of scaling laws for data mixture. However,\nour findings in Section 5.4 suggest that the relationship between domain weights and validation\nloss is more complex than scaling laws might imply. To visualize this complexity, we plotted all\nexperimental points of our 1M training logs in Figure 7. If the scaling law of data mixture held\ntrue, we would expect to see a clear log-log linear relationship across all domains. However, our\nresults reveal a more nuanced picture. For example, the DM Mathematics domain, possibly due to its\ndistinct distribution compared to other domains, exhibits a near log-log linear relationship between\nloss and domain weight. In contrast, for most domains like Pile-CC show more complex patterns,\nwhere predicting validation loss is non-trivial. As shown, domain interactions appear to be intricate,\nmaking it challenging to predict the validation loss for a domain based solely on its weight in the\nmixture. These findings suggest that while scaling laws provide valuable insights, they may not fully\ncapture the intricacies of data mixture dynamics. Our approach addresses the challenge by modeling\nthe entire data mixture as input for the regression model, providing a more comprehensive framework"}, {"title": "6 Conclusion", "content": "In this paper, we present a novel approach, REGMIX, for automatically selecting the optimal data\nmixture for pre-training large language models. REGMIX formulates the data mixture problem as a\nregression task and trains small models to predict the impact of different data mixtures. This enables\nefficient identification of the best mixture, which we then generalize to large-scale model training.\nREGMIX predicts the best data mixture among 64 x 1B models demonstrating its effectiveness.\nMoreover, our large-scale study provides valuable insights into the impact of data mixture, the\nrelationship between loss and downstream performance, and the domain interaction challenges for\nhuman experts in determining the optimal mixture."}, {"title": "A Limitations", "content": "Despite making progress in understanding and optimizing data mixtures for better performance, our\nmethod still has several limitations.\nThe maximum model parameters. We have verified that small models can be used to predict the\noptimal data mixture for large-scale runs with up to 1B parameters. However, much larger models\nare commonly trained with 7B or 70B parameters [58]. Due to compute constraints we leave the\nverification of REGMIX at larger scales to future work.\nThe benchmark coverage. Owing to the scarcity of relevant data in the Pile corpus and the\nrelatively small size of our model at 1B scale, their performance on the MMLU benchmark [22] is\nnearly random and negligible on GSM8K [12]. Consequently, we do not compute the correlation\nbetween the validation loss and scores on these challenging benchmarks.\nThe infinite data assumption. Most existing data mixing methods assume the availability of\nunlimited data for each domain. Although we consider this issue in our no Pile-CC experiments\nin Section 5.3, systematically incorporating the effect of available data into the method remains\nchallenging. Combining our method with the decay coefficient of data reuse proposed in Muennighoff\net al. [41] could be an interesting future work to explore, potentially addressing the limited data\navailability scenario.\nThe domain assumption. A common assumption of existing data mixture methods (including\nours) is that the domain each example belongs to is known. However, this may not always be the case\nand the domain needs to be obtained first. Assigning examples to domains is a hard task, which may\nmake it challenging to apply our methods when the domain boundaries are unclear.\nThe tokenizer assumption. All existing data mixture methods require the use of proxy models\nto obtain domain weights. However, a fundamental assumption of these methods is that the proxy\nmodel uses the same tokenizer and vocabulary size as the large model. Generalizing weights across\ndifferent tokenizers poses significant challenges."}, {"title": "B Ethic statements", "content": "Optimizing the data mixture for LLM pre-training raises several ethical issues. First, the optimized\ndata mixture might be biased toward certain domains, which is good for achieving better performance.\nHowever, certain domains might be underrepresented or misrepresented, leading the trained models\nto perform poorly or produce biased results for these domains. Second, though our method aims to\noptimize the data mixture efficiently, searching for the optimal data mixture still requires computa-tional resources, leading to high energy consumption and environmental impact. It is worthwhile to\nexplore how to further reduce the computation cost."}, {"title": "C.1 The regression prediction visualization", "content": "As shown in Figure 8, we visualize the predicted and true loss pairs of the linear model and LightGBM\nmodel on the 1M models. The LightGBM model performs better than the linear model, achieving\nnear 100% Spearman Rank Correlation \u03c1."}, {"title": "C.2 Loss and rank prediction on small models for out-of-distribution setting", "content": "In Section 5, we verify the effectiveness of our method in out-of-distribution scenarios where we\nfully exclude the Pile-CC domain from the pre-training corpus and use the remaining domains to\nfind the optimal data mixture that minimizes Pile-CC validation loss. We also provide the results of\nregression evaluation under this setting in Figure 5. Similarly, LightGBM model outperforms the\nlinear model and achieves nearly 100% Spearman Rank Correlation \u03c1."}, {"title": "C.3 The derived data mixtures", "content": "Table 6 presents the derived data mixture weights for different methods. As illustrated, REGMIX\nassigns a high weight of 0.87 to the Pile-CC dataset, aligning with human intuition."}, {"title": "C.4 The evaluation results using LightEval", "content": "Following the approach of FineWeb [44], we employ the LightEval 7 library to evaluate our models\nusing a suite of benchmarks selected for their stability and suitability. The chosen benchmarks\nexhibit three key characteristics: low score variance across different data samples, monotonic score\nimprovement during training, and above-random baseline scores for models in the 1B parameter\nrange. Table 7 presents the evaluation results. Our method, REGMIX, consistently outperforms the\nHuman baseline on 6 benchmarks. Moreover, REGMIX demonstrates superior average performance\ncompared to the DoReMi and the Pile-CC Only methods."}, {"title": "E Implementation details", "content": "We utilize the model architecture proposed by Zhang et al. [71] and create various model variants\nby modifying the number of layers, the number of attention heads, and the dimensions of token\nembeddings and hidden states, as illustrated in Figure 8. For tokenization, we employ the GPTNeoX\ntokenizer [6], which has a vocabulary size of 50,432.\nFor models with 1M and 60M parameters, we set the training iterations as 1000 and the batch size as\n1M tokens, which means the training budget is 1B tokens. Similarly, we train the larger model with\n1B parameters with 25000 training iterations and the same batch size thus consuming 25B tokens in\ntotal. We set the learning rate as 4e-4 and use the cosine learning rate scheduler.\nFor linear regression, we employ 5-fold cross-validation with ridge regression to determine the\noptimal l\u2082 regularization weight from the set [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]. For LightGBM,\nwe manually set the number of iterations to 1000 and the learning rate to le-2. leaving all other\nhyperparameters at their default values."}, {"title": "F The stability of our method", "content": "Previous research [64, 16, 2] has employed small-scale proxy models, trained on substantial volumes\nof tokens, to predict optimal data mixtures for large language models. However, these approaches\noften suffer from instability issues. For example, DoReMi [64] reported that different proxy model\nsizes can result in significantly different predicted data mixtures. Their findings (Figure 8, Appendix)\nshow that using a 280M proxy model resulted in a Pile-CC weight of 0.67, while a 1B proxy model\nyielded a Pile-CC weight below 0.20. The large discrepancy highlights potential instabilities in\nprevious approaches. To evaluate the robustness of REGMIX against such instabilities, we conducted\ncomparative experiments using two distinct model scales: a 1M proxy model and a 60M proxy\nmodel. We used their respective training logs to fit regression models and subsequently simulated the\ntop 1024 predictions. The resulting distributions are plotted in Figure 13. Our results demonstrate\nthat while the prediction distributions for the 1M and 60M models are not identical, they exhibit\nremarkably similar patterns. This consistency suggests that REGMIX achieves improved stability\ncompared to previous approaches, even when varying the scale of proxy training models."}, {"title": "G Detailed experimental results", "content": "To facilitate future research, we share all the data mixtures and the corresponding downstream\nperformances of the 64 trained models with 1B parameters."}]}