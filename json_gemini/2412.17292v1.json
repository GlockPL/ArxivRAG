{"title": "AV-EmoDialog: Chat with Audio-Visual Users\nLeveraging Emotional Cues", "authors": ["Se Jin Park", "Yeonju Kim", "Hyeongseop Rha", "Bella Godiva", "Yong Man Ro"], "abstract": "In human communication, both verbal and non-verbal cues\nplay a crucial role in conveying emotions, intentions, and\nmeaning beyond words alone. These non-linguistic in-\nformation, such as facial expressions, eye contact, voice\ntone, and pitch, are fundamental elements of effective in-\nteractions, enriching conversations by adding emotional\nand contextual depth. Recognizing the importance of\nnon-linguistic content in communication, we present AV-\nEmoDialog, a dialogue system designed to exploit verbal\nand non-verbal information from users\u2019 audio-visual inputs\nto generate more responsive and empathetic interactions.\nAV-EmoDialog systematically exploits the emotional cues\nin audio-visual dialogues; extracting speech content and\nemotional tones from speech, analyzing fine-grained facial\nexpressions from visuals, and integrating these cues to gen-\nerate emotionally aware responses in an end-to-end man-\nner. Through extensive experiments, we validate that the\nproposed AV-EmoDialog outperforms existing multimodal\nLLMs in generating not only emotionally appropriate but\nalso contextually appropriate responses.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) [3, 5, 11, 45] excel in gen-\nerating coherent and contextually appropriate text. Trained\non extensive dialogue text, they facilitate human-machine\ninteraction by generating relevant responses to user queries.\nHowever, the non-linguistic aspect of dialogue, crucial for\neffective and human-like communication has not been suf-\nficiently explored.\nHuman communication is inherently multi-modal, incor-\nporating rich verbal and non-verbal cues. The non-linguistic\ncues, defined as any transfer of messages that do not involve\nthe use of words, include facial expressions, eye move-\nments, and paralinguistic features such as tone and pitch\nof voice [7, 46]. These cues convey emotions and inten-\ntions beyond words, significantly changing the meaning of\na word. For example, the word \"fine\" when said with a\ncheerful tone and a smile, can genuinely mean everything is\ngood, but when uttered with a flat tone and a stern face, it\nmay imply dissatisfaction. This is the primary reason why\nmany prefer face-to-face interactions over pure text-based\nones to reduce potential misunderstandings. Therefore, the\nuser's emotional state should be precisely and sufficiently\nconsidered to generate a contextually appropriate response.\nRecent research [4, 18, 21] has attempted to address\nthe emotional aspect of dialogue systems. Typically, these\nmethods follow a cascade approach. They rely on text input,\nrequiring a cascade of ASR module to obtain a transcription\nof spoken words [4, 21]. Furthermore, they employ a pre-\ntrained emotion classifier on either audio or visual [1] to cat-\negorize into one of seven labels (i.e. happy, sad, surprised,\nfearful, disgusted, angry, and neutral). However, relying on\na single modality to extract emotional cues limits the rich-\nness of the emotional context, and categorizing emotion into\nseven labels simplifies the complex and subtle emotional\nstates that naturally evolve during interactions. Such clas-\nsification limits the system's ability to accurately learn the\nrich emotional nuances from the multimodal input. More-\noever, they are not optimized end-to-end across both audio\nand visual modalities, resulting in suboptimal emotional un-\nderstanding and reduced efficiency in response generation.\nIn this paper, we introduce AV-EmoDialog, which di-\nrectly processes audio-visual user input and generates an\nempathetic response based on the analyzed linguistic and\nnon-linguistic information in end-to-end. We propose a sys-\ntematic scheme to exploit fine-grained emotional cues in the\naudio-visual input by utilizing detailed descriptions relevant\nto the emotional context. The training proceeds in three\nstages. First, a speech encoder is trained to extract both\nspeech content and nuanced emotion from the user's speech\ninput through a high-performing chat-LLM. Second, a face\nencoder is trained to extract the fine-grained emotion from\nthe user's facial video through the LLM. For fine-grained\nemotion extraction, we create detailed facial descriptions\nfrom the speaker face video. Last, it puts the speech and\nface encoders together with the LLM to directly aggregate"}, {"title": "2. Related Works", "content": "capture the communicative signals.\n2.2. Multimodal Large Language Models\nThere has been a surge of Multimodal Large Language\nModels (MM-LLMs) [26, 28, 31, 32, 34, 41, 44, 47, 48]\nthat are capable of processing multimodal input. BLIP-2\n[28] introduces a Q-Former module to bridge the modal-\nity gap between the visual and text. Llava [31, 32] aligns\nthe visual and text modality by utilizing a large-scale mul-\ntimodal instruction-following dataset on image QA, image\ndescription, and complex reasoning. Video-LlaMA [51]\nand Video-ChatGPT [34] process the visual and audio con-\ntent in a video to engage in a conversation describing the\nvisual scenes and actions in a spatio-temporal context. Hug-\ngingGPT [41] conducts task planning to connect various AI\nmodels upon receiving a user request and NextGPT [48]\nbuilds an end-to-end system that is capable of understand-\ning and generating audio, visual, and text modality. How-\never, they primarily concentrate on multimodal grounding\ntasks. They excel in integrating multimodal data for content\nreasoning but do not leverage this information to enhance\ninteraction with the human user.\nRecently, a few multimodal large language models [4,\n18, 21] tailored for emotion-aware dialogue system at-\ntempted to incorporate emotional context. FaceChat [4] em-\nploys a pre-trained facial emotion classifier [42] to predict\ntextual emotion labels from the user's facial expression and\nincorporate this label into the LLM's template prompt. It\nalso needs an ASR module to transcribe speech into text,\nand utilizes the zero-shot capabilities of a pre-trained text\nLLM for response generation. SMES [18] utilizes a pre-\ntrained Video-Llama [51] to extract emotion labels of each\ntime step via prompting. These emotional labels are then\ncombined with text inputs and fed into an LLM for re-\nsponse generation. EmpathyEar [21] utilizes an ImageBind\n[22] to extract multimodal embeddings, which are also com-\nbined with text embeddings to be fed into an LLM. No-\ntably, these approaches follow a cascade approach, rely-\ning on pre-trained components such as emotion classifiers\nand ASR modules, which are not optimized end-to-end on\naudio-visual dialogue. Moreoever, they depend on emo-\ntion classifiers that assign emotions to one of only seven\nbroad categories, restricting emotional understanding and\nleading to a simplified interpretation of temporally varying\nand complex user emotions.\nIn contrast, our work presents an audio-visual spoken\ndialogue system that directly processes both linguistic and\nnon-linguistic content from audio-visual input, without re-\nlying on intermediate text. Our system is trained end-to-end on audio-visual dialogue data, ensuring that the com-\nmunicative signals from the input modalities are jointly\nlearned. We demonstrate our proposed method's strong\nability to generate responses that are both contextually and"}, {"title": "2.1. Spoken Dialogue Models", "content": "Recent efforts have aimed to leverage the capabilities of\nlarge language models (LLMs) [8, 16, 45, 52] for enhanc-\ning spoken dialogue systems. They aim to autoregressively\nmodel the semantic and acoustic content of raw speech to\nprocess and generate speech. d-GSLM [35] models two-\nchannel conversations to produce natural turn-taking con-\nversations. SpeechGPT [50] initially converts speech to\ndiscrete speech tokens, followed by a three-stage train-\ning pipeline involving paired speech data, speech instruc-\ntion data, and chain-of-modality instruction data. Audio-\nGPT [24] guides LLMs to generate commands for interact-\ning with external tools before processing these commands\nwithin the models. Qwen-Audio-Chat [17] is trained on ex-\ntensive audio understanding tasks and instruction fine-tuned\non dialogue framework.\nSome spoken models [2, 30, 49] have incorporated emo-\ntional cues from speech to enhance the model's ability to\ntailor responses to diverse emotional contexts. By predict-\ning emotion labels alongside each response, it helps the\nLLM better understand the user's emotional state and en-\nables it to generate responses that are more emotionally\nattuned and contextually appropriate. While these studies\nhave discerned emotional cues within speech, they overlook\nnon-verbal cues present in the visual modality. As human\ninteraction is inherently multimodal, with visual cues such\nas facial expressions and eye contact playing a crucial role\nin conveying emotions and intentions, our work aims to in-\ntegrate the audio and visual information of the user to better"}, {"title": "3. Proposed Method", "content": "Our approach to empowering large language models\n(LLMs) with the ability to interpret and respond to both\nverbal and non-verbal cues involves three stages: Sec 3.1\nextracting speech content verbal emotional cues from the\naudio input, Sec 3.2 extracting non-verbal emotional cues\nfrom the user video input, and Sec 3.3 integrating the ver-\nbal and non-verbal cues to build an audio-visual user en-\ngaged dialogue system. Below, we detail each stage of our\nmethodology."}, {"title": "3.1. Extracting Verbal Cues with Speech Encoder", "content": "From the raw audio input, we extract both the linguistic\ncontent (i.e. what the speaker says) and verbal emotional\ncues. This stage equips an audio encoder with the abil-\nity to understand such communicative signals directly from\nspeech such that the LLM can directly understand. We em-\nploy a strong audio encoder, Whisper model [40] which\nhas shown powerful capabilities in speech-text processing\ntasks such as speech recognition (ASR), speech translation,\nvoice activity detection. The audio encoder is connected\nto the high-performing chat-LLM, Llama-3-Instruct\u00b9, as il-\nlustrated in Figure 1(a). We directly feed audio features $f_a$\nfrom Whisper model to the LLM and align the feature space\nbetween the audio and text using an ASR training objec-\ntive as in [17, 43, 48, 50] on extensive audio-text datasets;\nGigaspeech [15], Common Voice [6], and LibriSpeech [36].\nBy training on ASR objective, the model can directly under-\nstand the speech content and align the speech feature that\ncan be interpreted by the LLM.\nAdditionally, we incorporate speech emotion recogni-\ntion (SER) objective to extract emotional tone from speech.\nBy inputting speech features into the LLM instead of tran-\nscribed text, the LLM can infer deeper verbal aspects of\nspeech such as the tone and pitch. We use speech data with\nemotion labels; RAVDESS [33], CREMA-D [13], MultiDi-\nalog [37]. The LLM remains frozen while we fine-tune the\naudio encoder. This dual training objective empowers the\naudio encoder to extract both linguistic content and emo-\ntional context that can be directly understood by the LLM.\nThe hard prompt, namely the speech understanding prompt,\ngiven during this stage is shown in Figure 2(a)."}, {"title": "3.2. Extracting Non-Verbal Cues with Face Encoder", "content": "The face video contains rich non-verbal cues essential for\nnuanced human communication. To capture these subtle\nnon-verbal cues, we deploy a face encoder with the ability\nto recognize the fine-grained facial emotion that can be di-\nrectly interpreted by the LLM. As shown in Figure 1(b), the\nface encoder consists of two components; the frame encoder\nand the temporal encoder. The frame encoder, CLIP-ViT\n[39], known for strong visual-text alignment, extracts facial\nfeatures at the frame level. Then, the temporal encoder ag-\ngregates the frame-level features to learn facial dynamics as\nemotion varies over time. Similar to [14, 25], we employ\nlearnable queries that cross-attend to the frame-level fea-\ntures to output a fixed-length video feature $f_v$. This allows\nfor the efficient processing of videos of varying lengths,\nwhich not only reduces computational costs but also facili-\ntates handling longer dialogue sequences. The output visual\nfeatures are directly fed into the LLM, and trained to recog-\nnize emotion labels on RAVDESS [33], CREMA-D [13],\nand MultiDialog [37] datasets.\nTo optimize the training of the face encoder, we incorpo-\nrate detailed descriptions of the facial expressions. Specifi-\ncally, emotion-labeled face videos in the training datasets\nare further annotated with comprehensive descriptions of\nfacial dynamics using GPT-4, as illustrated in Figure 2.\nThese descriptions include granular details such as move-\nments of the cheeks, lips, and eyes, as well as the pro-\ngression of emotional expressions over time. By incor-\nporating comprehensive facial descriptions along with the\nemotion labels as the training objectives, the face encoder\nlearns to recognize the nuanced and dynamic aspects of\nhuman expressions that go beyond simplified emotion cat-\negories. Emotions during communication are inherently\ntime-varying, often not consistently maintaining the same\nemotion. The time-considered detailed descriptions help\nthe model recognize the evolving emotional states that go\nbeyond the static and singular emotion categorization. It\nalso guides to focus on specific facial attributes for accu-\nrate emotion detection, akin to how humans process emo-\ntion from the face. The face encoder is fine-tuned while\nthe LLM remains frozen. The hard prompt, namely the\nface video understanding prompt, given to LLM during this\nstage is shown in Figure 2 (b)."}, {"title": "3.3. Audio-Visual Dialogue Modeling", "content": "We build an emotion-aware audio-visual dialogue frame-\nwork, namely AV-EmoDialog, by integrating the speech en-\ncoder, face encoder, and a LLM as shown in Figure 1 (c).\nSince the speech encoder and the face encoder are equipped\nwith strong capabilities to process the linguistic and non-\nlinguistic cues from audio-visual user input from the pre-\nvious stages, the LLM is simply adapted to incorporate the\nmultimodal communicative signals to generate contextually\nappropriate responses. In our setting, each dialogue consists\nof R rounds of turns between the AI and the user, with the\nAI producing textual responses to audio-visual provided by\nthe user. We use audio-visual dialogue language modeling"}, {"title": "4. Experiments", "content": "4.1. Evaluation Metrics\nWe evaluate the dialogue generation in terms of semantic\nquality and emotional quality. For the semantic quality, we\nemploy standard metrics used for text-based dialogue gen-\neration: BLEU-1, BLEU-4 [38], ROUGE [29], METEOR\n[9], and PPL [10]. PPL is calculated using Dialog-GPT [54]\nand it is calculated across the test set. For the emotional\nquality, we measure EmoBERT score [53] which calculates\nsemantic similarity between the generated response and ref-\nerence response using a BERT model finetuned on GoE-"}, {"title": "5. Results", "content": "5.1. Automatic Evaluation of Dialogue Generation\nWe quantitatively compare the dialogue generation results\nin Table 1. Our experiments were conducted on the test rare\nsplit of the MultiDialog dataset. For each conversation, we\nrandomly selected four turns from the assistant's side. Com-\npared with state-of-the-art multimodal large language mod-\nels, our method significantly improves the EmoBERT score,\nwhich assesses the emotion alignment of the generated re-\nsponses with the ground truth emotion. In addition, the se-"}, {"title": "5.2. GPT Evaluation", "content": "We incorporated GPT-4 evaluation to further assess the\ngenerated response in Table 2. Since EmoBERT only\nassesses alignment with ground truth emotional labels, a\nmore thorough evaluation was necessary to understand the"}, {"title": "5.3. Human Evaluation", "content": "We conducted a human evaluation through Prolific to com-\npare the quality of responses generated by various dialogue\nmodels. 10 gathered English-speaking partic-\nipants were instructed to rank generated responses from\nfour different models according to predefined criteria: rel-\nevance, coherence, contextual appropriateness, emotional\ncoherence, and engagement. Table 3 shows the percentage\nwith which each model was ranked first. The results indi-\ncate that AV-EmoDialog was most frequently selected as the\ntop-performing model, which coincides with our automatic\nand GPT evaluations."}, {"title": "5.4. Analysis on Audio-Visual Emotion-Aware Re-sponse", "content": "We analyzed the emotional context of response enhanced\nby AV-EmoDialog in Figure 4. In (a), the user speaks in a\ncheerful tone with a happy smile, asking the AI about its\npreference for football. The proposed method effectively\nrecognizes the user's happy emotion through audio-visual\ncues, which would not have been possible with text input\nalone. AV-EmoDialog responds accurately to the happy\nemotional tone of the user by saying \"Are you excited for\nthe Super Bowl?\". On the other hand, the baseline methods\nfail to reflect the user's emotional tone, resulting in less res-\nonant responses such as \"I'm sorry. I'm not familiar with\nthis topic\" or \"Good evening! I'm a big fan of football.\".\nIn (c), the user talks with a sad and depressed face about\nhow he feels stuck in the snow. Although it may sound\nlike the user is frustrated from the text alone, the proposed\nmethod most accurately empathizes with the user's sadness,\nresponding with \"Ughh...\", and suggests one way to get out\nof the bad feeling. In contrast, baseline methods misin-\nterpret the emotional context, responding with surprise or\nchanging the topic. Likewise in (d), the user expresses sur-\nprise about the explosion through face, tone, and the content\nitself. While the baseline methods moves on to a different\naspect of the topic, the proposed method mirrors the user's\nsurprise and engages further in the same emotional tone,\nby responding \"That is just crazy money... That is more\nthan the budget of the Star Wars trilogy.\". In each example,\nthe proposed method effectively recognizes the user's emo-\ntion from the speech tone and facial expressions, providing\nthe most emotionally appropriate responses that would have\nbeen difficult with pure text-only input. Please refer to ap-\npendix for more generation results. Note that we provide\nresponses in speech using an off-the-shelf TTS model for\ndemonstration."}, {"title": "5.5. Effectiveness of Exploiting Emotion in Audio-Visual Dialogue", "content": "We tested AV-EmoDialog with emotion excluded from the\nlast stage of training on the audio-visual dialogue task. The\nresults are shown in the second to last row in Table 1. There\nwas a huge drop in emotion performance - from 0.3 to 0.2,\nwhich suggests that having the emotion label in the dialogue\nallows the model to keep track of the understanding of the\nemotional context.\nWe evaluated the training schemes used for the speech\nand face encoders in Table 3. We observed that incor-\nporating the detailed descriptions of the facial expressions\ngreatly increased the emotion recognition performance of\nthe face encoder by 70%. Also, the emotion recognition\nperformance of the speech encoder is higher than from the"}, {"title": "5.6. Comparison of Audio, Visual, and Text as Input\nfor Emotion-aware Dialogue", "content": "In Table 5, we conducted experiments with various modal-\nity inputs to validate the efficacy of each modality in di-\nalogue generation. The text-only input has the highest se-\nmantic quality overall, which can largely be attributed to the\nfact that the underlying Ilm has been extensively trained on\ntextual data. Yet, utilizing both audio-visual modalities was\nable to achieve on-par semantic performance while achiev-\ning superior emotional intelligence.\nAnother finding is that using both audio-visual modal-"}, {"title": "6. Conclusion and Limitation", "content": "ities results in enhanced semantic and emotional perfor-\nmance compared to using audio modality alone. This im-\nprovement underscores the value of visual cues such as fa-\ncial expressions and eye contact, which provide comple-\nmentary information to audio cues such as pitch and tone\nof voice. Our approach reflects natural human communica-\ntion, where verbal cues are typically supported and enriched\nby non-verbal expressions, creating a more holistic and ef-\nfective interaction.\nThe machine's ability to recognize the user's emotional\nstate has great implications for human-machine communi-\ncations; a virtual assistant that recognizes frustration can\noffer more targeted help, a customer service bot equipped\nwith empathy can better handle complaints, and therapeutic\nservice can comprehensively diagnose patients. Our pro-\nposed AV-EmoDialog contributes to the field of emotion\naware dialogue systems by exploiting linguistic and non\nlinguistic information directly from audio-visual inputs.\nAV-EmoDialog relies on audio-visual input without ad-\nditional text input to optimize the system end-to-end for\nemotion-aware audio-visual dialogue. By leveraging fine-\ngrained facial descriptions, AV-EmoDialog not only aligns\nmore closely with human communication patterns but also\nsets a new standard for the design of empathetic and effec-\ntive dialogue systems. Our extensive experiments confirm\nthat AV-EmoDialog surpasses existing baseline models in\nboth emotional and semantic dimensions.\nTo further explore the potential of our proposed model,\nwhich is designed to handle emotional interplays in dia-\nlogues, it would be beneficial to have a more diverse audio\nvisual dataset from real-world scenarios, where emotional\ninteractions are more prevalent. Also, the future research\ndirection would be to generate the speech in end-to-end. By\ngenerating speech that reflects the emotional tone of the re-\nsponse, it would be possible to create more immersive con-\nversations for users. For ethical considerations, as the sys-\ntem directly utilizes audio-visual user data, it is crucial to\nensure the privacy and security of sensitive information."}, {"title": "7. Evaluation Metrics", "content": "BLEU [38] evaluates the fluency and adequacy of generated\nresponses based on n-gram overlap. A higher BLEU score\nindicates a more natural and engaging dialogue model. We\nmeasure BLEU-1 and BLEU-4 score where BLEU-1 mea-\nsures the overlap at unigram and BLEU-4 considers up to\n4-grams, focusing more on the contextual and syntactical\nrelationships between words.\nPPL [10] measures how well a language model predicts the\ngenerated response. A lower perplexity indicates that the\nmodel is more confident and accurate in predicting the next\nword, suggesting higher quality in generating coherent and\ncontextually relevant responses.\nMETEOR [9] assesses the quality of generated response\nby computing the alignment-based precision and recall be-\ntween the generated output and the ground truth, consider-\ning synonyms and paraphrases.\nROUGE [29] measures the overlap of n-grams between the\ngenerated output and a set of reference outputs based on\nrecall (i.e. how much of the reference is captured by the\noutput).\nEmoBERTscore [53] assess the semantic similarity be-\ntween the generated response emotion and the reference re-\nsponse emotion. We utilize the BERT model specifically\nfine-tuned on GoEmotions [19] dataset by Google, which\nincludes 58k Reddit comments labeled for 27 emotion cat-\negories including neutral. This model is adept at detecting\na broad range of emotions. The features extracted from the\nBERT model are used to calculate the similarity distance\nbetween the ground truth and the generated response."}, {"title": "8. GPT evaluation", "content": "We ran GPT evaluation on the following criteria; coherence,\nfluency, and emotional intelligence. Fluency evaluates the\ngrammatical correctness, smoothness, and natural flow of\nthe response. The response should read as if it were spo-\nken or written by a fluent language user. Emotional con-\ntext evaluates how appropriate the emotion conveyed in re-\nsponse is within the context of the dialogue history. Empa-\nthy evaluates how well the response demonstrates an aware-\nness of the user's feelings. We instruct GPT to provide a\nscore in the range of 0 to 5 with regard to the three criteria.\nThe prompt given to the GPT is shown in Figure 6."}, {"title": "9. Human evaluation", "content": "We conducted a human evaluation through Prolific to com-\npare the quality of responses generated by various dialogue\nmodels. Ten English-speaking participants were recruited\nand tasked with ranking the responses generated by four\ndifferent models based on predefined criteria: relevance, co-\nherence, contextual appropriateness, emotional coherence,\nand engagement. Detailed instructions provided to the par-\nticipants are shown in Figure 7."}, {"title": "10. Datasets", "content": "Common Voice [6] is a large-scale multilingual speech-text\ncorpus collected from volunteers around the world. It con-\ntains 2,615 hours of English speech from 92,325 voices with\ndiverse genders, ages, and accents.\nGigaSpeech [15] an English speech-text corpus from vari-\nous sources such as podcasts, audiobooks, and YouTube, ac-\ncompanied by accurate transcriptions. We use a large subset\nwhich is 2,500 hours. It is the common benchmark used for\nspeech processing tasks.\nLibriSpeech [36] is approximately 1,000 hours of English\nspeech derived from audiobooks in the LibriVox project.\nMultiDialog [37] is a large-scale audio-visual dialogue\ndataset, consisting of approximately 370 hours of 9,000 di-\nalogues between 6 pairs of speakers. It is based on the Top-\nical Chat dataset which is a knowledge grounded human\nhuman conversation covering 9 broad topics. There are\nemotion annotations for each utterance. It is the only avail-\nable large-scale audio-visual dialogue dataset to date.\nRAVDESS [33] is an emotional audio-visual data consist-\ning of 1,4440 files from 24 actors (12 female and 12 male)\nspeaking in a natural North American accent and in six dif-\nferent emotions (happy, sad, angry, fearful, surprise, dis-\ngust, and calm) with two different intensity levels (normal\nand strong)\nCREMA-D [13] is an emotional audio-visual data consist-\ning of 7,442 audio-visual clips from 91 actors (48 male and\n43 female) between the ages of 20 to 74 from various races\n(African American, Asian, Caucasian, Hispanic, and un-\nspecified). They speak in six different emotions (anger, dis-\ngust, fear, happy, neutral, and sad) in four different emotion\nlevels (low, medium, high, and unspecified)."}, {"title": "11. Extracting Facial Descriptions", "content": "We use GPT to annotate the face video with rich facial de-\nscriptions relevant to emotions. The prompt given to the\nGPT is: These are the frames in a video. In the video, a per-\nson expresses emotion through facial expressions. Describe\nthe subtle and fine-grained emotion precisely as you see in\nthe video in two sentences. We only annotate a subset of\nRAVDESS, CREMA-D, and Multidialog for training. We\nwill open-source the annotations. The samples are shown in\nFigure 5. The generated descriptions provide detailed infor-\nmation of facial dynamics associated with emotions, captur-\ning the progression and intensity of the emotion-nuances\nthat cannot be conveyed by a single-word emotion category."}, {"title": "12. More Generated Results", "content": "We have included more generation results in Figure 8,\nwhich presents a comparison of the generation results\nfrom our proposed method against those from LLaVA-\nNext [31], SpeechGPT [50], and the Cascade model. Our\nAV-EmoDialog directly audio-visual as the input and gen-\nerated more emotion-aware response, guided by the audio\nvisual input."}]}