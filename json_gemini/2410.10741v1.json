{"title": "SensorBench: Benchmarking LLMs in Coding-Based Sensor Processing", "authors": ["Pengrui Quan", "Xiaomin Ouyang", "Jeya Vikranth Jeyakumar", "Ziqi Wang", "Yang Xing", "Mani Srivastava"], "abstract": "Effective processing, interpretation, and management of sensor data have emerged as a critical component of cyber-physical systems. Traditionally, processing sensor data requires profound theoretical knowledge and proficiency in signal-processing tools. However, recent works show that Large Language Models (LLMs) have promising capabilities in processing sensory data, suggesting their potential as copilots for developing sensing systems.\nTo explore this potential, we construct a comprehensive benchmark, SensorBench, to establish a quantifiable objective. The benchmark incorporates diverse real-world sensor datasets for various tasks. The results show that while LLMs exhibit considerable proficiency in simpler tasks, they face inherent challenges in processing compositional tasks with parameter selections compared to engineering experts. Additionally, we investigate four prompting strategies for sensor processing and show that self-verification can outperform all other baselines in 48% of tasks. Our study provides a comprehensive benchmark and prompting analysis for future developments, paving the way toward an LLM-based sensor processing copilot.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Large Language Models (LLMs) have sparked interest in their application to sensor signal processing and physical-world interactions. LLMs have shown promise in tasks such as health trace analysis and motion sensor interpretation [6, 14, 21]. For example, LLMs have been used to analyze complex health datasets, including spatiotemporal household sensor traces, aiding in the diagnosis and monitoring of Alzheimer's patients [14]. Additionally, LLMs have demonstrated the ability to interpret data in mobile sensing applications [6, 21].\nDespite these promising results, integrating LLMs into sensor processing systems faces significant challenges. The studies in existing literature are fragmented, using varied methodologies, datasets, and evaluation metrics, making it difficult to form a cohesive understanding of LLM performance in this domain. Furthermore, debates persist in both academic and industrial circles about the reliability of LLMs for tasks like planning and reasoning [18, 19]. Without a structured approach, assessing the full potential and limitations of LLM-based sensor processing systems remains challenging.\nRecognizing the need for a structured evaluation of LLMs' sensor processing abilities, this paper makes the first systematic attempt to address the gap. Specifically, we aim to answer two key research questions:\n(Q1) How well do LLMs perform sensor processing? What is their absolute and relative performance compared to human experts? This analysis helps determine whether LLMs can serve as viable alternatives to human experts or if they fall short in key areas of the sensor processing workflow.\n(Q2) Can LLMs benefit from a reasoning approach inspired by the iterative problem-solving mindset of human experts? We explore the effectiveness of advanced prompting strategies, such as Chain-of-Thought (CoT) [20], ReAct [22], and self-verification [17], to see if mimicking expert reasoning improves LLM performance.\nTo address these questions, we construct SensorBench, a comprehensive benchmark comprising a diverse range of sensory types and tasks that are meticulously crafted to test various aspects of LLM performance in realistic scenarios. Our evaluation provides answers to Q1, revealing that LLMs perform comparably to experts on simpler tasks but struggle with more complex compositional tasks and parameter selection. Furthermore, to enhance LLMs' capabilities as sensor processors, we explicitly employ existing state-of-the-art prompting strategies to guide LLMs to mimic the human \"cognitive process.\" Out of the four prompting strategies we tested, our adapted self-verification approach demonstrates significant advantages, outperforming other baselines 48% of the time. The evaluation results answer Q2, showing that LLMs benefit significantly from iterative sanity checks and reflection on the solutions.\nOur main contributions are summarized as follows:\n\u2022 We propose a comprehensive benchmark for evaluating LLM performance in sensory data processing, providing a structured framework for future research.\n\u2022 We provide an in-depth comparison of LLM performance against human experts, highlighting the current state of LLM capabilities and identifying areas for improvement.\n\u2022 We investigate various prompting strategies for sensor processing and demonstrate that our adapted self-verification [17] outperforms other prompting methods in 48% of tasks, including the base model, CoT, and ReAct approaches."}, {"title": "2 Related Works", "content": "LLM for Sensor Data Analysis. Sensor data are sequences of measurements recorded of the physical world. As extensive physical-world knowledge was obtained from text, LLMs have recently been explored for their potential in sensor data analysis. Recently, [21] investigated the use of LLMs in processing and reasoning about ECG signals and satellite WiFi SSID signals. [14] focused on high-level reasoning tasks using sensor traces from household sensors, such as air quality sensors and occupancy sensors. It applied LLMs to diagnose and monitor Alzheimer's disease from the sensor traces. Besides, an application of LLMs in human activity recognition (HAR) using IMU sensors has also been investigated in [6].\nLLM-based Agents. There has been a long history of building LLM-based agents by combining existing tools with LLM [3, 15, 23], which can lead to more potent and up-to-date solutions to real-world applications. However, the current solution differs from our work from the following few perspectives: 1) The problem-solving abilities of LLMs in real-world sensing tasks is still an open problem. 2) Existing works study problems with a combination of well-performed APIs that do not require expert-level knowledge."}, {"title": "3 SensorBench", "content": "We focus on multichannel temporal sensor signals, such as audio, ECG, PPG, motion, and pressure signals, as shown in Table 1. Since sensor processing typically involves digital signal processing (DSP) methods, we start by meticulously selecting possible DSP tasks commonly seen in engineering, health care, and industrial settings. We select examples from MATLAB tutorials [9, 10], leveraging their practical and instructional approach, and subsequently refer to established DSP textbooks [13, 16] to create our benchmark. This method ensures that we capture real-world applications and validate them against established literature.\nTo assign a level of difficulty to each task, we further categorize the tasks by the following factors:\n\u2022 Single/Compositional. Single tasks involve straightforward, isolated API calls, while compositional tasks require integrating multiple processes or signal understanding. Single tasks are assigned a difficulty level of 1, and compositional tasks are assigned a difficulty level of 2.\n\u2022 Parameterized/Non-parameterized. Parameterized tasks involve specific critical parameters determining the processing quality, such as setting stop-band frequency for spectral filtering tasks, while non-parameterized tasks do not include such procedures. Similarly, Non-parameterized tasks have a difficulty level of 1, while parameterized tasks have a difficulty level of 2.\nWe calculated the overall difficulty for each task by summing the two factors. Overall, table 1 summarizes SensorBench, with the share of each category in the benchmark composition."}, {"title": "3.2 Evaluation Settings", "content": "We instruct LLMs to perform automated coding for signal processing tasks and provide LLMs with a Python coding environment with API access (the defined list of available APIs includes numpy, scipy, pandas, pmdarima, statsmodels, and ruptures). Fig. 2 shows the setting of using LLMs. We benchmark the performance of the four different LLMs: GPT-40 [12], GPT-4 [1], GPT-3.5-turbo [11], and Llama-3-70b [2].\nFor each task and dataset combination, we selected 3 distinct data samples. Each experiment was repeated 3 times to account for variability. For each combination, this approach yields a total of 9 examples (3 signals \u00d7 3 repetitions). To analyze the results, we calculate the trimmed mean, which involves dropping the highest and lowest scores for the 9 examples and calculating the mean of the 7 middle examples. This approach ensures that our benchmarking results are less affected by possible extreme values produced by unbounded metrics such as MSE and SDR."}, {"title": "3.2.3 Evaluation Metric", "content": "We explained the metrics in Table 1 as follows:\nSignal-to-Distortion Ratio (SDR). A higher SDR indicates better performance, reflecting the accuracy of audio signal reconstruction.\nF1 Score. A higher F1 score signifies better performance in tasks involving detection, balancing precision, and recall.\nMean Squared Error (MSE). A lower MSE indicates a closer match between the processed signal and the ground truth.\nTo obtain the order of performance across models, we use the following metrics:\nWin rate. Eqn. 1 shows the percentage of times a model achieves the best performance across a set of tasks\nFailure rate. Eqn. 2 indicates the percentage of times a model outputs invalid signals.\nwin rate = $\\frac{\\sum_{i=1}^{N} I(\\text{output is the best for task }i)}{N} \\times 100\\%$ (1)\nfailure rate = $\\frac{\\sum_{i=1}^{N} I(\\text{output is invalid for task }i)}{N} \\times 100\\%$ (2)"}, {"title": "4 Experimental Results", "content": "We compare LLMs with two experts in sensor processing. We gave both LLMs and experts access to Python coding environments and APIs. Table 2 shows relevant details, including win rates against all LLMs. Fig. 3 shows the taskwise performance for LLMs and the domain experts (we show the reciprocal of MSE to ease the comparison).\nExperts are better: Change point detection, Spectral filtering-speech, Outlier detection, Echo cancellation, Heart rate calculation, and Period detection. Figure 3a, 3b, 3k, 3j, 3f, and 3h demonstrates that both Expert 1 and Expert 2 outperform the LLMs in the tasks by more than 50% Among the LLMs, Llama-3, GPT-40, and GPT-4 has the highest performance, followed by GPT-3.5. None of the LLMs are close to the experts' performance. These tasks usually require multi-step planning, iterative problem-solving, and parameter selection, with the highest difficulty levels.\nExperts are comparable: Spectral filtering-ECG, Resampling, Delay detection, Imputation, and Extrapolation. As shown by Fig. 3, GPT-40, GPT-4, GPT-3.5, and Llama-3 outperform or achieve comparable performance in Spectral filtering-ECG and Resampling compared to the domain experts, which indicates the potential of using LLMs in these tasks. It is worth noticing that these tasks can be done by calling APIs with existing world knowledge obtained through training corpus. For example, LLMs can rely on scipy to design a bandpass filter from 0.5 Hz to 45 Hz, to remove almost all possible noise in ECG. The tasks have difficulties \u2264 3 by our definition.\nFinally, we want to point out that it takes experts a significant amount of time to solve these tasks - both experts spent more than 10 hours on the tasks. However, the latency of LLMs is around 2 to 5s for each task, depending on the size and the deployment of models.\nBased on the results, we answer Q1 - While the current LLMs can achieve comparable performance on simple tasks (difficulties \u2264 3) in our benchmark, there are significant gaps between harder tasks (difficulties > 4) that require iterative problem-solving and parameter tuning."}, {"title": "4.2 Results of Prompting Strategies", "content": "In this section, we compare the performance of our design with three state-of-the-art prompt engineering approaches: Base, CoT, ReACT, and self-verification. Given the most advanced reasoning and planning abilities reported in existing literature [2, 3], our experiments focus on GPT-40 with prompting variations."}, {"title": "4.2.1 Baselines", "content": "The Base represents the simplest form of interaction with the LLM. It involves straightforward queries and a basic coding environment without additional design. ReAct[22]. ReAct is a prompt engineering strategy where the model is encouraged to reflect on its previous outputs and actions before proceeding with the next step. This method aims to simulate a more iterative and thoughtful problem-solving approach.\nCoT[20]. Chain of Thought (CoT) prompting involves guiding the model through a series of logical steps to reach a solution. This method breaks down complex tasks into smaller, manageable steps, encouraging the model to reason through each part sequentially.\nSelf-verification[17] asks LLMs to propose tentative solutions, obtain initial feedback, and then refine their strategies based on task feedback. Specifically, [17] relies on external environmental feedback, which is impractical for real-world sensor processing applications. For example, in industrial machinery monitoring, there is no oracle to confirm whether the detected data point is an outlier, and relying on such an oracle would negate the need for LLMs. Building on [17], we employ another LLM as an internal sanity checker to provide autonomous feedback."}, {"title": "4.2.2 Results", "content": "Table 3 shows the performance comparisons across different signal processing tasks.\nTable 3 shows that self-verification achieves the highest win, 48%, and lowest failure rates, 5.75%, among all the methods. Moreover, ReAct also performed better than CoT and baseline in general, especially in compositional tasks. Lastly, CoT showed comparable success compared with the base prompting approach.\nThe results demonstrate that self-verification can provide more gains for the task with a difficulty level of 4 as it guides the model to reflect on its solutions. However, we observe that Expert 1 outperforms self-verification prompting 72% of the time, and Expert 2 does so 68% of the time."}, {"title": "5 Conclusion and Discussion", "content": "In this work, we systematically investigate whether LLMs can perform sensor processing. Our study begins with introducing a comprehensive SensorBench to systematically analyze the LLMs' signal processing abilities for sensing tasks.\nOur findings reveal that while LLMs demonstrate competence in simpler tasks, there is a significant performance gap when it comes to more challenging tasks. Despite improvements from self-verification prompting\u2014which enhances models' reasoning abilities and yields better performance relative to baseline strategies\u2014the LLM still falls short of human-level performance. In fact, human experts outperform LLMs by over 60% on these demanding tasks, underscoring the current limitations in LLM planning and reasoning abilities for compound and parameterized sensor processing.\nDespite these challenges, it is worth noting that the complex tasks in SensorBench are also challenging for human experts, who often spend hours solving them effectively. This highlights the inherent difficulty of the tasks rather than the inadequacy of the models.\nLooking ahead, enhancing LLMs' planning and reasoning abilities remains crucial for closing the performance gap between human experts. We encourage the research community to invest further effort in exploring the strategies of task-specific fine-tuning and scaling test-time computation. These advancements can potentially push LLMs closer to human-level performance in complex sensor processing tasks."}]}