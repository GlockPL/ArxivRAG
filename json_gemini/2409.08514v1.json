{"title": "Apollo: Band-sequence Modeling for High-Quality Audio Restoration", "authors": ["Kai Li", "Yi Luo"], "abstract": "Audio restoration has become increasingly significant in modern society, not only due to the demand for high-quality auditory experiences enabled by advanced playback devices, but also because the growing capabilities of generative audio models necessitate high-fidelity audio. Typically, audio restoration is defined as a task of predicting undistorted audio from damaged input, often trained using a GAN framework to balance perception and distortion. Since audio degradation is primarily concentrated in mid- and high-frequency ranges, especially due to codecs, a key challenge lies in designing a generator capable of preserving low-frequency information while accurately reconstructing high-quality mid- and high-frequency content. Inspired by recent advancements in high-sample-rate music separation, speech enhancement, and audio codec models, we propose Apollo, a generative model designed for high-sample-rate audio restoration. Apollo employs an explicit frequency band split module to model the relationships between different frequency bands, allowing for more coherent and higher-quality restored audio. Evaluated on the MUSDB18-HQ and MoisesDB datasets, Apollo consistently outperforms existing SR-GAN models across various bit rates and music genres, particularly excelling in complex scenarios involving mixtures of multiple instruments and vocals. Apollo significantly improves music restoration quality while maintaining computational efficiency. The source code for Apollo is publicly available at https://github.com/JusperLee/Apollo.", "sections": [{"title": "I. INTRODUCTION", "content": "Audio restoration has gained widespread application across various scenarios, ranging from music playback to real-time communication systems. For instance, in restoring vintage music, audio restoration methods effectively rejuvenate classic music pieces eroded by time or constrained by outdated equipment [1], [2]. Moreover, these methods are found to be extensively used in speech communication, particularly in telephone or internet calls, by repairing low-quality or distorted codec audio at the receiving end, thereby delivering a clearer and more natural auditory experience [3]-[6]. In music playback, audio restoration mitigates the degradation caused by compression, ensuring that users enjoy high-fidelity audio [3], [7], [8]. For generative models, such as those used in music generation and speech synthesis, the audio quality is crucial, and restoration methods can enhance data quality, thus significantly improving model performance [9], [10]. Robust audio restoration methods have become indispensable components of modern audio processing systems.\nAudio restoration involves predicting high-quality, undistorted audio from degraded or compressed inputs. Current audio restoration technologies primarily focus on vocal recovery [3]\u2013[5]. In traditional methods, a common technique is bandwidth extension [4], [5], which aims to reconstruct lost high-frequency information and improve the perceptual quality of highly compressed audio signals. High-frequency spectral extension enhances encoding efficiency and proves crucial in low-bitrate scenarios [11]. However, in some cases, bandwidth extension can introduce high-frequency artifacts that may degrade the overall audio signal quality.\nWith the rapid advancement of deep learning, NN-based methods have gradually replaced traditional signal-processing methods. Recently, GANs [12] have demonstrated substantial potential in audio super-resolution and restoration tasks [1], [13], especially in achieving high-quality restoration. In audio codecs [14]\u2013[16], GANs effectively balance perceptual audio quality with distortion, offering superior restoration performance compared to traditional methods. Audio degradation typically affects the mid-to-high-frequency bands, particularly when using lossy codecs such as MP3 or AAC [17], where high-frequency information is prone to compression artifacts. An ideal generator should retain the original audio\u2019s low-frequency components and supplement smooth and delicate mid-to-high-frequency details, thereby achieving a more realistic audio restoration effect. The Gull codec [16] has successfully demonstrated the effectiveness of GANs in the audio codec, showing significant progress in the super-resolution reconstruction of music and speech during the decoding phase of lossy codecs.\nInspired by Gull, we propose the Apollo model, a generative model specifically designed for high-sampling-rate audio restoration tasks. Apollo supports restoring audio quality at different compression rates. It comprises three main modules: a frequency band split module, a frequency band sequence modeling module, and a frequency band reconstruction module. Unlike Gull, we employ Roformer [18] in the frequency band sequence modeling module to capture frequency features and use TCN to model temporal features, enabling more efficient audio restoration. Specifically, Apollo first divides the spectrogram into sub-band spectrograms with predefined bandwidths, extracts gain-shape representations for each sub-band spectrogram, and encodes them through a bottleneck layer. Subsequently, stacked frequency band-sequence modeling modules perform interleaved modeling across frequency bands and sequences. Finally, each sub-band feature is mapped through nonlinear layers to generate the estimated restored sub-band spectrogram. These modules\u2019 design ensures the preservation of low-frequency information while restoring high-quality mid and high-frequency components. Additionally, with causal convolution and causal Roformer, our model supports streaming processing, making it suitable for real-time audio restoration.\nWe evaluated Apollo on the MUSDB18-HQ [19] and MoisesDB [20] datasets, comparing it with state-of-the-art models such as SR-GAN [1]. The experimental results showed that Apollo performed exceptionally well across various compression bitrates and music genres, particularly in complex scenarios involving a mixture of multiple instruments and vocals. Additionally, Apollo\u2019s efficiency in streaming audio applications has been validated, demonstrating its potential in real-time, high-quality audio restoration."}, {"title": "II. APOLLO", "content": "Fig.1(a) presents the proposed Apollo pipeline. Apollo operates in the time-frequency domain and comprises a band-split module, a band-sequence modeling module, and a band-reconstruction module. Specifically, given compressed or distorted audio $S\\in R^{1\\times L}$, we first transfer $S$ to its time-frequency domain representation $X \\in C^{F \\times T}$ using the Short-Time Fourier Transform (STFT), where $L$ denotes the length of audio, $F$ and $T$ denote the number of frequency bins and frames, respectively. Then, the band-split module maps to sub-band embeddings $Z\\in R^{N\\times T}$ using gain-shape representations $G\\in R^{3\\times M\\times T}$ for each sub-band, where $N$ and $M$ denote the number of channels in sub-band embeddings and gain-shape representations, respectively. Next, the band-sequence modeling module performs joint modeling of temporal and sub-band using a stacked architecture based on Roformer [18] and temporal convolutional network (TCN) [21], [22]. Finally, the band-reconstruction module converts the output $Q\\in R^{N\\times T}$ of the band-sequence modeling module into the reconstructed complex-valued spectrogram $Y \\in C^{F \\times T}$. It uses the inverse Short-Time Fourier Transform (iSTFT) to convert $Y$ to a waveform $S\\in R^{1\\times L}$.\nAs shown in Fig.1(b), given compressed or distorted audio spectrogram $X$, we first split its frequency dimension $F$ into $K$ sub-band spectrograms $\\{X_k \\in C^{M_k \\times T}|k \\in [1, K]\\}$. Inspired by the Gull codec [16], we extract gain-shape representations $G_k \\in R^{3\\times M_k\\times T}$ for each sub-band spectrogram:\n$G_k = Concat\\left[\\frac{Re(X_k)}{||X_k||_2}, \\frac{Im(X_k)}{||X_k||_2}, log(||X_k ||_2)\\right]$, (1)\nwhere $Re(X_k)$ and $Im(X_k)$ denote the real and imaginary parts, respectively. $||X_k||_2$ represents the $l_2$-norm of $X_k$, given by:\n$||X_k ||_2 = \\sqrt{Re(X_k)^2 + Im(X_k)^2}$ (2)\n$log(||X_k ||_2)$ is the logarithm of the $l_2$-norm of $X_k$. Concat refers to the concatenation of components. The gain-shape representation decouples the sub-band spectrogram\u2019s content and energy, allowing the reconstruction model to learn appropriate mappings that preserve the audio content. Subsequently, we map the gain-shape representations G into high-dimensional embeddings Z through a bottleneck layer, which consists of RMSNorm [23] and a 1D convolutional layer.\nIn Apollo, we employ stacked Band-sequence modeling modules (BS modules, Fig.1(c)) to perform joint sub-band and temporal modeling with a stacking depth of B. Unlike BSRNN [24] and Gull [16], each BS module consists of a series of residual Roformers [18] and TCNs, which sequentially scan along the sub-band and time dimensions, and can increase the modeling capacity to improve the model performance. First, the residual Roformer is applied to the input Z along the frequency band dimension K to obtain $Z' \\in R^{N\\times T}$, capturing global dependencies between sub-bands while preserving"}, {"title": "III. EXPERIMENT CONFIGURATIONS", "content": "We trained and tested Apollo on the combined MUSDB18-HQ [19] and MoisesDB [20] datasets. By integrating these two datasets, we leveraged their rich diversity and comprehensive musical resources to evaluate Apollo\u2019s restoration performance across different music genres more thoroughly. During the data preprocessing stage, inspired by music separation techniques [10], [26], we employed a Source Activity Detector (SAD) to remove silent regions from the tracks, retaining only the significant portions for training. Throughout training, we implemented real-time data augmentation by randomly mixing tracks from different songs. Specifically, we randomly selected between 1 and 8 stems from 11 available tracks and extracted 3-second clips from each selected stem. These clips were then randomly scaled in energy within a range of [-10, 10] dB relative to their original levels. All selected stem clips were summed together to generate simulated music. Subsequently, we simulated dynamic bitrate scenarios by applying MP3 codecs with bitrates of [24000, 32000, 48000, 64000, 96000, 128000] to generate the compressed music. To ensure all samples were on the same scale, we rescaled both the target audio and the encoded audio based on the maximum absolute value."}, {"title": "IV. RESULTS", "content": "We evaluated the restoration performance of the Stochastic-Restoration-GAN (SR-GAN) [1] and Apollo models across various bitrates and music genres on the combined test set of MUSDB18-HQ and MoisesDB (with 5000 samples for each case). The test set encompasses a wide range of music genres, including vocals, single instruments, and mixed instruments, aiming to comprehensively assess each model\u2019s restoration capabilities.\nFig.2 compares the performance of the Apollo model and the Stochastic-Restoration-GAN (SR-GAN) at different bitrates (ranging from 24 kHz to 128 kHz). The experimental results demonstrated that Apollo consistently outperformed SR-GAN across all bitrates, particularly in addressing issues such as frequency band voids or reduced signal bandwidth, as reflected by SI-SNR and SDR scores. Additionally, Apollo significantly improved audio restoration quality as measured by VISQOL. Project page for Apollo's reconstructed audio given multiple MP3 bitrates.\nTable II further illustrates the performance of both models across different music genres. In audio scenarios involving vocals, single instruments, mixed instruments, and a combination of instruments with vocals, Apollo consistently surpasses SR-GAN, with its advantage being especially pronounced in complex scenarios with mixed instruments and vocals. This is attributed to Apollo\u2019s alternating band and sequence modeling design, which emphasizes capturing and restoring complex spectral information. Compared to SR-GAN, Apollo delivers higher user ratings (VISQOL) with comparable inference speed while maintaining a more compact model size. This is especially important for real-time communications and live audio restoration, where low latency is critical to the user experience."}, {"title": "V. CONCLUSION", "content": "We propose Apollo, a novel method specifically designed for compressed audio restoration. Apollo significantly enhances audio quality in the frequency domain through band split, sequence modeling, and reconstruction modules. Empirical evaluations on the integrated MUSDB18-HQ and MoisesDB datasets validate Apollo\u2019s outstanding performance. Notably, Apollo achieves substantial improvements in music restoration while maintaining a smaller model size and high computational efficiency. The experimental results demonstrated that when addressing the complex acoustic characteristics of music, band-split, and band-sequence modeling more effectively captured and restored audio information lost during compression."}]}