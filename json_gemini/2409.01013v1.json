{"title": "SeCo-INR: Semantically Conditioned Implicit Neural Representations for\nImproved Medical Image Super-Resolution", "authors": ["Mevan Ekanayake", "Zhifeng Chen", "Gary Egan", "Mehrtash Harandi", "Zhaolin Chen"], "abstract": "Implicit Neural Representations (INRs) have recently advanced the field of deep learning due to their ability to learn continuous representations of signals without the need for large training datasets. Although INR methods have been studied for medical image super-resolution, their adaptability to localized priors in medical images has not been extensively explored. Medical images contain rich anatomical divisions that could provide valuable local prior information to enhance the accuracy and robustness of INRs. In this work, we propose a novel framework, referred to as the Semantically Conditioned INR (SeCo-INR), that conditions an INR using local priors from a medical image, enabling accurate model fitting and interpolation capabilities to achieve super-resolution. Our framework learns a continuous representation of the semantic segmentation features of a medical image and utilizes it to derive the optimal INR for each semantic region of the image. We tested our framework using several medical imaging modalities and achieved higher quantitative scores and more realistic super-resolution outputs compared to state-of-the-art methods.", "sections": [{"title": "1. Introduction", "content": "Advancements in deep learning (DL) research have revolutionized medical image super-resolution [3, 22, 35, 36]. These DL methods often rely on supervised learning techniques to transform low-resolution images into high-resolution ones, using large cohorts of datasets. While this approach demonstrates remarkable success in specific scenarios, its performance deteriorates when faced with out-of-distribution cases, as many models struggle to generalize beyond the extent of the training data. Furthermore, for medical imaging applications, it is challenging to obtain massive amounts of training data due to privacy concerns and high acquisition costs. As a result, exploring unsupervised or semi-supervised DL methods for image super-resolution has become important with the ultimate goal of advancing the precision and reliability in medical imaging [9].\nImplicit Neural Representations (INRs) have recently advanced the field of signal representation due to their ability to learn continuous representations of signals using a discrete set of samples without the requirement of large training datasets [27]. In contrast to conventional approaches that discretely store signal values on coordinate grids, INRs train neural networks, particularly Multilayer Perceptrons (MLPs), to represent continuous signals [18]. The objective is to approximate the intricate connection between coordinates and their corresponding signal values, ultimately producing a continuous representation of the signal. Due to their immense success in signal representation and solving inverse problems, INRs have been utilized in many tasks recently [2, 5, 19]. Some of the recent medical imaging-specific advancements include arbitrary scale 3D super-resolution [30, 31], assessing tumor progression with sparsely sampled images [25], jointly learning different medical imaging modalities of complementary views [16] and reconstruction of isotropic images from anisotropic images [34] belonging to a variety of medical imaging modalities like magnetic resonance imaging (MRI) and computed tomography (CT) imaging.\nWhile INR techniques have been recently explored to enhance medical image super-resolution [16, 33], their potential to benefit from localized anatomical priors has not been thoroughly investigated. Medical imaging modalities contain valuable local priors that could effectively enhance the representation of images [4, 21]. For instance, different cortical regions of a brain image exhibit various contrasts and geometrically varied anatomical structures due to the unique signal attributes captured by the imaging system [6, 14]. In this work, we hypothesize that this semantic contrast and shape information can benefit INRs in learning the continuous representation of a medical image. To this end, we proposed a framework, referred to"}, {"title": "2. Methods", "content": ""}, {"title": "2.1. Implicit Neural Representations", "content": "Implicit Neural Representations encode an underlying continuous signal, $G$ via a deep neural network, $f_\\theta : \\mathbb{R}^M \\rightarrow \\mathbb{R}^P$, where $M$ is the dimension of input coordinates and $P$ is the dimension of signal value at that particular coordinate. As the deep neural network, often an MLP with 3-4 layers is utilized with suitable activation functions in between. This network learns a functional mapping between the coordinates, $v \\in \\mathbb{R}^M$ (e.g., $v = (x,y)$ for 2D, $v = (x, y, z)$ for 3D, etc.) and the underlying signal, $G(v) \\in \\mathbb{R}^P$ (e.g., intensity, color, occupancy in 3D space, camera view, etc.) [32]. This can be achieved by optimizing the discrepancy between a set of discrete samples from $G(v)$ and the corresponding network output $f_\\theta(v)$, given by $|| f_\\theta(v) \u2013 G(v)||_2$. Each layer of $f_\\theta$ is activated using $y_\\iota = \\sigma(W_\\iota y_{\\iota-1} + b_\\iota)$, where $\\iota = 1, 2, . . ., L - 1$, and $L$ is the total number of layers in the MLP. Here $W_\\iota \\in \\mathbb{R}^{d_{\\iota-1}\\times d_\\iota}$ and $b_\\iota \\in \\mathbb{R}^{d_\\iota}$ denote the weights matrix and the bias vector at the $[th layer of the network. $y_\\iota \\in \\mathbb{R}^{d_\\iota}$ is the output of the $[th layer and $\\sigma$ is the activation function.\nWhen Rectified Linear Unit (ReLU) is used as the activation function, i.e., $\\sigma(x) = max(0, x)$, the network tends to favor the representation of low-frequency signals which often leads to lower-quality signal representations [24]. To address this issue, Sitzmann et al. [27] proposed sinusoidal representation networks (SIRENs), which utilized periodic activation functions to fit the signal as well as its higher-order derivatives, i.e., $\\sigma(x) = sin(x)$. Although SIREN outperforms ReLU in terms of image representation and various other tasks, there remains room for improvement in managing the sine activation function more effectively. Subsequent works demonstrated that the choice of $w_0$ is critical to achieving higher representation accuracies [24] and explored control over the parameters of an extended sinusoidal function for better representation quality [11]."}, {"title": "2.2. SeCo-INR", "content": "Our proposed SeCo-INR framework contains three major components: Adaptive SIREN Network, Pixel Class Representation Network, and Conditioner Network which we discuss in detail below. The overall framework is"}, {"title": "2.2.1 Adaptive SIREN Network", "content": "This is the central part of the framework that maps the input coordinates to their corresponding output intensities of the image. We define this network, $f_\\theta : \\mathbb{R}^M \\rightarrow \\mathbb{R}^P$ as an MLP and periodic nonlinear activation functions with additional parametrization as below:\n$y_\\iota = p \\sin(qw_0 (W_\\iota y_{\\iota-1} + b) + r) + s$ .  (1)\nNote that the above function is reduced to SIREN [27] if $p = 1, q = 1,r = 0, s = 0$. In our framework, the values of $p,q,r,s$ are dynamically learned in every iteration of the training process, with assistance from the semantic prior information of the underlying image. Each parameter uniquely influences the characterization of the representation. The parameter $p$ sets the amplitude of the periodic signal and affects the strength of the activation, aiding in noise suppression. The parameter $q$ controls the frequency scaling of the periodic signal, focusing on the fine details of the representation and reducing high-frequency noise. The parameter $r$ adjusts the phase shift of the periodic signal, influencing the spatial arrangement in the representation. Lastly, the parameter $s$ establishes the vertical shift of the periodic signal, managing the overall contrast [11]."}, {"title": "2.2.2 Pixel Class Representation Network", "content": "In our work, we introduce a Pixel Class Representation Network to learn a continuous representation of the semantic segmentation map of a medical image. Such a segmentation map contains pixel-wise physical localization pertinent to the target image. The Pixel Class Representation Network $g(\\phi)$, which is an INR model with a softmax output layer is trained to iteratively learn a continuous representation of segmentation mask $S$. It takes the coordinates $v$ as the input to generate an output semantic class distribution of those coordinates, $\\hat{S}(v) = g(v)$. We train the weights of this network using a cross-entropy loss as $\\cdot \\Sigma_i S(v)_i log(\\hat{S}(v)_i)$, considering $S(v)$ as the ground truth semantic class distribution for each coordinate $v$. The details about the ground truth segmentation masks are given in Section 3.1."}, {"title": "2.2.3 Conditioner Network", "content": "This network $h_\\psi : \\mathbb{R}^h \\rightarrow \\mathbb{R}^{4L}$ utilizes an MLP structure and takes the local prior latent code $\\hat{S}(v) \\in \\mathbb{R}^h$ (derived by the Pixel Class Representation Network) as input and produces the optimal parameters for each semantic region of the underlying image, i.e., $p(v), q(v), r(v), s(v) = h_\\psi(\\hat{S}(v))$ which is then utilized to condition each layer of the Adaptive SIREN Network separately. The Conditioner"}, {"title": "2.2.4 Loss Function", "content": "The Loss function for the training of the overall framework is based on the reconstruction loss which enforces the Adaptive SIREN Network to fit the target image and the classification loss which enforces the Pixel Class Representation Network to learn the semantic class distributions of the coordinates. We further add a regularization term to the loss function, which mandates that the parameters $p, q, r, s$ maintain non-negative values. This type of regularization has been proven to steer the model toward more applicable solutions, accelerate model convergence, and diminish the chance of the model getting stuck in local minima during training. All trainable networks of the framework are trained in a data-driven end-to-end approach for a given target image, $I(v)$, and the overall loss function can be presented below:\n$\\underset{\\theta,\\psi,\\varphi}{\\text{argmin}} \\mathbb{E}_{v\\in V} [||\\hat{I}(v) \u2013 I(v)||^2 + \\beta \\sum_i S(v)_i \\log(\\hat{S}(v)_i)]$\ns.t. $p(v) \\geq 0, q(v) \\geq 0, r(v) \\geq 0, s(v) \\geq 0, \\forall v$ (2)\nwhere $\\hat{I}(v) = f_{\\theta}(v|p(v), q(v), r(v), s(v))$,\n$p(v), q(v), r(v), s(v) = h_{\\psi}(\\hat{S}(v))$ and $\\hat{S}(v) = g(\\upsilon)$.\nThe hyperparameter $\\mathcal{B}$ determines the trade-off between reconstruction loss and the classification loss."}, {"title": "2.2.5 Super-resolution Inference", "content": "Having trained the framework and obtained the optimum model parameters, $h^*_{\\psi}$, $f^*_{\\theta}$, and $g^*_{\\phi}$ on a given low-resolution image until convergence, the super-resolution inference can be obtained by first generating a set of extended pixel coordinates (i.e. super-resolution), $v'$. Next, the Pixel Class Representation Network can be used to generate a high-resolution semantic segmentation mask using:\n$\\hat{S}(v') = g_{\\phi}(v')$ (3)\nThen, the Conditioner Network can be used to generate the optimum $p, q, r, s$ parameters for each layer of the Adaptive SIREN Network using:\n$p(v'), q(v'), r(v'), s(v') = h_{\\psi}(\\hat{S}(v'))$. (4)\nFinally, the image intensities of the super-resolution image can be derived using the optimized Adaptive SIREN Network conditioned on the optimized parameters:\n$I(v') = f_{\\theta}(v'|p(v'), q(v'), r(v'), s(v'))$. (5)"}, {"title": "2.3. Rationale behind feeding local semantic features", "content": "The rationale behind the utilization of semantic information is to condition the adaptive SIREN Network with the local prior knowledge, thereby learning distinct sets of $p, q, r, s$ parameters for each semantic region of the image. For instance, the segmentation masks of a brain MRI (e.g., gray and white matter divisions, brain tumor segmentation, abnormalities, sub-cortical structures derived from a brain atlas, etc.) play a pivotal role in distinguishing different cortical regions of a brain image that exhibit various contrasts due to proton density, biophysical relaxation parameters, and geometrically varied anatomical structures [6, 14]. Similarly, segmentation masks of an abdominal CT image, which delineate various organs (e.g., liver, kidneys, stomach, spleen, pancreas, bowel, gallbladder, etc.) are essential for accurately analyzing the anatomical structures within the abdomen that exhibit various contrasts and geometrically varied organs [15]."}, {"title": "3. Experiments", "content": ""}, {"title": "3.1. Datasets", "content": "Our proposed framework was validated using multiple medical imaging datasets including brain MRI datasets and abdominal CT datasets. The brain MRI datasets included fluid-attenuated inversion recovery (FLAIR) images from the fastMRI dataset [20] as well as T1-weighted (T1-w) images from the Brain Tumor Segmentation (BraTS) datasets [17]. The abdominal CT datasets are from the multi-atlas labeling challenge which had been captured during portal venous contrast phase [12]. For the experiments in this paper, we use the axial brain slices from the fastMRI data, which have a spatial dimension of 320 \u00d7 320; axial brain slices from the BraTS MRI data, which have a spatial dimension of 240 x 240; and axial abdominal CT slices which have a spatial dimension of 512 \u00d7 512. To obtain the ground truth segmentation masks, S, for the fastMRI brain data, we employed the FSL's Brain Extraction Tool (BET) and the Automated Segmentation Tool (FAST) [10, 28] which uses a combination of intensity-based thresholding, surface deformation, and atlas-guided techniques. For abdominal CT data, we used the multi-atlas labeled data [12] which had used manually labeled 13 abdominal organs verified by a radiologist on a volumetric basis. For BraTS brain MRI data, we used the labeled data [17] segmented manually and approved by experienced neuro-radiologists."}, {"title": "3.2. Experiments", "content": "For super-resolution experiments, we obtained images with the full spatial resolution and retrospectively resized the images by factors of 1.5\u00d7, 2x, and 2.5\u00d7 for fastMRI brain data, 1.5\u00d7 and 2\u00d7 for BraTS MRI brain data, and 2x, 3x, and 4\u00d7 for abdominal CT data. These retrospectively obtained low-resolution images contained pixelated artifacts at various degrees. Then, we implemented the super-resolution methods and assessed the outputs both qualitatively and quantitatively (w.r.t. to root-mean-square-error (RMSE), peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) following the standard definitions in [20]). To compare the performance of our framework, we implemented several state-of-the-art INR-based algorithms: ReLU with Positional-Encoding (ReLU+P.E.) [29], Gauss [23], Multiplicative Filter Networks (MFN) [7], SIREN [27], Wavelet INR (WIRE) [24], and Fourier Reparameterization (FR) [26]. For the abdominal CT data, fastMRI brain data, and BraTS brain tumor data, the qualitative results are illustrated in Fig. 2, Fig. 3, and Fig. 4, respectively, and the quantitative scores are tabulated in"}, {"title": "3.3. Implementation", "content": "In our experiments, a 5-layer MLP was utilized as the Adaptive SIREN Network consisting of 256 neurons in each layer. To implement the constraints on the $p, q, r, s$ parameters, we enforced ReLU functions and added to the overall loss to penalize the generation of negative values [11]. We implemented the framework on a single NVIDIA GeForce RTX 3060 GPU with 12GB memory using the PyTorch framework. The optimization process involved using the Adam optimizer with a learning rate scheduler, i.e. decreasing the learning rate by 0.1 after each epoch. We set $\\beta = 1$. For convergence, ReLU+P.E., Gauss, and MFN models were trained for 2000 epochs, whereas SIREN, WIRE, FR, and SeCo-INR models were trained for 1000 epochs. All codes for reproducibility are attached to the Supplementary Materials."}, {"title": "3.4. Ablation Studies", "content": "We conducted an ablation study to demonstrate that the performance gains are indeed due to the incorporation of semantic segmentation information in the INR framework. We utilized the extended SIREN network without feeding semantic features into the model. Table 5 presents the quantitative scores for the fastMRI data at a resolution factor of 2x and they clearly indicate that the performance gains stem from the semantic features. This validates that a key strength of SeCo-INR is its ability to leverage additional strong medical image segmentation information.\nWe also compared the computational costs of each algorithm on the fastMRI brain dataset at a resolution factor of"}, {"title": "4. Results and Discussion", "content": "As seen in Fig. 2, SeCo-INR is able to achieve high-quality CT super-resolution with minimal noise and blurring. The 2x example highlights the kidney region where Seco-INR is able to reconstruct the edges and contrast information accurately, whereas Gauss and MFN methods produce noisy outputs, and SIREN and FR methods produce blurry outputs. The 3x example highlights the spinal"}, {"title": "5. Conclusion", "content": "This paper presents SeCo-INR, an INR-based medical image super-resolution method that leverages local prior information derived from a semantic segmentation mask of the underlying image. SeCo-INR enables an INR net-"}]}