{"title": "An Adaptive Open-Source Dataset Generation Framework for Machine Learning Tasks in Logic Synthesis", "authors": ["Liwei Ni", "Rui Wang", "Miao Liu", "Xingyu Meng", "Xiaoze Lin", "Junfeng Liu", "Guojie Luo", "Zhufei Chu", "Weikang Qian", "Xiaoyan Yang", "Biwei Xie", "Xingquan Li", "Huawei Li"], "abstract": "This paper introduces an adaptive logic synthesis dataset generation framework designed to enhance machine learning applications within the logic synthesis process. Unlike previous dataset generation flows that were tailored for specific tasks or lacked integrated machine learning capabilities, the proposed framework supports a comprehensive range of machine learning tasks by encapsulating the three fundamental steps of logic synthesis: Boolean representation, logic optimization, and technology mapping. It preserves the original information in the intermediate files that can be stored in both Verilog and Graphmal format. Verilog files enable semi-customizability, allowing researchers to add steps and incrementally refine the generated dataset. The framework also includes an adaptive circuit engine to facilitate the loading of GraphML files for final dataset packaging and sub-dataset extraction. The generated OpenLS-D dataset comprises 46 combinational designs from established benchmarks, totaling over 966,000 Boolean circuits, with each design containing 21,000 circuits generated from 1000 synthesis recipes, including 7000 Boolean networks, 7000 ASIC netlists, and 7000 FPGA netlists. Furthermore, OpenLS-D supports integrating newly desired data features, making it more versatile for new challenges. The utility of OpenLS-D is demonstrated through four distinct downstream tasks: circuit classification, circuit ranking, quality of results (QoR) prediction, and probability prediction. Each task highlights different internal steps of logic synthesis, with the datasets extracted and relabeled from the OpenLS-D dataset using the circuit engine. The experimental results confirm the dataset's diversity and extensive applicability. The source code and datasets are available at https://github.com/Logic-Factory/ACE/blob/master/OpenLS-D/readme.md.", "sections": [{"title": "I. INTRODUCTION", "content": "Logic synthesis is a key phase in the electronic design automation (EDA) flow of digital circuits, translating high-level specifications into a gate-level netlist. In recent years, various machine learning methodologies have been proposed, demonstrating improvements in different aspects of the logic synthesis process, including logic optimization [1], [2], [3], [4], [5], technology mapping [6], [7], [8], and formal verification [9], [10]. These machine learning-based techniques have shown their promise in improving the efficiency and quality of logic synthesis steps. In order to further develop these techniques, it is crucial to introduce more comprehensive and reliable datasets.\nPrevious benchmarks [11], [12], [13], [14], [15], [16], [17] have significantly advanced the development of EDA tools and methodologies, providing a solid foundation for testing, comparison, and enhancement. Many existing logic synthesis datasets [18], [19], [10] have been derived from these foundational benchmarks. However, these datasets are often tailored for specific tasks, resulting in redundant efforts and a lack of broadly adaptive datasets suitable for diverse applications. This limitation underscores the need for a more versatile and adaptive dataset capable of supporting a variety of machine-learning tasks in logic synthesis. Such a dataset should ideally possess the following attributes:\n\u2022 Diversity: A dataset covers a wide range of design types and categories, ensuring it can cater to a diverse array of use cases and applications;\n\u2022 Versatility: A dataset has the capacity to support various machine learning tasks, facilitating the sharing of the same dataset across different tasks;\n\u2022 Adaptivity: A dataset can adapt to different tasks, en-abling the extraction of sub-datasets tailored to specific downstream tasks.\nAdditionally, while the EDA flows like OpenLane [20], [21] are primarily aimed at facilitating the chip tape-out process, they do not inherently provide specialized support for dataset generation. This further emphasizes the demand for a dedi-"}, {"title": null, "content": "cated, adaptable dataset framework within the logic synthesis domain.\nTo address the limitations of previous dataset generation flow, we introduce an adaptive logic synthesis dataset genera-tion framework designed to support a wide range of machine learning tasks within logic synthesis. The proposed framework covers the three fundamental stages of logic synthesis: Boolean representation, Logic optimization, and Technology mapping. The comprehensive workflow includes seven distinct steps: step 1, input designs are translated into the defined bit-wise generic technology circuit; step 2, these generated circuits are then converted into And-Inverter Graphs (AIG); step 3: logic optimization recipes are applied to the converted AIGS to generate a set of variant AIGs; step 4: the set of optimized AIGs is converted into the other defined five types of Boolean networks by logic blasting; step 5: ASIC/FPGA technology mapping is performed on these Boolean networks to produce gate-level netlists; step 6: static timing analysis (STA) is conducted on these ASIC netlist for timing information; step 7: All Boolean circuits of different designs are packaged into respective items for the generated dataset. It preserves all original information in the intermediate files, which are stored in both Verilog and Graphmal format. Furthermore, it also allows for semi-customization, enabling researchers to integrate desired intermediate steps and utilize previously gen-erated Verilog files. Additionally, the framework also includes a specialized circuit engine, which was developed to facilitate effective dataset packaging and extraction of adaptive sub-datasets for multiple tasks. This circuit engine can faithfully reconstruct the original Boolean circuit information, enabling a wide range of operations to be directly applied for further processing."}, {"title": null, "content": "We have released the OpenLS-D dataset utilizing the above framework to facilitate multiple machine-learning tasks. OpenLS-D starts from 46 combinational designs from well-established benchmarks [15], [16], [17], including a diverse circuit type, such as arithmetic circuits, control circuits, and IP cores. This comprehensive dataset encompasses more than 966,000 Boolean circuits, each derived from 1,000 unique synthesis recipes. The breakdown of the dataset includes 7000 Boolean networks across 7 logic types, alongside 7000 ASIC and 7000 FPGA netlists. Moreover, evaluation scores are pre-served in JSON format alongside their corresponding Boolean circuits. To showcase the versatility of OpenLS-D, we have implemented and tested four typical machine-learning tasks within logic synthesis: circuit classification, circuit ranking, QoR prediction, and probability prediction. Each task explores unique dimensions of logic synthesis, employing datasets that are directly extracted and specifically reformatted from OpenLS-D. The experimental results substantiate the dataset's diversity and comprehensive utility, confirming its value across various machine-learning applications.\nThe contributions can be summarised as follows:\n\u2022 We crafted an adaptive logic synthesis dataset generation framework that covers three pivotal stages: Boolean rep-resentation, logic optimization, and technology mapping. This semi-customized framework supports the reuse of"}, {"title": null, "content": "intermediate files, allowing researchers to incorporate desired steps as needed;\n\u2022 We developed an adaptive circuit engine capable of loading multiple types of Boolean circuits without losing any information. This engine serves as a bridge between the framework and various downstream tasks, facilitating the extension of operations to generate desired features;\n\u2022 We introduced OpenLS-D, an adaptive logic synthesis dataset generated using our framework, designed to sup-port various machine learning tasks. This ensures that all feasible downstream tasks can derive their specific datasets directly from OpenLS-D;\n\u2022 We implemented four typical downstream tasks utilizing the OpenLS-D dataset to demonstrate its diversity and effectiveness. Moreover, the circuit ranking is a novel task in logic synthesis introduced by this study, highlighting improvements in technology mapping.\nThis paper is structured as follows: Section II provides the background and related works; Section III presents the proposed dataset generation framework; Section IV introduce the proposed OpenLS-D dataset and its key characteristics; Section V formulates the selected four downstream tasks on OpenLS-D and gives the experimental results; Section VI gives the discussion, and Section VII draws the conclusion."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "The Fig. 1 illustrates the primary steps of the logic synthesis flow The following subsections will introduce the fundamental concepts and related works."}, {"title": "A. Background", "content": "1) Boolean circuit and Functional completeness\nThe Boolean circuit C is defined as a computational graph representation with specific Boolean function. It can be for-mulated by the following: $C = (V,E),V = V_{PI} \\cup V_{LG} \\cup V_{PO}, (v_i \\rightarrow v_j) \\in E | V_i \\in V, v_j \\in V$, where $V_{PI}$ represents the primary input nodes (PIs), $V_{PO}$ represents the primary output nodes (POs), and $V_{LG}$ represents the internal logic gates. The Table I shows the used logic gates in this paper but the technology-dependent cells. Each edge $v_i \\rightarrow v_j$ in E represents the connected signals between nodes.\nFurthermore, the technology-independent Boolean circuits, also referred to as the Boolean network, primarily concentrate on the topology and Boolean function. On the other hand, the technology-dependent Boolean circuit, which represents a"}, {"title": null, "content": "gate-level netlist, incorporates physical attributes such as area, and timing. It should be noted that the sequential Boolean circuits are not discussed in this work.\nDefinition 1 (Functional completeness). A set of logical gates S is called functionally complete, if for any Boolean function f, there exists a circuit using only gates from S that can represent f.\nAs defined at Definition 1, the Boolean circuit employs a functionally complete set, thereby enabling the representation of any Boolean function through circuit type. The Table II illustrates the Boolean networks utilized in this work, includ-ing AIG, OIG, XAG, MIG, PRIMARY, and GTG. While the gate-level netlists are involved after the technology mapping.\nBoolean representation task: Different Boolean circuits, based on functional completeness, can exhibit varying perfor-mances across different stages of the EDA flow, a phenomenon known as the Boolean representation problem."}, {"title": "2) Logic Optimization and Technology Mapping", "content": "The Boolean equivalence [22] asserts that the different Boolean circuit graph structure may lead to the same Boolean function. Moreover, it is the fundamental theory for logic optimization and technology mapping. The logic optimization algorithms aim to reduce the cost of the Boolean network to improve the desired criterion (area, timing, ...). Then, the optimized Boolean networks are translated into the gate-level netlists by technology mapping with the given standard cell library. This standard cell library encompasses a functionally complete set of gate-level netlists equipped with essential physical attributes required for technology mapping.\nCircuit classification task: According to the Boolean equiv-alence theory, the Boolean networks derived from the same design have the same functionality. From this viewpoint, these Boolean networks are in the same class.\nProbability prediction task: The functionally equivalent nodes within one Boolean network can be merged to reduce the size. By computing the probability of nodes, it is possible to effectively identify and filter the functionally equivalent nodes."}, {"title": "3) Static Timing Analysis", "content": "Static Timing Analysis (STA) is a critical technique used to ensure that the gate-level netlist meets specified timing requirements. STA involves calculating the delay for each signal path within the circuit. The total delay for any path is determined by the equation:\n$path\\_delay = \\sum(gate\\_delay) + \\sum(wire\\_delay)$,\nwhere $gate\\_delay$ represents the internal delay of each gate and $wire\\_delay$ accounts for the delay of the wires between gates. The maximum path delay of the critical path (arrival time) is typically used to assess if the circuit can operate within the desired timing period.\nQoR prediction task: Different logic optimization and technology mapping configurations can lead to different QOR results. If the QoR distribution is determined, it is possible to predict the related QoR."}, {"title": "4) Graph Neural Network (GNN)", "content": "GNNs are particularly adept at handling data structured in the form of graphs, offering important insights in appli-cations where relationships and interactions are crucial. A GNN utilizes a multi-layered structure where each layer K updates a node's representation by aggregating features from its neighbors. This aggregation follows the update rule:\n$h_v^k = AGGREGATE_r({h_u^{k-1}, \\forall u \\in N(v)})$, $h_v^0 = (W_v^k \\cdot CONCAT(h_v^{k-1}h_{N(v)}^{k-1}))$.\nwhere $h_v^k$ represents the embedding of node v in depth k; the AGGREGATE is the differentiable aggregator function; N(v) represents the neighborhood nodes of node v; $W$ is the weight matrices and $o$ is the non-linearity function."}, {"title": "B. Related Work", "content": "Since the release of the \"ImageNet\" dataset [23], the field of artificial intelligence has experienced significant advance-ments in computer vision. ImageNet has enabled a variety of applications, including image classification, segmentation, and detection. This progress has led to the development of innovative AI algorithms that are transforming fields such as autonomous driving, robotics, and natural language process-ing."}, {"title": "III. ADAPTIVE DATASET GENERATION FRAMEWORK", "content": "In this section, we introduce the proposed adaptive logic synthesis dataset generation framework along with the circuit engine."}, {"title": "A. Overview", "content": "The Fig. 2 illustrates the dataset generation framework. The proposed framework covers the three fundamental steps in logic synthesis: Boolean representation, logic synthesis, and technology mapping. To streamline the process, all pro-cesses are integrated into the open-source platform, Logic-Factory [24], utilizing the TCL command environment. This framework involves 7 distinct steps, starting from the initial design input to the final dataset packaging. The first three"}, {"title": null, "content": "steps (1-3) involve preprocessing the input design to generate the generic technology circuit and its optimized AIGs. The subsequent steps (4-6) are dedicated to producing intermediate Boolean circuits derived from these optimized AIGs, including logic blasting, technology mapping, and physical design. The final step (7) packages these Boolean circuits into PyTorch format data using our specially developed circuit engine, which facilitates efficient dataset management. This systematic methodology ensures that the design inputs are processed and transformed into a comprehensive dataset, ready for various logic synthesis applications. Further details are given in the subsequent sections."}, {"title": "B. Dataset Generation Steps", "content": "Step 1: Generic Technology Circuit Synthesis. The first step involves synthesizing the generic technology circuit (GTG). Given that the source designs are provided in various for-mats such as Verilog, AIG, and BLIF, adopting GTG as the standardized representation for these inputs is crucial. This approach ensures uniform processing across different design types, facilitating more streamlined and consistent handling in subsequent stages of logic synthesis. We utilize Yosys [25] served as the frontend parser for these format designs, conse-quently, the input source designs are translated to GTG using Yosys' \"techmap\" method. The generic technology cells de-fined in GTG correspond to the basic functionally complete set of RTL intermediate language (RTLIL) within Yosys. Besides the primitive gates, GTG includes several complex gates such as NAND3, MUX21 AOI21, and OAI21, which preserve the coarse-grained attributes similarities of the RTLIL.\nNotably, similar generic technology cells are also utilized in commercial logic synthesis tools like Design Compiler for"}, {"title": null, "content": "their intermediate representations. This highlights the practical relevance of creating GTG for the input designs. We document the GTG in both Verilog and GraphML formats, facilitating further processing and exploration in subsequent processes.\nStep 2: And-Inverter Graph Generation. AIGs, composed solely of AND2 and INVERTER gates, are fundamental to most logic optimization techniques in logic synthesis due to their simplicity and structural directness. Open-source tools such as ABC [26] and LSILS [27] have implemented numer-ous logic optimization algorithms on AIG, including rewrite, balance, refactor, resubstitution, etc. To enable these optimiza-tions, we convert the GTG generated in Step 1 into AIG. This conversion leverages the GTG parser and conversion method provided by LogicFactory. Additionally, we document each AIG in binary format alongside its corresponding Verilog and GraphML files.\nStep 3: Logic Optimization Recipes. Logic optimization aims to minimize the cost of Boolean circuits. It also generates different structural Boolean circuits with different optimization configurations (the optimization sequence). These Boolean cir-cuit' variants can substantially influence the quality of results (QoR) during technology mapping and subsequent physical design processes. In this step, we utilize ABC to generate diverse structural Boolean circuits from a specific design's AIG. These Boolean circuits facilitate the exploration of the QoR distribution for a given design, which is crucial for the tasks related to QoR distribution."}, {"title": null, "content": "We utilize a comprehensive set of optimization commands frequently employed in logic optimization:\nbalance,\nrewrite, rewrite -l, rewrite -z, rewrite -l-z,\nrefactor, refactor -l, refactor -z, refactor -l -z,\nresub, resub -l, resub -z, resub -l -z.\nAdditionally, the heuristic optimization sequence such as resyn, resyn2, along with sequence exploration tasks like BOILS [28] and DRILLS [29], are performed based on the above command pool. We generate 1000 distinct optimization sequences for each input design, with each sequence randomly composed of 10 commands from the command pool. To ensure equal selection probability, the balance command is repeated, appearing four times in the command pool. These 1000 distinct optimization sequences facilitate the exploration of the different sequences on the same design as well as the same sequence on different designs, enhancing the analysis of learning of their impact on design optimization. Following this step, we generated 1000 variant AIGs for each design, with each AIG indexed according to its corresponding optimization sequence. Additionally, these parameters can be customized.\nStep 4: Logic Blasting. Logic blasting is a process designed to transform Boolean networks into various formats. The Table II shows the 6 logic types of Boolean network, including AIG, OIG, XAG, MIG, PRIMARY, and GTG. Despite the relative scarcity of optimization algorithms for these circuit types compared to AIG, logic blasting provides an avenue"}, {"title": null, "content": "to potentially generate superior gate-level netlists through technology mapping for other logic types.\nIn this process, we utilize the LSILS tool to execute the logic blasting. Initially, the AIG groups generated by ABC are translated into corresponding AIG groups using the LSILS tool. Both AIG groups maintain identical structures for each corresponding item. Subsequently, each AIG in the LSILS AIG groups is converted into other logic types through the logic blasting method, which covers the AIG by the specific standard cell library. For example, the PRIMARY circuit consists of primitive gates {NOT, AND2, NAND2, OR2, NOR2, XOR2, XNOR2}. By utilizing standard cells composed of these gates, we can generate the PRIMARY circuit of the corresponding AIG. The large gates have a higher priority during the covering process.\nThe coverage of AIG, OIG, XAG, PRIMARY, and GTG are capable of generating the necessary supergate library during the covering algorithm by technology mapping. However, the functionally complete set SMIG of the MIG circuit, {NOT, MAJ3}, are inadequate for generating a functionally complete supergate library due to the hardness of generating basic {\"and2\", \"inverter\"} set, which makes it complicated to cover the AIG to MIG by technology mapping. Instead, we employ a topological node-wise conversion method to achieve this conversion.\nTheorem 1. The logic blasting method preserves the depen-dency relationships of the original circuit."}, {"title": null, "content": "Proof 1. The logic blasting step relies on the mapping step, ensuring that nodes between the circuits, both before and after the blasting, can be precisely matched. Thus, they retain the same topological structure. The matched nodes preserve the dependency relationships. Additionally, each MAJ3 gate can represent an AND gate, and it is still feasible to meet the Theorem 1.\nLemma 1. We can equip the logic optimization capability of AIG to the other logic types through logic blasting.\nSince most logic optimization algorithms are implemented on AIGs. According to Theorem 1, we can generate similar Boolean circuits by logic blasting on AIGs. In this manner, the logic optimization capabilities of AIG are extended to other types of Boolean circuits. Following this step, we are able to generate corresponding groups of AIG, OIG, XAG, MIG, PRIMARY, and GTG Boolean circuits. All these Boolean circuits are written in Verilog and GraphML file formats.\nStep 5: Technology Mapping. For each of the 6 Boolean network groups, we generated a gate-level netlist using the same technology mapping algorithm provided by LSILS [27], specifically through its \"mapper_asic\u201d and \u201cmapper_fpga\" methods, which are based on their respective template op-eration. For ASIC technology mapping, we employed the sky130 [30] standard cell library. Similarly, FPGA technology mapping was constrained to the LUT6 cell configuration. Given that certain tasks are exclusively relevant to ABC AIG, we will also employ ABC's technology mapping algorithms for these specific instances. For ASIC technology mapping,\""}, {"title": null, "content": "we use the \"amap\" command, and for FPGA technology mapping, we apply the \"if -K 6\" command to accommodate the requirements. All resulting gate-level netlists, whether for ASIC or FPGA, are subsequently saved in both Verilog and GraphML file formats to ensure broad compatibility and facilitate downstream applications.\nStep 6: Static Timing Analysis. The primary goal of logic synthesis is to produce a better gate-level netlist that en-hances the subsequent physical design steps. Assessing the performance of the existing Boolean circuit is crucial, as the timing information significantly influences the gate-level netlist's performance and serves as a key metric for evaluating the results of logic synthesis. In this step, we utilize the static timing analysis (STA) tool provided by the open-source physical design tool, iEDA [31], to compute the arrival time information for specific ASIC gate-level netlists. For FPGA netlists, the depth of the circuit provides a precise indicator for timing evaluation, offering a dependable metric for assessing the performance of the output LUT netlist. All acquired timing information is documented in JSON format, ensuring that it is accessible for further analysis.\nStep 7: Dataset Packing To streamline the management of generated raw files, these files are organized and translated into the PyTorch files by different designs. This organization is executed based on the circuit engine, which will be discussed further in Section III-C.\nThe Fig. 3 illustrates the components of an individual item in the assembled dataset. Each item is segmented into 8 PyTorch files: \u201craw.pt\u201d, \u201cabc.aig.pt\u201d, \u201clsils.aig.pt", "lsils.oig.pt\u201d, \u201clsils.xag.pt\u201d, \u201clsils.mig.pt\u201d, \u201clsils.primary.pt\u201d, and \u201clsils.gtg.pt\u201d. The \u201craw.pt\u201d file consists of 4 files: the source design, the transformed GTG circuit, the con-verted AIG circuit, and a fixed set of 1000 optimization sequences. The \u201cabc.aig.pt\u201d file contains optimized AIGs generated using the ABC tool with the generated opti-mization sequence. Additionally, the ASIC/FPGA gate-level netlists and their corresponding QoR (timing) are also stored. The accompanying": "cl", "lsils.aig.pt": "lsils.oig.pt\u201d, \u201clsils.xag.pt", "lsils.mig.pt": "lsils.primary.pt\u201d and \u201clsils.gtg.pt\" share similar components with the \"abc.aig.pt\u201d file. However, the Boolean circuits and the technology mapping algorithms they utilize are specifically based on the LSILS tool.\nAll generated raw files undergo verification using com-"}, {"title": null, "content": "binational equivalence-checking tools. The files within the \"raw.pt\" can be checked using Yosys, while the files in the \"abc.aig.pt\" are checked by comparing the AIG circuits and their corresponding gate-level netlists through ABC. Similarly, the remaining files generated by LSILS are checked against their corresponding gate-level netlists in \u201cabc.aig.pt\".\nThis structured approach to dataset management avoids the creation of excessively large or numerous PyTorch files for any single design. It allows for selective loading of task-related PyTorch files to derive sub-datasets as needed, enhancing efficiency. The flexibility of this organization allows for the regeneration of necessary files as required, either before or after certain steps. Additionally, the process can be tailored by inserting appropriate steps between the established ones, thus customizing the flow to better meet specific requirements.\""}, {"title": "C. Circuit Engine", "content": "As mentioned in step (7) in Section III-B, the raw files generated are subsequently packaged into the dataset through the proposed circuit engine. Next, we will delve into the details and functionalities of this circuit engine.\nThe circuit engine is primarily comprised of two main parts: the \"Circuit\" class, and operations for \u201cCircuit", "Circuit": "lass within the circuit engine. Each node in the Circuit class has 5 attributes: type, name, index, fanins, and truth table. For logic type circuits as listed in Table II, the node's type and name corre-spond directly to their respective gate names such as \u201cAND2\u201d,", "AOI21": "whereas for ASIC/FPGA gate-level netlists, the node type is designated as \"CELL"}, {"title": null, "content": "standardized to 64 bits to accommodate the function of each gate. Operations on the \"Circuit\" class include operators like to_torch_geometry(circuit: Circuit), simulate(circuit: Circuit), etc. The to_torch_geometry(circuit: Circuit) operator ensures consistency in node indices with the Circuit class, facilitating various operations that leverage the torch_geometry [32] class. Meanwhile, the simulate(circuit: Circuit) operator allows for the simulation of the circuit using a range of input test cases, thereby providing valuable insights into the circuit's behavior.\nThe aforementioned Boolean circuits are stored in two formats: Verilog and GraphML. We use the load_graphml(path:str) -\u00bf Circuit operator to load the generated GraphML files into the \"Circuit\" class. The GraphML files are firstly loaded into a NetworkX [33] graph, and the circuit is constructed by traversing the nodes and edges within this graph. Since the nodes and edges in the NetworkX graph are stored separately, we establish connected signals between nodes using the \"add_fain(fanins)\" function of the Circuit class during the edge traversal after all nodes have been created. Additionally, users can easily introduce custom operations within the \u201cCircuit\u201d class to meet specific requirements, enhancing the functionality of this circuit engine.\nThe Fig. 5 illustrates the node correspondence among the three types of graphs: the Boolean circuits, the proposed \"Circuit\", and the \u201ctorch_geometry", "Circuit": "raph acts as a pivotal bridge, maintaining the original index (indexori) from the input Boolean circuit while assigning a new index (index) for internal management. The to_torch_geometry(circuit: Circuit) operation ensures that the transformed torch_geometry graph retains the same node in-dices as the Circuit"}, {"title": "IV. OPENLS-D DATASET", "content": "We have successfully generated the OpenLS-D dataset utilizing our proposed dataset generation framework. This section provides a detailed overview of the structure and characteristics of the OpenLS-D dataset."}, {"title": "A. Data Source", "content": "1) Design Selection\nThe Table III shows the source designs for OpenLS-D dataset generation. These designs are selected from established benchmarks, like IWLS2005 [15], IWLS2015 [16], and Open-Cores [17]. All the presented designs are combinational AIG, with the number of Primary Inputs (PIs) ranging from 7 to 17322, Primary Outputs (POs) from 1 to 17063, AND gates from 112 to 119908, Inverter gates from 119 to 152637, Edge signals from 483 to 488364, and depths from 11 to 10384.\nIt contains diverse types of designs, including arithmetic circuits, control circuits, and IP cores, making it a compre-hensive resource for testing and benchmarking logic synthesis"}, {"title": null, "content": "algorithms. In addition, new designs or internal steps can be added to update the generated dataset for specific demands incrementally.\n2) Designs Diversity\nThe Fig. 6 presents the cosine similarity for the source designs as listed in Table III. Each design is represented by the graph embedding computed using a combination of the heuris-tic features and features aggregated from Graph2Vec [34]:\n$feature1 = [pis, pos, ands, invs, edges, depth]$,\n$feature2 = graph2vec(circuit, dimension = 128)$,\n$feature = concatenate(feature1, feature2)$.\nThe heuristic features provide a coarse-grained view of the design, while Graph2Vec provides a deeper insight into the internal structural intricacies. The combination of these fea-tures yields a robust graph embedding for each design.\nCosine similarity calculations are conducted using the \"sklearn.metrics.pairwise.cosine_similarity\" function. To en-hance the visualization of this correlation, the similarity scores ranging from [-1, 1] are normalized to [0.3, 0.6]. Additionally, the diagonal and the top-right corner of the matrix are cleared to eliminate redundancy. The average cosine similarity score"}, {"title": null, "content": "is around 0.44. This visualization distinctly highlights the variability across different design embeddings.\n3) Dataset Generation\nThe Table IV provides a detailed comparison between the OpenABC-D and OpenLS-D datasets. The OpenLS-D dataset includes over 966,000 Boolean circuits, structured into groups where each consists of 21,000 circuits generated from 1,000 distinct synthesis recipes. This collection includes 7,000 cir-cuits across 7 Boolean network types, along with 7,000 ASIC and 7,000 FPGA netlists.\nTo optimize storage efficiency, we employed the \"zstandard\" compression tool [35], which significantly reduced the storage footprint. The entire dataset generation process, including compression, was executed over approximately 76 hours for raw file generation and 65 hours for compression, using 32 threads on an Intel Xeon Platinum 8380 CPU with a 16T SEAGATE EXOS HDD. The raw files occupy about 410 GB, while the generated PyTorch files take up about 700 GB.\nIn comparison, although the OpenABC-D dataset involves"}, {"title": null, "content": "generating a larger number of AIGs per design (30,000) from 1,500 recipes, this often results in redundancy within the generated AIGs (due to the same optimization sequence by sub-sequence extraction). Conversely, OpenLS-D adopts a more diverse approach by generating different types of Boolean networks for each design using varied logic types and synthesis sequences. This method provides a richer and more distinct dataset, better suited for various machine learning applications."}, {"title": "B. Dataset Characteristics", "content": "The QoR within the OpenLS-D dataset is differentiated into two main categories: technology-independent and technology-dependent metrics. The technology-independent QoR corre-sponds to the Boolean circuits' structure, including the node size and maximum graph depth. Meanwhile, technology-dependent QoR relates to the physical attributes such as the total area and arrival time characteristics. The technology-independent QoR is applicable for types of Boolean circuits, while the technology-dependent QoR is specific to the gate-level netlists. Since the FPGA netlist's area and timing are generally directly related to the size and depth of its graph structure, we just explore the ASIC-specific characteristics here.\nThe Fig. 7 illustrates the QoR distributions for var-ious design parts, highlighting three types of distribu-tions: technology-independent QoR for Boolean networks, technology-dependent QoR for ASIC netlists, and technology-dependent QoR for different logic types of Boolean networks' corresponding ASIC netlists. From this illustration, the fol-lowing observations can be drawn:"}, {"title": null, "content": "Observation 1: Boolean networks with the same node size and graph depth can still have different QoR distributions of their corresponding ASIC netlist. Logic optimization is more concerned about the local gain that can lead to global gain, it does not necessarily affect the size and depth of the optimized Boolean networks for many optimization operators. Although there are many same QoR of one AIG as shown in Fig. 7 (a) to (d), the area and timing distributions of their corresponding ASIC netlists are significantly dispersed. This variance is likely due to ASIC technology mapping's sensitiv-ity to the local structures within the AIG. Thus, it suggests that the QoR distribution of the logic circuit is less indicative of performance compared to that of the corresponding gate-level netlist.\nObservation 2: Different Boolean representations of one design may exhibit different behaviors. The cut-based technology mapping method is particularly sensitive to the exploration space of the local structures, where different local structures constructed into a circuit can lead to significantly different physical properties. Consequently, the technology-dependent QoR of gate-level netlist varies among different logic types of Boolean networks with the same index in OpenLS-D of one design. The Fig. 7 shows the total area and arrival time distribution for the provided logic types of Boolean networks. It highlights that there are non-overlapping regions between the convex hulls of distributions for different"}, {"title": null, "content": "types of Boolean networks. This indicates that the QoR distributions for different types of Boolean networks are not identical; in some cases, there is no overlap at all between the QoR of different Boolean representations. Furthermore, the difference between the Boolean networks' QoR distributions also varies across different designs.\nThe Fig. 8 shows the QoR distribution under the incremental recipe size of the logic optimization of one design. From this, we can get:\nObservation 3: After a certain number of optimized sequences are generated, a QoR interval typically forms, with new optimization sequences likely falling within this range. Since logic optimization recipes primarily target AIGs, and other Boolean networks are translated from AIGs through"}, {"title": null, "content": "the logic blasting method, the focus can remain on technology-independent QoR distribution for AIGs. Theoretically, these derived Boolean networks exhibit similar distributions under certain affine transformations. The Fig. 8 not only showcases the node size and graph depth distribution for a selection of source designs but also highlights the incremental QOR changes across optimization sequence milestones at 250, 50"}]}