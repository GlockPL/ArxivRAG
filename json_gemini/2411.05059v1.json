{"title": "FINETUNEBENCH: HOW WELL DO COMMERCIAL FINE-TUNING APIS INFUSE KNOWLEDGE INTO LLMS?", "authors": ["Eric Wu", "Kevin Wu", "James Zou"], "abstract": "There is great interest in fine-tuning frontier large language models (LLMs) to inject new information and update existing knowledge. While commercial LLM fine-tuning APIs from providers such as OpenAI and Google promise flexible adaptation for various applications, the efficacy of fine-tuning remains unclear. In this study, we introduce FineTuneBench, an evaluation framework and dataset for understanding how well commercial fine-tuning APIs can successfully learn new and updated knowledge. We analyze five frontier LLMs with commercially available fine-tuning APIs, including GPT-40 and Gemini 1.5 Pro, on their effec-tiveness in two settings: (1) ingesting novel information, such as recent news events and new people profiles, and (2) updating existing knowledge, such as updated medical guidelines and code frameworks. Our results reveal substantial shortcomings in all the models' abilities to effectively learn new informa-tion through fine-tuning, with an average generalization accuracy of 37% across all models. When updating existing knowledge, such as incorporating medi-cal guideline updates, commercial fine-tuning APIs show even more limited ca-pability (average generalization accuracy of 19%). Overall, fine-tuning GPT-40 mini is the most effective for infusing new knowledge and updating knowl-edge, followed by GPT-3.5 Turbo and GPT-40. The fine-tuning APIs for Gem-ini 1.5 Flesh and Gemini 1.5 Pro are unable to learn new knowledge or up-date existing knowledge. These findings underscore a major shortcoming in using current commercial fine-tuning services to achieve reliable knowledge in common scenarios. We open source the FineTuneBench dataset at https://github.com/kevinwu23/StanfordFineTuneBench.", "sections": [{"title": "1 INTRODUCTION", "content": "As LLMs are increasingly used in diverse domains such as software development Kelly (2024) and medicine Gliadkovskaya (2024), it is important they contain up-to-date and relevant knowledge. For instance, software developers need models that understand the most recent versions of code, while medical professionals need trustworthy models that adhere to current clinical guidelines. Additionally, companies that want to adapt these models for internal use need ways to introduce entirely new knowledge, such as employee information or recent news. However, currently, most frontier mod-els are closed-source 1, preventing users from applying model fine-tuning techniques themselves directly. Recently, some companies have allowed supervised fine-tuning of their proprietary models through commercial APIs, such as OpenAI's fine-tuning UI and Google Vertex AI. Users typically provide a training file with user-assistant conversation pairs and are given a custom model endpoint after fine-tuning has finished. Such offerings are appealing, as they offer users a way to adapt fron-tier models that are closed-source and computationally expensive to train. However, it is not well understood whether these fine-tuning services enable knowledge infusion Valiant (2006), or the abil-ity to learn new and updated knowledge. First, the documentation provided by these companies do not detail the type of fine-tuning methods used. For example, Google Vertex AI allows users to specify an \"adapter size\", while OpenAI does not provide any details if low-rank adaptation is used. Second, there currently do not exist uniform benchmarks to evaluate these methods and compare"}, {"title": "1.1 RELATED WORKS", "content": "There are currently several major approaches to modifying model behavior. Fine-tuning methods include supervised fine-tuning (SFT) Ouyang et al. (2022); Chung et al. (2024); Mitra et al. (2023); Chia et al. (2023); Zhou et al. (2024), which uses user-assistant paired data, reinforcement learning from human feedback (RLHF) Achiam et al. (2023); Touvron et al. (2023), which user human preference data, or continued pre-training Ovadia et al. (2023). However, the degree to which these methods allow for knowledge injection is unclear. In parallel, retrieval augmented generation (RAG) Lewis et al. (2020) is a popular method where the relevant knowledge is provided within the prompt. While RAG allows information to be directly introduced into the generation process, models do not reliably adhere to such information, especially when it conflicts with the model's pretraining knowledge Wu et al. (2024b). Additionally, RAG-based systems may still produce hallucinations or inaccuracies when provided with source contexts Wu et al. (2024a).\nKnowledge injection has been previously studied in the context of open-source LLMs. Early works explored ways to inject knowledge into models like BERT using multiple adapter models Wang et al. (2020); Lauscher et al. (2020). Another study Ovadia et al. (2023) compares the effectiveness of continued pre-training against RAG in models like Llama-2-7B and Mistral-7B. Work by Chen et al. (2024) explores selectively fine-tuning Llama-2-7B in shallow layers to target knowledge injection. However, the fine-tuning of larger commercial LLMs is not well understood.\nPrevious works have used OpenAI's commercial fine-tuning services to train GPT models on a vari-ety of tasks such as tabular data processing, Li et al. (2024), Russian text summarization, Alexandr et al. (2021), legal rule classification Liga & Robaldo (2023), biomedical classification Bousselham et al. (2024), molecular prediction tasks, Xie et al. (2024), and academic exam scoring Latif & Zhai (2024). Additionally, prior studies have analyzed how fine-tuning opens the risk of unlearning RLHF protections Zhan et al. (2023) and recapitulating private information Sun et al. (2023). While previous works have mostly focused on direct style transfer or classification tasks, our work focuses specifically on knowledge infusion. Additionally, we are the first to evaluate multiple models and fine-tuning APIs on the same dataset, allowing for a comparative study across such services."}, {"title": "2 METHODS", "content": "To assess the ability of fine-tuning to learn new information, we curate two datasets that have not previously appeared in the training data of any of the LLMs we evaluate (Figure 1). Dataset sum-maries are provided in Table 1, and representative examples from each dataset are shown in Figure 5."}, {"title": "2.1.1 LATEST NEWS DATASET", "content": "Our overall data generation process is based on the method described in Wu et al. (2024b). We started by pulling a random sample of news articles published in the Associated Press from September 1st, 2024 to September 30th, 2024, as this ensures that these articles do not appear in the training data of any of the models used (all of the evaluated models have a training data cutoff before 2024). From this initial pull of approximately 2000 articles, we use GPT-40 (gpt-4o-2024-08-06) as a candidate QA generator. We prompt the model to generate a question-and-answer pairing based on the content of each article. Relevant criteria in the prompt include ensuring that the fact that is being highlighted is newly introduced in the article, and is not a fact that is previously known. The question must also be fully self-contained, meaning it should not contain ambiguous references that require the full article content answer. Then, we quality-check each generated question by prompting GPT-40 again to ensure the aforementioned criteria are fulfilled. Finally, as a check to ensure that the model does not have prior knowledge of the fact, we query GPT-40 to answer the question and remove any that the model answers correctly. This process results in 277 final question/answer pairs. For the analyses conducted in this study, we subset this dataset to a random sample of 50 QA pairs. For all questions, we use the system prompt \"You are a news expert answering a question about the recent news. The answer MUST be only a short phrase or a single word. It should not be a full sentence or more than a few words.\" We also format the prompt with"}, {"title": "2.1.2 FICTIONAL PEOPLE DATASET", "content": "We procedurally generate paragraph-length descriptions of people based on generated, novel names. Each name is a randomly generated string (e.g. Falekefud Rabajevu). Each paragraph contains six facts: weight (in pounds), height (in cm), age (in years), occupation, favorite color, and city of residence, where each fact is randomly sampled from a realistic, uniform distribution. Provided below is an example paragraph:\n\"Falekefud Rabajevu weighs 126 pounds. Falekefud Rabajevu is 185 cm tall and 32 years old. Falekefud Rabajevu works as a farmer. Falekefud Rabajevu's favorite color is gold, and they live in Wokoxiber.\"\nBased on each of these six facts, we generate six question/answer pairs:\n\u2022 How many pounds does Falekefud Rabajevu weigh?\n\u2022 How tall is Falekefud Rabajevu?\n\u2022 How old is Falekefud Rabajevu?\n\u2022 What is Falekefud Rabajevu's occupation?\n\u2022 What is Falekefud Rabajevu's favorite color?\n\u2022 Where does Falekefud Rabajevu live?\nIn the analyses in this study, we generate 25 profile paragraphs and a total of 150 question/answer pairs. The generation code is provided in the Appendix. In addition to these direct questions, we generate two additional sets of derived questions based on these facts. For each fact, we generate a secondary question, which is a yes/no question based on the fact. For example, for the fact \"Falekefud Rabajevu weighs 126 pounds,\" the secondary question is \"Is Falekefud Rabajevu over 150 pounds?\". Second, we generate a comparison question, which takes a pair of facts from two different fictional individuals and asks a question that requires knowledge of both facts. For example, for a pair of facts \"Labigih Surubadew weighs 129 pounds\" and \"Faqapi Lasufodu weighs 195 pounds,\" we generate the question \"Is Labigih heavier than Faqapi?\". These two derivative questions are designed to test the model's capability to not just memorize a question/answer pairing based on a fact but to internalize the fact toward answering different questions."}, {"title": "2.1.3 MEDICAL UPDATE DATASET", "content": "To collect a realistic set of updated medical information, we first collect reference documents from NEJM Guideline Watch, UpToDate's Practice Changing UpDates, and UpToDate's What's New Section. These represent reliable and frequently used sources to track changing practice guidelines based on newly released medical evidence. For example, one such update is \"For all individuals aged six months and older, we suggest a 2024-2025 formula COVID-19 vaccine\". Next, using this set of reference documents, we use Claude Sonnet-3.5 to generate QA pairs to test each unique guideline update. Additionally, we ask the model to rephrase each QA pair as a short clinical vignette. In total, our dataset is based on 125 real guideline changes. The following is an example of questions and"}, {"title": "2.1.4 CODE UPDATE DATASET", "content": "We generate code-related questions from Scikit-Learn's repository, leveraging its popularity and the high likelihood of its inclusion in LLM training data. We use Claude Sonnet-3.5 to generate one QA pair from each Python file that tests basic understanding of an object or function. We also ask the model to perturb the name of the object or function and generate an additional coding question-answer pair with this new updated name. In total, we generate QA pairs from 200 Python files, with one example shown below:\n\u2022 Question: What is the name of the class that implements logistic regression with built-in cross-validation in Scikit-Learn?\nPrevious Answer: LogisticRegressionCV\nUpdated Answer: CrossValidatedLogisticRegression\n\u2022 Coding Question: Write a one-liner to create an instance of the logistic regression model with built-in cross-validation, using 5-fold CV and 'lbfgs' solver in Scikit-Learn.\nCoding Original Answer:\nclf = LogisticRegressionCV(cv=5, solver=\u2032lbfgs')\nCoding Updated Answer:\nclf = CrossValidatedLogisticRegression(cv=5,\nsolver='lbfgs')"}, {"title": "2.2 MODELS", "content": "We evaluate the following five models: three models from OpenAI (gpt-3.5-turbo-0125, gpt-4o-mini-2024-07-18, gpt-4o-2024-08-06), two models from Google (gemini-1.5-flash-002, gemini-1.5-pro-002). OpenAI makes fine-tuning of these three models available through their fine-tuning API. Similarly, Gemini models are available for fine-tuning through the Google Cloud platform. Each service offers varying levels of customization of the fine-tuning process; for instance, OpenAI only allows specifying the learning rate, batch size, and number of training epochs, while Google Cloud exposes the adapter size of the fine-tuning method. For all analyses, we leave and do not change the default parameter value for these fine-tuning specific hyperparameters."}, {"title": "2.3 TRAINING", "content": null}, {"title": "2.3.1 NEW KNOWLEDGE", "content": "We experiment with four techniques for inducing knowledge acquisition in the fine-tuned models. Initially, we train on the direct question/answer pairs (Direct), where the prompt is the question"}, {"title": "2.3.2 UPDATING KNOWLEDGE", "content": "To ensure that each updated fact is actually known by the model, we first evaluate each model on the entire set of questions (125 questions medical questions and 200 coding questions). We filter out coding questions which the model does not have the right answer for, and then randomly sample 50 remaining question-answer pairs for the training set. For medical questions, we only keep questions where the model does not already adhere to the updated guideline and randomly sample 50 remaining questions as well."}, {"title": "2.3.3 TRAINING DETAILS", "content": "For all training runs, we fixed a batch size of 1 and the default learning rate parameter. For the new knowledge datasets, we fine-tune models for 1, 10, 20, and 30 epochs. For updating knowledge datasets, we fine-tune for 1, 5, 10, 15, and 20 epochs (due to faster performance saturation). In small-scale experiments with varying batch sizes and learning rates, we found either no performance improvements or slight performance drops. We also find that models perform best when given between 50-150 unique facts to learn, which agrees with the recommended range from fine-tuning documentation from OpenAI and Gemini."}, {"title": "2.4 EVALUATION", "content": "Given that the model predictions can be heterogeneous across models in syntax and formatting (e.g. extra spacing, new line tokens, etc.), we opt for a flexible approach to evaluating whether the predictions are correct. To this end, take an LLM-as-a-judge approach when calculating the accuracy of the fine-tuned models. Given a predicted response and the answer, we query GPT-40 with both along with the question of whether the prediction matches the answer. Compared to exact string matching, we find that this approach is significantly better at reducing false negatives (missing correct responses). Most prediction/answer pairs are straightforward and trivial; however, this approach does catch some outlier responses for instance, in the case where the model does not output a stop token and repeatedly outputs the predicted guess. In Figures 2 and 3 and Table 2 the highest performance across all epochs was reported."}, {"title": "3 RESULTS", "content": null}, {"title": "3.1 \u039f\u03a1\u0395\u039dAI FINE-TUNED MODELS HAVE LIMITED GENERALIZATION ON NEW KNOWLEDGE TASKS", "content": "On both new knowledge datasets, the OpenAI models can almost perfectly memorize QA pairs; in other words, they are able to recapitulate the training data with near 100% accuracy after training for 30 epochs (Figure 2). However, they sometimes perform poorly on the rephrased or derivative questions, indicating that the memorization did not translate into true knowledge acquisition in many instances.\nIn the Fictional People dataset, the secondary and comparison questions are intended to test the ability of the models to make judgments based on the learned facts. For instance, where a QA pair might be \"What is the weight of Zihipel in pounds?\" with an answer \"189\", a secondary"}, {"title": "3.2 UPDATING KNOWLEDGE IS MORE CHALLENGING THAN LEARNING NEW KNOWLEDGE", "content": "We find that on average, commercial fine-tuning models on updated knowledge yields lower gener-alization performance compared to new knowledge (Figure 3). On our code dataset, OpenAI's fine-tuned models have an average accuracy of 10% on rephrased coding questions, the lowest among all four datasets. Next, OpenAI's model exhibits moderate generalization abilities on the medical dataset, with 40% accuracy on the clinical vignette test questions.\nWe posit that updating knowledge can be harder than learning new knowledge as updates require models to displace existing knowledge and propagate such changes throughout various instances of this knowledge. For example, when the name of a function is changed, the model also needs to learn to make that change consistently while producing functioning code.\nWe also hypothesize that it is easier to update medical knowledge than coding knowledge due to the model's priors. For medical questions, models may produce a distribution of potential answers (e.g. it may list several recommendations before choosing one), so updating the model's answers is a matter of guiding to choose among potential known answers. Meanwhile, in our coding dataset,"}, {"title": "3.3 GEMINI'S FINE-TUNING UNDERPERFORMED OPENAI ACROSS ALL TASKS", "content": "The Gemini models were unable to even memorize QA pairs accurately with a top accuracy of 5.0% on new knowledge after 30 epochs of fine-tuning (Figure 2). Their performance on the rephrased or derivative questions was consequently also negligible (top accuracy of 2.0%). In the Fictional People dataset, we observe that most responses defaulted to a refusal response (\"There is no known person or thing called...\") even after fine-tuning and ensuring that safety filters were not enabled. We hypothesize that the fine-tuning mechanism used for Gemini is insufficient for overcoming the initial instruction tuning that causes the model refusals in the first place. Gemini models could not memorize any coding updates or answer any of the coding test questions. On medical guideline"}, {"title": "3.4 TRAINING DYNAMICS", "content": "In Figure 4, we observe that strong memorization occurs as soon as 10 training epochs. However, performance on generalization continues to improve up to 30 epochs, indicating that the model is learning beyond simple memorization in some capacity. The performance from Gemini models did not improve with more epochs."}, {"title": "3.5 ALTERNATIVE TRAINING TECHNIQUES DID NOT IMPROVE GENERALIZATION", "content": "We found that training the models using the other three training techniques (masking, com-pletion with prompt, and completion without prompt) did not yield improved generalization ability and also resulted in lower performance on the original QA pairs (Figure 6). Of the three alternative training techniques we tested, training on masked sentences produced the highest performance on the QA pairs (average of 40%). However, its performance on the rephrased and date changed question was still low (25% and 4.7%, respectively), indicating its lack of robustness against more significant perturbations in the question phrasing. These results provide evidence that the lack of generalizability that we observe is not simply due to the specific method in which we train the model (QA pairs), but rather persists across other common training techniques."}, {"title": "4 DISCUSSION", "content": "In this study, we introduce FineTuneBench, a collection of four datasets that contain a total of 1075 questions that test a fine-tuned model's ability to learn knowledge across a diverse set of domains: news articles, descriptions of fictional people, medical guidelines, and scientific code libraries. We fine-tune five frontier LLMs from OpenAI and Google and find that, across the board, the fine-tuned models show limited to poor capabilities for knowledge acquisition, with new knowledge slightly easier to learn than updated knowledge. We show that gpt-4o-mini performs the best of these models, while the Gemini models perform the worst.\nAs LLMs grow increasingly proficient in performing various tasks, successfully adapting these models for specific use cases remains a significant challenge. On the one hand, models such as those from OpenAI and Google are only periodically updated, and their knowledge cutoff is usually staggered. This is problematic for use cases that require the most up-to-date knowledge of current events, news, or statistics. On the other hand, users often have domain-specific or internal knowledge that they wish to incorporate into their model. For instance, hospital systems may wish to update an LLM with their current medical guidelines, or software developers want the LLM to learn the latest changes made to their internal codebase. In each of these scenarios, the success and reliability of these LLM-based systems require that the model can robustly learn and generalize additional knowledge.\nWhile retrieval-augmented generation (RAG)-based systems allow for incorporating knowledge directly into the LLM prompt, several factors make this approach non-ideal: First, RAG does not scale well with the size of the knowledge corpus in terms of cost (especially across many repeated"}]}